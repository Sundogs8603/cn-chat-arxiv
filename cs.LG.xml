<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#38598;&#20013;&#24335;&#35757;&#32451;&#21644;&#20998;&#24067;&#24335;&#25191;&#34892;&#26041;&#27861;&#65292;&#20197;&#22312;&#32447;&#35299;&#20915;&#21160;&#24577;&#38556;&#30861;&#29289;&#36991;&#38556;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16659</link><description>&lt;p&gt;
&#26080;&#20154;&#26426;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21160;&#24577;&#36991;&#38556;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
UAV Pathfinding in Dynamic Obstacle Avoidance with Multi-agent Reinforcement Learning. (arXiv:2310.16659v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#38598;&#20013;&#24335;&#35757;&#32451;&#21644;&#20998;&#24067;&#24335;&#25191;&#34892;&#26041;&#27861;&#65292;&#20197;&#22312;&#32447;&#35299;&#20915;&#21160;&#24577;&#38556;&#30861;&#29289;&#36991;&#38556;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#21160;&#24577;&#21644;&#19981;&#30830;&#23450;&#30340;&#22330;&#26223;&#20013;&#22312;&#32447;&#35268;&#21010;&#26234;&#33021;&#20307;&#30340;&#21487;&#34892;&#19988;&#23433;&#20840;&#30340;&#36335;&#24452;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#38598;&#20013;&#24335;&#35757;&#32451;&#21644;&#20998;&#24067;&#24335;&#25191;&#34892;&#26041;&#27861;&#65292;&#20197;&#22312;&#32447;&#35299;&#20915;&#21160;&#24577;&#38556;&#30861;&#29289;&#36991;&#38556;&#38382;&#39064;&#12290;&#22312;&#35813;&#26041;&#27861;&#20013;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#20165;&#19982;&#20013;&#22830;&#35268;&#21010;&#32773;&#25110;&#20854;&#37051;&#23621;&#36827;&#34892;&#36890;&#20449;&#65292;&#20197;&#22312;&#32447;&#35268;&#21010;&#21487;&#34892;&#19988;&#23433;&#20840;&#30340;&#36335;&#24452;&#12290;&#25105;&#20204;&#22522;&#20110;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#24605;&#24819;&#25913;&#36827;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#26234;&#33021;&#20307;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#37319;&#26679;&#21033;&#29992;&#29575;&#12290;&#22312;&#27169;&#25311;&#12289;&#23460;&#20869;&#21644;&#23460;&#22806;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent reinforcement learning based methods are significant for online planning of feasible and safe paths for agents in dynamic and uncertain scenarios. Although some methods like fully centralized and fully decentralized methods achieve a certain measure of success, they also encounter problems such as dimension explosion and poor convergence, respectively. In this paper, we propose a novel centralized training with decentralized execution method based on multi-agent reinforcement learning to solve the dynamic obstacle avoidance problem online. In this approach, each agent communicates only with the central planner or only with its neighbors, respectively, to plan feasible and safe paths online. We improve our methods based on the idea of model predictive control to increase the training efficiency and sample utilization of agents. The experimental results in both simulation, indoor, and outdoor environments validate the effectiveness of our method. The video is available at htt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#37325;&#20889;&#26041;&#27861;&#26469;&#25913;&#21892;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#33258;&#21160;&#23383;&#24149;&#27169;&#22411;&#37325;&#26032;&#26631;&#27880;&#35821;&#26009;&#24211;&#65292;&#24182;&#22312;&#37325;&#20889;&#21518;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#27169;&#22411;&#22312;&#25972;&#20307;&#22270;&#20687;&#36136;&#37327;&#26041;&#38754;&#24471;&#21040;&#20102;&#26174;&#33879;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2310.16656</link><description>&lt;p&gt;
&#19968;&#24352;&#22270;&#29255;&#32988;&#36807;&#21315;&#35328;&#19975;&#35821;&#65306;&#21407;&#21017;&#24615;&#37325;&#20889;&#25913;&#21892;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
A Picture is Worth a Thousand Words: Principled Recaptioning Improves Image Generation. (arXiv:2310.16656v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#37325;&#20889;&#26041;&#27861;&#26469;&#25913;&#21892;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#33258;&#21160;&#23383;&#24149;&#27169;&#22411;&#37325;&#26032;&#26631;&#27880;&#35821;&#26009;&#24211;&#65292;&#24182;&#22312;&#37325;&#20889;&#21518;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#27169;&#22411;&#22312;&#25972;&#20307;&#22270;&#20687;&#36136;&#37327;&#26041;&#38754;&#24471;&#21040;&#20102;&#26174;&#33879;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#31361;&#30772;&#65292;&#20351;&#24471;&#20174;&#25991;&#26412;&#25552;&#31034;&#20013;&#39640;&#36136;&#37327;&#19988;&#22810;&#26679;&#21270;&#22320;&#21512;&#25104;&#22270;&#20687;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20063;&#24120;&#24120;&#38590;&#20197;&#20934;&#30830;&#22320;&#36981;&#24490;&#20854;&#25552;&#31034;&#20013;&#30340;&#25152;&#26377;&#25351;&#20196;&#12290;&#36825;&#20123;&#27169;&#22411;&#20013;&#32477;&#22823;&#37096;&#20998;&#26159;&#22312;&#30001;&#65288;&#22270;&#20687;&#65292;&#23383;&#24149;&#65289;&#23545;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#20854;&#20013;&#22270;&#20687;&#36890;&#24120;&#26469;&#33258;&#32593;&#32476;&#65292;&#32780;&#23383;&#24149;&#21017;&#26159;&#23427;&#20204;&#30340;HTML&#26367;&#20195;&#25991;&#26412;&#12290;&#19968;&#20010;&#26174;&#33879;&#30340;&#20363;&#23376;&#26159;LAION&#25968;&#25454;&#38598;&#65292;&#34987;Stable Diffusion&#21644;&#20854;&#20182;&#27169;&#22411;&#20351;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#23383;&#24149;&#36890;&#24120;&#36136;&#37327;&#36739;&#20302;&#65292;&#24182;&#35748;&#20026;&#36825;&#26174;&#33879;&#24433;&#21709;&#20102;&#27169;&#22411;&#29702;&#35299;&#25991;&#26412;&#25552;&#31034;&#20013;&#24494;&#22937;&#35821;&#20041;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#33258;&#21160;&#23383;&#24149;&#27169;&#22411;&#37325;&#26032;&#26631;&#27880;&#35821;&#26009;&#24211;&#65292;&#24182;&#22312;&#37325;&#20889;&#21518;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#27169;&#22411;&#22312;&#21508;&#20010;&#26041;&#38754;&#37117;&#20250;&#24471;&#21040;&#22823;&#24133;&#24230;&#30340;&#25913;&#21892;&#12290;&#39318;&#20808;&#65292;&#22312;&#25972;&#20307;&#22270;&#20687;&#36136;&#37327;&#26041;&#38754;&#65306;&#20363;&#22914;FID 14.84 vs. t
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models achieved a remarkable leap in capabilities over the last few years, enabling high-quality and diverse synthesis of images from a textual prompt. However, even the most advanced models often struggle to precisely follow all of the directions in their prompts. The vast majority of these models are trained on datasets consisting of (image, caption) pairs where the images often come from the web, and the captions are their HTML alternate text. A notable example is the LAION dataset, used by Stable Diffusion and other models. In this work we observe that these captions are often of low quality, and argue that this significantly affects the model's capability to understand nuanced semantics in the textual prompts. We show that by relabeling the corpus with a specialized automatic captioning model and training a text-to-image model on the recaptioned dataset, the model benefits substantially across the board. First, in overall image quality: e.g. FID 14.84 vs. t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ReBis&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#26080;&#22870;&#21169;&#25511;&#21046;&#20449;&#24687;&#21644;&#22870;&#21169;&#29305;&#23450;&#30693;&#35782;&#65292;&#26469;&#25429;&#25417;&#22270;&#20687;&#20013;&#30340;&#25511;&#21046;&#20013;&#24515;&#20449;&#24687;&#12290;ReBis&#21033;&#29992;&#21464;&#24418;&#22120;&#26550;&#26500;&#26469;&#24314;&#27169;&#21160;&#21147;&#23398;&#65292;&#24182;&#36890;&#36807;&#20998;&#22359;&#25513;&#30721;&#28040;&#38500;&#26102;&#31354;&#20887;&#20313;&#12290;&#27492;&#22806;&#65292;ReBis&#36824;&#32467;&#21512;&#20102;&#31561;&#20223;&#20989;&#24335;&#25439;&#22833;&#21644;&#38750;&#23545;&#31216;&#37325;&#24314;&#25439;&#22833;&#65292;&#20197;&#38450;&#27490;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#29305;&#24449;&#23849;&#28291;&#12290;</title><link>http://arxiv.org/abs/2310.16655</link><description>&lt;p&gt;
&#20174;&#22270;&#20687;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#30340;&#25511;&#21046;&#20013;&#24515;&#34920;&#31034;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Control-Centric Representations in Reinforcement Learning from Images. (arXiv:2310.16655v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ReBis&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#26080;&#22870;&#21169;&#25511;&#21046;&#20449;&#24687;&#21644;&#22870;&#21169;&#29305;&#23450;&#30693;&#35782;&#65292;&#26469;&#25429;&#25417;&#22270;&#20687;&#20013;&#30340;&#25511;&#21046;&#20013;&#24515;&#20449;&#24687;&#12290;ReBis&#21033;&#29992;&#21464;&#24418;&#22120;&#26550;&#26500;&#26469;&#24314;&#27169;&#21160;&#21147;&#23398;&#65292;&#24182;&#36890;&#36807;&#20998;&#22359;&#25513;&#30721;&#28040;&#38500;&#26102;&#31354;&#20887;&#20313;&#12290;&#27492;&#22806;&#65292;ReBis&#36824;&#32467;&#21512;&#20102;&#31561;&#20223;&#20989;&#24335;&#25439;&#22833;&#21644;&#38750;&#23545;&#31216;&#37325;&#24314;&#25439;&#22833;&#65292;&#20197;&#38450;&#27490;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#29305;&#24449;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#30340;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#39033;&#23454;&#38469;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#22312;&#20110;&#25552;&#21462;&#25511;&#21046;&#20013;&#24515;&#30340;&#34920;&#31034;&#65292;&#21516;&#26102;&#24573;&#30053;&#19981;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#34429;&#28982;&#36981;&#24490;&#31561;&#20223;&#20989;&#24335;&#21407;&#21017;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#38754;&#20020;&#30528;&#28508;&#22312;&#21160;&#21147;&#23398;&#30340;&#26377;&#38480;&#34920;&#36798;&#33021;&#21147;&#21644;&#36866;&#24212;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ReBis&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;&#20813;&#22870;&#21169;&#25511;&#21046;&#20449;&#24687;&#19982;&#22870;&#21169;&#29305;&#23450;&#30693;&#35782;&#38598;&#25104;&#26469;&#25429;&#25417;&#25511;&#21046;&#20013;&#24515;&#20449;&#24687;&#12290;ReBis&#21033;&#29992;&#21464;&#24418;&#22120;&#26550;&#26500;&#38544;&#24335;&#24314;&#27169;&#21160;&#21147;&#23398;&#65292;&#24182;&#32467;&#21512;&#20998;&#22359;&#25513;&#30721;&#28040;&#38500;&#26102;&#31354;&#20887;&#20313;&#12290;&#27492;&#22806;&#65292;ReBis&#23558;&#31561;&#20223;&#20989;&#24335;&#25439;&#22833;&#19982;&#38750;&#23545;&#31216;&#37325;&#24314;&#25439;&#22833;&#30456;&#32467;&#21512;&#65292;&#20197;&#38450;&#27490;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#29305;&#24449;&#23849;&#28291;&#12290;&#22312;Atari&#28216;&#25103;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Image-based Reinforcement Learning is a practical yet challenging task. A major hurdle lies in extracting control-centric representations while disregarding irrelevant information. While approaches that follow the bisimulation principle exhibit the potential in learning state representations to address this issue, they still grapple with the limited expressive capacity of latent dynamics and the inadaptability to sparse reward environments. To address these limitations, we introduce ReBis, which aims to capture control-centric information by integrating reward-free control information alongside reward-specific knowledge. ReBis utilizes a transformer architecture to implicitly model the dynamics and incorporates block-wise masking to eliminate spatiotemporal redundancy. Moreover, ReBis combines bisimulation-based loss with asymmetric reconstruction loss to prevent feature collapse in environments with sparse rewards. Empirical studies on two large benchmarks, including Atari games and D
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#23545;&#19978;&#34892;&#21644;&#19979;&#34892;&#36890;&#20449;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#40065;&#26834;&#24615;&#21462;&#20915;&#20110;&#23458;&#25143;&#31471;&#25968;&#37327;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#20540;&#33539;&#22260;&#12290;&#23454;&#39564;&#35777;&#26126;&#19978;&#34892;&#36890;&#20449;&#21487;&#20197;&#23481;&#24525;&#26356;&#39640;&#30340;&#35823;&#30721;&#29575;&#27604;&#19979;&#34892;&#36890;&#20449;&#12290;</title><link>http://arxiv.org/abs/2310.16652</link><description>&lt;p&gt;
Federated Learning&#22312;&#36890;&#20449;&#38169;&#35823;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#26377;&#22810;&#24378;&#65311;&#26469;&#33258;&#19978;&#34892;&#21644;&#19979;&#34892;&#36890;&#36947;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
How Robust is Federated Learning to Communication Error? A Comparison Study Between Uplink and Downlink Channels. (arXiv:2310.16652v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#23545;&#19978;&#34892;&#21644;&#19979;&#34892;&#36890;&#20449;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#40065;&#26834;&#24615;&#21462;&#20915;&#20110;&#23458;&#25143;&#31471;&#25968;&#37327;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#20540;&#33539;&#22260;&#12290;&#23454;&#39564;&#35777;&#26126;&#19978;&#34892;&#36890;&#20449;&#21487;&#20197;&#23481;&#24525;&#26356;&#39640;&#30340;&#35823;&#30721;&#29575;&#27604;&#19979;&#34892;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#20445;&#25252;&#38544;&#31169;&#30340;&#33021;&#21147;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24403;&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#23454;&#29616;&#26102;&#65292;FL&#33021;&#22815;&#23481;&#24525;&#22810;&#23569;&#36890;&#20449;&#38169;&#35823;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;FL&#23545;&#19978;&#34892;&#21644;&#19979;&#34892;&#36890;&#20449;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#25581;&#31034;&#20102;&#40065;&#26834;&#24615;&#21462;&#20915;&#20110;&#20004;&#20010;&#20851;&#38190;&#21442;&#25968;&#65292;&#21363;&#23458;&#25143;&#31471;&#25968;&#37327;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#20540;&#33539;&#22260;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;FL&#30340;&#19978;&#34892;&#36890;&#20449;&#21487;&#20197;&#23481;&#24525;&#26356;&#39640;&#30340;&#35823;&#30721;&#29575;&#65288;BER&#65289;&#27604;&#19979;&#34892;&#36890;&#20449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#37327;&#30340;&#24046;&#24322;&#20844;&#24335;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#36825;&#20123;&#21457;&#29616;&#21644;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Because of its privacy-preserving capability, federated learning (FL) has attracted significant attention from both academia and industry. However, when being implemented over wireless networks, it is not clear how much communication error can be tolerated by FL. This paper investigates the robustness of FL to the uplink and downlink communication error. Our theoretical analysis reveals that the robustness depends on two critical parameters, namely the number of clients and the numerical range of model parameters. It is also shown that the uplink communication in FL can tolerate a higher bit error rate (BER) than downlink communication, and this difference is quantified by a proposed formula. The findings and theoretical analyses are further validated by extensive experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21253;&#21547;&#32570;&#22833;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#27491;&#21017;&#21270;&#32534;&#30721;&#22120;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#21518;&#39564;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#32570;&#22833;&#20540;&#35774;&#32622;&#19979;&#21487;&#20197;&#25552;&#39640;&#37325;&#24314;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16648</link><description>&lt;p&gt;
&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#20013;&#32570;&#22833;&#25968;&#25454;&#30340;&#21518;&#39564;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Posterior Consistency for Missing Data in Variational Autoencoders. (arXiv:2310.16648v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21253;&#21547;&#32570;&#22833;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#27491;&#21017;&#21270;&#32534;&#30721;&#22120;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#21518;&#39564;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#32570;&#22833;&#20540;&#35774;&#32622;&#19979;&#21487;&#20197;&#25552;&#39640;&#37325;&#24314;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20174;&#21253;&#21547;&#32570;&#22833;&#20540;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#65292;&#21363;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#23436;&#25972;&#25968;&#25454;&#36890;&#24120;&#26080;&#27861;&#33719;&#24471;&#25110;&#25104;&#26412;&#22826;&#39640;&#65292;&#36825;&#31181;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#29616;&#23454;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#25913;&#36827;VAE&#30340;&#25674;&#38144;&#21518;&#39564;&#25512;&#26029;&#65292;&#21363;&#22312;&#32570;&#22833;&#25968;&#25454;&#24773;&#20917;&#19979;&#21487;&#20197;&#23398;&#20064;&#21040;&#19981;&#19968;&#33268;&#21518;&#39564;&#20998;&#24067;&#30340;&#32534;&#30721;&#22120;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21518;&#39564;&#19968;&#33268;&#24615;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#27491;&#21017;&#21270;&#32534;&#30721;&#22120;&#21518;&#39564;&#20998;&#24067;&#20197;&#20419;&#36827;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25152;&#25552;&#20986;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#38754;&#23545;&#32570;&#22833;&#20540;&#26102;&#24314;&#35758;&#20102;&#19982;&#25991;&#29486;&#20013;&#36890;&#24120;&#32771;&#34385;&#30340;&#35757;&#32451;&#30446;&#26631;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#37325;&#26500;&#36136;&#37327;&#21644;d&#26041;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#25552;&#39640;&#20102;&#32570;&#22833;&#20540;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning Variational Autoencoders (VAEs), i.e., a type of deep generative model, from data with missing values. Such data is omnipresent in real-world applications of machine learning because complete data is often impossible or too costly to obtain. We particularly focus on improving a VAE's amortized posterior inference, i.e., the encoder, which in the case of missing data can be susceptible to learning inconsistent posterior distributions regarding the missingness. To this end, we provide a formal definition of posterior consistency and propose an approach for regularizing an encoder's posterior distribution which promotes this consistency. We observe that the proposed regularization suggests a different training objective than that typically considered in the literature when facing missing values. Furthermore, we empirically demonstrate that our regularization leads to improved performance in missing value settings in terms of reconstruction quality and d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#23454;&#29616;&#26356;&#28789;&#27963;&#21644;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#22312;&#40657;&#30418;&#21644;&#30333;&#30418;&#27169;&#22411;&#19978;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.16647</link><description>&lt;p&gt;
&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#32422;&#26463;&#65306;&#19968;&#31181;&#38543;&#26426;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Achieving Constraints in Neural Networks: A Stochastic Augmented Lagrangian Approach. (arXiv:2310.16647v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#23454;&#29616;&#26356;&#28789;&#27963;&#21644;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#22312;&#40657;&#30418;&#21644;&#30333;&#30418;&#27169;&#22411;&#19978;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#23545;&#20110;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#21644;&#38450;&#27490;&#36807;&#25311;&#21512;&#33267;&#20851;&#37325;&#35201;&#12290;&#22266;&#23450;&#24809;&#32602;&#26041;&#27861;&#34429;&#28982;&#24120;&#35265;&#65292;&#20294;&#32570;&#20047;&#36866;&#24212;&#24615;&#24182;&#19988;&#23545;&#36229;&#21442;&#25968;&#25935;&#24863;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DNN&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#36807;&#31243;&#26500;&#24314;&#20026;&#19968;&#20010;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#20854;&#20013;&#25968;&#25454;&#20445;&#30495;&#24230;&#39033;&#26159;&#26368;&#23567;&#21270;&#30446;&#26631;&#65292;&#27491;&#21017;&#21270;&#39033;&#20316;&#20026;&#32422;&#26463;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#38543;&#26426;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#65288;SAL&#65289;&#26041;&#27861;&#23454;&#29616;&#26356;&#21152;&#28789;&#27963;&#21644;&#39640;&#25928;&#30340;&#27491;&#21017;&#21270;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#36866;&#29992;&#20110;&#40657;&#30418;&#27491;&#21017;&#21270;&#65292;&#36824;&#22312;&#30333;&#30418;&#27169;&#22411;&#20013;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65292;&#20854;&#20013;&#26435;&#37325;&#24120;&#24120;&#21463;&#21040;&#30828;&#32422;&#26463;&#20197;&#30830;&#20445;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;MNIST&#12289;CIFAR10&#21644;CIFAR100&#25968;&#25454;&#38598;&#19978;&#30340;&#22270;&#20687;&#20998;&#31867;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;SAL&#22987;&#32456;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24230;&#21516;&#26102;&#23454;&#29616;&#26356;&#22909;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regularizing Deep Neural Networks (DNNs) is essential for improving generalizability and preventing overfitting. Fixed penalty methods, though common, lack adaptability and suffer from hyperparameter sensitivity. In this paper, we propose a novel approach to DNN regularization by framing the training process as a constrained optimization problem. Where the data fidelity term is the minimization objective and the regularization terms serve as constraints. Then, we employ the Stochastic Augmented Lagrangian (SAL) method to achieve a more flexible and efficient regularization mechanism. Our approach extends beyond black-box regularization, demonstrating significant improvements in white-box models, where weights are often subject to hard constraints to ensure interpretability. Experimental results on image-based classification on MNIST, CIFAR10, and CIFAR100 datasets validate the effectiveness of our approach. SAL consistently achieves higher Accuracy while also achieving better constrain
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#29615;&#22659;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#65292;&#24182;&#36827;&#34892;&#22810;&#27493;&#39044;&#27979;&#20197;&#20272;&#35745;&#20540;&#20989;&#25968;&#24182;&#20248;&#21270;&#31574;&#30053;&#65292;&#23637;&#31034;&#20986;&#26356;&#39640;&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#26679;&#26412;&#23481;&#37327;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.16646</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#20540;&#20272;&#35745;&#29992;&#20110;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model predictive control-based value estimation for efficient reinforcement learning. (arXiv:2310.16646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16646
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#29615;&#22659;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#65292;&#24182;&#36827;&#34892;&#22810;&#27493;&#39044;&#27979;&#20197;&#20272;&#35745;&#20540;&#20989;&#25968;&#24182;&#20248;&#21270;&#31574;&#30053;&#65292;&#23637;&#31034;&#20986;&#26356;&#39640;&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#26679;&#26412;&#23481;&#37327;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#36341;&#20013;&#23384;&#22312;&#30528;&#19982;&#34394;&#25311;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#27425;&#25968;&#25152;&#24102;&#26469;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22522;&#20110;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#23545;&#29615;&#22659;&#36827;&#34892;&#24314;&#27169;&#12290;&#22522;&#20110;&#23398;&#20064;&#21040;&#30340;&#29615;&#22659;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#36827;&#34892;&#22810;&#27493;&#39044;&#27979;&#20197;&#20272;&#35745;&#20540;&#20989;&#25968;&#24182;&#20248;&#21270;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#23398;&#20064;&#25928;&#29575;&#65292;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#20197;&#25509;&#36817;&#26368;&#20248;&#20540;&#65292;&#24182;&#19988;&#22312;&#32463;&#39564;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#38656;&#35201;&#26356;&#23569;&#30340;&#26679;&#26412;&#23481;&#37327;&#12290;&#22312;&#32463;&#20856;&#25968;&#25454;&#24211;&#21644;&#26080;&#20154;&#26426;&#21160;&#24577;&#36991;&#38556;&#22330;&#26223;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning suffers from limitations in real practices primarily due to the numbers of required interactions with virtual environments. It results in a challenging problem that we are implausible to obtain an optimal strategy only with a few attempts for many learning method. Hereby, we design an improved reinforcement learning method based on model predictive control that models the environment through a data-driven approach. Based on learned environmental model, it performs multi-step prediction to estimate the value function and optimize the policy. The method demonstrates higher learning efficiency, faster convergent speed of strategies tending to the optimal value, and fewer sample capacity space required by experience replay buffers. Experimental results, both in classic databases and in a dynamic obstacle avoidance scenario for unmanned aerial vehicle, validate the proposed approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#24565;&#38459;&#22622;&#20316;&#20026;&#25511;&#21046;&#21629;&#20196;&#39044;&#27979;&#21644;&#29992;&#25143;&#36710;&#36742;&#34892;&#20026;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#23618;&#35299;&#37322;&#39034;&#24207;&#39550;&#39542;&#22330;&#26223;&#65292;&#21516;&#26102;&#33719;&#24471;&#31454;&#20105;&#24615;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16639</link><description>&lt;p&gt;
&#39550;&#39542;&#36890;&#36807;&#27010;&#24565;&#38459;&#22622;&#65306;&#35299;&#24320;&#21487;&#35299;&#37322;&#24615;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Driving through the Concept Gridlock: Unraveling Explainability Bottlenecks. (arXiv:2310.16639v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#24565;&#38459;&#22622;&#20316;&#20026;&#25511;&#21046;&#21629;&#20196;&#39044;&#27979;&#21644;&#29992;&#25143;&#36710;&#36742;&#34892;&#20026;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#23618;&#35299;&#37322;&#39034;&#24207;&#39550;&#39542;&#22330;&#26223;&#65292;&#21516;&#26102;&#33719;&#24471;&#31454;&#20105;&#24615;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#27010;&#24565;&#38459;&#22622;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#19968;&#32452;&#20154;&#20026;&#23450;&#20041;&#30340;&#27010;&#24565;&#22312;&#27169;&#22411;&#20013;&#32534;&#30721;&#20449;&#24687;&#65292;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#22312;&#20154;&#31867;&#36741;&#21161;&#25110;&#33258;&#21160;&#39550;&#39542;&#30340;&#32972;&#26223;&#19979;&#65292;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#25509;&#21463;&#21644;&#29702;&#35299;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25152;&#20570;&#30340;&#20915;&#31574;&#65292;&#24182;&#29992;&#20110;&#21512;&#29702;&#21270;&#21644;&#35299;&#37322;&#39550;&#39542;&#21592;&#25110;&#36710;&#36742;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#27010;&#24565;&#38459;&#22622;&#20316;&#20026;&#25511;&#21046;&#21629;&#20196;&#39044;&#27979;&#21644;&#29992;&#25143;&#36710;&#36742;&#34892;&#20026;&#35299;&#37322;&#30340;&#21487;&#35270;&#29305;&#24449;&#12290;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#23618;&#65292;&#29992;&#20110;&#35299;&#37322;&#39034;&#24207;&#39550;&#39542;&#22330;&#26223;&#21516;&#26102;&#23398;&#20064;&#36710;&#36742;&#30340;&#25511;&#21046;&#21629;&#20196;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#26469;&#30830;&#23450;&#20154;&#31867;&#65288;&#25110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65289;&#23545;&#39318;&#36873;&#32541;&#38553;&#25110;&#36716;&#21521;&#21629;&#20196;&#30340;&#25913;&#21464;&#26159;&#21542;&#30001;&#22806;&#37096;&#21050;&#28608;&#25110;&#20559;&#22909;&#30340;&#25913;&#21464;&#25152;&#24341;&#23548;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#33719;&#24471;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept bottleneck models have been successfully used for explainable machine learning by encoding information within the model with a set of human-defined concepts. In the context of human-assisted or autonomous driving, explainability models can help user acceptance and understanding of decisions made by the autonomous vehicle, which can be used to rationalize and explain driver or vehicle behavior. We propose a new approach using concept bottlenecks as visual features for control command predictions and explanations of user and vehicle behavior. We learn a human-understandable concept layer that we use to explain sequential driving scenes while learning vehicle control commands. This approach can then be used to determine whether a change in a preferred gap or steering commands from a human (or autonomous vehicle) is led by an external stimulus or change in preferences. We achieve competitive performance to latent visual features while gaining interpretability within our model setup
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21327;&#21464;&#37327;&#20559;&#31227;&#19979;&#30340;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#32597;&#35265;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#26041;&#27861;&#26469;&#20943;&#36731;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#20559;&#24046;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.16638</link><description>&lt;p&gt;
&#36866;&#24212;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#21327;&#21464;&#37327;&#20559;&#31227;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Covariate Shift Adaptation Robust to Density-Ratio Estimation. (arXiv:2310.16638v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16638
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21327;&#21464;&#37327;&#20559;&#31227;&#19979;&#30340;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#32597;&#35265;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#26041;&#27861;&#26469;&#20943;&#36731;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#20559;&#24046;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#35775;&#38382;&#20855;&#26377;&#21327;&#21464;&#37327;&#21644;&#32467;&#26524;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#27979;&#35797;&#25968;&#25454;&#21482;&#21253;&#21547;&#21327;&#21464;&#37327;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#39044;&#27979;&#27979;&#35797;&#25968;&#25454;&#20013;&#32570;&#22833;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#22312;&#21327;&#21464;&#37327;&#20559;&#31227;&#19979;&#35757;&#32451;&#21442;&#25968;&#22238;&#24402;&#27169;&#22411;&#65292;&#20854;&#20013;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#21327;&#21464;&#37327;&#20998;&#24067;&#19981;&#21516;&#12290;&#23545;&#20110;&#36825;&#20010;&#38382;&#39064;&#65292;&#29616;&#26377;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#20351;&#29992;&#23494;&#24230;&#27604;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#26469;&#36827;&#34892;&#21327;&#21464;&#37327;&#20559;&#31227;&#36866;&#24212;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#35757;&#32451;&#25968;&#25454;&#25439;&#22833;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#27599;&#20010;&#26435;&#37325;&#26159;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#21327;&#21464;&#37327;&#23494;&#24230;&#27604;&#30340;&#20272;&#35745;&#65292;&#20197;&#36817;&#20284;&#27979;&#35797;&#25968;&#25454;&#30340;&#39118;&#38505;&#12290;&#23613;&#31649;&#23427;&#20801;&#35768;&#25105;&#20204;&#33719;&#24471;&#19968;&#20010;&#26368;&#23567;&#21270;&#27979;&#35797;&#25968;&#25454;&#39118;&#38505;&#30340;&#27169;&#22411;&#65292;&#20294;&#20854;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23494;&#24230;&#27604;&#21487;&#20197;&#19968;&#33268;&#22320;&#20272;&#35745;&#65292;&#23494;&#24230;&#27604;&#30340;&#20272;&#35745;&#35823;&#24046;&#20063;&#20250;&#23548;&#33268;&#22238;&#24402;&#27169;&#22411;&#30340;&#20272;&#35745;&#22120;&#20135;&#29983;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider a scenario where we have access to train data with both covariates and outcomes while test data only contains covariates. In this scenario, our primary aim is to predict the missing outcomes of the test data. With this objective in mind, we train parametric regression models under a covariate shift, where covariate distributions are different between the train and test data. For this problem, existing studies have proposed covariate shift adaptation via importance weighting using the density ratio. This approach averages the train data losses, each weighted by an estimated ratio of the covariate densities between the train and test data, to approximate the test-data risk. Although it allows us to obtain a test-data risk minimizer, its performance heavily relies on the accuracy of the density ratio estimation. Moreover, even if the density ratio can be consistently estimated, the estimation errors of the density ratio also yield bias in the estimators of the regression model's 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Copula&#29109;&#30340;&#20809;&#24230;&#32418;&#31227;&#30740;&#31350;&#26041;&#27861;&#65292;&#22312;SDSS&#31867;&#26143;&#20307;&#25968;&#25454;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#39640;Copula&#29109;&#30340;&#27979;&#37327;&#32467;&#26524;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20809;&#24230;&#32418;&#31227;&#30340;&#20934;&#30830;&#24615;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#39640;&#32418;&#31227;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.16633</link><description>&lt;p&gt;
&#22522;&#20110;Copula&#29109;&#30340;&#20809;&#24230;&#32418;&#31227;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Photometric Redshifts with Copula Entropy. (arXiv:2310.16633v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Copula&#29109;&#30340;&#20809;&#24230;&#32418;&#31227;&#30740;&#31350;&#26041;&#27861;&#65292;&#22312;SDSS&#31867;&#26143;&#20307;&#25968;&#25454;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#39640;Copula&#29109;&#30340;&#27979;&#37327;&#32467;&#26524;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20809;&#24230;&#32418;&#31227;&#30340;&#20934;&#30830;&#24615;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#39640;&#32418;&#31227;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#23558;Copula&#29109;&#65288;CE&#65289;&#24212;&#29992;&#20110;&#20809;&#24230;&#32418;&#31227;&#30740;&#31350;&#12290;CE&#29992;&#20110;&#27979;&#37327;&#20809;&#24230;&#27979;&#37327;&#21644;&#32418;&#31227;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#36873;&#25321;&#19982;&#39640;CE&#30456;&#20851;&#30340;&#27979;&#37327;&#26469;&#39044;&#27979;&#32418;&#31227;&#12290;&#25105;&#20204;&#22312;SDSS&#31867;&#26143;&#20307;&#25968;&#25454;&#19978;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#23454;&#39564;&#20013;&#20351;&#29992;&#20102;&#25152;&#26377;&#27979;&#37327;&#32467;&#26524;&#30456;&#27604;&#65292;&#36873;&#25321;&#20351;&#29992;CE&#30340;&#27979;&#37327;&#32467;&#26524;&#26174;&#33879;&#25552;&#39640;&#20102;&#20809;&#24230;&#32418;&#31227;&#30340;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#39640;&#32418;&#31227;&#26679;&#26412;&#12290;&#20351;&#29992;CE&#36873;&#25321;&#30340;&#27979;&#37327;&#32467;&#26524;&#21253;&#25324;&#20142;&#24230;&#22823;&#23567;&#65292;&#32043;&#22806;&#27874;&#27573;&#20142;&#24230;&#30340;&#26631;&#20934;&#24046;&#20197;&#21450;&#20854;&#20182;&#22235;&#20010;&#27874;&#27573;&#30340;&#20142;&#24230;&#12290;&#30001;&#20110;CE&#26159;&#19968;&#20010;&#20005;&#26684;&#23450;&#20041;&#30340;&#25968;&#23398;&#27010;&#24565;&#65292;&#22240;&#27492;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#26159;&#21487;&#35299;&#37322;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we propose to apply copula entropy (CE) to photometric redshifts. CE is used to measure the correlations between photometric measurements and redshifts and then the measurements associated with high CEs are selected for predicting redshifts. We verified the proposed method on the SDSS quasar data. Experimental results show that the accuracy of photometric redshifts is improved with the selected measurements compared to the results with all the measurements used in the experiments, especially for the samples with high redshifts. The measurements selected with CE include luminosity magnitude, the brightness in ultraviolet band with standard deviation, and the brightness of the other four bands. Since CE is a rigorously defined mathematical concept, the models such derived is interpretable.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#36807;&#31243;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#37327;&#36716;&#25442;&#20844;&#24335;&#26799;&#24230;&#30340;&#39640;&#25928;&#20272;&#35745;&#22120;&#65292;&#20811;&#26381;&#20102;&#24402;&#19968;&#21270;&#27969;&#35774;&#35745;&#22312;&#35299;&#26512;&#36870;&#21464;&#25442;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#36825;&#20351;&#24471;&#20219;&#20309;&#20445;&#25345;&#32500;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#37117;&#21487;&#20197;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#65292;&#24182;&#22312;&#20998;&#23376;&#29983;&#25104;&#21644;&#21453;&#38382;&#39064;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.16624</link><description>&lt;p&gt;
&#33258;&#30001;&#24418;&#24335;&#27969;&#21160;&#65306;&#20351;&#20219;&#20309;&#26550;&#26500;&#25104;&#20026;&#24402;&#19968;&#21270;&#27969;
&lt;/p&gt;
&lt;p&gt;
Free-form Flows: Make Any Architecture a Normalizing Flow. (arXiv:2310.16624v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#36807;&#31243;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#37327;&#36716;&#25442;&#20844;&#24335;&#26799;&#24230;&#30340;&#39640;&#25928;&#20272;&#35745;&#22120;&#65292;&#20811;&#26381;&#20102;&#24402;&#19968;&#21270;&#27969;&#35774;&#35745;&#22312;&#35299;&#26512;&#36870;&#21464;&#25442;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#36825;&#20351;&#24471;&#20219;&#20309;&#20445;&#25345;&#32500;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#37117;&#21487;&#20197;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#65292;&#24182;&#22312;&#20998;&#23376;&#29983;&#25104;&#21644;&#21453;&#38382;&#39064;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#19968;&#21270;&#27969;&#26159;&#30452;&#25509;&#26368;&#22823;&#21270;&#21487;&#33021;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#20197;&#21069;&#65292;&#24402;&#19968;&#21270;&#27969;&#30340;&#35774;&#35745;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#23545;&#35299;&#26512;&#36870;&#21464;&#25442;&#30340;&#38656;&#35201;&#38480;&#21046;&#12290;&#36890;&#36807;&#20351;&#29992;&#23545;&#21464;&#37327;&#36716;&#25442;&#20844;&#24335;&#30340;&#26799;&#24230;&#30340;&#39640;&#25928;&#20272;&#35745;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#20811;&#26381;&#20102;&#36825;&#20010;&#38480;&#21046;&#12290;&#36825;&#20351;&#24471;&#20219;&#20309;&#20445;&#25345;&#32500;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#37117;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#23558;&#37325;&#28857;&#25918;&#22312;&#31934;&#30830;&#35843;&#25972;&#24402;&#32435;&#20559;&#35265;&#20197;&#36866;&#24212;&#25163;&#22836;&#30340;&#20219;&#21153;&#19978;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#20998;&#23376;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#21033;&#29992;$E(n)$-&#31561;&#21464;&#32593;&#32476;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#20010;&#21453;&#38382;&#39064;&#22522;&#20934;&#27979;&#35797;&#20013;&#20063;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#21516;&#26102;&#37319;&#29992;&#29616;&#25104;&#30340;ResNet&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalizing Flows are generative models that directly maximize the likelihood. Previously, the design of normalizing flows was largely constrained by the need for analytical invertibility. We overcome this constraint by a training procedure that uses an efficient estimator for the gradient of the change of variables formula. This enables any dimension-preserving neural network to serve as a generative model through maximum likelihood training. Our approach allows placing the emphasis on tailoring inductive biases precisely to the task at hand. Specifically, we achieve excellent results in molecule generation benchmarks utilizing $E(n)$-equivariant networks. Moreover, our method is competitive in an inverse problem benchmark, while employing off-the-shelf ResNet architectures.
&lt;/p&gt;</description></item><item><title>SpikingJelly&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#26426;&#22120;&#23398;&#20064;&#22522;&#30784;&#35774;&#26045;&#24179;&#21488;&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#24037;&#20855;&#21644;&#21151;&#33021;&#65292;&#29992;&#20110;&#22788;&#29702;&#31070;&#32463;&#24418;&#24577;&#25968;&#25454;&#38598;&#12289;&#26500;&#24314;&#28145;&#24230;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#12289;&#20248;&#21270;&#21442;&#25968;&#24182;&#22312;&#31070;&#32463;&#24418;&#24577;&#33455;&#29255;&#19978;&#37096;&#32626;SNNs&#12290;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;SpikingJelly&#30340;&#35757;&#32451;&#36895;&#24230;&#25552;&#39640;&#20102;11&#20493;&#65292;&#24182;&#36890;&#36807;&#22810;&#32423;&#32487;&#25215;&#21644;&#21322;&#33258;&#21160;&#20195;&#30721;&#29983;&#25104;&#25552;&#20379;&#20102;&#21331;&#36234;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16620</link><description>&lt;p&gt;
SpikingJelly&#65306;&#19968;&#20010;&#29992;&#20110;&#22522;&#20110;&#33033;&#20914;&#30340;&#26234;&#33021;&#30340;&#24320;&#28304;&#26426;&#22120;&#23398;&#20064;&#22522;&#30784;&#35774;&#26045;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
SpikingJelly: An open-source machine learning infrastructure platform for spike-based intelligence. (arXiv:2310.16620v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16620
&lt;/p&gt;
&lt;p&gt;
SpikingJelly&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#26426;&#22120;&#23398;&#20064;&#22522;&#30784;&#35774;&#26045;&#24179;&#21488;&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#24037;&#20855;&#21644;&#21151;&#33021;&#65292;&#29992;&#20110;&#22788;&#29702;&#31070;&#32463;&#24418;&#24577;&#25968;&#25454;&#38598;&#12289;&#26500;&#24314;&#28145;&#24230;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#12289;&#20248;&#21270;&#21442;&#25968;&#24182;&#22312;&#31070;&#32463;&#24418;&#24577;&#33455;&#29255;&#19978;&#37096;&#32626;SNNs&#12290;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;SpikingJelly&#30340;&#35757;&#32451;&#36895;&#24230;&#25552;&#39640;&#20102;11&#20493;&#65292;&#24182;&#36890;&#36807;&#22810;&#32423;&#32487;&#25215;&#21644;&#21322;&#33258;&#21160;&#20195;&#30721;&#29983;&#25104;&#25552;&#20379;&#20102;&#21331;&#36234;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#31070;&#32463;&#21160;&#21147;&#23398;&#21644;&#33033;&#20914;&#29305;&#24615;&#65292;&#23454;&#29616;&#31070;&#32463;&#24418;&#24577;&#33455;&#29255;&#19978;&#30340;&#31867;&#33041;&#26234;&#33021;&#65292;&#20855;&#26377;&#39640;&#33021;&#25928;&#24615;&#12290;&#38543;&#30528;&#26032;&#20852;&#30340;&#33033;&#20914;&#28145;&#24230;&#23398;&#20064;&#33539;&#24335;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20256;&#32479;&#30340;&#32534;&#31243;&#26694;&#26550;&#26080;&#27861;&#28385;&#36275;&#33258;&#21160;&#24494;&#20998;&#12289;&#24182;&#34892;&#35745;&#31639;&#21152;&#36895;&#21644;&#22788;&#29702;&#31070;&#32463;&#24418;&#24577;&#25968;&#25454;&#38598;&#20197;&#21450;&#37096;&#32626;&#30340;&#35201;&#27714;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SpikingJelly&#26694;&#26550;&#26469;&#35299;&#20915;&#19978;&#36848;&#22256;&#22659;&#12290;&#25105;&#20204;&#20026;&#31070;&#32463;&#24418;&#24577;&#25968;&#25454;&#38598;&#30340;&#39044;&#22788;&#29702;&#12289;&#26500;&#24314;&#28145;&#24230;SNNs&#12289;&#20248;&#21270;&#21442;&#25968;&#20197;&#21450;&#22312;&#31070;&#32463;&#24418;&#24577;&#33455;&#29255;&#19978;&#37096;&#32626;SNNs&#36129;&#29486;&#20102;&#19968;&#20010;&#20840;&#26632;&#24037;&#20855;&#21253;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#28145;&#24230;SNNs&#30340;&#35757;&#32451;&#21487;&#20197;&#21152;&#36895;11&#20493;&#65292;&#24182;&#19988;SpikingJelly&#30340;&#21331;&#36234;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#20351;&#29992;&#25143;&#33021;&#22815;&#36890;&#36807;&#22810;&#32423;&#32487;&#25215;&#21644;&#21322;&#33258;&#21160;&#20195;&#30721;&#29983;&#25104;&#20197;&#20302;&#25104;&#26412;&#21152;&#36895;&#23450;&#21046;&#27169;&#22411;&#12290;SpikingJelly&#23637;&#31034;&#20102;&#20854;&#22312;&#22810;&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) aim to realize brain-inspired intelligence on neuromorphic chips with high energy efficiency by introducing neural dynamics and spike properties. As the emerging spiking deep learning paradigm attracts increasing interest, traditional programming frameworks cannot meet the demands of the automatic differentiation, parallel computation acceleration, and high integration of processing neuromorphic datasets and deployment. In this work, we present the SpikingJelly framework to address the aforementioned dilemma. We contribute a full-stack toolkit for pre-processing neuromorphic datasets, building deep SNNs, optimizing their parameters, and deploying SNNs on neuromorphic chips. Compared to existing methods, the training of deep SNNs can be accelerated $11\times$, and the superior extensibility and flexibility of SpikingJelly enable users to accelerate custom models at low costs through multilevel inheritance and semiautomatic code generation. SpikingJelly pav
&lt;/p&gt;</description></item><item><title>&#34920;&#28436;&#24615;&#39044;&#27979;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#36890;&#36807;&#23450;&#20041;&#21644;&#30740;&#31350;&#39044;&#27979;&#23545;&#30446;&#26631;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#23545;&#20110;&#20998;&#24067;&#21464;&#21270;&#21644;&#20248;&#21270;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.16608</link><description>&lt;p&gt;
&#34920;&#28436;&#24615;&#39044;&#27979;&#65306;&#36807;&#21435;&#19982;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
Performative Prediction: Past and Future. (arXiv:2310.16608v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16608
&lt;/p&gt;
&lt;p&gt;
&#34920;&#28436;&#24615;&#39044;&#27979;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#36890;&#36807;&#23450;&#20041;&#21644;&#30740;&#31350;&#39044;&#27979;&#23545;&#30446;&#26631;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#23545;&#20110;&#20998;&#24067;&#21464;&#21270;&#21644;&#20248;&#21270;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20250;&#19990;&#30028;&#20013;&#65292;&#39044;&#27979;&#36890;&#24120;&#20250;&#24433;&#21709;&#39044;&#27979;&#30340;&#30446;&#26631;&#65292;&#36825;&#19968;&#29616;&#35937;&#34987;&#31216;&#20026;&#34920;&#28436;&#24615;&#12290;&#33258;&#25105;&#23454;&#29616;&#21644;&#33258;&#25105;&#21542;&#23450;&#30340;&#39044;&#27979;&#26159;&#34920;&#28436;&#24615;&#30340;&#20363;&#23376;&#12290;&#23545;&#32463;&#27982;&#23398;&#12289;&#37329;&#34701;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#33267;&#20851;&#37325;&#35201;&#30340;&#27010;&#24565;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#21457;&#23637;&#20013;&#19968;&#30452;&#32570;&#22833;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#34920;&#28436;&#24615;&#36890;&#24120;&#34920;&#29616;&#20026;&#20998;&#24067;&#21464;&#21270;&#12290;&#20363;&#22914;&#65292;&#22312;&#25968;&#23383;&#24179;&#21488;&#19978;&#37096;&#32626;&#30340;&#39044;&#27979;&#27169;&#22411;&#20250;&#24433;&#21709;&#28040;&#36153;&#65292;&#20174;&#32780;&#25913;&#21464;&#25968;&#25454;&#29983;&#25104;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#26368;&#36817;&#25104;&#31435;&#30340;&#34920;&#28436;&#24615;&#39044;&#27979;&#39046;&#22495;&#65292;&#35813;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#23450;&#20041;&#21644;&#27010;&#24565;&#26694;&#26550;&#26469;&#30740;&#31350;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#34920;&#28436;&#24615;&#12290;&#34920;&#28436;&#24615;&#39044;&#27979;&#30340;&#19968;&#20010;&#32467;&#26524;&#26159;&#33258;&#28982;&#22343;&#34913;&#27010;&#24565;&#30340;&#20135;&#29983;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#26032;&#30340;&#20248;&#21270;&#25361;&#25112;&#12290;&#21478;&#19968;&#20010;&#32467;&#26524;&#26159;&#23398;&#20064;&#21644;&#25805;&#25511;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#36825;&#26159;&#34920;&#28436;&#24615;&#39044;&#27979;&#20013;&#30340;&#20004;&#31181;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictions in the social world generally influence the target of prediction, a phenomenon known as performativity. Self-fulfilling and self-negating predictions are examples of performativity. Of fundamental importance to economics, finance, and the social sciences, the notion has been absent from the development of machine learning. In machine learning applications, performativity often surfaces as distribution shift. A predictive model deployed on a digital platform, for example, influences consumption and thereby changes the data-generating distribution. We survey the recently founded area of performative prediction that provides a definition and conceptual framework to study performativity in machine learning. A consequence of performative prediction is a natural equilibrium notion that gives rise to new optimization challenges. Another consequence is a distinction between learning and steering, two mechanisms at play in performative prediction. The notion of steering is in turn i
&lt;/p&gt;</description></item><item><title>AirFL-Mem&#36890;&#36807;&#38271;&#26399;&#35760;&#24518;&#26426;&#21046;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#29942;&#39048;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#19982;&#29702;&#24819;&#36890;&#20449;&#30456;&#21516;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.16606</link><description>&lt;p&gt;
AirFL-Mem: &#36890;&#36807;&#38271;&#26399;&#35760;&#24518;&#25913;&#21892;&#36890;&#20449;&#21644;&#23398;&#20064;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
AirFL-Mem: Improving Communication-Learning Trade-Off by Long-Term Memory. (arXiv:2310.16606v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16606
&lt;/p&gt;
&lt;p&gt;
AirFL-Mem&#36890;&#36807;&#38271;&#26399;&#35760;&#24518;&#26426;&#21046;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#29942;&#39048;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#19982;&#29702;&#24819;&#36890;&#20449;&#30456;&#21516;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#36890;&#20449;&#29942;&#39048;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#8212;&#8212;AirFL-Mem&#65292;&#36890;&#36807;&#23454;&#29616;&#19968;&#31181;&#8220;&#38271;&#26399;&#8221;&#35760;&#24518;&#26426;&#21046;&#26469;&#32531;&#35299;&#28145;&#24230;&#34928;&#33853;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25910;&#25947;&#30028;&#38480;&#65292;&#32771;&#34385;&#20102;&#38271;&#26399;&#35760;&#24518;&#21644;&#29616;&#26377;&#20855;&#26377;&#30701;&#26399;&#35760;&#24518;&#30340;AirFL&#21464;&#31181;&#30340;&#24773;&#20917;&#65292;&#36866;&#29992;&#20110;&#19968;&#33324;&#30340;&#38750;&#20984;&#30446;&#26631;&#12290;&#29702;&#35770;&#35777;&#26126;&#20102;AirFL-Mem&#34920;&#29616;&#20986;&#19982;&#29702;&#24819;&#36890;&#20449;&#30340;&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65289;&#30456;&#21516;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#32780;&#29616;&#26377;&#26041;&#26696;&#30340;&#24615;&#33021;&#36890;&#24120;&#21463;&#21040;&#38169;&#35823;&#19979;&#38480;&#30340;&#38480;&#21046;&#12290;&#21033;&#29992;&#29702;&#35770;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20984;&#20248;&#21270;&#31574;&#30053;&#65292;&#29992;&#20110;&#22312;&#38647;&#21033;&#34928;&#33853;&#20449;&#36947;&#20013;&#36827;&#34892;&#21151;&#29575;&#25511;&#21046;&#30340;&#25130;&#26029;&#38408;&#20540;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#20998;&#26512;&#30340;&#20248;&#21183;&#65292;&#39564;&#35777;&#20102;&#38271;&#26399;&#35760;&#24518;&#26426;&#21046;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Addressing the communication bottleneck inherent in federated learning (FL), over-the-air FL (AirFL) has emerged as a promising solution, which is, however, hampered by deep fading conditions. In this paper, we propose AirFL-Mem, a novel scheme designed to mitigate the impact of deep fading by implementing a \emph{long-term} memory mechanism. Convergence bounds are provided that account for long-term memory, as well as for existing AirFL variants with short-term memory, for general non-convex objectives. The theory demonstrates that AirFL-Mem exhibits the same convergence rate of federated averaging (FedAvg) with ideal communication, while the performance of existing schemes is generally limited by error floors. The theoretical results are also leveraged to propose a novel convex optimization strategy for the truncation threshold used for power control in the presence of Rayleigh fading channels. Experimental results validate the analysis, confirming the advantages of a long-term memor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#26368;&#21518;&#19968;&#20844;&#37324;&#20132;&#20184;&#20013;&#30340;&#21253;&#35065;&#20002;&#22833;&#12290;&#36890;&#36807;&#22312;&#27604;&#21033;&#26102;&#36135;&#36816;&#25968;&#25454;&#19978;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#28145;&#24230;&#28151;&#21512;&#38598;&#25104;&#23398;&#20064;&#65288;DHEL&#65289;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#21487;&#24110;&#21161;&#30005;&#23376;&#21830;&#21153;&#38646;&#21806;&#21830;&#20248;&#21270;&#20915;&#31574;&#25919;&#31574;&#12290;</title><link>http://arxiv.org/abs/2310.16602</link><description>&lt;p&gt;
&#26368;&#21518;&#19968;&#20844;&#37324;&#20132;&#20184;&#20013;&#30340;&#21253;&#35065;&#20002;&#22833;&#39044;&#27979;&#65306;&#28145;&#24230;&#23398;&#20064;&#21644;&#38750;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#32467;&#21512;&#21487;&#35299;&#37322;&#24615;AI&#30340;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Parcel loss prediction in last-mile delivery: deep and non-deep approaches with insights from Explainable AI. (arXiv:2310.16602v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#26368;&#21518;&#19968;&#20844;&#37324;&#20132;&#20184;&#20013;&#30340;&#21253;&#35065;&#20002;&#22833;&#12290;&#36890;&#36807;&#22312;&#27604;&#21033;&#26102;&#36135;&#36816;&#25968;&#25454;&#19978;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#28145;&#24230;&#28151;&#21512;&#38598;&#25104;&#23398;&#20064;&#65288;DHEL&#65289;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#21487;&#24110;&#21161;&#30005;&#23376;&#21830;&#21153;&#38646;&#21806;&#21830;&#20248;&#21270;&#20915;&#31574;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#38646;&#21806;&#39046;&#22495;&#65292;&#20943;&#23569;&#26368;&#21518;&#19968;&#20844;&#37324;&#20132;&#20184;&#38454;&#27573;&#30340;&#21253;&#35065;&#20002;&#22833;&#26159;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#12290;&#21253;&#25324;&#20135;&#21697;&#12289;&#23458;&#25143;&#21644;&#35746;&#21333;&#20449;&#24687;&#22312;&#20869;&#30340;&#25968;&#25454;&#30340;&#19981;&#26029;&#22686;&#21152;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#22312;&#21253;&#35065;&#20002;&#22833;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#22825;&#28982;&#19981;&#24179;&#34913;&#38382;&#39064;&#65288;&#21363;&#21482;&#26377;&#26497;&#23569;&#37096;&#20998;&#21253;&#35065;&#20002;&#22833;&#65289;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#25968;&#25454;&#24179;&#34913;&#19982;&#26377;&#30417;&#30563;&#23398;&#20064;&#65288;DBSL&#65289;&#21644;&#28145;&#24230;&#28151;&#21512;&#38598;&#25104;&#23398;&#20064;&#65288;DHEL&#65289;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#21253;&#35065;&#20002;&#22833;&#12290;&#36825;&#20123;&#39044;&#27979;&#30340;&#23454;&#38469;&#24847;&#20041;&#22312;&#20110;&#24110;&#21161;&#30005;&#23376;&#21830;&#21153;&#38646;&#21806;&#21830;&#20248;&#21270;&#19982;&#20445;&#38505;&#30456;&#20851;&#30340;&#20915;&#31574;&#25919;&#31574;&#12290;&#25105;&#20204;&#20351;&#29992;&#27604;&#21033;&#26102;&#36135;&#36816;&#30340;&#19968;&#24180;&#25968;&#25454;&#23545;&#25152;&#25552;&#20986;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;DHEL&#27169;&#22411;&#65288;&#23558;&#21069;&#39304;&#33258;&#21160;&#32534;&#30721;&#22120;&#19982;&#38543;&#26426;&#26862;&#26519;&#38598;&#25104;&#26041;&#27861;&#30456;&#32467;&#21512;&#65289;&#30340;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within the domain of e-commerce retail, an important objective is the reduction of parcel loss during the last-mile delivery phase. The ever-increasing availability of data, including product, customer, and order information, has made it possible for the application of machine learning in parcel loss prediction. However, a significant challenge arises from the inherent imbalance in the data, i.e., only a very low percentage of parcels are lost. In this paper, we propose two machine learning approaches, namely, Data Balance with Supervised Learning (DBSL) and Deep Hybrid Ensemble Learning (DHEL), to accurately predict parcel loss. The practical implication of such predictions is their value in aiding e-commerce retailers in optimizing insurance-related decision-making policies. We conduct a comprehensive evaluation of the proposed machine learning models using one year data from Belgian shipments. The findings show that the DHEL model, which combines a feed-forward autoencoder with a ra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32452;&#21512;&#29420;&#31435;&#26174;&#33879;&#24615;&#26816;&#39564;&#26102;&#24179;&#34913;&#20013;&#24515;&#21644;&#36793;&#32536;&#25298;&#32477;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#20004;&#32773;&#24179;&#34913;&#30340;&#32452;&#21512;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.16600</link><description>&lt;p&gt;
&#22312;&#32452;&#21512;&#29420;&#31435;&#26174;&#33879;&#24615;&#26816;&#39564;&#26102;&#24179;&#34913;&#20013;&#24515;&#21644;&#36793;&#32536;&#25298;&#32477;
&lt;/p&gt;
&lt;p&gt;
Balancing central and marginal rejection when combining independent significance tests. (arXiv:2310.16600v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16600
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32452;&#21512;&#29420;&#31435;&#26174;&#33879;&#24615;&#26816;&#39564;&#26102;&#24179;&#34913;&#20013;&#24515;&#21644;&#36793;&#32536;&#25298;&#32477;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#20004;&#32773;&#24179;&#34913;&#30340;&#32452;&#21512;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21407;&#22987;&#25968;&#25454;&#19981;&#21487;&#29992;&#26102;&#65292;&#35780;&#20272;&#19968;&#32452;p&#20540;&#30340;&#26174;&#33879;&#24615;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#23558;&#23427;&#20204;&#19982;&#27719;&#38598;&#20989;&#25968;&#36827;&#34892;&#32452;&#21512;&#12290;&#36825;&#20123;&#27719;&#38598;&#30340;p&#20540;&#23558;p&#20540;&#26679;&#26412;&#36716;&#21270;&#20026;&#19968;&#20010;&#34920;&#29616;&#31867;&#20284;&#20110;&#21333;&#21464;&#37327;p&#20540;&#30340;&#21333;&#19968;&#25968;&#20540;&#12290;&#20026;&#20102;&#26126;&#30830;&#35752;&#35770;&#36825;&#20123;&#20989;&#25968;&#65292;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#20132;&#21449;&#20551;&#35774;&#65292;&#20197;&#20256;&#36798;p&#20540;&#20013;&#38750;&#38646;&#35777;&#25454;&#30340;&#24378;&#24230;&#21644;&#26222;&#36941;&#24615;&#65292;&#28982;&#21518;&#35752;&#35770;&#20102;&#24120;&#35268;&#27719;&#38598;&#20844;&#24335;&#12290;&#22312;&#29305;&#23450;&#20132;&#21449;&#20551;&#35774;&#30340;UMP&#27719;&#38598;p&#20540;&#20013;&#35266;&#23519;&#21040;&#30340;&#27169;&#24335;&#25512;&#21160;&#20102;&#23545;&#20110;&#20013;&#24515;&#21644;&#36793;&#32536;&#25298;&#32477;&#27700;&#24179;&#22312;&#945;&#22788;&#30340;&#23450;&#20041;&#21644;&#35752;&#35770;&#12290;&#35777;&#26126;&#20102;&#20013;&#24515;&#25298;&#32477;&#24635;&#26159;&#22823;&#20110;&#31561;&#20110;&#36793;&#32536;&#25298;&#32477;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#20004;&#32773;&#22312;&#27719;&#38598;&#30340;p&#20540;&#20013;&#24179;&#34913;&#30340;&#21830;&#12290;&#22522;&#20110;&#967;&#178;_&#954;&#20998;&#20301;&#25968;&#21464;&#25442;&#30340;&#32452;&#21512;&#20989;&#25968;&#34987;&#25552;&#20986;&#20197;&#25511;&#21046;&#36825;&#20010;&#21830;&#65292;&#24182;&#19988;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common approach to evaluating the significance of a collection of $p$-values combines them with a pooling function, in particular when the original data are not available. These pooled $p$-values convert a sample of $p$-values into a single number which behaves like a univariate $p$-value. To clarify discussion of these functions, a telescoping series of alternative hypotheses are introduced that communicate the strength and prevalence of non-null evidence in the $p$-values before general pooling formulae are discussed. A pattern noticed in the UMP pooled $p$-value for a particular alternative motivates the definition and discussion of central and marginal rejection levels at $\alpha$. It is proven that central rejection is always greater than or equal to marginal rejection, motivating a quotient to measure the balance between the two for pooled $p$-values. A combining function based on the $\chi^2_{\kappa}$ quantile transformation is proposed to control this quotient and shown to be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#20043;&#21069;&#30340;&#30740;&#31350;&#65292;&#23558;&#35777;&#26126;&#30340;&#33539;&#22260;&#20174;&#29420;&#31435;&#21516;&#20998;&#24067;&#26435;&#37325;&#25193;&#23637;&#21040;&#20102;&#26356;&#22823;&#30340;&#26435;&#37325;&#20998;&#24067;&#31867;&#21035;(PSEUDO-IID)&#65292;&#21253;&#25324;&#20302;&#31209;&#21644;&#31232;&#30095;&#35774;&#32622;&#12290;&#20316;&#32773;&#21457;&#29616;&#20351;&#29992;PSEUDO-IID&#20998;&#24067;&#21021;&#22987;&#21270;&#30340;&#20840;&#36830;&#25509;&#21644;&#21367;&#31215;&#32593;&#32476;&#22312;&#26041;&#24046;&#19978;&#37117;&#26159;&#31561;&#25928;&#30340;&#12290;&#36825;&#20123;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#35782;&#21035;&#26356;&#24191;&#27867;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#36793;&#30028;&#28151;&#27788;&#29366;&#24577;&#65292;&#24182;&#36827;&#34892;&#24615;&#33021;&#35843;&#20248;&#12290;</title><link>http://arxiv.org/abs/2310.16597</link><description>&lt;p&gt;
&#36229;&#36234;&#29420;&#31435;&#21516;&#20998;&#24067;&#26435;&#37325;&#65306;&#31232;&#30095;&#21644;&#20302;&#31209;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20063;&#26159;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Beyond IID weights: sparse and low-rank deep Neural Networks are also Gaussian Processes. (arXiv:2310.16597v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#20043;&#21069;&#30340;&#30740;&#31350;&#65292;&#23558;&#35777;&#26126;&#30340;&#33539;&#22260;&#20174;&#29420;&#31435;&#21516;&#20998;&#24067;&#26435;&#37325;&#25193;&#23637;&#21040;&#20102;&#26356;&#22823;&#30340;&#26435;&#37325;&#20998;&#24067;&#31867;&#21035;(PSEUDO-IID)&#65292;&#21253;&#25324;&#20302;&#31209;&#21644;&#31232;&#30095;&#35774;&#32622;&#12290;&#20316;&#32773;&#21457;&#29616;&#20351;&#29992;PSEUDO-IID&#20998;&#24067;&#21021;&#22987;&#21270;&#30340;&#20840;&#36830;&#25509;&#21644;&#21367;&#31215;&#32593;&#32476;&#22312;&#26041;&#24046;&#19978;&#37117;&#26159;&#31561;&#25928;&#30340;&#12290;&#36825;&#20123;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#35782;&#21035;&#26356;&#24191;&#27867;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#36793;&#30028;&#28151;&#27788;&#29366;&#24577;&#65292;&#24182;&#36827;&#34892;&#24615;&#33021;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#38480;&#23485;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;&#26377;&#29992;&#19988;&#21487;&#31649;&#29702;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#35768;&#22810;&#29616;&#35937;&#12290;&#20854;&#20013;&#19968;&#20010;&#20363;&#23376;&#26159;&#38543;&#26426;&#28145;&#23618;&#32593;&#32476;&#25910;&#25947;&#21040;&#39640;&#26031;&#36807;&#31243;&#65292;&#20174;&#32780;&#33021;&#22815;&#23545;&#28608;&#27963;&#20989;&#25968;&#21644;&#32593;&#32476;&#26435;&#37325;&#36873;&#25321;&#23545;&#35757;&#32451;&#21160;&#24577;&#30340;&#24433;&#21709;&#36827;&#34892;&#20005;&#26684;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;Matthews&#31561;&#20154;(2018)&#30340;&#24320;&#21019;&#24615;&#35777;&#26126;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#21021;&#22987;&#26435;&#37325;&#20998;&#24067;&#31867;&#21035;(&#25105;&#20204;&#31216;&#20043;&#20026;PSEUDO-IID)&#65292;&#20854;&#20013;&#21253;&#25324;&#29420;&#31435;&#21516;&#20998;&#24067;&#21644;&#27491;&#20132;&#26435;&#37325;&#30340;&#24050;&#26377;&#24773;&#20917;&#65292;&#20197;&#21450;&#22240;&#20854;&#35745;&#31639;&#21152;&#36895;&#20248;&#21183;&#32780;&#21463;&#21040;&#36190;&#35465;&#30340;&#26032;&#20852;&#20302;&#31209;&#21644;&#32467;&#26500;&#31232;&#30095;&#35774;&#32622;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;PSEUDO-IID&#20998;&#24067;&#21021;&#22987;&#21270;&#30340;&#20840;&#36830;&#25509;&#21644;&#21367;&#31215;&#32593;&#32476;&#22312;&#26041;&#24046;&#19978;&#37117;&#26159;&#31561;&#25928;&#30340;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#21487;&#20197;&#35782;&#21035;&#26356;&#24191;&#27867;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#36793;&#30028;&#28151;&#27788;&#29366;&#24577;&#65292;&#24182;&#35843;&#25972;&#23427;&#20204;&#30340;&#20020;&#30028;&#24615;&#65292;&#20197;&#22686;&#24378;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The infinitely wide neural network has been proven a useful and manageable mathematical model that enables the understanding of many phenomena appearing in deep learning. One example is the convergence of random deep networks to Gaussian processes that allows a rigorous analysis of the way the choice of activation function and network weights impacts the training dynamics. In this paper, we extend the seminal proof of Matthews et al. (2018) to a larger class of initial weight distributions (which we call PSEUDO-IID), including the established cases of IID and orthogonal weights, as well as the emerging low-rank and structured sparse settings celebrated for their computational speed-up benefits. We show that fully-connected and convolutional networks initialized with PSEUDO-IID distributions are all effectively equivalent up to their variance. Using our results, one can identify the Edge-of-Chaos for a broader class of neural networks and tune them at criticality in order to enhance the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36807;&#31354;&#20013;&#32852;&#21512;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#36890;&#36807;&#26080;&#32447;&#20449;&#36947;&#24191;&#25773;&#25658;&#24102;&#26412;&#22320;&#20449;&#24687;&#30340;&#27169;&#25311;&#20449;&#21495;&#23454;&#29616;&#26356;&#26032;&#31574;&#30053;&#21442;&#25968;&#65292;&#30740;&#31350;&#20102;&#22122;&#22768;&#21644;&#20449;&#36947;&#22833;&#30495;&#23545;&#31639;&#27861;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#32467;&#26524;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16592</link><description>&lt;p&gt;
&#26080;&#32447;&#32852;&#21512;&#31574;&#30053;&#26799;&#24230;&#30340;&#36807;&#31354;&#20013;&#32858;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Over-the-air Federated Policy Gradient. (arXiv:2310.16592v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36807;&#31354;&#20013;&#32852;&#21512;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#36890;&#36807;&#26080;&#32447;&#20449;&#36947;&#24191;&#25773;&#25658;&#24102;&#26412;&#22320;&#20449;&#24687;&#30340;&#27169;&#25311;&#20449;&#21495;&#23454;&#29616;&#26356;&#26032;&#31574;&#30053;&#21442;&#25968;&#65292;&#30740;&#31350;&#20102;&#22122;&#22768;&#21644;&#20449;&#36947;&#22833;&#30495;&#23545;&#31639;&#27861;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#32467;&#26524;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36807;&#31354;&#20013;&#32858;&#21512;&#22312;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#23398;&#20064;&#12289;&#20248;&#21270;&#21644;&#24863;&#30693;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36807;&#31354;&#20013;&#32852;&#21512;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20854;&#20013;&#25152;&#26377;&#30340;&#26234;&#33021;&#20307;&#21516;&#26102;&#21521;&#20849;&#20139;&#30340;&#26080;&#32447;&#20449;&#36947;&#24191;&#25773;&#25658;&#24102;&#26412;&#22320;&#20449;&#24687;&#30340;&#27169;&#25311;&#20449;&#21495;&#65292;&#20013;&#22830;&#25511;&#21046;&#22120;&#20351;&#29992;&#25509;&#25910;&#21040;&#30340;&#27719;&#24635;&#27874;&#24418;&#26469;&#26356;&#26032;&#31574;&#30053;&#21442;&#25968;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22122;&#22768;&#21644;&#20449;&#36947;&#22833;&#30495;&#23545;&#25152;&#25552;&#20986;&#31639;&#27861;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#24314;&#31435;&#20102;&#36890;&#20449;&#21644;&#37319;&#26679;&#30340;&#22797;&#26434;&#24230;&#26469;&#25214;&#21040;&#19968;&#20010;$\epsilon$-&#36817;&#20284;&#30340;&#31283;&#23450;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20123;&#20223;&#30495;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, over-the-air aggregation has been widely considered in large-scale distributed learning, optimization, and sensing. In this paper, we propose the over-the-air federated policy gradient algorithm, where all agents simultaneously broadcast an analog signal carrying local information to a common wireless channel, and a central controller uses the received aggregated waveform to update the policy parameters. We investigate the effect of noise and channel distortion on the convergence of the proposed algorithm, and establish the complexities of communication and sampling for finding an $\epsilon$-approximate stationary point. Finally, we present some simulation results to show the effectiveness of the algorithm.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#29615;&#30340;&#26102;&#38388;&#24310;&#36831;&#20648;&#22791;&#35745;&#31639;&#26041;&#26696;&#65292;&#33021;&#22815;&#21516;&#26102;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12289;&#20998;&#31867;&#21644;&#26080;&#32447;&#36890;&#36947;&#22343;&#34913;&#31561;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#27599;&#20010;&#36890;&#36947;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16588</link><description>&lt;p&gt;
&#22810;&#24182;&#34892;&#20219;&#21153;&#24310;&#36831;&#20648;&#22791;&#35745;&#31639;&#32467;&#21512;&#30789;&#24494;&#29615;&#19982;WDM&#30340;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Multi-parallel-task Time-delay Reservoir Computing combining a Silicon Microring with WDM. (arXiv:2310.16588v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16588
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#29615;&#30340;&#26102;&#38388;&#24310;&#36831;&#20648;&#22791;&#35745;&#31639;&#26041;&#26696;&#65292;&#33021;&#22815;&#21516;&#26102;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12289;&#20998;&#31867;&#21644;&#26080;&#32447;&#36890;&#36947;&#22343;&#34913;&#31561;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#27599;&#20010;&#36890;&#36947;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#29615;&#30340;&#26102;&#38388;&#24310;&#36831;&#20648;&#22791;&#35745;&#31639;&#26041;&#26696;&#65292;&#33021;&#22815;&#21516;&#26102;&#35299;&#20915;&#28041;&#21450;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12289;&#20998;&#31867;&#21644;&#26080;&#32447;&#36890;&#36947;&#22343;&#34913;&#30340;&#19977;&#20010;&#20219;&#21153;&#12290;&#22312;&#27599;&#20010;&#36827;&#34892;&#27874;&#38271;&#22797;&#29992;&#30340;&#36890;&#36947;&#19978;&#65292;&#36890;&#36807;&#20248;&#21270;&#21151;&#29575;&#21644;&#39057;&#29575;&#22833;&#35856;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We numerically demonstrate a microring-based time-delay reservoir computing scheme that simultaneously solves three tasks involving time-series prediction, classification, and wireless channel equalization. Each task performed on a wavelength-multiplexed channel achieves state-of-the-art performance with optimized power and frequency detuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#34920;&#31034;&#30340;&#32479;&#35745;&#29305;&#24615;&#65292;&#22312;&#28508;&#22312;&#34920;&#31034;&#20013;&#20351;&#29992;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#39640;&#32500;&#20551;&#35774;&#26816;&#39564;&#26469;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#29616;&#26377;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#23545;&#20302;&#32500;&#20998;&#24067;&#20551;&#35774;&#30340;&#20381;&#36182;&#21644;&#25968;&#25454;&#22495;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#20026;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16587</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#39640;&#32500;&#26816;&#39564;&#22312;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Adaptive Uncertainty Estimation via High-Dimensional Testing on Latent Representations. (arXiv:2310.16587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#34920;&#31034;&#30340;&#32479;&#35745;&#29305;&#24615;&#65292;&#22312;&#28508;&#22312;&#34920;&#31034;&#20013;&#20351;&#29992;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#39640;&#32500;&#20551;&#35774;&#26816;&#39564;&#26469;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#29616;&#26377;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#23545;&#20302;&#32500;&#20998;&#24067;&#20551;&#35774;&#30340;&#20381;&#36182;&#21644;&#25968;&#25454;&#22495;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#20026;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26088;&#22312;&#35780;&#20272;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32622;&#20449;&#24230;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#20381;&#36182;&#20110;&#20302;&#32500;&#20998;&#24067;&#20551;&#35774;&#65292;&#22240;&#27492;&#21463;&#21040;&#28508;&#22312;&#29305;&#24449;&#30340;&#39640;&#32500;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#20851;&#27880;&#20110;&#31163;&#25955;&#20998;&#31867;&#27010;&#29575;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#23548;&#33268;&#23545;&#20110;&#20854;&#20182;&#20219;&#21153;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#27492;&#22806;&#65292;&#22823;&#37096;&#20998;&#25991;&#29486;&#35201;&#27714;&#22312;&#35757;&#32451;&#26102;&#35201;&#30475;&#21040;&#22806;&#22495;&#65288;OOD&#65289;&#25968;&#25454;&#20197;&#26356;&#22909;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#38480;&#21046;&#20102;&#23454;&#38469;&#20013;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#22806;&#22495;&#25968;&#25454;&#36890;&#24120;&#26159;&#26410;&#35265;&#36807;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#39640;&#32500;&#20551;&#35774;&#27979;&#35797;&#26469;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#36825;&#21033;&#29992;&#20102;&#29305;&#24449;&#34920;&#31034;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30452;&#25509;&#22312;&#28508;&#22312;&#34920;&#31034;&#19978;&#25805;&#20316;&#65292;&#22240;&#27492;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#29305;&#24449;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty estimation aims to evaluate the confidence of a trained deep neural network. However, existing uncertainty estimation approaches rely on low-dimensional distributional assumptions and thus suffer from the high dimensionality of latent features. Existing approaches tend to focus on uncertainty on discrete classification probabilities, which leads to poor generalizability to uncertainty estimation for other tasks. Moreover, most of the literature requires seeing the out-of-distribution (OOD) data in the training for better estimation of uncertainty, which limits the uncertainty estimation performance in practice because the OOD data are typically unseen. To overcome these limitations, we propose a new framework using data-adaptive high-dimensional hypothesis testing for uncertainty estimation, which leverages the statistical properties of the feature representations. Our method directly operates on latent representations and thus does not require retraining the feature encode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#30913;&#21147;&#35745;&#38453;&#21015;&#36827;&#34892;&#30913;&#22330;&#26144;&#23556;&#65292;&#36890;&#36807;&#26032;&#39062;&#26041;&#27861;&#23558;&#30913;&#21147;&#35745;&#30340;&#20301;&#32622;&#20449;&#24687;&#32435;&#20837;&#65292;&#25552;&#39640;&#20102;&#22320;&#22270;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.16577</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#22122;&#22768;&#36755;&#20837;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#30913;&#21147;&#35745;&#38453;&#21015;&#36827;&#34892;&#30913;&#22330;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Mapping the magnetic field using a magnetometer array with noisy input Gaussian process regression. (arXiv:2310.16577v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#30913;&#21147;&#35745;&#38453;&#21015;&#36827;&#34892;&#30913;&#22330;&#26144;&#23556;&#65292;&#36890;&#36807;&#26032;&#39062;&#26041;&#27861;&#23558;&#30913;&#21147;&#35745;&#30340;&#20301;&#32622;&#20449;&#24687;&#32435;&#20837;&#65292;&#25552;&#39640;&#20102;&#22320;&#22270;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;&#38081;&#30913;&#26448;&#26009;&#20250;&#20135;&#29983;&#29615;&#22659;&#30913;&#22330;&#30340;&#25200;&#21160;&#12290;&#36890;&#36807;&#20351;&#29992;&#30913;&#21147;&#35745;&#27979;&#37327;&#21644;&#26377;&#20851;&#30913;&#21147;&#35745;&#20301;&#32622;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#23398;&#20064;&#30913;&#22330;&#30340;&#31354;&#38388;&#21464;&#21270;&#24133;&#24230;&#12290;&#28982;&#32780;&#65292;&#30913;&#21147;&#35745;&#30340;&#20301;&#32622;&#36890;&#24120;&#21482;&#26159;&#22823;&#33268;&#24050;&#30693;&#65292;&#36825;&#23545;&#30913;&#22330;&#22320;&#22270;&#30340;&#36136;&#37327;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#30913;&#21147;&#35745;&#38453;&#21015;&#26469;&#25552;&#39640;&#30913;&#22330;&#22320;&#22270;&#30340;&#36136;&#37327;&#12290;&#38453;&#21015;&#30340;&#20301;&#32622;&#22823;&#33268;&#24050;&#30693;&#65292;&#20294;&#26159;&#38453;&#21015;&#19978;&#30913;&#21147;&#35745;&#30340;&#30456;&#23545;&#20301;&#32622;&#24050;&#30693;&#12290;&#25105;&#20204;&#22312;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#20013;&#21253;&#21547;&#20102;&#36825;&#20123;&#20449;&#24687;&#65292;&#20197;&#21046;&#20316;&#29615;&#22659;&#30913;&#22330;&#30340;&#22320;&#22270;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#36136;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#22320;&#22270;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ferromagnetic materials in indoor environments give rise to disturbances in the ambient magnetic field. Maps of these magnetic disturbances can be used for indoor localisation. A Gaussian process can be used to learn the spatially varying magnitude of the magnetic field using magnetometer measurements and information about the position of the magnetometer. The position of the magnetometer, however, is frequently only approximately known. This negatively affects the quality of the magnetic field map. In this paper, we investigate how an array of magnetometers can be used to improve the quality of the magnetic field map. The position of the array is approximately known, but the relative locations of the magnetometers on the array are known. We include this information in a novel method to make a map of the ambient magnetic field. We study the properties of our method in simulation and show that our method improves the map quality. We also demonstrate the efficacy of our method with exper
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#21270;&#26680;&#25554;&#20540;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26041;&#27861;&#65292;&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#29983;&#25104;&#22823;&#35268;&#27169;&#30913;&#22330;&#22320;&#22270;&#12290;&#36890;&#36807;&#23558;&#32467;&#26500;&#21270;&#26680;&#25554;&#20540;&#19982;&#23548;&#25968;&#30456;&#32467;&#21512;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#20869;&#35745;&#31639;&#39044;&#27979;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.16574</link><description>&lt;p&gt;
&#20351;&#29992;&#32467;&#26500;&#21270;&#26680;&#25554;&#20540;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#22312;&#22823;&#35268;&#27169;&#29615;&#22659;&#20013;&#29983;&#25104;&#30913;&#22330;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Large-scale magnetic field maps using structured kernel interpolation for Gaussian process regression. (arXiv:2310.16574v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#21270;&#26680;&#25554;&#20540;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26041;&#27861;&#65292;&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#29983;&#25104;&#22823;&#35268;&#27169;&#30913;&#22330;&#22320;&#22270;&#12290;&#36890;&#36807;&#23558;&#32467;&#26500;&#21270;&#26680;&#25554;&#20540;&#19982;&#23548;&#25968;&#30456;&#32467;&#21512;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#20869;&#35745;&#31639;&#39044;&#27979;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26144;&#23556;&#31639;&#27861;&#65292;&#20351;&#29992;&#36817;&#20284;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#22238;&#24402;&#35745;&#31639;&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;&#22823;&#35268;&#27169;&#30913;&#22330;&#22320;&#22270;&#12290;&#26144;&#23556;&#29615;&#22659;&#20013;&#29615;&#22659;&#30913;&#22330;&#30340;&#31354;&#38388;&#21464;&#21270;&#21487;&#20197;&#29992;&#20110;&#23460;&#20869;&#23450;&#20301;&#31639;&#27861;&#12290;&#20026;&#20102;&#35745;&#31639;&#36825;&#26679;&#30340;&#22320;&#22270;&#65292;GP&#22238;&#24402;&#26159;&#19968;&#31181;&#36866;&#21512;&#30340;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#25552;&#20379;&#20102;&#22312;&#26032;&#20301;&#32622;&#30340;&#30913;&#22330;&#39044;&#27979;&#20197;&#21450;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#30001;&#20110;&#20840;GP&#22238;&#24402;&#30340;&#22797;&#26434;&#24230;&#38543;&#30528;&#25968;&#25454;&#28857;&#30340;&#25968;&#37327;&#21576;&#31435;&#26041;&#22686;&#38271;&#65292;&#22240;&#27492;&#23545;&#20110;GP&#30340;&#36817;&#20284;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#32467;&#26500;&#21270;&#26680;&#25554;&#20540;&#65288;SKI&#65289;&#26694;&#26550;&#19978;&#26500;&#24314;&#65292;&#36890;&#36807;&#21033;&#29992;&#39640;&#25928;&#30340;Krylov&#23376;&#31354;&#38388;&#26041;&#27861;&#21152;&#36895;&#25512;&#26029;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#24102;&#23548;&#25968;&#30340;SKI&#65288;D-SKI&#65289;&#32435;&#20837;&#20102;&#29992;&#20110;&#30913;&#22330;&#24314;&#27169;&#30340;&#26631;&#37327;&#21183;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;&#25968;&#25454;&#28857;&#22797;&#26434;&#24230;&#35745;&#31639;&#39044;&#27979;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#25311;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a mapping algorithm to compute large-scale magnetic field maps in indoor environments with approximate Gaussian process (GP) regression. Mapping the spatial variations in the ambient magnetic field can be used for localization algorithms in indoor areas. To compute such a map, GP regression is a suitable tool because it provides predictions of the magnetic field at new locations along with uncertainty quantification. Because full GP regression has a complexity that grows cubically with the number of data points, approximations for GPs have been extensively studied. In this paper, we build on the structured kernel interpolation (SKI) framework, speeding up inference by exploiting efficient Krylov subspace methods. More specifically, we incorporate SKI with derivatives (D-SKI) into the scalar potential model for magnetic field modeling and compute both predictive mean and covariance with a complexity that is linear in the data points. In our simulations, we show that our metho
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#22686;&#24378;&#30340;&#23545;&#27604;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#21644;&#36807;&#20272;&#35745;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.16566</link><description>&lt;p&gt;
&#27169;&#22411;&#22686;&#24378;&#30340;&#23545;&#27604;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#24207;&#21015;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Model-enhanced Contrastive Reinforcement Learning for Sequential Recommendation. (arXiv:2310.16566v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16566
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#22686;&#24378;&#30340;&#23545;&#27604;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#21644;&#36807;&#20272;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#65292;&#22240;&#20026;&#20854;&#28508;&#21147;&#22312;&#20110;&#20248;&#21270;&#29992;&#25143;&#30340;&#38271;&#26399;&#21442;&#19982;&#24230;&#12290;&#20174;&#24378;&#21270;&#23398;&#20064;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25512;&#33616;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#65292;&#20854;&#20013;&#25512;&#33616;&#31995;&#32479;(&#20195;&#29702;)&#21487;&#20197;&#19982;&#29992;&#25143;(&#29615;&#22659;)&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#33719;&#24471;&#21453;&#39304;(&#22870;&#21169;&#20449;&#21495;)&#12290;&#28982;&#32780;&#65292;&#20986;&#20110;&#23545;&#29992;&#25143;&#20307;&#39564;&#21644;&#23454;&#29616;&#22797;&#26434;&#24615;&#30340;&#32771;&#34385;&#65292;&#36827;&#34892;&#22312;&#32447;&#20132;&#20114;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#25105;&#20204;&#21482;&#33021;&#20351;&#29992;&#21253;&#21547;&#26377;&#38480;&#22870;&#21169;&#20449;&#21495;&#21644;&#29366;&#24577;&#36716;&#25442;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;RL&#25512;&#33616;&#32773;&#12290;&#22240;&#27492;&#65292;&#22870;&#21169;&#20449;&#21495;&#21644;&#29366;&#24577;&#36716;&#25442;&#30340;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#38750;&#24120;&#20005;&#37325;&#65292;&#32780;&#36825;&#19968;&#38382;&#39064;&#19968;&#30452;&#34987;&#29616;&#26377;&#30340;RL&#25512;&#33616;&#31995;&#32479;&#24573;&#35270;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;RL&#26041;&#27861;&#36890;&#36807;&#35797;&#38169;&#27169;&#24335;&#26469;&#23398;&#20064;&#65292;&#20294;&#22312;&#38544;&#24335;&#21453;&#39304;&#25512;&#33616;&#20219;&#21153;&#20013;&#26080;&#27861;&#33719;&#24471;&#36127;&#21453;&#39304;&#65292;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#31163;&#32447;RL&#25512;&#33616;&#32773;&#30340;&#36807;&#20272;&#35745;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#22686;&#24378;&#30340;&#23545;&#27604;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has been widely applied in recommendation systems due to its potential in optimizing the long-term engagement of users. From the perspective of RL, recommendation can be formulated as a Markov decision process (MDP), where recommendation system (agent) can interact with users (environment) and acquire feedback (reward signals).However, it is impractical to conduct online interactions with the concern on user experience and implementation complexity, and we can only train RL recommenders with offline datasets containing limited reward signals and state transitions. Therefore, the data sparsity issue of reward signals and state transitions is very severe, while it has long been overlooked by existing RL recommenders.Worse still, RL methods learn through the trial-and-error mode, but negative feedback cannot be obtained in implicit feedback recommendation tasks, which aggravates the overestimation problem of offline RL recommender. To address these challenges, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#20256;&#25773;&#30340;&#31639;&#27861;&#26469;&#22788;&#29702;&#20219;&#24847;&#24322;&#36136;&#24615;&#30340;&#22270;&#26631;&#31614;&#22122;&#22768;&#65292;&#20197;&#32416;&#27491;&#22122;&#22768;&#26631;&#31614;&#24182;&#20026;&#26410;&#26631;&#35760;&#30340;&#33410;&#28857;&#20998;&#37197;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2310.16560</link><description>&lt;p&gt;
&#22270;&#26631;&#31614;&#20256;&#25773;&#31639;&#27861;&#24212;&#23545;&#22270;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Label Propagation for Graph Label Noise. (arXiv:2310.16560v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#20256;&#25773;&#30340;&#31639;&#27861;&#26469;&#22788;&#29702;&#20219;&#24847;&#24322;&#36136;&#24615;&#30340;&#22270;&#26631;&#31614;&#22122;&#22768;&#65292;&#20197;&#32416;&#27491;&#22122;&#22768;&#26631;&#31614;&#24182;&#20026;&#26410;&#26631;&#35760;&#30340;&#33410;&#28857;&#20998;&#37197;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#22122;&#22768;&#26159;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#24120;&#35265;&#30340;&#25361;&#25112;&#65292;&#23427;&#20250;&#26174;&#33879;&#38477;&#20302;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22122;&#22768;&#26631;&#31614;&#65292;&#28982;&#32780;&#65292;&#22270;&#27169;&#22411;&#23558;&#33410;&#28857;&#29305;&#24449;&#21644;&#22270;&#25299;&#25169;&#32467;&#26500;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#26356;&#23481;&#26131;&#21463;&#21040;&#26631;&#31614;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#36817;&#26399;&#65292;&#21482;&#26377;&#23569;&#25968;&#20960;&#31687;&#25991;&#31456;&#25552;&#20986;&#20102;&#35299;&#20915;&#22270;&#20013;&#26631;&#31614;&#22122;&#22768;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#23427;&#20204;&#20551;&#35774;&#22270;&#26159;&#21516;&#26500;&#30340;&#65292;&#24182;&#19988;&#26631;&#31614;&#26159;&#24179;&#28369;&#20998;&#24067;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#21487;&#33021;&#21253;&#21547;&#19981;&#21516;&#31243;&#24230;&#30340;&#24322;&#36136;&#24615;&#29978;&#33267;&#26159;&#24322;&#36136;&#24615;&#30340;&#20027;&#23548;&#65292;&#23548;&#33268;&#24403;&#21069;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;&#26412;&#25991;&#30740;&#31350;&#20219;&#24847;&#24322;&#36136;&#24615;&#26465;&#20214;&#19979;&#30340;&#22270;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#26088;&#22312;&#32416;&#27491;&#22122;&#22768;&#26631;&#31614;&#24182;&#20026;&#20043;&#21069;&#26410;&#26631;&#35760;&#30340;&#33410;&#28857;&#20998;&#37197;&#26631;&#31614;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#20004;&#20010;&#23454;&#35777;&#20998;&#26512;&#65292;&#25506;&#35752;&#22270;&#21516;&#36136;&#24615;&#23545;&#22270;&#26631;&#31614;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#20256;&#25773;&#30340;&#31639;&#27861;&#26469;&#22788;&#29702;&#20219;&#24847;&#24322;&#36136;&#24615;&#30340;&#22270;&#26631;&#31614;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label noise is a common challenge in large datasets, as it can significantly degrade the generalization ability of deep neural networks. Most existing studies focus on noisy labels in computer vision; however, graph models encompass both node features and graph topology as input, and become more susceptible to label noise through message-passing mechanisms. Recently, only a few works have been proposed to tackle the label noise on graphs. One major limitation is that they assume the graph is homophilous and the labels are smoothly distributed. Nevertheless, real-world graphs may contain varying degrees of heterophily or even be heterophily-dominated, leading to the inadequacy of current methods. In this paper, we study graph label noise in the context of arbitrary heterophily, with the aim of rectifying noisy labels and assigning labels to previously unlabeled nodes. We begin by conducting two empirical analyses to explore the impact of graph homophily on graph label noise. Following o
&lt;/p&gt;</description></item><item><title>DECWA&#26159;&#19968;&#31181;&#20351;&#29992;Wasserstein&#36317;&#31163;&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#26032;&#30340;&#32676;&#38598;&#29305;&#24449;&#25551;&#36848;&#21644;&#32858;&#31867;&#31639;&#27861;&#22312;&#20219;&#24847;&#24418;&#29366;&#30340;&#32676;&#38598;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#35299;&#20915;&#20102;&#20302;&#23494;&#24230;&#32676;&#38598;&#12289;&#30456;&#20284;&#23494;&#24230;&#32676;&#38598;&#21644;&#39640;&#32500;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.16552</link><description>&lt;p&gt;
DECWA: &#20351;&#29992;Wasserstein&#36317;&#31163;&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DECWA : Density-Based Clustering using Wasserstein Distance. (arXiv:2310.16552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16552
&lt;/p&gt;
&lt;p&gt;
DECWA&#26159;&#19968;&#31181;&#20351;&#29992;Wasserstein&#36317;&#31163;&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#26032;&#30340;&#32676;&#38598;&#29305;&#24449;&#25551;&#36848;&#21644;&#32858;&#31867;&#31639;&#27861;&#22312;&#20219;&#24847;&#24418;&#29366;&#30340;&#32676;&#38598;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#35299;&#20915;&#20102;&#20302;&#23494;&#24230;&#32676;&#38598;&#12289;&#30456;&#20284;&#23494;&#24230;&#32676;&#38598;&#21644;&#39640;&#32500;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#26159;&#19968;&#31181;&#36890;&#36807;&#21457;&#29616;&#25968;&#25454;&#32452;&#20043;&#38388;&#30340;&#32676;&#38598;&#26469;&#25552;&#21462;&#30693;&#35782;&#30340;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#23545;&#20219;&#24847;&#24418;&#29366;&#30340;&#32676;&#38598;&#38750;&#24120;&#26377;&#25928;&#12290;&#23613;&#31649;&#32467;&#26524;&#20196;&#20154;&#40723;&#33310;&#65292;&#20294;&#23427;&#20204;&#38590;&#20197;&#25214;&#21040;&#20302;&#23494;&#24230;&#32676;&#38598;&#65292;&#19982;&#30456;&#20284;&#23494;&#24230;&#30340;&#32676;&#38598;&#65292;&#20197;&#21450;&#39640;&#32500;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#26159;&#19968;&#31181;&#26032;&#30340;&#32676;&#38598;&#29305;&#24449;&#25551;&#36848;&#21644;&#22522;&#20110;&#31354;&#38388;&#23494;&#24230;&#21644;&#27010;&#29575;&#26041;&#27861;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#20351;&#29992;&#28857;&#20043;&#38388;&#30340;&#25104;&#23545;&#36317;&#31163;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65288;$p.d.f$&#65289;&#34920;&#31034;&#31354;&#38388;&#23494;&#24230;&#65292;&#26500;&#24314;&#23376;&#32676;&#38598;&#12290;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36890;&#36807;&#20351;&#29992;&#23376;&#32676;&#38598;&#30340;&#23494;&#24230;&#65288;$p.d.f$&#65289;&#21644;&#31354;&#38388;&#36317;&#31163;&#26469;&#32858;&#21512;&#30456;&#20284;&#30340;&#23376;&#32676;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20851;&#38190;&#24819;&#27861;&#26159;&#20351;&#29992;Wasserstein&#36317;&#31163;&#24230;&#37327;&#23376;&#32676;&#38598;&#30340;$p.d.f$&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#36825;&#26159;&#19968;&#31181;&#34913;&#37327;&#36317;&#31163;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustering is a data analysis method for extracting knowledge by discovering groups of data called clusters. Among these methods, state-of-the-art density-based clustering methods have proven to be effective for arbitrary-shaped clusters. Despite their encouraging results, they suffer to find low-density clusters, near clusters with similar densities, and high-dimensional data. Our proposals are a new characterization of clusters and a new clustering algorithm based on spatial density and probabilistic approach. First of all, sub-clusters are built using spatial density represented as probability density function ($p.d.f$) of pairwise distances between points. A method is then proposed to agglomerate similar sub-clusters by using both their density ($p.d.f$) and their spatial distance. The key idea we propose is to use the Wasserstein metric, a powerful tool to measure the distance between $p.d.f$ of sub-clusters. We show that our approach outperforms other state-of-the-art density-bas
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38543;&#26426;&#21270;&#39118;&#38505;&#26631;&#20934;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#36991;&#20813;&#22312;&#39118;&#38505;&#19978;&#30340;&#20559;&#21521;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21253;&#25324;Atari 55&#28216;&#25103;&#22312;&#20869;&#30340;&#21508;&#31181;&#29615;&#22659;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.16546</link><description>&lt;p&gt;
&#20048;&#35266;&#20027;&#20041;&#30340;&#38519;&#38449;&#65306;&#36890;&#36807;&#38543;&#26426;&#21270;&#39118;&#38505;&#26631;&#20934;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion. (arXiv:2310.16546v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38543;&#26426;&#21270;&#39118;&#38505;&#26631;&#20934;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#36991;&#20813;&#22312;&#39118;&#38505;&#19978;&#30340;&#20559;&#21521;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21253;&#25324;Atari 55&#28216;&#25103;&#22312;&#20869;&#30340;&#21508;&#31181;&#29615;&#22659;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35797;&#22270;&#21033;&#29992;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#25506;&#32034;&#65292;&#22914;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#30340;&#20048;&#35266;&#20027;&#20041;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20272;&#35745;&#30340;&#26041;&#24046;&#36827;&#34892;&#20048;&#35266;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#25910;&#38598;&#30340;&#20559;&#24046;&#65292;&#38459;&#30861;&#25910;&#25947;&#25110;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#39118;&#38505;&#26631;&#20934;&#26469;&#36873;&#25321;&#21160;&#20316;&#65292;&#36991;&#20813;&#22312;&#39118;&#38505;&#19978;&#30340;&#21333;&#21521;&#20542;&#21521;&#12290;&#25105;&#20204;&#36890;&#36807;&#25197;&#26354;&#39118;&#38505;&#24230;&#37327;&#25552;&#20379;&#20102;&#19968;&#20010;&#25200;&#21160;&#30340;&#20998;&#24067;&#36125;&#23572;&#26364;&#26368;&#20248;&#24615;&#31639;&#23376;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#20855;&#26377;&#36739;&#24369;&#30340;&#25910;&#32553;&#24615;&#36136;&#30340;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#25903;&#25345;&#65292;&#25152;&#25552;&#26041;&#27861;&#19981;&#20250;&#38519;&#20837;&#20559;&#21521;&#24615;&#30340;&#25506;&#32034;&#65292;&#24182;&#30830;&#20445;&#25910;&#25947;&#21040;&#26368;&#20248;&#22238;&#25253;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#21253;&#25324;Atari 55&#28216;&#25103;&#22312;&#20869;&#30340;&#21508;&#31181;&#29615;&#22659;&#20013;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#30340;&#22522;&#20110;&#20998;&#24067;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributional reinforcement learning algorithms have attempted to utilize estimated uncertainty for exploration, such as optimism in the face of uncertainty. However, using the estimated variance for optimistic exploration may cause biased data collection and hinder convergence or performance. In this paper, we present a novel distributional reinforcement learning algorithm that selects actions by randomizing risk criterion to avoid one-sided tendency on risk. We provide a perturbed distributional Bellman optimality operator by distorting the risk measure and prove the convergence and optimality of the proposed method with the weaker contraction property. Our theoretical results support that the proposed method does not fall into biased exploration and is guaranteed to converge to an optimal return. Finally, we empirically show that our method outperforms other existing distribution-based algorithms in various environments including Atari 55 games.
&lt;/p&gt;</description></item><item><title>FedTherapist&#26159;&#19968;&#31181;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#36827;&#34892;&#29992;&#25143;&#29983;&#25104;&#30340;&#35821;&#35328;&#34920;&#36798;&#30340;&#31934;&#31070;&#20581;&#24247;&#30417;&#27979;&#30340;&#31995;&#32479;&#12290;&#23427;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#25345;&#32493;&#35821;&#38899;&#21644;&#38190;&#30424;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#24863;&#30693;&#35821;&#35328;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;&#27604;&#36739;&#20102;&#38750;&#35821;&#35328;&#29305;&#24449;&#65292;&#32467;&#26524;&#26174;&#31034;FedTherapist&#22312;&#39044;&#27979;&#25233;&#37057;&#12289;&#21387;&#21147;&#12289;&#28966;&#34385;&#21644;&#24515;&#24773;&#26041;&#38754;&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.16538</link><description>&lt;p&gt;
FedTherapist&#65306;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#20351;&#29992;&#29992;&#25143;&#29983;&#25104;&#30340;&#35821;&#35328;&#34920;&#36798;&#36827;&#34892;&#31934;&#31070;&#20581;&#24247;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
FedTherapist: Mental Health Monitoring with User-Generated Linguistic Expressions on Smartphones via Federated Learning. (arXiv:2310.16538v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16538
&lt;/p&gt;
&lt;p&gt;
FedTherapist&#26159;&#19968;&#31181;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#36827;&#34892;&#29992;&#25143;&#29983;&#25104;&#30340;&#35821;&#35328;&#34920;&#36798;&#30340;&#31934;&#31070;&#20581;&#24247;&#30417;&#27979;&#30340;&#31995;&#32479;&#12290;&#23427;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#25345;&#32493;&#35821;&#38899;&#21644;&#38190;&#30424;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#24863;&#30693;&#35821;&#35328;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;&#27604;&#36739;&#20102;&#38750;&#35821;&#35328;&#29305;&#24449;&#65292;&#32467;&#26524;&#26174;&#31034;FedTherapist&#22312;&#39044;&#27979;&#25233;&#37057;&#12289;&#21387;&#21147;&#12289;&#28966;&#34385;&#21644;&#24515;&#24773;&#26041;&#38754;&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#31070;&#31185;&#21307;&#29983;&#36890;&#36807;&#24739;&#32773;&#30340;&#35821;&#35328;&#20351;&#29992;&#26469;&#35786;&#26029;&#31934;&#31070;&#30142;&#30149;&#12290;&#20294;&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#34987;&#21160;&#31934;&#31070;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#20351;&#29992;&#25163;&#26426;&#35774;&#22791;&#30340;&#27963;&#21160;&#12289;&#24212;&#29992;&#20351;&#29992;&#21644;&#20301;&#32622;&#31561;&#26367;&#20195;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FedTherapist&#65292;&#19968;&#31181;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#20197;&#38544;&#31169;&#20445;&#25252;&#26041;&#24335;&#20351;&#29992;&#25345;&#32493;&#35821;&#38899;&#21644;&#38190;&#30424;&#36755;&#20837;&#30340;&#31227;&#21160;&#31934;&#31070;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#35774;&#35745;&#30340;&#24615;&#33021;&#21644;&#24320;&#38144;&#26469;&#20811;&#26381;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#36827;&#34892;&#35774;&#22791;&#20869;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#35821;&#35328;&#23398;&#20064;&#65288;CALL&#65289;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#21033;&#29992;&#26234;&#33021;&#25163;&#26426;&#30340;&#22823;&#35268;&#27169;&#21644;&#22024;&#26434;&#25991;&#26412;&#36827;&#34892;&#31934;&#31070;&#20581;&#24247;&#20449;&#21495;&#24863;&#30693;&#12290;&#25105;&#20204;&#22312;46&#21517;&#21442;&#19982;&#32773;&#20013;&#36827;&#34892;&#20102;&#32463;IRB&#25209;&#20934;&#30340;&#33258;&#25105;&#25253;&#21578;&#25233;&#37057;&#12289;&#21387;&#21147;&#12289;&#28966;&#34385;&#21644;&#24515;&#24773;&#30340;&#39044;&#27979;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;FedTherapist&#30456;&#27604;&#20110;&#38750;&#35821;&#35328;&#29305;&#24449;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;0.15 AUROC&#12290;
&lt;/p&gt;
&lt;p&gt;
Psychiatrists diagnose mental disorders via the linguistic use of patients. Still, due to data privacy, existing passive mental health monitoring systems use alternative features such as activity, app usage, and location via mobile devices. We propose FedTherapist, a mobile mental health monitoring system that utilizes continuous speech and keyboard input in a privacy-preserving way via federated learning. We explore multiple model designs by comparing their performance and overhead for FedTherapist to overcome the complex nature of on-device language model training on smartphones. We further propose a Context-Aware Language Learning (CALL) methodology to effectively utilize smartphones' large and noisy text for mental health signal sensing. Our IRB-approved evaluation of the prediction of self-reported depression, stress, anxiety, and mood from 46 participants shows higher accuracy of FedTherapist compared with the performance with non-language features, achieving 0.15 AUROC improveme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26469;&#22686;&#24378;&#25991;&#26723;&#20449;&#24687;&#20998;&#26512;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#20449;&#24687;&#30340;&#25552;&#21462;&#12290;&#27169;&#22411;&#21033;&#29992;transformer-based&#27169;&#22411;&#32534;&#30721;&#25991;&#26723;&#22270;&#20687;&#30340;&#21508;&#31181;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26469;&#20248;&#21270;&#21508;&#31181;&#25991;&#26723;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#12290;&#21478;&#22806;&#65292;&#27169;&#22411;&#36824;&#21253;&#25324;&#20102;&#39069;&#22806;&#30340;&#20219;&#21153;&#21644;&#38598;&#20307;&#39044;&#35757;&#32451;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.16527</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#22686;&#24378;&#25991;&#26723;&#20449;&#24687;&#20998;&#26512;&#65306;&#19968;&#31181;&#38754;&#21521;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#20449;&#24687;&#25552;&#21462;&#30340;&#40065;&#26834;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Document Information Analysis with Multi-Task Pre-training: A Robust Approach for Information Extraction in Visually-Rich Documents. (arXiv:2310.16527v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26469;&#22686;&#24378;&#25991;&#26723;&#20449;&#24687;&#20998;&#26512;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#20449;&#24687;&#30340;&#25552;&#21462;&#12290;&#27169;&#22411;&#21033;&#29992;transformer-based&#27169;&#22411;&#32534;&#30721;&#25991;&#26723;&#22270;&#20687;&#30340;&#21508;&#31181;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26469;&#20248;&#21270;&#21508;&#31181;&#25991;&#26723;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#12290;&#21478;&#22806;&#65292;&#27169;&#22411;&#36824;&#21253;&#25324;&#20102;&#39069;&#22806;&#30340;&#20219;&#21153;&#21644;&#38598;&#20307;&#39044;&#35757;&#32451;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25991;&#26723;&#20449;&#24687;&#20998;&#26512;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#37325;&#28857;&#20851;&#27880;&#25991;&#26723;&#20998;&#31867;&#12289;&#23454;&#20307;&#20851;&#31995;&#25552;&#21462;&#21644;&#25991;&#26723;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21033;&#29992;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#26469;&#32534;&#30721;&#25991;&#26723;&#22270;&#20687;&#20013;&#30340;&#25152;&#26377;&#20449;&#24687;&#65292;&#21253;&#25324;&#25991;&#26412;&#12289;&#35270;&#35273;&#21644;&#24067;&#23616;&#20449;&#24687;&#12290;&#35813;&#27169;&#22411;&#32463;&#36807;&#39044;&#35757;&#32451;&#21518;&#65292;&#32487;&#32493;&#23545;&#21508;&#31181;&#25991;&#26723;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36824;&#21253;&#25324;&#19977;&#20010;&#39069;&#22806;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35782;&#21035;&#25991;&#26723;&#22270;&#20687;&#20013;&#19981;&#21516;&#24067;&#23616;&#27573;&#33853;&#30340;&#38405;&#35835;&#39034;&#24207;&#12289;&#26681;&#25454;PubLayNet&#23545;&#24067;&#23616;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#20197;&#21450;&#29983;&#25104;&#32473;&#23450;&#24067;&#23616;&#27573;&#33853;&#65288;&#25991;&#26412;&#22359;&#65289;&#20869;&#30340;&#25991;&#26412;&#24207;&#21015;&#12290;&#35813;&#27169;&#22411;&#36824;&#37319;&#29992;&#20102;&#19968;&#31181;&#38598;&#20307;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#20854;&#20013;&#32771;&#34385;&#20102;&#25152;&#26377;&#32771;&#34385;&#30340;&#20219;&#21153;&#30340;&#25439;&#22833;&#65292;&#21253;&#25324;&#20351;&#29992;&#25152;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36824;&#28155;&#21152;&#20102;&#39069;&#22806;&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a deep learning model tailored for document information analysis, emphasizing document classification, entity relation extraction, and document visual question answering. The proposed model leverages transformer-based models to encode all the information present in a document image, including textual, visual, and layout information. The model is pre-trained and subsequently fine-tuned for various document image analysis tasks. The proposed model incorporates three additional tasks during the pre-training phase, including reading order identification of different layout segments in a document image, layout segments categorization as per PubLayNet, and generation of the text sequence within a given layout segment (text block). The model also incorporates a collective pre-training scheme where losses of all the tasks under consideration, including pre-training and fine-tuning tasks with all datasets, are considered. Additional encoder and decoder blocks are added to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21270;&#32467;&#26524;&#30340;&#24490;&#29615;&#26377;&#21521;&#27010;&#29575;&#22270;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#25429;&#25417;&#22312;&#32467;&#26500;&#23398;&#20064;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#26041;&#21521;&#24615;&#24490;&#29615;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.16525</link><description>&lt;p&gt;
&#24490;&#29615;&#26377;&#21521;&#27010;&#29575;&#22270;&#27169;&#22411;&#65306;&#22522;&#20110;&#32467;&#26500;&#21270;&#32467;&#26524;&#30340;&#25552;&#26696;
&lt;/p&gt;
&lt;p&gt;
Cyclic Directed Probabilistic Graphical Model: A Proposal Based on Structured Outcomes. (arXiv:2310.16525v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21270;&#32467;&#26524;&#30340;&#24490;&#29615;&#26377;&#21521;&#27010;&#29575;&#22270;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#25429;&#25417;&#22312;&#32467;&#26500;&#23398;&#20064;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#26041;&#21521;&#24615;&#24490;&#29615;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20174;&#19968;&#32452;&#35266;&#27979;&#25968;&#25454;&#20013;&#26500;&#24314;&#65288;&#32467;&#26500;&#23398;&#20064;&#65289;&#27010;&#29575;&#22270;&#27169;&#22411;&#30340;&#36807;&#31243;&#20013;&#65292;&#24120;&#24120;&#21457;&#29616;&#27169;&#22411;&#30340;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#26041;&#21521;&#24615;&#30340;&#24490;&#29615;&#20381;&#36182;&#20851;&#31995;&#12290;&#29616;&#26377;&#30340;&#22270;&#27169;&#22411;&#22914;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#39532;&#23572;&#21487;&#22827;&#32593;&#32476;&#21487;&#20197;&#21453;&#26144;&#36825;&#31181;&#20381;&#36182;&#20851;&#31995;&#65292;&#20294;&#38656;&#35201;&#22797;&#26434;&#21270;&#27169;&#22411;&#65292;&#20363;&#22914;&#28155;&#21152;&#39069;&#22806;&#21464;&#37327;&#25110;&#23558;&#27169;&#22411;&#22270;&#20998;&#21106;&#20026;&#19981;&#21516;&#23376;&#22270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#27010;&#29575;&#22270;&#27169;&#22411;&#8212;&#8212;&#27010;&#29575;&#20851;&#31995;&#32593;&#32476;&#65292;&#23427;&#20801;&#35768;&#22312;&#32467;&#26500;&#23398;&#20064;&#36807;&#31243;&#20013;&#30452;&#25509;&#25429;&#25417;&#26041;&#21521;&#24615;&#24490;&#29615;&#20381;&#36182;&#20851;&#31995;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#24819;&#27861;&#65292;&#21363;&#35266;&#27979;&#25968;&#25454;&#30340;&#27599;&#20010;&#26679;&#26412;&#21487;&#20197;&#30001;&#19968;&#20010;&#20219;&#24847;&#22270;&#65288;&#32467;&#26500;&#21270;&#32467;&#26524;&#65289;&#34920;&#31034;&#65292;&#35813;&#22270;&#21453;&#26144;&#20102;&#26679;&#26412;&#20013;&#21253;&#21547;&#30340;&#21464;&#37327;&#20381;&#36182;&#30340;&#32467;&#26500;&#12290;&#27599;&#20010;&#32467;&#26524;&#21482;&#21253;&#21547;&#27010;&#29575;&#27169;&#22411;&#32467;&#26500;&#30340;&#19968;&#37096;&#20998;&#65292;&#20294;&#36890;&#36807;&#32452;&#21512;&#36825;&#20123;&#32467;&#26524;&#21487;&#20197;&#24471;&#21040;&#23436;&#25972;&#30340;&#27010;&#29575;&#27169;&#22411;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the process of building (structural learning) a probabilistic graphical model from a set of observed data, the directional, cyclic dependencies between the random variables of the model are often found. Existing graphical models such as Bayesian and Markov networks can reflect such dependencies. However, this requires complicating those models, such as adding additional variables or dividing the model graph into separate subgraphs. Herein, we describe a probabilistic graphical model - probabilistic relation network - that allows the direct capture of directional cyclic dependencies during structural learning. This model is based on the simple idea that each sample of the observed data can be represented by an arbitrary graph (structured outcome), which reflects the structure of the dependencies of the variables included in the sample. Each of the outcomes contains only a part of the graphical model structure; however, a complete graph of the probabilistic model is obtained by combin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;3S Testing&#65292;&#36825;&#26159;&#19968;&#20010;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#27979;&#35797;&#38598;&#21644;&#27169;&#25311;&#20998;&#24067;&#20559;&#31227;&#26469;&#25913;&#36827;&#27169;&#22411;&#35780;&#20272;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;3S Testing&#22312;&#35780;&#20272;&#23569;&#25968;&#23376;&#32676;&#20307;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2310.16524</link><description>&lt;p&gt;
&#20320;&#33021;&#20381;&#38752;&#27169;&#22411;&#35780;&#20272;&#21527;&#65311;&#20351;&#29992;&#21512;&#25104;&#27979;&#35797;&#25968;&#25454;&#25913;&#36827;&#27169;&#22411;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can You Rely on Your Model Evaluation? Improving Model Evaluation with Synthetic Test Data. (arXiv:2310.16524v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;3S Testing&#65292;&#36825;&#26159;&#19968;&#20010;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#27979;&#35797;&#38598;&#21644;&#27169;&#25311;&#20998;&#24067;&#20559;&#31227;&#26469;&#25913;&#36827;&#27169;&#22411;&#35780;&#20272;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;3S Testing&#22312;&#35780;&#20272;&#23569;&#25968;&#23376;&#32676;&#20307;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20445;&#35777;&#20844;&#24179;&#21644;&#21487;&#38752;&#24615;&#30340;&#30495;&#23454;&#24212;&#29992;&#20013;&#65292;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22810;&#26679;&#21270;&#21644;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#23376;&#32676;&#20307;&#19978;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#20934;&#30830;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#65288;1&#65289;&#27979;&#35797;&#25968;&#25454;&#31232;&#32570;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23567;&#30340;&#23376;&#32676;&#20307;&#65292;&#65288;2&#65289;&#27169;&#22411;&#22312;&#37096;&#32626;&#29615;&#22659;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#20998;&#24067;&#20559;&#31227;&#19982;&#21487;&#29992;&#30340;&#27979;&#35797;&#25968;&#25454;&#19981;&#19968;&#33268;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;3S Testing&#30340;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#23567;&#22411;&#23376;&#32676;&#20307;&#30340;&#21512;&#25104;&#27979;&#35797;&#38598;&#21644;&#27169;&#25311;&#20998;&#24067;&#20559;&#31227;&#26469;&#20419;&#36827;&#27169;&#22411;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#20165;&#20351;&#29992;&#30495;&#23454;&#27979;&#35797;&#25968;&#25454;&#65292;3S Testing&#22312;&#35780;&#20272;&#23569;&#25968;&#23376;&#32676;&#20307;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#22312;&#21487;&#33021;&#30340;&#20998;&#24067;&#20559;&#31227;&#19979;&#34920;&#29616;&#26356;&#20248;&#12290;&#27492;&#22806;&#65292;3S Testing&#25552;&#20379;&#20102;&#24615;&#33021;&#20272;&#35745;&#30340;&#21306;&#38388;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#23637;&#29616;&#20102;&#26356;&#20248;&#30340;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the performance of machine learning models on diverse and underrepresented subgroups is essential for ensuring fairness and reliability in real-world applications. However, accurately assessing model performance becomes challenging due to two main issues: (1) a scarcity of test data, especially for small subgroups, and (2) possible distributional shifts in the model's deployment setting, which may not align with the available test data. In this work, we introduce 3S Testing, a deep generative modeling framework to facilitate model evaluation by generating synthetic test sets for small subgroups and simulating distributional shifts. Our experiments demonstrate that 3S Testing outperforms traditional baselines -- including real test data alone -- in estimating model performance on minority subgroups and under plausible distributional shifts. In addition, 3S offers intervals around its performance estimates, exhibiting superior coverage of the ground truth compared to existing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#35299;&#37322;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65288;SIGNET&#65289;&#65292;&#33021;&#22815;&#35782;&#21035;&#24322;&#24120;&#22270;&#24182;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#26080;&#27861;&#25552;&#20379;&#26377;&#25928;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.16520</link><description>&lt;p&gt;
&#38754;&#21521;&#33258;&#35299;&#37322;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Self-Interpretable Graph-Level Anomaly Detection. (arXiv:2310.16520v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#35299;&#37322;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65288;SIGNET&#65289;&#65292;&#33021;&#22815;&#35782;&#21035;&#24322;&#24120;&#22270;&#24182;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#26080;&#27861;&#25552;&#20379;&#26377;&#25928;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#26088;&#22312;&#35782;&#21035;&#19982;&#38598;&#21512;&#20013;&#30340;&#22823;&#22810;&#25968;&#22270;&#30456;&#27604;&#20855;&#26377;&#26174;&#33879;&#24046;&#24322;&#30340;&#22270;&#24418;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#35780;&#20272;&#22270;&#32423;&#24322;&#24120;&#24615;&#65292;&#32780;&#26410;&#33021;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#39044;&#27979;&#35299;&#37322;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#21644;&#24212;&#29992;&#33539;&#22260;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#26032;&#30340;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#21363;&#21487;&#35299;&#37322;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#65292;&#20854;&#20013;&#23398;&#20064;&#30446;&#26631;&#26159;&#39044;&#27979;&#27599;&#20010;&#22270;&#26679;&#26412;&#30340;&#24322;&#24120;&#24615;&#65292;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#35299;&#37322;&#65292;&#21363;&#23548;&#33268;&#39044;&#27979;&#30340;&#20851;&#38190;&#23376;&#22270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#35299;&#37322;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65288;SIGNET&#65289;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#26816;&#27979;&#24322;&#24120;&#22270;&#24182;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#35299;&#37322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#22810;&#35270;&#22270;&#23376;&#22270;&#20449;&#24687;&#29942;&#39048;&#65288;MSIB&#65289;&#26694;&#26550;&#65292;&#20316;&#20026;&#25105;&#20204;&#33258;&#35299;&#37322;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#35774;&#35745;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-level anomaly detection (GLAD) aims to identify graphs that exhibit notable dissimilarity compared to the majority in a collection. However, current works primarily focus on evaluating graph-level abnormality while failing to provide meaningful explanations for the predictions, which largely limits their reliability and application scope. In this paper, we investigate a new challenging problem, explainable GLAD, where the learning objective is to predict the abnormality of each graph sample with corresponding explanations, i.e., the vital subgraph that leads to the predictions. To address this challenging problem, we propose a Self-Interpretable Graph aNomaly dETection model (SIGNET for short) that detects anomalous graphs as well as generates informative explanations simultaneously. Specifically, we first introduce the multi-view subgraph information bottleneck (MSIB) framework, serving as the design basis of our self-interpretable GLAD approach. This way SIGNET is able to not o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;Wasserstein&#26799;&#24230;&#27969;&#30340;ParVI&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20984;&#20989;&#25968;&#24341;&#23548;&#30340;&#26356;&#24191;&#27867;&#31867;&#21035;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#26041;&#27861;&#35774;&#35745;&#22256;&#38590;&#21644;&#38480;&#21046;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20855;&#26377;&#24378;&#22823;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#21644;&#39640;&#25928;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16516</link><description>&lt;p&gt;
&#22522;&#20110;&#31890;&#23376;&#30340;&#24191;&#20041;Wasserstein&#26799;&#24230;&#27969;&#30340;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Particle-based Variational Inference with Generalized Wasserstein Gradient Flow. (arXiv:2310.16516v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;Wasserstein&#26799;&#24230;&#27969;&#30340;ParVI&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20984;&#20989;&#25968;&#24341;&#23548;&#30340;&#26356;&#24191;&#27867;&#31867;&#21035;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#26041;&#27861;&#35774;&#35745;&#22256;&#38590;&#21644;&#38480;&#21046;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20855;&#26377;&#24378;&#22823;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#21644;&#39640;&#25928;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31890;&#23376;&#30340;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#65288;ParVIs&#65289;&#65292;&#22914;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#65288;SVGD&#65289;&#65292;&#36890;&#36807;&#22522;&#20110;&#26680;&#21270;&#30340;Wasserstein&#26799;&#24230;&#27969;&#26356;&#26032;&#31890;&#23376;&#65292;&#29992;&#20110;Kullback-Leibler&#65288;KL&#65289;&#25955;&#24230;&#12290;&#28982;&#32780;&#65292;&#26680;&#20989;&#25968;&#30340;&#35774;&#35745;&#36890;&#24120;&#26159;&#38750;&#24179;&#20961;&#30340;&#65292;&#24182;&#19988;&#21487;&#33021;&#23545;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#26377;&#38480;&#21046;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20855;&#26377;&#20108;&#27425;&#24418;&#24335;&#27491;&#21017;&#21270;&#39033;&#30340;&#21151;&#33021;&#26799;&#24230;&#27969;&#36924;&#36817;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;Wasserstein&#26799;&#24230;&#27969;&#30340;ParVI&#26694;&#26550;&#65292;&#31216;&#20026;&#24191;&#20041;Wasserstein&#26799;&#24230;&#19979;&#38477;&#65288;GWG&#65289;&#65292;&#20854;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#20855;&#26377;&#20984;&#20989;&#25968;&#24341;&#23548;&#30340;&#26356;&#24191;&#27867;&#31867;&#21035;&#30340;&#27491;&#21017;&#21270;&#22120;&#30340;&#21151;&#33021;&#26799;&#24230;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;GWG&#20855;&#26377;&#24378;&#22823;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#29256;&#26412;&#65292;&#21487;&#20197;&#33258;&#21160;&#36873;&#25321;Wasserstein&#24230;&#37327;&#26469;&#21152;&#36895;&#25910;&#25947;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Particle-based variational inference methods (ParVIs) such as Stein variational gradient descent (SVGD) update the particles based on the kernelized Wasserstein gradient flow for the Kullback-Leibler (KL) divergence. However, the design of kernels is often non-trivial and can be restrictive for the flexibility of the method. Recent works show that functional gradient flow approximations with quadratic form regularization terms can improve performance. In this paper, we propose a ParVI framework, called generalized Wasserstein gradient descent (GWG), based on a generalized Wasserstein gradient flow of the KL divergence, which can be viewed as a functional gradient method with a broader class of regularizers induced by convex functions. We show that GWG exhibits strong convergence guarantees. We also provide an adaptive version that automatically chooses Wasserstein metric to accelerate convergence. In experiments, we demonstrate the effectiveness and efficiency of the proposed framework
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35770;&#35777;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#20026;&#20160;&#20040;&#19968;&#20010;&#20010;&#20307;&#34987;&#20998;&#31867;&#19982;&#30456;&#20284;&#20010;&#20307;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#23450;&#37327;&#35770;&#35777;&#26694;&#26550;&#26469;&#34920;&#31034;&#20010;&#20307;&#21644;&#19982;&#20854;&#30456;&#20284;&#20010;&#20307;&#30340;&#23646;&#24615;-&#20540;&#23545;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#35821;&#20041;&#26469;&#30830;&#23450;&#23545;&#20010;&#20307;&#20998;&#31867;&#20135;&#29983;&#26368;&#22823;&#36129;&#29486;&#30340;&#23646;&#24615;-&#20540;&#23545;&#12290;</title><link>http://arxiv.org/abs/2310.16506</link><description>&lt;p&gt;
&#35782;&#21035;&#20559;&#35265;&#30340;&#21407;&#22240;&#65306;&#19968;&#31181;&#22522;&#20110;&#35770;&#35777;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Identifying Reasons for Bias: An Argumentation-Based Approach. (arXiv:2310.16506v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35770;&#35777;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#20026;&#20160;&#20040;&#19968;&#20010;&#20010;&#20307;&#34987;&#20998;&#31867;&#19982;&#30456;&#20284;&#20010;&#20307;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#23450;&#37327;&#35770;&#35777;&#26694;&#26550;&#26469;&#34920;&#31034;&#20010;&#20307;&#21644;&#19982;&#20854;&#30456;&#20284;&#20010;&#20307;&#30340;&#23646;&#24615;-&#20540;&#23545;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#35821;&#20041;&#26469;&#30830;&#23450;&#23545;&#20010;&#20307;&#20998;&#31867;&#20135;&#29983;&#26368;&#22823;&#36129;&#29486;&#30340;&#23646;&#24615;-&#20540;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31639;&#27861;&#20915;&#31574;&#31995;&#32479;&#22312;&#31038;&#20250;&#20013;&#30340;&#26222;&#21450;&#65292;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34429;&#28982;&#22312;&#26500;&#24314;&#20844;&#24179;&#31639;&#27861;&#20915;&#31574;&#31995;&#32479;&#26041;&#38754;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#20854;&#20013;&#22823;&#37096;&#20998;&#26041;&#27861;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65292;&#21253;&#25324;&#20010;&#20154;&#29305;&#24449;&#65292;&#24182;&#19988;&#23545;&#20110;&#21738;&#20123;&#20010;&#20307;&#34987;&#19981;&#20844;&#24179;&#22320;&#20998;&#31867;&#27809;&#26377;&#36879;&#26126;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#22522;&#20110;&#35770;&#35777;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#20026;&#20160;&#20040;&#19968;&#20010;&#20010;&#20307;&#34987;&#20998;&#31867;&#19982;&#30456;&#20284;&#20010;&#20307;&#19981;&#21516;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#23450;&#37327;&#35770;&#35777;&#26694;&#26550;&#26469;&#34920;&#31034;&#20010;&#20307;&#21644;&#19982;&#20854;&#30456;&#20284;&#20010;&#20307;&#30340;&#23646;&#24615;-&#20540;&#23545;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#35821;&#20041;&#26469;&#30830;&#23450;&#23545;&#20010;&#20307;&#20998;&#31867;&#20135;&#29983;&#26368;&#22823;&#36129;&#29486;&#30340;&#23646;&#24615;-&#20540;&#23545;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22312;&#20844;&#24179;&#39046;&#22495;&#24120;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#35782;&#21035;&#24046;&#24322;&#20998;&#31867;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As algorithmic decision-making systems become more prevalent in society, ensuring the fairness of these systems is becoming increasingly important. Whilst there has been substantial research in building fair algorithmic decision-making systems, the majority of these methods require access to the training data, including personal characteristics, and are not transparent regarding which individuals are classified unfairly. In this paper, we propose a novel model-agnostic argumentation-based method to determine why an individual is classified differently in comparison to similar individuals. Our method uses a quantitative argumentation framework to represent attribute-value pairs of an individual and of those similar to them, and uses a well-known semantics to identify the attribute-value pairs in the individual contributing most to their different classification. We evaluate our method on two datasets commonly used in the fairness literature and illustrate its effectiveness in the identi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#21512;&#20102;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20840;&#38754;&#30340;&#20998;&#31867;&#20307;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.16499</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#20248;&#21270;&#65306;&#19968;&#39033;&#32508;&#36848;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Data Optimization in Deep Learning: A Survey. (arXiv:2310.16499v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16499
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#21512;&#20102;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20840;&#38754;&#30340;&#20998;&#31867;&#20307;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#34987;&#35748;&#20026;&#26159;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25104;&#21151;&#24212;&#29992;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#23454;&#38469;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20173;&#28982;&#38754;&#20020;&#30528;&#32570;&#20047;&#36275;&#22815;&#25968;&#37327;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#21487;&#20449;&#24230;&#31561;&#38382;&#39064;&#20063;&#19982;&#35757;&#32451;&#25968;&#25454;&#23494;&#20999;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#22823;&#37327;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#26041;&#38754;&#12290;&#19968;&#20123;&#20856;&#22411;&#30340;&#25968;&#25454;&#20248;&#21270;&#25216;&#26415;&#21253;&#25324;&#25968;&#25454;&#22686;&#24378;&#12289;&#36923;&#36753;&#25200;&#21160;&#12289;&#26679;&#26412;&#21152;&#26435;&#21644;&#25968;&#25454;&#21387;&#32553;&#12290;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#26469;&#33258;&#20110;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#65292;&#23427;&#20204;&#30340;&#29702;&#35770;&#28789;&#24863;&#25110;&#21551;&#21457;&#21160;&#26426;&#21487;&#33021;&#30475;&#20284;&#26080;&#20851;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#32452;&#32455;&#24191;&#27867;&#30340;&#29616;&#26377;&#25968;&#25454;&#20248;&#21270;&#26041;&#27861;&#35770;&#24182;&#26500;&#24314;&#19968;&#20010;&#20840;&#38754;&#30340;&#20998;&#31867;&#20307;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale, high-quality data are considered an essential factor for the successful application of many deep learning techniques. Meanwhile, numerous real-world deep learning tasks still have to contend with the lack of sufficient amounts of high-quality data. Additionally, issues such as model robustness, fairness, and trustworthiness are also closely related to training data. Consequently, a huge number of studies in the existing literature have focused on the data aspect in deep learning tasks. Some typical data optimization techniques include data augmentation, logit perturbation, sample weighting, and data condensation. These techniques usually come from different deep learning divisions and their theoretical inspirations or heuristic motivations may seem unrelated to each other. This study aims to organize a wide range of existing data optimization methodologies for deep learning from the previous literature, and makes the effort to construct a comprehensive taxonomy for them. T
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23460;&#20869;&#23450;&#20301;&#26381;&#21153;&#26041;&#27861;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#30828;&#20214;&#35774;&#26045;&#65292;&#25552;&#20379;&#20102;&#23567;&#20110;2&#31859;&#30340;&#31934;&#30830;&#24230;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#20002;&#22833;&#22823;&#37327;BSSIDs&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#33021;&#20445;&#25345;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16496</link><description>&lt;p&gt;
&#20844;&#27665;&#21442;&#19982;&#65306;&#22522;&#20110;&#20247;&#24863;&#24212;&#30340;&#21487;&#25345;&#32493;&#23460;&#20869;&#23450;&#20301;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Citizen participation: crowd-sensed sustainable indoor location services. (arXiv:2310.16496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23460;&#20869;&#23450;&#20301;&#26381;&#21153;&#26041;&#27861;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#30828;&#20214;&#35774;&#26045;&#65292;&#25552;&#20379;&#20102;&#23567;&#20110;2&#31859;&#30340;&#31934;&#30830;&#24230;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#20002;&#22833;&#22823;&#37327;BSSIDs&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#33021;&#20445;&#25345;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#25345;&#32493;&#21019;&#26032;&#30340;&#26102;&#20195;&#65292;&#24490;&#29615;&#32463;&#27982;&#33539;&#24335;&#35201;&#27714;&#23545;&#29616;&#26377;&#26377;&#38480;&#36164;&#28304;&#36827;&#34892;&#26368;&#20339;&#21033;&#29992;&#21644;&#24320;&#21457;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21521;&#26234;&#33021;&#22522;&#30784;&#35774;&#26045;&#30340;&#36807;&#28193;&#38656;&#35201;&#22823;&#37327;&#30340;&#36164;&#26412;&#12289;&#36164;&#28304;&#21644;&#20154;&#21147;&#25237;&#20837;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#20379;&#23460;&#20869;&#23450;&#20301;&#26381;&#21153;&#65292;&#26080;&#38656;&#25237;&#36164;&#39069;&#22806;&#21644;&#19987;&#38376;&#30340;&#30828;&#20214;&#35774;&#26045;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20123;&#20351;&#29992;&#26696;&#20363;&#65292;&#35775;&#23458;&#36890;&#36807;&#20182;&#20204;&#30340;&#26234;&#33021;&#25163;&#26426;&#19982;&#21487;&#29992;&#30340;WiFi&#22522;&#30784;&#35774;&#26045;&#36827;&#34892;&#20132;&#20114;&#65292;&#20197;&#20272;&#35745;&#20182;&#20204;&#30340;&#20301;&#32622;&#65292;&#22240;&#20026;&#23460;&#20869;&#38656;&#27714;&#23545;&#26631;&#20934;GPS&#35299;&#20915;&#26041;&#26696;&#23384;&#22312;&#38480;&#21046;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#23567;&#20110;2&#31859;&#30340;&#31934;&#24230;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#20002;&#22833;&#22823;&#37327;BSSIDs&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#20063;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the present era of sustainable innovation, the circular economy paradigm dictates the optimal use and exploitation of existing finite resources. At the same time, the transition to smart infrastructures requires considerable investment in capital, resources and people. In this work, we present a general machine learning approach for offering indoor location awareness without the need to invest in additional and specialised hardware. We explore use cases where visitors equipped with their smart phone would interact with the available WiFi infrastructure to estimate their location, since the indoor requirement poses a limitation to standard GPS solutions. Results have shown that the proposed approach achieves a less than 2m accuracy and the model is resilient even in the case where a substantial number of BSSIDs are dropped.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#24322;&#24120;&#26333;&#20809;&#22312;&#35270;&#35273;&#22806;&#30028;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#24322;&#24120;&#20540;&#20195;&#26367;&#22270;&#20687;&#20013;&#30340;&#24322;&#24120;&#20540;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20351;&#29992;&#25991;&#26412;&#24322;&#24120;&#20540;&#30340;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#31181;&#29983;&#25104;&#25991;&#26412;&#24322;&#24120;&#20540;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.16492</link><description>&lt;p&gt;
&#20851;&#20110;&#25991;&#26412;&#24322;&#24120;&#26333;&#20809;&#22312;&#35270;&#35273;&#22806;&#30028;&#26816;&#27979;&#20013;&#30340;&#25928;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the Powerfulness of Textual Outlier Exposure for Visual OoD Detection. (arXiv:2310.16492v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#24322;&#24120;&#26333;&#20809;&#22312;&#35270;&#35273;&#22806;&#30028;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#24322;&#24120;&#20540;&#20195;&#26367;&#22270;&#20687;&#20013;&#30340;&#24322;&#24120;&#20540;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20351;&#29992;&#25991;&#26412;&#24322;&#24120;&#20540;&#30340;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#31181;&#29983;&#25104;&#25991;&#26412;&#24322;&#24120;&#20540;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#26816;&#27979;&#36234;&#30028;&#25968;&#25454;&#23545;&#20110;&#30830;&#20445;&#31070;&#32463;&#32593;&#32476;&#30340;&#23433;&#20840;&#37096;&#32626;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36234;&#30028;&#26816;&#27979;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#31070;&#32463;&#32593;&#32476;&#22312;&#36234;&#30028;&#25968;&#25454;&#19978;&#36755;&#20986;&#36807;&#24230;&#33258;&#20449;&#30340;&#39044;&#27979;&#65292;&#36825;&#20351;&#24471;&#20165;&#26681;&#25454;&#39044;&#27979;&#26469;&#30830;&#23450;&#25968;&#25454;&#30340;&#36234;&#30028;&#24615;&#21464;&#24471;&#22256;&#38590;&#12290;&#24322;&#24120;&#26333;&#20809;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#39069;&#22806;&#30340;&#25439;&#22833;&#65292;&#40723;&#21169;&#23545;&#36234;&#30028;&#25968;&#25454;&#36827;&#34892;&#20302;&#32622;&#20449;&#24230;&#39044;&#27979;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#23613;&#31649;&#24322;&#24120;&#26333;&#20809;&#22312;&#25552;&#39640;&#36234;&#30028;&#26816;&#27979;&#24615;&#33021;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#28508;&#21147;&#65292;&#20294;&#26159;&#20043;&#21069;&#25152;&#26377;&#20851;&#20110;&#24322;&#24120;&#26333;&#20809;&#30340;&#30740;&#31350;&#37117;&#38480;&#20110;&#20351;&#29992;&#35270;&#35273;&#24322;&#24120;&#12290;&#21463;&#21040;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;&#25991;&#26412;&#24322;&#24120;&#26333;&#20809;&#22312;&#23578;&#26410;&#24320;&#25299;&#30340;&#39046;&#22495;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#29992;&#25991;&#26412;&#31561;&#20215;&#29289;&#26367;&#25442;&#22270;&#20687;&#22495;&#20013;&#30340;&#30495;&#23454;&#25110;&#34394;&#25311;&#24322;&#24120;&#20540;&#26469;&#25581;&#31034;&#20351;&#29992;&#25991;&#26412;&#24322;&#24120;&#20540;&#30340;&#22909;&#22788;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#31181;&#29983;&#25104;&#25991;&#26412;&#24322;&#24120;&#20540;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successful detection of Out-of-Distribution (OoD) data is becoming increasingly important to ensure safe deployment of neural networks. One of the main challenges in OoD detection is that neural networks output overconfident predictions on OoD data, make it difficult to determine OoD-ness of data solely based on their predictions. Outlier exposure addresses this issue by introducing an additional loss that encourages low-confidence predictions on OoD data during training. While outlier exposure has shown promising potential in improving OoD detection performance, all previous studies on outlier exposure have been limited to utilizing visual outliers. Drawing inspiration from the recent advancements in vision-language pre-training, this paper venture out to the uncharted territory of textual outlier exposure. First, we uncover the benefits of using textual outliers by replacing real or virtual outliers in the image-domain with textual equivalents. Then, we propose various ways of genera
&lt;/p&gt;</description></item><item><title>TSONN&#26159;&#19968;&#31181;&#23558;&#26102;&#38388;&#27493;&#26041;&#27861;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#31995;&#21015;&#33391;&#22909;&#26465;&#20214;&#30340;&#23376;&#38382;&#39064;&#26469;&#35299;&#20915;PDE&#27714;&#35299;&#20013;&#30340;&#31283;&#23450;&#35757;&#32451;&#21644;&#27491;&#30830;&#32467;&#26524;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.16491</link><description>&lt;p&gt;
TSONN: &#26102;&#38388;&#27493;&#23548;&#21521;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
TSONN: Time-stepping-oriented neural network for solving partial differential equations. (arXiv:2310.16491v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16491
&lt;/p&gt;
&lt;p&gt;
TSONN&#26159;&#19968;&#31181;&#23558;&#26102;&#38388;&#27493;&#26041;&#27861;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#31995;&#21015;&#33391;&#22909;&#26465;&#20214;&#30340;&#23376;&#38382;&#39064;&#26469;&#35299;&#20915;PDE&#27714;&#35299;&#20013;&#30340;&#31283;&#23450;&#35757;&#32451;&#21644;&#27491;&#30830;&#32467;&#26524;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#65292;&#23588;&#20854;&#26159;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#65292;&#26368;&#36817;&#25104;&#20026;&#27714;&#35299;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#20915;&#23450;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#30340;&#26032;&#28909;&#38376;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20351;&#29992;&#22522;&#20110;PDE&#30340;&#36719;&#32422;&#26463;&#26469;&#26368;&#23567;&#21270;PDE&#27531;&#24046;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#38754;&#20020;&#30528;&#22312;&#35768;&#22810;&#38382;&#39064;&#20013;&#23454;&#29616;&#31283;&#23450;&#35757;&#32451;&#21644;&#33719;&#24471;&#27491;&#30830;&#32467;&#26524;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20351;&#24471;&#38382;&#39064;&#26465;&#20214;&#19981;&#33391;&#12290;&#19982;&#25152;&#26377;&#29616;&#26377;&#30340;&#30452;&#25509;&#26368;&#23567;&#21270;PDE&#27531;&#24046;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#26412;&#25991;&#23558;&#26102;&#38388;&#27493;&#26041;&#27861;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#23558;&#21407;&#26412;&#30340;&#26465;&#20214;&#19981;&#33391;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#31995;&#21015;&#22312;&#32473;&#23450;&#20266;&#26102;&#38388;&#38388;&#38548;&#20869;&#20855;&#26377;&#33391;&#22909;&#26465;&#20214;&#30340;&#23376;&#38382;&#39064;&#12290;&#36890;&#36807;&#36319;&#36394;&#20266;&#26102;&#38388;&#27493;&#36827;&#36807;&#31243;&#30340;&#36712;&#36857;&#65292;&#27169;&#22411;&#35757;&#32451;&#30340;&#25910;&#25947;&#24615;&#24471;&#21040;&#26174;&#33879;&#25913;&#21892;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#31283;&#20581;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;PDE&#27714;&#35299;&#22120;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35768;&#22810;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#31283;&#23450;&#30340;&#35757;&#32451;&#21644;&#27491;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs), especially physics-informed neural networks (PINNs), have recently become a new popular method for solving forward and inverse problems governed by partial differential equations (PDEs). However, these methods still face challenges in achieving stable training and obtaining correct results in many problems, since minimizing PDE residuals with PDE-based soft constraint make the problem ill-conditioned. Different from all existing methods that directly minimize PDE residuals, this work integrates time-stepping method with deep learning, and transforms the original ill-conditioned optimization problem into a series of well-conditioned sub-problems over given pseudo time intervals. The convergence of model training is significantly improved by following the trajectory of the pseudo time-stepping process, yielding a robust optimization-based PDE solver. Our results show that the proposed method achieves stable training and correct results in many problems that s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.16487</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter Optimization for Multi-Objective Reinforcement Learning. (arXiv:2310.16487v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16487
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#19968;&#31181;&#24378;&#22823;&#26041;&#27861;&#12290;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;(MORL)&#30340;&#24341;&#20837;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#24378;&#21270;&#23398;&#20064;&#30340;&#33539;&#22260;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;&#22810;&#20010;&#30446;&#26631;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#36825;&#19968;&#36827;&#23637;&#19981;&#20165;&#25193;&#22823;&#20102;&#21487;&#20197;&#35299;&#20915;&#30340;&#38382;&#39064;&#33539;&#22260;&#65292;&#36824;&#20026;&#25506;&#32034;&#21644;&#36827;&#27493;&#21019;&#36896;&#20102;&#35768;&#22810;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#20381;&#36182;&#20110;&#36866;&#24403;&#35774;&#32622;&#23427;&#20204;&#30340;&#36229;&#21442;&#25968;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#19968;&#20219;&#21153;&#24120;&#24120;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23548;&#33268;&#36825;&#20123;&#25216;&#26415;&#22312;&#21508;&#31181;&#23454;&#20363;&#20013;&#26080;&#27861;&#25104;&#21151;&#37096;&#32626;&#12290;&#22240;&#27492;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#32034;&#20102;&#36229;&#21442;&#25968;&#20248;&#21270;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#26412;&#25991;&#38024;&#23545;MORL&#23545;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#25361;&#25112;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#12290;&#25105;&#20204;&#35268;&#33539;&#21270;&#20102;&#35813;&#38382;&#39064;&#65292;&#31361;&#20986;&#20102;&#20854;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has emerged as a powerful approach for tackling complex problems. The recent introduction of multi-objective reinforcement learning (MORL) has further expanded the scope of RL by enabling agents to make trade-offs among multiple objectives. This advancement not only has broadened the range of problems that can be tackled but also created numerous opportunities for exploration and advancement. Yet, the effectiveness of RL agents heavily relies on appropriately setting their hyperparameters. In practice, this task often proves to be challenging, leading to unsuccessful deployments of these techniques in various instances. Hence, prior research has explored hyperparameter optimization in RL to address this concern.  This paper presents an initial investigation into the challenge of hyperparameter optimization specifically for MORL. We formalize the problem, highlight its distinctive challenges, and propose a systematic methodology to address it. The proposed me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20107;&#20214;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#22238;&#24402;&#12289;&#26631;&#35760;&#25968;&#25454;&#38598;&#38656;&#27714;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20855;&#26377;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2310.16485</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20107;&#20214;&#26816;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20449;&#24687;&#26816;&#32034;&#30340;&#20840;&#38754;Python&#24211;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Python Library for Deep Learning-Based Event Detection in Multivariate Time Series Data and Information Retrieval in NLP. (arXiv:2310.16485v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20107;&#20214;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#22238;&#24402;&#12289;&#26631;&#35760;&#25968;&#25454;&#38598;&#38656;&#27714;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20855;&#26377;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20107;&#20214;&#26816;&#27979;&#22312;&#37329;&#34701;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#31185;&#23398;&#31561;&#21508;&#20010;&#39046;&#22495;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#20934;&#30830;&#22320;&#35782;&#21035;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20107;&#20214;&#23545;&#20110;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#12289;&#26816;&#27979;&#24322;&#24120;&#21644;&#39044;&#27979;&#26410;&#26469;&#36235;&#21183;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#20107;&#20214;&#26816;&#27979;&#24050;&#32463;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20854;&#20013;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26159;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#20294;&#22312;&#36825;&#20010;&#39046;&#22495;&#20173;&#26377;&#25913;&#36827;&#21644;&#21019;&#26032;&#30340;&#31354;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20107;&#20214;&#12290;&#19982;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#22235;&#20010;&#29420;&#29305;&#30340;&#21019;&#26032;&#12290;&#39318;&#20808;&#65292;&#23427;&#22522;&#20110;&#22238;&#24402;&#32780;&#19981;&#26159;&#20108;&#20803;&#20998;&#31867;&#12290;&#20854;&#27425;&#65292;&#23427;&#19981;&#38656;&#35201;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#25968;&#25454;&#28857;&#37117;&#26377;&#26631;&#31614;&#65307;&#30456;&#21453;&#65292;&#23427;&#21482;&#38656;&#35201;&#23450;&#20041;&#20026;&#26102;&#38388;&#28857;&#25110;&#26102;&#38388;&#38388;&#38548;&#30340;&#21442;&#32771;&#20107;&#20214;&#12290;&#31532;&#19977;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#22534;&#21472;&#38598;&#25104;&#27169;&#22411;&#35774;&#35745;&#24471;&#21040;&#20102;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event detection in time series data is crucial in various domains, including finance, healthcare, cybersecurity, and science. Accurately identifying events in time series data is vital for making informed decisions, detecting anomalies, and predicting future trends. Despite extensive research exploring diverse methods for event detection in time series, with deep learning approaches being among the most advanced, there is still room for improvement and innovation in this field. In this paper, we present a new deep learning supervised method for detecting events in multivariate time series data. Our method combines four distinct novelties compared to existing deep-learning supervised methods. Firstly, it is based on regression instead of binary classification. Secondly, it does not require labeled datasets where each point is labeled; instead, it only requires reference events defined as time points or intervals of time. Thirdly, it is designed to be robust by using a stacked ensemble l
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#19987;&#23478;&#31574;&#30053;&#36827;&#34892;&#20915;&#31574;&#25351;&#23548;&#30340;&#32534;&#25490;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23545;&#25239;&#24615;&#35774;&#32622;&#20013;&#30340;&#21518;&#24724;&#36793;&#30028;&#32467;&#26524;&#36716;&#31227;&#21040;&#34920;&#26684;&#35774;&#32622;&#19979;&#30340;&#32534;&#25490;&#20013;&#65292;&#25512;&#24191;&#20102;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#30340;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#27934;&#23519;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20851;&#38190;&#28857;&#22312;&#20110;&#20854;&#36879;&#26126;&#30340;&#35777;&#26126;&#12290;&#22312;&#38543;&#26426;&#21305;&#37197;&#29609;&#20855;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#27169;&#25311;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2310.16473</link><description>&lt;p&gt;
&#19987;&#23478;&#30340;&#20132;&#21709;&#26354;&#65306;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#36816;&#29992;&#23545;&#25239;&#24615;&#27934;&#23519;&#21147;&#30340;&#32534;&#25490;
&lt;/p&gt;
&lt;p&gt;
Symphony of experts: orchestration with adversarial insights in reinforcement learning. (arXiv:2310.16473v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16473
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#19987;&#23478;&#31574;&#30053;&#36827;&#34892;&#20915;&#31574;&#25351;&#23548;&#30340;&#32534;&#25490;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23545;&#25239;&#24615;&#35774;&#32622;&#20013;&#30340;&#21518;&#24724;&#36793;&#30028;&#32467;&#26524;&#36716;&#31227;&#21040;&#34920;&#26684;&#35774;&#32622;&#19979;&#30340;&#32534;&#25490;&#20013;&#65292;&#25512;&#24191;&#20102;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#30340;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#27934;&#23519;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20851;&#38190;&#28857;&#22312;&#20110;&#20854;&#36879;&#26126;&#30340;&#35777;&#26126;&#12290;&#22312;&#38543;&#26426;&#21305;&#37197;&#29609;&#20855;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#27169;&#25311;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#24378;&#21270;&#23398;&#20064;&#21033;&#29992;&#20855;&#26377;&#20248;&#21183;&#29305;&#24615;&#30340;&#31574;&#30053;&#20197;&#36798;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#25506;&#32034;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#32534;&#25490;&#30340;&#27010;&#24565;&#26469;&#25506;&#32034;&#36825;&#19968;&#39046;&#22495;&#65292;&#20854;&#20013;&#19968;&#32452;&#65288;&#23569;&#37327;&#65289;&#19987;&#23478;&#31574;&#30053;&#25351;&#23548;&#20915;&#31574;&#65307;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#24314;&#31435;&#20102;&#27492;&#24314;&#27169;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20174;&#23545;&#25239;&#24615;&#35774;&#32622;&#20013;&#36716;&#31227;&#21518;&#24724;&#36793;&#30028;&#32467;&#26524;&#65292;&#22312;&#34920;&#26684;&#35774;&#32622;&#19979;&#24314;&#31435;&#20102;&#32534;&#25490;&#30340;&#20215;&#20540;&#20989;&#25968;&#21518;&#24724;&#36793;&#30028;&#12290;&#25105;&#20204;&#23558;&#23545; Agarwal &#31561;&#20154; [2021, &#31532;5.3&#33410;] &#20013;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#30340;&#20998;&#26512;&#25512;&#24191;&#24182;&#25193;&#23637;&#21040;&#20219;&#24847;&#23545;&#25239;&#24615;&#32858;&#21512;&#31574;&#30053;&#12290;&#25105;&#20204;&#36824;&#23558;&#20854;&#25193;&#23637;&#21040;&#20272;&#35745;&#20248;&#21183;&#20989;&#25968;&#30340;&#24773;&#20917;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#26399;&#26395;&#20540;&#21644;&#39640;&#27010;&#29575;&#19979;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#27934;&#23519;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#28857;&#22312;&#20110;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#32780;&#35328;&#35777;&#26126;&#36739;&#20026;&#36879;&#26126;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#38024;&#23545;&#38543;&#26426;&#21305;&#37197;&#29609;&#20855;&#27169;&#22411;&#36827;&#34892;&#20102;&#27169;&#25311;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured reinforcement learning leverages policies with advantageous properties to reach better performance, particularly in scenarios where exploration poses challenges. We explore this field through the concept of orchestration, where a (small) set of expert policies guides decision-making; the modeling thereof constitutes our first contribution. We then establish value-functions regret bounds for orchestration in the tabular setting by transferring regret-bound results from adversarial settings. We generalize and extend the analysis of natural policy gradient in Agarwal et al. [2021, Section 5.3] to arbitrary adversarial aggregation strategies. We also extend it to the case of estimated advantage functions, providing insights into sample complexity both in expectation and high probability. A key point of our approach lies in its arguably more transparent proofs compared to existing methods. Finally, we present simulations for a stochastic matching toy model.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NDP4ND&#30340;&#31070;&#32463;ODE&#36827;&#31243;&#65292;&#20197;&#20174;&#31232;&#32570;&#35266;&#27979;&#20013;&#23398;&#20064;&#36830;&#32493;&#32593;&#32476;&#26032;&#20852;&#21160;&#24577;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24314;&#31435;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#38543;&#26426;&#32593;&#32476;&#21160;&#21147;&#23398;&#65292;&#20811;&#26381;&#20102;&#30001;&#20110;&#31232;&#30095;&#12289;&#19981;&#35268;&#21017;&#37319;&#26679;&#12289;&#37096;&#20998;&#21644;&#22122;&#22768;&#35266;&#27979;&#32780;&#23548;&#33268;&#30340;&#23398;&#20064;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2310.16466</link><description>&lt;p&gt;
&#20174;&#26377;&#38480;&#35266;&#27979;&#20013;&#23398;&#20064;&#36830;&#32493;&#32593;&#32476;&#26032;&#20852;&#21160;&#24577;&#30340;&#25968;&#25454;&#33258;&#36866;&#24212;&#38543;&#26426;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning Continuous Network Emerging Dynamics from Scarce Observations via Data-Adaptive Stochastic Processes. (arXiv:2310.16466v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NDP4ND&#30340;&#31070;&#32463;ODE&#36827;&#31243;&#65292;&#20197;&#20174;&#31232;&#32570;&#35266;&#27979;&#20013;&#23398;&#20064;&#36830;&#32493;&#32593;&#32476;&#26032;&#20852;&#21160;&#24577;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24314;&#31435;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#38543;&#26426;&#32593;&#32476;&#21160;&#21147;&#23398;&#65292;&#20811;&#26381;&#20102;&#30001;&#20110;&#31232;&#30095;&#12289;&#19981;&#35268;&#21017;&#37319;&#26679;&#12289;&#37096;&#20998;&#21644;&#22122;&#22768;&#35266;&#27979;&#32780;&#23548;&#33268;&#30340;&#23398;&#20064;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#35777;&#32467;&#26500;&#21644;&#26102;&#31354;&#35266;&#27979;&#25968;&#25454;&#23398;&#20064;&#32593;&#32476;&#21160;&#24577;&#23545;&#20110;&#25581;&#31034;&#22797;&#26434;&#32593;&#32476;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#30340;&#20132;&#20114;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#21482;&#38024;&#23545;&#23398;&#20064;&#30001;&#29305;&#23450;&#24120;&#24494;&#20998;&#26041;&#31243;&#23454;&#20363;&#29983;&#25104;&#30340;&#32593;&#32476;&#21160;&#24577;&#34892;&#20026;&#65292;&#23545;&#20110;&#26032;&#30340;&#26041;&#31243;&#23454;&#20363;&#25928;&#26524;&#19981;&#20339;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#23494;&#38598;&#35266;&#27979;&#12290;&#35266;&#27979;&#25968;&#25454;&#65292;&#23588;&#20854;&#26159;&#26469;&#33258;&#32593;&#32476;&#26032;&#20852;&#21160;&#24577;&#30340;&#25968;&#25454;&#36890;&#24120;&#24456;&#38590;&#33719;&#24471;&#65292;&#36825;&#32473;&#27169;&#22411;&#23398;&#20064;&#24102;&#26469;&#20102;&#22256;&#25200;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#36890;&#36807;&#31232;&#30095;&#12289;&#19981;&#35268;&#21017;&#37319;&#26679;&#12289;&#37096;&#20998;&#21644;&#22122;&#22768;&#35266;&#27979;&#26469;&#23398;&#20064;&#20934;&#30830;&#30340;&#32593;&#32476;&#21160;&#24577;&#20173;&#28982;&#26159;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;ODE&#36827;&#31243;&#29992;&#20110;&#32593;&#32476;&#21160;&#21147;&#23398;&#65288;NDP4ND&#65289;&#65292;&#36825;&#26159;&#19968;&#31867;&#30001;&#38543;&#26426;&#25968;&#25454;&#33258;&#36866;&#24212;&#32593;&#32476;&#21160;&#21147;&#23398;&#25511;&#21046;&#30340;&#38543;&#26426;&#36807;&#31243;&#65292;&#20197;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#24182;&#20174;&#26377;&#38480;&#35266;&#27979;&#20013;&#23398;&#20064;&#36830;&#32493;&#32593;&#32476;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning network dynamics from the empirical structure and spatio-temporal observation data is crucial to revealing the interaction mechanisms of complex networks in a wide range of domains. However, most existing methods only aim at learning network dynamic behaviors generated by a specific ordinary differential equation instance, resulting in ineffectiveness for new ones, and generally require dense observations. The observed data, especially from network emerging dynamics, are usually difficult to obtain, which brings trouble to model learning. Therefore, how to learn accurate network dynamics with sparse, irregularly-sampled, partial, and noisy observations remains a fundamental challenge. We introduce Neural ODE Processes for Network Dynamics (NDP4ND), a new class of stochastic processes governed by stochastic data-adaptive network dynamics, to overcome the challenge and learn continuous network dynamics from scarce observations. Intensive experiments conducted on various network 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#30524;&#28145;&#24230;&#20272;&#35745;&#26041;&#27861;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20154;&#31867;&#24863;&#30693;&#28145;&#24230;&#30340;&#19968;&#20010;&#26368;&#37325;&#35201;&#30340;&#35270;&#35273;&#32447;&#32034;&#8212;&#8212;&#30456;&#23545;&#23610;&#23544;&#12290;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#23454;&#39564;&#21644;&#27979;&#35797;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#24179;&#22343;&#20934;&#30830;&#29575;&#32422;&#20026;77%&#30340;&#32467;&#26524;&#65292;&#38388;&#25509;&#25581;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16457</link><description>&lt;p&gt;
&#12298;&#21333;&#30524;&#28145;&#24230;&#20272;&#35745;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#12299;
&lt;/p&gt;
&lt;p&gt;
Towards Explainability in Monocular Depth Estimation. (arXiv:2310.16457v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#30524;&#28145;&#24230;&#20272;&#35745;&#26041;&#27861;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20154;&#31867;&#24863;&#30693;&#28145;&#24230;&#30340;&#19968;&#20010;&#26368;&#37325;&#35201;&#30340;&#35270;&#35273;&#32447;&#32034;&#8212;&#8212;&#30456;&#23545;&#23610;&#23544;&#12290;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#23454;&#39564;&#21644;&#27979;&#35797;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#24179;&#22343;&#20934;&#30830;&#29575;&#32422;&#20026;77%&#30340;&#32467;&#26524;&#65292;&#38388;&#25509;&#25581;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#32500;&#22270;&#20687;&#28145;&#24230;&#20272;&#35745;&#19968;&#30452;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#24191;&#27867;&#30740;&#31350;&#30340;&#35838;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#20986;&#29616;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#19988;&#21462;&#24471;&#20102;&#26497;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;&#26412;&#25991;&#20851;&#27880;&#21333;&#30524;&#28145;&#24230;&#20272;&#35745;&#26041;&#27861;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21363;&#20154;&#31867;&#22914;&#20309;&#24863;&#30693;&#28145;&#24230;&#12290;&#36825;&#39033;&#21021;&#27493;&#30740;&#31350;&#24378;&#35843;&#20102;&#26368;&#26174;&#33879;&#30340;&#35270;&#35273;&#32447;&#32034;&#20043;&#19968;&#65292;&#21363;&#30456;&#23545;&#23610;&#23544;&#65292;&#22312;&#20960;&#20046;&#25152;&#26377;&#35266;&#23519;&#30340;&#22270;&#20687;&#20013;&#37117;&#38750;&#24120;&#31361;&#20986;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#23454;&#39564;&#26469;&#27169;&#25311;&#20154;&#31867;&#23454;&#39564;&#65292;&#24182;&#27979;&#35797;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20197;&#38388;&#25509;&#35780;&#20272;&#22312;&#25152;&#23450;&#20041;&#30340;&#19978;&#19979;&#25991;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27979;&#37327;&#20934;&#30830;&#24615;&#38656;&#35201;&#36827;&#19968;&#27493;&#20851;&#27880;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;&#32422;77%&#65292;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#65292;&#20174;&#32780;&#38388;&#25509;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The estimation of depth in two-dimensional images has long been a challenging and extensively studied subject in computer vision. Recently, significant progress has been made with the emergence of Deep Learning-based approaches, which have proven highly successful. This paper focuses on the explainability in monocular depth estimation methods, in terms of how humans perceive depth. This preliminary study emphasizes on one of the most significant visual cues, the relative size, which is prominent in almost all viewed images. We designed a specific experiment to mimic the experiments in humans and have tested state-of-the-art methods to indirectly assess the explainability in the context defined. In addition, we observed that measuring the accuracy required further attention and a particular approach is proposed to this end. The results show that a mean accuracy of around 77% across methods is achieved, with some of the methods performing markedly better, thus, indirectly revealing their
&lt;/p&gt;</description></item><item><title>ClearMark&#26159;&#31532;&#19968;&#31181;&#20026;&#30452;&#35266;&#20154;&#31867;&#35780;&#20272;&#32780;&#35774;&#35745;&#30340;DNN&#27700;&#21360;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23884;&#20837;&#21487;&#35265;&#27700;&#21360;&#26469;&#20801;&#35768;&#20154;&#31867;&#20915;&#31574;&#65292;&#21516;&#26102;&#20351;&#29992;&#36716;&#32622;&#27169;&#22411;&#26550;&#26500;&#23558;&#27700;&#21360;&#19982;&#20027;&#35201;&#20219;&#21153;&#20132;&#32455;&#22312;&#19968;&#36215;&#12290;</title><link>http://arxiv.org/abs/2310.16453</link><description>&lt;p&gt;
ClearMark&#65306;&#36890;&#36807;&#36716;&#32622;&#27169;&#22411;&#35757;&#32451;&#30340;&#30452;&#35266;&#19988;&#40065;&#26834;&#30340;&#27169;&#22411;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
ClearMark: Intuitive and Robust Model Watermarking via Transposed Model Training. (arXiv:2310.16453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16453
&lt;/p&gt;
&lt;p&gt;
ClearMark&#26159;&#31532;&#19968;&#31181;&#20026;&#30452;&#35266;&#20154;&#31867;&#35780;&#20272;&#32780;&#35774;&#35745;&#30340;DNN&#27700;&#21360;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23884;&#20837;&#21487;&#35265;&#27700;&#21360;&#26469;&#20801;&#35768;&#20154;&#31867;&#20915;&#31574;&#65292;&#21516;&#26102;&#20351;&#29992;&#36716;&#32622;&#27169;&#22411;&#26550;&#26500;&#23558;&#27700;&#21360;&#19982;&#20027;&#35201;&#20219;&#21153;&#20132;&#32455;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#25968;&#25454;&#37319;&#38598;&#21644;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#38656;&#35201;&#20184;&#20986;&#24040;&#22823;&#30340;&#21162;&#21147;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#23646;&#20110;&#27169;&#22411;&#21019;&#24314;&#32773;&#30340;&#30693;&#35782;&#20135;&#26435;&#12290;&#22240;&#27492;&#65292;&#26410;&#32463;&#25480;&#26435;&#30340;&#20351;&#29992;&#12289;&#30423;&#31363;&#25110;&#20462;&#25913;&#21487;&#33021;&#23548;&#33268;&#27861;&#24459;&#21518;&#26524;&#12290;&#29616;&#26377;&#30340;DNN&#27700;&#21360;&#26041;&#27861;&#24448;&#24448;&#38750;&#30452;&#35266;&#65292;&#23884;&#20837;&#20154;&#30524;&#19981;&#21487;&#35265;&#30340;&#26631;&#35760;&#65292;&#38656;&#35201;&#23545;&#32570;&#20047;&#20154;&#21487;&#29702;&#35299;&#23646;&#24615;&#30340;&#31639;&#27861;&#35780;&#20272;&#20135;&#29983;&#20449;&#20219;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#21018;&#24615;&#30340;&#38408;&#20540;&#65292;&#26131;&#21463;&#21040;&#37096;&#20998;&#27700;&#21360;&#25830;&#38500;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ClearMark&#65292;&#36825;&#26159;&#31532;&#19968;&#31181;&#20026;&#30452;&#35266;&#20154;&#31867;&#35780;&#20272;&#32780;&#35774;&#35745;&#30340;DNN&#27700;&#21360;&#26041;&#27861;&#12290;ClearMark&#23884;&#20837;&#21487;&#35265;&#27700;&#21360;&#65292;&#20351;&#20154;&#31867;&#21487;&#20197;&#36827;&#34892;&#20915;&#31574;&#65292;&#32780;&#26080;&#38656;&#21018;&#24615;&#30340;&#20540;&#38408;&#20540;&#65292;&#21516;&#26102;&#20801;&#35768;&#25216;&#26415;&#36741;&#21161;&#35780;&#20272;&#12290;ClearMark&#23450;&#20041;&#20102;&#19968;&#20010;&#36716;&#32622;&#27169;&#22411;&#26550;&#26500;&#65292;&#21487;&#20197;&#23558;&#27700;&#21360;&#19982;&#25152;&#26377;&#27169;&#22411;&#21442;&#25968;&#20013;&#30340;&#20027;&#35201;&#20219;&#21153;&#20132;&#32455;&#22312;&#19968;&#36215;&#12290;&#19982;&#29616;&#26377;&#30340;&#27700;&#21360;&#26041;&#27861;&#30456;&#27604;&#65292;
&lt;/p&gt;
&lt;p&gt;
Due to costly efforts during data acquisition and model training, Deep Neural Networks (DNNs) belong to the intellectual property of the model creator. Hence, unauthorized use, theft, or modification may lead to legal repercussions. Existing DNN watermarking methods for ownership proof are often non-intuitive, embed human-invisible marks, require trust in algorithmic assessment that lacks human-understandable attributes, and rely on rigid thresholds, making it susceptible to failure in cases of partial watermark erasure.  This paper introduces ClearMark, the first DNN watermarking method designed for intuitive human assessment. ClearMark embeds visible watermarks, enabling human decision-making without rigid value thresholds while allowing technology-assisted evaluations. ClearMark defines a transposed model architecture allowing to use of the model in a backward fashion to interwove the watermark with the main task within all model parameters. Compared to existing watermarking methods
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PEARLM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#24320;&#23637;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#33616;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23545;&#39044;&#35757;&#32451;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20381;&#36182;&#20197;&#21450;&#26410;&#20805;&#20998;&#21033;&#29992;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30456;&#20114;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#65292;&#36824;&#36991;&#20813;&#20102;&#29983;&#25104;&#19981;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2310.16452</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#25512;&#33616;&#20013;&#30340;&#24544;&#23454;&#36335;&#24452;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Faithful Path Language Modelling for Explainable Recommendation over Knowledge Graph. (arXiv:2310.16452v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PEARLM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#24320;&#23637;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#33616;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23545;&#39044;&#35757;&#32451;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20381;&#36182;&#20197;&#21450;&#26410;&#20805;&#20998;&#21033;&#29992;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30456;&#20114;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#65292;&#36824;&#36991;&#20813;&#20102;&#29983;&#25104;&#19981;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#36335;&#24452;&#25512;&#29702;&#26041;&#27861;&#22312;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#36879;&#26126;&#24230;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PEARLM&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#26377;&#25928;&#25429;&#33719;&#29992;&#25143;&#34892;&#20026;&#21644;&#20135;&#21697;&#31471;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#20174;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#36335;&#24452;&#20013;&#23398;&#20064;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#24182;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#32479;&#19968;&#22312;&#21516;&#19968;&#20248;&#21270;&#31354;&#38388;&#20013;&#12290;&#24207;&#21015;&#35299;&#30721;&#30340;&#32422;&#26463;&#20445;&#35777;&#20102;&#36335;&#24452;&#23545;&#30693;&#35782;&#22270;&#35889;&#30340;&#24544;&#23454;&#24615;&#12290;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Path reasoning methods over knowledge graphs have gained popularity for their potential to improve transparency in recommender systems. However, the resulting models still rely on pre-trained knowledge graph embeddings, fail to fully exploit the interdependence between entities and relations in the KG for recommendation, and may generate inaccurate explanations. In this paper, we introduce PEARLM, a novel approach that efficiently captures user behaviour and product-side knowledge through language modelling. With our approach, knowledge graph embeddings are directly learned from paths over the KG by the language model, which also unifies entities and relations in the same optimisation space. Constraints on the sequence decoding additionally guarantee path faithfulness with respect to the KG. Experiments on two datasets show the effectiveness of our approach compared to state-of-the-art baselines. Source code and datasets: AVAILABLE AFTER GETTING ACCEPTED.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25581;&#31034;&#20102;&#22312;&#32447;&#24615;&#32593;&#32476;&#20013;&#30340;&#39046;&#24735;&#29616;&#35937;&#65292;&#30740;&#31350;&#20102;&#39046;&#24735;&#26102;&#38388;&#19982;&#36755;&#20837;&#36755;&#20986;&#32500;&#24230;&#12289;&#35757;&#32451;&#26679;&#26412;&#37327;&#12289;&#27491;&#21017;&#21270;&#21644;&#32593;&#32476;&#21021;&#22987;&#21270;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#27867;&#21270;&#20934;&#30830;&#24615;&#30340;&#22823;&#24133;&#25552;&#21319;&#24182;&#19981;&#19968;&#23450;&#24847;&#21619;&#30528;&#20174;&#8220;&#35760;&#24518;&#8221;&#21040;&#8220;&#29702;&#35299;&#8221;&#30340;&#36716;&#21464;&#12290;</title><link>http://arxiv.org/abs/2310.16441</link><description>&lt;p&gt;
&#22312;&#32447;&#24615;&#20272;&#35745;&#22120;&#20013;&#30340;&#39046;&#24735;&#8212;&#8212;&#19968;&#20010;&#21487;&#35299;&#30340;&#27169;&#22411;&#22312;&#19981;&#29702;&#35299;&#30340;&#24773;&#20917;&#19979;&#39046;&#24735;&#12290; &#65288;arXiv&#65306;2310.16441v1 [stat.ML]&#65289;
&lt;/p&gt;
&lt;p&gt;
Grokking in Linear Estimators -- A Solvable Model that Groks without Understanding. (arXiv:2310.16441v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16441
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25581;&#31034;&#20102;&#22312;&#32447;&#24615;&#32593;&#32476;&#20013;&#30340;&#39046;&#24735;&#29616;&#35937;&#65292;&#30740;&#31350;&#20102;&#39046;&#24735;&#26102;&#38388;&#19982;&#36755;&#20837;&#36755;&#20986;&#32500;&#24230;&#12289;&#35757;&#32451;&#26679;&#26412;&#37327;&#12289;&#27491;&#21017;&#21270;&#21644;&#32593;&#32476;&#21021;&#22987;&#21270;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#27867;&#21270;&#20934;&#30830;&#24615;&#30340;&#22823;&#24133;&#25552;&#21319;&#24182;&#19981;&#19968;&#23450;&#24847;&#21619;&#30528;&#20174;&#8220;&#35760;&#24518;&#8221;&#21040;&#8220;&#29702;&#35299;&#8221;&#30340;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#24735;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#29616;&#35937;&#65292;&#25351;&#30340;&#26159;&#27169;&#22411;&#22312;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#21518;&#20173;&#33021;&#27867;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#35299;&#26512;&#21644;&#25968;&#20540;&#26041;&#27861;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#31616;&#21333;&#30340;&#24072;&#29983;&#35774;&#32622;&#20013;&#65292;&#20855;&#26377;&#39640;&#26031;&#36755;&#20837;&#30340;&#32447;&#24615;&#32593;&#32476;&#25191;&#34892;&#32447;&#24615;&#20219;&#21153;&#26102;&#65292;&#39046;&#24735;&#20063;&#20250;&#20986;&#29616;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#23436;&#25972;&#30340;&#35757;&#32451;&#21160;&#21147;&#23398;&#65292;&#20197;&#35757;&#32451;&#21644;&#27867;&#21270;&#25968;&#25454;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#39046;&#24735;&#26102;&#38388;&#22914;&#20309;&#21462;&#20915;&#20110;&#36755;&#20837;&#21644;&#36755;&#20986;&#32500;&#24230;&#65292;&#35757;&#32451;&#26679;&#26412;&#37327;&#65292;&#27491;&#21017;&#21270;&#21644;&#32593;&#32476;&#21021;&#22987;&#21270;&#30340;&#31934;&#30830;&#39044;&#27979;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27867;&#21270;&#20934;&#30830;&#24615;&#30340;&#24613;&#21095;&#22686;&#21152;&#21487;&#33021;&#24182;&#19981;&#24847;&#21619;&#30528;&#20174;&#8220;&#35760;&#24518;&#8221;&#21040;&#8220;&#29702;&#35299;&#8221;&#30340;&#36716;&#21464;&#65292;&#32780;&#21482;&#26159;&#20934;&#30830;&#24615;&#24230;&#37327;&#30340;&#20135;&#29289;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#25105;&#20204;&#35745;&#31639;&#30340;&#32463;&#39564;&#35777;&#23454;&#65292;&#24182;&#21021;&#27493;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26576;&#20123;&#39044;&#27979;&#20063;&#36866;&#29992;&#20110;&#20855;&#26377;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#26356;&#28145;&#23618;&#27425;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grokking is the intriguing phenomenon where a model learns to generalize long after it has fit the training data. We show both analytically and numerically that grokking can surprisingly occur in linear networks performing linear tasks in a simple teacher-student setup with Gaussian inputs. In this setting, the full training dynamics is derived in terms of the training and generalization data covariance matrix. We present exact predictions on how the grokking time depends on input and output dimensionality, train sample size, regularization, and network initialization. We demonstrate that the sharp increase in generalization accuracy may not imply a transition from "memorization" to "understanding", but can simply be an artifact of the accuracy measure. We provide empirical verification for our calculations, along with preliminary results indicating that some predictions also hold for deeper networks, with non-linear activations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21508;&#21521;&#24322;&#24615;&#30340;&#25345;&#20037;&#21516;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#21270;&#24213;&#23618;&#31354;&#38388;&#19978;&#30340;&#36317;&#31163;&#20989;&#25968;&#24182;&#20998;&#26512;&#25345;&#20037;&#22270;&#30340;&#20559;&#31227;&#65292;&#21487;&#20197;&#25552;&#21462;&#39069;&#22806;&#30340;&#25299;&#25169;&#21644;&#20960;&#20309;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2310.16437</link><description>&lt;p&gt;
&#38750;&#21508;&#21521;&#24322;&#24615;&#30340;&#25345;&#20037;&#21516;&#35843;&#65306;&#21033;&#29992;&#25345;&#20037;&#21516;&#35843;&#30340;&#24230;&#37327;&#20381;&#36182;&#24615;
&lt;/p&gt;
&lt;p&gt;
Non-isotropic Persistent Homology: Leveraging the Metric Dependency of PH. (arXiv:2310.16437v1 [math.AT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21508;&#21521;&#24322;&#24615;&#30340;&#25345;&#20037;&#21516;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#21270;&#24213;&#23618;&#31354;&#38388;&#19978;&#30340;&#36317;&#31163;&#20989;&#25968;&#24182;&#20998;&#26512;&#25345;&#20037;&#22270;&#30340;&#20559;&#31227;&#65292;&#21487;&#20197;&#25552;&#21462;&#39069;&#22806;&#30340;&#25299;&#25169;&#21644;&#20960;&#20309;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#20037;&#21516;&#35843;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#65292;&#23427;&#22522;&#20110;&#25351;&#23450;&#30340;&#28388;&#27874;&#22120;&#21019;&#24314;&#19968;&#20010;&#23545;&#28857;&#20113;&#30340;&#25299;&#25169;&#24615;&#36136;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;&#22823;&#22810;&#25968;&#29992;&#20110;&#25345;&#20037;&#21516;&#35843;&#30340;&#28388;&#27874;&#22120;&#65288;&#38544;&#21547;&#22320;&#65289;&#20381;&#36182;&#20110;&#36873;&#25321;&#30340;&#24230;&#37327;&#65292;&#36890;&#24120;&#20250;&#20197;$\mathbb{R}^n$&#19978;&#30340;&#26631;&#20934;&#27431;&#27663;&#24230;&#37327;&#20026;&#36873;&#25321;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#35797;&#22270;&#20351;&#29992;&#36317;&#31163;&#21040;&#24230;&#37327;&#20989;&#25968;&#26469;&#25581;&#31034;&#28857;&#20113;&#19978;&#30340;&#8220;&#30495;&#23454;&#8221;&#24230;&#37327;&#65292;&#20197;&#33719;&#24471;&#26356;&#26377;&#24847;&#20041;&#30340;&#25345;&#20037;&#21516;&#35843;&#32467;&#26524;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#26367;&#20195;&#24615;&#30475;&#27861;&#65306;&#25105;&#20204;&#35748;&#20026;&#24403;&#23558;&#25345;&#20037;&#21516;&#35843;&#38480;&#21046;&#22312;&#19968;&#20010;&#21333;&#19968;&#65288;&#27491;&#30830;&#30340;&#65289;&#36317;&#31163;&#20989;&#25968;&#19978;&#26102;&#65292;&#28857;&#20113;&#19978;&#30340;&#20449;&#24687;&#20250;&#20002;&#22833;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#24213;&#23618;&#31354;&#38388;&#19978;&#21464;&#21270;&#36317;&#31163;&#20989;&#25968;&#24182;&#20998;&#26512;&#30456;&#24212;&#30340;&#25345;&#20037;&#22270;&#20013;&#30340;&#20559;&#31227;&#65292;&#21487;&#20197;&#25552;&#21462;&#39069;&#22806;&#30340;&#25299;&#25169;&#21644;&#20960;&#20309;&#20449;&#24687;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#38750;&#21508;&#21521;&#24322;&#24615;&#25345;&#20037;&#21516;&#35843;&#33021;&#22815;&#23454;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Persistent Homology is a widely used topological data analysis tool that creates a concise description of the topological properties of a point cloud based on a specified filtration. Most filtrations used for persistent homology depend (implicitly) on a chosen metric, which is typically agnostically chosen as the standard Euclidean metric on $\mathbb{R}^n$. Recent work has tried to uncover the 'true' metric on the point cloud using distance-to-measure functions, in order to obtain more meaningful persistent homology results. Here we propose an alternative look at this problem: we posit that information on the point cloud is lost when restricting persistent homology to a single (correct) distance function. Instead, we show how by varying the distance function on the underlying space and analysing the corresponding shifts in the persistence diagrams, we can extract additional topological and geometrical information. Finally, we numerically show that non-isotropic persistent homology can 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FlatMatch&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20132;&#21449;&#38155;&#21033;&#24230;&#24230;&#37327;&#26469;&#30830;&#20445;&#26631;&#35760;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20043;&#38388;&#30340;&#19968;&#33268;&#23398;&#20064;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16412</link><description>&lt;p&gt;
FlatMatch: &#20351;&#29992;&#20132;&#21449;&#38155;&#21033;&#24230;&#26469;&#34900;&#25509;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FlatMatch: Bridging Labeled Data and Unlabeled Data with Cross-Sharpness for Semi-Supervised Learning. (arXiv:2310.16412v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FlatMatch&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20132;&#21449;&#38155;&#21033;&#24230;&#24230;&#37327;&#26469;&#30830;&#20445;&#26631;&#35760;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20043;&#38388;&#30340;&#19968;&#33268;&#23398;&#20064;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#19968;&#30452;&#26159;&#19968;&#31181;&#26377;&#25928;&#21033;&#29992;&#20016;&#23500;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#19982;&#26497;&#20854;&#31232;&#32570;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;SSL&#26041;&#27861;&#36890;&#24120;&#22522;&#20110;&#19981;&#21516;&#25968;&#25454;&#36716;&#25442;&#20043;&#38388;&#30340;&#23454;&#20363;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#23545;&#26631;&#35760;&#25968;&#25454;&#30340;&#26631;&#31614;&#25351;&#23548;&#24456;&#38590;&#20256;&#25773;&#21040;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#12290;&#32467;&#26524;&#65292;&#26631;&#35760;&#25968;&#25454;&#19978;&#30340;&#23398;&#20064;&#36807;&#31243;&#27604;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#30340;&#23398;&#20064;&#36807;&#31243;&#24555;&#24471;&#22810;&#65292;&#24456;&#21487;&#33021;&#38519;&#20837;&#19981;&#21033;&#20110;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#65292;&#23548;&#33268;&#27425;&#20248;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FlatMatch&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20132;&#21449;&#38155;&#21033;&#24230;&#24230;&#37327;&#26469;&#30830;&#20445;&#20004;&#20010;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#19968;&#33268;&#23398;&#20064;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22686;&#21152;&#20102;&#26631;&#35760;&#25968;&#25454;&#19978;&#30340;&#32463;&#39564;&#39118;&#38505;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#26368;&#22351;&#24773;&#20917;&#27169;&#22411;&#65292;&#21363;&#38656;&#35201;&#22686;&#24378;&#30340;&#22833;&#36133;&#24773;&#20917;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#20016;&#23500;&#24615;&#65292;&#25105;&#20204;&#24809;&#32602;&#26368;&#22351;&#24773;&#20917;&#27169;&#22411;&#19982;&#21407;&#22987;&#27169;&#22411;&#20043;&#38388;&#30340;&#39044;&#27979;&#24046;&#24322;&#65288;&#21363;&#20132;&#21449;&#38155;&#21033;&#24230;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Learning (SSL) has been an effective way to leverage abundant unlabeled data with extremely scarce labeled data. However, most SSL methods are commonly based on instance-wise consistency between different data transformations. Therefore, the label guidance on labeled data is hard to be propagated to unlabeled data. Consequently, the learning process on labeled data is much faster than on unlabeled data which is likely to fall into a local minima that does not favor unlabeled data, leading to sub-optimal generalization performance. In this paper, we propose FlatMatch which minimizes a cross-sharpness measure to ensure consistent learning performance between the two datasets. Specifically, we increase the empirical risk on labeled data to obtain a worst-case model which is a failure case that needs to be enhanced. Then, by leveraging the richness of unlabeled data, we penalize the prediction difference (i.e., cross-sharpness) between the worst-case model and the original 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;AlphaZero&#20013;&#25552;&#21462;&#26032;&#30340;&#22269;&#38469;&#35937;&#26827;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#27010;&#24565;&#21487;&#20197;&#34987;&#39030;&#32423;&#22269;&#38469;&#35937;&#26827;&#22823;&#24072;&#25152;&#23398;&#20064;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.16410</link><description>&lt;p&gt;
&#24357;&#21512;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30693;&#35782;&#30340;&#24046;&#36317;&#65306;&#22312;AlphaZero&#20013;&#36827;&#34892;&#27010;&#24565;&#21457;&#29616;&#21644;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in AlphaZero. (arXiv:2310.16410v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;AlphaZero&#20013;&#25552;&#21462;&#26032;&#30340;&#22269;&#38469;&#35937;&#26827;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#27010;&#24565;&#21487;&#20197;&#34987;&#39030;&#32423;&#22269;&#38469;&#35937;&#26827;&#22823;&#24072;&#25152;&#23398;&#20064;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#36229;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36827;&#19968;&#27493;&#25552;&#21319;&#20154;&#31867;&#30693;&#35782;&#21644;&#25552;&#39640;&#20154;&#31867;&#19987;&#23478;&#34920;&#29616;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39640;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#21253;&#21547;&#30340;&#30693;&#35782;&#24448;&#24448;&#38590;&#20197;&#25552;&#21462;&#65292;&#20063;&#21487;&#33021;&#38590;&#20197;&#29702;&#35299;&#25110;&#23398;&#20064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;AlphaZero&#20013;&#25552;&#21462;&#26032;&#30340;&#22269;&#38469;&#35937;&#26827;&#27010;&#24565;&#65292;AlphaZero&#26159;&#19968;&#20010;&#36890;&#36807;&#33258;&#25105;&#23545;&#24328;&#32780;&#25484;&#25569;&#22269;&#38469;&#35937;&#26827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;AlphaZero&#21487;&#33021;&#32534;&#30721;&#20102;&#36229;&#36234;&#29616;&#26377;&#20154;&#31867;&#30693;&#35782;&#30340;&#30693;&#35782;&#65292;&#20294;&#36825;&#20123;&#30693;&#35782;&#26368;&#32456;&#24182;&#19981;&#36229;&#20986;&#20154;&#31867;&#30340;&#29702;&#35299;&#33539;&#22260;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#23398;&#20064;&#12290;&#22312;&#20154;&#31867;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#27010;&#24565;&#26159;&#21487;&#20197;&#34987;&#39030;&#32423;&#22269;&#38469;&#35937;&#26827;&#22823;&#24072;&#25152;&#23398;&#20064;&#30340;&#65292;&#22240;&#20026;&#22235;&#21517;&#39030;&#32423;&#22269;&#38469;&#35937;&#26827;&#22823;&#24072;&#22312;&#35299;&#20915;&#25152;&#21576;&#29616;&#30340;&#27010;&#24565;&#21407;&#22411;&#20301;&#32622;&#26102;&#26174;&#31034;&#20986;&#20102;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) systems have made remarkable progress, attaining super-human performance across various domains. This presents us with an opportunity to further human knowledge and improve human expert performance by leveraging the hidden knowledge encoded within these highly performant AI systems. Yet, this knowledge is often hard to extract, and may be hard to understand or learn from. Here, we show that this is possible by proposing a new method that allows us to extract new chess concepts in AlphaZero, an AI system that mastered the game of chess via self-play without human supervision. Our analysis indicates that AlphaZero may encode knowledge that extends beyond the existing human knowledge, but knowledge that is ultimately not beyond human grasp, and can be successfully learned from. In a human study, we show that these concepts are learnable by top human experts, as four top chess grandmasters show improvements in solving the presented concept prototype positions. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#25512;&#33616;&#31995;&#32479;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20197;&#23454;&#29616;&#39034;&#24207;&#25512;&#33616;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#32771;&#34385;&#21333;&#38190;&#24773;&#20917;&#65292;&#32780;&#24573;&#30053;&#20102;&#22810;&#38190;&#20540;&#25968;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#36129;&#29486;&#22312;&#20110;&#35299;&#20915;&#20102;&#23454;&#38469;&#24212;&#29992;&#20013;&#22810;&#38190;&#20540;&#25968;&#25454;&#30340;&#25512;&#33616;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.16409</link><description>&lt;p&gt;
&#34701;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#22810;&#38190;&#20540;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Multiple Key-value Strategy in Recommendation Systems Incorporating Large Language Model. (arXiv:2310.16409v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16409
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#25512;&#33616;&#31995;&#32479;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20197;&#23454;&#29616;&#39034;&#24207;&#25512;&#33616;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#32771;&#34385;&#21333;&#38190;&#24773;&#20917;&#65292;&#32780;&#24573;&#30053;&#20102;&#22810;&#38190;&#20540;&#25968;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#36129;&#29486;&#22312;&#20110;&#35299;&#20915;&#20102;&#23454;&#38469;&#24212;&#29992;&#20013;&#22810;&#38190;&#20540;&#25968;&#25454;&#30340;&#25512;&#33616;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#28385;&#36275;&#20114;&#32852;&#32593;&#24212;&#29992;&#20013;&#29992;&#25143;&#20449;&#24687;&#38656;&#27714;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36890;&#24120;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#23884;&#20837;&#32454;&#33410;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#21462;&#24471;&#37325;&#22823;&#31361;&#30772;&#12290;&#22240;&#27492;&#65292;&#23558;&#25512;&#33616;&#31995;&#32479;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#22909;&#22320;&#32467;&#21512;&#36215;&#26469;&#25104;&#20026;&#20102;&#26032;&#20852;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#23613;&#31649;&#19968;&#20123;&#29616;&#26377;&#24037;&#20316;&#23545;&#27492;&#38382;&#39064;&#26377;&#25152;&#36129;&#29486;&#65292;&#20294;&#20027;&#35201;&#32771;&#34385;&#21333;&#38190;&#24773;&#20917;&#65288;&#22914;&#21382;&#21490;&#20132;&#20114;&#65289;&#65292;&#29305;&#21035;&#26159;&#22312;&#39034;&#24207;&#25512;&#33616;&#20013;&#65292;&#22810;&#38190;&#20540;&#25968;&#25454;&#30340;&#24773;&#20917;&#34987;&#31616;&#21333;&#24573;&#30053;&#12290;&#28982;&#32780;&#65292;&#22810;&#38190;&#20540;&#25968;&#25454;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#20027;&#27969;&#22330;&#26223;&#65292;&#29992;&#25143;&#30340;&#20449;&#24687;&#65288;&#22914;&#24180;&#40836;&#12289;&#32844;&#19994;&#31561;&#65289;&#21644;&#29289;&#21697;&#30340;&#20449;&#24687;&#65288;&#22914;&#26631;&#39064;&#12289;&#31867;&#21035;&#31561;&#65289;&#20855;&#26377;&#22810;&#20010;&#38190;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#22522;&#20110;&#22810;&#38190;&#20540;&#25968;&#25454;&#23454;&#29616;&#39034;&#24207;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation system (RS) plays significant roles in matching users information needs for Internet applications, and it usually utilizes the vanilla neural network as the backbone to handle embedding details. Recently, the large language model (LLM) has exhibited emergent abilities and achieved great breakthroughs both in the CV and NLP communities. Thus, it is logical to incorporate RS with LLM better, which has become an emerging research direction. Although some existing works have made their contributions to this issue, they mainly consider the single key situation (e.g. historical interactions), especially in sequential recommendation. The situation of multiple key-value data is simply neglected. This significant scenario is mainstream in real practical applications, where the information of users (e.g. age, occupation, etc) and items (e.g. title, category, etc) has more than one key. Therefore, we aim to implement sequential recommendations based on multiple key-value data by in
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#25299;&#25169;&#24863;&#30693;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#30340;&#27867;&#21270;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedGMIR&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16407</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#25299;&#25169;&#24863;&#30693;&#24322;&#26500;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#22312;&#22122;&#22768;&#36890;&#36947;&#19978;&#30340;&#27867;&#21270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Information-Theoretic Generalization Analysis for Topology-aware Heterogeneous Federated Edge Learning over Noisy Channels. (arXiv:2310.16407v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16407
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#25299;&#25169;&#24863;&#30693;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#30340;&#27867;&#21270;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedGMIR&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36793;&#32536;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#26080;&#32447;&#32593;&#32476;&#19978;&#30340;&#32852;&#37030;&#23398;&#20064;&#37096;&#32626;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#34987;&#31216;&#20026;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#65288;FEEL&#65289;&#12290;&#22312;FEEL&#20013;&#65292;&#31227;&#21160;&#35774;&#22791;&#36890;&#36807;&#22122;&#22768;&#36890;&#36947;&#20256;&#36755;&#27169;&#22411;&#21442;&#25968;&#21644;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#25910;&#38598;&#25968;&#25454;&#65292;&#36825;&#32473;&#35757;&#32451;&#27169;&#22411;&#30340;&#27867;&#21270;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#35774;&#22791;&#21487;&#20197;&#36890;&#36807;&#35774;&#22791;&#38388;&#36890;&#20449;&#36827;&#34892;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65292;&#32780;&#36830;&#25509;&#35774;&#22791;&#30340;&#36890;&#20449;&#25299;&#25169;&#20063;&#24433;&#21709;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26368;&#36817;&#30340;&#29702;&#35770;&#30740;&#31350;&#22312;&#24320;&#23637;&#27867;&#21270;&#20998;&#26512;&#26102;&#24573;&#35270;&#20102;&#25152;&#26377;&#36825;&#20123;&#25928;&#24212;&#30340;&#32435;&#20837;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#25299;&#25169;&#24863;&#30693;FEEL&#30340;&#27867;&#21270;&#20998;&#26512;&#26041;&#27861;&#65292;&#32771;&#34385;&#21040;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#22122;&#22768;&#36890;&#36947;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32852;&#37030;&#20840;&#23616;&#20114;&#20449;&#24687;&#20943;&#23569;&#65288;FedGMIR&#65289;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid growth of edge intelligence, the deployment of federated learning (FL) over wireless networks has garnered increasing attention, which is called Federated Edge Learning (FEEL). In FEEL, both mobile devices transmitting model parameters over noisy channels and collecting data in diverse environments pose challenges to the generalization of trained models. Moreover, devices can engage in decentralized FL via Device-to-Device communication while the communication topology of connected devices also impacts the generalization of models. Most recent theoretical studies overlook the incorporation of all these effects into FEEL when developing generalization analyses. In contrast, our work presents an information-theoretic generalization analysis for topology-aware FEEL in the presence of data heterogeneity and noisy channels. Additionally, we propose a novel regularization method called Federated Global Mutual Information Reduction (FedGMIR) to enhance the performance of models
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21442;&#25968;&#21270;&#21644;&#29983;&#25104;&#22810;&#20010;&#22270;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20165;&#20351;&#29992;&#21333;&#20010;&#35266;&#23519;&#22270;&#30340;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#22312;&#33410;&#28857;&#20998;&#31867;&#21644;&#22270;&#22238;&#24402;&#20219;&#21153;&#19978;&#65292;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#32593;&#32476;&#21442;&#25968;&#65292;&#22312;&#22810;&#20010;&#22270;&#19978;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#32467;&#21512;PAC-Bayesian&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#21462;&#24471;&#20102;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.16401</link><description>&lt;p&gt;
&#20855;&#26377;&#21442;&#25968;&#21270;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks with a Distribution of Parametrized Graphs. (arXiv:2310.16401v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16401
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21442;&#25968;&#21270;&#21644;&#29983;&#25104;&#22810;&#20010;&#22270;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20165;&#20351;&#29992;&#21333;&#20010;&#35266;&#23519;&#22270;&#30340;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#22312;&#33410;&#28857;&#20998;&#31867;&#21644;&#22270;&#22238;&#24402;&#20219;&#21153;&#19978;&#65292;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#32593;&#32476;&#21442;&#25968;&#65292;&#22312;&#22810;&#20010;&#22270;&#19978;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#32467;&#21512;PAC-Bayesian&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#21462;&#24471;&#20102;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#20351;&#29992;&#21333;&#20010;&#35266;&#23519;&#21040;&#30340;&#22270;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#28982;&#32780;&#65292;&#35266;&#23519;&#21040;&#30340;&#22270;&#20165;&#20195;&#34920;&#20102;&#19968;&#31181;&#21487;&#33021;&#30340;&#23454;&#29616;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#22270;&#21487;&#33021;&#20250;&#36935;&#21040;&#19981;&#30830;&#23450;&#24615;&#65292;&#20363;&#22914;&#23384;&#22312;&#38169;&#35823;&#25110;&#32570;&#22833;&#30340;&#36793;&#65292;&#20197;&#21450;&#25552;&#20379;&#24456;&#23569;&#20449;&#24687;&#20215;&#20540;&#30340;&#36793;&#26435;&#37325;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#24182;&#25429;&#25417;&#20808;&#21069;&#22312;&#35266;&#23519;&#21040;&#30340;&#22270;&#20013;&#32570;&#22833;&#30340;&#38468;&#21152;&#20449;&#24687;&#65292;&#25105;&#20204;&#24341;&#20837;&#28508;&#22312;&#21464;&#37327;&#26469;&#21442;&#25968;&#21270;&#21644;&#29983;&#25104;&#22810;&#20010;&#22270;&#12290;&#25105;&#20204;&#22522;&#20110;&#22810;&#20010;&#22270;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#26694;&#26550;&#65292;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#32593;&#32476;&#21442;&#25968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#26041;&#27861;&#36845;&#20195;&#22320;&#30830;&#23450;&#22270;&#30340;&#20998;&#24067;&#65292;&#32467;&#21512;PAC-Bayesian&#29702;&#35770;&#30340;&#21407;&#21017;&#12290;&#25968;&#20540;&#23454;&#39564;&#22312;&#24322;&#36136;&#22270;&#30340;&#33410;&#28857;&#20998;&#31867;&#21644;&#21270;&#23398;&#25968;&#25454;&#38598;&#30340;&#22270;&#22238;&#24402;&#19978;&#25913;&#36827;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, graph neural networks have been trained using a single observed graph. However, the observed graph represents only one possible realization. In many applications, the graph may encounter uncertainties, such as having erroneous or missing edges, as well as edge weights that provide little informative value. To address these challenges and capture additional information previously absent in the observed graph, we introduce latent variables to parameterize and generate multiple graphs. We obtain the maximum likelihood estimate of the network parameters in an Expectation-Maximization (EM) framework based on the multiple graphs. Specifically, we iteratively determine the distribution of the graphs using a Markov Chain Monte Carlo (MCMC) method, incorporating the principles of PAC-Bayesian theory. Numerical experiments demonstrate improvements in performance against baseline models on node classification for heterogeneous graphs and graph regression on chemistry datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;GraphSplineNets&#65292;&#36890;&#36807;&#20943;&#23569;&#28145;&#24230;&#26367;&#20195;&#27169;&#22411;&#30340;&#32593;&#26684;&#22823;&#23567;&#21644;&#36845;&#20195;&#27493;&#25968;&#26469;&#21152;&#36895;&#29289;&#29702;&#31995;&#32479;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#20351;&#29992;&#20004;&#31181;&#21487;&#24494;&#30340;&#27491;&#20132;&#26679;&#26465;&#25554;&#20540;&#26041;&#27861;&#21644;&#33258;&#36866;&#24212;&#25554;&#20540;&#31574;&#30053;&#25552;&#39640;&#20934;&#30830;&#24230;&#21644;&#36895;&#24230;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.16397</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#26679;&#26465;&#32593;&#32476;&#23398;&#20064;&#39640;&#25928;&#26367;&#20195;&#21160;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Efficient Surrogate Dynamic Models with Graph Spline Networks. (arXiv:2310.16397v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;GraphSplineNets&#65292;&#36890;&#36807;&#20943;&#23569;&#28145;&#24230;&#26367;&#20195;&#27169;&#22411;&#30340;&#32593;&#26684;&#22823;&#23567;&#21644;&#36845;&#20195;&#27493;&#25968;&#26469;&#21152;&#36895;&#29289;&#29702;&#31995;&#32479;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#20351;&#29992;&#20004;&#31181;&#21487;&#24494;&#30340;&#27491;&#20132;&#26679;&#26465;&#25554;&#20540;&#26041;&#27861;&#21644;&#33258;&#36866;&#24212;&#25554;&#20540;&#31574;&#30053;&#25552;&#39640;&#20934;&#30830;&#24230;&#21644;&#36895;&#24230;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#31243;&#21644;&#31185;&#23398;&#35745;&#31639;&#20013;&#65292;&#22797;&#26434;&#30340;&#29289;&#29702;&#31995;&#32479;&#27169;&#25311;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#36817;&#24180;&#26469;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;GraphSplineNets&#65292;&#36890;&#36807;&#20943;&#23569;&#28145;&#24230;&#26367;&#20195;&#27169;&#22411;&#30340;&#32593;&#26684;&#22823;&#23567;&#21644;&#36845;&#20195;&#27493;&#25968;&#26469;&#21152;&#36895;&#29289;&#29702;&#31995;&#32479;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20004;&#31181;&#21487;&#24494;&#30340;&#27491;&#20132;&#26679;&#26465;&#25554;&#20540;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#39044;&#27979;&#20219;&#24847;&#26102;&#31354;&#20301;&#32622;&#30340;&#21709;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#25554;&#20540;&#31574;&#30053;&#65292;&#20197;&#20248;&#20808;&#20174;&#26368;&#37325;&#35201;&#30340;&#21306;&#22495;&#36827;&#34892;&#37319;&#26679;&#12290;GraphSplineNets&#22312;&#39044;&#27979;&#19981;&#26029;&#22797;&#26434;&#21270;&#30340;&#21508;&#31181;&#21160;&#24577;&#31995;&#32479;&#20013;&#65292;&#21253;&#25324;&#28909;&#26041;&#31243;&#12289;&#38459;&#23612;&#27874;&#20256;&#25773;&#12289;Navier-Stokes&#26041;&#31243;&#21644;&#30495;&#23454;&#19990;&#30028;&#28023;&#27969;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24230;&#21644;&#36895;&#24230;&#30340;&#26435;&#34913;&#65292;&#36866;&#29992;&#20110;&#35268;&#21017;&#21644;&#38750;&#35268;&#21017;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
While complex simulations of physical systems have been widely used in engineering and scientific computing, lowering their often prohibitive computational requirements has only recently been tackled by deep learning approaches. In this paper, we present GraphSplineNets, a novel deep-learning method to speed up the forecasting of physical systems by reducing the grid size and number of iteration steps of deep surrogate models. Our method uses two differentiable orthogonal spline collocation methods to efficiently predict response at any location in time and space. Additionally, we introduce an adaptive collocation strategy in space to prioritize sampling from the most important regions. GraphSplineNets improve the accuracy-speedup tradeoff in forecasting various dynamical systems with increasing complexity, including the heat equation, damped wave propagation, Navier-Stokes equations, and real-world ocean currents in both regular and irregular domains.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EVIL&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#20998;&#24067;&#30693;&#35782;&#20013;&#30340;&#21464;&#37327;&#21442;&#25968;&#65292;&#23558;&#20854;&#25490;&#38500;&#22312;&#19981;&#21464;&#24615;&#23398;&#20064;&#20043;&#22806;&#65292;&#20174;&#32780;&#25214;&#21040;&#19968;&#20010;&#25269;&#25239;&#20998;&#24067;&#36716;&#31227;&#30340;&#40065;&#26834;&#23376;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2310.16391</link><description>&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#21464;&#37327;&#21442;&#25968;&#25552;&#39640;&#19981;&#21516;&#20998;&#24067;&#30340;&#19981;&#21464;&#24615;&#23398;&#20064;&#65306;&#20174;&#22833;&#36133;&#24425;&#31080;&#20013;&#33719;&#21462;&#30340;&#33719;&#22870;
&lt;/p&gt;
&lt;p&gt;
Winning Prize Comes from Losing Tickets: Improve Invariant Learning by Exploring Variant Parameters for Out-of-Distribution Generalization. (arXiv:2310.16391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16391
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EVIL&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#20998;&#24067;&#30693;&#35782;&#20013;&#30340;&#21464;&#37327;&#21442;&#25968;&#65292;&#23558;&#20854;&#25490;&#38500;&#22312;&#19981;&#21464;&#24615;&#23398;&#20064;&#20043;&#22806;&#65292;&#20174;&#32780;&#25214;&#21040;&#19968;&#20010;&#25269;&#25239;&#20998;&#24067;&#36716;&#31227;&#30340;&#40065;&#26834;&#23376;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#20998;&#24067;&#30340;&#19981;&#21464;&#24615;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#21040;&#23545;&#21508;&#31181;&#29615;&#22659;&#37117;&#20855;&#26377;&#24456;&#22909;&#27867;&#21270;&#24615;&#33021;&#30340;&#40065;&#26834;&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#36866;&#24212;&#29305;&#23450;&#20998;&#24067;&#30340;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22522;&#20110;"&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;"&#35299;&#20915;&#20102;&#27492;&#38382;&#39064;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#23398;&#20064;&#30446;&#26631;&#26469;&#25214;&#21040;&#19968;&#20123;&#23545;&#20219;&#21153;&#20851;&#38190;&#30340;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#21516;&#20998;&#24067;&#30340;&#38382;&#39064;&#20013;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#22240;&#20026;&#23398;&#20064;&#20219;&#21153;&#21253;&#21547;&#20005;&#37325;&#30340;&#20998;&#24067;&#22122;&#22768;&#65292;&#36825;&#21487;&#33021;&#20250;&#35823;&#23548;&#20248;&#21270;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#38500;&#20102;&#25214;&#21040;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#21442;&#25968;&#65288;&#21363;&#19981;&#21464;&#21442;&#25968;&#65289;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;"&#25506;&#32034;&#21464;&#37327;&#21442;&#25968;&#36827;&#34892;&#19981;&#21464;&#24615;&#23398;&#20064;"&#65288;EVIL&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#20998;&#24067;&#30693;&#35782;&#26469;&#23547;&#25214;&#23545;&#20998;&#24067;&#21464;&#21270;&#25935;&#24863;&#30340;&#21442;&#25968;&#65288;&#21363;&#21464;&#37327;&#21442;&#25968;&#65289;&#12290;&#19968;&#26086;&#23558;&#21464;&#37327;&#21442;&#25968;&#20174;&#19981;&#21464;&#24615;&#23398;&#20064;&#20013;&#21076;&#38500;&#65292;&#23601;&#21487;&#20197;&#25214;&#21040;&#19968;&#20010;&#25269;&#25239;&#20998;&#24067;&#36716;&#31227;&#30340;&#40065;&#26834;&#23376;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#30456;&#23545;&#31283;&#23450;&#30340;&#21442;&#25968;&#36824;&#21487;&#20197;&#36890;&#36807;&#36328;&#20998;&#24067;&#30340;&#35299;&#32806;&#26469;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution (OOD) Generalization aims to learn robust models that generalize well to various environments without fitting to distribution-specific features. Recent studies based on Lottery Ticket Hypothesis (LTH) address this problem by minimizing the learning target to find some of the parameters that are critical to the task. However, in OOD problems, such solutions are suboptimal as the learning task contains severe distribution noises, which can mislead the optimization process. Therefore, apart from finding the task-related parameters (i.e., invariant parameters), we propose Exploring Variant parameters for Invariant Learning (EVIL) which also leverages the distribution knowledge to find the parameters that are sensitive to distribution shift (i.e., variant parameters). Once the variant parameters are left out of invariant learning, a robust subnetwork that is resistant to distribution shift can be found. Additionally, the parameters that are relatively stable across distr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21644;&#30740;&#31350;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#25554;&#20540;&#26041;&#27861;&#65292;&#29992;&#20110;&#31649;&#29702;&#21644;&#37327;&#21270;&#36890;&#36807;&#25554;&#20540;&#29699;&#38754;&#22024;&#26434;&#25968;&#25454;&#32780;&#24102;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25968;&#20540;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#35745;&#31639;&#29615;&#22659;&#20013;&#30340;&#22122;&#22768;&#25968;&#25454;&#26041;&#38754;&#26159;&#23454;&#29992;&#21644;&#24378;&#22823;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.16384</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#30340;&#19981;&#30830;&#23450;&#24230;&#37327;&#21270;&#65306;&#29699;&#38754;&#19978;&#30340;&#26680;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Distributed Uncertainty Quantification of Kernel Interpolation on Spheres. (arXiv:2310.16384v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21644;&#30740;&#31350;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#25554;&#20540;&#26041;&#27861;&#65292;&#29992;&#20110;&#31649;&#29702;&#21644;&#37327;&#21270;&#36890;&#36807;&#25554;&#20540;&#29699;&#38754;&#22024;&#26434;&#25968;&#25454;&#32780;&#24102;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25968;&#20540;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#35745;&#31639;&#29615;&#22659;&#20013;&#30340;&#22122;&#22768;&#25968;&#25454;&#26041;&#38754;&#26159;&#23454;&#29992;&#21644;&#24378;&#22823;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#25955;&#20081;&#25968;&#25454;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#65288;RBF&#65289;&#26680;&#25554;&#20540;&#65292;Schaback&#22312;1995&#24180;&#35777;&#26126;&#20102;&#21487;&#36798;&#21040;&#30340;&#36924;&#36817;&#35823;&#24046;&#21644;&#22522;&#30784;&#25554;&#20540;&#30697;&#38453;&#30340;&#26465;&#20214;&#25968;&#19981;&#33021;&#21516;&#26102;&#21464;&#23567;&#12290;&#20182;&#23558;&#36825;&#19968;&#21457;&#29616;&#31216;&#20026;&#8220;&#19981;&#30830;&#23450;&#20851;&#31995;&#8221;&#65292;&#20854;&#20013;&#30340;&#19968;&#20010;&#19981;&#21487;&#21462;&#30340;&#32467;&#26524;&#26159;RBF&#26680;&#25554;&#20540;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#25968;&#25454;&#30340;&#24178;&#25200;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#25554;&#20540;&#26041;&#27861;&#65292;&#29992;&#20110;&#31649;&#29702;&#21644;&#37327;&#21270;&#36890;&#36807;&#25554;&#20540;&#38750;&#38646;&#31243;&#24230;&#30340;&#29699;&#38754;&#22024;&#26434;&#25968;&#25454;&#32780;&#24102;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#25968;&#20540;&#27169;&#25311;&#32467;&#26524;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#26469;&#33258;&#20855;&#26377;&#25361;&#25112;&#24615;&#35745;&#31639;&#29615;&#22659;&#30340;&#22122;&#22768;&#25968;&#25454;&#26041;&#38754;&#26159;&#23454;&#29992;&#21644;&#24378;&#22823;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
For radial basis function (RBF) kernel interpolation of scattered data, Schaback in 1995 proved that the attainable approximation error and the condition number of the underlying interpolation matrix cannot be made small simultaneously. He referred to this finding as an "uncertainty relation", an undesirable consequence of which is that RBF kernel interpolation is susceptible to noisy data. In this paper, we propose and study a distributed interpolation method to manage and quantify the uncertainty brought on by interpolating noisy spherical data of non-negligible magnitude. We also present numerical simulation results showing that our method is practical and robust in terms of handling noisy data from challenging computing environments.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25913;&#36827;&#20837;&#20405;&#26816;&#27979;&#24615;&#33021;&#30340;&#22810;&#25915;&#20987;&#20998;&#31867;&#27169;&#22411;&#65292;&#36825;&#20010;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20934;&#30830;&#24230;&#12289;&#26816;&#27979;&#29575;&#21644;&#20302;&#35823;&#25253;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;</title><link>http://arxiv.org/abs/2310.16380</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25913;&#36827;&#20837;&#20405;&#26816;&#27979;&#24615;&#33021;&#30340;&#22810;&#25915;&#20987;&#20998;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A model for multi-attack classification to improve intrusion detection performance using deep learning approaches. (arXiv:2310.16380v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16380
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25913;&#36827;&#20837;&#20405;&#26816;&#27979;&#24615;&#33021;&#30340;&#22810;&#25915;&#20987;&#20998;&#31867;&#27169;&#22411;&#65292;&#36825;&#20010;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20934;&#30830;&#24230;&#12289;&#26816;&#27979;&#29575;&#21644;&#20302;&#35823;&#25253;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#25552;&#35758;&#30340;&#27169;&#22411;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#20854;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#20010;&#21487;&#38752;&#30340;&#20837;&#20405;&#26816;&#27979;&#26426;&#21046;&#65292;&#20197;&#24110;&#21161;&#35782;&#21035;&#24694;&#24847;&#25915;&#20987;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#26694;&#26550;&#21253;&#25324;&#19977;&#31181;&#26041;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#24102;&#26377;&#19971;&#31181;&#20248;&#21270;&#22120;&#20989;&#25968;&#65288;&#22914;adamax&#12289;SGD&#12289;adagrad&#12289;adam&#12289;RMSprop&#12289;nadam&#21644;adadelta&#65289;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;LSTM-RNN&#65289;&#12290;&#35813;&#27169;&#22411;&#22312;NSL-KDD&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#36827;&#34892;&#22810;&#25915;&#20987;&#20998;&#31867;&#12290;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#24230;&#12289;&#26816;&#27979;&#29575;&#21644;&#20302;&#35823;&#25253;&#29575;&#26041;&#38754;&#20248;&#20110;adamax&#20248;&#21270;&#22120;&#12290;&#23558;LSTM-RNN&#19982;adamax&#20248;&#21270;&#22120;&#30340;&#32467;&#26524;&#19982;&#29616;&#26377;&#30340;&#27973;&#23618;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20934;&#30830;&#24230;&#12289;&#26816;&#27979;&#29575;&#21644;&#20302;&#35823;&#25253;&#29575;&#26041;&#38754;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22810;&#27169;&#22411;&#26041;&#27861;&#21253;&#25324;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;LSTM-RNN&#65289;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This proposed model introduces novel deep learning methodologies. The objective here is to create a reliable intrusion detection mechanism to help identify malicious attacks. Deep learning based solution framework is developed consisting of three approaches. The first approach is Long-Short Term Memory Recurrent Neural Network (LSTM-RNN) with seven optimizer functions such as adamax, SGD, adagrad, adam, RMSprop, nadam and adadelta. The model is evaluated on NSL-KDD dataset and classified multi attack classification. The model has outperformed with adamax optimizer in terms of accuracy, detection rate and low false alarm rate. The results of LSTM-RNN with adamax optimizer is compared with existing shallow machine and deep learning models in terms of accuracy, detection rate and low false alarm rate. The multi model methodology consisting of Recurrent Neural Network (RNN), Long-Short Term Memory Recurrent Neural Network (LSTM-RNN), and Deep Neural Network (DNN). The multi models are eval
&lt;/p&gt;</description></item><item><title>GADY&#26159;&#19968;&#31181;&#22312;&#21160;&#24577;&#22270;&#19978;&#36827;&#34892;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#36830;&#32493;&#21160;&#24577;&#22270;&#27169;&#22411;&#21644;&#24341;&#20837;&#36127;&#37319;&#26679;&#27169;&#22359;&#26469;&#35299;&#20915;&#20102;&#21160;&#24577;&#32467;&#26500;&#26500;&#24314;&#25361;&#25112;&#21644;&#36127;&#37319;&#26679;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.16376</link><description>&lt;p&gt;
GADY: &#21160;&#24577;&#22270;&#19978;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
GADY: Unsupervised Anomaly Detection on Dynamic Graphs. (arXiv:2310.16376v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16376
&lt;/p&gt;
&lt;p&gt;
GADY&#26159;&#19968;&#31181;&#22312;&#21160;&#24577;&#22270;&#19978;&#36827;&#34892;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#36830;&#32493;&#21160;&#24577;&#22270;&#27169;&#22411;&#21644;&#24341;&#20837;&#36127;&#37319;&#26679;&#27169;&#22359;&#26469;&#35299;&#20915;&#20102;&#21160;&#24577;&#32467;&#26500;&#26500;&#24314;&#25361;&#25112;&#21644;&#36127;&#37319;&#26679;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#19978;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#25351;&#26816;&#27979;&#34892;&#20026;&#26126;&#26174;&#20559;&#31163;&#22270;&#20013;&#35266;&#23519;&#21040;&#30340;&#35268;&#33539;&#34892;&#20026;&#30340;&#23454;&#20307;&#21644;&#23427;&#20204;&#30340;&#26102;&#38388;&#20449;&#24687;&#12290;&#36825;&#20010;&#39046;&#22495;&#22240;&#20854;&#22312;&#37329;&#34701;&#12289;&#32593;&#32476;&#23433;&#20840;&#12289;&#31038;&#20132;&#32593;&#32476;&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65306;&#21160;&#24577;&#32467;&#26500;&#26500;&#24314;&#25361;&#25112;-&#38590;&#20197;&#25429;&#25417;&#24102;&#26377;&#22797;&#26434;&#26102;&#38388;&#20449;&#24687;&#30340;&#22270;&#32467;&#26500;&#65292;&#20197;&#21450;&#36127;&#37319;&#26679;&#25361;&#25112;-&#26080;&#27861;&#26500;&#24314;&#20248;&#31168;&#30340;&#36127;&#37319;&#26679;&#36827;&#34892;&#26080;&#30417;&#30563;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#21160;&#24577;&#22270;&#19978;&#30340;&#26080;&#30417;&#30563;&#29983;&#25104;&#24322;&#24120;&#26816;&#27979;&#65288;GADY&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#31532;&#19968;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36830;&#32493;&#30340;&#21160;&#24577;&#22270;&#27169;&#22411;&#26469;&#25429;&#25417;&#32454;&#31890;&#24230;&#20449;&#24687;&#65292;&#31361;&#30772;&#20102;&#29616;&#26377;&#31163;&#25955;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#28040;&#24687;&#20256;&#36882;&#26694;&#26550;&#32467;&#21512;&#20301;&#32622;&#29305;&#24449;&#26469;&#33719;&#21462;&#36793;&#30340;&#23884;&#20837;&#65292;&#36890;&#36807;&#35299;&#30721;&#26469;&#35782;&#21035;&#24322;&#24120;&#12290;&#23545;&#20110;&#31532;&#20108;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#36127;&#37319;&#26679;&#27169;&#22359;&#26469;&#26500;&#36896;&#20248;&#31168;&#30340;&#36127;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection on dynamic graphs refers to detecting entities whose behaviors obviously deviate from the norms observed within graphs and their temporal information. This field has drawn increasing attention due to its application in finance, network security, social networks, and more. However, existing methods face two challenges: dynamic structure constructing challenge - difficulties in capturing graph structure with complex time information and negative sampling challenge - unable to construct excellent negative samples for unsupervised learning. To address these challenges, we propose Unsupervised Generative Anomaly Detection on Dynamic Graphs (GADY). To tackle the first challenge, we propose a continuous dynamic graph model to capture the fine-grained information, which breaks the limit of existing discrete methods. Specifically, we employ a message-passing framework combined with positional features to get edge embeddings, which are decoded to identify anomalies. For the sec
&lt;/p&gt;</description></item><item><title>DyExplainer&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#35757;&#32451;&#19968;&#20010;&#21160;&#24577;GNNs&#39592;&#26550;&#65292;&#33021;&#22815;&#25552;&#21462;&#27599;&#20010;&#24555;&#29031;&#20013;&#22270;&#30340;&#34920;&#31034;&#65292;&#24182;&#19988;&#21516;&#26102;&#36827;&#34892;&#25506;&#32034;&#65292;&#20197;&#26377;&#25928;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#32467;&#26500;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.16375</link><description>&lt;p&gt;
DyExplainer: &#21487;&#35299;&#37322;&#30340;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DyExplainer: Explainable Dynamic Graph Neural Networks. (arXiv:2310.16375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16375
&lt;/p&gt;
&lt;p&gt;
DyExplainer&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#35757;&#32451;&#19968;&#20010;&#21160;&#24577;GNNs&#39592;&#26550;&#65292;&#33021;&#22815;&#25552;&#21462;&#27599;&#20010;&#24555;&#29031;&#20013;&#22270;&#30340;&#34920;&#31034;&#65292;&#24182;&#19988;&#21516;&#26102;&#36827;&#34892;&#25506;&#32034;&#65292;&#20197;&#26377;&#25928;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#32467;&#26500;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22240;&#20854;&#33021;&#20174;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#25429;&#25417;&#34920;&#31034;&#30340;&#33021;&#21147;&#32780;&#37325;&#26032;&#20852;&#36215;&#20026;&#19968;&#20010;&#30740;&#31350;&#28909;&#28857;&#12290;&#28982;&#32780;&#65292;GNNs&#30340;&#40657;&#30418;&#29305;&#24615;&#22312;&#29702;&#35299;&#21644;&#20449;&#20219;&#36825;&#20123;&#27169;&#22411;&#26041;&#38754;&#23384;&#22312;&#30528;&#26174;&#33879;&#25361;&#25112;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20851;&#38190;&#20219;&#21153;&#22330;&#26223;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#34429;&#28982;&#36817;&#24180;&#26469;&#22312;&#35299;&#37322;GNNs&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20854;&#20013;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#38745;&#24577;&#22270;&#19978;&#65292;&#23545;&#21160;&#24577;GNNs&#30340;&#35299;&#37322;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#21160;&#24577;GNNs&#20197;&#20854;&#19981;&#26029;&#28436;&#21270;&#30340;&#22270;&#32467;&#26500;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#24182;&#38656;&#35201;&#39069;&#22806;&#30340;&#21162;&#21147;&#26469;&#26377;&#25928;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#32467;&#26500;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DyExplainer&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;GNNs&#35299;&#37322;&#26041;&#27861;&#12290;DyExplainer&#35757;&#32451;&#19968;&#20010;&#21160;&#24577;GNNs&#39592;&#26550;&#65292;&#22312;&#27599;&#20010;&#24555;&#29031;&#20013;&#25552;&#21462;&#22270;&#30340;&#34920;&#31034;&#65292;&#24182;&#21516;&#26102;&#36827;&#34892;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) resurge as a trending research subject owing to their impressive ability to capture representations from graph-structured data. However, the black-box nature of GNNs presents a significant challenge in terms of comprehending and trusting these models, thereby limiting their practical applications in mission-critical scenarios. Although there has been substantial progress in the field of explaining GNNs in recent years, the majority of these studies are centered on static graphs, leaving the explanation of dynamic GNNs largely unexplored. Dynamic GNNs, with their ever-evolving graph structures, pose a unique challenge and require additional efforts to effectively capture temporal dependencies and structural relationships. To address this challenge, we present DyExplainer, a novel approach to explaining dynamic GNNs on the fly. DyExplainer trains a dynamic GNN backbone to extract representations of the graph at each snapshot, while simultaneously exploring st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#20811;&#25289;&#40664;&#27779;&#23572;&#24503;&#36317;&#31163;&#27491;&#21017;&#21270;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#38598;&#21644;&#35266;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20851;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#20004;&#27493;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#20808;&#39564;&#24314;&#27169;&#30340;&#28789;&#27963;&#24615;&#21644;&#32858;&#21512;&#21518;&#39564;&#19982;&#20808;&#39564;&#20998;&#24067;&#20043;&#38388;&#30340;&#23545;&#40784;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.16374</link><description>&lt;p&gt;
&#36890;&#36807;&#20811;&#25289;&#40664;&#27779;&#23572;&#24503;&#36317;&#31163;&#36827;&#34892;&#32852;&#21512;&#20998;&#24067;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Joint Distributional Learning via Cramer-Wold Distance. (arXiv:2310.16374v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#20811;&#25289;&#40664;&#27779;&#23572;&#24503;&#36317;&#31163;&#27491;&#21017;&#21270;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#38598;&#21644;&#35266;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20851;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#20004;&#27493;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#20808;&#39564;&#24314;&#27169;&#30340;&#28789;&#27963;&#24615;&#21644;&#32858;&#21512;&#21518;&#39564;&#19982;&#20808;&#39564;&#20998;&#24067;&#20043;&#38388;&#30340;&#23545;&#40784;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#38598;&#25110;&#35266;&#27979;&#21464;&#37327;&#20043;&#38388;&#22797;&#26434;&#30456;&#20851;&#32467;&#26500;&#26102;&#65292;&#22522;&#20110;&#26465;&#20214;&#29420;&#31435;&#24615;&#30340;&#20551;&#35774;&#22312;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#35299;&#30721;&#22120;&#24314;&#27169;&#20013;&#20855;&#26377;&#23616;&#38480;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20811;&#25289;&#40664;&#27779;&#23572;&#24503;&#36317;&#31163;&#27491;&#21017;&#21270;&#65292;&#21487;&#20197;&#36890;&#36807;&#38381;&#21512;&#24418;&#24335;&#35745;&#31639;&#65292;&#20197;&#20419;&#36827;&#39640;&#32500;&#25968;&#25454;&#38598;&#30340;&#32852;&#21512;&#20998;&#24067;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20004;&#27493;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#28789;&#27963;&#30340;&#20808;&#39564;&#24314;&#27169;&#65292;&#24182;&#25552;&#39640;&#32858;&#21512;&#21518;&#39564;&#19982;&#20808;&#39564;&#20998;&#24067;&#20043;&#38388;&#30340;&#23545;&#40784;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#23545;&#35813;&#31867;&#21035;&#20013;&#30340;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#21306;&#20998;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#22312;&#20855;&#26377;&#22810;&#20010;&#31867;&#21035;&#21464;&#37327;&#30340;&#39640;&#32500;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32771;&#34385;&#21040;&#35768;&#22810;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#21644;&#25968;&#25454;&#31185;&#23398;&#24212;&#29992;&#28041;&#21450;&#27492;&#31867;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#26377;&#30528;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
The assumption of conditional independence among observed variables, primarily used in the Variational Autoencoder (VAE) decoder modeling, has limitations when dealing with high-dimensional datasets or complex correlation structures among observed variables. To address this issue, we introduced the Cramer-Wold distance regularization, which can be computed in a closed-form, to facilitate joint distributional learning for high-dimensional datasets. Additionally, we introduced a two-step learning method to enable flexible prior modeling and improve the alignment between the aggregated posterior and the prior distribution. Furthermore, we provide theoretical distinctions from existing methods within this category. To evaluate the synthetic data generation performance of our proposed approach, we conducted experiments on high-dimensional datasets with multiple categorical variables. Given that many readily available datasets and data science applications involve such datasets, our experime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21463;&#32422;&#26463;&#30340;Actor Critic&#21644;&#21463;&#32422;&#26463;&#30340;Natural Actor Critic&#31639;&#27861;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#33021;&#25214;&#21040;&#24615;&#33021;&#20989;&#25968;&#30340;&#19968;&#38454;&#31283;&#23450;&#28857;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.16363</link><description>&lt;p&gt;
&#21463;&#32422;&#26463;&#30340;Actor Critic&#21644;&#21463;&#32422;&#26463;&#30340;Natural Actor Critic&#31639;&#27861;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Finite Time Analysis of Constrained Actor Critic and Constrained Natural Actor Critic Algorithms. (arXiv:2310.16363v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21463;&#32422;&#26463;&#30340;Actor Critic&#21644;&#21463;&#32422;&#26463;&#30340;Natural Actor Critic&#31639;&#27861;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#33021;&#25214;&#21040;&#24615;&#33021;&#20989;&#25968;&#30340;&#19968;&#38454;&#31283;&#23450;&#28857;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Actor Critic&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#25214;&#21040;&#20102;&#24040;&#22823;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#24403;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#24456;&#22823;&#30340;&#26102;&#20505;&#12290;&#26412;&#25991;&#32771;&#34385;&#20351;&#29992;&#20989;&#25968;&#36924;&#36817;&#30340;actor critic&#21644;natural actor critic&#31639;&#27861;&#26469;&#22788;&#29702;&#28041;&#21450;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;C-MDP&#65289;&#65292;&#24182;&#22312;&#38750; i.i.d&#65288;&#39532;&#23572;&#21487;&#22827;&#65289;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#38750;&#28176;&#36817;&#20998;&#26512;&#12290;&#25105;&#20204;&#32771;&#34385;&#38271;&#26399;&#24179;&#22343;&#25104;&#26412;&#20934;&#21017;&#65292;&#20854;&#20013;&#30446;&#26631;&#21644;&#32422;&#26463;&#20989;&#25968;&#37117;&#26159;&#26576;&#20123;&#35268;&#23450;&#25104;&#26412;&#20989;&#25968;&#30340;&#36866;&#24403;&#31574;&#30053;&#20381;&#36182;&#30340;&#38271;&#26399;&#24179;&#22343;&#12290;&#25105;&#20204;&#20351;&#29992;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#27861;&#22788;&#29702;&#19981;&#31561;&#24335;&#32422;&#26463;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#31639;&#27861;&#20445;&#35777;&#33021;&#25214;&#21040;&#24615;&#33021;&#65288;&#25289;&#26684;&#26391;&#26085;&#65289;&#20989;&#25968;$L(\theta,\gamma)$&#30340;&#19968;&#38454;&#31283;&#23450;&#28857;&#65288;&#21363;$\Vert \nabla L(\theta,\gamma)\Vert_2^2 \leq \epsilon$&#65289;&#65292;&#24182;&#19988;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\mathcal{\tilde{O}}(\epsilon^{-2.5})$&#12290;
&lt;/p&gt;
&lt;p&gt;
Actor Critic methods have found immense applications on a wide range of Reinforcement Learning tasks especially when the state-action space is large. In this paper, we consider actor critic and natural actor critic algorithms with function approximation for constrained Markov decision processes (C-MDP) involving inequality constraints and carry out a non-asymptotic analysis for both of these algorithms in a non-i.i.d (Markovian) setting. We consider the long-run average cost criterion where both the objective and the constraint functions are suitable policy-dependent long-run averages of certain prescribed cost functions. We handle the inequality constraints using the Lagrange multiplier method. We prove that these algorithms are guaranteed to find a first-order stationary point (i.e., $\Vert \nabla L(\theta,\gamma)\Vert_2^2 \leq \epsilon$) of the performance (Lagrange) function $L(\theta,\gamma)$, with a sample complexity of $\mathcal{\tilde{O}}(\epsilon^{-2.5})$ in the case of both C
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#23616;&#37096;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#28508;&#21147;&#22330;&#27169;&#22411;&#36820;&#22238;&#21487;&#24494;&#20998;&#30340;&#30896;&#25758;&#25104;&#26412;&#65292;&#21033;&#29992;&#31070;&#32463;&#22270;&#20687;&#32534;&#30721;&#22120;&#23558;&#38382;&#39064;&#32500;&#24230;&#38477;&#20302;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2310.16362</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#28508;&#21147;&#22330;&#30340;&#32771;&#34385;&#38556;&#30861;&#29289;&#30340;&#23616;&#37096;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Neural Potential Field for Obstacle-Aware Local Motion Planning. (arXiv:2310.16362v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16362
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#23616;&#37096;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#28508;&#21147;&#22330;&#27169;&#22411;&#36820;&#22238;&#21487;&#24494;&#20998;&#30340;&#30896;&#25758;&#25104;&#26412;&#65292;&#21033;&#29992;&#31070;&#32463;&#22270;&#20687;&#32534;&#30721;&#22120;&#23558;&#38382;&#39064;&#32500;&#24230;&#38477;&#20302;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#21487;&#33021;&#20026;&#31227;&#21160;&#26426;&#22120;&#20154;&#24179;&#21488;&#25552;&#20379;&#23616;&#37096;&#36816;&#21160;&#35268;&#21010;&#12290;&#25361;&#25112;&#22312;&#20110;&#24403;&#38556;&#30861;&#29289;&#22320;&#22270;&#21644;&#26426;&#22120;&#20154;&#36275;&#36857;&#37117;&#26159;&#20219;&#24847;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#30896;&#25758;&#25104;&#26412;&#36827;&#34892;&#20998;&#26512;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#28508;&#21147;&#22330;&#65306;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#26681;&#25454;&#26426;&#22120;&#20154;&#23039;&#24577;&#12289;&#38556;&#30861;&#29289;&#22320;&#22270;&#21644;&#26426;&#22120;&#20154;&#36275;&#36857;&#36820;&#22238;&#21487;&#24494;&#20998;&#30340;&#30896;&#25758;&#25104;&#26412;&#12290;&#25105;&#20204;&#27169;&#22411;&#30340;&#21487;&#24494;&#24615;&#20801;&#35768;&#20854;&#22312;MPC&#27714;&#35299;&#22120;&#20013;&#20351;&#29992;&#12290;&#23545;&#20110;&#20855;&#26377;&#38750;&#24120;&#22810;&#21442;&#25968;&#30340;&#38382;&#39064;&#65292;&#35745;&#31639;&#19978;&#38750;&#24120;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#21253;&#25324;&#31070;&#32463;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#23558;&#38556;&#30861;&#29289;&#22320;&#22270;&#21644;&#26426;&#22120;&#20154;&#36275;&#36857;&#36716;&#21270;&#20026;&#23884;&#20837;&#65292;&#20174;&#32780;&#23558;&#38382;&#39064;&#30340;&#32500;&#24230;&#38477;&#20302;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;&#32593;&#32476;&#35757;&#32451;&#30340;&#21442;&#32771;&#25968;&#25454;&#26159;&#22522;&#20110;&#31639;&#27861;&#35745;&#31639;&#30340;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#12290;&#27604;&#36739;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#23616;&#37096;&#35268;&#21010;&#22120;&#30456;&#24403;&#65306;&#23427;&#25552;&#20379;&#36712;&#36857;...
&lt;/p&gt;
&lt;p&gt;
Model predictive control (MPC) may provide local motion planning for mobile robotic platforms. The challenging aspect is the analytic representation of collision cost for the case when both the obstacle map and robot footprint are arbitrary. We propose a Neural Potential Field: a neural network model that returns a differentiable collision cost based on robot pose, obstacle map, and robot footprint. The differentiability of our model allows its usage within the MPC solver. It is computationally hard to solve problems with a very high number of parameters. Therefore, our architecture includes neural image encoders, which transform obstacle maps and robot footprints into embeddings, which reduce problem dimensionality by two orders of magnitude. The reference data for network training are generated based on algorithmic calculation of a signed distance function. Comparative experiments showed that the proposed approach is comparable with existing local planners: it provides trajectories w
&lt;/p&gt;</description></item><item><title>Redco&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#24037;&#20855;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#20998;&#24067;&#24335;&#35757;&#32451;LLMs&#65292;&#24182;&#31616;&#21270;ML&#27969;&#31243;&#30340;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2310.16355</link><description>&lt;p&gt;
Redco:&#19968;&#20010;&#36731;&#37327;&#32423;&#24037;&#20855;&#65292;&#21487;&#22312;&#20219;&#20309;GPU/TPUs&#19978;&#33258;&#21160;&#21270;&#20998;&#24067;&#24335;&#35757;&#32451;LLMs
&lt;/p&gt;
&lt;p&gt;
Redco: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs. (arXiv:2310.16355v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16355
&lt;/p&gt;
&lt;p&gt;
Redco&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#24037;&#20855;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#20998;&#24067;&#24335;&#35757;&#32451;LLMs&#65292;&#24182;&#31616;&#21270;ML&#27969;&#31243;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#20027;&#35201;&#24402;&#21151;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#26029;&#22686;&#38271;&#30340;&#20869;&#23384;&#38656;&#27714;&#32473;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#31243;&#24072;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#38656;&#35201;&#24320;&#21457;&#20154;&#21592;&#23558;&#22823;&#22411;&#27169;&#22411;&#20998;&#21306;&#20197;&#20998;&#24067;&#22312;&#22810;&#20010;GPU&#25110;TPU&#19978;&#12290;&#36825;&#38656;&#35201;&#20351;&#29992;&#29616;&#26377;&#27169;&#22411;&#24182;&#34892;&#24037;&#20855;&#65288;&#22914;Megatron-LM&#12289;DeepSpeed&#21644;Alpa&#65289;&#36827;&#34892;&#30456;&#24403;&#30340;&#32534;&#30721;&#21644;&#22797;&#26434;&#30340;&#37197;&#32622;&#24037;&#20316;&#12290;&#36825;&#20123;&#24037;&#20855;&#38656;&#35201;&#29992;&#25143;&#20855;&#22791;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65288;MLSys&#65289;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#32473;LLM&#24320;&#21457;&#24102;&#26469;&#20102;&#29942;&#39048;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#27809;&#26377;MLSys&#32972;&#26223;&#30340;&#24320;&#21457;&#20154;&#21592;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Redco&#65292;&#36825;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#24037;&#20855;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;LLMs&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#20197;&#21450;&#31616;&#21270;ML&#27969;&#31243;&#30340;&#24320;&#21457;&#12290;Redco&#30340;&#35774;&#35745;&#24378;&#35843;&#20102;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#33258;&#21160;&#21270;&#27169;&#22411;&#24182;&#34892;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#30830;&#23450;&#20102;&#20004;&#20010;&#31616;&#21333;&#30340;&#35268;&#21017;&#65292;&#29992;&#20110;&#20026;&#20219;&#20309;GPU / TPU&#29983;&#25104;&#24352;&#37327;&#24182;&#34892;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent progress of AI can be largely attributed to large language models (LLMs). However, their escalating memory requirements introduce challenges for machine learning (ML) researchers and engineers. Addressing this requires developers to partition a large model to distribute it across multiple GPUs or TPUs. This necessitates considerable coding and intricate configuration efforts with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa. These tools require users' expertise in machine learning systems (MLSys), creating a bottleneck in LLM development, particularly for developers without MLSys background. In this work, we present Redco, a lightweight and user-friendly tool crafted to automate distributed training and inference for LLMs, as well as to simplify ML pipeline development. The design of Redco emphasizes two key aspects. Firstly, to automate model parallism, our study identifies two straightforward rules to generate tensor parallel strategies for any g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#20351;&#29992;&#27969;&#21305;&#37197;&#30340;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#24182;&#33719;&#24471;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22312;60k&#23567;&#26102;&#30340;&#26410;&#36716;&#24405;&#35821;&#38899;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#19987;&#23478;&#27169;&#22411;&#22312;&#35821;&#38899;&#22686;&#24378;&#12289;&#20998;&#31163;&#21644;&#21512;&#25104;&#26041;&#38754;&#36827;&#34892;&#21305;&#37197;&#25110;&#36229;&#36234;&#12290;</title><link>http://arxiv.org/abs/2310.16338</link><description>&lt;p&gt;
&#24102;&#26377;&#27969;&#21305;&#37197;&#30340;&#35821;&#38899;&#29983;&#25104;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-training for Speech with Flow Matching. (arXiv:2310.16338v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#20351;&#29992;&#27969;&#21305;&#37197;&#30340;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#24182;&#33719;&#24471;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22312;60k&#23567;&#26102;&#30340;&#26410;&#36716;&#24405;&#35821;&#38899;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#19987;&#23478;&#27169;&#22411;&#22312;&#35821;&#38899;&#22686;&#24378;&#12289;&#20998;&#31163;&#21644;&#21512;&#25104;&#26041;&#38754;&#36827;&#34892;&#21305;&#37197;&#25110;&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#27169;&#22411;&#22312;&#38656;&#35201;&#20272;&#35745;&#21644;&#25277;&#26679;&#25968;&#25454;&#20998;&#24067;&#20197;&#29983;&#25104;&#39640;&#20445;&#30495;&#21512;&#25104;&#25968;&#25454;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#22240;&#27492;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#35821;&#38899;&#39046;&#22495;&#65292;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#21644;&#31070;&#32463;&#22768;&#30721;&#22120;&#26159;&#29983;&#25104;&#27169;&#22411;&#25104;&#21151;&#24212;&#29992;&#30340;&#20856;&#22411;&#20363;&#23376;&#12290;&#23613;&#31649;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#22312;&#35821;&#38899;&#30340;&#19981;&#21516;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#65292;&#20294;&#36824;&#27809;&#26377;&#19968;&#20010;&#36890;&#29992;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#24314;&#27169;&#35821;&#38899;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#21333;&#19968;&#30340;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#24182;&#33719;&#24471;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36808;&#20986;&#20102;&#36825;&#20010;&#26041;&#21521;&#30340;&#19968;&#27493;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#27969;&#21305;&#37197;&#21644;&#33945;&#29256;&#26465;&#20214;&#22312;60k&#23567;&#26102;&#30340;&#26410;&#36716;&#24405;&#35821;&#38899;&#19978;&#39044;&#35757;&#32451;&#20102;&#19968;&#20010;&#21517;&#20026;SpeechFlow&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#29305;&#23450;&#20219;&#21153;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22312;&#35821;&#38899;&#22686;&#24378;&#12289;&#20998;&#31163;&#21644;&#21512;&#25104;&#26041;&#38754;&#36798;&#21040;&#25110;&#36229;&#36807;&#29616;&#26377;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models have gained more and more attention in recent years for their remarkable success in tasks that required estimating and sampling data distribution to generate high-fidelity synthetic data. In speech, text-to-speech synthesis and neural vocoder are good examples where generative models have shined. While generative models have been applied to different applications in speech, there exists no general-purpose generative model that models speech directly. In this work, we take a step toward this direction by showing a single pre-trained generative model can be adapted to different downstream tasks with strong performance. Specifically, we pre-trained a generative model, named SpeechFlow, on 60k hours of untranscribed speech with Flow Matching and masked conditions. Experiment results show the pre-trained generative model can be fine-tuned with task-specific data to match or surpass existing expert models on speech enhancement, separation, and synthesis. Our work suggested 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SMURF-THP&#26041;&#27861;&#26469;&#23398;&#20064;Transformer Hawkes&#36807;&#31243;&#24182;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#36890;&#36807;&#23398;&#20064;&#21040;&#30340;&#20998;&#25968;&#20989;&#25968;&#65292;&#21487;&#20197;&#20174;&#39044;&#27979;&#20998;&#24067;&#20013;&#37319;&#26679;&#20107;&#20214;&#21040;&#36798;&#26102;&#38388;&#65292;&#24182;&#35745;&#31639;&#32622;&#20449;&#21306;&#38388;&#26469;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16336</link><description>&lt;p&gt;
SMURF-THP&#65306;&#22522;&#20110;&#20998;&#25968;&#21305;&#37197;&#30340;Transformer Hawkes&#36807;&#31243;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SMURF-THP: Score Matching-based UnceRtainty quantiFication for Transformer Hawkes Process. (arXiv:2310.16336v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16336
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SMURF-THP&#26041;&#27861;&#26469;&#23398;&#20064;Transformer Hawkes&#36807;&#31243;&#24182;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#36890;&#36807;&#23398;&#20064;&#21040;&#30340;&#20998;&#25968;&#20989;&#25968;&#65292;&#21487;&#20197;&#20174;&#39044;&#27979;&#20998;&#24067;&#20013;&#37319;&#26679;&#20107;&#20214;&#21040;&#36798;&#26102;&#38388;&#65292;&#24182;&#35745;&#31639;&#32622;&#20449;&#21306;&#38388;&#26469;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer Hawkes&#36807;&#31243;&#27169;&#22411;&#22312;&#24314;&#27169;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#35757;&#32451;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#26368;&#22823;&#21270;&#20107;&#20214;&#24207;&#21015;&#30340;&#20284;&#28982;&#24615;&#65292;&#36825;&#28041;&#21450;&#21040;&#19968;&#20123;&#38590;&#20197;&#35745;&#31639;&#30340;&#31215;&#20998;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#20026;&#27169;&#22411;&#39044;&#27979;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20363;&#22914;&#23545;&#39044;&#27979;&#20107;&#20214;&#21040;&#36798;&#26102;&#38388;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SMURF-THP&#65292;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;Transformer Hawkes&#36807;&#31243;&#24182;&#37327;&#21270;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SMURF-THP&#36890;&#36807;&#36991;&#20813;&#38590;&#20197;&#35745;&#31639;&#30340;&#31215;&#20998;&#65292;&#23398;&#20064;&#20107;&#20214;&#21040;&#36798;&#26102;&#38388;&#30340;&#20998;&#25968;&#20989;&#25968;&#12290;&#36890;&#36807;&#36825;&#26679;&#19968;&#20010;&#23398;&#20064;&#30340;&#20998;&#25968;&#20989;&#25968;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#39044;&#27979;&#20998;&#24067;&#20013;&#37319;&#26679;&#20107;&#20214;&#21040;&#36798;&#26102;&#38388;&#12290;&#36825;&#33258;&#28982;&#22320;&#36890;&#36807;&#35745;&#31639;&#29983;&#25104;&#30340;&#26679;&#26412;&#19978;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#23454;&#29616;&#20102;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#12290;&#25105;&#20204;&#22312;&#20107;&#20214;&#31867;&#22411;&#39044;&#27979;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer Hawkes process models have shown to be successful in modeling event sequence data. However, most of the existing training methods rely on maximizing the likelihood of event sequences, which involves calculating some intractable integral. Moreover, the existing methods fail to provide uncertainty quantification for model predictions, e.g., confidence intervals for the predicted event's arrival time. To address these issues, we propose SMURF-THP, a score-based method for learning Transformer Hawkes process and quantifying prediction uncertainty. Specifically, SMURF-THP learns the score function of events' arrival time based on a score-matching objective that avoids the intractable computation. With such a learned score function, we can sample arrival time of events from the predictive distribution. This naturally allows for the quantification of uncertainty by computing confidence intervals over the generated samples. We conduct extensive experiments in both event type predic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#25490;&#24207;&#20248;&#21270;&#65288;GRO&#65289;&#20316;&#20026;&#31532;&#19968;&#20010;&#38450;&#24481;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#25277;&#21462;&#25915;&#20987;&#30340;&#31574;&#30053;&#65292;&#26368;&#23567;&#21270;&#21463;&#20445;&#25252;&#30340;&#30446;&#26631;&#27169;&#22411;&#30340;&#25439;&#22833;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#25915;&#20987;&#32773;&#30340;&#20195;&#29702;&#27169;&#22411;&#30340;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2310.16335</link><description>&lt;p&gt;
&#38450;&#24481;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#25277;&#21462;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defense Against Model Extraction Attacks on Recommender Systems. (arXiv:2310.16335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#25490;&#24207;&#20248;&#21270;&#65288;GRO&#65289;&#20316;&#20026;&#31532;&#19968;&#20010;&#38450;&#24481;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#25277;&#21462;&#25915;&#20987;&#30340;&#31574;&#30053;&#65292;&#26368;&#23567;&#21270;&#21463;&#20445;&#25252;&#30340;&#30446;&#26631;&#27169;&#22411;&#30340;&#25439;&#22833;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#25915;&#20987;&#32773;&#30340;&#20195;&#29702;&#27169;&#22411;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#24050;&#25104;&#20026;&#30740;&#31350;&#30028;&#30340;&#31361;&#20986;&#38382;&#39064;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25932;&#23545;&#25915;&#20987;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#20381;&#36182;&#20110;&#24191;&#27867;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#22914;&#25152;&#26377;&#30340;&#30333;&#30418;&#25915;&#20987;&#25110;&#22823;&#37096;&#20998;&#40657;&#30418;&#25915;&#20987;&#37117;&#20551;&#35774;&#26576;&#20123;&#22806;&#37096;&#30693;&#35782;&#21487;&#29992;&#12290;&#22312;&#36825;&#20123;&#25915;&#20987;&#20013;&#65292;&#27169;&#22411;&#25277;&#21462;&#25915;&#20987;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#19988;&#23454;&#29992;&#30340;&#26041;&#27861;&#65292;&#28041;&#21450;&#36890;&#36807;&#21453;&#22797;&#26597;&#35810;&#30446;&#26631;&#27169;&#22411;&#26469;&#35757;&#32451;&#20195;&#29702;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#22312;&#38450;&#24481;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#25277;&#21462;&#25915;&#20987;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#25490;&#24207;&#20248;&#21270;&#65288;GRO&#65289;&#20316;&#20026;&#31532;&#19968;&#20010;&#38024;&#23545;&#27492;&#31867;&#25915;&#20987;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#38450;&#24481;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#21463;&#20445;&#25252;&#30340;&#30446;&#26631;&#27169;&#22411;&#30340;&#25439;&#22833;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#25915;&#20987;&#32773;&#30340;&#20195;&#29702;&#27169;&#22411;&#30340;&#25439;&#22833;&#12290;&#30001;&#20110;&#21069;k&#20010;&#25490;&#24207;&#21015;&#34920;&#26159;&#19981;&#21487;&#24494;&#20998;&#30340;&#65292;&#24182;&#19988;&#38382;&#39064;&#30340;&#22797;&#26434;&#24230;&#24456;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robustness of recommender systems has become a prominent topic within the research community. Numerous adversarial attacks have been proposed, but most of them rely on extensive prior knowledge, such as all the white-box attacks or most of the black-box attacks which assume that certain external knowledge is available. Among these attacks, the model extraction attack stands out as a promising and practical method, involving training a surrogate model by repeatedly querying the target model. However, there is a significant gap in the existing literature when it comes to defending against model extraction attacks on recommender systems. In this paper, we introduce Gradient-based Ranking Optimization (GRO), which is the first defense strategy designed to counter such attacks. We formalize the defense as an optimization problem, aiming to minimize the loss of the protected target model while maximizing the loss of the attacker's surrogate model. Since top-k ranking lists are non-differ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#31070;&#32463;&#20803;&#35299;&#37322;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#24615;&#20998;&#26512;&#65292;&#21457;&#29616;&#36825;&#20123;&#35299;&#37322;&#21487;&#20197;&#34987;&#38543;&#26426;&#22122;&#22768;&#21644;&#31934;&#24515;&#35774;&#35745;&#30340;&#25200;&#21160;&#20005;&#37325;&#30772;&#22351;&#65292;&#21363;&#20351;&#28155;&#21152;&#23567;&#30340;&#38543;&#26426;&#22122;&#22768;&#20063;&#21487;&#20197;&#25913;&#21464;&#39640;&#36798;28&#65285;&#30340;&#31070;&#32463;&#20803;&#30340;&#27010;&#24565;&#20998;&#37197;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#27745;&#26579;&#31639;&#27861;&#65292;&#36890;&#36807;&#27745;&#26579;&#19981;&#21040;10&#65285;&#30340;&#25506;&#27979;&#25968;&#25454;&#21487;&#20197;&#25805;&#32437;&#36229;&#36807;80&#65285;&#30340;&#31070;&#32463;&#20803;&#30340;&#35299;&#37322;&#12290;&#36825;&#24341;&#21457;&#20102;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#23545;&#31070;&#32463;&#20803;&#35299;&#37322;&#26041;&#27861;&#30340;&#20449;&#20219;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.16332</link><description>&lt;p&gt;
&#27745;&#26579;&#31070;&#32463;&#20803;&#35299;&#37322;&#28145;&#24230;&#35270;&#35273;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Corrupting Neuron Explanations of Deep Visual Features. (arXiv:2310.16332v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#31070;&#32463;&#20803;&#35299;&#37322;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#24615;&#20998;&#26512;&#65292;&#21457;&#29616;&#36825;&#20123;&#35299;&#37322;&#21487;&#20197;&#34987;&#38543;&#26426;&#22122;&#22768;&#21644;&#31934;&#24515;&#35774;&#35745;&#30340;&#25200;&#21160;&#20005;&#37325;&#30772;&#22351;&#65292;&#21363;&#20351;&#28155;&#21152;&#23567;&#30340;&#38543;&#26426;&#22122;&#22768;&#20063;&#21487;&#20197;&#25913;&#21464;&#39640;&#36798;28&#65285;&#30340;&#31070;&#32463;&#20803;&#30340;&#27010;&#24565;&#20998;&#37197;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#27745;&#26579;&#31639;&#27861;&#65292;&#36890;&#36807;&#27745;&#26579;&#19981;&#21040;10&#65285;&#30340;&#25506;&#27979;&#25968;&#25454;&#21487;&#20197;&#25805;&#32437;&#36229;&#36807;80&#65285;&#30340;&#31070;&#32463;&#20803;&#30340;&#35299;&#37322;&#12290;&#36825;&#24341;&#21457;&#20102;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#23545;&#31070;&#32463;&#20803;&#35299;&#37322;&#26041;&#27861;&#30340;&#20449;&#20219;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#35299;&#37322;&#20854;&#40657;&#30418;&#34892;&#20026;&#30340;&#33021;&#21147;&#65292;&#23548;&#33268;&#36817;&#26469;&#20986;&#29616;&#20102;&#22823;&#37327;&#35299;&#37322;&#24615;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#25285;&#24515;&#36825;&#20123;&#35299;&#37322;&#24615;&#26041;&#27861;&#32570;&#20047;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#27969;&#31243;&#19979;&#23545;&#31070;&#32463;&#20803;&#35299;&#37322;&#26041;&#27861;&#36827;&#34892;&#20102;&#40065;&#26834;&#24615;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#35299;&#37322;&#21487;&#20197;&#34987;&#38543;&#26426;&#22122;&#22768;&#21644;&#31934;&#24515;&#35774;&#35745;&#30340;&#25200;&#21160;&#25152;&#20005;&#37325;&#30772;&#22351;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#28155;&#21152;&#26631;&#20934;&#24046;&#20026;0.02&#30340;&#23567;&#38543;&#26426;&#22122;&#22768;&#65292;&#20063;&#21487;&#20197;&#25913;&#21464;&#28145;&#23618;&#20013;&#39640;&#36798;28%&#30340;&#31070;&#32463;&#20803;&#25152;&#20998;&#37197;&#30340;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27745;&#26579;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#19981;&#21040;10%&#30340;&#25506;&#27979;&#25968;&#25454;&#26469;&#25805;&#32437;&#36229;&#36807;80%&#30340;&#31070;&#32463;&#20803;&#30340;&#35299;&#37322;&#12290;&#36825;&#24341;&#21457;&#20102;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#23545;&#31070;&#32463;&#20803;&#35299;&#37322;&#26041;&#27861;&#30340;&#20449;&#20219;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#28041;&#21450;&#23433;&#20840;&#21644;&#20844;&#24179;&#37325;&#35201;&#24212;&#29992;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inability of DNNs to explain their black-box behavior has led to a recent surge of explainability methods. However, there are growing concerns that these explainability methods are not robust and trustworthy. In this work, we perform the first robustness analysis of Neuron Explanation Methods under a unified pipeline and show that these explanations can be significantly corrupted by random noises and well-designed perturbations added to their probing data. We find that even adding small random noise with a standard deviation of 0.02 can already change the assigned concepts of up to 28% neurons in the deeper layers. Furthermore, we devise a novel corruption algorithm and show that our algorithm can manipulate the explanation of more than 80% neurons by poisoning less than 10% of probing data. This raises the concern of trusting Neuron Explanation Methods in real-life safety and fairness critical applications.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#21487;&#35843;&#21160;&#21147;&#23398;&#21644;&#30701;&#26399;&#21487;&#22609;&#24615;&#30340;&#35760;&#24518;&#30005;&#38459;&#22120;&#36827;&#34892;&#31867;&#33041;&#20648;&#23618;&#35745;&#31639;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#20449;&#24687;&#22788;&#29702;&#36895;&#24230;&#12289;&#26356;&#20302;&#30340;&#33021;&#32791;&#21644;&#26356;&#23567;&#30340;&#38754;&#31215;&#21344;&#29992;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#23545;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#32534;&#30721;&#65292;&#20351;&#24471;&#35760;&#24518;&#30005;&#38459;&#22120;&#30340;&#21160;&#21147;&#23398;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#65292;&#20811;&#26381;&#20102;&#20808;&#21069;&#23454;&#29616;&#20013;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.16331</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#35843;&#21160;&#21147;&#23398;&#21644;&#30701;&#26399;&#21487;&#22609;&#24615;&#30340;&#31867;&#33041;&#20648;&#23618;&#35745;&#31639;&#30340;&#35760;&#24518;&#30005;&#38459;&#22120;
&lt;/p&gt;
&lt;p&gt;
Brain-Inspired Reservoir Computing Using Memristors with Tunable Dynamics and Short-Term Plasticity. (arXiv:2310.16331v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16331
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#21487;&#35843;&#21160;&#21147;&#23398;&#21644;&#30701;&#26399;&#21487;&#22609;&#24615;&#30340;&#35760;&#24518;&#30005;&#38459;&#22120;&#36827;&#34892;&#31867;&#33041;&#20648;&#23618;&#35745;&#31639;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#20449;&#24687;&#22788;&#29702;&#36895;&#24230;&#12289;&#26356;&#20302;&#30340;&#33021;&#32791;&#21644;&#26356;&#23567;&#30340;&#38754;&#31215;&#21344;&#29992;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#23545;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#32534;&#30721;&#65292;&#20351;&#24471;&#35760;&#24518;&#30005;&#38459;&#22120;&#30340;&#21160;&#21147;&#23398;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#65292;&#20811;&#26381;&#20102;&#20808;&#21069;&#23454;&#29616;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20648;&#23618;&#35745;&#31639;&#30740;&#31350;&#30340;&#20808;&#36827;&#36827;&#23637;&#20419;&#20351;&#20154;&#20204;&#38656;&#35201;&#20855;&#26377;&#21487;&#20419;&#36827;&#20648;&#23618;&#29289;&#29702;&#23454;&#29616;&#30340;&#21160;&#21147;&#23398;&#30340;&#27169;&#25311;&#35774;&#22791;&#65292;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#20449;&#24687;&#22788;&#29702;&#36895;&#24230;&#65292;&#21516;&#26102;&#28040;&#32791;&#36739;&#23569;&#33021;&#37327;&#21644;&#21344;&#29992;&#26356;&#23567;&#30340;&#38754;&#31215;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20855;&#26377;&#38750;&#32447;&#24615;&#21644;&#30701;&#26399;&#35760;&#24518;&#21160;&#21147;&#23398;&#30340;&#21160;&#24577;&#35760;&#24518;&#30005;&#38459;&#22120;&#65292;&#26159;&#29992;&#20110;&#26102;&#38388;&#20998;&#31867;&#21644;&#39044;&#27979;&#20219;&#21153;&#30340;&#20449;&#24687;&#22788;&#29702;&#35774;&#22791;&#25110;&#20648;&#23618;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#20197;&#24448;&#30340;&#23454;&#29616;&#20381;&#36182;&#20110;&#21517;&#20041;&#19978;&#30456;&#21516;&#30340;&#35760;&#24518;&#30005;&#38459;&#22120;&#23545;&#36755;&#20837;&#25968;&#25454;&#24212;&#29992;&#30456;&#21516;&#30340;&#38750;&#32447;&#24615;&#21464;&#25442;&#65292;&#36825;&#19981;&#36275;&#20197;&#23454;&#29616;&#20016;&#23500;&#30340;&#29366;&#24577;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#30740;&#31350;&#20154;&#21592;&#35201;&#20040;&#22312;&#22810;&#20010;&#35760;&#24518;&#30005;&#38459;&#22120;&#20043;&#38388;&#25193;&#23637;&#25968;&#25454;&#32534;&#30721;&#65292;&#35201;&#20040;&#21033;&#29992;&#35760;&#24518;&#30005;&#38459;&#22120;&#20043;&#38388;&#30340;&#38543;&#26426;&#35774;&#22791;&#21464;&#24322;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#24182;&#19988;&#20250;&#23548;&#33268;&#21516;&#27493;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26368;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23545;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#32534;&#30721;&#65292;&#20351;&#24471;&#35760;&#24518;&#30005;&#38459;&#22120;&#30340;&#21160;&#21147;&#23398;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in reservoir computing research have created a demand for analog devices with dynamics that can facilitate the physical implementation of reservoirs, promising faster information processing while consuming less energy and occupying a smaller area footprint. Studies have demonstrated that dynamic memristors, with nonlinear and short-term memory dynamics, are excellent candidates as information-processing devices or reservoirs for temporal classification and prediction tasks. Previous implementations relied on nominally identical memristors that applied the same nonlinear transformation to the input data, which is not enough to achieve a rich state space. To address this limitation, researchers either diversified the data encoding across multiple memristors or harnessed the stochastic device-to-device variability among the memristors. However, this approach requires additional pre-processing steps and leads to synchronization issues. Instead, it is preferable to encod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20877;&#37319;&#26679;&#30340;SBM&#22270;&#21160;&#24577;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31574;&#30053;&#38236;&#20687;&#19978;&#21319;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#22810;&#31181;&#32676;&#22343;&#22330;&#21338;&#24328;&#30340;&#32435;&#20160;&#22343;&#34913;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26679;&#26412;&#38598;&#25104;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#35299;&#20915;&#23454;&#38469;&#24773;&#20917;&#19979;&#38543;&#26426;&#22359;&#27169;&#22411;&#26410;&#30693;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2310.16326</link><description>&lt;p&gt;
&#22522;&#20110;&#20877;&#37319;&#26679;&#30340;SBM&#22270;&#21160;&#24577;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for SBM Graphon Games with Re-Sampling. (arXiv:2310.16326v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20877;&#37319;&#26679;&#30340;SBM&#22270;&#21160;&#24577;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31574;&#30053;&#38236;&#20687;&#19978;&#21319;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#22810;&#31181;&#32676;&#22343;&#22330;&#21338;&#24328;&#30340;&#32435;&#20160;&#22343;&#34913;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26679;&#26412;&#38598;&#25104;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#35299;&#20915;&#23454;&#38469;&#24773;&#20917;&#19979;&#38543;&#26426;&#22359;&#27169;&#22411;&#26410;&#30693;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22343;&#22330;&#36817;&#20284;&#26159;&#30740;&#31350;&#22823;&#35268;&#27169;&#20154;&#21475;&#21160;&#24577;&#30340;&#19968;&#31181;&#21487;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#23545;&#21516;&#36136;&#24615;&#21644;&#25152;&#26377;&#20195;&#29702;&#20043;&#38388;&#30340;&#36890;&#29992;&#36830;&#25509;&#30340;&#20551;&#35774;&#38480;&#21046;&#20102;&#20854;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25991;&#29486;&#20013;&#24341;&#20837;&#20102;&#22810;&#31181;&#32676;&#22343;&#22330;&#21338;&#24328;&#27169;&#22411;&#12290;&#24403;&#24050;&#30693;&#22522;&#26412;&#30340;&#38543;&#26426;&#22359;&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#31574;&#30053;&#38236;&#20687;&#19978;&#21319;&#31639;&#27861;&#25214;&#21040;&#20102;&#22810;&#31181;&#32676;&#22343;&#22330;&#21338;&#24328;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;&#22312;&#26356;&#29616;&#23454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#38543;&#26426;&#22359;&#27169;&#22411;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#22270;&#21160;&#24577;&#38598;&#25104;&#19982;&#26377;&#38480;&#30340;N-player&#22810;&#32676;&#22343;&#22330;&#21338;&#24328;&#27169;&#22411;&#30340;&#20877;&#37319;&#26679;&#26041;&#26696;&#12290;&#25105;&#20204;&#22522;&#20110;&#22270;&#21160;&#24577;&#19982;&#20877;&#37319;&#26679;&#30340;Game (GGR-S)&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#27169;&#22411;&#25429;&#25417;&#20102;&#20195;&#29702;&#20043;&#38388;&#30340;&#22797;&#26434;&#32593;&#32476;&#32467;&#26500;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;GGR-S&#21160;&#24577;&#24182;&#24314;&#31435;&#20102;&#19982;&#22810;&#31181;&#32676;&#22343;&#22330;&#21338;&#24328;&#21160;&#24577;&#30340;&#25910;&#25947;&#24615;&#12290;&#21033;&#29992;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#26679;&#26412;&#30340;N-player&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#29992;&#20110;GGR-S&#12290;
&lt;/p&gt;
&lt;p&gt;
The Mean-Field approximation is a tractable approach for studying large population dynamics. However, its assumption on homogeneity and universal connections among all agents limits its applicability in many real-world scenarios. Multi-Population Mean-Field Game (MP-MFG) models have been introduced in the literature to address these limitations. When the underlying Stochastic Block Model is known, we show that a Policy Mirror Ascent algorithm finds the MP-MFG Nash Equilibrium. In more realistic scenarios where the block model is unknown, we propose a re-sampling scheme from a graphon integrated with the finite N-player MP-MFG model. We develop a novel learning framework based on a Graphon Game with Re-Sampling (GGR-S) model, which captures the complex network structures of agents' connections. We analyze GGR-S dynamics and establish the convergence to dynamics of MP-MFG. Leveraging this result, we propose an efficient sample-based N-player Reinforcement Learning algorithm for GGR-S wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;PF-PNE&#31639;&#27861;&#65292;&#36890;&#36807;&#21452;&#37325;&#28120;&#27760;&#31574;&#30053;&#21644;&#26377;&#25928;&#30340;&#26412;&#22320;&#30446;&#26631;&#35780;&#20272;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#20248;&#21270;&#24322;&#36136;&#26412;&#22320;&#30446;&#26631;&#21644;&#40723;&#21169;&#32852;&#37030;&#21512;&#20316;&#65292;&#35813;&#31639;&#27861;&#22312;&#22810;&#20010;&#22522;&#32447;&#31639;&#27861;&#21644;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#37117;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16323</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Personalized Federated X -armed Bandit. (arXiv:2310.16323v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;PF-PNE&#31639;&#27861;&#65292;&#36890;&#36807;&#21452;&#37325;&#28120;&#27760;&#31574;&#30053;&#21644;&#26377;&#25928;&#30340;&#26412;&#22320;&#30446;&#26631;&#35780;&#20272;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#20248;&#21270;&#24322;&#36136;&#26412;&#22320;&#30446;&#26631;&#21644;&#40723;&#21169;&#32852;&#37030;&#21512;&#20316;&#65292;&#35813;&#31639;&#27861;&#22312;&#22810;&#20010;&#22522;&#32447;&#31639;&#27861;&#21644;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#37117;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#20854;&#20013;&#22312;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#20013;&#21516;&#26102;&#20248;&#21270;&#20102;&#23458;&#25143;&#31471;&#30340;&#24322;&#36136;&#26412;&#22320;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#29420;&#29305;&#21452;&#37325;&#28120;&#27760;&#31574;&#30053;&#30340;PF-PNE&#31639;&#27861;&#65292;&#36890;&#36807;&#20559;&#24046;&#20294;&#26377;&#25928;&#30340;&#26412;&#22320;&#30446;&#26631;&#35780;&#20272;&#65292;&#23433;&#20840;&#22320;&#28040;&#38500;&#38750;&#26368;&#20248;&#21306;&#22495;&#21516;&#26102;&#40723;&#21169;&#32852;&#37030;&#21512;&#20316;&#12290;&#25152;&#25552;&#20986;&#30340;PF-PNE&#31639;&#27861;&#33021;&#22815;&#20248;&#21270;&#20855;&#26377;&#20219;&#24847;&#24322;&#36136;&#24615;&#27700;&#24179;&#30340;&#26412;&#22320;&#30446;&#26631;&#65292;&#24182;&#19988;&#20854;&#26377;&#38480;&#36890;&#20449;&#20445;&#25252;&#20102;&#23458;&#25143;&#31471;&#22870;&#21169;&#25968;&#25454;&#30340;&#26426;&#23494;&#24615;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30456;&#23545;&#20110;&#21333;&#23458;&#25143;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;PF-PNE&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#22810;&#20010;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study the personalized federated $\mathcal{X}$-armed bandit problem, where the heterogeneous local objectives of the clients are optimized simultaneously in the federated learning paradigm. We propose the \texttt{PF-PNE} algorithm with a unique double elimination strategy, which safely eliminates the non-optimal regions while encouraging federated collaboration through biased but effective evaluations of the local objectives. The proposed \texttt{PF-PNE} algorithm is able to optimize local objectives with arbitrary levels of heterogeneity, and its limited communications protects the confidentiality of the client-wise reward data. Our theoretical analysis shows the benefit of the proposed algorithm over single-client algorithms. Experimentally, \texttt{PF-PNE} outperforms multiple baselines on both synthetic and real life datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20302;&#31934;&#24230;&#21644;&#20840;&#31934;&#24230;&#26799;&#24230;&#32047;&#21152;&#22120;&#30340;&#38543;&#26426;&#26799;&#24230;Hamiltonian Monte Carlo (SGHMC)&#22312;&#20302;&#31934;&#24230;&#37319;&#26679;&#20013;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#19979;&#65292;&#20302;&#31934;&#24230;SGHMC&#30456;&#23545;&#20110;&#20302;&#31934;&#24230;&#37319;&#26679;&#22120;&#65288;SGLD&#65289;&#23454;&#29616;&#20102;&#20108;&#27425;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.16320</link><description>&lt;p&gt;
&#22686;&#24378;&#20302;&#31934;&#24230;&#37319;&#26679;&#65306;&#38543;&#26426;&#26799;&#24230;Hamiltonian Monte Carlo
&lt;/p&gt;
&lt;p&gt;
Enhancing Low-Precision Sampling via Stochastic Gradient Hamiltonian Monte Carlo. (arXiv:2310.16320v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20302;&#31934;&#24230;&#21644;&#20840;&#31934;&#24230;&#26799;&#24230;&#32047;&#21152;&#22120;&#30340;&#38543;&#26426;&#26799;&#24230;Hamiltonian Monte Carlo (SGHMC)&#22312;&#20302;&#31934;&#24230;&#37319;&#26679;&#20013;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#19979;&#65292;&#20302;&#31934;&#24230;SGHMC&#30456;&#23545;&#20110;&#20302;&#31934;&#24230;&#37319;&#26679;&#22120;&#65288;SGLD&#65289;&#23454;&#29616;&#20102;&#20108;&#27425;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31934;&#24230;&#35757;&#32451;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#20302;&#25104;&#26412;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#22826;&#22810;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#20854;&#36125;&#21494;&#26031;&#23545;&#24212;&#29289;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#25913;&#36827;&#30340;&#27867;&#21270;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#23545;&#25968;&#20985;&#21644;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#19979;&#65292;&#20351;&#29992;&#20302;&#31934;&#24230;&#21644;&#20840;&#31934;&#24230;&#26799;&#24230;&#32047;&#21152;&#22120;&#30340;&#38543;&#26426;&#26799;&#24230;Hamiltonian Monte Carlo (SGHMC)&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20026;&#20102;&#22312;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#19979;&#23454;&#29616;2-Wasserstein&#36317;&#31163;&#30340;&#949;&#35823;&#24046;&#65292;&#20302;&#31934;&#24230;SGHMC&#30456;&#23545;&#20110;&#20302;&#31934;&#24230;&#37319;&#26679;&#22120;&#65288;&#38543;&#26426;&#26799;&#24230;Langevin&#21160;&#21147;&#23398;&#65292;SGLD&#65289;&#23454;&#29616;&#20102;&#20108;&#27425;&#25913;&#36827;&#65288;$\widetilde{\mathbf{O}}\left({\epsilon^{-2}{\mu^*}^{-2}\log^2\left({\epsilon^{-1}}\right)}\right)$ vs $\widetilde{\mathbf{O}}\left({{\epsilon}^{-4}{\lambda^{*}}^{-1}\log^5\left({\epsilon^{-1}}\right)}\right)$&#65289;&#12290;&#21478;&#22806;&#65292;&#22522;&#20110;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20302;&#31934;&#24230;SGHMC&#30456;&#23545;&#20110;SGLD&#22312;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-precision training has emerged as a promising low-cost technique to enhance the training efficiency of deep neural networks without sacrificing much accuracy. Its Bayesian counterpart can further provide uncertainty quantification and improved generalization accuracy. This paper investigates low-precision sampling via Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) with low-precision and full-precision gradient accumulators for both strongly log-concave and non-log-concave distributions. Theoretically, our results show that, to achieve $\epsilon$-error in the 2-Wasserstein distance for non-log-concave distributions, low-precision SGHMC achieves quadratic improvement ($\widetilde{\mathbf{O}}\left({\epsilon^{-2}{\mu^*}^{-2}\log^2\left({\epsilon^{-1}}\right)}\right)$) compared to the state-of-the-art low-precision sampler, Stochastic Gradient Langevin Dynamics (SGLD) ($\widetilde{\mathbf{O}}\left({{\epsilon}^{-4}{\lambda^{*}}^{-1}\log^5\left({\epsilon^{-1}}\right)}\right)$). Moreo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#35821;&#35328;&#24418;&#24335;&#26080;&#20851;&#30340;&#20803;&#23398;&#20064;&#33945;&#29256;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;MetaMAE&#65292;&#36890;&#36807;&#23558;&#33945;&#29256;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#30340;&#33945;&#29256;&#37325;&#26500;&#35270;&#20026;&#20803;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#37319;&#29992;&#36716;&#25442;&#22120;&#20803;&#23398;&#20064;&#25216;&#26415;&#26469;&#25913;&#36827;MAE&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#19981;&#21516;&#35821;&#35328;&#24418;&#24335;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.16318</link><description>&lt;p&gt;
&#19968;&#31181;&#19982;&#35821;&#35328;&#24418;&#24335;&#26080;&#20851;&#30340;&#20803;&#23398;&#20064;&#33945;&#29256;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Modality-Agnostic Self-Supervised Learning with Meta-Learned Masked Auto-Encoder. (arXiv:2310.16318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#35821;&#35328;&#24418;&#24335;&#26080;&#20851;&#30340;&#20803;&#23398;&#20064;&#33945;&#29256;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;MetaMAE&#65292;&#36890;&#36807;&#23558;&#33945;&#29256;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#30340;&#33945;&#29256;&#37325;&#26500;&#35270;&#20026;&#20803;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#37319;&#29992;&#36716;&#25442;&#22120;&#20803;&#23398;&#20064;&#25216;&#26415;&#26469;&#25913;&#36827;MAE&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#19981;&#21516;&#35821;&#35328;&#24418;&#24335;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#21508;&#31181;&#35821;&#35328;&#24418;&#24335;&#20013;&#20855;&#26377;&#23454;&#38469;&#37325;&#35201;&#24615;&#65292;&#20294;&#36817;&#26399;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23569;&#25968;&#32463;&#36807;&#31934;&#36873;&#30340;&#39046;&#22495;&#65292;&#22914;&#35270;&#35273;&#21644;&#35821;&#35328;&#65292;&#24182;&#19988;&#24120;&#24120;&#20381;&#36182;&#20110;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#33945;&#29256;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#20316;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#12289;&#19982;&#35821;&#35328;&#24418;&#24335;&#26080;&#20851;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#36827;&#34892;&#20102;&#25299;&#23637;&#12290;&#25105;&#20204;&#35748;&#20026;&#20803;&#23398;&#20064;&#26159;&#35299;&#37322;MAE&#20316;&#20026;&#19982;&#35821;&#35328;&#24418;&#24335;&#26080;&#20851;&#23398;&#20064;&#22120;&#30340;&#20851;&#38190;&#65292;&#24182;&#20174;&#20849;&#21516;&#25552;&#39640;&#20854;&#22312;&#22810;&#31181;&#35821;&#35328;&#24418;&#24335;&#19978;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#30340;&#21160;&#26426;&#20986;&#21457;&#65292;&#25552;&#20986;&#20102;MetaMAE&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;MAE&#30340;&#33945;&#29256;&#37325;&#26500;&#35270;&#20026;&#19968;&#20010;&#20803;&#23398;&#20064;&#20219;&#21153;&#65306;&#36890;&#36807;&#23545;&#26410;&#33945;&#29256;&#26631;&#35760;&#36827;&#34892;&#33258;&#36866;&#24212;&#26469;&#39044;&#27979;&#33945;&#29256;&#26631;&#35760;&#65292;&#20174;&#32780;&#36890;&#36807;&#36716;&#25442;&#22120;&#20803;&#23398;&#20064;&#23454;&#29616;&#23545;&#20854;&#36827;&#34892;&#24635;&#35823;&#24046;&#20943;&#23567;&#12290;&#22522;&#20110;&#36825;&#19968;&#26032;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#20004;&#31181;&#39640;&#32423;&#20803;&#23398;&#20064;&#25216;&#26415;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its practical importance across a wide range of modalities, recent advances in self-supervised learning (SSL) have been primarily focused on a few well-curated domains, e.g., vision and language, often relying on their domain-specific knowledge. For example, Masked Auto-Encoder (MAE) has become one of the popular architectures in these domains, but less has explored its potential in other modalities. In this paper, we develop MAE as a unified, modality-agnostic SSL framework. In turn, we argue meta-learning as a key to interpreting MAE as a modality-agnostic learner, and propose enhancements to MAE from the motivation to jointly improve its SSL across diverse modalities, coined MetaMAE as a result. Our key idea is to view the mask reconstruction of MAE as a meta-learning task: masked tokens are predicted by adapting the Transformer meta-learner through the amortization of unmasked tokens. Based on this novel interpretation, we propose to integrate two advanced meta-learning tec
&lt;/p&gt;</description></item><item><title>Sum-of-Parts&#27169;&#22411;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#29305;&#24449;&#32452;&#24402;&#22240;&#30340;&#24544;&#23454;&#24615;&#65292;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#20998;&#25968;&#20043;&#21644;&#65292;&#24110;&#21161;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#21457;&#29616;&#20102;&#20851;&#20110;&#26143;&#31995;&#24418;&#25104;&#30340;&#26032;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.16316</link><description>&lt;p&gt;
Sum-of-Parts&#27169;&#22411;&#65306;&#23545;&#29305;&#24449;&#32452;&#30340;&#24544;&#23454;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Sum-of-Parts Models: Faithful Attributions for Groups of Features. (arXiv:2310.16316v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16316
&lt;/p&gt;
&lt;p&gt;
Sum-of-Parts&#27169;&#22411;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#29305;&#24449;&#32452;&#24402;&#22240;&#30340;&#24544;&#23454;&#24615;&#65292;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#20998;&#25968;&#20043;&#21644;&#65292;&#24110;&#21161;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#21457;&#29616;&#20102;&#20851;&#20110;&#26143;&#31995;&#24418;&#25104;&#30340;&#26032;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#20934;&#30830;&#21453;&#26144;&#20102;&#20854;&#20915;&#31574;&#36807;&#31243;&#65292;&#21017;&#34987;&#35748;&#20026;&#26159;&#8220;&#24544;&#23454;&#8221;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#20363;&#22914;&#28145;&#24230;&#23398;&#20064;&#30340;&#29305;&#24449;&#24402;&#22240;&#31561;&#35299;&#37322;&#24182;&#19981;&#33021;&#20445;&#35777;&#24544;&#23454;&#65292;&#26377;&#21487;&#33021;&#20135;&#29983;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Sum-of-Parts&#65288;SOP&#65289;&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#31867;&#27169;&#22411;&#65292;&#20854;&#39044;&#27979;&#20855;&#26377;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#24544;&#23454;&#30340;&#29305;&#24449;&#32452;&#24402;&#22240;&#12290;&#35813;&#27169;&#22411;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#20998;&#25968;&#20043;&#21644;&#65292;&#27599;&#20010;&#20998;&#25968;&#30452;&#25509;&#24402;&#22240;&#20110;&#19968;&#32452;&#31232;&#30095;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#23545;SOP&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#22312;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#21033;&#29992;SOP&#25552;&#20379;&#30340;&#24544;&#23454;&#35299;&#37322;&#24110;&#21161;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#21457;&#29616;&#20102;&#20851;&#20110;&#26143;&#31995;&#24418;&#25104;&#30340;&#26032;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
An explanation of a machine learning model is considered "faithful" if it accurately reflects the model's decision-making process. However, explanations such as feature attributions for deep learning are not guaranteed to be faithful, and can produce potentially misleading interpretations. In this work, we develop Sum-of-Parts (SOP), a class of models whose predictions come with grouped feature attributions that are faithful-by-construction. This model decomposes a prediction into an interpretable sum of scores, each of which is directly attributable to a sparse group of features. We evaluate SOP on benchmarks with standard interpretability metrics, and in a case study, we use the faithful explanations from SOP to help astrophysicists discover new knowledge about galaxy formation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#20195;&#30721;&#25688;&#35201;&#30340;&#26377;&#25928;&#24615;&#21644;&#24341;&#20837;&#23545;&#25239;&#24615;&#26696;&#20363;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#20195;&#30721;&#29702;&#35299;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#22312;&#29702;&#35299;&#20195;&#30721;&#35821;&#20041;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#65292;&#36825;&#23545;&#20110;&#25552;&#39640;&#36719;&#20214;&#24320;&#21457;&#21644;&#32500;&#25252;&#25928;&#29575;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.16314</link><description>&lt;p&gt;
&#20102;&#35299;&#20195;&#30721;&#35821;&#20041;: &#23545;Transformer&#27169;&#22411;&#22312;&#25688;&#35201;&#20013;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Understanding Code Semantics: An Evaluation of Transformer Models in Summarization. (arXiv:2310.16314v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#20195;&#30721;&#25688;&#35201;&#30340;&#26377;&#25928;&#24615;&#21644;&#24341;&#20837;&#23545;&#25239;&#24615;&#26696;&#20363;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#20195;&#30721;&#29702;&#35299;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#22312;&#29702;&#35299;&#20195;&#30721;&#35821;&#20041;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#65292;&#36825;&#23545;&#20110;&#25552;&#39640;&#36719;&#20214;&#24320;&#21457;&#21644;&#32500;&#25252;&#25928;&#29575;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#20808;&#36827;&#30340;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#28145;&#20837;&#30740;&#31350;&#20102;&#20195;&#30721;&#25688;&#35201;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#36890;&#36807;&#25913;&#21464;&#20989;&#25968;&#21644;&#21464;&#37327;&#21517;&#26469;&#35780;&#20272;&#20195;&#30721;&#25688;&#35201;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#25506;&#32034;&#27169;&#22411;&#26159;&#21542;&#30495;&#27491;&#29702;&#35299;&#20195;&#30721;&#35821;&#20041;&#65292;&#36824;&#26159;&#20165;&#20165;&#20381;&#36182;&#25991;&#26412;&#32447;&#32034;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#27515;&#20195;&#30721;&#21644;&#27880;&#37322;&#20195;&#30721;&#31561;&#23545;&#25239;&#24615;&#26696;&#20363;&#65292;&#28085;&#30422;&#20102;&#19977;&#31181;&#32534;&#31243;&#35821;&#35328;(Python&#12289;Javascript&#21644;Java)&#65292;&#20197;&#36827;&#19968;&#27493;&#23457;&#26597;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#21152;&#24378;Transformer&#27169;&#22411;&#29702;&#35299;&#20195;&#30721;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#20026;&#26356;&#39640;&#25928;&#30340;&#36719;&#20214;&#24320;&#21457;&#23454;&#36341;&#21644;&#32500;&#25252;&#24037;&#20316;&#27969;&#31243;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper delves into the intricacies of code summarization using advanced transformer-based language models. Through empirical studies, we evaluate the efficacy of code summarization by altering function and variable names to explore whether models truly understand code semantics or merely rely on textual cues. We have also introduced adversaries like dead code and commented code across three programming languages (Python, Javascript, and Java) to further scrutinize the model's understanding. Ultimately, our research aims to offer valuable insights into the inner workings of transformer-based LMs, enhancing their ability to understand code and contributing to more efficient software development practices and maintenance workflows.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SMASH&#26041;&#27861;&#65292;&#22522;&#20110;&#20998;&#25968;&#21305;&#37197;&#30340;&#20266;&#20284;&#28982;&#24230;&#37327;&#65292;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26631;&#35760;&#26102;&#31354;&#28857;&#36807;&#31243;&#65288;STPPs&#65289;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#24402;&#19968;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.16310</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#21305;&#37197;&#30340;&#31070;&#32463;&#26631;&#35760;&#26102;&#31354;&#28857;&#36807;&#31243;&#30340;&#20266;&#20284;&#28982;&#24230;&#37327;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Score Matching-based Pseudolikelihood Estimation of Neural Marked Spatio-Temporal Point Process with Uncertainty Quantification. (arXiv:2310.16310v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16310
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SMASH&#26041;&#27861;&#65292;&#22522;&#20110;&#20998;&#25968;&#21305;&#37197;&#30340;&#20266;&#20284;&#28982;&#24230;&#37327;&#65292;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26631;&#35760;&#26102;&#31354;&#28857;&#36807;&#31243;&#65288;STPPs&#65289;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#24402;&#19968;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#28857;&#36807;&#31243;&#65288;STPPs&#65289;&#26159;&#24314;&#27169;&#21644;&#39044;&#27979;&#20855;&#26377;&#26102;&#38388;&#21644;&#31354;&#38388;&#29305;&#24449;&#30340;&#20107;&#20214;&#30340;&#24378;&#22823;&#25968;&#23398;&#24037;&#20855;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#23398;&#20064;STPPs&#30340;&#26041;&#27861;&#35201;&#20040;&#20551;&#35774;&#20102;&#19968;&#31181;&#21463;&#38480;&#30340;&#26102;&#31354;&#20998;&#24067;&#24418;&#24335;&#65292;&#35201;&#20040;&#30001;&#20110;&#22797;&#26434;&#31215;&#20998;&#30340;&#19981;&#20934;&#30830;&#36817;&#20284;&#32780;&#36973;&#21463;&#20102;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#36890;&#24120;&#26469;&#33258;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#24402;&#19968;&#21270;&#39033;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#25216;&#26415;&#26080;&#27861;&#20026;&#27169;&#22411;&#39044;&#27979;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20363;&#22914;&#39044;&#27979;&#20107;&#20214;&#21040;&#36798;&#26102;&#38388;&#30340;&#32622;&#20449;&#21306;&#38388;&#21644;&#20107;&#20214;&#20301;&#32622;&#30340;&#32622;&#20449;&#21306;&#22495;&#65292;&#36825;&#22312;&#32771;&#34385;&#21040;&#25968;&#25454;&#30340;&#30456;&#24403;&#38543;&#26426;&#24615;&#30340;&#24773;&#20917;&#19979;&#26159;&#20851;&#38190;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SMASH&#65306;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#21305;&#37197;&#30340;&#20266;&#20284;&#28982;&#24230;&#37327;&#65292;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26631;&#35760;STPPs&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#20272;&#35745;&#26469;&#33258;&#26631;&#35760;&#25968;&#25454;&#30340;&#37319;&#26679;&#39044;&#27979;&#20998;&#24067;&#26469;&#37319;&#29992;&#26080;&#24402;&#19968;&#21270;&#30340;&#30446;&#26631;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20998;&#25968;&#21305;&#37197;&#20174;&#27169;&#22411;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#24182;&#26368;&#23567;&#21270;&#19982;&#30495;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;&#20998;&#25968;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatio-temporal point processes (STPPs) are potent mathematical tools for modeling and predicting events with both temporal and spatial features. Despite their versatility, most existing methods for learning STPPs either assume a restricted form of the spatio-temporal distribution, or suffer from inaccurate approximations of the intractable integral in the likelihood training objective. These issues typically arise from the normalization term of the probability density function. Moreover, current techniques fail to provide uncertainty quantification for model predictions, such as confidence intervals for the predicted event's arrival time and confidence regions for the event's location, which is crucial given the considerable randomness of the data. To tackle these challenges, we introduce SMASH: a Score MAtching-based pSeudolikeliHood estimator for learning marked STPPs with uncertainty quantification. Specifically, our framework adopts a normalization-free objective by estimating the
&lt;/p&gt;</description></item><item><title>Dolfin&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#24067;&#23616;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;Transformer-based&#25193;&#25955;&#36807;&#31243;&#21644;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;(Dolfin-AR)&#23454;&#29616;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#24314;&#27169;&#33021;&#21147;&#21644;&#36739;&#20302;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16305</link><description>&lt;p&gt;
Dolfin: &#26080;&#33258;&#32534;&#30721;&#22120;&#30340;&#25193;&#25955;&#24067;&#23616;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Dolfin: Diffusion Layout Transformers without Autoencoder. (arXiv:2310.16305v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16305
&lt;/p&gt;
&lt;p&gt;
Dolfin&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#24067;&#23616;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;Transformer-based&#25193;&#25955;&#36807;&#31243;&#21644;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;(Dolfin-AR)&#23454;&#29616;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#24314;&#27169;&#33021;&#21147;&#21644;&#36739;&#20302;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#31216;&#20026;&#26080;&#33258;&#32534;&#30721;&#22120;&#30340;&#25193;&#25955;&#24067;&#23616;&#21464;&#25442;&#22120;&#65288;Dolfin&#65289;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#23427;&#22312;&#20943;&#23567;&#22797;&#26434;&#24615;&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#24314;&#27169;&#33021;&#21147;&#12290; Dolfin&#37319;&#29992;&#22522;&#20110;Transformer&#30340;&#25193;&#25955;&#36807;&#31243;&#26469;&#36827;&#34892;&#24067;&#23616;&#29983;&#25104;&#12290;&#38500;&#20102;&#39640;&#25928;&#30340;&#21452;&#21521;&#65288;&#38750;&#22240;&#26524;&#36830;&#25509;&#65289;&#24207;&#21015;&#34920;&#31034;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#65288;Dolfin-AR&#65289;&#65292;&#29305;&#21035;&#25797;&#38271;&#25429;&#25417;&#37051;&#36817;&#23545;&#35937;&#30340;&#20016;&#23500;&#35821;&#20041;&#30456;&#20851;&#24615;&#65292;&#22914;&#23545;&#40784;&#12289;&#22823;&#23567;&#21644;&#37325;&#21472;&#12290;&#22312;&#26631;&#20934;&#29983;&#25104;&#24067;&#23616;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;Dolfin&#22312;&#21508;&#31181;&#25351;&#26631;&#65288;fid&#12289;&#23545;&#40784;&#12289;&#37325;&#21472;&#12289;MaxIoU&#21644;DocSim&#20998;&#25968;&#65289;&#19978;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#22686;&#24378;&#20102;&#36879;&#26126;&#24230;&#21644;&#20114;&#25805;&#20316;&#24615;&#12290;&#27492;&#22806;&#65292;Dolfin&#30340;&#24212;&#29992;&#19981;&#20165;&#23616;&#38480;&#20110;&#24067;&#23616;&#29983;&#25104;&#65292;&#36824;&#36866;&#29992;&#20110;&#24314;&#27169;&#20960;&#20309;&#32467;&#26500;&#65292;&#22914;&#32447;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel generative model, Diffusion Layout Transformers without Autoencoder (Dolfin), which significantly improves the modeling capability with reduced complexity compared to existing methods. Dolfin employs a Transformer-based diffusion process to model layout generation. In addition to an efficient bi-directional (non-causal joint) sequence representation, we further propose an autoregressive diffusion model (Dolfin-AR) that is especially adept at capturing rich semantic correlations for the neighboring objects, such as alignment, size, and overlap. When evaluated against standard generative layout benchmarks, Dolfin notably improves performance across various metrics (fid, alignment, overlap, MaxIoU and DocSim scores), enhancing transparency and interoperability in the process. Moreover, Dolfin's applications extend beyond layout generation, making it suitable for modeling geometric structures, such as line segments. Our experiments present both qualitati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#19981;&#23436;&#32654;&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#24378;&#21270;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#26469;&#20943;&#23569;&#23454;&#38469;&#35757;&#32451;&#30340;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#21644;&#34394;&#25311;&#29983;&#25104;&#30340;&#26080;&#20154;&#26426;&#28151;&#21512;&#37096;&#32626;&#26041;&#27861;&#26469;&#24179;&#34913;&#35757;&#32451;&#25104;&#26412;&#12289;&#25968;&#23383;&#23402;&#29983;&#24314;&#35774;&#25104;&#26412;&#21644;&#25968;&#23383;&#23402;&#29983;&#20559;&#24046;&#23545;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.16302</link><description>&lt;p&gt;
&#22810;&#26080;&#20154;&#26426;&#32593;&#32476;&#30340;&#20302;&#25104;&#26412;&#19981;&#23436;&#32654;&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#24378;&#21270;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Imperfect Digital Twin Assisted Low Cost Reinforcement Training for Multi-UAV Networks. (arXiv:2310.16302v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#19981;&#23436;&#32654;&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#24378;&#21270;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#26469;&#20943;&#23569;&#23454;&#38469;&#35757;&#32451;&#30340;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#21644;&#34394;&#25311;&#29983;&#25104;&#30340;&#26080;&#20154;&#26426;&#28151;&#21512;&#37096;&#32626;&#26041;&#27861;&#26469;&#24179;&#34913;&#35757;&#32451;&#25104;&#26412;&#12289;&#25968;&#23383;&#23402;&#29983;&#24314;&#35774;&#25104;&#26412;&#21644;&#25968;&#23383;&#23402;&#29983;&#20559;&#24046;&#23545;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#20248;&#21270;&#22810;&#26080;&#20154;&#26426;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;DRL&#30340;&#35757;&#32451;&#20381;&#36182;&#20110;&#26080;&#20154;&#26426;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#39057;&#32321;&#20132;&#20114;&#65292;&#22312;&#23454;&#38469;&#23454;&#39564;&#20013;&#65292;&#30001;&#20110;&#26080;&#20154;&#26426;&#30340;&#39134;&#34892;&#21644;&#36890;&#20449;&#65292;&#36825;&#28040;&#32791;&#20102;&#22823;&#37327;&#33021;&#28304;&#12290;&#21463;&#25968;&#23383;&#23402;&#29983;&#65288;DT&#65289;&#25216;&#26415;&#30340;&#21551;&#21457;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#36890;&#36807;&#22797;&#21046;&#29289;&#29702;&#31354;&#38388;&#30340;&#29305;&#24449;&#26500;&#24314;&#25968;&#23383;&#31354;&#38388;&#26469;&#27169;&#25311;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DT&#26469;&#38477;&#20302;&#23454;&#38469;&#35757;&#32451;&#30340;&#25104;&#26412;&#65292;&#20363;&#22914;&#33021;&#28304;&#21644;&#30828;&#20214;&#36141;&#20080;&#12290;&#19982;&#20808;&#21069;&#30340;DT&#36741;&#21161;&#24037;&#20316;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20559;&#31163;&#30495;&#23454;&#29289;&#29702;&#24773;&#20917;&#30340;&#19981;&#23436;&#32654;DT&#27169;&#22411;&#26469;&#36741;&#21161;&#22810;&#26080;&#20154;&#26426;&#32593;&#32476;&#30340;&#35757;&#32451;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20026;&#20102;&#26435;&#34913;&#35757;&#32451;&#25104;&#26412;&#12289;DT&#26500;&#24314;&#25104;&#26412;&#21644;DT&#20559;&#24046;&#23545;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#21644;&#34394;&#25311;&#29983;&#25104;&#30340;&#26080;&#20154;&#26426;&#28151;&#21512;&#37096;&#32626;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (DRL) is widely used to optimize the performance of multi-UAV networks. However, the training of DRL relies on the frequent interactions between the UAVs and the environment, which consumes lots of energy due to the flying and communication of UAVs in practical experiments. Inspired by the growing digital twin (DT) technology, which can simulate the performance of algorithms in the digital space constructed by coping features of the physical space, the DT is introduced to reduce the costs of practical training, e.g., energy and hardware purchases. Different from previous DT-assisted works with an assumption of perfect reflecting real physics by virtual digital, we consider an imperfect DT model with deviations for assisting the training of multi-UAV networks. Remarkably, to trade off the training cost, DT construction cost, and the impact of deviations of DT on training, the natural and virtually generated UAV mixing deployment method is proposed. Two cascad
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#20363;&#21270;&#32447;&#24615;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#36890;&#36807;&#32473;&#27169;&#22411;&#20869;&#37096;&#30340;&#27599;&#20010;&#36755;&#20837;&#29305;&#24449;&#20998;&#37197;&#37325;&#35201;&#24471;&#20998;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#22914;&#20309;&#20351;&#29992;&#29305;&#24449;&#20570;&#20986;&#20915;&#31574;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#24403;&#21069;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16295</link><description>&lt;p&gt;
&#23545;&#20110;&#27169;&#22411;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#23454;&#20363;&#21270;&#32447;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Instance-wise Linearization of Neural Network for Model Interpretation. (arXiv:2310.16295v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16295
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#20363;&#21270;&#32447;&#24615;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#36890;&#36807;&#32473;&#27169;&#22411;&#20869;&#37096;&#30340;&#27599;&#20010;&#36755;&#20837;&#29305;&#24449;&#20998;&#37197;&#37325;&#35201;&#24471;&#20998;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#22914;&#20309;&#20351;&#29992;&#29305;&#24449;&#20570;&#20986;&#20915;&#31574;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#24403;&#21069;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#26159;&#23558;&#36825;&#31181;&#25216;&#26415;&#24212;&#29992;&#20110;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#25361;&#25112;&#22312;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#34892;&#20026;&#65292;&#36825;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#38190;&#24615;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#22914;&#20309;&#20351;&#29992;&#36755;&#20837;&#29305;&#24449;&#36827;&#34892;&#20915;&#31574;&#12290;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#32463;&#20856;&#26041;&#27861;&#26159;&#29305;&#24449;&#24402;&#22240;&#65292;&#23427;&#20026;&#27599;&#20010;&#36755;&#20837;&#29305;&#24449;&#20998;&#37197;&#19968;&#20010;&#37325;&#35201;&#24471;&#20998;&#65292;&#24182;&#25581;&#31034;&#20854;&#23545;&#24403;&#21069;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#32463;&#24120;&#25351;&#31034;&#27599;&#20010;&#36755;&#20837;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#27809;&#26377;&#35814;&#32454;&#35828;&#26126;&#23427;&#20204;&#22312;&#27169;&#22411;&#20869;&#37096;&#23454;&#38469;&#19978;&#26159;&#22914;&#20309;&#22788;&#29702;&#30340;&#12290;&#36825;&#20123;&#24402;&#22240;&#26041;&#27861;&#24120;&#24120;&#24341;&#21457;&#19968;&#20010;&#20851;&#27880;&#28857;&#65292;&#21363;&#23427;&#20204;&#26159;&#21542;&#27491;&#30830;&#22320;&#24378;&#35843;&#20102;&#27169;&#22411;&#39044;&#27979;&#30340;&#29305;&#24449;&#12290;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#38750;&#32447;&#24615;&#34892;&#20026;&#36890;&#24120;&#26159;&#30001;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#28608;&#27963;&#21333;&#20803;&#24341;&#36215;&#30340;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#30340;&#35745;&#31639;&#34892;&#20026;&#24448;&#24448;&#26159;&#22797;&#26434;&#30340;&#65292;&#36825;&#20351;&#24471;&#35299;&#37322;&#21644;&#29702;&#35299;&#27169;&#22411;&#30340;&#20915;&#31574;&#21464;&#24471;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network have achieved remarkable successes in many scientific fields. However, the interpretability of the neural network model is still a major bottlenecks to deploy such technique into our daily life. The challenge can dive into the non-linear behavior of the neural network, which rises a critical question that how a model use input feature to make a decision. The classical approach to address this challenge is feature attribution, which assigns an important score to each input feature and reveal its importance of current prediction. However, current feature attribution approaches often indicate the importance of each input feature without detail of how they are actually processed by a model internally. These attribution approaches often raise a concern that whether they highlight correct features for a model prediction.  For a neural network model, the non-linear behavior is often caused by non-linear activation units of a model. However, the computation behavior of a predict
&lt;/p&gt;</description></item><item><title>Crowd-Certain&#26159;&#19968;&#31181;&#22312;&#20247;&#21253;&#21644;&#38598;&#25104;&#23398;&#20064;&#20998;&#31867;&#20013;&#36827;&#34892;&#26631;&#31614;&#32858;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#26631;&#27880;&#32773;&#30340;&#19968;&#33268;&#24615;&#19982;&#35757;&#32451;&#22909;&#30340;&#20998;&#31867;&#22120;&#65292;&#30830;&#23450;&#27599;&#20010;&#26631;&#27880;&#32773;&#30340;&#21487;&#38752;&#24615;&#24471;&#20998;&#65292;&#24182;&#21033;&#29992;&#39044;&#27979;&#27010;&#29575;&#23454;&#29616;&#20102;&#23545;&#26410;&#26469;&#26679;&#26412;&#25968;&#25454;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.16293</link><description>&lt;p&gt;
Crowd-Certain: &#20247;&#21253;&#21644;&#38598;&#25104;&#23398;&#20064;&#20998;&#31867;&#20013;&#30340;&#26631;&#31614;&#32858;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Crowd-Certain: Label Aggregation in Crowdsourced and Ensemble Learning Classification. (arXiv:2310.16293v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16293
&lt;/p&gt;
&lt;p&gt;
Crowd-Certain&#26159;&#19968;&#31181;&#22312;&#20247;&#21253;&#21644;&#38598;&#25104;&#23398;&#20064;&#20998;&#31867;&#20013;&#36827;&#34892;&#26631;&#31614;&#32858;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#26631;&#27880;&#32773;&#30340;&#19968;&#33268;&#24615;&#19982;&#35757;&#32451;&#22909;&#30340;&#20998;&#31867;&#22120;&#65292;&#30830;&#23450;&#27599;&#20010;&#26631;&#27880;&#32773;&#30340;&#21487;&#38752;&#24615;&#24471;&#20998;&#65292;&#24182;&#21033;&#29992;&#39044;&#27979;&#27010;&#29575;&#23454;&#29616;&#20102;&#23545;&#26410;&#26469;&#26679;&#26412;&#25968;&#25454;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#21253;&#31995;&#32479;&#24050;&#34987;&#29992;&#20110;&#31215;&#32047;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#65292;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20247;&#21253;&#26631;&#35760;&#30340;&#21160;&#24577;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#24320;&#21457;&#19968;&#20010;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#37117;&#33021;&#24037;&#20316;&#30340;&#25216;&#26415;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Crowd-Certain&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#29992;&#20110;&#20247;&#21253;&#21644;&#38598;&#25104;&#23398;&#20064;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26631;&#31614;&#32858;&#21512;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#26631;&#27880;&#32773;&#25968;&#37327;&#21644;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#33021;&#22815;&#25552;&#20379;&#25913;&#36827;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#26631;&#27880;&#32773;&#30340;&#19968;&#33268;&#24615;&#19982;&#35757;&#32451;&#22909;&#30340;&#20998;&#31867;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#20026;&#27599;&#20010;&#26631;&#27880;&#32773;&#30830;&#23450;&#21487;&#38752;&#24615;&#24471;&#20998;&#12290;&#27492;&#22806;&#65292;Crowd-Certain&#21033;&#29992;&#39044;&#27979;&#27010;&#29575;&#65292;&#20351;&#24471;&#35757;&#32451;&#22909;&#30340;&#20998;&#31867;&#22120;&#21487;&#20197;&#22312;&#26410;&#26469;&#30340;&#26679;&#26412;&#25968;&#25454;&#19978;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#22266;&#26377;&#30340;&#37325;&#22797;&#27169;&#25311;&#36807;&#31243;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#19982;&#20854;&#20182;&#21313;&#31181;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crowdsourcing systems have been used to accumulate massive amounts of labeled data for applications such as computer vision and natural language processing. However, because crowdsourced labeling is inherently dynamic and uncertain, developing a technique that can work in most situations is extremely challenging. In this paper, we introduce Crowd-Certain, a novel approach for label aggregation in crowdsourced and ensemble learning classification tasks that offers improved performance and computational efficiency for different numbers of annotators and a variety of datasets. The proposed method uses the consistency of the annotators versus a trained classifier to determine a reliability score for each annotator. Furthermore, Crowd-Certain leverages predicted probabilities, enabling the reuse of trained classifiers on future sample data, thereby eliminating the need for recurrent simulation processes inherent in existing methods. We extensively evaluated our approach against ten existing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#21435;&#38500;CMB&#35266;&#27979;&#20013;&#30340;&#23576;&#22467;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#24456;&#22909;&#22320;&#27169;&#25311;&#23576;&#22467;&#21069;&#26223;&#24182;&#36827;&#34892;&#25104;&#20998;&#20998;&#31163;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#19982;CMB&#23431;&#23449;&#23398;&#26465;&#20214;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#22312;&#25104;&#20998;&#20998;&#31163;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16285</link><description>&lt;p&gt;
&#29992;&#25193;&#25955;&#27169;&#22411;&#21435;&#38500;CMB&#35266;&#27979;&#20013;&#30340;&#23576;&#22467;
&lt;/p&gt;
&lt;p&gt;
Removing Dust from CMB Observations with Diffusion Models. (arXiv:2310.16285v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#21435;&#38500;CMB&#35266;&#27979;&#20013;&#30340;&#23576;&#22467;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#24456;&#22909;&#22320;&#27169;&#25311;&#23576;&#22467;&#21069;&#26223;&#24182;&#36827;&#34892;&#25104;&#20998;&#20998;&#31163;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#19982;CMB&#23431;&#23449;&#23398;&#26465;&#20214;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#22312;&#25104;&#20998;&#20998;&#31163;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23431;&#23449;&#23398;&#20013;&#65292;&#36861;&#23547;&#23431;&#23449;&#24494;&#27874;&#32972;&#26223;(CMB)&#35266;&#27979;&#20013;&#30340;&#21407;&#22987;$B$-mode&#65292;&#31361;&#20986;&#20102;&#23545;&#38134;&#27827;&#23576;&#22467;&#21069;&#26223;&#30340;&#31934;&#30830;&#24314;&#27169;&#30340;&#20851;&#38190;&#38656;&#27714;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#23576;&#22467;&#21069;&#26223;&#24314;&#27169;&#21450;&#20854;&#22312;&#25104;&#20998;&#20998;&#31163;&#20013;&#30340;&#24847;&#20041;&#12290;&#22312;&#20551;&#35774;&#20855;&#26377;&#24050;&#30693;&#23431;&#23449;&#23398;(&#25110;&#21327;&#26041;&#24046;&#30697;&#38453;)&#30340;&#39640;&#26031;CMB&#30340;&#21069;&#25552;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#22312;&#23576;&#22467;&#36752;&#23556;&#22320;&#22270;&#30340;&#23454;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#20854;&#37319;&#26679;&#36807;&#31243;&#30452;&#25509;&#19982;&#25104;&#20998;&#20998;&#31163;&#19978;&#19979;&#25991;&#20013;&#30340;&#21518;&#39564;&#37319;&#26679;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#29992;&#27169;&#25311;&#30340;&#23576;&#22467;&#36752;&#23556;&#21644;CMB&#30340;&#28151;&#21512;&#29289;&#26469;&#35828;&#26126;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#24456;&#22909;&#22320;&#24674;&#22797;&#32452;&#20998;&#30340;&#24120;&#35265;&#25688;&#35201;&#32479;&#35745;&#37327;(&#21151;&#29575;&#35889;&#12289;&#38389;&#21487;&#22827;&#26031;&#22522;&#20989;&#25968;)&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19982;CMB&#23431;&#23449;&#23398;&#26465;&#20214;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#27604;&#21333;&#19968;&#23431;&#23449;&#23398;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#25104;&#20998;&#20998;&#31163;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#23558;&#22312;&#26410;&#26469;&#30340;&#24037;&#20316;&#20013;&#29992;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#23431;&#23449;&#23398;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
In cosmology, the quest for primordial $B$-modes in cosmic microwave background (CMB) observations has highlighted the critical need for a refined model of the Galactic dust foreground. We investigate diffusion-based modeling of the dust foreground and its interest for component separation. Under the assumption of a Gaussian CMB with known cosmology (or covariance matrix), we show that diffusion models can be trained on examples of dust emission maps such that their sampling process directly coincides with posterior sampling in the context of component separation. We illustrate this on simulated mixtures of dust emission and CMB. We show that common summary statistics (power spectrum, Minkowski functionals) of the components are well recovered by this process. We also introduce a model conditioned by the CMB cosmology that outperforms models trained using a single cosmology on component separation. Such a model will be used in future work for diffusion-based cosmological inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23398;&#20064;&#32593;&#32476;&#21442;&#25968;&#30340;&#39046;&#22495;&#19981;&#21464;&#21518;&#39564;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PosTerior Generalization&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2310.16277</link><description>&lt;p&gt;
Bayesian&#39046;&#22495;&#19981;&#21464;&#23398;&#20064;&#36890;&#36807;&#21442;&#25968;&#20998;&#24067;&#30340;&#21518;&#39564;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bayesian Domain Invariant Learning via Posterior Generalization of Parameter Distributions. (arXiv:2310.16277v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23398;&#20064;&#32593;&#32476;&#21442;&#25968;&#30340;&#39046;&#22495;&#19981;&#21464;&#21518;&#39564;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PosTerior Generalization&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#19981;&#21464;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#33021;&#22815;&#25552;&#21462;&#21508;&#31181;&#35757;&#32451;&#39046;&#22495;&#20013;&#19981;&#21464;&#29305;&#24449;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;&#39046;&#22495;&#12290;&#26368;&#36817;&#65292;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#22312;&#39046;&#22495;&#19981;&#21464;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#23545;&#40784;&#29305;&#24449;&#20998;&#24067;&#32780;&#19981;&#26159;&#21442;&#25968;&#20998;&#24067;&#12290;&#21463;&#21040;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21407;&#29702;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35797;&#22270;&#30452;&#25509;&#23398;&#20064;&#32593;&#32476;&#21442;&#25968;&#30340;&#39046;&#22495;&#19981;&#21464;&#21518;&#39564;&#20998;&#24067;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#29702;&#65292;&#34920;&#26126;&#21487;&#20197;&#36890;&#36807;&#32858;&#21512;&#19981;&#21516;&#35757;&#32451;&#39046;&#22495;&#19978;&#30340;&#21518;&#39564;&#26469;&#38544;&#24335;&#25512;&#26029;&#21442;&#25968;&#30340;&#19981;&#21464;&#21518;&#39564;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26356;&#20855;&#23485;&#26494;&#24615;&#65292;&#21487;&#20197;&#25552;&#21462;&#26356;&#22810;&#30340;&#39046;&#22495;&#19981;&#21464;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"PosTerior Generalization (PTG)"&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#19981;&#21464;&#30340;&#21442;&#25968;&#20998;&#24067;&#12290;PTG&#20805;&#20998;&#21033;&#29992;&#20102;&#21464;&#20998;&#25512;&#26029;&#26469;&#36817;&#20284;&#21442;&#25968;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain invariant learning aims to learn models that extract invariant features over various training domains, resulting in better generalization to unseen target domains. Recently, Bayesian Neural Networks have achieved promising results in domain invariant learning, but most works concentrate on aligning features distributions rather than parameter distributions. Inspired by the principle of Bayesian Neural Network, we attempt to directly learn the domain invariant posterior distribution of network parameters. We first propose a theorem to show that the invariant posterior of parameters can be implicitly inferred by aggregating posteriors on different training domains. Our assumption is more relaxed and allows us to extract more domain invariant information. We also propose a simple yet effective method, named PosTerior Generalization (PTG), that can be used to estimate the invariant parameter distribution. PTG fully exploits variational inference to approximate parameter distribution
&lt;/p&gt;</description></item><item><title>Attention Lens&#26159;&#19968;&#31181;&#24037;&#20855;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#23398;&#20064;&#30340;&#27880;&#24847;&#21147;&#22836;&#29305;&#23450;&#36716;&#25442;&#23558;&#27880;&#24847;&#21147;&#22836;&#30340;&#36755;&#20986;&#32763;&#35793;&#20026;&#35789;&#27719;&#26631;&#35760;&#12290;&#20351;&#29992;Attention Lens&#65292;&#25105;&#20204;&#21487;&#20197;&#35299;&#37322;&#27880;&#24847;&#21147;&#22836;&#22312;&#29983;&#25104;&#26368;&#32456;&#26631;&#35760;&#39044;&#27979;&#20013;&#30340;&#20316;&#29992;&#12290;&#27880;&#24847;&#21147;&#22836;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#25198;&#28436;&#30528;&#39640;&#24230;&#19987;&#38376;&#21270;&#30340;&#35282;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.16270</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#38236;&#22836;&#65306;&#19968;&#31181;&#35299;&#37322;&#27880;&#24847;&#21147;&#22836;&#20449;&#24687;&#26816;&#32034;&#26426;&#21046;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism. (arXiv:2310.16270v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16270
&lt;/p&gt;
&lt;p&gt;
Attention Lens&#26159;&#19968;&#31181;&#24037;&#20855;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#23398;&#20064;&#30340;&#27880;&#24847;&#21147;&#22836;&#29305;&#23450;&#36716;&#25442;&#23558;&#27880;&#24847;&#21147;&#22836;&#30340;&#36755;&#20986;&#32763;&#35793;&#20026;&#35789;&#27719;&#26631;&#35760;&#12290;&#20351;&#29992;Attention Lens&#65292;&#25105;&#20204;&#21487;&#20197;&#35299;&#37322;&#27880;&#24847;&#21147;&#22836;&#22312;&#29983;&#25104;&#26368;&#32456;&#26631;&#35760;&#39044;&#27979;&#20013;&#30340;&#20316;&#29992;&#12290;&#27880;&#24847;&#21147;&#22836;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#25198;&#28436;&#30528;&#39640;&#24230;&#19987;&#38376;&#21270;&#30340;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;&#32447;&#24615;&#23618;&#30340;&#20316;&#29992;&#65292;&#35299;&#30721;LLMs&#20026;&#25991;&#26412;&#23436;&#25104;&#20219;&#21153;&#20570;&#20986;&#26368;&#32456;&#39044;&#27979;&#30340;&#20869;&#37096;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#27880;&#24847;&#21147;&#22836;&#22312;&#29983;&#25104;&#26368;&#32456;&#26631;&#35760;&#39044;&#27979;&#20013;&#30340;&#20855;&#20307;&#20316;&#29992;&#36824;&#30693;&#20043;&#29978;&#23569;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Attention Lens&#65292;&#19968;&#20010;&#24037;&#20855;&#65292;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#30340;&#27880;&#24847;&#21147;&#22836;&#29305;&#23450;&#36716;&#25442;(&#31216;&#20026;&#38236;&#22836;)&#23558;&#27880;&#24847;&#21147;&#22836;&#30340;&#36755;&#20986;&#32763;&#35793;&#20026;&#35789;&#27719;&#26631;&#35760;&#12290;&#25105;&#20204;&#35757;&#32451;&#30340;&#38236;&#22836;&#30340;&#21021;&#27493;&#21457;&#29616;&#34920;&#26126;&#65292;&#27880;&#24847;&#21147;&#22836;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#25198;&#28436;&#30528;&#39640;&#24230;&#19987;&#38376;&#21270;&#30340;&#35282;&#33394;&#12290;Attention Lens&#30340;&#20195;&#30721;&#21487;&#22312;github.com/msakarvadia/AttentionLens&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Large Language Models (LLMs) are the state-of-the-art for natural language tasks. Recent work has attempted to decode, by reverse engineering the role of linear layers, the internal mechanisms by which LLMs arrive at their final predictions for text completion tasks. Yet little is known about the specific role of attention heads in producing the final token prediction. We propose Attention Lens, a tool that enables researchers to translate the outputs of attention heads into vocabulary tokens via learned attention-head-specific transformations called lenses. Preliminary findings from our trained lenses indicate that attention heads play highly specialized roles in language models. The code for Attention Lens is available at github.com/msakarvadia/AttentionLens.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#35299;&#32544;&#31163;&#25955;&#22810;&#31890;&#24230;&#22270;&#20998;&#31867;&#26041;&#27861;&#65288;CDM-GNN&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#22270;&#25968;&#25454;&#30340;&#22810;&#31890;&#24230;&#29305;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#20013;&#37325;&#35201;&#23376;&#32467;&#26500;&#21644;&#20559;&#24046;&#37096;&#20998;&#30340;&#35299;&#26512;&#65292;&#24182;&#29992;&#20110;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2310.16256</link><description>&lt;p&gt;
&#19968;&#31181;&#22240;&#26524;&#35299;&#32544;&#31163;&#25955;&#22810;&#31890;&#24230;&#22270;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Causal Disentangled Multi-Granularity Graph Classification Method. (arXiv:2310.16256v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16256
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#35299;&#32544;&#31163;&#25955;&#22810;&#31890;&#24230;&#22270;&#20998;&#31867;&#26041;&#27861;&#65288;CDM-GNN&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#22270;&#25968;&#25454;&#30340;&#22810;&#31890;&#24230;&#29305;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#20013;&#37325;&#35201;&#23376;&#32467;&#26500;&#21644;&#20559;&#24046;&#37096;&#20998;&#30340;&#35299;&#26512;&#65292;&#24182;&#29992;&#20110;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#29983;&#27963;&#20013;&#24191;&#27867;&#23384;&#22312;&#22823;&#37327;&#25968;&#25454;&#21644;&#22797;&#26434;&#32467;&#26500;&#30340;&#22270;&#25968;&#25454;&#12290;&#23558;&#22270;&#25968;&#25454;&#26144;&#23556;&#21040;&#20302;&#32500;&#23884;&#20837;&#31354;&#38388;&#26159;&#24517;&#35201;&#30340;&#12290;&#22270;&#20998;&#31867;&#26159;&#19968;&#39033;&#20851;&#38190;&#30340;&#22270;&#20219;&#21153;&#65292;&#20027;&#35201;&#20381;&#36182;&#20110;&#35782;&#21035;&#22270;&#20013;&#30340;&#37325;&#35201;&#23376;&#32467;&#26500;&#12290;&#30446;&#21069;&#65292;&#19968;&#20123;&#22270;&#20998;&#31867;&#26041;&#27861;&#27809;&#26377;&#32467;&#21512;&#22270;&#25968;&#25454;&#30340;&#22810;&#31890;&#24230;&#29305;&#24615;&#12290;&#24314;&#27169;&#20013;&#32570;&#20047;&#31890;&#24230;&#21306;&#20998;&#23548;&#33268;&#27169;&#22411;&#20013;&#20851;&#38190;&#20449;&#24687;&#21644;&#34394;&#20551;&#30456;&#20851;&#24615;&#28151;&#28102;&#12290;&#22240;&#27492;&#65292;&#23454;&#29616;&#21487;&#20449;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#30340;&#30446;&#26631;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#35299;&#32544;&#31163;&#25955;&#22810;&#31890;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65288;CDM-GNN&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;CDM-GNN&#27169;&#22411;&#20174;&#22810;&#31890;&#24230;&#30340;&#35282;&#24230;&#23545;&#22270;&#20013;&#30340;&#37325;&#35201;&#23376;&#32467;&#26500;&#21644;&#20559;&#24046;&#37096;&#20998;&#36827;&#34892;&#35299;&#32544;&#31163;&#12290;CDM-GNN&#27169;&#22411;&#30340;&#35299;&#32544;&#31163;&#25581;&#31034;&#20102;&#37325;&#35201;&#21644;&#20559;&#24046;&#37096;&#20998;&#65292;&#20026;&#20854;&#20998;&#31867;&#20219;&#21153;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph data widely exists in real life, with large amounts of data and complex structures. It is necessary to map graph data to low-dimensional embedding. Graph classification, a critical graph task, mainly relies on identifying the important substructures within the graph. At present, some graph classification methods do not combine the multi-granularity characteristics of graph data. This lack of granularity distinction in modeling leads to a conflation of key information and false correlations within the model. So, achieving the desired goal of a credible and interpretable model becomes challenging. This paper proposes a causal disentangled multi-granularity graph representation learning method (CDM-GNN) to solve this challenge. The CDM-GNN model disentangles the important substructures and bias parts within the graph from a multi-granularity perspective. The disentanglement of the CDM-GNN model reveals important and bias parts, forming the foundation for its classification task, spe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20855;&#26377;&#22122;&#22768;&#30340;&#20004;&#20154;&#38646;&#21644;&#30697;&#38453;&#28216;&#25103;&#20013;&#32431;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#36817;&#26368;&#20248;&#31639;&#27861;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#19979;&#30028;&#30456;&#21305;&#37197;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#20915;&#26007;&#36172;&#21330;&#20013;&#30340;&#32431;&#25506;&#32034;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.16252</link><description>&lt;p&gt;
Matrix Games&#20013;&#30340;&#36817;&#26368;&#20248;&#32431;&#25506;&#32034;&#65306;&#38543;&#26426;&#36172;&#24466;&#19982;&#20915;&#26007;&#36172;&#24466;&#30340;&#25512;&#24191;
&lt;/p&gt;
&lt;p&gt;
Near-Optimal Pure Exploration in Matrix Games: A Generalization of Stochastic Bandits &amp; Dueling Bandits. (arXiv:2310.16252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20855;&#26377;&#22122;&#22768;&#30340;&#20004;&#20154;&#38646;&#21644;&#30697;&#38453;&#28216;&#25103;&#20013;&#32431;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#36817;&#26368;&#20248;&#31639;&#27861;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#19979;&#30028;&#30456;&#21305;&#37197;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#20915;&#26007;&#36172;&#21330;&#20013;&#30340;&#32431;&#25506;&#32034;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#20855;&#26377;&#22122;&#22768;&#30340;&#20004;&#20154;&#38646;&#21644;&#30697;&#38453;&#28216;&#25103;&#20013;&#65292;&#32431;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#65288;PSNE&#65289;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#22312;&#32473;&#23450;&#30340;&#38543;&#26426;&#27169;&#22411;&#20013;&#65292;&#20219;&#20309;&#23398;&#20064;&#22120;&#21487;&#20197;&#23545;&#36755;&#20837;&#30697;&#38453;A&#30340;&#26576;&#20010;&#20803;&#32032;&#65288;i&#65292;j&#65289;&#36827;&#34892;&#37319;&#26679;&#65292;&#24182;&#35266;&#23519;&#21040;A_{i&#65292;j} + \eta&#30340;&#20540;&#65292;&#20854;&#20013;\eta&#26159;&#19968;&#20010;&#38646;&#22343;&#20540;&#30340;1-&#23376;&#39640;&#26031;&#22122;&#22768;&#12290;&#23398;&#20064;&#22120;&#30340;&#30446;&#26631;&#26159;&#22312;&#23613;&#21487;&#33021;&#23569;&#30340;&#37319;&#26679;&#27425;&#25968;&#19979;&#65292;&#20197;&#39640;&#27010;&#29575;&#30830;&#23450;A&#30340;PSNE&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36817;&#26368;&#20248;&#31639;&#27861;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#19979;&#30028;&#30456;&#21305;&#37197;&#65292;&#21482;&#26377;&#23545;&#25968;&#22240;&#23376;&#30340;&#24046;&#36317;&#12290;&#30830;&#23450;PSNE&#30340;&#38382;&#39064;&#20063;&#25512;&#24191;&#20102;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#20915;&#26007;&#36172;&#24466;&#20013;&#30340;&#32431;&#25506;&#32034;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#36825;&#20004;&#20010;&#35774;&#32622;&#20013;&#19982;&#26368;&#20248;&#30028;&#38480;&#30456;&#21305;&#37197;&#65292;&#21482;&#26377;&#23545;&#25968;&#22240;&#23376;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the sample complexity of identifying the pure strategy Nash equilibrium (PSNE) in a two-player zero-sum matrix game with noise. Formally, we are given a stochastic model where any learner can sample an entry $(i,j)$ of the input matrix $A\in[-1,1]^{n\times m}$ and observe $A_{i,j}+\eta$ where $\eta$ is a zero-mean 1-sub-Gaussian noise. The aim of the learner is to identify the PSNE of $A$, whenever it exists, with high probability while taking as few samples as possible. Zhou et al. (2017) presents an instance-dependent sample complexity lower bound that depends only on the entries in the row and column in which the PSNE lies. We design a near-optimal algorithm whose sample complexity matches the lower bound, up to log factors. The problem of identifying the PSNE also generalizes the problem of pure exploration in stochastic multi-armed bandits and dueling bandits, and our result matches the optimal bounds, up to log factors, in both the settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ZzzGPT&#30340;&#20132;&#20114;&#24335;GPT&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#30561;&#30496;&#36136;&#37327;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#21644;&#21453;&#39304;&#65292;&#35813;&#26041;&#27861;&#34701;&#21512;&#20102;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#29992;&#25143;&#23548;&#21521;&#35774;&#35745;&#65292;&#20197;&#25552;&#20379;&#20934;&#30830;&#21644;&#26377;&#20215;&#20540;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.16242</link><description>&lt;p&gt;
ZzzGPT: &#25552;&#39640;&#30561;&#30496;&#36136;&#37327;&#30340;&#20132;&#20114;&#24335;GPT&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ZzzGPT: An Interactive GPT Approach to Enhance Sleep Quality. (arXiv:2310.16242v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ZzzGPT&#30340;&#20132;&#20114;&#24335;GPT&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#30561;&#30496;&#36136;&#37327;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#21644;&#21453;&#39304;&#65292;&#35813;&#26041;&#27861;&#34701;&#21512;&#20102;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#29992;&#25143;&#23548;&#21521;&#35774;&#35745;&#65292;&#20197;&#25552;&#20379;&#20934;&#30830;&#21644;&#26377;&#20215;&#20540;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#19990;&#30028;&#20013;&#65292;&#30561;&#30496;&#36136;&#37327;&#23545;&#24635;&#20307;&#20581;&#24247;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25552;&#20379;&#23454;&#26102;&#30417;&#27979;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#32570;&#20047;&#26377;&#38024;&#23545;&#24615;&#30340;&#35265;&#35299;&#65292;&#23548;&#33268;&#29992;&#25143;&#25918;&#24323;&#20351;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25216;&#26415;&#22312;&#29702;&#35299;&#30561;&#30496;&#27169;&#24335;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs)&#65292;&#26088;&#22312;&#25552;&#20379;&#20934;&#30830;&#30340;&#30561;&#30496;&#39044;&#27979;&#21644;&#26377;&#20215;&#20540;&#30340;&#21453;&#39304;&#12290;&#21033;&#29992;GLOBEM&#25968;&#25454;&#38598;&#21644;LLMs&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;XGBoost&#31561;&#27169;&#22411;&#30456;&#27604;&#30340;&#22686;&#24378;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#19982;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#35774;&#35745;&#30456;&#32467;&#21512;&#65292;&#23558;&#31185;&#23398;&#20934;&#30830;&#24615;&#19982;&#23454;&#29992;&#24615;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's world, sleep quality is pivotal for overall well-being. While wearable sensors offer real-time monitoring, they often lack actionable insights, leading to user abandonment. This paper delves into the role of technology in understanding sleep patterns. We introduce a two-stage framework, utilizing Large Language Models (LLMs), aiming to provide accurate sleep predictions with actionable feedback. Leveraging the GLOBEM dataset and synthetic data from LLMs, we highlight enhanced results with models like XGBoost. Our approach merges advanced machine learning with user-centric design, blending scientific accuracy with practicality.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20219;&#21153;&#20998;&#32452;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#20219;&#21153;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20146;&#21644;&#21147;&#20197;&#21450;&#20219;&#21153;&#30340;&#22266;&#26377;&#29305;&#24449;&#21644;&#21333;&#20219;&#21153;&#23398;&#20064;&#29305;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#20219;&#21153;&#30340;&#33258;&#21160;&#20998;&#32452;&#12290;</title><link>http://arxiv.org/abs/2310.16241</link><description>&lt;p&gt;
&#36890;&#36807;&#20219;&#21153;&#20146;&#21644;&#21147;&#39044;&#27979;&#23454;&#29616;&#33258;&#21160;&#21270;&#22810;&#20219;&#21153;&#26426;&#22120;&#23398;&#20064;&#30340;&#20219;&#21153;&#20998;&#32452;
&lt;/p&gt;
&lt;p&gt;
Task Grouping for Automated Multi-Task Machine Learning via Task Affinity Prediction. (arXiv:2310.16241v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20219;&#21153;&#20998;&#32452;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#20219;&#21153;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20146;&#21644;&#21147;&#20197;&#21450;&#20219;&#21153;&#30340;&#22266;&#26377;&#29305;&#24449;&#21644;&#21333;&#20219;&#21153;&#23398;&#20064;&#29305;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#20219;&#21153;&#30340;&#33258;&#21160;&#20998;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#38656;&#35201;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#30456;&#20284;&#20219;&#21153;&#26102;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#27169;&#22411;&#21487;&#20197;&#27604;&#21333;&#20219;&#21153;&#23398;&#20064;&#65288;STL&#65289;&#27169;&#22411;&#36798;&#21040;&#26174;&#33879;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;MTL&#30340;&#20248;&#21183;&#21462;&#20915;&#20110;&#21508;&#31181;&#22240;&#32032;&#65292;&#22914;&#20219;&#21153;&#30340;&#30456;&#20284;&#24615;&#12289;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#31561;&#65307;&#23454;&#38469;&#19978;&#65292;&#19968;&#20123;&#20219;&#21153;&#21487;&#33021;&#24182;&#19981;&#36866;&#21512;MTL&#65292;&#29978;&#33267;&#19982;STL&#30456;&#27604;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#38382;&#39064;&#23601;&#20986;&#29616;&#20102;&#65306;&#24212;&#35813;&#23558;&#21738;&#20123;&#20219;&#21153;&#19968;&#36215;&#23398;&#20064;&#65311;&#39046;&#22495;&#19987;&#23478;&#21487;&#20197;&#26681;&#25454;&#30452;&#35273;&#12289;&#32463;&#39564;&#21644;&#26368;&#20339;&#23454;&#36341;&#26469;&#23581;&#35797;&#23558;&#20219;&#21153;&#20998;&#32452;&#22312;&#19968;&#36215;&#65292;&#20294;&#25163;&#21160;&#20998;&#32452;&#21487;&#33021;&#26082;&#32791;&#26102;&#21448;&#36828;&#38750;&#26368;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#20219;&#21153;&#20998;&#32452;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#22235;&#20010;&#22312;MTL&#25991;&#29486;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#30740;&#31350;&#20102;&#20219;&#21153;&#23545;MTL&#30340;&#20146;&#21644;&#21147;&#65292;&#37325;&#28857;&#26159;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;MTL&#27169;&#22411;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20219;&#21153;&#30340;&#22266;&#26377;&#29305;&#24449;&#21644;STL&#29305;&#24615;&#65292;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#39044;&#27979;&#19968;&#20010;&#20219;&#21153;&#32452;&#26159;&#21542;&#36866;&#21512;&#36827;&#34892;MTL&#12290;
&lt;/p&gt;
&lt;p&gt;
When a number of similar tasks have to be learned simultaneously, multi-task learning (MTL) models can attain significantly higher accuracy than single-task learning (STL) models. However, the advantage of MTL depends on various factors, such as the similarity of the tasks, the sizes of the datasets, and so on; in fact, some tasks might not benefit from MTL and may even incur a loss of accuracy compared to STL. Hence, the question arises: which tasks should be learned together? Domain experts can attempt to group tasks together following intuition, experience, and best practices, but manual grouping can be labor-intensive and far from optimal. In this paper, we propose a novel automated approach for task grouping. First, we study the affinity of tasks for MTL using four benchmark datasets that have been used extensively in the MTL literature, focusing on neural network-based MTL models. We identify inherent task features and STL characteristics that can help us to predict whether a gro
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#38598;&#25104;&#27744;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#21152;&#26435;&#24179;&#22343;&#20540;&#26469;&#26435;&#34913;&#19981;&#21516;&#39044;&#27979;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#24182;&#22312;&#20004;&#20010;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#38598;&#25104;&#27744;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.16231</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#38598;&#25104;&#27744;&#21270;&#26041;&#27861;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Attention-Based Ensemble Pooling for Time Series Forecasting. (arXiv:2310.16231v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16231
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#38598;&#25104;&#27744;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#21152;&#26435;&#24179;&#22343;&#20540;&#26469;&#26435;&#34913;&#19981;&#21516;&#39044;&#27979;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#24182;&#22312;&#20004;&#20010;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#38598;&#25104;&#27744;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#65292;&#20943;&#23569;&#27169;&#22411;&#20559;&#24046;&#30340;&#24120;&#35265;&#25216;&#26415;&#26159;&#20351;&#29992;&#22810;&#20010;&#39044;&#27979;&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;&#24182;&#23558;&#20854;&#36755;&#20986;&#27719;&#24635;&#20026;&#19968;&#20010;&#38598;&#25104;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22312;&#27599;&#20010;&#39044;&#27979;&#27169;&#22411;&#20855;&#26377;&#19981;&#21516;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#22312;&#27719;&#24635;&#20013;&#26435;&#34913;&#27599;&#20010;&#27169;&#22411;&#30340;&#39044;&#27979;&#24182;&#19981;&#24635;&#26159;&#28165;&#26224;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#27744;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#38598;&#25104;&#27744;&#21270;&#27169;&#22411;&#23398;&#20064;&#39044;&#27979;&#20505;&#36873;&#27169;&#22411;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#20004;&#20010;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#65306;&#38750;&#24179;&#31283;&#30340;Lorenz `63&#26041;&#31243;&#30340;&#22810;&#27493;&#39044;&#27979;&#21644;COVID-19&#27599;&#21608;&#21457;&#29983;&#30340;&#27515;&#20129;&#20107;&#20214;&#30340;&#21333;&#27493;&#39044;&#27979;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#39044;&#27979;&#38750;&#24179;&#31283;&#30340;Lorenz `63&#26041;&#31243;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#39044;&#27979;COVID-19&#27599;&#21608;&#21457;&#29983;&#30340;&#27515;&#20129;&#20107;&#20214;&#26102;&#65292;&#24182;&#19981;&#19968;&#30452;&#27604;&#29616;&#26377;&#30340;&#38598;&#25104;&#27744;&#21270;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common technique to reduce model bias in time-series forecasting is to use an ensemble of predictive models and pool their output into an ensemble forecast. In cases where each predictive model has different biases, however, it is not always clear exactly how each model forecast should be weighed during this pooling. We propose a method for pooling that performs a weighted average over candidate model forecasts, where the weights are learned by an attention-based ensemble pooling model. We test this method on two time-series forecasting problems: multi-step forecasting of the dynamics of the non-stationary Lorenz `63 equation, and one-step forecasting of the weekly incident deaths due to COVID-19. We find that while our model achieves excellent valid times when forecasting the non-stationary Lorenz `63 equation, it does not consistently perform better than the existing ensemble pooling when forecasting COVID-19 weekly incident deaths.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24555;&#36895;&#23398;&#20064;&#30340;&#22522;&#30784;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#23545;&#21738;&#20123;&#29305;&#24449;&#26356;&#20559;&#22909;&#65292;&#21363;&#21487;&#39044;&#27979;&#24615;&#21644;&#21487;&#29992;&#24615;&#22914;&#20309;&#30456;&#20114;&#24433;&#21709;&#27169;&#22411;&#30340;&#29305;&#24449;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.16228</link><description>&lt;p&gt;
&#20851;&#20110;&#24555;&#36895;&#23398;&#20064;&#30340;&#22522;&#30784;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Foundations of Shortcut Learning. (arXiv:2310.16228v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16228
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24555;&#36895;&#23398;&#20064;&#30340;&#22522;&#30784;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#23545;&#21738;&#20123;&#29305;&#24449;&#26356;&#20559;&#22909;&#65292;&#21363;&#21487;&#39044;&#27979;&#24615;&#21644;&#21487;&#29992;&#24615;&#22914;&#20309;&#30456;&#20114;&#24433;&#21709;&#27169;&#22411;&#30340;&#29305;&#24449;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#20016;&#23500;&#30340;&#29305;&#24449;&#12290;&#27169;&#22411;&#20351;&#29992;&#21738;&#20123;&#29305;&#24449;&#19981;&#20165;&#21462;&#20915;&#20110;&#39044;&#27979;&#33021;&#21147; - &#19968;&#20010;&#29305;&#24449;&#21487;&#38752;&#22320;&#25351;&#31034;&#35757;&#32451;&#38598;&#26631;&#31614;&#30340;&#31243;&#24230;&#65292;&#36824;&#21462;&#20915;&#20110;&#21487;&#29992;&#24615; - &#19968;&#20010;&#29305;&#24449;&#21487;&#20197;&#20174;&#36755;&#20837;&#20013;&#34987;&#36731;&#26494;&#25552;&#21462;&#25110;&#21033;&#29992;&#30340;&#31243;&#24230;&#12290;&#26377;&#20851;&#24555;&#36895;&#23398;&#20064;&#30340;&#25991;&#29486;&#24050;&#32463;&#25351;&#20986;&#20102;&#27169;&#22411;&#20559;&#22909;&#19968;&#20010;&#29305;&#24449;&#32780;&#19981;&#26159;&#21478;&#19968;&#20010;&#29305;&#24449;&#30340;&#20363;&#23376;&#65292;&#20363;&#22914;&#22312;&#32441;&#29702;&#21644;&#24418;&#29366;&#20043;&#38388;&#20197;&#21450;&#22312;&#22270;&#20687;&#32972;&#26223;&#21644;&#21069;&#26223;&#23545;&#35937;&#20043;&#38388;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#27979;&#35797;&#20851;&#20110;&#21738;&#20123;&#36755;&#20837;&#23646;&#24615;&#23545;&#20110;&#27169;&#22411;&#26356;&#23481;&#26131;&#33719;&#21462;&#30340;&#20551;&#35774;&#65292;&#24182;&#31995;&#32479;&#22320;&#30740;&#31350;&#39044;&#27979;&#33021;&#21147;&#21644;&#21487;&#29992;&#24615;&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#26469;&#22609;&#36896;&#27169;&#22411;&#30340;&#29305;&#24449;&#20351;&#29992;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26368;&#23567;&#30340;&#12289;&#26126;&#30830;&#30340;&#29983;&#25104;&#26694;&#26550;&#26469;&#21512;&#25104;&#20855;&#26377;&#20004;&#20010;&#28508;&#22312;&#29305;&#24449;&#30340;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#36825;&#20004;&#20010;&#29305;&#24449;&#22312;&#39044;&#27979;&#33021;&#21147;&#21644;&#25105;&#20204;&#20551;&#35774;&#19982;&#21487;&#29992;&#24615;&#26377;&#20851;&#30340;&#22240;&#32032;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#24182;&#37327;&#21270;&#20102;&#27169;&#22411;&#30340;&#24555;&#25463;&#20559;&#24046; - &#23427;&#36807;&#24230;&#20381;&#36182;&#24555;&#25463;&#65288;&#26356;&#21487;&#29992;&#12289;&#19981;&#22826;&#39044;&#27979;&#65289;&#29305;&#24449;&#32780;&#24573;&#35270;&#20102;&#26680;&#24515;&#65288;&#19981;&#22826;&#21487;&#29992;)&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep-learning models can extract a rich assortment of features from data. Which features a model uses depends not only on predictivity-how reliably a feature indicates train-set labels-but also on availability-how easily the feature can be extracted, or leveraged, from inputs. The literature on shortcut learning has noted examples in which models privilege one feature over another, for example texture over shape and image backgrounds over foreground objects. Here, we test hypotheses about which input properties are more available to a model, and systematically study how predictivity and availability interact to shape models' feature use. We construct a minimal, explicit generative framework for synthesizing classification datasets with two latent features that vary in predictivity and in factors we hypothesize to relate to availability, and quantify a model's shortcut bias-its over-reliance on the shortcut (more available, less predictive) feature at the expense of the core (less avail
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#26102;&#38388;&#36830;&#32493; (TiC) &#22522;&#20934;&#65292;&#20351;&#29992;&#36825;&#20123;&#22522;&#20934;&#35780;&#20272;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#26102;&#38388;&#40065;&#26834;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#25490;&#32451;&#26041;&#27861;&#26469;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.16226</link><description>&lt;p&gt;
TiC-CLIP: CLIP&#27169;&#22411;&#30340;&#25345;&#32493;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
TiC-CLIP: Continual Training of CLIP Models. (arXiv:2310.16226v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16226
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#26102;&#38388;&#36830;&#32493; (TiC) &#22522;&#20934;&#65292;&#20351;&#29992;&#36825;&#20123;&#22522;&#20934;&#35780;&#20272;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#26102;&#38388;&#40065;&#26834;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#25490;&#32451;&#26041;&#27861;&#26469;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25345;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#19982;&#26368;&#26032;&#25968;&#25454;&#20445;&#25345;&#21516;&#27493;&#26412;&#36523;&#23601;&#26159;&#26114;&#36149;&#30340;&#12290;&#20026;&#20102;&#36991;&#20813;&#19981;&#26029;&#37325;&#26032;&#35757;&#32451;&#30340;&#39640;&#25104;&#26412;&#65292;&#25345;&#32493;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#32570;&#20047;&#22823;&#35268;&#27169;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#25110;&#22522;&#32447;&#25152;&#21152;&#21095;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#31532;&#19968;&#25209; Web &#35268;&#27169;&#26102;&#38388;&#36830;&#32493;&#65288;TiC&#65289;&#22522;&#20934;&#65306;TiC-DataCompt&#12289;TiC-YFCC &#21644; TiC-RedCaps&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807; 127 &#20159;&#20010;&#26102;&#38388;&#25139;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65292;&#36328;&#36234;&#20102; 9 &#24180;&#30340;&#26102;&#38388;&#65288;2014-2022&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#36825;&#20123;&#22522;&#20934;&#26469;&#31574;&#21010;&#21508;&#31181;&#21160;&#24577;&#35780;&#20272;&#65292;&#20197;&#34913;&#37327;&#29616;&#26377;&#27169;&#22411;&#30340;&#26102;&#38388;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; OpenAI &#30340; CLIP &#27169;&#22411;&#65288;&#20351;&#29992; 2020 &#24180;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65289;&#22312;&#25105;&#20204;&#31574;&#21010;&#30340;&#20174; 2021 &#24180;&#21040; 2022 &#24180;&#30340;&#26816;&#32034;&#20219;&#21153;&#20013;&#65292;&#22833;&#21435;&#20102;&#32422; 8% &#30340;&#38646;-shot&#20934;&#30830;&#29575;&#65292;&#32780;&#19982; OpenCLIP &#23384;&#20648;&#24211;&#20013;&#26368;&#36817;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#39640;&#25928;&#22320;&#23545;&#26102;&#38388;&#36830;&#32493;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25490;&#32451;&#26041;&#27861;&#65292;&#20174;&#19978;&#27425;&#30340;&#35757;&#32451;&#20013;&#32487;&#32493;&#35757;&#32451;&#65292;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keeping large foundation models up to date on latest data is inherently expensive. To avoid the prohibitive costs of constantly retraining, it is imperative to continually train these models. This problem is exacerbated by the lack of any large scale continual learning benchmarks or baselines. We introduce the first set of web-scale Time-Continual (TiC) benchmarks for training vision-language models: TiC-DataCompt, TiC-YFCC, and TiC-RedCaps with over 12.7B timestamped image-text pairs spanning 9 years (2014--2022). We first use our benchmarks to curate various dynamic evaluations to measure temporal robustness of existing models. We show OpenAI's CLIP (trained on data up to 2020) loses $\approx 8\%$ zero-shot accuracy on our curated retrieval task from 2021--2022 compared with more recently trained models in OpenCLIP repository. We then study how to efficiently train models on time-continuous data. We demonstrate that a simple rehearsal-based approach that continues training from the l
&lt;/p&gt;</description></item><item><title>CleanCoNLL&#26159;&#19968;&#31181;&#20960;&#20046;&#26080;&#22122;&#22768;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20840;&#38754;&#37325;&#26631;&#35760;&#21644;&#33258;&#21160;&#19968;&#33268;&#24615;&#26816;&#26597;&#26469;&#32416;&#27491;CoNLL-03&#20013;&#30340;&#27880;&#37322;&#38169;&#35823;&#65292;&#25552;&#39640;&#20102;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;F1&#20998;&#25968;&#65292;&#24182;&#20943;&#23569;&#20102;&#22240;&#27880;&#37322;&#32570;&#22833;&#32780;&#35823;&#21028;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2310.16225</link><description>&lt;p&gt;
CleanCoNLL: &#19968;&#31181;&#20960;&#20046;&#26080;&#22122;&#22768;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CleanCoNLL: A Nearly Noise-Free Named Entity Recognition Dataset. (arXiv:2310.16225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16225
&lt;/p&gt;
&lt;p&gt;
CleanCoNLL&#26159;&#19968;&#31181;&#20960;&#20046;&#26080;&#22122;&#22768;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20840;&#38754;&#37325;&#26631;&#35760;&#21644;&#33258;&#21160;&#19968;&#33268;&#24615;&#26816;&#26597;&#26469;&#32416;&#27491;CoNLL-03&#20013;&#30340;&#27880;&#37322;&#38169;&#35823;&#65292;&#25552;&#39640;&#20102;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;F1&#20998;&#25968;&#65292;&#24182;&#20943;&#23569;&#20102;&#22240;&#27880;&#37322;&#32570;&#22833;&#32780;&#35823;&#21028;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CoNLL-03&#35821;&#26009;&#24211;&#34987;&#35748;&#20026;&#26159;&#26368;&#33879;&#21517;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#22823;&#37327;&#30340;&#27880;&#37322;&#38169;&#35823;&#12289;&#19981;&#23436;&#25972;&#24615;&#21644;&#25968;&#25454;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#32473;&#23458;&#35266;&#27604;&#36739;NER&#26041;&#27861;&#21644;&#20998;&#26512;&#20854;&#38169;&#35823;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;CoNLL-03&#20013;&#36798;&#21040;&#30340;F1&#20998;&#25968;&#19982;&#20272;&#35745;&#30340;&#22122;&#22768;&#27700;&#24179;&#30456;&#24403;&#29978;&#33267;&#26356;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#19968;&#33268;&#24615;&#26816;&#26597;&#36741;&#21161;&#30340;&#20840;&#38754;&#37325;&#26631;&#35760;&#24037;&#20316;&#26469;&#32416;&#27491;&#33521;&#25991;CoNLL-03&#20013;&#25152;&#26377;&#26631;&#31614;&#30340;7.0&#65285;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#20026;&#20102;&#26356;&#22909;&#22320;&#35299;&#37322;NER&#26631;&#31614;&#21644;&#20316;&#20026;&#38468;&#21152;&#20445;&#35777;&#27880;&#37322;&#36136;&#37327;&#32780;&#28155;&#21152;&#20102;&#19968;&#20010;&#23454;&#20307;&#38142;&#25509;&#27880;&#37322;&#23618;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#19978;&#36798;&#21040;&#20102;&#26174;&#33879;&#26356;&#39640;&#30340;F1&#20998;&#25968;&#65288;97.1&#65285;&#65289;&#65292;&#32780;&#19988;&#20851;&#38190;&#26159;&#27491;&#30830;&#39044;&#27979;&#34987;&#38169;&#35823;&#22320;&#35745;&#31639;&#20026;&#38169;&#35823;&#30340;&#27604;&#20363;&#30001;&#20110;&#27880;&#37322;&#30340;&#32570;&#22833;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
The CoNLL-03 corpus is arguably the most well-known and utilized benchmark dataset for named entity recognition (NER). However, prior works found significant numbers of annotation errors, incompleteness, and inconsistencies in the data. This poses challenges to objectively comparing NER approaches and analyzing their errors, as current state-of-the-art models achieve F1-scores that are comparable to or even exceed the estimated noise level in CoNLL-03. To address this issue, we present a comprehensive relabeling effort assisted by automatic consistency checking that corrects 7.0% of all labels in the English CoNLL-03. Our effort adds a layer of entity linking annotation both for better explainability of NER labels and as additional safeguard of annotation quality. Our experimental evaluation finds not only that state-of-the-art approaches reach significantly higher F1-scores (97.1%) on our data, but crucially that the share of correct predictions falsely counted as errors due to annota
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#19981;&#21487;&#30693;&#30340;&#26694;&#26550;DIVA&#65292;&#36890;&#36807;&#20998;&#26512;&#28508;&#22312;&#30340;&#27602;&#23475;&#25968;&#25454;&#38598;&#26469;&#26816;&#27979;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36973;&#21463;&#30340;&#25915;&#20987;&#12290;&#36890;&#36807;&#27604;&#36739;&#27169;&#22411;&#22312;&#21463;&#21040;&#27602;&#23475;&#21644;&#24178;&#20928;&#25968;&#25454;&#19978;&#30340;&#20934;&#30830;&#24615;&#65292;DIVA&#33021;&#22815;&#39044;&#27979;&#26410;&#30693;&#30340;&#24178;&#20928;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#19968;&#33324;&#27602;&#23475;&#25915;&#20987;&#30340;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.16224</link><description>&lt;p&gt;
&#27602;&#33647;&#19981;&#26159;&#26080;&#30165;&#30340;&#65306;&#20840;&#38754;&#19981;&#21487;&#30693;&#30340;&#26816;&#27979;&#27602;&#23475;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Poison is Not Traceless: Fully-Agnostic Detection of Poisoning Attacks. (arXiv:2310.16224v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16224
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#19981;&#21487;&#30693;&#30340;&#26694;&#26550;DIVA&#65292;&#36890;&#36807;&#20998;&#26512;&#28508;&#22312;&#30340;&#27602;&#23475;&#25968;&#25454;&#38598;&#26469;&#26816;&#27979;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36973;&#21463;&#30340;&#25915;&#20987;&#12290;&#36890;&#36807;&#27604;&#36739;&#27169;&#22411;&#22312;&#21463;&#21040;&#27602;&#23475;&#21644;&#24178;&#20928;&#25968;&#25454;&#19978;&#30340;&#20934;&#30830;&#24615;&#65292;DIVA&#33021;&#22815;&#39044;&#27979;&#26410;&#30693;&#30340;&#24178;&#20928;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#19968;&#33324;&#27602;&#23475;&#25915;&#20987;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#24213;&#23618;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#24694;&#24847;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27602;&#21270;&#35757;&#32451;&#25968;&#25454;&#26469;&#25915;&#20987;&#27169;&#22411;&#12290;&#24403;&#21069;&#30340;&#26816;&#27979;&#22120;&#38024;&#23545;&#29305;&#23450;&#25968;&#25454;&#31867;&#22411;&#12289;&#27169;&#22411;&#25110;&#25915;&#20987;&#31867;&#22411;&#65292;&#22240;&#27492;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#20855;&#26377;&#26377;&#38480;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#38754;&#19981;&#21487;&#30693;&#26694;&#26550;DIVA&#65288;&#26816;&#27979;&#19981;&#21487;&#35265;&#25915;&#20987;&#65289;&#65292;&#20165;&#36890;&#36807;&#20998;&#26512;&#28508;&#22312;&#27602;&#23475;&#25968;&#25454;&#38598;&#26469;&#26816;&#27979;&#25915;&#20987;&#12290;DIVA&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#24605;&#24819;&#65292;&#21363;&#36890;&#36807;&#27604;&#36739;&#20998;&#31867;&#22120;&#22312;&#21463;&#21040;&#27602;&#23475;&#21644;&#24178;&#20928;&#25968;&#25454;&#19978;&#30340;&#20934;&#30830;&#24615;&#26469;&#26816;&#27979;&#27602;&#23475;&#25915;&#20987;&#65292;&#24182;&#20351;&#29992;&#22797;&#26434;&#24615;&#24230;&#37327;&#39044;&#35757;&#32451;&#20803;&#23398;&#20064;&#22120;&#26469;&#20272;&#35745;&#22312;&#20551;&#35774;&#30340;&#24178;&#20928;&#25968;&#25454;&#38598;&#19978;&#30340;&#26410;&#30693;&#20934;&#30830;&#24615;&#12290;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#19968;&#33324;&#30340;&#27602;&#23475;&#25915;&#20987;&#12290;&#20026;&#20102;&#35780;&#20272;&#30446;&#30340;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#19978;&#27979;&#35797;&#20102;DIVA&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of machine learning models depends on the quality of the underlying data. Malicious actors can attack the model by poisoning the training data. Current detectors are tied to either specific data types, models, or attacks, and therefore have limited applicability in real-world scenarios. This paper presents a novel fully-agnostic framework, DIVA (Detecting InVisible Attacks), that detects attacks solely relying on analyzing the potentially poisoned data set. DIVA is based on the idea that poisoning attacks can be detected by comparing the classifier's accuracy on poisoned and clean data and pre-trains a meta-learner using Complexity Measures to estimate the otherwise unknown accuracy on a hypothetical clean dataset. The framework applies to generic poisoning attacks. For evaluation purposes, in this paper, we test DIVA on label-flipping attacks.
&lt;/p&gt;</description></item><item><title>&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#22312;&#22797;&#26434;&#25968;&#25454;&#19978;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21482;&#22312;&#19968;&#20010;&#23545;&#35937;&#30340;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#65292;&#20197;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#21644;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16221</link><description>&lt;p&gt;
&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Randomized Smoothing. (arXiv:2310.16221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16221
&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#22312;&#22797;&#26434;&#25968;&#25454;&#19978;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21482;&#22312;&#19968;&#20010;&#23545;&#35937;&#30340;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#65292;&#20197;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#21644;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#26159;&#22797;&#26434;&#30340;&#65292;&#36890;&#24120;&#30001;&#21487;&#20998;&#35299;&#20026;&#22810;&#20010;&#23454;&#20307;&#30340;&#23545;&#35937;&#32452;&#25104;&#65288;&#20363;&#22914;&#65292;&#23558;&#22270;&#20687;&#20998;&#35299;&#20026;&#20687;&#32032;&#65292;&#23558;&#22270;&#24418;&#20998;&#35299;&#20026;&#30456;&#20114;&#36830;&#25509;&#30340;&#33410;&#28857;&#65289;&#12290;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#22312;&#20854;&#36755;&#20837;&#30340;&#24494;&#23567;&#21464;&#21270;&#19978;&#20855;&#26377;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;-&#36890;&#36807;&#22312;&#20998;&#31867;&#20043;&#21069;&#38543;&#26426;&#28155;&#21152;&#22122;&#22768;&#26469;&#20445;&#35777;&#22810;&#25968;&#25237;&#31080;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#23545;&#25163;&#19981;&#26159;&#20219;&#24847;&#24178;&#25200;&#25972;&#20010;&#23545;&#35937;&#65288;&#20363;&#22914;&#22270;&#20687;&#65289;&#65292;&#32780;&#26159;&#23545;&#35937;&#30340;&#26576;&#20010;&#23454;&#20307;&#30340;&#23376;&#38598;&#65288;&#20363;&#22914;&#20687;&#32032;&#65289;&#26102;&#65292;&#36890;&#36807;&#38543;&#26426;&#24179;&#28369;&#23545;&#36825;&#31181;&#22797;&#26434;&#25968;&#25454;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#65306;&#25105;&#20204;&#36890;&#36807;&#20165;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#23454;&#20307;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#26469;&#37096;&#20998;&#24179;&#28369;&#23545;&#35937;&#12290;&#36890;&#36807;&#20197;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#28155;&#21152;&#22122;&#22768;&#65292;&#25105;&#20204;&#33719;&#24471;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#22122;&#22768;&#20998;&#24067;&#21021;&#22987;&#21270;&#20998;&#23618;&#24179;&#28369;&#65292;&#24471;&#21040;&#20102;&#26032;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). Randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs - by guaranteeing robustness of the majority vote when randomly adding noise before classification. Yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). As a solution, we introduce hierarchical randomized smoothing: We partially smooth objects by adding random noise only on a randomly selected subset of their entities. By adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. We initialize hierarchical smoothing using different noising distributions, yielding novel robustness
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#26512;&#27169;&#22411;&#21644;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;GPU&#23884;&#20837;&#24335;&#31995;&#32479;&#20248;&#21270;&#26041;&#27861;&#65292;&#38024;&#23545;&#24182;&#34892;&#21069;&#32512;&#25805;&#20316;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#65292;&#24182;&#25552;&#20379;&#20102;&#24615;&#33021;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2310.16214</link><description>&lt;p&gt;
GPU&#23884;&#20837;&#24335;&#31995;&#32479;&#30340;&#24615;&#33021;&#20248;&#21270;&#65306;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#20998;&#26512;&#27169;&#22411;&#30340;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Performance Tuning for GPU-Embedded Systems: Machine-Learning-based and Analytical Model-driven Tuning Methodologies. (arXiv:2310.16214v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#26512;&#27169;&#22411;&#21644;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;GPU&#23884;&#20837;&#24335;&#31995;&#32479;&#20248;&#21270;&#26041;&#27861;&#65292;&#38024;&#23545;&#24182;&#34892;&#21069;&#32512;&#25805;&#20316;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#65292;&#24182;&#25552;&#20379;&#20102;&#24615;&#33021;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPU&#23884;&#20837;&#24335;&#31995;&#32479;&#22240;&#20854;&#39640;&#25928;&#30340;&#33021;&#32791;&#32780;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#21464;&#24471;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#28385;&#36275;&#36825;&#20123;&#31995;&#32479;&#36816;&#34892;&#23454;&#26102;&#25110;&#32791;&#26102;&#24212;&#29992;&#31243;&#24207;&#30340;&#38656;&#27714;&#65292;&#23558;&#20854;&#35843;&#25972;&#20197;&#23637;&#29616;&#20986;&#39640;&#24615;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#24320;&#21457;&#21644;&#27604;&#36739;&#20004;&#31181;&#22312;GPU&#23884;&#20837;&#24335;&#31995;&#32479;&#19978;&#30340;&#20248;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#20026;&#23547;&#27714;&#20248;&#21270;&#36816;&#34892;&#22312;&#36825;&#20123;&#26550;&#26500;&#19978;&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#24320;&#21457;&#20154;&#21592;&#21644;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#24615;&#33021;&#27934;&#23519;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#24182;&#34892;&#21069;&#32512;&#25805;&#20316;&#65292;&#22914;FFT&#12289;&#25195;&#25551;&#21407;&#35821;&#21644;&#19977;&#23545;&#35282;&#31995;&#32479;&#27714;&#35299;&#22120;&#65292;&#36825;&#20123;&#26159;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#24615;&#33021;&#20851;&#38190;&#30340;&#32452;&#20214;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#26512;&#27169;&#22411;&#30340;&#20248;&#21270;&#26041;&#27861;&#21644;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20004;&#31181;&#20248;&#21270;&#26041;&#27861;&#22312;NVIDIA Jetson&#31995;&#32479;&#20013;BPLG&#24211;&#30340;&#19981;&#21516;&#24182;&#34892;&#21069;&#32512;&#23454;&#29616;&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#20854;&#19982;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPU-embedded systems have gained popularity across various domains due to their efficient power consumption. However, in order to meet the demands of real-time or time-consuming applications running on these systems, it is crucial for them to be tuned to exhibit high performance. This paper addresses the issue by developing and comparing two tuning methodologies on GPU-embedded systems, and also provides performance insights for developers and researchers seeking to optimize applications running on these architectures. We focus on parallel prefix operations, such as FFT, scan primitives, and tridiagonal system solvers, which are performance-critical components in many applications. The study introduces an analytical model-driven tuning methodology and a Machine Learning (ML)-based tuning methodology. We evaluate the performance of the two tuning methodologies for different parallel prefix implementations of the BPLG library in an NVIDIA Jetson system, and compare their performance to t
&lt;/p&gt;</description></item><item><title>ELM&#23725;&#22238;&#24402;&#22686;&#24378;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;ELM&#30340;&#20998;&#31867;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16209</link><description>&lt;p&gt;
ELM&#23725;&#22238;&#24402;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
ELM Ridge Regression Boosting. (arXiv:2310.16209v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16209
&lt;/p&gt;
&lt;p&gt;
ELM&#23725;&#22238;&#24402;&#22686;&#24378;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;ELM&#30340;&#20998;&#31867;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#31181;&#23545;&#23725;&#22238;&#24402;&#65288;RR&#65289;&#26041;&#27861;&#36827;&#34892;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#26497;&#38480;&#23398;&#20064;&#26426;&#65288;ELM&#65289;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;ELM&#30340;&#20998;&#31867;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We discuss a boosting approach for the Ridge Regression (RR) method, with applications to the Extreme Learning Machine (ELM), and we show that the proposed method significantly improves the classification performance and robustness of ELMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20302;&#31209;&#33258;&#32534;&#30721;&#22120;&#65288;LoRAE&#65289;&#65292;&#36890;&#36807;&#21152;&#20837;&#20302;&#31209;&#27491;&#21017;&#21270;&#39033;&#65292;&#33258;&#36866;&#24212;&#22320;&#37325;&#26500;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#33258;&#32534;&#30721;&#22120;&#30340;&#22522;&#26412;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#29983;&#25104;&#21644;&#19979;&#28216;&#20998;&#31867;&#31561;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16194</link><description>&lt;p&gt;
&#23398;&#20064;&#20302;&#31209;&#28508;&#31354;&#38388;&#30340;&#31616;&#21333;&#30830;&#23450;&#24615;&#33258;&#32534;&#30721;&#22120;&#65306;&#29702;&#35770;&#21644;&#32463;&#39564;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Learning Low-Rank Latent Spaces with Simple Deterministic Autoencoder: Theoretical and Empirical Insights. (arXiv:2310.16194v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20302;&#31209;&#33258;&#32534;&#30721;&#22120;&#65288;LoRAE&#65289;&#65292;&#36890;&#36807;&#21152;&#20837;&#20302;&#31209;&#27491;&#21017;&#21270;&#39033;&#65292;&#33258;&#36866;&#24212;&#22320;&#37325;&#26500;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#33258;&#32534;&#30721;&#22120;&#30340;&#22522;&#26412;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#29983;&#25104;&#21644;&#19979;&#28216;&#20998;&#31867;&#31561;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#32534;&#30721;&#22120;&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#37325;&#26500;&#25439;&#22833;&#26469;&#21019;&#24314;&#25968;&#25454;&#30340;&#32039;&#20945;&#28508;&#22312;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#23427;&#24448;&#24448;&#24573;&#35270;&#20102;&#22823;&#22810;&#25968;&#25968;&#25454;&#65288;&#22270;&#20687;&#65289;&#34987;&#23884;&#20837;&#22312;&#19968;&#20010;&#20302;&#32500;&#31354;&#38388;&#20013;&#30340;&#20107;&#23454;&#65292;&#36825;&#23545;&#20110;&#26377;&#25928;&#30340;&#25968;&#25454;&#34920;&#31034;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20302;&#31209;&#33258;&#32534;&#30721;&#22120;&#65288;LoRAE&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;LoRAE&#20013;&#65292;&#25105;&#20204;&#21152;&#20837;&#20102;&#19968;&#20010;&#20302;&#31209;&#27491;&#21017;&#21270;&#39033;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#37325;&#26500;&#19968;&#20010;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#33258;&#32534;&#30721;&#22120;&#30340;&#22522;&#26412;&#30446;&#26631;&#12290;&#36825;&#26377;&#21161;&#20110;&#23558;&#25968;&#25454;&#23884;&#20837;&#21040;&#19968;&#20010;&#20302;&#32500;&#31354;&#38388;&#20013;&#65292;&#21516;&#26102;&#20445;&#30041;&#37325;&#35201;&#20449;&#24687;&#12290;&#23427;&#26159;&#19968;&#20010;&#23398;&#20064;&#20302;&#31209;&#28508;&#31354;&#38388;&#30340;&#31616;&#21333;&#33258;&#32534;&#30721;&#22120;&#25193;&#23637;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#27169;&#22411;&#24314;&#31435;&#20102;&#19968;&#20010;&#26356;&#32039;&#23494;&#30340;&#35823;&#24046;&#30028;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#21644;&#19979;&#28216;&#20998;&#31867;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#12290;&#29702;&#35770;&#21644;&#23454;&#36341;&#32467;&#26524;&#37117;&#24378;&#35843;&#20102;&#33719;&#21462;&#20302;&#31209;&#28508;&#31354;&#38388;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The autoencoder is an unsupervised learning paradigm that aims to create a compact latent representation of data by minimizing the reconstruction loss. However, it tends to overlook the fact that most data (images) are embedded in a lower-dimensional space, which is crucial for effective data representation. To address this limitation, we propose a novel approach called Low-Rank Autoencoder (LoRAE). In LoRAE, we incorporated a low-rank regularizer to adaptively reconstruct a low-dimensional latent space while preserving the basic objective of an autoencoder. This helps embed the data in a lower-dimensional space while preserving important information. It is a simple autoencoder extension that learns low-rank latent space. Theoretically, we establish a tighter error bound for our model. Empirically, our model's superiority shines through various tasks such as image generation and downstream classification. Both theoretical and practical outcomes highlight the importance of acquiring low
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#25968;&#25454;&#21516;&#21270;&#26041;&#26696;&#65288;VIVID&#65289;&#65292;&#32467;&#21512;&#20102;Voronoi-tessellation&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#65292;&#29992;&#20110;&#22788;&#29702;&#31232;&#30095;&#12289;&#38750;&#32467;&#26500;&#21270;&#21644;&#26102;&#21464;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#25968;&#25454;&#21516;&#21270;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2310.16187</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#28145;&#24230;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#22312;&#31232;&#30095;&#35266;&#27979;&#21644;&#26102;&#21464;&#20256;&#24863;&#22120;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient deep data assimilation with sparse observations and time-varying sensors. (arXiv:2310.16187v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#25968;&#25454;&#21516;&#21270;&#26041;&#26696;&#65288;VIVID&#65289;&#65292;&#32467;&#21512;&#20102;Voronoi-tessellation&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#65292;&#29992;&#20110;&#22788;&#29702;&#31232;&#30095;&#12289;&#38750;&#32467;&#26500;&#21270;&#21644;&#26102;&#21464;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#25968;&#25454;&#21516;&#21270;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#25968;&#25454;&#21516;&#21270;&#65288;DA&#65289;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#31243;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#23545;&#22810;&#28304;&#22122;&#22768;&#25968;&#25454;&#30340;&#21152;&#26435;&#32452;&#21512;&#26469;&#36827;&#34892;&#22330;&#26223;&#37325;&#24314;&#21644;&#39044;&#27979;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#25216;&#26415;&#22312;&#25968;&#25454;&#21516;&#21270;&#20013;&#30340;&#24212;&#29992;&#26174;&#31034;&#20986;&#22312;&#39640;&#32500;&#21160;&#21147;&#31995;&#32479;&#20013;&#25552;&#39640;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#22312;&#22788;&#29702;&#38750;&#32467;&#26500;&#21270;&#35266;&#27979;&#25968;&#25454;&#26102;&#38754;&#20020;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#24403;&#20256;&#24863;&#22120;&#30340;&#20301;&#32622;&#21644;&#25968;&#37327;&#38543;&#26102;&#38388;&#21160;&#24577;&#21464;&#21270;&#26102;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#25968;&#25454;&#21516;&#21270;&#26041;&#26696;&#65292;&#21517;&#20026;Voronoi-tessellation Inverse operator for VariatIonal Data assimilation&#65288;VIVID&#65289;&#65292;&#23427;&#23558;DL&#36870;&#31639;&#23376;&#32435;&#20837;&#21516;&#21270;&#30446;&#26631;&#20989;&#25968;&#20013;&#12290;&#36890;&#36807;&#21033;&#29992;Voronoi-tessellation&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#65292;VIVID&#33021;&#22815;&#22788;&#29702;&#31232;&#30095;&#12289;&#38750;&#32467;&#26500;&#21270;&#21644;&#26102;&#21464;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;DL&#36870;&#31639;&#23376;&#30340;&#24341;&#20837;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#25968;&#25454;&#21516;&#21270;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational Data Assimilation (DA) has been broadly used in engineering problems for field reconstruction and prediction by performing a weighted combination of multiple sources of noisy data. In recent years, the integration of deep learning (DL) techniques in DA has shown promise in improving the efficiency and accuracy in high-dimensional dynamical systems. Nevertheless, existing deep DA approaches face difficulties in dealing with unstructured observation data, especially when the placement and number of sensors are dynamic over time. We introduce a novel variational DA scheme, named Voronoi-tessellation Inverse operator for VariatIonal Data assimilation (VIVID), that incorporates a DL inverse operator into the assimilation objective function. By leveraging the capabilities of the Voronoi-tessellation and convolutional neural networks, VIVID is adept at handling sparse, unstructured, and time-varying sensor data. Furthermore, the incorporation of the DL inverse operator establishes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;U-Net&#26550;&#26500;&#36827;&#34892;&#31881;&#26411;X&#23556;&#32447;&#34893;&#23556;&#22270;&#20687;&#30340;&#22270;&#20687;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#21644;&#25490;&#38500;&#23454;&#39564;X&#23556;&#32447;&#34893;&#23556;&#22270;&#20687;&#20013;&#30340;&#20266;&#24433;&#65292;&#24182;&#22312;&#21484;&#22238;&#29575;&#21644;&#20551;&#38451;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.16186</link><description>&lt;p&gt;
&#20351;&#29992;U-Net&#26550;&#26500;&#36827;&#34892;&#31881;&#26411;X&#23556;&#32447;&#34893;&#23556;&#22270;&#20687;&#30340;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Image Segmentation using U-Net Architecture for Powder X-ray Diffraction Images. (arXiv:2310.16186v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;U-Net&#26550;&#26500;&#36827;&#34892;&#31881;&#26411;X&#23556;&#32447;&#34893;&#23556;&#22270;&#20687;&#30340;&#22270;&#20687;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#21644;&#25490;&#38500;&#23454;&#39564;X&#23556;&#32447;&#34893;&#23556;&#22270;&#20687;&#20013;&#30340;&#20266;&#24433;&#65292;&#24182;&#22312;&#21484;&#22238;&#29575;&#21644;&#20551;&#38451;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#20351;&#29992;&#21407;&#20301;&#21516;&#27493;&#36752;&#23556;&#39640;&#33021;&#31881;&#26411;X&#23556;&#32447;&#34893;&#23556;&#65288;XRD&#65289;&#25216;&#26415;&#26469;&#30740;&#31350;&#21151;&#33021;&#22120;&#20214;&#20013;&#26448;&#26009;&#30340;&#26230;&#20307;&#32467;&#26500;&#65292;&#27604;&#22914;&#21487;&#20805;&#30005;&#30005;&#27744;&#26448;&#26009;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#23454;&#39564;X&#23556;&#32447;&#34893;&#23556;&#22270;&#20687;&#20013;&#20266;&#24433;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22914;&#21487;&#35843;&#33410;&#30340;U-Net&#26469;&#35782;&#21035;&#20266;&#24433;&#12290;&#29305;&#21035;&#26159;&#65292;&#20351;&#29992;&#25972;&#20307;&#30495;&#38451;&#24615;&#29575;&#25110;&#21484;&#22238;&#29575;&#23545;&#39044;&#27979;&#30340;&#20266;&#24433;&#19982;&#30456;&#24212;&#30340;&#22522;&#20934;&#65288;&#25163;&#21160;&#23454;&#29616;&#65289;&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#65292;U-Net&#33021;&#22815;&#22987;&#32456;&#20135;&#29983;92.4%&#30340;&#24456;&#22909;&#30340;&#21484;&#22238;&#29575;&#34920;&#29616;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#24179;&#22343;&#20551;&#38451;&#24615;&#38477;&#20302;&#20102;34%&#12290;U-Net&#36824;&#23558;&#35782;&#21035;&#21644;&#20998;&#31163;&#20266;&#24433;&#25152;&#38656;&#30340;&#26102;&#38388;&#32553;&#30701;&#20102;50%&#20197;&#19978;&#12290;&#27492;&#22806;&#65292;&#25490;&#38500;&#20266;&#24433;&#26174;&#31034;&#20986;&#20027;&#35201;&#30340;&#21019;&#26032;&#28857;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific researchers frequently use the in situ synchrotron high-energy powder X-ray diffraction (XRD) technique to examine the crystallographic structures of materials in functional devices such as rechargeable battery materials. We propose a method for identifying artifacts in experimental XRD images. The proposed method uses deep learning convolutional neural network architectures, such as tunable U-Nets to identify the artifacts. In particular, the predicted artifacts are evaluated against the corresponding ground truth (manually implemented) using the overall true positive rate or recall. The result demonstrates that the U-Nets can consistently produce great recall performance at 92.4% on the test dataset, which is not included in the training, with a 34% reduction in average false positives in comparison to the conventional method. The U-Nets also reduce the time required to identify and separate artifacts by more than 50%. Furthermore, the exclusion of the artifacts shows majo
&lt;/p&gt;</description></item><item><title>BLP 2023&#20219;&#21153;2&#26159;&#20851;&#20110;&#24773;&#24863;&#20998;&#26512;&#30340;&#20849;&#20139;&#20219;&#21153;&#65292;&#21560;&#24341;&#20102;71&#20010;&#21442;&#19982;&#32773;&#12290;&#21442;&#19982;&#32773;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20132;&#20102;597&#20010;&#36816;&#34892;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20219;&#21153;&#30340;&#35814;&#32454;&#35774;&#32622;&#21644;&#21442;&#19982;&#32773;&#25552;&#20132;&#31995;&#32479;&#30340;&#27010;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.16183</link><description>&lt;p&gt;
BLP 2023&#20219;&#21153;2&#65306;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
BLP 2023 Task 2: Sentiment Analysis. (arXiv:2310.16183v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16183
&lt;/p&gt;
&lt;p&gt;
BLP 2023&#20219;&#21153;2&#26159;&#20851;&#20110;&#24773;&#24863;&#20998;&#26512;&#30340;&#20849;&#20139;&#20219;&#21153;&#65292;&#21560;&#24341;&#20102;71&#20010;&#21442;&#19982;&#32773;&#12290;&#21442;&#19982;&#32773;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20132;&#20102;597&#20010;&#36816;&#34892;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20219;&#21153;&#30340;&#35814;&#32454;&#35774;&#32622;&#21644;&#21442;&#19982;&#32773;&#25552;&#20132;&#31995;&#32479;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24635;&#32467;&#20102;&#20316;&#20026;BLP 2023&#21019;&#26032;&#24037;&#20316;&#22346;&#30340;&#19968;&#37096;&#20998;&#20030;&#21150;&#30340;BLP&#24773;&#24863;&#20849;&#20139;&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#30340;&#23450;&#20041;&#26159;&#22312;&#32473;&#23450;&#30340;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#26816;&#27979;&#24773;&#24863;&#12290;&#35813;&#20219;&#21153;&#21560;&#24341;&#20102;71&#20010;&#21442;&#19982;&#32773;&#30340;&#20851;&#27880;&#65292;&#20854;&#20013;&#22312;&#24320;&#21457;&#21644;&#35780;&#20272;&#38454;&#27573;&#20998;&#21035;&#26377;29&#20010;&#21644;30&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;&#31995;&#32479;&#12290;&#24635;&#20849;&#65292;&#21442;&#19982;&#32773;&#25552;&#20132;&#20102;597&#20010;&#36816;&#34892;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24635;&#20849;&#26377;15&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;&#31995;&#32479;&#25551;&#36848;&#35770;&#25991;&#12290;&#25552;&#20132;&#30340;&#31995;&#32479;&#28085;&#30422;&#20102;&#20174;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12289;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21040;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#35813;&#20219;&#21153;&#30340;&#35774;&#32622;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#31616;&#35201;&#27010;&#36848;&#20102;&#21442;&#19982;&#32773;&#25552;&#20132;&#30340;&#31995;&#32479;&#12290;&#20849;&#20139;&#20219;&#21153;&#30340;&#25152;&#26377;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#33050;&#26412;&#24050;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an overview of the BLP Sentiment Shared Task, organized as part of the inaugural BLP 2023 workshop, co-located with EMNLP 2023. The task is defined as the detection of sentiment in a given piece of social media text. This task attracted interest from 71 participants, among whom 29 and 30 teams submitted systems during the development and evaluation phases, respectively. In total, participants submitted 597 runs. However, a total of 15 teams submitted system description papers. The range of approaches in the submitted systems spans from classical machine learning models, fine-tuning pre-trained models, to leveraging Large Language Model (LLMs) in zero- and few-shot settings. In this paper, we provide a detailed account of the task setup, including dataset development and evaluation setup. Additionally, we provide a brief overview of the systems submitted by the participants. All datasets and evaluation scripts from the shared task have been made publicly available for the res
&lt;/p&gt;</description></item><item><title>G-CASCADE&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#30340;&#35299;&#30721;&#22120;&#65292;&#29992;&#20110;2D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#36890;&#36807;&#26377;&#25928;&#30340;&#22270;&#21367;&#31215;&#22359;&#36880;&#28176;&#25913;&#36827;&#20998;&#23618;Transformer&#32534;&#30721;&#22120;&#29983;&#25104;&#30340;&#29305;&#24449;&#22270;&#65292;&#20445;&#30041;&#38271;&#31243;&#20449;&#24687;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.16175</link><description>&lt;p&gt;
G-CASCADE: &#39640;&#25928;&#30340;&#32423;&#32852;&#22270;&#21367;&#31215;&#35299;&#30721;&#22120;&#29992;&#20110;2D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
G-CASCADE: Efficient Cascaded Graph Convolutional Decoding for 2D Medical Image Segmentation. (arXiv:2310.16175v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16175
&lt;/p&gt;
&lt;p&gt;
G-CASCADE&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#30340;&#35299;&#30721;&#22120;&#65292;&#29992;&#20110;2D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#36890;&#36807;&#26377;&#25928;&#30340;&#22270;&#21367;&#31215;&#22359;&#36880;&#28176;&#25913;&#36827;&#20998;&#23618;Transformer&#32534;&#30721;&#22120;&#29983;&#25104;&#30340;&#29305;&#24449;&#22270;&#65292;&#20445;&#30041;&#38271;&#31243;&#20449;&#24687;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#24050;&#25104;&#20026;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#24212;&#29992;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#30340;&#35299;&#30721;&#22120;&#65292;&#21363;&#32423;&#32852;&#22270;&#21367;&#31215;&#27880;&#24847;&#21147;&#35299;&#30721;&#22120;&#65288;G-CASCADE&#65289;&#65292;&#29992;&#20110;2D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;G-CASCADE&#36890;&#36807;&#39640;&#25928;&#30340;&#22270;&#21367;&#31215;&#22359;&#36880;&#28176;&#25913;&#36827;&#30001;&#20998;&#23618;Transformer&#32534;&#30721;&#22120;&#29983;&#25104;&#30340;&#22810;&#38454;&#27573;&#29305;&#24449;&#22270;&#12290;&#32534;&#30721;&#22120;&#21033;&#29992;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#24615;&#65292;&#32780;&#35299;&#30721;&#22120;&#36890;&#36807;&#22270;&#21367;&#31215;&#22359;&#30340;&#20840;&#23616;&#24863;&#21463;&#37326;&#25913;&#21892;&#29305;&#24449;&#22270;&#65292;&#20445;&#30041;&#38271;&#31243;&#20449;&#24687;&#12290;&#23545;&#25105;&#20204;&#30340;&#35299;&#30721;&#22120;&#22312;&#20116;&#20010;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#65288;&#21363;&#33145;&#37096;&#22120;&#23448;&#12289;&#24515;&#33039;&#22120;&#23448;&#12289;&#24687;&#32905;&#30149;&#21464;&#12289;&#30382;&#25439;&#21644;&#35270;&#32593;&#33180;&#34880;&#31649;&#65289;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
In recent years, medical image segmentation has become an important application in the field of computer-aided diagnosis. In this paper, we are the first to propose a new graph convolution-based decoder namely, Cascaded Graph Convolutional Attention Decoder (G-CASCADE), for 2D medical image segmentation. G-CASCADE progressively refines multi-stage feature maps generated by hierarchical transformer encoders with an efficient graph convolution block. The encoder utilizes the self-attention mechanism to capture long-range dependencies, while the decoder refines the feature maps preserving long-range information due to the global receptive fields of the graph convolution block. Rigorous evaluations of our decoder with multiple transformer encoders on five medical image segmentation tasks (i.e., Abdomen organs, Cardiac organs, Polyp lesions, Skin lesions, and Retinal vessels) show that our model outperforms other state-of-the-art (SOTA) methods. We also demonstrate that our decoder achieves
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25552;&#20379;&#20102;&#20851;&#20110;&#23454;&#38469;&#35774;&#32622;&#19979;&#20855;&#26377;epsilon-greedy&#31574;&#30053;&#30340;Deep Q&#32593;&#32476;&#65288;DQN&#65289;&#30340;&#25910;&#25947;&#24615;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.16173</link><description>&lt;p&gt;
&#20851;&#20110;&#20855;&#26377;epsilon-greedy&#25506;&#32034;&#30340;Deep Q&#32593;&#32476;&#30340;&#25910;&#25947;&#24615;&#19982;&#26679;&#26412;&#22797;&#26434;&#24230;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On the Convergence and Sample Complexity Analysis of Deep Q-Networks with $\epsilon$-Greedy Exploration. (arXiv:2310.16173v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20379;&#20102;&#20851;&#20110;&#23454;&#38469;&#35774;&#32622;&#19979;&#20855;&#26377;epsilon-greedy&#31574;&#30053;&#30340;Deep Q&#32593;&#32476;&#65288;DQN&#65289;&#30340;&#25910;&#25947;&#24615;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20379;&#20102;&#23545;Deep Q&#32593;&#32476;&#65288;DQN&#65289;&#20855;&#26377;epsilon-greedy&#25506;&#32034;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#23613;&#31649;DQN&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#23454;&#35777;&#25104;&#23601;&#65292;&#20294;&#20854;&#29702;&#35770;&#25551;&#36848;&#20173;&#28982;&#19981;&#23436;&#21892;&#12290;&#39318;&#20808;&#65292;&#29616;&#26377;&#20998;&#26512;&#20013;&#30340;&#25506;&#32034;&#31574;&#30053;&#35201;&#20040;&#19981;&#20999;&#23454;&#38469;&#65292;&#35201;&#20040;&#34987;&#24573;&#30053;&#12290;&#20854;&#27425;&#65292;&#19982;&#20256;&#32479;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#30456;&#27604;&#65292;DQN&#37319;&#29992;&#30446;&#26631;&#32593;&#32476;&#21644;&#32463;&#39564;&#22238;&#25918;&#26469;&#33719;&#24471;&#35757;&#32451;Q&#32593;&#32476;&#25152;&#20351;&#29992;&#30340;&#22343;&#26041;&#36125;&#23572;&#26364;&#35823;&#24046;&#65288;MSBE&#65289;&#30340;&#26080;&#20559;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DQN&#29702;&#35770;&#20998;&#26512;&#32570;&#20047;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#25110;&#32773;&#36890;&#36807;&#20351;&#29992;&#35745;&#31639;&#37327;&#38750;&#24120;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#35268;&#36991;&#25216;&#26415;&#25361;&#25112;&#65292;&#36825;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#24182;&#19981;&#39640;&#25928;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#20851;&#20110;DQN&#23454;&#38469;&#35774;&#32622;&#30340;&#25910;&#25947;&#24615;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#20854;&#20013;&#21253;&#25324;epsilon-greedy&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#20855;&#26377;&#36882;&#20943;epsilon&#30340;&#36845;&#20195;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a theoretical understanding of Deep Q-Network (DQN) with the $\varepsilon$-greedy exploration in deep reinforcement learning. Despite the tremendous empirical achievement of the DQN, its theoretical characterization remains underexplored. First, the exploration strategy is either impractical or ignored in the existing analysis. Second, in contrast to conventional Q-learning algorithms, the DQN employs the target network and experience replay to acquire an unbiased estimation of the mean-square Bellman error (MSBE) utilized in training the Q-network. However, the existing theoretical analysis of DQNs lacks convergence analysis or bypasses the technical challenges by deploying a significantly overparameterized neural network, which is not computationally efficient. This paper provides the first theoretical convergence and sample complexity analysis of the practical setting of DQNs with $\epsilon$-greedy policy. We prove an iterative procedure with decaying $\epsilon$ 
&lt;/p&gt;</description></item><item><title>Brainchop&#26159;&#19968;&#31181;&#22522;&#20110;&#27983;&#35272;&#22120;&#30340;&#31070;&#32463;&#24433;&#20687;&#24037;&#20855;&#65292;&#37319;&#29992;&#21069;&#31471;&#26426;&#22120;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#25552;&#20379;&#20840;&#38754;&#30340;&#22823;&#33041;&#39044;&#22788;&#29702;&#21644;&#20998;&#21106;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.16162</link><description>&lt;p&gt;
Brainchop:&#19979;&#19968;&#20195;&#22522;&#20110;Web&#30340;&#31070;&#32463;&#24433;&#20687;&#24212;&#29992;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Brainchop: Next Generation Web-Based Neuroimaging Application. (arXiv:2310.16162v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16162
&lt;/p&gt;
&lt;p&gt;
Brainchop&#26159;&#19968;&#31181;&#22522;&#20110;&#27983;&#35272;&#22120;&#30340;&#31070;&#32463;&#24433;&#20687;&#24037;&#20855;&#65292;&#37319;&#29992;&#21069;&#31471;&#26426;&#22120;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#25552;&#20379;&#20840;&#38754;&#30340;&#22823;&#33041;&#39044;&#22788;&#29702;&#21644;&#20998;&#21106;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#22312;&#27983;&#35272;&#22120;&#20013;&#36827;&#34892;&#20307;&#31215;&#22270;&#20687;&#22788;&#29702;&#65292;&#23588;&#20854;&#26159;&#22788;&#29702;&#21307;&#23398;&#25968;&#25454;&#65292;&#19982;&#20256;&#32479;&#30340;&#21518;&#31471;&#24037;&#20855;&#30456;&#27604;&#65292;&#38754;&#20020;&#30528;&#21069;&#25152;&#26410;&#26377;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#28304;&#20110;&#27983;&#35272;&#22120;&#29615;&#22659;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#22914;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#21069;&#31471;&#26426;&#22120;&#23398;&#20064;&#24211;&#30340;&#21487;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#32570;&#20047;&#33021;&#25552;&#20379;&#20840;&#38754;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#25972;&#20010;&#22823;&#33041;&#39044;&#22788;&#29702;&#21644;&#20998;&#21106;&#65292;&#21516;&#26102;&#20445;&#25252;&#31471;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#30340;&#31070;&#32463;&#24433;&#20687;&#21069;&#31471;&#24037;&#20855;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Brainchop&#20316;&#20026;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#27983;&#35272;&#22120;&#30340;&#31070;&#32463;&#24433;&#20687;&#24037;&#20855;&#65292;&#23427;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#20840;&#33041;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#32467;&#26500;&#24615;MRI&#36827;&#34892;&#20307;&#31215;&#20998;&#26512;&#65292;&#32780;&#26080;&#38656;&#25216;&#26415;&#19987;&#19994;&#30693;&#35782;&#25110;&#22797;&#26434;&#30340;&#35774;&#32622;&#31243;&#24207;&#12290;&#38500;&#20102;&#33268;&#21147;&#20110;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#65292;&#36825;&#20010;&#21069;&#31471;&#24037;&#20855;&#36824;&#25552;&#20379;&#20102;&#22810;&#20010;&#29305;&#24615;&#65292;&#21253;&#25324;&#21487;&#25193;&#23637;&#24615;&#12289;&#20302;&#24310;&#36831;&#12289;&#29992;&#25143;&#21451;&#22909;&#30340;&#25805;&#20316;&#30028;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performing volumetric image processing directly within the browser, particularly with medical data, presents unprecedented challenges compared to conventional backend tools. These challenges arise from limitations inherent in browser environments, such as constrained computational resources and the availability of frontend machine learning libraries. Consequently, there is a shortage of neuroimaging frontend tools capable of providing comprehensive end-to-end solutions for whole brain preprocessing and segmentation while preserving end-user data privacy and residency. In light of this context, we introduce Brainchop (this http URL) as a groundbreaking in-browser neuroimaging tool that enables volumetric analysis of structural MRI using pre-trained full-brain deep learning models, all without requiring technical expertise or intricate setup procedures. Beyond its commitment to data privacy, this frontend tool offers multiple features, including scalability, low latency, user-friendly op
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35770;&#35777;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#20998;&#26512;&#20013;&#29305;&#24449;&#24402;&#22240;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#21644;&#26799;&#24230;&#26041;&#27861;&#19982;&#26367;&#20195;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#32771;&#34385;&#29992;&#25143;&#30340;&#32972;&#26223;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24402;&#22240;&#30340;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16157</link><description>&lt;p&gt;
&#22522;&#20110;&#35770;&#35777;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#29305;&#24449;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Context-aware feature attribution through argumentation. (arXiv:2310.16157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35770;&#35777;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#20998;&#26512;&#20013;&#29305;&#24449;&#24402;&#22240;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#21644;&#26799;&#24230;&#26041;&#27861;&#19982;&#26367;&#20195;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#32771;&#34385;&#29992;&#25143;&#30340;&#32972;&#26223;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24402;&#22240;&#30340;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#28041;&#21450;&#30830;&#23450;&#20010;&#21035;&#29305;&#24449;&#25110;&#21464;&#37327;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#36129;&#29486;&#12290;&#36825;&#20010;&#36807;&#31243;&#26377;&#21161;&#20110;&#30830;&#23450;&#39044;&#27979;&#32467;&#26524;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#30340;&#21382;&#21490;&#21487;&#20197;&#36861;&#28335;&#21040;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411; (GAMs)&#65292;&#23427;&#36890;&#36807;&#23558;&#22240;&#21464;&#37327;&#21644;&#33258;&#21464;&#37327;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#32435;&#20837;&#27169;&#22411;&#65292;&#25193;&#23637;&#20102;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#21644;&#26367;&#20195;&#27169;&#22411;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#25581;&#31034;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021; (AI) &#31995;&#32479;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;GAMs &#24448;&#24448;&#33021;&#22815;&#36798;&#21040;&#36739;&#20302;&#30340;&#20934;&#30830;&#24615;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#24456;&#38590;&#35299;&#37322;&#65292;&#26367;&#20195;&#27169;&#22411;&#36890;&#24120;&#23384;&#22312;&#31283;&#23450;&#24615;&#21644;&#20445;&#30495;&#24230;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#26041;&#27861;&#37117;&#27809;&#26377;&#32771;&#34385;&#29992;&#25143;&#30340;&#32972;&#26223;&#65292;&#32780;&#29992;&#25143;&#30340;&#32972;&#26223;&#21487;&#33021;&#20250;&#23545;&#20182;&#20204;&#30340;&#20559;&#22909;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#24182;&#25512;&#36827;&#24403;&#21069;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Feature attribution is a fundamental task in both machine learning and data analysis, which involves determining the contribution of individual features or variables to a model's output. This process helps identify the most important features for predicting an outcome. The history of feature attribution methods can be traced back to General Additive Models (GAMs), which extend linear regression models by incorporating non-linear relationships between dependent and independent variables. In recent years, gradient-based methods and surrogate models have been applied to unravel complex Artificial Intelligence (AI) systems, but these methods have limitations. GAMs tend to achieve lower accuracy, gradient-based methods can be difficult to interpret, and surrogate models often suffer from stability and fidelity issues. Furthermore, most existing methods do not consider users' contexts, which can significantly influence their preferences. To address these limitations and advance the current s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#32467;&#26500;&#19982;&#22788;&#29702;&#25968;&#25454;&#20013;&#22266;&#26377;&#32467;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#26088;&#22312;&#25171;&#30772;&#25152;&#35859;&#30340;&#32500;&#24230;&#35781;&#21650;&#65292;&#24182;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20854;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2310.16154</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#25171;&#30772;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#32500;&#24230;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
Breaking the Curse of Dimensionality in Deep Neural Networks by Learning Invariant Representations. (arXiv:2310.16154v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#32467;&#26500;&#19982;&#22788;&#29702;&#25968;&#25454;&#20013;&#22266;&#26377;&#32467;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#26088;&#22312;&#25171;&#30772;&#25152;&#35859;&#30340;&#32500;&#24230;&#35781;&#21650;&#65292;&#24182;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20854;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65292;&#29305;&#21035;&#26159;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#24050;&#32463;&#32463;&#21382;&#20102;&#20174;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#21040;&#22522;&#20110;&#25968;&#25454;&#23398;&#20064;&#21644;&#36866;&#24212;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#36825;&#23548;&#33268;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36827;&#23637;&#65292;&#36825;&#20027;&#35201;&#24402;&#21151;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#19968;&#31181;&#29305;&#27530;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28145;&#24230;&#23398;&#20064;&#36890;&#36807;&#19968;&#31995;&#21015;&#35745;&#31639;&#23618;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#23398;&#20064;&#30456;&#20851;&#29305;&#24449;&#65292;&#21487;&#20197;&#35828;&#36229;&#36234;&#20102;&#20256;&#32479;&#26041;&#27861;&#12290;&#26412;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#30340;&#32467;&#26500;&#21644;&#22788;&#29702;&#30340;&#25968;&#25454;&#20013;&#22266;&#26377;&#32467;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#38382;&#65306;&#26159;&#20160;&#20040;&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#26377;&#25928;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#25112;&#32988;&#25152;&#35859;&#30340;&#32500;&#24230;&#35781;&#21650;&#8212;&#8212;&#21363;&#30001;&#20110;&#32500;&#24230;&#22686;&#21152;&#23548;&#33268;&#23545;&#25968;&#25454;&#28857;&#30340;&#25351;&#25968;&#32423;&#38656;&#27714;&#22686;&#21152;&#65292;&#20174;&#32780;&#22312;&#39640;&#32500;&#24230;&#20013;&#36890;&#24120;&#23398;&#20064;&#20989;&#25968;&#21464;&#24471;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence, particularly the subfield of machine learning, has seen a paradigm shift towards data-driven models that learn from and adapt to data. This has resulted in unprecedented advancements in various domains such as natural language processing and computer vision, largely attributed to deep learning, a special class of machine learning models. Deep learning arguably surpasses traditional approaches by learning the relevant features from raw data through a series of computational layers.  This thesis explores the theoretical foundations of deep learning by studying the relationship between the architecture of these models and the inherent structures found within the data they process. In particular, we ask What drives the efficacy of deep learning algorithms and allows them to beat the so-called curse of dimensionality-i.e. the difficulty of generally learning functions in high dimensions due to the exponentially increasing need for data points with increased dimensio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;FLTrojan&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26435;&#37325;&#31713;&#25913;&#65292;&#20174;&#32852;&#37030;&#35821;&#35328;&#27169;&#22411;&#20013;&#27844;&#38706;&#38544;&#31169;&#25935;&#24863;&#29992;&#25143;&#25968;&#25454;&#12290;&#36890;&#36807;&#35266;&#23519;&#21040;FL&#20013;&#20013;&#38388;&#36718;&#27425;&#30340;&#27169;&#22411;&#24555;&#29031;&#21487;&#20197;&#24341;&#36215;&#26356;&#22823;&#30340;&#38544;&#31169;&#27844;&#38706;&#65292;&#24182;&#21457;&#29616;&#38544;&#31169;&#27844;&#38706;&#21487;&#20197;&#36890;&#36807;&#31713;&#25913;&#27169;&#22411;&#30340;&#36873;&#25321;&#24615;&#26435;&#37325;&#26469;&#21152;&#21095;&#12290;</title><link>http://arxiv.org/abs/2310.16152</link><description>&lt;p&gt;
FLTrojan: &#36890;&#36807;&#36873;&#25321;&#24615;&#26435;&#37325;&#31713;&#25913;&#23545;&#32852;&#37030;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38544;&#31169;&#27844;&#38706;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
FLTrojan: Privacy Leakage Attacks against Federated Language Models Through Selective Weight Tampering. (arXiv:2310.16152v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;FLTrojan&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26435;&#37325;&#31713;&#25913;&#65292;&#20174;&#32852;&#37030;&#35821;&#35328;&#27169;&#22411;&#20013;&#27844;&#38706;&#38544;&#31169;&#25935;&#24863;&#29992;&#25143;&#25968;&#25454;&#12290;&#36890;&#36807;&#35266;&#23519;&#21040;FL&#20013;&#20013;&#38388;&#36718;&#27425;&#30340;&#27169;&#22411;&#24555;&#29031;&#21487;&#20197;&#24341;&#36215;&#26356;&#22823;&#30340;&#38544;&#31169;&#27844;&#38706;&#65292;&#24182;&#21457;&#29616;&#38544;&#31169;&#27844;&#38706;&#21487;&#20197;&#36890;&#36807;&#31713;&#25913;&#27169;&#22411;&#30340;&#36873;&#25321;&#24615;&#26435;&#37325;&#26469;&#21152;&#21095;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(Federated learning, FL)&#27491;&#25104;&#20026;&#35768;&#22810;&#25216;&#26415;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;&#65292;&#21253;&#25324;&#35821;&#35328;&#24314;&#27169;&#39046;&#22495;&#65292;&#20854;&#20013;&#20010;&#20307;FL&#21442;&#19982;&#32773;&#22312;&#20854;&#26412;&#22320;&#25968;&#25454;&#38598;&#20013;&#24448;&#24448;&#20855;&#26377;&#25935;&#24863;&#30340;&#25991;&#26412;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#32852;&#37030;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#27844;&#38706;&#31243;&#24230;&#24182;&#19981;&#31616;&#21333;&#65292;&#29616;&#26377;&#30340;&#25915;&#20987;&#21482;&#26159;&#35797;&#22270;&#25552;&#21462;&#25968;&#25454;&#65292;&#32780;&#19981;&#32771;&#34385;&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#25110;&#22825;&#30495;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20851;&#20110;&#20174;&#32852;&#37030;&#35821;&#35328;&#27169;&#22411;&#20013;&#27844;&#38706;&#38544;&#31169;&#25935;&#24863;&#29992;&#25143;&#25968;&#25454;&#30340;&#20004;&#20010;&#26032;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;FL&#20013;&#20013;&#38388;&#36718;&#27425;&#30340;&#27169;&#22411;&#24555;&#29031;&#27604;&#26368;&#32456;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#36896;&#25104;&#26356;&#22823;&#30340;&#38544;&#31169;&#27844;&#38706;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30830;&#23450;&#38544;&#31169;&#27844;&#38706;&#21487;&#20197;&#36890;&#36807;&#31713;&#25913;&#27169;&#22411;&#30340;&#36873;&#25321;&#24615;&#26435;&#37325;&#26469;&#21152;&#21095;&#65292;&#36825;&#20123;&#26435;&#37325;&#29305;&#21035;&#36127;&#36131;&#35760;&#24518;&#25935;&#24863;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24694;&#24847;&#23458;&#25143;&#31471;&#22914;&#20309;&#22312;FL&#20013;&#27844;&#38706;&#20854;&#20182;&#29992;&#25143;&#30340;&#38544;&#31169;&#25935;&#24863;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is becoming a key component in many technology-based applications including language modeling -- where individual FL participants often have privacy-sensitive text data in their local datasets. However, realizing the extent of privacy leakage in federated language models is not straightforward and the existing attacks only intend to extract data regardless of how sensitive or naive it is. To fill this gap, in this paper, we introduce two novel findings with regard to leaking privacy-sensitive user data from federated language models. Firstly, we make a key observation that model snapshots from the intermediate rounds in FL can cause greater privacy leakage than the final trained model. Secondly, we identify that privacy leakage can be aggravated by tampering with a model's selective weights that are specifically responsible for memorizing the sensitive training data. We show how a malicious client can leak the privacy-sensitive data of some other user in FL even
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#20010;&#24490;&#29615;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#33258;&#25105;&#27880;&#24847;&#22836;&#32039;&#23494;&#27169;&#25311;&#20102;&#35748;&#30693;&#29702;&#35770;&#20013;&#20551;&#35774;&#30340;&#35760;&#24518;&#31995;&#32479;&#65292;&#24182;&#25429;&#25417;&#21040;&#20154;&#31867;&#21477;&#23376;&#22788;&#29702;&#20013;&#30340;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2310.16142</link><description>&lt;p&gt;
&#26377;&#38480;&#35760;&#24518;&#23481;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#25429;&#25417;&#20154;&#31867;&#21477;&#23376;&#22788;&#29702;&#20013;&#30340;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
A Language Model with Limited Memory Capacity Captures Interference in Human Sentence Processing. (arXiv:2310.16142v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16142
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#20010;&#24490;&#29615;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#33258;&#25105;&#27880;&#24847;&#22836;&#32039;&#23494;&#27169;&#25311;&#20102;&#35748;&#30693;&#29702;&#35770;&#20013;&#20551;&#35774;&#30340;&#35760;&#24518;&#31995;&#32479;&#65292;&#24182;&#25429;&#25417;&#21040;&#20154;&#31867;&#21477;&#23376;&#22788;&#29702;&#20013;&#30340;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21477;&#23376;&#22788;&#29702;&#22256;&#38590;&#30340;&#20004;&#20010;&#26680;&#24515;&#22240;&#32032;&#34987;&#35748;&#20026;&#26159;&#26399;&#26395;&#21644;&#26469;&#33258;&#24037;&#20316;&#35760;&#24518;&#30340;&#26816;&#32034;&#12290;&#26368;&#36817;&#30340;&#19968;&#20010;&#23581;&#35797;&#65292;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#32508;&#21512;&#35748;&#30693;&#27169;&#22411;&#65292;&#23558;&#36825;&#20004;&#20010;&#22240;&#32032;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#20381;&#36182;&#20110;transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#21644;&#20154;&#31867;&#21477;&#23376;&#22788;&#29702;&#20013;&#22522;&#20110;&#26263;&#31034;&#30340;&#24037;&#20316;&#35760;&#24518;&#26816;&#32034;&#29702;&#35770;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#65288;Ryu and Lewis, 2021&#65289;.&#34429;&#28982;Ryu&#21644;Lewis&#23637;&#31034;&#20102;GPT-2&#30340;&#29305;&#27530;&#33258;&#27880;&#24847;&#22836;&#20013;&#30340;&#27880;&#24847;&#27169;&#24335;&#19982;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#24178;&#25200;&#30340;&#20851;&#38190;&#39044;&#27979;&#19968;&#33268;&#65292;&#36825;&#26159;&#22522;&#20110;&#26263;&#31034;&#30340;&#26816;&#32034;&#27169;&#22411;&#65292;&#20294;&#20182;&#20204;&#30340;&#26041;&#27861;&#38656;&#35201;&#35782;&#21035;&#20986;&#21477;&#27861;&#29305;&#21270;&#30340;&#33258;&#27880;&#24847;&#22836;&#65292;&#24182;&#20570;&#20986;&#35748;&#30693;&#19978;&#19981;&#21512;&#29702;&#30340;&#20551;&#35774;&#65292;&#21363;&#25968;&#30334;&#27425;&#30340;&#20869;&#23384;&#26816;&#32034;&#25805;&#20316;&#26159;&#24182;&#34892;&#36827;&#34892;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20855;&#26377;&#21333;&#20010;&#33258;&#25105;&#27880;&#24847;&#22836;&#30340;&#24490;&#29615;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65292;&#26356;&#36148;&#36817;&#35748;&#30693;&#29702;&#35770;&#25152;&#20551;&#35774;&#30340;&#35760;&#24518;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Two of the central factors believed to underpin human sentence processing difficulty are expectations and retrieval from working memory. A recent attempt to create a unified cognitive model integrating these two factors relied on the parallels between the self-attention mechanism of transformer language models and cue-based retrieval theories of working memory in human sentence processing (Ryu and Lewis 2021). While Ryu and Lewis show that attention patterns in specialized attention heads of GPT-2 are consistent with similarity-based interference, a key prediction of cue-based retrieval models, their method requires identifying syntactically specialized attention heads, and makes the cognitively implausible assumption that hundreds of memory retrieval operations take place in parallel. In the present work, we develop a recurrent neural language model with a single self-attention head, which more closely parallels the memory system assumed by cognitive theories. We show that our model's
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#28909;&#22330;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#26144;&#23556;&#21644;&#37325;&#24314;&#25216;&#26415;&#65292;&#22312;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#36807;&#31243;&#20013;&#20351;&#29992;&#23569;&#37327;&#20256;&#24863;&#22120;&#23454;&#29616;&#23578;&#26410;&#25171;&#21360;&#37096;&#20214;&#30340;&#28909;&#22330;&#39044;&#27979;&#21644;&#24615;&#33021;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.16125</link><description>&lt;p&gt;
&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#20013;&#34180;&#22721;&#37096;&#20214;&#30340;&#22312;&#32447;&#28909;&#22330;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Online Thermal Field Prediction for Metal Additive Manufacturing of Thin Walls. (arXiv:2310.16125v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#28909;&#22330;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#26144;&#23556;&#21644;&#37325;&#24314;&#25216;&#26415;&#65292;&#22312;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#36807;&#31243;&#20013;&#20351;&#29992;&#23569;&#37327;&#20256;&#24863;&#22120;&#23454;&#29616;&#23578;&#26410;&#25171;&#21360;&#37096;&#20214;&#30340;&#28909;&#22330;&#39044;&#27979;&#21644;&#24615;&#33021;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#20013;&#30340;&#19968;&#20010;&#23454;&#38469;&#38382;&#39064;&#65292;&#21363;&#24403;&#21482;&#26377;&#23569;&#25968;&#20256;&#24863;&#22120;&#21487;&#29992;&#26102;&#65292;&#22914;&#20309;&#22312;&#32447;&#39044;&#27979;&#23578;&#26410;&#25171;&#21360;&#30340;&#38646;&#20214;&#30340;&#28909;&#22330;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26144;&#23556;&#21644;&#37325;&#24314;&#30340;&#22312;&#32447;&#28909;&#22330;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#38598;&#25104;&#21040;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#36807;&#31243;&#20013;&#36827;&#34892;&#22312;&#32447;&#24615;&#33021;&#25511;&#21046;&#12290;&#22522;&#20110;&#28201;&#24230;&#26354;&#32447;&#30340;&#30456;&#20284;&#24615;&#65288;&#19968;&#20010;&#28857;&#30340;&#28201;&#24230;&#26354;&#32447;&#30340;&#26354;&#32447;&#27573;&#65289;&#65292;&#28909;&#22330;&#26144;&#23556;&#24212;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#23578;&#26410;&#25171;&#21360;&#23618;&#19978;&#28857;&#30340;&#28201;&#24230;&#26354;&#32447;&#65292;&#35813;&#26354;&#32447;&#26159;&#30001;&#20043;&#21069;&#25171;&#21360;&#23618;&#19978;&#26576;&#20123;&#28857;&#30340;&#27979;&#37327;&#28201;&#24230;&#24471;&#21040;&#12290;&#21033;&#29992;&#21516;&#19968;&#23618;&#19978;&#20960;&#20010;&#28857;&#30340;&#27979;&#37327;/&#39044;&#27979;&#28201;&#24230;&#26354;&#32447;&#65292;&#28909;&#22330;&#37325;&#24314;&#25552;&#20986;&#20102;&#19968;&#20010;&#38477;&#38454;&#27169;&#22411;&#65288;ROM&#65289;&#26469;&#26500;&#24314;&#21516;&#19968;&#23618;&#19978;&#25152;&#26377;&#28857;&#30340;&#28201;&#24230;&#26354;&#32447;&#65292;&#20174;&#32780;&#26500;&#24314;&#25972;&#20010;&#23618;&#30340;&#28201;&#24230;&#22330;&#12290;ROM&#30340;&#35757;&#32451;&#26159;&#36890;&#36807;&#26497;&#38480;&#23398;&#20064;&#26426;&#31639;&#27861;&#36827;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to study a practical issue in metal AM, i.e., how to predict the thermal field of yet-to-print parts online when only a few sensors are available. This work proposes an online thermal field prediction method using mapping and reconstruction, which could be integrated into a metal AM process for online performance control. Based on the similarity of temperature curves (curve segments of a temperature profile of one point), the thermal field mapping applies an artificial neural network to estimate the temperature curves of points on the yet-to-print layer from measured temperatures of certain points on the previously printed layer. With measured/predicted temperature profiles of several points on the same layer, the thermal field reconstruction proposes a reduced order model (ROM) to construct the temperature profiles of all points on the same layer, which could be used to build the temperature field of the entire layer. The training of ROM is performed with an extreme le
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38170;&#23450;&#31354;&#38388;&#20248;&#21270;&#20256;&#36755;&#65288;ASOT&#65289;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#20998;&#24067;&#26144;&#23556;&#21040;&#20849;&#20139;&#30340;&#38170;&#28857;&#31354;&#38388;&#65292;&#23398;&#20064;&#20854;&#28508;&#22312;&#30340;&#20849;&#21516;&#29305;&#24449;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#22810;&#20010;&#20256;&#36755;&#38382;&#39064;&#30340;&#25209;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2310.16123</link><description>&lt;p&gt;
&#38170;&#23450;&#31354;&#38388;&#20248;&#21270;&#20256;&#36755;&#65306;&#21152;&#36895;&#22810;&#20010;&#20256;&#36755;&#38382;&#39064;&#30340;&#25209;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Anchor Space Optimal Transport: Accelerating Batch Processing of Multiple OT Problems. (arXiv:2310.16123v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16123
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38170;&#23450;&#31354;&#38388;&#20248;&#21270;&#20256;&#36755;&#65288;ASOT&#65289;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#20998;&#24067;&#26144;&#23556;&#21040;&#20849;&#20139;&#30340;&#38170;&#28857;&#31354;&#38388;&#65292;&#23398;&#20064;&#20854;&#28508;&#22312;&#30340;&#20849;&#21516;&#29305;&#24449;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#22810;&#20010;&#20256;&#36755;&#38382;&#39064;&#30340;&#25209;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#29702;&#35770;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#27604;&#36739;&#23450;&#20041;&#22312;&#25351;&#23450;&#24230;&#37327;&#31354;&#38388;&#19978;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20026;&#31435;&#26041;&#32423;&#12290;&#23613;&#31649;Sinkhorn&#31639;&#27861;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;OT&#35299;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#22810;&#20010;OT&#38382;&#39064;&#30340;&#35299;&#20173;&#28982;&#32791;&#26102;&#21644;&#21344;&#29992;&#20869;&#23384;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;OT&#35745;&#31639;&#21152;&#36895;&#30340;&#35768;&#22810;&#24037;&#20316;&#36890;&#24120;&#22522;&#20110;&#21333;&#20010;OT&#38382;&#39064;&#30340;&#21069;&#25552;&#65292;&#24573;&#35270;&#20102;&#19968;&#20010;&#23567;&#25209;&#37327;&#20013;&#20998;&#24067;&#30340;&#28508;&#22312;&#20849;&#21516;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32763;&#35793;&#30340;OT&#38382;&#39064;&#65292;&#31216;&#20026;&#38170;&#23450;&#31354;&#38388;&#20248;&#21270;&#20256;&#36755;&#65288;ASOT&#65289;&#38382;&#39064;&#65292;&#19987;&#38376;&#29992;&#20110;&#25209;&#22788;&#29702;&#22810;&#20010;OT&#38382;&#39064;&#30340;&#35299;&#12290;&#23545;&#20110;&#25552;&#20986;&#30340;ASOT&#38382;&#39064;&#65292;&#20998;&#24067;&#23558;&#34987;&#26144;&#23556;&#21040;&#19968;&#20010;&#20849;&#20139;&#30340;&#38170;&#28857;&#31354;&#38388;&#65292;&#35813;&#31354;&#38388;&#23398;&#20064;&#28508;&#22312;&#30340;&#20849;&#21516;&#29305;&#24449;&#65292;&#20174;&#32780;&#24110;&#21161;&#21152;&#36895;OT&#25209;&#22788;&#29702;&#12290;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;ASOT&#65292;Wasserstei
&lt;/p&gt;
&lt;p&gt;
The optimal transport (OT) theory provides an effective way to compare probability distributions on a defined metric space, but it suffers from cubic computational complexity. Although the Sinkhorn's algorithm greatly reduces the computational complexity of OT solutions, the solutions of multiple OT problems are still time-consuming and memory-comsuming in practice. However, many works on the computational acceleration of OT are usually based on the premise of a single OT problem, ignoring the potential common characteristics of the distributions in a mini-batch. Therefore, we propose a translated OT problem designated as the anchor space optimal transport (ASOT) problem, which is specially designed for batch processing of multiple OT problem solutions. For the proposed ASOT problem, the distributions will be mapped into a shared anchor point space, which learns the potential common characteristics and thus help accelerate OT batch processing. Based on the proposed ASOT, the Wasserstei
&lt;/p&gt;</description></item><item><title>&#21482;&#38656;19&#20010;&#21442;&#25968;&#30340;&#24494;&#22411;&#31070;&#32463;&#32593;&#32476;&#22312;&#39030;&#22840;&#20811;&#21943;&#27880;&#30340;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#20110;&#25968;&#20197;&#19975;&#35745;&#21442;&#25968;&#30340;&#36890;&#29992;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16121</link><description>&lt;p&gt;
&#21482;&#38656;19&#20010;&#21442;&#25968;&#65306;&#29992;&#20110;&#31890;&#23376;&#29289;&#29702;&#23398;&#30340;&#24494;&#22411;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
19 Parameters Is All You Need: Tiny Neural Networks for Particle Physics. (arXiv:2310.16121v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16121
&lt;/p&gt;
&lt;p&gt;
&#21482;&#38656;19&#20010;&#21442;&#25968;&#30340;&#24494;&#22411;&#31070;&#32463;&#32593;&#32476;&#22312;&#39030;&#22840;&#20811;&#21943;&#27880;&#30340;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#20110;&#25968;&#20197;&#19975;&#35745;&#21442;&#25968;&#30340;&#36890;&#29992;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31890;&#23376;&#21152;&#36895;&#22120;&#22686;&#21152;&#30896;&#25758;&#36895;&#29575;&#21644;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#24471;&#21040;&#35777;&#23454;&#65292;&#23545;&#20110;&#20302;&#24310;&#36831;&#20219;&#21153;&#65288;&#22914;&#35302;&#21457;&#22120;&#65289;&#65292;&#38656;&#35201;&#36731;&#37327;&#19988;&#24555;&#36895;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#36817;&#30340;&#27931;&#20262;&#20857;&#21644;&#32622;&#25442;&#23545;&#31216;&#26550;&#26500;PELICAN&#30340;&#28508;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26550;&#26500;&#30340;&#20960;&#20010;&#23454;&#20363;&#65292;&#20854;&#20013;&#21487;&#35757;&#32451;&#21442;&#25968;&#20165;&#20026;19&#20010;&#65292;&#22312;&#39030;&#22840;&#20811;&#21943;&#27880;&#30340;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#24615;&#33021;&#36229;&#36807;&#20102;&#25968;&#20197;&#19975;&#35745;&#21442;&#25968;&#30340;&#36890;&#29992;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
As particle accelerators increase their collision rates, and deep learning solutions prove their viability, there is a growing need for lightweight and fast neural network architectures for low-latency tasks such as triggering. We examine the potential of one recent Lorentz- and permutation-symmetric architecture, PELICAN, and present its instances with as few as 19 trainable parameters that outperform generic architectures with tens of thousands of parameters when compared on the binary classification task of top quark jet tagging.
&lt;/p&gt;</description></item><item><title>Alquist 5.0&#26159;&#19968;&#31181;&#26032;&#30340;SocialBot&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#23545;&#35805;&#26641;&#21644;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#21450;&#24341;&#20837;NRG Barista&#21644;&#25903;&#25345;&#22810;&#27169;&#24335;&#35774;&#22791;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#23545;&#35805;&#20307;&#39564;&#65292;&#24182;&#20445;&#25345;&#20102;&#20849;&#24773;&#21644;&#30693;&#35782;&#22411;&#23545;&#35805;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.16119</link><description>&lt;p&gt;
Alquist 5.0&#65306;&#23545;&#35805;&#26641;&#19982;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#22686;&#24378;SocialBot&#23545;&#35805;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alquist 5.0: Dialogue Trees Meet Generative Models. A Novel Approach for Enhancing SocialBot Conversations. (arXiv:2310.16119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16119
&lt;/p&gt;
&lt;p&gt;
Alquist 5.0&#26159;&#19968;&#31181;&#26032;&#30340;SocialBot&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#23545;&#35805;&#26641;&#21644;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#21450;&#24341;&#20837;NRG Barista&#21644;&#25903;&#25345;&#22810;&#27169;&#24335;&#35774;&#22791;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#23545;&#35805;&#20307;&#39564;&#65292;&#24182;&#20445;&#25345;&#20102;&#20849;&#24773;&#21644;&#30693;&#35782;&#22411;&#23545;&#35805;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;SocialBot- Alquist 5.0-&#65292;&#35813;&#31995;&#32479;&#26159;&#20026;Alexa Prize SocialBot&#22823;&#25361;&#25112;5&#24320;&#21457;&#30340;&#12290;&#22312;&#25105;&#20204;&#31995;&#32479;&#30340;&#21069;&#20960;&#20010;&#29256;&#26412;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NRG Barista&#65292;&#24182;&#27010;&#36848;&#20102;&#23558;Barista&#25972;&#21512;&#21040;&#25105;&#20204;&#30340;SocialBot&#20013;&#30340;&#20960;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#25972;&#20307;&#30340;&#23545;&#35805;&#20307;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;SocialBot&#20197;&#25903;&#25345;&#22810;&#27169;&#24335;&#35774;&#22791;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;Alquist 5.0&#24320;&#21457;&#30340;&#35265;&#35299;&#65292;&#35813;&#31995;&#32479;&#22312;&#28385;&#36275;&#29992;&#25143;&#19981;&#26029;&#21464;&#21270;&#30340;&#26399;&#26395;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20102;&#23545;&#21508;&#31181;&#20027;&#39064;&#30340;&#20849;&#24773;&#21644;&#30693;&#35782;&#22411;&#23545;&#35805;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present our SocialBot -- Alquist~5.0 -- developed for the Alexa Prize SocialBot Grand Challenge~5. Building upon previous versions of our system, we introduce the NRG Barista and outline several innovative approaches for integrating Barista into our SocialBot, improving the overall conversational experience. Additionally, we extend our SocialBot to support multimodal devices. This paper offers insights into the development of Alquist~5.0, which meets evolving user expectations while maintaining empathetic and knowledgeable conversational abilities across diverse topics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#22312;&#32447;&#23433;&#24944;&#21058;&#30340;&#26087;&#31867;&#21035;&#25968;&#25454;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#65292;&#35299;&#20915;&#20102;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#38271;&#26399;&#35760;&#24518;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20102;&#26087;&#31867;&#21035;&#30693;&#35782;&#20445;&#30041;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.16115</link><description>&lt;p&gt;
&#26080;&#38656;&#36807;&#21435;&#25968;&#25454;&#65292;&#21796;&#37266;&#36807;&#21435;&#30340;&#27010;&#24565;&#65306;&#20174;&#22312;&#32447;&#23433;&#24944;&#21058;&#36827;&#34892;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Wakening Past Concepts without Past Data: Class-Incremental Learning from Online Placebos. (arXiv:2310.16115v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#22312;&#32447;&#23433;&#24944;&#21058;&#30340;&#26087;&#31867;&#21035;&#25968;&#25454;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#65292;&#35299;&#20915;&#20102;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#38271;&#26399;&#35760;&#24518;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20102;&#26087;&#31867;&#21035;&#30693;&#35782;&#20445;&#30041;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27169;&#22411;&#19981;&#26029;&#36866;&#24212;&#26032;&#31867;&#21035;&#26102;&#65292;&#19981;&#24536;&#35760;&#26087;&#31867;&#21035;&#30340;&#30693;&#35782;&#26159;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#24120;&#29992;&#25216;&#26415;&#26159;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#65292;&#23427;&#24809;&#32602;&#20102;&#26087;&#27169;&#22411;&#21644;&#26032;&#27169;&#22411;&#20043;&#38388;&#30340;&#39044;&#27979;&#19981;&#19968;&#33268;&#24615;&#12290;&#30001;&#20110;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#20869;&#23384;&#38480;&#21046;&#20005;&#26684;&#65292;&#26087;&#31867;&#21035;&#25968;&#25454;&#26497;&#20026;&#31232;&#32570;&#65292;&#22240;&#27492;&#36825;&#26679;&#30340;&#39044;&#27979;&#20960;&#20046;&#37117;&#26159;&#30001;&#26032;&#31867;&#21035;&#25968;&#25454;&#36827;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#30693;&#35782;&#33976;&#39311;&#25439;&#22833;&#65292;&#24182;&#21457;&#29616;&#8220;&#20351;&#29992;&#26032;&#31867;&#21035;&#25968;&#25454;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#8221;&#19981;&#20165;&#38459;&#30861;&#20102;&#27169;&#22411;&#23545;&#26032;&#31867;&#21035;&#30340;&#36866;&#24212;&#65292;&#32780;&#19988;&#23545;&#20445;&#30041;&#26087;&#31867;&#21035;&#30693;&#35782;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#20351;&#29992;&#26087;&#31867;&#21035;&#30340;&#23433;&#24944;&#21058;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#8221;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#23433;&#24944;&#21058;&#26159;&#20174;&#33258;&#30001;&#22270;&#20687;&#27969;&#20013;&#36873;&#25321;&#30340;&#65292;&#27604;&#22914;&#35895;&#27468;&#22270;&#29255;&#65292;&#20197;&#33258;&#21160;&#21644;&#32463;&#27982;&#30340;&#26041;&#24335;&#36827;&#34892;&#36873;&#25321;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#22312;&#32447;&#23433;&#24944;&#21058;&#36873;&#25321;&#31574;&#30053;&#26469;&#24555;&#36895;&#35780;&#20272;&#27969;&#24335;&#22270;&#20687;&#30340;&#36136;&#37327;&#65288;&#22909;&#30340;&#25110;&#22351;&#30340;&#23433;&#24944;&#21058;&#65289;&#65292;&#24182;&#19988;&#21482;&#20351;&#29992;&#22909;&#30340;&#23433;&#24944;&#21058;&#36827;&#34892;&#19968;&#27425;&#21890;&#39135;&#12290;
&lt;/p&gt;
&lt;p&gt;
Not forgetting old class knowledge is a key challenge for class-incremental learning (CIL) when the model continuously adapts to new classes. A common technique to address this is knowledge distillation (KD), which penalizes prediction inconsistencies between old and new models. Such prediction is made with almost new class data, as old class data is extremely scarce due to the strict memory limitation in CIL. In this paper, we take a deep dive into KD losses and find that "using new class data for KD" not only hinders the model adaption (for learning new classes) but also results in low efficiency for preserving old class knowledge. We address this by "using the placebos of old classes for KD", where the placebos are chosen from a free image stream, such as Google Images, in an automatical and economical fashion. To this end, we train an online placebo selection policy to quickly evaluate the quality of streaming images (good or bad placebos) and use only good ones for one-time feed-f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#33041;&#22522;&#22240;&#36716;&#24405;&#30340;&#21387;&#32553;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#26041;&#27861;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#37325;&#24314;&#12289;&#35299;&#21078;&#21644;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16113</link><description>&lt;p&gt;
&#22823;&#33041;&#22522;&#22240;&#36716;&#24405;&#30340;&#21387;&#32553;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Compressed representation of brain genetic transcription. (arXiv:2310.16113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#33041;&#22522;&#22240;&#36716;&#24405;&#30340;&#21387;&#32553;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#26041;&#27861;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#37325;&#24314;&#12289;&#35299;&#21078;&#21644;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#30340;&#32467;&#26500;&#36807;&#20110;&#22797;&#26434;&#65292;&#26080;&#27861;&#30452;&#35266;&#22320;&#36827;&#34892;&#35266;&#23519;&#65292;&#38656;&#35201;&#20351;&#29992;&#21387;&#32553;&#34920;&#31034;&#23558;&#20854;&#21464;&#21270;&#25237;&#24433;&#21040;&#32039;&#20945;&#12289;&#21487;&#23548;&#33322;&#30340;&#31354;&#38388;&#20013;&#12290;&#22312;&#39640;&#32500;&#25968;&#25454;&#65288;&#22914;&#22522;&#22240;&#34920;&#36798;&#65289;&#20013;&#65292;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20854;&#20013;&#35299;&#21078;&#21644;&#36716;&#24405;&#27169;&#24335;&#30340;&#32852;&#21512;&#22797;&#26434;&#24615;&#35201;&#27714;&#26368;&#22823;&#21387;&#32553;&#12290;&#30446;&#21069;&#30340;&#23454;&#36341;&#26159;&#20351;&#29992;&#26631;&#20934;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#65292;&#20854;&#35745;&#31639;&#25928;&#29575;&#21463;&#21040;&#38480;&#21046;&#65292;&#23588;&#20854;&#22312;&#22823;&#21387;&#32553;&#27604;&#19979;&#34920;&#29616;&#21147;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#20840;&#33041;&#20307;&#32032;&#32423;Allen&#22823;&#33041;&#22270;&#35889;&#36716;&#24405;&#25968;&#25454;&#65292;&#31995;&#32479;&#27604;&#36739;&#20102;&#22522;&#20110;&#26368;&#24191;&#27867;&#25903;&#25345;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#26041;&#27861;&#65288;PCA&#65292;&#26680;PCA&#65292;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#65292;t-&#38543;&#26426;&#37051;&#23621;&#23884;&#20837;&#65288;t-SNE&#65289;&#65292;&#32479;&#19968;&#27969;&#24418;&#36924;&#36817;&#21644;&#25237;&#24433;&#65288;UMAP&#65289;&#65292;&#28145;&#24230;&#33258;&#32534;&#30721;&#65289;&#30340;&#21387;&#32553;&#34920;&#31034;&#65292;&#37327;&#21270;&#37325;&#24314;&#20445;&#30495;&#24230;&#65292;&#35299;&#21078;&#36830;&#36143;&#24615;&#21644;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The architecture of the brain is too complex to be intuitively surveyable without the use of compressed representations that project its variation into a compact, navigable space. The task is especially challenging with high-dimensional data, such as gene expression, where the joint complexity of anatomical and transcriptional patterns demands maximum compression. Established practice is to use standard principal component analysis (PCA), whose computational felicity is offset by limited expressivity, especially at great compression ratios. Employing whole-brain, voxel-wise Allen Brain Atlas transcription data, here we systematically compare compressed representations based on the most widely supported linear and non-linear methods-PCA, kernel PCA, non-negative matrix factorization (NMF), t-stochastic neighbour embedding (t-SNE), uniform manifold approximation and projection (UMAP), and deep auto-encoding-quantifying reconstruction fidelity, anatomical coherence, and predictive utility
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#25991;&#26723;&#29983;&#25104;&#26426;&#21046;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#38454;&#25552;&#31034;&#23545;&#25239;&#20316;&#32773;&#21435;&#21311;&#21517;&#25915;&#20987;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#19979;&#28216;&#25928;&#29992;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26426;&#21046;&#22312;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#21516;&#26102;&#33021;&#22815;&#23436;&#20840;&#24674;&#22797;&#28165;&#27905;&#30340;&#24773;&#24863;&#20998;&#25968;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.16111</link><description>&lt;p&gt;
&#20351;&#29992;&#38646;&#38454;&#25552;&#31034;&#30340;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#25991;&#26723;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Locally Differentially Private Document Generation Using Zero Shot Prompting. (arXiv:2310.16111v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#25991;&#26723;&#29983;&#25104;&#26426;&#21046;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#38454;&#25552;&#31034;&#23545;&#25239;&#20316;&#32773;&#21435;&#21311;&#21517;&#25915;&#20987;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#19979;&#28216;&#25928;&#29992;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26426;&#21046;&#22312;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#21516;&#26102;&#33021;&#22815;&#23436;&#20840;&#24674;&#22797;&#28165;&#27905;&#30340;&#24773;&#24863;&#20998;&#25968;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30740;&#31350;&#24050;&#32463;&#24378;&#35843;&#20102;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#35270;&#35282;&#65292;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#20026;&#38544;&#31169;&#20445;&#25252;&#20570;&#20986;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DP-Prompt&#30340;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#38454;&#25552;&#31034;&#26469;&#23545;&#25239;&#20316;&#32773;&#21435;&#21311;&#21517;&#25915;&#20987;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#19979;&#28216;&#25928;&#29992;&#30340;&#24433;&#21709;&#12290;&#24403;DP-Prompt&#19982;&#20687;ChatGPT&#65288;gpt-3.5&#65289;&#36825;&#26679;&#30340;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#19968;&#36215;&#20351;&#29992;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#21435;&#21311;&#21517;&#25915;&#20987;&#25104;&#21151;&#29575;&#26174;&#33879;&#38477;&#20302;&#65292;&#24182;&#19988;&#23613;&#31649;&#20854;&#35774;&#35745;&#26356;&#31616;&#21333;&#65292;&#20294;&#23427;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#24456;&#22823;&#31243;&#24230;&#12290;&#20363;&#22914;&#65292;&#22312;IMDB&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;DP-Prompt&#65288;&#20351;&#29992;ChatGPT&#65289;&#23436;&#20840;&#24674;&#22797;&#20102;&#28165;&#27905;&#30340;&#24773;&#24863;F1&#20998;&#25968;&#65292;&#24182;&#22312;&#38745;&#24577;&#25915;&#20987;&#32773;&#30340;&#20316;&#32773;&#35782;&#21035;F1&#20998;&#25968;&#19978;&#23454;&#29616;&#20102;46&#65285;&#30340;&#38477;&#20302;&#21644;26&#65285;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous studies have highlighted the privacy risks associated with pretrained large language models. In contrast, our research offers a unique perspective by demonstrating that pretrained large language models can effectively contribute to privacy preservation. We propose a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility. When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5), we observe a notable reduction in the success rate of de-anonymization attacks, showing that it surpasses existing approaches by a considerable margin despite its simpler design. For instance, in the case of the IMDB dataset, DP-Prompt (with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving a 46\% reduction in author identification F1 score against static attackers and a 26\% reduc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#36827;&#34892;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#30340;&#36890;&#20449;&#26694;&#26550;BASS&#65292;&#37319;&#29992;&#24191;&#25773;&#20256;&#36755;&#21644;&#27010;&#29575;&#23376;&#22270;&#37319;&#26679;&#12290;&#36890;&#36807;&#25511;&#21046;&#23376;&#38598;&#30340;&#38543;&#26426;&#28608;&#27963;&#21644;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#36890;&#20449;&#25104;&#26412;&#32422;&#26463;&#65292;&#23454;&#29616;&#20102;&#31639;&#27861;&#30340;&#24555;&#36895;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2310.16106</link><description>&lt;p&gt;
&#26080;&#32447;&#32593;&#32476;&#19978;&#22522;&#20110;&#24191;&#25773;&#23376;&#22270;&#37319;&#26679;&#30340;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decentralized Learning over Wireless Networks with Broadcast-Based Subgraph Sampling. (arXiv:2310.16106v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#36827;&#34892;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#30340;&#36890;&#20449;&#26694;&#26550;BASS&#65292;&#37319;&#29992;&#24191;&#25773;&#20256;&#36755;&#21644;&#27010;&#29575;&#23376;&#22270;&#37319;&#26679;&#12290;&#36890;&#36807;&#25511;&#21046;&#23376;&#38598;&#30340;&#38543;&#26426;&#28608;&#27963;&#21644;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#36890;&#20449;&#25104;&#26412;&#32422;&#26463;&#65292;&#23454;&#29616;&#20102;&#31639;&#27861;&#30340;&#24555;&#36895;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#26080;&#32447;&#32593;&#32476;&#19978;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#30340;&#36890;&#20449;&#26041;&#38754;&#65292;&#20351;&#29992;&#22522;&#20110;&#20849;&#35782;&#30340;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;D-SGD&#65289;&#12290;&#32771;&#34385;&#21040;&#36845;&#20195;&#36807;&#31243;&#20013;&#22240;&#32593;&#32476;&#20869;&#20449;&#24687;&#20132;&#25442;&#32780;&#23548;&#33268;&#30340;&#23454;&#38469;&#36890;&#20449;&#25104;&#26412;&#25110;&#24310;&#36831;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#27599;&#20010;&#20256;&#36755;&#26102;&#38553;&#30340;&#25913;&#36827;&#26469;&#23454;&#29616;&#31639;&#27861;&#30340;&#24555;&#36895;&#25910;&#25947;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;BASS&#65292;&#19968;&#20010;&#29992;&#20110;&#26080;&#32447;&#32593;&#32476;&#19978;D-SGD&#30340;&#39640;&#25928;&#36890;&#20449;&#26694;&#26550;&#65292;&#20854;&#20013;&#37319;&#29992;&#24191;&#25773;&#20256;&#36755;&#21644;&#27010;&#29575;&#23376;&#22270;&#37319;&#26679;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#28608;&#27963;&#22810;&#20010;&#38750;&#24178;&#25200;&#33410;&#28857;&#30340;&#23376;&#38598;&#65292;&#23558;&#27169;&#22411;&#26356;&#26032;&#24191;&#25773;&#32473;&#23427;&#20204;&#30340;&#37051;&#23621;&#12290;&#36825;&#20123;&#23376;&#38598;&#20250;&#38543;&#26426;&#38543;&#26102;&#38388;&#28608;&#27963;&#65292;&#24182;&#26681;&#25454;&#20854;&#22312;&#32593;&#32476;&#36830;&#36890;&#24615;&#20013;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#36890;&#20449;&#25104;&#26412;&#32422;&#26463;&#65288;&#20363;&#22914;&#27599;&#27425;&#36845;&#20195;&#30340;&#24179;&#22343;&#20256;&#36755;&#26102;&#38553;&#25968;&#65289;&#36827;&#34892;&#25511;&#21046;&#12290;&#22312;&#20849;&#35782;&#26356;&#26032;&#27493;&#39588;&#20013;&#65292;&#20165;&#20445;&#30041;&#21452;&#21521;&#38142;&#25509;&#20197;&#20445;&#25345;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work centers on the communication aspects of decentralized learning over wireless networks, using consensus-based decentralized stochastic gradient descent (D-SGD). Considering the actual communication cost or delay caused by in-network information exchange in an iterative process, our goal is to achieve fast convergence of the algorithm measured by improvement per transmission slot. We propose BASS, an efficient communication framework for D-SGD over wireless networks with broadcast transmission and probabilistic subgraph sampling. In each iteration, we activate multiple subsets of non-interfering nodes to broadcast model updates to their neighbors. These subsets are randomly activated over time, with probabilities reflecting their importance in network connectivity and subject to a communication cost constraint (e.g., the average number of transmission slots per iteration). During the consensus update step, only bi-directional links are effectively preserved to maintain communic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#30340;&#26799;&#24230;&#36319;&#36394;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#20445;&#25345;&#23398;&#20064;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#26377;&#25928;&#22320;&#20445;&#25252;&#20102;&#23398;&#20064;&#32773;&#30340;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2310.16105</link><description>&lt;p&gt;
&#22312;&#26377;&#21521;&#22270;&#19978;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#20013;&#22522;&#20110;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#30340;&#26799;&#24230;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Locally Differentially Private Gradient Tracking for Distributed Online Learning over Directed Graphs. (arXiv:2310.16105v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#30340;&#26799;&#24230;&#36319;&#36394;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#20445;&#25345;&#23398;&#20064;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#26377;&#25928;&#22320;&#20445;&#25252;&#20102;&#23398;&#20064;&#32773;&#30340;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#22312;&#35299;&#20915;&#28041;&#21450;&#27969;&#25968;&#25454;&#30340;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#23398;&#20064;&#32773;&#20043;&#38388;&#30340;&#20449;&#24687;&#20849;&#20139;&#20063;&#24341;&#36215;&#20102;&#20010;&#20307;&#23398;&#20064;&#32773;&#25935;&#24863;&#25968;&#25454;&#21487;&#33021;&#27844;&#38706;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#39118;&#38505;&#65292;&#24046;&#20998;&#38544;&#31169;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#29616;&#26377;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#32467;&#26524;&#20013;&#65292;&#34987;&#35748;&#20026;&#26159;&#38544;&#31169;&#20445;&#25252;&#30340;&#8220;&#37329;&#26631;&#20934;&#8221;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32467;&#26524;&#24448;&#24448;&#38754;&#20020;&#23398;&#20064;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#26799;&#24230;&#36319;&#36394;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#25104;&#21151;&#22320;&#36991;&#20813;&#20102;&#36825;&#31181;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#22343;&#26041;&#24847;&#20041;&#19978;&#25910;&#25947;&#21040;&#31934;&#30830;&#30340;&#26368;&#20248;&#35299;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#20005;&#26684;&#30340;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#65292;&#21363;&#20351;&#36845;&#20195;&#27425;&#25968;&#22686;&#21152;&#65292;&#32047;&#31215;&#38544;&#31169;&#39044;&#31639;&#20063;&#20445;&#35777;&#26159;&#26377;&#38480;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed online learning has been proven extremely effective in solving large-scale machine learning problems involving streaming data. However, information sharing between learners in distributed learning also raises concerns about the potential leakage of individual learners' sensitive data. To mitigate this risk, differential privacy, which is widely regarded as the "gold standard" for privacy protection, has been widely employed in many existing results on distributed online learning. However, these results often face a fundamental tradeoff between learning accuracy and privacy. In this paper, we propose a locally differentially private gradient tracking based distributed online learning algorithm that successfully circumvents this tradeoff. Our analysis shows that the proposed algorithm converges in mean square to the exact optimal solution while ensuring rigorous local differential privacy, with the cumulative privacy budget guaranteed to be finite even when the number of iter
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#21078;&#23398;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20998;&#21106;&#25513;&#27169;&#20013;&#30340;&#20840;&#23616;&#20449;&#24687;&#26469;&#20272;&#35745;&#21322;&#30417;&#30563;&#22270;&#20687;&#20998;&#21106;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#20256;&#32479;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#30340;&#39640;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#23545;&#20687;&#32032;&#32423;&#24046;&#24322;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.16099</link><description>&lt;p&gt;
&#35299;&#21078;&#23398;&#24863;&#30693;&#30340;&#21322;&#30417;&#30563;&#22270;&#20687;&#20998;&#21106;&#30340;&#19981;&#30830;&#23450;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Anatomically-aware Uncertainty for Semi-supervised Image Segmentation. (arXiv:2310.16099v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#21078;&#23398;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20998;&#21106;&#25513;&#27169;&#20013;&#30340;&#20840;&#23616;&#20449;&#24687;&#26469;&#20272;&#35745;&#21322;&#30417;&#30563;&#22270;&#20687;&#20998;&#21106;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#20256;&#32479;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#30340;&#39640;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#23545;&#20687;&#32032;&#32423;&#24046;&#24322;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#36890;&#36807;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#25918;&#23485;&#23545;&#22270;&#20687;&#20998;&#21106;&#22823;&#35268;&#27169;&#20687;&#32032;&#32423;&#26631;&#27880;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#27861;&#26159;&#23545;&#27169;&#22411;&#39044;&#27979;&#36827;&#34892;&#35268;&#33539;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#39044;&#27979;&#21487;&#33021;&#19981;&#21487;&#38752;&#65292;&#36890;&#24120;&#38656;&#35201;&#20511;&#21161;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#26041;&#27861;&#36880;&#28176;&#23398;&#20064;&#26377;&#24847;&#20041;&#21644;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#20381;&#36182;&#20110;&#27169;&#22411;&#39044;&#27979;&#30340;&#22810;&#27425;&#25512;&#26029;&#65292;&#27599;&#20010;&#35757;&#32451;&#27493;&#39588;&#37117;&#38656;&#35201;&#35745;&#31639;&#65292;&#36825;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#22270;&#20687;&#25429;&#25417;&#20687;&#32032;&#32423;&#24046;&#24322;&#65292;&#24182;&#19981;&#33021;&#32771;&#34385;&#20840;&#23616;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20998;&#21106;&#25513;&#27169;&#20013;&#30340;&#20840;&#23616;&#20449;&#24687;&#26469;&#20272;&#35745;&#20998;&#21106;&#19981;&#30830;&#23450;&#24615;&#12290;&#26356;&#30830;&#20999;&#22320;&#35828;&#65292;&#39318;&#20808;&#23398;&#20064;&#35299;&#21078;&#23398;&#24863;&#30693;&#30340;&#34920;&#31034;&#26469;&#24314;&#27169;&#21487;&#29992;&#30340;&#20998;&#21106;&#25513;&#27169;&#12290;&#28982;&#21518;&#65292;&#35813;&#23398;&#20064;&#34920;&#31034;&#23558;&#26032;&#30340;&#20998;&#21106;&#39044;&#27979;&#26144;&#23556;&#21040;&#19968;&#20010;&#35299;&#21078;&#32467;&#26500;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning relaxes the need of large pixel-wise labeled datasets for image segmentation by leveraging unlabeled data. A prominent way to exploit unlabeled data is to regularize model predictions. Since the predictions of unlabeled data can be unreliable, uncertainty-aware schemes are typically employed to gradually learn from meaningful and reliable predictions. Uncertainty estimation methods, however, rely on multiple inferences from the model predictions that must be computed for each training step, which is computationally expensive. Moreover, these uncertainty maps capture pixel-wise disparities and do not consider global information. This work proposes a novel method to estimate segmentation uncertainty by leveraging global information from the segmentation masks. More precisely, an anatomically-aware representation is first learnt to model the available segmentation masks. The learnt representation thereupon maps the prediction of a new segmentation into an anatomic
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#22343;&#34913;&#31574;&#30053;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#24378;&#30423;&#30340;&#31639;&#27861;&#26469;&#35780;&#20272;&#21644;&#25913;&#36827;&#24211;&#23384;&#25511;&#21046;&#31574;&#30053;&#65292;&#36825;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#26377;&#21033;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.16096</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#24378;&#30423;&#29992;&#20110;&#35780;&#20272;&#21644;&#25913;&#36827;&#24211;&#23384;&#25511;&#21046;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Contextual Bandits for Evaluating and Improving Inventory Control Policies. (arXiv:2310.16096v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16096
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#22343;&#34913;&#31574;&#30053;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#24378;&#30423;&#30340;&#31639;&#27861;&#26469;&#35780;&#20272;&#21644;&#25913;&#36827;&#24211;&#23384;&#25511;&#21046;&#31574;&#30053;&#65292;&#36825;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#26377;&#21033;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#24211;&#23384;&#25511;&#21046;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#28041;&#21450;&#23545;&#38750;&#24179;&#31283;&#38543;&#26426;&#38656;&#27714;&#12289;&#22833;&#21435;&#38144;&#21806;&#21644;&#20855;&#26377;&#38543;&#26426;&#20379;&#24212;&#21830;&#20132;&#36135;&#26102;&#38388;&#30340;&#21608;&#26399;&#24615;&#26816;&#26597;&#30340;&#21160;&#21147;&#23398;&#36827;&#34892;&#24378;&#20551;&#35774;&#30340;&#36924;&#36817;&#25110;&#27169;&#25311;&#65292;&#24182;&#24212;&#29992;&#20248;&#21270;&#12289;&#21160;&#24577;&#35268;&#21010;&#25110;&#24378;&#21270;&#23398;&#20064;&#31561;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#20998;&#26512;&#21644;&#35780;&#20272;&#20219;&#20309;&#24211;&#23384;&#25511;&#21046;&#31574;&#30053;&#26159;&#37325;&#35201;&#30340;&#65292;&#29305;&#21035;&#26159;&#30475;&#26159;&#21542;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22343;&#34913;&#31574;&#30053;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#20010;&#31574;&#30053;&#30340;&#29702;&#24819;&#29305;&#24615;&#65292;&#30452;&#35266;&#22320;&#24847;&#21619;&#30528;&#20107;&#21518;&#21482;&#25913;&#21464;&#23569;&#37096;&#20998;&#25805;&#20316;&#19981;&#20250;&#20135;&#29983;&#23454;&#36136;&#19978;&#26356;&#22810;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#24378;&#30423;&#31639;&#27861;&#26469;&#35780;&#20272;&#21644;&#20598;&#23572;&#24494;&#35843;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#30740;&#31350;&#20013;&#22343;&#21462;&#24471;&#20102;&#26377;&#21033;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solutions to address the periodic review inventory control problem with nonstationary random demand, lost sales, and stochastic vendor lead times typically involve making strong assumptions on the dynamics for either approximation or simulation, and applying methods such as optimization, dynamic programming, or reinforcement learning. Therefore, it is important to analyze and evaluate any inventory control policy, in particular to see if there is room for improvement. We introduce the concept of an equilibrium policy, a desirable property of a policy that intuitively means that, in hindsight, changing only a small fraction of actions does not result in materially more reward. We provide a light-weight contextual bandit-based algorithm to evaluate and occasionally tweak policies, and show that this method achieves favorable guarantees, both theoretically and in empirical studies.
&lt;/p&gt;</description></item><item><title>&#32447;&#24615;&#21464;&#25442;&#22120;&#65288;LTs&#65289;&#25110;&#24555;&#36895;&#26435;&#37325;&#31243;&#24207;&#21592;&#65288;FWPs&#65289;&#26159;&#19968;&#31181;&#29305;&#27530;&#30340;&#24207;&#21015;&#22788;&#29702;&#22120;&#65292;&#21487;&#20197;&#29992;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#31867;&#20284;&#20110;&#26631;&#20934;&#21464;&#25442;&#22120;&#30340;&#24456;&#22810;&#32467;&#26524;&#20063;&#36866;&#29992;&#20110;LTs/FWPs&#12290;&#24490;&#29615;FWP&#21644;&#33258;&#24341;&#29992;&#26435;&#37325;&#30697;&#38453;&#30340;&#25193;&#23637;&#25104;&#21151;&#22320;&#20811;&#26381;&#20102;LT&#30340;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#22312;&#22855;&#20598;&#38382;&#39064;&#19978;&#30340;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.16076</link><description>&lt;p&gt;
&#32447;&#24615;&#21464;&#25442;&#22120;&#21450;&#20854;&#24490;&#29615;&#21644;&#33258;&#25351;&#25193;&#23637;&#30340;&#23454;&#29992;&#35745;&#31639;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Practical Computational Power of Linear Transformers and Their Recurrent and Self-Referential Extensions. (arXiv:2310.16076v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16076
&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#21464;&#25442;&#22120;&#65288;LTs&#65289;&#25110;&#24555;&#36895;&#26435;&#37325;&#31243;&#24207;&#21592;&#65288;FWPs&#65289;&#26159;&#19968;&#31181;&#29305;&#27530;&#30340;&#24207;&#21015;&#22788;&#29702;&#22120;&#65292;&#21487;&#20197;&#29992;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#31867;&#20284;&#20110;&#26631;&#20934;&#21464;&#25442;&#22120;&#30340;&#24456;&#22810;&#32467;&#26524;&#20063;&#36866;&#29992;&#20110;LTs/FWPs&#12290;&#24490;&#29615;FWP&#21644;&#33258;&#24341;&#29992;&#26435;&#37325;&#30697;&#38453;&#30340;&#25193;&#23637;&#25104;&#21151;&#22320;&#20811;&#26381;&#20102;LT&#30340;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#22312;&#22855;&#20598;&#38382;&#39064;&#19978;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#35745;&#31639;&#33021;&#21147;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19968;&#31181;&#32473;&#23450;&#23454;&#26102;&#21644;&#26377;&#38480;&#31934;&#24230;&#20551;&#35774;&#30340;RNN&#20307;&#31995;&#32467;&#26500;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#32447;&#24615;&#21270;&#27880;&#24847;&#21147;&#30340;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#65292;&#20063;&#31216;&#20026;&#32447;&#24615;&#21464;&#25442;&#22120;&#65288;LT&#65289;&#25110;&#24555;&#36895;&#26435;&#37325;&#31243;&#24207;&#21592;&#65288;FWP&#65289;&#12290;LT&#22312;&#29305;&#23450;&#24847;&#20041;&#19978;&#26159;&#29305;&#27530;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#31561;&#21516;&#20110;&#20855;&#26377;&#22266;&#23450;&#22823;&#23567;&#29366;&#24577;&#30340;&#31867;&#20284;RNN&#30340;&#24207;&#21015;&#22788;&#29702;&#22120;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#29616;&#22312;&#27969;&#34892;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;&#20851;&#20110;&#26631;&#20934;&#21464;&#25442;&#22120;&#30340;&#30693;&#21517;&#32467;&#26524;&#22914;&#20309;&#30452;&#25509;&#36716;&#31227;&#21040;LTs/FWPs&#12290;&#25105;&#20204;&#30340;&#24418;&#24335;&#35821;&#35328;&#35782;&#21035;&#23454;&#39564;&#28436;&#31034;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;FWP&#25193;&#23637;&#65292;&#22914;&#24490;&#29615;FWPs&#21644;&#33258;&#24341;&#29992;&#26435;&#37325;&#30697;&#38453;&#65292;&#22914;&#20309;&#25104;&#21151;&#20811;&#26381;LT&#30340;&#26576;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#22312;&#22855;&#20598;&#38382;&#39064;&#19978;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#26159;&#20844;&#24320;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies of the computational power of recurrent neural networks (RNNs) reveal a hierarchy of RNN architectures, given real-time and finite-precision assumptions. Here we study auto-regressive Transformers with linearised attention, a.k.a. linear Transformers (LTs) or Fast Weight Programmers (FWPs). LTs are special in the sense that they are equivalent to RNN-like sequence processors with a fixed-size state, while they can also be expressed as the now-popular self-attention networks. We show that many well-known results for the standard Transformer directly transfer to LTs/FWPs. Our formal language recognition experiments demonstrate how recently proposed FWP extensions such as recurrent FWPs and self-referential weight matrices successfully overcome certain limitations of the LT, e.g., allowing for generalisation on the parity problem. Our code is public.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#24314;&#31435;&#31283;&#20581;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#30005;&#32593;&#39057;&#29575;&#12290;&#20010;&#20307;&#21270;&#30340;&#21367;&#31215;LSTM&#27169;&#22411;&#21487;&#20197;&#29420;&#31435;&#22320;&#20026;&#22823;&#23398;&#26657;&#22253;&#20869;&#30340;&#24314;&#31569;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#24182;&#19988;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16071</link><description>&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;LSTM&#22312;&#22823;&#23398;&#26657;&#22253;&#20013;&#39044;&#27979;&#30005;&#32593;&#39057;&#29575;
&lt;/p&gt;
&lt;p&gt;
Grid Frequency Forecasting in University Campuses using Convolutional LSTM. (arXiv:2310.16071v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16071
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#24314;&#31435;&#31283;&#20581;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#30005;&#32593;&#39057;&#29575;&#12290;&#20010;&#20307;&#21270;&#30340;&#21367;&#31215;LSTM&#27169;&#22411;&#21487;&#20197;&#29420;&#31435;&#22320;&#20026;&#22823;&#23398;&#26657;&#22253;&#20869;&#30340;&#24314;&#31569;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#24182;&#19988;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#30005;&#32593;&#38754;&#20020;&#30528;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#20027;&#35201;&#28304;&#20110;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#25972;&#21512;&#21644;&#28040;&#36153;&#27169;&#24335;&#30340;&#28436;&#21464;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#24314;&#31435;&#31283;&#20581;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#30005;&#32593;&#39057;&#29575;&#12290;&#36825;&#20123;&#27169;&#22411;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#30005;&#32593;&#39057;&#29575;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#26102;&#31354;&#22797;&#26434;&#24615;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#22686;&#24378;&#20102;&#30005;&#32593;&#30340;&#21487;&#38752;&#24615;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#23398;&#26657;&#22253;&#20869;&#20010;&#20307;&#21270;&#30340;&#21367;&#31215;LSTM&#65288;ConvLSTM&#65289;&#27169;&#22411;&#30340;&#28508;&#21147;&#21644;&#21457;&#23637;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#29420;&#31435;&#22320;&#38024;&#23545;&#27599;&#19968;&#26635;&#24314;&#31569;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#20010;&#20307;ConvLSTM&#27169;&#22411;&#22522;&#20110;&#27599;&#26635;&#26657;&#22253;&#24314;&#31569;&#30340;&#29992;&#30005;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#26681;&#25454;&#21382;&#21490;&#36235;&#21183;&#39044;&#27979;&#30005;&#32593;&#39057;&#29575;&#12290;&#32467;&#26524;&#26377;&#21147;&#22320;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The modern power grid is facing increasing complexities, primarily stemming from the integration of renewable energy sources and evolving consumption patterns. This paper introduces an innovative methodology that harnesses Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks to establish robust time series forecasting models for grid frequency. These models effectively capture the spatiotemporal intricacies inherent in grid frequency data, significantly enhancing prediction accuracy and bolstering power grid reliability. The research explores the potential and development of individualized Convolutional LSTM (ConvLSTM) models for buildings within a university campus, enabling them to be independently trained and evaluated for each building. Individual ConvLSTM models are trained on power consumption data for each campus building and forecast the grid frequency based on historical trends. The results convincingly demonstrate the superiority of the proposed mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STHODE&#30340;&#31354;&#38388;-&#26102;&#38388;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#65292;&#35813;&#32593;&#32476;&#32467;&#21512;&#20102;&#36947;&#36335;&#32593;&#32476;&#25299;&#25169;&#21644;&#20132;&#36890;&#21160;&#24577;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#39640;&#38454;&#31354;&#38388;-&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16070</link><description>&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatial-Temporal Hypergraph Neural Network for Traffic Forecasting. (arXiv:2310.16070v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STHODE&#30340;&#31354;&#38388;-&#26102;&#38388;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#65292;&#35813;&#32593;&#32476;&#32467;&#21512;&#20102;&#36947;&#36335;&#32593;&#32476;&#25299;&#25169;&#21644;&#20132;&#36890;&#21160;&#24577;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#39640;&#38454;&#31354;&#38388;-&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#23427;&#20381;&#38752;&#31227;&#21160;&#20114;&#32852;&#32593;&#21457;&#23637;&#21644;&#23450;&#20301;&#25216;&#26415;&#65292;&#36890;&#36807;&#25910;&#38598;&#30340;&#20132;&#36890;&#25968;&#25454;&#26469;&#23454;&#29616;&#20016;&#23500;&#22810;&#26679;&#30340;&#20132;&#36890;&#24212;&#29992;&#65292;&#24182;&#20026;&#20154;&#20204;&#24102;&#26469;&#20415;&#25463;&#30340;&#20132;&#36890;&#26381;&#21153;&#12290;&#30446;&#21069;&#22823;&#37096;&#20998;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#27973;&#23618;&#24314;&#27169;&#22797;&#26434;&#30340;&#36947;&#36335;&#32593;&#32476;&#36827;&#34892;&#20132;&#36890;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#22312;&#23436;&#20840;&#25429;&#25417;&#36947;&#36335;&#32593;&#32476;&#25299;&#25169;&#24341;&#36215;&#30340;&#39640;&#38454;&#31354;&#38388;&#20381;&#36182;&#24615;&#21644;&#20132;&#36890;&#21160;&#24577;&#24341;&#36215;&#30340;&#39640;&#38454;&#26102;&#31354;&#20381;&#36182;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#25105;&#20204;&#20851;&#27880;&#20132;&#36890;&#31995;&#32479;&#30340;&#26412;&#36136;&#65292;&#25552;&#20986;&#20102;STHODE&#65288;Spatio-Temporal Hypergraph Neural Ordinary Differential Equation Network&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#36947;&#36335;&#32593;&#32476;&#25299;&#25169;&#21644;&#20132;&#36890;&#21160;&#24577;&#65292;&#20197;&#25429;&#25417;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#39640;&#38454;&#31354;&#38388;-&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#20174;&#25216;&#26415;&#19978;&#35762;&#65292;STHODE&#30001;&#19968;&#20010;&#31354;&#38388;&#37096;&#20998;&#21644;&#19968;&#20010;&#26102;&#38388;&#37096;&#20998;&#32452;&#25104;&#65292;&#22312;&#39640;&#38454;&#31354;&#38388;&#20381;&#36182;&#24615;&#24314;&#27169;&#26041;&#38754;&#20351;&#29992;&#36229;&#22270;&#34920;&#31034;&#36947;&#36335;&#32593;&#32476;&#25299;&#25169;&#65292;&#21516;&#26102;&#20351;&#29992;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#32593;&#32476;&#26469;&#24314;&#27169;&#39640;&#38454;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic forecasting, which benefits from mobile Internet development and position technologies, plays a critical role in Intelligent Transportation Systems. It helps to implement rich and varied transportation applications and bring convenient transportation services to people based on collected traffic data. Most existing methods usually leverage graph-based deep learning networks to model the complex road network for traffic forecasting shallowly. Despite their effectiveness, these methods are generally limited in fully capturing high-order spatial dependencies caused by road network topology and high-order temporal dependencies caused by traffic dynamics. To tackle the above issues, we focus on the essence of traffic system and propose STHODE: Spatio-Temporal Hypergraph Neural Ordinary Differential Equation Network, which combines road network topology and traffic dynamics to capture high-order spatio-temporal dependencies in traffic data. Technically, STHODE consists of a spatial m
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#31215;&#20998;&#21464;&#25442;-&#36229;&#32500;&#21464;&#25442;&#65292;&#23427;&#23558;&#20989;&#25968;&#36716;&#25442;&#20026;&#22122;&#22768;&#40065;&#26834;&#12289;&#20840;&#24687;&#12289;&#39640;&#32500;&#34920;&#31034;&#30340;&#36229;&#32500;&#21521;&#37327;&#65292;&#19982;&#20854;&#20182;&#31215;&#20998;&#21464;&#25442;&#32039;&#23494;&#30456;&#20851;&#65292;&#24182;&#20026;&#36229;&#32500;&#39046;&#22495;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#21644;&#26032;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2310.16065</link><description>&lt;p&gt;
&#36229;&#32500;&#21464;&#25442;&#65306;&#20989;&#25968;&#30340;&#20840;&#24687;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
The Hyperdimensional Transform: a Holographic Representation of Functions. (arXiv:2310.16065v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16065
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#31215;&#20998;&#21464;&#25442;-&#36229;&#32500;&#21464;&#25442;&#65292;&#23427;&#23558;&#20989;&#25968;&#36716;&#25442;&#20026;&#22122;&#22768;&#40065;&#26834;&#12289;&#20840;&#24687;&#12289;&#39640;&#32500;&#34920;&#31034;&#30340;&#36229;&#32500;&#21521;&#37327;&#65292;&#19982;&#20854;&#20182;&#31215;&#20998;&#21464;&#25442;&#32039;&#23494;&#30456;&#20851;&#65292;&#24182;&#20026;&#36229;&#32500;&#39046;&#22495;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#21644;&#26032;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31215;&#20998;&#21464;&#25442;&#26159;&#23558;&#20989;&#25968;&#26144;&#23556;&#21040;&#26356;&#23481;&#26131;&#34920;&#24449;&#30340;&#31354;&#38388;&#20013;&#30340;&#23453;&#36149;&#25968;&#23398;&#24037;&#20855;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36229;&#32500;&#21464;&#25442;&#20316;&#20026;&#19968;&#31181;&#26032;&#22411;&#30340;&#31215;&#20998;&#21464;&#25442;&#12290;&#23427;&#23558;&#21487;&#31215;&#20989;&#25968;&#36716;&#25442;&#20026;&#22122;&#22768;&#40065;&#26834;&#12289;&#20840;&#24687;&#12289;&#39640;&#32500;&#34920;&#31034;&#65292;&#31216;&#20026;&#36229;&#32500;&#21521;&#37327;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#29992;&#38543;&#26426;&#20989;&#25968;&#30340;&#32447;&#24615;&#32452;&#21512;&#26469;&#36924;&#36817;&#19968;&#20010;&#20989;&#25968;&#12290;&#25105;&#20204;&#27491;&#24335;&#24341;&#20837;&#20102;&#19968;&#32452;&#38543;&#26426;&#30340;&#27491;&#20132;&#22522;&#20989;&#25968;&#65292;&#24182;&#23450;&#20041;&#20102;&#36229;&#32500;&#21464;&#25442;&#21450;&#20854;&#36870;&#21464;&#25442;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#33324;&#21464;&#25442;&#30456;&#20851;&#30340;&#24615;&#36136;&#65292;&#22914;&#20854;&#21807;&#19968;&#24615;&#12289;&#36870;&#21464;&#25442;&#30340;&#36924;&#36817;&#24615;&#36136;&#20197;&#21450;&#31215;&#20998;&#21644;&#23548;&#25968;&#30340;&#34920;&#31034;&#12290;&#36229;&#32500;&#21464;&#25442;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#32780;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#19982;&#20613;&#37324;&#21494;&#12289;&#25289;&#26222;&#25289;&#26031;&#21644;&#27169;&#31946;&#21464;&#25442;&#31561;&#20854;&#20182;&#31215;&#20998;&#21464;&#25442;&#23494;&#20999;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#23427;&#20026;&#36229;&#32500;&#39046;&#22495;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#21644;&#26032;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integral transforms are invaluable mathematical tools to map functions into spaces where they are easier to characterize. We introduce the hyperdimensional transform as a new kind of integral transform. It converts square-integrable functions into noise-robust, holographic, high-dimensional representations called hyperdimensional vectors. The central idea is to approximate a function by a linear combination of random functions. We formally introduce a set of stochastic, orthogonal basis functions and define the hyperdimensional transform and its inverse. We discuss general transform-related properties such as its uniqueness, approximation properties of the inverse transform, and the representation of integrals and derivatives. The hyperdimensional transform offers a powerful, flexible framework that connects closely with other integral transforms, such as the Fourier, Laplace, and fuzzy transforms. Moreover, it provides theoretical foundations and new insights for the field of hyperdim
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21487;&#23398;&#20064;&#28388;&#27874;&#27169;&#22359;&#26469;&#36866;&#24212;&#24615;&#22320;&#36807;&#28388;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20613;&#37324;&#21494;&#21464;&#25442;&#23558;&#25968;&#25454;&#36716;&#25442;&#20026;&#39057;&#22495;&#65292;&#28982;&#21518;&#26681;&#25454;&#22122;&#22768;&#30340;&#27169;&#24335;&#36827;&#34892;&#28388;&#27874;&#65292;&#26368;&#21518;&#20877;&#36890;&#36807;&#36870;&#20613;&#37324;&#21494;&#21464;&#25442;&#23558;&#25968;&#25454;&#24674;&#22797;&#21040;&#26102;&#22495;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#21319;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;&#30340;&#36755;&#20837;&#25968;&#25454;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.16063</link><description>&lt;p&gt;
&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#28388;&#27874;&#27169;&#22359;&#25552;&#21319;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhancing Traffic Prediction with Learnable Filter Module. (arXiv:2310.16063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21487;&#23398;&#20064;&#28388;&#27874;&#27169;&#22359;&#26469;&#36866;&#24212;&#24615;&#22320;&#36807;&#28388;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20613;&#37324;&#21494;&#21464;&#25442;&#23558;&#25968;&#25454;&#36716;&#25442;&#20026;&#39057;&#22495;&#65292;&#28982;&#21518;&#26681;&#25454;&#22122;&#22768;&#30340;&#27169;&#24335;&#36827;&#34892;&#28388;&#27874;&#65292;&#26368;&#21518;&#20877;&#36890;&#36807;&#36870;&#20613;&#37324;&#21494;&#21464;&#25442;&#23558;&#25968;&#25454;&#24674;&#22797;&#21040;&#26102;&#22495;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#21319;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;&#30340;&#36755;&#20837;&#25968;&#25454;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#26410;&#26469;&#20132;&#36890;&#26465;&#20214;&#30340;&#24314;&#27169;&#36890;&#24120;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#26102;&#31354;&#31070;&#32463;&#32593;&#32476;&#26469;&#25429;&#25417;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#20294;&#36825;&#21487;&#33021;&#24573;&#35270;&#20102;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#22122;&#22768;&#12290;&#36825;&#31181;&#22122;&#22768;&#36890;&#24120;&#34920;&#29616;&#20026;&#20132;&#36890;&#35266;&#27979;&#20013;&#30340;&#24847;&#22806;&#30701;&#26399;&#27874;&#23792;&#25110;&#27874;&#35895;&#65292;&#36890;&#24120;&#26159;&#30001;&#20132;&#36890;&#20107;&#25925;&#25110;&#20256;&#24863;&#22120;&#22266;&#26377;&#25391;&#21160;&#23548;&#33268;&#30340;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#20854;&#38543;&#26426;&#24615;&#65292;&#36825;&#31181;&#22122;&#22768;&#24456;&#38590;&#24314;&#27169;&#65292;&#24182;&#19988;&#22914;&#26524;&#31070;&#32463;&#32593;&#32476;&#34987;&#35774;&#35745;&#25104;&#23398;&#20064;&#36825;&#31181;&#34892;&#20026;&#65292;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#28388;&#27874;&#27169;&#22359;&#65292;&#26469;&#33258;&#36866;&#24212;&#24615;&#22320;&#36807;&#28388;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#12290;&#35813;&#27169;&#22359;&#21033;&#29992;&#20613;&#37324;&#21494;&#21464;&#25442;&#23558;&#25968;&#25454;&#36716;&#25442;&#20026;&#39057;&#22495;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#26681;&#25454;&#22122;&#22768;&#30340;&#27169;&#24335;&#36827;&#34892;&#28388;&#27874;&#12290;&#32463;&#36807;&#21435;&#22122;&#22788;&#29702;&#21518;&#30340;&#25968;&#25454;&#20877;&#36890;&#36807;&#36870;&#20613;&#37324;&#21494;&#21464;&#25442;&#24674;&#22797;&#21040;&#26102;&#22495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30528;&#37325;&#25552;&#21319;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;&#30340;&#36755;&#20837;&#25968;&#25454;&#36136;&#37327;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#20294;&#24120;&#34987;&#24573;&#35270;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling future traffic conditions often relies heavily on complex spatial-temporal neural networks to capture spatial and temporal correlations, which can overlook the inherent noise in the data. This noise, often manifesting as unexpected short-term peaks or drops in traffic observation, is typically caused by traffic accidents or inherent sensor vibration. In practice, such noise can be challenging to model due to its stochastic nature and can lead to overfitting risks if a neural network is designed to learn this behavior. To address this issue, we propose a learnable filter module to filter out noise in traffic data adaptively. This module leverages the Fourier transform to convert the data to the frequency domain, where noise is filtered based on its pattern. The denoised data is then recovered to the time domain using the inverse Fourier transform. Our approach focuses on enhancing the quality of the input data for traffic prediction models, which is a critical yet often overloo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#39046;&#22495;&#36866;&#24212;&#19982;&#28151;&#28102;&#24179;&#34913;&#65288;ADA-CBF&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39044;&#35757;&#32451;&#22823;&#27169;&#22411;&#24494;&#35843;&#20013;&#32771;&#34385;&#28151;&#28102;&#22240;&#32032;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#39044;&#35757;&#32451;&#22823;&#27169;&#22411;&#12289;&#39046;&#22495;&#20998;&#31867;&#22120;&#21644;&#28151;&#28102;&#20998;&#31867;&#22120;&#65292;&#24182;&#20351;&#29992;&#23545;&#25239;&#25439;&#22833;&#26469;&#25913;&#21892;&#39046;&#22495;&#19981;&#21464;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#24179;&#34913;&#28151;&#28102;&#22240;&#23376;&#30340;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2310.16062</link><description>&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#22823;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#23545;&#25239;&#39046;&#22495;&#36866;&#24212;&#20013;&#36827;&#34892;&#28151;&#28102;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Confounder Balancing in Adversarial Domain Adaptation for Pre-Trained Large Models Fine-Tuning. (arXiv:2310.16062v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#39046;&#22495;&#36866;&#24212;&#19982;&#28151;&#28102;&#24179;&#34913;&#65288;ADA-CBF&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39044;&#35757;&#32451;&#22823;&#27169;&#22411;&#24494;&#35843;&#20013;&#32771;&#34385;&#28151;&#28102;&#22240;&#32032;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#39044;&#35757;&#32451;&#22823;&#27169;&#22411;&#12289;&#39046;&#22495;&#20998;&#31867;&#22120;&#21644;&#28151;&#28102;&#20998;&#31867;&#22120;&#65292;&#24182;&#20351;&#29992;&#23545;&#25239;&#25439;&#22833;&#26469;&#25913;&#21892;&#39046;&#22495;&#19981;&#21464;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#24179;&#34913;&#28151;&#28102;&#22240;&#23376;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22823;&#27169;&#22411;&#65288;PLMs&#65289;&#20855;&#26377;&#20986;&#33394;&#30340;&#27867;&#21270;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#20986;&#29616;&#33021;&#21147;&#65292;&#21487;&#20197;&#22788;&#29702;&#27809;&#26377;&#30452;&#25509;&#35757;&#32451;&#25968;&#25454;&#30340;&#29305;&#23450;&#20219;&#21153;&#65292;&#20351;&#23427;&#20204;&#25104;&#20026;&#23545;&#25239;&#39046;&#22495;&#36866;&#24212;&#65288;ADA&#65289;&#26041;&#27861;&#20013;&#23558;&#20174;&#28304;&#39046;&#22495;&#23398;&#20064;&#30340;&#30693;&#35782;&#36716;&#31227;&#33267;&#30446;&#26631;&#39046;&#22495;&#30340;&#26356;&#22909;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;ADA&#26041;&#27861;&#26410;&#33021;&#27491;&#30830;&#32771;&#34385;&#28151;&#28102;&#22240;&#32032;&#65292;&#28151;&#28102;&#22240;&#32032;&#26159;&#23548;&#33268;&#28304;&#25968;&#25454;&#20998;&#24067;&#19982;&#30446;&#26631;&#39046;&#22495;&#19981;&#21516;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;PLMs&#24494;&#35843;&#30340;&#23545;&#25239;&#39046;&#22495;&#36866;&#24212;&#19982;&#28151;&#28102;&#24179;&#34913;&#65288;ADA-CBF&#65289;&#26041;&#27861;&#12290;ADA-CBF&#21253;&#25324;&#19968;&#20010;PLM&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#21450;&#19968;&#20010;&#39046;&#22495;&#20998;&#31867;&#22120;&#21644;&#19968;&#20010;&#28151;&#28102;&#20998;&#31867;&#22120;&#65292;&#23427;&#20204;&#36890;&#36807;&#23545;&#25239;&#25439;&#22833;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#12290;&#36825;&#20010;&#25439;&#22833;&#26088;&#22312;&#36890;&#36807;&#20943;&#24369;&#39046;&#22495;&#20998;&#31867;&#22120;&#20013;&#30340;&#27495;&#35270;&#26469;&#25913;&#36827;&#39046;&#22495;&#19981;&#21464;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;&#21516;&#26102;&#65292;&#23545;&#25239;&#25439;&#22833;&#20063;&#24179;&#34913;&#28151;&#28102;&#22240;&#23376;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
The excellent generalization, contextual learning, and emergence abilities in the pre-trained large models (PLMs) handle specific tasks without direct training data, making them the better foundation models in the adversarial domain adaptation (ADA) methods to transfer knowledge learned from the source domain to target domains. However, existing ADA methods fail to account for the confounder properly, which is the root cause of the source data distribution that differs from the target domains. This study proposes an adversarial domain adaptation with confounder balancing for PLMs fine-tuning (ADA-CBF). The ADA-CBF includes a PLM as the foundation model for a feature extractor, a domain classifier and a confounder classifier, and they are jointly trained with an adversarial loss. This loss is designed to improve the domain-invariant representation learning by diluting the discrimination in the domain classifier. At the same time, the adversarial loss also balances the confounder distrib
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21046;&#36896;&#31995;&#32479;&#25925;&#38556;&#35786;&#26029;&#30340;&#26032;&#26041;&#27861;&#65306;&#31354;&#38388;&#30456;&#20851;&#31232;&#30095;&#36125;&#21494;&#26031;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#21487;&#24212;&#23545;&#20256;&#24863;&#22120;&#25968;&#37327;&#26377;&#38480;&#21644;&#38750;&#24179;&#31283;&#31354;&#38388;&#30456;&#20851;&#25925;&#38556;&#31561;&#25361;&#25112;&#65292;&#24182;&#22312;&#22810;&#31449;&#35013;&#37197;&#31995;&#32479;&#20013;&#23637;&#31034;&#20102;&#20854;&#24212;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16058</link><description>&lt;p&gt;
&#19968;&#31181;&#24212;&#29992;&#20110;&#22810;&#31449;&#35013;&#37197;&#31995;&#32479;&#30340;&#31232;&#30095;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#24179;&#31283;&#21644;&#31354;&#38388;&#30456;&#20851;&#25925;&#38556;&#30340;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
A Sparse Bayesian Learning for Diagnosis of Nonstationary and Spatially Correlated Faults with Application to Multistation Assembly Systems. (arXiv:2310.16058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21046;&#36896;&#31995;&#32479;&#25925;&#38556;&#35786;&#26029;&#30340;&#26032;&#26041;&#27861;&#65306;&#31354;&#38388;&#30456;&#20851;&#31232;&#30095;&#36125;&#21494;&#26031;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#21487;&#24212;&#23545;&#20256;&#24863;&#22120;&#25968;&#37327;&#26377;&#38480;&#21644;&#38750;&#24179;&#31283;&#31354;&#38388;&#30456;&#20851;&#25925;&#38556;&#31561;&#25361;&#25112;&#65292;&#24182;&#22312;&#22810;&#31449;&#35013;&#37197;&#31995;&#32479;&#20013;&#23637;&#31034;&#20102;&#20854;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#24863;&#22120;&#25216;&#26415;&#30340;&#21457;&#23637;&#20026;&#21046;&#36896;&#31995;&#32479;&#20013;&#30340;&#25925;&#38556;&#35786;&#26029;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29289;&#29702;&#38480;&#21046;&#25110;&#19981;&#24517;&#35201;&#30340;&#25104;&#26412;&#65292;&#20256;&#24863;&#22120;&#25968;&#37327;&#26377;&#38480;&#65292;&#36825;&#24433;&#21709;&#20102;&#23454;&#38469;&#36807;&#31243;&#20013;&#30340;&#20934;&#30830;&#35786;&#26029;&#12290;&#27492;&#22806;&#65292;&#29983;&#25104;&#38750;&#24179;&#31283;&#36807;&#31243;&#25925;&#38556;&#20197;&#21450;&#36807;&#31243;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#38656;&#35201;&#32771;&#34385;&#21046;&#36896;&#31995;&#32479;&#20013;&#20934;&#30830;&#25925;&#38556;&#35786;&#26029;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#65306;&#31354;&#38388;&#30456;&#20851;&#31232;&#30095;&#36125;&#21494;&#26031;&#23398;&#20064;&#65288;CSSBL&#65289;&#65292;&#24182;&#26126;&#30830;&#23637;&#31034;&#20102;&#20854;&#22312;&#23481;&#26131;&#21463;&#21040;&#19978;&#36848;&#25361;&#25112;&#30340;&#22810;&#31449;&#35013;&#37197;&#31995;&#32479;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#23454;&#29992;&#20551;&#35774;&#65292;&#21363;&#23427;&#21487;&#33021;&#26377;&#20960;&#20010;&#36807;&#31243;&#25925;&#38556;&#65288;&#31232;&#30095;&#65289;&#12290;&#27492;&#22806;&#65292;CSSBL&#30340;&#20998;&#23618;&#32467;&#26500;&#20855;&#26377;&#22810;&#20010;&#21442;&#25968;&#21270;&#20808;&#39564;&#20998;&#24067;&#20197;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#12290;&#30001;&#20110;&#36807;&#31243;&#25925;&#38556;&#30340;&#21518;&#39564;&#20998;&#24067;&#27809;&#26377;
&lt;/p&gt;
&lt;p&gt;
Sensor technology developments provide a basis for effective fault diagnosis in manufacturing systems. However, the limited number of sensors due to physical constraints or undue costs hinders the accurate diagnosis in the actual process. In addition, time-varying operational conditions that generate nonstationary process faults and the correlation information in the process require to consider for accurate fault diagnosis in the manufacturing systems. This article proposes a novel fault diagnosis method: clustering spatially correlated sparse Bayesian learning (CSSBL), and explicitly demonstrates its applicability in a multistation assembly system that is vulnerable to the above challenges. Specifically, the method is based on a practical assumption that it will likely have a few process faults (sparse). In addition, the hierarchical structure of CSSBL has several parameterized prior distributions to address the above challenges. As posterior distributions of process faults do not hav
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#22522;&#20110;&#22810;&#36890;&#36947;&#32418;&#22806;&#21355;&#26143;&#35266;&#27979;&#25968;&#25454;&#30340;&#21487;&#35299;&#37322;&#24615;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#22823;&#27668;&#23545;&#27969;&#36215;&#22987;&#12290;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#65292;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#23545;&#20113;&#23618;&#21644;&#28287;&#24230;&#29305;&#24615;&#30340;&#20381;&#36182;&#24615;&#65292;&#24182;&#22312;&#35823;&#25253;&#29575;&#19978;&#26174;&#33879;&#20248;&#20110;&#32463;&#20856;&#30340;&#36923;&#36753;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.16015</link><description>&lt;p&gt;
&#20351;&#29992;GOES-16&#21355;&#26143;&#35266;&#27979;&#25968;&#25454;&#30340;&#21487;&#35299;&#37322;&#24615;&#28145;&#24230;&#23398;&#20064;&#23545;&#22823;&#27668;&#23545;&#27969;&#36215;&#22987;&#36827;&#34892;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Physically Explainable Deep Learning for Convective Initiation Nowcasting Using GOES-16 Satellite Observations. (arXiv:2310.16015v1 [physics.ao-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#22522;&#20110;&#22810;&#36890;&#36947;&#32418;&#22806;&#21355;&#26143;&#35266;&#27979;&#25968;&#25454;&#30340;&#21487;&#35299;&#37322;&#24615;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#22823;&#27668;&#23545;&#27969;&#36215;&#22987;&#12290;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#65292;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#23545;&#20113;&#23618;&#21644;&#28287;&#24230;&#29305;&#24615;&#30340;&#20381;&#36182;&#24615;&#65292;&#24182;&#22312;&#35823;&#25253;&#29575;&#19978;&#26174;&#33879;&#20248;&#20110;&#32463;&#20856;&#30340;&#36923;&#36753;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#27668;&#23545;&#27969;&#36215;&#22987;&#65288;CI&#65289;&#30340;&#39044;&#27979;&#22312;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#21644;&#29616;&#26377;&#30340;&#39044;&#27979;&#31639;&#27861;&#20013;&#20173;&#28982;&#26159;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#22522;&#20110;&#22810;&#36890;&#36947;&#32418;&#22806;GOES-R&#21355;&#26143;&#35266;&#27979;&#25968;&#25454;&#30340;&#22522;&#20110;&#23545;&#35937;&#30340;&#27010;&#29575;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;CI&#12290;&#25968;&#25454;&#26469;&#33258;&#20110;2020&#24180;6&#26376;&#21644;7&#26376;&#20197;&#21450;2021&#24180;6&#26376;&#22312;&#32654;&#22269;&#22823;&#24179;&#21407;&#22320;&#21306;&#22810;&#38647;&#36798;&#22810;&#20256;&#24863;&#22120;&#22810;&#26222;&#21202;&#22825;&#27668;&#38647;&#36798;&#20135;&#21697;&#20013;&#35782;&#21035;&#20986;&#30340;&#28508;&#22312;CI&#20107;&#20214;&#21608;&#22260;&#30340;&#34917;&#19969;&#12290;&#20351;&#29992;&#23458;&#35266;&#30340;&#22522;&#20110;&#38647;&#36798;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#36825;&#20123;&#20107;&#20214;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#25552;&#21069;&#26102;&#38388;&#20026;1&#23567;&#26102;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#20248;&#20110;&#32463;&#20856;&#30340;&#36923;&#36753;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#35823;&#25253;&#29575;&#19978;&#12290;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23637;&#31034;&#20102;&#23545;&#20113;&#23618;&#21644;&#22810;&#23618;&#27425;&#28287;&#24230;&#29305;&#24615;&#30340;&#20381;&#36182;&#24615;&#12290;&#27169;&#22411;&#35299;&#37322;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#22522;&#32447;&#19979;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#35299;&#37322;&#32467;&#26524;&#31361;&#20986;&#20102;&#28287;&#24230;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convection initiation (CI) nowcasting remains a challenging problem for both numerical weather prediction models and existing nowcasting algorithms. In this study, object-based probabilistic deep learning models are developed to predict CI based on multichannel infrared GOES-R satellite observations. The data come from patches surrounding potential CI events identified in Multi-Radar Multi-Sensor Doppler weather radar products over the Great Plains region from June and July 2020 and June 2021. An objective radar-based approach is used to identify these events. The deep learning models significantly outperform the classical logistic model at lead times up to 1 hour, especially on the false alarm ratio. Through case studies, the deep learning model exhibits the dependence on the characteristics of clouds and moisture at multiple levels. Model explanation further reveals the model's decision-making process with different baselines. The explanation results highlight the importance of moist
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#19987;&#38376;&#21475;&#38899;&#20195;&#30721;&#26412;&#30340;&#21475;&#38899;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#21487;&#35757;&#32451;&#20195;&#30721;&#26412;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;ASR&#31995;&#32479;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#24050;&#35265;&#21644;&#26410;&#35265;&#30340;&#21475;&#38899;&#19978;&#37117;&#33021;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.15970</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#19987;&#38376;&#21475;&#38899;&#20195;&#30721;&#26412;&#30340;&#21475;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Accented Speech Recognition With Accent-specific Codebooks. (arXiv:2310.15970v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#19987;&#38376;&#21475;&#38899;&#20195;&#30721;&#26412;&#30340;&#21475;&#38899;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#21487;&#35757;&#32451;&#20195;&#30721;&#26412;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;ASR&#31995;&#32479;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#24050;&#35265;&#21644;&#26410;&#35265;&#30340;&#21475;&#38899;&#19978;&#37117;&#33021;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21475;&#38899;&#23545;&#20110;&#29616;&#26377;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#22312;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#21475;&#38899;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#20005;&#37325;&#38459;&#30861;&#20102;ASR&#30340;&#26222;&#21450;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21475;&#38899;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#21487;&#35757;&#32451;&#20195;&#30721;&#26412;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;ASR&#31995;&#32479;&#12290;&#36825;&#20123;&#21487;&#23398;&#20064;&#30340;&#20195;&#30721;&#26412;&#25429;&#25417;&#20102;&#21475;&#38899;&#29305;&#23450;&#20449;&#24687;&#65292;&#24182;&#34987;&#25972;&#21512;&#21040;ASR&#32534;&#30721;&#22120;&#23618;&#20013;&#12290;&#27169;&#22411;&#22312;&#24102;&#21475;&#38899;&#30340;&#33521;&#35821;&#35821;&#38899;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#27979;&#35797;&#25968;&#25454;&#20013;&#20063;&#21253;&#21547;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#21475;&#38899;&#12290;&#22312;Mozilla Common Voice&#22810;&#21475;&#38899;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19981;&#20165;&#22312;&#24050;&#35265;&#30340;&#33521;&#35821;&#21475;&#38899;&#20013;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65288;&#21333;&#35789;&#38169;&#35823;&#29575;&#30456;&#23545;&#25552;&#21319;&#39640;&#36798;37%&#65289;&#65292;&#32780;&#19988;&#22312;&#26410;&#35265;&#30340;&#21475;&#38899;&#19978;&#20063;&#33719;&#24471;&#20102;5%&#30340;&#30456;&#23545;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;L2Artic&#25968;&#25454;&#38598;&#19978;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#35774;&#32622;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23545;&#27604;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech accents pose a significant challenge to state-of-the-art automatic speech recognition (ASR) systems. Degradation in performance across underrepresented accents is a severe deterrent to the inclusive adoption of ASR. In this work, we propose a novel accent adaptation approach for end-to-end ASR systems using cross-attention with a trainable set of codebooks. These learnable codebooks capture accent-specific information and are integrated within the ASR encoder layers. The model is trained on accented English speech, while the test data also contained accents which were not seen during training. On the Mozilla Common Voice multi-accented dataset, we show that our proposed approach yields significant performance gains not only on the seen English accents (up to $37\%$ relative improvement in word error rate) but also on the unseen accents (up to $5\%$ relative improvement in WER). Further, we illustrate benefits for a zero-shot transfer setup on the L2Artic dataset. We also compare
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#25442;&#22120;&#21644;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#25913;&#21892;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#23545;&#23454;&#38469;&#24212;&#29992;&#20013;&#24120;&#35265;&#25104;&#20687;&#21464;&#24322;&#24615;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15952</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#24341;&#23548;&#25193;&#25955;&#21644;&#23884;&#22871;&#38598;&#25104;&#25913;&#36827;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Robustness and Reliability in Medical Image Classification with Latent-Guided Diffusion and Nested-Ensembles. (arXiv:2310.15952v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#25442;&#22120;&#21644;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#25913;&#21892;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#23545;&#23454;&#38469;&#24212;&#29992;&#20013;&#24120;&#35265;&#25104;&#20687;&#21464;&#24322;&#24615;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#30495;&#23454;&#20020;&#24202;&#29615;&#22659;&#20013;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#23427;&#20204;&#23545;&#25152;&#33719;&#21462;&#30340;&#22270;&#20687;&#30340;&#21464;&#24322;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#35768;&#22810;&#26041;&#27861;&#20250;&#23545;&#35757;&#32451;&#25968;&#25454;&#24212;&#29992;&#39044;&#23450;&#20041;&#30340;&#36716;&#25442;&#65292;&#20197;&#22686;&#24378;&#27979;&#35797;&#26102;&#30340;&#40065;&#26834;&#24615;&#65292;&#20294;&#36825;&#20123;&#36716;&#25442;&#21487;&#33021;&#26080;&#27861;&#30830;&#20445;&#27169;&#22411;&#23545;&#24739;&#32773;&#22270;&#20687;&#20013;&#30340;&#22810;&#26679;&#24615;&#21464;&#24322;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#25442;&#22120;&#21644;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#22411;&#19977;&#38454;&#27573;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#23545;&#23454;&#36341;&#20013;&#24120;&#35265;&#30340;&#25104;&#20687;&#21464;&#24322;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#39044;&#20808;&#30830;&#23450;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#22810;&#20010;&#22270;&#20687;&#32534;&#30721;&#22120;&#39318;&#20808;&#23398;&#20064;&#20998;&#23618;&#29305;&#24449;&#34920;&#31034;&#26469;&#26500;&#24314;&#36776;&#21035;&#28508;&#22312;&#31354;&#38388;&#12290;&#25509;&#19979;&#26469;&#65292;&#19968;&#20010;&#30001;&#28508;&#22312;&#20195;&#30721;&#24341;&#23548;&#30340;&#36870;&#25193;&#25955;&#36807;&#31243;&#20316;&#29992;&#20110;&#26377;&#20449;&#24687;&#20808;&#39564;&#65292;&#24182;&#25552;&#20986;&#39044;&#27979;&#20505;&#36873;&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep learning models have achieved remarkable success across a range of medical image analysis tasks, deployment of these models in real clinical contexts requires that they be robust to variability in the acquired images. While many methods apply predefined transformations to augment the training data to enhance test-time robustness, these transformations may not ensure the model's robustness to the diverse variability seen in patient images. In this paper, we introduce a novel three-stage approach based on transformers coupled with conditional diffusion models, with the goal of improving model robustness to the kinds of imaging variability commonly encountered in practice without the need for pre-determined data augmentation strategies. To this end, multiple image encoders first learn hierarchical feature representations to build discriminative latent spaces. Next, a reverse diffusion process, guided by the latent code, acts on an informative prior and proposes prediction candi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#39640;&#32500;&#20581;&#22766;&#22343;&#20540;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.15932</link><description>&lt;p&gt;
&#22312;&#32447;&#20581;&#22766;&#22343;&#20540;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Online Robust Mean Estimation. (arXiv:2310.15932v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#39640;&#32500;&#20581;&#22766;&#22343;&#20540;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#20013;&#39640;&#32500;&#20581;&#22766;&#22343;&#20540;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#22330;&#26223;&#65292;&#22312;&#36825;&#20010;&#22330;&#26223;&#20013;&#65292;n&#20010;&#20256;&#24863;&#22120;&#27491;&#22312;&#27979;&#37327;&#26576;&#20010;&#20849;&#21516;&#30340;&#25345;&#32493;&#29616;&#35937;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;t=1,2,...,T&#65292;&#31532;i&#20010;&#20256;&#24863;&#22120;&#25253;&#21578;&#20854;&#22312;&#35813;&#26102;&#38388;&#27493;&#30340;&#35835;&#25968;x^(i)_t&#12290;&#28982;&#21518;&#65292;&#31639;&#27861;&#24517;&#39035;&#23545;&#35813;&#26102;&#21051;&#30340;&#30495;&#23454;&#22343;&#20540;&#956;_t&#36827;&#34892;&#20272;&#35745;&#12290;&#25105;&#20204;&#20551;&#35774;&#22823;&#37096;&#20998;&#20256;&#24863;&#22120;&#35266;&#27979;&#21040;&#20102;&#26469;&#33258;&#26576;&#20010;&#20844;&#20849;&#20998;&#24067;X&#30340;&#29420;&#31435;&#26679;&#26412;&#65292;&#20294;&#26159;&#20854;&#20013;&#19968;&#20010;&#949;&#20998;&#25968;&#30340;&#20256;&#24863;&#22120;&#21487;&#33021;&#34920;&#29616;&#20986;&#24694;&#24847;&#34892;&#20026;&#12290;&#31639;&#27861;&#24076;&#26395;&#35745;&#31639;&#20986;&#23545;&#30495;&#23454;&#22343;&#20540;&#956;*:=E[X]&#30340;&#33391;&#22909;&#36817;&#20284;&#20540;&#956;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#22914;&#26524;&#20801;&#35768;&#31639;&#27861;&#31561;&#24453;&#21040;&#26102;&#38388;T&#25165;&#25253;&#21578;&#20854;&#20272;&#35745;&#20540;&#65292;&#37027;&#20040;&#36825;&#23601;&#21464;&#25104;&#20102;&#19968;&#20010;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#20581;&#22766;&#22343;&#20540;&#20272;&#35745;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#35201;&#27714;&#22312;&#25968;&#25454;&#36827;&#26469;&#26102;&#29983;&#25104;&#37096;&#20998;&#20272;&#35745;&#20540;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20351;&#24773;&#20917;&#21464;&#24471;&#22797;&#26434;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20851;&#20110;&#36825;&#20010;&#38382;&#39064;&#30340;&#20004;&#20010;&#20027;&#35201;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of high-dimensional robust mean estimation in an online setting. Specifically, we consider a scenario where $n$ sensors are measuring some common, ongoing phenomenon. At each time step $t=1,2,\ldots,T$, the $i^{th}$ sensor reports its readings $x^{(i)}_t$ for that time step. The algorithm must then commit to its estimate $\mu_t$ for the true mean value of the process at time $t$. We assume that most of the sensors observe independent samples from some common distribution $X$, but an $\epsilon$-fraction of them may instead behave maliciously. The algorithm wishes to compute a good approximation $\mu$ to the true mean $\mu^\ast := \mathbf{E}[X]$. We note that if the algorithm is allowed to wait until time $T$ to report its estimate, this reduces to the well-studied problem of robust mean estimation. However, the requirement that our algorithm produces partial estimates as the data is coming in substantially complicates the situation.  We prove two main results about 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;COPF&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26159;&#20351;&#29992;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#21644;&#20989;&#25968;&#27491;&#21017;&#21270;&#26469;&#25345;&#32493;&#23398;&#20064;&#21644;&#36866;&#24212;&#20154;&#31867;&#20559;&#22909;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.15694</link><description>&lt;p&gt;
COPF: &#36890;&#36807;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#20154;&#31867;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
COPF: Continual Learning Human Preference through Optimal Policy Fitting. (arXiv:2310.15694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15694
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;COPF&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26159;&#20351;&#29992;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#21644;&#20989;&#25968;&#27491;&#21017;&#21270;&#26469;&#25345;&#32493;&#23398;&#20064;&#21644;&#36866;&#24212;&#20154;&#31867;&#20559;&#22909;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#25216;&#26415;&#26159;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20197;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#24120;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22522;&#20110;RLHF&#30340;LM&#22312;&#24341;&#20837;&#26032;&#30340;&#26597;&#35810;&#25110;&#21453;&#39304;&#26102;&#38656;&#35201;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#20154;&#31867;&#20559;&#22909;&#22312;&#19981;&#21516;&#39046;&#22495;&#25110;&#20219;&#21153;&#20043;&#38388;&#21487;&#33021;&#20250;&#26377;&#25152;&#21464;&#21270;&#12290;&#30001;&#20110;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#20197;&#21450;&#19982;&#25968;&#25454;&#38544;&#31169;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#37325;&#26032;&#35757;&#32451;LM&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#23384;&#22312;&#23454;&#38469;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#25345;&#32493;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#65288;COPF&#65289;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#27861;&#20272;&#35745;&#19968;&#31995;&#21015;&#26368;&#20248;&#31574;&#30053;&#65292;&#28982;&#21518;&#36890;&#36807;&#20989;&#25968;&#27491;&#21017;&#21270;&#19981;&#26029;&#25311;&#21512;&#31574;&#30053;&#24207;&#21015;&#12290;COPF&#21253;&#21547;&#19968;&#20010;&#21333;&#19968;&#30340;&#23398;&#20064;&#38454;&#27573;&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The technique of Reinforcement Learning from Human Feedback (RLHF) is a commonly employed method to improve pre-trained Language Models (LM), enhancing their ability to conform to human preferences. Nevertheless, the current RLHF-based LMs necessitate full retraining each time novel queries or feedback are introduced, which becomes a challenging task because human preferences can vary between different domains or tasks. Retraining LMs poses practical difficulties in many real-world situations due to the significant time and computational resources required, along with concerns related to data privacy. To address this limitation, we propose a new method called Continual Optimal Policy Fitting (COPF), in which we estimate a series of optimal policies using the Monte Carlo method, and then continually fit the policy sequence with the function regularization. COPF involves a single learning phase and doesn't necessitate complex reinforcement learning. Importantly, it shares the capability 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#37325;&#26032;&#22312;PyTorch&#19978;&#23454;&#29616;&#20102;VMAF&#65292;&#19982;&#26631;&#20934;&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;VMAF&#21333;&#20301;&#19978;&#30340;&#24046;&#24322;&#23567;&#20110;$10^{-2}$&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20102;&#22312;&#20351;&#29992;VMAF&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#26102;&#30340;&#26799;&#24230;&#35745;&#31639;&#65292;&#24182;&#35777;&#26126;&#20351;&#29992;&#35813;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#19981;&#20250;&#23548;&#33268;&#26799;&#24230;&#19981;&#33391;&#12290;</title><link>http://arxiv.org/abs/2310.15578</link><description>&lt;p&gt;
&#22312;PyTorch&#19978;&#37325;&#26032;&#23454;&#29616;&#30340;VMAF&#65306;&#19968;&#20123;&#23454;&#39564;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
VMAF Re-implementation on PyTorch: Some Experimental Results. (arXiv:2310.15578v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15578
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#37325;&#26032;&#22312;PyTorch&#19978;&#23454;&#29616;&#20102;VMAF&#65292;&#19982;&#26631;&#20934;&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;VMAF&#21333;&#20301;&#19978;&#30340;&#24046;&#24322;&#23567;&#20110;$10^{-2}$&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20102;&#22312;&#20351;&#29992;VMAF&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#26102;&#30340;&#26799;&#24230;&#35745;&#31639;&#65292;&#24182;&#35777;&#26126;&#20351;&#29992;&#35813;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#19981;&#20250;&#23548;&#33268;&#26799;&#24230;&#19981;&#33391;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26631;&#20934;&#30340;VMAF&#23454;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;PyTorch&#26694;&#26550;&#23454;&#29616;VMAF&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#36825;&#20010;&#23454;&#29616;&#65292;&#19982;&#26631;&#20934;&#30340;(libvmaf)&#36827;&#34892;&#27604;&#36739;&#65292;VMAF&#21333;&#20301;&#19978;&#30340;&#24046;&#24322;&#23567;&#20110;$10^{-2}$&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20351;&#29992;VMAF&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#26102;&#30340;&#26799;&#24230;&#35745;&#31639;&#65292;&#24182;&#35777;&#26126;&#20351;&#29992;&#35813;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#19981;&#20250;&#23548;&#33268;&#26799;&#24230;&#19981;&#33391;&#12290;
&lt;/p&gt;
&lt;p&gt;
Based on the standard VMAF implementation we propose an implementation of VMAF using PyTorch framework. For this implementation comparisons with the standard (libvmaf) show the discrepancy $\lesssim 10^{-2}$ in VMAF units. We investigate gradients computation when using VMAF as an objective function and demonstrate that training using this function does not result in ill-behaving gradients.
&lt;/p&gt;</description></item><item><title>MGAS&#26159;&#19968;&#20010;&#22810;&#31890;&#24230;&#26550;&#26500;&#25628;&#32034;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#29305;&#23450;&#31890;&#24230;&#32423;&#21035;&#30340;&#31163;&#25955;&#21270;&#20989;&#25968;&#65292;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#21097;&#20313;&#27604;&#20363;&#65292;&#20174;&#32780;&#23454;&#29616;&#21516;&#26102;&#20248;&#21270;&#27169;&#22411;&#22823;&#23567;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15074</link><description>&lt;p&gt;
MGAS: &#22810;&#31890;&#24230;&#26550;&#26500;&#25628;&#32034;&#20197;&#23454;&#29616;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
MGAS: Multi-Granularity Architecture Search for Effective and Efficient Neural Networks. (arXiv:2310.15074v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15074
&lt;/p&gt;
&lt;p&gt;
MGAS&#26159;&#19968;&#20010;&#22810;&#31890;&#24230;&#26550;&#26500;&#25628;&#32034;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#29305;&#23450;&#31890;&#24230;&#32423;&#21035;&#30340;&#31163;&#25955;&#21270;&#20989;&#25968;&#65292;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#21097;&#20313;&#27604;&#20363;&#65292;&#20174;&#32780;&#23454;&#29616;&#21516;&#26102;&#20248;&#21270;&#27169;&#22411;&#22823;&#23567;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#20998;&#26550;&#26500;&#25628;&#32034;(DAS)&#36890;&#36807;&#26102;&#38388;&#39640;&#25928;&#30340;&#33258;&#21160;&#21270;&#25913;&#21464;&#20102;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;(NAS)&#30340;&#26041;&#24335;&#65292;&#20174;&#31163;&#25955;&#20505;&#36873;&#37319;&#26679;&#21644;&#35780;&#20272;&#36716;&#21464;&#20026;&#21487;&#24494;&#20998;&#36229;&#32593;&#32476;&#20248;&#21270;&#21644;&#31163;&#25955;&#21270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DAS&#26041;&#27861;&#35201;&#20040;&#21482;&#36827;&#34892;&#31895;&#31890;&#24230;&#30340;&#25805;&#20316;&#32423;&#25628;&#32034;&#65292;&#35201;&#20040;&#25163;&#21160;&#23450;&#20041;&#21097;&#20313;&#30340;&#32454;&#31890;&#24230;&#30340;&#26680;&#32423;&#21644;&#26435;&#37325;&#32423;&#21333;&#20301;&#30340;&#27604;&#20363;&#65292;&#20174;&#32780;&#26080;&#27861;&#21516;&#26102;&#20248;&#21270;&#27169;&#22411;&#22823;&#23567;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20026;&#20102;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#32780;&#29306;&#29298;&#20102;&#25628;&#32034;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#31890;&#24230;&#26550;&#26500;&#25628;&#32034;(MGAS)&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20840;&#38754;&#32780;&#20869;&#23384;&#39640;&#25928;&#22320;&#25506;&#32034;&#22810;&#31890;&#24230;&#25628;&#32034;&#31354;&#38388;&#65292;&#21457;&#29616;&#26082;&#26377;&#25928;&#21448;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#38024;&#23545;&#27599;&#20010;&#31890;&#24230;&#32423;&#21035;&#30340;&#31163;&#25955;&#21270;&#20989;&#25968;&#65292;&#26681;&#25454;&#19981;&#26029;&#28436;&#21270;&#30340;&#26550;&#26500;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#21097;&#20313;&#30340;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable architecture search (DAS) revolutionizes neural architecture search (NAS) with time-efficient automation, transitioning from discrete candidate sampling and evaluation to differentiable super-net optimization and discretization. However, existing DAS methods either only conduct coarse-grained operation-level search or manually define the remaining ratios for fine-grained kernel-level and weight-level units, which fail to simultaneously optimize model size and model performance. Furthermore, these methods compromise search quality to reduce memory consumption. To tackle these issues, we introduce multi-granularity architecture search (MGAS), a unified framework which aims to comprehensively and memory-efficiently explore the multi-granularity search space to discover both effective and efficient neural networks. Specifically, we learn discretization functions specific to each granularity level to adaptively determine the remaining ratios according to the evolving architec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#20462;&#21098;&#26041;&#27861;MoSo&#65292;&#23427;&#36890;&#36807;&#35780;&#20272;&#26679;&#26412;&#23545;&#26368;&#20248;&#32463;&#39564;&#39118;&#38505;&#30340;&#24433;&#21709;&#26469;&#30830;&#23450;&#27599;&#20010;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#19968;&#38454;&#36817;&#20284;&#22120;&#26469;&#35745;&#31639;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#65292;&#35813;&#36817;&#20284;&#22120;&#21482;&#38656;&#35201;&#26799;&#24230;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2310.14664</link><description>&lt;p&gt;
&#36890;&#36807;&#31227;&#38500;&#21333;&#20010;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Data Pruning via Moving-one-Sample-out. (arXiv:2310.14664v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#20462;&#21098;&#26041;&#27861;MoSo&#65292;&#23427;&#36890;&#36807;&#35780;&#20272;&#26679;&#26412;&#23545;&#26368;&#20248;&#32463;&#39564;&#39118;&#38505;&#30340;&#24433;&#21709;&#26469;&#30830;&#23450;&#27599;&#20010;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#19968;&#38454;&#36817;&#20284;&#22120;&#26469;&#35745;&#31639;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#65292;&#35813;&#36817;&#20284;&#22120;&#21482;&#38656;&#35201;&#26799;&#24230;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#20462;&#21098;&#26041;&#27861;&#31216;&#20026;&#31227;&#38500;&#21333;&#20010;&#26679;&#26412;(MoSo)&#65292;&#26088;&#22312;&#20174;&#35757;&#32451;&#38598;&#20013;&#35782;&#21035;&#24182;&#31227;&#38500;&#26368;&#19981;&#30456;&#20851;&#30340;&#26679;&#26412;&#12290;MoSo&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#35780;&#20272;&#26679;&#26412;&#23545;&#26368;&#20248;&#32463;&#39564;&#39118;&#38505;&#30340;&#24433;&#21709;&#26469;&#30830;&#23450;&#27599;&#20010;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#36890;&#36807;&#34913;&#37327;&#20174;&#35757;&#32451;&#38598;&#20013;&#25490;&#38500;&#19968;&#20010;&#29305;&#23450;&#26679;&#26412;&#26102;&#65292;&#32463;&#39564;&#39118;&#38505;&#30340;&#21464;&#21270;&#31243;&#24230;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#19968;&#38454;&#36817;&#20284;&#22120;&#65292;&#23427;&#20165;&#38656;&#35201;&#26469;&#33258;&#19981;&#21516;&#35757;&#32451;&#38454;&#27573;&#30340;&#26799;&#24230;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#35745;&#31639;&#19978;&#26114;&#36149;&#30340;&#36880;&#20010;&#26679;&#26412;&#37325;&#26032;&#35757;&#32451;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#36817;&#20284;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#65292;&#26799;&#24230;&#19982;&#35757;&#32451;&#38598;&#30340;&#24179;&#22343;&#26799;&#24230;&#19968;&#33268;&#30340;&#26679;&#26412;&#26356;&#20855;&#20449;&#24687;&#37327;&#65292;&#24182;&#19988;&#24212;&#35813;&#33719;&#24471;&#26356;&#39640;&#30340;&#20998;&#25968;&#65292;&#21487;&#20197;&#30452;&#35266;&#22320;&#29702;&#35299;&#20026;&#65306;&#22914;&#26524;&#26469;&#33258;&#29305;&#23450;&#26679;&#26412;&#30340;&#26799;&#24230;&#19982;&#24179;&#22343;&#26799;&#24230;&#21521;&#37327;&#19968;&#33268;&#65292;&#21017;&#24847;&#21619;&#30528;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel data-pruning approach called moving-one-sample-out (MoSo), which aims to identify and remove the least informative samples from the training set. The core insight behind MoSo is to determine the importance of each sample by assessing its impact on the optimal empirical risk. This is achieved by measuring the extent to which the empirical risk changes when a particular sample is excluded from the training set. Instead of using the computationally expensive leaving-one-out-retraining procedure, we propose an efficient first-order approximator that only requires gradient information from different training stages. The key idea behind our approximation is that samples with gradients that are consistently aligned with the average gradient of the training set are more informative and should receive higher scores, which could be intuitively understood as follows: if the gradient from a specific sample is consistent with the average gradient vector, it implies
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#22686;&#24378;&#27880;&#24847;&#21147;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#25805;&#32437;&#27880;&#24847;&#21147;&#27169;&#24335;&#26469;&#22686;&#24378;&#29305;&#27931;&#20234;&#34892;&#20026;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#27745;&#26579;&#29575;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.14480</link><description>&lt;p&gt;
Attention-Enhancing Backdoor Attacks Against BERT-based Models&#65288;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#22686;&#24378;&#27880;&#24847;&#21147;&#30340;&#21518;&#38376;&#25915;&#20987;&#65289;
&lt;/p&gt;
&lt;p&gt;
Attention-Enhancing Backdoor Attacks Against BERT-based Models. (arXiv:2310.14480v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#22686;&#24378;&#27880;&#24847;&#21147;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#25805;&#32437;&#27880;&#24847;&#21147;&#27169;&#24335;&#26469;&#22686;&#24378;&#29305;&#27931;&#20234;&#34892;&#20026;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#27745;&#26579;&#29575;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#8220;&#21518;&#38376;&#25915;&#20987;&#8221;&#21487;&#33021;&#23041;&#32961;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#12290;&#30740;&#31350;&#21518;&#38376;&#25915;&#20987;&#30340;&#31574;&#30053;&#23558;&#26377;&#21161;&#20110;&#20102;&#35299;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#38598;&#20013;&#22312;&#29983;&#25104;&#38544;&#34109;&#30340;&#35302;&#21457;&#35789;&#25110;&#20462;&#25913;&#27169;&#22411;&#26435;&#37325;&#19978;&#12290;&#26412;&#25991;&#30452;&#25509;&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#37096;&#32467;&#26500;&#21644;&#21518;&#38376;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#27931;&#20234;&#27880;&#24847;&#21147;&#25439;&#22833;&#65288;TAL&#65289;&#65292;&#36890;&#36807;&#30452;&#25509;&#25805;&#32437;&#27880;&#24847;&#21147;&#27169;&#24335;&#26469;&#22686;&#24378;&#29305;&#27931;&#20234;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#23427;&#20204;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#27745;&#26579;&#29575;&#12290;&#23427;&#19981;&#20165;&#36866;&#29992;&#20110;&#20256;&#32479;&#30340;&#33039;&#26631;&#31614;&#25915;&#20987;&#65292;&#20063;&#36866;&#29992;&#20110;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#39592;&#24178;&#27169;&#22411;&#65288;BERT&#65292;RoBERTa&#21644;DistilBERT&#65289;&#21644;&#21508;&#31181;&#20219;&#21153;&#65288;&#24773;&#24863;&#20998;&#26512;&#65292;&#26377;&#23475;&#26816;&#27979;&#21644;&#20027;&#39064;&#20998;&#31867;&#65289;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have revealed that \textit{Backdoor Attacks} can threaten the safety of natural language processing (NLP) models. Investigating the strategies of backdoor attacks will help to understand the model's vulnerability. Most existing textual backdoor attacks focus on generating stealthy triggers or modifying model weights. In this paper, we directly target the interior structure of neural networks and the backdoor mechanism. We propose a novel Trojan Attention Loss (TAL), which enhances the Trojan behavior by directly manipulating the attention patterns. Our loss can be applied to different attacking methods to boost their attack efficacy in terms of attack successful rates and poisoning rates. It applies to not only traditional dirty-label attacks, but also the more challenging clean-label attacks. We validate our method on different backbone models (BERT, RoBERTa, and DistilBERT) and various tasks (Sentiment Analysis, Toxic Detection, and Topic Classification).
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21453;&#21521;&#22270;&#21367;&#31215;&#36827;&#34892;&#30340;&#40065;&#26834;&#36328;&#27169;&#24577;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#34920;&#31034;&#36864;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22686;&#21152;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#26377;&#25928;&#20998;&#31163;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.13276</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#22270;&#21367;&#31215;&#23454;&#29616;&#40065;&#26834;&#30340;&#36328;&#27169;&#24577;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
InvGC: Robust Cross-Modal Retrieval by Inverse Graph Convolution. (arXiv:2310.13276v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13276
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#22270;&#21367;&#31215;&#36827;&#34892;&#30340;&#40065;&#26834;&#36328;&#27169;&#24577;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#34920;&#31034;&#36864;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22686;&#21152;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#26377;&#25928;&#20998;&#31163;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36328;&#27169;&#24577;&#26816;&#32034;&#30340;&#37325;&#22823;&#36827;&#23637;&#20027;&#35201;&#26159;&#36890;&#36807;&#35270;&#35273;&#21644;&#35821;&#35328;&#24314;&#27169;&#30340;&#31361;&#30772;&#25512;&#21160;&#30340;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22810;&#27169;&#24577;&#25968;&#25454;&#34920;&#31034;&#24448;&#24448;&#22312;&#26377;&#38480;&#30340;&#20984;&#38181;&#20869;&#32858;&#38598;&#65288;&#20316;&#20026;&#34920;&#31034;&#36864;&#21270;&#38382;&#39064;&#65289;&#65292;&#36825;&#30001;&#20110;&#36825;&#20123;&#34920;&#31034;&#30340;&#19981;&#21487;&#20998;&#31163;&#24615;&#32780;&#38459;&#30861;&#20102;&#26816;&#32034;&#24615;&#33021;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#22810;&#20010;&#36328;&#27169;&#24577;&#22522;&#20934;&#21644;&#26041;&#27861;&#32463;&#39564;&#35777;&#23454;&#20102;&#34920;&#31034;&#36864;&#21270;&#38382;&#39064;&#30340;&#23384;&#22312;&#12290;&#25509;&#19979;&#26469;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;InvGC&#65292;&#23427;&#26159;&#19968;&#31181;&#21463;&#22270;&#21367;&#31215;&#21644;&#24179;&#22343;&#27744;&#21270;&#21551;&#21457;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;InvGC&#22312;&#25968;&#25454;&#38598;&#20013;&#23450;&#20041;&#22270;&#25299;&#25169;&#65292;&#28982;&#21518;&#24212;&#29992;&#22270;&#21367;&#31215;&#20197;&#19968;&#31181;&#20943;&#27861;&#30340;&#26041;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22686;&#21152;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#26377;&#25928;&#22320;&#20998;&#31163;&#34920;&#31034;&#12290;&#20026;&#20102;&#25552;&#39640;InvGC&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#32423;&#22270;&#25299;&#25169;&#65292;Lo
&lt;/p&gt;
&lt;p&gt;
Over recent decades, significant advancements in cross-modal retrieval are mainly driven by breakthroughs in visual and linguistic modeling. However, a recent study shows that multi-modal data representations tend to cluster within a limited convex cone (as representation degeneration problem), which hinders retrieval performance due to the inseparability of these representations. In our study, we first empirically validate the presence of the representation degeneration problem across multiple cross-modal benchmarks and methods. Next, to address it, we introduce a novel method, called InvGC, a post-processing technique inspired by graph convolution and average pooling. Specifically, InvGC defines the graph topology within the datasets and then applies graph convolution in a subtractive manner. This method effectively separates representations by increasing the distances between data points. To improve the efficiency and effectiveness of InvGC, we propose an advanced graph topology, Lo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#38382;&#39064;&#20197;&#21450;&#32531;&#35299;&#24615;&#21035;&#20559;&#35265;&#30340;&#26041;&#27861;&#26469;&#22635;&#34917;&#29616;&#26377;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;&#30740;&#31350;&#21457;&#29616;&#25351;&#23548;&#24494;&#35843;&#27169;&#22411;&#22312;&#40664;&#35748;&#20026;&#30007;&#24615;&#32763;&#35793;&#19978;&#23384;&#22312;&#24615;&#21035;&#20559;&#35265;&#65292;&#21516;&#26102;&#24573;&#35270;&#20102;&#25351;&#31034;&#32844;&#19994;&#24615;&#21035;&#30340;&#20195;&#35789;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#21487;&#34892;&#30340;&#32531;&#35299;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.12127</link><description>&lt;p&gt;
&#20195;&#35789;&#25925;&#20107;&#65306;&#21487;&#35299;&#37322;&#24615;&#25351;&#23548;&#19979;&#30340;&#20844;&#24179;&#25351;&#23548;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation. (arXiv:2310.12127v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#38382;&#39064;&#20197;&#21450;&#32531;&#35299;&#24615;&#21035;&#20559;&#35265;&#30340;&#26041;&#27861;&#26469;&#22635;&#34917;&#29616;&#26377;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;&#30740;&#31350;&#21457;&#29616;&#25351;&#23548;&#24494;&#35843;&#27169;&#22411;&#22312;&#40664;&#35748;&#20026;&#30007;&#24615;&#32763;&#35793;&#19978;&#23384;&#22312;&#24615;&#21035;&#20559;&#35265;&#65292;&#21516;&#26102;&#24573;&#35270;&#20102;&#25351;&#31034;&#32844;&#19994;&#24615;&#21035;&#30340;&#20195;&#35789;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#21487;&#34892;&#30340;&#32531;&#35299;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25351;&#23548;&#24494;&#35843;&#27169;&#22411;&#21487;&#22312;&#22810;&#20010;NLP&#20219;&#21153;&#20013;&#35299;&#20915;&#38382;&#39064;&#65292;&#20854;&#20013;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#26159;&#19968;&#20010;&#31361;&#20986;&#30340;&#29992;&#20363;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#38598;&#20013;&#22312;&#26631;&#20934;&#24615;&#33021;&#22522;&#20934;&#19978;&#65292;&#24573;&#35270;&#20102;&#24341;&#20154;&#27880;&#30446;&#30340;&#20844;&#24179;&#21644;&#20262;&#29702;&#32771;&#34385;&#12290;&#22312;MT&#20013;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#24615;&#21035;&#38169;&#35823;&#30340;&#32763;&#35793;&#65292;&#20174;&#32780;&#23548;&#33268;&#21051;&#26495;&#21360;&#35937;&#21644;&#20559;&#35265;&#30340;&#25345;&#32493;&#23384;&#22312;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35843;&#26597;&#36825;&#20123;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#26159;&#21542;&#23384;&#22312;&#24615;&#21035;&#20559;&#35265;&#20197;&#21450;&#22914;&#20309;&#32531;&#35299;&#24615;&#21035;&#20559;&#35265;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#20174;&#33521;&#25991;&#21040;&#24503;&#25991;&#21644;&#35199;&#29677;&#29273;&#25991;&#30340;WinoMT&#35821;&#26009;&#24211;&#19978;&#35745;&#31639;&#24050;&#24314;&#31435;&#30340;&#24615;&#21035;&#20559;&#35265;&#25351;&#26631;&#12290;&#25105;&#20204;&#21457;&#29616;&#25351;&#23548;&#24494;&#35843;&#27169;&#22411;&#40664;&#35748;&#20026;&#30007;&#24615;&#23624;&#20174;&#32763;&#35793;&#65292;&#29978;&#33267;&#24573;&#35270;&#22899;&#24615;&#32844;&#19994;&#21051;&#26495;&#21360;&#35937;&#12290;&#25509;&#19979;&#26469;&#65292;&#20351;&#29992;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#27169;&#22411;&#31995;&#32479;&#24615;&#22320;&#24573;&#35270;&#25351;&#31034;&#30446;&#26631;&#32844;&#19994;&#24615;&#21035;&#30340;&#20195;&#35789;&#22312;&#21516;&#26102;&#24615;&#21035;&#38169;&#35823;&#30340;&#32763;&#35793;&#20013;&#12290;&#26368;&#21518;&#65292;&#26681;&#25454;&#21487;&#35299;&#37322;&#24615;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24615;&#21035;&#20559;&#35265;&#32531;&#35299;&#30340;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;MT&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent instruction fine-tuned models can solve multiple NLP tasks when prompted to do so, with machine translation (MT) being a prominent use case. However, current research often focuses on standard performance benchmarks, leaving compelling fairness and ethical considerations behind. In MT, this might lead to misgendered translations, resulting, among other harms, in the perpetuation of stereotypes and prejudices. In this work, we address this gap by investigating whether and to what extent such models exhibit gender bias in machine translation and how we can mitigate it. Concretely, we compute established gender bias metrics on the WinoMT corpus from English to German and Spanish. We discover that IFT models default to male-inflected translations, even disregarding female occupational stereotypes. Next, using interpretability methods, we unveil that models systematically overlook the pronoun indicating the gender of a target occupation in misgendered translations. Finally, based on 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#20154;&#28216;&#25103;&#20013;&#33258;&#25105;&#23545;&#25239;&#30340;&#20445;&#35777;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#30697;&#38453;&#21487;&#20998;&#35299;&#24615;&#65292;&#22312;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#33258;&#25105;&#23545;&#25239;&#23398;&#20064;&#30340;&#31639;&#27861;&#33021;&#22815;&#20135;&#29983;&#26377;&#30028;&#33030;&#24369;&#24615;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.11518</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#30697;&#38453;&#21487;&#20998;&#35299;&#24615;&#22312;&#22810;&#20154;&#28216;&#25103;&#20013;&#23545;&#33258;&#25105;&#23545;&#25239;&#30340;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Guarantees for Self-Play in Multiplayer Games via Polymatrix Decomposability. (arXiv:2310.11518v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11518
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#20154;&#28216;&#25103;&#20013;&#33258;&#25105;&#23545;&#25239;&#30340;&#20445;&#35777;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#30697;&#38453;&#21487;&#20998;&#35299;&#24615;&#65292;&#22312;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#33258;&#25105;&#23545;&#25239;&#23398;&#20064;&#30340;&#31639;&#27861;&#33021;&#22815;&#20135;&#29983;&#26377;&#30028;&#33030;&#24369;&#24615;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#23545;&#25239;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#25216;&#26415;&#65292;&#20854;&#20013;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#19982;&#33258;&#36523;&#30340;&#21103;&#26412;&#20132;&#20114;&#26469;&#23398;&#20064;&#12290;&#33258;&#25105;&#23545;&#25239;&#23545;&#20110;&#29983;&#25104;&#22823;&#37327;&#30340;&#23398;&#20064;&#25968;&#25454;&#24456;&#26377;&#29992;&#65292;&#20294;&#23427;&#30340;&#32570;&#28857;&#26159;&#35757;&#32451;&#21518;&#23398;&#20064;&#32773;&#23558;&#38754;&#23545;&#30340;&#26234;&#33021;&#20307;&#21487;&#33021;&#19982;&#36890;&#36807;&#19982;&#33258;&#36523;&#20132;&#20114;&#26102;&#25152;&#26399;&#26395;&#30340;&#26234;&#33021;&#20307;&#34892;&#20026;&#25130;&#28982;&#19981;&#21516;&#12290;&#23545;&#20110;&#20004;&#20154;&#24120;&#21644;&#28216;&#25103;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#36798;&#21040;&#32435;&#20160;&#22343;&#34913;&#30340;&#33258;&#25105;&#23545;&#25239;&#33021;&#22815;&#20445;&#35777;&#20135;&#29983;&#23545;&#20219;&#20309;&#35757;&#32451;&#21518;&#23545;&#25163;&#34920;&#29616;&#33391;&#22909;&#30340;&#31574;&#30053;&#65307;&#28982;&#32780;&#65292;&#23545;&#20110;&#22810;&#20154;&#28216;&#25103;&#26469;&#35828;&#27809;&#26377;&#36825;&#26679;&#30340;&#20445;&#35777;&#23384;&#22312;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36817;&#20284;&#20998;&#35299;&#20026;&#19968;&#32452;&#20004;&#20154;&#24120;&#21644;&#28216;&#25103;&#65288;&#31216;&#20026;&#22810;&#30697;&#38453;&#28216;&#25103;&#65289;&#30340;&#28216;&#25103;&#20013;&#65292;&#20854;&#20013;&#20840;&#23616; $\epsilon$-&#32435;&#20160;&#22343;&#34913;&#22312;&#27599;&#20010;&#23376;&#28216;&#25103;&#20013;&#37117;&#19982;&#32435;&#20160;&#22343;&#34913;&#26377;&#26377;&#30028;&#36317;&#31163;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#33258;&#25105;&#23545;&#25239;&#23398;&#20064;&#30340;&#26080;&#22806;&#37096;&#36951;&#25022;&#31639;&#27861;&#23558;&#20135;&#29983;&#19968;&#20010;&#26377;&#30028;&#33030;&#24369;&#24615;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#39318;&#27425;&#30830;&#23450;&#20102;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Self-play is a technique for machine learning in multi-agent systems where a learning algorithm learns by interacting with copies of itself. Self-play is useful for generating large quantities of data for learning, but has the drawback that the agents the learner will face post-training may have dramatically different behavior than the learner came to expect by interacting with itself. For the special case of two-player constant-sum games, self-play that reaches Nash equilibrium is guaranteed to produce strategies that perform well against any post-training opponent; however, no such guarantee exists for multi-player games. We show that in games that approximately decompose into a set of two-player constant-sum games (called polymatrix games) where global $\epsilon$-Nash equilibria are boundedly far from Nash-equilibria in each subgame, any no-external-regret algorithm that learns by self-play will produce a strategy with bounded vulnerability. For the first time, our results identify 
&lt;/p&gt;</description></item><item><title>ACES&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#25551;&#36848;&#31526;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#32534;&#31243;&#38590;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20248;&#21270;&#26377;&#36259;&#30340;&#22810;&#26679;&#24615;&#21644;&#23569;&#26679;&#26412;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.10692</link><description>&lt;p&gt;
ACES: &#20351;&#29992;&#33258;&#25105;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#25551;&#36848;&#31526;&#29983;&#25104;&#22810;&#26679;&#30340;&#32534;&#31243;&#38590;&#39064;
&lt;/p&gt;
&lt;p&gt;
ACES: generating diverse programming puzzles with autotelic language models and semantic descriptors. (arXiv:2310.10692v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10692
&lt;/p&gt;
&lt;p&gt;
ACES&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#25551;&#36848;&#31526;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#32534;&#31243;&#38590;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20248;&#21270;&#26377;&#36259;&#30340;&#22810;&#26679;&#24615;&#21644;&#23569;&#26679;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#21644;&#36873;&#25321;&#26032;&#39062;&#26377;&#36259;&#30340;&#38382;&#39064;&#26159;&#22909;&#22855;&#24515;&#12289;&#31185;&#23398;&#21644;&#21019;&#26032;&#30340;&#26680;&#24515;&#12290;&#22312;Python&#32534;&#31243;&#38590;&#39064;&#30340;&#26080;&#38480;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#21160;&#38382;&#39064;&#29983;&#25104;&#12290;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#36890;&#24120;&#26088;&#22312;&#24314;&#27169;&#21442;&#32771;&#20998;&#24067;&#65292;&#27809;&#26377;&#26126;&#30830;&#30340;&#22810;&#26679;&#24615;&#20248;&#21270;&#12290;&#20854;&#20182;&#26041;&#27861;&#22312;&#26377;&#38480;&#30340;&#25163;&#24037;&#32534;&#30721;&#34920;&#31034;&#31354;&#38388;&#25110;&#19981;&#21487;&#35299;&#37322;&#30340;&#23398;&#20064;&#23884;&#20837;&#31354;&#38388;&#20013;&#26126;&#30830;&#20248;&#21270;&#22810;&#26679;&#24615;&#65292;&#36825;&#20123;&#23884;&#20837;&#31354;&#38388;&#21487;&#33021;&#19982;&#20154;&#31867;&#23545;&#26377;&#36259;&#21464;&#21270;&#30340;&#24863;&#30693;&#19981;&#31526;&#12290;&#36890;&#36807;ACES&#65288;&#33258;&#25105;&#30446;&#26631;&#20195;&#30721;&#25506;&#32034;&#19982;&#35821;&#20041;&#25551;&#36848;&#31526;&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#30446;&#26631;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#35821;&#20041;&#25551;&#36848;&#31526;&#65292;&#30452;&#25509;&#20248;&#21270;&#26377;&#36259;&#30340;&#22810;&#26679;&#24615;&#65292;&#20197;&#21450;&#23569;&#26679;&#26412;&#29983;&#25104;&#12290;&#27599;&#20010;&#38590;&#39064;&#37117;&#26631;&#35760;&#26377;10&#20010;&#32500;&#24230;&#65292;&#27599;&#20010;&#32500;&#24230;&#25429;&#25417;&#20102;&#35299;&#20915;&#23427;&#25152;&#38656;&#30340;&#32534;&#31243;&#25216;&#33021;&#12290;ACES&#29983;&#25104;&#24182;&#36861;&#27714;&#26032;&#39062;&#21487;&#34892;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding and selecting new and interesting problems to solve is at the heart of curiosity, science and innovation. We here study automated problem generation in the context of the open-ended space of python programming puzzles. Existing generative models often aim at modeling a reference distribution without any explicit diversity optimization. Other methods explicitly optimizing for diversity do so either in limited hand-coded representation spaces or in uninterpretable learned embedding spaces that may not align with human perceptions of interesting variations. With ACES (Autotelic Code Exploration via Semantic descriptors), we introduce a new autotelic generation method that leverages semantic descriptors produced by a large language model (LLM) to directly optimize for interesting diversity, as well as few-shot-based generation. Each puzzle is labeled along 10 dimensions, each capturing a programming skill required to solve it. ACES generates and pursues novel and feasible goals to 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#26080;&#38480;&#23485;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#36830;&#25509;&#28145;&#24230;&#23398;&#20064;&#21644;&#39640;&#26031;&#36807;&#31243;/&#26680;&#26041;&#27861;&#65292;&#30740;&#31350;&#25512;&#24191;&#20102;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#38381;&#24335;&#24418;&#24335;&#30340;&#26680;&#20989;&#25968;&#21644;&#39640;&#26031;&#36807;&#31243;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#26031;&#36807;&#31243;&#21644;&#26680;&#26041;&#27861;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#26356;&#21152;&#29992;&#25143;&#21451;&#22909;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22238;&#24402;/&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.08176</link><description>&lt;p&gt;
&#26080;&#38480;&#23485;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#33410;&#28857;&#22238;&#24402;/&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Infinite Width Graph Neural Networks for Node Regression/ Classification. (arXiv:2310.08176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#26080;&#38480;&#23485;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#36830;&#25509;&#28145;&#24230;&#23398;&#20064;&#21644;&#39640;&#26031;&#36807;&#31243;/&#26680;&#26041;&#27861;&#65292;&#30740;&#31350;&#25512;&#24191;&#20102;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#38381;&#24335;&#24418;&#24335;&#30340;&#26680;&#20989;&#25968;&#21644;&#39640;&#26031;&#36807;&#31243;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#26031;&#36807;&#31243;&#21644;&#26680;&#26041;&#27861;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#26356;&#21152;&#29992;&#25143;&#21451;&#22909;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22238;&#24402;/&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#27599;&#20010;&#20840;&#36830;&#25509;&#23618;&#30340;&#33410;&#28857;&#25968;&#37327;&#36235;&#36817;&#26080;&#31351;&#22823;&#26102;&#65292;&#23427;&#26159;&#23545;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#20840;&#36830;&#25509;&#28145;&#24230;&#31070;&#32463;&#32593;&#30340;&#19968;&#31181;&#25512;&#24191;&#12290;&#26080;&#38480;&#23485;&#24230;&#31070;&#32463;&#32593;&#32476;&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#39640;&#26031;&#36807;&#31243;&#21644;&#26680;&#26041;&#27861;&#30456;&#36830;&#25509;&#65292;&#21518;&#32773;&#37117;&#26159;&#20855;&#26377;&#24736;&#20037;&#20256;&#32479;&#21644;&#20016;&#23500;&#29702;&#35770;&#22522;&#30784;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#12290;&#39640;&#26031;&#36807;&#31243;&#21644;&#26680;&#26041;&#27861;&#30340;&#36229;&#21442;&#25968;&#36739;&#23569;&#65292;&#21487;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20351;&#20854;&#22312;&#24212;&#29992;&#20013;&#26356;&#21152;&#29992;&#25143;&#21451;&#22909;&#12290;&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#23558;&#39640;&#26031;&#36807;&#31243;&#21644;&#26680;&#26041;&#27861;&#19982;&#31070;&#32463;&#32593;&#32476;&#30456;&#36830;&#25509;&#30340;&#30740;&#31350;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#30340;&#36235;&#21183;&#12290;&#23545;&#20110;&#22810;&#31181;&#26550;&#26500;&#65288;&#21253;&#25324;&#26631;&#20934;&#22270;&#31070;&#32463;&#32593;&#32476;&#12289;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#65289;&#65292;&#25512;&#23548;&#20986;&#20102;&#26680;&#20989;&#25968;&#21644;&#39640;&#26031;&#36807;&#31243;&#30340;&#38381;&#24335;&#24418;&#24335;&#12290;&#23545;&#36825;&#20123;&#26550;&#26500;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#36827;&#34892;&#20102;&#22238;&#24402;/&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work analyzes Graph Neural Networks, a generalization of Fully-Connected Deep Neural Nets on Graph structured data, when their width, that is the number of nodes in each fullyconnected layer is increasing to infinity. Infinite Width Neural Networks are connecting Deep Learning to Gaussian Processes and Kernels, both Machine Learning Frameworks with long traditions and extensive theoretical foundations. Gaussian Processes and Kernels have much less hyperparameters then Neural Networks and can be used for uncertainty estimation, making them more user friendly for applications. This works extends the increasing amount of research connecting Gaussian Processes and Kernels to Neural Networks. The Kernel and Gaussian Process closed forms are derived for a variety of architectures, namely the standard Graph Neural Network, the Graph Neural Network with Skip-Concatenate Connections and the Graph Attention Neural Network. All architectures are evaluated on a variety of datasets on the task
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;Graph Attention Network (GAT)&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#30340;&#23432;&#24658;&#23450;&#24459;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#26631;&#20934;&#21021;&#22987;&#21270;&#19979;&#30340;GAT&#20013;&#39640;&#27604;&#20363;&#30340;&#21442;&#25968;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24456;&#38590;&#25913;&#21464;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;GAT&#32593;&#32476;&#30340;&#21021;&#22987;&#21270;&#26041;&#26696;&#65292;&#20351;&#24471;&#28145;&#23618;&#32593;&#32476;&#26356;&#23481;&#26131;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#30456;&#27604;&#26631;&#20934;&#21021;&#22987;&#21270;&#65292;&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.07235</link><description>&lt;p&gt;
GATs&#26159;&#21542;&#22833;&#34913;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are GATs Out of Balance?. (arXiv:2310.07235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;Graph Attention Network (GAT)&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#30340;&#23432;&#24658;&#23450;&#24459;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#26631;&#20934;&#21021;&#22987;&#21270;&#19979;&#30340;GAT&#20013;&#39640;&#27604;&#20363;&#30340;&#21442;&#25968;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24456;&#38590;&#25913;&#21464;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;GAT&#32593;&#32476;&#30340;&#21021;&#22987;&#21270;&#26041;&#26696;&#65292;&#20351;&#24471;&#28145;&#23618;&#32593;&#32476;&#26356;&#23481;&#26131;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#30456;&#27604;&#26631;&#20934;&#21021;&#22987;&#21270;&#65292;&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#35745;&#31639;&#33021;&#21147;&#24050;&#32463;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20102;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#30340;&#20248;&#21270;&#21644;&#23398;&#20064;&#21160;&#24577;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20173;&#28982;&#26410;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#38024;&#23545;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GAT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;GNN&#26550;&#26500;&#65292;&#20854;&#20013;&#33410;&#28857;&#30340;&#37051;&#22495;&#32858;&#21512;&#30001;&#21442;&#25968;&#21270;&#30340;&#27880;&#24847;&#21147;&#31995;&#25968;&#21152;&#26435;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;GAT&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#30340;&#23432;&#24658;&#23450;&#24459;&#65292;&#36825;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#26631;&#20934;&#21021;&#22987;&#21270;&#19979;&#30340;GAT&#20013;&#39640;&#27604;&#20363;&#30340;&#21442;&#25968;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24456;&#38590;&#25913;&#21464;&#12290;&#36825;&#31181;&#25928;&#24212;&#22312;&#28145;&#23618;&#30340;GAT&#20013;&#34987;&#25918;&#22823;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#35201;&#26126;&#26174;&#24046;&#20110;&#27973;&#23618;&#30340;GAT&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#24179;&#34913;GAT&#32593;&#32476;&#30340;&#21021;&#22987;&#21270;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861; i) &#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#20256;&#25773;&#26799;&#24230;&#65292;&#20174;&#32780;&#20351;&#28145;&#23618;&#32593;&#32476;&#21487;&#35757;&#32451;&#65292;ii) &#30456;&#27604;&#20110;&#26631;&#20934;&#21021;&#22987;&#21270;&#65292;&#21487;&#20197;&#23454;&#29616;&#35757;&#32451;&#21644;&#25910;&#25947;&#26102;&#38388;&#30340;&#26174;&#33879;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the expressive power and computational capabilities of graph neural networks (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a node's neighborhood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change during training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. O
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#33021;&#22815;&#21516;&#26102;&#36827;&#34892;&#26368;&#20248;&#25506;&#32034;&#21644;&#21482;&#38656;&#35201;&#30456;&#21516;&#35745;&#31639;&#25805;&#20316;&#30340;&#31639;&#27861;&#65311;</title><link>http://arxiv.org/abs/2310.06069</link><description>&lt;p&gt;
&#26368;&#20248;&#25506;&#32034;&#19981;&#27604;&#27748;&#26222;&#26862;&#37319;&#26679;&#26356;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Optimal Exploration is no harder than Thompson Sampling. (arXiv:2310.06069v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06069
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#33021;&#22815;&#21516;&#26102;&#36827;&#34892;&#26368;&#20248;&#25506;&#32034;&#21644;&#21482;&#38656;&#35201;&#30456;&#21516;&#35745;&#31639;&#25805;&#20316;&#30340;&#31639;&#27861;&#65311;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#19968;&#32452;&#33218;$\mathcal{Z}\subset \mathbb{R}^d$&#21644;&#26410;&#30693;&#21442;&#25968;&#21521;&#37327;$\theta_\ast\in\mathbb{R}^d$&#30340;&#24773;&#20917;&#19979;&#65292;&#32431;&#25506;&#32034;&#32447;&#24615;&#33218;&#38382;&#39064;&#26088;&#22312;&#36890;&#36807;&#23545;$x^{\top}\theta_{\ast}$&#30340;&#22122;&#22768;&#27979;&#37327;&#65292;&#36820;&#22238;$\arg\max_{z\in \mathcal{Z}} z^{\top}\theta_{\ast}$&#65292;&#24182;&#20197;&#39640;&#27010;&#29575;&#25214;&#21040;&#27491;&#30830;&#35299;&#12290;&#29616;&#26377;&#30340;&#65288;&#28176;&#36817;&#65289;&#26368;&#20248;&#26041;&#27861;&#35201;&#27714;&#35201;&#20040;&#20026;&#27599;&#20010;&#33218;$z\in \mathcal{Z}$&#36827;&#34892;&#28508;&#22312;&#26114;&#36149;&#30340;&#25237;&#24433;&#65292;&#35201;&#20040;&#22312;&#27599;&#20010;&#26102;&#38388;&#28857;&#26126;&#30830;&#22320;&#32500;&#25252;&#19968;&#37096;&#20998;&#27491;&#22312;&#32771;&#34385;&#30340;$\mathcal{Z}$&#12290;&#36825;&#31181;&#22797;&#26434;&#24615;&#19982;&#27969;&#34892;&#19988;&#31616;&#21333;&#30340;&#27748;&#26222;&#26862;&#37319;&#26679;&#31639;&#27861;&#29992;&#20110;&#26368;&#23567;&#21270;&#21518;&#24724;&#30340;&#24773;&#20917;&#23436;&#20840;&#30456;&#21453;&#65292;&#21518;&#32773;&#21482;&#38656;&#35201;&#35775;&#38382;&#21518;&#39564;&#37319;&#26679;&#21644;argmax oracle&#65292;&#24182;&#19988;&#22312;&#20219;&#20309;&#26102;&#38388;&#28857;&#37117;&#19981;&#38656;&#35201;&#26522;&#20030;$\mathcal{Z}$&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24050;&#30693;&#27748;&#26222;&#26862;&#37319;&#26679;&#23545;&#20110;&#32431;&#25506;&#32034;&#26159;&#27425;&#20248;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#65306;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#31639;&#27861;&#33021;&#22815;&#36827;&#34892;&#26368;&#20248;&#25506;&#32034;&#65292;&#32780;&#19988;&#21482;&#38656;&#35201;&#30456;&#21516;&#30340;&#35745;&#31639;&#25805;&#20316;&#65311;
&lt;/p&gt;
&lt;p&gt;
Given a set of arms $\mathcal{Z}\subset \mathbb{R}^d$ and an unknown parameter vector $\theta_\ast\in\mathbb{R}^d$, the pure exploration linear bandit problem aims to return $\arg\max_{z\in \mathcal{Z}} z^{\top}\theta_{\ast}$, with high probability through noisy measurements of $x^{\top}\theta_{\ast}$ with $x\in \mathcal{X}\subset \mathbb{R}^d$. Existing (asymptotically) optimal methods require either a) potentially costly projections for each arm $z\in \mathcal{Z}$ or b) explicitly maintaining a subset of $\mathcal{Z}$ under consideration at each time. This complexity is at odds with the popular and simple Thompson Sampling algorithm for regret minimization, which just requires access to a posterior sampling and argmax oracle, and does not need to enumerate $\mathcal{Z}$ at any point. Unfortunately, Thompson sampling is known to be sub-optimal for pure exploration. In this work, we pose a natural question: is there an algorithm that can explore optimally and only needs the same comput
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;DSAC-T&#65292;&#36890;&#36807;&#35780;&#35770;&#32773;&#26799;&#24230;&#35843;&#25972;&#12289;&#21452;&#20540;&#20998;&#24067;&#23398;&#20064;&#21644;&#22522;&#20110;&#26041;&#24046;&#30340;&#30446;&#26631;&#22238;&#25253;&#35009;&#21098;&#31561;&#19977;&#20010;&#25913;&#36827;&#23545;&#26631;&#20934;DSAC&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;DSAC&#23384;&#22312;&#30340;&#19981;&#31283;&#23450;&#23398;&#20064;&#36807;&#31243;&#21644;&#23545;&#20219;&#21153;&#29305;&#23450;&#22870;&#21169;&#32553;&#25918;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.05858</link><description>&lt;p&gt;
DSAC-T: &#24102;&#26377;&#19977;&#20010;&#25913;&#36827;&#30340;&#20998;&#24067;&#24335;&#36719;&#35282;&#33394;&#25198;&#28436;&#32773;&#8212;&#35780;&#35770;&#32773;
&lt;/p&gt;
&lt;p&gt;
DSAC-T: Distributional Soft Actor-Critic with Three Refinements. (arXiv:2310.05858v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;DSAC-T&#65292;&#36890;&#36807;&#35780;&#35770;&#32773;&#26799;&#24230;&#35843;&#25972;&#12289;&#21452;&#20540;&#20998;&#24067;&#23398;&#20064;&#21644;&#22522;&#20110;&#26041;&#24046;&#30340;&#30446;&#26631;&#22238;&#25253;&#35009;&#21098;&#31561;&#19977;&#20010;&#25913;&#36827;&#23545;&#26631;&#20934;DSAC&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;DSAC&#23384;&#22312;&#30340;&#19981;&#31283;&#23450;&#23398;&#20064;&#36807;&#31243;&#21644;&#23545;&#20219;&#21153;&#29305;&#23450;&#22870;&#21169;&#32553;&#25918;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#20915;&#31574;&#21644;&#25511;&#21046;&#20219;&#21153;&#26041;&#38754;&#24050;&#32463;&#34987;&#35777;&#26126;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#26080;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#38754;&#20020;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#36807;&#20272;&#35745;&#38382;&#39064;&#25152;&#24341;&#36215;&#30340;&#12290;&#20316;&#20026;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#22238;&#24212;&#65292;&#25105;&#20204;&#26368;&#36817;&#24341;&#20837;&#20102;&#19968;&#31181;&#31163;&#32447;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#20998;&#24067;&#24335;&#36719;&#35282;&#33394;&#25198;&#28436;&#32773;&#35780;&#35770;&#32773;&#65288;DSAC&#25110;DSAC-v1&#65289;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#36830;&#32493;&#30340;&#39640;&#26031;&#20540;&#20998;&#24067;&#26469;&#26377;&#25928;&#25552;&#39640;&#20540;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;DSAC&#20063;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#21253;&#25324;&#26102;&#32780;&#19981;&#31283;&#23450;&#30340;&#23398;&#20064;&#36807;&#31243;&#21644;&#23545;&#20219;&#21153;&#29305;&#23450;&#30340;&#22870;&#21169;&#32553;&#25918;&#30340;&#38656;&#27714;&#65292;&#36825;&#21487;&#33021;&#20250;&#38459;&#30861;&#20854;&#22312;&#19968;&#20123;&#29305;&#27530;&#20219;&#21153;&#20013;&#30340;&#25972;&#20307;&#24615;&#33021;&#21644;&#36866;&#24212;&#24615;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19977;&#20010;&#23545;&#26631;&#20934;DSAC&#30340;&#37325;&#35201;&#25913;&#36827;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36825;&#20123;&#25913;&#36827;&#21253;&#25324;&#35780;&#35770;&#32773;&#26799;&#24230;&#35843;&#25972;&#12289;&#21452;&#20540;&#20998;&#24067;&#23398;&#20064;&#21644;&#22522;&#20110;&#26041;&#24046;&#30340;&#30446;&#26631;&#22238;&#25253;&#35009;&#21098;&#12290;&#20462;&#25913;&#21518;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#31216;&#20026;DSAC-T&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has proven to be highly effective in tackling complex decision-making and control tasks. However, prevalent model-free RL methods often face severe performance degradation due to the well-known overestimation issue. In response to this problem, we recently introduced an off-policy RL algorithm, called distributional soft actor-critic (DSAC or DSAC-v1), which can effectively improve the value estimation accuracy by learning a continuous Gaussian value distribution. Nonetheless, standard DSAC has its own shortcomings, including occasionally unstable learning processes and needs for task-specific reward scaling, which may hinder its overall performance and adaptability in some special tasks. This paper further introduces three important refinements to standard DSAC in order to address these shortcomings. These refinements consist of critic gradient adjusting, twin value distribution learning, and variance-based target return clipping. The modified RL algorithm 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#25688;&#35201;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20154;&#24037;&#32534;&#36753;&#30340;&#21453;&#39304;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#24207;&#21015;&#23545;&#40784;&#65288;&#19981;&#65289;&#20284;&#28982;&#35757;&#32451;(SALT)&#25216;&#26415;&#23558;&#20154;&#24037;&#32534;&#36753;&#25968;&#25454;&#19982;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#21307;&#23398;&#39046;&#22495;&#25688;&#35201;&#29983;&#25104;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.05857</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#32534;&#36753;&#25913;&#36827;&#25688;&#35201;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Improving Summarization with Human Edits. (arXiv:2310.05857v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#25688;&#35201;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20154;&#24037;&#32534;&#36753;&#30340;&#21453;&#39304;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#24207;&#21015;&#23545;&#40784;&#65288;&#19981;&#65289;&#20284;&#28982;&#35757;&#32451;(SALT)&#25216;&#26415;&#23558;&#20154;&#24037;&#32534;&#36753;&#25968;&#25454;&#19982;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#21307;&#23398;&#39046;&#22495;&#25688;&#35201;&#29983;&#25104;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#33539;&#24335;&#23398;&#20064;&#21487;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#22312;&#36890;&#29992;&#39046;&#22495;&#25277;&#35937;&#21270;&#25688;&#35201;&#29983;&#25104;&#20013;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#26469;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#24182;&#33719;&#24471;&#20102;&#36229;&#36234;&#20256;&#32479;&#20284;&#28982;&#35757;&#32451;&#30340;&#25688;&#35201;&#36136;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#19968;&#31181;&#36739;&#23569;&#25506;&#32034;&#30340;&#20154;&#31867;&#21453;&#39304;&#24418;&#24335;&#8212;&#8212;&#20154;&#24037;&#32534;&#36753;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#8212;&#8212;&#24207;&#21015;&#23545;&#40784;&#65288;&#19981;&#65289;&#20284;&#28982;&#35757;&#32451;(SALT)&#65292;&#22312;&#35757;&#32451;&#24490;&#29615;&#20013;&#21516;&#26102;&#20351;&#29992;&#20154;&#24037;&#32534;&#36753;&#21644;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20351;&#29992;&#29616;&#26377;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#22522;&#20934;&#25688;&#35201;&#26469;&#27169;&#25311;&#20154;&#24037;&#32534;&#36753;&#65292;&#20197;&#21450;&#22312;&#35757;&#32451;&#21518;&#33719;&#21462;&#30340;&#27169;&#22411;&#29983;&#25104;&#25688;&#35201;&#65292;&#20197;&#20943;&#23569;&#23545;&#26114;&#36149;&#30340;&#20154;&#24037;&#32534;&#36753;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#20154;&#31867;&#21453;&#39304;&#30340;&#25506;&#32034;&#20174;&#36890;&#29992;&#39046;&#22495;&#25688;&#35201;&#29983;&#25104;&#25193;&#23637;&#21040;&#21307;&#23398;&#39046;&#22495;&#25688;&#35201;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;SALT&#22312;&#25913;&#36827;&#25688;&#35201;&#29983;&#25104;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown the promise of learning with human feedback paradigms to produce human-determined high-quality text. Existing works use human feedback to train large language models (LLMs) in general domain abstractive summarization and have obtained summary quality exceeding traditional likelihood training. In this paper, we focus on a less explored form of human feedback -- Human Edits. We propose Sequence Alignment (un)Likelihood Training (SALT), a novel technique to use both the human-edited and model-generated data together in the training loop. In addition, we demonstrate simulating Human Edits with ground truth summaries coming from existing training data -Imitation edits, along with the model-generated summaries obtained after the training, to reduce the need for expensive human-edit data. In our experiments, we extend human feedback exploration from general domain summarization to medical domain summarization. Our results demonstrate the effectiveness of SALT in improv
&lt;/p&gt;</description></item><item><title>OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02031</link><description>&lt;p&gt;
OceanGPT&#65306;&#29992;&#20110;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02031
&lt;/p&gt;
&lt;p&gt;
OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#31185;&#23398;&#26159;&#25506;&#32034;&#20805;&#28385;&#29983;&#21629;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#30340;&#28023;&#27915;&#30340;&#31185;&#23398;&#65292;&#32771;&#34385;&#21040;&#28023;&#27915;&#35206;&#30422;&#20102;&#22320;&#29699;&#34920;&#38754;&#30340;70&#65285;&#20197;&#19978;&#65292;&#36825;&#19968;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#25913;&#21464;&#20102;&#31185;&#23398;&#30340;&#33539;&#24335;&#12290;&#23613;&#31649;&#22312;&#20854;&#20182;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;LLM&#36890;&#24120;&#26080;&#27861;&#28385;&#36275;&#28023;&#27915;&#23398;&#23478;&#31561;&#39046;&#22495;&#19987;&#23478;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#23545;LLM&#22312;&#28023;&#27915;&#31185;&#23398;&#20013;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#36825;&#20854;&#20013;&#30340;&#26681;&#26412;&#21407;&#22240;&#21487;&#33021;&#26159;&#28023;&#27915;&#25968;&#25454;&#30340;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#24615;&#36136;&#65292;&#20197;&#21450;&#23545;&#26356;&#39640;&#30340;&#31890;&#24230;&#21644;&#20016;&#23500;&#30340;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#39318;&#20010;&#28023;&#27915;&#39046;&#22495;&#30340;LLM&#8212;&#8212;OceanGPT&#65292;&#35813;&#27169;&#22411;&#25797;&#38271;&#21508;&#31181;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;DoInstruct&#65292;&#29992;&#20110;&#33258;&#21160;&#33719;&#21462;&#22823;&#37327;&#30340;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#65292;&#23427;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#29983;&#25104;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22320;&#38663;&#22270;&#21464;&#21387;&#22120;&#65288;SeisT&#65289;&#30340;&#36890;&#29992;&#28145;&#24230;&#23398;&#20064;&#39592;&#24178;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#31181;&#22320;&#38663;&#30417;&#27979;&#20219;&#21153;&#12290;SeisT&#30340;&#39640;&#25928;&#32593;&#32476;&#26550;&#26500;&#20351;&#20854;&#22312;&#22320;&#38663;&#26816;&#27979;&#12289;&#22320;&#38663;&#30456;&#20301;&#25342;&#21462;&#12289;&#39318;&#27425;&#36816;&#21160;&#26497;&#24615;&#20998;&#31867;&#12289;&#38663;&#32423;&#20272;&#35745;&#21644;&#21453;&#26041;&#20301;&#35282;&#20272;&#35745;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#29305;&#21035;&#26159;&#22312;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2310.01037</link><description>&lt;p&gt;
&#22320;&#38663;&#22270;&#21464;&#21387;&#22120;&#65306;&#29992;&#20110;&#22810;&#31181;&#22320;&#38663;&#30417;&#27979;&#20219;&#21153;&#30340;&#36890;&#29992;&#28145;&#24230;&#23398;&#20064;&#39592;&#24178;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Seismogram Transformer: A generic deep learning backbone network for multiple earthquake monitoring tasks. (arXiv:2310.01037v2 [physics.geo-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22320;&#38663;&#22270;&#21464;&#21387;&#22120;&#65288;SeisT&#65289;&#30340;&#36890;&#29992;&#28145;&#24230;&#23398;&#20064;&#39592;&#24178;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#31181;&#22320;&#38663;&#30417;&#27979;&#20219;&#21153;&#12290;SeisT&#30340;&#39640;&#25928;&#32593;&#32476;&#26550;&#26500;&#20351;&#20854;&#22312;&#22320;&#38663;&#26816;&#27979;&#12289;&#22320;&#38663;&#30456;&#20301;&#25342;&#21462;&#12289;&#39318;&#27425;&#36816;&#21160;&#26497;&#24615;&#20998;&#31867;&#12289;&#38663;&#32423;&#20272;&#35745;&#21644;&#21453;&#26041;&#20301;&#35282;&#20272;&#35745;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#29305;&#21035;&#26159;&#22312;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#38663;&#35760;&#24405;&#65292;&#21363;&#22320;&#38663;&#22270;&#65292;&#26159;&#30001;&#22320;&#38663;&#20107;&#20214;&#24341;&#36215;&#30340;&#22320;&#38754;&#36816;&#21160;&#30340;&#37325;&#35201;&#35760;&#24405;&#65292;&#26159;&#22320;&#38663;&#30740;&#31350;&#21644;&#30417;&#27979;&#30340;&#22522;&#30784;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#21508;&#31181;&#22320;&#38663;&#20449;&#21495;&#22788;&#29702;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39592;&#24178;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#21508;&#31181;&#22320;&#38663;&#30417;&#27979;&#20219;&#21153;&#65292;&#21517;&#20026;&#22320;&#38663;&#22270;&#21464;&#21387;&#22120;&#65288;SeisT&#65289;&#12290;&#30001;&#20110;&#20854;&#39640;&#25928;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;SeisT&#22312;&#22320;&#38663;&#26816;&#27979;&#12289;&#22320;&#38663;&#30456;&#20301;&#25342;&#21462;&#12289;&#39318;&#27425;&#36816;&#21160;&#26497;&#24615;&#20998;&#31867;&#12289;&#38663;&#32423;&#20272;&#35745;&#21644;&#21453;&#26041;&#20301;&#35282;&#20272;&#35745;&#31561;&#20219;&#21153;&#20013;&#33021;&#22815;&#19982;&#29978;&#33267;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#21305;&#37197;&#65292;&#29305;&#21035;&#26159;&#22312;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#12290;SeisT&#30001;&#22810;&#20010;&#32593;&#32476;&#23618;&#32452;&#25104;&#65292;&#36825;&#20123;&#23618;&#30001;&#19981;&#21516;&#30340;&#22522;&#30784;&#27169;&#22359;&#32452;&#25104;&#65292;&#24110;&#21161;&#27169;&#22411;&#29702;&#35299;&#22320;&#38663;&#22270;&#30340;&#22810;&#23618;&#27425;&#29305;&#24449;&#34920;&#36798;&#65292;&#20174;&#20302;&#23618;&#27425;&#21040;&#39640;&#23618;&#27425;&#30340;&#22797;&#26434;&#29305;&#24449;&#65292;&#26377;&#25928;&#22320;&#25552;&#21462;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Seismic records, known as seismograms, are crucial records of ground motion resulting from seismic events, constituting the backbone of earthquake research and monitoring. The latest advancements in deep learning have significantly facilitated various seismic signal processing tasks. This paper introduces a novel backbone neural network model designed for various seismic monitoring tasks, named Seismogram Transformer (SeisT). Thanks to its efficient network architecture, SeisT matches or even outperforms the state-of-the-art models in earthquake detection, seismic phase picking, first-motion polarity classification, magnitude estimation, and back-azimuth estimation tasks, particularly in terms of out-of-distribution generalization performance. SeisT consists of multiple network layers composed of different foundational blocks, which help the model understand multi-level feature representations of seismograms from low-level to high-level complex features, effectively extracting features
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#35757;&#32451;&#38598;&#20013;&#30340;&#23569;&#37327;&#25968;&#25454;&#28857;&#34987;&#25490;&#38500;&#21518;&#36755;&#20986;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#21644;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#32463;&#39564;&#20998;&#26512;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#35760;&#24518;&#21644;&#20449;&#24687;&#27844;&#28431;&#30340;&#26377;&#25928;&#34913;&#37327;&#21644;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.17310</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#27425;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#36776;&#35782;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Leave-one-out Distinguishability in Machine Learning. (arXiv:2309.17310v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17310
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#35757;&#32451;&#38598;&#20013;&#30340;&#23569;&#37327;&#25968;&#25454;&#28857;&#34987;&#25490;&#38500;&#21518;&#36755;&#20986;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#21644;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#32463;&#39564;&#20998;&#26512;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#35760;&#24518;&#21644;&#20449;&#24687;&#27844;&#28431;&#30340;&#26377;&#25928;&#34913;&#37327;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#35757;&#32451;&#38598;&#20013;&#21253;&#21547;&#23569;&#37327;&#25968;&#25454;&#28857;&#21518;&#36755;&#20986;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#27010;&#24565;&#23450;&#20041;&#20026;&#19968;&#27425;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#36776;&#35782;&#24615;(LOOD)&#12290;&#36825;&#20010;&#38382;&#39064;&#23545;&#20110;&#34913;&#37327;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#35760;&#24518;&#21644;&#20449;&#24687;&#27844;&#28431;&#20197;&#21450;&#35757;&#32451;&#25968;&#25454;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#26469;&#24314;&#27169;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#38543;&#26426;&#24615;&#65292;&#24182;&#36890;&#36807;&#23545;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#20351;&#29992;&#24191;&#27867;&#30340;&#32463;&#39564;&#20998;&#26512;&#39564;&#35777;&#20102;LOOD&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;&#20449;&#24687;&#27844;&#28431;&#30340;&#21407;&#22240;&#20197;&#21450;&#27844;&#28431;&#31243;&#24230;&#39640;&#30340;&#20301;&#32622;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#28608;&#27963;&#20989;&#25968;&#23545;&#25968;&#25454;&#35760;&#24518;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#20248;&#21270;...
&lt;/p&gt;
&lt;p&gt;
We introduce a new analytical framework to quantify the changes in a machine learning algorithm's output distribution following the inclusion of a few data points in its training set, a notion we define as leave-one-out distinguishability (LOOD). This problem is key to measuring data **memorization** and **information leakage** in machine learning, and the **influence** of training data points on model predictions. We illustrate how our method broadens and refines existing empirical measures of memorization and privacy risks associated with training data. We use Gaussian processes to model the randomness of machine learning algorithms, and validate LOOD with extensive empirical analysis of information leakage using membership inference attacks. Our theoretical framework enables us to investigate the causes of information leakage and where the leakage is high. For example, we analyze the influence of activation functions, on data memorization. Additionally, our method allows us to optim
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24178;&#39044;&#24863;&#30693;&#30340;&#27010;&#24565;&#23884;&#20837;&#27169;&#22411;&#65292;&#29992;&#20110;&#25552;&#39640;&#31070;&#32463;&#26550;&#26500;&#23545;&#27010;&#24565;&#24178;&#39044;&#30340;&#21709;&#24212;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#27010;&#24565;&#24178;&#39044;&#39034;&#24207;&#21644;&#27169;&#22411;&#26550;&#26500;&#30340;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16928</link><description>&lt;p&gt;
&#23398;&#20064;&#25509;&#21463;&#24110;&#21161;&#65306;&#24178;&#39044;&#24863;&#30693;&#30340;&#27010;&#24565;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning to Receive Help: Intervention-Aware Concept Embedding Models. (arXiv:2309.16928v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16928
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24178;&#39044;&#24863;&#30693;&#30340;&#27010;&#24565;&#23884;&#20837;&#27169;&#22411;&#65292;&#29992;&#20110;&#25552;&#39640;&#31070;&#32463;&#26550;&#26500;&#23545;&#27010;&#24565;&#24178;&#39044;&#30340;&#21709;&#24212;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#27010;&#24565;&#24178;&#39044;&#39034;&#24207;&#21644;&#27169;&#22411;&#26550;&#26500;&#30340;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#39640;&#32423;&#27010;&#24565;&#26500;&#24314;&#21644;&#35299;&#37322;&#31070;&#32463;&#26550;&#26500;&#30340;&#39044;&#27979;&#65292;&#20197;&#35299;&#20915;&#20854;&#19981;&#36879;&#26126;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#19968;&#20010;&#29305;&#27530;&#23646;&#24615;&#26159;&#23427;&#20204;&#20801;&#35768;&#27010;&#24565;&#24178;&#39044;&#65292;&#29992;&#25143;&#21487;&#20197;&#32416;&#27491;&#34987;&#38169;&#35823;&#39044;&#27979;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24178;&#39044;&#26377;&#25928;&#24615;&#21487;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#24178;&#39044;&#27010;&#24565;&#30340;&#39034;&#24207;&#20197;&#21450;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#28304;&#20110;CBM&#22312;&#35757;&#32451;&#26102;&#32570;&#20047;&#27169;&#22411;&#36866;&#24212;&#27010;&#24565;&#24178;&#39044;&#30340;&#28608;&#21169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24178;&#39044;&#24863;&#30693;&#30340;&#27010;&#24565;&#23884;&#20837;&#27169;&#22411;&#65288;IntCEMs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;CBM&#30340;&#26032;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#33539;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#27979;&#35797;&#26102;&#24178;&#39044;&#30340;&#21709;&#24212;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23398;&#20064;&#20102;&#19968;&#20010;&#27010;&#24565;&#24178;&#39044;&#31574;&#30053;&#65292;&#20174;&#20013;&#21487;&#20197;&#37319;&#26679;&#26377;&#24847;&#20041;&#30340;&#24178;&#39044;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept Bottleneck Models (CBMs) tackle the opacity of neural architectures by constructing and explaining their predictions using a set of high-level concepts. A special property of these models is that they permit concept interventions, wherein users can correct mispredicted concepts and thus improve the model's performance. Recent work, however, has shown that intervention efficacy can be highly dependent on the order in which concepts are intervened on and on the model's architecture and training hyperparameters. We argue that this is rooted in a CBM's lack of train-time incentives for the model to be appropriately receptive to concept interventions. To address this, we propose Intervention-aware Concept Embedding models (IntCEMs), a novel CBM-based architecture and training paradigm that improves a model's receptiveness to test-time interventions. Our model learns a concept intervention policy in an end-to-end fashion from where it can sample meaningful intervention trajectories a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861; DJINN&#65292;&#29992;&#20110;&#29983;&#25104;&#20132;&#36890;&#22330;&#26223;&#12290;&#36890;&#36807;&#32852;&#21512;&#25193;&#25955;&#25152;&#26377;&#20195;&#29702;&#30340;&#36712;&#36857;&#65292;&#24182;&#20197;&#28789;&#27963;&#30340;&#29366;&#24577;&#35266;&#23519;&#20026;&#26465;&#20214;&#65292;&#25105;&#20204;&#22312;&#36712;&#36857;&#39044;&#27979;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;DJINN&#36824;&#33021;&#28789;&#27963;&#22320;&#20174;&#22810;&#31181;&#26377;&#20215;&#20540;&#30340;&#26465;&#20214;&#20998;&#24067;&#20013;&#36827;&#34892;&#27979;&#35797;&#26102;&#25277;&#26679;&#12290;</title><link>http://arxiv.org/abs/2309.12508</link><description>&lt;p&gt;
&#19968;&#31181;&#32852;&#21512;&#20132;&#20114;&#24335;&#23548;&#33322;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Diffusion-Model of Joint Interactive Navigation. (arXiv:2309.12508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861; DJINN&#65292;&#29992;&#20110;&#29983;&#25104;&#20132;&#36890;&#22330;&#26223;&#12290;&#36890;&#36807;&#32852;&#21512;&#25193;&#25955;&#25152;&#26377;&#20195;&#29702;&#30340;&#36712;&#36857;&#65292;&#24182;&#20197;&#28789;&#27963;&#30340;&#29366;&#24577;&#35266;&#23519;&#20026;&#26465;&#20214;&#65292;&#25105;&#20204;&#22312;&#36712;&#36857;&#39044;&#27979;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;DJINN&#36824;&#33021;&#28789;&#27963;&#22320;&#20174;&#22810;&#31181;&#26377;&#20215;&#20540;&#30340;&#26465;&#20214;&#20998;&#24067;&#20013;&#36827;&#34892;&#27979;&#35797;&#26102;&#25277;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#38656;&#35201;&#27169;&#25311;&#20986;&#23637;&#29616;&#22810;&#26679;&#21644;&#30495;&#23454;&#34892;&#20026;&#30340;&#20132;&#36890;&#21442;&#19982;&#32773;&#12290;&#22312;&#27169;&#25311;&#20013;&#20351;&#29992;&#23454;&#38469;&#19990;&#30028;&#20132;&#36890;&#22330;&#26223;&#30830;&#20445;&#20102;&#30495;&#23454;&#24615;&#65292;&#20294;&#26159;&#23433;&#20840;&#20851;&#38190;&#20107;&#20214;&#30340;&#32597;&#35265;&#24615;&#20351;&#24471;&#22823;&#35268;&#27169;&#25910;&#38598;&#39550;&#39542;&#22330;&#26223;&#20855;&#26377;&#39640;&#26114;&#30340;&#25104;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#20132;&#36890;&#22330;&#26223;&#30340;&#26041;&#27861;DJINN&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32852;&#21512;&#25193;&#25955;&#25152;&#26377;&#20195;&#29702;&#30340;&#36712;&#36857;&#65292;&#20197;&#36807;&#21435;&#12289;&#29616;&#22312;&#25110;&#26410;&#26469;&#30340;&#19968;&#31995;&#21015;&#28789;&#27963;&#30340;&#29366;&#24577;&#35266;&#23519;&#20026;&#26465;&#20214;&#12290;&#22312;&#27969;&#34892;&#30340;&#36712;&#36857;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#25253;&#36947;&#20102;&#22312;&#32852;&#21512;&#36712;&#36857;&#35780;&#20272;&#25351;&#26631;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DJINN&#22914;&#20309;&#28789;&#27963;&#22320;&#20351;&#24471;&#20174;&#21508;&#31181;&#26377;&#20215;&#20540;&#30340;&#26465;&#20214;&#20998;&#24067;&#20013;&#30452;&#25509;&#36827;&#34892;&#27979;&#35797;&#26102;&#25277;&#26679;&#65292;&#21253;&#25324;&#22522;&#20110;&#30446;&#26631;&#30340;&#25277;&#26679;&#12289;&#34892;&#20026;&#31867;&#21035;&#25277;&#26679;&#21644;&#22330;&#26223;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation of autonomous vehicle systems requires that simulated traffic participants exhibit diverse and realistic behaviors. The use of prerecorded real-world traffic scenarios in simulation ensures realism but the rarity of safety critical events makes large scale collection of driving scenarios expensive. In this paper, we present DJINN - a diffusion based method of generating traffic scenarios. Our approach jointly diffuses the trajectories of all agents, conditioned on a flexible set of state observations from the past, present, or future. On popular trajectory forecasting datasets, we report state of the art performance on joint trajectory metrics. In addition, we demonstrate how DJINN flexibly enables direct test-time sampling from a variety of valuable conditional distributions including goal-based sampling, behavior-class sampling, and scenario editing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36820;&#22238;&#26465;&#20214;&#31574;&#30053;&#65288;ARP&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#21644;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#26469;&#25552;&#21319;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#23884;&#20837;&#31354;&#38388;&#20013;&#35745;&#31639;&#35270;&#35273;&#35266;&#23519;&#21644;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#22870;&#21169;&#20449;&#21495;&#65292;ARP&#26377;&#25928;&#32531;&#35299;&#20102;&#30446;&#26631;&#35823;&#27867;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#38754;&#23545;&#26410;&#30693;&#30340;&#25991;&#26412;&#25351;&#20196;&#26102;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10790</link><description>&lt;p&gt;
&#29992;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#22870;&#21169;&#24341;&#23548;&#20320;&#30340;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Guide Your Agent with Adaptive Multimodal Rewards. (arXiv:2309.10790v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36820;&#22238;&#26465;&#20214;&#31574;&#30053;&#65288;ARP&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#21644;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#26469;&#25552;&#21319;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#23884;&#20837;&#31354;&#38388;&#20013;&#35745;&#31639;&#35270;&#35273;&#35266;&#23519;&#21644;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#22870;&#21169;&#20449;&#21495;&#65292;ARP&#26377;&#25928;&#32531;&#35299;&#20102;&#30446;&#26631;&#35823;&#27867;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#38754;&#23545;&#26410;&#30693;&#30340;&#25991;&#26412;&#25351;&#20196;&#26102;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#65292;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#36866;&#24212;&#26410;&#30693;&#29615;&#22659;&#30340;&#26234;&#33021;&#20307;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#36820;&#22238;&#26465;&#20214;&#31574;&#30053;(ARP)&#30340;&#39640;&#25928;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#21644;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#26469;&#25552;&#21319;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#23884;&#20837;&#31354;&#38388;(&#20363;&#22914;CLIP)&#20013;&#35745;&#31639;&#35270;&#35273;&#35266;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#22870;&#21169;&#20449;&#21495;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#29992;&#22810;&#27169;&#24577;&#22870;&#21169;&#26631;&#35760;&#30340;&#19987;&#23478;&#28436;&#31034;&#26469;&#35757;&#32451;&#19968;&#20010;&#36820;&#22238;&#26465;&#20214;&#31574;&#30053;&#12290;&#30001;&#20110;&#22810;&#27169;&#24577;&#22870;&#21169;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#25552;&#20379;&#33258;&#36866;&#24212;&#20449;&#21495;&#65292;&#25105;&#20204;&#30340;ARP&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#30446;&#26631;&#35823;&#27867;&#21270;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#30340;&#25991;&#26412;&#26465;&#20214;&#31574;&#30053;&#30456;&#27604;&#65292;&#21363;&#20351;&#38754;&#23545;&#26410;&#30693;&#30340;&#25991;&#26412;&#25351;&#20196;&#65292;&#25105;&#20204;&#30340;ARP&#22312;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#20247;&#12290;&#20026;&#20102;&#25552;&#39640;&#22870;&#21169;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing an agent capable of adapting to unseen environments remains a difficult challenge in imitation learning. This work presents Adaptive Return-conditioned Policy (ARP), an efficient framework designed to enhance the agent's generalization ability using natural language task descriptions and pre-trained multimodal encoders. Our key idea is to calculate a similarity between visual observations and natural language instructions in the pre-trained multimodal embedding space (such as CLIP) and use it as a reward signal. We then train a return-conditioned policy using expert demonstrations labeled with multimodal rewards. Because the multimodal rewards provide adaptive signals at each timestep, our ARP effectively mitigates the goal misgeneralization. This results in superior generalization performances even when faced with unseen text instructions, compared to existing text-conditioned policies. To improve the quality of rewards, we also introduce a fine-tuning method for pre-traine
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#37197;&#32622;&#30340;&#24211;&#65292;&#29992;&#20110;&#29983;&#25104;&#21644;&#22788;&#29702;&#36855;&#23467;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#35813;&#24211;&#29983;&#25104;&#19981;&#21516;&#20998;&#24067;&#30340;&#36855;&#23467;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#29983;&#25104;&#21442;&#25968;&#21644;&#29983;&#25104;&#35268;&#21017;&#36827;&#34892;&#33258;&#23450;&#20041;&#25511;&#21046;&#12290;&#21487;&#20197;&#25903;&#25345;&#22810;&#31181;&#36755;&#20986;&#26684;&#24335;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.10498</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#37197;&#32622;&#30340;&#24211;&#29992;&#20110;&#29983;&#25104;&#21644;&#25805;&#20316;&#36855;&#23467;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Configurable Library for Generating and Manipulating Maze Datasets. (arXiv:2309.10498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10498
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#37197;&#32622;&#30340;&#24211;&#65292;&#29992;&#20110;&#29983;&#25104;&#21644;&#22788;&#29702;&#36855;&#23467;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#35813;&#24211;&#29983;&#25104;&#19981;&#21516;&#20998;&#24067;&#30340;&#36855;&#23467;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#29983;&#25104;&#21442;&#25968;&#21644;&#29983;&#25104;&#35268;&#21017;&#36827;&#34892;&#33258;&#23450;&#20041;&#25511;&#21046;&#12290;&#21487;&#20197;&#25903;&#25345;&#22810;&#31181;&#36755;&#20986;&#26684;&#24335;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20998;&#24067;&#20559;&#31227;&#30340;&#21709;&#24212;&#26041;&#24335;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#25361;&#25112;&#12290;&#30001;&#20110;&#19981;&#21516;&#30340;&#29983;&#25104;&#31639;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#32454;&#33268;&#30340;&#24179;&#21488;&#26469;&#27169;&#25311;&#24494;&#22937;&#21644;&#26174;&#33879;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#36855;&#23467;&#20316;&#20026;&#19968;&#20010;&#20248;&#31168;&#30340;&#27979;&#35797;&#22522;&#20934;&#12290;&#20026;&#20102;&#25903;&#25345;&#23545;&#27169;&#22411;&#22312;&#20998;&#24067;&#20559;&#31163;&#25968;&#25454;&#19978;&#34892;&#20026;&#30340;&#31995;&#32479;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;maze-dataset&#8221;&#65292;&#19968;&#20010;&#21253;&#21547;&#36855;&#23467;&#27714;&#35299;&#20219;&#21153;&#30340;&#29983;&#25104;&#12289;&#22788;&#29702;&#21644;&#21487;&#35270;&#21270;&#25968;&#25454;&#38598;&#30340;&#32508;&#21512;&#24211;&#12290;&#20511;&#21161;&#36825;&#20010;&#24211;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36731;&#26494;&#21019;&#24314;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#23545;&#20351;&#29992;&#30340;&#29983;&#25104;&#31639;&#27861;&#12289;&#20256;&#36882;&#32473;&#36873;&#25321;&#31639;&#27861;&#30340;&#21442;&#25968;&#21644;&#29983;&#25104;&#30340;&#36855;&#23467;&#24517;&#39035;&#28385;&#36275;&#30340;&#31579;&#36873;&#22120;&#36827;&#34892;&#24191;&#27867;&#30340;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#23427;&#25903;&#25345;&#22810;&#31181;&#36755;&#20986;&#26684;&#24335;&#65292;&#21253;&#25324;&#26629;&#26684;&#21270;&#21644;&#22522;&#20110;&#25991;&#26412;&#30340;&#26684;&#24335;&#65292;&#36866;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#33258;&#22238;&#24402;&#21464;&#25442;&#27169;&#22411;&#12290;&#36825;&#20123;&#26684;&#24335;&#20197;&#21450;&#29992;&#20110;&#21487;&#35270;&#21270;&#21644;&#36716;&#25442;&#30340;&#24037;&#20855;&#30830;&#20445;&#20102;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how machine learning models respond to distributional shifts is a key research challenge. Mazes serve as an excellent testbed due to varied generation algorithms offering a nuanced platform to simulate both subtle and pronounced distributional shifts. To enable systematic investigations of model behavior on out-of-distribution data, we present $\texttt{maze-dataset}$, a comprehensive library for generating, processing, and visualizing datasets consisting of maze-solving tasks. With this library, researchers can easily create datasets, having extensive control over the generation algorithm used, the parameters fed to the algorithm of choice, and the filters that generated mazes must satisfy. Furthermore, it supports multiple output formats, including rasterized and text-based, catering to convolutional neural networks and autoregressive transformer models. These formats, along with tools for visualizing and converting between them, ensure versatility and adaptability in re
&lt;/p&gt;</description></item><item><title>&#22312;&#26080;&#32422;&#26463;&#29305;&#24449;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#19979;&#19981;&#22343;&#34913;&#25968;&#25454;&#30340;&#31070;&#32463;&#22604;&#32553;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2309.09725</link><description>&lt;p&gt;
&#26080;&#32422;&#26463;&#29305;&#24449;&#27169;&#22411;&#20013;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#19979;&#19981;&#21463;&#38480;&#30340;&#31070;&#32463;&#22604;&#32553;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Neural Collapse for Unconstrained Feature Model under Cross-entropy Loss with Imbalanced Data. (arXiv:2309.09725v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09725
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#32422;&#26463;&#29305;&#24449;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#19979;&#19981;&#22343;&#34913;&#25968;&#25454;&#30340;&#31070;&#32463;&#22604;&#32553;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25991;&#26412;&#22788;&#29702;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36825;&#20123;&#20855;&#26377;&#22823;&#37327;&#21442;&#25968;&#30340;DNNs&#22312;&#35757;&#32451;&#30340;&#26411;&#26399;&#38454;&#27573;&#65288;TPT&#65289;&#30340;&#29305;&#24449;&#34920;&#31034;&#21644;&#26411;&#23618;&#20998;&#31867;&#22120;&#20855;&#26377;&#30456;&#20284;&#30340;&#32467;&#26500;&#29305;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22914;&#26524;&#35757;&#32451;&#25968;&#25454;&#26159;&#24179;&#34913;&#30340;&#65288;&#27599;&#20010;&#31867;&#21035;&#20855;&#26377;&#30456;&#21516;&#25968;&#37327;&#30340;&#26679;&#26412;&#65289;&#65292;&#35266;&#23519;&#21040;&#26469;&#33258;&#21516;&#19968;&#31867;&#21035;&#30340;&#26679;&#26412;&#30340;&#29305;&#24449;&#21521;&#37327;&#25910;&#25947;&#21040;&#30456;&#24212;&#30340;&#31867;&#20869;&#22343;&#20540;&#29305;&#24449;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#25104;&#23545;&#35282;&#24230;&#30456;&#21516;&#12290;&#36825;&#19968;&#36855;&#20154;&#30340;&#29616;&#35937;&#34987;&#31216;&#20026;&#31070;&#32463;&#22604;&#32553;&#65288;NC&#65289;&#65292;&#30001;Papyan&#65292;Han&#21644;Donoho&#22312;2019&#24180;&#39318;&#27425;&#25552;&#20986;&#12290;&#26368;&#36817;&#30340;&#35768;&#22810;&#24037;&#20316;&#36890;&#36807;&#37319;&#29992;&#25152;&#35859;&#30340;&#26080;&#32422;&#26463;&#29305;&#24449;&#27169;&#22411;&#65288;UFM&#65289;&#22312;&#29702;&#35770;&#19978;&#35299;&#37322;&#20102;&#36825;&#19968;&#29616;&#35937;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26080;&#32422;&#26463;&#29305;&#24449;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#24357;&#34917;&#20102;NC&#29616;&#35937;&#23545;&#19981;&#22343;&#34913;&#25968;&#25454;&#22312;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#19979;&#30340;&#25299;&#23637;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the huge success of deep neural networks (DNNs) in various tasks of computer vision and text processing. Interestingly, these DNNs with massive number of parameters share similar structural properties on their feature representation and last-layer classifier at terminal phase of training (TPT). Specifically, if the training data are balanced (each class shares the same number of samples), it is observed that the feature vectors of samples from the same class converge to their corresponding in-class mean features and their pairwise angles are the same. This fascinating phenomenon is known as Neural Collapse (N C), first termed by Papyan, Han, and Donoho in 2019. Many recent works manage to theoretically explain this phenomenon by adopting so-called unconstrained feature model (UFM). In this paper, we study the extension of N C phenomenon to the imbalanced data under cross-entropy loss function in the context of unconstrained feature model. Our contribution is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RePo&#31639;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#21518;&#39564;&#21487;&#39044;&#27979;&#24615;&#30340;&#26041;&#24335;&#65292;&#22686;&#24378;&#20102;&#35270;&#35273;&#27169;&#22411;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#24377;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#23545;&#20887;&#20313;&#21644;&#20266;&#21464;&#21270;&#20855;&#26377;&#24377;&#24615;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#26041;&#27861;&#23545;&#35270;&#35273;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36816;&#34892;&#12290;</title><link>http://arxiv.org/abs/2309.00082</link><description>&lt;p&gt;
RePo: &#36890;&#36807;&#27491;&#21017;&#21270;&#21518;&#39564;&#21487;&#39044;&#27979;&#24615;&#22686;&#24378;&#24377;&#24615;&#27169;&#22411;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RePo: Resilient Model-Based Reinforcement Learning by Regularizing Posterior Predictability. (arXiv:2309.00082v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RePo&#31639;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#21518;&#39564;&#21487;&#39044;&#27979;&#24615;&#30340;&#26041;&#24335;&#65292;&#22686;&#24378;&#20102;&#35270;&#35273;&#27169;&#22411;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#24377;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#23545;&#20887;&#20313;&#21644;&#20266;&#21464;&#21270;&#20855;&#26377;&#24377;&#24615;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#26041;&#27861;&#23545;&#35270;&#35273;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#27169;&#22411;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#23558;&#22270;&#20687;&#35266;&#27979;&#32534;&#30721;&#20026;&#20302;&#32500;&#34920;&#31034;&#26041;&#24335;&#65292;&#36825;&#31181;&#26041;&#24335;&#26410;&#33021;&#28040;&#38500;&#20887;&#20313;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;&#36825;&#20123;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#20266;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#21363;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#32452;&#25104;&#37096;&#20998;&#30340;&#21464;&#21270;&#65292;&#22914;&#32972;&#26223;&#24178;&#25200;&#22240;&#32032;&#25110;&#20809;&#29031;&#26465;&#20214;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#35273;&#27169;&#22411;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#21040;&#20102;&#19968;&#31181;&#23545;&#36825;&#31181;&#20266;&#21464;&#21270;&#20855;&#26377;&#24377;&#24615;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#30446;&#26631;&#40723;&#21169;&#35813;&#34920;&#31034;&#22312;&#21160;&#21147;&#23398;&#21644;&#22870;&#21169;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#26368;&#22823;&#30340;&#39044;&#27979;&#24615;&#65292;&#21516;&#26102;&#38480;&#21046;&#20102;&#35266;&#27979;&#21040;&#28508;&#22312;&#34920;&#31034;&#30340;&#20449;&#24687;&#27969;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#19968;&#30446;&#26631;&#26497;&#22823;&#22686;&#24378;&#20102;&#35270;&#35273;&#27169;&#22411;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23545;&#35270;&#35273;&#24178;&#25200;&#30340;&#24377;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36816;&#34892;&#12290;&#28982;&#21518;&#25105;&#20204;&#23637;&#31034;&#20102;&#34429;&#28982;&#23398;&#20064;&#21040;&#30340;&#32534;&#30721;&#22120;&#23545;&#20266;&#21464;&#21270;&#20855;&#26377;&#24377;&#24615;&#65292;&#20294;&#22312;&#26174;&#33879;&#20998;&#24067;&#21464;&#21270;&#19979;&#24182;&#27809;&#26377;&#19981;&#21464;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22870;&#21169;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual model-based RL methods typically encode image observations into low-dimensional representations in a manner that does not eliminate redundant information. This leaves them susceptible to spurious variations -- changes in task-irrelevant components such as background distractors or lighting conditions. In this paper, we propose a visual model-based RL method that learns a latent representation resilient to such spurious variations. Our training objective encourages the representation to be maximally predictive of dynamics and reward, while constraining the information flow from the observation to the latent representation. We demonstrate that this objective significantly bolsters the resilience of visual model-based RL methods to visual distractors, allowing them to operate in dynamic environments. We then show that while the learned encoder is resilient to spirious variations, it is not invariant under significant distribution shift. To address this, we propose a simple reward-f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#23398;&#20064;&#30340;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#65288;CL-MAE&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#36974;&#32617;&#27169;&#22359;&#65292;&#36890;&#36807;&#26356;&#26032;&#36974;&#32617;&#31574;&#30053;&#26469;&#22686;&#21152;&#33258;&#30417;&#30563;&#37325;&#26500;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#26356;&#22797;&#26434;&#21644;&#21487;&#36801;&#31227;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.16572</link><description>&lt;p&gt;
CL-MAE: &#35838;&#31243;&#23398;&#20064;&#30340;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
CL-MAE: Curriculum-Learned Masked Autoencoders. (arXiv:2308.16572v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#23398;&#20064;&#30340;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#65288;CL-MAE&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#36974;&#32617;&#27169;&#22359;&#65292;&#36890;&#36807;&#26356;&#26032;&#36974;&#32617;&#31574;&#30053;&#26469;&#22686;&#21152;&#33258;&#30417;&#30563;&#37325;&#26500;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#26356;&#22797;&#26434;&#21644;&#21487;&#36801;&#31227;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#39044;&#25991;&#26412;&#20219;&#21153;&#65292;&#29992;&#20110;&#29983;&#25104;&#33021;&#22815;&#26377;&#25928;&#27867;&#21270;&#21040;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#40065;&#26834;&#34920;&#31034;&#12290;&#36890;&#24120;&#65292;&#36825;&#31181;&#26041;&#27861;&#28041;&#21450;&#22312;&#36755;&#20837;&#22270;&#20687;&#20013;&#38543;&#26426;&#36974;&#32617;&#34917;&#19969;&#65288;&#26631;&#35760;&#65289;&#65292;&#24182;&#19988;&#36974;&#32617;&#31574;&#30053;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#26032;&#36974;&#32617;&#31574;&#30053;&#20197;&#25345;&#32493;&#22686;&#21152;&#33258;&#30417;&#30563;&#37325;&#26500;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#25512;&#27979;&#65292;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#26356;&#22797;&#26434;&#21644;&#21487;&#36801;&#31227;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#23398;&#20064;&#36974;&#32617;&#27169;&#22359;&#65292;&#20855;&#26377;&#29983;&#25104;&#19981;&#21516;&#22797;&#26434;&#24230;&#36974;&#32617;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#35813;&#27169;&#22359;&#19982;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#27169;&#22359;&#19982;MAE&#19968;&#21516;&#35757;&#32451;&#65292;&#21516;&#26102;&#35843;&#25972;&#20854;&#34892;&#20026;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20174;MAE&#30340;&#21442;&#19982;&#32773;&#36807;&#28193;&#21040;MAE&#65288;&#20248;&#21270;&#30456;&#21516;&#30340;&#37325;&#26500;&#30446;&#26631;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked image modeling has been demonstrated as a powerful pretext task for generating robust representations that can be effectively generalized across multiple downstream tasks. Typically, this approach involves randomly masking patches (tokens) in input images, with the masking strategy remaining unchanged during training. In this paper, we propose a curriculum learning approach that updates the masking strategy to continually increase the complexity of the self-supervised reconstruction task. We conjecture that, by gradually increasing the task complexity, the model can learn more sophisticated and transferable representations. To facilitate this, we introduce a novel learnable masking module that possesses the capability to generate masks of different complexities, and integrate the proposed module into masked autoencoders (MAE). Our module is jointly trained with the MAE, while adjusting its behavior during training, transitioning from a partner to the MAE (optimizing the same rec
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;QMLS&#30340;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#32553;&#30701;&#33647;&#29289;&#30740;&#21457;&#30340;&#26102;&#38388;&#21644;&#38477;&#20302;&#25104;&#26412;&#12290;&#36890;&#36807;&#29983;&#25104;&#21629;&#20013;&#29289;&#21644;&#20248;&#21270;&#20998;&#23376;&#30340;&#36807;&#31243;&#65292;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#33647;&#29289;&#21457;&#29616;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.08561</link><description>&lt;p&gt;
&#26410;&#26469;&#33647;&#29289;&#21457;&#29616;&#30340;&#23454;&#26045;&#65306;&#22522;&#20110;&#37327;&#23376;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#25311;(QMLS)&#12290;
&lt;/p&gt;
&lt;p&gt;
Implementation of The Future of Drug Discovery: QuantumBased Machine Learning Simulation (QMLS). (arXiv:2308.08561v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08561
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;QMLS&#30340;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#32553;&#30701;&#33647;&#29289;&#30740;&#21457;&#30340;&#26102;&#38388;&#21644;&#38477;&#20302;&#25104;&#26412;&#12290;&#36890;&#36807;&#29983;&#25104;&#21629;&#20013;&#29289;&#21644;&#20248;&#21270;&#20998;&#23376;&#30340;&#36807;&#31243;&#65292;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#33647;&#29289;&#21457;&#29616;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#30740;&#21457;&#30340;&#30740;&#31350;&#19982;&#24320;&#21457;(R&amp;D)&#38454;&#27573;&#26159;&#19968;&#20010;&#28459;&#38271;&#32780;&#26114;&#36149;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#25913;&#38761;&#36825;&#20010;&#36807;&#31243;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#27010;&#24565;QMLS&#65292;&#23558;&#25972;&#20010;R&amp;D&#38454;&#27573;&#32553;&#30701;&#21040;&#19977;&#21040;&#20845;&#20010;&#26376;&#65292;&#25104;&#26412;&#20165;&#20026;&#20116;&#21040;&#20843;&#19975;&#32654;&#20803;&#12290;&#23545;&#20110;&#21629;&#20013;&#20135;&#29983;&#65292;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#29983;&#25104;(MLMG)&#26681;&#25454;&#30446;&#26631;&#34507;&#30333;&#30340;&#20998;&#23376;&#32467;&#26500;&#29983;&#25104;&#21487;&#33021;&#30340;&#21629;&#20013;&#29289;&#65292;&#32780;&#37327;&#23376;&#27169;&#25311;(QS)&#26681;&#25454;&#19982;&#30446;&#26631;&#34507;&#30333;&#30340;&#21453;&#24212;&#21644;&#32467;&#21512;&#25928;&#26524;&#36807;&#28388;&#21407;&#22987;&#23454;&#39564;&#20013;&#30340;&#20998;&#23376;&#12290;&#28982;&#21518;&#65292;&#23545;&#20110;&#38085;&#20248;&#21270;&#65292;&#20174;MLMG&#21644;QS&#29983;&#25104;&#21644;&#36807;&#28388;&#30340;&#32467;&#26524;&#20998;&#23376;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#21464;&#24322;(MLMV)&#23558;&#37027;&#20123;&#20986;&#29616;&#22312;&#20004;&#20010;&#36807;&#31243;&#20013;&#30340;&#20998;&#23376;&#21046;&#25104;&#25968;&#21313;&#31181;&#20998;&#23376;&#21464;&#20307;&#65292;&#32780;&#20854;&#20182;&#20998;&#23376;&#21482;&#21046;&#25104;&#20960;&#31181;&#21464;&#20307;&#12290;&#26368;&#21518;&#65292;&#25152;&#26377;&#20248;&#21270;&#30340;&#20998;&#23376;&#23558;&#32463;&#36807;&#22810;&#36718;&#39640;&#26631;&#20934;&#30340;QS&#36807;&#28388;&#65292;&#20197;&#30830;&#20445;&#21453;&#24212;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Research &amp; Development (R&amp;D) phase of drug development is a lengthy and costly process. To revolutionize this process, we introduce our new concept QMLS to shorten the whole R&amp;D phase to three to six months and decrease the cost to merely fifty to eighty thousand USD. For Hit Generation, Machine Learning Molecule Generation (MLMG) generates possible hits according to the molecular structure of the target protein while the Quantum Simulation (QS) filters molecules from the primary essay based on the reaction and binding effectiveness with the target protein. Then, For Lead Optimization, the resultant molecules generated and filtered from MLMG and QS are compared, and molecules that appear as a result of both processes will be made into dozens of molecular variations through Machine Learning Molecule Variation (MLMV), while others will only be made into a few variations. Lastly, all optimized molecules would undergo multiple rounds of QS filtering with a high standard for reaction ef
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#21644;&#23618;&#27425;&#32858;&#31867;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20892;&#23398;&#30740;&#31350;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#36890;&#36807;&#25972;&#21512;&#38543;&#26426;&#25928;&#24212;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#32467;&#26500;&#23398;&#20064;&#33021;&#21147;&#65292;&#23454;&#29616;&#22240;&#26524;&#20851;&#31995;&#32593;&#32476;&#30340;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.06399</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#21644;&#23618;&#27425;&#32858;&#31867;&#23398;&#20064;&#20855;&#26377;&#24322;&#26500;&#20892;&#19994;&#25968;&#25454;&#38598;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering. (arXiv:2308.06399v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#21644;&#23618;&#27425;&#32858;&#31867;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20892;&#23398;&#30740;&#31350;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#36890;&#36807;&#25972;&#21512;&#38543;&#26426;&#25928;&#24212;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#32467;&#26500;&#23398;&#20064;&#33021;&#21147;&#65292;&#23454;&#29616;&#22240;&#26524;&#20851;&#31995;&#32593;&#32476;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28041;&#21450;&#22810;&#26679;&#20294;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;&#20013;&#65292;&#20854;&#20013;&#21327;&#21464;&#37327;&#19982;&#32467;&#26524;&#20043;&#38388;&#30340;&#20851;&#32852;&#21487;&#33021;&#20250;&#26377;&#25152;&#19981;&#21516;&#65292;&#22312;&#21253;&#25324;&#20892;&#23398;&#30740;&#31350;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#37117;&#24456;&#26222;&#36941;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24120;&#24120;&#20351;&#29992;&#23618;&#27425;&#27169;&#22411;&#65292;&#20063;&#34987;&#31216;&#20026;&#22810;&#23618;&#27169;&#22411;&#65292;&#26469;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#65292;&#24182;&#36866;&#24212;&#23427;&#20204;&#30340;&#19981;&#21516;&#29305;&#28857;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#32467;&#26500;&#36229;&#20986;&#20102;&#31616;&#21333;&#30340;&#24322;&#36136;&#24615;&#65292;&#22240;&#20026;&#21464;&#37327;&#36890;&#24120;&#24418;&#25104;&#22797;&#26434;&#30340;&#22240;&#26524;&#20851;&#31995;&#32593;&#32476;&#12290;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;BNs&#65289;&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#26469;&#27169;&#25311;&#36825;&#31181;&#20851;&#31995;&#30340;&#24378;&#22823;&#26694;&#26550;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#38543;&#26426;&#25928;&#24212;&#25972;&#21512;&#21040;BN&#23398;&#20064;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#23618;&#27425;&#25968;&#25454;&#12290;&#26469;&#33258;&#30495;&#23454;&#20892;&#23398;&#35797;&#39564;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#32467;&#26500;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Research involving diverse but related data sets, where associations between covariates and outcomes may vary, is prevalent in various fields including agronomic studies. In these scenarios, hierarchical models, also known as multilevel models, are frequently employed to assimilate information from different data sets while accommodating their distinct characteristics. However, their structure extend beyond simple heterogeneity, as variables often form complex networks of causal relationships.  Bayesian networks (BNs) provide a powerful framework for modelling such relationships using directed acyclic graphs to illustrate the connections between variables. This study introduces a novel approach that integrates random effects into BN learning. Rooted in linear mixed-effects models, this approach is particularly well-suited for handling hierarchical data. Results from a real-world agronomic trial suggest that employing this approach enhances structural learning, leading to the discovery 
&lt;/p&gt;</description></item><item><title>AgentBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#20195;&#29702;&#20154;&#30340;&#22810;&#32500;&#24230;&#22522;&#20934;&#65292;&#21457;&#29616;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#65292;&#21830;&#19994;LLMs&#22312;&#20805;&#24403;&#20195;&#29702;&#20154;&#26041;&#38754;&#34920;&#29616;&#24378;&#21170;&#65292;&#20294;&#19982;&#24320;&#28304;&#31454;&#20105;&#23545;&#25163;&#30456;&#27604;&#65292;&#23384;&#22312;&#26174;&#33879;&#24615;&#33021;&#24046;&#36317;&#12290;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#22312;&#38271;&#26399;&#25512;&#29702;&#12289;&#20915;&#31574;&#21644;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#19978;&#30340;&#29942;&#39048;&#12290;</title><link>http://arxiv.org/abs/2308.03688</link><description>&lt;p&gt;
AgentBench: &#35780;&#20272;LLMs&#20316;&#20026;&#20195;&#29702;&#20154;
&lt;/p&gt;
&lt;p&gt;
AgentBench: Evaluating LLMs as Agents. (arXiv:2308.03688v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03688
&lt;/p&gt;
&lt;p&gt;
AgentBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#20195;&#29702;&#20154;&#30340;&#22810;&#32500;&#24230;&#22522;&#20934;&#65292;&#21457;&#29616;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#65292;&#21830;&#19994;LLMs&#22312;&#20805;&#24403;&#20195;&#29702;&#20154;&#26041;&#38754;&#34920;&#29616;&#24378;&#21170;&#65292;&#20294;&#19982;&#24320;&#28304;&#31454;&#20105;&#23545;&#25163;&#30456;&#27604;&#65292;&#23384;&#22312;&#26174;&#33879;&#24615;&#33021;&#24046;&#36317;&#12290;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#22312;&#38271;&#26399;&#25512;&#29702;&#12289;&#20915;&#31574;&#21644;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#19978;&#30340;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21464;&#24471;&#36234;&#26469;&#36234;&#26234;&#33021;&#21644;&#33258;&#20027;&#65292;&#38024;&#23545;&#20256;&#32479;&#30340;NLP&#20219;&#21153;&#20043;&#22806;&#30340;&#29616;&#23454;&#19990;&#30028;&#23454;&#38469;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#22312;&#20114;&#21160;&#29615;&#22659;&#20013;&#35780;&#20272;LLMs&#20316;&#20026;&#20195;&#29702;&#20154;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AgentBench&#65292;&#19968;&#20010;&#22810;&#32500;&#24230;&#28436;&#21464;&#30340;&#22522;&#20934;&#65292;&#30446;&#21069;&#21253;&#25324;8&#20010;&#19981;&#21516;&#30340;&#29615;&#22659;&#65292;&#20197;&#35780;&#20272;LLM&#20316;&#20026;&#20195;&#29702;&#20154;&#22312;&#22810;&#36718;&#24320;&#25918;&#24335;&#29983;&#25104;&#35774;&#32622;&#20013;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;27&#20010;&#22522;&#20110;API&#21644;&#24320;&#28304;&#30340;LLM&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#39030;&#32423;&#21830;&#19994;LLM&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#20195;&#29702;&#20154;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#19982;&#24320;&#28304;&#31454;&#20105;&#23545;&#25163;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#24456;&#22823;&#12290;&#25105;&#20204;&#25214;&#20986;&#20102;&#29615;&#22659;&#21644;LLM&#20013;&#22833;&#36133;&#30340;&#20856;&#22411;&#21407;&#22240;&#65292;&#34920;&#26126;&#38271;&#26399;&#25512;&#29702;&#12289;&#20915;&#31574;&#21644;&#36981;&#24490;&#25351;&#31034;&#33021;&#21147;&#19981;&#20339;&#26159;&#24320;&#21457;&#21487;&#29992;LLM&#20195;&#29702;&#20154;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#36890;&#36807;&#23545;&#20195;&#30721;&#21644;&#39640;&#36136;&#37327;&#36827;&#34892;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks. As a result, there has been an urgent need to evaluate LLMs as agents on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional evolving benchmark that currently consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting. Our extensive test over 27 API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and OSS competitors. We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents. Training on code and high quality 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36229;&#36234;&#39640;&#26031;&#25968;&#25454;&#30340;&#21333;&#25351;&#25968;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#23545;&#31283;&#23450;&#24615;&#21644;&#23545;&#31216;&#24615;&#30340;&#36829;&#21453;&#24773;&#20917;&#19979;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.15804</link><description>&lt;p&gt;
&#20851;&#20110;&#39640;&#26031;&#25968;&#25454;&#20043;&#22806;&#30340;&#21333;&#25351;&#25968;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
On Single Index Models beyond Gaussian Data. (arXiv:2307.15804v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15804
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36229;&#36234;&#39640;&#26031;&#25968;&#25454;&#30340;&#21333;&#25351;&#25968;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#23545;&#31283;&#23450;&#24615;&#21644;&#23545;&#31216;&#24615;&#30340;&#36829;&#21453;&#24773;&#20917;&#19979;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#39640;&#32500;&#20989;&#25968;&#24050;&#25104;&#20026;&#30740;&#31350;&#20351;&#29992;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#34892;&#20026;&#30340;&#20016;&#23500;&#26694;&#26550;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#32447;&#24615;&#27169;&#22411;&#20043;&#22806;&#36827;&#34892;&#29305;&#24449;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#20854;&#20013;&#26368;&#31616;&#21333;&#30340;&#26159;&#21333;&#25351;&#25968;&#27169;&#22411; $f(x) = \phi( x \cdot \theta^*)$&#65292;&#20854;&#20013;&#26631;&#31614;&#30001;&#19968;&#20010;&#26410;&#30693;&#30340;&#19968;&#32500;&#25237;&#24433; $\theta^*$ &#24212;&#29992;&#20110;&#20219;&#24847;&#38750;&#32447;&#24615;&#26631;&#37327;&#36830;&#25509;&#20989;&#25968; $\phi$ &#20135;&#29983;&#12290;&#36890;&#36807;&#19987;&#27880;&#20110;&#39640;&#26031;&#25968;&#25454;&#65292;&#26368;&#36817;&#20960;&#39033;&#30740;&#31350;&#24037;&#20316;&#24314;&#31435;&#20102;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#22270;&#26223;&#65292;&#23558;&#20449;&#24687;&#25351;&#25968;&#65288;&#19982;&#36830;&#25509;&#20989;&#25968;&#30340;&#27491;&#21017;&#24615;&#30456;&#20851;&#65289;&#19982;&#25152;&#38656;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#36827;&#34892;&#20102;&#25511;&#21046;&#12290;&#23454;&#36136;&#19978;&#65292;&#36825;&#20123;&#24037;&#20855;&#21033;&#29992;&#20102;&#39640;&#26031;&#20998;&#24067;&#30340;&#31283;&#23450;&#24615;&#21644;&#29699;&#23545;&#31216;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174; \cite{arous2020online} &#30340;&#26694;&#26550;&#20986;&#21457;&#65292;&#25506;&#32034;&#20102;&#36229;&#36234;&#39640;&#26031;&#35774;&#23450;&#30340;&#36825;&#20010;&#22270;&#26223;&#30340;&#25193;&#23637;&#65292;&#20854;&#20013;&#31283;&#23450;&#24615;&#25110;&#23545;&#31216;&#24615;&#21487;&#33021;&#34987;&#36829;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse high-dimensional functions have arisen as a rich framework to study the behavior of gradient-descent methods using shallow neural networks, showcasing their ability to perform feature learning beyond linear models. Amongst those functions, the simplest are single-index models $f(x) = \phi( x \cdot \theta^*)$, where the labels are generated by an arbitrary non-linear scalar link function $\phi$ applied to an unknown one-dimensional projection $\theta^*$ of the input data. By focusing on Gaussian data, several recent works have built a remarkable picture, where the so-called information exponent (related to the regularity of the link function) controls the required sample complexity. In essence, these tools exploit the stability and spherical symmetry of Gaussian distributions. In this work, building from the framework of \cite{arous2020online}, we explore extensions of this picture beyond the Gaussian setting, where both stability or symmetry might be violated. Focusing on the pl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#36807;&#23616;&#37096;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#22810;&#20803;&#27491;&#24577;CDF&#65292;&#39640;&#25928;&#35745;&#31639;&#22810;&#31867;&#21035;&#21028;&#21035;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#30340;&#20998;&#26512;&#20272;&#35745;&#22120;&#12290;&#23454;&#39564;&#35777;&#23454;&#36825;&#20123;&#20272;&#35745;&#22120;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#35745;&#31639;&#20102;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13885</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#30340;&#39640;&#25928;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Efficient Estimation of the Local Robustness of Machine Learning Models. (arXiv:2307.13885v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#36807;&#23616;&#37096;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#22810;&#20803;&#27491;&#24577;CDF&#65292;&#39640;&#25928;&#35745;&#31639;&#22810;&#31867;&#21035;&#21028;&#21035;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#30340;&#20998;&#26512;&#20272;&#35745;&#22120;&#12290;&#23454;&#39564;&#35777;&#23454;&#36825;&#20123;&#20272;&#35745;&#22120;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#35745;&#31639;&#20102;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#23545;&#22122;&#22768;&#36755;&#20837;&#25968;&#25454;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22122;&#22768;&#65288;&#36890;&#24120;&#26159;&#38543;&#26426;&#30340;&#65289;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#21487;&#20197;&#36890;&#36807;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#26469;&#25429;&#25417;&#65292;&#21363;&#22312;&#36755;&#20837;&#21608;&#22260;&#30340;&#23616;&#37096;&#21306;&#22495;&#20869;&#27169;&#22411;&#39044;&#27979;&#30340;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#30340;&#35745;&#31639;&#23616;&#37096;&#40065;&#26834;&#24615;&#30340;&#26420;&#32032;&#26041;&#27861;&#22312;&#32479;&#35745;&#19978;&#26159;&#20302;&#25928;&#30340;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#24212;&#29992;&#32780;&#35328;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23616;&#37096;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#22810;&#20803;&#27491;&#24577;CDF&#24320;&#21457;&#20102;&#39318;&#20010;&#20998;&#26512;&#20272;&#35745;&#22120;&#65292;&#20197;&#39640;&#25928;&#35745;&#31639;&#22810;&#31867;&#21035;&#21028;&#21035;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#36825;&#20123;&#20272;&#35745;&#22120;&#30340;&#25512;&#23548;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23616;&#37096;&#40065;&#26834;&#24615;&#19982;&#38543;&#26426;&#24179;&#28369;&#21644;softmax&#27010;&#29575;&#31561;&#27010;&#24565;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#36825;&#20123;&#20272;&#35745;&#22120;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#35745;&#31639;&#20102;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models often need to be robust to noisy input data. The effect of real-world noise (which is often random) on model predictions is captured by a model's local robustness, i.e., the consistency of model predictions in a local region around an input. However, the na\"ive approach to computing local robustness based on Monte-Carlo sampling is statistically inefficient, leading to prohibitive computational costs for large-scale applications. In this work, we develop the first analytical estimators to efficiently compute local robustness of multi-class discriminative models using local linear function approximation and the multivariate Normal CDF. Through the derivation of these estimators, we show how local robustness is connected to concepts such as randomized smoothing and softmax probability. We also confirm empirically that these estimators accurately and efficiently compute the local robustness of standard deep learning models. In addition, we demonstrate these estima
&lt;/p&gt;</description></item><item><title>WebArena&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;&#65292;&#23427;&#21253;&#21547;&#20102;&#23436;&#20840;&#21151;&#33021;&#30340;&#32593;&#31449;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;WebArena&#36824;&#21457;&#24067;&#20102;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.13854</link><description>&lt;p&gt;
WebArena: &#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
WebArena: A Realistic Web Environment for Building Autonomous Agents. (arXiv:2307.13854v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13854
&lt;/p&gt;
&lt;p&gt;
WebArena&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;&#65292;&#23427;&#21253;&#21547;&#20102;&#23436;&#20840;&#21151;&#33021;&#30340;&#32593;&#31449;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;WebArena&#36824;&#21457;&#24067;&#20102;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#36827;&#34892;&#26085;&#24120;&#20219;&#21153;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#28508;&#21147;&#36880;&#28176;&#26174;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26234;&#33021;&#20307;&#20027;&#35201;&#26159;&#22312;&#31616;&#21270;&#30340;&#21512;&#25104;&#29615;&#22659;&#20013;&#21019;&#24314;&#21644;&#27979;&#35797;&#30340;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#24230;&#36924;&#30495;&#19988;&#21487;&#22797;&#29616;&#30340;&#26234;&#33021;&#20307;&#25351;&#20196;&#21644;&#25511;&#21046;&#29615;&#22659;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#22312;&#32593;&#31449;&#19978;&#25191;&#34892;&#20219;&#21153;&#30340;&#26234;&#33021;&#20307;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;&#22235;&#20010;&#24120;&#35265;&#39046;&#22495;&#30340;&#23436;&#20840;&#21151;&#33021;&#32593;&#31449;&#30340;&#29615;&#22659;&#65292;&#20998;&#21035;&#26159;&#30005;&#23376;&#21830;&#21153;&#12289;&#31038;&#20132;&#35770;&#22363;&#35752;&#35770;&#12289;&#21327;&#21516;&#36719;&#20214;&#24320;&#21457;&#21644;&#20869;&#23481;&#31649;&#29702;&#12290;&#25105;&#20204;&#30340;&#29615;&#22659;&#20351;&#29992;&#24037;&#20855;&#65288;&#22914;&#22320;&#22270;&#65289;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#65288;&#22914;&#29992;&#25143;&#25163;&#20876;&#65289;&#26469;&#40723;&#21169;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#22312;&#25105;&#20204;&#30340;&#29615;&#22659;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#32452;&#37325;&#28857;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;&#25105;&#20204;&#22522;&#20934;&#20219;&#21153;&#20855;&#26377;&#22810;&#26679;&#24615;&#21644;&#38271;&#36828;&#30340;&#35270;&#37326;&#65292;&#24182;&#19988;&#34987;&#35774;&#35745;&#20026;&#40723;&#21169;&#26234;&#33021;&#20307;&#36827;&#34892;&#26356;&#28145;&#23618;&#27425;&#30340;&#20219;&#21153;&#29702;&#35299;&#21644;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged. However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation. In this paper, we build an environment for agent command and control that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and are desi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#20010;&#24615;&#21270;&#33647;&#29289;&#21327;&#21516;&#20316;&#29992;&#24182;&#36827;&#34892;&#33647;&#29289;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23567;&#22411;&#30340;&#20010;&#24615;&#21270;&#25968;&#25454;&#38598;&#65292;&#19981;&#20381;&#36182;&#20110;&#25991;&#26412;&#35821;&#26009;&#24211;&#12289;&#20998;&#23376;&#25351;&#32441;&#25110;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11694</link><description>&lt;p&gt;
SynerGPT:&#19978;&#19979;&#25991;&#23398;&#20064;&#29992;&#20110;&#20010;&#24615;&#21270;&#33647;&#29289;&#21327;&#21516;&#20316;&#29992;&#39044;&#27979;&#21644;&#33647;&#29289;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design. (arXiv:2307.11694v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#20010;&#24615;&#21270;&#33647;&#29289;&#21327;&#21516;&#20316;&#29992;&#24182;&#36827;&#34892;&#33647;&#29289;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23567;&#22411;&#30340;&#20010;&#24615;&#21270;&#25968;&#25454;&#38598;&#65292;&#19981;&#20381;&#36182;&#20110;&#25991;&#26412;&#35821;&#26009;&#24211;&#12289;&#20998;&#23376;&#25351;&#32441;&#25110;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#33647;&#29289;&#30340;&#21327;&#21516;&#32452;&#21512;&#21487;&#20197;&#21152;&#36895;&#30284;&#30151;&#27835;&#30103;&#30340;&#21457;&#29616;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#27963;&#26816;&#32454;&#32990;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#32622;&#21644;&#27169;&#22411;&#29992;&#20110;&#19978;&#19979;&#25991;&#20013;&#30340;&#33647;&#29289;&#21327;&#21516;&#23398;&#20064;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#23567;&#30340;&#8220;&#20010;&#24615;&#21270;&#25968;&#25454;&#38598;&#8221;&#65292;&#20854;&#20013;&#21253;&#21547;&#29305;&#23450;&#30284;&#30151;&#38774;&#32454;&#32990;&#19978;&#19979;&#25991;&#20013;&#30340;10-20&#20010;&#33647;&#29289;&#21327;&#21516;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#39044;&#27979;&#35813;&#19978;&#19979;&#25991;&#20013;&#30340;&#39069;&#22806;&#33647;&#29289;&#21327;&#21516;&#20851;&#31995;&#12290;&#21463;&#26368;&#36817;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#35813;&#24037;&#20316;&#36890;&#36807;&#39044;&#35757;&#32451;GPT&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#26469;&#8220;&#19978;&#19979;&#25991;&#23398;&#20064;&#8221;&#24120;&#35265;&#30340;&#21151;&#33021;&#31867;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181; &#26032;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#20351;GPT&#27169;&#22411;&#33021;&#22815;&#19978;&#19979;&#25991;&#23398;&#20064;&#8220;&#33647;&#29289;&#21327;&#21516;&#21151;&#33021;&#8221;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411; - &#19981;&#20351;&#29992;&#20219;&#20309;&#25991;&#26412;&#35821;&#26009;&#24211;&#65292;&#20998;&#23376;&#25351;&#32441;&#65292;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#25110;&#20219;&#20309;&#20854;&#20182;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782; - &#33021;&#22815;&#21462;&#24471;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25105;&#20204;&#30340;&#19978;&#19979;&#25991;&#26041;&#27861;&#19982;&#36951;&#20256;&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#20248;&#21270;&#27169;&#22411;&#25552;&#31034;&#24182;&#36873;&#25321;&#21327;&#21516;&#20505;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting synergistic drug combinations can help accelerate discovery of cancer treatments, particularly therapies personalized to a patient's specific tumor via biopsied cells. In this paper, we propose a novel setting and models for in-context drug synergy learning. We are given a small "personalized dataset" of 10-20 drug synergy relationships in the context of specific cancer cell targets. Our goal is to predict additional drug synergy relationships in that context. Inspired by recent work that pre-trains a GPT language model (LM) to "in-context learn" common function classes, we devise novel pre-training schemes that enable a GPT model to in-context learn "drug synergy functions". Our model -- which does not use any textual corpora, molecular fingerprints, protein interaction or any other domain-specific knowledge -- is able to achieve competitive results. We further integrate our in-context approach with a genetic algorithm to optimize model prompts and select synergy candidates
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#25163;&#24037;&#26631;&#35760;&#26118;&#34411;&#22270;&#20687;&#25968;&#25454;&#38598;BIOSCAN-Insect&#65292;&#29992;&#20110;&#23545;&#26118;&#34411;&#29983;&#29289;&#22810;&#26679;&#24615;&#36827;&#34892;&#32534;&#30446;&#12290;&#35813;&#25968;&#25454;&#38598;&#36824;&#20855;&#26377;&#24341;&#20154;&#27880;&#30446;&#30340;&#29305;&#24449;&#65292;&#23545;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20063;&#20855;&#26377;&#30740;&#31350;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.10455</link><description>&lt;p&gt;
&#26397;&#30528;&#20840;&#29699;&#29983;&#29289;&#22810;&#26679;&#24615;&#35780;&#20272;&#36808;&#20986;&#30340;&#19968;&#27493;&#65306;BIOSCAN-1M&#26118;&#34411;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset. (arXiv:2307.10455v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10455
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#25163;&#24037;&#26631;&#35760;&#26118;&#34411;&#22270;&#20687;&#25968;&#25454;&#38598;BIOSCAN-Insect&#65292;&#29992;&#20110;&#23545;&#26118;&#34411;&#29983;&#29289;&#22810;&#26679;&#24615;&#36827;&#34892;&#32534;&#30446;&#12290;&#35813;&#25968;&#25454;&#38598;&#36824;&#20855;&#26377;&#24341;&#20154;&#27880;&#30446;&#30340;&#29305;&#24449;&#65292;&#23545;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20063;&#20855;&#26377;&#30740;&#31350;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23545;&#26118;&#34411;&#29983;&#29289;&#22810;&#26679;&#24615;&#36827;&#34892;&#32534;&#30446;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#25163;&#24037;&#26631;&#35760;&#26118;&#34411;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#21363;BIOSCAN-Insect&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#35760;&#24405;&#37117;&#30001;&#19987;&#23478;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#19988;&#20855;&#26377;&#30456;&#20851;&#30340;&#36951;&#20256;&#20449;&#24687;&#65292;&#21253;&#25324;&#21407;&#22987;&#26680;&#33527;&#37240;&#26465;&#24418;&#30721;&#24207;&#21015;&#21644;&#20998;&#37197;&#30340;&#26465;&#24418;&#30721;&#32034;&#24341;&#21495;&#65292;&#36825;&#20123;&#26159;&#22522;&#20110;&#36951;&#20256;&#30340;&#29289;&#31181;&#20998;&#31867;&#30340;&#20195;&#29702;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31934;&#36873;&#30340;&#30334;&#19975;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#20027;&#35201;&#29992;&#20110;&#35757;&#32451;&#33021;&#22815;&#25552;&#20379;&#22522;&#20110;&#22270;&#20687;&#30340;&#20998;&#31867;&#35780;&#20272;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#65292;&#20294;&#35813;&#25968;&#25454;&#38598;&#36824;&#20855;&#26377;&#24341;&#20154;&#27880;&#30446;&#30340;&#29305;&#24449;&#65292;&#23545;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20063;&#20855;&#26377;&#30740;&#31350;&#20215;&#20540;&#12290;&#30001;&#20110;&#25968;&#25454;&#38598;&#22266;&#26377;&#30340;&#29983;&#29289;&#24615;&#36136;&#65292;&#23637;&#29616;&#20986;&#20102;&#20855;&#26377;&#38271;&#23614;&#31867;&#21035;&#19981;&#24179;&#34913;&#20998;&#24067;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#20998;&#31867;&#26631;&#31614;&#26159;&#19968;&#20010;&#20998;&#23618;&#20998;&#31867;&#26041;&#26696;&#65292;&#22312;&#36739;&#20302;&#32423;&#21035;&#19978;&#21576;&#29616;&#20986;&#39640;&#24230;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#38500;&#20102;&#28608;&#21457;&#23545;&#29983;&#29289;&#22810;&#26679;&#24615;&#30740;&#31350;&#30340;&#20852;&#36259;&#22806;&#65292;&#35813;&#25968;&#25454;&#38598;&#36824;&#20419;&#36827;&#20102;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#28145;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an effort to catalog insect biodiversity, we propose a new large dataset of hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is taxonomically classified by an expert, and also has associated genetic information including raw nucleotide barcode sequences and assigned barcode index numbers, which are genetically-based proxies for species classification. This paper presents a curated million-image dataset, primarily to train computer-vision models capable of providing image-based taxonomic assessment, however, the dataset also presents compelling characteristics, the study of which would be of interest to the broader machine learning community. Driven by the biological nature inherent to the dataset, a characteristic long-tailed class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is a hierarchical classification scheme, presenting a highly fine-grained classification problem at lower levels. Beyond spurring interest in biodiversity research w
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#21069;softmax&#20998;&#25968;&#30340;&#24402;&#23646;&#26041;&#27861;&#30340;&#19968;&#20010;&#28431;&#27934;&#65292;&#35813;&#26041;&#27861;&#29992;&#20110;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36755;&#20986;&#12290;&#19982;&#23545;&#25239;&#24615;&#25915;&#20987;&#19981;&#21516;&#65292;&#20316;&#32773;&#20851;&#27880;&#30340;&#26159;&#23545;&#24402;&#23646;&#26041;&#27861;&#36827;&#34892;&#23567;&#20462;&#25913;&#21487;&#33021;&#23548;&#33268;&#30340;&#24433;&#21709;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2307.03305</link><description>&lt;p&gt;
&#20351;&#29992;&#21069;softmax&#20998;&#25968;&#30340;&#24402;&#23646;&#26041;&#27861;&#30340;&#19968;&#20010;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
A Vulnerability of Attribution Methods Using Pre-Softmax Scores. (arXiv:2307.03305v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03305
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#21069;softmax&#20998;&#25968;&#30340;&#24402;&#23646;&#26041;&#27861;&#30340;&#19968;&#20010;&#28431;&#27934;&#65292;&#35813;&#26041;&#27861;&#29992;&#20110;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36755;&#20986;&#12290;&#19982;&#23545;&#25239;&#24615;&#25915;&#20987;&#19981;&#21516;&#65292;&#20316;&#32773;&#20851;&#27880;&#30340;&#26159;&#23545;&#24402;&#23646;&#26041;&#27861;&#36827;&#34892;&#23567;&#20462;&#25913;&#21487;&#33021;&#23548;&#33268;&#30340;&#24433;&#21709;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#31867;&#29992;&#20110;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36755;&#20986;&#30340;&#24402;&#23646;&#26041;&#27861;&#30340;&#19968;&#20010;&#28431;&#27934;&#12290;&#24050;&#30693;&#36825;&#31181;&#31867;&#22411;&#30340;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21363;&#36755;&#20837;&#30340;&#24494;&#23567;&#25200;&#21160;&#21487;&#33021;&#20250;&#25913;&#21464;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#19982;&#27492;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#23545;&#24402;&#23646;&#26041;&#27861;&#36827;&#34892;&#23567;&#20462;&#25913;&#21487;&#33021;&#23548;&#33268;&#30340;&#24433;&#21709;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
We discuss a vulnerability involving a category of attribution methods used to provide explanations for the outputs of convolutional neural networks working as classifiers. It is known that this type of networks are vulnerable to adversarial attacks, in which imperceptible perturbations of the input may alter the outputs of the model. In contrast, here we focus on effects that small modifications in the model may cause on the attribution method without altering the model outputs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#29992;&#20110;&#39640;&#25928;&#23398;&#20064;&#24179;&#38754;&#22270;&#23436;&#22791;&#19981;&#21464;&#37327;&#30340;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2307.01180</link><description>&lt;p&gt;
PlanE: &#24179;&#38754;&#22270;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PlanE: Representation Learning over Planar Graphs. (arXiv:2307.01180v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#29992;&#20110;&#39640;&#25928;&#23398;&#20064;&#24179;&#38754;&#22270;&#23436;&#22791;&#19981;&#21464;&#37327;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26480;&#20986;&#27169;&#22411;&#65292;&#20854;&#24605;&#24819;&#26159;&#36890;&#36807;&#19968;&#31995;&#21015;&#21464;&#25442;&#26469;&#36845;&#20195;&#35745;&#31639;&#36755;&#20837;&#22270;&#20013;&#33410;&#28857;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#23398;&#20064;&#21040;&#30340;&#22270;&#20989;&#25968;&#22312;&#22270;&#21516;&#26500;&#26102;&#26159;&#19981;&#21464;&#30340;&#65292;&#20174;&#32780;&#20351;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#25104;&#20026;&#22270;&#19981;&#21464;&#37327;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#36825;&#31867;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#22270;&#19981;&#21464;&#37327;&#26159;&#19981;&#23436;&#22791;&#30340;&#65306;&#23384;&#22312;&#19968;&#20123;&#38750;&#21516;&#26500;&#30340;&#22270;&#23545;&#65292;&#26631;&#20934;&#22270;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#21306;&#20998;&#23427;&#20204;&#12290;&#36825;&#22312;&#23545;&#19968;&#33324;&#22270;&#36827;&#34892;&#21516;&#26500;&#24615;&#27979;&#35797;&#30340;&#35745;&#31639;&#22256;&#38590;&#24615;&#30340;&#24773;&#20917;&#19979;&#24182;&#19981;&#20196;&#20154;&#24778;&#35766;&#65292;&#20294;&#23545;&#20110;&#19968;&#20123;&#29305;&#27530;&#30340;&#22270;&#31867;&#26469;&#35828;&#65292;&#24773;&#20917;&#21487;&#33021;&#26377;&#25152;&#19981;&#21516;&#65292;&#20363;&#22914;&#24179;&#38754;&#22270;&#65292;&#23545;&#20110;&#36825;&#20123;&#22270;&#65292;&#24050;&#30693;&#23384;&#22312;&#39640;&#25928;&#30340;&#22270;&#21516;&#26500;&#27979;&#35797;&#31639;&#27861;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#29992;&#20110;&#39640;&#25928;&#23398;&#20064;&#24179;&#38754;&#22270;&#23436;&#22791;&#19981;&#21464;&#37327;&#30340;&#26550;&#26500;&#12290;&#21463;Hopcroft&#21644;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks are prominent models for representation learning over graphs, where the idea is to iteratively compute representations of nodes of an input graph through a series of transformations in such a way that the learned graph function is isomorphism invariant on graphs, which makes the learned representations graph invariants. On the other hand, it is well-known that graph invariants learned by these class of models are incomplete: there are pairs of non-isomorphic graphs which cannot be distinguished by standard graph neural networks. This is unsurprising given the computational difficulty of graph isomorphism testing on general graphs, but the situation begs to differ for special graph classes, for which efficient graph isomorphism testing algorithms are known, such as planar graphs. The goal of this work is to design architectures for efficiently learning complete invariants of planar graphs. Inspired by the classical planar graph isomorphism algorithm of Hopcroft and
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;SPINN&#65289;&#65292;&#36890;&#36807;&#36880;&#20010;&#22788;&#29702;&#36724;&#26469;&#26174;&#33879;&#20943;&#23569;&#20102;&#22810;&#32500; PDE &#20013;&#30340;&#32593;&#32476;&#20256;&#25773;&#25968;&#37327;&#65292;&#24182;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#21333;&#20010;&#26222;&#36890; GPU &#19978;&#20351;&#29992;&#22823;&#37327;&#30340;&#37197;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.15969</link><description>&lt;p&gt;
&#21487;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Separable Physics-Informed Neural Networks. (arXiv:2306.15969v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15969
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;SPINN&#65289;&#65292;&#36890;&#36807;&#36880;&#20010;&#22788;&#29702;&#36724;&#26469;&#26174;&#33879;&#20943;&#23569;&#20102;&#22810;&#32500; PDE &#20013;&#30340;&#32593;&#32476;&#20256;&#25773;&#25968;&#37327;&#65292;&#24182;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#21333;&#20010;&#26222;&#36890; GPU &#19978;&#20351;&#29992;&#22823;&#37327;&#30340;&#37197;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;&#26377;&#24076;&#26395;&#30340;&#22522;&#20110;&#25968;&#25454;&#30340;PDE&#27714;&#35299;&#22120;&#65292;&#22312;&#21508;&#31181;PDE&#19978;&#26174;&#31034;&#20986;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;PINNs&#26469;&#35299;&#20915;&#22810;&#32500;PDE&#21644;&#36924;&#36817;&#39640;&#24230;&#22797;&#26434;&#35299;&#20989;&#25968;&#23384;&#22312;&#26681;&#26412;&#38480;&#21046;&#12290;&#22312;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;PDE&#19978;&#25152;&#38656;&#30340;&#35757;&#32451;&#28857;&#25968;&#37327;(&#37197;&#28857;)&#22823;&#22823;&#22686;&#21152;&#65292;&#20294;&#30001;&#20110;&#26114;&#36149;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#24222;&#22823;&#30340;&#20869;&#23384;&#24320;&#38144;&#65292;&#20854;&#21463;&#21040;&#20005;&#37325;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;PINNs&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#31639;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21487;&#20998;&#31163;&#30340;PINN (SPINN)&#65292;&#22312;&#22810;&#32500;PDE&#20013;&#25353;&#36724;&#36880;&#20010;&#22788;&#29702;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#32593;&#32476;&#20256;&#25773;&#30340;&#25968;&#37327;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;PINNs&#20013;&#30340;&#36880;&#28857;&#22788;&#29702;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#26469;&#38477;&#20302;&#35745;&#31639;PDE&#27531;&#24046;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20174;&#32780;&#22312;&#21333;&#20010;&#26222;&#36890;GPU&#19978;&#21487;&#20197;&#20351;&#29992;&#22823;&#37327;&#30340;&#37197;&#28857;(&gt;10^7)&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) have recently emerged as promising data-driven PDE solvers showing encouraging results on various PDEs. However, there is a fundamental limitation of training PINNs to solve multi-dimensional PDEs and approximate highly complex solution functions. The number of training points (collocation points) required on these challenging PDEs grows substantially, but it is severely limited due to the expensive computational costs and heavy memory overhead. To overcome this issue, we propose a network architecture and training algorithm for PINNs. The proposed method, separable PINN (SPINN), operates on a per-axis basis to significantly reduce the number of network propagations in multi-dimensional PDEs unlike point-wise processing in conventional PINNs. We also propose using forward-mode automatic differentiation to reduce the computational cost of computing PDE residuals, enabling a large number of collocation points (&gt;10^7) on a single commodity GPU. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#35299;&#20915;&#27809;&#26377;&#26631;&#31614;&#30340;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#12290;&#23427;&#20204;&#20805;&#20998;&#21033;&#29992;&#25152;&#26377;&#33410;&#28857;&#20449;&#24687;&#65292;&#24182;&#19988;&#36890;&#36807;&#27867;&#21270;&#33021;&#21147;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15217</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#29992;&#20110;&#22270;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Episode Generation for Graph Meta-learning. (arXiv:2306.15217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#35299;&#20915;&#27809;&#26377;&#26631;&#31614;&#30340;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#12290;&#23427;&#20204;&#20805;&#20998;&#21033;&#29992;&#25152;&#26377;&#33410;&#28857;&#20449;&#24687;&#65292;&#24182;&#19988;&#36890;&#36807;&#27867;&#21270;&#33021;&#21147;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#26469;&#35299;&#20915;&#27809;&#26377;&#26631;&#31614;&#30340;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#12290;&#20027;&#27969;&#30340;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#26159;&#22312;&#23384;&#22312;&#22823;&#37327;&#26377;&#26631;&#31614;&#33410;&#28857;&#29992;&#20110;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#24320;&#21457;&#30340;&#65292;&#28982;&#32780;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#36825;&#26679;&#30340;&#25968;&#25454;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#35299;&#20915;&#26631;&#31614;&#31232;&#32570;&#24615;&#38382;&#39064;&#30340;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#20381;&#36182;&#20110;&#26377;&#38480;&#25968;&#37327;&#30340;&#26377;&#26631;&#31614;&#25968;&#25454;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#22270;&#20013;&#25152;&#26377;&#33410;&#28857;&#20449;&#24687;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#23613;&#31649;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#19978;&#24456;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#23398;&#20064;&#36890;&#29992;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#27809;&#26377;&#32771;&#34385;&#35201;&#35299;&#20915;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#20854;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#21033;&#29992;&#23427;&#20204;&#22312;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#35299;&#20915;&#26631;&#31614;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#22686;&#24378;&#26041;&#27861;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate Unsupervised Episode Generation methods to solve Few-Shot Node-Classification (FSNC) problem via Meta-learning without labels. Dominant meta-learning methodologies for FSNC were developed under the existence of abundant labeled nodes for training, which however may not be possible to obtain in the real-world. Although few studies have been proposed to tackle the label-scarcity problem, they still rely on a limited amount of labeled data, which hinders the full utilization of the information of all nodes in a graph. Despite the effectiveness of Self-Supervised Learning (SSL) approaches on FSNC without labels, they mainly learn generic node embeddings without consideration on the downstream task to be solved, which may limit its performance. In this work, we propose unsupervised episode generation methods to benefit from their generalization ability for FSNC tasks while resolving label-scarcity problem. We first propose a method that utilizes graph augmentat
&lt;/p&gt;</description></item><item><title>DiffInfinite &#26159;&#19968;&#31181;&#33021;&#22815;&#29983;&#25104;&#20219;&#24847;&#22823;&#30340;&#32452;&#32455;&#23398;&#22270;&#20687;&#65292;&#20351;&#29992;&#23567;&#34917;&#19969;&#36827;&#34892;&#24555;&#36895;&#35757;&#32451;&#65292;&#24182;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#24182;&#34892;&#21270;&#30340;&#26041;&#27861;&#65292;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#25104;&#20687;&#23454;&#36341;&#20013;&#26377;&#25928;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#20449;&#24687;&#12289;&#26114;&#36149;&#30340;&#25163;&#21160;&#27880;&#37322;&#20197;&#21450;&#20445;&#25252;&#24615;&#25968;&#25454;&#22788;&#29702;&#31561;&#29420;&#29305;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.13384</link><description>&lt;p&gt;
DiffInfinite: &#36890;&#36807;&#24182;&#34892;&#38543;&#26426;&#34917;&#19969;&#25193;&#25955;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#23454;&#29616;&#22823;&#22411;&#33945;&#29256;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
DiffInfinite: Large Mask-Image Synthesis via Parallel Random Patch Diffusion in Histopathology. (arXiv:2306.13384v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13384
&lt;/p&gt;
&lt;p&gt;
DiffInfinite &#26159;&#19968;&#31181;&#33021;&#22815;&#29983;&#25104;&#20219;&#24847;&#22823;&#30340;&#32452;&#32455;&#23398;&#22270;&#20687;&#65292;&#20351;&#29992;&#23567;&#34917;&#19969;&#36827;&#34892;&#24555;&#36895;&#35757;&#32451;&#65292;&#24182;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#24182;&#34892;&#21270;&#30340;&#26041;&#27861;&#65292;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#25104;&#20687;&#23454;&#36341;&#20013;&#26377;&#25928;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#20449;&#24687;&#12289;&#26114;&#36149;&#30340;&#25163;&#21160;&#27880;&#37322;&#20197;&#21450;&#20445;&#25252;&#24615;&#25968;&#25454;&#22788;&#29702;&#31561;&#29420;&#29305;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; DiffInfinite&#65292;&#36825;&#26159;&#19968;&#31181;&#23618;&#27425;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#20219;&#24847;&#22823;&#30340;&#32452;&#32455;&#23398;&#22270;&#20687;&#65292;&#21516;&#26102;&#20445;&#25345;&#38271;&#31243;&#30456;&#20851;&#24615;&#32467;&#26500;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20808;&#29983;&#25104;&#21512;&#25104;&#20998;&#21106;&#25513;&#27169;&#65292;&#38543;&#21518;&#29992;&#20316;&#39640;&#20445;&#30495;&#24230;&#29983;&#25104;&#25193;&#25955;&#36807;&#31243;&#30340;&#26465;&#20214;&#12290;&#25152;&#25552;&#20986;&#30340;&#37319;&#26679;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#20219;&#24847;&#25152;&#38656;&#22270;&#20687;&#23610;&#23544;&#65292;&#32780;&#21482;&#38656;&#35201;&#23567;&#34917;&#19969;&#36827;&#34892;&#24555;&#36895;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#30456;&#36739;&#20110;&#20197;&#24448;&#30340;&#22823;&#22411;&#20869;&#23481;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#24182;&#34892;&#21270;&#65292;&#21516;&#26102;&#36991;&#20813;&#24179;&#38138;&#21453;&#23556;&#24335;&#20266;&#24433;&#12290;&#35757;&#32451;&#21033;&#29992;&#20102;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26080;&#26631;&#27880;&#25968;&#25454;&#25193;&#20805;&#23567;&#22411;&#12289;&#31232;&#30095;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32531;&#35299;&#20102;&#32452;&#32455;&#30149;&#29702;&#23398;&#25104;&#20687;&#23454;&#36341;&#20013;&#30340;&#29420;&#29305;&#25361;&#25112;&#65306;&#22823;&#35268;&#27169;&#20449;&#24687;&#12289;&#26114;&#36149;&#30340;&#25163;&#21160;&#27880;&#37322;&#20197;&#21450;&#20445;&#25252;&#24615;&#25968;&#25454;&#22788;&#29702;&#12290;DiffInfinite &#25968;&#25454;&#30340;&#29983;&#29289;&#21512;&#29702;&#24615;&#30001;&#21313;&#21517;&#26377;&#32463;&#39564;&#30340;&#30149;&#29702;&#23398;&#23478;&#36827;&#34892;&#35843;&#26597;&#39564;&#35777;&#65292;&#20197;&#21450;&#19979;&#28216;&#20998;&#21106;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present DiffInfinite, a hierarchical diffusion model that generates arbitrarily large histological images while preserving long-range correlation structural information. Our approach first generates synthetic segmentation masks, subsequently used as conditions for the high-fidelity generative diffusion process. The proposed sampling method can be scaled up to any desired image size while only requiring small patches for fast training. Moreover, it can be parallelized more efficiently than previous large-content generation methods while avoiding tiling artefacts. The training leverages classifier-free guidance to augment a small, sparsely annotated dataset with unlabelled data. Our method alleviates unique challenges in histopathological imaging practice: large-scale information, costly manual annotation, and protective data handling. The biological plausibility of DiffInfinite data is validated in a survey by ten experienced pathologists as well as a downstream segmentation task. Fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#24102;&#26377;&#22024;&#26434;&#36125;&#21494;&#26031;&#21453;&#39304;&#30340;&#38646;&#21644;&#30697;&#38453;&#21338;&#24328;&#20013;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#36951;&#25022;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.13233</link><description>&lt;p&gt;
&#22522;&#20110;&#24102;&#26377;&#22024;&#26434;&#36125;&#21494;&#26031;&#21453;&#39304;&#30340;&#38646;&#21644;&#30697;&#38453;&#21338;&#24328;&#30340;&#23545;&#25968;&#36951;&#25022;&#23545;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Logarithmic Regret for Matrix Games against an Adversary with Noisy Bandit Feedback. (arXiv:2306.13233v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#24102;&#26377;&#22024;&#26434;&#36125;&#21494;&#26031;&#21453;&#39304;&#30340;&#38646;&#21644;&#30697;&#38453;&#21338;&#24328;&#20013;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#36951;&#25022;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#21644;&#30697;&#38453;&#21338;&#24328;&#30340;&#21464;&#31181;&#65292;&#20854;&#20013;&#27599;&#27493;&#34892;&#36873;&#25163;&#36873;&#25321;&#19968;&#34892;$i$&#65292;&#21015;&#36873;&#25163;&#36873;&#25321;&#19968;&#21015;$j$&#65292;&#34892;&#36873;&#25163;&#25910;&#21040;&#24179;&#22343;&#20540;&#20026;$A_{i,j}$&#30340;&#22024;&#26434;&#22870;&#21169;&#12290;&#34892;&#36873;&#25163;&#30340;&#30446;&#26631;&#26159;&#23613;&#21487;&#33021;&#22320;&#32047;&#31215;&#22870;&#21169;&#65292;&#21363;&#20351;&#23545;&#25163;&#26159;&#19968;&#20010;&#23545;&#25163;&#24615;&#21015;&#36873;&#25163;&#12290;&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#35777;&#26126;&#22312;$m \times n$&#30697;&#38453;&#21338;&#24328;&#20013;&#65292;&#23454;&#29616;&#20102;$O(\sqrt{mnT})$&#23545;&#25968;&#36951;&#25022;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;UCB&#39118;&#26684;&#31639;&#27861;&#25152;&#33719;&#24471;&#30340;$O(m\sqrt{nT})$&#23545;&#25968;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers a variant of zero-sum matrix games where at each timestep the row player chooses row $i$, the column player chooses column $j$, and the row player receives a noisy reward with mean $A_{i,j}$. The objective of the row player is to accumulate as much reward as possible, even against an adversarial column player. If the row player uses the EXP3 strategy, an algorithm known for obtaining $\sqrt{T}$ regret against an arbitrary sequence of rewards, it is immediate that the row player also achieves $\sqrt{T}$ regret relative to the Nash equilibrium in this game setting. However, partly motivated by the fact that the EXP3 strategy is myopic to the structure of the game, O'Donoghue et al. (2021) proposed a UCB-style algorithm that leverages the game structure and demonstrated that this algorithm greatly outperforms EXP3 empirically. While they showed that this UCB-style algorithm achieved $\sqrt{T}$ regret, in this paper we ask if there exists an algorithm that provably ach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#26410;&#35265;&#36807;&#30340;&#27169;&#24577;&#32452;&#21512;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#19968;&#20010;&#27169;&#22359;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#29305;&#24449;&#25237;&#24433;&#21040;&#19968;&#20010;&#20849;&#20139;&#30340;&#31354;&#38388;&#20013;&#65292;&#24182;&#36890;&#36807;&#20266;&#30417;&#30563;&#26469;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#27169;&#24577;&#19978;&#37117;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.12795</link><description>&lt;p&gt;
&#23398;&#20064;&#26410;&#35265;&#36807;&#30340;&#27169;&#24577;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Learning Unseen Modality Interaction. (arXiv:2306.12795v2 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#26410;&#35265;&#36807;&#30340;&#27169;&#24577;&#32452;&#21512;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#19968;&#20010;&#27169;&#22359;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#29305;&#24449;&#25237;&#24433;&#21040;&#19968;&#20010;&#20849;&#20139;&#30340;&#31354;&#38388;&#20013;&#65292;&#24182;&#36890;&#36807;&#20266;&#30417;&#30563;&#26469;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#27169;&#24577;&#19978;&#37117;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#20551;&#23450;&#22312;&#35757;&#32451;&#26399;&#38388;&#21487;&#20197;&#20351;&#29992;&#25152;&#26377;&#24863;&#20852;&#36259;&#30340;&#27169;&#24577;&#32452;&#21512;&#26469;&#23398;&#20064;&#36328;&#27169;&#24577;&#23545;&#24212;&#20851;&#31995;&#12290;&#26412;&#25991;&#23545;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#27169;&#24577;&#23436;&#25972;&#24615;&#20570;&#20986;&#20102;&#25361;&#25112;&#65292;&#32780;&#26159;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21162;&#21147;&#23454;&#29616;&#23545;&#26410;&#35265;&#36807;&#30340;&#27169;&#24577;&#32452;&#21512;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#35265;&#36807;&#30340;&#27169;&#24577;&#20132;&#20114;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#23427;&#21033;&#29992;&#19968;&#20010;&#27169;&#22359;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#22810;&#32500;&#29305;&#24449;&#25237;&#24433;&#21040;&#19968;&#20010;&#20855;&#26377;&#22810;&#26679;&#20449;&#24687;&#30340;&#20844;&#20849;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#27714;&#21644;&#25805;&#20316;&#23545;&#21487;&#29992;&#30340;&#27169;&#24577;&#36827;&#34892;&#20449;&#24687;&#32047;&#31215;&#12290;&#20026;&#20102;&#20943;&#23569;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#36739;&#23569;&#36776;&#21035;&#21147;&#30340;&#27169;&#24577;&#32452;&#21512;&#30340;&#36807;&#25311;&#21512;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#27169;&#22411;&#23398;&#20064;&#65292;&#36890;&#36807;&#20266;&#30417;&#30563;&#25351;&#31034;&#27169;&#24577;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#22810;&#27169;&#24577;&#35270;&#39057;&#20998;&#31867;&#12289;&#26426;&#22120;&#20154;&#29366;&#24577;&#22238;&#24402;&#31561;&#36827;&#34892;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#19981;&#21516;&#20219;&#21153;&#21644;&#27169;&#24577;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning assumes all modality combinations of interest are available during training to learn cross-modal correspondences.In this paper, we challenge this modality-complete assumption for multimodal learning and instead strive for generalization to unseen modality combinations during inference. We pose the problem of unseen modality interaction and introduce a first solution. It exploits a module that projects the multidimensional features of different modalities into a common space with rich information preserved. This allows the information to be accumulated with a simple summation operation across available modalities. To reduce overfitting to less discriminative modality combinations during training, we further improve the model learning with pseudo-supervision indicating the reliability of a modality's prediction. We demonstrate that our approach is effective for diverse tasks and modalities by evaluating it for multimodal video classification, robot state regression, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36229;&#21442;&#25968;&#27169;&#22411;&#20013;&#35299;&#20915;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21644;Adam&#30340;&#36895;&#24230;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21333;&#35843;&#32447;&#25628;&#32034;&#26041;&#27861;&#65292;&#21462;&#24471;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#32467;&#21512;&#21516;&#27493;&#30340;Polyak&#21021;&#22987;&#21270;&#27493;&#20240;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.12747</link><description>&lt;p&gt;
&#19981;&#35201;&#22826;&#21333;&#35843;&#65306;&#25918;&#23485;&#36229;&#21442;&#25968;&#27169;&#22411;&#20013;&#30340;&#38543;&#26426;&#32447;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Don't be so Monotone: Relaxing Stochastic Line Search in Over-Parameterized Models. (arXiv:2306.12747v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36229;&#21442;&#25968;&#27169;&#22411;&#20013;&#35299;&#20915;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21644;Adam&#30340;&#36895;&#24230;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21333;&#35843;&#32447;&#25628;&#32034;&#26041;&#27861;&#65292;&#21462;&#24471;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#32467;&#21512;&#21516;&#27493;&#30340;Polyak&#21021;&#22987;&#21270;&#27493;&#20240;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#32447;&#25628;&#32034;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#29616;&#20195;&#36229;&#21442;&#25968;&#35774;&#32622;&#19979;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21644;Adam&#30340;&#36895;&#24230;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#38656;&#35201;&#65288;&#23567;&#25209;&#37327;&#65289;&#30446;&#26631;&#20989;&#25968;&#30340;&#21333;&#35843;&#20943;&#23569;&#65292;&#29616;&#26377;&#30340;&#32447;&#25628;&#32034;&#21487;&#33021;&#20250;&#37319;&#21462;&#27604;&#24517;&#35201;&#26356;&#23567;&#30340;&#27493;&#39588;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#38750;&#21333;&#35843;&#32447;&#25628;&#32034;&#26041;&#27861;&#26469;&#25918;&#23485;&#36825;&#20010;&#26465;&#20214;&#65292;&#24182;&#21487;&#33021;&#25509;&#21463;&#26356;&#22823;&#30340;&#27493;&#38271;&#12290;&#23613;&#31649;&#32570;&#20047;&#21333;&#35843;&#36882;&#20943;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#19982;&#21333;&#35843;&#24773;&#20917;&#30456;&#21516;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#38750;&#21333;&#35843;&#26041;&#27861;&#22312;SGD / Adam&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#29978;&#33267;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#21333;&#35843;&#32447;&#25628;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;POlyak NOnmonotone&#38543;&#26426;&#65288;PoNoS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38750;&#21333;&#35843;&#32447;&#25628;&#32034;&#19982;Polyak&#21021;&#22987;&#27493;&#38271;&#32467;&#21512;&#32780;&#24471;&#21040;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#37325;&#32622;&#25216;&#26415;&#65292;&#22312;&#22823;&#22810;&#25968;&#36845;&#20195;&#20013;&#23558;&#22238;&#28335;&#30340;&#25968;&#37327;&#20943;&#23569;&#21040;&#38646;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#36739;&#22823;&#30340;&#21021;&#22987;s&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown that line search methods can speed up Stochastic Gradient Descent (SGD) and Adam in modern over-parameterized settings. However, existing line searches may take steps that are smaller than necessary since they require a monotone decrease of the (mini-)batch objective function. We explore nonmonotone line search methods to relax this condition and possibly accept larger step sizes. Despite the lack of a monotonic decrease, we prove the same fast rates of convergence as in the monotone case. Our experiments show that nonmonotone methods improve the speed of convergence and generalization properties of SGD/Adam even beyond the previous monotone line searches. We propose a POlyak NOnmonotone Stochastic (PoNoS) method, obtained by combining a nonmonotone line search with a Polyak initial step size. Furthermore, we develop a new resetting technique that in the majority of the iterations reduces the amount of backtracks to zero while still maintaining a large initial s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#19978;&#23545;&#29616;&#20195;BDL&#31639;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#35780;&#20272;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26657;&#20934;&#33021;&#21147;&#65292;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#24102;&#31526;&#21495;&#30340;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.12306</link><description>&lt;p&gt;
&#36229;&#36234;&#28145;&#24230;&#38598;&#25104;&#8212;&#8212;&#22522;&#20110;&#20998;&#24067;&#20559;&#31227;&#19979;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Beyond Deep Ensembles -- A Large-Scale Evaluation of Bayesian Deep Learning under Distribution Shift. (arXiv:2306.12306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12306
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#19978;&#23545;&#29616;&#20195;BDL&#31639;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#35780;&#20272;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26657;&#20934;&#33021;&#21147;&#65292;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#24102;&#31526;&#21495;&#30340;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#65288;BDL&#65289;&#26159;&#23454;&#29616;&#22312;&#20998;&#24067;&#20559;&#31227;&#25968;&#25454;&#19978;&#36827;&#34892;&#33391;&#22909;&#26657;&#20934;&#39044;&#27979;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#22823;&#35268;&#27169;&#35843;&#26597;&#65292;&#20197;&#31995;&#32479;&#26041;&#24335;&#35780;&#20272;&#26368;&#36817;&#30340; SOTA &#26041;&#27861;&#22312;&#22810;&#26679;&#12289;&#29616;&#23454;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#28165;&#26224;&#20102;&#35299;BDL&#30740;&#31350;&#30340;&#24403;&#21069;&#29366;&#20917;&#65292;&#25105;&#20204;&#22312;&#26469;&#33258;WILDS&#38598;&#21512;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#29616;&#20195;BDL&#31639;&#27861;&#65292;&#21253;&#21547;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#65292;&#37325;&#28857;&#20851;&#27880;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#26657;&#20934;&#33021;&#21147;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19968;&#31995;&#21015;&#22823;&#22411;&#65292;&#21367;&#31215;&#21644;&#22522;&#20110; transformer &#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#19978;&#30340;&#31639;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#19968;&#20010;&#24102;&#31526;&#21495;&#30340;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#29256;&#26412;&#65292;&#25581;&#31034;&#20986;&#26041;&#27861;&#26159;&#36807;&#24230;&#33258;&#20449;&#36824;&#26159;&#20302;&#25391;&#24133;&#65292;&#36827;&#19968;&#27493;&#28145;&#20837;&#30740;&#31350;&#26041;&#27861;&#30340;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#20102;&#39318;&#27425;&#31995;&#32479;&#35780;&#20272;BDL&#22312;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#34920;&#29616;&#65292;&#20570;&#20102;&#26356;&#22810;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian deep learning (BDL) is a promising approach to achieve well-calibrated predictions on distribution-shifted data. Nevertheless, there exists no large-scale survey that evaluates recent SOTA methods on diverse, realistic, and challenging benchmark tasks in a systematic manner. To provide a clear picture of the current state of BDL research, we evaluate modern BDL algorithms on real-world datasets from the WILDS collection containing challenging classification and regression tasks, with a focus on generalization capability and calibration under distribution shift. We compare the algorithms on a wide range of large, convolutional and transformer-based neural network architectures. In particular, we investigate a signed version of the expected calibration error that reveals whether the methods are over- or under-confident, providing further insight into the behavior of the methods. Further, we provide the first systematic evaluation of BDL for fine-tuning large pre-trained models, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#37327;&#23376;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#65292;&#24182;&#21457;&#29616;&#19968;&#20010;&#31934;&#24515;&#21046;&#20316;&#30340;&#36890;&#29992;&#25200;&#21160;&#21487;&#20197;&#25104;&#21151;&#22320;&#27450;&#39575;&#20004;&#20010;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#30340;&#37327;&#23376;&#20998;&#31867;&#22120;&#65292;&#36825;&#20026;&#26500;&#24314;&#23433;&#20840;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#24102;&#26469;&#28508;&#22312;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2306.11974</link><description>&lt;p&gt;
&#29992;&#20110;&#22810;&#20998;&#31867;&#20219;&#21153;&#30340;&#37327;&#23376;&#20998;&#31867;&#22120;&#30340;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
Universal adversarial perturbations for multiple classification tasks with quantum classifiers. (arXiv:2306.11974v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#37327;&#23376;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#65292;&#24182;&#21457;&#29616;&#19968;&#20010;&#31934;&#24515;&#21046;&#20316;&#30340;&#36890;&#29992;&#25200;&#21160;&#21487;&#20197;&#25104;&#21151;&#22320;&#27450;&#39575;&#20004;&#20010;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#30340;&#37327;&#23376;&#20998;&#31867;&#22120;&#65292;&#36825;&#20026;&#26500;&#24314;&#23433;&#20840;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#24102;&#26469;&#28508;&#22312;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#38376;&#26032;&#20852;&#30340;&#39046;&#22495;&#65292;&#23427;&#30740;&#31350;&#20102;&#37327;&#23376;&#23398;&#20064;&#31995;&#32479;&#23545;&#25239;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#24182;&#24320;&#21457;&#20102;&#21487;&#33021;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#37327;&#23376;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#26159;&#19968;&#31181;&#23567;&#30340;&#25200;&#21160;&#65292;&#21487;&#20197;&#20351;&#19981;&#21516;&#30340;&#36755;&#20837;&#26679;&#26412;&#25104;&#20026;&#35823;&#23548;&#32473;&#23450;&#37327;&#23376;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#31034;&#20363;&#12290;&#23613;&#31649;&#27492;&#39046;&#22495;&#20043;&#21069;&#40092;&#26377;&#25506;&#31350;&#65292;&#20294;&#26159;&#36890;&#29992;&#25200;&#21160;&#21487;&#33021;&#26497;&#22823;&#22320;&#31616;&#21270;&#24694;&#24847;&#25915;&#20987;&#65292;&#23545;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36896;&#25104;&#24847;&#24819;&#19981;&#21040;&#30340;&#30772;&#22351;&#12290;&#26412;&#25991;&#22312;&#24322;&#26500;&#20998;&#31867;&#20219;&#21153;&#30340;&#32972;&#26223;&#19979;&#65292;&#36827;&#19968;&#27493;&#25506;&#35752;&#37327;&#23376;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#20960;&#20046;&#22312;&#20004;&#20010;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#30340;&#37327;&#23376;&#20998;&#31867;&#22120;&#65292;&#37117;&#21487;&#20197;&#34987;&#19968;&#20010;&#31934;&#24515;&#21046;&#20316;&#30340;&#36890;&#29992;&#25200;&#21160;&#25152;&#35825;&#23548;&#25104;&#21151;&#22320;&#27450;&#39575;&#12290;&#36825;&#19968;&#32467;&#26524;&#24050;&#32463;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;CIFAR-10&#21644;Iris&#20013;&#24471;&#21040;&#26126;&#30830;&#30340;&#35777;&#26126;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#26159;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#28508;&#22312;&#23041;&#32961;&#65292;&#24182;&#21487;&#33021;&#32473;&#26500;&#24314;&#23433;&#20840;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#24102;&#26469;&#24040;&#22823;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum adversarial machine learning is an emerging field that studies the vulnerability of quantum learning systems against adversarial perturbations and develops possible defense strategies. Quantum universal adversarial perturbations are small perturbations, which can make different input samples into adversarial examples that may deceive a given quantum classifier. This is a field that was rarely looked into but worthwhile investigating because universal perturbations might simplify malicious attacks to a large extent, causing unexpected devastation to quantum machine learning models. In this paper, we take a step forward and explore the quantum universal perturbations in the context of heterogeneous classification tasks. In particular, we find that quantum classifiers that achieve almost state-of-the-art accuracy on two different classification tasks can be both conclusively deceived by one carefully-crafted universal perturbation. This result is explicitly demonstrated with well-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26799;&#24230;&#28040;&#22833;&#21644;&#26799;&#24230;&#29190;&#28856;&#31561;&#20248;&#21270;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#25552;&#39640;&#26799;&#24230;&#27969;&#21644;&#23545;&#32593;&#32476;Lipschitz&#24120;&#25968;&#26045;&#21152;&#32422;&#26463;&#31561;&#25514;&#26045;&#36827;&#34892;&#20102;&#35299;&#20915;&#12290;&#26174;&#24335;&#20248;&#21270;&#21644;&#38544;&#24335;&#20248;&#21270;&#26159;&#20004;&#31181;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#19981;&#21516;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.09338</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#20248;&#21270;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Understanding Optimization of Deep Learning. (arXiv:2306.09338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26799;&#24230;&#28040;&#22833;&#21644;&#26799;&#24230;&#29190;&#28856;&#31561;&#20248;&#21270;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#25552;&#39640;&#26799;&#24230;&#27969;&#21644;&#23545;&#32593;&#32476;Lipschitz&#24120;&#25968;&#26045;&#21152;&#32422;&#26463;&#31561;&#25514;&#26045;&#36827;&#34892;&#20102;&#35299;&#20915;&#12290;&#26174;&#24335;&#20248;&#21270;&#21644;&#38544;&#24335;&#20248;&#21270;&#26159;&#20004;&#31181;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#19981;&#21516;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#20248;&#21270;&#29702;&#35770;&#65292;&#20027;&#35201;&#20851;&#27880;&#26799;&#24230;&#28040;&#22833;&#21644;&#26799;&#24230;&#29190;&#28856;&#31561;&#38382;&#39064;&#25152;&#24102;&#26469;&#30340;&#27169;&#22411;&#34920;&#31034;&#33021;&#21147;&#38477;&#20302;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#31561;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#39640;&#26799;&#24230;&#27969;&#21644;&#23545;&#32593;&#32476;Lipschitz &#24120;&#25968;&#26045;&#21152;&#32422;&#26463;&#31561;&#25514;&#26045;&#26469;&#20998;&#26512;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#24110;&#21161;&#29702;&#35299;&#24403;&#21069;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#20026;&#26174;&#24335;&#20248;&#21270;&#26041;&#27861;&#21644;&#38544;&#24335;&#20248;&#21270;&#26041;&#27861;&#12290;&#26174;&#24335;&#20248;&#21270;&#26041;&#27861;&#28041;&#21450;&#30452;&#25509;&#25805;&#20316;&#20248;&#21270;&#22120;&#21442;&#25968;&#65292;&#21253;&#25324;&#26435;&#37325;&#12289;&#26799;&#24230;&#12289;&#23398;&#20064;&#29575;&#21644;&#26435;&#37325;&#34928;&#20943;&#31561;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#38544;&#24335;&#20248;&#21270;&#26041;&#27861;&#20391;&#37325;&#20110;&#36890;&#36807;&#22686;&#24378;&#32593;&#32476;&#27169;&#22359;&#65288;&#22914;&#27531;&#24046;&#24555;&#25463;&#26041;&#24335;&#12289;&#26631;&#20934;&#21270;&#26041;&#27861;&#12289;&#27880;&#24847;&#26426;&#21046;&#21644;&#28608;&#27963;&#65289;&#26469;&#25913;&#21892;&#32593;&#32476;&#25972;&#20307;&#24418;&#21183;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#26356;&#22909;&#22320;&#20102;&#35299;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article provides a comprehensive understanding of optimization in deep learning, with a primary focus on the challenges of gradient vanishing and gradient exploding, which normally lead to diminished model representational ability and training instability, respectively. We analyze these two challenges through several strategic measures, including the improvement of gradient flow and the imposition of constraints on a network's Lipschitz constant. To help understand the current optimization methodologies, we categorize them into two classes: explicit optimization and implicit optimization. Explicit optimization methods involve direct manipulation of optimizer parameters, including weight, gradient, learning rate, and weight decay. Implicit optimization methods, by contrast, focus on improving the overall landscape of a network by enhancing its modules, such as residual shortcuts, normalization methods, attention mechanisms, and activations. In this article, we provide an in-depth a
&lt;/p&gt;</description></item><item><title>&#36328;&#22269;&#21644;&#26102;&#38388;&#30340;&#31038;&#20250;&#21160;&#33633;&#30456;&#21464;&#30740;&#31350;&#25506;&#32034;&#20102;&#38598;&#20307;&#31038;&#20250;&#21160;&#33633;&#26159;&#21542;&#21487;&#20197;&#34987;&#25551;&#36848;&#20026;&#19968;&#31995;&#21015;&#20855;&#26377;&#21487;&#27979;&#37327;&#21644;&#21487;&#35782;&#21035;&#29305;&#24449;&#30340;&#24490;&#29615;&#30456;&#21464;&#65292;&#24182;&#35777;&#26126;&#20102;&#23439;&#35266;&#30456;&#21464;&#27169;&#22411;&#26377;&#25928;&#22320;&#25429;&#25417;&#21040;&#20102;&#20840;&#29699;&#21508;&#22269;&#31038;&#20250;&#21160;&#33633;&#25968;&#25454;&#30340;&#29305;&#24449;&#65292;&#20197;&#21450;&#26222;&#36941;&#26426;&#21046;&#21487;&#33021;&#25903;&#25745;&#30528;&#31038;&#20250;&#21160;&#33633;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2306.08698</link><description>&lt;p&gt;
&#36328;&#22269;&#21644;&#26102;&#38388;&#30340;&#31038;&#20250;&#21160;&#33633;&#30456;&#21464;
&lt;/p&gt;
&lt;p&gt;
Phase Transitions of Civil Unrest across Countries and Time. (arXiv:2306.08698v2 [physics.soc-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08698
&lt;/p&gt;
&lt;p&gt;
&#36328;&#22269;&#21644;&#26102;&#38388;&#30340;&#31038;&#20250;&#21160;&#33633;&#30456;&#21464;&#30740;&#31350;&#25506;&#32034;&#20102;&#38598;&#20307;&#31038;&#20250;&#21160;&#33633;&#26159;&#21542;&#21487;&#20197;&#34987;&#25551;&#36848;&#20026;&#19968;&#31995;&#21015;&#20855;&#26377;&#21487;&#27979;&#37327;&#21644;&#21487;&#35782;&#21035;&#29305;&#24449;&#30340;&#24490;&#29615;&#30456;&#21464;&#65292;&#24182;&#35777;&#26126;&#20102;&#23439;&#35266;&#30456;&#21464;&#27169;&#22411;&#26377;&#25928;&#22320;&#25429;&#25417;&#21040;&#20102;&#20840;&#29699;&#21508;&#22269;&#31038;&#20250;&#21160;&#33633;&#25968;&#25454;&#30340;&#29305;&#24449;&#65292;&#20197;&#21450;&#26222;&#36941;&#26426;&#21046;&#21487;&#33021;&#25903;&#25745;&#30528;&#31038;&#20250;&#21160;&#33633;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#21464;&#26159;&#22797;&#26434;&#31995;&#32479;&#20013;&#31361;&#21457;&#36716;&#21464;&#30340;&#29305;&#24449;&#65292;&#23613;&#31649;&#22312;&#29289;&#29702;&#21644;&#33258;&#28982;&#31185;&#23398;&#20013;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#22312;&#31038;&#20250;&#31995;&#32479;&#20013;&#23545;&#36825;&#19968;&#29616;&#35937;&#30340;&#23454;&#35777;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;&#38598;&#20307;&#31038;&#20250;&#21160;&#33633;&#30340;&#21160;&#21147;&#23398;&#26159;&#21542;&#21487;&#20197;&#34987;&#21512;&#29702;&#22320;&#25551;&#36848;&#20026;&#19968;&#31995;&#21015;&#24490;&#29615;&#30456;&#21464;&#65292;&#20854;&#20013;&#27599;&#20010;&#38454;&#27573;&#20855;&#26377;&#21487;&#27979;&#37327;&#21644;&#21487;&#35782;&#21035;&#30340;&#28508;&#22312;&#29305;&#24449;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23439;&#35266;&#27700;&#24179;&#30340;&#31038;&#20250;&#21160;&#33633;&#32479;&#35745;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21253;&#25324;1946&#24180;&#33267;2017&#24180;&#22312;&#20869;&#30340;170&#20010;&#22269;&#23478;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#20854;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20010;&#23439;&#35266;&#30456;&#21464;&#27169;&#22411;&#26377;&#25928;&#22320;&#25429;&#25417;&#21040;&#20102;&#20840;&#29699;&#21508;&#22269;&#31038;&#20250;&#21160;&#33633;&#25968;&#25454;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#26222;&#36941;&#30340;&#26426;&#21046;&#21487;&#33021;&#28508;&#22312;&#22320;&#25903;&#25745;&#30528;&#31038;&#20250;&#21160;&#33633;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#37327;&#34920;&#26469;&#34913;&#37327;&#19968;&#20010;&#22269;&#23478;&#30340;&#31038;&#20250;&#21160;&#33633;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phase transitions, characterized by abrupt shifts between macroscopic patterns of organization, are ubiquitous in complex systems. Despite considerable research in the physical and natural sciences, the empirical study of this phenomenon in societal systems is relatively underdeveloped. The goal of this study is to explore whether the dynamics of collective civil unrest can be plausibly characterized as a sequence of recurrent phase shifts, with each phase having measurable and identifiable latent characteristics. We introduce a macro-level statistical model of civil unrest and evaluate its plausibility using a comprehensive dataset of civil unrest events in 170 countries from 1946 to 2017. Our findings demonstrate that the macro-level phase model effectively captures the characteristics of civil unrest data from diverse countries globally and that universal mechanisms may underlie certain aspects of the dynamics of civil unrest. We also introduce a new scale to quantify a country's lo
&lt;/p&gt;</description></item><item><title>PeFLL&#26159;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#23884;&#20837;&#32593;&#32476;&#21644;&#36229;&#32593;&#32476;&#65292;PeFLL&#33021;&#22815;&#23398;&#20064;&#36755;&#20986;&#29305;&#23450;&#20110;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#20854;&#23427;&#26032;&#20986;&#29616;&#30340;&#23458;&#25143;&#31471;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.05515</link><description>&lt;p&gt;
&#19968;&#31181;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#32456;&#36523;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PeFLL: A Lifelong Learning Approach to Personalized Federated Learning. (arXiv:2306.05515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05515
&lt;/p&gt;
&lt;p&gt;
PeFLL&#26159;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#23884;&#20837;&#32593;&#32476;&#21644;&#36229;&#32593;&#32476;&#65292;PeFLL&#33021;&#22815;&#23398;&#20064;&#36755;&#20986;&#29305;&#23450;&#20110;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#20854;&#23427;&#26032;&#20986;&#29616;&#30340;&#23458;&#25143;&#31471;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;pFL&#65289;&#24050;&#25104;&#20026;&#24212;&#23545;&#21442;&#19982;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#25361;&#25112;&#30340;&#24120;&#29992;&#26041;&#27861;&#12290;pFL&#19981;&#26159;&#23398;&#20064;&#21333;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26159;&#26088;&#22312;&#23398;&#20064;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#20010;&#20307;&#27169;&#22411;&#65292;&#21516;&#26102;&#20173;&#28982;&#21033;&#29992;&#20854;&#20182;&#23458;&#25143;&#31471;&#21487;&#29992;&#30340;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PeFLL&#65292;&#36825;&#26159;&#19968;&#31181;&#26681;&#26893;&#20110;&#32456;&#36523;&#23398;&#20064;&#30340;&#26032;&#22411;pFL&#26041;&#27861;&#65292;&#19981;&#20165;&#22312;&#35757;&#32451;&#38454;&#27573;&#23384;&#22312;&#30340;&#23458;&#25143;&#31471;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#19988;&#22312;&#26410;&#26469;&#21487;&#33021;&#20986;&#29616;&#30340;&#23458;&#25143;&#31471;&#19978;&#20063;&#34920;&#29616;&#33391;&#22909;&#12290;PeFLL&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#23884;&#20837;&#32593;&#32476;&#21644;&#36229;&#32593;&#32476;&#26469;&#23398;&#20064;&#36755;&#20986;&#29305;&#23450;&#20110;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#12290;&#23884;&#20837;&#32593;&#32476;&#23398;&#20064;&#20197;&#19968;&#31181;&#21453;&#26144;&#23427;&#20204;&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#28508;&#22312;&#25551;&#36848;&#31526;&#31354;&#38388;&#20013;&#34920;&#31034;&#23458;&#25143;&#31471;&#12290;&#36229;&#32593;&#32476;&#23398;&#20064;&#20174;&#36825;&#20010;&#28508;&#22312;&#31354;&#38388;&#21040;&#21487;&#33021;&#30340;&#23458;&#25143;&#27169;&#22411;&#31354;&#38388;&#30340;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;PeFLL&#20135;&#29983;&#20102;&#26356;&#39640;&#20934;&#30830;&#29575;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized federated learning (pFL) has emerged as a popular approach to dealing with the challenge of statistical heterogeneity between the data distributions of the participating clients. Instead of learning a single global model, pFL aims to learn an individual model for each client while still making use of the data available at other clients. In this work, we present PeFLL, a new pFL approach rooted in lifelong learning that performs well not only on clients present during its training phase, but also on any that may emerge in the future. PeFLL learns to output client specific models by jointly training an embedding network and a hypernetwork. The embedding network learns to represent clients in a latent descriptor space in a way that reflects their similarity to each other. The hypernetwork learns a mapping from this latent space to the space of possible client models. We demonstrate experimentally that PeFLL produces models of superior accuracy compared to previous methods, es
&lt;/p&gt;</description></item><item><title>AMEE&#26159;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#35299;&#37322;&#35780;&#20215;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#21644;&#27604;&#36739;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#22810;&#31181;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#20449;&#24687;&#20215;&#20540;&#65292;&#24110;&#21161;&#35299;&#20915;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#35299;&#37322;&#26041;&#27861;&#36873;&#25321;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.05501</link><description>&lt;p&gt;
AMEE&#65306;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#35299;&#37322;&#35780;&#20215;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AMEE: A Robust Framework for Explanation Evaluation in Time Series Classification. (arXiv:2306.05501v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05501
&lt;/p&gt;
&lt;p&gt;
AMEE&#26159;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#35299;&#37322;&#35780;&#20215;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#21644;&#27604;&#36739;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#22810;&#31181;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#20449;&#24687;&#20215;&#20540;&#65292;&#24110;&#21161;&#35299;&#20915;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#35299;&#37322;&#26041;&#27861;&#36873;&#25321;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#23450;&#37327;&#35780;&#20272;&#21644;&#25490;&#21517;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#35813;&#20219;&#21153;&#28041;&#21450;&#21040;&#21355;&#29983;&#20445;&#20581;&#21644;&#37329;&#34701;&#31561;&#20851;&#38190;&#39046;&#22495;&#30340;&#26222;&#36941;&#25968;&#25454;&#31867;&#22411;&#12290;&#26368;&#36817;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35299;&#37322;&#26041;&#27861;&#30340;&#30740;&#31350;&#20852;&#36259;&#28608;&#22686;&#65292;&#25552;&#20379;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#35299;&#37322;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20123;&#35299;&#37322;&#25216;&#26415;&#22312;&#29305;&#23450;&#38382;&#39064;&#19978;&#20135;&#29983;&#20998;&#27495;&#26102;&#65292;&#20173;&#28982;&#19981;&#28165;&#26970;&#20351;&#29992;&#21738;&#31181;&#25216;&#26415;&#12290;&#27604;&#36739;&#35299;&#37322;&#20197;&#25214;&#21040;&#27491;&#30830;&#31572;&#26696;&#24182;&#19981;&#23481;&#26131;&#12290;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65306;&#22914;&#20309;&#23450;&#37327;&#21644;&#31283;&#20581;&#22320;&#35780;&#20272;&#32473;&#23450;&#35299;&#37322;&#26041;&#27861;&#30340;&#20449;&#24687;&#20215;&#20540;&#65288;&#21363;&#19982;&#20998;&#31867;&#20219;&#21153;&#30456;&#20851;&#24615;&#65289;&#65292;&#20197;&#21450;&#22914;&#20309;&#24182;&#25490;&#27604;&#36739;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AMEE&#65292;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#35299;&#37322;&#35780;&#20215;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#21644;&#27604;&#36739;&#22810;&#31181;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#22312;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#20013;&#22686;&#21152;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
This paper aims to provide a framework to quantitatively evaluate and rank explanation methods for the time series classification task, which deals with a prevalent data type in critical domains such as healthcare and finance. The recent surge of research interest in explanation methods for time series classification has provided a great variety of explanation techniques. Nevertheless, when these explanation techniques disagree on a specific problem, it remains unclear which of them to use. Comparing the explanations to find the right answer is non-trivial. Two key challenges remain: how to quantitatively and robustly evaluate the informativeness (i.e., relevance for the classification task) of a given explanation method, and how to compare explanation methods side-by-side. We propose AMEE, a Model-Agnostic Explanation Evaluation framework for quantifying and comparing multiple saliency-based explanations for time series classification. Perturbation is added to the input time series gu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#23545;&#27604;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#23545;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#21487;&#35299;&#37322;&#20998;&#31867;&#65292;&#36798;&#21040;&#20102;&#19982;&#40657;&#30418;&#26041;&#27861;&#30456;&#24403;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#24182;&#20135;&#29983;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2306.03022</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Interpretable Alzheimer's Disease Classification Via a Contrastive Diffusion Autoencoder. (arXiv:2306.03022v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#23545;&#27604;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#23545;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#21487;&#35299;&#37322;&#20998;&#31867;&#65292;&#36798;&#21040;&#20102;&#19982;&#40657;&#30418;&#26041;&#27861;&#30456;&#24403;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#24182;&#20135;&#29983;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35270;&#21270;&#23545;&#35937;&#20998;&#31867;&#20013;&#65292;&#20154;&#31867;&#32463;&#24120;&#36890;&#36807;&#23558;&#23545;&#35937;&#19982;&#31867;&#21035;&#20869;&#30340;&#20856;&#22411;&#31034;&#20363;&#36827;&#34892;&#27604;&#36739;&#26469;&#35777;&#26126;&#20182;&#20204;&#30340;&#36873;&#25321;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#36171;&#20104;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#31867;&#20284;&#30340;&#25512;&#29702;&#26041;&#24335;&#26469;&#22686;&#21152;&#20854;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#24433;&#20687;&#19982;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#35757;&#32451;&#26679;&#26412;&#30340;&#30456;&#20284;&#24615;&#26469;&#23545;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#32467;&#21512;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#39592;&#24178;&#65292;&#29983;&#25104;&#19968;&#20010;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#30456;&#37051;&#30340;&#28508;&#22312;&#31354;&#38388;&#20855;&#26377;&#31867;&#20284;&#30340;&#22270;&#20687;&#32423;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;2D MRI&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#19982;&#40657;&#30418;&#26041;&#27861;&#30456;&#24403;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20135;&#29983;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#35299;&#37322;&#12290;&#22240;&#27492;&#65292;&#36825;&#39033;&#24037;&#20316;&#23545;&#20110;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#30340;&#30456;&#20851;&#21457;&#23637;&#26159;&#19968;&#39033;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In visual object classification, humans often justify their choices by comparing objects to prototypical examples within that class. We may therefore increase the interpretability of deep learning models by imbuing them with a similar style of reasoning. In this work, we apply this principle by classifying Alzheimer's Disease based on the similarity of images to training examples within the latent space. We use a contrastive loss combined with a diffusion autoencoder backbone, to produce a semantically meaningful latent space, such that neighbouring latents have similar image-level features. We achieve a classification accuracy comparable to black box approaches on a dataset of 2D MRI images, whilst producing human interpretable model explanations. Therefore, this work stands as a contribution to the pertinent development of accurate and interpretable deep learning within medical imaging.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#30417;&#30563;&#27169;&#22411;&#34892;&#20026;&#19968;&#33268;&#24615;&#32534;&#30721;&#26102;&#38388;&#24207;&#21015;&#35299;&#37322;&#65292;&#25552;&#20379;&#31163;&#25955;&#30340;&#24402;&#22240;&#22270;&#65292;&#24182;&#23398;&#20064;&#20102;&#19968;&#31181;&#21487;&#29992;&#20110;&#21508;&#31181;&#26041;&#24335;&#30340;&#35299;&#37322;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.02109</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#27169;&#22411;&#34892;&#20026;&#19968;&#33268;&#24615;&#23545;&#26102;&#38388;&#24207;&#21015;&#35299;&#37322;&#36827;&#34892;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Encoding Time-Series Explanations through Self-Supervised Model Behavior Consistency. (arXiv:2306.02109v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02109
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#27169;&#22411;&#34892;&#20026;&#19968;&#33268;&#24615;&#32534;&#30721;&#26102;&#38388;&#24207;&#21015;&#35299;&#37322;&#65292;&#25552;&#20379;&#31163;&#25955;&#30340;&#24402;&#22240;&#22270;&#65292;&#24182;&#23398;&#20064;&#20102;&#19968;&#31181;&#21487;&#29992;&#20110;&#21508;&#31181;&#26041;&#24335;&#30340;&#35299;&#37322;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#26159;&#19968;&#39033;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#30830;&#23450;&#39537;&#21160;&#27169;&#22411;&#39044;&#27979;&#30340;&#26102;&#38388;&#24207;&#21015;&#20449;&#21495;&#30340;&#20301;&#32622;&#20197;&#21450;&#23427;&#20204;&#19982;&#21487;&#35299;&#37322;&#30340;&#26102;&#38388;&#27169;&#24335;&#30340;&#21305;&#37197;&#12290;&#23613;&#31649;&#21487;&#20197;&#24212;&#29992;&#20854;&#20182;&#24418;&#24335;&#30340;&#35299;&#37322;&#22120;&#26469;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#65292;&#20294;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#24046;&#19981;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#25361;&#25112;&#24615;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TimeX&#65292;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#35299;&#37322;&#22120;&#30340;&#26102;&#38388;&#24207;&#21015;&#19968;&#33268;&#24615;&#27169;&#22411;&#12290;TimeX&#35757;&#32451;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#20195;&#29702;&#27169;&#22411;&#26469;&#27169;&#20223;&#39044;&#35757;&#32451;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#34892;&#20026;&#19968;&#33268;&#24615;&#26469;&#35299;&#20915;&#27169;&#22411;&#30340;&#24544;&#23454;&#24230;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20844;&#24335;&#65292;&#23427;&#20445;&#30041;&#20102;&#30001;&#39044;&#35757;&#32451;&#27169;&#22411;&#24341;&#36215;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20851;&#31995;&#19982;&#30001;TimeX&#24341;&#36215;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20851;&#31995;&#12290;TimeX&#25552;&#20379;&#31163;&#25955;&#30340;&#24402;&#22240;&#22270;&#65292;&#19981;&#21516;&#20110;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#20102;&#19968;&#31181;&#21487;&#20197;&#20197;&#21508;&#31181;&#26041;&#24335;&#20351;&#29992;&#30340;&#35299;&#37322;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20363;&#22914;&#65292;&#29992;&#20110;&#25552;&#20379;&#21487;&#35270;&#21270;&#30340;&#22320;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpreting time series models is uniquely challenging because it requires identifying both the location of time series signals that drive model predictions and their matching to an interpretable temporal pattern. While explainers from other modalities can be applied to time series, their inductive biases do not transfer well to the inherently challenging interpretation of time series. We present TimeX, a time series consistency model for training explainers. TimeX trains an interpretable surrogate to mimic the behavior of a pretrained time series model. It addresses the issue of model faithfulness by introducing model behavior consistency, a novel formulation that preserves relations in the latent space induced by the pretrained model with relations in the latent space induced by TimeX. TimeX provides discrete attribution maps and, unlike existing interpretability methods, it learns a latent space of explanations that can be used in various ways, such as to provide landmarks to visua
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NUDGE&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#35757;&#32451;&#22909;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#26469;&#24341;&#23548;&#36923;&#36753;&#35268;&#21017;&#30340;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.01439</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#24341;&#23548;&#31526;&#21495;&#25277;&#35937;&#23454;&#29616;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#36923;&#36753;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Interpretable and Explainable Logical Policies via Neurally Guided Symbolic Abstraction. (arXiv:2306.01439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01439
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NUDGE&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#35757;&#32451;&#22909;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#26469;&#24341;&#23548;&#36923;&#36753;&#35268;&#21017;&#30340;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#35201;&#30340;&#26377;&#38480;&#20808;&#39564;&#20351;&#20854;&#25104;&#20026;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#32534;&#30721;&#21644;&#23398;&#20064;&#31574;&#30053;&#30340;&#20027;&#35201;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#26159;&#40657;&#21283;&#23376;&#65292;&#22312;&#24037;&#20316;&#22312;&#22270;&#20687;&#32423;&#21035;&#26102;&#38590;&#20197;&#29702;&#35299;&#20195;&#29702;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#31070;&#32463;&#31526;&#21495;RL&#26088;&#22312;&#39318;&#20808;&#21019;&#24314;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21487;&#35299;&#37322;&#24615;&#19981;&#24847;&#21619;&#30528;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#24341;&#23548;&#21487;&#24494;&#20998;&#36923;&#36753;&#31574;&#30053;&#65288;NUDGE&#65289;&#12290;NUDGE&#21033;&#29992;&#35757;&#32451;&#22909;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#26469;&#24341;&#23548;&#20505;&#36873;&#21152;&#26435;&#36923;&#36753;&#35268;&#21017;&#30340;&#25628;&#32034;&#65292;&#28982;&#21518;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#36923;&#36753;&#26469;&#35757;&#32451;&#36923;&#36753;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;NUDGE&#20195;&#29702;&#21487;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#65292;&#21516;&#26102;&#32988;&#36807;&#32431;&#31070;&#32463;&#20195;&#29702;&#65292;&#24182;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#28789;&#27963;&#24615;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#21021;&#22987;&#29366;&#24577;&#21644;&#38382;&#39064;&#22823;&#23567;&#30340;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
The limited priors required by neural networks make them the dominating choice to encode and learn policies using reinforcement learning (RL). However, they are also black-boxes, making it hard to understand the agent's behaviour, especially when working on the image level. Therefore, neuro-symbolic RL aims at creating policies that are interpretable in the first place. Unfortunately, interpretability is not explainability. To achieve both, we introduce Neurally gUided Differentiable loGic policiEs (NUDGE). NUDGE exploits trained neural network-based agents to guide the search of candidate-weighted logic rules, then uses differentiable logic to train the logic agents. Our experimental evaluation demonstrates that NUDGE agents can induce interpretable and explainable policies while outperforming purely neural ones and showing good flexibility to environments of different initial states and problem sizes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36870;&#38382;&#39064;&#30340;&#30452;&#25509;&#25193;&#25955;&#38142;&#26725;&#31639;&#27861;&#65292;&#25552;&#39640;&#20102;&#36870;&#38382;&#39064;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#25968;&#25454;&#19968;&#33268;&#24615;&#35299;&#20915;&#20102;&#24403;&#21069;DDB&#26694;&#26550;&#23384;&#22312;&#30340;&#20851;&#38190;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.19809</link><description>&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#30452;&#25509;&#25193;&#25955;&#38142;&#26725;&#35299;&#20915;&#36870;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Direct Diffusion Bridge using Data Consistency for Inverse Problems. (arXiv:2305.19809v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36870;&#38382;&#39064;&#30340;&#30452;&#25509;&#25193;&#25955;&#38142;&#26725;&#31639;&#27861;&#65292;&#25552;&#39640;&#20102;&#36870;&#38382;&#39064;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#25968;&#25454;&#19968;&#33268;&#24615;&#35299;&#20915;&#20102;&#24403;&#21069;DDB&#26694;&#26550;&#23384;&#22312;&#30340;&#20851;&#38190;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#36870;&#38382;&#39064;&#27714;&#35299;&#22120;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#36895;&#24230;&#21463;&#38480;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#38656;&#35201;&#20174;&#22122;&#22768;&#24320;&#22987;&#36827;&#34892;&#21453;&#21521;&#25193;&#25955;&#37319;&#26679;&#12290;&#36817;&#26399;&#30340;&#19968;&#20123;&#24037;&#20316;&#23581;&#35797;&#36890;&#36807;&#26500;&#24314;&#25193;&#25955;&#36807;&#31243;&#26469;&#30452;&#25509;&#26725;&#25509;&#29305;&#23450;&#36870;&#38382;&#39064;&#30340;&#28165;&#27905;&#21644;&#27745;&#26579;&#25968;&#25454;&#20197;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#36825;&#20123;&#29616;&#26377;&#24037;&#20316;&#32479;&#19968;&#21629;&#21517;&#20026;&#30452;&#25509;&#25193;&#25955;&#38142;&#26725;&#65288;DDB&#65289;&#65292;&#35777;&#26126;&#23613;&#31649;&#21463;&#19981;&#21516;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#20294;&#30001;&#27492;&#20135;&#29983;&#30340;&#31639;&#27861;&#22312;&#21442;&#25968;&#36873;&#25321;&#19978;&#30340;&#19981;&#21516;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#24403;&#21069;DDB&#26694;&#26550;&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#65292;&#21363;&#23427;&#19981;&#33021;&#20445;&#35777;&#25968;&#25454;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#25512;&#26029;&#31243;&#24207;&#65292;&#23427;&#22312;&#19981;&#38656;&#35201;&#31934;&#32454;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#24378;&#21046;&#25968;&#25454;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#23558;&#24471;&#21040;&#30340;&#26041;&#27861;&#31216;&#20026;&#25968;&#25454;&#19968;&#33268;&#30340;DDB&#65288;CDDB&#65289;&#65292;&#23427;&#22312;&#24863;&#30693;&#21644;&#22833;&#30495;&#25351;&#26631;&#26041;&#38754;&#37117;&#20248;&#20110;&#19981;&#19968;&#33268;&#30340;&#23545;&#24212;&#29289;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#25512;&#21160;&#20102;&#36870;&#38382;&#39064;&#27714;&#35299;&#22120;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion model-based inverse problem solvers have shown impressive performance, but are limited in speed, mostly as they require reverse diffusion sampling starting from noise. Several recent works have tried to alleviate this problem by building a diffusion process, directly bridging the clean and the corrupted for specific inverse problems. In this paper, we first unify these existing works under the name Direct Diffusion Bridges (DDB), showing that while motivated by different theories, the resulting algorithms only differ in the choice of parameters. Then, we highlight a critical limitation of the current DDB framework, namely that it does not ensure data consistency. To address this problem, we propose a modified inference procedure that imposes data consistency without the need for fine-tuning. We term the resulting method data Consistent DDB (CDDB), which outperforms its inconsistent counterpart in terms of both perception and distortion metrics, thereby effectively pushing the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#26469;&#20248;&#21270;&#20219;&#20309;&#32452;&#21512;&#30340;&#21487;&#20998;&#31163;&#30446;&#26631;&#21644;&#32422;&#26463;&#26465;&#20214;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#27604;&#36890;&#29992;&#27714;&#35299;&#22120;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.19706</link><description>&lt;p&gt;
&#21487;&#20998;&#30446;&#26631;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#65306;&#25512;&#21160;&#21160;&#24577;&#35268;&#21010;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Optimal Decision Trees for Separable Objectives: Pushing the Limits of Dynamic Programming. (arXiv:2305.19706v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#26469;&#20248;&#21270;&#20219;&#20309;&#32452;&#21512;&#30340;&#21487;&#20998;&#31163;&#30446;&#26631;&#21644;&#32422;&#26463;&#26465;&#20214;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#27604;&#36890;&#29992;&#27714;&#35299;&#22120;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#30340;&#20840;&#23616;&#20248;&#21270;&#22312;&#20934;&#30830;&#24615;&#65292;&#22823;&#23567;&#21644;&#20154;&#31867;&#21487;&#29702;&#35299;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#36890;&#29992;&#27714;&#35299;&#22120;&#65292;&#21487;&#25193;&#23637;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#23558;&#23376;&#26641;&#20316;&#20026;&#29420;&#31435;&#30340;&#23376;&#38382;&#39064;&#35299;&#20915;&#26469;&#21033;&#29992;&#26641;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#20165;&#36866;&#29992;&#20110;&#21487;&#20197;&#20998;&#21035;&#20248;&#21270;&#23376;&#26641;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#35814;&#32454;&#30740;&#31350;&#20102;&#36825;&#31181;&#20851;&#31995;&#65292;&#24182;&#23637;&#31034;&#20102;&#23454;&#29616;&#36825;&#31181;&#21487;&#20998;&#31163;&#32422;&#26463;&#21644;&#30446;&#26631;&#20219;&#24847;&#32452;&#21512;&#30340;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#12290;&#22312;&#22235;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#65292;&#21516;&#26102;&#20063;&#27604;&#36890;&#29992;&#27714;&#35299;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global optimization of decision trees has shown to be promising in terms of accuracy, size, and consequently human comprehensibility. However, many of the methods used rely on general-purpose solvers for which scalability remains an issue. Dynamic programming methods have been shown to scale much better because they exploit the tree structure by solving subtrees as independent subproblems. However, this only works when an objective can be optimized separately for subtrees. We explore this relationship in detail and show necessary and sufficient conditions for such separability and generalize previous dynamic programming approaches into a framework that can optimize any combination of separable objectives and constraints. Experiments on four application domains show the general applicability of this framework, while outperforming the scalability of general-purpose solvers by a large margin.
&lt;/p&gt;</description></item><item><title>&#26080;&#27861;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#26041;&#27861;&#20855;&#26377;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#28508;&#21147;&#65292;&#20294;&#23454;&#38469;&#20351;&#29992;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#22312;&#26080;&#27861;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#19978;&#21487;&#20197;&#23398;&#20064;&#21040;&#26377;&#29992;&#30340;&#29305;&#24449;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#31616;&#21333;&#35268;&#21017;&#65292;&#36825;&#23545;&#22270;&#20687;&#20445;&#25252;&#30340;&#25928;&#26524;&#19981;&#30830;&#23450;&#12290;&#27492;&#22806;&#65292;&#32447;&#24615;&#21487;&#20998;&#30340;&#25200;&#21160;&#24182;&#19981;&#26159;&#35825;&#23548;&#23398;&#20064;&#25463;&#24452;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#22240;&#27492;&#19981;&#33021;&#20381;&#36182;&#23427;&#20204;&#12290;</title><link>http://arxiv.org/abs/2305.19254</link><description>&lt;p&gt;
&#26080;&#27861;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#32473;&#25105;&#20204;&#24102;&#26469;&#21738;&#20123;&#21551;&#31034;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Can We Learn from Unlearnable Datasets?. (arXiv:2305.19254v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19254
&lt;/p&gt;
&lt;p&gt;
&#26080;&#27861;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#26041;&#27861;&#20855;&#26377;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#28508;&#21147;&#65292;&#20294;&#23454;&#38469;&#20351;&#29992;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#22312;&#26080;&#27861;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#19978;&#21487;&#20197;&#23398;&#20064;&#21040;&#26377;&#29992;&#30340;&#29305;&#24449;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#31616;&#21333;&#35268;&#21017;&#65292;&#36825;&#23545;&#22270;&#20687;&#20445;&#25252;&#30340;&#25928;&#26524;&#19981;&#30830;&#23450;&#12290;&#27492;&#22806;&#65292;&#32447;&#24615;&#21487;&#20998;&#30340;&#25200;&#21160;&#24182;&#19981;&#26159;&#35825;&#23548;&#23398;&#20064;&#25463;&#24452;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#22240;&#27492;&#19981;&#33021;&#20381;&#36182;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26222;&#36941;&#36827;&#34892;&#32593;&#32476;&#29228;&#34411;&#30340;&#26102;&#20195;&#65292;&#26080;&#27861;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#26041;&#27861;&#20855;&#26377;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12289;&#38450;&#27490;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#30340;&#28508;&#21147;&#12290;&#20294;&#38500;&#20102;&#19968;&#20123;&#23454;&#38469;&#38480;&#21046;&#20351;&#24471;&#23427;&#20204;&#30340;&#20351;&#29992;&#19981;&#22826;&#21487;&#33021;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#32467;&#26524;&#23545;&#20854;&#20445;&#25252;&#25968;&#25454;&#33021;&#21147;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#39318;&#20808;&#65292;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#22312;&#26080;&#27861;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21482;&#20250;&#23398;&#20064;&#21040;&#25463;&#24452;&#65292;&#21363;&#24182;&#19981;&#36866;&#29992;&#20110;&#27867;&#21270;&#30340;&#31616;&#21333;&#35268;&#21017;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#32593;&#32476;&#23454;&#38469;&#19978;&#21487;&#20197;&#23398;&#20064;&#21040;&#26377;&#29992;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#37325;&#26032;&#21152;&#26435;&#20197;&#33719;&#24471;&#39640;&#27979;&#35797;&#24615;&#33021;&#65292;&#36825;&#34920;&#26126;&#22270;&#20687;&#30340;&#20445;&#25252;&#24182;&#19981;&#33021;&#24471;&#21040;&#20445;&#35777;&#12290;&#26080;&#27861;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#25454;&#20449;&#36890;&#36807;&#28155;&#21152;&#25200;&#21160;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#26469;&#35825;&#23548;&#23398;&#20064;&#25463;&#24452;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21453;&#20363;&#65292;&#35777;&#26126;&#20102;&#25200;&#21160;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#24182;&#19981;&#26159;&#24517;&#35201;&#26465;&#20214;&#12290;&#20026;&#20102;&#24378;&#35843;&#20026;&#20160;&#20040;&#19981;&#33021;&#20381;&#36182;&#32447;&#24615;&#21487;&#20998;&#30340;&#25200;&#21160;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#20132;&#25237;&#24433;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an era of widespread web scraping, unlearnable dataset methods have the potential to protect data privacy by preventing deep neural networks from generalizing. But in addition to a number of practical limitations that make their use unlikely, we make a number of findings that call into question their ability to safeguard data. First, it is widely believed that neural networks trained on unlearnable datasets only learn shortcuts, simpler rules that are not useful for generalization. In contrast, we find that networks actually can learn useful features that can be reweighed for high test performance, suggesting that image protection is not assured. Unlearnable datasets are also believed to induce learning shortcuts through linear separability of added perturbations. We provide a counterexample, demonstrating that linear separability of perturbations is not a necessary condition. To emphasize why linearly separable perturbations should not be relied upon, we propose an orthogonal proje
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;Maximize to Explore (MEX)&#65292;&#21482;&#38656;&#20248;&#21270;&#19968;&#20010;&#26080;&#32422;&#26463;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#33258;&#21160;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#65292;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2305.18258</link><description>&lt;p&gt;
&#19968;&#31181;&#34701;&#21512;&#20272;&#35745;&#21644;&#35268;&#21010;&#23454;&#29616;&#25506;&#32034;&#30340;&#26368;&#22823;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One Objective to Rule Them All: A Maximization Objective Fusing Estimation and Planning for Exploration. (arXiv:2305.18258v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18258
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;Maximize to Explore (MEX)&#65292;&#21482;&#38656;&#20248;&#21270;&#19968;&#20010;&#26080;&#32422;&#26463;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#33258;&#21160;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#65292;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#23545;&#20110;&#20197;&#26377;&#25928;&#30340;&#26041;&#24335;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#29616;&#26377;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#21253;&#25324;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#20272;&#35745;&#12289;&#35268;&#21010;&#21644;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#24212;&#23545;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#37117;&#38656;&#35201;&#20351;&#29992;&#19981;&#20999;&#23454;&#38469;&#30340;&#31639;&#27861;&#32452;&#20214;&#26469;&#28608;&#21169;&#25506;&#32034;&#65292;&#20363;&#22914;&#25968;&#25454;&#30456;&#20851;&#30340;&#32423;&#21035;&#38598;&#20869;&#20248;&#21270;&#25110;&#32321;&#29712;&#30340;&#37319;&#26679;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#23454;&#29616;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;Maximize to Explore (MEX) &#65292;&#23427;&#21482;&#38656;&#35201;&#26080;&#32422;&#26463;&#22320;&#20248;&#21270;&#19968;&#20010;&#38598;&#25104;&#20102;&#20272;&#35745;&#21644;&#35268;&#21010;&#32452;&#20214;&#30340;&#21333;&#19968;&#30446;&#26631;&#20989;&#25968;&#65292;&#21516;&#26102;&#33258;&#21160;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#65292;MEX&#23454;&#29616;&#20102;&#19968;&#20010;&#27425;&#32447;&#24615;&#30340;&#36951;&#25022;&#65292;&#36827;&#19968;&#27493;&#65306;
&lt;/p&gt;
&lt;p&gt;
In online reinforcement learning (online RL), balancing exploration and exploitation is crucial for finding an optimal policy in a sample-efficient way. To achieve this, existing sample-efficient online RL algorithms typically consist of three components: estimation, planning, and exploration. However, in order to cope with general function approximators, most of them involve impractical algorithmic components to incentivize exploration, such as optimization within data-dependent level-sets or complicated sampling procedures. To address this challenge, we propose an easy-to-implement RL framework called \textit{Maximize to Explore} (\texttt{MEX}), which only needs to optimize \emph{unconstrainedly} a single objective that integrates the estimation and planning components while balancing exploration and exploitation automatically. Theoretically, we prove that \texttt{MEX} achieves a sublinear regret with general function approximations for Markov decision processes (MDP) and is further 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#20998;&#27573;&#20223;&#23556;&#25805;&#20316;&#20195;&#26367;&#20256;&#32479;&#20056;&#27861;&#30340;&#39640;&#25928;Transformer&#35757;&#32451;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#26356;&#25913;&#35757;&#32451;&#36229;&#21442;&#25968;&#21363;&#21487;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#25104;&#21151;&#23454;&#29616;&#35757;&#32451;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25152;&#26377;&#20056;&#27861;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2305.17190</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#27573;&#20223;&#23556;&#25805;&#20316;&#23454;&#29616;&#39640;&#25928;Transformer&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Hardware-Efficient Transformer Training via Piecewise Affine Operations. (arXiv:2305.17190v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#20998;&#27573;&#20223;&#23556;&#25805;&#20316;&#20195;&#26367;&#20256;&#32479;&#20056;&#27861;&#30340;&#39640;&#25928;Transformer&#35757;&#32451;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#26356;&#25913;&#35757;&#32451;&#36229;&#21442;&#25968;&#21363;&#21487;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#25104;&#21151;&#23454;&#29616;&#35757;&#32451;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25152;&#26377;&#20056;&#27861;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#25512;&#29702;&#20013;&#65292;&#22823;&#22810;&#25968;&#35745;&#31639;&#25104;&#26412;&#37117;&#26159;&#30001;&#20056;&#27861;&#25152;&#36129;&#29486;&#30340;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#20943;&#23569;&#30001;&#27492;&#24102;&#26469;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#21463;Mogami&#65288;2020&#65289;&#21551;&#21457;&#65292;&#29992;&#19968;&#31181;&#24265;&#20215;&#30340;&#20998;&#27573;&#20223;&#23556;&#36924;&#36817;&#26367;&#25442;&#20056;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#28014;&#28857;&#25968;&#30340;&#20301;&#34920;&#31034;&#20316;&#20026;&#25972;&#25968;&#30456;&#21152;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#65292;&#21487;&#20197;&#20351;&#29992;&#25152;&#24471;&#21040;&#30340;&#20462;&#25913;&#21518;&#30340;&#30697;&#38453;&#20056;&#27861;&#35757;&#32451;Transformer&#65292;&#20960;&#20046;&#27809;&#26377;&#24615;&#33021;&#24433;&#21709;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#26356;&#25913;&#35757;&#32451;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#29992;&#20998;&#27573;&#20223;&#23556;&#26367;&#25442;&#20102;&#32593;&#32476;&#20013;&#30340;&#25152;&#26377;&#38750;&#32447;&#24615;&#65292;&#20351;&#23427;&#20204;&#22312;&#36755;&#20837;&#21644;&#26435;&#37325;&#26041;&#38754;&#37117;&#25104;&#20026;&#23436;&#20840;&#32852;&#21512;&#30340;&#20998;&#27573;&#20223;&#23556;&#20989;&#25968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#28040;&#38500;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25152;&#26377;&#20056;&#27861;&#25805;&#20316;&#65292;&#21253;&#25324;&#21069;&#21521;&#20256;&#25773;&#12289;&#21453;&#21521;&#20256;&#25773;&#21644;&#20248;&#21270;&#22120;&#26356;&#26032;&#30340;&#25805;&#20316;&#65292;&#23637;&#31034;&#20102;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#39318;&#27425;&#25104;&#21151;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiplications are responsible for most of the computational cost involved in neural network training and inference. Recent research has thus looked for ways to reduce the cost associated with them. Inspired by Mogami (2020), we replace multiplication with a cheap piecewise affine approximation that is achieved by adding the bit representation of the floating point numbers together as integers. We show that transformers can be trained with the resulting modified matrix multiplications on both vision and language tasks with little to no performance impact, and without changes to the training hyperparameters. We further replace all non-linearities in the networks making them fully and jointly piecewise affine in both inputs and weights. Finally, we show that we can eliminate all multiplications in the entire training process, including operations in the forward pass, backward pass and optimizer update, demonstrating the first successful training of modern neural network architectures in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#20219;&#21153;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#36870;&#21160;&#21147;&#23398;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#23398;&#20064;&#39640;&#32500;&#35266;&#23519;&#31354;&#38388;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#36716;&#31227;&#21040;&#24494;&#35843;&#25968;&#25454;&#38598;&#36827;&#34892;&#20219;&#21153;&#25191;&#34892;&#12290;</title><link>http://arxiv.org/abs/2305.16985</link><description>&lt;p&gt;
&#36870;&#21160;&#21147;&#23398;&#39044;&#35757;&#32451;&#20026;&#22810;&#20219;&#21153;&#27169;&#20223;&#23398;&#20064;&#23398;&#20064;&#33391;&#22909;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Inverse Dynamics Pretraining Learns Good Representations for Multitask Imitation. (arXiv:2305.16985v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#20219;&#21153;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#36870;&#21160;&#21147;&#23398;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#23398;&#20064;&#39640;&#32500;&#35266;&#23519;&#31354;&#38388;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#36716;&#31227;&#21040;&#24494;&#35843;&#25968;&#25454;&#38598;&#36827;&#34892;&#20219;&#21153;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22270;&#20687;&#35782;&#21035;&#31561;&#39046;&#22495;&#26222;&#21450;&#20102;&#20351;&#29992;&#22823;&#22411;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#34920;&#31034;&#30340;&#33539;&#20363;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#24212;&#35813;&#22914;&#20309;&#36827;&#34892;&#36825;&#26679;&#30340;&#33539;&#20363;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#25968;&#25454;&#37117;&#26159;&#30001;&#19987;&#23478;&#19982;&#26410;&#30693;&#29615;&#22659;&#20132;&#20114;&#20135;&#29983;&#30340;&#36712;&#36857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#24773;&#20917;&#65292;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30001;&#22810;&#20219;&#21153;&#28436;&#31034;&#32452;&#25104;&#65292;&#27599;&#20010;&#28436;&#31034;&#30340;&#20219;&#21153;&#30001;&#19968;&#20010;&#26410;&#35266;&#23519;&#21040;&#30340;&#28508;&#22312;&#19978;&#19979;&#25991;&#21464;&#37327;&#35774;&#23450;&#12290;&#30446;&#26631;&#26159;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#23398;&#20064;&#39640;&#32500;&#65288;&#20363;&#22914;&#35270;&#35273;&#65289;&#35266;&#23519;&#31354;&#38388;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#35813;&#34920;&#31034;&#21487;&#20197;&#36716;&#31227;&#21040;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#20197;&#22312;&#26377;&#38480;&#30340;&#28436;&#31034;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#21508;&#31181;&#21487;&#33021;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36870;&#21160;&#21147;&#23398;&#24314;&#27169;&#26159;&#21512;&#36866;&#30340;&#65292;&#21363;&#26681;&#25454;&#35266;&#23519;&#26469;&#39044;&#27979;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, domains such as natural language processing and image recognition have popularized the paradigm of using large datasets to pretrain representations that can be effectively transferred to downstream tasks. In this work we evaluate how such a paradigm should be done in imitation learning, where both pretraining and finetuning data are trajectories collected by experts interacting with an unknown environment. Namely, we consider a setting where the pretraining corpus consists of multitask demonstrations and the task for each demonstration is set by an unobserved latent context variable. The goal is to use the pretraining corpus to learn a low dimensional representation of the high dimensional (e.g., visual) observation space which can be transferred to a novel context for finetuning on a limited dataset of demonstrations. Among a variety of possible pretraining objectives, we argue that inverse dynamics modeling -- i.e., predicting an action given the observations appeari
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#38543;&#26426;&#24120;&#25968;&#28145;&#24230;&#32593;&#32476;&#30340;PTAS&#26041;&#27861;&#65292;&#23545;&#20110;&#20219;&#20309;&#22266;&#23450;&#35823;&#24046;&#21644;&#28145;&#24230;&#65292;&#20960;&#20046;&#25152;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#37117;&#26159;&#21487;&#23398;&#20064;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.16508</link><description>&lt;p&gt;
&#22823;&#37096;&#20998;&#31070;&#32463;&#32593;&#32476;&#20960;&#20046;&#26159;&#21487;&#23398;&#20064;&#30340;
&lt;/p&gt;
&lt;p&gt;
Most Neural Networks Are Almost Learnable. (arXiv:2305.16508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#38543;&#26426;&#24120;&#25968;&#28145;&#24230;&#32593;&#32476;&#30340;PTAS&#26041;&#27861;&#65292;&#23545;&#20110;&#20219;&#20309;&#22266;&#23450;&#35823;&#24046;&#21644;&#28145;&#24230;&#65292;&#20960;&#20046;&#25152;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#37117;&#26159;&#21487;&#23398;&#20064;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;PTAS&#26469;&#23398;&#20064;&#38543;&#26426;&#24120;&#25968;&#28145;&#24230;&#32593;&#32476;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20219;&#20309;&#22266;&#23450;&#30340;$\epsilon&gt;0$&#21644;&#28145;&#24230;$i$&#65292;&#23384;&#22312;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#23545;&#20110;$\sqrt{d} \cdot \mathbb{S}^{d-1}$&#19978;&#30340;&#20219;&#20309;&#20998;&#24067;&#65292;&#23398;&#20064;&#38543;&#26426;Xavier&#32593;&#32476;&#30340;&#28145;&#24230;$i$&#65292;&#35823;&#24046;&#20026;$\epsilon$&#12290;&#35813;&#31639;&#27861;&#30340;&#26102;&#38388;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$(\bar{d})^{\mathrm{poly}(\epsilon^{-1})}$&#65292;&#20854;&#20013;$\bar d$&#26159;&#32593;&#32476;&#30340;&#22823;&#23567;&#12290;&#23545;&#20110;&#26576;&#20123;&#31867;&#20284;&#20110;Sigmoid&#21644;ReLU&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#21487;&#20197;&#23558;&#35823;&#24046;&#30028;&#38480;&#25913;&#36827;&#20026;$(\bar{d})^{\mathrm{polylog}(\epsilon^{-1})}$&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#31181;&#20960;&#20046;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#23398;&#20064;&#24120;&#25968;&#28145;&#24230;&#38543;&#26426;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a PTAS for learning random constant-depth networks. We show that for any fixed $\epsilon&gt;0$ and depth $i$, there is a poly-time algorithm that for any distribution on $\sqrt{d} \cdot \mathbb{S}^{d-1}$ learns random Xavier networks of depth $i$, up to an additive error of $\epsilon$. The algorithm runs in time and sample complexity of $(\bar{d})^{\mathrm{poly}(\epsilon^{-1})}$, where $\bar d$ is the size of the network. For some cases of sigmoid and ReLU-like activations the bound can be improved to $(\bar{d})^{\mathrm{polylog}(\epsilon^{-1})}$, resulting in a quasi-poly-time algorithm for learning constant depth random networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#32447;&#24615;&#39044;&#27979;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#30456;&#20851;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#26032;&#32467;&#26524;&#65292;&#35299;&#20915;&#20102;&#19968;&#20123;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#26032;&#30340;&#20984;&#32447;&#24615;&#39044;&#27979;&#38382;&#39064;&#21487;&#20197;&#34987;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.16475</link><description>&lt;p&gt;
&#32447;&#24615;&#39044;&#27979;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#21021;&#22987;&#21270;&#30456;&#20851;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Initialization-Dependent Sample Complexity of Linear Predictors and Neural Networks. (arXiv:2305.16475v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#32447;&#24615;&#39044;&#27979;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#30456;&#20851;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#26032;&#32467;&#26524;&#65292;&#35299;&#20915;&#20102;&#19968;&#20123;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#26032;&#30340;&#20984;&#32447;&#24615;&#39044;&#27979;&#38382;&#39064;&#21487;&#20197;&#34987;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#21521;&#37327;&#20540;&#32447;&#24615;&#39044;&#27979;&#22120;(&#30001;&#30697;&#38453;&#21442;&#25968;&#21270;)&#12289;&#26356;&#19968;&#33324;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#26032;&#32467;&#26524;&#12290;&#19987;&#27880;&#20110;&#22823;&#23567;&#26080;&#20851;&#30340;&#30028;&#38480;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20165;&#25511;&#21046;&#20174;&#26576;&#20010;&#22266;&#23450;&#21442;&#32771;&#30697;&#38453;$W_0$&#30340;&#21442;&#25968;&#30340;Frobenius&#33539;&#25968;&#36317;&#31163;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#34892;&#20026;&#21487;&#20197;&#20986;&#20154;&#24847;&#26009;&#22320;&#19981;&#21516;&#20110;&#25105;&#20204;&#22312;&#30740;&#31350;&#26631;&#37327;&#20540;&#32447;&#24615;&#39044;&#27979;&#22120;&#26041;&#38754;&#25152;&#26399;&#26395;&#30340;&#12290;&#36825;&#36824;&#23548;&#33268;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#35299;&#20915;&#20102;&#19968;&#20123;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#30830;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#20984;&#32447;&#24615;&#39044;&#27979;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#32479;&#19968;&#25910;&#25947;&#30340;&#24773;&#20917;&#19979;&#34987;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide several new results on the sample complexity of vector-valued linear predictors (parameterized by a matrix), and more generally neural networks. Focusing on size-independent bounds, where only the Frobenius norm distance of the parameters from some fixed reference matrix $W_0$ is controlled, we show that the sample complexity behavior can be surprisingly different than what we may expect considering the well-studied setting of scalar-valued linear predictors. This also leads to new sample complexity bounds for feed-forward neural networks, tackling some open questions in the literature, and establishing a new convex linear prediction problem that is provably learnable without uniform convergence.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#24102;&#26377;&#25311;&#23545;&#25239;&#25200;&#21160;&#21644;&#26102;&#21464;&#23545;&#25239;&#24615;&#36172;&#21338;&#25439;&#22833;&#20989;&#25968;&#30340;LQR&#21644;LQG&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#35760;&#24518;&#30340;&#36172;&#21338;&#20984;&#20248;&#21270;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.15352</link><description>&lt;p&gt;
&#24102;&#26377;&#25311;&#23545;&#25239;&#25200;&#21160;&#21644;&#26102;&#21464;&#23545;&#25239;&#24615;&#36172;&#21338;&#25439;&#22833;&#20989;&#25968;&#30340;&#26368;&#20248;&#29575;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Optimal Rates for Bandit Nonstochastic Control. (arXiv:2305.15352v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#24102;&#26377;&#25311;&#23545;&#25239;&#25200;&#21160;&#21644;&#26102;&#21464;&#23545;&#25239;&#24615;&#36172;&#21338;&#25439;&#22833;&#20989;&#25968;&#30340;LQR&#21644;LQG&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#35760;&#24518;&#30340;&#36172;&#21338;&#20984;&#20248;&#21270;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#21644;&#32447;&#24615;&#20108;&#27425;&#39640;&#26031;&#65288;LQG&#65289;&#25511;&#21046;&#26159;&#26368;&#20248;&#25511;&#21046;&#20013;&#22522;&#30784;&#19988;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#25311;&#23545;&#25239;&#25200;&#21160;&#21644;&#26102;&#21464;&#23545;&#25239;&#24615;&#36172;&#21338;&#25439;&#22833;&#20989;&#25968;&#30340;LQR&#21644;LQG&#38382;&#39064;&#12290;&#24050;&#30693;&#30340;&#26368;&#20339;&#20122;&#32447;&#24615;&#36951;&#25022;&#31639;&#27861;~\cite{gradu2020non}&#22312;&#26102;&#38388;&#27178;&#36328;&#24230;&#19978;&#20855;&#26377;$T^{\frac{3}{4}}$&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20854;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#26159;&#21542;&#21487;&#20197;&#36798;&#21040;$\sqrt{T}$&#30340;&#32039;&#33268;&#29575;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#25105;&#20204;&#22238;&#31572;&#32943;&#23450;&#22320;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#23545;&#20110;&#24050;&#30693;&#21644;&#26410;&#30693;&#31995;&#32479;&#37117;&#33021;&#36798;&#21040;&#26368;&#20248;&#65288;&#38500;&#23545;&#25968;&#22240;&#23376;&#22806;&#65289;&#36951;&#25022;&#30340;&#36172;&#21338;LQR&#21644;LQG&#31639;&#27861;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#26159;&#19968;&#31181;&#20855;&#26377;&#35760;&#24518;&#30340;&#36172;&#21338;&#20984;&#20248;&#21270;&#26032;&#26041;&#26696;&#65292;&#36825;&#20855;&#26377;&#29420;&#31435;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear Quadratic Regulator (LQR) and Linear Quadratic Gaussian (LQG) control are foundational and extensively researched problems in optimal control. We investigate LQR and LQG problems with semi-adversarial perturbations and time-varying adversarial bandit loss functions. The best-known sublinear regret algorithm of~\cite{gradu2020non} has a $T^{\frac{3}{4}}$ time horizon dependence, and its authors posed an open question about whether a tight rate of $\sqrt{T}$ could be achieved. We answer in the affirmative, giving an algorithm for bandit LQR and LQG which attains optimal regret (up to logarithmic factors) for both known and unknown systems. A central component of our method is a new scheme for bandit convex optimization with memory, which is of independent interest.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#20108;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#21508;&#31181;&#20551;&#35774;&#19979;&#36807;&#25311;&#21512;&#30340;&#31867;&#22411;&#20250;&#20174;&#19968;&#32500;&#25968;&#25454;&#30340;&#26497;&#31471;&#24773;&#20917;&#19979;&#32531;&#21644;&#21040;&#39640;&#32500;&#30340;&#33391;&#24615;&#65292;&#25581;&#31034;&#20102;&#36755;&#20837;&#32500;&#24230;&#22312;&#31070;&#32463;&#32593;&#32476;&#36807;&#25311;&#21512;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.15141</link><description>&lt;p&gt;
&#20174;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#32531;&#21644;&#36807;&#25311;&#21512;&#21040;&#33391;&#24615;&#36807;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
From Tempered to Benign Overfitting in ReLU Neural Networks. (arXiv:2305.15141v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#20108;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#21508;&#31181;&#20551;&#35774;&#19979;&#36807;&#25311;&#21512;&#30340;&#31867;&#22411;&#20250;&#20174;&#19968;&#32500;&#25968;&#25454;&#30340;&#26497;&#31471;&#24773;&#20917;&#19979;&#32531;&#21644;&#21040;&#39640;&#32500;&#30340;&#33391;&#24615;&#65292;&#25581;&#31034;&#20102;&#36755;&#20837;&#32500;&#24230;&#22312;&#31070;&#32463;&#32593;&#32476;&#36807;&#25311;&#21512;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#34987;&#35266;&#23519;&#21040;&#21363;&#20351;&#35757;&#32451;&#27169;&#22411;&#26469;&#23436;&#32654;&#22320;&#36866;&#24212;&#22024;&#26434;&#30340;&#25968;&#25454;&#20063;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#12290;&#36825;&#19968;&#29616;&#35937;&#24341;&#21457;&#20102;&#22823;&#37327;&#20851;&#20110;&#8220;&#33391;&#24615;&#36807;&#25311;&#21512;&#8221;&#30340;&#24037;&#20316;&#65292;&#20854;&#20013;&#20869;&#25554;&#39044;&#27979;&#22120;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#26377;&#20154;&#29468;&#27979;&#24182;&#32463;&#39564;&#24615;&#22320;&#35266;&#23519;&#21040;&#31070;&#32463;&#32593;&#32476;&#30340;&#34892;&#20026;&#36890;&#24120;&#26356;&#22909;&#22320;&#25551;&#36848;&#20026;&#8220;&#32531;&#21644;&#36807;&#25311;&#21512;&#8221;&#65292;&#20854;&#20013;&#24615;&#33021;&#26082;&#38750;&#26368;&#20248;&#65292;&#20063;&#38750;&#24494;&#19981;&#36275;&#36947;&#65292;&#24182;&#38543;&#22122;&#22768;&#27700;&#24179;&#30340;&#21464;&#21270;&#32780;&#38477;&#20302;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#19968;&#20027;&#24352;&#23578;&#32570;&#20047;&#20851;&#20110;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#29702;&#35770;&#30340;&#35777;&#26126;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20960;&#20010;&#32467;&#26524;&#65292;&#26088;&#22312;&#24357;&#21512;&#36825;&#20123;&#20114;&#34917;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20998;&#31867;&#35774;&#32622;&#65292;&#20351;&#29992;&#20108;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#35777;&#26126;&#22312;&#21508;&#31181;&#20551;&#35774;&#19979;&#65292;&#36807;&#25311;&#21512;&#30340;&#31867;&#22411;&#20174;&#19968;&#32500;&#25968;&#25454;&#30340;&#26497;&#31471;&#24773;&#20917;&#19979;&#32531;&#21644;&#21040;&#39640;&#32500;&#30340;&#33391;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#36755;&#20837;&#32500;&#24230;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26377;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Overparameterized neural networks (NNs) are observed to generalize well even when trained to perfectly fit noisy data. This phenomenon motivated a large body of work on "benign overfitting", where interpolating predictors achieve near-optimal performance. Recently, it was conjectured and empirically observed that the behavior of NNs is often better described as "tempered overfitting", where the performance is non-optimal yet also non-trivial, and degrades as a function of the noise level. However, a theoretical justification of this claim for non-linear NNs has been lacking so far. In this work, we provide several results that aim at bridging these complementing views. We study a simple classification setting with 2-layer ReLU NNs, and prove that under various assumptions, the type of overfitting transitions from tempered in the extreme case of one-dimensional data, to benign in high dimensions. Thus, we show that the input dimension has a crucial role on the type of overfitting in thi
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#27969;&#24418;&#30340;&#25299;&#25169;&#29305;&#24449;&#20272;&#35745;&#25968;&#25454;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#65292;&#35813;&#26041;&#27861;&#19982;&#26377;&#30417;&#30563;&#24230;&#37327;&#20855;&#26377;&#30456;&#20851;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#21322;&#30417;&#30563;&#21644;&#24863;&#30693;&#23398;&#20064;&#12290;&#21516;&#26102;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#24212;&#29992;&#35813;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#20572;&#27490;&#20934;&#21017;&#12290;</title><link>http://arxiv.org/abs/2305.15016</link><description>&lt;p&gt;
&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#29992;&#20110;&#20272;&#35745;&#25968;&#25454;&#38598;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#65292;&#24182;&#24212;&#29992;&#21040;LLMs&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
An Unsupervised Method for Estimating Class Separability of Datasets with Application to LLMs Fine-Tuning. (arXiv:2305.15016v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15016
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#27969;&#24418;&#30340;&#25299;&#25169;&#29305;&#24449;&#20272;&#35745;&#25968;&#25454;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#65292;&#35813;&#26041;&#27861;&#19982;&#26377;&#30417;&#30563;&#24230;&#37327;&#20855;&#26377;&#30456;&#20851;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#21322;&#30417;&#30563;&#21644;&#24863;&#30693;&#23398;&#20064;&#12290;&#21516;&#26102;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#24212;&#29992;&#35813;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#20572;&#27490;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#27969;&#24418;&#30340;&#25299;&#25169;&#29305;&#24449;&#26469;&#20272;&#35745;&#25968;&#25454;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#65292;&#32780;&#19981;&#38656;&#35201;&#26631;&#31614;&#20449;&#24687;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#23545;&#20960;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20272;&#35745;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#19982;&#38656;&#35201;&#26631;&#31614;&#20449;&#24687;&#30340;&#30417;&#30563;&#24230;&#37327;&#65288;&#22914;Fisher&#21028;&#21035;&#27604;&#29575;&#65288;FDR&#65289;&#21644;&#20998;&#31867;&#22120;&#30340;&#20132;&#21449;&#39564;&#35777;&#65289;&#20043;&#38388;&#23384;&#22312;&#28165;&#26224;&#30340;&#30456;&#20851;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#36825;&#21487;&#20197;&#23454;&#29616;&#20174;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#22914;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#24863;&#30693;&#23398;&#20064;&#12290;&#24403;&#25105;&#20204;&#26377;&#38480;&#30340;&#26377;&#26631;&#31614;&#25968;&#25454;&#21644;&#30456;&#23545;&#36739;&#22823;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#26469;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#26102;&#65292;&#36825;&#23558;&#29305;&#21035;&#26377;&#29992;&#12290;&#35813;&#26041;&#27861;&#22312;&#26080;&#30417;&#30563;&#30340;&#35774;&#32622;&#19979;&#65292;&#36890;&#36807;&#30417;&#27979;&#23884;&#20837;&#31354;&#38388;&#27969;&#24418;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#21644;&#33258;&#21160;&#20572;&#27490;&#20934;&#21017;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an unsupervised method that leverages topological characteristics of data manifolds to estimate class separability of the data without requiring labels. Experiments conducted in this paper on several datasets demonstrate a clear correlation and consistency between the class separability estimated by the proposed method with supervised metrics like Fisher Discriminant Ratio~(FDR) and cross-validation of a classifier, which both require labels. This can enable implementing learning paradigms aimed at learning from both labeled and unlabeled data, like semi-supervised and transductive learning. This would be particularly useful when we have limited labeled data and a relatively large unlabeled dataset that can be used to enhance the learning process. The proposed method is implemented for language model fine-tuning with automated stopping criterion by monitoring class separability of the embedding-space manifold in an unsupervised setting. The proposed methodology has 
&lt;/p&gt;</description></item><item><title>S-CLIP&#26159;&#19968;&#31181;&#21033;&#29992;&#23569;&#37327;&#19987;&#19994;&#23383;&#24149;&#36827;&#34892;&#21322;&#30417;&#30563;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#20266;&#26631;&#31614;&#31574;&#30053;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#19987;&#19994;&#39046;&#22495;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14095</link><description>&lt;p&gt;
S-CLIP: &#20351;&#29992;&#23569;&#37327;&#19987;&#19994;&#23383;&#24149;&#30340;&#21322;&#30417;&#30563;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
S-CLIP: Semi-supervised Vision-Language Learning using Few Specialist Captions. (arXiv:2305.14095v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14095
&lt;/p&gt;
&lt;p&gt;
S-CLIP&#26159;&#19968;&#31181;&#21033;&#29992;&#23569;&#37327;&#19987;&#19994;&#23383;&#24149;&#36827;&#34892;&#21322;&#30417;&#30563;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#20266;&#26631;&#31614;&#31574;&#30053;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#19987;&#19994;&#39046;&#22495;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451; (CLIP)&#65292;&#22312;&#33258;&#28982;&#22270;&#20687;&#39046;&#22495;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#24212;&#29992;&#20110;&#36965;&#24863;&#31561;&#19987;&#19994;&#39046;&#22495;&#26102;&#24448;&#24448;&#36935;&#21040;&#22256;&#38590;&#65292;&#30001;&#20110;&#35757;&#32451;&#26102;&#21487;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#25968;&#37327;&#26377;&#38480;&#65292;&#23545;&#36825;&#20123;&#39046;&#22495;&#30340;&#36866;&#24212;&#24615;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;S-CLIP&#65292;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;CLIP&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#39069;&#22806;&#30340;&#26410;&#37197;&#23545;&#22270;&#20687;&#12290;S-CLIP&#37319;&#29992;&#20004;&#31181;&#20266;&#26631;&#31614;&#31574;&#30053;&#65292;&#19987;&#38376;&#38024;&#23545;&#23545;&#27604;&#23398;&#20064;&#21644;&#35821;&#35328;&#27169;&#24577;&#35774;&#35745;&#12290;&#23383;&#24149;&#32423;&#20266;&#26631;&#31614;&#30001;&#37197;&#23545;&#22270;&#20687;&#30340;&#23383;&#24149;&#32452;&#21512;&#32473;&#20986;&#65292;&#36890;&#36807;&#35299;&#20915;&#26410;&#37197;&#23545;&#21644;&#37197;&#23545;&#22270;&#20687;&#20043;&#38388;&#30340;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#33719;&#24471;&#12290;&#20851;&#38190;&#35789;&#32423;&#20266;&#26631;&#31614;&#30001;&#26368;&#36817;&#30340;&#37197;&#23545;&#22270;&#20687;&#23383;&#24149;&#20013;&#30340;&#20851;&#38190;&#35789;&#32473;&#20986;&#65292;&#36890;&#36807;&#20551;&#35774;&#20505;&#36873;&#26631;&#31614;&#38598;&#21512;&#36827;&#34892;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#20934;&#30830;&#30340;&#26631;&#31614;&#12290;&#36890;&#36807;&#32467;&#21512;&#36825;&#20004;&#31181;&#20266;&#26631;&#31614;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22312;&#19987;&#19994;&#39046;&#22495;&#36827;&#34892;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#30340;&#25928;&#26524;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models, such as contrastive language-image pre-training (CLIP), have demonstrated impressive results in natural image domains. However, these models often struggle when applied to specialized domains like remote sensing, and adapting to such domains is challenging due to the limited number of image-text pairs available for training. To address this, we propose S-CLIP, a semi-supervised learning method for training CLIP that utilizes additional unpaired images. S-CLIP employs two pseudo-labeling strategies specifically designed for contrastive learning and the language modality. The caption-level pseudo-label is given by a combination of captions of paired images, obtained by solving an optimal transport problem between unpaired and paired images. The keyword-level pseudo-label is given by a keyword in the caption of the nearest paired image, trained through partial label learning that assumes a candidate set of labels for supervision instead of the exact one. By combini
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25552;&#31034;&#26469;&#25191;&#34892;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;LLM&#22312;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#24615;&#33021;&#21464;&#24322;&#24040;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.11430</link><description>&lt;p&gt;
TELeR&#65306;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#22797;&#26434;&#20219;&#21153;&#30340;LLM&#25552;&#31034;&#30340;&#36890;&#29992;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks. (arXiv:2305.11430v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25552;&#31034;&#26469;&#25191;&#34892;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;LLM&#22312;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#24615;&#33021;&#21464;&#24322;&#24040;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;LLM&#22312;&#20256;&#32479;&#23545;&#35805;&#29615;&#22659;&#20013;&#29702;&#35299;&#21644;&#29983;&#25104;&#25991;&#26412;&#26102;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#25191;&#34892;&#19981;&#26126;&#30830;&#30340;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#20173;&#28982;&#21463;&#21040;&#24456;&#23569;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25552;&#31034;&#65292;&#20197;&#25191;&#34892;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20351;&#29992;&#19981;&#21516;&#25552;&#31034;&#31867;&#22411;/&#39118;&#26684;&#21644;&#25552;&#31034;&#25552;&#20379;&#30340;&#19981;&#21516;&#35814;&#32454;&#31243;&#24230;&#26102;LLM&#24615;&#33021;&#21464;&#21270;&#24040;&#22823;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#20998;&#31867;&#27861;&#23558;&#20351;&#26410;&#26469;&#30340;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#33021;&#22815;&#25253;&#21578;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#29305;&#23450;&#25552;&#31034;&#31867;&#21035;&#65292;&#20174;&#32780;&#23454;&#29616;&#36328;&#19981;&#21516;&#30740;&#31350;&#30340;&#26377;&#24847;&#20041;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
While LLMs have shown great success in understanding and generating text in traditional conversational settings, their potential for performing ill-defined complex tasks is largely under-studied. Indeed, we are yet to conduct comprehensive benchmarking studies with multiple LLMs that are exclusively focused on a complex task. However, conducting such benchmarking studies is challenging because of the large variations in LLMs' performance when different prompt types/styles are used and different degrees of detail are provided in the prompts. To address this issue, the paper proposes a general taxonomy that can be used to design prompts with specific properties in order to perform a wide range of complex tasks. This taxonomy will allow future benchmarking studies to report the specific categories of prompts used as part of the study, enabling meaningful comparisons across different studies. Also, by establishing a common standard through this taxonomy, researchers will be able to draw mo
&lt;/p&gt;</description></item><item><title>DoReMi&#26041;&#27861;&#20351;&#29992;&#20998;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#35757;&#32451;&#23567;&#22411;&#20195;&#29702;&#27169;&#22411;&#20197;&#20135;&#29983;&#22495;&#26435;&#37325;&#65292;&#20877;&#20351;&#29992;&#36825;&#20123;&#26435;&#37325;&#37325;&#26032;&#37319;&#26679;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#30456;&#27604;&#20351;&#29992;&#40664;&#35748;&#26435;&#37325;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#22312;The Pile&#21644;GLaM&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;6.5%&#21644;4.7%&#30340;few-shot&#19979;&#28216;&#20934;&#30830;&#24230;&#65292;&#20998;&#21035;&#20351;&#29992;2.6&#20493;&#21644;&#30456;&#21516;&#30340;&#35757;&#32451;&#27493;&#39588;&#36798;&#21040;&#22522;&#32447;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.10429</link><description>&lt;p&gt;
DoReMi: &#20248;&#21270;&#25968;&#25454;&#28151;&#21512;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining. (arXiv:2305.10429v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10429
&lt;/p&gt;
&lt;p&gt;
DoReMi&#26041;&#27861;&#20351;&#29992;&#20998;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#35757;&#32451;&#23567;&#22411;&#20195;&#29702;&#27169;&#22411;&#20197;&#20135;&#29983;&#22495;&#26435;&#37325;&#65292;&#20877;&#20351;&#29992;&#36825;&#20123;&#26435;&#37325;&#37325;&#26032;&#37319;&#26679;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#30456;&#27604;&#20351;&#29992;&#40664;&#35748;&#26435;&#37325;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#22312;The Pile&#21644;GLaM&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;6.5%&#21644;4.7%&#30340;few-shot&#19979;&#28216;&#20934;&#30830;&#24230;&#65292;&#20998;&#21035;&#20351;&#29992;2.6&#20493;&#21644;&#30456;&#21516;&#30340;&#35757;&#32451;&#27493;&#39588;&#36798;&#21040;&#22522;&#32447;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#25968;&#25454;&#22495;&#30340;&#28151;&#21512;&#27604;&#20363;&#65288;&#20363;&#22914;&#65292;&#32500;&#22522;&#30334;&#31185;&#12289;&#22270;&#20070;&#12289;&#32593;&#39029;&#25991;&#26412;&#65289;&#26497;&#22823;&#22320;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DoReMi&#30340;Domain Reweighting with Minimax Optimization&#26041;&#27861;&#65292;&#23427;&#39318;&#20808;&#20351;&#29992;&#20998;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#65288;Group DRO&#65289;&#35757;&#32451;&#19968;&#20010;&#23567;&#20195;&#29702;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#22495;&#26435;&#37325;&#65288;&#28151;&#21512;&#27604;&#20363;&#65289;&#65292;&#32780;&#19981;&#38656;&#35201;&#30693;&#36947;&#19979;&#28216;&#20219;&#21153;&#30340;&#30693;&#35782;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#22495;&#26435;&#37325;&#37325;&#26032;&#37319;&#26679;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#26356;&#22823;&#30340;&#65292;&#20840;&#23610;&#23544;&#30340;&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;DoReMi&#22312;&#19968;&#20010;280M&#21442;&#25968;&#30340;&#20195;&#29702;&#27169;&#22411;&#19978;&#65292;&#26356;&#26377;&#25928;&#22320;&#25214;&#21040;&#35757;&#32451;&#19968;&#20010;8B&#21442;&#25968;&#27169;&#22411;&#65288;30&#20493;&#22823;&#65289;&#30340;&#22495;&#26435;&#37325;&#12290;&#22312;The Pile&#19978;&#65292;&#21363;&#20351;&#22312;&#20943;&#23567;&#19968;&#20123;&#22495;&#30340;&#27604;&#37325;&#26102;&#65292;DoReMi&#20063;&#33021;&#25552;&#39640;&#25152;&#26377;&#22495;&#30340;perplexity&#12290;&#30456;&#27604;&#20351;&#29992;The Pile&#30340;&#40664;&#35748;&#22495;&#26435;&#37325;&#35757;&#32451;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;DoReMi&#23558;&#24179;&#22343;few-shot&#19979;&#28216;&#20934;&#30830;&#24230;&#25552;&#39640;&#20102;6.5%&#65292;&#24182;&#20351;&#29992;2.6&#20493;&#30340;&#35757;&#32451;&#27493;&#39588;&#36798;&#21040;&#22522;&#32447;&#20934;&#30830;&#24230;&#12290;&#22312;GLaM&#25968;&#25454;&#38598;&#19978;&#65292;DoReMi&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;4.7%&#65288;&#27425;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65289;&#30340;few-shot&#20934;&#30830;&#24230;&#65292;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#27493;&#39588;&#19979;&#25552;&#39640;&#20102;9.0%&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full-sized model. In our experiments, we use DoReMi on a 280M-parameter proxy model to find domain weights for training an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves perplexity across all domains, even when it downweights a domain. DoReMi improves average few-shot downstream accuracy by 6.5% over a baseline model trained using The Pile's default domain weights and reaches the baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has no know
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;StrAE&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#21477;&#23376;&#32467;&#26500;&#26080;&#30417;&#30563;&#22320;&#23398;&#20064;&#22810;&#32423;&#33410;&#28857;&#23884;&#20837;&#65292;&#24182;&#19988;&#21457;&#29616;&#20351;&#29992;&#26174;&#24335;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#23884;&#20837;&#34920;&#29616;&#65292;&#26032;&#30340;&#23545;&#27604;&#30446;&#26631;&#20248;&#20110;&#26631;&#20934;&#30340;&#20132;&#21449;&#29109;&#30446;&#26631;&#12290;&#21516;&#26102;&#65292;&#23436;&#20840;&#24544;&#23454;&#20110;&#32467;&#26500;&#30830;&#23454;&#33021;&#22815;&#26681;&#25454;&#30456;&#24212;&#27169;&#22411;&#30340;&#24615;&#33021;&#28040;&#38500;&#32467;&#26500;&#31867;&#22411;&#20043;&#38388;&#30340;&#27495;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.05588</link><description>&lt;p&gt;
StrAE&#65306;&#20351;&#29992;&#26174;&#24335;&#32467;&#26500;&#30340;&#33258;&#32534;&#30721;&#39044;&#35757;&#32451;&#23884;&#20837;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
StrAE: Autoencoding for Pre-Trained Embeddings using Explicit Structure. (arXiv:2305.05588v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;StrAE&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#21477;&#23376;&#32467;&#26500;&#26080;&#30417;&#30563;&#22320;&#23398;&#20064;&#22810;&#32423;&#33410;&#28857;&#23884;&#20837;&#65292;&#24182;&#19988;&#21457;&#29616;&#20351;&#29992;&#26174;&#24335;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#23884;&#20837;&#34920;&#29616;&#65292;&#26032;&#30340;&#23545;&#27604;&#30446;&#26631;&#20248;&#20110;&#26631;&#20934;&#30340;&#20132;&#21449;&#29109;&#30446;&#26631;&#12290;&#21516;&#26102;&#65292;&#23436;&#20840;&#24544;&#23454;&#20110;&#32467;&#26500;&#30830;&#23454;&#33021;&#22815;&#26681;&#25454;&#30456;&#24212;&#27169;&#22411;&#30340;&#24615;&#33021;&#28040;&#38500;&#32467;&#26500;&#31867;&#22411;&#20043;&#38388;&#30340;&#27495;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24320;&#21457;StrAE&#36825;&#19968;&#33258;&#32534;&#30721;&#26694;&#26550;&#65292;&#25506;&#31350;&#20102;&#22312;NLP&#20013;&#20351;&#29992;&#26174;&#24335;&#32467;&#26500;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#23454;&#29992;&#24615;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#21477;&#23376;&#32467;&#26500;&#26080;&#30417;&#30563;&#22320;&#23398;&#20064;&#22810;&#32423;&#33410;&#28857;&#23884;&#20837;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#22312;&#20869;&#30340;&#19981;&#21516;&#31867;&#22411;&#21477;&#23376;&#32467;&#26500;&#21644;&#30446;&#26631;&#19979;&#20351;&#29992;StrAE&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#20869;&#22312;&#21644;&#22806;&#22312;&#20219;&#21153;&#19978;&#35780;&#20272;&#25152;&#23398;&#23884;&#20837;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;StrAE&#21033;&#29992;&#26174;&#24335;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#23884;&#20837;&#65292;&#26032;&#30340;&#23545;&#27604;&#30446;&#26631;&#20248;&#20110;&#26631;&#20934;&#30340;&#20132;&#21449;&#29109;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#19982;&#20197;&#24448;&#30340;&#20570;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;&#23436;&#20840;&#24544;&#23454;&#20110;&#32467;&#26500;&#30830;&#23454;&#33021;&#22815;&#26681;&#25454;&#30456;&#24212;&#27169;&#22411;&#30340;&#24615;&#33021;&#28040;&#38500;&#32467;&#26500;&#31867;&#22411;&#20043;&#38388;&#30340;&#27495;&#20041;&#12290;&#20316;&#20026;StrAE&#23454;&#29992;&#24615;&#30340;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;p
&lt;/p&gt;
&lt;p&gt;
This work explores the utility of explicit structure for representation learning in NLP by developing StrAE -- an autoencoding framework that faithfully leverages sentence structure to learn multi-level node embeddings in an unsupervised fashion. We use StrAE to train models across different types of sentential structure and objectives, including a novel contrastive loss over structure, and evaluate the learnt embeddings on a series of both intrinsic and extrinsic tasks. Our experiments indicate that leveraging explicit structure through StrAE leads to improved embeddings over prior work, and that our novel contrastive objective over structure outperforms the standard cross-entropy objective. Moreover, in contrast to findings from prior work that weakly leverages structure, we find that being completely faithful to structure does enable disambiguation between types of structure based on the corresponding model's performance. As further evidence of StrAE's utility, we develop a simple p
&lt;/p&gt;</description></item><item><title>TAPS&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;IBP&#21644;PGD&#35757;&#32451;&#30340;&#35748;&#35777;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20135;&#29983;&#31934;&#30830;&#20294;&#19981;&#19968;&#23450;&#27491;&#30830;&#30340;&#26368;&#22351;&#24773;&#20917;&#25439;&#22833;&#36924;&#36817;&#65292;&#20174;&#32780;&#20943;&#23569;&#36807;&#24230;&#27491;&#21017;&#21270;&#65292;&#24182;&#22312;&#35768;&#22810;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#35748;&#35777;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.04574</link><description>&lt;p&gt;
TAPS: &#36830;&#25509;&#35748;&#35777;&#21644;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
TAPS: Connecting Certified and Adversarial Training. (arXiv:2305.04574v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04574
&lt;/p&gt;
&lt;p&gt;
TAPS&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;IBP&#21644;PGD&#35757;&#32451;&#30340;&#35748;&#35777;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20135;&#29983;&#31934;&#30830;&#20294;&#19981;&#19968;&#23450;&#27491;&#30830;&#30340;&#26368;&#22351;&#24773;&#20917;&#25439;&#22833;&#36924;&#36817;&#65292;&#20174;&#32780;&#20943;&#23569;&#36807;&#24230;&#27491;&#21017;&#21270;&#65292;&#24182;&#22312;&#35768;&#22810;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#35748;&#35777;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#21487;&#35748;&#35777;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#20173;&#28982;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#38590;&#39064;&#12290;&#19968;&#26041;&#38754;&#65292;&#23545;&#25239;&#35757;&#32451;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#25439;&#22833;&#30340;&#27424;&#32570;&#36924;&#36817;&#23548;&#33268;&#35748;&#35777;&#30340;&#27491;&#21017;&#21270;&#19981;&#36275;&#65292;&#21478;&#19968;&#26041;&#38754;&#65292;&#22768;&#38899;&#35748;&#35777;&#35757;&#32451;&#26041;&#27861;&#20248;&#21270;&#23485;&#26494;&#30340;&#36924;&#36817;&#65292;&#23548;&#33268;&#36807;&#24230;&#27491;&#21017;&#21270;&#21644;&#31934;&#24230;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TAPS&#65292;&#19968;&#31181;&#32467;&#21512;&#20102;IBP&#21644;PGD&#35757;&#32451;&#30340;&#65288;&#19981;&#19968;&#23450;&#27491;&#30830;&#30340;&#65289;&#35748;&#35777;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#20135;&#29983;&#31934;&#30830;&#20294;&#19981;&#19968;&#23450;&#27491;&#30830;&#30340;&#26368;&#22351;&#24773;&#20917;&#25439;&#22833;&#36924;&#36817;&#65292;&#20174;&#32780;&#20943;&#23569;&#36807;&#24230;&#27491;&#21017;&#21270;&#24182;&#25552;&#39640;&#35748;&#35777;&#21644;&#26631;&#20934;&#31934;&#24230;&#12290;&#26681;&#25454;&#23454;&#39564;&#35777;&#25454;&#65292;TAPS&#22312;&#35768;&#22810;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#20363;&#22914;&#65292;&#22312;$\ell_\infty$&#25200;&#21160;&#21322;&#24452;$\epsilon=1/255$&#30340;TinyImageNet&#19978;&#23454;&#29616;&#20102;$22\%$&#30340;&#35748;&#35777;&#31934;&#24230;&#12290;&#25105;&#20204;&#22312;https://github.com/eth-sri/taps&#19978;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#23454;&#29616;&#21644;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training certifiably robust neural networks remains a notoriously hard problem. On one side, adversarial training optimizes under-approximations of the worst-case loss, which leads to insufficient regularization for certification, while on the other, sound certified training methods optimize loose over-approximations, leading to over-regularization and poor (standard) accuracy. In this work we propose TAPS, an (unsound) certified training method that combines IBP and PGD training to yield precise, although not necessarily sound, worst-case loss approximations, reducing over-regularization and increasing certified and standard accuracies. Empirically, TAPS achieves a new state-of-the-art in many settings, e.g., reaching a certified accuracy of $22\%$ on TinyImageNet for $\ell_\infty$-perturbations with radius $\epsilon=1/255$. We make our implementation and networks public at https://github.com/eth-sri/taps.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#21453;&#39304;&#30340;&#23454;&#29616;&#24335;&#39044;&#27979;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#27169;&#22411;&#37096;&#32626;&#33258;&#36523;&#25913;&#21464;&#25968;&#25454;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01094</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#23398;&#20064;&#23454;&#29616;&#22312;&#32447;&#21453;&#39304;&#30340;&#23454;&#29616;&#24335;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Performative Prediction with Bandit Feedback: Learning through Reparameterization. (arXiv:2305.01094v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#21453;&#39304;&#30340;&#23454;&#29616;&#24335;&#39044;&#27979;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#27169;&#22411;&#37096;&#32626;&#33258;&#36523;&#25913;&#21464;&#25968;&#25454;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#25968;&#25454;&#20998;&#24067;&#30001;&#27169;&#22411;&#37096;&#32626;&#33258;&#36523;&#25913;&#21464;&#30340;&#24773;&#24418;&#19979;&#39044;&#27979;&#30340;&#19968;&#20010;&#26694;&#26550;&#8212;&#8212;&#23454;&#29616;&#24335;&#39044;&#27979;&#12290;&#29616;&#26377;&#30740;&#31350;&#30340;&#37325;&#28857;&#22312;&#20110;&#20248;&#21270;&#20934;&#30830;&#24615;&#65292;&#20294;&#26159;&#20854;&#20551;&#35774;&#24448;&#24448;&#38590;&#20197;&#22312;&#23454;&#36341;&#20013;&#24471;&#21040;&#28385;&#36275;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#23618;&#38646;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#23454;&#29616;&#24335;&#39044;&#27979;&#30446;&#26631;&#65292;&#20174;&#32780;&#23558;&#38750;&#20984;&#30340;&#30446;&#26631;&#36716;&#21270;&#20026;&#20984;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performative prediction, as introduced by Perdomo et al. (2020), is a framework for studying social prediction in which the data distribution itself changes in response to the deployment of a model. Existing work on optimizing accuracy in this setting hinges on two assumptions that are easily violated in practice: that the performative risk is convex over the deployed model, and that the mapping from the model to the data distribution is known to the model designer in advance. In this paper, we initiate the study of tractable performative prediction problems that do not require these assumptions. To tackle this more challenging setting, we develop a two-level zeroth-order optimization algorithm, where one level aims to compute the distribution map, and the other level reparameterizes the performative prediction objective as a function of the induced data distribution. Under mild conditions, this reparameterization allows us to transform the non-convex objective into a convex one and ac
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#21452;&#26102;&#38388;&#23610;&#24230;&#21046;&#24230;&#19979;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#26080;&#38656;&#31070;&#32463;&#20803;&#25968;&#37327;&#36235;&#20110;&#26080;&#38480;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2304.09576</link><description>&lt;p&gt;
&#21033;&#29992;&#21452;&#26102;&#38388;&#23610;&#24230;&#21046;&#24230;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Leveraging the two timescale regime to demonstrate convergence of neural networks. (arXiv:2304.09576v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09576
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#21452;&#26102;&#38388;&#23610;&#24230;&#21046;&#24230;&#19979;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#26080;&#38656;&#31070;&#32463;&#20803;&#25968;&#37327;&#36235;&#20110;&#26080;&#38480;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#22312;&#20869;&#23618;&#27493;&#38271;&#36828;&#23567;&#20110;&#22806;&#23618;&#27493;&#38271;&#30340;&#21452;&#26102;&#38388;&#23610;&#24230;&#21046;&#24230;&#19979;&#12290;&#22312;&#36825;&#20010;&#21046;&#24230;&#19979;&#65292;&#22312;&#31616;&#21333;&#30340;&#21333;&#21464;&#37327;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#25910;&#25947;&#20110;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#38656;&#35201;&#31070;&#32463;&#20803;&#25968;&#37327;&#36235;&#20110;&#26080;&#38480;&#65292;&#36825;&#20351;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#21516;&#20110;&#26368;&#36817;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#22914;&#31070;&#32463;&#20999;&#21521;&#26680;&#25110;&#24179;&#22343;&#22330;&#21046;&#24230;&#12290;&#25105;&#20204;&#25552;&#20379;&#23454;&#39564;&#35828;&#26126;&#65292;&#26174;&#31034;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#25353;&#29031;&#25105;&#20204;&#23545;&#26799;&#24230;&#27969;&#30340;&#25551;&#36848;&#36827;&#34892;&#34892;&#20026;&#65292;&#24182;&#22240;&#27492;&#22312;&#21452;&#26102;&#38388;&#23610;&#24230;&#21046;&#24230;&#19979;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#20294;&#22312;&#27492;&#21046;&#24230;&#20043;&#22806;&#21487;&#33021;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the training dynamics of shallow neural networks, in a two-timescale regime in which the stepsizes for the inner layer are much smaller than those for the outer layer. In this regime, we prove convergence of the gradient flow to a global optimum of the non-convex optimization problem in a simple univariate setting. The number of neurons need not be asymptotically large for our result to hold, distinguishing our result from popular recent approaches such as the neural tangent kernel or mean-field regimes. Experimental illustration is provided, showing that the stochastic gradient descent behaves according to our description of the gradient flow and thus converges to a global optimum in the two-timescale regime, but can fail outside of this regime.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;Sheaf&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21521;&#37327;&#31354;&#38388;&#34920;&#31034;&#26041;&#27861;&#65292;&#20351;&#24471;&#20854;&#22312;&#22522;&#20934;&#25512;&#33616;&#20219;&#21153;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.09097</link><description>&lt;p&gt;
&#22522;&#20110;Sheaf&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Sheaf Neural Networks for Graph-based Recommender Systems. (arXiv:2304.09097v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09097
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Sheaf&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21521;&#37327;&#31354;&#38388;&#34920;&#31034;&#26041;&#27861;&#65292;&#20351;&#24471;&#20854;&#22312;&#22522;&#20934;&#25512;&#33616;&#20219;&#21153;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Graph&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#21253;&#25324;&#25512;&#33616;&#31995;&#32479;&#12290;Graph&#31070;&#32463;&#32593;&#32476;&#23545;&#20854;&#20182;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#22312;&#20110;&#65292;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#35768;&#22810;&#38382;&#39064;&#21487;&#20197;&#33258;&#28982;&#22320;&#24314;&#27169;&#20026;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#21487;&#20197;&#26159;&#29992;&#25143;&#25110;&#39033;&#30446;&#65292;&#36793;&#20195;&#34920;&#20559;&#22909;&#20851;&#31995;&#12290; &#22312;&#24403;&#21069;&#30340;Graph&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#20013;&#65292;&#33410;&#28857;&#29992;&#22312;&#35757;&#32451;&#26102;&#23398;&#20064;&#21040;&#30340;&#38745;&#24577;&#21521;&#37327;&#34920;&#31034;&#12290;&#36825;&#31181;&#38745;&#24577;&#21521;&#37327;&#21487;&#33021;&#21482;&#36866;&#29992;&#20110;&#25429;&#25417;&#23450;&#20041;&#23427;&#20204;&#30340;&#19968;&#20123;&#29992;&#25143;&#25110;&#39033;&#30446;&#30340;&#24494;&#22937;&#24046;&#21035;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;&#21551;&#21457;&#33539;&#30068;&#35770;&#30340;&#27169;&#22411;&#65306;Sheaf&#31070;&#32463;&#32593;&#32476;&#12290;Sheaf&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#36830;&#25509;&#30340;&#25289;&#26222;&#25289;&#26031;&#21487;&#20197;&#36890;&#36807;&#23558;&#27599;&#20010;&#33410;&#28857;&#65288;&#20197;&#21450;&#36793;&#65289;&#19982;&#21521;&#37327;&#31354;&#38388;&#32780;&#19981;&#26159;&#21333;&#20010;&#21521;&#37327;&#30456;&#20851;&#32852;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#21521;&#37327;&#31354;&#38388;&#34920;&#31034;&#26356;&#20016;&#23500;&#65292;&#24182;&#20801;&#35768;&#22312;&#25512;&#29702;&#26102;&#36873;&#25321;&#27491;&#30830;&#30340;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#26356;&#20855;&#34920;&#29616;&#21147;&#21644;&#28789;&#27963;&#24615;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25512;&#33616;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in Graph Neural Networks has resulted in wide adoption by many applications, including recommendation systems. The reason for Graph Neural Networks' superiority over other approaches is that many problems in recommendation systems can be naturally modeled as graphs, where nodes can be either users or items and edges represent preference relationships. In current Graph Neural Network approaches, nodes are represented with a static vector learned at training time. This static vector might only be suitable to capture some of the nuances of users or items they define. To overcome this limitation, we propose using a recently proposed model inspired by category theory: Sheaf Neural Networks. Sheaf Neural Networks, and its connected Laplacian, can address the previous problem by associating every node (and edge) with a vector space instead than a single vector. The vector space representation is richer and allows picking the proper representation at inference time. This approa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25581;&#31034;&#20197;&#21450;&#25552;&#20986;&#20102;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24040;&#22823;&#27700;&#36275;&#36857;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#20854;&#28129;&#27700;&#28040;&#32791;&#24050;&#32463;&#24341;&#36215;&#22269;&#38469;&#31038;&#20250;&#30340;&#37325;&#35270;&#65292;&#24182;&#19988;AI&#27169;&#22411;&#24212;&#35813;&#25215;&#25285;&#31038;&#20250;&#36131;&#20219;&#65292;&#20570;&#20986;&#38754;&#23545;&#27700;&#21361;&#26426;&#30340;&#34920;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.03271</link><description>&lt;p&gt;
&#20351;AI&#8220;&#21475;&#28212;&#8221;&#20943;&#23569;&#30340;&#26041;&#27861;&#65306;&#25581;&#31034;&#21644;&#35299;&#20915;AI&#27169;&#22411;&#30340;&#31192;&#23494;&#27700;&#28040;&#32791;
&lt;/p&gt;
&lt;p&gt;
Making AI Less "Thirsty": Uncovering and Addressing the Secret Water Footprint of AI Models. (arXiv:2304.03271v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25581;&#31034;&#20197;&#21450;&#25552;&#20986;&#20102;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24040;&#22823;&#27700;&#36275;&#36857;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#20854;&#28129;&#27700;&#28040;&#32791;&#24050;&#32463;&#24341;&#36215;&#22269;&#38469;&#31038;&#20250;&#30340;&#37325;&#35270;&#65292;&#24182;&#19988;AI&#27169;&#22411;&#24212;&#35813;&#25215;&#25285;&#31038;&#20250;&#36131;&#20219;&#65292;&#20570;&#20986;&#38754;&#23545;&#27700;&#21361;&#26426;&#30340;&#34920;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#30340;&#30899;&#36275;&#36857;&#19981;&#26029;&#22686;&#38271;&#65292;&#29305;&#21035;&#26159;&#20687;GPT-3&#21644;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#27169;&#22411;&#65292;&#24050;&#32463;&#21463;&#21040;&#20844;&#20247;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#21516;&#31561;&#37325;&#35201;&#19988;&#24040;&#22823;&#30340;AI&#27169;&#22411;&#27700;&#21360;&#23578;&#26410;&#24341;&#36215;&#20154;&#20204;&#30340;&#27880;&#24847;&#12290;&#20363;&#22914;&#65292;&#22312;&#24494;&#36719;&#26368;&#20808;&#36827;&#30340;&#32654;&#22269;&#25968;&#25454;&#20013;&#24515;&#20013;&#35757;&#32451;GPT-3&#21487;&#20197;&#30452;&#25509;&#28040;&#32791;70&#19975;&#21319;&#28165;&#27905;&#28129;&#27700;&#65288;&#30456;&#24403;&#20110;&#29983;&#20135;370&#36742;&#23453;&#39532;&#27773;&#36710;&#25110;320&#36742;&#29305;&#26031;&#25289;&#30005;&#21160;&#27773;&#36710;&#65289;&#65292;&#22914;&#26524;&#22312;&#24494;&#36719;&#30340;&#20122;&#27954;&#25968;&#25454;&#20013;&#24515;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20010;&#27700;&#28040;&#32791;&#37327;&#23558;&#22686;&#21152;&#19977;&#20493;&#65292;&#20294;&#36825;&#26679;&#30340;&#20449;&#24687;&#19968;&#30452;&#34987;&#20445;&#23494;&#12290;&#36825;&#26497;&#20854;&#20196;&#20154;&#25285;&#24551;&#65292;&#22240;&#20026;&#28129;&#27700;&#30701;&#32570;&#24050;&#25104;&#20026;&#22312;&#20154;&#21475;&#36805;&#36895;&#22686;&#38271;&#12289;&#27700;&#36164;&#28304;&#20943;&#23569;&#21644;&#32769;&#21270;&#30340;&#27700;&#22522;&#30784;&#35774;&#26045;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25152;&#26377;&#20154;&#38754;&#20020;&#30340;&#26368;&#32039;&#36843;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290;&#20026;&#20102;&#24212;&#23545;&#20840;&#29699;&#27700;&#36164;&#28304;&#30340;&#25361;&#25112;&#65292;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21487;&#20197;&#65292;&#32780;&#19988;&#24212;&#35813;&#65292;&#25215;&#25285;&#31038;&#20250;&#36131;&#20219;&#65292;&#20197;&#36523;&#20316;&#21017;&#35299;&#20915;&#33258;&#24049;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing carbon footprint of artificial intelligence (AI) models, especially large ones such as GPT-3 and GPT-4, has been undergoing public scrutiny. Unfortunately, however, the equally important and enormous water footprint of AI models has remained under the radar. For example, training GPT-3 in Microsoft's state-of-the-art U.S. data centers can directly consume 700,000 liters of clean freshwater (enough for producing 370 BMW cars or 320 Tesla electric vehicles) and the water consumption would have been tripled if training were done in Microsoft's Asian data centers, but such information has been kept as a secret. This is extremely concerning, as freshwater scarcity has become one of the most pressing challenges shared by all of us in the wake of the rapidly growing population, depleting water resources, and aging water infrastructures. To respond to the global water challenges, AI models can, and also should, take social responsibility and lead by example by addressing their own 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#35282;&#32447;&#24615;&#32593;&#32476;&#20013;&#30340;&#38797;&#28857;&#21040;&#38797;&#28857;&#21160;&#21147;&#23398;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21021;&#22987;&#21270;&#36235;&#36817;&#20110;&#38646;&#30340;&#24773;&#20917;&#19979;&#65292;&#26497;&#38480;&#27969;&#22914;&#20309;&#20174;&#19968;&#20010;&#35757;&#32451;&#25439;&#22833;&#30340;&#38797;&#28857;&#36339;&#21040;&#21478;&#19968;&#20010;&#38797;&#28857;&#65292;&#30452;&#21040;&#36798;&#21040;&#26368;&#23567;&#30340;$\ell_1$-&#33539;&#25968;&#35299;&#12290;&#36890;&#36807;&#36882;&#24402;&#31639;&#27861;&#21644;&#24359;&#38271;&#26102;&#38388;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#25105;&#20204;&#26126;&#30830;&#21051;&#30011;&#20102;&#35775;&#38382;&#36807;&#30340;&#38797;&#28857;&#21644;&#36339;&#36291;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36866;&#29992;&#20110;&#27424;&#21442;&#25968;&#21270;&#21644;&#36807;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#65292;&#20063;&#28085;&#30422;&#20102;&#27963;&#21160;&#25968;&#37327;&#21333;&#35843;&#24615;&#38382;&#39064;&#30340;&#22797;&#26434;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2304.00488</link><description>&lt;p&gt;
&#23545;&#35282;&#32447;&#24615;&#32593;&#32476;&#20013;&#30340;&#38797;&#28857;&#21040;&#38797;&#28857;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Saddle-to-Saddle Dynamics in Diagonal Linear Networks. (arXiv:2304.00488v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#35282;&#32447;&#24615;&#32593;&#32476;&#20013;&#30340;&#38797;&#28857;&#21040;&#38797;&#28857;&#21160;&#21147;&#23398;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21021;&#22987;&#21270;&#36235;&#36817;&#20110;&#38646;&#30340;&#24773;&#20917;&#19979;&#65292;&#26497;&#38480;&#27969;&#22914;&#20309;&#20174;&#19968;&#20010;&#35757;&#32451;&#25439;&#22833;&#30340;&#38797;&#28857;&#36339;&#21040;&#21478;&#19968;&#20010;&#38797;&#28857;&#65292;&#30452;&#21040;&#36798;&#21040;&#26368;&#23567;&#30340;$\ell_1$-&#33539;&#25968;&#35299;&#12290;&#36890;&#36807;&#36882;&#24402;&#31639;&#27861;&#21644;&#24359;&#38271;&#26102;&#38388;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#25105;&#20204;&#26126;&#30830;&#21051;&#30011;&#20102;&#35775;&#38382;&#36807;&#30340;&#38797;&#28857;&#21644;&#36339;&#36291;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36866;&#29992;&#20110;&#27424;&#21442;&#25968;&#21270;&#21644;&#36807;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#65292;&#20063;&#28085;&#30422;&#20102;&#27963;&#21160;&#25968;&#37327;&#21333;&#35843;&#24615;&#38382;&#39064;&#30340;&#22797;&#26434;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23436;&#20840;&#25551;&#36848;&#20102;&#22312;&#21021;&#22987;&#21270;&#36235;&#36817;&#20110;&#38646;&#30340;&#24773;&#20917;&#19979;&#65292;&#26799;&#24230;&#27969;&#22312;&#23545;&#35282;&#32447;&#24615;&#32593;&#32476;&#19978;&#30340;&#36712;&#36857;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26497;&#38480;&#27969;&#20174;&#19968;&#20010;&#35757;&#32451;&#25439;&#22833;&#30340;&#38797;&#28857;&#36339;&#21040;&#21478;&#19968;&#20010;&#38797;&#28857;&#65292;&#30452;&#21040;&#21040;&#36798;&#26368;&#23567;&#30340;$\ell_1$-&#33539;&#25968;&#35299;&#12290;&#36825;&#31181;&#38797;&#28857;&#21040;&#38797;&#28857;&#30340;&#21160;&#21147;&#23398;&#36716;&#21270;&#20026;&#19968;&#20010;&#22686;&#37327;&#24335;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#22240;&#20026;&#27599;&#20010;&#38797;&#28857;&#23545;&#24212;&#20110;&#22312;&#27963;&#21160;&#38598;&#20043;&#22806;&#22352;&#26631;&#24517;&#39035;&#20026;&#38646;&#30340;&#25439;&#22833;&#26368;&#23567;&#21270;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#36882;&#24402;&#31639;&#27861;&#26126;&#30830;&#21051;&#30011;&#20102;&#35775;&#38382;&#36807;&#30340;&#38797;&#28857;&#20197;&#21450;&#36339;&#36291;&#26102;&#38388;&#65292;&#36825;&#20010;&#31639;&#27861;&#31867;&#20284;&#20110;&#29992;&#20110;&#35745;&#31639;Lasso&#36335;&#24452;&#30340;LARS&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#21033;&#29992;&#20102;&#26041;&#20415;&#30340;&#24359;&#38271;&#26102;&#38388;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#36319;&#36394;&#36339;&#36291;&#20043;&#38388;&#30340;&#24322;&#23460;&#25151;&#36716;&#25442;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#23545;&#25968;&#25454;&#30340;&#35201;&#27714;&#24456;&#20302;&#65292;&#36866;&#29992;&#20110;&#27424;&#21442;&#25968;&#21270;&#21644;&#36807;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#65292;&#24182;&#28085;&#30422;&#20102;&#22797;&#26434;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#27963;&#21160;&#25968;&#37327;&#30340;&#21333;&#35843;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we fully describe the trajectory of gradient flow over diagonal linear networks in the limit of vanishing initialisation. We show that the limiting flow successively jumps from a saddle of the training loss to another until reaching the minimum $\ell_1$-norm solution. This saddle-to-saddle dynamics translates to an incremental learning process as each saddle corresponds to the minimiser of the loss constrained to an active set outside of which the coordinates must be zero. We explicitly characterise the visited saddles as well as the jumping times through a recursive algorithm reminiscent of the LARS algorithm used for computing the Lasso path. Our proof leverages a convenient arc-length time-reparametrisation which enables to keep track of the heteroclinic transitions between the jumps. Our analysis requires negligible assumptions on the data, applies to both under and overparametrised settings and covers complex cases where there is no monotonicity of the number of acti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fides&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#39564;&#35777;&#22806;&#21327;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#36138;&#24515;&#33976;&#39311;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#21160;&#24577;&#33976;&#39311;&#24182;&#20248;&#21270;&#39564;&#35777;&#27169;&#22411;&#65292;&#20197;&#39564;&#35777;&#23545;&#24212;&#30340;&#26381;&#21153;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#35813;&#26694;&#26550;&#36824;&#33021;&#22312;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#20197;&#25552;&#20379;&#26356;&#39640;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.00083</link><description>&lt;p&gt;
Fides&#65306;&#19968;&#31181;&#21033;&#29992;&#23433;&#20840;&#25191;&#34892;&#29615;&#22659;&#23545;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#36827;&#34892;&#32467;&#26524;&#39564;&#35777;&#30340;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Fides: A Generative Framework for Result Validation of Outsourced Machine Learning Workloads via TEE. (arXiv:2304.00083v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fides&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#39564;&#35777;&#22806;&#21327;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#36138;&#24515;&#33976;&#39311;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#21160;&#24577;&#33976;&#39311;&#24182;&#20248;&#21270;&#39564;&#35777;&#27169;&#22411;&#65292;&#20197;&#39564;&#35777;&#23545;&#24212;&#30340;&#26381;&#21153;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#35813;&#26694;&#26550;&#36824;&#33021;&#22312;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#20197;&#25552;&#20379;&#26356;&#39640;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#25935;&#24863;&#39046;&#22495;&#30340;&#37096;&#32626;&#23548;&#33268;&#20102;&#23545;&#20854;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#30340;&#37325;&#35270;&#65292;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#22810;&#26041;&#35745;&#31639;&#21644;&#22522;&#20110;&#35777;&#26126;&#30340;&#31995;&#32479;&#65292;&#32473;&#23454;&#26102;&#24212;&#29992;&#24102;&#26469;&#20102;&#24456;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Fides&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#39564;&#35777;&#22806;&#21327;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#65292;&#20854;&#20013;&#37319;&#29992;&#26032;&#39062;&#19988;&#39640;&#25928;&#30340;&#36138;&#24515;&#33976;&#39311;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#23454;&#29616;&#19968;&#31181;&#23454;&#26102;&#39564;&#35777;&#27169;&#22411;&#26469;&#36739;&#23569;&#22320;&#28040;&#32791;&#31354;&#38388;&#21644;&#35745;&#31639;&#33021;&#21147;&#65292;&#21516;&#26102;&#36816;&#34892;&#22312;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing popularity of Machine Learning (ML) has led to its deployment in various sensitive domains, which has resulted in significant research focused on ML security and privacy. However, in some applications, such as autonomous driving, integrity verification of the outsourced ML workload is more critical-a facet that has not received much attention. Existing solutions, such as multi-party computation and proof-based systems, impose significant computation overhead, which makes them unfit for real-time applications. We propose Fides, a novel framework for real-time validation of outsourced ML workloads. Fides features a novel and efficient distillation technique-Greedy Distillation Transfer Learning-that dynamically distills and fine-tunes a space and compute-efficient verification model for verifying the corresponding service model while running inside a trusted execution environment. Fides features a client-side attack detection model that uses statistical analysis and divergenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22270;&#34920;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#28145;&#24230;&#38750;&#21442;&#25968;&#20272;&#35745;&#20869;&#37096;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24191;&#20041;&#35823;&#24046;&#20445;&#35777;&#21644;&#21435;&#22122;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.09863</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#34920;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#20869;&#37096;&#25968;&#25454;&#32467;&#26500;&#30340;&#28145;&#24230;&#38750;&#21442;&#25968;&#20272;&#35745;&#65306;&#24191;&#20041;&#35823;&#24046;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Nonparametric Estimation of Intrinsic Data Structures by Chart Autoencoders: Generalization Error and Robustness. (arXiv:2303.09863v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22270;&#34920;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#28145;&#24230;&#38750;&#21442;&#25968;&#20272;&#35745;&#20869;&#37096;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24191;&#20041;&#35823;&#24046;&#20445;&#35777;&#21644;&#21435;&#22122;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#32534;&#30721;&#22120;&#22312;&#23398;&#20064;&#39640;&#32500;&#25968;&#25454;&#30340;&#20302;&#32500;&#28508;&#22312;&#29305;&#24449;&#26041;&#38754;&#24050;&#32463;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#20551;&#35774;&#25968;&#25454;&#22312;&#20302;&#32500;&#27969;&#24418;&#38468;&#36817;&#37319;&#26679;&#65292;&#25105;&#20204;&#37319;&#29992;&#22270;&#34920;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#25968;&#25454;&#32534;&#30721;&#20026;&#19968;&#32452;&#22270;&#34920;&#19978;&#30340;&#20302;&#32500;&#28508;&#22312;&#29305;&#24449;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#25968;&#25454;&#27969;&#24418;&#30340;&#25299;&#25169;&#21644;&#20960;&#20309;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#20026;&#22270;&#34920;&#33258;&#32534;&#30721;&#22120;&#30340;&#24191;&#20041;&#35823;&#24046;&#24314;&#31435;&#20102;&#32479;&#35745;&#20445;&#35777;&#65292;&#24182;&#19988;&#36890;&#36807;&#32771;&#34385;$d$&#32500;&#27969;&#24418;&#19978;$n$&#20010;&#24102;&#22122;&#22768;&#35757;&#32451;&#26679;&#26412;&#21450;&#20854;&#26080;&#22122;&#22768;&#23545;&#24212;&#29289;&#26469;&#23637;&#31034;&#23427;&#20204;&#30340;&#21435;&#22122;&#33021;&#21147;&#12290;&#36890;&#36807;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22270;&#34920;&#33258;&#32534;&#30721;&#22120;&#33021;&#22815;&#26377;&#25928;&#22320;&#21435;&#22122;&#36755;&#20837;&#25968;&#25454;&#21644;&#27491;&#24577;&#20998;&#24067;&#22122;&#22768;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36866;&#24403;&#30340;&#32593;&#32476;&#26550;&#26500;&#19979;&#65292;&#22270;&#34920;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#19968;&#20010;&#22823;&#33268;&#20026;$\displaystyle n^{-\frac{2}{d+2}}\log^4 n$&#38454;&#30340;&#24179;&#26041;&#24191;&#20041;&#35823;&#24046;&#65292;&#35813;&#35823;&#24046;&#21462;&#20915;&#20110;&#27969;&#24418;&#30340;&#20869;&#22312;&#32500;&#24230;&#65292;&#24182;&#19988;&#20165;&#24369;&#20381;&#36182;&#20110;&#26679;&#26412;&#25968;&#37327;$n$&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoencoders have demonstrated remarkable success in learning low-dimensional latent features of high-dimensional data across various applications. Assuming that data are sampled near a low-dimensional manifold, we employ chart autoencoders, which encode data into low-dimensional latent features on a collection of charts, preserving the topology and geometry of the data manifold. Our paper establishes statistical guarantees on the generalization error of chart autoencoders, and we demonstrate their denoising capabilities by considering $n$ noisy training samples, along with their noise-free counterparts, on a $d$-dimensional manifold. By training autoencoders, we show that chart autoencoders can effectively denoise the input data with normal noise. We prove that, under proper network architectures, chart autoencoders achieve a squared generalization error in the order of $\displaystyle n^{-\frac{2}{d+2}}\log^4 n$, which depends on the intrinsic dimension of the manifold and only weakly
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#38750;&#20984;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#37327;&#30340;&#26799;&#24230;&#21452;&#23618;&#26041;&#27861;(MGBiO)&#26469;&#35299;&#20915;&#36825;&#20123;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31867;&#22522;&#20110;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#21452;&#23618;&#26041;&#27861;(MSGBiO&#21644;VR-MSGBiO)&#26469;&#35299;&#20915;&#36825;&#20123;&#38543;&#26426;&#38382;&#39064;&#12290;&#36890;&#36807;&#25910;&#25947;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;MGBiO&#26041;&#27861;&#20855;&#26377;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.03944</link><description>&lt;p&gt;
&#22522;&#20110;&#21160;&#37327;&#30340;&#26799;&#24230;&#26041;&#27861;&#22312;&#38750;&#20984;&#19979;&#23618;&#21452;&#23618;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On Momentum-Based Gradient Methods for Bilevel Optimization with Nonconvex Lower-Level. (arXiv:2303.03944v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#38750;&#20984;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#37327;&#30340;&#26799;&#24230;&#21452;&#23618;&#26041;&#27861;(MGBiO)&#26469;&#35299;&#20915;&#36825;&#20123;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31867;&#22522;&#20110;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#21452;&#23618;&#26041;&#27861;(MSGBiO&#21644;VR-MSGBiO)&#26469;&#35299;&#20915;&#36825;&#20123;&#38543;&#26426;&#38382;&#39064;&#12290;&#36890;&#36807;&#25910;&#25947;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;MGBiO&#26041;&#27861;&#20855;&#26377;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#20004;&#32423;&#23618;&#27425;&#20248;&#21270;&#26041;&#27861;&#65292;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#22914;&#36229;&#21442;&#25968;&#23398;&#20064;&#12289;&#20803;&#23398;&#20064;&#21644;&#25345;&#32493;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#22312;&#19979;&#23618;&#38382;&#39064;&#20026;&#38750;&#20984;&#26102;&#65292;&#21452;&#23618;&#26041;&#27861;&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#20805;&#20998;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#38750;&#20984;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#19978;&#23618;&#38382;&#39064;&#21644;&#19979;&#23618;&#38382;&#39064;&#22343;&#20026;&#38750;&#20984;&#38382;&#39064;&#65292;&#24182;&#19988;&#19979;&#23618;&#38382;&#39064;&#28385;&#36275;Polyak-Lojasiewicz (PL)&#26465;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#21160;&#37327;&#30340;&#26799;&#24230;&#21452;&#23618;&#26041;&#27861;(MGBiO)&#26469;&#35299;&#20915;&#36825;&#20123;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#39640;&#25928;&#30340;&#22522;&#20110;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#21452;&#23618;&#26041;&#27861;(MSGBiO&#21644;VR-MSGBiO)&#26469;&#35299;&#20915;&#36825;&#20123;&#38543;&#26426;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#29992;&#30340;&#25910;&#25947;&#20998;&#26512;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#19968;&#20123;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;MGBiO&#26041;&#27861;&#20855;&#26377;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel optimization is a popular two-level hierarchical optimization, which has been widely applied to many machine learning tasks such as hyperparameter learning, meta learning and continual learning. Although many bilevel optimization methods recently have been developed, the bilevel methods are not well studied when the lower-level problem is nonconvex. To fill this gap, in the paper, we study a class of nonconvex bilevel optimization problems, where both upper-level and lower-level problems are nonconvex, and the lower-level problem satisfies Polyak-{\L}ojasiewicz (PL) condition. We propose an efficient momentum-based gradient bilevel method (MGBiO) to solve these deterministic problems. Meanwhile, we propose a class of efficient momentum-based stochastic gradient bilevel methods (MSGBiO and VR-MSGBiO) to solve these stochastic problems. Moreover, we provide a useful convergence analysis framework for our methods. Specifically, under some mild conditions, we prove that our MGBiO m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;D5&#65292;&#36890;&#36807;&#30446;&#26631;&#39537;&#21160;&#30340;&#26041;&#24335;&#33258;&#21160;&#21457;&#29616;&#20004;&#20010;&#22823;&#22411;&#35821;&#26009;&#24211;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;D5&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#32479;&#19968;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#34913;&#37327;&#20854;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#30446;&#26631;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#35821;&#26009;&#24211;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2302.14233</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#30446;&#26631;&#30340;&#35821;&#35328;&#25551;&#36848;&#21457;&#29616;&#20998;&#24067;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Goal Driven Discovery of Distributional Differences via Language Descriptions. (arXiv:2302.14233v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;D5&#65292;&#36890;&#36807;&#30446;&#26631;&#39537;&#21160;&#30340;&#26041;&#24335;&#33258;&#21160;&#21457;&#29616;&#20004;&#20010;&#22823;&#22411;&#35821;&#26009;&#24211;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;D5&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#32479;&#19968;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#34913;&#37327;&#20854;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#30446;&#26631;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#35821;&#26009;&#24211;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25366;&#25496;&#22823;&#22411;&#35821;&#26009;&#24211;&#21487;&#20197;&#20135;&#29983;&#26377;&#29992;&#30340;&#21457;&#29616;&#65292;&#20294;&#23545;&#20154;&#31867;&#26469;&#35828;&#32791;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;D5&#65292;&#23427;&#20197;&#30446;&#26631;&#39537;&#21160;&#30340;&#26041;&#24335;&#33258;&#21160;&#21457;&#29616;&#20004;&#20010;&#22823;&#22411;&#35821;&#26009;&#24211;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20219;&#21153;&#36755;&#20837;&#26159;&#19968;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#19968;&#20010;&#30740;&#31350;&#30446;&#26631;&#8220;&#27604;&#36739;&#33647;&#29289;A&#21644;&#33647;&#29289;B&#30340;&#21103;&#20316;&#29992;&#8221;&#65292;&#20197;&#21450;&#19968;&#20010;&#35821;&#26009;&#24211;&#23545;&#65288;&#20004;&#20010;&#22823;&#22411;&#24739;&#32773;&#33258;&#25253;&#21453;&#24212;&#30340;&#38598;&#21512;&#65289;&#12290;&#36755;&#20986;&#26159;&#23545;&#36825;&#20123;&#35821;&#26009;&#24211;&#24046;&#24322;&#30340;&#35821;&#35328;&#25551;&#36848;&#65288;&#21457;&#29616;&#65289;&#65288;&#20351;&#29992;&#33647;&#29289;A&#21518;&#65292;&#24739;&#32773;&#26356;&#32463;&#24120;&#25552;&#21040;&#8220;&#20559;&#25191;&#24863;&#8221;&#65289;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;D5&#31995;&#32479;&#65292;&#24182;&#20026;&#20102;&#23450;&#37327;&#34913;&#37327;&#20854;&#24615;&#33021;&#65292;&#25105;&#20204;&#36129;&#29486;&#20102;&#19968;&#20010;&#20803;&#25968;&#25454;&#38598;OpenD5&#65292;&#32858;&#21512;&#20102;675&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#21830;&#19994;&#12289;&#31038;&#20250;&#31185;&#23398;&#12289;&#20154;&#25991;&#23398;&#31185;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#20581;&#24247;&#31561;&#39046;&#22495;&#65292;&#21516;&#26102;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#32479;&#19968;&#30340;&#35780;&#20272;&#25351;&#26631;&#65306;&#26377;&#25928;&#24615;&#12289;&#30456;&#20851;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#26174;&#33879;&#24615;&#12290;&#36890;&#36807;&#25968;&#25454;&#38598;&#21644;&#32479;&#19968;&#25351;&#26631;&#65292;&#25105;&#20204;&#30830;&#35748;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#30446;&#26631;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#35821;&#26009;&#24211;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mining large corpora can generate useful discoveries but is time-consuming for humans. We formulate a new task, D5, that automatically discovers differences between two large corpora in a goal-driven way. The task input is a problem comprising a research goal "$\textit{comparing the side effects of drug A and drug B}$" and a corpus pair (two large collections of patients' self-reported reactions after taking each drug). The output is a language description (discovery) of how these corpora differ (patients taking drug A "$\textit{mention feelings of paranoia}$" more often). We build a D5 system, and to quantitatively measure its performance, we 1) contribute a meta-dataset, OpenD5, aggregating 675 open-ended problems ranging across business, social sciences, humanities, machine learning, and health, and 2) propose a set of unified evaluation metrics: validity, relevance, novelty, and significance. With the dataset and the unified metrics, we confirm that language models can use the goal
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#22810;&#26679;&#21270;&#20998;&#24067;&#36716;&#25442;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#38024;&#23545;&#24615;&#22320;&#35774;&#35745;&#20102;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#20998;&#24067;&#36716;&#25442;&#23545;&#20110;&#29616;&#26377;&#30340;&#22270;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.13875</link><description>&lt;p&gt;
&#22312;&#32467;&#26500;&#20998;&#24067;&#20559;&#31227;&#26465;&#20214;&#19979;&#35780;&#20272;&#22270;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Robustness and Uncertainty of Graph Models Under Structural Distributional Shifts. (arXiv:2302.13875v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13875
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#22810;&#26679;&#21270;&#20998;&#24067;&#36716;&#25442;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#38024;&#23545;&#24615;&#22320;&#35774;&#35745;&#20102;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#20998;&#24067;&#36716;&#25442;&#23545;&#20110;&#29616;&#26377;&#30340;&#22270;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21487;&#38752;&#20915;&#31574;&#31995;&#32479;&#20013;&#65292;&#27169;&#22411;&#24517;&#39035;&#23545;&#20998;&#24067;&#20559;&#31227;&#20855;&#26377;&#40065;&#26834;&#24615;&#25110;&#25552;&#20379;&#20854;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#22270;&#23398;&#20064;&#30340;&#33410;&#28857;&#32423;&#38382;&#39064;&#20013;&#65292;&#20998;&#24067;&#20559;&#31227;&#21487;&#33021;&#23588;&#20026;&#22797;&#26434;&#65292;&#22240;&#20026;&#26679;&#26412;&#26159;&#30456;&#20114;&#20381;&#36182;&#30340;&#12290;&#20026;&#20102;&#35780;&#20272;&#22270;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#37325;&#35201;&#30340;&#26159;&#22312;&#21508;&#31181;&#26377;&#24847;&#20041;&#30340;&#20998;&#24067;&#20559;&#31227;&#19979;&#23545;&#23427;&#20204;&#36827;&#34892;&#27979;&#35797;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#32771;&#34385;&#33410;&#28857;&#32423;&#20998;&#24067;&#20559;&#31227;&#30340;&#22270;&#22522;&#20934;&#20027;&#35201;&#20851;&#27880;&#33410;&#28857;&#29305;&#24449;&#65292;&#32780;&#32467;&#26500;&#23646;&#24615;&#23545;&#22270;&#38382;&#39064;&#20063;&#24456;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#32467;&#26500;&#24341;&#20986;&#22810;&#26679;&#21270;&#20998;&#24067;&#20559;&#31227;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#26681;&#25454;&#20960;&#20010;&#33410;&#28857;&#30340;&#32467;&#26500;&#23646;&#24615;&#65306;&#27969;&#34892;&#24230;&#12289;&#23616;&#37096;&#24615;&#21644;&#23494;&#24230;&#26469;&#21019;&#24314;&#25968;&#25454;&#20998;&#21106;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#23545;&#20110;&#29616;&#26377;&#30340;&#22270;&#27169;&#22411;&#21487;&#33021;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#36824;&#20462;&#35746;&#20102;&#19968;&#20123;&#20851;&#20110;&#22522;&#20934;&#27979;&#35797;&#22270;&#27169;&#22411;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#32771;&#34385;&#20102;&#32467;&#26500;&#20998;&#24067;&#20559;&#31227;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reliable decision-making systems based on machine learning, models have to be robust to distributional shifts or provide the uncertainty of their predictions. In node-level problems of graph learning, distributional shifts can be especially complex since the samples are interdependent. To evaluate the performance of graph models, it is important to test them on diverse and meaningful distributional shifts. However, most graph benchmarks considering distributional shifts for node-level problems focus mainly on node features, while structural properties are also essential for graph problems. In this work, we propose a general approach for inducing diverse distributional shifts based on graph structure. We use this approach to create data splits according to several structural node properties: popularity, locality, and density. In our experiments, we thoroughly evaluate the proposed distributional shifts and show that they can be quite challenging for existing graph models. We also rev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#35282;&#32447;&#32447;&#24615;&#32593;&#32476;&#19978;&#65292;&#38543;&#26426;&#24615;&#21644;&#22823;&#27493;&#38271;&#23545;&#26799;&#24230;&#19979;&#38477;(GD)&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(SGD)&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22823;&#27493;&#38271;&#23545;&#31232;&#30095;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;SGD&#26377;&#30410;&#22788;&#65292;&#20294;&#23545;GD&#21487;&#33021;&#26377;&#23475;&#12290;&#36825;&#31181;&#24433;&#21709;&#22312;&#25509;&#36817;&#21457;&#25955;&#38408;&#20540;&#30340;&#32039;&#23494;&#27493;&#38271;&#19979;&#34987;&#25918;&#22823;&#12290;</title><link>http://arxiv.org/abs/2302.08982</link><description>&lt;p&gt;
(S)GD&#22312;&#23545;&#35282;&#32447;&#32447;&#24615;&#32593;&#32476;&#19978;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#12289;&#22823;&#27493;&#38271;&#21644;&#31283;&#23450;&#36793;&#32536;&#30340;&#24433;&#21709;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
(S)GD over Diagonal Linear Networks: Implicit Regularisation, Large Stepsizes and Edge of Stability. (arXiv:2302.08982v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#35282;&#32447;&#32447;&#24615;&#32593;&#32476;&#19978;&#65292;&#38543;&#26426;&#24615;&#21644;&#22823;&#27493;&#38271;&#23545;&#26799;&#24230;&#19979;&#38477;(GD)&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(SGD)&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22823;&#27493;&#38271;&#23545;&#31232;&#30095;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;SGD&#26377;&#30410;&#22788;&#65292;&#20294;&#23545;GD&#21487;&#33021;&#26377;&#23475;&#12290;&#36825;&#31181;&#24433;&#21709;&#22312;&#25509;&#36817;&#21457;&#25955;&#38408;&#20540;&#30340;&#32039;&#23494;&#27493;&#38271;&#19979;&#34987;&#25918;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#24615;&#21644;&#22823;&#27493;&#38271;&#23545;&#26799;&#24230;&#19979;&#38477;(GD)&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(SGD)&#22312;&#23545;&#35282;&#32447;&#32447;&#24615;&#32593;&#32476;&#19978;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36807;&#21442;&#25968;&#21270;&#30340;&#22238;&#24402;&#35774;&#32622;&#20013;&#65292;&#20351;&#29992;&#23439;&#35266;&#27493;&#38271;&#30340;GD&#21644;SGD&#25910;&#25947;&#65292;&#24182;&#36890;&#36807;&#38544;&#24335;&#27491;&#21017;&#21270;&#38382;&#39064;&#25551;&#36848;&#23427;&#20204;&#30340;&#35299;&#12290;&#25105;&#20204;&#30340;&#28165;&#26224;&#25551;&#36848;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#38543;&#26426;&#24615;&#21644;&#27493;&#38271;&#23545;&#24674;&#22797;&#35299;&#30340;&#24433;&#21709;&#30340;&#23450;&#24615;&#35265;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#34920;&#26126;&#22312;&#31232;&#30095;&#22238;&#24402;&#38382;&#39064;&#20013;&#65292;&#22823;&#27493;&#38271;&#23545;SGD&#26377;&#31283;&#23450;&#30340;&#22909;&#22788;&#65292;&#20294;&#23545;GD&#30340;&#31232;&#30095;&#35299;&#24674;&#22797;&#21487;&#33021;&#20135;&#29983;&#38459;&#30861;&#12290;&#36825;&#20123;&#25928;&#24212;&#22312;&#32039;&#23494;&#31383;&#21475;&#20869;&#30340;&#27493;&#38271;&#19979;&#34987;&#25918;&#22823;&#65292;&#20301;&#20110;&#8220;&#31283;&#23450;&#36793;&#32536;&#8221;&#21306;&#22495;&#30340;&#27493;&#38271;&#12290;&#23454;&#39564;&#32467;&#26524;&#25903;&#25345;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the impact of stochasticity and large stepsizes on the implicit regularisation of gradient descent (GD) and stochastic gradient descent (SGD) over diagonal linear networks. We prove the convergence of GD and SGD with macroscopic stepsizes in an overparametrised regression setting and characterise their solutions through an implicit regularisation problem. Our crisp characterisation leads to qualitative insights about the impact of stochasticity and stepsizes on the recovered solution. Specifically, we show that large stepsizes consistently benefit SGD for sparse regression problems, while they can hinder the recovery of sparse solutions for GD. These effects are magnified for stepsizes in a tight window just below the divergence threshold, in the "edge of stability" regime. Our findings are supported by experimental results.
&lt;/p&gt;</description></item><item><title>ReDi&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26080;&#23398;&#20064;&#25193;&#25955;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#36712;&#36857;&#26816;&#32034;&#26469;&#21152;&#36895;&#25512;&#26029;&#36807;&#31243;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#22312;&#25552;&#39640;&#27169;&#22411;&#25512;&#26029;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;2&#20493;&#30340;&#21152;&#36895;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#38646;&#26679;&#26412;&#36328;&#22495;&#22270;&#20687;&#29983;&#25104;&#20013;&#27867;&#21270;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2302.02285</link><description>&lt;p&gt;
ReDi: &#39640;&#25928;&#30340;&#26080;&#23398;&#20064;&#25193;&#25955;&#25512;&#26029;&#36890;&#36807;&#36712;&#36857;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
ReDi: Efficient Learning-Free Diffusion Inference via Trajectory Retrieval. (arXiv:2302.02285v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02285
&lt;/p&gt;
&lt;p&gt;
ReDi&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26080;&#23398;&#20064;&#25193;&#25955;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#36712;&#36857;&#26816;&#32034;&#26469;&#21152;&#36895;&#25512;&#26029;&#36807;&#31243;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#22312;&#25552;&#39640;&#27169;&#22411;&#25512;&#26029;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;2&#20493;&#30340;&#21152;&#36895;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#38646;&#26679;&#26412;&#36328;&#22495;&#22270;&#20687;&#29983;&#25104;&#20013;&#27867;&#21270;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#23637;&#29616;&#20102;&#21508;&#31181;&#25968;&#25454;&#30340;&#20248;&#24322;&#29983;&#25104;&#33021;&#21147;&#12290;&#23613;&#31649;&#20854;&#29983;&#25104;&#36136;&#37327;&#36739;&#39640;&#65292;&#20294;&#25193;&#25955;&#27169;&#22411;&#30340;&#25512;&#26029;&#20173;&#28982;&#32791;&#26102;&#65292;&#22240;&#20026;&#38656;&#35201;&#22823;&#37327;&#30340;&#37319;&#26679;&#36845;&#20195;&#12290;&#20026;&#20102;&#21152;&#36895;&#25512;&#26029;&#36807;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26080;&#38656;&#23398;&#20064;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#25193;&#25955;&#37319;&#26679;&#26694;&#26550;&#65288;ReDi&#65289;&#12290;&#36890;&#36807;&#39044;&#20808;&#35745;&#31639;&#30340;&#30693;&#35782;&#24211;&#65292;ReDi&#20174;&#19968;&#20010;&#22312;&#29983;&#25104;&#30340;&#26089;&#26399;&#38454;&#27573;&#19982;&#37096;&#20998;&#29983;&#25104;&#36712;&#36857;&#30456;&#20284;&#30340;&#36712;&#36857;&#20013;&#26816;&#32034;&#65292;&#36339;&#36807;&#22823;&#37096;&#20998;&#20013;&#38388;&#27493;&#39588;&#65292;&#24182;&#22312;&#26816;&#32034;&#21040;&#30340;&#36712;&#36857;&#30340;&#21518;&#32493;&#27493;&#39588;&#20013;&#32487;&#32493;&#37319;&#26679;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;ReDi&#30340;&#29983;&#25104;&#24615;&#33021;&#26159;&#26377;&#20445;&#35777;&#30340;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ReDi&#25552;&#39640;&#20102;&#27169;&#22411;&#25512;&#26029;&#30340;&#25928;&#29575;&#65292;&#21152;&#36895;&#20102;2&#20493;&#12290;&#27492;&#22806;&#65292;ReDi&#22312;&#38646;&#26679;&#26412;&#36328;&#22495;&#22270;&#20687;&#29983;&#25104;&#65288;&#22914;&#22270;&#20687;&#39118;&#26684;&#21270;&#65289;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models show promising generation capability for a variety of data. Despite their high generation quality, the inference for diffusion models is still time-consuming due to the numerous sampling iterations required. To accelerate the inference, we propose ReDi, a simple yet learning-free Retrieval-based Diffusion sampling framework. From a precomputed knowledge base, ReDi retrieves a trajectory similar to the partially generated trajectory at an early stage of generation, skips a large portion of intermediate steps, and continues sampling from a later step in the retrieved trajectory. We theoretically prove that the generation performance of ReDi is guaranteed. Our experiments demonstrate that ReDi improves the model inference efficiency by 2x speedup. Furthermore, ReDi is able to generalize well in zero-shot cross-domain image generation such as image stylization.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24130;&#24459;&#35268;&#24459;&#30340;Deep Power Laws&#65288;DPL&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36229;&#21442;&#25968;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#34920;&#26684;&#12289;&#22270;&#20687;&#21644;NLP&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.00441</link><description>&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#24130;&#24459;&#27861;&#21017;
&lt;/p&gt;
&lt;p&gt;
Power Laws for Hyperparameter Optimization. (arXiv:2302.00441v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00441
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24130;&#24459;&#35268;&#24459;&#30340;Deep Power Laws&#65288;DPL&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36229;&#21442;&#25968;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#34920;&#26684;&#12289;&#22270;&#20687;&#21644;NLP&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#23376;&#39046;&#22495;&#65292;&#23427;&#19987;&#27880;&#20110;&#35843;&#25972;&#25152;&#36873;&#31639;&#27861;&#30340;&#36229;&#21442;&#25968;&#20197;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#26377;&#19968;&#31995;&#21015;&#26041;&#27861;&#35299;&#20915;&#20102;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#27809;&#26377;&#21033;&#29992;&#23398;&#20064;&#26354;&#32447;&#30340;&#32553;&#25918;&#35268;&#24459;&#29305;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;Deep Power Laws&#65288;DPL&#65289;&#65292;&#19968;&#32452;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#20204;&#30340;&#39044;&#27979;&#36981;&#24490;&#19968;&#20010;&#24130;&#24459;&#32553;&#25918;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#28784;&#30418;&#35780;&#20272;&#21160;&#24577;&#20915;&#23450;&#26242;&#20572;&#21644;&#22686;&#37327;&#35757;&#32451;&#21738;&#20123;&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;&#19982;3&#20010;&#22522;&#20934;&#30456;&#20851;&#30340;&#34920;&#26684;&#65292;&#22270;&#20687;&#21644;NLP&#25968;&#25454;&#38598;&#19978;&#19982;7&#31181;&#26368;&#20808;&#36827;&#30340;&#31454;&#20105;&#23545;&#25163;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#28085;&#30422;59&#39033;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#65292;&#24182;&#33719;&#24471;&#20102;&#27604;&#25152;&#26377;&#31454;&#20105;&#23545;&#25163;&#26356;&#22909;&#30340;&#20219;&#20309;&#26102;&#20505;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization is an important subfield of machine learning that focuses on tuning the hyperparameters of a chosen algorithm to achieve peak performance. Recently, there has been a stream of methods that tackle the issue of hyperparameter optimization, however, most of the methods do not exploit the scaling law property of learning curves. In this work, we propose Deep Power Laws (DPL), an ensemble of neural network models conditioned to yield predictions that follow a power-law scaling pattern. Our method dynamically decides which configurations to pause and train incrementally by making use of gray-box evaluations. We compare our method against 7 state-of-the-art competitors on 3 benchmarks related to tabular, image, and NLP datasets covering 59 diverse tasks. Our method achieves the best results across all benchmarks by obtaining the best any-time results compared to all competitors.
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#33976;&#39311;&#21644;&#26631;&#31614;&#24179;&#28369;&#34987;&#35748;&#20026;&#26159;&#31561;&#20215;&#30340;&#26041;&#27861;&#65292;&#20294;&#23454;&#39564;&#35777;&#26126;&#23427;&#20204;&#23545;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#24433;&#21709;&#26041;&#21521;&#23436;&#20840;&#30456;&#21453;&#12290;&#30693;&#35782;&#33976;&#39311;&#19981;&#20165;&#20256;&#36882;&#30693;&#35782;&#65292;&#36824;&#20256;&#36882;&#20102;&#33258;&#20449;&#24515;&#12290;</title><link>http://arxiv.org/abs/2301.12609</link><description>&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#8776;&#26631;&#31614;&#24179;&#28369;&#65306;&#20107;&#23454;&#36824;&#26159;&#35884;&#35823;&#65311;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation $\approx$ Label Smoothing: Fact or Fallacy?. (arXiv:2301.12609v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12609
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#21644;&#26631;&#31614;&#24179;&#28369;&#34987;&#35748;&#20026;&#26159;&#31561;&#20215;&#30340;&#26041;&#27861;&#65292;&#20294;&#23454;&#39564;&#35777;&#26126;&#23427;&#20204;&#23545;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#24433;&#21709;&#26041;&#21521;&#23436;&#20840;&#30456;&#21453;&#12290;&#30693;&#35782;&#33976;&#39311;&#19981;&#20165;&#20256;&#36882;&#30693;&#35782;&#65292;&#36824;&#20256;&#36882;&#20102;&#33258;&#20449;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#21021;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#20174;&#19968;&#20010;&#27169;&#22411;&#21521;&#21478;&#19968;&#20010;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#26041;&#27861;&#65292;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#30693;&#35782;&#33976;&#39311;(KD)&#23454;&#38469;&#19978;&#26159;&#19968;&#31181;&#27491;&#21017;&#21270;&#30340;&#24418;&#24335;&#12290;&#26368;&#24378;&#26377;&#21147;&#30340;&#25903;&#25345;&#26469;&#33258;&#20110;&#23427;&#19982;&#26631;&#31614;&#24179;&#28369;(LS)&#26041;&#27861;&#30340;&#26126;&#26174;&#30456;&#20284;&#20043;&#22788;&#12290;&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#23427;&#20204;&#25152;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#65292;&#37325;&#26032;&#32771;&#23519;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#31561;&#20215;&#20851;&#31995;&#12290;&#22312;&#28041;&#21450;&#19981;&#21516;&#35268;&#27169;&#27169;&#22411;&#30340;&#22235;&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;&#26174;&#31034;&#20986;&#65306;(a)&#22312;&#22823;&#22810;&#25968;&#35774;&#32622;&#20013;&#65292;KD&#21644;LS&#20250;&#23436;&#20840;&#30456;&#21453;&#22320;&#24433;&#21709;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#12290;(b) &#22312;KD&#20013;&#65292;&#23398;&#29983;&#19981;&#20165;&#32487;&#25215;&#30693;&#35782;&#65292;&#32780;&#19988;&#36824;&#20174;&#32769;&#24072;&#37027;&#37324;&#32487;&#25215;&#33258;&#20449;&#24515;&#65292;&#21152;&#24378;&#20102;&#20256;&#32479;&#30340;&#30693;&#35782;&#20256;&#36882;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Originally proposed as a method for knowledge transfer from one model to another, some recent studies have suggested that knowledge distillation (KD) is in fact a form of regularization. Perhaps the strongest support of all for this new perspective comes from its apparent similarities with label smoothing (LS). Here we re-examine this stated equivalence between the two methods by comparing the predictive confidences of the models they train. Experiments on four text classification tasks involving models of different sizes show that: (a) In most settings, KD and LS drive model confidence in completely opposite directions, and (b) In KD, the student inherits not only its knowledge but also its confidence from the teacher, reinforcing the classical knowledge transfer view.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#24191;&#20041;&#30340;Tsallis KL&#25955;&#24230;&#65292;&#25193;&#23637;&#20102;Munchausen&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;KL&#27491;&#21017;&#21270;&#32435;&#20837;&#23454;&#38469;&#31639;&#27861;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;Tsallis KL&#65292;&#24403;$q &gt; 1$&#26102;&#65292;&#21487;&#20197;&#33719;&#24471;&#26032;&#30340;&#31574;&#30053;&#20248;&#21270;&#36873;&#39033;&#12290;</title><link>http://arxiv.org/abs/2301.11476</link><description>&lt;p&gt;
&#20351;&#29992;Tsallis KL&#25955;&#24230;&#30340;&#24191;&#20041;Munchausen&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generalized Munchausen Reinforcement Learning using Tsallis KL Divergence. (arXiv:2301.11476v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11476
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#24191;&#20041;&#30340;Tsallis KL&#25955;&#24230;&#65292;&#25193;&#23637;&#20102;Munchausen&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;KL&#27491;&#21017;&#21270;&#32435;&#20837;&#23454;&#38469;&#31639;&#27861;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;Tsallis KL&#65292;&#24403;$q &gt; 1$&#26102;&#65292;&#21487;&#20197;&#33719;&#24471;&#26032;&#30340;&#31574;&#30053;&#20248;&#21270;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#37117;&#37319;&#29992;Kullback-Leibler&#65288;KL&#65289;&#25955;&#24230;&#21040;&#19978;&#19968;&#20010;&#31574;&#30053;&#65292;&#20197;&#38450;&#27490;&#31574;&#30053;&#21464;&#21270;&#36807;&#24555;&#12290;&#36825;&#20010;&#24819;&#27861;&#26368;&#21021;&#26159;&#22312;Conservative Policy Iteration&#30340;&#19968;&#31687;&#37325;&#35201;&#35770;&#25991;&#20013;&#25552;&#20986;&#30340;&#65292;&#36817;&#20284;&#31639;&#27861;&#22914;TRPO&#21644;Munchausen Value Iteration&#65288;MVI&#65289;&#32473;&#20986;&#20102;&#26377;&#38480;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#19968;&#31181;&#24191;&#20041;&#30340;KL&#25955;&#24230; - &#31216;&#20026;Tsallis KL&#25955;&#24230; - &#26469;&#32487;&#32493;&#36825;&#19968;&#24037;&#20316;&#65292;&#23427;&#22312;&#23450;&#20041;&#20013;&#20351;&#29992;&#20102;$q$-&#23545;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#26159;&#19968;&#31181;&#20005;&#26684;&#30340;&#25512;&#24191;&#65292;&#22240;&#20026;$q = 1$&#23545;&#24212;&#20110;&#26631;&#20934;&#30340;KL&#25955;&#24230;&#65307;$q &gt; 1$&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#36873;&#39033;&#12290;&#25105;&#20204;&#23545;&#22312;Tsallis KL&#19979;&#23398;&#20064;&#30340;&#31574;&#30053;&#31867;&#22411;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#38416;&#36848;&#20102;&#20309;&#26102;$ q &gt; 1 $&#21487;&#33021;&#26159;&#26377;&#30410;&#30340;&#12290;&#20026;&#20102;&#33719;&#24471;&#19968;&#20010;&#23558;Tsallis KL&#27491;&#21017;&#21270;&#32435;&#20837;&#23454;&#38469;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;MVI&#65292;&#23427;&#26159;&#19968;&#31181;&#26368;&#31616;&#21333;&#30340;&#21253;&#21547;KL&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#24191;&#20041;MVI&#65288;$q$&#65289;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many policy optimization approaches in reinforcement learning incorporate a Kullback-Leilbler (KL) divergence to the previous policy, to prevent the policy from changing too quickly. This idea was initially proposed in a seminal paper on Conservative Policy Iteration, with approximations given by algorithms like TRPO and Munchausen Value Iteration (MVI). We continue this line of work by investigating a generalized KL divergence -- called the Tsallis KL divergence -- which use the $q$-logarithm in the definition. The approach is a strict generalization, as $q = 1$ corresponds to the standard KL divergence; $q &gt; 1$ provides a range of new options. We characterize the types of policies learned under the Tsallis KL, and motivate when $q &gt;1$ could be beneficial. To obtain a practical algorithm that incorporates Tsallis KL regularization, we extend MVI, which is one of the simplest approaches to incorporate KL regularization. We show that this generalized MVI($q$) obtains significant improve
&lt;/p&gt;</description></item><item><title>AtMan&#26159;&#19968;&#31181;&#36890;&#36807;&#22312;&#29983;&#25104;&#24335;Transformer&#27169;&#22411;&#20013;&#25805;&#32437;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35299;&#37322;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20960;&#20046;&#19981;&#21344;&#29992;&#39069;&#22806;&#20869;&#23384;&#65292;&#21487;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2301.08110</link><description>&lt;p&gt;
AtMan:&#36890;&#36807;&#33410;&#32422;&#20869;&#23384;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#29702;&#35299;Transformer&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
AtMan: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation. (arXiv:2301.08110v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08110
&lt;/p&gt;
&lt;p&gt;
AtMan&#26159;&#19968;&#31181;&#36890;&#36807;&#22312;&#29983;&#25104;&#24335;Transformer&#27169;&#22411;&#20013;&#25805;&#32437;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35299;&#37322;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20960;&#20046;&#19981;&#21344;&#29992;&#39069;&#22806;&#20869;&#23384;&#65292;&#21487;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#30340;Transformer&#27169;&#22411;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#21442;&#25968;&#25968;&#37327;&#22823;&#19988;&#20855;&#22791;&#22788;&#29702;&#22810;&#36755;&#20837;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#30446;&#21069;&#35299;&#37322;&#23427;&#20204;&#30340;&#39044;&#27979;&#30340;&#26041;&#27861;&#36164;&#28304;&#23494;&#38598;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#20204;&#38656;&#35201;&#36807;&#22810;&#30340;&#39069;&#22806;&#20869;&#23384;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#21453;&#21521;&#20256;&#25773;&#65292;&#32780;&#21453;&#21521;&#20256;&#25773;&#20250;&#20998;&#37197;&#30340;GPU&#20869;&#23384;&#20960;&#20046;&#26159;&#21069;&#21521;&#20256;&#25773;&#30340;&#20004;&#20493;&#12290;&#36825;&#20351;&#24471;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#20351;&#29992;&#23427;&#20204;&#38750;&#24120;&#22256;&#38590;&#65292;&#29978;&#33267;&#19981;&#21487;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AtMan&#65292;&#23427;&#20960;&#20046;&#19981;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#25104;&#26412;&#65292;&#29992;&#20110;&#35299;&#37322;&#29983;&#25104;&#24335;Transformer&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;AtMan&#26159;&#19968;&#31181;&#27169;&#24577;&#26080;&#20851;&#30340;&#25200;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#25805;&#32437;Transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#29983;&#25104;&#19982;&#36755;&#20986;&#39044;&#27979;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#24615;&#22270;&#12290;AtMan&#19981;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#65292;&#32780;&#26159;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#24212;&#29992;&#19968;&#31181;&#22522;&#20110;&#20313;&#24358;&#30456;&#20284;&#24230;&#37051;&#36817;&#24615;&#30340;&#21487;&#24182;&#34892;&#21270;&#22522;&#20110;&#35760;&#21495;&#30340;&#25628;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;-&#25991;&#26412;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Generative transformer models have become increasingly complex, with large numbers of parameters and the ability to process multiple input modalities. Current methods for explaining their predictions are resource-intensive. Most crucially, they require prohibitively large amounts of extra memory, since they rely on backpropagation which allocates almost twice as much GPU memory as the forward pass. This makes it difficult, if not impossible, to use them in production. We present AtMan that provides explanations of generative transformer models at almost no extra cost. Specifically, AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction. Instead of using backpropagation, AtMan applies a parallelizable token-based search method based on cosine similarity neighborhood in the embedding space. Our exhaustive experiments on text and image-text benchmarks demonstra
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#35752;&#35770;&#20102;&#39640;&#24615;&#33021;&#35745;&#31639;&#31038;&#21306;&#20013;&#27969;&#20256;&#30340;&#31070;&#35805;&#21644;&#20256;&#35828;&#65292;&#36825;&#20123;&#31070;&#35805;&#24448;&#24448;&#19981;&#22522;&#20110;&#31185;&#23398;&#20107;&#23454;&#65292;&#32780;&#26159;&#22522;&#20110;&#19968;&#20123;&#35777;&#25454;&#25110;&#35770;&#35777;&#12290;&#34429;&#28982;&#26377;&#20123;&#38382;&#39064;&#20173;&#28982;&#26159;&#26080;&#20241;&#27490;&#30340;&#21746;&#23398;&#36777;&#35770;&#65292;&#20294;&#26032;&#30340;&#26041;&#21521;&#27491;&#22312;&#20986;&#29616;&#65292;&#22914;&#31639;&#27861;&#30340;&#35268;&#27169;&#21270;&#25110;&#26032;&#30340;&#26550;&#26500;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2301.02432</link><description>&lt;p&gt;
&#39640;&#24615;&#33021;&#35745;&#31639;&#20013;&#30340;&#31070;&#35805;&#19982;&#20256;&#35828;
&lt;/p&gt;
&lt;p&gt;
Myths and Legends in High-Performance Computing. (arXiv:2301.02432v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02432
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#35752;&#35770;&#20102;&#39640;&#24615;&#33021;&#35745;&#31639;&#31038;&#21306;&#20013;&#27969;&#20256;&#30340;&#31070;&#35805;&#21644;&#20256;&#35828;&#65292;&#36825;&#20123;&#31070;&#35805;&#24448;&#24448;&#19981;&#22522;&#20110;&#31185;&#23398;&#20107;&#23454;&#65292;&#32780;&#26159;&#22522;&#20110;&#19968;&#20123;&#35777;&#25454;&#25110;&#35770;&#35777;&#12290;&#34429;&#28982;&#26377;&#20123;&#38382;&#39064;&#20173;&#28982;&#26159;&#26080;&#20241;&#27490;&#30340;&#21746;&#23398;&#36777;&#35770;&#65292;&#20294;&#26032;&#30340;&#26041;&#21521;&#27491;&#22312;&#20986;&#29616;&#65292;&#22914;&#31639;&#27861;&#30340;&#35268;&#27169;&#21270;&#25110;&#26032;&#30340;&#26550;&#26500;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#21457;&#20154;&#28145;&#30465;&#30340;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#39640;&#24615;&#33021;&#35745;&#31639;&#31038;&#21306;&#25104;&#21592;&#38388;&#27969;&#20256;&#30340;&#19968;&#20123;&#31070;&#35805;&#21644;&#20256;&#35828;&#12290;&#25105;&#20204;&#20174;&#20250;&#35758;&#21644;&#20250;&#35758;&#19978;&#30340;&#23545;&#35805;&#12289;&#20135;&#21697;&#24191;&#21578;&#12289;&#35770;&#25991;&#20197;&#21450;&#31038;&#21306;&#20869;&#22806;&#30340;&#25512;&#29305;&#12289;&#21338;&#23458;&#21644;&#26032;&#38395;&#25991;&#31456;&#20013;&#25910;&#38598;&#20102;&#36825;&#20123;&#31070;&#35805;&#12290;&#25105;&#20204;&#35748;&#20026;&#23427;&#20204;&#20195;&#34920;&#20102;&#24403;&#21069;&#26102;&#20195;&#30340;&#26102;&#20195;&#31934;&#31070;&#65292;&#36825;&#20010;&#26102;&#20195;&#27491;&#22312;&#32463;&#21382;&#35768;&#22810;&#35268;&#27169;&#23450;&#24459;&#30340;&#32456;&#32467;&#65292;&#22914;Dennard&#23450;&#24459;&#21644;&#25705;&#23572;&#23450;&#24459;&#12290;&#34429;&#28982;&#19968;&#20123;&#23450;&#24459;&#32467;&#26463;&#20102;&#65292;&#20294;&#26032;&#30340;&#26041;&#21521;&#27491;&#22312;&#20986;&#29616;&#65292;&#27604;&#22914;&#31639;&#27861;&#30340;&#35268;&#27169;&#21270;&#25110;&#26032;&#30340;&#26550;&#26500;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31070;&#35805;&#24456;&#23569;&#22522;&#20110;&#31185;&#23398;&#20107;&#23454;&#65292;&#32780;&#26159;&#22522;&#20110;&#19968;&#20123;&#35777;&#25454;&#25110;&#35770;&#35777;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#27491;&#26159;&#35768;&#22810;&#31070;&#35805;&#23384;&#22312;&#30340;&#21407;&#22240;&#65292;&#20063;&#26159;&#20026;&#20160;&#20040;&#23427;&#20204;&#26080;&#27861;&#24471;&#21040;&#26126;&#30830;&#31572;&#26696;&#30340;&#21407;&#22240;&#12290;&#34429;&#28982;&#27599;&#20010;&#31070;&#35805;&#37117;&#24212;&#35813;&#26377;&#26126;&#30830;&#30340;&#31572;&#26696;&#65292;&#20294;&#26377;&#20123;&#38382;&#39064;&#21487;&#33021;&#20173;&#28982;&#26159;&#26080;&#20241;&#27490;&#30340;&#21746;&#23398;&#36777;&#35770;&#65292;&#27604;&#22914;&#36125;&#22810;&#33452;&#27604;&#35841;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this thought-provoking article, we discuss certain myths and legends that are folklore among members of the high-performance computing community. We gathered these myths from conversations at conferences and meetings, product advertisements, papers, and other communications such as tweets, blogs, and news articles within and beyond our community. We believe they represent the zeitgeist of the current era of massive change, driven by the end of many scaling laws such as Dennard scaling and Moore's law. While some laws end, new directions are emerging, such as algorithmic scaling or novel architecture research. Nevertheless, these myths are rarely based on scientific facts, but rather on some evidence or argumentation. In fact, we believe that this is the very reason for the existence of many myths and why they cannot be answered clearly. While it feels like there should be clear answers for each, some may remain endless philosophical debates, such as whether Beethoven was better than
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#39044;&#27979;&#34507;&#30333;&#36136;&#21151;&#33021;&#30340; CNN DeepGOPlus &#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#25968;&#20540;&#19981;&#30830;&#23450;&#24615;&#21644;&#25968;&#20540;&#31283;&#23450;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#20351;&#29992;&#38477;&#20302;&#31934;&#24230;&#28014;&#28857;&#26684;&#24335;&#36827;&#34892;&#25512;&#29702;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.06361</link><description>&lt;p&gt;
DeepGOPlus &#25512;&#29702;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Numerical Stability of DeepGOPlus Inference. (arXiv:2212.06361v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06361
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#39044;&#27979;&#34507;&#30333;&#36136;&#21151;&#33021;&#30340; CNN DeepGOPlus &#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#25968;&#20540;&#19981;&#30830;&#23450;&#24615;&#21644;&#25968;&#20540;&#31283;&#23450;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#20351;&#29992;&#38477;&#20302;&#31934;&#24230;&#28014;&#28857;&#26684;&#24335;&#36827;&#34892;&#25512;&#29702;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (CNNs) &#26159;&#30446;&#21069;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#20043;&#19968;&#65292;&#22312;&#35768;&#22810;&#38382;&#39064;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#26368;&#21021;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#65292;&#20294; CNNs &#19982;&#20855;&#26377;&#31354;&#38388;&#20851;&#31995;&#30340;&#20219;&#20309;&#25968;&#25454;&#37117;&#33021;&#24456;&#22909;&#22320;&#37197;&#21512;&#20351;&#29992;&#65292;&#24182;&#24050;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102; CNNs&#65292;&#19982;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19968;&#26679;&#65292;&#23545;&#22122;&#22768;&#27880;&#20837;&#30340;&#25935;&#24863;&#24615;&#21487;&#33021;&#20250;&#21361;&#21450;&#20854;&#24615;&#33021;&#12290;&#26412;&#25991;&#37327;&#21270;&#20102; DeepGOPlus &#30340;&#28014;&#28857;&#31934;&#24230;&#19981;&#30830;&#23450;&#24615;&#20197;&#30830;&#23450;&#20854;&#25968;&#20540;&#31283;&#23450;&#24615;&#65292;DeepGOPlus &#26159;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#34507;&#30333;&#36136;&#21151;&#33021;&#30340; CNN&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38477;&#20302;&#31934;&#24230;&#28014;&#28857;&#26684;&#24335;&#36827;&#34892; DeepGOPlus &#25512;&#29702;&#20197;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#21644;&#24310;&#36831;&#30340;&#21487;&#33021;&#24615;&#12290;&#36825;&#26159;&#36890;&#36807; Monte Carlo Arithmetic &#23454;&#29616;&#30340;&#65292;&#35813;&#25216;&#26415;&#23454;&#39564;&#24615;&#22320;&#37327;&#21270;&#20102;&#28014;&#28857;&#36816;&#31639;&#38169;&#35823;&#21644; VPR&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks (CNNs) are currently among the most widely-used neural networks available and achieve state-of-the-art performance for many problems. While originally applied to computer vision tasks, CNNs work well with any data with a spatial relationship, besides images, and have been applied to different fields. However, recent works have highlighted how CNNs, like other deep learning models, are sensitive to noise injection which can jeopardise their performance. This paper quantifies the numerical uncertainty of the floating point arithmetic inaccuracies of the inference stage of DeepGOPlus, a CNN that predicts protein function, in order to determine its numerical stability. In addition, this paper investigates the possibility to use reduced-precision floating point formats for DeepGOPlus inference to reduce memory consumption and latency. This is achieved with Monte Carlo Arithmetic, a technique that experimentally quantifies floating point operation errors and VPR
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CTCO&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#20195;&#29702;&#36873;&#25321;&#36873;&#39033;&#20316;&#20026;&#21487;&#21464;&#25345;&#32493;&#26102;&#38388;&#30340;&#23376;&#31574;&#30053;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#20197;&#20219;&#20309;&#25152;&#38656;&#39057;&#29575;&#19982;&#31995;&#32479;&#20132;&#20114;&#65292;&#20174;&#32780;&#25552;&#20379;&#24179;&#28369;&#30340;&#21160;&#20316;&#21464;&#21270;&#65292;&#30456;&#27604;&#20256;&#32479;RL&#21644;&#26102;&#38388;&#25277;&#35937;RL&#26041;&#27861;&#65292;&#20854;&#24615;&#33021;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2212.04407</link><description>&lt;p&gt;
&#21487;&#21464;&#21270;&#20915;&#31574;&#39057;&#29575;&#30340;&#36873;&#39033;&#35780;&#35770;&#32773;
&lt;/p&gt;
&lt;p&gt;
Variable Decision-Frequency Option Critic. (arXiv:2212.04407v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04407
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CTCO&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#20195;&#29702;&#36873;&#25321;&#36873;&#39033;&#20316;&#20026;&#21487;&#21464;&#25345;&#32493;&#26102;&#38388;&#30340;&#23376;&#31574;&#30053;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#20197;&#20219;&#20309;&#25152;&#38656;&#39057;&#29575;&#19982;&#31995;&#32479;&#20132;&#20114;&#65292;&#20174;&#32780;&#25552;&#20379;&#24179;&#28369;&#30340;&#21160;&#20316;&#21464;&#21270;&#65292;&#30456;&#27604;&#20256;&#32479;RL&#21644;&#26102;&#38388;&#25277;&#35937;RL&#26041;&#27861;&#65292;&#20854;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#20195;&#29702;&#22312;&#31163;&#25955;&#21644;&#22266;&#23450;&#30340;&#26102;&#38388;&#38388;&#38548;&#20869;&#20570;&#20986;&#20915;&#31574;&#12290;&#20915;&#31574;&#20043;&#38388;&#30340;&#25345;&#32493;&#26102;&#38388;&#21464;&#25104;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#36229;&#21442;&#25968;&#65292;&#22240;&#20026;&#35774;&#32622;&#24471;&#22826;&#30701;&#21487;&#33021;&#20250;&#22686;&#21152;&#38382;&#39064;&#30340;&#38590;&#24230;&#65292;&#38656;&#35201;&#20195;&#29702;&#36827;&#34892;&#22810;&#27425;&#20915;&#31574;&#25165;&#33021;&#23454;&#29616;&#20854;&#30446;&#26631;&#65292;&#32780;&#35774;&#32622;&#24471;&#22826;&#38271;&#20250;&#23548;&#33268;&#20195;&#29702;&#22833;&#21435;&#23545;&#31995;&#32479;&#30340;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#29289;&#29702;&#31995;&#32479;&#19981;&#19968;&#23450;&#38656;&#35201;&#24658;&#23450;&#30340;&#25511;&#21046;&#39057;&#29575;&#65292;&#23545;&#20110;&#23398;&#20064;&#20195;&#29702;&#26469;&#35828;&#65292;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#24403;&#38656;&#35201;&#26102;&#20197;&#39640;&#39057;&#29575;&#36816;&#34892;&#65292;&#32780;&#22312;&#21487;&#33021;&#26102;&#20197;&#20302;&#39057;&#29575;&#36816;&#34892;&#26356;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#36830;&#32493;&#26102;&#38388;&#36830;&#32493;&#36873;&#39033; (CTCO) &#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#20195;&#29702;&#36873;&#25321;&#36873;&#39033;&#20316;&#20026;&#21487;&#21464;&#25345;&#32493;&#26102;&#38388;&#30340;&#23376;&#31574;&#30053;&#12290;&#36825;&#20123;&#36873;&#39033;&#26159;&#26102;&#38388;&#36830;&#32493;&#30340;&#65292;&#21487;&#20197;&#20197;&#20219;&#20309;&#25152;&#38656;&#39057;&#29575;&#19982;&#31995;&#32479;&#20132;&#20114;&#65292;&#20174;&#32780;&#25552;&#20379;&#24179;&#28369;&#30340;&#21160;&#20316;&#21464;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24615;&#33021;&#19982;&#20256;&#32479; RL &#21644;&#26102;&#38388;&#25277;&#35937; RL &#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102; CTCO &#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In classic reinforcement learning algorithms, agents make decisions at discrete and fixed time intervals. The duration between decisions becomes a crucial hyperparameter, as setting it too short may increase the difficulty of the problem by requiring the agent to make numerous decisions to achieve its goal, while setting it too long can result in the agent losing control over the system. However, physical systems do not necessarily require a constant control frequency, and for learning agents, it is often preferable to operate with a low frequency when possible and a high frequency when necessary. We propose a framework called Continuous-Time Continuous-Options (CTCO), where the agent chooses options as sub-policies of variable durations. These options are time-continuous and can interact with the system at any desired frequency providing a smooth change of actions. We demonstrate the effectiveness of CTCO by comparing its performance to classical RL and temporal-abstraction RL methods
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31119;&#21033;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#32447;&#24615;&#31119;&#21033;&#20989;&#25968;&#30340;Q-learning&#31639;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#26631;&#37327;&#21270;&#23398;&#20064;&#26356;&#26032;&#21644;&#38750;&#31283;&#24577;&#21160;&#20316;&#36873;&#25321;&#26469;&#20248;&#21270;&#31574;&#30053;&#12290;&#31639;&#27861;&#34987;&#35777;&#26126;&#26159;&#21487;&#25910;&#25947;&#30340;&#12290;</title><link>http://arxiv.org/abs/2212.01382</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31119;&#21033;&#21644;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Welfare and Fairness in Multi-objective Reinforcement Learning. (arXiv:2212.01382v4 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31119;&#21033;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#32447;&#24615;&#31119;&#21033;&#20989;&#25968;&#30340;Q-learning&#31639;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#26631;&#37327;&#21270;&#23398;&#20064;&#26356;&#26032;&#21644;&#38750;&#31283;&#24577;&#21160;&#20316;&#36873;&#25321;&#26469;&#20248;&#21270;&#31574;&#30053;&#12290;&#31639;&#27861;&#34987;&#35777;&#26126;&#26159;&#21487;&#25910;&#25947;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20844;&#24179;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65292;&#20854;&#20013;&#19968;&#20010;&#20195;&#29702;&#24517;&#39035;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#22312;&#22810;&#20010;&#32500;&#24230;&#30340;&#21521;&#37327;&#20540;&#22870;&#21169;&#19978;&#21516;&#26102;&#33719;&#24471;&#39640;&#22238;&#25253;&#30340;&#31574;&#30053;&#12290;&#21463;&#20844;&#24179;&#36164;&#28304;&#20998;&#37197;&#25991;&#29486;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#20854;&#24314;&#27169;&#20026;&#26399;&#26395;&#31119;&#21033;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#38024;&#23545;&#21521;&#37327;&#30340;&#38271;&#26399;&#32047;&#31215;&#22870;&#21169;&#30340;&#38750;&#32447;&#24615;&#20844;&#24179;&#31119;&#21033;&#20989;&#25968;&#12290;&#20854;&#20013;&#19968;&#20010;&#32463;&#20856;&#30340;&#20363;&#23376;&#26159;&#32435;&#20160;&#31038;&#20250;&#31119;&#21033;&#20989;&#25968;&#65292;&#25110;&#32773;&#20960;&#20309;&#24179;&#22343;&#25968;&#65292;&#20854;&#23545;&#25968;&#21464;&#25442;&#20063;&#34987;&#31216;&#20026;&#27604;&#20363;&#20844;&#24179;&#30446;&#26631;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#34920;&#26684;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#26399;&#26395;&#32435;&#20160;&#31038;&#20250;&#31119;&#21033;&#36827;&#34892;&#36817;&#20284;&#26368;&#20248;&#21270;&#20063;&#26159;&#35745;&#31639;&#19978;&#38590;&#20197;&#22788;&#29702;&#30340;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;Q-learning&#25913;&#36827;&#26041;&#27861;&#65292;&#32467;&#21512;&#38750;&#32447;&#24615;&#26631;&#37327;&#21270;&#23398;&#20064;&#26356;&#26032;&#21644;&#38750;&#31283;&#24577;&#21160;&#20316;&#36873;&#25321;&#65292;&#20197;&#23398;&#20064;&#26377;&#25928;&#30340;&#20248;&#21270;&#38750;&#32447;&#24615;&#31119;&#21033;&#20989;&#25968;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#21487;&#25910;&#25947;&#30340;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study fair multi-objective reinforcement learning in which an agent must learn a policy that simultaneously achieves high reward on multiple dimensions of a vector-valued reward. Motivated by the fair resource allocation literature, we model this as an expected welfare maximization problem, for some non-linear fair welfare function of the vector of long-term cumulative rewards. One canonical example of such a function is the Nash Social Welfare, or geometric mean, the log transform of which is also known as the Proportional Fairness objective. We show that even approximately optimal optimization of the expected Nash Social Welfare is computationally intractable even in the tabular case. Nevertheless, we provide a novel adaptation of Q-learning that combines non-linear scalarized learning updates and non-stationary action selection to learn effective policies for optimizing nonlinear welfare functions. We show that our algorithm is provably convergent, and we demonstrate experimental
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#19988;&#39640;&#36136;&#37327;&#30340;&#20154;&#33041;&#21151;&#33021;&#32593;&#32476;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#31070;&#32463;&#31185;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20998;&#26512;&#30340;&#20132;&#21449;&#30740;&#31350;&#12290;&#23427;&#21033;&#29992;&#35299;&#21078;&#23398;&#21644;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#26469;&#29702;&#35299;&#20154;&#33041;&#30340;&#21151;&#33021;&#36830;&#25509;&#65292;&#24182;&#20855;&#26377;&#35782;&#21035;&#28508;&#22312;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20998;&#26512;&#30740;&#31350;&#22823;&#33041;&#30340;&#32593;&#32476;&#24418;&#24335;&#65292;&#33021;&#22815;&#39044;&#27979;&#36825;&#20123;&#30142;&#30149;&#30340;&#26089;&#26399;&#21457;&#29983;&#12290;&#33041;&#32593;&#32476;&#20197;&#22270;&#24418;&#26041;&#24335;&#34920;&#31034;&#65292;&#20445;&#30041;&#20102;&#20016;&#23500;&#30340;&#32467;&#26500;&#21644;&#20301;&#32622;&#20449;&#24687;&#65292;&#20256;&#32479;&#30340;&#26816;&#26597;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#20123;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#33041;&#32593;&#32476;&#25968;&#25454;&#38480;&#21046;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2211.12421</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#32593;&#32476;&#31070;&#32463;&#31185;&#23398;&#65306;&#20851;&#20110;&#25968;&#25454;&#25910;&#38598;&#19982;&#22522;&#20934;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Network Neuroscience: On Data Collection and Benchmark. (arXiv:2211.12421v3 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#19988;&#39640;&#36136;&#37327;&#30340;&#20154;&#33041;&#21151;&#33021;&#32593;&#32476;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#31070;&#32463;&#31185;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20998;&#26512;&#30340;&#20132;&#21449;&#30740;&#31350;&#12290;&#23427;&#21033;&#29992;&#35299;&#21078;&#23398;&#21644;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#26469;&#29702;&#35299;&#20154;&#33041;&#30340;&#21151;&#33021;&#36830;&#25509;&#65292;&#24182;&#20855;&#26377;&#35782;&#21035;&#28508;&#22312;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20998;&#26512;&#30740;&#31350;&#22823;&#33041;&#30340;&#32593;&#32476;&#24418;&#24335;&#65292;&#33021;&#22815;&#39044;&#27979;&#36825;&#20123;&#30142;&#30149;&#30340;&#26089;&#26399;&#21457;&#29983;&#12290;&#33041;&#32593;&#32476;&#20197;&#22270;&#24418;&#26041;&#24335;&#34920;&#31034;&#65292;&#20445;&#30041;&#20102;&#20016;&#23500;&#30340;&#32467;&#26500;&#21644;&#20301;&#32622;&#20449;&#24687;&#65292;&#20256;&#32479;&#30340;&#26816;&#26597;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#20123;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#33041;&#32593;&#32476;&#25968;&#25454;&#38480;&#21046;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#19988;&#39640;&#36136;&#37327;&#30340;&#20154;&#33041;&#21151;&#33021;&#32593;&#32476;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#31070;&#32463;&#31185;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20998;&#26512;&#30340;&#20132;&#21449;&#30740;&#31350;&#12290;&#35299;&#21078;&#23398;&#21644;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#34987;&#29992;&#20110;&#29702;&#35299;&#20154;&#33041;&#30340;&#21151;&#33021;&#36830;&#25509;&#65292;&#24182;&#19988;&#22312;&#35782;&#21035;&#38463;&#23572;&#33576;&#28023;&#40664;&#27663;&#30151;&#12289;&#24085;&#37329;&#26862;&#30151;&#21644;&#33258;&#38381;&#30151;&#31561;&#28508;&#22312;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#26041;&#38754;&#23588;&#20026;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20998;&#26512;&#30740;&#31350;&#20197;&#33041;&#32593;&#32476;&#30340;&#24418;&#24335;&#26469;&#30740;&#31350;&#22823;&#33041;&#30340;&#26041;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#39044;&#27979;&#36825;&#20123;&#30142;&#30149;&#30340;&#26089;&#26399;&#21457;&#29983;&#12290;&#20316;&#20026;&#19968;&#20010;&#22270;&#24418;&#34920;&#31034;&#30340;&#33041;&#32593;&#32476;&#20445;&#30041;&#20102;&#20016;&#23500;&#30340;&#32467;&#26500;&#21644;&#20301;&#32622;&#20449;&#24687;&#65292;&#20256;&#32479;&#30340;&#26816;&#26597;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#20123;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#33041;&#32593;&#32476;&#25968;&#25454;&#38480;&#21046;&#20102;&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#30340;&#25506;&#32034;&#12290;&#20854;&#20013;&#20027;&#35201;&#30340;&#22256;&#38590;&#22312;&#20110;&#22797;&#26434;&#30340;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive and quality collection of functional human brain \emph{network} data for potential research in the intersection of neuroscience, machine learning, and graph analytics. Anatomical and functional MRI images have been used to understand the functional connectivity of the human brain and are particularly important in identifying underlying neurodegenerative conditions such as Alzheimer's, Parkinson's, and Autism. Recently, the study of the brain in the form of brain networks using machine learning and graph analytics has become increasingly popular, especially to predict the early onset of these conditions. A brain network, represented as a graph, retains rich structural and positional information that traditional examination methods are unable to capture. However, the lack of publicly accessible brain network data prevents researchers from data-driven explorations. One of the main difficulties lies in the complicated domain-specific preprocessing steps 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#30740;&#31350;&#20102;&#22270;&#24418;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#27010;&#24565;&#12289;&#26041;&#27861;&#12289;&#35780;&#20272;&#21450;&#20854;&#24212;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24773;&#20917;&#65292;&#25552;&#20379;&#20102;&#20998;&#31867;&#27861;&#12289;&#32479;&#19968;&#30340;&#31526;&#21495;&#34920;&#31034;&#12289;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#23545;&#21313;&#22235;&#31181;&#26041;&#27861;&#12289;&#20108;&#21313;&#20108;&#20010;&#25968;&#25454;&#38598;&#21644;&#21313;&#20061;&#20010;&#25351;&#26631;&#36827;&#34892;&#20102;&#35752;&#35770;&#21644;&#25972;&#21512;&#12290;&#26410;&#26469;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35299;&#20915;&#24320;&#25918;&#30340;&#25361;&#25112;&#19978;&#12290;</title><link>http://arxiv.org/abs/2210.12089</link><description>&lt;p&gt;
&#22270;&#24418;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#32508;&#36848;: &#23450;&#20041;, &#26041;&#27861;, &#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Survey on Graph Counterfactual Explanations: Definitions, Methods, Evaluation. (arXiv:2210.12089v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12089
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#30740;&#31350;&#20102;&#22270;&#24418;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#27010;&#24565;&#12289;&#26041;&#27861;&#12289;&#35780;&#20272;&#21450;&#20854;&#24212;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24773;&#20917;&#65292;&#25552;&#20379;&#20102;&#20998;&#31867;&#27861;&#12289;&#32479;&#19968;&#30340;&#31526;&#21495;&#34920;&#31034;&#12289;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#23545;&#21313;&#22235;&#31181;&#26041;&#27861;&#12289;&#20108;&#21313;&#20108;&#20010;&#25968;&#25454;&#38598;&#21644;&#21313;&#20061;&#20010;&#25351;&#26631;&#36827;&#34892;&#20102;&#35752;&#35770;&#21644;&#25972;&#21512;&#12290;&#26410;&#26469;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35299;&#20915;&#24320;&#25918;&#30340;&#25361;&#25112;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476; (GNNs) &#22312;&#31038;&#21306;&#26816;&#27979;&#21644;&#20998;&#23376;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#21453;&#20107;&#23454;&#35299;&#37322; (CE) &#25552;&#20379;&#21453;&#20363;&#26469;&#20811;&#26381;&#40657;&#30418;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#38480;&#21046;&#12290;&#30001;&#20110;&#23545;&#22270;&#23398;&#20064;&#30340;&#20851;&#27880;&#19981;&#26029;&#22686;&#38271;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#20851;&#27880; GNNs &#30340; CE &#27010;&#24565;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#38750;&#24120;&#35268;&#30340;&#25163;&#27573;&#65292;&#25552;&#20379;&#20102;&#20998;&#31867;&#27861;&#65292;&#32479;&#19968;&#30340;&#31526;&#21495;&#34920;&#31034;&#65292;&#20197;&#21450;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#21313;&#22235;&#31181;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#20108;&#21313;&#20108;&#20010;&#25968;&#25454;&#38598;&#21644;&#21313;&#20061;&#20010;&#25351;&#26631;&#12290;&#25105;&#20204;&#25972;&#21512;&#20102;&#22823;&#22810;&#25968;&#26041;&#27861;&#21040; GRETEL &#24211;&#20013;&#65292;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#32570;&#28857;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#24320;&#25918;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) perform well in community detection and molecule classification. Counterfactual Explanations (CE) provide counter-examples to overcome the transparency limitations of black-box models. Due to the growing attention in graph learning, we focus on the concepts of CE for GNNs. We analysed the SoA to provide a taxonomy, a uniform notation, and the benchmarking datasets and evaluation metrics. We discuss fourteen methods, their evaluation protocols, twenty-two datasets, and nineteen metrics. We integrated the majority of methods into the GRETEL library to conduct an empirical evaluation to understand their strengths and pitfalls. We highlight open challenges and future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#36125;&#21494;&#26031;&#20808;&#39564;&#36873;&#25321;&#30340;&#27491;&#21017;&#21270;&#25968;&#25454;&#32534;&#31243;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#20837;&#27491;&#21017;&#21270;&#39033;&#26469;&#25552;&#21319;&#25968;&#25454;&#32534;&#31243;&#22312;&#24369;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;&#20302;&#25968;&#25454;&#24773;&#22659;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#26356;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.08677</link><description>&lt;p&gt;
&#33258;&#21160;&#36125;&#21494;&#26031;&#20808;&#39564;&#36873;&#25321;&#30340;&#27491;&#21017;&#21270;&#25968;&#25454;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Regularized Data Programming with Automated Bayesian Prior Selection. (arXiv:2210.08677v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#36125;&#21494;&#26031;&#20808;&#39564;&#36873;&#25321;&#30340;&#27491;&#21017;&#21270;&#25968;&#25454;&#32534;&#31243;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#20837;&#27491;&#21017;&#21270;&#39033;&#26469;&#25552;&#21319;&#25968;&#25454;&#32534;&#31243;&#22312;&#24369;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;&#20302;&#25968;&#25454;&#24773;&#22659;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#26356;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#25163;&#21160;&#25968;&#25454;&#26631;&#35760;&#30340;&#25104;&#26412;&#21487;&#33021;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38556;&#30861;&#12290;&#25968;&#25454;&#32534;&#31243;&#65288;DP&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#65292;&#20854;&#20013;&#29992;&#25143;&#23450;&#20041;&#30340;&#32534;&#31243;&#26631;&#31614;&#20989;&#25968;&#65288;LFs&#65289;&#30340;&#36755;&#20986;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#35843;&#21644;&#12290;&#28982;&#32780;&#65292;DP&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#33021;&#26080;&#27861;&#32988;&#36807;&#21152;&#26435;&#22810;&#25968;&#25237;&#31080;&#65292;&#21253;&#25324;&#20302;&#25968;&#25454;&#24773;&#22659;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#25193;&#23637;&#30340;&#32463;&#20856;DP&#65292;&#36890;&#36807;&#22686;&#21152;&#27491;&#21017;&#21270;&#39033;&#26469;&#20943;&#36731;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#22833;&#36133;&#12290;&#36890;&#36807;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#21644;&#20449;&#24687;&#20808;&#39564;&#23454;&#29616;&#20102;&#27491;&#21017;&#21270;&#23398;&#20064;&#12290;&#25552;&#20986;&#21033;&#29992;&#22810;&#25968;&#25237;&#31080;&#20316;&#20026;&#33258;&#21160;&#20808;&#39564;&#21442;&#25968;&#36873;&#25321;&#30340;&#20195;&#29702;&#20449;&#21495;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27491;&#21017;&#21270;&#30340;DP&#30456;&#23545;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#21644;&#22810;&#25968;&#25237;&#31080;&#21487;&#20197;&#25913;&#21892;&#24615;&#33021;&#65292;&#25552;&#20379;&#26356;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#20302;&#25968;&#25454;&#24773;&#22659;&#20013;&#22686;&#24378;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cost of manual data labeling can be a significant obstacle in supervised learning. Data programming (DP) offers a weakly supervised solution for training dataset creation, wherein the outputs of user-defined programmatic labeling functions (LFs) are reconciled through unsupervised learning. However, DP can fail to outperform an unweighted majority vote in some scenarios, including low-data contexts. This work introduces a Bayesian extension of classical DP that mitigates failures of unsupervised learning by augmenting the DP objective with regularization terms. Regularized learning is achieved through maximum a posteriori estimation with informative priors. Majority vote is proposed as a proxy signal for automated prior parameter selection. Results suggest that regularized DP improves performance relative to maximum likelihood and majority voting, confers greater interpretability, and bolsters performance in low-data regimes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#32959;&#30244;&#20301;&#32622;&#27010;&#29575;&#26469;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#20799;&#31461;&#20302;&#32423;&#21035;&#33014;&#36136;&#30244;&#20998;&#23376;&#20122;&#22411;&#37492;&#23450;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.07287</link><description>&lt;p&gt;
&#20351;&#29992;&#32959;&#30244;&#20301;&#32622;&#30340;3D&#27010;&#29575;&#20998;&#24067;&#25913;&#36827;&#20799;&#31461;&#20302;&#32423;&#21035;&#33014;&#36136;&#30244;&#20998;&#23376;&#20122;&#22411;&#37492;&#23450;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Deep Learning Models for Pediatric Low-Grade Glioma Tumors Molecular Subtype Identification Using 3D Probability Distributions of Tumor Location. (arXiv:2210.07287v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#32959;&#30244;&#20301;&#32622;&#27010;&#29575;&#26469;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#20799;&#31461;&#20302;&#32423;&#21035;&#33014;&#36136;&#30244;&#20998;&#23376;&#20122;&#22411;&#37492;&#23450;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#19982;&#30446;&#30340;&#65306;&#20799;&#31461;&#20302;&#32423;&#21035;&#33014;&#36136;&#30244;&#65288;pLGG&#65289;&#26159;&#20799;&#31461;&#26368;&#24120;&#35265;&#30340;&#33041;&#32959;&#30244;&#31867;&#22411;&#65292;&#23547;&#25214;pLGG&#30340;&#20998;&#23376;&#26631;&#35760;&#23545;&#20110;&#25104;&#21151;&#30340;&#27835;&#30103;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#27169;&#22411;&#29992;&#20110;pLGG&#20122;&#22411;&#37492;&#23450;&#20381;&#36182;&#20110;&#32959;&#30244;&#20998;&#21106;&#12290;&#25105;&#20204;&#20551;&#35774;&#32959;&#30244;&#20998;&#21106;&#26159;&#27425;&#20248;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;MRI&#25968;&#25454;&#20013;&#30340;&#32959;&#30244;&#20301;&#32622;&#27010;&#29575;&#26469;&#22686;&#24378;CNN&#27169;&#22411;&#12290;&#26448;&#26009;&#19982;&#26041;&#27861;&#65306;&#25105;&#20204;&#30340;&#32463;&#30740;&#31350;&#20262;&#29702;&#22996;&#21592;&#20250;&#25209;&#20934;&#30340;&#22238;&#39038;&#24615;&#30740;&#31350;&#21253;&#25324;&#20102;143&#20363;BRAF&#34701;&#21512;&#22411;&#21644;71&#20363;BRAF V600E&#31361;&#21464;&#22411;&#32959;&#30244;&#30340;MRI FLAIR&#24207;&#21015;&#12290;&#32959;&#30244;&#20998;&#21106;&#65288;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;ROI&#65289;&#65289;&#30001;&#19968;&#21517;&#20799;&#31461;&#31070;&#32463;&#25918;&#23556;&#21307;&#23398;&#30740;&#31350;&#21592;&#25552;&#20379;&#65292;&#24182;&#30001;&#19968;&#21517;&#39640;&#32423;&#20799;&#31461;&#31070;&#32463;&#25918;&#23556;&#21307;&#23398;&#30740;&#31350;&#21592;&#36827;&#34892;&#26680;&#23454;&#12290;&#22312;&#27599;&#20010;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#38543;&#26426;&#20998;&#25104;&#24320;&#21457;&#38598;&#21644;&#27979;&#35797;&#38598;&#65292;&#27604;&#20363;&#20026;80/20&#12290;&#25105;&#20204;&#23558;&#24320;&#21457;&#38598;&#20013;&#27599;&#20010;&#31867;&#21035;&#30340;3D&#20108;&#36827;&#21046;ROI&#25513;&#27169;&#32452;&#21512;&#36215;&#26469;&#65292;&#20197;&#24471;&#21040;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65288;
&lt;/p&gt;
&lt;p&gt;
Background and Purpose: Pediatric low-grade glioma (pLGG) is the most common type of brain tumor in children, and identification of molecular markers for pLGG is crucial for successful treatment planning. Convolutional Neural Network (CNN) models for pLGG subtype identification rely on tumor segmentation. We hypothesize tumor segmentations are suboptimal and thus, we propose to augment the CNN models using tumor location probability in MRI data.  Materials and Methods: Our REB-approved retrospective study included MRI Fluid-Attenuated Inversion Recovery (FLAIR) sequences of 143 BRAF fused and 71 BRAF V600E mutated tumors. Tumor segmentations (regions of interest (ROIs)) were provided by a pediatric neuroradiology fellow and verified by a senior pediatric neuroradiologist. In each experiment, we randomly split the data into development and test with an 80/20 ratio. We combined the 3D binary ROI masks for each class in the development dataset to derive the probability density functions (
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27969;&#31243;&#65292;&#22312;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#20013;&#21462;&#24471;&#20102;&#21487;&#38752;&#30340;&#32467;&#26524;&#65292;&#19982;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#30456;&#20284;&#30340;&#27979;&#37327;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#24322;&#24120;&#24515;&#33108;&#22823;&#23567;&#21644;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.04979</link><description>&lt;p&gt;
&#26080;&#26631;&#31614;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Label-free segmentation from cardiac ultrasound using self-supervised learning. (arXiv:2210.04979v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27969;&#31243;&#65292;&#22312;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#20013;&#21462;&#24471;&#20102;&#21487;&#38752;&#30340;&#32467;&#26524;&#65292;&#19982;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#30456;&#20284;&#30340;&#27979;&#37327;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#24322;&#24120;&#24515;&#33108;&#22823;&#23567;&#21644;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#30340;&#20998;&#21106;&#21644;&#27979;&#37327;&#23545;&#20110;&#24515;&#33039;&#36229;&#22768;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26159;&#36825;&#20123;&#20219;&#21153;&#32791;&#26102;&#19988;&#38590;&#20197;&#37325;&#29616;&#12290;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#25552;&#20379;&#36741;&#21161;&#65292;&#20294;&#26159;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#20154;&#21147;&#36827;&#34892;&#25163;&#21160;&#26631;&#27880;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27969;&#31243;&#65292;&#32467;&#21512;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#20020;&#24202;&#39046;&#22495;&#30693;&#35782;&#21644;&#28145;&#24230;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;450&#20010;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#65288;93000&#24352;&#22270;&#29255;&#65289;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#24182;&#22312;8393&#20010;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#65288;4476266&#24352;&#22270;&#29255;&#65292;&#24179;&#22343;&#24180;&#40836;61&#23681;&#65292;&#22899;&#24615;&#21344;51%&#65289;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21033;&#29992;&#20998;&#21106;&#32467;&#26524;&#36827;&#34892;&#29983;&#29289;&#27979;&#37327;&#12290;&#25105;&#20204;&#36824;&#23545;&#26469;&#33258;&#39069;&#22806;10030&#21517;&#24739;&#32773;&#30340;&#22806;&#37096;&#22270;&#20687;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#36825;&#20123;&#22270;&#20687;&#20855;&#26377;&#25163;&#21160;&#25551;&#36857;&#30340;&#24038;&#23460;&#20449;&#24687;&#12290;&#22312;&#20960;&#31181;&#19981;&#21516;&#30340;&#27979;&#37327;&#25351;&#26631;&#65288;r2 0.56-0.84&#65289;&#19978;&#65292;&#20020;&#24202;&#27979;&#37327;&#21644;&#25105;&#20204;&#30340;&#27969;&#31243;&#39044;&#27979;&#20043;&#38388;&#30340;r2&#20540;&#19982;&#24050;&#25253;&#36947;&#30340;&#20020;&#24202;&#21307;&#29983;&#20043;&#38388;&#30340;&#21464;&#24322;&#31243;&#24230;&#30456;&#20284;&#65292;&#24182;&#19988;&#19982;&#30417;&#30563;&#23398;&#20064;&#30340;&#32467;&#26524;&#30456;&#24403;&#12290;&#26816;&#27979;&#24322;&#24120;&#24515;&#33108;&#22823;&#23567;&#21644;&#21151;&#33021;&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#20026;0.85&#65288;&#33539;&#22260;0.71-0.97&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Segmentation and measurement of cardiac chambers is critical in cardiac ultrasound but is laborious and poorly reproducible. Neural networks can assist, but supervised approaches require the same laborious manual annotations. We built a pipeline for self-supervised (no manual labels) segmentation combining computer vision, clinical domain knowledge, and deep learning. We trained on 450 echocardiograms (93,000 images) and tested on 8,393 echocardiograms (4,476,266 images; mean 61 years, 51% female), using the resulting segmentations to calculate biometrics. We also tested against external images from an additional 10,030 patients with available manual tracings of the left ventricle. r2 between clinically measured and pipeline-predicted measurements were similar to reported inter-clinician variation and comparable to supervised learning across several different measurements (r2 0.56-0.84). Average accuracy for detecting abnormal chamber size and function was 0.85 (range 0.71-0.97) compar
&lt;/p&gt;</description></item><item><title>GLM-130B&#26159;&#19968;&#20010;&#20855;&#26377;1300&#20159;&#21442;&#25968;&#30340;&#24320;&#28304;&#21452;&#35821;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#33021;&#22815;&#36229;&#36234;GPT-3&#21644;&#26368;&#22823;&#30340;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;ERNIE TITAN 3.0 260B&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2210.02414</link><description>&lt;p&gt;
GLM-130B: &#19968;&#20010;&#24320;&#28304;&#30340;&#21452;&#35821;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GLM-130B: An Open Bilingual Pre-trained Model. (arXiv:2210.02414v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02414
&lt;/p&gt;
&lt;p&gt;
GLM-130B&#26159;&#19968;&#20010;&#20855;&#26377;1300&#20159;&#21442;&#25968;&#30340;&#24320;&#28304;&#21452;&#35821;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#33021;&#22815;&#36229;&#36234;GPT-3&#21644;&#26368;&#22823;&#30340;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;ERNIE TITAN 3.0 260B&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GLM-130B&#65292;&#19968;&#20010;&#20855;&#26377;1300&#20159;&#21442;&#25968;&#30340;&#21452;&#35821;&#65288;&#33521;&#35821;&#21644;&#20013;&#25991;&#65289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#26159;&#20026;&#20102;&#25171;&#24320;1000&#20159;&#35268;&#27169;&#30340;&#27169;&#22411;&#65292;&#33267;&#23569;&#19982;GPT-3&#65288;davinci&#65289;&#19968;&#26679;&#22909;&#65292;&#24182;&#25581;&#31034;&#22914;&#20309;&#25104;&#21151;&#22320;&#36827;&#34892;&#22914;&#27492;&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36935;&#21040;&#20102;&#35768;&#22810;&#24847;&#22806;&#30340;&#25216;&#26415;&#21644;&#24037;&#31243;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#25439;&#22833;&#23792;&#21644;&#21457;&#25955;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GLM-130B&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#21253;&#25324;&#35774;&#35745;&#36873;&#25321;&#12289;&#25552;&#39640;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#21450;&#24037;&#31243;&#21162;&#21147;&#12290;GLM-130B&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;&#33521;&#35821;&#22522;&#20934;&#27979;&#35797;&#20013;&#26126;&#26174;&#20248;&#20110;GPT-3 175B&#65288;davinci&#65289;&#65292;&#20294;&#22312;OPT-175B&#21644;BLOOM-176B&#20013;&#27809;&#26377;&#35266;&#23519;&#21040;&#24615;&#33021;&#20248;&#21183;&#12290;&#23427;&#36824;&#22312;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#20013;&#22987;&#32456;&#19988;&#26174;&#33879;&#20248;&#20110;&#26368;&#22823;&#30340;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;ERNIE TITAN 3.0 260B&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;GLM-130B&#30340;&#29420;&#29305;&#32553;&#25918;&#24615;&#33021;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and divergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B (davinci) on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to rea
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#23558;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#22320;&#28909;&#36164;&#28304;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#21644;&#24050;&#30693;&#29305;&#24449;&#22270;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#21487;&#29992;&#20110;&#25214;&#20986;&#28508;&#21147;&#26356;&#39640;&#30340;&#21306;&#22495;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#35843;&#26597;&#12290;</title><link>http://arxiv.org/abs/2209.15543</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#22312;&#22320;&#28909;&#36164;&#28304;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;&#65306;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Bayesian Neural Networks for Geothermal Resource Assessment: Prediction with Uncertainty. (arXiv:2209.15543v2 [physics.geo-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#23558;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#22320;&#28909;&#36164;&#28304;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#21644;&#24050;&#30693;&#29305;&#24449;&#22270;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#21487;&#29992;&#20110;&#25214;&#20986;&#28508;&#21147;&#26356;&#39640;&#30340;&#21306;&#22495;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#22320;&#28909;&#36164;&#28304;&#28508;&#21147;&#35780;&#20272;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#21033;&#29992;&#32654;&#22269;&#20869;&#21326;&#36798;&#24030;&#30340;10&#20010;&#22320;&#36136;&#21644;&#22320;&#29699;&#29289;&#29702;&#29305;&#24449;&#22270;&#26469;&#30028;&#23450;&#24191;&#27867;&#21306;&#22495;&#20869;&#30340;&#22320;&#28909;&#28508;&#21147;&#12290;&#25105;&#20204;&#26377;&#19968;&#32452;&#30456;&#23545;&#36739;&#23567;&#30340;&#27491;&#26679;&#26412;&#35757;&#32451;&#28857;&#65288;&#24050;&#30693;&#36164;&#28304;&#25110;&#27963;&#36291;&#21457;&#30005;&#21378;&#65289;&#21644;&#36127;&#26679;&#26412;&#35757;&#32451;&#28857;&#65288;&#24050;&#30693;&#38075;&#25506;&#28857;&#20294;&#23384;&#22312;&#19981;&#36866;&#21512;&#30340;&#22320;&#28909;&#26465;&#20214;&#65289;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#35757;&#32451;&#28857;&#26469;&#32422;&#26463;&#21644;&#20248;&#21270;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#24050;&#30693;&#23450;&#20041;&#29305;&#24449;&#30340;&#22823;&#38754;&#31215;&#22320;&#29702;&#21306;&#22495;&#20869;&#39044;&#27979;&#26410;&#30693;&#28857;&#30340;&#22320;&#28909;&#36164;&#28304;&#28508;&#21147;&#12290;&#36825;&#20123;&#39044;&#27979;&#21487;&#20197;&#29992;&#20110;&#30830;&#23450;&#26377;&#21069;&#26223;&#30340;&#21306;&#22495;&#36827;&#34892;&#35814;&#32454;&#35843;&#26597;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#30340;&#24037;&#20316;&#20174;&#23450;&#20041;&#29305;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21040;&#35757;&#32451;&#21644;&#20248;&#21270;&#35797;&#39564;&#30340;&#28436;&#21464;&#12290;&#36890;&#36807;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19981;&#21487;&#36991;&#20813;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the application of machine learning to the evaluation of geothermal resource potential. A supervised learning problem is defined where maps of 10 geological and geophysical features within the state of Nevada, USA are used to define geothermal potential across a broad region. We have available a relatively small set of positive training sites (known resources or active power plants) and negative training sites (known drill sites with unsuitable geothermal conditions) and use these to constrain and optimize artificial neural networks for this classification task. The main objective is to predict the geothermal resource potential at unknown sites within a large geographic area where the defining features are known. These predictions could be used to target promising areas for further detailed investigations. We describe the evolution of our work from defining a specific neural network architecture to training and optimization trials. Upon analysis we expose the inevitable pro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#37096;&#20998;&#35266;&#27979;&#36712;&#36857;&#20013;&#23398;&#20064;&#20316;&#29992;Koopman&#29983;&#25104;&#22120;&#30340;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#22522;&#20989;&#25968;&#36873;&#25321;&#21644;&#35266;&#27979;&#19981;&#23436;&#25972;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2209.09977</link><description>&lt;p&gt;
&#20174;&#37096;&#20998;&#35266;&#27979;&#36712;&#36857;&#20013;&#23398;&#20064;&#20316;&#29992;Koopman&#29983;&#25104;&#22120;&#30340;&#21452;&#32447;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Bilinear Models of Actuated Koopman Generators from Partially-Observed Trajectories. (arXiv:2209.09977v2 [math.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09977
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#37096;&#20998;&#35266;&#27979;&#36712;&#36857;&#20013;&#23398;&#20064;&#20316;&#29992;Koopman&#29983;&#25104;&#22120;&#30340;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#22522;&#20989;&#25968;&#36873;&#25321;&#21644;&#35266;&#27979;&#19981;&#23436;&#25972;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36924;&#36817;&#28508;&#22312;&#30340;Koopman&#31639;&#23376;&#25110;&#29983;&#25104;&#22120;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#25104;&#21151;&#30340;&#39044;&#27979;&#12289;&#29305;&#24449;&#23398;&#20064;&#12289;&#29366;&#24577;&#20272;&#35745;&#21644;&#25511;&#21046;&#24037;&#20855;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#25511;&#21046;&#20223;&#23556;&#31995;&#32479;&#30340;Koopman&#29983;&#25104;&#22120;&#22312;&#36755;&#20837;&#26041;&#38754;&#20063;&#20855;&#26377;&#20223;&#23556;&#20381;&#36182;&#24615;&#65292;&#36827;&#32780;&#23548;&#33268;&#26041;&#20415;&#30340;&#26377;&#38480;&#32500;&#21452;&#32447;&#24615;&#36817;&#20284;&#21160;&#21147;&#23398;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;&#36817;&#20284;&#20855;&#26377;&#20316;&#29992;&#30340;Koopman&#29983;&#25104;&#22120;&#30340;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38556;&#30861;&#12290;&#39318;&#20808;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#36873;&#25321;&#29992;&#20110;&#36924;&#36817;Koopman&#29983;&#25104;&#22120;&#30340;&#22522;&#20989;&#25968;&#65292;&#32780;&#23545;&#20110;&#38750;&#27979;&#24230;&#20445;&#25345;&#30340;&#31995;&#32479;&#30446;&#21069;&#27809;&#26377;&#26222;&#36866;&#30340;&#36873;&#25321;&#26041;&#24335;&#12290;&#20854;&#27425;&#65292;&#22914;&#26524;&#25105;&#20204;&#27809;&#26377;&#35266;&#27979;&#21040;&#23436;&#25972;&#30340;&#29366;&#24577;&#65292;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#36275;&#22815;&#20016;&#23500;&#30340;&#36825;&#31867;&#20989;&#25968;&#38598;&#21512;&#26469;&#25551;&#36848;&#21160;&#24577;&#12290;&#36825;&#26159;&#22240;&#20026;&#36890;&#24120;&#24773;&#20917;&#19979;&#25105;&#20204;&#26080;&#27861;&#33719;&#24471;&#25551;&#36848;&#21160;&#24577;&#25152;&#38656;&#30340;&#36275;&#22815;&#20016;&#23500;&#30340;&#20989;&#25968;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven models for nonlinear dynamical systems based on approximating the underlying Koopman operator or generator have proven to be successful tools for forecasting, feature learning, state estimation, and control. It has become well known that the Koopman generators for control-affine systems also have affine dependence on the input, leading to convenient finite-dimensional bilinear approximations of the dynamics. Yet there are still two main obstacles that limit the scope of current approaches for approximating the Koopman generators of systems with actuation. First, the performance of existing methods depends heavily on the choice of basis functions over which the Koopman generator is to be approximated; and there is currently no universal way to choose them for systems that are not measure preserving. Secondly, if we do not observe the full state, we may not gain access to a sufficiently rich collection of such functions to describe the dynamics. This is because the commonly u
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FLASH&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#25277;&#22870;&#24863;&#30693;&#30340;&#31232;&#30095;&#24230;&#25628;&#32034;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#36793;&#32536;&#29615;&#22659;&#20013;&#35757;&#32451;&#31232;&#30095;&#23376;&#27169;&#22411;&#65292;&#20174;&#32780;&#20445;&#25345;&#24615;&#33021;&#24182;&#33719;&#24471;&#30456;&#24212;&#30340;&#36890;&#20449;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2208.13092</link><description>&lt;p&gt;
&#25277;&#22870;&#24863;&#30693;&#30340;&#31232;&#30095;&#24230;&#25628;&#32034;: &#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#36793;&#32536;&#29615;&#22659;&#20013;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Lottery Aware Sparsity Hunting: Enabling Federated Learning on Resource-Limited Edge. (arXiv:2208.13092v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.13092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FLASH&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#25277;&#22870;&#24863;&#30693;&#30340;&#31232;&#30095;&#24230;&#25628;&#32034;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#36793;&#32536;&#29615;&#22659;&#20013;&#35757;&#32451;&#31232;&#30095;&#23376;&#27169;&#22411;&#65292;&#20174;&#32780;&#20445;&#25345;&#24615;&#33021;&#24182;&#33719;&#24471;&#30456;&#24212;&#30340;&#36890;&#20449;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36793;&#32536;&#35774;&#22791;&#30340;&#20998;&#24067;&#29305;&#24615;&#65292;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#26497;&#22823;&#22320;&#21463;&#30410;&#20110;&#23427;&#20204;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#36164;&#28304;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#38480;&#21046;&#65292;&#37096;&#32626;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#21487;&#33021;&#30340;&#26041;&#27861;&#26159;&#22312;&#23458;&#25143;&#31471;&#21033;&#29992;&#29616;&#25104;&#30340;&#31232;&#30095;&#23398;&#20064;&#31639;&#27861;&#26469;&#28385;&#36275;&#23427;&#20204;&#30340;&#36164;&#28304;&#39044;&#31639;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#26420;&#32032;&#22320;&#22312;&#23458;&#25143;&#31471;&#37096;&#32626;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#20934;&#30830;&#24230;&#19979;&#38477;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#23458;&#25143;&#31471;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#31232;&#30095;&#24230;&#25513;&#30721;&#32570;&#20047;&#20849;&#35782;&#21487;&#33021;&#20250;&#20943;&#24930;&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#24182;&#23548;&#33268;&#26174;&#33879;&#30340;&#20934;&#30830;&#24230;&#19979;&#38477;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#31232;&#30095;&#23398;&#20064;&#26694;&#26550;&#8212;&#8212;&#32852;&#37030;&#25277;&#22870;&#24863;&#30693;&#31232;&#30095;&#24230;&#25628;&#32034;(FLASH)&#65292;&#29992;&#20110;&#35757;&#32451;&#19968;&#20010;&#22312;&#36229;&#20302;&#21442;&#25968;&#23494;&#24230;&#19979;&#20445;&#25345;&#24615;&#33021;&#30340;&#31232;&#30095;&#23376;&#27169;&#22411;&#65292;&#21516;&#26102;&#20135;&#29983;&#30456;&#24212;&#30340;&#36890;&#20449;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Edge devices can benefit remarkably from federated learning due to their distributed nature; however, their limited resource and computing power poses limitations in deployment. A possible solution to this problem is to utilize off-the-shelf sparse learning algorithms at the clients to meet their resource budget. However, such naive deployment in the clients causes significant accuracy degradation, especially for highly resource-constrained clients. In particular, our investigations reveal that the lack of consensus in the sparsity masks among the clients may potentially slow down the convergence of the global model and cause a substantial accuracy drop. With these observations, we present \textit{federated lottery aware sparsity hunting} (FLASH), a unified sparse learning framework for training a sparse sub-model that maintains the performance under ultra-low parameter density while yielding proportional communication benefits. Moreover, given that different clients may have different
&lt;/p&gt;</description></item><item><title>&#38544;&#24335;&#21452;&#22612;&#31574;&#30053;&#65288;ITT&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#21270;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#20307;&#31995;&#65292;&#36890;&#36807;&#22312;&#31574;&#30053;&#22534;&#26632;&#20013;&#26174;&#24335;&#21306;&#20998;&#21160;&#20316;&#21644;&#29366;&#24577;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#25928;&#30410;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#22312;&#40657;&#30418;/&#36827;&#21270;&#20248;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2208.01191</link><description>&lt;p&gt;
&#38544;&#24335;&#21452;&#22612;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Implicit Two-Tower Policies. (arXiv:2208.01191v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.01191
&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#21452;&#22612;&#31574;&#30053;&#65288;ITT&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#21270;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#20307;&#31995;&#65292;&#36890;&#36807;&#22312;&#31574;&#30053;&#22534;&#26632;&#20013;&#26174;&#24335;&#21306;&#20998;&#21160;&#20316;&#21644;&#29366;&#24577;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#25928;&#30410;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#22312;&#40657;&#30418;/&#36827;&#21270;&#20248;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#21270;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#20307;&#31995;&#65292;&#21363;&#38544;&#24335;&#21452;&#22612;&#65288;ITT&#65289;&#31574;&#30053;&#65292;&#20854;&#20013;&#21160;&#20316;&#22522;&#20110;&#20854;&#21487;&#23398;&#20064;&#30340;&#28508;&#22312;&#34920;&#31034;&#19982;&#36755;&#20837;&#29366;&#24577;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#36827;&#34892;&#36873;&#25321;&#12290;&#36890;&#36807;&#22312;&#31574;&#30053;&#22534;&#26632;&#20013;&#26174;&#24335;&#21306;&#20998;&#21160;&#20316;&#21644;&#29366;&#24577;&#22788;&#29702;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20004;&#20010;&#20027;&#35201;&#30446;&#26631;&#65306;&#26174;&#33879;&#30340;&#35745;&#31639;&#25928;&#30410;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#36866;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#12290;&#36890;&#36807;&#22312;OpenAI Gym&#21644;DeepMind Control Suite&#30340;15&#20010;&#29615;&#22659;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ITT&#26550;&#26500;&#29305;&#21035;&#36866;&#29992;&#20110;&#40657;&#30418;/&#36827;&#21270;&#20248;&#21270;&#65292;&#30456;&#24212;&#30340;&#31574;&#30053;&#35757;&#32451;&#31639;&#27861;&#20248;&#20110;&#20854;&#33609;&#29575;&#30340;&#38544;&#24335;&#23545;&#24212;&#29289;&#20197;&#21450;&#24120;&#29992;&#30340;&#26174;&#24335;&#31574;&#30053;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#22914;&#20309;&#24212;&#29992;&#21704;&#24076;&#21644;&#24816;&#24615;&#22612;&#26356;&#26032;&#31561;&#25216;&#26415;&#65292;&#20851;&#38190;&#20381;&#36182;&#20110;ITT&#30340;&#21452;&#22612;&#32467;&#26500;&#65292;&#26469;&#34917;&#20805;&#25105;&#20204;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new class of structured reinforcement learning policy-architectures, Implicit Two-Tower (ITT) policies, where the actions are chosen based on the attention scores of their learnable latent representations with those of the input states. By explicitly disentangling action from state processing in the policy stack, we achieve two main goals: substantial computational gains and better performance. Our architectures are compatible with both: discrete and continuous action spaces. By conducting tests on 15 environments from OpenAI Gym and DeepMind Control Suite, we show that ITT-architectures are particularly suited for blackbox/evolutionary optimization and the corresponding policy training algorithms outperform their vanilla unstructured implicit counterparts as well as commonly used explicit policies. We complement our analysis by showing how techniques such as hashing and lazy tower updates, critically relying on the two-tower structure of ITTs, can be applied to obtain add
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#22871;&#24320;&#25918;&#25918;&#23556;&#32452;&#23398;&#25968;&#25454;&#38598;&#21644;&#25216;&#26415;&#21327;&#35758;&#65292;&#26088;&#22312;&#35299;&#20915;&#25918;&#23556;&#32452;&#23398;&#22312;&#32467;&#26524;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#35775;&#38382;&#24615;&#26041;&#38754;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#22312;BraTS 2020&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#25918;&#23556;&#32452;&#23398;&#29305;&#24449;&#25552;&#21462;&#23545;&#32467;&#26524;&#21487;&#37325;&#22797;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2207.14776</link><description>&lt;p&gt;
&#24320;&#25918;&#25918;&#23556;&#32452;&#23398;&#65306;&#19968;&#31995;&#21015;&#26631;&#20934;&#21270;&#25968;&#25454;&#38598;&#21644;&#21487;&#37325;&#22797;&#25918;&#23556;&#32452;&#23398;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#25216;&#26415;&#21327;&#35758;
&lt;/p&gt;
&lt;p&gt;
Open-radiomics: A Collection of Standardized Datasets and a Technical Protocol for Reproducible Radiomics Machine Learning Pipelines. (arXiv:2207.14776v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#22871;&#24320;&#25918;&#25918;&#23556;&#32452;&#23398;&#25968;&#25454;&#38598;&#21644;&#25216;&#26415;&#21327;&#35758;&#65292;&#26088;&#22312;&#35299;&#20915;&#25918;&#23556;&#32452;&#23398;&#22312;&#32467;&#26524;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#35775;&#38382;&#24615;&#26041;&#38754;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#22312;BraTS 2020&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#25918;&#23556;&#32452;&#23398;&#29305;&#24449;&#25552;&#21462;&#23545;&#32467;&#26524;&#21487;&#37325;&#22797;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#20316;&#20026;&#21307;&#23398;&#24433;&#20687;&#20013;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#19968;&#20010;&#37325;&#35201;&#20998;&#25903;&#65292;&#25918;&#23556;&#32452;&#23398;&#38754;&#20020;&#30528;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#21363;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24320;&#25918;&#25918;&#23556;&#32452;&#23398;&#65292;&#19968;&#22871;&#25918;&#23556;&#32452;&#23398;&#25968;&#25454;&#38598;&#20197;&#21450;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#21327;&#35758;&#30340;&#32508;&#21512;&#25918;&#23556;&#32452;&#23398;&#27969;&#31243;&#65292;&#20197;&#30740;&#31350;&#25918;&#23556;&#32452;&#23398;&#29305;&#24449;&#25552;&#21462;&#23545;&#32467;&#26524;&#21487;&#37325;&#22797;&#24615;&#30340;&#24433;&#21709;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#23454;&#39564;&#20351;&#29992;BraTS 2020&#24320;&#28304;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#65292;&#21253;&#25324;369&#21517;&#24739;&#26377;&#33041;&#32959;&#30244;&#30340;&#25104;&#24180;&#24739;&#32773;&#65288;76&#20363;&#20302;&#32423;&#21035;&#33014;&#36136;&#30244;&#65288;LGG&#65289;&#21644;293&#20363;&#39640;&#32423;&#21035;&#33014;&#36136;&#30244;&#65288;HGG&#65289;&#65289;&#12290;&#20351;&#29992;PyRadiomics&#24211;&#36827;&#34892;LGG&#19982;HGG&#20998;&#31867;&#65292;&#24418;&#25104;&#20102;288&#20010;&#25918;&#23556;&#32452;&#23398;&#25968;&#25454;&#38598;&#65307;&#20854;&#20013;&#21253;&#25324;4&#20010;MRI&#24207;&#21015;&#12289;3&#20010;binWidths&#12289;6&#31181;&#22270;&#20687;&#24402;&#19968;&#21270;&#26041;&#27861;&#21644;4&#20010;&#32959;&#30244;&#27425;&#21306;&#22495;&#30340;&#32452;&#21512;&#12290;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#65292;&#24182;&#20026;&#27599;&#20010;&#25918;&#23556;&#32452;&#23398;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;-&#39564;&#35777;-&#27979;&#35797;&#65288;60%/20%/20%&#65289;&#23454;&#39564;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#21010;&#20998;&#21644;m
&lt;/p&gt;
&lt;p&gt;
Purpose: As an important branch of machine learning pipelines in medical imaging, radiomics faces two major challenges namely reproducibility and accessibility. In this work, we introduce open-radiomics, a set of radiomics datasets along with a comprehensive radiomics pipeline based on our proposed technical protocol to investigate the effects of radiomics feature extraction on the reproducibility of the results.  Materials and Methods: Experiments are conducted on BraTS 2020 open-source Magnetic Resonance Imaging (MRI) dataset that includes 369 adult patients with brain tumors (76 low-grade glioma (LGG), and 293 high-grade glioma (HGG)). Using PyRadiomics library for LGG vs. HGG classification, 288 radiomics datasets are formed; the combinations of 4 MRI sequences, 3 binWidths, 6 image normalization methods, and 4 tumor subregions.  Random Forest classifiers were used, and for each radiomics dataset the training-validation-test (60%/20%/20%) experiment with different data splits and m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#33021;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#21367;&#31215;&#31232;&#30095;&#32534;&#30721;&#23558;&#21151;&#32791;&#20998;&#31867;&#20026;&#26377;&#21151;&#25110;&#32773;&#26080;&#21151;&#65292;&#20197;&#23454;&#29616;&#23545;&#31169;&#20154;&#20303;&#23429;&#23621;&#27665;&#27963;&#21160;&#21644;&#23384;&#22312;&#24773;&#20917;&#30340;&#25253;&#21578;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#33021;&#37327;&#27867;&#20989;&#26368;&#23567;&#21270;&#21644;&#25910;&#25947;&#24615;&#31561;&#26041;&#38754;&#20855;&#26377;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.09785</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#21367;&#31215;&#31232;&#30095;&#32534;&#30721;&#33021;&#37327;&#20998;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised energy disaggregation via convolutional sparse coding. (arXiv:2207.09785v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#33021;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#21367;&#31215;&#31232;&#30095;&#32534;&#30721;&#23558;&#21151;&#32791;&#20998;&#31867;&#20026;&#26377;&#21151;&#25110;&#32773;&#26080;&#21151;&#65292;&#20197;&#23454;&#29616;&#23545;&#31169;&#20154;&#20303;&#23429;&#23621;&#27665;&#27963;&#21160;&#21644;&#23384;&#22312;&#24773;&#20917;&#30340;&#25253;&#21578;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#33021;&#37327;&#27867;&#20989;&#26368;&#23567;&#21270;&#21644;&#25910;&#25947;&#24615;&#31561;&#26041;&#38754;&#20855;&#26377;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31169;&#20154;&#20303;&#23429;&#20013;&#37197;&#22791;&#26234;&#33021;&#30005;&#34920;&#30340;&#26080;&#30417;&#30563;&#33021;&#37327;&#20998;&#35299;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#23558;&#21151;&#32791;&#20998;&#31867;&#20026;&#26377;&#21151;&#25110;&#32773;&#26080;&#21151;&#65292;&#20174;&#32780;&#33021;&#22815;&#25253;&#21578;&#23621;&#27665;&#30340;&#27963;&#21160;&#21644;&#23384;&#22312;&#24773;&#20917;&#65292;&#32780;&#26080;&#38656;&#30452;&#25509;&#20132;&#20114;&#12290;&#36825;&#20026;&#38750;&#20405;&#20837;&#24335;&#31169;&#20154;&#20303;&#23429;&#30340;&#20581;&#24247;&#30417;&#27979;&#31561;&#24212;&#29992;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22522;&#20110;&#26368;&#23567;&#21270;&#36866;&#24403;&#30340;&#33021;&#37327;&#27867;&#20989;&#65292;&#37319;&#29992;&#20102;iPALM&#65288;&#24815;&#24615;&#36817;&#31471;&#20132;&#26367;&#32447;&#24615;&#21270;&#26368;&#23567;&#21270;&#65289;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#28385;&#36275;&#25910;&#25947;&#30340;&#21508;&#31181;&#26465;&#20214;&#12290;&#20026;&#20102;&#30830;&#35748;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#23545;&#21322;&#21512;&#25104;&#27979;&#35797;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, a method for unsupervised energy disaggregation in private households equipped with smart meters is proposed. This method aims to classify power consumption as active or passive, granting the ability to report on the residents' activity and presence without direct interaction. This lays the foundation for applications like non-intrusive health monitoring of private homes.  The proposed method is based on minimizing a suitable energy functional, for which the iPALM (inertial proximal alternating linearized minimization) algorithm is employed, demonstrating that various conditions guaranteeing convergence are satisfied.  In order to confirm feasibility of the proposed method, experiments on semi-synthetic test data sets and a comparison to existing, supervised methods are provided.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20960;&#20309;&#21487;&#35270;&#21270;&#20998;&#26512;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;&#23545;&#32593;&#32476;&#20869;&#37096;&#29305;&#24449;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#24182;&#23545;&#27169;&#22411;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2206.07918</link><description>&lt;p&gt;
"&#29702;&#35299;&#40065;&#26834;&#24615;&#20043;&#24425;&#31080;": &#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;&#30340;&#20960;&#20309;&#21487;&#35270;&#21270;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
"Understanding Robustness Lottery": A Geometric Visual Comparative Analysis of Neural Network Pruning Approaches. (arXiv:2206.07918v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20960;&#20309;&#21487;&#35270;&#21270;&#20998;&#26512;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;&#23545;&#32593;&#32476;&#20869;&#37096;&#29305;&#24449;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#24182;&#23545;&#27169;&#22411;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20381;&#36182;&#22823;&#32780;&#36807;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#32593;&#32476;&#34987;&#35777;&#26126;&#38750;&#24120;&#33030;&#24369;&#65292;&#38590;&#20197;&#37096;&#32626;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24179;&#21488;&#19978;&#12290;&#27169;&#22411;&#21098;&#26525;&#65292;&#21363;&#20943;&#23567;&#32593;&#32476;&#30340;&#35268;&#27169;&#65292;&#26159;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#20135;&#29983;&#26356;&#24378;&#38887;&#21644;&#32039;&#20945;&#30340;&#27169;&#22411;&#12290;&#23384;&#22312;&#35768;&#22810;&#27169;&#22411;&#21098;&#26525;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20294;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#26576;&#20123;&#21551;&#21457;&#24335;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;&#24615;&#33021;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#21487;&#33021;&#20351;&#27169;&#22411;&#26356;&#21152;&#33030;&#24369;&#25110;&#20135;&#29983;&#20854;&#20182;&#21103;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25581;&#31034;&#19981;&#21516;&#21098;&#26525;&#26041;&#27861;&#22914;&#20309;&#25913;&#21464;&#32593;&#32476;&#30340;&#20869;&#37096;&#29305;&#24449;&#34920;&#31034;&#20197;&#21450;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#30456;&#24212;&#24433;&#21709;&#12290;&#20026;&#20102;&#20415;&#20110;&#20840;&#38754;&#27604;&#36739;&#21644;&#25551;&#36848;&#39640;&#32500;&#27169;&#22411;&#29305;&#24449;&#31354;&#38388;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35270;&#21270;&#20960;&#20309;&#20998;&#26512;&#30340;&#29305;&#24449;&#34920;&#31034;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#24120;&#35265;&#30340;&#20960;&#20309;&#27010;&#24565;&#36827;&#34892;&#20102;&#20998;&#35299;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning approaches have provided state-of-the-art performance in many applications by relying on large and overparameterized neural networks. However, such networks have been shown to be very brittle and are difficult to deploy on resource-limited platforms. Model pruning, i.e., reducing the size of the network, is a widely adopted strategy that can lead to a more robust and compact model. Many heuristics exist for model pruning, but empirical studies show that some heuristics improve performance whereas others can make models more brittle or have other side effects. This work aims to shed light on how different pruning methods alter the network's internal feature representation and the corresponding impact on model performance. To facilitate a comprehensive comparison and characterization of the high-dimensional model feature space, we introduce a visual geometric analysis of feature representations. We decomposed and evaluated a set of critical geometric concepts from the commo
&lt;/p&gt;</description></item><item><title>AlphaZero-style reinforcement learning algorithms excel in various board games but face challenges with impartial games. The researchers present a concrete example of the game nim, and show that AlphaZero-style algorithms have difficulty learning these impartial games on larger board sizes. The difference between impartial games and partisan games can be explained by the vulnerability to adversarial attacks and perturbations.</title><link>http://arxiv.org/abs/2205.12787</link><description>&lt;p&gt;
&#20844;&#27491;&#28216;&#25103;&#65306;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Impartial Games: A Challenge for Reinforcement Learning. (arXiv:2205.12787v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12787
&lt;/p&gt;
&lt;p&gt;
AlphaZero-style reinforcement learning algorithms excel in various board games but face challenges with impartial games. The researchers present a concrete example of the game nim, and show that AlphaZero-style algorithms have difficulty learning these impartial games on larger board sizes. The difference between impartial games and partisan games can be explained by the vulnerability to adversarial attacks and perturbations.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;AlphaZero&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#26827;&#30424;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20844;&#27491;&#28216;&#25103;&#20013;&#21364;&#38754;&#20020;&#25361;&#25112;&#65292;&#36825;&#20123;&#28216;&#25103;&#20013;&#29609;&#23478;&#20849;&#20139;&#26827;&#23376;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;&#28216;&#25103;&#20363;&#23376;&#65292;&#21363;&#23567;&#23401;&#20204;&#29609;&#30340;&#23612;&#22982;&#28216;&#25103;&#65292;&#20197;&#21450;&#20854;&#20182;&#19968;&#20123;&#20844;&#27491;&#28216;&#25103;&#65292;&#36825;&#20123;&#28216;&#25103;&#20284;&#20046;&#25104;&#20026;AlphaZero&#21644;&#31867;&#20284;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#32458;&#33050;&#30707;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#19982;&#26368;&#36817;&#30340;&#30740;&#31350;&#19968;&#33268;&#65292;&#34920;&#26126;AlphaZero-style&#31639;&#27861;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25915;&#20987;&#21644;&#25932;&#23545;&#25200;&#21160;&#30340;&#24433;&#21709;&#65292;&#26174;&#31034;&#20102;&#22312;&#25152;&#26377;&#21512;&#27861;&#29366;&#24577;&#19979;&#23398;&#20064;&#25484;&#25569;&#36825;&#20123;&#28216;&#25103;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#21457;&#29616;&#23612;&#22982;&#28216;&#25103;&#22312;&#23567;&#22411;&#26827;&#30424;&#19978;&#21487;&#20197;&#23398;&#20064;&#65292;&#20294;&#24403;&#26827;&#30424;&#23610;&#23544;&#22686;&#22823;&#26102;&#65292;AlphaZero-style&#31639;&#27861;&#30340;&#23398;&#20064;&#36895;&#24230;&#26174;&#33879;&#20943;&#24930;&#12290;&#30452;&#35266;&#19978;&#65292;&#23612;&#22982;&#31561;&#20844;&#27491;&#28216;&#25103;&#19982;&#35937;&#26827;&#21644;&#22260;&#26827;&#31561;&#20826;&#27966;&#28216;&#25103;&#20043;&#38388;&#30340;&#21306;&#21035;&#22312;&#20110;&#65292;&#22914;&#26524;&#31995;&#32479;&#20013;&#28155;&#21152;&#20102;&#24494;&#23567;&#30340;&#22122;&#38899;&#65288;&#20363;&#22914;&#65292;&#26827;&#30424;&#30340;&#19968;&#23567;&#37096;&#20998;&#34987;&#35206;&#30422;&#65289;&#65292;&#23545;&#20110;&#20844;&#27491;&#28216;&#25103;&#26469;&#35828;&#65292;&#36825;&#26159;&#19968;&#31181;&#20856;&#22411;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
AlphaZero-style reinforcement learning (RL) algorithms excel in various board games but face challenges with impartial games, where players share pieces. We present a concrete example of a game - namely the children's game of nim - and other impartial games that seem to be a stumbling block for AlphaZero-style and similar reinforcement learning algorithms.  Our findings are consistent with recent studies showing that AlphaZero-style algorithms are vulnerable to adversarial attacks and adversarial perturbations, showing the difficulty of learning to master the games in all legal states.  We show that nim can be learned on small boards, but AlphaZero-style algorithms learning dramatically slows down when the board size increases. Intuitively, the difference between impartial games like nim and partisan games like Chess and Go can be explained by the fact that if a tiny amount of noise is added to the system (e.g. if a small part of the board is covered), for impartial games, it is typica
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#21472;&#21152;(FES)&#12290;FES&#36890;&#36807;&#21472;&#21152;&#22810;&#20010;&#20027;&#24178;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#21033;&#29992;&#24322;&#26500;&#39044;&#35757;&#32451;&#30340;&#20027;&#24178;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#32500;&#25252;&#19968;&#20010;&#38656;&#35201;&#37325;&#26032;&#35745;&#31639;&#30340;&#36890;&#29992;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2205.05831</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#21472;&#21152;
&lt;/p&gt;
&lt;p&gt;
Feature Extractor Stacking for Cross-domain Few-shot Meta-learning. (arXiv:2205.05831v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.05831
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#21472;&#21152;(FES)&#12290;FES&#36890;&#36807;&#21472;&#21152;&#22810;&#20010;&#20027;&#24178;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#21033;&#29992;&#24322;&#26500;&#39044;&#35757;&#32451;&#30340;&#20027;&#24178;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#32500;&#25252;&#19968;&#20010;&#38656;&#35201;&#37325;&#26032;&#35745;&#31639;&#30340;&#36890;&#29992;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;(CDFSML)&#35299;&#20915;&#20102;&#38656;&#35201;&#23558;&#30693;&#35782;&#20174;&#22810;&#20010;&#28304;&#39046;&#22495;&#36716;&#31227;&#21040;&#19968;&#20010;&#23454;&#20363;&#31232;&#32570;&#30340;&#30446;&#26631;&#39046;&#22495;&#65292;&#32780;&#30446;&#26631;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#24067;&#26126;&#26174;&#19981;&#21516;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#26368;&#36817;&#21457;&#24067;&#30340;CDFSML&#26041;&#27861;&#36890;&#24120;&#26500;&#24314;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#23558;&#22810;&#20010;&#28304;&#39046;&#22495;&#30340;&#30693;&#35782;&#21512;&#24182;&#21040;&#19968;&#20010;&#20027;&#24178;&#29305;&#24449;&#25552;&#21462;&#22120;&#20013;&#12290;&#36825;&#26679;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#25512;&#29702;&#65292;&#20294;&#38656;&#35201;&#22312;&#28155;&#21152;&#26032;&#30340;&#28304;&#39046;&#22495;&#26102;&#37325;&#26032;&#35745;&#31639;&#20027;&#24178;&#12290;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#36824;&#19982;&#24322;&#26500;&#28304;&#39046;&#22495;&#20027;&#24178;&#26550;&#26500;&#19981;&#20860;&#23481;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29305;&#24449;&#25552;&#21462;&#22120;&#21472;&#21152;(FES)&#65292;&#19968;&#31181;&#23558;&#26469;&#33258;&#19968;&#32452;&#20027;&#24178;&#30340;&#20449;&#24687;&#36827;&#34892;&#32452;&#21512;&#30340;&#26032;CDFSML&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#30452;&#25509;&#20351;&#29992;&#24322;&#26500;&#39044;&#35757;&#32451;&#30340;&#20027;&#24178;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#32500;&#25252;&#19968;&#20010;&#38656;&#35201;&#22312;&#20027;&#24178;&#38598;&#21512;&#26356;&#26032;&#26102;&#37325;&#26032;&#35745;&#31639;&#30340;&#36890;&#29992;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#26412;&#30340;FES&#31639;&#27861;&#65292;&#23427;&#21463;&#32463;&#20856;&#21472;&#21152;&#26041;&#27861;&#20803;&#23398;&#20064;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain few-shot meta-learning (CDFSML) addresses learning problems where knowledge needs to be transferred from several source domains into an instance-scarce target domain with an explicitly different distribution. Recently published CDFSML methods generally construct a universal model that combines knowledge of multiple source domains into one backbone feature extractor. This enables efficient inference but necessitates re-computation of the backbone whenever a new source domain is added. Some of these methods are also incompatible with heterogeneous source domain backbone architectures. We propose feature extractor stacking (FES), a new CDFSML method for combining information from a collection of backbones, which can utilise heterogeneous pretrained backbones out of the box, and does not maintain a universal model that needs to be re-computed when its backbone collection is updated. We present the basic FES algorithm, which is inspired by the classic stacking approach to meta-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#25968;&#23383;&#30828;&#20214;&#19978;&#35299;&#20915;&#21453;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#24182;&#35777;&#26126;&#20102;&#23545;&#20110;&#23567;&#30340;&#26494;&#24347;&#21442;&#25968;&#65292;&#26377;&#38480;&#32500;&#21453;&#38382;&#39064;&#26080;&#27861;&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#35299;&#20915;&#12290;&#36825;&#20123;&#32467;&#26524;&#36824;&#24341;&#20837;&#20102;&#31639;&#27861;&#21487;&#33719;&#24471;&#20934;&#30830;&#24230;&#30340;&#19979;&#38480;&#12290;</title><link>http://arxiv.org/abs/2202.13490</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#25968;&#23383;&#30828;&#20214;&#19978;&#30340;&#21453;&#38382;&#39064;&#23384;&#22312;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Limitations of Deep Learning for Inverse Problems on Digital Hardware. (arXiv:2202.13490v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.13490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#25968;&#23383;&#30828;&#20214;&#19978;&#35299;&#20915;&#21453;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#24182;&#35777;&#26126;&#20102;&#23545;&#20110;&#23567;&#30340;&#26494;&#24347;&#21442;&#25968;&#65292;&#26377;&#38480;&#32500;&#21453;&#38382;&#39064;&#26080;&#27861;&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#35299;&#20915;&#12290;&#36825;&#20123;&#32467;&#26524;&#36824;&#24341;&#20837;&#20102;&#31639;&#27861;&#21487;&#33719;&#24471;&#20934;&#30830;&#24230;&#30340;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#30001;&#20110;&#35757;&#32451;&#26159;&#22312;&#25968;&#23383;&#30828;&#20214;&#19978;&#36827;&#34892;&#30340;&#65292;&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#24403;&#21069;&#27169;&#25311;&#20026;&#22270;&#28789;&#26426;&#30340;&#30828;&#20214;&#24179;&#21488;&#19978;&#23454;&#38469;&#21487;&#20197;&#35745;&#31639;&#30340;&#20869;&#23481;&#65292;&#36825;&#20250;&#23548;&#33268;&#28145;&#24230;&#23398;&#20064;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#21453;&#38382;&#39064;&#31867;&#65292;&#29305;&#21035;&#26159;&#28085;&#30422;&#20102;&#20174;&#27979;&#37327;&#20013;&#37325;&#26500;&#25968;&#25454;&#30340;&#20219;&#20309;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23545;&#20110;&#23567;&#30340;&#26494;&#24347;&#21442;&#25968;&#65292;&#26377;&#38480;&#32500;&#21453;&#38382;&#39064;&#26080;&#27861;&#36890;&#36807;&#24052;&#25343;&#36203;-&#39532;&#20857;&#23572;&#35745;&#31639;&#26041;&#27861;&#35299;&#20915;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#24341;&#20837;&#20102;&#22312;&#31639;&#27861;&#19978;&#21487;&#20197;&#33719;&#24471;&#30340;&#20934;&#30830;&#24230;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have seen tremendous success over the last years. Since the training is performed on digital hardware, in this paper, we analyze what actually can be computed on current hardware platforms modeled as Turing machines, which would lead to inherent restrictions of deep learning. For this, we focus on the class of inverse problems, which, in particular, encompasses any task to reconstruct data from measurements. We prove that finite-dimensional inverse problems are not Banach-Mazur computable for small relaxation parameters. Even more, our results introduce a lower bound on the accuracy that can be obtained algorithmically.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#21069;&#21521;&#32452;&#21512;&#20256;&#25773;&#65288;FCP&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#22312;&#32467;&#26500;&#21270;&#20998;&#31867;&#38382;&#39064;&#19978;&#30340;&#39044;&#27979;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#32452;&#21512;&#21521;&#37327;&#25551;&#36848;&#27599;&#20010;&#31070;&#32463;&#20803;&#20013;&#38382;&#39064;&#29305;&#24449;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#32452;&#21512;&#20540;&#19982;&#21463;&#20445;&#25252;&#29305;&#24449;&#30340;&#39044;&#26399;&#34892;&#20026;&#32039;&#23494;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2112.12717</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#25512;&#29702;&#30340;&#21069;&#21521;&#32452;&#21512;&#20256;&#25773;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Forward Composition Propagation for Explainable Neural Reasoning. (arXiv:2112.12717v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.12717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#21069;&#21521;&#32452;&#21512;&#20256;&#25773;&#65288;FCP&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#22312;&#32467;&#26500;&#21270;&#20998;&#31867;&#38382;&#39064;&#19978;&#30340;&#39044;&#27979;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#32452;&#21512;&#21521;&#37327;&#25551;&#36848;&#27599;&#20010;&#31070;&#32463;&#20803;&#20013;&#38382;&#39064;&#29305;&#24449;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#32452;&#21512;&#20540;&#19982;&#21463;&#20445;&#25252;&#29305;&#24449;&#30340;&#39044;&#26399;&#34892;&#20026;&#32039;&#23494;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#21069;&#21521;&#32452;&#21512;&#20256;&#25773;&#65288;FCP&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#22312;&#32467;&#26500;&#21270;&#20998;&#31867;&#38382;&#39064;&#19978;&#25805;&#20316;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#12290;&#22312;&#25552;&#20986;&#30340;FCP&#31639;&#27861;&#20013;&#65292;&#27599;&#20010;&#31070;&#32463;&#20803;&#37117;&#30001;&#19968;&#20010;&#32452;&#21512;&#21521;&#37327;&#25551;&#36848;&#65292;&#35813;&#21521;&#37327;&#25351;&#31034;&#20102;&#27599;&#20010;&#38382;&#39064;&#29305;&#24449;&#22312;&#35813;&#31070;&#32463;&#20803;&#20013;&#30340;&#20316;&#29992;&#12290;&#32452;&#21512;&#21521;&#37327;&#20351;&#29992;&#32473;&#23450;&#30340;&#36755;&#20837;&#23454;&#20363;&#21021;&#22987;&#21270;&#65292;&#24182;&#38543;&#21518;&#36890;&#36807;&#25972;&#20010;&#32593;&#32476;&#20256;&#25773;&#65292;&#30452;&#21040;&#36798;&#21040;&#36755;&#20986;&#23618;&#12290;&#27599;&#20010;&#32452;&#21512;&#20540;&#30340;&#31526;&#21495;&#34920;&#31034;&#30456;&#24212;&#29305;&#24449;&#26159;&#21542;&#28608;&#27963;&#25110;&#25233;&#21046;&#31070;&#32463;&#20803;&#65292;&#32780;&#32477;&#23545;&#20540; quantifies &#20102;&#20854;&#24433;&#21709;&#12290;FCP&#31639;&#27861;&#26159;&#22312;&#21518;&#32493;&#22522;&#30784;&#19978;&#25191;&#34892;&#30340;&#65292;&#21363;&#22312;&#23398;&#20064;&#36807;&#31243;&#23436;&#25104;&#21518;&#12290;&#20026;&#20102;&#35828;&#26126;FCP&#31639;&#27861;&#65292;&#26412;&#25991;&#24320;&#23637;&#20102;&#19968;&#20010;&#20851;&#20110;&#20844;&#24179;&#38382;&#39064;&#20013;&#20559;&#35265;&#26816;&#27979;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#20854;&#20013;&#22320;&#38754;&#30495;&#30456;&#26159;&#24050;&#30693;&#30340;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#32452;&#21512;&#20540;&#19982;&#21463;&#20445;&#25252;&#29305;&#24449;&#30340;&#39044;&#26399;&#34892;&#20026;&#32039;&#23494;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an algorithm called Forward Composition Propagation (FCP) to explain the predictions of feed-forward neural networks operating on structured classification problems. In the proposed FCP algorithm, each neuron is described by a composition vector indicating the role of each problem feature in that neuron. Composition vectors are initialized using a given input instance and subsequently propagated through the whole network until reaching the output layer. The sign of each composition value indicates whether the corresponding feature excites or inhibits the neuron, while the absolute value quantifies its impact. The FCP algorithm is executed on a post-hoc basis, i.e., once the learning process is completed. Aiming to illustrate the FCP algorithm, this paper develops a case study concerning bias detection in a fairness problem in which the ground truth is known. The simulation results show that the composition values closely align with the expected behavior of protected
&lt;/p&gt;</description></item><item><title>&#29992;&#22522;&#20110;&#27169;&#22411;&#26500;&#24314;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#36866;&#24212;&#24615;&#22320;&#35843;&#25972;&#27493;&#38271;&#21644;&#25628;&#32034;&#26041;&#21521;&#65292;&#25552;&#39640;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2111.07058</link><description>&lt;p&gt;
&#29992;&#27169;&#22411;&#26500;&#24314;&#22686;&#24378;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bolstering Stochastic Gradient Descent with Model Building. (arXiv:2111.07058v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.07058
&lt;/p&gt;
&lt;p&gt;
&#29992;&#22522;&#20110;&#27169;&#22411;&#26500;&#24314;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#36866;&#24212;&#24615;&#22320;&#35843;&#25972;&#27493;&#38271;&#21644;&#25628;&#32034;&#26041;&#21521;&#65292;&#25552;&#39640;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#21450;&#20854;&#21464;&#31181;&#26159;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#26680;&#24515;&#20248;&#21270;&#31639;&#27861;&#65292;&#33021;&#22815;&#33719;&#24471;&#33391;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#38024;&#23545;&#29305;&#23450;&#24212;&#29992;&#36827;&#34892;&#35843;&#20248;&#26102;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#36825;&#20123;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#32447;&#25628;&#32034;&#26041;&#27861;&#36845;&#20195;&#35843;&#25972;&#27493;&#38271;&#65292;&#21487;&#20197;&#38477;&#20302;&#35843;&#20248;&#36807;&#31243;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#21521;&#27493;&#27169;&#22411;&#26500;&#24314;&#30340;&#38543;&#26426;&#32447;&#25628;&#32034;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#36825;&#20010;&#27169;&#22411;&#26500;&#24314;&#27493;&#39588;&#34701;&#20837;&#20102;&#20108;&#38454;&#20449;&#24687;&#65292;&#19981;&#20165;&#21487;&#20197;&#35843;&#25972;&#27493;&#38271;&#65292;&#36824;&#21487;&#20197;&#35843;&#25972;&#25628;&#32034;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#20998;&#32452;&#65288;&#24352;&#37327;&#30340;&#23618;&#65289;&#65292;&#20026;&#27599;&#20010;&#21442;&#25968;&#32452;&#24314;&#31435;&#27169;&#22411;&#24182;&#35745;&#31639;&#26032;&#30340;&#27493;&#38271;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#23545;&#35282;&#21270;&#26041;&#27861;&#20351;&#24471;&#36873;&#25321;&#30340;&#27493;&#38271;&#26159;&#33258;&#36866;&#24212;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25910;&#25947;&#36895;&#24230;&#20998;&#26512;&#65292;
&lt;/p&gt;
&lt;p&gt;
Stochastic gradient descent method and its variants constitute the core optimization algorithms that achieve good convergence rates for solving machine learning problems. These rates are obtained especially when these algorithms are fine-tuned for the application at hand. Although this tuning process can require large computational costs, recent work has shown that these costs can be reduced by line search methods that iteratively adjust the step length. We propose an alternative approach to stochastic line search by using a new algorithm based on forward step model building. This model building step incorporates second-order information that allows adjusting not only the step length but also the search direction. Noting that deep learning model parameters come in groups (layers of tensors), our method builds its model and calculates a new step for each parameter group. This novel diagonalization approach makes the selected step lengths adaptive. We provide convergence rate analysis, a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#35782;&#21035;&#26041;&#27861;&#65292;&#20351;&#29992;&#26757;&#23572;&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#29305;&#24449;&#12290;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#24471;&#20986;&#20102;&#19968;&#20123;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2110.03427</link><description>&lt;p&gt;
&#26159;&#21542;&#24635;&#26159;&#38656;&#35201;&#27880;&#24847;&#21147;&#65311;&#35821;&#35328;&#35782;&#21035;&#26696;&#20363;&#30740;&#31350;&#12290;(arXiv:2110.03427v3 [cs.LG] &#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Is Attention always needed? A Case Study on Language Identification from Speech. (arXiv:2110.03427v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#35782;&#21035;&#26041;&#27861;&#65292;&#20351;&#29992;&#26757;&#23572;&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#29305;&#24449;&#12290;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#24471;&#20986;&#20102;&#19968;&#20123;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#35782;&#21035;&#65288;LID&#65289;&#26159;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#39044;&#22788;&#29702;&#36807;&#31243;&#65292;&#28041;&#21450;&#20174;&#38899;&#39057;&#26679;&#26412;&#20013;&#35782;&#21035;&#20986;&#35762;&#35805;&#35821;&#35328;&#12290;&#24403;&#21069;&#33021;&#22815;&#22788;&#29702;&#22810;&#31181;&#35821;&#35328;&#30340;&#31995;&#32479;&#35201;&#27714;&#29992;&#25143;&#22312;&#20351;&#29992;&#20043;&#21069;&#26126;&#30830;&#25351;&#23450;&#19968;&#31181;&#25110;&#22810;&#31181;&#35821;&#35328;&#12290;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#65292;&#24403;ASR&#31995;&#32479;&#26080;&#27861;&#29702;&#35299;&#35762;&#35805;&#35821;&#35328;&#26102;&#65292;LID&#20219;&#21153;&#25198;&#28436;&#30528;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#23548;&#33268;&#35821;&#38899;&#35782;&#21035;&#32467;&#26524;&#22833;&#36133;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;&#21367;&#31215;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;CRNN&#65289;&#30340;LID&#26041;&#27861;&#65292;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#38899;&#39057;&#26679;&#26412;&#30340;&#26757;&#23572;&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#65288;MFCC&#65289;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22797;&#21046;&#20102;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21367;&#31215;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;&#24102;&#26377;&#27880;&#24847;&#21147;&#30340;CRNN&#65289;&#65292;&#24182;&#19982;&#25105;&#20204;&#30340;&#22522;&#20110;CRNN&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Language Identification (LID) is a crucial preliminary process in the field of Automatic Speech Recognition (ASR) that involves the identification of a spoken language from audio samples. Contemporary systems that can process speech in multiple languages require users to expressly designate one or more languages prior to utilization. The LID task assumes a significant role in scenarios where ASR systems are unable to comprehend the spoken language in multilingual settings, leading to unsuccessful speech recognition outcomes. The present study introduces convolutional recurrent neural network (CRNN) based LID, designed to operate on the Mel-frequency Cepstral Coefficient (MFCC) characteristics of audio samples. Furthermore, we replicate certain state-of-the-art methodologies, specifically the Convolutional Neural Network (CNN) and Attention-based Convolutional Recurrent Neural Network (CRNN with attention), and conduct a comparative analysis with our CRNN-based approach. We conducted co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#39044;&#27979;&#36807;&#31243;&#30417;&#25511;&#27169;&#22411;&#30340;&#38887;&#24615;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#19977;&#31181;&#19981;&#21516;&#31574;&#30053;&#65292;&#24182;&#21457;&#29616;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#20855;&#26377;&#28508;&#21147;&#26469;&#35299;&#20915;&#39044;&#27979;&#27169;&#22411;&#30340;&#26356;&#26032;&#21644;&#21487;&#21464;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2109.03501</link><description>&lt;p&gt;
&#25105;&#35813;&#22914;&#20309;&#26356;&#26032;&#25105;&#30340;&#27169;&#22411;&#65311;&#20851;&#20110;&#39044;&#27979;&#36807;&#31243;&#30417;&#25511;&#27169;&#22411;&#23545;&#21464;&#21270;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do I update my model? On the resilience of Predictive Process Monitoring models to change. (arXiv:2109.03501v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.03501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#39044;&#27979;&#36807;&#31243;&#30417;&#25511;&#27169;&#22411;&#30340;&#38887;&#24615;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#19977;&#31181;&#19981;&#21516;&#31574;&#30053;&#65292;&#24182;&#21457;&#29616;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#20855;&#26377;&#28508;&#21147;&#26469;&#35299;&#20915;&#39044;&#27979;&#27169;&#22411;&#30340;&#26356;&#26032;&#21644;&#21487;&#21464;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#32463;&#36807;&#20805;&#20998;&#30740;&#31350;&#30340;&#39044;&#27979;&#36807;&#31243;&#30417;&#25511;&#25216;&#26415;&#36890;&#24120;&#22522;&#20110;&#36807;&#21435;&#30340;&#27969;&#31243;&#25191;&#34892;&#26500;&#24314;&#39044;&#27979;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#35813;&#27169;&#22411;&#39044;&#27979;&#26032;&#36827;&#34892;&#20013;&#26696;&#20363;&#30340;&#26410;&#26469;&#65292;&#26080;&#27861;&#36890;&#36807;&#26032;&#26696;&#20363;&#30340;&#25191;&#34892;&#26469;&#26356;&#26032;&#23427;&#12290;&#36825;&#20351;&#24471;&#39044;&#27979;&#36807;&#31243;&#30417;&#25511;&#23545;&#20110;&#22312;&#19981;&#26029;&#28436;&#21464;&#21644;/&#25110;&#38543;&#26102;&#38388;&#23637;&#29616;&#26032;&#21464;&#20307;&#34892;&#20026;&#30340;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#36807;&#31243;&#30340;&#21487;&#21464;&#24615;&#22826;&#36807;&#20725;&#21270;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#31574;&#30053;&#65292;&#20801;&#35768;&#21608;&#26399;&#24615;&#22320;&#37325;&#26032;&#21457;&#29616;&#25110;&#22686;&#37327;&#26500;&#24314;&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#21033;&#29992;&#26032;&#30340;&#21487;&#29992;&#25968;&#25454;&#12290;&#35780;&#20272;&#37325;&#28857;&#25918;&#22312;&#26032;&#23398;&#20064;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#19978;&#65292;&#21253;&#25324;&#20934;&#30830;&#24615;&#21644;&#26102;&#38388;&#65292;&#19982;&#21407;&#22987;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#20351;&#29992;&#20102;&#19968;&#20123;&#30495;&#23454;&#21644;&#21512;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26126;&#30830;&#30340;&#27010;&#24565;&#28418;&#31227;&#21644;&#27809;&#26377;&#26126;&#30830;&#30340;&#27010;&#24565;&#28418;&#31227;&#12290;&#32467;&#26524;&#35777;&#26126;&#20102;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing well investigated Predictive Process Monitoring techniques typically construct a predictive model based on past process executions, and then use it to predict the future of new ongoing cases, without the possibility of updating it with new cases when they complete their execution. This can make Predictive Process Monitoring too rigid to deal with the variability of processes working in real environments that continuously evolve and/or exhibit new variant behaviours over time. As a solution to this problem, we evaluate the use of three different strategies that allow the periodic rediscovery or incremental construction of the predictive model so as to exploit new available data. The evaluation focuses on the performance of the new learned predictive models, in terms of accuracy and time, against the original one, and uses a number of real and synthetic datasets with and without explicit Concept Drift. The results provide an evidence of the potential of incremental learning algo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#19988;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20316;&#20026;&#24615;&#33021;&#22522;&#20934;&#65292;&#21253;&#25324;&#31867;&#20284;&#20110;ResNet&#30340;&#26550;&#26500;&#21644;&#36866;&#24212;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;Transformer&#26550;&#26500;&#12290;&#36825;&#20123;&#22522;&#20934;&#27169;&#22411;&#22312;&#19981;&#21516;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2106.11959</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Revisiting Deep Learning Models for Tabular Data. (arXiv:2106.11959v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.11959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#19988;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20316;&#20026;&#24615;&#33021;&#22522;&#20934;&#65292;&#21253;&#25324;&#31867;&#20284;&#20110;ResNet&#30340;&#26550;&#26500;&#21644;&#36866;&#24212;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;Transformer&#26550;&#26500;&#12290;&#36825;&#20123;&#22522;&#20934;&#27169;&#22411;&#22312;&#19981;&#21516;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#20851;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#30340;&#25991;&#29486;&#25552;&#20986;&#20102;&#21508;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#25253;&#21578;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25552;&#20986;&#30340;&#27169;&#22411;&#36890;&#24120;&#27809;&#26377;&#36827;&#34892;&#36866;&#24403;&#30340;&#27604;&#36739;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#24120;&#24120;&#20351;&#29992;&#19981;&#21516;&#30340;&#22522;&#20934;&#21644;&#23454;&#39564;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#32780;&#35328;&#65292;&#20160;&#20040;&#26679;&#30340;&#27169;&#22411;&#24615;&#33021;&#26368;&#22909;&#26159;&#19981;&#28165;&#26970;&#30340;&#12290;&#21478;&#22806;&#65292;&#35813;&#39046;&#22495;&#20173;&#28982;&#32570;&#20047;&#26377;&#25928;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#21363;&#22312;&#19981;&#21516;&#38382;&#39064;&#19978;&#25552;&#20379;&#26377;&#31454;&#20105;&#21147;&#24615;&#33021;&#30340;&#26131;&#20110;&#20351;&#29992;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#20027;&#35201;DL&#26550;&#26500;&#36827;&#34892;&#20102;&#27010;&#36848;&#65292;&#24182;&#36890;&#36807;&#30830;&#23450;&#20004;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#28145;&#24230;&#26550;&#26500;&#65292;&#25552;&#39640;&#20102;&#34920;&#26684;DL&#30340;&#22522;&#20934;&#12290;&#31532;&#19968;&#31181;&#26159;&#31867;&#20284;&#20110;ResNet&#30340;&#26550;&#26500;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#26159;&#24120;&#35265;&#30340;&#20808;&#21069;&#24037;&#20316;&#20013;&#24120;&#32570;&#22833;&#30340;&#24378;&#22522;&#20934;&#12290;&#31532;&#20108;&#20010;&#27169;&#22411;&#26159;&#25105;&#20204;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;Transformer&#26550;&#26500;&#30340;&#31616;&#21333;&#36866;&#24212;&#65292;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The existing literature on deep learning for tabular data proposes a wide range of novel architectures and reports competitive results on various datasets. However, the proposed models are usually not properly compared to each other and existing works often use different benchmarks and experiment protocols. As a result, it is unclear for both researchers and practitioners what models perform best. Additionally, the field still lacks effective baselines, that is, the easy-to-use models that provide competitive performance across different problems.  In this work, we perform an overview of the main families of DL architectures for tabular data and raise the bar of baselines in tabular DL by identifying two simple and powerful deep architectures. The first one is a ResNet-like architecture which turns out to be a strong baseline that is often missing in prior works. The second model is our simple adaptation of the Transformer architecture for tabular data, which outperforms other solution
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;P-Tuning&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#36830;&#32493;&#25552;&#31034;&#23884;&#20837;&#21644;&#31163;&#25955;&#25552;&#31034;&#30340;&#25340;&#25509;&#65292;&#31283;&#23450;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#22312;&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2103.10385</link><description>&lt;p&gt;
GPT&#20063;&#29702;&#35299;&#20102;
&lt;/p&gt;
&lt;p&gt;
GPT Understands, Too. (arXiv:2103.10385v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.10385
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;P-Tuning&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#36830;&#32493;&#25552;&#31034;&#23884;&#20837;&#21644;&#31163;&#25955;&#25552;&#31034;&#30340;&#25340;&#25509;&#65292;&#31283;&#23450;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#22312;&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#27169;&#24335;&#26469;&#20419;&#20351;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#20855;&#26377;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#26041;&#38754;&#30340;&#25928;&#26524;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#21457;&#29616;&#65292;&#25163;&#21160;&#31163;&#25955;&#30340;&#25552;&#31034;&#24448;&#24448;&#23548;&#33268;&#24615;&#33021;&#19981;&#31283;&#23450;&#8212;&#8212;&#20363;&#22914;&#65292;&#22312;&#25552;&#31034;&#20013;&#25913;&#21464;&#19968;&#20010;&#21333;&#35789;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#22823;&#24133;&#19979;&#38477;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;P-Tuning&#65292;&#23427;&#20351;&#29992;&#35757;&#32451;&#21487;&#23398;&#20064;&#30340;&#36830;&#32493;&#25552;&#31034;&#23884;&#20837;&#20197;&#21450;&#31163;&#25955;&#25552;&#31034;&#30340;&#25340;&#25509;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;P-Tuning&#19981;&#20165;&#36890;&#36807;&#20943;&#23567;&#21508;&#31181;&#31163;&#25955;&#25552;&#31034;&#20043;&#38388;&#30340;&#24046;&#36317;&#26469;&#31283;&#23450;&#35757;&#32451;&#65292;&#36824;&#36890;&#36807;&#36739;&#22823;&#24133;&#24230;&#30340;&#25552;&#21319;&#22312;&#21253;&#25324;LAMA&#21644;SuperGLUE&#22312;&#20869;&#30340;&#21508;&#31181;NLU&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#26080;&#35770;&#26159;&#20923;&#32467;&#30340;&#36824;&#26159;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#20840;&#30417;&#30563;&#21644;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;P-Tuning&#36890;&#24120;&#37117;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting a pretrained language model with natural language patterns has been proved effective for natural language understanding (NLU). However, our preliminary study reveals that manual discrete prompts often lead to unstable performance -- e.g., changing a single word in the prompt might result in substantial performance drop. We propose a novel method P-Tuning that employs trainable continuous prompt embeddings in concatenation with discrete prompts. Empirically, P-Tuning not only stabilizes training by minimizing the gap between various discrete prompts, but also improves performance by a sizeable margin on a wide range of NLU tasks including LAMA and SuperGLUE. P-Tuning is generally effective for both frozen and tuned language models, under both the fully-supervised and few-shot settings.
&lt;/p&gt;</description></item><item><title>&#31163;&#32447;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#20013;&#65292;&#23558;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#24378;&#21046;&#21464;&#25104;&#23616;&#37096;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#26159;&#21462;&#24471;&#33391;&#22909;&#34920;&#29616;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#24182;&#19988;&#28385;&#36275;&#22870;&#21169;&#30340;&#21033;&#26222;&#24076;&#33576;&#24615;&#32422;&#26463;&#23545;&#27169;&#20223;&#24615;&#33021;&#20855;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2006.16785</link><description>&lt;p&gt;
&#21807;&#26377;&#21033;&#26222;&#24076;&#33576;&#24615;&#33021;&#22815;&#39535;&#26381;&#31163;&#32447;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Lipschitzness Is All You Need To Tame Off-policy Generative Adversarial Imitation Learning. (arXiv:2006.16785v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.16785
&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#20013;&#65292;&#23558;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#24378;&#21046;&#21464;&#25104;&#23616;&#37096;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#26159;&#21462;&#24471;&#33391;&#22909;&#34920;&#29616;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#24182;&#19988;&#28385;&#36275;&#22870;&#21169;&#30340;&#21033;&#26222;&#24076;&#33576;&#24615;&#32422;&#26463;&#23545;&#27169;&#20223;&#24615;&#33021;&#20855;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#25104;&#21151;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#22823;&#22810;&#23545;&#36229;&#21442;&#25968;&#25935;&#24863;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#19968;&#20123;&#20851;&#38190;&#30340;&#24037;&#31243;&#25805;&#20316;&#25165;&#33021;&#21462;&#24471;&#25104;&#21151;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#31163;&#32447;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#30340;&#24773;&#20917;&#65292;&#24182;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23558;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#24378;&#21046;&#21464;&#25104;&#23616;&#37096;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#26159;&#35813;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#24517;&#35201;&#26465;&#20214;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#28041;&#21450;&#29366;&#24577;-&#20215;&#20540;&#20989;&#25968;&#30340;&#23616;&#37096;&#21033;&#26222;&#24076;&#33576;&#24615;&#36136;&#30340;&#20960;&#20010;&#29702;&#35770;&#32467;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#35777;&#25454;&#35777;&#26126;&#65292;&#22870;&#21169;&#30340;&#21033;&#26222;&#24076;&#33576;&#24615;&#32422;&#26463;&#30340;&#19968;&#33268;&#28385;&#36275;&#23545;&#27169;&#20223;&#24615;&#33021;&#20855;&#26377;&#26497;&#24378;&#30340;&#31215;&#26497;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24754;&#35266;&#22870;&#21169;&#39044;&#22788;&#29702;&#38468;&#21152;&#39033;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#22823;&#31867;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent success of reinforcement learning in various domains, these approaches remain, for the most part, deterringly sensitive to hyper-parameters and are often riddled with essential engineering feats allowing their success. We consider the case of off-policy generative adversarial imitation learning, and perform an in-depth review, qualitative and quantitative, of the method. We show that forcing the learned reward function to be local Lipschitz-continuous is a sine qua non condition for the method to perform well. We then study the effects of this necessary condition and provide several theoretical results involving the local Lipschitzness of the state-value function. We complement these guarantees with empirical evidence attesting to the strong positive effect that the consistent satisfaction of the Lipschitzness constraint on the reward has on imitation performance. Finally, we tackle a generic pessimistic reward preconditioning add-on spawning a large class of reward 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#39044;&#35757;&#32451;&#30340;&#38477;&#22122;&#33258;&#32534;&#30721;&#22120;&#26469;&#20445;&#30041;&#36755;&#20986;&#32467;&#26500;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#39044;&#27979;&#22120;&#30340;&#22797;&#26434;&#24615;&#24182;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2006.16205</link><description>&lt;p&gt;
&#32452;&#21512;&#24494;&#35843;&#65306;&#20923;&#32467;&#39044;&#35757;&#32451;&#30340;&#38477;&#22122;&#33258;&#32534;&#30721;&#22120;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Composed Fine-Tuning: Freezing Pre-Trained Denoising Autoencoders for Improved Generalization. (arXiv:2006.16205v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.16205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#39044;&#35757;&#32451;&#30340;&#38477;&#22122;&#33258;&#32534;&#30721;&#22120;&#26469;&#20445;&#30041;&#36755;&#20986;&#32467;&#26500;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#39044;&#27979;&#22120;&#30340;&#22797;&#26434;&#24615;&#24182;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#21463;&#36755;&#20986;&#26377;&#25928;&#24615;&#32422;&#26463;&#30340;&#32467;&#26500;&#21270;&#36755;&#20986;&#30340;&#39044;&#27979;&#38382;&#39064;&#65292;&#20363;&#22914;&#23558;&#20266;&#20195;&#30721;&#32763;&#35793;&#20026;&#20195;&#30721;&#26102;&#65292;&#20195;&#30721;&#24517;&#39035;&#33021;&#22815;&#32534;&#35793;&#12290;&#34429;&#28982;&#26631;&#35760;&#30340;&#36755;&#20837;-&#36755;&#20986;&#23545;&#24456;&#38590;&#33719;&#21462;&#65292;&#20294;&#26159;&#8220;&#26080;&#26631;&#31614;&#8221;&#30340;&#36755;&#20986;&#65292;&#21363;&#27809;&#26377;&#23545;&#24212;&#36755;&#20837;&#30340;&#36755;&#20986;&#65292;&#26159;&#20813;&#36153;&#25552;&#20379;&#30340;&#65288;&#20363;&#22914;GitHub&#19978;&#30340;&#20195;&#30721;&#65289;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#26377;&#20851;&#36755;&#20986;&#26377;&#25928;&#24615;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#39044;&#35757;&#32451;&#38477;&#22122;&#22120;&#26469;&#25429;&#25417;&#36755;&#20986;&#32467;&#26500;&#65292;&#35813;&#38477;&#22122;&#22120;&#29992;&#20110;&#21435;&#22122;&#26080;&#26631;&#31614;&#36755;&#20986;&#30340;&#25439;&#22351;&#29256;&#26412;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#22312;&#39044;&#35757;&#32451;&#20043;&#21518;&#36827;&#34892;&#26631;&#20934;&#24494;&#35843;&#20250;&#30772;&#22351;&#37096;&#20998;&#36755;&#20986;&#32467;&#26500;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32452;&#21512;&#24494;&#35843;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#39044;&#35757;&#32451;&#30340;&#38477;&#22122;&#22120;&#19982;&#39044;&#27979;&#22120;&#32452;&#21512;&#36827;&#34892;&#24494;&#35843;&#65292;&#20854;&#20013;&#38477;&#22122;&#22120;&#34987;&#20923;&#32467;&#20197;&#20445;&#30041;&#36755;&#20986;&#32467;&#26500;&#12290;&#23545;&#20110;&#20004;&#23618;ReLU&#32593;&#32476;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#32452;&#21512;&#24494;&#35843;&#26174;&#33879;&#38477;&#20302;&#20102;&#39044;&#27979;&#22120;&#30340;&#22797;&#26434;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32452;&#21512;&#24494;&#35843;&#22312;&#20004;&#20010;&#20266;&#20195;&#30721;&#21040;&#20195;&#30721;&#32763;&#35793;&#20219;&#21153;&#19978;&#20248;&#20110;&#26631;&#20934;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on prediction problems with structured outputs that are subject to output validity constraints, e.g. pseudocode-to-code translation where the code must compile. While labeled input-output pairs are expensive to obtain, "unlabeled" outputs, i.e. outputs without corresponding inputs, are freely available (e.g. code on GitHub) and provide information about output validity. We can capture the output structure by pre-training a denoiser to denoise corrupted versions of unlabeled outputs. We first show that standard fine-tuning after pre-training destroys some of this structure. We then propose composed fine-tuning, which fine-tunes a predictor composed with the pre-trained denoiser, which is frozen to preserve output structure. For two-layer ReLU networks, we prove that composed fine-tuning significantly reduces the complexity of the predictor, thus improving generalization. Empirically, we show that composed fine-tuning improves over standard fine-tuning on two pseudocode-to-code 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#35745;&#31639;&#26694;&#26550;&#19979;&#30340;&#24847;&#35782;&#29702;&#35770;&#65292;&#31216;&#20026;&#8220;&#30446;&#26631;&#23545;&#40784;&#30340;&#20869;&#37096;&#34920;&#31034;&#25805;&#20316;&#8221;&#65288;GARIM&#65289;&#12290;&#35813;&#29702;&#35770;&#35748;&#20026;&#24847;&#35782;&#25903;&#25345;&#23545;&#30446;&#26631;&#30456;&#20851;&#30340;&#20869;&#37096;&#34920;&#31034;&#36827;&#34892;&#20027;&#21160;&#25805;&#20316;&#65292;&#20351;&#20854;&#19982;&#36861;&#27714;&#30340;&#30446;&#26631;&#26356;&#21152;&#23545;&#40784;&#65292;&#20174;&#32780;&#22686;&#21152;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/1912.13490</link><description>&lt;p&gt;
&#24847;&#35782;&#30340;&#31070;&#32463;&#35745;&#31639;&#27169;&#22411;&#65306;&#30446;&#26631;&#23545;&#40784;&#30340;&#20869;&#37096;&#34920;&#31034;&#25805;&#20316;&#29702;&#35770;&#65288;GARIM&#65289;
&lt;/p&gt;
&lt;p&gt;
A Neurocomputational Account of Consciousness: The Goal-Aligning Representation Internal Manipulation Theory (GARIM). (arXiv:1912.13490v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1912.13490
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#35745;&#31639;&#26694;&#26550;&#19979;&#30340;&#24847;&#35782;&#29702;&#35770;&#65292;&#31216;&#20026;&#8220;&#30446;&#26631;&#23545;&#40784;&#30340;&#20869;&#37096;&#34920;&#31034;&#25805;&#20316;&#8221;&#65288;GARIM&#65289;&#12290;&#35813;&#29702;&#35770;&#35748;&#20026;&#24847;&#35782;&#25903;&#25345;&#23545;&#30446;&#26631;&#30456;&#20851;&#30340;&#20869;&#37096;&#34920;&#31034;&#36827;&#34892;&#20027;&#21160;&#25805;&#20316;&#65292;&#20351;&#20854;&#19982;&#36861;&#27714;&#30340;&#30446;&#26631;&#26356;&#21152;&#23545;&#40784;&#65292;&#20174;&#32780;&#22686;&#21152;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#35782;&#20316;&#20026;&#20154;&#31867;&#35748;&#30693;&#30340;&#26680;&#24515;&#35201;&#32032;&#65292;&#24050;&#32463;&#36890;&#36807;&#31070;&#32463;&#31185;&#23398;&#12289;&#24515;&#29702;&#23398;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#31561;&#22810;&#31181;&#31185;&#23398;&#26041;&#27861;&#36827;&#34892;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39046;&#22495;&#20043;&#38388;&#30340;&#19981;&#33391;&#25972;&#21512;&#38480;&#21046;&#20102;&#23545;&#24847;&#35782;&#30340;&#23436;&#25972;&#21644;&#28165;&#26224;&#29702;&#35299;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#31070;&#32463;&#35745;&#31639;&#26694;&#26550;&#19979;&#30340;&#8220;&#30446;&#26631;&#23545;&#40784;&#30340;&#20869;&#37096;&#34920;&#31034;&#25805;&#20316;&#8221;&#65288;GARIM&#65289;&#24847;&#35782;&#29702;&#35770;&#65292;&#20026;&#25913;&#21892;&#36825;&#31181;&#25972;&#21512;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;GARIM&#29702;&#35770;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#65292;&#24847;&#35782;&#25903;&#25345;&#23545;&#30446;&#26631;&#30456;&#20851;&#30340;&#20869;&#37096;&#34920;&#31034;&#65288;&#22914;&#19990;&#30028;&#29366;&#24577;&#12289;&#23545;&#35937;&#21644;&#34892;&#20026;&#24207;&#21015;&#65289;&#36827;&#34892;&#20027;&#21160;&#25805;&#20316;&#65292;&#20351;&#23427;&#20204;&#19982;&#36861;&#27714;&#30340;&#30446;&#26631;&#26356;&#21152;&#23545;&#40784;&#12290;&#36825;&#20123;&#25805;&#20316;&#20351;&#24471;&#24847;&#35782;&#20195;&#29702;&#33021;&#22815;&#22312;&#20869;&#37096;&#20135;&#29983;&#20854;&#25152;&#32570;&#20047;&#30340;&#30693;&#35782;&#65292;&#20197;&#24212;&#23545;&#26032;&#26465;&#20214;&#21644;&#30446;&#26631;&#65292;&#20174;&#32780;&#22686;&#21152;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#28789;&#27963;&#24615;&#12290;&#34920;&#31034;&#30340;&#25805;&#20316;&#30001;&#22235;&#20010;&#31070;&#32463;&#21151;&#33021;&#23439;&#31995;&#32479;&#65288;Hierarc...
&lt;/p&gt;
&lt;p&gt;
Consciousness, a central element of human cognition, has been studied with multiple scientific approaches spanning neuroscience, psychology, artificial intelligence and robotics. Unfortunately, poor integration between these fields limits a full and clear understanding of consciousness. Here we contribute to improving this integration by proposing, within a neurocomputational framework, the `Goal-Aligning Representations Internal Manipulation' (GARIM) theory of consciousness. The central idea of the GARIM theory is that consciousness supports the active manipulation of goal-relevant internal representations (e.g., world states, objects, and action sequences), making them more aligned with the goals pursued. These manipulations allow the conscious agent to internally produce the knowledge it lacks to cope with novel conditions and goals, increasing the flexibility of goal-directed behaviour. The manipulation of representations is supported by four neuro-functional macro-systems (hierarc
&lt;/p&gt;</description></item></channel></rss>