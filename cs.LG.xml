<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>SHACIRA&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#31070;&#32463;&#34920;&#31034;&#20013;&#30340;&#29305;&#24449;&#32593;&#26684;&#36827;&#34892;&#39640;&#27700;&#24179;&#21387;&#32553;&#65292;&#36890;&#36807;&#37327;&#21270;&#28508;&#22312;&#26435;&#37325;&#21644;&#24212;&#29992;&#29109;&#27491;&#21017;&#21270;&#26469;&#23454;&#29616;&#21387;&#32553;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#19978;&#30340;&#22909;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.15848</link><description>&lt;p&gt;
SHACIRA: &#21487;&#25193;&#23637;&#30340;&#21704;&#24076;&#32593;&#26684;&#21387;&#32553;&#25216;&#26415;&#29992;&#20110;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
SHACIRA: Scalable HAsh-grid Compression for Implicit Neural Representations. (arXiv:2309.15848v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15848
&lt;/p&gt;
&lt;p&gt;
SHACIRA&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#31070;&#32463;&#34920;&#31034;&#20013;&#30340;&#29305;&#24449;&#32593;&#26684;&#36827;&#34892;&#39640;&#27700;&#24179;&#21387;&#32553;&#65292;&#36890;&#36807;&#37327;&#21270;&#28508;&#22312;&#26435;&#37325;&#21644;&#24212;&#29992;&#29109;&#27491;&#21017;&#21270;&#26469;&#23454;&#29616;&#21387;&#32553;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#19978;&#30340;&#22909;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#25110;&#31070;&#32463;&#22330;&#24050;&#25104;&#20026;&#32534;&#30721;&#22810;&#23186;&#20307;&#20449;&#21495;&#65288;&#22914;&#22270;&#20687;&#21644;&#36752;&#23556;&#22330;&#65289;&#24182;&#20445;&#25345;&#39640;&#36136;&#37327;&#30340;&#27969;&#34892;&#26694;&#26550;&#12290;&#26368;&#36817;&#65292;&#30001;Instant-NGP&#25552;&#20986;&#30340;&#21487;&#23398;&#20064;&#29305;&#24449;&#32593;&#26684;&#22312;&#35757;&#32451;&#21644;&#37319;&#26679;INR&#26041;&#38754;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#65292;&#23427;&#36890;&#36807;&#29992;&#19968;&#20010;&#22810;&#20998;&#36776;&#29575;&#26597;&#25214;&#34920;&#30340;&#29305;&#24449;&#21521;&#37327;&#21644;&#19968;&#20010;&#26356;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#26367;&#20195;&#19968;&#20010;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29305;&#24449;&#32593;&#26684;&#30340;&#20869;&#23384;&#28040;&#32791;&#24456;&#22823;&#65292;&#36825;&#21487;&#33021;&#25104;&#20026;&#23384;&#20648;&#21644;&#27969;&#23186;&#20307;&#24212;&#29992;&#30340;&#29942;&#39048;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SHACIRA&#65292;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#36825;&#20123;&#29305;&#24449;&#32593;&#26684;&#36827;&#34892;&#21387;&#32553;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#21518;&#22788;&#29702;&#20462;&#21098;/&#37327;&#21270;&#38454;&#27573;&#12290;&#25105;&#20204;&#20351;&#29992;&#37327;&#21270;&#30340;&#28508;&#22312;&#26435;&#37325;&#23545;&#29305;&#24449;&#32593;&#26684;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#24182;&#22312;&#28508;&#22312;&#31354;&#38388;&#24212;&#29992;&#29109;&#27491;&#21017;&#21270;&#65292;&#20197;&#23454;&#29616;&#22312;&#19981;&#21516;&#39046;&#22495;&#38388;&#30340;&#39640;&#27700;&#24179;&#21387;&#32553;&#12290;&#23545;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit Neural Representations (INR) or neural fields have emerged as a popular framework to encode multimedia signals such as images and radiance fields while retaining high-quality. Recently, learnable feature grids proposed by Instant-NGP have allowed significant speed-up in the training as well as the sampling of INRs by replacing a large neural network with a multi-resolution look-up table of feature vectors and a much smaller neural network. However, these feature grids come at the expense of large memory consumption which can be a bottleneck for storage and streaming applications. In this work, we propose SHACIRA, a simple yet effective task-agnostic framework for compressing such feature grids with no additional post-hoc pruning/quantization stages. We reparameterize feature grids with quantized latent weights and apply entropy regularization in the latent space to achieve high levels of compression across various domains. Quantitative and qualitative results on diverse datase
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#29616;&#26377;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#20449;&#21495;&#27844;&#28431;&#20559;&#24046;&#65292;&#20197;&#23454;&#29616;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#26356;&#22909;&#25511;&#21046;&#65292;&#24182;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#30340;&#20142;&#24230;&#20197;&#21450;&#26356;&#28385;&#36275;&#29305;&#23450;&#39118;&#26684;&#21644;&#39068;&#33394;&#35201;&#27714;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2309.15842</link><description>&lt;p&gt;
&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#20449;&#21495;&#27844;&#28431;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Exploiting the Signal-Leak Bias in Diffusion Models. (arXiv:2309.15842v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#29616;&#26377;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#20449;&#21495;&#27844;&#28431;&#20559;&#24046;&#65292;&#20197;&#23454;&#29616;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#26356;&#22909;&#25511;&#21046;&#65292;&#24182;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#30340;&#20142;&#24230;&#20197;&#21450;&#26356;&#28385;&#36275;&#29305;&#23450;&#39118;&#26684;&#21644;&#39068;&#33394;&#35201;&#27714;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#25193;&#25955;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#23384;&#22312;&#20559;&#24046;&#12290;&#36825;&#31181;&#20559;&#24046;&#26159;&#30001;&#20449;&#21495;&#27844;&#28431;&#24341;&#36215;&#30340;&#65292;&#20854;&#20998;&#24067;&#19982;&#22122;&#22768;&#20998;&#24067;&#19981;&#19968;&#33268;&#65292;&#23548;&#33268;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#27169;&#22411;&#38024;&#23545;&#29305;&#23450;&#39118;&#26684;&#36827;&#34892;&#35843;&#20248;&#26102;&#65292;&#36825;&#31181;&#20449;&#21495;&#27844;&#28431;&#20559;&#24046;&#29305;&#21035;&#26174;&#33879;&#65292;&#23548;&#33268;&#39118;&#26684;&#21305;&#37197;&#19981;&#22815;&#20248;&#21270;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#35797;&#22270;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36991;&#20813;&#20449;&#21495;&#27844;&#28431;&#12290;&#25105;&#20204;&#30456;&#21453;&#22320;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#20449;&#21495;&#27844;&#28431;&#20559;&#24046;&#65292;&#20197;&#23454;&#29616;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#26356;&#22909;&#25511;&#21046;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#20142;&#24230;&#26356;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#20197;&#21450;&#26356;&#33021;&#21305;&#37197;&#25152;&#38656;&#39118;&#26684;&#25110;&#39068;&#33394;&#30340;&#22270;&#20687;&#12290;&#36890;&#36807;&#23545;&#31354;&#38388;&#39057;&#29575;&#21644;&#20687;&#32032;&#22495;&#20013;&#30340;&#20449;&#21495;&#27844;&#28431;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#22312;&#21021;&#22987;&#28508;&#21464;&#37327;&#20013;&#24341;&#20837;&#20449;&#21495;&#27844;&#28431;&#65292;&#25105;&#20204;&#29983;&#25104;&#26356;&#31526;&#21512;&#39044;&#26399;&#32467;&#26524;&#30340;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a bias in the inference pipeline of most diffusion models. This bias arises from a signal leak whose distribution deviates from the noise distribution, creating a discrepancy between training and inference processes. We demonstrate that this signal-leak bias is particularly significant when models are tuned to a specific style, causing sub-optimal style matching. Recent research tries to avoid the signal leakage during training. We instead show how we can exploit this signal-leak bias in existing diffusion models to allow more control over the generated images. This enables us to generate images with more varied brightness, and images that better match a desired style or color. By modeling the distribution of the signal leak in the spatial frequency and pixel domains, and including a signal leak in the initial latent, we generate images that better match expected results without any additional training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#39640;&#31934;&#30830;&#24230;&#30340;&#35854;&#35328;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#22312;&#24576;&#30097;&#26377;&#35854;&#35328;&#30340;&#24773;&#20917;&#19979;&#38382;&#19968;&#32452;&#26080;&#20851;&#30340;&#21518;&#32493;&#38382;&#39064;&#65292;&#24182;&#23558;LLM&#30340;&#26159;/&#21542;&#31572;&#26696;&#36755;&#20837;&#21040;&#19968;&#20010;&#36923;&#36753;&#22238;&#24402;&#20998;&#31867;&#22120;&#20013;&#65292;&#35813;&#26816;&#27979;&#22120;&#33021;&#22815;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;LLM&#26550;&#26500;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#35854;&#35328;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.15840</link><description>&lt;p&gt;
&#22914;&#20309;&#25429;&#25417;AI&#35854;&#35328;&#65306;&#36890;&#36807;&#38382;&#26080;&#20851;&#38382;&#39064;&#22312;&#40657;&#30418;LLMs&#20013;&#36827;&#34892;&#35854;&#35328;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions. (arXiv:2309.15840v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#39640;&#31934;&#30830;&#24230;&#30340;&#35854;&#35328;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#22312;&#24576;&#30097;&#26377;&#35854;&#35328;&#30340;&#24773;&#20917;&#19979;&#38382;&#19968;&#32452;&#26080;&#20851;&#30340;&#21518;&#32493;&#38382;&#39064;&#65292;&#24182;&#23558;LLM&#30340;&#26159;/&#21542;&#31572;&#26696;&#36755;&#20837;&#21040;&#19968;&#20010;&#36923;&#36753;&#22238;&#24402;&#20998;&#31867;&#22120;&#20013;&#65292;&#35813;&#26816;&#27979;&#22120;&#33021;&#22815;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;LLM&#26550;&#26500;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#35854;&#35328;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20250;&#8220;&#35828;&#35854;&#8221;&#65292;&#20063;&#23601;&#26159;&#22312;&#26126;&#30693;&#36947;&#30495;&#30456;&#30340;&#24773;&#20917;&#19979;&#36755;&#20986;&#34394;&#20551;&#38472;&#36848;&#12290;&#24403;&#25351;&#31034;&#36755;&#20986;&#38169;&#35823;&#20449;&#24687;&#26102;&#65292;LLMs&#21487;&#33021;&#20250;&#8220;&#35828;&#35854;&#8221;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35854;&#35328;&#26816;&#27979;&#22120;&#65292;&#26082;&#19981;&#38656;&#35201;&#35775;&#38382;LLM&#30340;&#28608;&#27963;&#65288;&#40657;&#30418;&#65289;&#65292;&#20063;&#19981;&#38656;&#35201;&#20107;&#23454;&#38382;&#39064;&#30340;&#30495;&#30456;&#30693;&#35782;&#12290;&#36825;&#20010;&#26816;&#27979;&#22120;&#36890;&#36807;&#22312;&#24576;&#30097;&#26377;&#35854;&#35328;&#30340;&#24773;&#20917;&#19979;&#38382;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#26080;&#20851;&#21518;&#32493;&#38382;&#39064;&#65292;&#24182;&#23558;LLM&#30340;&#26159;/&#21542;&#31572;&#26696;&#36755;&#20837;&#21040;&#36923;&#36753;&#22238;&#24402;&#20998;&#31867;&#22120;&#20013;&#26469;&#24037;&#20316;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;&#36825;&#20010;&#35854;&#35328;&#26816;&#27979;&#22120;&#38750;&#24120;&#20934;&#30830;&#24182;&#19988;&#20196;&#20154;&#24778;&#35766;&#22320;&#36890;&#29992;&#12290;&#24403;&#22312;&#21333;&#19968;&#24773;&#22659;&#30340;&#31034;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451; - &#20419;&#20351;GPT-3.5&#22312;&#20107;&#23454;&#38382;&#39064;&#19978;&#25746;&#35854; - &#35813;&#26816;&#27979;&#22120;&#21487;&#20197;&#25512;&#24191;&#21040;&#20197;&#19979;&#24773;&#20917;&#65306;&#65288;1&#65289;&#20854;&#20182;LLM&#26550;&#26500;&#65292;&#65288;2&#65289;&#32454;&#35843;&#20026;&#35828;&#35854;&#30340;LLMs&#65292;&#65288;3&#65289;&#35844;&#23194;&#30340;&#35854;&#35328;&#65292;&#21644;&#65288;4&#65289;&#20986;&#29616;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#35854;&#35328;&#65292;&#27604;&#22914;&#38144;&#21806;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#20855;&#26377;&#29305;&#27530;&#30340;&#19982;&#35854;&#35328;&#30456;&#20851;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can "lie", which we define as outputting false statements despite "knowing" the truth in a demonstrable sense. LLMs might "lie", for example, when instructed to output misinformation. Here, we develop a simple lie detector that requires neither access to the LLM's activations (black-box) nor ground-truth knowledge of the fact in question. The detector works by asking a predefined set of unrelated follow-up questions after a suspected lie, and feeding the LLM's yes/no answers into a logistic regression classifier. Despite its simplicity, this lie detector is highly accurate and surprisingly general. When trained on examples from a single setting -prompting GPT-3.5 to lie about factual questions -- the detector generalises out-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie, (3) sycophantic lies, and (4) lies emerging in real-life scenarios such as sales. These results indicate that LLMs have distinctive lie-related behavioural pa
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#26816;&#27979;COVID-19&#21518;&#24739;&#32773;&#20013;&#30340;&#25345;&#32493;&#24615;&#28814;&#30151;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#22312;&#20234;&#25289;&#20811;&#21307;&#38498;&#25910;&#38598;&#30340;&#21307;&#30103;&#25968;&#25454;&#19978;&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#26500;&#24314;&#20102;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.15838</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#26816;&#27979;COVID-19&#21518;&#24739;&#32773;&#20013;&#30340;&#25345;&#32493;&#24615;&#28814;&#30151;&#29983;&#29289;&#26631;&#24535;&#29289;
&lt;/p&gt;
&lt;p&gt;
Automated Detection of Persistent Inflammatory Biomarkers in Post-COVID-19 Patients Using Machine Learning Techniques. (arXiv:2309.15838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15838
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#26816;&#27979;COVID-19&#21518;&#24739;&#32773;&#20013;&#30340;&#25345;&#32493;&#24615;&#28814;&#30151;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#22312;&#20234;&#25289;&#20811;&#21307;&#38498;&#25910;&#38598;&#30340;&#21307;&#30103;&#25968;&#25454;&#19978;&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#26500;&#24314;&#20102;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#22823;&#27969;&#34892;&#23545;&#20010;&#20307;&#20135;&#29983;&#20102;&#25345;&#20037;&#24433;&#21709;&#65292;&#35768;&#22810;&#20154;&#22312;&#30142;&#30149;&#21518;&#24613;&#24615;&#26399;&#32463;&#21382;&#25345;&#32493;&#30340;&#30151;&#29366;&#65292;&#21253;&#25324;&#28814;&#30151;&#12290;&#26816;&#27979;&#21644;&#30417;&#27979;&#36825;&#20123;&#28814;&#30151;&#29983;&#29289;&#26631;&#24535;&#29289;&#23545;&#21450;&#26102;&#24178;&#39044;&#21644;&#25913;&#21892;&#24739;&#32773;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#35782;&#21035;290&#21517;COVID-19&#21518;&#24739;&#32773;&#20013;&#30340;&#25345;&#20037;&#24615;&#28814;&#30151;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#26681;&#25454;&#20234;&#25289;&#20811;&#21307;&#38498;&#25910;&#38598;&#30340;&#21307;&#30103;&#25968;&#25454;&#12290;&#25968;&#25454;&#21253;&#25324;&#21508;&#31181;&#20020;&#24202;&#21442;&#25968;&#65292;&#22914;C-&#21453;&#24212;&#34507;&#30333;&#21644;&#30333;&#32454;&#32990;&#20171;&#32032;-6&#27700;&#24179;&#12289;&#24739;&#32773;&#20154;&#21475;&#32479;&#35745;&#23398;&#12289;&#21512;&#24182;&#30151;&#21644;&#27835;&#30103;&#21382;&#21490;&#12290;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#65292;&#20197;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#30340;&#25968;&#25454;&#38598;&#12290;&#37096;&#32626;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21253;&#25324;&#36923;&#36753;&#22238;&#24402;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#26799;&#24230;&#25552;&#21319;&#65292;&#26469;&#26500;&#24314;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has left a lasting impact on individuals, with many experiencing persistent symptoms, including inflammation, in the post-acute phase of the disease. Detecting and monitoring these inflammatory biomarkers is critical for timely intervention and improved patient outcomes. This study employs machine learning techniques to automate the identification of persistent inflammatory biomarkers in 290 post-COVID-19 patients, based on medical data collected from hospitals in Iraq. The data encompassed a wide array of clinical parameters, such as C-reactive protein and interleukin-6 levels, patient demographics, comorbidities, and treatment histories. Rigorous data preprocessing and feature selection processes were implemented to optimize the dataset for machine learning analysis. Various machine learning algorithms, including logistic regression, random forests, support vector machines, and gradient boosting, were deployed to construct predictive models. These models exhibit
&lt;/p&gt;</description></item><item><title>&#22810;&#21333;&#20803;&#36719;&#27979;&#37327;&#26159;&#21033;&#29992;&#21487;&#36716;&#31227;&#24615;&#23398;&#20064;&#31639;&#27861;&#25913;&#36827;&#36719;&#27979;&#37327;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#26469;&#22686;&#24378;&#36719;&#27979;&#37327;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#29305;&#21035;&#36866;&#29992;&#20110;&#20855;&#26377;&#22810;&#20010;&#23454;&#29616;&#30340;&#36827;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.15828</link><description>&lt;p&gt;
&#22810;&#21333;&#20803;&#36719;&#27979;&#37327;&#20801;&#35768;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-unit soft sensing permits few-shot learning. (arXiv:2309.15828v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15828
&lt;/p&gt;
&lt;p&gt;
&#22810;&#21333;&#20803;&#36719;&#27979;&#37327;&#26159;&#21033;&#29992;&#21487;&#36716;&#31227;&#24615;&#23398;&#20064;&#31639;&#27861;&#25913;&#36827;&#36719;&#27979;&#37327;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#26469;&#22686;&#24378;&#36719;&#27979;&#37327;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#29305;&#21035;&#36866;&#29992;&#20110;&#20855;&#26377;&#22810;&#20010;&#23454;&#29616;&#30340;&#36827;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#20855;&#26377;&#21487;&#36716;&#31227;&#24615;&#30340;&#23398;&#20064;&#31639;&#27861;&#26469;&#25913;&#36827;&#36719;&#27979;&#37327;&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#24403;&#19968;&#20010;&#36719;&#27979;&#37327;&#36890;&#36807;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#26469;&#23398;&#20064;&#26102;&#65292;&#20854;&#24615;&#33021;&#21487;&#20197;&#24471;&#21040;&#21152;&#24378;&#12290;&#21487;&#36716;&#31227;&#24615;&#30340;&#26377;&#29992;&#24615;&#21462;&#20915;&#20110;&#25152;&#35774;&#35745;&#30340;&#23398;&#20064;&#20219;&#21153;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#36719;&#27979;&#37327;&#35201;&#24212;&#29992;&#20110;&#26377;&#22810;&#20010;&#23454;&#29616;&#30340;&#36827;&#31243;&#65288;&#20363;&#22914;&#65292;&#26377;&#22810;&#20010;&#21487;&#29992;&#25968;&#25454;&#30340;&#31995;&#32479;&#25110;&#35774;&#22791;&#65289;&#26102;&#65292;&#23588;&#20854;&#30456;&#20851;&#12290;&#28982;&#21518;&#65292;&#27599;&#20010;&#23454;&#29616;&#37117;&#25552;&#20379;&#19968;&#20010;&#36719;&#27979;&#37327;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#19988;&#21512;&#29702;&#22320;&#26399;&#26395;&#36825;&#20123;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#20855;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#24212;&#29992;&#21487;&#36716;&#31227;&#24615;&#23548;&#33268;&#20102;&#25105;&#20204;&#25152;&#31216;&#30340;&#22810;&#21333;&#20803;&#36719;&#27979;&#37327;&#65292;&#20854;&#20013;&#36719;&#27979;&#37327;&#36890;&#36807;&#20174;&#25152;&#26377;&#23454;&#29616;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#26469;&#24314;&#27169;&#19968;&#20010;&#36827;&#31243;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22810;&#21333;&#20803;&#36719;&#27979;&#37327;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#23427;&#34987;&#26500;&#24314;&#20026;&#19968;&#20010;&#20998;&#23618;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;...
&lt;/p&gt;
&lt;p&gt;
Recent literature has explored various ways to improve soft sensors using learning algorithms with transferability. Broadly put, the performance of a soft sensor may be strengthened when it is learned by solving multiple tasks. The usefulness of transferability depends on how strongly related the devised learning tasks are. A particularly relevant case for transferability, is when a soft sensor is to be developed for a process of which there are many realizations, e.g. system or device with many implementations from which data is available. Then, each realization presents a soft sensor learning task, and it is reasonable to expect that the different tasks are strongly related. Applying transferability in this setting leads to what we call multi-unit soft sensing, where a soft sensor models a process by learning from data from all of its realizations.  This paper explores the learning abilities of a multi-unit soft sensor, which is formulated as a hierarchical model and implemented usin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;LM&#27169;&#25311;&#24037;&#20855;&#25191;&#34892;&#21644;&#24320;&#21457;&#22522;&#20110;LM&#30340;&#33258;&#21160;&#23433;&#20840;&#35780;&#20272;&#22120;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#27979;&#35797;LM&#20195;&#29702;&#30340;&#39640;&#25104;&#26412;&#21644;&#23547;&#25214;&#39640;&#39118;&#38505;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.15817</link><description>&lt;p&gt;
&#20351;&#29992;LM&#27169;&#25311;&#27801;&#30418;&#35782;&#21035;LM&#20195;&#29702;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Identifying the Risks of LM Agents with an LM-Emulated Sandbox. (arXiv:2309.15817v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15817
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;LM&#27169;&#25311;&#24037;&#20855;&#25191;&#34892;&#21644;&#24320;&#21457;&#22522;&#20110;LM&#30340;&#33258;&#21160;&#23433;&#20840;&#35780;&#20272;&#22120;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#27979;&#35797;LM&#20195;&#29702;&#30340;&#39640;&#25104;&#26412;&#21644;&#23547;&#25214;&#39640;&#39118;&#38505;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20195;&#29702;&#21644;&#24037;&#20855;&#20351;&#29992;&#30340;&#25216;&#26415;&#36827;&#27493;&#65292;&#20363;&#22914;ChatGPT&#25554;&#20214;&#65292;&#20351;&#24471;&#20195;&#29702;&#20855;&#22791;&#20102;&#20016;&#23500;&#30340;&#21151;&#33021;&#65292;&#20294;&#20063;&#25918;&#22823;&#20102;&#28508;&#22312;&#30340;&#39118;&#38505;&#65292;&#22914;&#27844;&#38706;&#31169;&#20154;&#25968;&#25454;&#25110;&#24341;&#21457;&#36130;&#21153;&#25439;&#22833;&#12290;&#35782;&#21035;&#36825;&#20123;&#39118;&#38505;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#24037;&#20316;&#65292;&#38656;&#35201;&#23454;&#26045;&#24037;&#20855;&#65292;&#25163;&#21160;&#35774;&#32622;&#27599;&#20010;&#27979;&#35797;&#22330;&#26223;&#30340;&#29615;&#22659;&#65292;&#24182;&#25214;&#21040;&#39118;&#38505;&#26696;&#20363;&#12290;&#38543;&#30528;&#24037;&#20855;&#21644;&#20195;&#29702;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#27979;&#35797;&#36825;&#20123;&#20195;&#29702;&#30340;&#39640;&#25104;&#26412;&#23558;&#20351;&#23547;&#25214;&#39640;&#39118;&#38505;&#12289;&#38271;&#23614;&#39118;&#38505;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ToolEmu&#65306;&#19968;&#20010;&#20351;&#29992;LM&#26469;&#27169;&#25311;&#24037;&#20855;&#25191;&#34892;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#25163;&#21160;&#23454;&#20363;&#21270;&#30340;&#24773;&#20917;&#19979;&#23545;LM&#20195;&#29702;&#36827;&#34892;&#21508;&#31181;&#24037;&#20855;&#21644;&#22330;&#26223;&#30340;&#27979;&#35797;&#12290;&#38500;&#20102;&#27169;&#25311;&#22120;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;LM&#30340;&#33258;&#21160;&#23433;&#20840;&#35780;&#20272;&#22120;&#65292;&#29992;&#20110;&#26816;&#26597;&#20195;&#29702;&#30340;&#22833;&#36133;&#24182;&#37327;&#21270;&#30456;&#20851;&#39118;&#38505;&#12290;&#25105;&#20204;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#27979;&#35797;&#20102;&#24037;&#20855;&#27169;&#25311;&#22120;&#21644;&#35780;&#20272;&#22120;&#65292;&#24182;&#21457;&#29616;&#20102;6&#20010;...
&lt;/p&gt;
&lt;p&gt;
Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks - such as leaking private data or causing financial losses. Identifying these risks is labor-intensive, necessitating implementing the tools, manually setting up the environment for each test scenario, and finding risky cases. As tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tailed risks. To address these challenges, we introduce ToolEmu: a framework that uses an LM to emulate tool execution and enables the testing of LM agents against a diverse range of tools and scenarios, without manual instantiation. Alongside the emulator, we develop an LM-based automatic safety evaluator that examines agent failures and quantifies associated risks. We test both the tool emulator and evaluator through human evaluation and find that 6
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20856;&#33539;&#30456;&#20851;&#20998;&#26512;&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#21270;&#30456;&#20851;&#24615;&#24046;&#24322;&#35823;&#24046;&#26469;&#20943;&#36731;&#19981;&#20844;&#24179;&#29616;&#35937;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;CCA&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#20943;&#23569;&#20102;&#30456;&#20851;&#24615;&#24046;&#24322;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.15809</link><description>&lt;p&gt;
&#20844;&#24179;&#30340;&#20856;&#33539;&#30456;&#20851;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Fair Canonical Correlation Analysis. (arXiv:2309.15809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20856;&#33539;&#30456;&#20851;&#20998;&#26512;&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#21270;&#30456;&#20851;&#24615;&#24046;&#24322;&#35823;&#24046;&#26469;&#20943;&#36731;&#19981;&#20844;&#24179;&#29616;&#35937;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;CCA&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#20943;&#23569;&#20102;&#30456;&#20851;&#24615;&#24046;&#24322;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20856;&#33539;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#20559;&#35265;&#38382;&#39064;&#65292;CCA&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#32479;&#35745;&#25216;&#26415;&#65292;&#29992;&#20110;&#30740;&#31350;&#20004;&#32452;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#19982;&#21463;&#20445;&#25252;&#23646;&#24615;&#30456;&#20851;&#30340;&#30456;&#20851;&#24615;&#24046;&#24322;&#35823;&#24046;&#65292;&#26469;&#20943;&#36731;&#19981;&#20844;&#24179;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;CCA&#33021;&#22815;&#20174;&#25152;&#26377;&#25968;&#25454;&#28857;&#20013;&#23398;&#20064;&#21040;&#20840;&#23616;&#25237;&#24433;&#30697;&#38453;&#65292;&#21516;&#26102;&#30830;&#20445;&#36825;&#20123;&#30697;&#38453;&#20135;&#29983;&#19982;&#32452;&#29305;&#23450;&#25237;&#24433;&#30697;&#38453;&#30456;&#24403;&#30340;&#30456;&#20851;&#24615;&#27700;&#24179;&#12290;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20943;&#23569;&#30456;&#20851;&#24615;&#24046;&#24322;&#35823;&#24046;&#30340;&#21516;&#26102;&#19981;&#20250;&#24433;&#21709;CCA&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates fairness and bias in Canonical Correlation Analysis (CCA), a widely used statistical technique for examining the relationship between two sets of variables. We present a framework that alleviates unfairness by minimizing the correlation disparity error associated with protected attributes. Our approach enables CCA to learn global projection matrices from all data points while ensuring that these matrices yield comparable correlation levels to group-specific projection matrices. Experimental evaluation on both synthetic and real-world datasets demonstrates the efficacy of our method in reducing correlation disparity error without compromising CCA accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#21644;&#39564;&#35777;&#19968;&#31181;&#26234;&#33021;&#25968;&#23398;&#27169;&#22411;&#65292;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#21069;&#21015;&#33146;&#30284;&#30340;&#26089;&#26399;&#26816;&#27979;&#65292;&#38477;&#20302;&#20551;&#38451;&#24615;&#29575;&#65292;&#25913;&#21892;&#24739;&#32773;&#39044;&#21518;&#12290;&#39044;&#35745;&#35813;&#27169;&#22411;&#32463;&#36807;&#36827;&#19968;&#27493;&#25913;&#36827;&#21644;&#39564;&#35777;&#21518;&#65292;&#21487;&#20197;&#25104;&#20026;&#19968;&#20010;&#21487;&#38752;&#30340;&#12289;&#24066;&#22330;&#21270;&#30340;&#21069;&#21015;&#33146;&#30284;&#26816;&#27979;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.15803</link><description>&lt;p&gt;
ANNCRIPS:&#30284;&#30151;&#30740;&#31350;&#20013;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#21644;&#29983;&#23384;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
ANNCRIPS: Artificial Neural Networks for Cancer Research In Prediction &amp; Survival. (arXiv:2309.15803v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#21644;&#39564;&#35777;&#19968;&#31181;&#26234;&#33021;&#25968;&#23398;&#27169;&#22411;&#65292;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#21069;&#21015;&#33146;&#30284;&#30340;&#26089;&#26399;&#26816;&#27979;&#65292;&#38477;&#20302;&#20551;&#38451;&#24615;&#29575;&#65292;&#25913;&#21892;&#24739;&#32773;&#39044;&#21518;&#12290;&#39044;&#35745;&#35813;&#27169;&#22411;&#32463;&#36807;&#36827;&#19968;&#27493;&#25913;&#36827;&#21644;&#39564;&#35777;&#21518;&#65292;&#21487;&#20197;&#25104;&#20026;&#19968;&#20010;&#21487;&#38752;&#30340;&#12289;&#24066;&#22330;&#21270;&#30340;&#21069;&#21015;&#33146;&#30284;&#26816;&#27979;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21069;&#21015;&#33146;&#30284;&#26159;50&#23681;&#21450;&#20197;&#19978;&#30007;&#24615;&#20013;&#24120;&#35265;&#30340;&#24694;&#24615;&#32959;&#30244;&#12290;&#30446;&#21069;&#30340;&#35786;&#26029;&#26041;&#27861;&#20027;&#35201;&#20381;&#38752;&#34880;&#28082;&#27979;&#35797;&#12289;&#21069;&#21015;&#33146;&#29305;&#24322;&#24615;&#25239;&#21407;(PSA)&#27700;&#24179;&#21644;&#30452;&#32928;&#25351;&#26816;(DRE)&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#26174;&#33879;&#30340;&#20551;&#38451;&#24615;&#29575;&#12290;&#35813;&#30740;&#31350;&#20851;&#27880;&#24320;&#21457;&#21644;&#39564;&#35777;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANNs)&#30340;&#26234;&#33021;&#25968;&#23398;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#21069;&#21015;&#33146;&#30284;&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23637;&#31034;&#19968;&#31181;&#35774;&#35745;&#29992;&#20110;&#24110;&#21161;&#26089;&#26399;&#21457;&#29616;&#21069;&#21015;&#33146;&#30284;&#12289;&#20419;&#36827;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#21450;&#26102;&#24178;&#39044;&#30340;&#26032;&#22411;&#25968;&#23398;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#30340;&#23454;&#26045;&#23637;&#31034;&#20102;&#38477;&#20302;&#20551;&#38451;&#24615;&#21457;&#29983;&#29575;&#12289;&#25913;&#21892;&#24739;&#32773;&#39044;&#21518;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39044;&#35265;&#22312;&#36827;&#19968;&#27493;&#25913;&#36827;&#12289;&#24191;&#27867;&#27979;&#35797;&#21644;&#39564;&#35777;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#19968;&#20010;&#21487;&#38752;&#30340;&#12289;&#24066;&#22330;&#21270;&#30340;&#21069;&#21015;&#33146;&#30284;&#26816;&#27979;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prostate cancer is a prevalent malignancy among men aged 50 and older. Current diagnostic methods primarily rely on blood tests, PSA:Prostate-Specific Antigen levels, and Digital Rectal Examinations (DRE). However, these methods suffer from a significant rate of false positive results. This study focuses on the development and validation of an intelligent mathematical model utilizing Artificial Neural Networks (ANNs) to enhance the early detection of prostate cancer. The primary objective of this research paper is to present a novel mathematical model designed to aid in the early detection of prostate cancer, facilitating prompt intervention by healthcare professionals. The model's implementation demonstrates promising potential in reducing the incidence of false positives, thereby improving patient outcomes. Furthermore, we envision that, with further refinement, extensive testing, and validation, this model can evolve into a robust, marketable solution for prostate cancer detection. 
&lt;/p&gt;</description></item><item><title>Node-Aligned Graph-to-Graph (NAG2G)&#26159;&#19968;&#20010;&#22522;&#20110;transformer&#30340;&#26080;&#27169;&#26495;&#27169;&#22411;&#65292;&#21033;&#29992;2D&#20998;&#23376;&#22270;&#21644;3D&#26500;&#35937;&#20449;&#24687;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#20998;&#23376;&#30340;&#25299;&#25169;&#20449;&#24687;&#21644;&#23545;&#40784;&#21407;&#23376;&#65292;&#25552;&#39640;&#21333;&#27493;&#21453;&#21512;&#25104;&#39044;&#27979;&#30340;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.15798</link><description>&lt;p&gt;
Node-Aligned Graph-to-Graph Generation for Retrosynthesis Prediction. (arXiv:2309.15798v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Node-Aligned Graph-to-Graph Generation for Retrosynthesis Prediction. (arXiv:2309.15798v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15798
&lt;/p&gt;
&lt;p&gt;
Node-Aligned Graph-to-Graph (NAG2G)&#26159;&#19968;&#20010;&#22522;&#20110;transformer&#30340;&#26080;&#27169;&#26495;&#27169;&#22411;&#65292;&#21033;&#29992;2D&#20998;&#23376;&#22270;&#21644;3D&#26500;&#35937;&#20449;&#24687;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#20998;&#23376;&#30340;&#25299;&#25169;&#20449;&#24687;&#21644;&#23545;&#40784;&#21407;&#23376;&#65292;&#25552;&#39640;&#21333;&#27493;&#21453;&#21512;&#25104;&#39044;&#27979;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#27493;&#21453;&#21512;&#25104;&#26159;&#26377;&#26426;&#21270;&#23398;&#21644;&#33647;&#29289;&#35774;&#35745;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#35782;&#21035;&#20986;&#21512;&#25104;&#29305;&#23450;&#21270;&#21512;&#29289;&#25152;&#38656;&#30340;&#21453;&#24212;&#29289;&#12290;&#38543;&#30528;&#35745;&#31639;&#26426;&#36741;&#21161;&#21512;&#25104;&#35268;&#21010;&#30340;&#20986;&#29616;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#20851;&#27880;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#31616;&#21270;&#36825;&#20010;&#36807;&#31243;&#12290;&#29616;&#26377;&#30340;&#26080;&#27169;&#26495;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;transformer&#32467;&#26500;&#65292;&#24182;&#23558;&#20998;&#23376;&#34920;&#31034;&#20026;ID&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#22312;&#20805;&#20998;&#21033;&#29992;&#20998;&#23376;&#30340;&#24191;&#27867;&#25299;&#25169;&#20449;&#24687;&#21644;&#22312;&#29983;&#25104;&#29289;&#21644;&#21453;&#24212;&#29289;&#20043;&#38388;&#23545;&#40784;&#21407;&#23376;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#23548;&#33268;&#32467;&#26524;&#19981;&#22914;&#21322;&#27169;&#26495;&#27169;&#22411;&#31454;&#20105;&#21147;&#24378;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;Node-Aligned Graph-to-Graph (NAG2G)&#65292;&#20063;&#26159;&#19968;&#20010;&#22522;&#20110;transformer&#30340;&#26080;&#27169;&#26495;&#27169;&#22411;&#65292;&#20294;&#26159;&#21033;&#29992;&#20102;2D&#20998;&#23376;&#22270;&#21644;3D&#26500;&#35937;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;n
&lt;/p&gt;
&lt;p&gt;
Single-step retrosynthesis is a crucial task in organic chemistry and drug design, requiring the identification of required reactants to synthesize a specific compound. with the advent of computer-aided synthesis planning, there is growing interest in using machine-learning techniques to facilitate the process. Existing template-free machine learning-based models typically utilize transformer structures and represent molecules as ID sequences. However, these methods often face challenges in fully leveraging the extensive topological information of the molecule and aligning atoms between the production and reactants, leading to results that are not as competitive as those of semi-template models. Our proposed method, Node-Aligned Graph-to-Graph (NAG2G), also serves as a transformer-based template-free model but utilizes 2D molecular graphs and 3D conformation information. Furthermore, our approach simplifies the incorporation of production-reactant atom mapping alignment by leveraging n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#65292;&#20351;&#29992;&#20840;&#26102;&#20998;&#31867;&#20934;&#21017;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#23398;&#20064;&#35821;&#38899;-&#25991;&#26412;&#23545;&#40784;&#65292;&#24182;&#36866;&#24212;&#35757;&#32451;&#36716;&#24405;&#20013;&#30340;&#38169;&#35823;&#65292;&#36991;&#20813;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2309.15796</link><description>&lt;p&gt;
&#23398;&#20064;&#26469;&#33258;&#26377;&#32570;&#38519;&#30340;&#25968;&#25454;&#65306;&#24369;&#30417;&#30563;&#24335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Learning from Flawed Data: Weakly Supervised Automatic Speech Recognition. (arXiv:2309.15796v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#65292;&#20351;&#29992;&#20840;&#26102;&#20998;&#31867;&#20934;&#21017;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#23398;&#20064;&#35821;&#38899;-&#25991;&#26412;&#23545;&#40784;&#65292;&#24182;&#36866;&#24212;&#35757;&#32451;&#36716;&#24405;&#20013;&#30340;&#38169;&#35823;&#65292;&#36991;&#20813;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#38656;&#35201;&#22823;&#37327;&#32463;&#36807;&#31934;&#24515;&#31579;&#36873;&#30340;&#37197;&#23545;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26631;&#27880;&#32773;&#36890;&#24120;&#25191;&#34892;&#8220;&#38750;&#36880;&#23383;&#8221;&#36716;&#24405;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#27169;&#22411;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#26102;&#20998;&#31867;&#65288;OTC&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#20934;&#21017;&#65292;&#26126;&#30830;&#22320;&#34701;&#20837;&#20102;&#30001;&#27492;&#31867;&#24369;&#30417;&#30563;&#24341;&#36215;&#30340;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#35821;&#38899;-&#25991;&#26412;&#23545;&#40784;&#65292;&#24182;&#36866;&#24212;&#35757;&#32451;&#36716;&#24405;&#20013;&#23384;&#22312;&#30340;&#38169;&#35823;&#12290;OTC&#36890;&#36807;&#21033;&#29992;&#21152;&#26435;&#26377;&#38480;&#29366;&#24577;&#36716;&#25442;&#22120;&#25193;&#23637;&#20102;&#20256;&#32479;&#30340;CTC&#30446;&#26631;&#20989;&#25968;&#29992;&#20110;&#19981;&#23436;&#32654;&#36716;&#24405;&#12290;&#36890;&#36807;&#22312;LibriSpeech&#21644;LibriVox&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;OTC&#35757;&#32451;ASR&#27169;&#22411;&#21487;&#20197;&#36991;&#20813;&#24615;&#33021;&#19979;&#38477;&#65292;&#21363;&#20351;&#36716;&#24405;&#20013;&#21253;&#21547;&#39640;&#36798;70&#65285;&#30340;&#38169;&#35823;&#65292;&#32780;CTC&#27169;&#22411;&#21017;&#23436;&#20840;&#22833;&#25928;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#21487;&#22312;https://github.com/k2-fsa/icefall&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training automatic speech recognition (ASR) systems requires large amounts of well-curated paired data. However, human annotators usually perform "non-verbatim" transcription, which can result in poorly trained models. In this paper, we propose Omni-temporal Classification (OTC), a novel training criterion that explicitly incorporates label uncertainties originating from such weak supervision. This allows the model to effectively learn speech-text alignments while accommodating errors present in the training transcripts. OTC extends the conventional CTC objective for imperfect transcripts by leveraging weighted finite state transducers. Through experiments conducted on the LibriSpeech and LibriVox datasets, we demonstrate that training ASR models with OTC avoids performance degradation even with transcripts containing up to 70% errors, a scenario where CTC models fail completely. Our implementation is available at https://github.com/k2-fsa/icefall.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20462;&#25913;&#22240;&#26524;&#26862;&#26519;&#26041;&#27861;&#65292;&#20197;&#30456;&#23545;&#39118;&#38505;&#20026;&#30446;&#26631;&#65292;&#20174;&#32780;&#25429;&#25417;&#21040;&#27835;&#30103;&#25928;&#24212;&#24322;&#36136;&#24615;&#30340;&#28508;&#22312;&#26469;&#28304;&#12290;</title><link>http://arxiv.org/abs/2309.15793</link><description>&lt;p&gt;
&#29992;&#22240;&#26524;&#26862;&#26519;&#38024;&#23545;&#30456;&#23545;&#39118;&#38505;&#24322;&#36136;&#24615;&#36827;&#34892;&#30446;&#26631;&#21270;
&lt;/p&gt;
&lt;p&gt;
Targeting Relative Risk Heterogeneity with Causal Forests. (arXiv:2309.15793v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20462;&#25913;&#22240;&#26524;&#26862;&#26519;&#26041;&#27861;&#65292;&#20197;&#30456;&#23545;&#39118;&#38505;&#20026;&#30446;&#26631;&#65292;&#20174;&#32780;&#25429;&#25417;&#21040;&#27835;&#30103;&#25928;&#24212;&#24322;&#36136;&#24615;&#30340;&#28508;&#22312;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#35797;&#39564;&#20998;&#26512;&#20013;&#65292;&#27835;&#30103;&#25928;&#24212;&#24322;&#36136;&#24615;&#65288;TEH&#65289;&#21363;&#31181;&#32676;&#20013;&#19981;&#21516;&#20122;&#32676;&#30340;&#27835;&#30103;&#25928;&#24212;&#30340;&#21464;&#24322;&#24615;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#22240;&#26524;&#26862;&#26519;&#65288;Wager&#21644;Athey&#65292;2018&#65289;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#38750;&#24120;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#20294;&#20687;&#35768;&#22810;&#20854;&#20182;&#21457;&#29616;TEH&#30340;&#26041;&#27861;&#19968;&#26679;&#65292;&#23427;&#29992;&#20110;&#20998;&#31163;&#20122;&#32676;&#30340;&#26631;&#20934;&#20391;&#37325;&#20110;&#32477;&#23545;&#39118;&#38505;&#30340;&#24046;&#24322;&#12290;&#36825;&#21487;&#33021;&#20250;&#21066;&#24369;&#32479;&#35745;&#21151;&#25928;&#65292;&#25513;&#30422;&#20102;&#30456;&#23545;&#39118;&#38505;&#20013;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#32780;&#30456;&#23545;&#39118;&#38505;&#36890;&#24120;&#26159;&#20020;&#24202;&#20851;&#27880;&#30340;&#26356;&#21512;&#36866;&#30340;&#25968;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#20462;&#25913;&#22240;&#26524;&#26862;&#26519;&#20197;&#38024;&#23545;&#30456;&#23545;&#39118;&#38505;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GLM&#65289;&#27604;&#36739;&#30340;&#26032;&#39062;&#33410;&#28857;&#20998;&#21106;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#32467;&#26524;&#65292;&#34920;&#26126;&#30456;&#23545;&#39118;&#38505;&#30340;&#22240;&#26524;&#26862;&#26519;&#21487;&#20197;&#25429;&#25417;&#21040;&#20854;&#20182;&#26410;&#35266;&#23519;&#21040;&#30340;&#24322;&#36136;&#24615;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Treatment effect heterogeneity (TEH), or variability in treatment effect for different subgroups within a population, is of significant interest in clinical trial analysis. Causal forests (Wager and Athey, 2018) is a highly popular method for this problem, but like many other methods for detecting TEH, its criterion for separating subgroups focuses on differences in absolute risk. This can dilute statistical power by masking nuance in the relative risk, which is often a more appropriate quantity of clinical interest. In this work, we propose and implement a methodology for modifying causal forests to target relative risk using a novel node-splitting procedure based on generalized linear model (GLM) comparison. We present results on simulated and real-world data that suggest relative risk causal forests can capture otherwise unobserved sources of heterogeneity.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#20174;&#19968;&#31995;&#21015;&#27169;&#22411;&#20013;&#20026;&#26032;&#20219;&#21153;&#36873;&#25321;&#26368;&#20339;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23398;&#20064;&#27169;&#22411;&#26469;&#36873;&#25321;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15789</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#19982;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Routing with Benchmark Datasets. (arXiv:2309.15789v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#20174;&#19968;&#31995;&#21015;&#27169;&#22411;&#20013;&#20026;&#26032;&#20219;&#21153;&#36873;&#25321;&#26368;&#20339;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23398;&#20064;&#27169;&#22411;&#26469;&#36873;&#25321;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#25968;&#37327;&#36805;&#36895;&#22686;&#38271;&#65292;&#29992;&#20110;&#27604;&#36739;&#23427;&#20204;&#12290;&#34429;&#28982;&#19968;&#20123;&#27169;&#22411;&#22312;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#20013;&#21344;&#20248;&#21183;&#65292;&#20294;&#36890;&#24120;&#27809;&#26377;&#21333;&#19968;&#27169;&#22411;&#22312;&#25152;&#26377;&#20219;&#21153;&#21644;&#29992;&#20363;&#20013;&#37117;&#33021;&#36798;&#21040;&#26368;&#20339;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20174;&#19968;&#31995;&#21015;&#27169;&#22411;&#20013;&#20026;&#26032;&#20219;&#21153;&#36873;&#25321;&#26368;&#20339;LLM&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#34920;&#36848;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#22522;&#20934;&#25968;&#25454;&#38598;&#34987;&#37325;&#26032;&#29992;&#20110;&#23398;&#20064;&#19968;&#20010;"&#36335;&#30001;&#22120;"&#27169;&#22411;&#26469;&#36873;&#25321;LLM&#65292;&#24182;&#19988;&#25105;&#20204;&#34920;&#26126;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36716;&#21270;&#20026;&#19968;&#31995;&#21015;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;&#38598;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#23398;&#20064;&#27169;&#22411;&#36335;&#30001;&#22120;&#30340;&#25928;&#29992;&#21644;&#38480;&#21046;&#65292;&#25105;&#20204;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#22987;&#32456;&#27604;&#20351;&#29992;&#20219;&#20309;&#21333;&#19968;&#27169;&#22411;&#37117;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a rapidly growing number of open-source Large Language Models (LLMs) and benchmark datasets to compare them. While some models dominate these benchmarks, no single model typically achieves the best accuracy in all tasks and use cases. In this work, we address the challenge of selecting the best LLM out of a collection of models for new tasks. We propose a new formulation for the problem, in which benchmark datasets are repurposed to learn a "router" model for this LLM selection, and we show that this problem can be reduced to a collection of binary classification tasks. We demonstrate the utility and limitations of learning model routers from various benchmark datasets, where we consistently improve performance upon using any single model for all tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#37096;&#20998;&#36755;&#36816;&#38382;&#39064;&#30340;&#28857;&#20113;&#37197;&#20934;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24212;&#23545;&#38750;&#21018;&#24615;&#21160;&#20316;&#21644;&#37096;&#20998;&#21487;&#35265;&#24615;&#31561;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#22797;&#26434;&#24615;&#12290;&#35813;&#26041;&#27861;&#23558;&#28857;&#20113;&#35270;&#20026;&#32463;&#39564;&#24230;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#23398;&#20005;&#26684;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#28304;&#28857;&#21644;&#30446;&#26631;&#28857;&#20043;&#38388;&#30340;`&#23545;&#24212;&#20851;&#31995;'&#12290;</title><link>http://arxiv.org/abs/2309.15787</link><description>&lt;p&gt;
&#28857;&#20113;&#37197;&#20934;&#30340;&#37096;&#20998;&#36716;&#36816;
&lt;/p&gt;
&lt;p&gt;
Partial Transport for Point-Cloud Registration. (arXiv:2309.15787v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#37096;&#20998;&#36755;&#36816;&#38382;&#39064;&#30340;&#28857;&#20113;&#37197;&#20934;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24212;&#23545;&#38750;&#21018;&#24615;&#21160;&#20316;&#21644;&#37096;&#20998;&#21487;&#35265;&#24615;&#31561;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#22797;&#26434;&#24615;&#12290;&#35813;&#26041;&#27861;&#23558;&#28857;&#20113;&#35270;&#20026;&#32463;&#39564;&#24230;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#23398;&#20005;&#26684;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#28304;&#28857;&#21644;&#30446;&#26631;&#28857;&#20043;&#38388;&#30340;`&#23545;&#24212;&#20851;&#31995;'&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20113;&#37197;&#20934;&#22312;&#26426;&#22120;&#20154;&#12289;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#21644;&#21307;&#23398;&#22270;&#20687;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#35813;&#36807;&#31243;&#28041;&#21450;&#30830;&#23450;&#19981;&#21516;&#28857;&#38598;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#65292;&#36890;&#24120;&#22312;3D&#31354;&#38388;&#20869;&#36827;&#34892;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#26223;&#20013;&#65292;&#30001;&#20110;&#38750;&#21018;&#24615;&#21160;&#20316;&#21644;&#37096;&#20998;&#21487;&#35265;&#24615;&#65288;&#20363;&#22914;&#36974;&#25377;&#25110;&#20256;&#24863;&#22120;&#22122;&#22768;&#65289;&#30340;&#22797;&#26434;&#24615;&#65292;&#20351;&#24471;&#38750;&#21018;&#24615;&#37197;&#20934;&#25104;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#32463;&#20856;&#30340;&#38750;&#21018;&#24615;&#37197;&#20934;&#26041;&#27861;&#36890;&#24120;&#35745;&#31639;&#22797;&#26434;&#65292;&#24182;&#19988;&#20855;&#26377;&#19981;&#31283;&#23450;&#30340;&#24615;&#33021;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#20854;&#29702;&#35770;&#20445;&#35777;&#26377;&#38480;&#12290;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#21450;&#20854;&#38750;&#24179;&#34913;&#21464;&#31181;&#65288;&#20363;&#22914;&#26368;&#20248;&#37096;&#20998;&#36755;&#36816;&#38382;&#39064;&#65289;&#24050;&#25104;&#20026;&#28857;&#20113;&#37197;&#20934;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#22312;&#35813;&#39046;&#22495;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#22522;&#20934;&#12290;&#36825;&#20123;&#26041;&#27861;&#23558;&#28857;&#20113;&#35270;&#20026;&#32463;&#39564;&#24230;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#23398;&#20005;&#26684;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#65288;&#21464;&#25442;&#30340;&#65289;&#28304;&#28857;&#21644;&#30446;&#26631;&#28857;&#20043;&#38388;&#30340;`&#23545;&#24212;&#20851;&#31995;'&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#37096;&#20998;&#36755;&#36816;&#38382;&#39064;&#30340;&#28857;&#20113;&#37197;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Point cloud registration plays a crucial role in various fields, including robotics, computer graphics, and medical imaging. This process involves determining spatial relationships between different sets of points, typically within a 3D space. In real-world scenarios, complexities arise from non-rigid movements and partial visibility, such as occlusions or sensor noise, making non-rigid registration a challenging problem. Classic non-rigid registration methods are often computationally demanding, suffer from unstable performance, and, importantly, have limited theoretical guarantees. The optimal transport problem and its unbalanced variations (e.g., the optimal partial transport problem) have emerged as powerful tools for point-cloud registration, establishing a strong benchmark in this field. These methods view point clouds as empirical measures and provide a mathematically rigorous way to quantify the `correspondence' between (the transformed) source and target points. In this paper,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;NeuralEF&#65292;&#19968;&#20010;&#24555;&#36895;&#30340;&#31070;&#32463;&#36924;&#36817;&#26694;&#26550;&#65292;&#33021;&#22815;&#40065;&#26834;&#22320;&#39044;&#27979;&#39640;&#25928;&#21069;&#27839;&#38382;&#39064;&#30340;&#35299;&#65292;&#21516;&#26102;&#22788;&#29702;&#24322;&#26500;&#32447;&#24615;&#32422;&#26463;&#21644;&#21487;&#21464;&#25968;&#37327;&#30340;&#20248;&#21270;&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2309.15775</link><description>&lt;p&gt;
&#23398;&#20064;&#39640;&#25928;&#21069;&#27839;
&lt;/p&gt;
&lt;p&gt;
Learning the Efficient Frontier. (arXiv:2309.15775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;NeuralEF&#65292;&#19968;&#20010;&#24555;&#36895;&#30340;&#31070;&#32463;&#36924;&#36817;&#26694;&#26550;&#65292;&#33021;&#22815;&#40065;&#26834;&#22320;&#39044;&#27979;&#39640;&#25928;&#21069;&#27839;&#38382;&#39064;&#30340;&#35299;&#65292;&#21516;&#26102;&#22788;&#29702;&#24322;&#26500;&#32447;&#24615;&#32422;&#26463;&#21644;&#21487;&#21464;&#25968;&#37327;&#30340;&#20248;&#21270;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#21069;&#27839;&#65288;EF&#65289;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#36164;&#28304;&#37197;&#32622;&#38382;&#39064;&#65292;&#22312;&#32473;&#23450;&#39118;&#38505;&#27700;&#24179;&#19979;&#23547;&#25214;&#26368;&#20248;&#25237;&#36164;&#32452;&#21512;&#20197;&#26368;&#22823;&#21270;&#25910;&#30410;&#12290;&#20256;&#32479;&#19978;&#65292;&#36890;&#36807;&#35299;&#19968;&#20010;&#20984;&#20248;&#21270;&#38382;&#39064;&#26469;&#25214;&#21040;&#26368;&#20248;&#35299;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;NeuralEF&#65306;&#19968;&#20010;&#24555;&#36895;&#30340;&#31070;&#32463;&#36924;&#36817;&#26694;&#26550;&#65292;&#21487;&#20197;&#40065;&#26834;&#22320;&#39044;&#27979;&#30456;&#23545;&#24322;&#26500;&#32447;&#24615;&#32422;&#26463;&#21644;&#21487;&#21464;&#25968;&#37327;&#30340;&#20248;&#21270;&#36755;&#20837;&#30340;EF&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#23558;&#20248;&#21270;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#24207;&#21015;&#21040;&#24207;&#21015;&#38382;&#39064;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NeuralEF&#26159;&#21152;&#36895;&#22823;&#35268;&#27169;&#27169;&#25311;&#24182;&#22788;&#29702;&#19981;&#36830;&#32493;&#34892;&#20026;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The efficient frontier (EF) is a fundamental resource allocation problem where one has to find an optimal portfolio maximizing a reward at a given level of risk. This optimal solution is traditionally found by solving a convex optimization problem. In this paper, we introduce NeuralEF: a fast neural approximation framework that robustly forecasts the result of the EF convex optimization problem with respect to heterogeneous linear constraints and variable number of optimization inputs. By reformulating an optimization problem as a sequence to sequence problem, we show that NeuralEF is a viable solution to accelerate large-scale simulation while handling discontinuous behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#19978;&#19979;&#25991;&#36172;&#21338;&#38382;&#39064;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#8220;&#38544;&#24335;&#25506;&#32034;&#8221;&#20272;&#35745;&#22120;&#26469;&#35745;&#31639;&#31574;&#30053;&#20215;&#20540;&#30340;&#26435;&#37325;&#37325;&#35201;&#20272;&#35745;&#12290;&#19982;&#20043;&#21069;&#30340;&#32467;&#26524;&#30456;&#27604;&#65292;&#22312;&#20960;&#20046;&#25152;&#26377;&#24773;&#20917;&#19979;&#37117;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#20445;&#35777;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#20043;&#21069;&#25152;&#20570;&#30340;&#38750;&#24120;&#33499;&#21051;&#30340;&#8220;&#22343;&#21248;&#35206;&#30422;&#8221;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2309.15771</link><description>&lt;p&gt;
&#26435;&#37325;&#37325;&#35201;&#30340;&#31163;&#32447;&#23398;&#20064;&#27491;&#30830;&#22320;&#23436;&#25104;
&lt;/p&gt;
&lt;p&gt;
Importance-Weighted Offline Learning Done Right. (arXiv:2309.15771v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#19978;&#19979;&#25991;&#36172;&#21338;&#38382;&#39064;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#8220;&#38544;&#24335;&#25506;&#32034;&#8221;&#20272;&#35745;&#22120;&#26469;&#35745;&#31639;&#31574;&#30053;&#20215;&#20540;&#30340;&#26435;&#37325;&#37325;&#35201;&#20272;&#35745;&#12290;&#19982;&#20043;&#21069;&#30340;&#32467;&#26524;&#30456;&#27604;&#65292;&#22312;&#20960;&#20046;&#25152;&#26377;&#24773;&#20917;&#19979;&#37117;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#20445;&#35777;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#20043;&#21069;&#25152;&#20570;&#30340;&#38750;&#24120;&#33499;&#21051;&#30340;&#8220;&#22343;&#21248;&#35206;&#30422;&#8221;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38543;&#26426;&#19978;&#19979;&#25991;&#36172;&#21338;&#38382;&#39064;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#22522;&#20110;&#30001;&#27425;&#20248;&#34892;&#20026;&#31574;&#30053;&#25910;&#38598;&#30340;&#20915;&#31574;&#25968;&#25454;&#38598;&#23398;&#20064;&#19968;&#20010;&#36817;&#20284;&#26368;&#20248;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#19981;&#23545;&#22870;&#21169;&#20989;&#25968;&#20570;&#20219;&#20309;&#32467;&#26500;&#24615;&#20551;&#35774;&#65292;&#32780;&#26159;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#32473;&#23450;&#30340;&#31574;&#30053;&#31867;&#65292;&#24182;&#19988;&#26088;&#22312;&#19982;&#35813;&#31867;&#20013;&#30340;&#26368;&#20339;&#27604;&#36739;&#22120;&#31574;&#30053;&#31454;&#20105;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26631;&#20934;&#26041;&#27861;&#26159;&#35745;&#31639;&#27599;&#20010;&#31574;&#30053;&#20215;&#20540;&#30340;&#26435;&#37325;&#37325;&#35201;&#20272;&#35745;&#65292;&#24182;&#36873;&#25321;&#19968;&#20010;&#26368;&#23567;&#21270;&#20272;&#35745;&#20540;&#30340;&#31574;&#30053;&#65292;&#20943;&#21435;&#20272;&#35745;&#20540;&#20013;&#30340;&#8220;&#24754;&#35266;&#8221;&#35843;&#25972;&#20197;&#20943;&#23569;&#20854;&#38543;&#26426;&#27874;&#21160;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110; \citet{Neu2015} &#30340;&#8220;&#38544;&#24335;&#25506;&#32034;&#8221;&#20272;&#35745;&#22120;&#30340;&#31616;&#21333;&#26367;&#20195;&#26041;&#27861;&#65292;&#20854;&#24615;&#33021;&#20445;&#35777;&#22312;&#20960;&#20046;&#25152;&#26377;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#37117;&#20248;&#20110;&#20043;&#21069;&#30340;&#32467;&#26524;&#12290;&#23588;&#20854;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#28040;&#38500;&#20102;&#20043;&#21069;&#25152;&#26377;&#24037;&#20316;&#20013;&#38750;&#24120;&#33499;&#21051;&#30340;&#8220;&#22343;&#21248;&#35206;&#30422;&#8221;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of offline policy optimization in stochastic contextual bandit problems, where the goal is to learn a near-optimal policy based on a dataset of decision data collected by a suboptimal behavior policy. Rather than making any structural assumptions on the reward function, we assume access to a given policy class and aim to compete with the best comparator policy within this class. In this setting, a standard approach is to compute importance-weighted estimators of the value of each policy, and select a policy that minimizes the estimated value up to a "pessimistic" adjustment subtracted from the estimates to reduce their random fluctuations. In this paper, we show that a simple alternative approach based on the "implicit exploration" estimator of \citet{Neu2015} yields performance guarantees that are superior in nearly all possible terms to all previous results. Most notably, we remove an extremely restrictive "uniform coverage" assumption made in all previous works.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#25554;&#20540;&#22120;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#30340;&#20195;&#25968;&#21644;&#32479;&#35745;&#23646;&#24615;&#65292;&#24182;&#20026;&#26368;&#23567;l2&#33539;&#25968;OLS&#25554;&#20540;&#22120;&#25552;&#20379;&#20102;&#22522;&#26412;&#32467;&#26524;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#29702;&#35299;OLS&#25554;&#20540;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.15769</link><description>&lt;p&gt;
&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#25554;&#20540;&#22120;&#30340;&#20195;&#25968;&#21644;&#32479;&#35745;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Algebraic and Statistical Properties of the Ordinary Least Squares Interpolator. (arXiv:2309.15769v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#25554;&#20540;&#22120;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#30340;&#20195;&#25968;&#21644;&#32479;&#35745;&#23646;&#24615;&#65292;&#24182;&#20026;&#26368;&#23567;l2&#33539;&#25968;OLS&#25554;&#20540;&#22120;&#25552;&#20379;&#20102;&#22522;&#26412;&#32467;&#26524;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#29702;&#35299;OLS&#25554;&#20540;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#25581;&#31034;&#20102;&#23545;&#36229;&#21442;&#25968;&#21270;&#32479;&#35745;&#27169;&#22411;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#37325;&#22823;&#30340;&#29702;&#35770;&#20852;&#36259;&#12290;&#37492;&#20110;&#20854;&#31616;&#21333;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#65288;OLS&#65289;&#25554;&#20540;&#22120;&#24050;&#25104;&#20026;&#33719;&#24471;&#23545;&#36825;&#31181;&#29616;&#35937;&#22522;&#30784;&#27934;&#23519;&#21147;&#30340;&#20851;&#38190;&#25152;&#22312;&#12290;&#23613;&#31649;OLS&#22312;&#32463;&#20856;&#29615;&#22659;&#20013;&#30340;&#24615;&#36136;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#24314;&#31435;&#65292;&#20294;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#30340;&#34892;&#20026;&#36824;&#27809;&#26377;&#20687;&#23725;&#22238;&#24402;&#25110;&#22871;&#32034;&#22238;&#24402;&#37027;&#26679;&#34987;&#25506;&#32034;&#24471;&#37027;&#20040;&#36879;&#24443;&#65292;&#23613;&#31649;&#36817;&#24180;&#26469;&#24050;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#25105;&#20204;&#36890;&#36807;&#20026;&#26368;&#23567;l2&#33539;&#25968;OLS&#25554;&#20540;&#22120;&#25552;&#20379;&#22522;&#26412;&#30340;&#20195;&#25968;&#21644;&#32479;&#35745;&#32467;&#26524;&#26469;&#36129;&#29486;&#20110;&#36825;&#19968;&#26085;&#30410;&#22686;&#38271;&#30340;&#25991;&#29486;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#65288;i&#65289;&#30041;-k-out&#27531;&#24046;&#20844;&#24335;&#30340;&#39640;&#32500;&#20195;&#25968;&#31561;&#20215;&#29289;&#65292;&#65288;ii&#65289; Cochran&#20844;&#24335;&#65292;&#20197;&#21450;&#65288;iii&#65289;Frisch-Waugh-Lovell&#23450;&#29702;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#29702;&#35299;OLS&#25554;&#20540;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#20855;&#26377;&#23454;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning research has uncovered the phenomenon of benign overfitting for over-parameterized statistical models, which has drawn significant theoretical interest in recent years. Given its simplicity and practicality, the ordinary least squares (OLS) interpolator has become essential to gain foundational insights into this phenomenon. While properties of OLS are well established in classical settings, its behavior in high-dimensional settings is less explored (unlike for ridge or lasso regression) though significant progress has been made of late. We contribute to this growing literature by providing fundamental algebraic and statistical results for the minimum $\ell_2$-norm OLS interpolator. In particular, we provide high-dimensional algebraic equivalents of (i) the leave-$k$-out residual formula, (ii) Cochran's formula, and (iii) the Frisch-Waugh-Lovell theorem. These results aid in understanding the OLS interpolator's ability to generalize and have substantive implications for c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#32593;&#32476;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#27979;&#35797;&#26102;&#30340;&#21453;&#39304;&#20449;&#21495;&#26469;&#23454;&#26102;&#35843;&#25972;&#31070;&#32463;&#32593;&#32476;&#65292;&#30456;&#27604;&#22522;&#20934;&#26041;&#27861;&#26356;&#21152;&#28789;&#27963;&#19988;&#24555;&#36895;&#25968;&#20010;&#25968;&#37327;&#32423;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#20219;&#21153;&#21644;&#20998;&#24067;&#21464;&#21270;&#19979;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.15762</link><description>&lt;p&gt;
&#24555;&#36895;&#32593;&#32476;&#36866;&#24212;&#65306;&#21033;&#29992;&#27979;&#35797;&#21453;&#39304;&#23398;&#20064;&#36866;&#24212;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Rapid Network Adaptation: Learning to Adapt Neural Networks Using Test-Time Feedback. (arXiv:2309.15762v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#32593;&#32476;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#27979;&#35797;&#26102;&#30340;&#21453;&#39304;&#20449;&#21495;&#26469;&#23454;&#26102;&#35843;&#25972;&#31070;&#32463;&#32593;&#32476;&#65292;&#30456;&#27604;&#22522;&#20934;&#26041;&#27861;&#26356;&#21152;&#28789;&#27963;&#19988;&#24555;&#36895;&#25968;&#20010;&#25968;&#37327;&#32423;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#20219;&#21153;&#21644;&#20998;&#24067;&#21464;&#21270;&#19979;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27979;&#35797;&#26102;&#36866;&#24212;&#20998;&#24067;&#21464;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#12290;&#19982;&#35797;&#35757;&#32451;&#26102;&#30340;&#40065;&#26834;&#24615;&#26426;&#21046;&#19981;&#21516;&#65292;&#35797;&#22270;&#39044;&#27979;&#21644;&#23545;&#25239;&#21464;&#21270;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#38381;&#29615;&#31995;&#32479;&#65292;&#24182;&#21033;&#29992;&#27979;&#35797;&#26102;&#30340;&#21453;&#39304;&#20449;&#21495;&#23454;&#26102;&#35843;&#25972;&#32593;&#32476;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#32593;&#32476;&#30340;&#19968;&#31181;&#25674;&#38144;&#20248;&#21270;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;&#24555;&#36895;&#32593;&#32476;&#36866;&#24212;(RNA)&#30340;&#36866;&#24212;&#26041;&#27861;&#65292;&#23427;&#27604;&#22522;&#20934;&#26041;&#27861;&#26356;&#21152;&#28789;&#27963;&#19988;&#24555;&#36895;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;&#36890;&#36807;&#20351;&#29992;&#21508;&#31181;&#36866;&#24212;&#20449;&#21495;&#21644;&#30446;&#26631;&#20219;&#21153;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#21508;&#31181;&#25968;&#25454;&#38598;(Taskonomy&#12289;Replica&#12289;ScanNet&#12289;Hypersim&#12289;COCO&#12289;ImageNet)&#65292;&#20219;&#21153;(&#28145;&#24230;&#12289;&#20809;&#27969;&#12289;&#35821;&#20041;&#20998;&#21106;&#12289;&#20998;&#31867;)&#21644;&#20998;&#24067;&#21464;&#21270;(Cross-datasets&#12289;2D&#21644;3D Common Corruptions)&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method for adapting neural networks to distribution shifts at test-time. In contrast to training-time robustness mechanisms that attempt to anticipate and counter the shift, we create a closed-loop system and make use of a test-time feedback signal to adapt a network on the fly. We show that this loop can be effectively implemented using a learning-based function, which realizes an amortized optimizer for the network. This leads to an adaptation method, named Rapid Network Adaptation (RNA), that is notably more flexible and orders of magnitude faster than the baselines. Through a broad set of experiments using various adaptation signals and target tasks, we study the efficiency and flexibility of this method. We perform the evaluations using various datasets (Taskonomy, Replica, ScanNet, Hypersim, COCO, ImageNet), tasks (depth, optical flow, semantic segmentation, classification), and distribution shifts (Cross-datasets, 2D and 3D Common Corruptions) with promising results
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#22270;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#30340;&#34920;&#31034;&#26469;&#25429;&#25417;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23454;&#29616;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#30693;&#35782;&#30340;&#26377;&#25928;&#34701;&#21512;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.15757</link><description>&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#22270;&#30340;&#29983;&#29289;&#21307;&#23398;&#34920;&#26684;&#25968;&#25454;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Latent Graph Powered Semi-Supervised Learning on Biomedical Tabular Data. (arXiv:2309.15757v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#22270;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#30340;&#34920;&#31034;&#26469;&#25429;&#25417;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23454;&#29616;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#30693;&#35782;&#30340;&#26377;&#25928;&#34701;&#21512;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#20805;&#20998;&#21033;&#29992;&#65288;&#26377;&#65289;&#26631;&#35760;&#25968;&#25454;&#20043;&#38388;&#30340;&#23454;&#20363;&#38388;&#20851;&#31995;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#25512;&#26029;&#25429;&#25417;&#20869;&#22312;&#25968;&#25454;&#20851;&#31995;&#30340;&#28508;&#22312;&#22270;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20419;&#36827;&#20102;&#20449;&#24687;&#22312;&#25972;&#20010;&#22270;&#20013;&#30340;&#26080;&#32541;&#20256;&#25773;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#34701;&#21512;&#20840;&#23616;&#21644;&#23616;&#37096;&#30693;&#35782;&#12290;&#36890;&#36807;&#22312;&#29983;&#29289;&#21307;&#23398;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#24403;&#20195;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35777;&#26126;&#20102;&#21457;&#29616;&#23454;&#20363;&#38388;&#20851;&#31995;&#20316;&#20026;&#26500;&#24314;&#24378;&#21270;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#30340;&#40065;&#26834;&#28508;&#22312;&#22270;&#30340;&#23454;&#38469;&#25163;&#27573;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the domain of semi-supervised learning, the current approaches insufficiently exploit the potential of considering inter-instance relationships among (un)labeled data. In this work, we address this limitation by providing an approach for inferring latent graphs that capture the intrinsic data relationships. By leveraging graph-based representations, our approach facilitates the seamless propagation of information throughout the graph, enabling the effective incorporation of global and local knowledge. Through evaluations on biomedical tabular datasets, we compare the capabilities of our approach to other contemporary methods. Our work demonstrates the significance of inter-instance relationship discovery as practical means for constructing robust latent graphs to enhance semi-supervised learning techniques. Our method achieves state-of-the-art results on three biomedical datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#39564;&#25277;&#26679;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26080;&#38480;&#26399;&#26080;&#25240;&#25187;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#35813;&#31639;&#27861;&#22312;&#36951;&#25022;&#30028;&#38480;&#19978;&#25509;&#36817;&#26368;&#20248;&#65292;&#24182;&#22312;&#23454;&#35777;&#19978;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.15737</link><description>&lt;p&gt;
&#22312;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#20013;&#21487;&#20197;&#35777;&#26126;&#30340;&#39640;&#25928;&#25506;&#32034;&#65306;&#21518;&#39564;&#25277;&#26679;&#23601;&#36275;&#22815;&#20102;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient Exploration in Constrained Reinforcement Learning:Posterior Sampling Is All You Need. (arXiv:2309.15737v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15737
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#39564;&#25277;&#26679;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26080;&#38480;&#26399;&#26080;&#25240;&#25187;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#35813;&#31639;&#27861;&#22312;&#36951;&#25022;&#30028;&#38480;&#19978;&#25509;&#36817;&#26368;&#20248;&#65292;&#24182;&#22312;&#23454;&#35777;&#19978;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#39564;&#25277;&#26679;&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#26080;&#38480;&#26399;&#26080;&#25240;&#25187;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#20013;&#30340;&#23398;&#20064;&#12290;&#35813;&#31639;&#27861;&#22312;&#23454;&#35777;&#19978;&#30456;&#23545;&#20110;&#29616;&#26377;&#31639;&#27861;&#20855;&#26377;&#20248;&#21183;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#36817;&#20284;&#26368;&#20248;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#29702;&#35770;&#32467;&#26524;&#26159;&#23545;&#20110;&#20219;&#24847;&#36890;&#20449;CMDP&#65292;&#27599;&#20010;&#25104;&#26412;&#37096;&#20998;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#38480;&#20026;\tilde{O} (HS \sqrt{AT})&#65292;&#20854;&#20013;CMDP&#20855;&#26377;S&#20010;&#29366;&#24577;&#12289;A&#20010;&#34892;&#21160;&#21644;&#21629;&#20013;&#26102;&#38388;H&#30340;&#36793;&#30028;&#12290;&#36825;&#20010;&#36951;&#25022;&#30028;&#38480;&#19982;&#26102;&#38388;&#36328;&#24230;T&#30340;&#19979;&#30028;&#21305;&#37197;&#65292;&#24182;&#19988;&#26159;&#26080;&#38480;&#26399;&#26080;&#25240;&#25187;&#35774;&#32622;&#20013;&#36890;&#20449;CMDP&#30340;&#24050;&#30693;&#26368;&#20339;&#36951;&#25022;&#30028;&#38480;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#25105;&#20204;&#30340;&#21518;&#39564;&#25277;&#26679;&#31639;&#27861;&#38750;&#24120;&#31616;&#21333;&#65292;&#20294;&#22312;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#20013;&#32988;&#36807;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new algorithm based on posterior sampling for learning in constrained Markov decision processes (CMDP) in the infinite-horizon undiscounted setting. The algorithm achieves near-optimal regret bounds while being advantageous empirically compared to the existing algorithms. Our main theoretical result is a Bayesian regret bound for each cost component of \tilde{O} (HS \sqrt{AT}) for any communicating CMDP with S states, A actions, and bound on the hitting time H. This regret bound matches the lower bound in order of time horizon T and is the best-known regret bound for communicating CMDPs in the infinite-horizon undiscounted setting. Empirical results show that, despite its simplicity, our posterior sampling algorithm outperforms the existing algorithms for constrained reinforcement learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#22312;&#34920;&#24449;&#21508;&#31181;&#21160;&#21147;&#31995;&#32479;&#30340;&#21560;&#24341;&#30406;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#21487;&#39044;&#27979;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#19988;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.15732</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21560;&#24341;&#30406;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-based Analysis of Basins of Attraction. (arXiv:2309.15732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#22312;&#34920;&#24449;&#21508;&#31181;&#21160;&#21147;&#31995;&#32479;&#30340;&#21560;&#24341;&#30406;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#21487;&#39044;&#27979;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#19988;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#34920;&#24449;&#19981;&#21516;&#21160;&#21147;&#31995;&#32479;&#21560;&#24341;&#30406;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#21487;&#39044;&#27979;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#22312;&#25506;&#32034;&#21160;&#21147;&#31995;&#32479;&#30340;&#19981;&#21516;&#21442;&#25968;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#65292;&#22240;&#20026;&#20256;&#32479;&#26041;&#27861;&#22312;&#34920;&#24449;&#22810;&#20010;&#21560;&#24341;&#30406;&#26102;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#27604;&#36739;&#20102;&#19981;&#21516;CNN&#20307;&#31995;&#32467;&#26500;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#65292;&#21363;&#20351;&#20351;&#29992;&#36807;&#26102;&#30340;&#20307;&#31995;&#32467;&#26500;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study showcases the effectiveness of convolutional neural networks (CNNs) in characterizing the complexity and unpredictability of basins of attraction for diverse dynamical systems. This novel method is optimal for exploring different parameters of dynamical systems since the conventional methods are computationally expensive for characterizing multiple basins of attraction. Additionally, our research includes a comparison of different CNN architectures for this task showing the superiority of our proposed characterization method over the conventional methods, even with obsolete architectures.
&lt;/p&gt;</description></item><item><title>&#26102;&#38388;&#22270;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#20840;&#23616;&#26102;&#38388;&#21160;&#24577;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;"&#26368;&#36817;&#27969;&#34892;&#33410;&#28857;"&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#22312;&#26102;&#38388;&#22270;&#22522;&#20934;&#30340;&#20013;&#31561;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#20840;&#23616;&#21160;&#24577;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;&#30340;&#36127;&#37319;&#26679;&#35780;&#20272;&#26041;&#27861;&#22312;&#20855;&#26377;&#24378;&#28872;&#26102;&#38388;&#21160;&#24577;&#30340;&#25968;&#25454;&#38598;&#19978;&#21487;&#33021;&#19981;&#36866;&#29992;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36864;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#36127;&#37319;&#26679;&#26041;&#26696;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#23558;&#20854;&#19982;&#26080;&#36127;&#37319;&#26679;&#30340;&#38750;&#23545;&#27604;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2309.15730</link><description>&lt;p&gt;
&#26102;&#38388;&#22270;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#20840;&#23616;&#26102;&#38388;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Temporal graph models fail to capture global temporal dynamics. (arXiv:2309.15730v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15730
&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#22270;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#20840;&#23616;&#26102;&#38388;&#21160;&#24577;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;"&#26368;&#36817;&#27969;&#34892;&#33410;&#28857;"&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#22312;&#26102;&#38388;&#22270;&#22522;&#20934;&#30340;&#20013;&#31561;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#20840;&#23616;&#21160;&#24577;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;&#30340;&#36127;&#37319;&#26679;&#35780;&#20272;&#26041;&#27861;&#22312;&#20855;&#26377;&#24378;&#28872;&#26102;&#38388;&#21160;&#24577;&#30340;&#25968;&#25454;&#38598;&#19978;&#21487;&#33021;&#19981;&#36866;&#29992;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36864;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#36127;&#37319;&#26679;&#26041;&#26696;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#23558;&#20854;&#19982;&#26080;&#36127;&#37319;&#26679;&#30340;&#38750;&#23545;&#27604;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#38142;&#25509;&#23646;&#24615;&#39044;&#27979;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26368;&#36817;&#21457;&#24067;&#30340;&#26102;&#38388;&#22270;&#22522;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;"&#26368;&#36817;&#27969;&#34892;&#33410;&#28857;"&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#22312;&#26102;&#38388;&#22270;&#22522;&#20934;&#30340;&#20013;&#31561;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#20004;&#20010;&#24230;&#37327;&#65292;&#21487;&#20197;&#37327;&#21270;&#25968;&#25454;&#38598;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#20840;&#23616;&#21160;&#24577;&#30340;&#24378;&#24230;&#12290;&#36890;&#36807;&#20998;&#26512;&#25105;&#20204;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#22522;&#32447;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;&#30340;&#36127;&#37319;&#26679;&#35780;&#20272;&#26041;&#27861;&#22312;&#20855;&#26377;&#24378;&#28872;&#26102;&#38388;&#21160;&#24577;&#30340;&#25968;&#25454;&#38598;&#19978;&#21487;&#33021;&#19981;&#36866;&#29992;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36864;&#21270;&#65292;&#23548;&#33268;&#26080;&#27861;&#23545;&#26102;&#38388;&#22270;&#32593;&#32476;&#36827;&#34892;&#25490;&#24207;&#30340;&#39044;&#27979;&#23436;&#20840;&#39281;&#21644;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#36127;&#37319;&#26679;&#26041;&#26696;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#23558;&#20854;&#19982;&#26080;&#36127;&#37319;&#26679;&#30340;&#38750;&#23545;&#27604;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
A recently released Temporal Graph Benchmark is analyzed in the context of Dynamic Link Property Prediction. We outline our observations and propose a trivial optimization-free baseline of "recently popular nodes" outperforming other methods on all medium and large-size datasets in the Temporal Graph Benchmark. We propose two measures based on Wasserstein distance which can quantify the strength of short-term and long-term global dynamics of datasets. By analyzing our unexpectedly strong baseline, we show how standard negative sampling evaluation can be unsuitable for datasets with strong temporal dynamics. We also show how simple negative-sampling can lead to model degeneration during training, resulting in impossible to rank, fully saturated predictions of temporal graph networks. We propose improved negative sampling schemes for both training and evaluation and prove their usefulness. We conduct a comparison with a model trained non-contrastively without negative sampling. Our resul
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#30446;&#26631;&#35757;&#32451;&#27169;&#22411;&#26469;&#23454;&#29616;&#21516;&#26102;&#29983;&#25104;&#21644;&#20998;&#21106;&#22270;&#20687;&#12290;&#36825;&#31181;&#26550;&#26500;&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#21010;&#20998;&#21306;&#22495;&#24182;&#24182;&#34892;&#21435;&#22122;&#20197;&#21450;&#21512;&#24182;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#26080;&#30417;&#30563;&#22270;&#20687;&#20998;&#21106;&#21644;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#22270;&#20687;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.15726</link><description>&lt;p&gt;
&#22240;&#24335;&#20998;&#35299;&#25193;&#25955;&#26550;&#26500;&#29992;&#20110;&#26080;&#30417;&#30563;&#22270;&#20687;&#29983;&#25104;&#21644;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Factorized Diffusion Architectures for Unsupervised Image Generation and Segmentation. (arXiv:2309.15726v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15726
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#30446;&#26631;&#35757;&#32451;&#27169;&#22411;&#26469;&#23454;&#29616;&#21516;&#26102;&#29983;&#25104;&#21644;&#20998;&#21106;&#22270;&#20687;&#12290;&#36825;&#31181;&#26550;&#26500;&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#21010;&#20998;&#21306;&#22495;&#24182;&#24182;&#34892;&#21435;&#22122;&#20197;&#21450;&#21512;&#24182;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#26080;&#30417;&#30563;&#22270;&#20687;&#20998;&#21106;&#21644;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20197;&#38750;&#30417;&#30563;&#26041;&#24335;&#20316;&#20026;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;&#23398;&#20064;&#29983;&#25104;&#21644;&#20998;&#21106;&#22270;&#20687;&#12290;&#23398;&#20064;&#23436;&#20840;&#26159;&#30001;&#21435;&#22122;&#25193;&#25955;&#30446;&#26631;&#39537;&#21160;&#30340;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#27809;&#26377;&#20219;&#20309;&#27880;&#37322;&#25110;&#20851;&#20110;&#21306;&#22495;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#30340;&#35745;&#31639;&#29942;&#39048;&#40723;&#21169;&#21435;&#22122;&#32593;&#32476;&#23558;&#36755;&#20837;&#21010;&#20998;&#20026;&#21306;&#22495;&#65292;&#21516;&#26102;&#23545;&#23427;&#20204;&#36827;&#34892;&#21435;&#22122;&#65292;&#24182;&#23558;&#32467;&#26524;&#21512;&#24182;&#12290;&#25105;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;&#36890;&#36807;&#31616;&#21333;&#26816;&#26597;&#20854;&#20869;&#37096;&#39044;&#27979;&#30340;&#20998;&#21306;&#65292;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#21644;&#36825;&#20123;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;&#25105;&#20204;&#30452;&#25509;&#23558;&#25105;&#20204;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#24212;&#29992;&#20110;&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#28982;&#21518;&#21435;&#22122;&#26469;&#20998;&#21106;&#30495;&#23454;&#22270;&#20687;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#26080;&#38656;&#20219;&#20309;&#24494;&#35843;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#26080;&#30417;&#30563;&#22270;&#20687;&#20998;&#21106;&#21644;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a neural network architecture which, trained in an unsupervised manner as a denoising diffusion model, simultaneously learns to both generate and segment images. Learning is driven entirely by the denoising diffusion objective, without any annotation or prior knowledge about regions during training. A computational bottleneck, built into the neural architecture, encourages the denoising network to partition an input into regions, denoise them in parallel, and combine the results. Our trained model generates both synthetic images and, by simple examination of its internal predicted partitions, a semantic segmentation of those images. Without any finetuning, we directly apply our unsupervised model to the downstream task of segmenting real images via noising and subsequently denoising them. Experiments demonstrate that our model achieves accurate unsupervised image segmentation and high-quality synthetic image generation across multiple datasets.
&lt;/p&gt;</description></item><item><title>Model Share AI&#26159;&#19968;&#20010;&#29992;&#20110;&#21327;&#20316;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24320;&#21457;&#12289;&#26469;&#28304;&#36861;&#36394;&#21644;&#37096;&#32626;&#30340;&#38598;&#25104;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#21327;&#20316;&#39033;&#30446;&#31354;&#38388;&#12289;&#26631;&#20934;&#21270;&#30340;&#27169;&#22411;&#35780;&#20272;&#27969;&#31243;&#21644;&#33258;&#21160;&#21270;&#30340;&#27169;&#22411;&#37096;&#32626;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15719</link><description>&lt;p&gt;
Model Share AI: &#19968;&#20010;&#38598;&#25104;&#30340;Python&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#21327;&#20316;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24320;&#21457;&#12289;&#26469;&#28304;&#36861;&#36394;&#21644;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
Model Share AI: An Integrated Toolkit for Collaborative Machine Learning Model Development, Provenance Tracking, and Deployment in Python. (arXiv:2309.15719v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15719
&lt;/p&gt;
&lt;p&gt;
Model Share AI&#26159;&#19968;&#20010;&#29992;&#20110;&#21327;&#20316;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24320;&#21457;&#12289;&#26469;&#28304;&#36861;&#36394;&#21644;&#37096;&#32626;&#30340;&#38598;&#25104;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#21327;&#20316;&#39033;&#30446;&#31354;&#38388;&#12289;&#26631;&#20934;&#21270;&#30340;&#27169;&#22411;&#35780;&#20272;&#27969;&#31243;&#21644;&#33258;&#21160;&#21270;&#30340;&#27169;&#22411;&#37096;&#32626;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26377;&#28508;&#21147;&#24443;&#24213;&#25913;&#21464;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#21644;&#34892;&#19994;&#65292;&#20294;&#35768;&#22810;ML&#39033;&#30446;&#20174;&#27010;&#24565;&#39564;&#35777;&#38454;&#27573;&#23601;&#26080;&#27861;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Model Share AI&#65288;AIMS&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;MLOps&#24179;&#21488;&#65292;&#26088;&#22312;&#31616;&#21270;&#21327;&#20316;&#27169;&#22411;&#24320;&#21457;&#12289;&#27169;&#22411;&#26469;&#28304;&#36861;&#36394;&#21644;&#27169;&#22411;&#37096;&#32626;&#65292;&#20197;&#21450;&#19968;&#31995;&#21015;&#20854;&#20182;&#21151;&#33021;&#65292;&#20197;&#26368;&#22823;&#21270;ML&#30740;&#31350;&#30340;&#23454;&#38469;&#24433;&#21709;&#12290;AIMS&#20855;&#26377;&#21327;&#20316;&#39033;&#30446;&#31354;&#38388;&#21644;&#26631;&#20934;&#21270;&#30340;&#27169;&#22411;&#35780;&#20272;&#27969;&#31243;&#65292;&#26681;&#25454;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#35780;&#20272;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#23545;&#27169;&#22411;&#25552;&#20132;&#36827;&#34892;&#25490;&#21517;&#65292;&#23454;&#29616;&#20102;&#21327;&#20316;&#27169;&#22411;&#24320;&#21457;&#21644;&#20247;&#21253;&#12290;&#33258;&#21160;&#25429;&#33719;&#27169;&#22411;&#24615;&#33021;&#21644;&#21508;&#31181;&#27169;&#22411;&#20803;&#25968;&#25454;&#65292;&#20197;&#20419;&#36827;&#26469;&#28304;&#36861;&#36394;&#24182;&#20801;&#35768;&#29992;&#25143;&#23398;&#20064;&#21644;&#20511;&#37492;&#20043;&#21069;&#30340;&#25552;&#20132;&#12290;&#27492;&#22806;&#65292;AIMS&#20801;&#35768;&#29992;&#25143;&#23558;&#22312;Scikit-Learn&#12289;TensorFlow Keras&#12289;PyTorch&#21644;ONNX&#20013;&#26500;&#24314;&#30340;ML&#27169;&#22411;&#37096;&#32626;&#21040;&#23454;&#26102;&#30340;REST API&#21644;au
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) has the potential to revolutionize a wide range of research areas and industries, but many ML projects never progress past the proof-of-concept stage. To address this issue, we introduce Model Share AI (AIMS), an easy-to-use MLOps platform designed to streamline collaborative model development, model provenance tracking, and model deployment, as well as a host of other functions aiming to maximize the real-world impact of ML research. AIMS features collaborative project spaces and a standardized model evaluation process that ranks model submissions based on their performance on unseen evaluation data, enabling collaborative model development and crowd-sourcing. Model performance and various model metadata are automatically captured to facilitate provenance tracking and allow users to learn from and build on previous submissions. Additionally, AIMS allows users to deploy ML models built in Scikit-Learn, TensorFlow Keras, PyTorch, and ONNX into live REST APIs and au
&lt;/p&gt;</description></item><item><title>Timbre-Trap&#26159;&#19968;&#20010;&#20302;&#36164;&#28304;&#26694;&#26550;&#65292;&#23558;&#38899;&#20048;&#36716;&#24405;&#21644;&#38899;&#39057;&#37325;&#26500;&#32479;&#19968;&#36215;&#26469;&#65292;&#36890;&#36807;&#21033;&#29992;&#38899;&#39640;&#21644;&#38899;&#33394;&#30340;&#24378;&#20998;&#31163;&#24615;&#65292;&#21516;&#26102;&#20272;&#35745;&#38899;&#39640;&#26174;&#33879;&#24230;&#21644;&#37325;&#26500;&#39057;&#35889;&#31995;&#25968;&#65292;&#21462;&#24471;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15717</link><description>&lt;p&gt;
Timbre-Trap:&#19968;&#31181;&#20302;&#36164;&#28304;&#26694;&#26550;&#29992;&#20110;&#19982;&#20048;&#22120;&#26080;&#20851;&#30340;&#38899;&#20048;&#36716;&#24405;
&lt;/p&gt;
&lt;p&gt;
Timbre-Trap: A Low-Resource Framework for Instrument-Agnostic Music Transcription. (arXiv:2309.15717v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15717
&lt;/p&gt;
&lt;p&gt;
Timbre-Trap&#26159;&#19968;&#20010;&#20302;&#36164;&#28304;&#26694;&#26550;&#65292;&#23558;&#38899;&#20048;&#36716;&#24405;&#21644;&#38899;&#39057;&#37325;&#26500;&#32479;&#19968;&#36215;&#26469;&#65292;&#36890;&#36807;&#21033;&#29992;&#38899;&#39640;&#21644;&#38899;&#33394;&#30340;&#24378;&#20998;&#31163;&#24615;&#65292;&#21516;&#26102;&#20272;&#35745;&#38899;&#39640;&#26174;&#33879;&#24230;&#21644;&#37325;&#26500;&#39057;&#35889;&#31995;&#25968;&#65292;&#21462;&#24471;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38899;&#20048;&#36716;&#24405;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#26550;&#26500;&#35774;&#35745;&#21644;&#20048;&#22120;&#29305;&#23450;&#25968;&#25454;&#37319;&#38598;&#19978;&#12290;&#30001;&#20110;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#65292;&#36827;&#23637;&#36890;&#24120;&#20165;&#38480;&#20110;&#38050;&#29748;&#36716;&#24405;&#31561;&#21333;&#20048;&#22120;&#20219;&#21153;&#12290;&#19968;&#20123;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#20048;&#22120;&#36716;&#24405;&#20316;&#20026;&#19968;&#31181;&#22686;&#24378;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#20219;&#21153;&#19978;&#24615;&#33021;&#30340;&#25163;&#27573;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#30528;&#21516;&#26679;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;Timbre-Trap&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#38899;&#39640;&#21644;&#38899;&#33394;&#20043;&#38388;&#30340;&#24378;&#20998;&#31163;&#24615;&#23558;&#38899;&#20048;&#36716;&#24405;&#21644;&#38899;&#39057;&#37325;&#26500;&#32479;&#19968;&#36215;&#26469;&#12290;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#21333;&#29420;&#30340;U-Net&#26469;&#21516;&#26102;&#20272;&#35745;&#38899;&#39640;&#26174;&#33879;&#24230;&#21644;&#37325;&#26500;&#22797;&#26434;&#30340;&#39057;&#35889;&#31995;&#25968;&#65292;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#20999;&#25442;&#26426;&#21046;&#22312;&#35299;&#30721;&#38454;&#27573;&#36873;&#25321;&#20004;&#32773;&#20043;&#19968;&#30340;&#36755;&#20986;&#12290;&#36825;&#26679;&#65292;&#27169;&#22411;&#23398;&#20250;&#20102;&#20135;&#29983;&#23545;&#24212;&#20110;&#27809;&#26377;&#38899;&#33394;&#30340;&#38899;&#39057;&#30340;&#31995;&#25968;&#65292;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#38899;&#39640;&#26174;&#33879;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#33021;&#22815;&#21462;&#24471;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, research on music transcription has focused mainly on architecture design and instrument-specific data acquisition. With the lack of availability of diverse datasets, progress is often limited to solo-instrument tasks such as piano transcription. Several works have explored multi-instrument transcription as a means to bolster the performance of models on low-resource tasks, but these methods face the same data availability issues. We propose Timbre-Trap, a novel framework which unifies music transcription and audio reconstruction by exploiting the strong separability between pitch and timbre. We train a single U-Net to simultaneously estimate pitch salience and reconstruct complex spectral coefficients, selecting between either output during the decoding stage via a simple switch mechanism. In this way, the model learns to produce coefficients corresponding to timbre-less audio, which can be interpreted as pitch salience. We demonstrate that the framework leads to perf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20351;&#29992;&#26368;&#22823;&#29109;&#21407;&#29702;&#30340;&#26368;&#22823;&#26435;&#37325;&#29109;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#26435;&#37325;&#22810;&#26679;&#24615;&#26469;&#35299;&#20915;&#26631;&#20934;&#26041;&#27861;&#22312;&#36229;&#20986;&#20998;&#24067;&#24773;&#20917;&#19979;&#39044;&#27979;&#22810;&#26679;&#24615;&#32570;&#20047;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.15704</link><description>&lt;p&gt;
&#26368;&#22823;&#26435;&#37325;&#29109;
&lt;/p&gt;
&lt;p&gt;
Maximum Weight Entropy. (arXiv:2309.15704v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20351;&#29992;&#26368;&#22823;&#29109;&#21407;&#29702;&#30340;&#26368;&#22823;&#26435;&#37325;&#29109;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#26435;&#37325;&#22810;&#26679;&#24615;&#26469;&#35299;&#20915;&#26631;&#20934;&#26041;&#27861;&#22312;&#36229;&#20986;&#20998;&#24067;&#24773;&#20917;&#19979;&#39044;&#27979;&#22810;&#26679;&#24615;&#32570;&#20047;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20351;&#29992;&#36125;&#21494;&#26031;&#21644;&#38598;&#25104;&#26041;&#27861;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;&#24403;&#22312;&#36229;&#20986;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#26631;&#20934;&#26041;&#27861;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#39044;&#27979;&#22810;&#26679;&#24615;&#30340;&#32570;&#20047;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35748;&#20026;&#26631;&#20934;&#26041;&#27861;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#37319;&#26679;&#30340;&#8220;&#36807;&#24230;&#32422;&#26463;&#8221;&#23548;&#33268;&#20102;&#26435;&#37325;&#22810;&#26679;&#24615;&#30340;&#32570;&#20047;&#12290;&#26412;&#25991;&#24314;&#35758;&#37319;&#29992;&#26368;&#22823;&#29109;&#21407;&#29702;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#26435;&#37325;&#22810;&#26679;&#24615;&#65292;&#25551;&#36848;&#26368;&#22823;&#29109;&#26435;&#37325;&#20998;&#24067;&#26469;&#34920;&#31034;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#20135;&#29983;&#19982;&#35757;&#32451;&#35266;&#23519;&#19968;&#33268;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper deals with uncertainty quantification and out-of-distribution detection in deep learning using Bayesian and ensemble methods. It proposes a practical solution to the lack of prediction diversity observed recently for standard approaches when used out-of-distribution (Ovadia et al., 2019; Liu et al., 2021). Considering that this issue is mainly related to a lack of weight diversity, we claim that standard methods sample in "over-restricted" regions of the weight space due to the use of "over-regularization" processes, such as weight decay and zero-mean centered Gaussian priors. We propose to solve the problem by adopting the maximum entropy principle for the weight distribution, with the underlying idea to maximize the weight diversity. Under this paradigm, the epistemic uncertainty is described by the weight distribution of maximal entropy that produces neural networks "consistent" with the training observations. Considering stochastic neural networks, a practical optimizati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#20462;&#27491;&#65292;&#23454;&#29616;&#20102;&#19982;&#20154;&#31867;&#27700;&#24179;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.15701</link><description>&lt;p&gt;
HyPoradise&#65306;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24335;&#35821;&#38899;&#35782;&#21035;&#30340;&#24320;&#25918;&#22522;&#20934;&#32447;
&lt;/p&gt;
&lt;p&gt;
HyPoradise: An Open Baseline for Generative Speech Recognition with Large Language Models. (arXiv:2309.15701v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#20462;&#27491;&#65292;&#23454;&#29616;&#20102;&#19982;&#20154;&#31867;&#27700;&#24179;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36827;&#23637;&#20351;&#24471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#22312;&#20960;&#20010;&#20844;&#24320;&#30340;&#24178;&#20928;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#20154;&#31867;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#22312;&#38754;&#23545;&#36870;&#22659;&#26102;&#20063;&#20250;&#20986;&#29616;&#24615;&#33021;&#19979;&#38477;&#65292;&#22240;&#20026;&#33391;&#22909;&#35757;&#32451;&#30340;&#22768;&#23398;&#27169;&#22411;&#23545;&#20110;&#35821;&#38899;&#39046;&#22495;&#30340;&#21464;&#24322;&#24615;&#24456;&#25935;&#24863;&#65292;&#22914;&#32972;&#26223;&#22122;&#22768;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#21033;&#29992;&#22806;&#37096;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#20462;&#27491;&#65292;&#20854;&#20013;N&#26368;&#20339;&#35299;&#30721;&#20551;&#35774;&#20026;&#30495;&#23454;&#36716;&#24405;&#39044;&#27979;&#25552;&#20379;&#20102;&#26377;&#20449;&#24687;&#37327;&#30340;&#20803;&#32032;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#37325;&#35780;&#20998;&#31574;&#30053;&#19981;&#21516;&#65292;&#21518;&#32773;&#21482;&#33021;&#36873;&#25321;&#19968;&#20010;&#20505;&#36873;&#20551;&#35774;&#20316;&#20026;&#26368;&#32456;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in deep neural networks have allowed automatic speech recognition (ASR) systems to attain human parity on several publicly available clean speech datasets. However, even state-of-the-art ASR systems experience performance degradation when confronted with adverse conditions, as a well-trained acoustic model is sensitive to variations in the speech domain, e.g., background noise. Intuitively, humans address this issue by relying on their linguistic knowledge: the meaning of ambiguous spoken terms is usually inferred from contextual cues thereby reducing the dependency on the auditory system. Inspired by this observation, we introduce the first open-source benchmark to utilize external large language models (LLMs) for ASR error correction, where N-best decoding hypotheses provide informative elements for true transcription prediction. This approach is a paradigm shift from the traditional language model rescoring strategy that can only select one candidate hypothesis as the o
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#27169;&#22411;&#34701;&#21512;&#26159;&#19968;&#31181;&#26032;&#20852;&#25216;&#26415;&#65292;&#23558;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21442;&#25968;&#25110;&#39044;&#27979;&#21512;&#24182;&#21040;&#21333;&#20010;&#27169;&#22411;&#20013;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#65292;&#22914;&#39640;&#35745;&#31639;&#25104;&#26412;&#12289;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#12289;&#19981;&#21516;&#24322;&#26500;&#27169;&#22411;&#20043;&#38388;&#30340;&#24178;&#25200;&#31561;&#12290;&#26412;&#35770;&#25991;&#20026;&#20102;&#26356;&#22909;&#22320;&#20102;&#35299;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#24182;&#25512;&#21160;&#20854;&#21457;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#39033;&#32508;&#21512;&#35843;&#26597;&#65292;&#24635;&#32467;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.15698</link><description>&lt;p&gt;
&#28145;&#24230;&#27169;&#22411;&#34701;&#21512;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Model Fusion: A Survey. (arXiv:2309.15698v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15698
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#27169;&#22411;&#34701;&#21512;&#26159;&#19968;&#31181;&#26032;&#20852;&#25216;&#26415;&#65292;&#23558;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21442;&#25968;&#25110;&#39044;&#27979;&#21512;&#24182;&#21040;&#21333;&#20010;&#27169;&#22411;&#20013;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#65292;&#22914;&#39640;&#35745;&#31639;&#25104;&#26412;&#12289;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#12289;&#19981;&#21516;&#24322;&#26500;&#27169;&#22411;&#20043;&#38388;&#30340;&#24178;&#25200;&#31561;&#12290;&#26412;&#35770;&#25991;&#20026;&#20102;&#26356;&#22909;&#22320;&#20102;&#35299;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#24182;&#25512;&#21160;&#20854;&#21457;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#39033;&#32508;&#21512;&#35843;&#26597;&#65292;&#24635;&#32467;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#27169;&#22411;&#34701;&#21512;&#26159;&#19968;&#31181;&#26032;&#20852;&#25216;&#26415;&#65292;&#23558;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21442;&#25968;&#25110;&#39044;&#27979;&#21512;&#24182;&#21040;&#21333;&#20010;&#27169;&#22411;&#20013;&#12290;&#23427;&#32467;&#21512;&#20102;&#19981;&#21516;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#24357;&#34917;&#20102;&#21333;&#20010;&#27169;&#22411;&#30340;&#20559;&#24046;&#21644;&#35823;&#24046;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914;LLMs&#21644;&#22522;&#30784;&#27169;&#22411;&#65289;&#19978;&#36827;&#34892;&#28145;&#24230;&#27169;&#22411;&#34701;&#21512;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#39640;&#35745;&#31639;&#25104;&#26412;&#12289;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#12289;&#19981;&#21516;&#24322;&#26500;&#27169;&#22411;&#20043;&#38388;&#30340;&#24178;&#25200;&#31561;&#12290;&#23613;&#31649;&#27169;&#22411;&#34701;&#21512;&#30001;&#20110;&#20854;&#35299;&#20915;&#22797;&#26434;&#23454;&#38469;&#20219;&#21153;&#30340;&#28508;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#23545;&#35813;&#25216;&#26415;&#30340;&#23436;&#25972;&#21644;&#35814;&#32454;&#35843;&#26597;&#30740;&#31350;&#20173;&#28982;&#19981;&#36275;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#20102;&#35299;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#24182;&#25512;&#21160;&#20854;&#21457;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#32508;&#21512;&#35843;&#26597;&#65292;&#24635;&#32467;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#28145;&#24230;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#20998;&#20026;&#22235;&#20010;&#31867;&#21035;&#65306;&#65288;1&#65289;"&#27169;&#24335;&#36830;&#25509;"&#65292;&#21363;&#23558;&#22810;&#20010;&#27169;&#22411;&#36890;&#36807;&#36830;&#25509;&#26041;&#24335;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep model fusion/merging is an emerging technique that merges the parameters or predictions of multiple deep learning models into a single one. It combines the abilities of different models to make up for the biases and errors of a single model to achieve better performance. However, deep model fusion on large-scale deep learning models (e.g., LLMs and foundation models) faces several challenges, including high computational cost, high-dimensional parameter space, interference between different heterogeneous models, etc. Although model fusion has attracted widespread attention due to its potential to solve complex real-world tasks, there is still a lack of complete and detailed survey research on this technique. Accordingly, in order to understand the model fusion method better and promote its development, we present a comprehensive survey to summarize the recent progress. Specifically, we categorize existing deep model fusion methods as four-fold: (1) "Mode connectivity", which conne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24046;&#20998;&#38544;&#31169;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#31995;&#32479;&#21270;&#20102;&#26041;&#27861;&#65292;&#20026;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#38656;&#27714;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#32852;&#21512;&#35774;&#35745;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.15696</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#30340;&#32479;&#19968;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Unified View of Differentially Private Deep Generative Modeling. (arXiv:2309.15696v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24046;&#20998;&#38544;&#31169;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#31995;&#32479;&#21270;&#20102;&#26041;&#27861;&#65292;&#20026;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#38656;&#27714;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#32852;&#21512;&#35774;&#35745;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23500;&#39286;&#19988;&#24191;&#27867;&#30340;&#25968;&#25454;&#28304;&#30340;&#21487;&#29992;&#24615;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#21508;&#20010;&#39046;&#22495;&#20013;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#30340;&#25968;&#25454;&#24102;&#26469;&#20102;&#20005;&#26684;&#30340;&#38480;&#21046;&#65292;&#32463;&#24120;&#31105;&#27490;&#25968;&#25454;&#35775;&#38382;&#21644;&#25968;&#25454;&#20849;&#20139;&#12290;&#22312;&#36981;&#23432;&#38544;&#31169;&#32771;&#34385;&#30340;&#21069;&#25552;&#19979;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#23545;&#20110;&#28041;&#21450;&#38544;&#31169;&#25935;&#24863;&#25968;&#25454;&#30340;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#25216;&#26415;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#12290;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#25968;&#25454;&#21457;&#24067;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#21482;&#20844;&#24320;&#21457;&#24067;&#25968;&#25454;&#30340;&#19968;&#31181;&#32463;&#36807;&#20928;&#21270;&#22788;&#29702;&#30340;&#24418;&#24335;&#65292;&#23454;&#29616;&#20102;&#20445;&#25252;&#38544;&#31169;&#30340;&#19979;&#28216;&#20998;&#26512;&#21644;&#21487;&#37325;&#22797;&#30740;&#31350;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#36827;&#34892;&#31169;&#26377;&#22521;&#35757;&#65292;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#39640;&#32500;&#25968;&#25454;&#29983;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#31995;&#32479;&#21270;&#36825;&#20123;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#35270;&#35282;&#20026;&#31995;&#32479;&#22320;&#34893;&#29983;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#38656;&#27714;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#32852;&#21512;&#35774;&#35745;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
The availability of rich and vast data sources has greatly advanced machine learning applications in various domains. However, data with privacy concerns comes with stringent regulations that frequently prohibited data access and data sharing. Overcoming these obstacles in compliance with privacy considerations is key for technological progress in many real-world application scenarios that involve privacy sensitive data. Differentially private (DP) data publishing provides a compelling solution, where only a sanitized form of the data is publicly released, enabling privacy-preserving downstream analysis and reproducible research in sensitive domains. In recent years, various approaches have been proposed for achieving privacy-preserving high-dimensional data generation by private training on top of deep neural networks. In this paper, we present a novel unified view that systematizes these approaches. Our view provides a joint design space for systematically deriving methods that cater
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;NoC&#26550;&#26500;&#20013;&#29616;&#26377;&#21311;&#21517;&#36335;&#30001;&#21327;&#35758;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;&#21311;&#21517;&#36335;&#30001;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27969;&#30456;&#20851;&#25915;&#20987;&#26131;&#21463;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21311;&#21517;&#36335;&#30001;&#65292;&#20351;&#29992;&#27969;&#37327;&#28151;&#28102;&#25216;&#26415;&#65292;&#21487;&#20197;&#25269;&#24481;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27969;&#30456;&#20851;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2309.15687</link><description>&lt;p&gt;
&#25171;&#30772;NoC&#21311;&#21517;&#24615;&#20351;&#29992;&#27969;&#30456;&#20851;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Breaking NoC Anonymity using Flow Correlation Attack. (arXiv:2309.15687v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;NoC&#26550;&#26500;&#20013;&#29616;&#26377;&#21311;&#21517;&#36335;&#30001;&#21327;&#35758;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;&#21311;&#21517;&#36335;&#30001;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27969;&#30456;&#20851;&#25915;&#20987;&#26131;&#21463;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21311;&#21517;&#36335;&#30001;&#65292;&#20351;&#29992;&#27969;&#37327;&#28151;&#28102;&#25216;&#26415;&#65292;&#21487;&#20197;&#25269;&#24481;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27969;&#30456;&#20851;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#29255;&#19978;&#20114;&#36830;&#65288;NoC&#65289;&#24191;&#27867;&#29992;&#20316;&#24403;&#20170;&#22810;&#26680;&#29255;&#19978;&#31995;&#32479;&#65288;SoC&#65289;&#35774;&#35745;&#20013;&#30340;&#20869;&#37096;&#36890;&#20449;&#32467;&#26500;&#12290;&#29255;&#19978;&#36890;&#20449;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#21033;&#29992;&#20849;&#20139;&#30340;NoC&#20013;&#30340;&#20219;&#20309;&#28431;&#27934;&#23545;&#25915;&#20987;&#32773;&#26469;&#35828;&#37117;&#26159;&#19968;&#20010;&#23500;&#30719;&#12290;NoC&#23433;&#20840;&#20381;&#36182;&#20110;&#23545;&#21508;&#31181;&#25915;&#20987;&#30340;&#26377;&#25928;&#38450;&#33539;&#25514;&#26045;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;NoC&#26550;&#26500;&#20013;&#29616;&#26377;&#21311;&#21517;&#36335;&#30001;&#21327;&#35758;&#30340;&#23433;&#20840;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#20316;&#20986;&#20102;&#20004;&#20010;&#37325;&#35201;&#36129;&#29486;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;&#21311;&#21517;&#36335;&#30001;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#27969;&#30456;&#20851;&#25915;&#20987;&#26159;&#26131;&#21463;&#25915;&#20987;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21311;&#21517;&#36335;&#30001;&#65292;&#20351;&#29992;&#27969;&#37327;&#28151;&#28102;&#25216;&#26415;&#65292;&#21487;&#20197;&#25269;&#24481;&#22522;&#20110;ML&#30340;&#27969;&#30456;&#20851;&#25915;&#20987;&#12290;&#20351;&#29992;&#23454;&#38469;&#21644;&#21512;&#25104;&#27969;&#37327;&#36827;&#34892;&#30340;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#33021;&#22815;&#25104;&#21151;&#22320;&#23545;&#25239;NoC&#26550;&#26500;&#20013;&#26368;&#20808;&#36827;&#30340;&#21311;&#21517;&#36335;&#30001;&#65292;&#23545;&#20110;&#22810;&#31181;&#27969;&#37327;&#27169;&#24335;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#39640;&#36798;99&#65285;&#65292;&#21516;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network-on-Chip (NoC) is widely used as the internal communication fabric in today's multicore System-on-Chip (SoC) designs. Security of the on-chip communication is crucial because exploiting any vulnerability in shared NoC would be a goldmine for an attacker. NoC security relies on effective countermeasures against diverse attacks. We investigate the security strength of existing anonymous routing protocols in NoC architectures. Specifically, this paper makes two important contributions. We show that the existing anonymous routing is vulnerable to machine learning (ML) based flow correlation attacks on NoCs. We propose a lightweight anonymous routing that use traffic obfuscation techniques which can defend against ML-based flow correlation attacks. Experimental studies using both real and synthetic traffic reveal that our proposed attack is successful against state-of-the-art anonymous routing in NoC architectures with a high accuracy (up to 99%) for diverse traffic patterns, while o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36870;&#28210;&#26579;&#30340;&#32852;&#21512;&#37319;&#26679;&#19982;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#37319;&#26679;&#21644;&#20248;&#21270;&#20197;&#20943;&#23567;&#26041;&#24046;&#65292;&#20351;&#29992;&#26377;&#38480;&#24046;&#20998;&#20272;&#35745;&#22120;&#26356;&#26032;&#21644;&#37325;&#22797;&#20351;&#29992;&#36807;&#21435;&#30340;&#26679;&#26412;&#65292;&#22312;&#19982;Adam&#30456;&#32467;&#21512;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#31283;&#23450;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#21152;&#24555;&#20102;&#22256;&#38590;&#20248;&#21270;&#38382;&#39064;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.15676</link><description>&lt;p&gt;
&#36870;&#28210;&#26579;&#30340;&#32852;&#21512;&#37319;&#26679;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Joint Sampling and Optimisation for Inverse Rendering. (arXiv:2309.15676v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15676
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36870;&#28210;&#26579;&#30340;&#32852;&#21512;&#37319;&#26679;&#19982;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#37319;&#26679;&#21644;&#20248;&#21270;&#20197;&#20943;&#23567;&#26041;&#24046;&#65292;&#20351;&#29992;&#26377;&#38480;&#24046;&#20998;&#20272;&#35745;&#22120;&#26356;&#26032;&#21644;&#37325;&#22797;&#20351;&#29992;&#36807;&#21435;&#30340;&#26679;&#26412;&#65292;&#22312;&#19982;Adam&#30456;&#32467;&#21512;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#31283;&#23450;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#21152;&#24555;&#20102;&#22256;&#38590;&#20248;&#21270;&#38382;&#39064;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22788;&#29702;&#36870;&#28210;&#26579;&#31561;&#22256;&#38590;&#36870;&#38382;&#39064;&#26102;&#65292;&#20351;&#29992;Monte Carlo&#20272;&#35745;&#26799;&#24230;&#26469;&#20248;&#21270;&#21442;&#25968;&#21487;&#33021;&#20250;&#22240;&#26041;&#24046;&#32780;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#21464;&#24930;&#12290;&#27599;&#27425;&#36845;&#20195;&#20013;&#24179;&#22343;&#22810;&#20010;&#26799;&#24230;&#26679;&#26412;&#21487;&#20197;&#31616;&#21333;&#22320;&#20943;&#23567;&#36825;&#31181;&#26041;&#24046;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#38656;&#35201;&#36827;&#34892;&#25968;&#21315;&#27425;&#20248;&#21270;&#36845;&#20195;&#30340;&#38382;&#39064;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#35745;&#31639;&#25104;&#26412;&#20250;&#36805;&#36895;&#19978;&#21319;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#20132;&#26367;&#36827;&#34892;&#37319;&#26679;&#21644;&#20248;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#20302;&#26041;&#24046;&#26377;&#38480;&#24046;&#20998;&#20272;&#35745;&#22120;&#26469;&#26356;&#26032;&#21644;&#37325;&#22797;&#20351;&#29992;&#36807;&#21435;&#30340;&#26679;&#26412;&#65292;&#25551;&#36848;&#20102;&#27599;&#27425;&#36845;&#20195;&#20043;&#38388;&#20272;&#35745;&#26799;&#24230;&#30340;&#21464;&#21270;&#12290;&#36890;&#36807;&#32467;&#21512;&#27604;&#20363;&#21644;&#26377;&#38480;&#24046;&#20998;&#26679;&#26412;&#65292;&#25105;&#20204;&#22312;&#25972;&#20010;&#20248;&#21270;&#36807;&#31243;&#20013;&#19981;&#26029;&#20943;&#23567;&#20102;&#25105;&#20204;&#30340;&#26032;&#39062;&#26799;&#24230;&#20803;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#22914;&#20309;&#19982;Adam&#30456;&#20114;&#20851;&#32852;&#65292;&#24182;&#25512;&#23548;&#20986;&#19968;&#20010;&#31283;&#23450;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#36870;&#36335;&#24452;&#36319;&#36394;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#22914;&#20309;&#21152;&#36895;&#22256;&#38590;&#20248;&#21270;&#38382;&#39064;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
When dealing with difficult inverse problems such as inverse rendering, using Monte Carlo estimated gradients to optimise parameters can slow down convergence due to variance. Averaging many gradient samples in each iteration reduces this variance trivially. However, for problems that require thousands of optimisation iterations, the computational cost of this approach rises quickly.  We derive a theoretical framework for interleaving sampling and optimisation. We update and reuse past samples with low-variance finite-difference estimators that describe the change in the estimated gradients between each iteration. By combining proportional and finite-difference samples, we continuously reduce the variance of our novel gradient meta-estimators throughout the optimisation process. We investigate how our estimator interlinks with Adam and derive a stable combination.  We implement our method for inverse path tracing and demonstrate how our estimator speeds up convergence on difficult opti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25340;&#25509;&#21333;&#35821;&#35821;&#26009;&#29983;&#25104;&#28151;&#21512;&#35821;&#38899;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#28151;&#21512;&#35821;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#25104;&#30340;&#28151;&#21512;&#35821;&#38899;&#25968;&#25454;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35821;&#38899;&#35782;&#21035;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#20943;&#23569;&#27169;&#22411;&#23545;&#21333;&#35821;&#30340;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.15674</link><description>&lt;p&gt;
&#35821;&#38899;&#25340;&#36148;&#65306;&#36890;&#36807;&#25340;&#25509;&#21333;&#35821;&#35821;&#26009;&#29983;&#25104;&#28151;&#21512;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
Speech collage: code-switched audio generation by collaging monolingual corpora. (arXiv:2309.15674v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25340;&#25509;&#21333;&#35821;&#35821;&#26009;&#29983;&#25104;&#28151;&#21512;&#35821;&#38899;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#28151;&#21512;&#35821;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#25104;&#30340;&#28151;&#21512;&#35821;&#38899;&#25968;&#25454;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35821;&#38899;&#35782;&#21035;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#20943;&#23569;&#27169;&#22411;&#23545;&#21333;&#35821;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#26377;&#25928;&#30340;&#29992;&#20110;&#28151;&#21512;&#35821;&#35328;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#24448;&#24448;&#21462;&#20915;&#20110;&#21487;&#33719;&#24471;&#30340;&#28151;&#21512;&#35821;&#36164;&#28304;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#35821;&#38899;&#25340;&#36148;&#26041;&#27861;&#65292;&#36890;&#36807;&#25340;&#25509;&#38899;&#39057;&#29255;&#27573;&#20174;&#21333;&#35821;&#35821;&#26009;&#20013;&#21512;&#25104;&#28151;&#21512;&#35821;&#25968;&#25454;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#37325;&#21472;&#28155;&#21152;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#38899;&#39057;&#29983;&#25104;&#30340;&#24179;&#28369;&#24230;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#29983;&#25104;&#25968;&#25454;&#23545;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#35821;&#38899;&#35782;&#21035;&#30340;&#24433;&#21709;&#65306;&#20351;&#29992;&#39046;&#22495;&#20869;&#28151;&#21512;&#35821;&#25991;&#26412;&#21644;&#20351;&#29992;&#21512;&#25104;&#30340;&#28151;&#21512;&#35821;&#25991;&#26412;&#30340;&#38646;&#26679;&#26412;&#26041;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#39046;&#22495;&#20869;&#21644;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#28151;&#21512;&#38169;&#35823;&#29575;&#21644;&#35789;&#38169;&#35823;&#29575;&#20998;&#21035;&#30456;&#23545;&#20943;&#23569;&#20102;34.4%&#21644;16.2%&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#28151;&#21512;&#35821;&#35328;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#28151;&#21512;&#20542;&#21521;&#24182;&#20943;&#23569;&#20102;&#21333;&#35821;&#20542;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing effective automatic speech recognition (ASR) systems for Code-Switching (CS) often depends on the availability of the transcribed CS resources. To address data scarcity, this paper introduces Speech Collage, a method that synthesizes CS data from monolingual corpora by splicing audio segments. We further improve the smoothness quality of audio generation using an overlap-add approach. We investigate the impact of generated data on speech recognition in two scenarios: using in-domain CS text and a zero-shot approach with synthesized CS text. Empirical results highlight up to 34.4% and 16.2% relative reductions in Mixed-Error Rate and Word-Error Rate for in-domain and zero-shot scenarios, respectively. Lastly, we demonstrate that CS augmentation bolsters the model's code-switching inclination and reduces its monolingual bias.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#23391;&#21152;&#25289;&#35821;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#22810;&#26631;&#31614;&#24773;&#24863;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#20197;&#21450;BERT&#27169;&#22411;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#23398;&#31185;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2309.15670</link><description>&lt;p&gt;
MONOVAB: &#29992;&#20110;&#23391;&#21152;&#25289;&#35821;&#22810;&#26631;&#31614;&#24773;&#24863;&#26816;&#27979;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection. (arXiv:2309.15670v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15670
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#23391;&#21152;&#25289;&#35821;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#22810;&#26631;&#31614;&#24773;&#24863;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#20197;&#21450;BERT&#27169;&#22411;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#23398;&#31185;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24773;&#24863;&#20998;&#26512;(SA)&#21644;&#24773;&#24863;&#35782;&#21035;(ER)&#22312;&#23391;&#21152;&#25289;&#35821;&#20013;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#23391;&#21152;&#25289;&#35821;&#26159;&#19990;&#30028;&#19978;&#31532;&#19971;&#22823;&#20351;&#29992;&#20154;&#25968;&#26368;&#22810;&#30340;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#23391;&#21152;&#25289;&#35821;&#30340;&#32467;&#26500;&#22797;&#26434;&#65292;&#36825;&#20351;&#24471;&#20934;&#30830;&#25552;&#21462;&#24773;&#32490;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#24050;&#32463;&#37319;&#29992;&#20102;&#19968;&#20123;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#20363;&#22914;&#25552;&#21462;&#31215;&#26497;&#21644;&#28040;&#26497;&#24773;&#24863;&#20197;&#21450;&#22810;&#31867;&#24773;&#32490;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#35821;&#35328;&#20013;&#25552;&#21462;&#22810;&#31181;&#24773;&#32490;&#20960;&#20046;&#26159;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#65292;&#23427;&#28041;&#21450;&#22522;&#20110;&#19968;&#27573;&#25991;&#26412;&#35782;&#21035;&#20986;&#22810;&#31181;&#24773;&#24863;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#20174;Facebook&#19978;&#25235;&#21462;&#30340;&#25968;&#25454;&#26500;&#24314;&#27880;&#37322;&#35821;&#26009;&#24211;&#30340;&#35814;&#32454;&#26041;&#27861;&#65292;&#20197;&#22635;&#34917;&#36825;&#20010;&#23398;&#31185;&#39046;&#22495;&#30340;&#31354;&#30333;&#65292;&#20811;&#26381;&#25361;&#25112;&#12290;&#20026;&#20102;&#20351;&#36825;&#31181;&#27880;&#37322;&#26356;&#26377;&#25104;&#26524;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#12290;&#36716;&#25442;&#22120;&#20013;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;(BERT)&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) have been increasingly popular in the Bangla language, which is the seventh most spoken language throughout the entire world. However, the language is structurally complicated, which makes this field arduous to extract emotions in an accurate manner. Several distinct approaches such as the extraction of positive and negative sentiments as well as multiclass emotions, have been implemented in this field of study. Nevertheless, the extraction of multiple sentiments is an almost untouched area in this language. Which involves identifying several feelings based on a single piece of text. Therefore, this study demonstrates a thorough method for constructing an annotated corpus based on scrapped data from Facebook to bridge the gaps in this subject area to overcome the challenges. To make this annotation more fruitful, the context-based approach has been used. Bidirectional Encoder Representations from Transformers (BERT),
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#23558;&#20854;&#19982;&#29233;&#22240;&#26031;&#22374;&#30340;&#29305;&#27530;&#30456;&#23545;&#35770;&#20013;&#30340;&#32416;&#32544;&#27010;&#24565;&#32852;&#31995;&#36215;&#26469;&#65292;&#21457;&#29616;&#36828;&#31243;&#29305;&#24449;&#26679;&#26412;&#21487;&#20197;&#34920;&#29616;&#20986;&#32416;&#32544;&#29616;&#35937;&#65292;&#25361;&#25112;&#20102;&#23545;&#25239;&#21487;&#20256;&#36882;&#24615;&#29616;&#35937;&#30340;&#20256;&#32479;&#25551;&#36848;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.15669</link><description>&lt;p&gt;
&#20851;&#20110;&#35745;&#31639;&#32416;&#32544;&#21450;&#20854;&#22312;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
On Computational Entanglement and Its Interpretation in Adversarial Machine Learning. (arXiv:2309.15669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#23558;&#20854;&#19982;&#29233;&#22240;&#26031;&#22374;&#30340;&#29305;&#27530;&#30456;&#23545;&#35770;&#20013;&#30340;&#32416;&#32544;&#27010;&#24565;&#32852;&#31995;&#36215;&#26469;&#65292;&#21457;&#29616;&#36828;&#31243;&#29305;&#24449;&#26679;&#26412;&#21487;&#20197;&#34920;&#29616;&#20986;&#32416;&#32544;&#29616;&#35937;&#65292;&#25361;&#25112;&#20102;&#23545;&#25239;&#21487;&#20256;&#36882;&#24615;&#29616;&#35937;&#30340;&#20256;&#32479;&#25551;&#36848;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23545;&#25239;&#24615;&#26679;&#26412;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#27450;&#39575;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#28508;&#22312;&#22320;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#65292;&#22240;&#27492;&#24050;&#25104;&#20026;&#30740;&#31350;&#30340;&#28966;&#28857;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#25506;&#32034;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22797;&#26434;&#24615;&#19982;&#29233;&#22240;&#26031;&#22374;&#30340;&#29305;&#27530;&#30456;&#23545;&#35770;&#20043;&#38388;&#30340;&#26377;&#36259;&#32852;&#31995;&#65292;&#36890;&#36807;&#32416;&#32544;&#30340;&#27010;&#24565;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#23545;&#35745;&#31639;&#32416;&#32544;&#36827;&#34892;&#20102;&#23450;&#20041;&#65292;&#24182;&#35777;&#26126;&#20102;&#36828;&#31243;&#29305;&#24449;&#26679;&#26412;&#21487;&#20197;&#34920;&#29616;&#20986;&#24378;&#30456;&#20851;&#24615;&#65292;&#31867;&#20284;&#20110;&#37327;&#23376;&#39046;&#22495;&#20013;&#30340;&#32416;&#32544;&#12290;&#36825;&#19968;&#21457;&#29616;&#25361;&#25112;&#20102;&#23545;&#24403;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#23545;&#25239;&#21487;&#20256;&#36882;&#24615;&#29616;&#35937;&#30340;&#20256;&#32479;&#25551;&#36848;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples in machine learning has emerged as a focal point of research due to their remarkable ability to deceive models with seemingly inconspicuous input perturbations, potentially resulting in severe consequences. In this study, we embark on a comprehensive exploration of adversarial machine learning models, shedding light on their intrinsic complexity and interpretability. Our investigation reveals intriguing links between machine learning model complexity and Einstein's theory of special relativity, through the concept of entanglement. More specific, we define entanglement computationally and demonstrate that distant feature samples can exhibit strong correlations, akin to entanglement in quantum realm. This revelation challenges conventional perspectives in describing the phenomenon of adversarial transferability observed in contemporary machine learning models. By drawing parallels with the relativistic effects of time dilation and length contraction during computatio
&lt;/p&gt;</description></item><item><title>FeDEQ&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#28145;&#24230;&#24179;&#34913;&#23398;&#20064;&#21644;&#20849;&#35782;&#20248;&#21270;&#65292;&#36890;&#36807;&#32039;&#20945;&#30340;&#20849;&#20139;&#25968;&#25454;&#34920;&#31034;&#22312;&#36793;&#32536;&#33410;&#28857;&#20043;&#38388;&#20849;&#20139;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#32852;&#37030;&#23398;&#20064;&#22312;&#36793;&#32536;&#29615;&#22659;&#20013;&#30340;&#36890;&#20449;&#29942;&#39048;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.15659</link><description>&lt;p&gt;
&#32852;&#37030;&#28145;&#24230;&#24179;&#34913;&#23398;&#20064;&#65306;&#36793;&#32536;&#36890;&#20449;&#25928;&#29575;&#30340;&#32039;&#20945;&#20849;&#20139;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Federated Deep Equilibrium Learning: A Compact Shared Representation for Edge Communication Efficiency. (arXiv:2309.15659v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15659
&lt;/p&gt;
&lt;p&gt;
FeDEQ&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#28145;&#24230;&#24179;&#34913;&#23398;&#20064;&#21644;&#20849;&#35782;&#20248;&#21270;&#65292;&#36890;&#36807;&#32039;&#20945;&#30340;&#20849;&#20139;&#25968;&#25454;&#34920;&#31034;&#22312;&#36793;&#32536;&#33410;&#28857;&#20043;&#38388;&#20849;&#20139;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#32852;&#37030;&#23398;&#20064;&#22312;&#36793;&#32536;&#29615;&#22659;&#20013;&#30340;&#36890;&#20449;&#29942;&#39048;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#21331;&#36234;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#24335;&#65292;&#20419;&#36827;&#20102;&#36793;&#32536;&#32593;&#32476;&#33410;&#28857;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#20197;&#22312;&#19981;&#38598;&#20013;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#35745;&#31639;&#36716;&#31227;&#21040;&#32593;&#32476;&#36793;&#32536;&#65292;&#32852;&#37030;&#23398;&#20064;&#25552;&#20379;&#20102;&#40065;&#26834;&#21644;&#21709;&#24212;&#36805;&#36895;&#30340;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22686;&#24378;&#20102;&#38544;&#31169;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#22312;&#36793;&#32536;&#29615;&#22659;&#20013;&#37096;&#32626;&#28145;&#24230;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#21463;&#21040;&#36890;&#20449;&#29942;&#39048;&#12289;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#20869;&#23384;&#38480;&#21046;&#30340;&#38459;&#30861;&#12290;&#20026;&#20102;&#20849;&#21516;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FeDEQ&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#26377;&#25928;&#22320;&#37319;&#29992;&#28145;&#24230;&#24179;&#34913;&#23398;&#20064;&#21644;&#20849;&#35782;&#20248;&#21270;&#65292;&#22312;&#36793;&#32536;&#33410;&#28857;&#20043;&#38388;&#21033;&#29992;&#32039;&#20945;&#30340;&#20849;&#20139;&#25968;&#25454;&#34920;&#31034;&#65292;&#20801;&#35768;&#27966;&#29983;&#20986;&#38024;&#23545;&#27599;&#20010;&#33410;&#28857;&#29305;&#23450;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#27169;&#22411;&#32467;&#26500;&#65292;&#30001;&#19968;&#20010;&#24179;&#34913;&#23618;&#21644;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#23618;&#32452;&#25104;&#12290;&#22312;&#36825;&#37324;&#65292;&#24179;&#34913;&#23618;&#20805;&#24403;&#20840;&#23616;&#29305;&#24449;&#34920;&#31034;&#65292;&#36793;&#32536;&#33410;&#28857;&#21487;&#20197;&#26681;&#25454;&#33258;&#24049;&#30340;&#38656;&#27714;&#36827;&#34892;&#20010;&#24615;&#21270;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a prominent distributed learning paradigm facilitating collaboration among nodes within an edge network to co-train a global model without centralizing data. By shifting computation to the network edge, FL offers robust and responsive edge-AI solutions and enhance privacy-preservation. However, deploying deep FL models within edge environments is often hindered by communication bottlenecks, data heterogeneity, and memory limitations. To address these challenges jointly, we introduce FeDEQ, a pioneering FL framework that effectively employs deep equilibrium learning and consensus optimization to exploit a compact shared data representation across edge nodes, allowing the derivation of personalized models specific to each node. We delve into a unique model structure composed of an equilibrium layer followed by traditional neural network layers. Here, the equilibrium layer functions as a global feature representation that edge nodes can adapt to personalize thei
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;ASR&#21518;&#22788;&#29702;&#22120;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#37325;&#26032;&#35780;&#20998;&#21644;&#38169;&#35823;&#26657;&#27491;&#26469;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#25351;&#20196;&#25552;&#31034;&#21644;&#20219;&#21153;&#28608;&#27963;&#25552;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24494;&#35843;&#25216;&#26415;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15649</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Generative Speech Recognition Error Correction with Large Language Models. (arXiv:2309.15649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;ASR&#21518;&#22788;&#29702;&#22120;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#37325;&#26032;&#35780;&#20998;&#21644;&#38169;&#35823;&#26657;&#27491;&#26469;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#25351;&#20196;&#25552;&#31034;&#21644;&#20219;&#21153;&#28608;&#27963;&#25552;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24494;&#35843;&#25216;&#26415;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;ASR&#21518;&#22788;&#29702;&#22120;&#30340;&#33021;&#21147;&#65292;&#29992;&#20110;&#37325;&#26032;&#35780;&#20998;&#21644;&#38169;&#35823;&#26657;&#27491;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#20351;&#29992;&#25351;&#20196;&#25552;&#31034;&#35753;LLMs&#25191;&#34892;&#36825;&#20123;&#20219;&#21153;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#26696;&#65292;&#21253;&#25324;&#38646;-shot&#21644;&#23569;-shot&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#28608;&#27963;&#25552;&#31034;&#65288;TAP&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#25351;&#20196;&#21644;&#28436;&#31034;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#39046;&#22495;&#20043;&#22806;&#30340;&#20219;&#21153;&#65288;ATIS&#21644;WSJ&#65289;&#19978;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#31532;&#19968;&#27425;&#25195;&#25551;&#31995;&#32479;&#21644;&#37325;&#26032;&#35780;&#20998;&#36755;&#20986;&#65292;&#25105;&#20204;&#35777;&#26126;&#20165;&#36890;&#36807;&#20923;&#32467;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#36827;&#34892;&#37325;&#26032;&#35780;&#20998;&#21487;&#20197;&#36798;&#21040;&#19982;&#39046;&#22495;&#35843;&#20248;&#30340;LMs&#37325;&#26032;&#35780;&#20998;&#30456;&#31454;&#20105;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#23558;&#25552;&#31034;&#25216;&#26415;&#19982;&#24494;&#35843;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20302;&#20110;N-best Oracle&#27700;&#24179;&#30340;&#38169;&#35823;&#29575;&#65292;&#23637;&#31034;&#20102;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the ability of large language models (LLMs) to act as ASR post-processors that perform rescoring and error correction. Our focus is on instruction prompting to let LLMs perform these task without fine-tuning, for which we evaluate different prompting schemes, both zero- and few-shot in-context learning, and a novel task-activating prompting (TAP) method that combines instruction and demonstration. Using a pre-trained first-pass system and rescoring output on two out-of-domain tasks (ATIS and WSJ), we show that rescoring only by in-context learning with frozen LLMs achieves results that are competitive with rescoring by domain-tuned LMs. By combining prompting techniques with fine-tuning we achieve error rates below the N-best oracle level, showcasing the generalization power of the LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SANGEA&#65292;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#21512;&#25104;&#22270;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22270;&#20998;&#35299;&#20026;&#31038;&#21306;&#65292;&#27599;&#20010;&#31038;&#21306;&#35757;&#32451;&#19968;&#20010;&#21512;&#25104;&#22270;&#29983;&#25104;&#22120;&#65292;&#28982;&#21518;&#23558;&#31038;&#21306;&#22270;&#38142;&#25509;&#22312;&#19968;&#36215;&#29983;&#25104;&#19968;&#20010;&#31867;&#20284;&#21407;&#22987;&#22270;&#30340;&#22823;&#22411;&#21512;&#25104;&#22270;&#12290;</title><link>http://arxiv.org/abs/2309.15648</link><description>&lt;p&gt;
SANGEA&#65306;&#21487;&#25193;&#23637;&#21644;&#24102;&#23646;&#24615;&#30340;&#32593;&#32476;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SANGEA: Scalable and Attributed Network Generation. (arXiv:2309.15648v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SANGEA&#65292;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#21512;&#25104;&#22270;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22270;&#20998;&#35299;&#20026;&#31038;&#21306;&#65292;&#27599;&#20010;&#31038;&#21306;&#35757;&#32451;&#19968;&#20010;&#21512;&#25104;&#22270;&#29983;&#25104;&#22120;&#65292;&#28982;&#21518;&#23558;&#31038;&#21306;&#22270;&#38142;&#25509;&#22312;&#19968;&#36215;&#29983;&#25104;&#19968;&#20010;&#31867;&#20284;&#21407;&#22987;&#22270;&#30340;&#22823;&#22411;&#21512;&#25104;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#29983;&#25104;&#24314;&#27169;&#30340;&#26368;&#26032;&#31361;&#30772;&#65292;&#21512;&#25104;&#22270;&#29983;&#25104;&#22120;&#65288;SGGs&#65289;&#30340;&#35805;&#39064;&#21463;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;SGGs&#22312;&#22270;&#30340;&#22823;&#23567;&#26041;&#38754;&#19981;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20107;&#23454;&#19978;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#36890;&#24120;&#38656;&#35201;&#32771;&#34385;&#22266;&#23450;&#33410;&#28857;&#25968;&#30340;&#25152;&#26377;&#21487;&#33021;&#36793;&#65292;&#36825;&#20250;&#20197;$\mathcal{O}(N^2)$&#30340;&#35268;&#27169;&#22686;&#38271;&#65292;&#20854;&#20013;$N$&#26159;&#22270;&#20013;&#30340;&#33410;&#28857;&#25968;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;SGGs&#19981;&#36866;&#29992;&#20110;&#22823;&#22411;&#22270;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SANGEA&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#21512;&#25104;&#22270;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#23558;&#20219;&#20309;SGG&#30340;&#36866;&#29992;&#33539;&#22260;&#25193;&#23637;&#21040;&#22823;&#22411;&#22270;&#12290;&#36890;&#36807;&#39318;&#20808;&#23558;&#22823;&#22270;&#20998;&#25104;&#31038;&#21306;&#65292;SANGEA&#23545;&#27599;&#20010;&#31038;&#21306;&#35757;&#32451;&#19968;&#20010;SGG&#65292;&#28982;&#21518;&#23558;&#31038;&#21306;&#22270;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#21019;&#24314;&#19968;&#20010;&#21512;&#25104;&#30340;&#22823;&#22270;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SANGEA&#29983;&#25104;&#30340;&#22270;&#22312;&#25299;&#25169;&#21644;&#33410;&#28857;&#29305;&#24449;&#20998;&#24067;&#26041;&#38754;&#19982;&#21407;&#22987;&#22270;&#38750;&#24120;&#30456;&#20284;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#29983;&#25104;&#30340;&#22270;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The topic of synthetic graph generators (SGGs) has recently received much attention due to the wave of the latest breakthroughs in generative modelling. However, many state-of-the-art SGGs do not scale well with the graph size. Indeed, in the generation process, all the possible edges for a fixed number of nodes must often be considered, which scales in $\mathcal{O}(N^2)$, with $N$ being the number of nodes in the graph. For this reason, many state-of-the-art SGGs are not applicable to large graphs. In this paper, we present SANGEA, a sizeable synthetic graph generation framework which extends the applicability of any SGG to large graphs. By first splitting the large graph into communities, SANGEA trains one SGG per community, then links the community graphs back together to create a synthetic large graph. Our experiments show that the graphs generated by SANGEA have high similarity to the original graph, in terms of both topology and node feature distribution. Additionally, these gene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20919;&#21551;&#21160;&#21644;&#28909;&#21551;&#21160;&#32593;&#32476;&#30340;&#26041;&#27861;(Cold &amp; Warm Net)&#65292;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20919;&#21551;&#21160;&#29992;&#25143;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#19987;&#23478;&#27169;&#22411;&#20998;&#21035;&#24314;&#27169;&#20919;&#21551;&#21160;&#21644;&#28909;&#21551;&#21160;&#29992;&#25143;&#65292;&#24182;&#24341;&#20837;&#38376;&#25511;&#32593;&#32476;&#21644;&#21160;&#24577;&#30693;&#35782;&#33976;&#39311;&#26469;&#25552;&#39640;&#29992;&#25143;&#34920;&#31034;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#36890;&#36807;&#36873;&#25321;&#19982;&#29992;&#25143;&#34892;&#20026;&#39640;&#24230;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#24182;&#24314;&#31435;&#20559;&#24046;&#32593;&#32476;&#26469;&#26174;&#24335;&#24314;&#27169;&#29992;&#25143;&#34892;&#20026;&#20559;&#24046;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15646</link><description>&lt;p&gt;
&#20919;&#21551;&#21160;&#21644;&#28909;&#21551;&#21160;&#32593;&#32476;&#65306;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20919;&#21551;&#21160;&#29992;&#25143;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Cold &amp; Warm Net: Addressing Cold-Start Users in Recommender Systems. (arXiv:2309.15646v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20919;&#21551;&#21160;&#21644;&#28909;&#21551;&#21160;&#32593;&#32476;&#30340;&#26041;&#27861;(Cold &amp; Warm Net)&#65292;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20919;&#21551;&#21160;&#29992;&#25143;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#19987;&#23478;&#27169;&#22411;&#20998;&#21035;&#24314;&#27169;&#20919;&#21551;&#21160;&#21644;&#28909;&#21551;&#21160;&#29992;&#25143;&#65292;&#24182;&#24341;&#20837;&#38376;&#25511;&#32593;&#32476;&#21644;&#21160;&#24577;&#30693;&#35782;&#33976;&#39311;&#26469;&#25552;&#39640;&#29992;&#25143;&#34920;&#31034;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#36890;&#36807;&#36873;&#25321;&#19982;&#29992;&#25143;&#34892;&#20026;&#39640;&#24230;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#24182;&#24314;&#31435;&#20559;&#24046;&#32593;&#32476;&#26469;&#26174;&#24335;&#24314;&#27169;&#29992;&#25143;&#34892;&#20026;&#20559;&#24046;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20919;&#21551;&#21160;&#25512;&#33616;&#26159;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#20043;&#19968;&#12290;&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#29992;&#25143;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#20351;&#29992;&#20391;&#20449;&#24687;&#25110;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#34987;&#29992;&#26469;&#24314;&#27169;&#20919;&#21551;&#21160;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#24037;&#19994;&#32423;&#25512;&#33616;&#31995;&#32479;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#30446;&#21069;&#23545;&#20110;&#21305;&#37197;&#38454;&#27573;&#20013;&#30340;&#29992;&#25143;&#20919;&#21551;&#21160;&#38382;&#39064;&#30340;&#30740;&#31350;&#36824;&#19981;&#22810;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#19987;&#23478;&#27169;&#22411;&#30340;&#20919;&#21551;&#21160;&#21644;&#28909;&#21551;&#21160;&#29992;&#25143;&#24314;&#27169;&#26041;&#27861;&#65306;Cold &amp; Warm Net&#12290;&#36890;&#36807;&#24341;&#20837;&#38376;&#25511;&#32593;&#32476;&#26469;&#32467;&#21512;&#20004;&#20010;&#19987;&#23478;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#21160;&#24577;&#30693;&#35782;&#33976;&#39311;&#20316;&#20026;&#19968;&#20010;&#25945;&#24072;&#36873;&#25321;&#22120;&#65292;&#24110;&#21161;&#19987;&#23478;&#26356;&#22909;&#22320;&#23398;&#20064;&#29992;&#25143;&#34920;&#31034;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#20114;&#20449;&#24687;&#36873;&#25321;&#19982;&#29992;&#25143;&#34892;&#20026;&#39640;&#24230;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#29992;&#20110;&#26174;&#24335;&#24314;&#27169;&#29992;&#25143;&#34892;&#20026;&#20559;&#24046;&#30340;&#20559;&#24046;&#32593;&#32476;&#12290;&#26368;&#21518;&#65292;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23545;Cold &amp; Warm Net&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cold-start recommendation is one of the major challenges faced by recommender systems (RS). Herein, we focus on the user cold-start problem. Recently, methods utilizing side information or meta-learning have been used to model cold-start users. However, it is difficult to deploy these methods to industrial RS. There has not been much research that pays attention to the user cold-start problem in the matching stage. In this paper, we propose Cold &amp; Warm Net based on expert models who are responsible for modeling cold-start and warm-up users respectively. A gate network is applied to incorporate the results from two experts. Furthermore, dynamic knowledge distillation acting as a teacher selector is introduced to assist experts in better learning user representation. With comprehensive mutual information, features highly relevant to user behavior are selected for the bias net which explicitly models user behavior bias. Finally, we evaluate our Cold &amp; Warm Net on public datasets in compar
&lt;/p&gt;</description></item><item><title>&#35282;&#36793;&#36317;&#25439;&#22833;&#19982;&#36741;&#21161;&#20219;&#21153;&#32467;&#21512;&#22312;&#21322;&#30417;&#30563;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#35282;&#36793;&#36317;&#25439;&#22833;&#21516;&#26102;&#36798;&#21040;&#26368;&#23567;&#21270;&#32039;&#20945;&#24615;&#25439;&#22833;&#21644;&#38450;&#27490;&#23398;&#20064;&#24179;&#20961;&#35299;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.15643</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#35282;&#36793;&#36317;&#25439;&#22833;&#22312;&#21322;&#30417;&#30563;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#33394;&#65311;
&lt;/p&gt;
&lt;p&gt;
Why do Angular Margin Losses work well for Semi-Supervised Anomalous Sound Detection?. (arXiv:2309.15643v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15643
&lt;/p&gt;
&lt;p&gt;
&#35282;&#36793;&#36317;&#25439;&#22833;&#19982;&#36741;&#21161;&#20219;&#21153;&#32467;&#21512;&#22312;&#21322;&#30417;&#30563;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#35282;&#36793;&#36317;&#25439;&#22833;&#21516;&#26102;&#36798;&#21040;&#26368;&#23567;&#21270;&#32039;&#20945;&#24615;&#25439;&#22833;&#21644;&#38450;&#27490;&#23398;&#20064;&#24179;&#20961;&#35299;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#31995;&#32479;&#36890;&#24120;&#21033;&#29992;&#35282;&#36793;&#36317;&#25439;&#22833;&#26469;&#36890;&#36807;&#19968;&#20010;&#36741;&#21161;&#20219;&#21153;&#23398;&#20064;&#21512;&#36866;&#30340;&#22768;&#23398;&#25968;&#25454;&#34920;&#31034;&#65292;&#35813;&#20219;&#21153;&#36890;&#24120;&#26159;&#19968;&#20010;&#30417;&#30563;&#25110;&#33258;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#12290;&#20854;&#22522;&#26412;&#24605;&#24819;&#26159;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#36741;&#21161;&#20219;&#21153;&#65292;&#38656;&#35201;&#22312;&#23398;&#20064;&#30340;&#34920;&#31034;&#20013;&#25429;&#25417;&#21040;&#20851;&#20110;&#27491;&#24120;&#25968;&#25454;&#30340;&#29305;&#23450;&#20449;&#24687;&#65292;&#24182;&#19988;&#36825;&#20123;&#20449;&#24687;&#20063;&#36275;&#20197;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#26679;&#26412;&#12290;&#29305;&#21035;&#26159;&#22312;&#22122;&#22768;&#26465;&#20214;&#19979;&#65292;&#22522;&#20110;&#35282;&#36793;&#36317;&#25439;&#22833;&#30340;&#21028;&#21035;&#27169;&#22411;&#24448;&#24448;&#26126;&#26174;&#20248;&#20110;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#25110;&#21333;&#31867;&#27169;&#22411;&#30340;&#31995;&#32479;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#35843;&#26597;&#20026;&#20160;&#20040;&#22312;&#36741;&#21161;&#20219;&#21153;&#20013;&#20351;&#29992;&#35282;&#36793;&#36317;&#25439;&#22833;&#23545;&#20110;&#26816;&#27979;&#24322;&#24120;&#22768;&#38899;&#25928;&#26524;&#33391;&#22909;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#26368;&#23567;&#21270;&#35282;&#36793;&#36317;&#25439;&#22833;&#20063;&#26368;&#23567;&#21270;&#20102;&#32039;&#20945;&#24615;&#25439;&#22833;&#65292;&#21516;&#26102;&#22266;&#26377;&#22320;&#38450;&#27490;&#23398;&#20064;&#24179;&#20961;&#30340;&#35299;&#12290;&#27492;&#22806;&#65292;m
&lt;/p&gt;
&lt;p&gt;
State-of-the-art anomalous sound detection systems often utilize angular margin losses to learn suitable representations of acoustic data using an auxiliary task, which usually is a supervised or self-supervised classification task. The underlying idea is that, in order to solve this auxiliary task, specific information about normal data needs to be captured in the learned representations and that this information is also sufficient to differentiate between normal and anomalous samples. Especially in noisy conditions, discriminative models based on angular margin losses tend to significantly outperform systems based on generative or one-class models. The goal of this work is to investigate why using angular margin losses with auxiliary tasks works well for detecting anomalous sounds. To this end, it is shown, both theoretically and experimentally, that minimizing angular margin losses also minimizes compactness loss while inherently preventing learning trivial solutions. Furthermore, m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#20108;&#32500;&#24352;&#37327;&#32593;&#32476;&#39640;&#25928;&#27169;&#25311;IBM&#26368;&#22823;&#30340;&#37327;&#23376;&#22788;&#29702;&#22120;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#24352;&#37327;&#26356;&#26032;&#23454;&#29616;&#21069;&#25152;&#26410;&#26377;&#30340;&#20934;&#30830;&#24230;&#21644;&#26497;&#20302;&#30340;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#65292;&#24182;&#20026;&#26368;&#26032;&#30340;IBM&#37327;&#23376;&#26426;&#22120;&#35774;&#31435;&#20102;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2309.15642</link><description>&lt;p&gt;
IBM&#26368;&#22823;&#37327;&#23376;&#22788;&#29702;&#22120;&#30340;&#24352;&#37327;&#32593;&#32476;&#27169;&#25311;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient tensor network simulation of IBM's largest quantum processors. (arXiv:2309.15642v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#20108;&#32500;&#24352;&#37327;&#32593;&#32476;&#39640;&#25928;&#27169;&#25311;IBM&#26368;&#22823;&#30340;&#37327;&#23376;&#22788;&#29702;&#22120;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#24352;&#37327;&#26356;&#26032;&#23454;&#29616;&#21069;&#25152;&#26410;&#26377;&#30340;&#20934;&#30830;&#24230;&#21644;&#26497;&#20302;&#30340;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#65292;&#24182;&#20026;&#26368;&#26032;&#30340;IBM&#37327;&#23376;&#26426;&#22120;&#35774;&#31435;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#20108;&#32500;&#24352;&#37327;&#32593;&#32476;&#26469;&#39640;&#25928;&#20934;&#30830;&#22320;&#27169;&#25311;IBM&#26368;&#22823;&#30340;&#37327;&#23376;&#22788;&#29702;&#22120;&#65292;&#21363;Eagle&#65288;127&#20010;&#37327;&#23376;&#27604;&#29305;&#65289;&#65292;Osprey&#65288;433&#20010;&#37327;&#23376;&#27604;&#29305;&#65289;&#21644;Condor&#65288;1121&#20010;&#37327;&#23376;&#27604;&#29305;&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#25237;&#24433;&#32416;&#32544;&#23545;&#24577;&#65288;gPEPS&#65289;&#27169;&#25311;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#65292;&#20855;&#20307;&#26469;&#35828;&#65292;&#36825;&#26159;IBM&#26368;&#36817;&#22312;Nature 618&#24180;&#31532;500-505&#39029;&#65288;2023&#24180;&#65289;&#19978;&#32771;&#34385;&#30340;&#36386;&#20987;&#26131;&#36763;&#23454;&#39564;-&#25105;&#20204;&#22312;PRB 99, 195105&#65288;2019&#24180;&#65289;&#20013;&#25552;&#20986;&#20102;&#36825;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#35813;&#27169;&#22411;&#65292;&#31616;&#21333;&#30340;&#24352;&#37327;&#26356;&#26032;&#24050;&#32463;&#36275;&#20197;&#20197;&#26497;&#20302;&#30340;&#35745;&#31639;&#36164;&#28304;&#23454;&#29616;&#38750;&#24120;&#22823;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#31934;&#24230;&#12290;&#38500;&#20102;&#27169;&#25311;127&#20010;&#37327;&#23376;&#27604;&#29305;&#30340;&#21407;&#22987;&#23454;&#39564;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#32467;&#26524;&#25193;&#23637;&#21040;433&#20010;&#21644;1121&#20010;&#37327;&#23376;&#27604;&#29305;&#65292;&#20174;&#32780;&#20026;&#26368;&#26032;&#30340;IBM&#37327;&#23376;&#26426;&#22120;&#35774;&#23450;&#20102;&#19968;&#20010;&#22522;&#20934;&#12290;&#25105;&#20204;&#36824;&#25253;&#36947;&#20102;&#26080;&#38480;&#22810;&#20010;&#37327;&#23376;&#27604;&#29305;&#30340;&#20934;&#30830;&#27169;&#25311;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;gPEPS&#26159;&#39640;&#25928;&#27169;&#25311;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#33258;&#28982;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show how quantum-inspired 2d tensor networks can be used to efficiently and accurately simulate the largest quantum processors from IBM, namely Eagle (127 qubits), Osprey (433 qubits) and Condor (1121 qubits). We simulate the dynamics of a complex quantum many-body system -- specifically, the kicked Ising experiment considered recently by IBM in Nature 618, p. 500-505 (2023) -using graph-based Projected Entangled Pair States (gPEPS), which was proposed by some of us in PRB 99, 195105 (2019). Our results show that simple tensor updates are already sufficient to achieve very large unprecedented accuracy with remarkably low computational resources for this model. Apart from simulating the original experiment for 127 qubits, we also extend our results to 433 and 1121 qubits, thus setting a benchmark for the newest IBM quantum machines. We also report accurate simulations for infinitely-many qubits. Our results show that gPEPS are a natural tool to efficiently simulate quantum computer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#21644;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#26500;&#24314;&#31639;&#27861;&#25237;&#36164;&#31574;&#30053;&#30340;&#23545;&#20914;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#25237;&#36164;&#31574;&#30053;&#26469;&#23545;&#20914;&#39118;&#38505;&#36164;&#20135;&#32452;&#21512;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#37329;&#34701;&#24066;&#22330;&#30340;&#21160;&#33633;&#26102;&#26399;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.15640</link><description>&lt;p&gt;
&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#21644;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#31639;&#27861;&#25237;&#36164;&#31574;&#30053;&#23545;&#20914;&#30340;&#23545;&#20914;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Hedging Properties of Algorithmic Investment Strategies using Long Short-Term Memory and Time Series models for Equity Indices. (arXiv:2309.15640v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#21644;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#26500;&#24314;&#31639;&#27861;&#25237;&#36164;&#31574;&#30053;&#30340;&#23545;&#20914;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#25237;&#36164;&#31574;&#30053;&#26469;&#23545;&#20914;&#39118;&#38505;&#36164;&#20135;&#32452;&#21512;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#37329;&#34701;&#24066;&#22330;&#30340;&#21160;&#33633;&#26102;&#26399;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#37329;&#34701;&#24066;&#22330;&#21463;&#37329;&#34701;&#21160;&#33633;&#24433;&#21709;&#26102;&#23545;&#20914;&#39118;&#38505;&#36164;&#20135;&#32452;&#21512;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22810;&#20803;&#31639;&#27861;&#25237;&#36164;&#31574;&#30053;&#65288;AIS&#65289;&#30340;&#20998;&#25955;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#26159;&#22312;&#21333;&#20010;&#36164;&#20135;&#30340;&#32423;&#21035;&#19978;&#36827;&#34892;&#65292;&#32780;&#26159;&#22312;&#22522;&#20110;&#36825;&#20123;&#36164;&#20135;&#30340;&#20215;&#26684;&#30340;&#32423;&#21035;&#19978;&#36827;&#34892;&#12290;&#25105;&#20204;&#37319;&#29992;&#22235;&#31181;&#19981;&#21516;&#30340;&#29702;&#35770;&#27169;&#22411;&#65288;LSTM - &#38271;&#30701;&#26399;&#35760;&#24518;&#12289;ARIMA-GARCH - &#33258;&#22238;&#24402;&#31227;&#21160;&#24179;&#22343; - &#24191;&#20041;&#33258;&#22238;&#24402;&#26465;&#20214;&#24322;&#26041;&#24046;&#12289;&#21160;&#37327;&#21644;&#21453;&#21521;&#20132;&#26131;&#65289;&#26469;&#29983;&#25104;&#20215;&#26684;&#39044;&#27979;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#39044;&#27979;&#20135;&#29983;&#21333;&#20010;&#21644;&#22797;&#21512;&#30340;AIS&#30340;&#25237;&#36164;&#20449;&#21495;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#33021;&#22815;&#39564;&#35777;&#30001;&#21508;&#31181;&#36164;&#20135;&#65288;&#33021;&#28304;&#21830;&#21697;&#12289;&#36149;&#37329;&#23646;&#12289;&#21152;&#23494;&#36135;&#24065;&#25110;&#36719;&#21830;&#21697;&#65289;&#32452;&#25104;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#25237;&#36164;&#31574;&#30053;&#22312;&#23545;&#20914;&#29992;&#20110;&#32929;&#31080;&#25351;&#25968;&#65288;S&amp;P 500&#25351;&#25968;&#65289;&#30340;&#32452;&#21512;AIS&#20013;&#30340;&#22810;&#26679;&#21270;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel approach to hedging portfolios of risky assets when financial markets are affected by financial turmoils. We introduce a completely novel approach to diversification activity not on the level of single assets but on the level of ensemble algorithmic investment strategies (AIS) built based on the prices of these assets. We employ four types of diverse theoretical models (LSTM - Long Short-Term Memory, ARIMA-GARCH Autoregressive Integrated Moving Average - Generalized Autoregressive Conditional Heteroskedasticity, momentum, and contrarian) to generate price forecasts, which are then used to produce investment signals in single and complex AIS. In such a way, we are able to verify the diversification potential of different types of investment strategies consisting of various assets (energy commodities, precious metals, cryptocurrencies, or soft commodities) in hedging ensemble AIS built for equity indices (S&amp;P 500 index). Empirical data used in this study cov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26041;&#24046;&#25233;&#21046;&#30340;&#26041;&#27861;&#65288;VaSSO&#65289;&#22686;&#24378;&#20102;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#27169;&#22411;&#26080;&#20851;&#20219;&#21153;&#21644;&#23545;&#39640;&#27700;&#24179;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.15639</link><description>&lt;p&gt;
&#36890;&#36807;&#26041;&#24046;&#25233;&#21046;&#22686;&#24378;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhancing Sharpness-Aware Optimization Through Variance Suppression. (arXiv:2309.15639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26041;&#24046;&#25233;&#21046;&#30340;&#26041;&#27861;&#65288;VaSSO&#65289;&#22686;&#24378;&#20102;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#27169;&#22411;&#26080;&#20851;&#20219;&#21153;&#21644;&#23545;&#39640;&#27700;&#24179;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#22312;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#26377;&#30528;&#33391;&#22909;&#30340;&#35760;&#24405;&#65292;&#21363;&#20351;&#27809;&#26377;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;SAM&#20511;&#21161;&#25439;&#22833;&#20989;&#25968;&#30340;&#20960;&#20309;&#29305;&#24615;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#22312;&#37051;&#22495;&#20869;&#21442;&#25968;&#23545;&#25932;&#23545;&#25200;&#21160;&#24341;&#36215;&#30340;&#26368;&#22823;&#25439;&#22833;&#65292;&#23547;&#25214;&#8220;&#24179;&#22374;&#26368;&#23567;&#20540;&#8221;&#25152;&#22312;&#30340;&#8220;&#24179;&#22374;&#23665;&#35895;&#8221;&#65292;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#23613;&#31649;&#32771;&#34385;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;&#38160;&#24230;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20294;&#36825;&#31181;&#8220;&#36807;&#20110;&#21451;&#22909;&#30340;&#25932;&#23545;&#32773;&#8221;&#21487;&#33021;&#20250;&#38480;&#21046;&#27867;&#21270;&#30340;&#26368;&#39640;&#27700;&#24179;&#12290;&#26412;&#25991;&#30340;&#26032;&#26041;&#27861;&#36890;&#36807;&#26041;&#24046;&#25233;&#21046;&#65288;VaSSO&#65289;&#26469;&#31283;&#23450;&#25932;&#23545;&#32773;&#65292;&#36991;&#20813;&#36825;&#31181;&#21451;&#22909;&#24615;&#12290; VaSSO&#30340;&#31283;&#23450;&#24615;&#21487;&#35777;&#26126;&#65292;&#24182;&#22312;&#27169;&#22411;&#26080;&#20851;&#20219;&#21153;&#20013;&#65288;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#21644;&#26426;&#22120;&#32763;&#35793;&#65289;&#30456;&#23545;&#20110;SAM&#26377;&#30528;&#25968;&#20540;&#19978;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#35777;&#23454;VaSSO&#36171;&#20104;SAM&#23545;&#39640;&#27700;&#24179;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharpness-aware minimization (SAM) has well documented merits in enhancing generalization of deep neural networks, even without sizable data augmentation. Embracing the geometry of the loss function, where neighborhoods of 'flat minima' heighten generalization ability, SAM seeks 'flat valleys' by minimizing the maximum loss caused by an adversary perturbing parameters within the neighborhood. Although critical to account for sharpness of the loss function, such an 'over-friendly adversary' can curtail the outmost level of generalization. The novel approach of this contribution fosters stabilization of adversaries through variance suppression (VaSSO) to avoid such friendliness. VaSSO's provable stability safeguards its numerical improvement over SAM in model-agnostic tasks, including image classification and machine translation. In addition, experiments confirm that VaSSO endows SAM with robustness against high levels of label noise.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FRS-Nets&#30340;&#26032;&#22411;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21033;&#29992;&#20613;&#37324;&#21494;&#21442;&#25968;&#21270;&#23454;&#29616;&#20102;&#23545;&#26059;&#36716;&#21644;&#23610;&#24230;&#30340;&#31561;&#21464;&#24615;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#22312;U-Net&#21644;Iter-Net&#20013;&#26367;&#25442;&#20256;&#32479;&#21367;&#31215;&#28388;&#27874;&#22120;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20998;&#21106;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.15638</link><description>&lt;p&gt;
FRS-Nets: Fourier&#21442;&#25968;&#21270;&#30340;&#26059;&#36716;&#21644;&#23610;&#24230;&#31561;&#21464;&#32593;&#32476;&#29992;&#20110;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
FRS-Nets: Fourier Parameterized Rotation and Scale Equivariant Networks for Retinal Vessel Segmentation. (arXiv:2309.15638v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FRS-Nets&#30340;&#26032;&#22411;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21033;&#29992;&#20613;&#37324;&#21494;&#21442;&#25968;&#21270;&#23454;&#29616;&#20102;&#23545;&#26059;&#36716;&#21644;&#23610;&#24230;&#30340;&#31561;&#21464;&#24615;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#22312;U-Net&#21644;Iter-Net&#20013;&#26367;&#25442;&#20256;&#32479;&#21367;&#31215;&#28388;&#27874;&#22120;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20998;&#21106;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#24179;&#31227;&#31561;&#21464;&#24615;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#22312;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;CNNs&#27809;&#26377;&#23545;&#34880;&#31649;&#24418;&#24577;&#30340;&#20854;&#20182;&#23545;&#31216;&#24615;&#36827;&#34892;&#24314;&#27169;&#65292;&#20363;&#22914;&#26059;&#36716;&#21644;&#23610;&#24230;&#23545;&#31216;&#24615;&#12290;&#20026;&#20102;&#22312;CNNs&#20013;&#23884;&#20837;&#26356;&#22810;&#31561;&#21464;&#24615;&#24182;&#28385;&#36275;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#35201;&#27714;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21367;&#31215;&#31639;&#23376;&#65288;FRS-Conv&#65289;&#65292;&#23427;&#26159;&#20613;&#37324;&#21494;&#21442;&#25968;&#21270;&#30340;&#65292;&#24182;&#19988;&#23545;&#26059;&#36716;&#21644;&#32553;&#25918;&#31561;&#21464;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#37319;&#29992;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#21270;&#26041;&#26696;&#65292;&#20351;&#21367;&#31215;&#28388;&#27874;&#22120;&#33021;&#22815;&#20197;&#39640;&#31934;&#24230;&#20219;&#24847;&#36827;&#34892;&#21464;&#25442;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#26059;&#36716;&#21644;&#23610;&#24230;&#31561;&#21464;&#21367;&#31215;&#26144;&#23556;&#30340;&#20844;&#24335;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#25552;&#20986;&#30340;&#20844;&#24335;&#26500;&#24314;&#20102;FRS-Conv&#65292;&#24182;&#23558;U-Net&#21644;Iter-Net&#20013;&#30340;&#20256;&#32479;&#21367;&#31215;&#28388;&#27874;&#22120;&#26367;&#25442;&#20026;FRS-Conv&#65288;FRS-Nets&#65289;&#12290;&#25105;&#20204;&#24544;&#23454;&#22320;&#22797;&#29616;&#20102;&#25152;&#26377;&#23545;&#27604;&#26041;&#27861;&#65292;&#24182;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
With translation equivariance, convolution neural networks (CNNs) have achieved great success in retinal vessel segmentation. However, some other symmetries of the vascular morphology are not characterized by CNNs, such as rotation and scale symmetries. To embed more equivariance into CNNs and achieve the accuracy requirement for retinal vessel segmentation, we construct a novel convolution operator (FRS-Conv), which is Fourier parameterized and equivariant to rotation and scaling. Specifically, we first adopt a new parameterization scheme, which enables convolutional filters to arbitrarily perform transformations with high accuracy. Secondly, we derive the formulations for the rotation and scale equivariant convolution mapping. Finally, we construct FRS-Conv following the proposed formulations and replace the traditional convolution filters in U-Net and Iter-Net with FRS-Conv (FRS-Nets). We faithfully reproduce all compared methods and conduct comprehensive experiments on three public
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#21019;&#24314;&#20840;&#33258;&#21160;&#30340;&#20250;&#35758;&#35760;&#24405;&#21644;&#22810;&#35821;&#35328;&#32763;&#35793;&#65292;&#35299;&#20915;&#20102;&#20250;&#35758;&#31649;&#29702;&#25991;&#26723;&#20013;&#29616;&#26377;&#24037;&#20316;&#27969;&#31243;&#30340;&#26367;&#20195;&#21644;&#25913;&#21892;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.15609</link><description>&lt;p&gt;
&#21457;&#23637;&#22269;&#38469;&#22810;&#35821;&#31181;&#20250;&#35758;&#30340;&#33258;&#21160;&#36880;&#23383;&#36716;&#24405;&#65306;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Developing automatic verbatim transcripts for international multilingual meetings: an end-to-end solution. (arXiv:2309.15609v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#21019;&#24314;&#20840;&#33258;&#21160;&#30340;&#20250;&#35758;&#35760;&#24405;&#21644;&#22810;&#35821;&#35328;&#32763;&#35793;&#65292;&#35299;&#20915;&#20102;&#20250;&#35758;&#31649;&#29702;&#25991;&#26723;&#20013;&#29616;&#26377;&#24037;&#20316;&#27969;&#31243;&#30340;&#26367;&#20195;&#21644;&#25913;&#21892;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#25972;&#30340;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#21019;&#24314;&#20840;&#33258;&#21160;&#30340;&#20250;&#35758;&#35760;&#24405;&#21644;&#23545;&#23427;&#20204;&#36827;&#34892;&#22810;&#31181;&#35821;&#35328;&#30340;&#26426;&#22120;&#32763;&#35793;&#12290;&#35813;&#24037;&#20855;&#26159;&#22312;&#19990;&#30028;&#30693;&#35782;&#20135;&#26435;&#32452;&#32455;&#65288;WIPO&#65289;&#24320;&#21457;&#30340;&#12289;&#20351;&#29992;&#20854;&#20869;&#37096;&#24320;&#21457;&#30340;&#35821;&#38899;&#36716;&#25991;&#26412;&#65288;S2T&#65289;&#21644;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#32452;&#20214;&#30340;&#31995;&#32479;&#12290;&#38500;&#20102;&#25551;&#36848;&#25968;&#25454;&#25910;&#38598;&#21644;&#20248;&#21270;&#36807;&#31243;&#65292;&#29983;&#25104;&#39640;&#24230;&#23450;&#21046;&#21644;&#31283;&#20581;&#30340;&#31995;&#32479;&#22806;&#65292;&#26412;&#25991;&#36824;&#25551;&#36848;&#20102;&#25216;&#26415;&#32452;&#20214;&#30340;&#26550;&#26500;&#21644;&#28436;&#21464;&#65292;&#24182;&#31361;&#20986;&#20102;&#29992;&#25143;&#26041;&#38754;&#30340;&#21830;&#19994;&#24433;&#21709;&#21644;&#25910;&#30410;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25351;&#20986;&#20102;&#31995;&#32479;&#22312;&#28436;&#36827;&#21644;&#37319;&#29992;&#36807;&#31243;&#20013;&#30340;&#29305;&#27530;&#25361;&#25112;&#65292;&#24182;&#20171;&#32461;&#20102;&#36825;&#31181;&#26032;&#26041;&#27861;&#22914;&#20309;&#21019;&#36896;&#20102;&#19968;&#31181;&#26032;&#20135;&#21697;&#65292;&#24182;&#21462;&#20195;&#20102;&#20250;&#35758;&#31649;&#29702;&#25991;&#26723;&#20013;&#29616;&#26377;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an end-to-end solution for the creation of fully automated conference meeting transcripts and their machine translations into various languages. This tool has been developed at the World Intellectual Property Organization (WIPO) using in-house developed speech-to-text (S2T) and machine translation (MT) components. Beyond describing data collection and fine-tuning, resulting in a highly customized and robust system, this paper describes the architecture and evolution of the technical components as well as highlights the business impact and benefits from the user side. We also point out particular challenges in the evolution and adoption of the system and how the new approach created a new product and replaced existing established workflows in conference management documentation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26174;&#24335;&#28789;&#25935;&#24230;&#22270;&#30340;&#23637;&#24320;&#24515;&#33039;MRI&#37325;&#24314;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#31639;&#27861;&#23637;&#24320;&#65292;&#36890;&#36807;&#23398;&#20064;&#22270;&#20687;&#20043;&#38388;&#30340;&#25509;&#25910;&#32447;&#22280;&#20851;&#31995;&#26469;&#23454;&#29616;&#21152;&#36895;&#24515;&#33039;MRI&#37325;&#24314;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15608</link><description>&lt;p&gt;
NoSENSE&#65306;&#23398;&#20064;&#26080;&#38656;&#26174;&#24335;&#28789;&#25935;&#24230;&#22270;&#30340;&#23637;&#24320;&#24515;&#33039;MRI&#37325;&#24314;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NoSENSE: Learned unrolled cardiac MRI reconstruction without explicit sensitivity maps. (arXiv:2309.15608v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26174;&#24335;&#28789;&#25935;&#24230;&#22270;&#30340;&#23637;&#24320;&#24515;&#33039;MRI&#37325;&#24314;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#31639;&#27861;&#23637;&#24320;&#65292;&#36890;&#36807;&#23398;&#20064;&#22270;&#20687;&#20043;&#38388;&#30340;&#25509;&#25910;&#32447;&#22280;&#20851;&#31995;&#26469;&#23454;&#29616;&#21152;&#36895;&#24515;&#33039;MRI&#37325;&#24314;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#22270;&#20687;&#37325;&#24314;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22522;&#20110;&#22810;&#20010;&#25509;&#25910;&#32447;&#22280;&#30340;&#21152;&#36895;&#24515;&#33039;MRI&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#31639;&#27861;&#23637;&#24320;&#12290;&#19982;&#35768;&#22810;&#29616;&#26377;&#30340;&#23398;&#20064;MR&#22270;&#20687;&#37325;&#24314;&#25216;&#26415;&#19981;&#21516;&#65292;&#38656;&#35201;&#23558;&#28789;&#25935;&#24230;&#26144;&#23556;&#65288;CSM&#65289;&#20272;&#35745;&#20316;&#20026;&#19968;&#20010;&#29420;&#31435;&#30340;&#32593;&#32476;&#32452;&#20214;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;&#26174;&#24335;&#30340;CSM&#20272;&#35745;&#12290;&#30456;&#21453;&#65292;&#23427;&#38544;&#21547;&#22320;&#25429;&#25417;&#24182;&#23398;&#20064;&#21033;&#29992;&#22270;&#20687;&#20043;&#38388;&#30340;&#25509;&#25910;&#32447;&#22280;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30001;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#23398;&#20064;&#22270;&#20687;&#22359;&#21644;k&#31354;&#38388;&#22359;&#32452;&#25104;&#65292;&#20849;&#20139;&#28508;&#22312;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#35843;&#33410;&#21644;&#25509;&#25910;&#32447;&#22280;&#25968;&#25454;&#19968;&#33268;&#24615;&#23454;&#29616;&#23545;&#37319;&#38598;&#21442;&#25968;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;MICCAI STACOM CMRxRecon&#25361;&#25112;&#36187;&#30340;&#24433;&#29255;&#36861;&#36394;&#21644;&#26144;&#23556;&#36861;&#36394;&#39564;&#35777;&#25490;&#34892;&#27036;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#21035;&#22312;PSNR&#20540;&#19978;&#36798;&#21040;&#20102;34.89&#21644;35.56&#65292;SSIM&#20540;&#20998;&#21035;&#20026;0.920&#21644;0.942&#65292;&#22312;&#25776;&#20889;&#26412;&#25991;&#26102;&#20301;&#21015;&#19981;&#21516;&#23567;&#32452;&#31532;4&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel learned image reconstruction method for accelerated cardiac MRI with multiple receiver coils based on deep convolutional neural networks (CNNs) and algorithm unrolling. In contrast to many existing learned MR image reconstruction techniques that necessitate coil-sensitivity map (CSM) estimation as a distinct network component, our proposed approach avoids explicit CSM estimation. Instead, it implicitly captures and learns to exploit the inter-coil relationships of the images. Our method consists of a series of novel learned image and k-space blocks with shared latent information and adaptation to the acquisition parameters by feature-wise modulation (FiLM), as well as coil-wise data-consistency (DC) blocks.  Our method achieved PSNR values of 34.89 and 35.56 and SSIM values of 0.920 and 0.942 in the cine track and mapping track validation leaderboard of the MICCAI STACOM CMRxRecon Challenge, respectively, ranking 4th among different teams at the time of writing.  Cod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29109;&#21305;&#37197;&#26694;&#26550;&#30340;&#26032;&#30340;&#21487;&#22788;&#29702;&#30340;&#25512;&#26029;&#26041;&#26696;&#65292;&#21487;&#20197;&#23884;&#20837;&#21040;&#26399;&#26395;&#20256;&#25773;&#31639;&#27861;&#20013;&#65292;&#23545;&#20110;&#25551;&#36848;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#36807;&#31243;&#30340;Markov&#36339;&#36291;&#36807;&#31243;&#30340;&#32479;&#35745;&#25512;&#26029;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#19968;&#31867;&#36817;&#20284;&#20998;&#24067;&#30340;&#38381;&#24335;&#32467;&#26524;&#20197;&#21450;&#24212;&#29992;&#20110;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#30340;&#19968;&#33324;&#31867;&#21035;&#26469;&#21152;&#20197;&#35770;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#36817;&#20284;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#31243;&#24207;&#23548;&#20986;&#20102;&#28508;&#22312;&#21442;&#25968;&#30340;&#28857;&#20272;&#35745;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;&#24182;&#22312;&#21508;&#31181;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#31034;&#20363;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#35813;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.15604</link><description>&lt;p&gt;
Entropic Matching&#29992;&#20110;Markov&#36339;&#36291;&#36807;&#31243;&#30340;&#26399;&#26395;&#20256;&#25773;&#30340;&#29109;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Entropic Matching for Expectation Propagation of Markov Jump Processes. (arXiv:2309.15604v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29109;&#21305;&#37197;&#26694;&#26550;&#30340;&#26032;&#30340;&#21487;&#22788;&#29702;&#30340;&#25512;&#26029;&#26041;&#26696;&#65292;&#21487;&#20197;&#23884;&#20837;&#21040;&#26399;&#26395;&#20256;&#25773;&#31639;&#27861;&#20013;&#65292;&#23545;&#20110;&#25551;&#36848;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#36807;&#31243;&#30340;Markov&#36339;&#36291;&#36807;&#31243;&#30340;&#32479;&#35745;&#25512;&#26029;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#19968;&#31867;&#36817;&#20284;&#20998;&#24067;&#30340;&#38381;&#24335;&#32467;&#26524;&#20197;&#21450;&#24212;&#29992;&#20110;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#30340;&#19968;&#33324;&#31867;&#21035;&#26469;&#21152;&#20197;&#35770;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#36817;&#20284;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#31243;&#24207;&#23548;&#20986;&#20102;&#28508;&#22312;&#21442;&#25968;&#30340;&#28857;&#20272;&#35745;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;&#24182;&#22312;&#21508;&#31181;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#31034;&#20363;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#35813;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#28508;&#22312;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#36807;&#31243;&#30340;&#32479;&#35745;&#25512;&#26029;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#36890;&#24120;&#38590;&#20197;&#22788;&#29702;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#30001;Markov&#36339;&#36291;&#36807;&#31243;&#25551;&#36848;&#30340;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#36807;&#31243;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#22788;&#29702;&#30340;&#25512;&#26029;&#26041;&#26696;&#65292;&#22522;&#20110;&#29109;&#21305;&#37197;&#26694;&#26550;&#65292;&#21487;&#20197;&#23884;&#20837;&#21040;&#20247;&#25152;&#21608;&#30693;&#30340;&#26399;&#26395;&#20256;&#25773;&#31639;&#27861;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#20026;&#19968;&#31867;&#31616;&#21333;&#30340;&#36817;&#20284;&#20998;&#24067;&#25552;&#20379;&#38381;&#24335;&#32467;&#26524;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#30340;&#19968;&#33324;&#31867;&#21035;&#65292;&#35813;&#31867;&#21035;&#26159;&#31995;&#32479;&#29983;&#29289;&#23398;&#24314;&#27169;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#26469;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#36817;&#20284;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#31243;&#24207;&#23548;&#20986;&#20102;&#28508;&#22312;&#21442;&#25968;&#30340;&#28857;&#20272;&#35745;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#21508;&#31181;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#31034;&#20363;&#20013;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#38543;&#26426;&#30340;Lotka-Voltera&#31034;&#20363;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the problem of statistical inference for latent continuous-time stochastic processes, which is often intractable, particularly for discrete state space processes described by Markov jump processes. To overcome this issue, we propose a new tractable inference scheme based on an entropic matching framework that can be embedded into the well-known expectation propagation algorithm. We demonstrate the effectiveness of our method by providing closed-form results for a simple family of approximate distributions and apply it to the general class of chemical reaction networks, which are a crucial tool for modeling in systems biology. Moreover, we derive closed form expressions for point estimation of the underlying parameters using an approximate expectation maximization procedure. We evaluate the performance of our method on various chemical reaction network instantiations, including a stochastic Lotka-Voltera example, and discuss its limitations and potential for future 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#27491;&#21017;&#21270;&#26469;&#25552;&#21462;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36817;&#20284;&#35745;&#31639;&#20219;&#21153;&#29366;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#36317;&#31163;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#22870;&#21169;&#26469;&#35268;&#33539;&#20449;&#24687;&#20998;&#20139;&#30340;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#21152;&#36895;&#20195;&#29702;&#30340;&#23398;&#20064;&#36807;&#31243;&#24182;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.15603</link><description>&lt;p&gt;
&#22312;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#27491;&#21017;&#21270;&#26469;&#25552;&#21462;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Distill Knowledge in Multi-task Reinforcement Learning with Optimal-Transport Regularization. (arXiv:2309.15603v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15603
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#27491;&#21017;&#21270;&#26469;&#25552;&#21462;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36817;&#20284;&#35745;&#31639;&#20219;&#21153;&#29366;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#36317;&#31163;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#22870;&#21169;&#26469;&#35268;&#33539;&#20449;&#24687;&#20998;&#20139;&#30340;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#21152;&#36895;&#20195;&#29702;&#30340;&#23398;&#20064;&#36807;&#31243;&#24182;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#20174;&#20854;&#20182;&#19981;&#21516;&#20294;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#36716;&#31227;&#30693;&#35782;&#65292;&#21487;&#20197;&#25552;&#39640;&#35757;&#32451;&#20195;&#29702;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;Kullback-Leibler&#27491;&#21017;&#21270;&#26469;&#31283;&#23450;&#20174;&#19968;&#20010;&#20219;&#21153;&#21040;&#20854;&#20182;&#20219;&#21153;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#27491;&#21017;&#21270;&#26469;&#26367;&#20195;Kullback-Leibler&#25955;&#24230;&#30340;&#26041;&#21521;&#12290;&#36890;&#36807;&#20351;&#29992;Sinkhorn&#26144;&#23556;&#65292;&#25105;&#20204;&#21487;&#20197;&#36817;&#20284;&#35745;&#31639;&#20219;&#21153;&#30340;&#29366;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#36317;&#31163;&#12290;&#28982;&#21518;&#23558;&#35813;&#36317;&#31163;&#20316;&#20026;&#20998;&#25674;&#22870;&#21169;&#65292;&#29992;&#26469;&#35268;&#33539;&#20449;&#24687;&#20849;&#20139;&#30340;&#37327;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20110;&#26629;&#26684;&#30340;&#23548;&#33322;&#22810;&#30446;&#26631;&#29615;&#22659;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#28155;&#21152;&#30340;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#22870;&#21169;&#33021;&#22815;&#21152;&#36895;&#20195;&#29702;&#23398;&#20064;&#36807;&#31243;&#24182;&#36229;&#36807;&#20102;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multi-task reinforcement learning, it is possible to improve the data efficiency of training agents by transferring knowledge from other different but related tasks. Because the experiences from different tasks are usually biased toward the specific task goals. Traditional methods rely on Kullback-Leibler regularization to stabilize the transfer of knowledge from one task to the others. In this work, we explore the direction of replacing the Kullback-Leibler divergence with a novel Optimal transport-based regularization. By using the Sinkhorn mapping, we can approximate the Optimal transport distance between the state distribution of tasks. The distance is then used as an amortized reward to regularize the amount of sharing information. We experiment our frameworks on several grid-based navigation multi-goal to validate the effectiveness of the approach. The results show that our added Optimal transport-based rewards are able to speed up the learning process of agents and outperform
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21033;&#29992;&#28023;&#27915;&#21355;&#26143;&#25968;&#25454;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#28023;&#27915;&#34920;&#38754;&#39640;&#24230;&#21464;&#21270;&#65292;&#20174;&#32780;&#23545;&#20154;&#31867;&#27963;&#21160;&#21644;&#27668;&#20505;&#35843;&#33410;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.15599</link><description>&lt;p&gt;
OceanBench&#65306;&#28023;&#27915;&#34920;&#38754;&#39640;&#24230;&#29256;
&lt;/p&gt;
&lt;p&gt;
OceanBench: The Sea Surface Height Edition. (arXiv:2309.15599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15599
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21033;&#29992;&#28023;&#27915;&#21355;&#26143;&#25968;&#25454;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#28023;&#27915;&#34920;&#38754;&#39640;&#24230;&#21464;&#21270;&#65292;&#20174;&#32780;&#23545;&#20154;&#31867;&#27963;&#21160;&#21644;&#27668;&#20505;&#35843;&#33410;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#23545;&#20154;&#31867;&#27963;&#21160;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#65292;&#24182;&#22312;&#27668;&#20505;&#35843;&#33410;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#38543;&#30528;&#21355;&#26143;&#36965;&#24863;&#25968;&#25454;&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#23545;&#28023;&#27915;&#30340;&#29702;&#35299;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#37324;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25429;&#25417;&#21040;&#20840;&#29699;&#24517;&#35201;&#30340;&#25968;&#37327;&#65292;&#20363;&#22914;&#28023;&#27915;&#34920;&#38754;&#39640;&#24230; (SSH)&#12290;&#28982;&#32780;&#65292;&#28023;&#27915;&#21355;&#26143;&#25968;&#25454;&#30001;&#20110;&#31232;&#30095;&#21644;&#19981;&#35268;&#21017;&#37319;&#26679;&#12289;&#20449;&#21495;&#22797;&#26434;&#24615;&#21644;&#22122;&#22768;&#31561;&#38382;&#39064;&#65292;&#23545;&#20449;&#24687;&#25552;&#21462;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#26426;&#22120;&#23398;&#20064; (ML) &#25216;&#26415;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#12289;&#22797;&#26434;&#20449;&#21495;&#26041;&#38754;&#23637;&#31034;&#20102;&#20854;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026; ML &#27169;&#22411;&#26377;&#26426;&#20250;&#21033;&#29992;&#28023;&#27915;&#21355;&#26143;&#25968;&#25454;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#34920;&#31034;&#21644;&#30456;&#20851;&#35780;&#20272;&#25351;&#26631;&#21487;&#33021;&#26159;&#30830;&#23450;&#24212;&#29992; ML &#25104;&#21151;&#19982;&#21542;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20174;&#21407;&#22987;&#35266;&#27979;&#25968;&#25454;&#21040; ML &#21487;&#29992;&#29366;&#24577;&#30340;&#22788;&#29702;&#27493;&#39588;&#65292;&#20197;&#21450;&#20174;&#27169;&#22411;&#36755;&#20986;&#21040;&#21487;&#35299;&#37322;&#25968;&#37327;&#30340;&#22788;&#29702;&#27493;&#39588;&#65292;&#37117;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#36825;&#21487;&#33021;&#26159;&#23545; ML &#20837;&#38376;&#30340;&#37325;&#35201;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ocean profoundly influences human activities and plays a critical role in climate regulation. Our understanding has improved over the last decades with the advent of satellite remote sensing data, allowing us to capture essential quantities over the globe, e.g., sea surface height (SSH). However, ocean satellite data presents challenges for information extraction due to their sparsity and irregular sampling, signal complexity, and noise. Machine learning (ML) techniques have demonstrated their capabilities in dealing with large-scale, complex signals. Therefore we see an opportunity for ML models to harness the information contained in ocean satellite data. However, data representation and relevant evaluation metrics can be the defining factors when determining the success of applied ML. The processing steps from the raw observation data to a ML-ready state and from model outputs to interpretable quantities require domain expertise, which can be a significant barrier to entry for M
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#23558;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#26041;&#27861;&#24212;&#29992;&#20110;&#28608;&#23376;&#26497;&#21270;&#23376;&#24211;&#20262;&#20957;&#32858;&#31995;&#32479;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#23454;&#29616;&#23545;Gross-Pitaevskii&#26041;&#31243;&#30340;&#27714;&#35299;&#65292;&#21487;&#20197;&#20197;&#25509;&#36817;1000&#20493;&#30340;&#36895;&#24230;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#29366;&#24577;&#30340;&#35299;&#65292;&#20026;&#28608;&#23376;&#26497;&#21270;&#23376;&#24211;&#20262;&#20957;&#32858;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.15593</link><description>&lt;p&gt;
&#28608;&#23376;&#26497;&#21270;&#23376;&#24211;&#20262;&#20957;&#32858;&#65306;&#19968;&#31181;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exciton-Polariton Condensates: A Fourier Neural Operator Approach. (arXiv:2309.15593v1 [cond-mat.quant-gas])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23558;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#26041;&#27861;&#24212;&#29992;&#20110;&#28608;&#23376;&#26497;&#21270;&#23376;&#24211;&#20262;&#20957;&#32858;&#31995;&#32479;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#23454;&#29616;&#23545;Gross-Pitaevskii&#26041;&#31243;&#30340;&#27714;&#35299;&#65292;&#21487;&#20197;&#20197;&#25509;&#36817;1000&#20493;&#30340;&#36895;&#24230;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#29366;&#24577;&#30340;&#35299;&#65292;&#20026;&#28608;&#23376;&#26497;&#21270;&#23376;&#24211;&#20262;&#20957;&#32858;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#21322;&#23548;&#20307;&#21046;&#36896;&#25216;&#26415;&#30340;&#36827;&#23637;&#20652;&#29983;&#20102;&#23545;&#30001;&#28608;&#23376;&#26497;&#21270;&#23376;&#24211;&#20262;&#20957;&#32858;&#39537;&#21160;&#30340;&#20840;&#20809;&#23398;&#22120;&#20214;&#30340;&#24191;&#27867;&#30740;&#31350;&#12290;&#21253;&#25324;&#26230;&#20307;&#31649;&#22312;&#20869;&#30340;&#36825;&#31867;&#22120;&#20214;&#30340;&#21021;&#27493;&#39564;&#35777;&#24050;&#32463;&#22312;&#29615;&#22659;&#26465;&#20214;&#19979;&#21462;&#24471;&#20102;&#40723;&#33310;&#20154;&#24515;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#20110;&#22823;&#35268;&#27169;&#24212;&#29992;&#39046;&#22495;&#65306;&#32570;&#20047;&#19968;&#20010;&#20581;&#22766;&#30340;&#27714;&#35299;&#22120;&#65292;&#21487;&#20197;&#29992;&#20110;&#27169;&#25311;&#38656;&#35201;&#36739;&#38271;&#26102;&#38388;&#36798;&#21040;&#31283;&#23450;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#26041;&#27861;&#65292;&#29992;&#20110;&#27714;&#35299;&#19982;&#39069;&#22806;&#28608;&#23376;&#36895;&#29575;&#26041;&#31243;&#32806;&#21512;&#30340;Gross-Pitaevskii&#26041;&#31243;&#12290;&#36825;&#39033;&#24037;&#20316;&#26631;&#24535;&#30528;&#31070;&#32463;&#31639;&#23376;&#39318;&#27425;&#30452;&#25509;&#24212;&#29992;&#20110;&#28608;&#23376;&#26497;&#21270;&#23376;&#24211;&#20262;&#20957;&#32858;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#20110;CUDA&#30340;GPU&#27714;&#35299;&#22120;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#20197;&#25509;&#36817;1000&#20493;&#30340;&#36895;&#24230;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#29366;&#24577;&#30340;&#35299;&#12290;&#27492;&#22806;&#65292;&#36825;&#20026;&#20170;&#21518;&#30340;&#28145;&#20837;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in semiconductor fabrication over the past decade have catalyzed extensive research into all-optical devices driven by exciton-polariton condensates. Preliminary validations of such devices, including transistors, have shown encouraging results even under ambient conditions. A significant challenge still remains for large scale application however: the lack of a robust solver that can be used to simulate complex nonlinear systems which require an extended period of time to stabilize. Addressing this need, we propose the application of a machine-learning-based Fourier Neural Operator approach to find the solution to the Gross-Pitaevskii equations coupled with extra exciton rate equations. This work marks the first direct application of Neural Operators to an exciton-polariton condensate system. Our findings show that the proposed method can predict final-state solutions to a high degree of accuracy almost 1000 times faster than CUDA-based GPU solvers. Moreover, this paves t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20849;&#21516;&#35757;&#32451;&#22823;&#22411;&#33258;&#22238;&#24402;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#34701;&#21512;&#35821;&#35328;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;&#31574;&#30053;&#65292;&#20351;&#24471;&#35813;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22810;&#27169;&#24577;&#36755;&#20986;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15564</link><description>&lt;p&gt;
&#20849;&#21516;&#35757;&#32451;&#22823;&#22411;&#33258;&#22238;&#24402;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jointly Training Large Autoregressive Multimodal Models. (arXiv:2309.15564v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20849;&#21516;&#35757;&#32451;&#22823;&#22411;&#33258;&#22238;&#24402;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#34701;&#21512;&#35821;&#35328;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;&#31574;&#30053;&#65292;&#20351;&#24471;&#35813;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22810;&#27169;&#24577;&#36755;&#20986;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#35821;&#35328;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20004;&#31181;&#27169;&#24577;&#38598;&#25104;&#21040;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#26080;&#32541;&#22810;&#27169;&#24577;&#36755;&#20986;&#30340;&#21333;&#19968;&#24378;&#22823;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;&#33258;&#22238;&#24402;&#28151;&#21512;&#65288;JAM&#65289;&#26694;&#26550;&#65292;&#19968;&#31181;&#31995;&#32479;&#34701;&#21512;&#29616;&#26377;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#30340;&#12289;&#25968;&#25454;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;&#31574;&#30053;&#65292;&#38024;&#23545;&#28151;&#21512;&#27169;&#24577;&#29983;&#25104;&#20219;&#21153;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#25105;&#20204;&#26368;&#32456;&#30340;&#35843;&#20248;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22810;&#27169;&#24577;&#36755;&#20986;&#26041;&#38754;&#34920;&#29616;&#20986;&#26080;&#19982;&#20262;&#27604;&#30340;&#24615;&#33021;&#65292;&#24182;&#20195;&#34920;&#20102;&#31532;&#19968;&#20010;&#26126;&#30830;&#20026;&#27492;&#30446;&#30340;&#32780;&#35774;&#35745;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge. To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models. We also introduce a specialized, data-efficient instruction-tuning strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned model demonstrates unparalleled performance in generating high-quality multimodal outputs and represents the first model explicitly designed for this purpose.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#22312;&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;&#20013;&#65292;&#24403;&#28857;&#20987;&#25968;&#25454;&#19981;&#33021;&#23436;&#20840;&#25311;&#21512;&#26102;&#65292;&#26080;&#27861;&#24674;&#22797;&#30495;&#23454;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#25490;&#21517;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#65292;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#24615;&#22270;&#27169;&#22411;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.15560</link><description>&lt;p&gt;
&#35782;&#21035;&#24615;&#24456;&#37325;&#35201;&#65306;&#25581;&#31034;&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;&#20013;&#38544;&#34255;&#30340;&#21487;&#24674;&#22797;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Identifiability Matters: Revealing the Hidden Recoverable Condition in Unbiased Learning to Rank. (arXiv:2309.15560v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15560
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#22312;&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;&#20013;&#65292;&#24403;&#28857;&#20987;&#25968;&#25454;&#19981;&#33021;&#23436;&#20840;&#25311;&#21512;&#26102;&#65292;&#26080;&#27861;&#24674;&#22797;&#30495;&#23454;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#25490;&#21517;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#65292;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#24615;&#22270;&#27169;&#22411;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;(Unbiased Learning to Rank, ULTR)&#22312;&#20174;&#26377;&#20559;&#28857;&#20987;&#26085;&#24535;&#35757;&#32451;&#26080;&#20559;&#25490;&#21517;&#27169;&#22411;&#30340;&#29616;&#20195;&#31995;&#32479;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#20851;&#38190;&#22312;&#20110;&#26126;&#30830;&#22320;&#24314;&#27169;&#29992;&#25143;&#34892;&#20026;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#22522;&#20110;&#26816;&#39564;&#20551;&#35774;&#23545;&#28857;&#20987;&#25968;&#25454;&#36827;&#34892;&#25311;&#21512;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#32463;&#39564;&#24615;&#22320;&#21457;&#29616;&#21482;&#35201;&#28857;&#20987;&#23436;&#20840;&#25311;&#21512;&#65292;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#21487;&#20197;&#24674;&#22797;&#20986;&#30495;&#23454;&#28508;&#22312;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#24182;&#38750;&#24635;&#26159;&#33021;&#22815;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20174;&#32780;&#23548;&#33268;&#25490;&#21517;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#22238;&#31572;&#30495;&#23454;&#30456;&#20851;&#24615;&#26159;&#21542;&#33021;&#22815;&#20174;&#28857;&#20987;&#25968;&#25454;&#24674;&#22797;&#20986;&#26469;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;ULTR&#39046;&#22495;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#19968;&#20010;&#25490;&#21517;&#27169;&#22411;&#23450;&#20041;&#20026;&#21487;&#35782;&#21035;&#30340;&#65292;&#22914;&#26524;&#23427;&#21487;&#20197;&#24674;&#22797;&#20986;&#30495;&#23454;&#30456;&#20851;&#24615;&#65292;&#26368;&#22810;&#21482;&#26377;&#19968;&#20010;&#32553;&#25918;&#21464;&#25442;&#65292;&#36825;&#23545;&#20110;&#25104;&#23545;&#25490;&#21517;&#30446;&#26631;&#26469;&#35828;&#24050;&#36275;&#22815;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#31561;&#20215;&#30340;&#21487;&#35782;&#21035;&#26465;&#20214;&#65292;&#21487;&#20197;&#26032;&#39062;&#22320;&#34920;&#36798;&#20026;&#19968;&#20010;&#22270;&#36830;&#36890;&#24615;&#27979;&#35797;&#38382;&#39064;&#65306;&#24403;&#19988;&#20165;&#24403;&#19968;&#20010;&#22270;&#65288;&#21363;&#21487;&#35782;&#21035;&#24615;&#22270;&#65289;&#36830;&#36890;&#26102;&#65292;&#35813;&#25490;&#21517;&#27169;&#22411;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of Unbiased Learning to Rank (ULTR) is widespread in modern systems for training unbiased ranking models from biased click logs. The key is to explicitly model a generation process for user behavior and fit click data based on examination hypothesis. Previous research found empirically that the true latent relevance can be recovered in most cases as long as the clicks are perfectly fitted. However, we demonstrate that this is not always achievable, resulting in a significant reduction in ranking performance. In this work, we aim to answer if or when the true relevance can be recovered from click data, which is a foundation issue for ULTR field. We first define a ranking model as identifiable if it can recover the true relevance up to a scaling transformation, which is enough for pairwise ranking objective. Then we explore an equivalent condition for identifiability that can be novely expressed as a graph connectivity test problem: if and only if a graph (namely identifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Shapley&#22686;&#21152;&#30340;&#33258;&#25105;&#24402;&#22240;&#31070;&#32463;&#32593;&#32476;(SASANet)&#26469;&#23454;&#29616;&#24544;&#23454;&#30340;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#35299;&#37322;&#12290;&#36890;&#36807;&#24341;&#20837;Shapley&#20540;&#65292;SASANet&#33021;&#22815;&#30830;&#20445;&#33258;&#25105;&#24402;&#22240;&#20540;&#19982;&#36755;&#20986;&#30340;Shapley&#20540;&#30456;&#31561;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#35299;&#37322;&#26694;&#26550;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SASANet&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#33258;&#25105;&#24402;&#22240;&#27169;&#22411;&#65292;&#24182;&#19982;&#40657;&#30418;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#27492;&#22806;&#65292;SASANet&#22312;&#20132;&#20114;&#24335;&#35299;&#37322;&#21644;&#25928;&#29575;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#31934;&#30830;&#24230;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15559</link><description>&lt;p&gt;
&#20197;Shapley&#22686;&#21152;&#30340;&#33258;&#25105;&#24402;&#22240;&#23454;&#29616;&#24544;&#23454;&#30340;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Towards Faithful Neural Network Intrinsic Interpretation with Shapley Additive Self-Attribution. (arXiv:2309.15559v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Shapley&#22686;&#21152;&#30340;&#33258;&#25105;&#24402;&#22240;&#31070;&#32463;&#32593;&#32476;(SASANet)&#26469;&#23454;&#29616;&#24544;&#23454;&#30340;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#35299;&#37322;&#12290;&#36890;&#36807;&#24341;&#20837;Shapley&#20540;&#65292;SASANet&#33021;&#22815;&#30830;&#20445;&#33258;&#25105;&#24402;&#22240;&#20540;&#19982;&#36755;&#20986;&#30340;Shapley&#20540;&#30456;&#31561;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#35299;&#37322;&#26694;&#26550;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SASANet&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#33258;&#25105;&#24402;&#22240;&#27169;&#22411;&#65292;&#24182;&#19982;&#40657;&#30418;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#27492;&#22806;&#65292;SASANet&#22312;&#20132;&#20114;&#24335;&#35299;&#37322;&#21644;&#25928;&#29575;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#31934;&#30830;&#24230;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#30740;&#31350;&#20013;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#35813;&#39046;&#22495;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;(1)&#32570;&#20047;&#30830;&#20445;&#30495;&#27491;&#21487;&#35299;&#37322;&#24615;&#30340;&#22362;&#23454;&#29702;&#35770;&#22522;&#30784;&#65292;&#25110;&#32773;(2)&#29306;&#29298;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22686;&#21152;&#33258;&#25105;&#24402;&#22240;(ASA)&#26694;&#26550;&#12290;&#35266;&#23519;&#21040;&#22686;&#21152;&#33258;&#25105;&#24402;&#22240;&#20013;&#32570;&#20047;Shapley&#20540;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Shapley&#22686;&#21152;&#30340;&#33258;&#25105;&#24402;&#22240;&#31070;&#32463;&#32593;&#32476;(SASANet)&#65292;&#20854;&#20855;&#26377;&#20445;&#35777;&#33258;&#25105;&#24402;&#22240;&#20540;&#31561;&#20110;&#36755;&#20986;&#30340;Shapley&#20540;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SASANet&#20351;&#29992;&#22522;&#20110;&#36793;&#38469;&#36129;&#29486;&#30340;&#39034;&#24207;&#27169;&#24335;&#21644;&#20869;&#37096;&#33976;&#39311;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#20026;&#20219;&#24847;&#25968;&#37327;&#30340;&#29305;&#24449;&#24314;&#27169;&#26377;&#24847;&#20041;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#24471;&#21040;&#38750;&#36817;&#20284;&#30340;&#26377;&#24847;&#20041;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SASANet&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#33258;&#25105;&#24402;&#22240;&#27169;&#22411;&#65292;&#24182;&#19982;&#40657;&#30418;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#27492;&#22806;&#65292;SASANet&#22312;&#20132;&#20114;&#24335;&#35299;&#37322;&#21644;&#25928;&#29575;&#26041;&#38754;&#26174;&#31034;&#26356;&#31934;&#30830;&#21644;&#39640;&#25928;&#20110;&#20107;&#21518;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-interpreting neural networks have garnered significant interest in research. Existing works in this domain often (1) lack a solid theoretical foundation ensuring genuine interpretability or (2) compromise model expressiveness. In response, we formulate a generic Additive Self-Attribution (ASA) framework. Observing the absence of Shapley value in Additive Self-Attribution, we propose Shapley Additive Self-Attributing Neural Network (SASANet), with theoretical guarantees for the self-attribution value equal to the output's Shapley values. Specifically, SASANet uses a marginal contribution-based sequential schema and internal distillation-based training strategies to model meaningful outputs for any number of features, resulting in un-approximated meaningful value function. Our experimental results indicate SASANet surpasses existing self-attributing models in performance and rivals black-box models. Moreover, SASANet is shown more precise and efficient than post-hoc methods in inter
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;CrunchBase&#25968;&#25454;&#26469;&#39044;&#27979;&#21019;&#19994;&#25104;&#21151;&#21644;&#27169;&#25311;VC&#25237;&#36164;&#32452;&#21512;&#30340;&#26032;&#39062;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20840;&#38754;&#22238;&#28335;&#31639;&#27861;&#23545;&#27169;&#22411;&#22312;&#21382;&#21490;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.15552</link><description>&lt;p&gt;
&#20351;&#29992;CrunchBase&#25968;&#25454;&#39044;&#27979;&#21019;&#19994;&#20844;&#21496;&#25104;&#21151;&#21644;&#39118;&#38505;&#25237;&#36164;&#32452;&#21512;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Startup success prediction and VC portfolio simulation using CrunchBase data. (arXiv:2309.15552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;CrunchBase&#25968;&#25454;&#26469;&#39044;&#27979;&#21019;&#19994;&#25104;&#21151;&#21644;&#27169;&#25311;VC&#25237;&#36164;&#32452;&#21512;&#30340;&#26032;&#39062;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20840;&#38754;&#22238;&#28335;&#31639;&#27861;&#23545;&#27169;&#22411;&#22312;&#21382;&#21490;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#21019;&#19994;&#20844;&#21496;&#30340;&#25104;&#21151;&#23545;&#20110;&#21019;&#19994;&#29983;&#24577;&#31995;&#32479;&#30340;&#19981;&#31283;&#23450;&#24615;&#32780;&#35328;&#26159;&#19968;&#39033;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20511;&#21161;CrunchBase&#31561;&#24191;&#27867;&#25968;&#25454;&#24211;&#30340;&#20986;&#29616;&#65292;&#32467;&#21512;&#21487;&#29992;&#30340;&#24320;&#25918;&#25968;&#25454;&#65292;&#21487;&#20197;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#20998;&#26512;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#21019;&#19994;&#20844;&#21496;&#22312;B&#36718;&#21644;C&#36718;&#25237;&#36164;&#38454;&#27573;&#65292;&#26088;&#22312;&#39044;&#27979;&#20851;&#38190;&#30340;&#25104;&#21151;&#37324;&#31243;&#30865;&#65292;&#22914;&#23454;&#29616;&#39318;&#27425;&#20844;&#24320;&#21215;&#32929;&#65288;IPO&#65289;&#65292;&#36798;&#21040;&#29420;&#35282;&#20861;&#22320;&#20301;&#65292;&#25110;&#25104;&#21151;&#23454;&#26045;&#24182;&#36141;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#21019;&#19994;&#20844;&#21496;&#30340;&#25104;&#21151;&#65292;&#25972;&#21512;&#20102;&#21508;&#31181;&#22240;&#32032;&#65292;&#22914;&#36164;&#37329;&#25351;&#26631;&#12289;&#21019;&#22987;&#20154;&#29305;&#24449;&#21644;&#34892;&#19994;&#31867;&#21035;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#19968;&#20010;&#29420;&#29305;&#29305;&#28857;&#26159;&#20351;&#29992;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#22238;&#28335;&#31639;&#27861;&#26469;&#27169;&#25311;&#39118;&#38505;&#25237;&#36164;&#30340;&#25237;&#36164;&#36807;&#31243;&#12290;&#36825;&#31181;&#27169;&#25311;&#20801;&#35768;&#23545;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#38024;&#23545;&#21382;&#21490;&#25968;&#25454;&#30340;&#24378;&#22823;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting startup success presents a formidable challenge due to the inherently volatile landscape of the entrepreneurial ecosystem. The advent of extensive databases like Crunchbase jointly with available open data enables the application of machine learning and artificial intelligence for more accurate predictive analytics. This paper focuses on startups at their Series B and Series C investment stages, aiming to predict key success milestones such as achieving an Initial Public Offering (IPO), attaining unicorn status, or executing a successful Merger and Acquisition (M\&amp;A). We introduce novel deep learning model for predicting startup success, integrating a variety of factors such as funding metrics, founder features, industry category. A distinctive feature of our research is the use of a comprehensive backtesting algorithm designed to simulate the venture capital investment process. This simulation allows for a robust evaluation of our model's performance against historical data
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;DeepRepViz&#26694;&#26550;&#65292;&#29992;&#20110;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20013;&#35782;&#21035;&#28151;&#28102;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#24230;&#37327;&#21644;&#21487;&#35270;&#21270;&#24037;&#20855;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;DeepRepViz&#19982;DL&#27169;&#22411;&#32467;&#21512;&#33021;&#22815;&#24102;&#26469;&#26126;&#26174;&#30340;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2309.15551</link><description>&lt;p&gt;
&#20351;&#29992;DeepRepViz&#26469;&#35782;&#21035;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20013;&#30340;&#28151;&#28102;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Identifying confounders in deep-learning-based model predictions using DeepRepViz. (arXiv:2309.15551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15551
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;DeepRepViz&#26694;&#26550;&#65292;&#29992;&#20110;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20013;&#35782;&#21035;&#28151;&#28102;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#24230;&#37327;&#21644;&#21487;&#35270;&#21270;&#24037;&#20855;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;DeepRepViz&#19982;DL&#27169;&#22411;&#32467;&#21512;&#33021;&#22815;&#24102;&#26469;&#26126;&#26174;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#65292;&#25581;&#31034;&#22823;&#33041;&#12289;&#22823;&#33041;&#30149;&#29702;&#21644;&#24515;&#29702;&#29305;&#24449;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#35832;&#22914;&#21442;&#19982;&#32773;&#24180;&#40836;&#12289;&#24615;&#21035;&#25110;&#24433;&#20687;&#20266;&#24433;&#31561;&#22806;&#37096;&#30340;&#8220;&#28151;&#28102;&#22240;&#32032;&#8221;&#21464;&#37327;&#21487;&#33021;&#20250;&#20559;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#65292;&#20174;&#32780;&#38459;&#30861;&#27169;&#22411;&#23398;&#20064;&#30456;&#20851;&#30340;&#33041;-&#34920;&#22411;&#20851;&#31995;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;DeepRepViz&#8221;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#31995;&#32479;&#22320;&#26816;&#27979;DL&#27169;&#22411;&#39044;&#27979;&#20013;&#30340;&#28151;&#28102;&#22240;&#32032;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;(1)&#24230;&#37327;&#21487;&#33021;&#28151;&#28102;&#22240;&#32032;&#30340;&#24433;&#21709;&#31243;&#24230;&#30340;&#25351;&#26631;&#21644;(2)&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#23450;&#24615;&#26816;&#26597;DL&#27169;&#22411;&#23398;&#20064;&#20869;&#23481;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#12290;&#36890;&#36807;&#22312;&#27169;&#25311;&#21644;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#20351;&#29992;DeepRepViz&#19982;DL&#27169;&#22411;&#32467;&#21512;&#30340;&#30410;&#22788;&#12290;&#20363;&#22914;&#65292;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#24615;&#21035;&#26159;DL&#27169;&#22411;&#39044;&#27979;&#20013;&#30340;&#19968;&#20010;&#26174;&#33879;&#28151;&#28102;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) models are increasingly used to analyze neuroimaging data and uncover insights about the brain, brain pathologies, and psychological traits. However, extraneous `confounders' variables such as the age of the participants, sex, or imaging artifacts can bias model predictions, preventing the models from learning relevant brain-phenotype relationships. In this study, we provide a solution called the `DeepRepViz' framework that enables researchers to systematically detect confounders in their DL model predictions. The framework consists of (1) a metric that quantifies the effect of potential confounders and (2) a visualization tool that allows researchers to qualitatively inspect what the DL model is learning. By performing experiments on simulated and neuroimaging datasets, we demonstrate the benefits of using DeepRepViz in combination with DL models. For example, experiments on the neuroimaging datasets reveal that sex is a significant confounder in a DL model predicti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38170;&#23450;&#25968;&#25454;&#38598;&#21644;&#36827;&#19968;&#27493;&#36807;&#28388;&#30340;&#25552;&#21462;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22823;&#22411;&#22270;&#20687;&#24211;&#20013;&#25552;&#21462;&#21355;&#26143;&#22270;&#20687;&#12290;&#36825;&#23548;&#33268;&#20102;&#21457;&#24067;&#20102;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#30340;&#25991;&#26412;&#21644;&#21355;&#26143;&#22270;&#20687;&#23545;&#24212;&#30340;&#25968;&#25454;&#38598;LAION-EO&#12290;</title><link>http://arxiv.org/abs/2309.15535</link><description>&lt;p&gt;
&#20174;LAION-5B&#21040;LAION-EO&#65306;&#20351;&#29992;&#38170;&#23450;&#25968;&#25454;&#38598;&#36807;&#28388;&#25968;&#21313;&#20159;&#24352;&#22270;&#29255;&#36827;&#34892;&#21355;&#26143;&#22270;&#20687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
From LAION-5B to LAION-EO: Filtering Billions of Images Using Anchor Datasets for Satellite Image Extraction. (arXiv:2309.15535v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38170;&#23450;&#25968;&#25454;&#38598;&#21644;&#36827;&#19968;&#27493;&#36807;&#28388;&#30340;&#25552;&#21462;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22823;&#22411;&#22270;&#20687;&#24211;&#20013;&#25552;&#21462;&#21355;&#26143;&#22270;&#20687;&#12290;&#36825;&#23548;&#33268;&#20102;&#21457;&#24067;&#20102;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#30340;&#25991;&#26412;&#21644;&#21355;&#26143;&#22270;&#20687;&#23545;&#24212;&#30340;&#25968;&#25454;&#38598;LAION-EO&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#22914;LAION-5B&#65292;&#21253;&#21547;&#22312;&#32447;&#20849;&#20139;&#30340;&#21508;&#31181;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#25552;&#21462;&#22823;&#22411;&#22270;&#20687;&#24211;&#30340;&#39046;&#22495;&#29305;&#23450;&#23376;&#38598;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38170;&#23450;&#25968;&#25454;&#38598;&#30340;&#25552;&#21462;&#26041;&#27861;&#65292;&#32467;&#21512;&#36827;&#19968;&#27493;&#30340;&#36807;&#28388;&#65292;&#29992;&#20110;&#21355;&#26143;&#22270;&#20687;&#39046;&#22495;&#12290;&#36825;&#23548;&#33268;&#20102;LAION-EO&#30340;&#21457;&#24067;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20174;&#32593;&#32476;&#20013;&#33719;&#21462;&#30340;&#39640;&#65288;&#36880;&#20687;&#32032;&#65289;&#20998;&#36776;&#29575;&#30340;&#25991;&#26412;&#21644;&#21355;&#26143;&#22270;&#20687;&#23545;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#37319;&#38598;&#36807;&#31243;&#20197;&#21450;&#25968;&#25454;&#38598;&#30340;&#19968;&#20123;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large datasets, such as LAION-5B, contain a diverse distribution of images shared online. However, extraction of domain-specific subsets of large image corpora is challenging. The extraction approach based on an anchor dataset, combined with further filtering, is proposed here and demonstrated for the domain of satellite imagery. This results in the release of LAION-EO, a dataset sourced from the web containing pairs of text and satellite images in high (pixel-wise) resolution. The paper outlines the acquisition procedure as well as some of the features of the dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#39044;&#27979;&#20219;&#24847;&#36755;&#20837;&#22270;&#20687;&#21518;&#39564;&#20998;&#24067;&#30340;&#20027;&#25104;&#20998;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.15533</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#21518;&#39564;&#20027;&#25104;&#20998;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification via Neural Posterior Principal Components. (arXiv:2309.15533v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#39044;&#27979;&#20219;&#24847;&#36755;&#20837;&#22270;&#20687;&#21518;&#39564;&#20998;&#24067;&#30340;&#20027;&#25104;&#20998;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;&#20110;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#29983;&#29289;&#25104;&#20687;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20013;&#37096;&#32626;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#20851;&#20110;&#19981;&#30830;&#23450;&#24615;&#21487;&#35270;&#21270;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#27599;&#20687;&#32032;&#20272;&#35745;&#19978;&#12290;&#28982;&#32780;&#65292;&#27599;&#20687;&#32032;&#26041;&#24046;&#30340;&#28909;&#22270;&#36890;&#24120;&#22312;&#23454;&#38469;&#20013;&#29992;&#36884;&#26377;&#38480;&#65292;&#22240;&#20026;&#23427;&#26080;&#27861;&#25429;&#25417;&#20687;&#32032;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#26356;&#33258;&#28982;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#23545;&#24212;&#20110;&#21518;&#39564;&#20998;&#24067;&#30340;&#20027;&#25104;&#20998;&#65288;PCs&#65289;&#19978;&#30340;&#26041;&#24046;&#12290;&#29702;&#35770;&#19978;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#36755;&#20837;&#22270;&#20687;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#24212;&#29992;PCA&#26469;&#35745;&#31639;PCs&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#22312;&#27979;&#35797;&#26102;&#29983;&#25104;&#22823;&#37327;&#30340;&#26679;&#26412;&#65292;&#32780;&#22312;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#65288;&#25193;&#25955;&#65289;&#27169;&#22411;&#19979;&#38750;&#24120;&#32531;&#24930;&#12290;&#22312;&#35813;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#39044;&#27979;&#21518;&#39564;&#20998;&#24067;&#30340;PCs&#65292;&#36866;&#29992;&#20110;&#20219;&#24847;&#36755;&#20837;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification is crucial for the deployment of image restoration models in safety-critical domains, like autonomous driving and biological imaging. To date, methods for uncertainty visualization have mainly focused on per-pixel estimates. However, a heatmap of per-pixel variances is typically of little practical use, as it does not capture the strong correlations between pixels. A more natural measure of uncertainty corresponds to the variances along the principal components (PCs) of the posterior distribution. Theoretically, the PCs can be computed by applying PCA on samples generated from a conditional generative model for the input image. However, this requires generating a very large number of samples at test time, which is painfully slow with the current state-of-the-art (diffusion) models. In this work, we present a method for predicting the PCs of the posterior distribution for any input image, in a single forward pass of a neural network. Our method can either wrap
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#24605;&#32771;&#39057;&#36947;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#38548;&#31163;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20302;&#20301;&#26435;&#37325;&#37327;&#21270;&#20013;&#30340;&#24322;&#24120;&#20540;&#12290;&#36890;&#36807;&#23558;&#26435;&#37325;&#25353;&#36755;&#20837;&#36890;&#36947;&#20869;&#36827;&#34892;&#37327;&#21270;&#20998;&#32452;&#65292;&#21487;&#20197;&#35299;&#20915;&#28608;&#27963;&#24322;&#24120;&#20540;&#30340;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#22320;&#20351;&#24471;&#20302;&#20110;4&#20301;&#30340;&#37327;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15531</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#39057;&#36947;&#32500;&#24230;&#20197;&#38548;&#31163;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20302;&#20301;&#26435;&#37325;&#37327;&#21270;&#20013;&#30340;&#24322;&#24120;&#20540;
&lt;/p&gt;
&lt;p&gt;
Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models. (arXiv:2309.15531v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15531
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#24605;&#32771;&#39057;&#36947;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#38548;&#31163;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20302;&#20301;&#26435;&#37325;&#37327;&#21270;&#20013;&#30340;&#24322;&#24120;&#20540;&#12290;&#36890;&#36807;&#23558;&#26435;&#37325;&#25353;&#36755;&#20837;&#36890;&#36947;&#20869;&#36827;&#34892;&#37327;&#21270;&#20998;&#32452;&#65292;&#21487;&#20197;&#35299;&#20915;&#28608;&#27963;&#24322;&#24120;&#20540;&#30340;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#22320;&#20351;&#24471;&#20302;&#20110;4&#20301;&#30340;&#37327;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#36817;&#26399;&#23637;&#31034;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#26377;&#25928;&#22320;&#20026;LLMs&#25552;&#20379;&#26381;&#21153;&#26041;&#38754;&#19968;&#30452;&#26159;&#20010;&#25361;&#25112;&#65292;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#20854;&#22823;&#20869;&#23384;&#29942;&#39048;&#65292;&#29305;&#21035;&#26159;&#22312;&#23567;&#25209;&#37327;&#25512;&#29702;&#35774;&#32622;&#65288;&#22914;&#31227;&#21160;&#35774;&#22791;&#65289;&#20013;&#12290;&#20165;&#23545;&#26435;&#37325;&#36827;&#34892;&#37327;&#21270;&#21487;&#33021;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#20294;&#26159;&#30001;&#20110;&#23384;&#22312;&#22823;&#24133;&#24230;&#28608;&#27963;&#24322;&#24120;&#20540;&#65292;&#20302;&#20110;4&#20301;&#30340;&#37327;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#20943;&#36731;&#19981;&#21487;&#21462;&#30340;&#24322;&#24120;&#25928;&#26524;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#27599;&#20010;&#36755;&#20837;&#36890;&#36947;&#65288;IC&#65289;&#20869;&#36827;&#34892;&#37327;&#21270;&#20998;&#32452;&#30340;per-IC&#37327;&#21270;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#27599;&#20010;&#36755;&#20986;&#36890;&#36947;&#65288;OC&#65289;&#20869;&#36827;&#34892;&#37327;&#21270;&#20998;&#32452;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#21160;&#26426;&#26159;&#35266;&#23519;&#21040;&#28608;&#27963;&#24322;&#24120;&#20540;&#24433;&#21709;&#26435;&#37325;&#30697;&#38453;&#30340;&#36755;&#20837;&#32500;&#24230;&#65292;&#22240;&#27492;&#22312;IC&#26041;&#21521;&#19978;&#23545;&#26435;&#37325;&#36827;&#34892;&#31867;&#20284;&#20998;&#32452;&#21487;&#20197;&#23558;&#24322;&#24120;&#20540;&#38548;&#31163;&#21040;&#19968;&#20010;&#20998;&#32452;&#20869;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#28608;&#27963;&#30340;&#24322;&#24120;&#20540;&#24182;&#19981;&#20915;&#23450;&#37327;&#21270;&#30340;&#38590;&#24230;&#65292;&#20854;&#22266;&#26377;&#30340;&#26435;&#37325;&#25935;&#24863;&#24615;&#20063;&#23384;&#22312;&#12290;&#36890;&#36807;per-IC&#37327;&#21270;&#20316;&#20026;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20302;&#20301;&#26435;&#37325;&#37327;&#21270;&#20013;&#30340;&#24322;&#24120;&#20540;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently demonstrated a remarkable success across various tasks. However, efficiently serving LLMs has been a challenge due to its large memory bottleneck, specifically in small batch inference settings (e.g. mobile devices). Weight-only quantization can be a promising approach, but sub-4 bit quantization remains a challenge due to large-magnitude activation outliers. To mitigate the undesirable outlier effect, we first propose per-IC quantization, a simple yet effective method that creates quantization groups within each input channel (IC) rather than the conventional per-output channel (OC). Our method is motivated by the observation that activation outliers affect the input dimension of the weight matrix, so similarly grouping the weights in the IC direction can isolate outliers to be within a group. We also find that activation outliers do not dictate quantization difficulty, and inherent weight sensitivities also exist. With per-IC quantization as
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#21307;&#30103;&#25968;&#25454;&#20855;&#26377;&#40065;&#26834;&#24615;&#19988;&#33021;&#36827;&#19968;&#27493;&#25552;&#39640;&#30142;&#30149;&#35786;&#26029;&#24615;&#33021;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26550;&#26500;&#12290;&#36890;&#36807;&#34701;&#21512;X&#23556;&#32447;&#29031;&#29255;&#12289;&#25918;&#23556;&#23398;&#25253;&#21578;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20351;&#29992;Transformer&#27169;&#22359;&#36827;&#34892;&#34701;&#21512;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#22810;&#21464;&#37327;&#25439;&#22833;&#20989;&#25968;&#25552;&#39640;&#27169;&#22411;&#22312;&#32570;&#22833;&#27169;&#24577;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15529</link><description>&lt;p&gt;
&#21307;&#30103;&#25968;&#25454;&#30340;&#32570;&#22833;&#27169;&#24577;&#21551;&#29992;&#22810;&#27169;&#24577;&#34701;&#21512;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Missing-modality Enabled Multi-modal Fusion Architecture for Medical Data. (arXiv:2309.15529v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15529
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#21307;&#30103;&#25968;&#25454;&#20855;&#26377;&#40065;&#26834;&#24615;&#19988;&#33021;&#36827;&#19968;&#27493;&#25552;&#39640;&#30142;&#30149;&#35786;&#26029;&#24615;&#33021;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26550;&#26500;&#12290;&#36890;&#36807;&#34701;&#21512;X&#23556;&#32447;&#29031;&#29255;&#12289;&#25918;&#23556;&#23398;&#25253;&#21578;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20351;&#29992;Transformer&#27169;&#22359;&#36827;&#34892;&#34701;&#21512;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#22810;&#21464;&#37327;&#25439;&#22833;&#20989;&#25968;&#25552;&#39640;&#27169;&#22411;&#22312;&#32570;&#22833;&#27169;&#24577;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24739;&#32773;&#20855;&#20307;&#24773;&#20917;&#65292;&#21307;&#30103;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#27169;&#24577;&#26159;&#24120;&#35265;&#30340;&#65292;&#36825;&#23545;&#24212;&#29992;&#20013;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#23475;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#32570;&#22833;&#27169;&#24577;&#23545;&#27169;&#22411;&#36827;&#34892;&#36866;&#24212;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#23545;&#32570;&#22833;&#27169;&#24577;&#20855;&#26377;&#40065;&#26834;&#24615;&#19988;&#33021;&#36827;&#19968;&#27493;&#25913;&#21892;&#30142;&#30149;&#35786;&#26029;&#24615;&#33021;&#30340;&#21307;&#30103;&#25968;&#25454;&#30340;&#39640;&#25928;&#22810;&#27169;&#24577;&#34701;&#21512;&#26550;&#26500;&#12290;&#26412;&#30740;&#31350;&#34701;&#21512;&#20102;X&#23556;&#32447;&#33016;&#37096;&#25918;&#23556;&#29031;&#29255;&#20316;&#20026;&#22270;&#20687;&#27169;&#24577;&#65292;&#25918;&#23556;&#23398;&#25253;&#21578;&#20316;&#20026;&#25991;&#26412;&#27169;&#24577;&#65292;&#20197;&#21450;&#32467;&#26500;&#21270;&#20540;&#25968;&#25454;&#20316;&#20026;&#34920;&#26684;&#25968;&#25454;&#27169;&#24577;&#12290;&#27599;&#20010;&#27169;&#24577;&#23545;&#37117;&#36890;&#36807;&#22522;&#20110;Transformer&#30340;&#21452;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#36827;&#34892;&#34701;&#21512;&#65292;&#28982;&#21518;&#23558;&#36825;&#19977;&#20010;&#21452;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#32452;&#21512;&#25104;&#19977;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#22810;&#21464;&#37327;&#25439;&#22833;&#20989;&#25968;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fusing multi-modal data can improve the performance of deep learning models. However, missing modalities are common for medical data due to patients' specificity, which is detrimental to the performance of multi-modal models in applications. Therefore, it is critical to adapt the models to missing modalities. This study aimed to develop an efficient multi-modal fusion architecture for medical data that was robust to missing modalities and further improved the performance on disease diagnosis.X-ray chest radiographs for the image modality, radiology reports for the text modality, and structured value data for the tabular data modality were fused in this study. Each modality pair was fused with a Transformer-based bi-modal fusion module, and the three bi-modal fusion modules were then combined into a tri-modal fusion framework. Additionally, multivariate loss functions were introduced into the training process to improve model's robustness to missing modalities in the inference process. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#20316;&#32773;&#21033;&#29992;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#36716;&#31227;&#23398;&#20064;&#30740;&#31350;&#30340;&#25104;&#26524;&#65292;&#20027;&#35201;&#35752;&#35770;&#20102;&#25345;&#32493;&#23398;&#20064;&#21644;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#21487;&#29992;&#24615;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#20026;&#26410;&#26469;&#39046;&#22495;&#30340;&#36827;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2309.15522</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#20869;&#37096;&#34920;&#31034;&#22312;&#39046;&#22495;&#27867;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Robust Internal Representations for Domain Generalization. (arXiv:2309.15522v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#20316;&#32773;&#21033;&#29992;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#36716;&#31227;&#23398;&#20064;&#30740;&#31350;&#30340;&#25104;&#26524;&#65292;&#20027;&#35201;&#35752;&#35770;&#20102;&#25345;&#32493;&#23398;&#20064;&#21644;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#21487;&#29992;&#24615;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#20026;&#26410;&#26469;&#39046;&#22495;&#30340;&#36827;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#25105;&#22312;&#36716;&#31227;&#23398;&#20064;&#20013;&#21033;&#29992;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#30740;&#31350;&#30340;&#32508;&#21512;&#35843;&#26597;&#12290;&#26412;&#25991;&#20027;&#35201;&#35752;&#35770;&#20102;&#25345;&#32493;&#23398;&#20064;&#21644;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#21487;&#29992;&#24615;&#25152;&#24102;&#26469;&#30340;&#22266;&#26377;&#25361;&#25112;&#12290;&#36890;&#36807;&#24635;&#32467;&#25105;&#36807;&#21435;&#21644;&#27491;&#22312;&#36827;&#34892;&#20013;&#30340;&#36129;&#29486;&#65292;&#26412;&#25991;&#26088;&#22312;&#21576;&#29616;&#25105;&#30740;&#31350;&#30340;&#25972;&#20307;&#29702;&#35299;&#65292;&#20026;&#26410;&#26469;&#39046;&#22495;&#30340;&#25506;&#32034;&#21644;&#36827;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;&#25105;&#30340;&#30740;&#31350;&#28041;&#21450;&#20102;&#36716;&#31227;&#23398;&#20064;&#30340;&#21508;&#31181;&#35774;&#32622;&#65292;&#21253;&#25324;&#23569;&#26679;&#26412;&#23398;&#20064;&#12289;&#38646;&#26679;&#26412;&#23398;&#20064;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#20998;&#24067;&#24335;&#23398;&#20064;&#12290;&#25105;&#24076;&#26395;&#36825;&#20010;&#35843;&#26597;&#25552;&#20379;&#32473;&#37027;&#20123;&#24076;&#26395;&#19987;&#27880;&#20110;&#31867;&#20284;&#30740;&#31350;&#26041;&#21521;&#30340;&#30740;&#31350;&#32773;&#19968;&#20010;&#21069;&#30651;&#24615;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper which is part of the New Faculty Highlights Invited Speaker Program of AAAI'23, serves as a comprehensive survey of my research in transfer learning by utilizing embedding spaces. The work reviewed in this paper specifically revolves around the inherent challenges associated with continual learning and limited availability of labeled data. By providing an overview of my past and ongoing contributions, this paper aims to present a holistic understanding of my research, paving the way for future explorations and advancements in the field. My research delves into the various settings of transfer learning, including, few-shot learning, zero-shot learning, continual learning, domain adaptation, and distributed learning. I hope this survey provides a forward-looking perspective for researchers who would like to focus on similar research directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#31232;&#32570;&#25968;&#25454;&#20998;&#26512;&#20013;&#23436;&#20840;&#24212;&#29992;MLOps&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25972;&#20307;&#26041;&#27861;&#26469;&#22686;&#24378;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.15521</link><description>&lt;p&gt;
&#31232;&#32570;&#22270;&#20687;&#25968;&#25454;&#30340;MLOps&#65306;&#26174;&#24494;&#38236;&#22270;&#20687;&#20998;&#26512;&#30340;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
MLOps for Scarce Image Data: A Use Case in Microscopic Image Analysis. (arXiv:2309.15521v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#31232;&#32570;&#25968;&#25454;&#20998;&#26512;&#20013;&#23436;&#20840;&#24212;&#29992;MLOps&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25972;&#20307;&#26041;&#27861;&#26469;&#22686;&#24378;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27491;&#22312;&#32463;&#21382;&#21069;&#25152;&#26410;&#26377;&#30340;&#27969;&#34892;&#12290;ML&#27169;&#22411;&#30340;&#25805;&#20316;&#21270;&#30001;&#19968;&#32452;&#34987;&#31216;&#20026;&#26426;&#22120;&#23398;&#20064;&#25805;&#20316;&#65288;MLOps&#65289;&#30340;&#27010;&#24565;&#21644;&#26041;&#27861;&#25152;&#25351;&#23548;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#19987;&#19994;&#20154;&#21592;&#24448;&#24448;&#26356;&#22810;&#22320;&#20851;&#27880;&#33258;&#21160;&#21270;&#26041;&#38754;&#65292;&#24573;&#35270;MLOps&#30340;&#25345;&#32493;&#37096;&#32626;&#21644;&#30417;&#25511;&#26041;&#38754;&#12290;&#32467;&#26524;&#65292;&#30001;&#20110;&#27010;&#24565;&#28418;&#31227;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#31232;&#32570;&#25968;&#25454;&#26102;&#65292;&#20174;&#29983;&#20135;&#21040;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#21453;&#39304;&#32570;&#20047;&#36830;&#32493;&#23398;&#20064;&#65292;&#23548;&#33268;&#27169;&#22411;&#20250;&#38543;&#26102;&#38388;&#19981;&#26029;&#24694;&#21270;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#31232;&#32570;&#25968;&#25454;&#20998;&#26512;&#29615;&#22659;&#20013;&#23436;&#20840;&#24212;&#29992;MLOps&#30340;&#24773;&#20917;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25972;&#20307;&#26041;&#27861;&#26469;&#22686;&#24378;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#65306;&#25351;&#32441;&#21270;&#36807;&#31243;&#65292;&#20351;&#24471;&#26681;&#25454;&#25163;&#22836;&#30340;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#24320;&#21457;&#31574;&#30053;&#65307;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, Machine Learning (ML) is experiencing tremendous popularity that has never been seen before. The operationalization of ML models is governed by a set of concepts and methods referred to as Machine Learning Operations (MLOps). Nevertheless, researchers, as well as professionals, often focus more on the automation aspect and neglect the continuous deployment and monitoring aspects of MLOps. As a result, there is a lack of continuous learning through the flow of feedback from production to development, causing unexpected model deterioration over time due to concept drifts, particularly when dealing with scarce data. This work explores the complete application of MLOps in the context of scarce data analysis. The paper proposes a new holistic approach to enhance biomedical image analysis. Our method includes: a fingerprinting process that enables selecting the best models, datasets, and model development strategy relative to the image analysis task at hand; an automated model deve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#32593;&#32476;&#65288;SAF-Net&#65289;&#26469;&#26816;&#27979;&#22810;&#35270;&#35282;&#36229;&#22768;&#24515;&#21160;&#22270;&#20013;&#30340;&#24515;&#32908;&#26775;&#27515;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25552;&#21462;&#29305;&#24449;&#12289;&#23398;&#20064;&#29305;&#24449;&#30340;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#20351;&#29992;&#23494;&#38598;&#23618;&#36827;&#34892;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#23545;MI&#30340;&#26377;&#25928;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.15520</link><description>&lt;p&gt;
SAF-Net: &#20351;&#29992;&#22810;&#35270;&#35282;&#36229;&#22768;&#24515;&#21160;&#22270;&#36827;&#34892;&#24515;&#32908;&#26775;&#27515;&#26816;&#27979;&#30340;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SAF-Net: Self-Attention Fusion Network for Myocardial Infarction Detection using Multi-View Echocardiography. (arXiv:2309.15520v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#32593;&#32476;&#65288;SAF-Net&#65289;&#26469;&#26816;&#27979;&#22810;&#35270;&#35282;&#36229;&#22768;&#24515;&#21160;&#22270;&#20013;&#30340;&#24515;&#32908;&#26775;&#27515;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25552;&#21462;&#29305;&#24449;&#12289;&#23398;&#20064;&#29305;&#24449;&#30340;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#20351;&#29992;&#23494;&#38598;&#23618;&#36827;&#34892;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#23545;MI&#30340;&#26377;&#25928;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#32908;&#26775;&#27515;&#65288;MI&#65289;&#26159;&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#65288;CAD&#65289;&#30340;&#20005;&#37325;&#24773;&#20917;&#65292;&#20854;&#26816;&#27979;&#23545;&#20110;&#38450;&#27490;&#24515;&#32908;&#32452;&#32455;&#30340;&#36827;&#19968;&#27493;&#25439;&#20260;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#22270;&#34701;&#21512;&#27169;&#22411;&#65292;&#21517;&#20026;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#32593;&#32476;&#65288;SAF-Net&#65289;&#65292;&#29992;&#20110;&#20174;&#22810;&#35270;&#35282;&#36229;&#22768;&#24515;&#21160;&#22270;&#35760;&#24405;&#20013;&#26816;&#27979;MI&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21033;&#29992;&#39030;&#37096;&#20108;&#33108;&#24515;&#23460;&#65288;A2C&#65289;&#21644;&#39030;&#37096;&#22235;&#33108;&#24515;&#23460;&#65288;A4C&#65289;&#35270;&#22270;&#36229;&#22768;&#24515;&#21160;&#22270;&#35760;&#24405;&#36827;&#34892;&#20998;&#31867;&#12290;&#20174;&#20004;&#20010;&#35270;&#22270;&#30340;&#27599;&#26465;&#35760;&#24405;&#20013;&#25552;&#21462;&#19977;&#20010;&#21442;&#32771;&#24103;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#32593;&#32476;&#25552;&#21462;&#39640;&#24230;&#20195;&#34920;&#24615;&#30340;&#29305;&#24449;&#12290;SAF-Net&#27169;&#22411;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23398;&#20064;&#25552;&#21462;&#29305;&#24449;&#21521;&#37327;&#20013;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#30001;&#19977;&#20010;&#20027;&#35201;&#37096;&#20998;&#32452;&#25104;&#65292;&#20855;&#26377;&#32039;&#20945;&#30340;&#20307;&#31995;&#32467;&#26500;&#65306;&#29305;&#24449;&#23884;&#20837;&#20197;&#20943;&#23569;&#32500;&#24230;&#65292;&#33258;&#27880;&#24847;&#21147;&#36827;&#34892;&#35270;&#22270;&#27744;&#21270;&#65292;&#23494;&#38598;&#23618;&#36827;&#34892;&#20998;&#31867;&#12290;&#23454;&#39564;&#35780;&#20272;&#20351;&#29992;HMC&#25968;&#25454;&#38598;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Myocardial infarction (MI) is a severe case of coronary artery disease (CAD) and ultimately, its detection is substantial to prevent progressive damage to the myocardium. In this study, we propose a novel view-fusion model named self-attention fusion network (SAF-Net) to detect MI from multi-view echocardiography recordings. The proposed framework utilizes apical 2-chamber (A2C) and apical 4-chamber (A4C) view echocardiography recordings for classification. Three reference frames are extracted from each recording of both views and deployed pre-trained deep networks to extract highly representative features. The SAF-Net model utilizes a self-attention mechanism to learn dependencies in extracted feature vectors. The proposed model is computationally efficient thanks to its compact architecture having three main parts: a feature embedding to reduce dimensionality, self-attention for view-pooling, and dense layers for the classification. Experimental evaluation is performed using the HMC-
&lt;/p&gt;</description></item><item><title>GNN4EEG&#26159;&#19968;&#20010;&#29992;&#20110;&#33041;&#30005;&#22270;&#20998;&#31867;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#20934;&#21644;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#24314;&#27169;&#33041;&#30005;&#22270;&#36890;&#36947;&#36873;&#23450;&#30340;&#29305;&#24449;&#24182;&#21033;&#29992;&#25299;&#25169;&#20449;&#24687;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.15515</link><description>&lt;p&gt;
GNN4EEG&#65306;&#19968;&#31181;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#33041;&#30005;&#22270;&#20998;&#31867;&#30340;&#22522;&#20934;&#21644;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
GNN4EEG: A Benchmark and Toolkit for Electroencephalography Classification with Graph Neural Network. (arXiv:2309.15515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15515
&lt;/p&gt;
&lt;p&gt;
GNN4EEG&#26159;&#19968;&#20010;&#29992;&#20110;&#33041;&#30005;&#22270;&#20998;&#31867;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#20934;&#21644;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#24314;&#27169;&#33041;&#30005;&#22270;&#36890;&#36947;&#36873;&#23450;&#30340;&#29305;&#24449;&#24182;&#21033;&#29992;&#25299;&#25169;&#20449;&#24687;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20998;&#31867;&#26159;&#31070;&#32463;&#31185;&#23398;&#12289;&#31070;&#32463;&#24037;&#31243;&#21644;&#20960;&#20010;&#21830;&#19994;&#24212;&#29992;&#20013;&#20851;&#38190;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;EEG&#20998;&#31867;&#27169;&#22411;&#24120;&#24120;&#24573;&#35270;&#25110;&#19981;&#20805;&#20998;&#21033;&#29992;&#22823;&#33041;&#30340;&#25299;&#25169;&#20449;&#24687;&#12290;&#37492;&#20110;&#27492;&#65292;&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#24314;&#27169;&#30001;&#27599;&#20010;EEG&#36890;&#36947;&#36873;&#25321;&#30340;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#25299;&#25169;&#20449;&#24687;&#30340;&#28508;&#21147;&#24050;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20852;&#36259;&#12290;&#20026;&#36827;&#19968;&#27493;&#20419;&#36827;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GNN4EEG&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22522;&#20110;GNN&#23545;EEG&#20449;&#21495;&#24314;&#27169;&#30340;&#36890;&#29992;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#24037;&#20855;&#21253;&#12290;GNN4EEG&#21253;&#25324;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;i&#65289;&#19968;&#20010;&#22823;&#22411;&#22522;&#20934;&#65292;&#26681;&#25454;&#26469;&#33258;123&#21517;&#21442;&#19982;&#32773;&#30340;EEG&#25968;&#25454;&#26500;&#24314;&#20102;&#22235;&#20010;EEG&#20998;&#31867;&#20219;&#21153;&#12290;&#65288;ii&#65289;&#26131;&#20110;&#20351;&#29992;&#30340;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;GNN&#30340;EEG&#20998;&#31867;&#27169;&#22411;&#30340;&#23454;&#29616;&#65292;&#20363;&#22914;DGCNN&#12289;RGNN&#31561;&#12290;&#65288;iii&#65289;&#20840;&#38754;&#30340;&#23454;&#39564;&#35774;&#32622;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electroencephalography(EEG) classification is a crucial task in neuroscience, neural engineering, and several commercial applications. Traditional EEG classification models, however, have often overlooked or inadequately leveraged the brain's topological information. Recognizing this shortfall, there has been a burgeoning interest in recent years in harnessing the potential of Graph Neural Networks (GNN) to exploit the topological information by modeling features selected from each EEG channel in a graph structure. To further facilitate research in this direction, we introduce GNN4EEG, a versatile and user-friendly toolkit for GNN-based modeling of EEG signals. GNN4EEG comprises three components: (i)A large benchmark constructed with four EEG classification tasks based on EEG data collected from 123 participants. (ii)Easy-to-use implementations on various state-of-the-art GNN-based EEG classification models, e.g., DGCNN, RGNN, etc. (iii)Implementations of comprehensive experimental set
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#26377;&#38480;&#26631;&#37327;&#37327;&#21270; (FSQ) &#26041;&#27861;&#65292;&#29992;&#26469;&#31616;&#21270; VQ-VAE &#26041;&#27861;&#20013;&#30340;&#21521;&#37327;&#37327;&#21270; (VQ)&#12290;&#36890;&#36807;&#25237;&#24433;&#21644;&#37327;&#21270; VAE &#34920;&#31034;&#65292;&#25105;&#20204;&#24471;&#21040;&#19982; VQ &#30456;&#21516;&#22823;&#23567;&#30340;&#30721;&#26412;&#12290;&#22312;&#36825;&#31181;&#31163;&#25955;&#34920;&#31034;&#19978;&#65292;&#25105;&#20204;&#21487;&#20197;&#35757;&#32451;&#30456;&#21516;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#22270;&#20687;&#29983;&#25104;&#12289;&#22810;&#27169;&#24577;&#29983;&#25104;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15505</link><description>&lt;p&gt;
&#26377;&#38480;&#26631;&#37327;&#37327;&#21270;: &#31616;&#21270; VQ-VAE &#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Finite Scalar Quantization: VQ-VAE Made Simple. (arXiv:2309.15505v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15505
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#26377;&#38480;&#26631;&#37327;&#37327;&#21270; (FSQ) &#26041;&#27861;&#65292;&#29992;&#26469;&#31616;&#21270; VQ-VAE &#26041;&#27861;&#20013;&#30340;&#21521;&#37327;&#37327;&#21270; (VQ)&#12290;&#36890;&#36807;&#25237;&#24433;&#21644;&#37327;&#21270; VAE &#34920;&#31034;&#65292;&#25105;&#20204;&#24471;&#21040;&#19982; VQ &#30456;&#21516;&#22823;&#23567;&#30340;&#30721;&#26412;&#12290;&#22312;&#36825;&#31181;&#31163;&#25955;&#34920;&#31034;&#19978;&#65292;&#25105;&#20204;&#21487;&#20197;&#35757;&#32451;&#30456;&#21516;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#22270;&#20687;&#29983;&#25104;&#12289;&#22810;&#27169;&#24577;&#29983;&#25104;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#29992;&#26377;&#38480;&#26631;&#37327;&#37327;&#21270; (FSQ) &#26367;&#20195; VQ-VAE &#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#21521;&#37327;&#37327;&#21270; (VQ)&#12290;&#22312; FSQ &#20013;&#65292;&#25105;&#20204;&#23558; VAE &#34920;&#31034;&#25237;&#24433;&#21040;&#20960;&#20010;&#32500;&#24230; (&#36890;&#24120;&#23569;&#20110;10&#20010;)&#65292;&#27599;&#20010;&#32500;&#24230;&#34987;&#37327;&#21270;&#20026;&#19968;&#32452;&#22266;&#23450;&#30340;&#20540;&#65292;&#20174;&#32780;&#24418;&#25104;&#19968;&#20010;&#65288;&#38544;&#24335;&#30340;&#65289;&#30721;&#26412;&#65292;&#30001;&#36825;&#20123;&#20540;&#30340;&#20056;&#31215;&#32452;&#25104;&#12290;&#36890;&#36807;&#21512;&#36866;&#22320;&#36873;&#25321;&#32500;&#24230;&#21644;&#27599;&#20010;&#32500;&#24230;&#21487;&#20197;&#21462;&#30340;&#20540;&#30340;&#25968;&#37327;&#65292;&#25105;&#20204;&#33719;&#24471;&#19982; VQ &#20013;&#30456;&#21516;&#30340;&#30721;&#26412;&#22823;&#23567;&#12290;&#22312;&#36825;&#26679;&#30340;&#31163;&#25955;&#34920;&#31034;&#19978;&#65292;&#25105;&#20204;&#21487;&#20197;&#35757;&#32451;&#24050;&#32463;&#22312; VQ-VAE &#34920;&#31034;&#19978;&#35757;&#32451;&#36807;&#30340;&#30456;&#21516;&#27169;&#22411;&#65292;&#20363;&#22914;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#12289;&#22810;&#27169;&#24577;&#29983;&#25104;&#21644;&#23494;&#38598;&#39044;&#27979;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#33258;&#22238;&#24402;&#21644;&#25513;&#30721;&#21464;&#25442;&#22120;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#22270;&#20687;&#29983;&#25104;&#20013;&#20351;&#29992; FSQ &#21644; MaskGIT&#65292;&#22312;&#28145;&#24230;&#20272;&#35745;&#12289;&#30528;&#33394;&#21644;&#20840;&#26223;&#20998;&#21106;&#20013;&#20351;&#29992; FSQ &#21644; UViM&#12290;&#23613;&#31649; FSQ &#30340;&#35774;&#35745;&#35201;&#31616;&#21333;&#24471;&#22810;&#65292;&#25105;&#20204;&#22312;&#25152;&#26377;&#36825;&#20123;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to replace vector quantization (VQ) in the latent representation of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where we project the VAE representation down to a few dimensions (typically less than 10). Each dimension is quantized to a small set of fixed values, leading to an (implicit) codebook given by the product of these sets. By appropriately choosing the number of dimensions and values each dimension can take, we obtain the same codebook size as in VQ. On top of such discrete representations, we can train the same models that have been trained on VQ-VAE representations. For example, autoregressive and masked transformer models for image generation, multimodal generation, and dense prediction computer vision tasks. Concretely, we employ FSQ with MaskGIT for image generation, and with UViM for depth estimation, colorization, and panoptic segmentation. Despite the much simpler design of FSQ, we obtain competitive performance in all these tasks. W
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;BPFL&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#23458;&#25143;&#31471;&#19981;&#30830;&#23450;&#24615;&#21644;&#24322;&#36136;&#24615;&#12290;&#36825;&#20010;&#26694;&#26550;&#36890;&#36807;&#20998;&#35299;&#21644;&#20849;&#21516;&#23398;&#20064;&#32479;&#35745;&#24322;&#36136;&#24615;&#23458;&#25143;&#31471;&#25968;&#25454;&#19978;&#30340;&#20849;&#20139;&#21644;&#20010;&#24615;&#21270;&#19981;&#30830;&#23450;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.15499</link><description>&lt;p&gt;
&#20855;&#26377;&#20849;&#20139;&#21644;&#20010;&#24615;&#21270;&#19981;&#30830;&#23450;&#34920;&#31034;&#30340;&#36125;&#21494;&#26031;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bayesian Personalized Federated Learning with Shared and Personalized Uncertainty Representations. (arXiv:2309.15499v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;BPFL&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#23458;&#25143;&#31471;&#19981;&#30830;&#23450;&#24615;&#21644;&#24322;&#36136;&#24615;&#12290;&#36825;&#20010;&#26694;&#26550;&#36890;&#36807;&#20998;&#35299;&#21644;&#20849;&#21516;&#23398;&#20064;&#32479;&#35745;&#24322;&#36136;&#24615;&#23458;&#25143;&#31471;&#25968;&#25454;&#19978;&#30340;&#20849;&#20139;&#21644;&#20010;&#24615;&#21270;&#19981;&#30830;&#23450;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;BPFL&#65289;&#35299;&#20915;&#20102;&#29616;&#26377;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;BPFL&#26088;&#22312;&#36890;&#36807;&#22788;&#29702;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#26469;&#37327;&#21270;&#23458;&#25143;&#31471;&#20869;&#37096;&#21644;&#36328;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#24322;&#36136;&#24615;&#12290;&#22312;PFL&#20013;&#65292;&#26368;&#36817;&#19968;&#20123;&#21021;&#27493;&#24037;&#20316;&#25552;&#20986;&#23558;&#38544;&#34255;&#30340;&#31070;&#32463;&#34920;&#31034;&#20998;&#35299;&#20026;&#20849;&#20139;&#21644;&#23616;&#37096;&#32452;&#20214;&#65292;&#24182;&#23637;&#31034;&#20102;&#26377;&#36259;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24037;&#20316;&#27809;&#26377;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#23458;&#25143;&#31471;&#19981;&#30830;&#23450;&#24615;&#21644;&#24322;&#36136;&#24615;&#65292;&#21516;&#26102;&#36866;&#24403;&#22320;&#35299;&#32806;&#31070;&#32463;&#34920;&#31034;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#24120;&#24120;&#26159;&#20020;&#26102;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#24341;&#20837;&#19968;&#20010;&#36890;&#29992;&#30340;BPFL&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#35299;&#21644;&#20849;&#21516;&#23398;&#20064;&#32479;&#35745;&#24322;&#36136;&#24615;&#23458;&#25143;&#31471;&#25968;&#25454;&#19978;&#30340;&#20849;&#20139;&#21644;&#20010;&#24615;&#21270;&#19981;&#30830;&#23450;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian personalized federated learning (BPFL) addresses challenges in existing personalized FL (PFL). BPFL aims to quantify the uncertainty and heterogeneity within and across clients towards uncertainty representations by addressing the statistical heterogeneity of client data. In PFL, some recent preliminary work proposes to decompose hidden neural representations into shared and local components and demonstrates interesting results. However, most of them do not address client uncertainty and heterogeneity in FL systems, while appropriately decoupling neural representations is challenging and often ad hoc. In this paper, we make the first attempt to introduce a general BPFL framework to decompose and jointly learn shared and personalized uncertainty representations on statistically heterogeneous client data over time. A Bayesian federated neural network BPFed instantiates BPFL by jointly learning cross-client shared uncertainty and client-specific personalized uncertainty over stat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FastLSH&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#26041;&#26696;&#65292;&#36890;&#36807;&#32467;&#21512;&#38543;&#26426;&#37319;&#26679;&#21644;&#38543;&#26426;&#25237;&#24433;&#65292;&#23558;&#31639;&#27861;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#22823;&#24133;&#38477;&#20302;&#65292;&#24182;&#19988;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15479</link><description>&lt;p&gt;
&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#24555;&#36895;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;
&lt;/p&gt;
&lt;p&gt;
Fast Locality Sensitive Hashing with Theoretical Guarantee. (arXiv:2309.15479v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FastLSH&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#26041;&#26696;&#65292;&#36890;&#36807;&#32467;&#21512;&#38543;&#26426;&#37319;&#26679;&#21644;&#38543;&#26426;&#25237;&#24433;&#65292;&#23558;&#31639;&#27861;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#22823;&#24133;&#38477;&#20302;&#65292;&#24182;&#19988;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#65288;LSH&#65289;&#26159;&#19968;&#31181;&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#26377;&#25928;&#30340;&#38543;&#26426;&#21270;&#25216;&#26415;&#12290;&#21704;&#24076;&#30340;&#25104;&#26412;&#19982;&#25968;&#25454;&#32500;&#24230;&#25104;&#27491;&#27604;&#65292;&#22240;&#27492;&#24403;&#32500;&#24230;&#24456;&#39640;&#19988;&#28041;&#21450;&#22823;&#37327;&#21704;&#24076;&#20989;&#25968;&#26102;&#65292;&#21704;&#24076;&#24448;&#24448;&#25104;&#20026;&#24615;&#33021;&#29942;&#39048;&#12290;&#28982;&#32780;&#65292;&#20986;&#20154;&#24847;&#26009;&#30340;&#26159;&#65292;&#24456;&#23569;&#26377;&#24037;&#20316;&#22312;&#25913;&#36827;LSH&#35745;&#31639;&#30340;&#25928;&#29575;&#26041;&#38754;&#20570;&#20986;&#21162;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;LSH&#26041;&#26696;&#65292;&#21517;&#20026;FastLSH&#65292;&#20351;&#29992;l2&#33539;&#25968;&#12290;&#36890;&#36807;&#32467;&#21512;&#38543;&#26426;&#37319;&#26679;&#21644;&#38543;&#26426;&#25237;&#24433;&#65292;FastLSH&#23558;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;O&#65288;n&#65289;&#38477;&#20302;&#21040;O&#65288;m&#65289;&#65288;m &lt; n&#65289;&#65292;&#20854;&#20013;n&#26159;&#25968;&#25454;&#32500;&#24230;&#65292;m&#26159;&#25277;&#26679;&#32500;&#24230;&#30340;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;FastLSH&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;LSH&#23646;&#24615;&#65292;&#20351;&#20854;&#19982;&#38750;LSH&#24555;&#36895;&#33609;&#22270;&#26377;&#25152;&#21306;&#21035;&#12290;&#25105;&#20204;&#23545;&#19968;&#32452;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#29992;&#20110;&#26368;&#36817;&#37051;&#25628;&#32034;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FastLSH&#22312;&#31572;&#26696;&#36136;&#37327;&#26041;&#38754;&#19982;&#29616;&#26377;&#25216;&#26415;&#22788;&#20110;&#21516;&#19968;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Locality-sensitive hashing (LSH) is an effective randomized technique widely used in many machine learning tasks. The cost of hashing is proportional to data dimensions, and thus often the performance bottleneck when dimensionality is high and the number of hash functions involved is large. Surprisingly, however, little work has been done to improve the efficiency of LSH computation. In this paper, we design a simple yet efficient LSH scheme, named FastLSH, under l2 norm. By combining random sampling and random projection, FastLSH reduces the time complexity from O(n) to O(m) (m&lt;n), where n is the data dimensionality and m is the number of sampled dimensions. Moreover, FastLSH has provable LSH property, which distinguishes it from the non-LSH fast sketches. We conduct comprehensive experiments over a collection of real and synthetic datasets for the nearest neighbor search task. Experimental results demonstrate that FastLSH is on par with the state-of-the-arts in terms of answer qualit
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#24635;&#32467;&#20102;MUAD&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25361;&#25112;&#30340;&#33719;&#32988;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;19&#20010;&#25552;&#20132;&#20316;&#21697;&#30340;&#32467;&#26524;&#12290;&#25361;&#25112;&#20027;&#35201;&#38598;&#20013;&#22312;&#22478;&#24066;&#29615;&#22659;&#30340;&#35821;&#20041;&#20998;&#21106;&#65292;&#29305;&#21035;&#20851;&#27880;&#33258;&#28982;&#23545;&#25239;&#22330;&#26223;&#12290;&#36825;&#20010;&#35770;&#25991;&#25552;&#20379;&#20102;&#34920;&#29616;&#26368;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20840;&#38754;&#27010;&#36848;&#20102;&#25152;&#26377;&#21442;&#19982;&#32773;&#37096;&#32626;&#30340;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.15478</link><description>&lt;p&gt;
Robust Semantic Segmentation UNCV2023&#25361;&#25112;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
The Robust Semantic Segmentation UNCV2023 Challenge Results. (arXiv:2309.15478v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15478
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#24635;&#32467;&#20102;MUAD&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25361;&#25112;&#30340;&#33719;&#32988;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;19&#20010;&#25552;&#20132;&#20316;&#21697;&#30340;&#32467;&#26524;&#12290;&#25361;&#25112;&#20027;&#35201;&#38598;&#20013;&#22312;&#22478;&#24066;&#29615;&#22659;&#30340;&#35821;&#20041;&#20998;&#21106;&#65292;&#29305;&#21035;&#20851;&#27880;&#33258;&#28982;&#23545;&#25239;&#22330;&#26223;&#12290;&#36825;&#20010;&#35770;&#25991;&#25552;&#20379;&#20102;&#34920;&#29616;&#26368;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20840;&#38754;&#27010;&#36848;&#20102;&#25152;&#26377;&#21442;&#19982;&#32773;&#37096;&#32626;&#30340;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;&#22312;ICCV 2023&#20030;&#21150;&#30340;MUAD&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25361;&#25112;&#20013;&#37319;&#29992;&#30340;&#33719;&#32988;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#25361;&#25112;&#22260;&#32469;&#22478;&#24066;&#29615;&#22659;&#20013;&#30340;&#35821;&#20041;&#20998;&#21106;&#23637;&#24320;&#65292;&#29305;&#21035;&#20851;&#27880;&#33258;&#28982;&#23545;&#25239;&#22330;&#26223;&#12290;&#25253;&#21578;&#21576;&#29616;&#20102;19&#20010;&#25552;&#20132;&#20316;&#21697;&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;&#35768;&#22810;&#25216;&#26415;&#20511;&#37492;&#20102;&#36807;&#21435;&#20960;&#24180;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#37325;&#35201;&#20250;&#35758;&#21644;&#26399;&#21002;&#19978;&#20171;&#32461;&#30340;&#21069;&#27839;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#36825;&#20010;&#25361;&#25112;&#65292;&#38416;&#26126;&#20102;&#20854;&#30446;&#30340;&#21644;&#30446;&#26631;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#25913;&#21892;&#22312;&#19981;&#21516;&#33258;&#28982;&#23545;&#25239;&#26465;&#20214;&#19979;&#22478;&#24066;&#22330;&#26223;&#20013;&#30340;&#35821;&#20041;&#20998;&#21106;&#30340;&#40065;&#26834;&#24615;&#12290;&#25253;&#21578;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#34920;&#29616;&#26368;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#23545;&#25152;&#26377;&#21442;&#19982;&#32773;&#37096;&#32626;&#30340;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper outlines the winning solutions employed in addressing the MUAD uncertainty quantification challenge held at ICCV 2023. The challenge was centered around semantic segmentation in urban environments, with a particular focus on natural adversarial scenarios. The report presents the results of 19 submitted entries, with numerous techniques drawing inspiration from cutting-edge uncertainty quantification methodologies presented at prominent conferences in the fields of computer vision and machine learning and journals over the past few years. Within this document, the challenge is introduced, shedding light on its purpose and objectives, which primarily revolved around enhancing the robustness of semantic segmentation in urban scenes under varying natural adversarial conditions. The report then delves into the top-performing solutions. Moreover, the document aims to provide a comprehensive overview of the diverse solutions deployed by all participants. By doing so, it seeks to of
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;AIoT&#31995;&#32479;&#30340;&#36328;&#32423;&#21035;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;-&#31995;&#32479;&#20849;&#21516;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;DL&#27169;&#22411;&#21644;&#31995;&#32479;&#35843;&#24230;&#65292;&#25913;&#21892;&#20102;&#36816;&#34892;&#26102;&#36164;&#28304;&#21487;&#29992;&#24615;&#65292;&#25512;&#21160;AIoT&#24615;&#33021;&#30340;&#36827;&#19968;&#27493;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.15467</link><description>&lt;p&gt;
&#36890;&#36807;&#36328;&#32423;&#21035;&#20248;&#21270;&#23454;&#29616;&#36164;&#28304;&#39640;&#25928;&#30340;AIoT&#31995;&#32479;: &#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Enabling Resource-efficient AIoT System with Cross-level Optimization: A survey. (arXiv:2309.15467v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15467
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;AIoT&#31995;&#32479;&#30340;&#36328;&#32423;&#21035;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;-&#31995;&#32479;&#20849;&#21516;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;DL&#27169;&#22411;&#21644;&#31995;&#32479;&#35843;&#24230;&#65292;&#25913;&#21892;&#20102;&#36816;&#34892;&#26102;&#36164;&#28304;&#21487;&#29992;&#24615;&#65292;&#25512;&#21160;AIoT&#24615;&#33021;&#30340;&#36827;&#19968;&#27493;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#22522;&#30784;&#35774;&#26045;&#30340;&#24191;&#27867;&#20351;&#29992;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#21151;&#65292;&#20154;&#24037;&#26234;&#33021;&#29289;&#32852;&#32593;&#65288;AIoT&#65292;AI+IoT&#65289;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#24471;&#21040;&#20102;&#25512;&#21160;&#12290;DL&#27169;&#22411;&#36164;&#28304;&#23494;&#38598;&#65292;&#22240;&#27492;&#29616;&#26377;&#30740;&#31350;&#21162;&#21147;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22522;&#30784;&#35774;&#26045;&#19978;&#23454;&#29616;AIoT&#23454;&#26102;&#25512;&#29702;&#21644;&#20302;&#25104;&#26412;&#35757;&#32451;&#12290;&#20026;&#27492;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#36164;&#28304;&#21451;&#22909;&#30340;DL&#27169;&#22411;&#21644;&#27169;&#22411;&#33258;&#36866;&#24212;&#31995;&#32479;&#35843;&#24230;&#30340;&#31639;&#27861;-&#31995;&#32479;&#20849;&#21516;&#35774;&#35745;&#25913;&#36827;&#20102;&#36816;&#34892;&#26102;&#36164;&#28304;&#21487;&#29992;&#24615;&#65292;&#20174;&#32780;&#25512;&#21160;&#20102;&#30001;&#29420;&#31435;&#32423;&#21035;&#35774;&#23450;&#30340;&#24615;&#33021;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emerging field of artificial intelligence of things (AIoT, AI+IoT) is driven by the widespread use of intelligent infrastructures and the impressive success of deep learning (DL). With the deployment of DL on various intelligent infrastructures featuring rich sensors and weak DL computing capabilities, a diverse range of AIoT applications has become possible. However, DL models are notoriously resource-intensive. Existing research strives to realize near-/realtime inference of AIoT live data and low-cost training using AIoT datasets on resource-scare infrastructures. Accordingly, the accuracy and responsiveness of DL models are bounded by resource availability. To this end, the algorithm-system co-design that jointly optimizes the resource-friendly DL models and model-adaptive system scheduling improves the runtime resource availability and thus pushes the performance boundary set by the standalone level. Unlike previous surveys on resource-friendly DL models or hand-crafted DL com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#31070;&#32463;&#25552;&#31034;&#65288;GNP&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#30693;&#35782;&#22270;&#20013;&#23398;&#20064;&#26377;&#30410;&#30340;&#30693;&#35782;&#65292;&#20197;&#24357;&#34917;&#23427;&#20204;&#22312;&#20934;&#30830;&#25429;&#25417;&#21644;&#36820;&#22238;&#22522;&#20110;&#30693;&#35782;&#30340;&#20449;&#24687;&#26041;&#38754;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.15427</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#31070;&#32463;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Prompting with Large Language Models. (arXiv:2309.15427v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#31070;&#32463;&#25552;&#31034;&#65288;GNP&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#30693;&#35782;&#22270;&#20013;&#23398;&#20064;&#26377;&#30410;&#30340;&#30693;&#35782;&#65292;&#20197;&#24357;&#34917;&#23427;&#20204;&#22312;&#20934;&#30830;&#25429;&#25417;&#21644;&#36820;&#22238;&#22522;&#20110;&#30693;&#35782;&#30340;&#20449;&#24687;&#26041;&#38754;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#20934;&#30830;&#25429;&#25417;&#21644;&#36820;&#22238;&#22522;&#20110;&#30693;&#35782;&#30340;&#20449;&#24687;&#26041;&#38754;&#20173;&#23384;&#22312;&#22266;&#26377;&#38480;&#21046;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#21033;&#29992;&#30693;&#35782;&#22270;&#26469;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#21644;&#23450;&#21046;&#27169;&#22411;&#26550;&#26500;&#22686;&#24378;&#35821;&#35328;&#24314;&#27169;&#65292;&#20294;&#26159;&#23558;&#27492;&#24212;&#29992;&#20110;LLMs&#23384;&#22312;&#21442;&#25968;&#25968;&#37327;&#24222;&#22823;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22914;&#20309;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#24182;&#36991;&#20813;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#33258;&#23450;&#20041;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#31070;&#32463;&#25552;&#31034;&#65288;GNP&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#39044;&#35757;&#32451;&#30340;LLMs&#20174;&#30693;&#35782;&#22270;&#20013;&#23398;&#20064;&#26377;&#30410;&#30340;&#30693;&#35782;&#12290;GNP&#21253;&#25324;&#21508;&#31181;&#35774;&#35745;&#65292;&#21253;&#25324;&#26631;&#20934;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22120;&#12289;&#36328;&#27169;&#24577;&#27719;&#32858;&#27169;&#22359;&#12289;&#22495;&#25237;&#24433;&#22120;&#21644;&#33258;&#30417;&#30563;&#38142;&#25509;&#39044;&#27979;&#30446;&#26631;&#12290;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;GNP&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable generalization capability with exceptional performance in various language modeling tasks. However, they still exhibit inherent limitations in precisely capturing and returning grounded knowledge. While existing work has explored utilizing knowledge graphs to enhance language modeling via joint training and customized model architectures, applying this to LLMs is problematic owing to their large number of parameters and high computational cost. In addition, how to leverage the pre-trained LLMs and avoid training a customized model from scratch remains an open question. In this work, we propose Graph Neural Prompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in learning beneficial knowledge from KGs. GNP encompasses various designs, including a standard graph neural network encoder, a cross-modality pooling module, a domain projector, and a self-supervised link prediction objective. Extensive experiments on multiple
&lt;/p&gt;</description></item><item><title>NeuRBF&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#24452;&#21521;&#22522;&#20989;&#25968;&#36827;&#34892;&#20449;&#21495;&#34920;&#31034;&#30340;&#31070;&#32463;&#22330;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#31354;&#38388;&#36866;&#24212;&#24615;&#21644;&#33021;&#22815;&#26356;&#22909;&#22320;&#25311;&#21512;&#30446;&#26631;&#20449;&#21495;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#19982;&#22810;&#39057;&#27491;&#24358;&#20989;&#25968;&#32452;&#21512;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24452;&#21521;&#22522;&#20989;&#25968;&#30340;&#36890;&#36947;&#23481;&#37327;&#21644;&#34920;&#31034;&#32454;&#33410;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.15426</link><description>&lt;p&gt;
NeuRBF: &#19968;&#31181;&#20855;&#26377;&#33258;&#36866;&#24212;&#24452;&#21521;&#22522;&#20989;&#25968;&#30340;&#31070;&#32463;&#22330;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NeuRBF: A Neural Fields Representation with Adaptive Radial Basis Functions. (arXiv:2309.15426v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15426
&lt;/p&gt;
&lt;p&gt;
NeuRBF&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#24452;&#21521;&#22522;&#20989;&#25968;&#36827;&#34892;&#20449;&#21495;&#34920;&#31034;&#30340;&#31070;&#32463;&#22330;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#31354;&#38388;&#36866;&#24212;&#24615;&#21644;&#33021;&#22815;&#26356;&#22909;&#22320;&#25311;&#21512;&#30446;&#26631;&#20449;&#21495;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#19982;&#22810;&#39057;&#27491;&#24358;&#20989;&#25968;&#32452;&#21512;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24452;&#21521;&#22522;&#20989;&#25968;&#30340;&#36890;&#36947;&#23481;&#37327;&#21644;&#34920;&#31034;&#32454;&#33410;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#22330;&#31867;&#22411;&#65292;&#23427;&#20351;&#29992;&#36890;&#29992;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#26469;&#34920;&#31034;&#20449;&#21495;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#22330;&#36890;&#24120;&#20351;&#29992;&#22522;&#20110;&#32593;&#26684;&#30340;&#34920;&#31034;&#26041;&#27861;&#26469;&#23384;&#20648;&#23616;&#37096;&#31070;&#32463;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;N&#32500;&#32447;&#24615;&#26680;&#26469;&#22312;&#36830;&#32493;&#26597;&#35810;&#28857;&#19978;&#36827;&#34892;&#29305;&#24449;&#25554;&#20540;&#12290;&#23427;&#20204;&#30340;&#31070;&#32463;&#29305;&#24449;&#30340;&#31354;&#38388;&#20301;&#32622;&#22266;&#23450;&#22312;&#32593;&#26684;&#33410;&#28857;&#19978;&#65292;&#24182;&#19988;&#26080;&#27861;&#24456;&#22909;&#22320;&#36866;&#24212;&#30446;&#26631;&#20449;&#21495;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20855;&#26377;&#28789;&#27963;&#30340;&#26680;&#20301;&#32622;&#21644;&#24418;&#29366;&#30340;&#36890;&#29992;&#24452;&#21521;&#22522;&#20989;&#25968;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#31354;&#38388;&#36866;&#24212;&#24615;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25311;&#21512;&#30446;&#26631;&#20449;&#21495;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#24452;&#21521;&#22522;&#20989;&#25968;&#30340;&#36890;&#36947;&#23481;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#20854;&#19982;&#22810;&#39057;&#27491;&#24358;&#20989;&#25968;&#32452;&#21512;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#25216;&#26415;&#23558;&#24452;&#21521;&#22522;&#20989;&#25968;&#25193;&#23637;&#20026;&#19981;&#21516;&#39057;&#24102;&#30340;&#22810;&#20010;&#20613;&#37324;&#21494;&#24452;&#21521;&#22522;&#20989;&#25968;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#21442;&#25968;&#65292;&#20415;&#20110;&#34920;&#31034;&#32454;&#33410;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;&#33258;&#36866;&#24212;&#24452;&#21521;&#22522;&#20989;&#25968;&#19982;&#22522;&#20110;&#32593;&#26684;&#30340;&#26041;&#27861;&#32467;&#21512;&#65292;&#25105;&#20204;&#30340;&#28151;&#21512;&#32452;&#21512;&#32487;&#25215;&#20102;&#20004;&#32773;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel type of neural fields that uses general radial bases for signal representation. State-of-the-art neural fields typically rely on grid-based representations for storing local neural features and N-dimensional linear kernels for interpolating features at continuous query points. The spatial positions of their neural features are fixed on grid nodes and cannot well adapt to target signals. Our method instead builds upon general radial bases with flexible kernel position and shape, which have higher spatial adaptivity and can more closely fit target signals. To further improve the channel-wise capacity of radial basis functions, we propose to compose them with multi-frequency sinusoid functions. This technique extends a radial basis to multiple Fourier radial bases of different frequency bands without requiring extra parameters, facilitating the representation of details. Moreover, by marrying adaptive radial bases with grid-based ones, our hybrid combination inherits bo
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#22312;&#30830;&#23450;&#24615;&#35745;&#31639;&#21147;&#23398;&#39046;&#22495;&#30340;&#20116;&#20010;&#20027;&#35201;&#31867;&#21035;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#21253;&#25324;&#27169;&#25311;&#26367;&#20195;&#12289;&#27169;&#25311;&#22686;&#24378;&#12289;&#31070;&#32463;&#32593;&#32476;&#31163;&#25955;&#21270;&#12289;&#29983;&#25104;&#26041;&#27861;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2309.15421</link><description>&lt;p&gt;
&#30830;&#23450;&#24615;&#35745;&#31639;&#21147;&#23398;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Learning in Deterministic Computational Mechanics. (arXiv:2309.15421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15421
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#30830;&#23450;&#24615;&#35745;&#31639;&#21147;&#23398;&#39046;&#22495;&#30340;&#20116;&#20010;&#20027;&#35201;&#31867;&#21035;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#21253;&#25324;&#27169;&#25311;&#26367;&#20195;&#12289;&#27169;&#25311;&#22686;&#24378;&#12289;&#31070;&#32463;&#32593;&#32476;&#31163;&#25955;&#21270;&#12289;&#29983;&#25104;&#26041;&#27861;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#35745;&#31639;&#21147;&#23398;&#39046;&#22495;&#20013;&#27491;&#22312;&#24555;&#36895;&#21457;&#23637;&#65292;&#20135;&#29983;&#20102;&#22823;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#25991;&#29486;&#12290;&#20026;&#20102;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#25214;&#21040;&#36825;&#20010;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#27010;&#24565;&#21644;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#30830;&#23450;&#24615;&#35745;&#31639;&#21147;&#23398;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#27010;&#36848;&#12290;&#25105;&#20204;&#30830;&#35748;&#24182;&#25506;&#32034;&#20102;&#20116;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#27169;&#25311;&#26367;&#20195;&#12289;&#27169;&#25311;&#22686;&#24378;&#12289;&#31070;&#32463;&#32593;&#32476;&#31163;&#25955;&#21270;&#12289;&#29983;&#25104;&#26041;&#27861;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#32508;&#36848;&#20851;&#27880;&#30340;&#26159;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#32780;&#38750;&#35745;&#31639;&#21147;&#23398;&#30340;&#24212;&#29992;&#65292;&#20174;&#32780;&#35753;&#30740;&#31350;&#20154;&#21592;&#26356;&#26377;&#25928;&#22320;&#25506;&#32034;&#36825;&#20010;&#39046;&#22495;&#12290;&#22240;&#27492;&#65292;&#36825;&#31687;&#32508;&#36848;&#19981;&#19968;&#23450;&#26159;&#38024;&#23545;&#20855;&#26377;&#28145;&#24230;&#23398;&#20064;&#24191;&#27867;&#30693;&#35782;&#30340;&#30740;&#31350;&#20154;&#21592;&#65292;&#32780;&#26159;&#20027;&#35201;&#38754;&#21521;&#21363;&#23558;&#36827;&#20837;&#36825;&#20010;&#39046;&#22495;&#25110;&#32773;&#35797;&#22270;&#33719;&#24471;&#35745;&#31639;&#21147;&#23398;&#20013;&#28145;&#24230;&#23398;&#20064;&#27010;&#36848;&#30340;&#30740;&#31350;&#20154;&#21592;&#12290;&#22240;&#27492;&#65292;&#35752;&#35770;&#30340;&#27010;&#24565;&#26159;...
&lt;/p&gt;
&lt;p&gt;
The rapid growth of deep learning research, including within the field of computational mechanics, has resulted in an extensive and diverse body of literature. To help researchers identify key concepts and promising methodologies within this field, we provide an overview of deep learning in deterministic computational mechanics. Five main categories are identified and explored: simulation substitution, simulation enhancement, discretizations as neural networks, generative approaches, and deep reinforcement learning. This review focuses on deep learning methods rather than applications for computational mechanics, thereby enabling researchers to explore this field more effectively. As such, the review is not necessarily aimed at researchers with extensive knowledge of deep learning -- instead, the primary audience is researchers at the verge of entering this field or those who attempt to gain an overview of deep learning in computational mechanics. The discussed concepts are, therefore,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#35299;&#20915;&#32858;&#31867;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#34920;&#31034;&#23849;&#28291;&#12289;&#32858;&#31867;&#23849;&#28291;&#21644;&#23545;&#32858;&#31867;&#20998;&#37197;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#38382;&#39064;&#12290;&#35813;&#30446;&#26631;&#20989;&#25968;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#29702;&#35770;&#22522;&#30784;&#65292;&#36866;&#29992;&#20110;&#26631;&#20934;&#30340;&#20027;&#24178;&#32467;&#26500;&#35757;&#32451;&#65292;&#26080;&#38656;&#20351;&#29992;&#38750;&#23545;&#31216;&#20803;&#32032;&#12290;</title><link>http://arxiv.org/abs/2309.15420</link><description>&lt;p&gt;
&#22833;&#36133;&#27169;&#24335;&#19977;&#20803;&#32452;&#21450;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Triad of Failure Modes and a Possible Way Out. (arXiv:2309.15420v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#35299;&#20915;&#32858;&#31867;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#34920;&#31034;&#23849;&#28291;&#12289;&#32858;&#31867;&#23849;&#28291;&#21644;&#23545;&#32858;&#31867;&#20998;&#37197;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#38382;&#39064;&#12290;&#35813;&#30446;&#26631;&#20989;&#25968;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#29702;&#35770;&#22522;&#30784;&#65292;&#36866;&#29992;&#20110;&#26631;&#20934;&#30340;&#20027;&#24178;&#32467;&#26500;&#35757;&#32451;&#65292;&#26080;&#38656;&#20351;&#29992;&#38750;&#23545;&#31216;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32858;&#31867;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#26088;&#22312;&#35299;&#20915;&#34920;&#31034;&#23849;&#28291;&#12289;&#32858;&#31867;&#23849;&#28291;&#21644;&#23545;&#32858;&#31867;&#20998;&#37197;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;&#31561;&#19977;&#31181;&#22833;&#36133;&#27169;&#24335;&#12290;&#36825;&#20010;&#30446;&#26631;&#20989;&#25968;&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26500;&#25104;&#65306;&#65288;i&#65289;&#24809;&#32602;&#34920;&#31034;&#23849;&#28291;&#30340;&#29983;&#25104;&#39033;&#65292;&#65288;ii&#65289;&#20419;&#36827;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#19981;&#21464;&#24615;&#30340;&#39033;&#65292;&#20174;&#32780;&#35299;&#20915;&#26631;&#31614;&#32622;&#25442;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#24809;&#32602;&#32858;&#31867;&#23849;&#28291;&#30340;&#22343;&#21248;&#24615;&#39033;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#30446;&#26631;&#20989;&#25968;&#20855;&#26377;&#20004;&#20010;&#26174;&#33879;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#21487;&#20197;&#20174;&#36125;&#21494;&#26031;&#30340;&#35282;&#24230;&#35299;&#37322;&#20026;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#30340;&#19979;&#30028;&#12290;&#20854;&#27425;&#65292;&#23427;&#21487;&#20197;&#35757;&#32451;&#19968;&#20010;&#26631;&#20934;&#30340;&#20027;&#24178;&#32467;&#26500;&#65292;&#26080;&#38656;&#20351;&#29992;&#38750;&#23545;&#31216;&#20803;&#32032;&#65292;&#22914;&#20572;&#27490;&#26799;&#24230;&#12289;&#21160;&#37327;&#32534;&#30721;&#22120;&#25110;&#19987;&#38376;&#30340;&#32858;&#31867;&#23618;&#12290;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#21644;&#29702;&#35770;&#22522;&#30784;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#30446;&#26631;&#20989;&#25968;&#38750;&#24120;&#36866;&#21512;
&lt;/p&gt;
&lt;p&gt;
We present a novel objective function for cluster-based self-supervised learning (SSL) that is designed to circumvent the triad of failure modes, namely representation collapse, cluster collapse, and the problem of invariance to permutations of cluster assignments. This objective consists of three key components: (i) A generative term that penalizes representation collapse, (ii) a term that promotes invariance to data augmentations, thereby addressing the issue of label permutations and (ii) a uniformity term that penalizes cluster collapse. Additionally, our proposed objective possesses two notable advantages. Firstly, it can be interpreted from a Bayesian perspective as a lower bound on the data log-likelihood. Secondly, it enables the training of a standard backbone architecture without the need for asymmetric elements like stop gradients, momentum encoders, or specialized clustering layers. Due to its simplicity and theoretical foundation, our proposed objective is well-suited for 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25163;&#35757;&#32451;&#23454;&#29616;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29305;&#24449;&#20844;&#24179;&#24615;&#65292;&#25552;&#39640;&#25972;&#20307;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;</title><link>http://arxiv.org/abs/2309.15418</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25163;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29305;&#24449;&#20844;&#24179;&#24615;&#30340;&#33258;&#21160;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Automatic Feature Fairness in Recommendation via Adversaries. (arXiv:2309.15418v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15418
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25163;&#35757;&#32451;&#23454;&#29616;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29305;&#24449;&#20844;&#24179;&#24615;&#65292;&#25552;&#39640;&#25972;&#20307;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#24615;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#24191;&#27867;&#35752;&#35770;&#30340;&#19968;&#20010;&#20027;&#39064;&#65292;&#20294;&#20854;&#23454;&#36341;&#23454;&#29616;&#22312;&#23450;&#20041;&#25935;&#24863;&#29305;&#24449;&#30340;&#21516;&#26102;&#20445;&#25345;&#25512;&#33616;&#20934;&#30830;&#24615;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#29305;&#24449;&#20844;&#24179;&#24615;&#20316;&#20026;&#23454;&#29616;&#21508;&#20010;&#30001;&#19981;&#21516;&#29305;&#24449;&#32452;&#21512;&#23450;&#20041;&#30340;&#22810;&#26679;&#32676;&#20307;&#20043;&#38388;&#30340;&#20844;&#24179;&#24453;&#36935;&#30340;&#22522;&#30784;&#12290;&#36890;&#36807;&#24179;&#34913;&#29305;&#24449;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#25552;&#39640;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#25163;&#35757;&#32451;&#24341;&#20837;&#20102;&#26080;&#20559;&#29305;&#24449;&#23398;&#20064;&#65292;&#20351;&#29992;&#23545;&#25163;&#25200;&#21160;&#22686;&#24378;&#29305;&#24449;&#34920;&#31034;&#12290;&#23545;&#25163;&#25913;&#36827;&#20102;&#27169;&#22411;&#23545;&#23569;&#25968;&#29305;&#24449;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#26681;&#25454;&#29305;&#24449;&#20559;&#24046;&#30340;&#20004;&#31181;&#24418;&#24335;&#33258;&#21160;&#36866;&#24212;&#23545;&#25163;&#65306;&#29305;&#24449;&#20540;&#30340;&#39057;&#29575;&#21644;&#32452;&#21512;&#22810;&#26679;&#24615;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#21160;&#24577;&#35843;&#25972;&#25200;&#21160;&#24378;&#24230;&#21644;&#23545;&#25163;&#35757;&#32451;&#26435;&#37325;&#12290;&#26356;&#24378;&#30340;&#25200;&#21160;&#36866;&#29992;&#20110;&#32452;&#21512;&#21464;&#21270;&#23569;&#30340;&#29305;&#24449;&#20540;&#65292;&#20197;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#23545;&#20110;&#20302;&#39057;&#29305;&#24449;&#65292;&#36739;&#39640;&#30340;&#26435;&#37325;&#21487;&#20197;&#35299;&#20915;...
&lt;/p&gt;
&lt;p&gt;
Fairness is a widely discussed topic in recommender systems, but its practical implementation faces challenges in defining sensitive features while maintaining recommendation accuracy. We propose feature fairness as the foundation to achieve equitable treatment across diverse groups defined by various feature combinations. This improves overall accuracy through balanced feature generalizability. We introduce unbiased feature learning through adversarial training, using adversarial perturbation to enhance feature representation. The adversaries improve model generalization for under-represented features. We adapt adversaries automatically based on two forms of feature biases: frequency and combination variety of feature values. This allows us to dynamically adjust perturbation strengths and adversarial training weights. Stronger perturbations are applied to feature values with fewer combination varieties to improve generalization, while higher weights for low-frequency features address 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;AI&#39537;&#21160;&#30340;&#30693;&#35782;&#21457;&#29616;&#25216;&#26415;&#65292;&#39318;&#27425;&#25581;&#31034;&#20102;&#22320;&#24418;&#29305;&#24449;&#19982;&#38477;&#27700;&#27169;&#24335;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#24182;&#22312;1995&#24180;&#24038;&#21491;&#25581;&#31034;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#22320;&#24418;&#19982;&#38477;&#27700;&#20851;&#31995;&#36716;&#21464;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2309.15400</link><description>&lt;p&gt;
&#36890;&#36807;AI&#39537;&#21160;&#30340;&#30693;&#35782;&#21457;&#29616;&#65292;&#38761;&#26032;&#23545;&#22320;&#24418;&#38477;&#27700;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Terrain-Precipitation Understanding through AI-driven Knowledge Discovery. (arXiv:2309.15400v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15400
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;AI&#39537;&#21160;&#30340;&#30693;&#35782;&#21457;&#29616;&#25216;&#26415;&#65292;&#39318;&#27425;&#25581;&#31034;&#20102;&#22320;&#24418;&#29305;&#24449;&#19982;&#38477;&#27700;&#27169;&#24335;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#24182;&#22312;1995&#24180;&#24038;&#21491;&#25581;&#31034;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#22320;&#24418;&#19982;&#38477;&#27700;&#20851;&#31995;&#36716;&#21464;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#30340;&#27668;&#20505;&#31185;&#23398;&#20013;&#65292;&#25512;&#36827;&#25105;&#20204;&#23545;&#22797;&#26434;&#22320;&#24418;&#22320;&#21306;&#27668;&#20505;&#36807;&#31243;&#30340;&#29702;&#35299;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#20840;&#29699;&#27668;&#20505;&#21464;&#21270;&#30340;&#32972;&#26223;&#19979;&#12290;&#23588;&#20854;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#36825;&#20123;&#22320;&#21306;&#35266;&#27979;&#25968;&#25454;&#30340;&#32570;&#20047;&#23545;&#20110;&#29702;&#35299;&#20854;&#20013;&#24494;&#22937;&#30340;&#27668;&#20505;&#21160;&#21147;&#23398;&#20135;&#29983;&#20102;&#26174;&#33879;&#38480;&#21046;&#12290;&#39318;&#27425;&#21033;&#29992;&#23574;&#31471;&#30340;AI&#39537;&#21160;&#30340;&#30693;&#35782;&#21457;&#29616;&#25216;&#26415;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#26126;&#30830;&#30340;&#26041;&#31243;&#26469;&#38416;&#26126;&#22320;&#24418;&#29305;&#24449;&#21644;&#38477;&#27700;&#27169;&#24335;&#20043;&#38388;&#22797;&#26434;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20043;&#21069;&#38544;&#34255;&#30340;&#25511;&#21046;&#36825;&#20123;&#20851;&#31995;&#30340;&#22797;&#26434;&#24615;&#12290;&#36825;&#20123;&#36804;&#20170;&#26410;&#25259;&#38706;&#30340;&#26041;&#31243;&#22312;&#24212;&#29992;&#20110;&#38477;&#27700;&#25968;&#25454;&#26102;&#19982;&#20256;&#32479;&#32463;&#39564;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;&#8220;1995&#24180;&#36716;&#25240;&#28857;&#8221;&#30340;&#29616;&#35937;&#65292;&#34920;&#26126;&#20102;&#22823;&#32422;&#22312;1995&#24180;&#24038;&#21491;&#22320;&#24418;&#19982;&#38477;&#27700;&#20851;&#31995;&#30340;&#26174;&#33879;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancing our understanding of climate processes in regions characterized by intricate terrain complexity is a paramount challenge in contemporary climate science, particularly in the context of global climate change. Notably, the scarcity of observational data in these regions has imposed substantial limitations on understanding the nuanced climate dynamics therein. For the first time, utilizing cutting-edge AI-driven knowledge discovery techniques, we have uncovered explicit equations that elucidate the intricate relationship between terrain features and precipitation patterns, illuminating the previously concealed complexities governing these relationships. These equations, thus far undisclosed, exhibit remarkable accuracy compared to conventional empirical models when applied to precipitation data. Building on this foundation, we reveal a phenomenon known as the '1995 turning point,' indicating a significant shift in the terrain-precipitation relationship in approximately 1995, rel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;PRI&#65292;&#29992;&#20110;&#22312;&#32447;CMDPs&#20013;&#30340;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;CMDPs&#30340;&#26377;&#38480;&#38543;&#26426;&#24615;&#23646;&#24615;&#65292;&#33021;&#22815;&#20197;&#20302;&#36951;&#25022;&#24182;&#20197;&#39640;&#27010;&#29575;&#35782;&#21035;&#20986;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.15395</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;CMDPs&#20013;&#65292;&#26080;&#27169;&#22411;&#12289;&#36951;&#25022;&#26368;&#20248;&#30340;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Model-Free, Regret-Optimal Best Policy Identification in Online CMDPs. (arXiv:2309.15395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;PRI&#65292;&#29992;&#20110;&#22312;&#32447;CMDPs&#20013;&#30340;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;CMDPs&#30340;&#26377;&#38480;&#38543;&#26426;&#24615;&#23646;&#24615;&#65292;&#33021;&#22815;&#20197;&#20302;&#36951;&#25022;&#24182;&#20197;&#39640;&#27010;&#29575;&#35782;&#21035;&#20986;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#32447;&#32422;&#26463;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDPs&#65289;&#20013;&#30340;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#65288;BPI&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#20855;&#26377;&#20302;&#36951;&#25022;&#24182;&#19988;&#20197;&#39640;&#27010;&#29575;&#35782;&#21035;&#26368;&#20248;&#31574;&#30053;&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#24863;&#20852;&#36259;&#12290;&#29616;&#26377;&#30340;&#22312;&#32447;CMDPs&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#22312;&#27425;&#32447;&#24615;&#36951;&#25022;&#21644;&#36829;&#32422;&#26102;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#23545;&#26368;&#20248;&#31574;&#30053;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#19988;&#21482;&#22312;&#20174;&#20197;&#21069;&#20351;&#29992;&#30340;&#31574;&#30053;&#20013;&#38543;&#26426;&#22343;&#21248;&#25277;&#26679;&#26102;&#25552;&#20379;&#24179;&#22343;&#24615;&#33021;&#20445;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;PRUNING-REFINEMENT-IDENTIFICATION&#65288;PRI&#65289;&#65292;&#22522;&#20110;&#25105;&#20204;&#21457;&#29616;&#30340;CMDPs&#30340;&#19968;&#20010;&#22522;&#26412;&#32467;&#26500;&#24615;&#36136;&#65292;&#31216;&#20026;&#26377;&#38480;&#38543;&#26426;&#24615;&#12290;&#35813;&#23646;&#24615;&#34920;&#26126;&#23545;&#20110;&#20855;&#26377;N&#32422;&#26463;&#30340;CMDP&#65292;&#23384;&#22312;&#19968;&#20010;&#26368;&#20248;&#31574;&#30053;&#65292;&#20854;&#20013;&#33267;&#22810;&#26377;N&#20010;&#38543;&#26426;&#20915;&#31574;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#39318;&#20808;&#35782;&#21035;&#20986;&#22312;&#21738;&#20010;&#27493;&#39588;&#21644;&#21738;&#20010;&#29366;&#24577;&#38656;&#35201;&#36827;&#34892;&#38543;&#26426;&#20915;&#31574;&#65292;&#28982;&#21518;&#23545;&#36825;&#20123;&#20915;&#31574;&#30340;&#20998;&#24067;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the best policy identification (BPI) problem in online Constrained Markov Decision Processes (CMDPs). We are interested in algorithms that are model-free, have low regret, and identify an optimal policy with a high probability. Existing model-free algorithms for online CMDPs with sublinear regret and constraint violation do not provide any convergence guarantee to an optimal policy and provide only average performance guarantees when a policy is uniformly sampled at random from all previously used policies. In this paper, we develop a new algorithm, named Pruning-Refinement-Identification (PRI), based on a fundamental structural property of CMDPs we discover, called limited stochasticity. The property says for a CMDP with $N$ constraints, there exists an optimal policy with at most $N$ stochastic decisions.  The proposed algorithm first identifies at which step and in which state a stochastic decision has to be taken and then fine-tunes the distributions of these s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;ResNet-like&#27169;&#22411;&#22312;&#38750;&#24847;&#22270;&#36752;&#23556;&#21457;&#23556;&#65288;URE&#65289;&#20998;&#31867;&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDEs&#65289;&#30340;&#24212;&#29992;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#23545;&#22122;&#22768;&#30340;&#33030;&#24369;&#24615;&#20197;&#21450;&#35299;&#37322;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.15386</link><description>&lt;p&gt;
&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#29992;&#20110;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30005;&#30913;&#38750;&#24847;&#22270;&#36752;&#23556;&#21457;&#23556;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Neural Stochastic Differential Equations for Robust and Explainable Analysis of Electromagnetic Unintended Radiated Emissions. (arXiv:2309.15386v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;ResNet-like&#27169;&#22411;&#22312;&#38750;&#24847;&#22270;&#36752;&#23556;&#21457;&#23556;&#65288;URE&#65289;&#20998;&#31867;&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDEs&#65289;&#30340;&#24212;&#29992;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#23545;&#22122;&#22768;&#30340;&#33030;&#24369;&#24615;&#20197;&#21450;&#35299;&#37322;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#38750;&#24847;&#22270;&#36752;&#23556;&#21457;&#23556;&#65288;URE&#65289;&#20998;&#31867;&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;ResNet-like&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDEs&#65289;&#26469;&#35299;&#20915;&#24050;&#21457;&#29616;&#38480;&#21046;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;ResNet-like&#27169;&#22411;&#23545;&#39640;&#26031;&#22122;&#22768;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#65292;&#24403;&#39640;&#26031;&#22122;&#22768;&#20165;&#20026;0.5&#26631;&#20934;&#24046;&#26102;&#65292;&#27169;&#22411;&#24615;&#33021;&#24613;&#21095;&#19979;&#38477;&#65292;&#20854;F1-score&#38477;&#33267;&#25509;&#36817;&#26080;&#24847;&#20041;&#30340;0.008&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#21363;ResNet-like&#27169;&#22411;&#25552;&#20379;&#30340;&#35299;&#37322;&#19981;&#21453;&#26144;&#36755;&#20837;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#21608;&#26399;&#24615;&#65292;&#22312;&#31283;&#23450;&#35774;&#22791;&#20013;&#36827;&#34892;URE&#26816;&#27979;&#26102;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#23646;&#24615;&#12290;&#38024;&#23545;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24212;&#29992;&#31070;&#32463;SDEs&#26500;&#24314;URE&#20998;&#31867;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#27169;&#22411;&#19981;&#20165;&#23545;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36824;&#33021;&#25552;&#20379;&#26356;&#26377;&#24847;&#20041;&#21644;&#30452;&#35266;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive evaluation of the robustness and explainability of ResNet-like models in the context of Unintended Radiated Emission (URE) classification and suggest a new approach leveraging Neural Stochastic Differential Equations (SDEs) to address identified limitations. We provide an empirical demonstration of the fragility of ResNet-like models to Gaussian noise perturbations, where the model performance deteriorates sharply and its F1-score drops to near insignificance at 0.008 with a Gaussian noise of only 0.5 standard deviation. We also highlight a concerning discrepancy where the explanations provided by ResNet-like models do not reflect the inherent periodicity in the input data, a crucial attribute in URE detection from stable devices. In response to these findings, we propose a novel application of Neural SDEs to build models for URE classification that are not only robust to noise but also provide more meaningful and intuitive explanations. Neural SDE models mai
&lt;/p&gt;</description></item><item><title>ADGym&#26159;&#19968;&#27454;&#38024;&#23545;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#30340;&#35774;&#35745;&#36873;&#25321;&#30340;&#32508;&#21512;&#35780;&#20272;&#21644;&#33258;&#21160;&#36873;&#25321;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2309.15376</link><description>&lt;p&gt;
ADGym&#65306;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#30340;&#35774;&#35745;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
ADGym: Design Choices for Deep Anomaly Detection. (arXiv:2309.15376v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15376
&lt;/p&gt;
&lt;p&gt;
ADGym&#26159;&#19968;&#27454;&#38024;&#23545;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#30340;&#35774;&#35745;&#36873;&#25321;&#30340;&#32508;&#21512;&#35780;&#20272;&#21644;&#33258;&#21160;&#36873;&#25321;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#25216;&#26415;&#26368;&#36817;&#34987;&#24212;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#65292;&#22312;&#37329;&#34701;&#12289;&#21307;&#30103;&#26381;&#21153;&#21644;&#20113;&#35745;&#31639;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#24448;&#24448;&#23558;&#28145;&#24230;AD&#31639;&#27861;&#20316;&#20026;&#19968;&#20010;&#25972;&#20307;&#36827;&#34892;&#35780;&#20272;&#65292;&#26410;&#33021;&#29702;&#35299;&#20010;&#21035;&#35774;&#35745;&#36873;&#25321;&#65288;&#22914;&#25439;&#22833;&#20989;&#25968;&#21644;&#32593;&#32476;&#26550;&#26500;&#65289;&#30340;&#36129;&#29486;&#12290;&#22240;&#27492;&#65292;&#21407;&#22987;&#27493;&#39588;&#65288;&#22914;&#39044;&#22788;&#29702;&#65289;&#30340;&#37325;&#35201;&#24615;&#21487;&#33021;&#34987;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#26550;&#26500;&#25152;&#25513;&#30422;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#20004;&#20010;&#38382;&#39064;&#26469;&#35299;&#20915;&#36825;&#20123;&#30095;&#28431;&#65306;&#65288;i&#65289;&#28145;&#24230;AD&#26041;&#27861;&#30340;&#21738;&#20123;&#32452;&#25104;&#37096;&#20998;&#65288;&#21363;&#35774;&#35745;&#36873;&#25321;&#65289;&#22312;&#26816;&#27979;&#24322;&#24120;&#26041;&#38754;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65311;&#65288;ii&#65289;&#25105;&#20204;&#22914;&#20309;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#26368;&#20339;&#35774;&#35745;&#36873;&#25321;&#26469;&#26500;&#24314;&#38024;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#23450;&#21046;AD&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;&#36890;&#29992;&#30340;&#12289;&#39044;&#20808;&#23384;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ADGym&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20840;&#38754;&#35780;&#20272;&#21644;&#33258;&#21160;&#36873;&#25321;AD&#30340;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) techniques have recently been applied to anomaly detection (AD), yielding successful outcomes in areas such as finance, medical services, and cloud computing. However, much of the current research evaluates a deep AD algorithm holistically, failing to understand the contributions of individual design choices like loss functions and network architectures. Consequently, the importance of prerequisite steps, such as preprocessing, might be overshadowed by the spotlight on novel loss functions and architectures. In this paper, we address these oversights by posing two questions: (i) Which components (i.e., design choices) of deep AD methods are pivotal in detecting anomalies? (ii) How can we construct tailored AD algorithms for specific datasets by selecting the best design choices automatically, rather than relying on generic, pre-existing solutions? To this end, we introduce ADGym, the first platform designed for comprehensive evaluation and automatic selection of AD d
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#24314;&#27169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21463;&#20010;&#20307;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#23558;PPG&#20449;&#21495;&#36716;&#25442;&#20026;ECG&#65292;&#29992;&#20110;&#36830;&#32493;&#24615;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.15375</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#24314;&#27169;&#65292;&#23558;PPG&#20449;&#21495;&#36716;&#25442;&#20026;ECG&#65292;&#29992;&#20110;&#36830;&#32493;&#24615;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
PPG to ECG Signal Translation for Continuous Atrial Fibrillation Detection via Attention-based Deep State-Space Modeling. (arXiv:2309.15375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15375
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#24314;&#27169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21463;&#20010;&#20307;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#23558;PPG&#20449;&#21495;&#36716;&#25442;&#20026;ECG&#65292;&#29992;&#20110;&#36830;&#32493;&#24615;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#20449;&#21495;&#22270;&#65288;ECG&#25110;EKG&#65289;&#26159;&#19968;&#31181;&#27979;&#37327;&#24515;&#33039;&#30005;&#27963;&#21160;&#30340;&#21307;&#23398;&#27979;&#35797;&#12290;ECG&#24120;&#29992;&#20110;&#35786;&#26029;&#21644;&#30417;&#27979;&#21508;&#31181;&#24515;&#33039;&#30142;&#30149;&#65292;&#21253;&#25324;&#24515;&#24459;&#22833;&#24120;&#12289;&#24515;&#32908;&#26775;&#22622;&#21644;&#24515;&#21147;&#34928;&#31469;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;ECG&#38656;&#35201;&#20020;&#24202;&#27979;&#37327;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#21307;&#30103;&#26426;&#26500;&#30340;&#24212;&#29992;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21333;&#23548;&#32852;ECG&#24050;&#32463;&#22312;&#20329;&#25140;&#24335;&#35774;&#22791;&#19978;&#24212;&#29992;&#24191;&#27867;&#12290;&#21478;&#19968;&#31181;ECG&#30340;&#26367;&#20195;&#26041;&#27861;&#26159;&#20809;&#27978;&#24230;&#33033;&#25615;&#26816;&#27979;&#65288;PPG&#65289;&#65292;&#23427;&#37319;&#29992;&#38750;&#20405;&#20837;&#24615;&#12289;&#20302;&#25104;&#26412;&#30340;&#20809;&#23398;&#26041;&#27861;&#26469;&#27979;&#37327;&#24515;&#33039;&#29983;&#29702;&#23398;&#65292;&#20351;&#20854;&#25104;&#20026;&#25429;&#25417;&#26085;&#24120;&#29983;&#27963;&#20013;&#37325;&#35201;&#24515;&#33039;&#20449;&#21495;&#30340;&#21512;&#36866;&#36873;&#25321;&#12290;&#34429;&#28982;ECG&#21644;PPG&#20043;&#38388;&#20855;&#26377;&#24378;&#28872;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#21518;&#32773;&#24182;&#27809;&#26377;&#25552;&#20379;&#26126;&#26174;&#30340;&#20020;&#24202;&#35786;&#26029;&#20215;&#20540;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21463;&#20010;&#20307;&#38480;&#21046;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;PPG&#20449;&#21495;&#36716;&#25442;&#20026;ECG&#65292;&#20174;&#32780;&#23454;&#29616;&#36830;&#32493;&#24615;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
An electrocardiogram (ECG or EKG) is a medical test that measures the heart's electrical activity. ECGs are often used to diagnose and monitor a wide range of heart conditions, including arrhythmias, heart attacks, and heart failure. On the one hand, the conventional ECG requires clinical measurement, which restricts its deployment to medical facilities. On the other hand, single-lead ECG has become popular on wearable devices using administered procedures. An alternative to ECG is Photoplethysmography (PPG), which uses non-invasive, low-cost optical methods to measure cardiac physiology, making it a suitable option for capturing vital heart signs in daily life. As a result, it has become increasingly popular in health monitoring and is used in various clinical and commercial wearable devices. While ECG and PPG correlate strongly, the latter does not offer significant clinical diagnostic value. Here, we propose a subject-independent attention-based deep state-space model to translate P
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27979;&#24230;&#20256;&#36882;&#26041;&#27861;&#36827;&#34892;&#23494;&#24230;&#20272;&#35745;&#22312;&#29983;&#29289;&#31185;&#23398;&#20013;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#31232;&#30095;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#31232;&#30095;&#20256;&#36882;&#26144;&#23556;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.15366</link><description>&lt;p&gt;
&#36890;&#36807;&#27979;&#24230;&#20256;&#36882;&#36827;&#34892;&#23494;&#24230;&#20272;&#35745;&#65306;&#29983;&#29289;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Density Estimation via Measure Transport: Outlook for Applications in the Biological Sciences. (arXiv:2309.15366v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15366
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27979;&#24230;&#20256;&#36882;&#26041;&#27861;&#36827;&#34892;&#23494;&#24230;&#20272;&#35745;&#22312;&#29983;&#29289;&#31185;&#23398;&#20013;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#31232;&#30095;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#31232;&#30095;&#20256;&#36882;&#26144;&#23556;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#24230;&#20256;&#36882;&#26041;&#27861;&#30340;&#19968;&#20010;&#20248;&#21183;&#26159;&#20854;&#20801;&#35768;&#23545;&#26681;&#25454;&#24191;&#27867;&#27010;&#29575;&#27979;&#24230;&#20998;&#24067;&#30340;&#25968;&#25454;&#36827;&#34892;&#32479;&#19968;&#30340;&#22788;&#29702;&#21644;&#20998;&#26512;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#35745;&#31639;&#30740;&#31350;&#30340;&#32467;&#26524;&#26469;&#35780;&#20272;&#27979;&#24230;&#20256;&#36882;&#25216;&#26415;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#19977;&#35282;&#20256;&#36882;&#26144;&#23556;&#30340;&#20351;&#29992;&#65292;&#20316;&#20026;&#25903;&#25345;&#29983;&#29289;&#31185;&#23398;&#30740;&#31350;&#30340;&#24037;&#20316;&#27969;&#30340;&#19968;&#37096;&#20998;&#12290;&#31232;&#30095;&#25968;&#25454;&#22330;&#26223;&#22312;&#36752;&#23556;&#29983;&#29289;&#23398;&#31561;&#39046;&#22495;&#24456;&#24120;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25968;&#25454;&#31232;&#32570;&#26102;&#65292;&#31232;&#30095;&#20256;&#36882;&#26144;&#23556;&#26159;&#26377;&#20248;&#21183;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#35745;&#31639;&#19968;&#31995;&#21015;&#65288;&#31232;&#30095;&#30340;&#65289;&#33258;&#36866;&#24212;&#20256;&#36882;&#26144;&#23556;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#36825;&#20123;&#26144;&#23556;&#26159;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#19968;&#31995;&#21015;&#21487;&#29992;&#25968;&#25454;&#26679;&#26412;&#23376;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#32771;&#34385;&#30340;&#36752;&#23556;&#29983;&#29289;&#23398;&#24212;&#29992;&#20013;&#65292;&#27492;&#26041;&#27861;&#20026;&#29983;&#25104;&#20551;&#35774;&#25552;&#20379;&#20102;&#19968;&#20010;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
One among several advantages of measure transport methods is that they allow for a unified framework for processing and analysis of data distributed according to a wide class of probability measures. Within this context, we present results from computational studies aimed at assessing the potential of measure transport techniques, specifically, the use of triangular transport maps, as part of a workflow intended to support research in the biological sciences. Scarce data scenarios, which are common in domains such as radiation biology, are of particular interest. We find that when data is scarce, sparse transport maps are advantageous. In particular, statistics gathered from computing series of (sparse) adaptive transport maps, trained on a series of randomly chosen subsets of the set of available data samples, leads to uncovering information hidden in the data. As a result, in the radiation biology application considered here, this approach provides a tool for generating hypotheses ab
&lt;/p&gt;</description></item><item><title>C3Net&#26159;&#19968;&#31181;&#38754;&#21521;&#24322;&#36136;&#31995;&#32479;&#20013;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#39044;&#27979;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#23884;&#20837;&#21407;&#23376;&#31867;&#22411;&#22312;&#20998;&#23376;&#29615;&#22659;&#20013;&#24182;&#36981;&#24490;&#22522;&#26412;&#29289;&#29702;&#23450;&#24459;&#30340;&#21407;&#23376;&#38388;&#21183;&#12290;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20248;&#20110;&#22522;&#20110;&#37327;&#23376;&#21147;&#23398;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.15334</link><description>&lt;p&gt;
C3Net: &#38754;&#21521;&#24322;&#36136;&#31995;&#32479;&#20013;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#39044;&#27979;&#30340;&#21407;&#23376;&#38388;&#21183;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
C3Net: interatomic potential neural network for prediction of physicochemical properties in heterogenous systems. (arXiv:2309.15334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15334
&lt;/p&gt;
&lt;p&gt;
C3Net&#26159;&#19968;&#31181;&#38754;&#21521;&#24322;&#36136;&#31995;&#32479;&#20013;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#39044;&#27979;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#23884;&#20837;&#21407;&#23376;&#31867;&#22411;&#22312;&#20998;&#23376;&#29615;&#22659;&#20013;&#24182;&#36981;&#24490;&#22522;&#26412;&#29289;&#29702;&#23450;&#24459;&#30340;&#21407;&#23376;&#38388;&#21183;&#12290;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20248;&#20110;&#22522;&#20110;&#37327;&#23376;&#21147;&#23398;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#28342;&#36136;&#19982;&#20854;&#29615;&#22659;&#30340;&#30456;&#20114;&#20316;&#29992;&#22312;&#21270;&#23398;&#21644;&#29983;&#29289;&#23398;&#20013;&#20855;&#26377;&#22522;&#26412;&#37325;&#35201;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#21407;&#23376;&#31867;&#22411;&#22312;&#20998;&#23376;&#29615;&#22659;&#20013;&#30340;&#23884;&#20837;&#21644;&#36981;&#24490;&#22522;&#26412;&#29289;&#29702;&#23450;&#24459;&#30340;&#21407;&#23376;&#38388;&#21183;&#12290;&#35813;&#26550;&#26500;&#34987;&#24212;&#29992;&#20110;&#39044;&#27979;&#24322;&#36136;&#31995;&#32479;&#20013;&#30340;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#65292;&#21253;&#25324;&#28342;&#35299;&#22312;&#19981;&#21516;&#28342;&#21058;&#20013;&#12289;1-&#36763;&#37255;-&#27700;&#20998;&#37197;&#21644;PAMPA&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#22312;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#28342;&#35299;&#33258;&#30001;&#33021;&#39044;&#27979;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20110;&#37327;&#23376;&#21147;&#23398;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;&#20174;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#28342;&#36136;&#20013;&#27599;&#20010;&#21407;&#23376;&#30340;&#21407;&#23376;&#38388;&#21183;&#20801;&#35768;&#36827;&#34892;&#31526;&#21512;&#21270;&#23398;&#21644;&#29289;&#29702;&#25512;&#29702;&#30340;&#21407;&#23376;&#20998;&#36776;&#29575;&#30340;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#23450;&#37327;&#20998;&#26512;&#12290;&#35813;&#36719;&#20214;&#21487;&#22312;https://github.com/SehanLee/C3Net&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the interactions of a solute with its environment is of fundamental importance in chemistry and biology. In this work, we propose a deep neural network architecture for atom type embeddings in its molecular context and interatomic potential that follows fundamental physical laws. The architecture is applied to predict physicochemical properties in heterogeneous systems including solvation in diverse solvents, 1-octanol-water partitioning, and PAMPA with a single set of network weights. We show that our architecture is generalized well to the physicochemical properties and outperforms state-of-the-art approaches based on quantum mechanics and neural networks in the task of solvation free energy prediction. The interatomic potentials at each atom in a solute obtained from the model allow quantitative analysis of the physicochemical properties at atomic resolution consistent with chemical and physical reasoning. The software is available at https://github.com/SehanLee/C3Net.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#25506;&#32034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#23618;&#20013;&#21482;&#38656;&#35201;20%&#30340;&#29305;&#24449;&#31354;&#38388;&#26041;&#24046;&#23601;&#33021;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#20998;&#31867;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19977;&#20010;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#20223;&#23556;&#32447;&#24615;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2309.15328</link><description>&lt;p&gt;
&#20351;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Exploring Learned Representations of Neural Networks with Principal Component Analysis. (arXiv:2309.15328v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15328
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#25506;&#32034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#23618;&#20013;&#21482;&#38656;&#35201;20%&#30340;&#29305;&#24449;&#31354;&#38388;&#26041;&#24046;&#23601;&#33021;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#20998;&#31867;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19977;&#20010;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#20223;&#23556;&#32447;&#24615;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;AI&#39046;&#22495;&#20013;&#65292;&#29702;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#30340;&#29305;&#24449;&#34920;&#31034;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;(PCA)&#26469;&#30740;&#31350;&#22312;CIFAR-10&#19978;&#35757;&#32451;&#30340;ResNet-18&#23398;&#20064;&#30340;&#36880;&#23618;&#34920;&#31034;&#22312;k&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;(k-NN)&#12289;&#26368;&#36817;&#31867;&#20013;&#24515;&#20998;&#31867;&#22120;(NCC)&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26576;&#20123;&#23618;&#20013;&#65292;&#21482;&#26377;20%&#30340;&#20013;&#38388;&#29305;&#24449;&#31354;&#38388;&#26041;&#24046;&#23601;&#36275;&#20197;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#30340;&#20998;&#31867;&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#23618;&#20013;&#65292;&#21069;100&#20010;&#20027;&#25104;&#20998;&#23436;&#20840;&#20915;&#23450;&#20102;k-NN&#21644;NCC&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#21457;&#29616;&#19982;&#31070;&#32463;&#32593;&#32476;&#25910;&#32553;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#25552;&#20379;&#20102;&#20013;&#38388;&#31070;&#32463;&#32593;&#32476;&#25910;&#32553;&#30456;&#20851;&#29616;&#35937;&#30340;&#37096;&#20998;&#35777;&#25454;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#24037;&#20316;&#25552;&#20379;&#20102;&#19977;&#20010;&#19981;&#21516;&#20294;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#34920;&#31034;&#26367;&#20195;&#27169;&#22411;&#65292;&#20854;&#20013;&#26368;&#20339;&#24615;&#33021;&#26159;&#19968;&#20010;&#20223;&#23556;&#32447;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#21033;&#29992;&#20960;&#20010;&#26367;&#20195;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding feature representation for deep neural networks (DNNs) remains an open question within the general field of explainable AI. We use principal component analysis (PCA) to study the performance of a k-nearest neighbors classifier (k-NN), nearest class-centers classifier (NCC), and support vector machines on the learned layer-wise representations of a ResNet-18 trained on CIFAR-10. We show that in certain layers, as little as 20% of the intermediate feature-space variance is necessary for high-accuracy classification and that across all layers, the first ~100 PCs completely determine the performance of the k-NN and NCC classifiers. We relate our findings to neural collapse and provide partial evidence for the related phenomenon of intermediate neural collapse. Our preliminary work provides three distinct yet interpretable surrogate models for feature representation with an affine linear model the best performing. We also show that leveraging several surrogate models affords u
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#36816;&#31639;&#31526;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36830;&#32493;&#22495;&#20989;&#25968;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#21487;&#20197;&#21152;&#36895;&#31185;&#23398;&#27169;&#25311;&#21644;&#35774;&#35745;&#20013;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#23454;&#29616;&#38646;&#23556;&#36229;&#20998;&#36776;&#29575;&#20197;&#21450;&#26367;&#20195;&#29616;&#26377;&#30340;&#27169;&#25311;&#22120;&#12290;</title><link>http://arxiv.org/abs/2309.15325</link><description>&lt;p&gt;
&#31070;&#32463;&#36816;&#31639;&#31526;&#21152;&#36895;&#31185;&#23398;&#27169;&#25311;&#21644;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Neural Operators for Accelerating Scientific Simulations and Design. (arXiv:2309.15325v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#36816;&#31639;&#31526;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36830;&#32493;&#22495;&#20989;&#25968;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#21487;&#20197;&#21152;&#36895;&#31185;&#23398;&#27169;&#25311;&#21644;&#35774;&#35745;&#20013;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#23454;&#29616;&#38646;&#23556;&#36229;&#20998;&#36776;&#29575;&#20197;&#21450;&#26367;&#20195;&#29616;&#26377;&#30340;&#27169;&#25311;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#31185;&#23398;&#21457;&#29616;&#21644;&#24037;&#31243;&#35774;&#35745;&#21463;&#38480;&#20110;&#29289;&#29702;&#23454;&#39564;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#65292;&#36825;&#20123;&#23454;&#39564;&#36890;&#24120;&#26159;&#36890;&#36807;&#35797;&#39564;&#21644;&#30452;&#35273;&#36873;&#25321;&#30340;&#65292;&#24182;&#38656;&#35201;&#28145;&#20837;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#25968;&#20540;&#27169;&#25311;&#26159;&#29289;&#29702;&#23454;&#39564;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#29616;&#23454;&#39046;&#22495;&#26469;&#35828;&#65292;&#30001;&#20110;&#29616;&#26377;&#25968;&#20540;&#26041;&#27861;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36890;&#36807;&#24320;&#21457;&#24555;&#36895;&#30340;&#25968;&#25454;&#39537;&#21160;&#20195;&#29702;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#29305;&#21035;&#26159;&#65292;&#19968;&#20010;&#31216;&#20026;&#31070;&#32463;&#36816;&#31639;&#31526;&#30340;AI&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#36830;&#32493;&#22495;&#20989;&#25968;&#20043;&#38388;&#26144;&#23556;&#23398;&#20064;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#65292;&#20363;&#22914;&#26102;&#31354;&#36807;&#31243;&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#23427;&#20204;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#26032;&#20301;&#32622;&#36827;&#34892;&#22806;&#25512;&#21644;&#39044;&#27979;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#36827;&#34892;&#38646;&#23556;&#36229;&#20998;&#36776;&#29575;&#12290;&#31070;&#32463;&#36816;&#31639;&#31526;&#21487;&#20197;&#22686;&#24378;&#29978;&#33267;&#26367;&#20195;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#29616;&#26377;&#27169;&#25311;&#22120;&#65292;&#20363;&#22914;&#35745;&#31639;&#21147;&#23398;&#27969;&#20307;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific discovery and engineering design are currently limited by the time and cost of physical experiments, selected mostly through trial-and-error and intuition that require deep domain expertise. Numerical simulations present an alternative to physical experiments, but are usually infeasible for complex real-world domains due to the computational requirements of existing numerical methods. Artificial intelligence (AI) presents a potential paradigm shift through the development of fast data-driven surrogate models. In particular, an AI framework, known as neural operators, presents a principled framework for learning mappings between functions defined on continuous domains, e.g., spatiotemporal processes and partial differential equations (PDE). They can extrapolate and predict solutions at new locations unseen during training, i.e., perform zero-shot super-resolution. Neural operators can augment or even replace existing simulators in many applications, such as computational flui
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22855;&#24322;&#20540;&#20998;&#35299;&#31639;&#27861;&#22312;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#23545;&#31216;&#35774;&#32622;&#19979;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#27491;&#30830;&#24674;&#22797;&#25152;&#26377;&#30340;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2309.15322</link><description>&lt;p&gt;
&#20851;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#22312;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the Power of SVD in the Stochastic Block Model. (arXiv:2309.15322v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22855;&#24322;&#20540;&#20998;&#35299;&#31639;&#27861;&#22312;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#23545;&#31216;&#35774;&#32622;&#19979;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#27491;&#30830;&#24674;&#22797;&#25152;&#26377;&#30340;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36816;&#34892;&#32858;&#31867;&#31639;&#27861;&#20043;&#21069;&#65292;&#19968;&#31181;&#25913;&#21892;&#32858;&#31867;&#32467;&#26524;&#30340;&#24120;&#35265;&#21551;&#21457;&#24335;&#26041;&#27861;&#26159;&#24212;&#29992;&#38477;&#32500;&#25216;&#26415;&#12290;&#24050;&#32463;&#35266;&#23519;&#21040;&#65292;&#22522;&#20110;&#35889;&#30340;&#38477;&#32500;&#24037;&#20855;&#65292;&#22914;PCA&#25110;SVD&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#25552;&#39640;&#20102;&#32858;&#31867;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#29616;&#35937;&#34920;&#26126;&#65292;&#35889;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#20316;&#20026;&#38477;&#32500;&#24037;&#20855;&#65292;&#36824;&#21487;&#20197;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#23545;&#32858;&#31867;&#36807;&#31243;&#20316;&#20986;&#36129;&#29486;&#12290;&#29702;&#35299;&#32858;&#31867;&#38382;&#39064;&#20013;&#35889;&#27493;&#39588;&#30340;&#34892;&#20026;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#12290;&#20316;&#20026;&#36825;&#20010;&#26041;&#21521;&#30340;&#19968;&#20010;&#21021;&#22987;&#27493;&#39588;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22855;&#24322;&#20540;&#20998;&#35299;&#31639;&#27861;&#22312;&#38543;&#26426;&#22359;&#27169;&#22411; (SBM) &#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#23545;&#31216;&#35774;&#32622;&#20013;&#65292;&#26222;&#36890;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#31639;&#27861;&#21487;&#20197;&#27491;&#30830;&#24674;&#22797;&#25152;&#26377;&#30340;&#32858;&#31867;&#12290;&#36825;&#20010;&#32467;&#26524;&#22238;&#31572;&#20102;Van Vu&#22312;&#23545;&#31216;&#35774;&#32622;&#20013;&#25552;&#20986;&#30340;&#19968;&#20010;&#26410;&#35299;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A popular heuristic method for improving clustering results is to apply dimensionality reduction before running clustering algorithms. It has been observed that spectral-based dimensionality reduction tools, such as PCA or SVD, improve the performance of clustering algorithms in many applications. This phenomenon indicates that spectral method not only serves as a dimensionality reduction tool, but also contributes to the clustering procedure in some sense. It is an interesting question to understand the behavior of spectral steps in clustering problems.  As an initial step in this direction, this paper studies the power of vanilla-SVD algorithm in the stochastic block model (SBM). We show that, in the symmetric setting, vanilla-SVD algorithm recovers all clusters correctly. This result answers an open question posed by Van Vu (Combinatorics Probability and Computing, 2018) in the symmetric setting.
&lt;/p&gt;</description></item><item><title>DeepROCK&#26159;&#19968;&#31181;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#35823;&#24046;&#21487;&#25511;&#30340;&#20132;&#20114;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;knockoffs&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;DNN&#26550;&#26500;&#65292;&#21487;&#20197;&#21516;&#26102;&#25511;&#21046;&#34394;&#21457;&#29616;&#29575;(FDR)&#21644;&#26368;&#22823;&#21270;&#32479;&#35745;&#21151;&#25928;&#12290;</title><link>http://arxiv.org/abs/2309.15319</link><description>&lt;p&gt;
DeepROCK: &#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#35823;&#24046;&#21487;&#25511;&#30340;&#20132;&#20114;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DeepROCK: Error-controlled interaction detection in deep neural networks. (arXiv:2309.15319v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15319
&lt;/p&gt;
&lt;p&gt;
DeepROCK&#26159;&#19968;&#31181;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#35823;&#24046;&#21487;&#25511;&#30340;&#20132;&#20114;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;knockoffs&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;DNN&#26550;&#26500;&#65292;&#21487;&#20197;&#21516;&#26102;&#25511;&#21046;&#34394;&#21457;&#29616;&#29575;(FDR)&#21644;&#26368;&#22823;&#21270;&#32479;&#35745;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#30340;&#22797;&#26434;&#24615;&#20351;&#20854;&#24378;&#22823;&#65292;&#20294;&#20063;&#20351;&#20854;&#22312;&#35299;&#37322;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#22312;&#23481;&#24525;&#38169;&#35823;&#30340;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#35782;&#21035;&#24433;&#21709;&#39044;&#27979;&#32467;&#26524;&#30340;&#29305;&#24449;&#20132;&#20114;&#26469;&#25512;&#26029;DNNs&#30340;&#20869;&#37096;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#32570;&#20047;&#19968;&#31181;&#26377;&#31995;&#32479;&#30340;&#31574;&#30053;&#26469;&#20248;&#20808;&#32771;&#34385;&#20132;&#20114;&#20316;&#29992;&#24182;&#25511;&#21046;&#32622;&#20449;&#27700;&#24179;&#65292;&#20351;&#20854;&#38590;&#20197;&#22312;&#31185;&#23398;&#21457;&#29616;&#21644;&#20551;&#35774;&#39564;&#35777;&#20013;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#31216;&#20026;DeepROCK&#65292;&#36890;&#36807;&#20351;&#29992;knockoffs&#65288;&#19968;&#31181;&#35774;&#35745;&#25104;&#22312;&#32473;&#23450;&#29305;&#24449;&#38598;&#30340;&#26465;&#20214;&#19979;&#19982;&#21709;&#24212;&#21464;&#37327;&#30456;&#20114;&#29420;&#31435;&#30340;&#34394;&#25311;&#21464;&#37327;&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#32467;&#21512;&#19968;&#31181;&#26032;&#39062;&#30340;DNN&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#23618;&#25104;&#23545;&#32806;&#21512;&#23618;&#65292;DeepROCK&#21487;&#20197;&#21516;&#26102;&#25511;&#21046;&#34394;&#21457;&#29616;&#29575;(FDR)&#21644;&#26368;&#22823;&#21270;&#32479;&#35745;&#21151;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#19968;&#39033;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The complexity of deep neural networks (DNNs) makes them powerful but also makes them challenging to interpret, hindering their applicability in error-intolerant domains. Existing methods attempt to reason about the internal mechanism of DNNs by identifying feature interactions that influence prediction outcomes. However, such methods typically lack a systematic strategy to prioritize interactions while controlling confidence levels, making them difficult to apply in practice for scientific discovery and hypothesis validation. In this paper, we introduce a method, called DeepROCK, to address this limitation by using knockoffs, which are dummy variables that are designed to mimic the dependence structure of a given set of features while being conditionally independent of the response. Together with a novel DNN architecture involving a pairwise-coupling layer, DeepROCK jointly controls the false discovery rate (FDR) and maximizes statistical power. In addition, we identify a challenge in
&lt;/p&gt;</description></item><item><title>MAPTree&#26159;&#19968;&#31181;&#36890;&#36807;&#36125;&#21494;&#26031;&#26041;&#27861;&#23545;&#20915;&#31574;&#26641;&#36827;&#34892;&#24402;&#32435;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;AND/OR&#25628;&#32034;&#23454;&#29616;&#26368;&#22823;&#21518;&#39564;&#26641;&#30340;&#24674;&#22797;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;MAPTree&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#20197;&#26356;&#23567;&#30340;&#26641;&#26469;&#23454;&#29616;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;MAPTree&#36824;&#23637;&#31034;&#20986;&#26356;&#24378;&#30340;&#25239;&#22122;&#22768;&#33021;&#21147;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.15312</link><description>&lt;p&gt;
MAPTree: &#29992;&#36125;&#21494;&#26031;&#20915;&#31574;&#26641;&#20987;&#36133;&#8220;&#26368;&#20248;&#8221;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
MAPTree: Beating "Optimal" Decision Trees with Bayesian Decision Trees. (arXiv:2309.15312v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15312
&lt;/p&gt;
&lt;p&gt;
MAPTree&#26159;&#19968;&#31181;&#36890;&#36807;&#36125;&#21494;&#26031;&#26041;&#27861;&#23545;&#20915;&#31574;&#26641;&#36827;&#34892;&#24402;&#32435;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;AND/OR&#25628;&#32034;&#23454;&#29616;&#26368;&#22823;&#21518;&#39564;&#26641;&#30340;&#24674;&#22797;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;MAPTree&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#20197;&#26356;&#23567;&#30340;&#26641;&#26469;&#23454;&#29616;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;MAPTree&#36824;&#23637;&#31034;&#20986;&#26356;&#24378;&#30340;&#25239;&#22122;&#22768;&#33021;&#21147;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#20173;&#28982;&#26159;&#24403;&#20170;&#26368;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20043;&#19968;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#20854;&#24320;&#31665;&#21363;&#29992;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#26641;&#19978;&#30340;&#21518;&#39564;&#20998;&#24067;&#36827;&#34892;&#26368;&#22823;&#21518;&#39564;&#25512;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#20915;&#31574;&#26641;&#24402;&#32435;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#20915;&#31574;&#26641;&#30340;&#26368;&#22823;&#21518;&#39564;&#25512;&#29702;&#19982;AND/OR&#25628;&#32034;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#21033;&#29992;&#36825;&#19968;&#20851;&#32852;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MAPTree&#30340;AND/OR&#25628;&#32034;&#31639;&#27861;&#65292;&#33021;&#22815;&#24674;&#22797;&#20986;&#26368;&#22823;&#21518;&#39564;&#26641;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#19990;&#30028;&#22330;&#26223;&#20013;&#23637;&#31034;&#26368;&#22823;&#21518;&#39564;&#26641;&#30340;&#32463;&#39564;&#24615;&#33021;&#12290;&#22312;16&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#65292;MAPTree&#35201;&#20040;&#20248;&#20110;&#22522;&#20934;&#32447;&#65292;&#35201;&#20040;&#22312;&#24615;&#33021;&#30456;&#24403;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#23567;&#30340;&#26641;&#12290;&#22312;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#65292;MAPTree&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#24378;&#30340;&#25239;&#22122;&#22768;&#33021;&#21147;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;MAPTree&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#24555;&#22320;&#24674;&#22797;&#20986;&#26368;&#22823;&#21518;&#39564;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision trees remain one of the most popular machine learning models today, largely due to their out-of-the-box performance and interpretability. In this work, we present a Bayesian approach to decision tree induction via maximum a posteriori inference of a posterior distribution over trees. We first demonstrate a connection between maximum a posteriori inference of decision trees and AND/OR search. Using this connection, we propose an AND/OR search algorithm, dubbed MAPTree, which is able to recover the maximum a posteriori tree. Lastly, we demonstrate the empirical performance of the maximum a posteriori tree both on synthetic data and in real world settings. On 16 real world datasets, MAPTree either outperforms baselines or demonstrates comparable performance but with much smaller trees. On a synthetic dataset, MAPTree also demonstrates greater robustness to noise and better generalization than existing approaches. Finally, MAPTree recovers the maxiumum a posteriori tree faster tha
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;STERLING&#30340;&#33258;&#25105;&#30417;&#30563;&#22320;&#24418;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#32422;&#26463;&#30340;&#26426;&#22120;&#20154;&#32463;&#39564;&#23398;&#20064;&#26377;&#20851;&#22320;&#24418;&#30340;&#30456;&#20851;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#22320;&#24418;&#24863;&#30693;&#23548;&#33322;&#12290;</title><link>http://arxiv.org/abs/2309.15302</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#30340;&#26080;&#32422;&#26463;&#26426;&#22120;&#20154;&#32463;&#39564;&#20013;&#30340;&#22320;&#24418;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Terrain Representation Learning from Unconstrained Robot Experience. (arXiv:2309.15302v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15302
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;STERLING&#30340;&#33258;&#25105;&#30417;&#30563;&#22320;&#24418;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#32422;&#26463;&#30340;&#26426;&#22120;&#20154;&#32463;&#39564;&#23398;&#20064;&#26377;&#20851;&#22320;&#24418;&#30340;&#30456;&#20851;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#22320;&#24418;&#24863;&#30693;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#24418;&#35748;&#30693;&#65292;&#21363;&#36776;&#21035;&#21644;&#21306;&#20998;&#19981;&#21516;&#31867;&#22411;&#30340;&#22320;&#24418;&#65292;&#26159;&#26426;&#22120;&#20154;&#22312;&#33258;&#20027;&#36234;&#37326;&#23548;&#33322;&#20013;&#24517;&#39035;&#20855;&#22791;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;&#30446;&#21069;&#25552;&#20379;&#26426;&#22120;&#20154;&#36825;&#31181;&#35748;&#30693;&#33021;&#21147;&#30340;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#26114;&#36149;&#30340;&#26631;&#35760;&#25968;&#25454;&#25910;&#38598;&#65292;&#35201;&#20040;&#20381;&#36182;&#26080;&#27861;&#27867;&#21270;&#30340;&#24037;&#31243;&#29305;&#24449;&#21644;&#25104;&#26412;&#20989;&#25968;&#65292;&#25110;&#32773;&#20381;&#36182;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#30340;&#19987;&#23478;&#20154;&#31867;&#31034;&#33539;&#12290;&#20026;&#20102;&#20351;&#26426;&#22120;&#20154;&#19981;&#21463;&#36825;&#20123;&#38480;&#21046;&#22320;&#20855;&#22791;&#22320;&#24418;&#35748;&#30693;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#25105;&#30417;&#30563;&#22320;&#24418;&#34920;&#31034;&#23398;&#20064;&#65288;STERLING&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#22320;&#24418;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#20165;&#20381;&#36182;&#20110;&#26131;&#20110;&#25910;&#38598;&#30340;&#12289;&#26080;&#32422;&#26463;&#65288;&#20363;&#22914;&#65292;&#38750;&#19987;&#23478;&#30340;&#65289;&#21644;&#26410;&#26631;&#35760;&#30340;&#26426;&#22120;&#20154;&#32463;&#39564;&#65292;&#23545;&#25968;&#25454;&#37319;&#38598;&#27809;&#26377;&#39069;&#22806;&#30340;&#38480;&#21046;&#12290;STERLING&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#33258;&#25105;&#30417;&#30563;&#30446;&#26631;&#65292;&#36890;&#36807;&#38750;&#23545;&#27604;&#24230;&#34920;&#31034;&#23398;&#20064;&#26469;&#23398;&#20064;&#22320;&#24418;&#30456;&#20851;&#30340;&#34920;&#31034;&#20197;&#29992;&#20110;&#22320;&#24418;&#24863;&#30693;&#23548;&#33322;&#12290;&#36890;&#36807;&#23454;&#29289;&#26426;&#22120;&#20154;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Terrain awareness, i.e., the ability to identify and distinguish different types of terrain, is a critical ability that robots must have to succeed at autonomous off-road navigation. Current approaches that provide robots with this awareness either rely on labeled data which is expensive to collect, engineered features and cost functions that may not generalize, or expert human demonstrations which may not be available. Towards endowing robots with terrain awareness without these limitations, we introduce Self-supervised TErrain Representation LearnING (STERLING), a novel approach for learning terrain representations that relies solely on easy-to-collect, unconstrained (e.g., non-expert), and unlabelled robot experience, with no additional constraints on data collection. STERLING employs a novel multi-modal self-supervision objective through non-contrastive representation learning to learn relevant terrain representations for terrain-aware navigation. Through physical robot experiments
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25299;&#23637;&#20102;&#20984;&#20248;&#21270;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#27714;&#35299;&#21644;&#20989;&#25968;&#23545;&#25968;&#20985;&#20989;&#25968;&#26368;&#23567;&#21270;&#30340;&#31639;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#26827;&#30424;&#22238;&#24402;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#36923;&#36753;&#22238;&#24402;&#21040;&#38750;&#32447;&#24615;&#21487;&#20998;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.15298</link><description>&lt;p&gt;
&#36229;&#36234;&#23545;&#25968;&#20985;&#24615;&#65306;&#27714;&#35299;&#21644;&#20248;&#21270;&#21644;&#20989;&#25968;&#20043;&#21644;&#21462;&#36127;&#23545;&#25968;&#20043;&#26368;&#23567;&#21270;&#30340;&#29702;&#35770;&#21644;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Beyond Log-Concavity: Theory and Algorithm for Sum-Log-Concave Optimization. (arXiv:2309.15298v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25299;&#23637;&#20102;&#20984;&#20248;&#21270;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#27714;&#35299;&#21644;&#20989;&#25968;&#23545;&#25968;&#20985;&#20989;&#25968;&#26368;&#23567;&#21270;&#30340;&#31639;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#26827;&#30424;&#22238;&#24402;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#36923;&#36753;&#22238;&#24402;&#21040;&#38750;&#32447;&#24615;&#21487;&#20998;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#32463;&#20856;&#30340;&#20984;&#20248;&#21270;&#29702;&#35770;&#25299;&#23637;&#21040;&#23545;&#27714;&#35299;&#21644;&#20989;&#25968;&#20043;&#21644;&#21462;&#36127;&#23545;&#25968;&#30340;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#19978;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21644;&#20989;&#25968;&#23545;&#25968;&#20985;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#20989;&#25968;&#36890;&#24120;&#19981;&#26159;&#20984;&#20989;&#25968;&#65292;&#20294;&#20173;&#28982;&#28385;&#36275;&#24191;&#20041;&#20984;&#24615;&#19981;&#31561;&#24335;&#12290;&#36825;&#20123;&#19981;&#31561;&#24335;&#25581;&#31034;&#20102;&#25105;&#20204;&#31216;&#20043;&#20026;&#20132;&#21449;&#26799;&#24230;&#30340;&#26576;&#20010;&#21521;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#35813;&#21521;&#37327;&#36890;&#24120;&#19982;&#24120;&#35268;&#26799;&#24230;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20132;&#21449;&#26799;&#24230;&#19979;&#38477;&#65288;XGD&#65289;&#31639;&#27861;&#65292;&#23427;&#27839;&#30528;&#20132;&#21449;&#26799;&#24230;&#30340;&#30456;&#21453;&#26041;&#21521;&#31227;&#21160;&#65292;&#24182;&#25512;&#23548;&#20986;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#21644;&#20989;&#25968;&#23545;&#25968;&#20985;&#26694;&#26550;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25152;&#35859;&#30340;&#26827;&#30424;&#22238;&#24402;&#26041;&#27861;&#65292;&#23427;&#20381;&#36182;&#20110;&#21644;&#20989;&#25968;&#23545;&#25968;&#20985;&#20989;&#25968;&#12290;&#36825;&#31181;&#20998;&#31867;&#22120;&#23558;&#65288;&#22810;&#31867;&#65289;&#36923;&#36753;&#22238;&#24402;&#25193;&#23637;&#21040;&#38750;&#32447;&#24615;&#21487;&#20998;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20219;&#24847;&#25968;&#37327;&#30340;&#36229;&#24179;&#38754;&#26469;&#20998;&#21106;&#29305;&#24449;&#31354;&#38388;&#65292;&#21019;&#24314;&#19968;&#20010;&#26827;&#30424;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper extends the classic theory of convex optimization to the minimization of functions that are equal to the negated logarithm of what we term as a sum-log-concave function, i.e., a sum of log-concave functions. In particular, we show that such functions are in general not convex but still satisfy generalized convexity inequalities. These inequalities unveil the key importance of a certain vector that we call the cross-gradient and that is, in general, distinct from the usual gradient. Thus, we propose the Cross Gradient Descent (XGD) algorithm moving in the opposite direction of the cross-gradient and derive a convergence analysis. As an application of our sum-log-concave framework, we introduce the so-called checkered regression method relying on a sum-log-concave function. This classifier extends (multiclass) logistic regression to non-linearly separable problems since it is capable of tessellating the feature space by using any given number of hyperplanes, creating a checker
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#24773;&#20917;PINN&#26041;&#27861;&#22312;&#35745;&#31639;&#29983;&#29289;&#21307;&#23398;&#31649;&#36947;&#27969;&#21160;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#21442;&#25968;&#21270;&#19981;&#21516;&#20960;&#20309;&#24773;&#20917;&#65292;&#21487;&#20197;&#23454;&#26102;&#33719;&#21462;&#26410;&#35265;&#20960;&#20309;&#24418;&#29366;&#30340;&#32467;&#26524;&#65292;&#36827;&#32780;&#20248;&#21270;&#20102;&#20256;&#32479;CFD&#26041;&#27861;&#30340;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.15294</link><description>&lt;p&gt;
&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#31649;&#36947;&#27969;&#21160;&#30340;&#22810;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multiple Physics-Informed Neural Network for Biomedical Tube Flows. (arXiv:2309.15294v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#24773;&#20917;PINN&#26041;&#27861;&#22312;&#35745;&#31639;&#29983;&#29289;&#21307;&#23398;&#31649;&#36947;&#27969;&#21160;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#21442;&#25968;&#21270;&#19981;&#21516;&#20960;&#20309;&#24773;&#20917;&#65292;&#21487;&#20197;&#23454;&#26102;&#33719;&#21462;&#26410;&#35265;&#20960;&#20309;&#24418;&#29366;&#30340;&#32467;&#26524;&#65292;&#36827;&#32780;&#20248;&#21270;&#20102;&#20256;&#32479;CFD&#26041;&#27861;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31649;&#36947;&#20960;&#20309;&#30340;&#27969;&#20307;&#21147;&#23398;&#35745;&#31639;&#22312;&#29983;&#29289;&#21307;&#23398;&#34880;&#31649;&#21644;&#27668;&#36947;&#27969;&#20307;&#21147;&#23398;&#35780;&#20272;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#24050;&#32463;&#25104;&#20026;&#20256;&#32479;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#65288;CFD&#65289;&#26041;&#27861;&#30340;&#19968;&#20010;&#19981;&#38169;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#26222;&#36890;&#30340;PINN&#23545;&#20110;&#27599;&#31181;&#29305;&#23450;&#27969;&#21160;&#22330;&#26223;&#38656;&#35201;&#26356;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#22240;&#27492;&#26080;&#27861;&#35777;&#26126;&#20854;&#20027;&#27969;&#20351;&#29992;&#30340;&#21512;&#29702;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22810;&#24773;&#20917;PINN&#26041;&#27861;&#22312;&#35745;&#31639;&#29983;&#29289;&#21307;&#23398;&#31649;&#36947;&#27969;&#21160;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#21508;&#31181;&#19981;&#21516;&#20960;&#20309;&#24773;&#20917;&#32463;&#36807;&#21442;&#25968;&#21270;&#21644;&#39044;&#35757;&#32451;&#22312;PINN&#19978;&#65292;&#20174;&#32780;&#21487;&#20197;&#23454;&#26102;&#33719;&#21462;&#26410;&#35265;&#20960;&#20309;&#24418;&#29366;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;&#29702;&#24819;&#21270;&#30340;2D&#29421;&#31364;&#31649;&#36947;&#27969;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#26469;&#30830;&#23450;&#32593;&#32476;&#26550;&#26500;&#12289;&#31649;&#36947;&#29305;&#23450;&#21644;&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#20197;&#20248;&#21270;&#36825;&#19968;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fluid dynamics computations for tube-like geometries are important for biomedical evaluation of vascular and airway fluid dynamics. Physics-Informed Neural Networks (PINNs) have recently emerged as a good alternative to traditional computational fluid dynamics (CFD) methods. The vanilla PINN, however, requires much longer training time than the traditional CFD methods for each specific flow scenario and thus does not justify its mainstream use. Here, we explore the use of the multi-case PINN approach for calculating biomedical tube flows, where varied geometry cases are parameterized and pre-trained on the PINN, such that results for unseen geometries can be obtained in real time. Our objective is to identify network architecture, tube-specific, and regularization strategies that can optimize this, via experiments on a series of idealized 2D stenotic tube flows.
&lt;/p&gt;</description></item><item><title>&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#20811;&#26381;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#30456;&#20851;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#20195;&#29702;&#30340;&#32463;&#39564;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#22312;&#21508;&#31181;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.15293</link><description>&lt;p&gt;
&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Maximum Diffusion Reinforcement Learning. (arXiv:2309.15293v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15293
&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#20811;&#26381;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#30456;&#20851;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#20195;&#29702;&#30340;&#32463;&#39564;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#22312;&#21508;&#31181;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#26426;&#22120;&#23398;&#20064;&#37117;&#24314;&#31435;&#22312;&#25968;&#25454;&#29420;&#31435;&#19988;&#21516;&#20998;&#24067;&#30340;&#20551;&#35774;&#19978;&#12290;&#28982;&#32780;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24403;&#25968;&#25454;&#26159;&#20381;&#27425;&#20174;&#20195;&#29702;&#32463;&#39564;&#20013;&#25910;&#38598;&#32780;&#26469;&#26102;&#65292;&#36825;&#19968;&#20551;&#35774;&#36890;&#24120;&#19981;&#25104;&#31435;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32479;&#35745;&#21147;&#23398;&#20013;&#30340;&#36941;&#21382;&#36807;&#31243;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35299;&#32806;&#20195;&#29702;&#30340;&#32463;&#39564;&#65292;&#21487;&#35777;&#26126;&#22320;&#20351;&#20195;&#29702;&#22312;&#21333;&#27425;&#37096;&#32626;&#20013;&#33021;&#22815;&#25345;&#32493;&#23398;&#20064;&#65292;&#32780;&#19981;&#21463;&#21021;&#22987;&#21270;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25512;&#24191;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;&#26368;&#22823;&#29109;&#25216;&#26415;&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;&#27969;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#31283;&#23450;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25104;&#26524;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#29289;&#29702;&#23398;&#12289;&#23398;&#20064;&#21644;&#25511;&#21046;&#30340;&#20132;&#21449;&#39046;&#22495;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65288;&#22914;&#34892;&#36208;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65289;&#30340;&#36879;&#26126;&#21487;&#38752;&#20915;&#31574;&#25552;&#20379;&#20102;&#19968;&#26465;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The assumption that data are independent and identically distributed underpins all machine learning. When data are collected sequentially from agent experiences this assumption does not generally hold, as in reinforcement learning. Here, we derive a method that overcomes these limitations by exploiting the statistical mechanics of ergodic processes, which we term maximum diffusion reinforcement learning. By decorrelating agent experiences, our approach provably enables agents to learn continually in single-shot deployments regardless of how they are initialized. Moreover, we prove our approach generalizes well-known maximum entropy techniques, and show that it robustly exceeds state-of-the-art performance across popular benchmarks. Our results at the nexus of physics, learning, and control pave the way towards more transparent and reliable decision-making in reinforcement learning agents, such as locomoting robots and self-driving cars.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WildECG&#30340;&#39044;&#35757;&#32451;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#26080;&#22788;&#19981;&#22312;&#30340;&#24515;&#30005;&#22270;&#20013;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12290;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#30340;&#23454;&#38469;&#37326;&#22806;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#21644;&#19981;&#21516;&#19978;&#19979;&#25991;&#19979;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.15292</link><description>&lt;p&gt;
&#20174;&#26080;&#22788;&#19981;&#22312;&#30340;&#24515;&#30005;&#22270;&#20013;&#25193;&#23637;&#34920;&#31034;&#23398;&#20064;&#19982;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Representation Learning from Ubiquitous ECG with State-Space Models. (arXiv:2309.15292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WildECG&#30340;&#39044;&#35757;&#32451;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#26080;&#22788;&#19981;&#22312;&#30340;&#24515;&#30005;&#22270;&#20013;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12290;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#30340;&#23454;&#38469;&#37326;&#22806;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#21644;&#19981;&#21516;&#19978;&#19979;&#25991;&#19979;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#37326;&#22806;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#26080;&#22788;&#19981;&#22312;&#30340;&#20256;&#24863;&#20013;&#25552;&#21462;&#20449;&#24687;&#23545;&#20110;&#22686;&#24378;&#20154;&#31867;&#31119;&#31049;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20174;&#35786;&#26029;&#20020;&#24202;&#30149;&#30151;&#21644;&#27979;&#37327;&#21387;&#21147;&#21040;&#26500;&#24314;&#33258;&#36866;&#24212;&#20581;&#24247;&#20419;&#36827;&#25903;&#26550;&#12290;&#20294;&#26159;&#65292;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#30340;&#22823;&#37327;&#25968;&#25454;&#23545;&#20256;&#32479;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#34920;&#31034;&#23398;&#20064;&#20174;&#29983;&#29289;&#20449;&#21495;&#20013;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#39046;&#22495;&#65292;&#24471;&#30410;&#20110;&#35745;&#31639;&#24314;&#27169;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#20844;&#24320;&#20849;&#20139;&#25968;&#25454;&#24211;&#30340;&#20016;&#23500;&#12290;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#26159;&#36825;&#19968;&#39046;&#22495;&#20013;&#30740;&#31350;&#30340;&#20027;&#35201;&#27169;&#24577;&#65292;&#20854;&#22312;&#20581;&#24247;&#30417;&#27979;&#12289;&#21387;&#21147;&#21644;&#24773;&#24863;&#20272;&#35745;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#21463;&#21040;&#23567;&#35268;&#27169;&#21463;&#25511;&#25968;&#25454;&#25910;&#38598;&#21644;&#36807;&#21442;&#25968;&#21270;&#26550;&#26500;&#36873;&#25321;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;\textbf{WildECG}&#30340;&#39044;&#35757;&#32451;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;ECG&#20449;&#21495;&#20013;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#20197;&#26080;&#22788;&#19981;&#22312;&#25910;&#38598;&#30340;275,000&#20010;10&#31186;ECG&#35760;&#24405;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#26080;&#30417;&#30563;&#26041;&#24335;&#19979;&#35757;&#32451;&#35813;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ubiquitous sensing from wearable devices in the wild holds promise for enhancing human well-being, from diagnosing clinical conditions and measuring stress to building adaptive health promoting scaffolds. But the large volumes of data therein across heterogeneous contexts pose challenges for conventional supervised learning approaches. Representation Learning from biological signals is an emerging realm catalyzed by the recent advances in computational modeling and the abundance of publicly shared databases. The electrocardiogram (ECG) is the primary researched modality in this context, with applications in health monitoring, stress and affect estimation. Yet, most studies are limited by small-scale controlled data collection and over-parameterized architecture choices. We introduce \textbf{WildECG}, a pre-trained state-space model for representation learning from ECG signals. We train this model in a self-supervised manner with 275,000 10s ECG recordings collected in the wild and eval
&lt;/p&gt;</description></item><item><title>SEPT&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#22330;&#26223;&#34920;&#31034;&#23398;&#20064;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#25429;&#25417;&#36712;&#36857;&#30340;&#36816;&#21160;&#23398;&#29305;&#24449;&#12289;&#36947;&#36335;&#32593;&#32476;&#30340;&#31354;&#38388;&#32467;&#26500;&#20197;&#21450;&#36947;&#36335;&#21644;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#23454;&#29616;&#20102;&#22312;&#36816;&#21160;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15289</link><description>&lt;p&gt;
SEPT: &#20026;&#36816;&#21160;&#39044;&#27979;&#30340;&#39640;&#25928;&#22330;&#26223;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SEPT: Towards Efficient Scene Representation Learning for Motion Prediction. (arXiv:2309.15289v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15289
&lt;/p&gt;
&lt;p&gt;
SEPT&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#22330;&#26223;&#34920;&#31034;&#23398;&#20064;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#25429;&#25417;&#36712;&#36857;&#30340;&#36816;&#21160;&#23398;&#29305;&#24449;&#12289;&#36947;&#36335;&#32593;&#32476;&#30340;&#31354;&#38388;&#32467;&#26500;&#20197;&#21450;&#36947;&#36335;&#21644;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#23454;&#29616;&#20102;&#22312;&#36816;&#21160;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#39044;&#27979;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#22312;&#22797;&#26434;&#20132;&#36890;&#29615;&#22659;&#20013;&#23433;&#20840;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#25552;&#21462;&#20132;&#36890;&#20803;&#32032;&#20043;&#38388;&#30340;&#26377;&#25928;&#26102;&#31354;&#20851;&#31995;&#26159;&#20934;&#30830;&#39044;&#27979;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#21463;&#21040;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#21151;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;SEPT&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#24320;&#21457;&#22797;&#26434;&#20132;&#36890;&#22330;&#26223;&#20013;&#24378;&#22823;&#30340;&#26102;&#31354;&#29702;&#35299;&#33021;&#21147;&#30340;&#24314;&#27169;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#21040;&#22312;&#22330;&#26223;&#36755;&#20837;&#19978;&#36827;&#34892;&#19977;&#20010;&#25513;&#30721;&#37325;&#26500;&#24314;&#27169;&#20219;&#21153;&#65292;&#21253;&#25324;&#20195;&#29702;&#36335;&#24452;&#21644;&#36947;&#36335;&#32593;&#32476;&#65292;&#39044;&#35757;&#32451;&#22330;&#26223;&#32534;&#30721;&#22120;&#20197;&#25429;&#25417;&#36712;&#36857;&#30340;&#36816;&#21160;&#23398;&#29305;&#24449;&#65292;&#36947;&#36335;&#32593;&#32476;&#30340;&#31354;&#38388;&#32467;&#26500;&#20197;&#21450;&#36947;&#36335;&#21644;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#28982;&#21518;&#22312;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;SEPT&#22312;Argoverse 1&#21644;Argoverse&#19978;&#26080;&#38656;&#31934;&#24515;&#35774;&#35745;&#30340;&#26550;&#26500;&#25110;&#25163;&#21160;&#29305;&#24449;&#24037;&#31243;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motion prediction is crucial for autonomous vehicles to operate safely in complex traffic environments. Extracting effective spatiotemporal relationships among traffic elements is key to accurate forecasting. Inspired by the successful practice of pretrained large language models, this paper presents SEPT, a modeling framework that leverages self-supervised learning to develop powerful spatiotemporal understanding for complex traffic scenes. Specifically, our approach involves three masking-reconstruction modeling tasks on scene inputs including agents' trajectories and road network, pretraining the scene encoder to capture kinematics within trajectory, spatial structure of road network, and interactions among roads and agents. The pretrained encoder is then finetuned on the downstream forecasting task. Extensive experiments demonstrate that SEPT, without elaborate architectural design or manual feature engineering, achieves state-of-the-art performance on the Argoverse 1 and Argoverse
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#23637;&#31034;&#20102;&#22312;&#34892;&#21015;&#24335;&#26368;&#22823;&#21270;&#38382;&#39064;&#20013;&#65292;&#36138;&#24515;&#31639;&#27861;&#25552;&#20379;&#20102;&#20960;&#20046;&#26368;&#20248;&#30340;&#21487;&#32452;&#21512;&#26680;&#24515;&#38598;&#21512;&#65292;&#20855;&#26377;&#36817;&#20284;&#22240;&#23376;$O(k)^{3k}$&#12290;</title><link>http://arxiv.org/abs/2309.15286</link><description>&lt;p&gt;
&#21487;&#32452;&#21512;&#30340;&#26680;&#24515;&#38598;&#21512;&#29992;&#20110;&#34892;&#21015;&#24335;&#26368;&#22823;&#21270;&#65306;&#36138;&#24515;&#31639;&#27861;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;
&lt;/p&gt;
&lt;p&gt;
Composable Coresets for Determinant Maximization: Greedy is Almost Optimal. (arXiv:2309.15286v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15286
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#23637;&#31034;&#20102;&#22312;&#34892;&#21015;&#24335;&#26368;&#22823;&#21270;&#38382;&#39064;&#20013;&#65292;&#36138;&#24515;&#31639;&#27861;&#25552;&#20379;&#20102;&#20960;&#20046;&#26368;&#20248;&#30340;&#21487;&#32452;&#21512;&#26680;&#24515;&#38598;&#21512;&#65292;&#20855;&#26377;&#36817;&#20284;&#22240;&#23376;$O(k)^{3k}$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#32452;&#22312;$\mathbb{R}^d$&#20013;&#30340;$n$&#20010;&#21521;&#37327;&#65292;\emph{&#34892;&#21015;&#24335;&#26368;&#22823;&#21270;}&#38382;&#39064;&#30340;&#30446;&#26631;&#26159;&#36873;&#25321;&#20855;&#26377;&#26368;&#22823;&#20307;&#31215;&#30340;$k$&#20010;&#21521;&#37327;&#12290;&#34892;&#21015;&#24335;&#26368;&#22823;&#21270;&#26159;&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;&#65288;DPP&#65289;&#30340;MAP&#25512;&#26029;&#20219;&#21153;&#65292;&#26368;&#36817;&#22312;&#24314;&#27169;&#22810;&#26679;&#24615;&#26041;&#38754;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#38382;&#39064;&#24212;&#29992;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#65292;&#22240;&#27492;&#22312;&#30456;&#20851;&#30340;\textit{&#21487;&#32452;&#21512;&#30340;&#26680;&#24515;&#38598;&#21512;}&#35774;&#32622;&#20013;&#23545;&#35813;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#29305;&#21035;&#22320;&#65292;[Indyk-Mahabadi-OveisGharan-Rezaei--SODA'20, ICML'19]&#34920;&#26126;&#21487;&#20197;&#33719;&#24471;&#36817;&#20284;&#24230;&#20026;$\tilde O(k)^k$&#30340;&#38382;&#39064;&#30340;&#21487;&#32452;&#21512;&#26680;&#24515;&#38598;&#21512;&#65292;&#19988;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#36817;&#20284;&#20445;&#35777;$O(k)^{2k}$&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#36138;&#24515;&#31639;&#27861;&#20063;&#21487;&#20197;&#25552;&#20379;&#20855;&#26377;&#20960;&#20046;&#26368;&#20248;&#36817;&#20284;&#22240;&#23376;$O(k)^{3k}$&#30340;&#21487;&#32452;&#21512;&#26680;&#24515;&#38598;&#21512;&#65292;&#36825;&#25913;&#36827;&#20102;&#20808;&#21069;&#24050;&#30693;&#20445;&#35777;$C^{k^2}$&#30340;&#24773;&#20917;&#65292;&#24182;&#25903;&#25345;&#20808;&#21069;&#23454;&#39564;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a set of $n$ vectors in $\mathbb{R}^d$, the goal of the \emph{determinant maximization} problem is to pick $k$ vectors with the maximum volume. Determinant maximization is the MAP-inference task for determinantal point processes (DPP) and has recently received considerable attention for modeling diversity. As most applications for the problem use large amounts of data, this problem has been studied in the relevant \textit{composable coreset} setting. In particular, [Indyk-Mahabadi-OveisGharan-Rezaei--SODA'20, ICML'19] showed that one can get composable coresets with optimal approximation factor of $\tilde O(k)^k$ for the problem, and that a local search algorithm achieves an almost optimal approximation guarantee of $O(k)^{2k}$. In this work, we show that the widely-used Greedy algorithm also provides composable coresets with an almost optimal approximation factor of $O(k)^{3k}$, which improves over the previously known guarantee of $C^{k^2}$, and supports the prior experimental 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29289;&#29702;&#22686;&#24378;&#27531;&#24046;&#23398;&#20064;&#65288;PERL&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20132;&#36890;&#29366;&#24577;&#39044;&#27979;&#12290;PERL&#27169;&#22411;&#38598;&#25104;&#20102;&#29289;&#29702;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#23558;&#29289;&#29702;&#27169;&#22411;&#32467;&#26524;&#21644;&#39044;&#27979;&#27531;&#24046;&#20316;&#20026;&#20462;&#27491;&#30456;&#32467;&#21512;&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#19988;&#27604;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#35201;&#27714;&#26356;&#23569;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.15284</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20132;&#36890;&#29366;&#24577;&#39044;&#27979;&#30340;&#29289;&#29702;&#22686;&#24378;&#27531;&#24046;&#23398;&#20064;&#65288;PERL&#65289;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Physics Enhanced Residual Learning (PERL) Framework for Traffic State Prediction. (arXiv:2309.15284v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15284
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29289;&#29702;&#22686;&#24378;&#27531;&#24046;&#23398;&#20064;&#65288;PERL&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20132;&#36890;&#29366;&#24577;&#39044;&#27979;&#12290;PERL&#27169;&#22411;&#38598;&#25104;&#20102;&#29289;&#29702;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#23558;&#29289;&#29702;&#27169;&#22411;&#32467;&#26524;&#21644;&#39044;&#27979;&#27531;&#24046;&#20316;&#20026;&#20462;&#27491;&#30456;&#32467;&#21512;&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#19988;&#27604;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#35201;&#27714;&#26356;&#23569;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#20013;&#65292;&#29289;&#29702;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#26159;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#27599;&#31181;&#26041;&#27861;&#37117;&#23384;&#22312;&#33258;&#24049;&#30340;&#25361;&#25112;&#65306;&#29289;&#29702;&#27169;&#22411;&#22312;&#21487;&#39044;&#27979;&#24615;&#26041;&#38754;&#19981;&#36275;&#65292;&#32780;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#21017;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#38024;&#23545;&#36825;&#20123;&#24050;&#30830;&#23450;&#30340;&#32570;&#28857;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;&#29289;&#29702;&#22686;&#24378;&#27531;&#24046;&#23398;&#20064;&#65288;PERL&#65289;&#27169;&#22411;&#12290;PERL&#23558;&#29289;&#29702;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#20248;&#21183;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#29992;&#20110;&#20132;&#36890;&#29366;&#24577;&#39044;&#27979;&#12290;PERL&#21253;&#25324;&#19968;&#20010;&#29289;&#29702;&#27169;&#22411;&#21644;&#19968;&#20010;&#27531;&#24046;&#23398;&#20064;&#27169;&#22411;&#12290;&#23427;&#30340;&#39044;&#27979;&#32467;&#26524;&#26159;&#29289;&#29702;&#27169;&#22411;&#32467;&#26524;&#21644;&#39044;&#27979;&#27531;&#24046;&#30340;&#21644;&#20316;&#20026;&#23545;&#20854;&#30340;&#20462;&#27491;&#12290;PERL&#20445;&#30041;&#20102;&#29289;&#29702;&#27169;&#22411;&#22825;&#28982;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#30456;&#27604;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#20855;&#26377;&#36739;&#23567;&#30340;&#25968;&#25454;&#38656;&#27714;&#12290;&#25105;&#20204;&#20351;&#29992;&#23454;&#38469;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;PERL&#27169;&#22411;&#65292;&#20854;&#20013;&#20197;&#26234;&#33021;&#39550;&#39542;&#27169;&#22411;&#65288;IDM&#65289;&#20316;&#20026;&#20854;&#29289;&#29702;&#36710;&#36319;&#27169;&#22411;&#65292;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#29992;&#20110;&#23398;&#20064;&#27531;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In vehicle trajectory prediction, physics models and data-driven models are two predominant methodologies. However, each approach presents its own set of challenges: physics models fall short in predictability, while data-driven models lack interpretability. Addressing these identified shortcomings, this paper proposes a novel framework, the Physics-Enhanced Residual Learning (PERL) model. PERL integrates the strengths of physics-based and data-driven methods for traffic state prediction. PERL contains a physics model and a residual learning model. Its prediction is the sum of the physics model result and a predicted residual as a correction to it. It preserves the interpretability inherent to physics-based models and has reduced data requirements compared to data-driven methods. Experiments were conducted using a real-world vehicle trajectory dataset. We proposed a PERL model, with the Intelligent Driver Model (IDM) as its physics car-following model and Long Short-Term Memory (LSTM) 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23545;&#20808;&#21069;&#35266;&#23519;&#21040;&#20294;&#24403;&#21069;&#34987;&#36974;&#25377;&#30340;&#23545;&#35937;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#36716;&#25442;&#22120;&#20851;&#31995;&#21160;&#21147;&#23398;&#32534;&#30721;&#36712;&#36857;&#21382;&#21490;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.15278</link><description>&lt;p&gt;
&#30524;&#19981;&#35265;&#24515;&#19981;&#24565;&#65306;&#21033;&#29992;&#35270;&#39057;&#36319;&#36394;&#21551;&#29992;&#30340;&#35760;&#24518;&#27169;&#22411;&#23545;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#23545;&#35937;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Out of Sight, Still in Mind: Reasoning and Planning about Unobserved Objects with Video Tracking Enabled Memory Models. (arXiv:2309.15278v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23545;&#20808;&#21069;&#35266;&#23519;&#21040;&#20294;&#24403;&#21069;&#34987;&#36974;&#25377;&#30340;&#23545;&#35937;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#36716;&#25442;&#22120;&#20851;&#31995;&#21160;&#21147;&#23398;&#32534;&#30721;&#36712;&#36857;&#21382;&#21490;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#38656;&#35201;&#20855;&#26377;&#23545;&#20808;&#21069;&#35266;&#23519;&#21040;&#20294;&#24403;&#21069;&#34987;&#36974;&#25377;&#30340;&#23545;&#35937;&#30340;&#35760;&#24518;&#65292;&#20197;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#21487;&#38752;&#22320;&#24037;&#20316;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#38754;&#21521;&#23545;&#35937;&#30340;&#35760;&#24518;&#32534;&#30721;&#21040;&#22810;&#23545;&#35937;&#25805;&#32437;&#25512;&#29702;&#21644;&#35268;&#21010;&#26694;&#26550;&#20013;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DOOM&#21644;LOOM&#65292;&#23427;&#20204;&#21033;&#29992;&#36716;&#25442;&#22120;&#20851;&#31995;&#21160;&#21147;&#23398;&#26469;&#32534;&#30721;&#32473;&#23450;&#37096;&#20998;&#35270;&#28857;&#20113;&#21644;&#23545;&#35937;&#21457;&#29616;&#19982;&#36319;&#36394;&#24341;&#25806;&#30340;&#36712;&#36857;&#21382;&#21490;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25191;&#34892;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#22788;&#29702;&#34987;&#36974;&#25377;&#30340;&#23545;&#35937;&#65292;&#26032;&#20986;&#29616;&#30340;&#23545;&#35937;&#65292;&#20197;&#21450;&#29289;&#20307;&#37325;&#26032;&#20986;&#29616;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#23545;&#35937;&#21644;&#19981;&#21516;&#25968;&#37327;&#30340;&#24178;&#25200;&#21160;&#20316;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#38544;&#24335;&#35760;&#24518;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots need to have a memory of previously observed, but currently occluded objects to work reliably in realistic environments. We investigate the problem of encoding object-oriented memory into a multi-object manipulation reasoning and planning framework. We propose DOOM and LOOM, which leverage transformer relational dynamics to encode the history of trajectories given partial-view point clouds and an object discovery and tracking engine. Our approaches can perform multiple challenging tasks including reasoning with occluded objects, novel objects appearance, and object reappearance. Throughout our extensive simulation and real-world experiments, we find that our approaches perform well in terms of different numbers of objects and different numbers of distractor actions. Furthermore, we show our approaches outperform an implicit memory baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LBP-WHT&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#22312;&#21453;&#21521;&#20256;&#25773;&#20013;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#38656;&#27714;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;LBP-WHT&#26041;&#27861;&#36890;&#36807;&#23558;&#26799;&#24230;&#25237;&#24433;&#21040;&#20302;&#31209;&#31354;&#38388;&#24182;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#36866;&#24212;ViT&#25152;&#38656;&#30340;&#35745;&#31639;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LBP-WHT&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#19981;&#21516;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#33021;&#37117;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.15275</link><description>&lt;p&gt;
Vision Transformer&#36866;&#24212;&#30340;&#39640;&#25928;&#20302;&#31209;&#21453;&#21521;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Efficient Low-rank Backpropagation for Vision Transformer Adaptation. (arXiv:2309.15275v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LBP-WHT&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#22312;&#21453;&#21521;&#20256;&#25773;&#20013;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#38656;&#27714;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;LBP-WHT&#26041;&#27861;&#36890;&#36807;&#23558;&#26799;&#24230;&#25237;&#24433;&#21040;&#20302;&#31209;&#31354;&#38388;&#24182;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#36866;&#24212;ViT&#25152;&#38656;&#30340;&#35745;&#31639;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LBP-WHT&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#19981;&#21516;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#33021;&#37117;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#30340;&#35268;&#27169;&#19981;&#26029;&#22686;&#22823;&#65292;&#20351;&#24471;&#20026;&#29305;&#23450;&#38656;&#27714;&#23545;&#36825;&#20123;&#22823;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#24494;&#35843;&#25104;&#20026;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20010;&#38382;&#39064;&#28304;&#20110;ViT&#30340;&#32447;&#24615;&#23618;&#20013;&#38656;&#35201;&#30340;&#35745;&#31639;&#37327;&#22823;&#30340;&#30697;&#38453;&#20056;&#27861;&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Walsh-Hadamard&#21464;&#25442;&#30340;&#20302;&#31209;&#21453;&#21521;&#20256;&#25773;&#65288;LBP-WHT&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;LBP-WHT&#23558;&#26799;&#24230;&#25237;&#24433;&#21040;&#20302;&#31209;&#31354;&#38388;&#65292;&#24182;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#36825;&#31181;&#26041;&#27861;&#22823;&#22823;&#20943;&#23569;&#20102;&#36866;&#24212;ViT&#25152;&#38656;&#30340;&#35745;&#31639;&#37327;&#65292;&#22240;&#20026;&#20302;&#31209;&#31354;&#38388;&#20013;&#30340;&#30697;&#38453;&#20056;&#27861;&#36739;&#23569;&#21344;&#29992;&#36164;&#28304;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#19981;&#21516;&#27169;&#22411;&#65288;ViT&#12289;&#28151;&#21512;&#21367;&#31215;-ViT&#27169;&#22411;&#65289;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#23545;CIFAR100&#19978;&#30340;EfficientFormer-L1&#27169;&#22411;&#36827;&#34892;&#36866;&#24212;&#26102;&#65292;&#25105;&#20204;&#30340;LBP-WHT&#30456;&#27604;&#26631;&#20934;&#26041;&#27861;&#25552;&#39640;&#20102;10.4%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing scale of vision transformers (ViT) has made the efficient fine-tuning of these large models for specific needs a significant challenge in various applications. This issue originates from the computationally demanding matrix multiplications required during the backpropagation process through linear layers in ViT. In this paper, we tackle this problem by proposing a new Low-rank BackPropagation via Walsh-Hadamard Transformation (LBP-WHT) method. Intuitively, LBP-WHT projects the gradient into a low-rank space and carries out backpropagation. This approach substantially reduces the computation needed for adapting ViT, as matrix multiplication in the low-rank space is far less resource-intensive. We conduct extensive experiments with different models (ViT, hybrid convolution-ViT model) on multiple datasets to demonstrate the effectiveness of our method. For instance, when adapting an EfficientFormer-L1 model on CIFAR100, our LBP-WHT achieves 10.4% higher accuracy than the st
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65288;STARC&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#22870;&#21169;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#22635;&#34917;&#20102;&#22870;&#21169;&#23398;&#20064;&#29702;&#35770;&#22522;&#30784;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2309.15257</link><description>&lt;p&gt;
STARC:&#35780;&#20272;&#22870;&#21169;&#20989;&#25968;&#20043;&#38388;&#24046;&#24322;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
STARC: A General Framework For Quantifying Differences Between Reward Functions. (arXiv:2309.15257v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15257
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65288;STARC&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#22870;&#21169;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#22635;&#34917;&#20102;&#22870;&#21169;&#23398;&#20064;&#29702;&#35770;&#22522;&#30784;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#65292;&#38656;&#35201;&#23558;&#20219;&#21153;&#30340;&#30446;&#26631;&#24418;&#24335;&#21270;&#20026;&#22870;&#21169;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#26469;&#35828;&#65292;&#25163;&#21160;&#25351;&#23450;&#19968;&#20010;&#27704;&#19981;&#28608;&#21169;&#19981;&#33391;&#34892;&#20026;&#30340;&#22870;&#21169;&#20989;&#25968;&#38750;&#24120;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#22870;&#21169;&#23398;&#20064;&#31639;&#27861;&#26469;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#22870;&#21169;&#23398;&#20064;&#30340;&#29702;&#35770;&#22522;&#30784;&#23578;&#26410;&#23436;&#21892;&#12290;&#29305;&#21035;&#22320;&#65292;&#36890;&#24120;&#19981;&#30693;&#36947;&#32473;&#23450;&#30340;&#22870;&#21169;&#23398;&#20064;&#31639;&#27861;&#22312;&#39640;&#27010;&#29575;&#19979;&#26159;&#21542;&#20250;&#23398;&#20064;&#21040;&#19968;&#20010;&#23433;&#20840;&#20248;&#21270;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#36825;&#24847;&#21619;&#30528;&#22870;&#21169;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#24517;&#39035;&#32463;&#36807;&#32463;&#39564;&#35780;&#20272;&#65292;&#36825;&#26159;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#24456;&#38590;&#39044;&#27979;&#20854;&#22833;&#25928;&#27169;&#24335;&#12290;&#20854;&#20013;&#19968;&#20010;&#38459;&#30861;&#33719;&#24471;&#26356;&#22909;&#29702;&#35770;&#20445;&#35777;&#30340;&#38556;&#30861;&#26159;&#32570;&#20047;&#36739;&#22909;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#22870;&#21169;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to solve a task using reinforcement learning, it is necessary to first formalise the goal of that task as a reward function. However, for many real-world tasks, it is very difficult to manually specify a reward function that never incentivises undesirable behaviour. As a result, it is increasingly popular to use reward learning algorithms, which attempt to learn a reward function from data. However, the theoretical foundations of reward learning are not yet well-developed. In particular, it is typically not known when a given reward learning algorithm with high probability will learn a reward function that is safe to optimise. This means that reward learning algorithms generally must be evaluated empirically, which is expensive, and that their failure modes are difficult to predict in advance. One of the roadblocks to deriving better theoretical guarantees is the lack of good methods for quantifying the difference between reward functions. In this paper we provide a solution t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#39044;&#27979;NFL&#29699;&#21592;&#30340;&#34920;&#29616;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;&#35268;&#21010;&#25214;&#21040;&#26368;&#20339;&#38453;&#23481;&#26469;&#26368;&#22823;&#21270;&#22855;&#24187;&#20998;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#38453;&#23481;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15253</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#32447;&#24615;&#35268;&#21010;&#36827;&#34892;&#27599;&#26085;&#22855;&#24187;&#36275;&#29699;&#26368;&#20339;&#38453;&#23481;&#30340;&#26041;&#27861;&#21644;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Method and Validation for Optimal Lineup Creation for Daily Fantasy Football Using Machine Learning and Linear Programming. (arXiv:2309.15253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#39044;&#27979;NFL&#29699;&#21592;&#30340;&#34920;&#29616;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;&#35268;&#21010;&#25214;&#21040;&#26368;&#20339;&#38453;&#23481;&#26469;&#26368;&#22823;&#21270;&#22855;&#24187;&#20998;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#38453;&#23481;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#26085;&#22855;&#24187;&#20307;&#32946;&#26159;&#27599;&#21608;&#25110;&#27599;&#26085;&#30340;&#22312;&#32447;&#27604;&#36187;&#65292;&#20854;&#20013;&#20010;&#20154;&#36816;&#21160;&#21592;&#30340;&#30495;&#23454;&#27604;&#36187;&#34920;&#29616;&#34987;&#36716;&#25442;&#20026;&#22855;&#24187;&#20998;&#25968;&#65288;FPTS&#65289;&#12290;&#29992;&#25143;&#26681;&#25454;&#35774;&#23450;&#30340;&#29699;&#21592;&#24037;&#36164;&#19978;&#38480;&#36873;&#25321;&#38453;&#23481;&#20197;&#26368;&#22823;&#21270;&#20182;&#20204;&#30340;FPTS&#12290;&#26412;&#25991;&#38024;&#23545;&#20004;&#20010;&#37325;&#28857;&#36827;&#34892;&#30740;&#31350;&#65306;&#65288;1&#65289;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#39044;&#27979;NFL&#29699;&#21592;&#34920;&#29616;&#30340;&#26041;&#27861;&#30340;&#24320;&#21457;&#65292;&#65288;2&#65289;&#22312;&#35774;&#23450;&#30340;&#24037;&#36164;&#38480;&#21046;&#19979;&#30830;&#23450;&#26368;&#20339;&#38453;&#23481;&#20197;&#26368;&#22823;&#21270;FPTS&#12290;&#21019;&#24314;&#20102;&#19968;&#20010;&#30417;&#30563;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#65292;&#26681;&#25454;&#36807;&#21435;&#30340;&#29699;&#21592;&#34920;&#29616;&#65288;&#26412;&#24037;&#20316;&#20351;&#29992;2018NFL&#24120;&#35268;&#36187;&#23395;&#65289;&#39044;&#27979;FPTS&#65292;&#24182;&#23558;&#36825;&#20123;&#39044;&#27979;&#30340;FPTS&#29992;&#20110;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#20013;&#23547;&#25214;&#26368;&#20339;&#38453;&#23481;&#12290;&#23558;&#29983;&#25104;&#30340;&#38453;&#23481;&#30340;&#24615;&#33021;&#19982;&#38543;&#26426;&#29983;&#25104;&#30340;&#38453;&#23481;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#24179;&#22343;&#32780;&#35328;&#65292;&#26368;&#20339;&#38453;&#23481;&#30340;&#24615;&#33021;&#20248;&#20110;&#38543;&#26426;&#38453;&#23481;&#12290;&#28982;&#21518;&#23558;&#29983;&#25104;&#30340;&#38453;&#23481;&#19982;DraftKings&#29992;&#25143;&#30340;&#30495;&#23454;&#38453;&#23481;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#29983;&#25104;&#30340;&#38453;&#23481;&#36890;&#24120;&#25509;&#36817;31%&#30340;&#30495;&#23454;&#38453;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Daily fantasy sports (DFS) are weekly or daily online contests where real-game performances of individual players are converted to fantasy points (FPTS). Users select players for their lineup to maximize their FPTS within a set player salary cap. This paper focuses on (1) the development of a method to forecast NFL player performance under uncertainty and (2) determining an optimal lineup to maximize FPTS under a set salary limit. A supervised learning neural network was created and used to project FPTS based on past player performance (2018 NFL regular season for this work) prior to the upcoming week. These projected FPTS were used in a mixed integer linear program to find the optimal lineup. The performance of resultant lineups was compared to randomly-created lineups. On average, the optimal lineups outperformed the random lineups. The generated lineups were then compared to real-world lineups from users on DraftKings. The generated lineups generally fell in approximately the 31st p
&lt;/p&gt;</description></item><item><title>V2X-Lead&#26159;&#19968;&#31181;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#33258;&#21160;&#39550;&#39542;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#36710;&#32852;&#32593;&#36890;&#20449;&#26469;&#35299;&#20915;&#22478;&#24066;&#22797;&#26434;&#22330;&#26223;&#19979;&#30340;&#33258;&#21160;&#39550;&#39542;&#25361;&#25112;&#65292;&#36890;&#36807;&#34701;&#21512;&#28608;&#20809;&#38647;&#36798;&#21644;V2X&#36890;&#20449;&#25968;&#25454;&#65292;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#39550;&#39542;&#20195;&#29702;&#20197;&#23454;&#29616;&#26356;&#23433;&#20840;&#21644;&#39640;&#25928;&#30340;&#31359;&#36234;&#20132;&#21449;&#21475;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15252</link><description>&lt;p&gt;
V2X-Lead:&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#19982;&#36710;&#32852;&#32593;&#36890;&#20449;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
V2X-Lead: LiDAR-based End-to-End Autonomous Driving with Vehicle-to-Everything Communication Integration. (arXiv:2309.15252v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15252
&lt;/p&gt;
&lt;p&gt;
V2X-Lead&#26159;&#19968;&#31181;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#33258;&#21160;&#39550;&#39542;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#36710;&#32852;&#32593;&#36890;&#20449;&#26469;&#35299;&#20915;&#22478;&#24066;&#22797;&#26434;&#22330;&#26223;&#19979;&#30340;&#33258;&#21160;&#39550;&#39542;&#25361;&#25112;&#65292;&#36890;&#36807;&#34701;&#21512;&#28608;&#20809;&#38647;&#36798;&#21644;V2X&#36890;&#20449;&#25968;&#25454;&#65292;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#39550;&#39542;&#20195;&#29702;&#20197;&#23454;&#29616;&#26356;&#23433;&#20840;&#21644;&#39640;&#25928;&#30340;&#31359;&#36234;&#20132;&#21449;&#21475;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#26041;&#27861;&#65292;&#20854;&#20013;&#38598;&#25104;&#20102;&#36710;&#32852;&#32593;&#36890;&#20449;(V2X-Lead)&#65292;&#20197;&#35299;&#20915;&#22312;&#28151;&#21512;&#33258;&#21160;&#39550;&#39542;&#20132;&#36890;&#29615;&#22659;&#19979;&#23548;&#33322;&#38750;&#35268;&#33539;&#22478;&#24066;&#22330;&#26223;&#30340;&#25361;&#25112;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#34701;&#21512;&#36710;&#36733;&#28608;&#20809;&#38647;&#36798;&#20256;&#24863;&#22120;&#21644;V2X&#36890;&#20449;&#25968;&#25454;&#26469;&#22788;&#29702;&#19981;&#23436;&#32654;&#30340;&#37096;&#20998;&#35266;&#27979;&#12290;&#37319;&#29992;&#26080;&#27169;&#22411;&#21644;&#31163;&#31574;&#30053;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#31639;&#27861;&#26469;&#35757;&#32451;&#39550;&#39542;&#20195;&#29702;&#65292;&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;&#31934;&#24515;&#35774;&#35745;&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#23545;&#22810;&#26679;&#21270;&#39550;&#39542;&#20219;&#21153;&#21644;&#22330;&#26223;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#28151;&#21512;&#33258;&#21160;&#39550;&#39542;&#20132;&#36890;&#20013;&#31359;&#36234;&#38750;&#20449;&#21495;&#20132;&#21449;&#21475;&#30340;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#22312;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#65292;&#22914;&#29615;&#24418;&#20132;&#21449;&#21475;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a LiDAR-based end-to-end autonomous driving method with Vehicle-to-Everything (V2X) communication integration, termed V2X-Lead, to address the challenges of navigating unregulated urban scenarios under mixed-autonomy traffic conditions. The proposed method aims to handle imperfect partial observations by fusing the onboard LiDAR sensor and V2X communication data. A model-free and off-policy deep reinforcement learning (DRL) algorithm is employed to train the driving agent, which incorporates a carefully designed reward function and multi-task learning technique to enhance generalization across diverse driving tasks and scenarios. Experimental results demonstrate the effectiveness of the proposed approach in improving safety and efficiency in the task of traversing unsignalized intersections in mixed-autonomy traffic, and its generalizability to previously unseen scenarios, such as roundabouts. The integration of V2X communication offers a significant data source for
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SeMAnD&#30340;&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#26816;&#27979;&#22810;&#27169;&#24577;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#20013;&#30340;&#20960;&#20309;&#24322;&#24120;&#12290;&#35813;&#25216;&#26415;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#33258;&#30417;&#30563;&#35757;&#32451;&#30446;&#26631;&#23454;&#29616;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#34920;&#31034;&#21644;&#35782;&#21035;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#23616;&#37096;&#21464;&#21270;&#21644;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2309.15245</link><description>&lt;p&gt;
SeMAnD:&#33258;&#30417;&#30563;&#22810;&#27169;&#24335;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SeMAnD: Self-Supervised Anomaly Detection in Multimodal Geospatial Datasets. (arXiv:2309.15245v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15245
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SeMAnD&#30340;&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#26816;&#27979;&#22810;&#27169;&#24577;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#20013;&#30340;&#20960;&#20309;&#24322;&#24120;&#12290;&#35813;&#25216;&#26415;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#33258;&#30417;&#30563;&#35757;&#32451;&#30446;&#26631;&#23454;&#29616;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#34920;&#31034;&#21644;&#35782;&#21035;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#23616;&#37096;&#21464;&#21270;&#21644;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#65292;&#31216;&#20026;SeMAnD&#65292;&#29992;&#20110;&#26816;&#27979;&#22810;&#27169;&#24577;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#20013;&#30340;&#20960;&#20309;&#24322;&#24120;&#12290;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#21253;&#25324;&#33719;&#21462;&#21644;&#34893;&#29983;&#30340;&#24322;&#26500;&#25968;&#25454;&#27169;&#24577;&#65292;&#25105;&#20204;&#23558;&#20854;&#36716;&#25442;&#20026;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#12289;&#31867;&#20284;&#22270;&#20687;&#30340;&#24352;&#37327;&#65292;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#34920;&#31034;&#12289;&#23545;&#40784;&#21644;&#34701;&#21512;&#30340;&#25361;&#25112;&#12290;SeMAnD&#30001;&#20004;&#37096;&#20998;&#32452;&#25104;&#65306;&#65288;i&#65289;&#19968;&#31181;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#31216;&#20026;RandPolyAugment&#65292;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#30690;&#37327;&#20960;&#20309;&#22686;&#24378;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#20855;&#26377;&#19977;&#20010;&#32452;&#20214;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#30446;&#26631;&#65292;&#28608;&#21169;&#23398;&#20064;&#23545;&#19968;&#31181;&#27169;&#24577;&#20013;&#30340;&#23616;&#37096;&#21464;&#21270;&#20855;&#26377;&#21306;&#21035;&#24615;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#34920;&#31034;&#65292;&#36825;&#31181;&#21464;&#21270;&#22312;&#20854;&#20182;&#27169;&#24577;&#20013;&#27809;&#26377;&#24471;&#21040;&#35777;&#23454;&#12290;&#22312;&#22320;&#29702;&#31354;&#38388;&#24322;&#24120;&#26816;&#27979;&#20013;&#65292;&#26816;&#27979;&#23616;&#37096;&#32570;&#38519;&#33267;&#20851;&#37325;&#35201;&#65292;&#21363;&#20351;&#26159;&#23567;&#30340;&#24322;&#24120;&#65288;&#22914;&#31227;&#20301;&#12289;&#38169;&#35823;&#36830;&#25509;&#12289;&#30072;&#24418;&#25110;&#32570;&#22833;&#30340;&#22810;&#36793;&#24418;&#30690;&#37327;&#20960;&#20309;&#65292;&#22914;&#36947;&#36335;&#12289;&#24314;&#31569;&#29289;&#12289;&#22320;&#34920;&#35206;&#30422;&#31561;&#65289;&#20063;&#26159;&#26377;&#23475;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a Self-supervised Anomaly Detection technique, called SeMAnD, to detect geometric anomalies in Multimodal geospatial datasets. Geospatial data comprises of acquired and derived heterogeneous data modalities that we transform to semantically meaningful, image-like tensors to address the challenges of representation, alignment, and fusion of multimodal data. SeMAnD is comprised of (i) a simple data augmentation strategy, called RandPolyAugment, capable of generating diverse augmentations of vector geometries, and (ii) a self-supervised training objective with three components that incentivize learning representations of multimodal data that are discriminative to local changes in one modality which are not corroborated by the other modalities. Detecting local defects is crucial for geospatial anomaly detection where even small anomalies (e.g., shifted, incorrectly connected, malformed, or missing polygonal vector geometries like roads, buildings, landcover, etc.) are detrimenta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21516;&#20262;&#26494;&#24347;&#35757;&#32451;&#31639;&#27861;&#65288;HRTA&#65289;&#30340;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#26500;&#24314;&#26080;&#32541;&#36830;&#25509;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#21644;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#21516;&#20262;&#28608;&#27963;&#20989;&#25968;&#65292;&#24182;&#26494;&#24347;&#21516;&#20262;&#21442;&#25968;&#20197;&#22686;&#24378;&#35757;&#32451;&#31934;&#32454;&#21270;&#36807;&#31243;&#65292;&#21152;&#36895;&#20102;&#35757;&#32451;&#36807;&#31243;&#65292;&#22312;&#31070;&#32463;&#20999;&#32447;&#26680;&#65288;NTK&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.15244</link><description>&lt;p&gt;
&#26080;&#31351;&#23485;&#24230;&#20004;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#21516;&#20262;&#26494;&#24347;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Homotopy Relaxation Training Algorithms for Infinite-Width Two-Layer ReLU Neural Networks. (arXiv:2309.15244v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21516;&#20262;&#26494;&#24347;&#35757;&#32451;&#31639;&#27861;&#65288;HRTA&#65289;&#30340;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#26500;&#24314;&#26080;&#32541;&#36830;&#25509;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#21644;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#21516;&#20262;&#28608;&#27963;&#20989;&#25968;&#65292;&#24182;&#26494;&#24347;&#21516;&#20262;&#21442;&#25968;&#20197;&#22686;&#24378;&#35757;&#32451;&#31934;&#32454;&#21270;&#36807;&#31243;&#65292;&#21152;&#36895;&#20102;&#35757;&#32451;&#36807;&#31243;&#65292;&#22312;&#31070;&#32463;&#20999;&#32447;&#26680;&#65288;NTK&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#31216;&#20026;&#21516;&#20262;&#26494;&#24347;&#35757;&#32451;&#31639;&#27861;&#65288;HRTA&#65289;&#65292;&#26088;&#22312;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#32467;&#21512;&#20102;&#20004;&#20010;&#20851;&#38190;&#26426;&#21046;&#65306;&#19968;&#20010;&#26159;&#26500;&#24314;&#26080;&#32541;&#36830;&#25509;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#21644;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#21516;&#20262;&#28608;&#27963;&#20989;&#25968;&#65307;&#21478;&#19968;&#20010;&#25216;&#26415;&#26159;&#26494;&#24347;&#21516;&#20262;&#21442;&#25968;&#20197;&#22686;&#24378;&#35757;&#32451;&#31934;&#32454;&#21270;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#31070;&#32463;&#20999;&#32447;&#26680;&#65288;NTK&#65289;&#30340;&#32972;&#26223;&#19979;&#23545;&#36825;&#31181;&#26032;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#26174;&#33879;&#25913;&#36827;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#23588;&#20854;&#26159;&#22312;&#32771;&#34385;&#26356;&#22823;&#23485;&#24230;&#30340;&#32593;&#32476;&#26102;&#65292;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#35770;&#12290;&#36825;&#31181;&#25552;&#35758;&#30340;HRTA&#23637;&#31034;&#20102;&#23545;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel training approach called the Homotopy Relaxation Training Algorithm (HRTA), aimed at accelerating the training process in contrast to traditional methods. Our algorithm incorporates two key mechanisms: one involves building a homotopy activation function that seamlessly connects the linear activation function with the ReLU activation function; the other technique entails relaxing the homotopy parameter to enhance the training refinement process. We have conducted an in-depth analysis of this novel method within the context of the neural tangent kernel (NTK), revealing significantly improved convergence rates. Our experimental results, especially when considering networks with larger widths, validate the theoretical conclusions. This proposed HRTA exhibits the potential for other activation functions and deep neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#29305;&#26435;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#23398;&#29983;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15238</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#23398;&#20064;&#36890;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Using Generated Privileged Information by Text-to-Image Diffusion Models. (arXiv:2309.15238v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#29305;&#26435;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#23398;&#29983;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#26159;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#65292;&#20854;&#20013;&#25945;&#24072;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20174;&#39069;&#22806;&#30340;&#25968;&#25454;&#34920;&#31034;&#20013;&#33719;&#30410;&#65292;&#36825;&#34987;&#31216;&#20026;&#29305;&#26435;&#20449;&#24687;&#65292;&#24182;&#25913;&#21892;&#20102;&#19981;&#30475;&#21040;&#39069;&#22806;&#34920;&#31034;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#24456;&#23569;&#21487;&#33719;&#24471;&#29305;&#26435;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#20998;&#31867;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#20154;&#24037;&#29305;&#26435;&#20449;&#24687;&#12290;&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#21407;&#22987;&#25991;&#26412;&#26679;&#26412;&#36827;&#19968;&#27493;&#29992;&#20110;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#26550;&#26500;&#26469;&#35757;&#32451;&#22810;&#27169;&#24577;&#25945;&#24072;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#22810;&#27169;&#24577;&#25945;&#24072;&#30340;&#30693;&#35782;&#34987;&#33976;&#39311;&#21040;&#22522;&#20110;&#25991;&#26412;&#30340;&#65288;&#21333;&#27169;&#24577;&#65289;&#23398;&#29983;&#27169;&#22411;&#20013;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#29305;&#26435;&#20449;&#24687;&#65292;&#25105;&#20204;&#24341;&#23548;&#23398;&#29983;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#31216;&#20026;&#21033;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#65288;LUGPI&#65289;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning Using Privileged Information is a particular type of knowledge distillation where the teacher model benefits from an additional data representation during training, called privileged information, improving the student model, which does not see the extra representation. However, privileged information is rarely available in practice. To this end, we propose a text classification framework that harnesses text-to-image diffusion models to generate artificial privileged information. The generated images and the original text samples are further used to train multimodal teacher models based on state-of-the-art transformer-based architectures. Finally, the knowledge from multimodal teachers is distilled into a text-based (unimodal) student. Hence, by employing a generative model to produce synthetic data as privileged information, we guide the training of the student model. Our framework, called Learning Using Generated Privileged Information (LUGPI), yields noticeable performance g
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#20849;&#29616;&#32593;&#32476;&#25512;&#29702;&#31639;&#27861;&#30340;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#24494;&#29983;&#29289;&#32676;&#33853;&#21644;&#20854;&#30456;&#20114;&#20316;&#29992;&#65292;&#25552;&#20379;&#23545;&#21508;&#31181;&#30142;&#30149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.15225</link><description>&lt;p&gt;
&#29992;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#20849;&#29616;&#32593;&#32476;&#25512;&#29702;&#31639;&#27861;&#30340;&#20132;&#21449;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Cross-Validation for Training and Testing Co-occurrence Network Inference Algorithms. (arXiv:2309.15225v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15225
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#20849;&#29616;&#32593;&#32476;&#25512;&#29702;&#31639;&#27861;&#30340;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#24494;&#29983;&#29289;&#32676;&#33853;&#21644;&#20854;&#30456;&#20114;&#20316;&#29992;&#65292;&#25552;&#20379;&#23545;&#21508;&#31181;&#30142;&#30149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#29983;&#29289;&#23384;&#22312;&#20110;&#20960;&#20046;&#25152;&#26377;&#30340;&#29615;&#22659;&#20013;&#65292;&#21253;&#25324;&#22303;&#22756;&#12289;&#27700;&#12289;&#31354;&#27668;&#21644;&#20854;&#20182;&#29983;&#29289;&#20307;&#20869;&#65292;&#22914;&#21160;&#29289;&#21644;&#26893;&#29289;&#12290;&#34429;&#28982;&#19968;&#20123;&#24494;&#29983;&#29289;&#20250;&#24341;&#36215;&#30142;&#30149;&#65292;&#20294;&#22823;&#22810;&#25968;&#24494;&#29983;&#29289;&#22312;&#29983;&#29289;&#36807;&#31243;&#20013;&#36215;&#21040;&#24110;&#21161;&#20998;&#35299;&#12289;&#21457;&#37237;&#21644;&#20859;&#20998;&#24490;&#29615;&#30340;&#20316;&#29992;&#12290;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20197;&#30740;&#31350;&#21508;&#31181;&#29615;&#22659;&#20013;&#30340;&#24494;&#29983;&#29289;&#32676;&#33853;&#21450;&#20854;&#30456;&#20114;&#20316;&#29992;&#21644;&#20851;&#31995;&#22914;&#20309;&#20026;&#21508;&#31181;&#30142;&#30149;&#25552;&#20379;&#35265;&#35299;&#12290;&#20849;&#29616;&#32593;&#32476;&#25512;&#29702;&#31639;&#27861;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#24494;&#29983;&#29289;&#30340;&#22797;&#26434;&#20851;&#32852;&#65292;&#29305;&#21035;&#26159;&#32454;&#33740;&#12290;&#29616;&#26377;&#30340;&#32593;&#32476;&#25512;&#29702;&#31639;&#27861;&#37319;&#29992;&#30456;&#20851;&#24615;&#12289;&#27491;&#21017;&#21270;&#32447;&#24615;&#22238;&#24402;&#21644;&#26465;&#20214;&#20381;&#36182;&#31561;&#25216;&#26415;&#65292;&#36825;&#20123;&#25216;&#26415;&#20855;&#26377;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#65292;&#30830;&#23450;&#32593;&#32476;&#30340;&#31232;&#30095;&#31243;&#24230;&#12290;&#20197;&#24448;&#35780;&#20272;&#25512;&#26029;&#32593;&#32476;&#36136;&#37327;&#30340;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#22806;&#37096;&#25968;&#25454;&#21644;&#22312;&#23376;&#26679;&#26412;&#20013;&#32593;&#32476;&#30340;&#19968;&#33268;&#24615;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#37117;&#26377;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Microorganisms are found in almost every environment, including the soil, water, air, and inside other organisms, like animals and plants. While some microorganisms cause diseases, most of them help in biological processes such as decomposition, fermentation and nutrient cycling. A lot of research has gone into studying microbial communities in various environments and how their interactions and relationships can provide insights into various diseases. Co-occurrence network inference algorithms help us understand the complex associations of micro-organisms, especially bacteria. Existing network inference algorithms employ techniques such as correlation, regularized linear regression, and conditional dependence, which have different hyper-parameters that determine the sparsity of the network. Previous methods for evaluating the quality of the inferred network include using external data, and network consistency across sub-samples, both which have several drawbacks that limit their appli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35821;&#38899;&#21512;&#25104;&#30340;&#21327;&#21516;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#19982;&#29616;&#26377;&#23545;&#31574;&#27169;&#22411;&#21512;&#20316;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#35821;&#38899;&#30340;&#26377;&#25928;&#26816;&#27979;&#21644;&#27700;&#21360;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2309.15224</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#35821;&#38899;&#21512;&#25104;&#30340;&#21327;&#21516;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Collaborative Watermarking for Adversarial Speech Synthesis. (arXiv:2309.15224v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35821;&#38899;&#21512;&#25104;&#30340;&#21327;&#21516;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#19982;&#29616;&#26377;&#23545;&#31574;&#27169;&#22411;&#21512;&#20316;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#35821;&#38899;&#30340;&#26377;&#25928;&#26816;&#27979;&#21644;&#27700;&#21360;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#38899;&#21512;&#25104;&#30340;&#36827;&#23637;&#20351;&#24471;&#25216;&#26415;&#19981;&#20165;&#25509;&#36817;&#20154;&#31867;&#30340;&#33258;&#28982;&#24230;&#65292;&#32780;&#19988;&#33021;&#22815;&#20197;&#23569;&#37327;&#25968;&#25454;&#36827;&#34892;&#21363;&#26102;&#35821;&#38899;&#20811;&#38534;&#65292;&#24182;&#19988;&#20511;&#21161;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#39640;&#24230;&#21487;&#35775;&#38382;&#24615;&#12290;&#24403;&#28982;&#65292;&#29983;&#25104;&#20869;&#23481;&#30340;&#28508;&#22312;&#27867;&#28389;&#24341;&#36215;&#20102;&#23545;&#21512;&#25104;&#35821;&#38899;&#26816;&#27979;&#21644;&#27700;&#21360;&#25216;&#26415;&#30340;&#38656;&#27714;&#12290;&#26368;&#36817;&#65292;&#21512;&#25104;&#35821;&#38899;&#26816;&#27979;&#30340;&#30740;&#31350;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#33258;&#21160;&#35828;&#35805;&#20154;&#39564;&#35777;&#21644;&#27450;&#39575;&#23545;&#31574;&#25361;&#25112;&#65288;ASVspoof&#65289;&#19978;&#65292;&#35813;&#25361;&#25112;&#19987;&#27880;&#20110;&#34987;&#21160;&#23545;&#31574;&#12290;&#26412;&#25991;&#20174;&#21478;&#19968;&#35282;&#24230;&#20986;&#21457;&#65292;&#38024;&#23545;&#29983;&#25104;&#35821;&#38899;&#30340;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#35757;&#32451;&#26041;&#26696;&#65292;&#20197;&#22312;&#19981;&#24178;&#25200;&#20154;&#31867;&#21548;&#20247;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#36890;&#36807;&#21327;&#21516;&#26426;&#22120;&#26816;&#27979;&#21040;&#29983;&#25104;&#35821;&#38899;&#30340;&#27700;&#21360;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;ASVspoof 2021&#22522;&#32447;&#23545;&#31574;&#27169;&#22411;&#21512;&#20316;&#30340;HiFi-GAN&#31070;&#32463;&#22768;&#30721;&#22120;&#30340;&#21512;&#20316;&#35757;&#32451;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in neural speech synthesis have brought us technology that is not only close to human naturalness, but is also capable of instant voice cloning with little data, and is highly accessible with pre-trained models available. Naturally, the potential flood of generated content raises the need for synthetic speech detection and watermarking. Recently, considerable research effort in synthetic speech detection has been related to the Automatic Speaker Verification and Spoofing Countermeasure Challenge (ASVspoof), which focuses on passive countermeasures. This paper takes a complementary view to generated speech detection: a synthesis system should make an active effort to watermark the generated speech in a way that aids detection by another machine, but remains transparent to a human listener. We propose a collaborative training scheme for synthetic speech watermarking and show that a HiFi-GAN neural vocoder collaborating with the ASVspoof 2021 baseline countermeasure models consis
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#30340;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#31995;&#32479;&#65292;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#36755;&#20986;&#37325;&#35780;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;&#20302;&#31209;&#20998;&#35299;&#26041;&#27861;&#21644;&#20248;&#21270;&#25554;&#20837;&#30697;&#38453;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#23558;BERT&#27169;&#22411;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.15223</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#37325;&#35780;&#20998;&#30340;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#22312;&#21442;&#25968;&#39640;&#25928;&#30340;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Low-rank Adaptation of Large Language Model Rescoring for Parameter-Efficient Speech Recognition. (arXiv:2309.15223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15223
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#30340;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#31995;&#32479;&#65292;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#36755;&#20986;&#37325;&#35780;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;&#20302;&#31209;&#20998;&#35299;&#26041;&#27861;&#21644;&#20248;&#21270;&#25554;&#20837;&#30697;&#38453;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#23558;BERT&#27169;&#22411;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#31995;&#32479;&#65292;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#36755;&#20986;&#37325;&#35780;&#20998;&#12290;&#23613;&#31649;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#22312;&#31532;&#20108;&#27425;&#37325;&#35780;&#20998;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23558;&#39044;&#35757;&#32451;&#38454;&#27573;&#25193;&#23637;&#21644;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#21040;&#29305;&#23450;&#39046;&#22495;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#37325;&#35780;&#20998;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#39044;&#35757;&#32451;&#21442;&#25968;&#30340;&#19968;&#23567;&#37096;&#20998;&#65288;0.08%&#65289;&#26469;&#35757;&#32451;&#37325;&#35780;&#20998;&#30340;BERT&#27169;&#22411;&#24182;&#23558;&#20854;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#12290;&#36825;&#20123;&#25554;&#20837;&#30340;&#30697;&#38453;&#36890;&#36807;&#30456;&#20851;&#24615;&#27491;&#21017;&#21270;&#25439;&#22833;&#21644;&#21028;&#21035;&#24615;&#35757;&#32451;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#20302;&#31209;&#36866;&#24212;Rescore-BERT&#65288;LoRB&#65289;&#20307;&#31995;&#32467;&#26500;&#22312;LibriSpeech&#21644;&#20869;&#37096;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#65292;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;&#20102;5.4&#33267;3.6&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a neural language modeling system based on low-rank adaptation (LoRA) for speech recognition output rescoring. Although pretrained language models (LMs) like BERT have shown superior performance in second-pass rescoring, the high computational cost of scaling up the pretraining stage and adapting the pretrained models to specific domains limit their practical use in rescoring. Here we present a method based on low-rank decomposition to train a rescoring BERT model and adapt it to new domains using only a fraction (0.08%) of the pretrained parameters. These inserted matrices are optimized through a discriminative training objective along with a correlation-based regularization loss. The proposed low-rank adaptation Rescore-BERT (LoRB) architecture is evaluated on LibriSpeech and internal datasets with decreased training times by factors between 5.4 and 3.6.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23545;C&#32534;&#31243;&#20316;&#19994;&#36827;&#34892;&#33258;&#21160;&#35780;&#20998;&#30340;&#20998;&#26512;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#20195;&#30721;&#30340;&#36716;&#25442;&#22120;&#35789;&#23884;&#20837;&#27169;&#22411;CodeBERT&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#35813;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#32479;&#35745;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20043;&#38388;&#30340;&#23545;&#27604;&#12290;</title><link>http://arxiv.org/abs/2309.15216</link><description>&lt;p&gt;
&#20351;&#29992;CodeBERT&#21644;Random Forest Regressor&#36827;&#34892;&#33258;&#21160;&#35780;&#20998;&#30340;C&#32534;&#31243;&#20316;&#19994;
&lt;/p&gt;
&lt;p&gt;
Auto-grading C programming assignments with CodeBERT and Random Forest Regressor. (arXiv:2309.15216v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23545;C&#32534;&#31243;&#20316;&#19994;&#36827;&#34892;&#33258;&#21160;&#35780;&#20998;&#30340;&#20998;&#26512;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#20195;&#30721;&#30340;&#36716;&#25442;&#22120;&#35789;&#23884;&#20837;&#27169;&#22411;CodeBERT&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#35813;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#32479;&#35745;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20043;&#38388;&#30340;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#21160;&#35780;&#20998;&#32534;&#31243;&#20316;&#19994;&#22240;&#22797;&#26434;&#24615;&#21644;&#20027;&#35266;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#33258;&#21160;&#35780;&#20998;&#31616;&#21270;&#20102;&#20219;&#21153;&#12290;&#23427;&#23458;&#35266;&#22320;&#35780;&#20272;&#20195;&#30721;&#36136;&#37327;&#65292;&#26816;&#27979;&#38169;&#35823;&#65292;&#24182;&#20934;&#30830;&#22320;&#20998;&#37197;&#20998;&#25968;&#65292;&#20943;&#36731;&#20102;&#25945;&#24072;&#30340;&#36127;&#25285;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#39640;&#25928;&#21644;&#20844;&#24179;&#30340;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22238;&#24402;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#31561;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23545;C&#32534;&#31243;&#20316;&#19994;&#36827;&#34892;&#33258;&#21160;&#35780;&#20998;&#30340;&#20998;&#26512;&#12290;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#20195;&#30721;&#30340;&#36716;&#25442;&#22120;&#35789;&#23884;&#20837;&#27169;&#22411;CodeBERT&#65292;&#23558;&#25991;&#26412;&#20195;&#30721;&#36755;&#20837;&#36716;&#25442;&#20026;&#21521;&#37327;&#65292;&#28982;&#21518;&#23558;&#21521;&#37327;&#36755;&#20837;&#21040;&#20960;&#20010;&#27169;&#22411;&#20013;&#12290;&#27979;&#35797;&#32467;&#26524;&#35777;&#26126;&#20102;&#24314;&#35758;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#20026;1.89&#12290;&#26412;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#32479;&#35745;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20043;&#38388;&#30340;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grading coding assignments manually is challenging due to complexity and subjectivity. However, auto-grading with deep learning simplifies the task. It objectively assesses code quality, detects errors, and assigns marks accurately, reducing the burden on instructors while ensuring efficient and fair assessment. This study provides an analysis of auto-grading of the C programming assignments using machine learning and deep learning approaches like regression, convolutional neural networks (CNN) and long short-term memory (LSTM). Using a code-based transformer word embedding model called CodeBERT, the textual code inputs were transformed into vectors, and the vectors were then fed into several models. The testing findings demonstrated the efficacy of the suggested strategy with a root mean squared error (RMSE) of 1.89. The contrast between statistical methods and deep learning techniques is discussed in the study.
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#29992;&#20110;&#21315;&#31859;&#23610;&#24230;&#22823;&#27668;&#38477;&#23610;&#24230;&#30340;&#29983;&#25104;&#27531;&#24046;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22825;&#27668;&#21644;&#27668;&#20505;&#30340;&#29289;&#29702;&#28798;&#23475;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.15214</link><description>&lt;p&gt;
&#29992;&#20110;&#21315;&#31859;&#23610;&#24230;&#22823;&#27668;&#38477;&#23610;&#24230;&#30340;&#29983;&#25104;&#27531;&#24046;&#25193;&#25955;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Residual Diffusion Modeling for Km-scale Atmospheric Downscaling. (arXiv:2309.15214v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15214
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21315;&#31859;&#23610;&#24230;&#22823;&#27668;&#38477;&#23610;&#24230;&#30340;&#29983;&#25104;&#27531;&#24046;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22825;&#27668;&#21644;&#27668;&#20505;&#30340;&#29289;&#29702;&#28798;&#23475;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#20174;&#22825;&#27668;&#21644;&#27668;&#20505;&#20013;&#36827;&#34892;&#29289;&#29702;&#28798;&#23475;&#39044;&#27979;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#26114;&#36149;&#30340;&#21315;&#31859;&#23610;&#24230;&#25968;&#20540;&#27169;&#25311;&#65292;&#24182;&#39537;&#21160;&#36739;&#31895;&#20998;&#36776;&#29575;&#30340;&#20840;&#29699;&#36755;&#20837;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21315;&#31859;&#23610;&#24230;&#38477;&#23610;&#24230;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#35813;&#27169;&#22411;&#26159;&#20174;&#21488;&#28286;&#30340;&#21306;&#22495;&#39640;&#20998;&#36776;&#29575;&#22825;&#27668;&#27169;&#22411;&#35757;&#32451;&#24471;&#21040;&#30340;&#65292;&#24182;&#22312;ERA5&#20877;&#20998;&#26512;&#25968;&#25454;&#30340;&#22522;&#30784;&#19979;&#36827;&#34892;&#20102;&#26465;&#20214;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#38477;&#23610;&#24230;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22823;&#20998;&#36776;&#29575;&#27604;&#29575;&#65288;25km&#33267;2km&#65289;&#65292;&#19981;&#21516;&#23610;&#24230;&#19978;&#28041;&#21450;&#30340;&#19981;&#21516;&#29289;&#29702;&#36807;&#31243;&#20197;&#21450;&#22312;&#36755;&#20837;&#25968;&#25454;&#20013;&#19981;&#23384;&#22312;&#30340;&#39044;&#27979;&#36890;&#36947;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#20004;&#27493;&#30340;&#26041;&#27861;&#65288;ResDiff&#65289;&#65292;&#20854;&#20013;&#19968;&#20010;&#65288;UNet&#65289;&#22238;&#24402;&#22312;&#31532;&#19968;&#27493;&#39044;&#27979;&#24179;&#22343;&#20540;&#65292;&#32780;&#25193;&#25955;&#27169;&#22411;&#22312;&#31532;&#20108;&#27493;&#39044;&#27979;&#27531;&#24046;&#12290;\textit{ResDiff}&#22312;&#22359;&#22343;&#26041;&#26681;&#35823;&#24046;&#21644;CRPS&#24471;&#20998;&#19978;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#25216;&#33021;&#12290;ResDiff&#39044;&#27979;&#30340;&#20809;&#35889;&#21644;&#20998;&#24067;&#24544;&#23454;&#22320;&#24674;&#22797;&#20102;&#35843;&#33410;&#26377;&#23475;&#39118;&#21644;&#38632;&#30340;&#37325;&#35201;&#24130;&#24459;&#20851;&#31995;&#12290;&#32479;&#19968;&#30340;&#22825;&#27668;&#29616;&#35937;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The state of the art for physical hazard prediction from weather and climate requires expensive km-scale numerical simulations driven by coarser resolution global inputs. Here, a km-scale downscaling diffusion model is presented as a cost effective alternative. The model is trained from a regional high-resolution weather model over Taiwan, and conditioned on ERA5 reanalysis data. To address the downscaling uncertainties, large resolution ratios (25km to 2km), different physics involved at different scales and predict channels that are not in the input data, we employ a two-step approach (\textit{ResDiff}) where a (UNet) regression predicts the mean in the first step and a diffusion model predicts the residual in the second step. \textit{ResDiff} exhibits encouraging skill in bulk RMSE and CRPS scores. The predicted spectra and distributions from ResDiff faithfully recover important power law relationships regulating damaging wind and rain extremes. Case studies of coherent weather phen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#26102;&#23454;&#39564;&#65292;&#20197;&#27668;&#35937;&#39044;&#25253;&#20026;&#20363;&#65292;&#37327;&#21270;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#35745;&#31639;&#25104;&#26412;&#19982;&#39044;&#27979;&#35823;&#24046;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#24179;&#34913;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15207</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#24179;&#34913;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#35823;&#24046;&#65306;&#20174;&#27668;&#35937;&#39044;&#25253;&#30340;&#23454;&#26102;&#23454;&#39564;&#20013;&#33719;&#24471;&#30340;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Balancing Computational Efficiency and Forecast Error in Machine Learning-based Time-Series Forecasting: Insights from Live Experiments on Meteorological Nowcasting. (arXiv:2309.15207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#26102;&#23454;&#39564;&#65292;&#20197;&#27668;&#35937;&#39044;&#25253;&#20026;&#20363;&#65292;&#37327;&#21270;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#35745;&#31639;&#25104;&#26412;&#19982;&#39044;&#27979;&#35823;&#24046;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#24179;&#34913;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#23613;&#31649;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#25104;&#21151;&#24212;&#29992;&#65292;&#20294;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#35823;&#24046;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#26102;&#23454;&#39564;&#65292;&#20197;&#27668;&#35937;&#39044;&#25253;&#20316;&#20026;&#31034;&#20363;&#24212;&#29992;&#65292;&#37327;&#21270;&#35745;&#31639;&#25104;&#26412;&#19982;&#39044;&#27979;&#35823;&#24046;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#22810;&#31181;&#27969;&#34892;&#30340;&#22238;&#24402;&#25216;&#26415;&#65288;XGBoost&#65292;FC-MLP&#65292;Transformer&#21644;LSTM&#65289;&#23545;&#22810;&#22320;&#28857;&#30340;&#28201;&#24230;&#12289;&#39118;&#36895;&#21644;&#20113;&#37327;&#31561;&#19977;&#20010;&#21464;&#37327;&#36827;&#34892;&#22810;&#26102;&#27573;&#12289;&#30701;&#26399;&#39044;&#27979;&#12290;&#22312;&#20026;&#26399;5&#22825;&#30340;&#23454;&#26102;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20197;&#27599;&#23567;&#26102;&#25512;&#26029;144&#20010;&#27169;&#22411;&#30340;&#36895;&#24230;&#35757;&#32451;&#21644;&#25512;&#26029;&#20102;4000&#20010;&#25968;&#25454;&#28304;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#20102;&#20004;&#31181;&#35745;&#31639;&#25104;&#26412;&#26368;&#23567;&#21270;&#26041;&#27861;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#20998;&#21035;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#25968;&#25454;&#20943;&#23569;&#25216;&#26415;&#65288;Variance Horizon&#65289;&#21644;&#19968;&#31181;&#22522;&#20110;&#24615;&#33021;&#30340;&#27010;&#24565;&#28418;&#31227;&#26816;&#27979;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning for time-series forecasting remains a key area of research. Despite successful application of many machine learning techniques, relating computational efficiency to forecast error remains an under-explored domain. This paper addresses this topic through a series of real-time experiments to quantify the relationship between computational cost and forecast error using meteorological nowcasting as an example use-case. We employ a variety of popular regression techniques (XGBoost, FC-MLP, Transformer, and LSTM) for multi-horizon, short-term forecasting of three variables (temperature, wind speed, and cloud cover) for multiple locations. During a 5-day live experiment, 4000 data sources were streamed for training and inferencing 144 models per hour. These models were parameterized to explore forecast error for two computational cost minimization methods: a novel auto-adaptive data reduction technique (Variance Horizon) and a performance-based concept drift-detection mechani
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;ICML 2023&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#35201;&#27714;&#21442;&#19982;&#32773;&#22312;&#20004;&#20010;&#26376;&#20869;&#25552;&#20379;&#24320;&#28304;&#23454;&#29616;&#30340;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#65292;&#21560;&#24341;&#20102;28&#20010;&#21512;&#26684;&#30340;&#25552;&#20132;&#12290;</title><link>http://arxiv.org/abs/2309.15188</link><description>&lt;p&gt;
ICML 2023&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#25361;&#25112;&#65306;&#35774;&#35745;&#19982;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
ICML 2023 Topological Deep Learning Challenge : Design and Results. (arXiv:2309.15188v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ICML 2023&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#35201;&#27714;&#21442;&#19982;&#32773;&#22312;&#20004;&#20010;&#26376;&#20869;&#25552;&#20379;&#24320;&#28304;&#23454;&#29616;&#30340;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#65292;&#21560;&#24341;&#20102;28&#20010;&#21512;&#26684;&#30340;&#25552;&#20132;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ICML 2023&#25299;&#25169;&#19982;&#20960;&#20309;&#26426;&#22120;&#23398;&#20064;&#30740;&#35752;&#20250;&#20013;&#20030;&#21150;&#30340;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#35745;&#31639;&#25361;&#25112;&#12290;&#35813;&#27604;&#36187;&#35201;&#27714;&#21442;&#19982;&#32773;&#36890;&#36807;&#36129;&#29486;&#20110;python&#21253;TopoNetX&#65288;&#25968;&#25454;&#22788;&#29702;&#65289;&#21644;TopoModelX&#65288;&#28145;&#24230;&#23398;&#20064;&#65289;&#30340;&#24320;&#28304;&#23454;&#29616;&#26469;&#25552;&#20379;&#25991;&#29486;&#20013;&#30340;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#25361;&#25112;&#22312;&#20004;&#20010;&#26376;&#30340;&#26102;&#38388;&#20869;&#21560;&#24341;&#20102;28&#20010;&#21512;&#26684;&#30340;&#25552;&#20132;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#25361;&#25112;&#30340;&#35774;&#35745;&#24182;&#24635;&#32467;&#20102;&#20854;&#20027;&#35201;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the computational challenge on topological deep learning that was hosted within the ICML 2023 Workshop on Topology and Geometry in Machine Learning. The competition asked participants to provide open-source implementations of topological neural networks from the literature by contributing to the python packages TopoNetX (data processing) and TopoModelX (deep learning). The challenge attracted twenty-eight qualifying submissions in its two-month duration. This paper describes the design of the challenge and summarizes its main findings.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30417;&#27979;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22312;&#32447;&#26816;&#27979;&#26041;&#26696;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#22411;&#36136;&#37327;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#21487;&#20197;&#20943;&#23569;&#19981;&#24517;&#35201;&#30340;&#35686;&#25253;&#24182;&#20248;&#21270;&#23545;&#30456;&#20851;&#21464;&#21270;&#30340;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.15187</link><description>&lt;p&gt;
&#30417;&#27979;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65306;&#22312;&#32447;&#26816;&#27979;&#30456;&#20851;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Monitoring Machine Learning Models: Online Detection of Relevant Deviations. (arXiv:2309.15187v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30417;&#27979;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22312;&#32447;&#26816;&#27979;&#26041;&#26696;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#22411;&#36136;&#37327;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#21487;&#20197;&#20943;&#23569;&#19981;&#24517;&#35201;&#30340;&#35686;&#25253;&#24182;&#20248;&#21270;&#23545;&#30456;&#20851;&#21464;&#21270;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#21508;&#20010;&#39046;&#22495;&#20013;&#37325;&#35201;&#30340;&#24037;&#20855;&#65292;&#20294;&#20854;&#24615;&#33021;&#21487;&#33021;&#20250;&#38543;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#38477;&#20302;&#65292;&#21407;&#22240;&#26159;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#25110;&#20854;&#20182;&#22240;&#32032;&#12290;&#19968;&#26041;&#38754;&#65292;&#26816;&#27979;&#21644;&#35299;&#20915;&#36825;&#31181;&#38477;&#32423;&#23545;&#20110;&#20445;&#25345;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#32473;&#23450;&#36275;&#22815;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#26816;&#27979;&#21040;&#20219;&#24847;&#23567;&#30340;&#36136;&#37327;&#21464;&#21270;&#12290;&#30001;&#20110;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#25110;&#26367;&#25442;&#31561;&#24178;&#39044;&#25514;&#26045;&#21487;&#33021;&#20195;&#20215;&#39640;&#26114;&#65292;&#25105;&#20204;&#35748;&#20026;&#20165;&#24403;&#21464;&#21270;&#36229;&#36807;&#32473;&#23450;&#38408;&#20540;&#26102;&#25165;&#24212;&#35813;&#36827;&#34892;&#36825;&#20123;&#24178;&#39044;&#25514;&#26045;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#30417;&#27979;&#26041;&#26696;&#26469;&#26816;&#27979;&#36825;&#20123;&#30456;&#20851;&#21464;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#25152;&#27979;&#37327;&#27169;&#22411;&#36136;&#37327;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#26469;&#20943;&#23569;&#19981;&#24517;&#35201;&#30340;&#35686;&#25253;&#24182;&#20811;&#26381;&#22810;&#37325;&#27979;&#35797;&#38382;&#39064;&#12290;&#25991;&#20013;&#25552;&#20379;&#20102;&#19968;&#33268;&#24615;&#21644;&#25351;&#23450;&#28176;&#36817;&#27700;&#24179;&#30340;&#26465;&#20214;&#12290;&#20351;&#29992;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#30340;&#23454;&#35777;&#39564;&#35777;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26816;&#27979;&#27169;&#22411;&#36136;&#37327;&#30456;&#20851;&#21464;&#21270;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#65292;&#30456;&#27604;&#22522;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are essential tools in various domains, but their performance can degrade over time due to changes in data distribution or other factors. On one hand, detecting and addressing such degradations is crucial for maintaining the models' reliability. On the other hand, given enough data, any arbitrary small change of quality can be detected. As interventions, such as model re-training or replacement, can be expensive, we argue that they should only be carried out when changes exceed a given threshold. We propose a sequential monitoring scheme to detect these relevant changes. The proposed method reduces unnecessary alerts and overcomes the multiple testing problem by accounting for temporal dependence of the measured model quality. Conditions for consistency and specified asymptotic levels are provided. Empirical validation using simulated and real data demonstrates the superiority of our approach in detecting relevant changes in model quality compared to benchmark m
&lt;/p&gt;</description></item><item><title>&#22312;&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#20102;&#22312;&#23567;&#26679;&#26412;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20445;&#23432;&#24615;&#31639;&#27861;&#26469;&#32531;&#35299;&#27492;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20445;&#23432;&#30340;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#22312;&#24635;&#20307;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#22522;&#20934;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.15178</link><description>&lt;p&gt;
&#20445;&#23432;&#30340;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Conservative World Models. (arXiv:2309.15178v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15178
&lt;/p&gt;
&lt;p&gt;
&#22312;&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#20102;&#22312;&#23567;&#26679;&#26412;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20445;&#23432;&#24615;&#31639;&#27861;&#26469;&#32531;&#35299;&#27492;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20445;&#23432;&#30340;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#22312;&#24635;&#20307;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#22522;&#20934;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;&#25215;&#35834;&#22312;&#31163;&#32447;&#39044;&#35757;&#32451;&#38454;&#27573;&#21518;&#65292;&#25552;&#20379;&#33021;&#22815;&#22312;&#20219;&#20309;&#29615;&#22659;&#20013;&#25191;&#34892;&#20219;&#20309;&#20219;&#21153;&#30340;&#20195;&#29702;&#12290;&#21069;&#21521;-&#21518;&#21521;&#65288;FB&#65289;&#34920;&#31034;&#22312;&#36825;&#20010;&#29702;&#24819;&#30340;&#23454;&#29616;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21487;&#20197;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#36798;&#21040;&#29305;&#23450;&#20219;&#21153;&#20195;&#29702;&#30340;85%&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#23545;&#20110;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#30340;&#35775;&#38382;&#65292;&#32780;&#22823;&#22810;&#25968;&#30495;&#23454;&#38382;&#39064;&#26080;&#27861;&#26399;&#26395;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#35757;&#32451;&#38598;&#32570;&#20047;&#22810;&#26679;&#24615;&#30340;&#24773;&#20917;&#19979;FB&#24615;&#33021;&#22914;&#20309;&#38477;&#20302;&#65292;&#24182;&#36890;&#36807;&#20445;&#23432;&#24615;&#26469;&#20943;&#36731;&#36825;&#31181;&#24773;&#20917;&#65292;&#36825;&#26159;&#19968;&#20010;&#25104;&#29087;&#30340;&#31163;&#32447;RL&#31639;&#27861;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#39046;&#22495;&#21644;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23478;&#26063;&#65292;&#22312;&#24635;&#20307;&#19978;&#36798;&#21040;&#20102;150%&#30340;&#26222;&#36890;FB&#24615;&#33021;&#12290;&#26377;&#20123;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#20445;&#23432;&#30340;FB&#31639;&#27861;&#22312;&#27809;&#26377;&#35775;&#38382;&#22870;&#21169;&#26631;&#31614;&#19988;&#38656;&#35201;&#32500;&#25252;&#25152;&#26377;&#20219;&#21153;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#20248;&#20110;&#29305;&#23450;&#20219;&#21153;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot reinforcement learning (RL) promises to provide agents that can perform any task in an environment after an offline pre-training phase. Forward-backward (FB) representations represent remarkable progress towards this ideal, achieving 85% of the performance of task-specific agents in this setting. However, such performance is contingent on access to large and diverse datasets for pre-training, which cannot be expected for most real problems. Here, we explore how FB performance degrades when trained on small datasets that lack diversity, and mitigate it with conservatism, a well-established feature of performant offline RL algorithms. We evaluate our family of methods across various datasets, domains and tasks, reaching 150% of vanilla FB performance in aggregate. Somewhat surprisingly, conservative FB algorithms also outperform the task-specific baseline, despite lacking access to reward labels and being required to maintain policies for all tasks. Conservative FB algorithms p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#25513;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;STMAE&#65289;&#26469;&#25552;&#39640;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#21452;&#25513;&#34109;&#31574;&#30053;&#22788;&#29702;&#37096;&#20998;&#21487;&#35265;&#30340;MTS&#25968;&#25454;&#65292;&#21253;&#25324;&#31354;&#38388;&#25513;&#34109;&#21644;&#26102;&#38388;&#25513;&#34109;&#12290;</title><link>http://arxiv.org/abs/2309.15169</link><description>&lt;p&gt;
&#25581;&#31034;&#31354;&#38388;-&#26102;&#38388;&#25513;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Revealing the Power of Spatial-Temporal Masked Autoencoders in Multivariate Time Series Forecasting. (arXiv:2309.15169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15169
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#25513;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;STMAE&#65289;&#26469;&#25552;&#39640;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#21452;&#25513;&#34109;&#31574;&#30053;&#22788;&#29702;&#37096;&#20998;&#21487;&#35265;&#30340;MTS&#25968;&#25454;&#65292;&#21253;&#25324;&#31354;&#38388;&#25513;&#34109;&#21644;&#26102;&#38388;&#25513;&#34109;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#28041;&#21450;&#22522;&#20110;&#21382;&#21490;&#35266;&#27979;&#26469;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#24378;&#35843;&#24320;&#21457;&#33021;&#22815;&#26126;&#30830;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#21464;&#37327;&#20043;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#30340;&#22797;&#26434;&#31354;&#38388;-&#26102;&#38388;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31354;&#38388;-&#26102;&#38388;&#25513;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;STMAE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;MTS&#39044;&#27979;&#26694;&#26550;&#65292;&#21033;&#29992;&#25513;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#25552;&#39640;&#31354;&#38388;-&#26102;&#38388;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;STMAE&#21253;&#25324;&#20004;&#20010;&#23398;&#20064;&#38454;&#27573;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#12290;&#32534;&#30721;&#22120;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#25513;&#34109;&#31574;&#30053;&#22788;&#29702;&#37096;&#20998;&#21487;&#35265;&#30340;MTS&#25968;&#25454;&#65292;&#21253;&#25324;&#22522;&#20110;&#20559;&#32622;&#38543;&#26426;&#28216;&#36208;&#30340;&#31354;&#38388;&#25513;&#34109;&#21644;&#22522;&#20110;&#34917;&#19969;&#30340;&#26102;&#38388;&#25513;&#34109;&#12290;&#38543;&#21518;&#65292;&#35299;&#30721;&#22120;&#26088;&#22312;&#37325;&#26500;&#20004;&#20010;&#25513;&#34109;&#23545;&#24212;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate time series (MTS) forecasting involves predicting future time series data based on historical observations. Existing research primarily emphasizes the development of complex spatial-temporal models that capture spatial dependencies and temporal correlations among time series variables explicitly. However, recent advances have been impeded by challenges relating to data scarcity and model robustness. To address these issues, we propose Spatial-Temporal Masked Autoencoders (STMAE), an MTS forecasting framework that leverages masked autoencoders to enhance the performance of spatial-temporal baseline models. STMAE consists of two learning stages. In the pretraining stage, an encoder-decoder architecture is employed. The encoder processes the partially visible MTS data produced by a novel dual-masking strategy, including biased random walk-based spatial masking and patch-based temporal masking. Subsequently, the decoders aim to reconstruct the masked counterparts from both spa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;AI&#31639;&#27861;&#22312;&#30005;&#21160;&#27773;&#36710;&#33021;&#37327;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#35299;&#20915;&#21508;&#31181;&#25361;&#25112;&#21644;&#23454;&#29616;&#33021;&#37327;&#31649;&#29702;&#30340;&#26377;&#25928;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.15140</link><description>&lt;p&gt;
AI&#31639;&#27861;&#22312;&#30005;&#21160;&#27773;&#36710;&#33021;&#37327;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review on AI Algorithms for Energy Management in E-Mobility Services. (arXiv:2309.15140v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;AI&#31639;&#27861;&#22312;&#30005;&#21160;&#27773;&#36710;&#33021;&#37327;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#35299;&#20915;&#21508;&#31181;&#25361;&#25112;&#21644;&#23454;&#29616;&#33021;&#37327;&#31649;&#29702;&#30340;&#26377;&#25928;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21160;&#20986;&#34892;&#20316;&#20026;&#35299;&#20915;&#20132;&#36890;&#37096;&#38376;&#32039;&#36843;&#30340;&#29615;&#22659;&#21644;&#21487;&#25345;&#32493;&#24615;&#38382;&#39064;&#30340;&#20851;&#38190;&#35299;&#20915;&#26041;&#26696;&#24050;&#32463;&#23835;&#36215;&#12290;&#21270;&#30707;&#29123;&#26009;&#30340;&#28040;&#32791;&#12289;&#19981;&#26029;&#19978;&#21319;&#30340;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#20197;&#21450;&#23545;&#25239;&#27668;&#20505;&#21464;&#21270;&#30340;&#36843;&#20999;&#38656;&#27714;&#20984;&#26174;&#20102;&#21521;&#30005;&#21160;&#27773;&#36710;&#21457;&#23637;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#32034;&#20154;&#24037;&#26234;&#33021;&#22312;&#30005;&#21160;&#27773;&#36710;&#33021;&#37327;&#31649;&#29702;&#20013;&#38754;&#20020;&#30340;&#21508;&#31181;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#25361;&#25112;&#28041;&#21450;&#20851;&#38190;&#22240;&#32032;&#22914;&#32493;&#33322;&#28966;&#34385;&#12289;&#20805;&#30005;&#36895;&#29575;&#20248;&#21270;&#20197;&#21450;&#30005;&#21160;&#27773;&#36710;&#33021;&#37327;&#23384;&#20648;&#30340;&#23551;&#21629;&#12290;&#36890;&#36807;&#20998;&#26512;&#29616;&#26377;&#25991;&#29486;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#21644;&#23454;&#29616;&#30005;&#21160;&#27773;&#36710;&#33021;&#37327;&#31649;&#29702;&#26377;&#25928;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#24403;&#21069;&#30740;&#31350;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#27010;&#36848;&#65292;&#24182;&#25552;&#20986;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#25928;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
E-mobility, or electric mobility, has emerged as a pivotal solution to address pressing environmental and sustainability concerns in the transportation sector. The depletion of fossil fuels, escalating greenhouse gas emissions, and the imperative to combat climate change underscore the significance of transitioning to electric vehicles (EVs). This paper seeks to explore the potential of artificial intelligence (AI) in addressing various challenges related to effective energy management in e-mobility systems (EMS). These challenges encompass critical factors such as range anxiety, charge rate optimization, and the longevity of energy storage in EVs. By analyzing existing literature, we delve into the role that AI can play in tackling these challenges and enabling efficient energy management in EMS. Our objectives are twofold: to provide an overview of the current state-of-the-art in this research domain and propose effective avenues for future investigations. Through this analysis, we a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PINF&#65292;&#36825;&#26159;&#29289;&#29702;&#32422;&#26463;&#28145;&#24230;&#23398;&#20064;&#30340;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#30340;&#19968;&#31181;&#25193;&#23637;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#29305;&#24449;&#20540;&#27861;&#21017;&#21644;&#25193;&#25955;&#26469;&#35299;&#20915;&#39640;&#32500;&#26102;&#21464;&#21644;&#31283;&#24577;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.15139</link><description>&lt;p&gt;
PINF&#65306;&#29289;&#29702;&#32422;&#26463;&#28145;&#24230;&#23398;&#20064;&#30340;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;
&lt;/p&gt;
&lt;p&gt;
PINF: Continuous Normalizing Flows for Physics-Constrained Deep Learning. (arXiv:2309.15139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PINF&#65292;&#36825;&#26159;&#29289;&#29702;&#32422;&#26463;&#28145;&#24230;&#23398;&#20064;&#30340;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#30340;&#19968;&#31181;&#25193;&#23637;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#29305;&#24449;&#20540;&#27861;&#21017;&#21644;&#25193;&#25955;&#26469;&#35299;&#20915;&#39640;&#32500;&#26102;&#21464;&#21644;&#31283;&#24577;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#23494;&#24230;&#30340;&#26631;&#20934;&#21270;&#32422;&#26463;&#23545;&#20110;&#35299;&#20915;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26631;&#20934;&#21270;&#27969;&#26159;&#19968;&#31181;&#21487;&#36870;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#21464;&#37327;&#21464;&#25442;&#20844;&#24335;&#30830;&#20445;&#27010;&#29575;&#23494;&#24230;&#23432;&#24658;&#65292;&#24182;&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#30340;&#26032;&#39062;&#25193;&#23637;&#8212;&#8212;&#29289;&#29702;&#32422;&#26463;&#26631;&#20934;&#21270;&#27969;&#65288;PINF&#65289;&#65292;&#36890;&#36807;&#29305;&#24449;&#20540;&#27861;&#21017;&#32467;&#21512;&#25193;&#25955;&#65292;&#23454;&#29616;&#20102;&#26080;&#32593;&#26684;&#21644;&#26080;&#22240;&#26524;&#24615;&#30340;&#39640;&#25928;&#35299;&#20915;&#39640;&#32500;&#26102;&#21464;&#21644;&#31283;&#24577;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The normalization constraint on probability density poses a significant challenge for solving the Fokker-Planck equation. Normalizing Flow, an invertible generative model leverages the change of variables formula to ensure probability density conservation and enable the learning of complex data distributions. In this paper, we introduce Physics-Informed Normalizing Flows (PINF), a novel extension of continuous normalizing flows, incorporating diffusion through the method of characteristics. Our method, which is mesh-free and causality-free, can efficiently solve high dimensional time-dependent and steady-state Fokker-Planck equations.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#22312;&#21457;&#30005;&#31995;&#32479;&#39044;&#27979;&#36712;&#36857;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;adapt autoregressive networks&#21644;normalizing flows&#25429;&#25417;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#30456;&#27604;&#24403;&#21069;&#30340;copula-based&#32479;&#35745;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#22522;&#20110;&#27861;&#22269;TSO RTE&#39118;&#21147;&#39044;&#27979;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.15137</link><description>&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#29992;&#20110;&#21457;&#30005;&#31995;&#32479;&#39044;&#27979;&#36712;&#36857;&#30340;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Methods for Producing Forecast Trajectories in Power Systems. (arXiv:2309.15137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15137
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#22312;&#21457;&#30005;&#31995;&#32479;&#39044;&#27979;&#36712;&#36857;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;adapt autoregressive networks&#21644;normalizing flows&#25429;&#25417;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#30456;&#27604;&#24403;&#21069;&#30340;copula-based&#32479;&#35745;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#22522;&#20110;&#27861;&#22269;TSO RTE&#39118;&#21147;&#39044;&#27979;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21487;&#20877;&#29983;&#33021;&#28304;&#22312;&#30005;&#21147;&#28151;&#21512;&#20013;&#30340;&#25193;&#24352;&#65292;&#30005;&#32593;&#30340;&#21464;&#21270;&#24615;&#23558;&#22686;&#21152;&#65292;&#22240;&#27492;&#38656;&#35201;&#21152;&#24378;&#31995;&#32479;&#20197;&#20445;&#35777;&#20854;&#23433;&#20840;&#24615;&#12290;&#22240;&#27492;&#65292;&#36755;&#30005;&#31995;&#32479;&#36816;&#33829;&#21830;&#24517;&#39035;&#36827;&#34892;&#20998;&#26512;&#65292;&#27169;&#25311;&#26410;&#26469;&#21457;&#30005;&#31995;&#32479;&#30340;&#36816;&#34892;&#24773;&#20917;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#27169;&#25311;&#32467;&#26524;&#34987;&#29992;&#20316;&#20915;&#31574;&#36807;&#31243;&#30340;&#36755;&#20837;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#29983;&#25104;&#33021;&#28304;&#20135;&#37327;&#21644;&#36127;&#33655;&#39044;&#27979;&#36712;&#36857;&#12290;&#20026;&#20102;&#25429;&#25417;&#36825;&#20123;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#33258;&#22238;&#24402;&#32593;&#32476;&#21644;&#24402;&#19968;&#21270;&#27969;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#23545;&#27604;&#24403;&#21069;&#22522;&#20110;copula&#30340;&#32479;&#35745;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23545;&#27861;&#22269;&#36755;&#30005;&#31995;&#32479;&#36816;&#33829;&#21830;RTE&#30340;&#39118;&#21147;&#39044;&#27979;&#25968;&#25454;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#20351;&#29992;&#29305;&#23450;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#35780;&#20272;&#25351;&#26631;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the expansion of renewables in the electricity mix, power grid variability will increase, hence a need to robustify the system to guarantee its security. Therefore, Transport System Operators (TSOs) must conduct analyses to simulate the future functioning of power systems. Then, these simulations are used as inputs in decision-making processes. In this context, we investigate using deep learning models to generate energy production and load forecast trajectories. To capture the spatiotemporal correlations in these multivariate time series, we adapt autoregressive networks and normalizing flows, demonstrating their effectiveness against the current copula-based statistical approach. We conduct extensive experiments on the French TSO RTE wind forecast data and compare the different models with \textit{ad hoc} evaluation metrics for time series generation.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#24230;&#36830;&#32493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#36807;&#28388;&#32467;&#26500;&#34701;&#21512;&#65288;CCMVC-FSF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35270;&#35282;&#32858;&#31867;&#22312;&#23454;&#26102;&#25968;&#25454;&#25910;&#38598;&#20013;&#30340;&#22256;&#38590;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#38450;&#27490;&#20808;&#21069;&#30693;&#35782;&#36951;&#24536;&#21644;&#21033;&#29992;&#25968;&#25454;&#30456;&#20851;&#24615;&#25351;&#23548;&#26032;&#35270;&#22270;&#30340;&#32858;&#31867;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.15135</link><description>&lt;p&gt;
&#23545;&#27604;&#24230;&#36830;&#32493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#36807;&#28388;&#32467;&#26500;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Contrastive Continual Multi-view Clustering with Filtered Structural Fusion. (arXiv:2309.15135v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15135
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#24230;&#36830;&#32493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#36807;&#28388;&#32467;&#26500;&#34701;&#21512;&#65288;CCMVC-FSF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35270;&#35282;&#32858;&#31867;&#22312;&#23454;&#26102;&#25968;&#25454;&#25910;&#38598;&#20013;&#30340;&#22256;&#38590;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#38450;&#27490;&#20808;&#21069;&#30693;&#35782;&#36951;&#24536;&#21644;&#21033;&#29992;&#25968;&#25454;&#30456;&#20851;&#24615;&#25351;&#23548;&#26032;&#35270;&#22270;&#30340;&#32858;&#31867;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#32858;&#31867;&#36866;&#29992;&#20110;&#20808;&#21069;&#25910;&#38598;&#35270;&#22270;&#24182;&#25552;&#21462;&#19968;&#33268;&#21644;&#20114;&#34917;&#20449;&#24687;&#30340;&#24212;&#29992;&#65292;&#20294;&#24573;&#30053;&#20102;&#25968;&#25454;&#35270;&#22270;&#25353;&#39034;&#24207;&#25910;&#38598;&#30340;&#23454;&#26102;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#24230;&#36830;&#32493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#36807;&#28388;&#32467;&#26500;&#34701;&#21512;&#65288;CCMVC-FSF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#20808;&#21069;&#30693;&#35782;&#36951;&#24536;&#21644;&#26032;&#35270;&#22270;&#32858;&#31867;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view clustering thrives in applications where views are collected in advance by extracting consistent and complementary information among views. However, it overlooks scenarios where data views are collected sequentially, i.e., real-time data. Due to privacy issues or memory burden, previous views are not available with time in these situations. Some methods are proposed to handle it but are trapped in a stability-plasticity dilemma. In specific, these methods undergo a catastrophic forgetting of prior knowledge when a new view is attained. Such a catastrophic forgetting problem (CFP) would cause the consistent and complementary information hard to get and affect the clustering performance. To tackle this, we propose a novel method termed Contrastive Continual Multi-view Clustering with Filtered Structural Fusion (CCMVC-FSF). Precisely, considering that data correlations play a vital role in clustering and prior knowledge ought to guide the clustering process of a new view, we de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27604;&#29305;&#24065;&#30340;&#26089;&#26399;&#24694;&#24847;&#26816;&#27979;&#30340;&#24847;&#22270;&#30417;&#25511;&#31995;&#32479;&#65292;&#36890;&#36807;&#23450;&#20041;&#36164;&#20135;&#36716;&#31227;&#36335;&#24452;&#21644;&#25552;&#21462;&#29366;&#24577;&#21644;&#34892;&#21160;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#24694;&#24847;&#31867;&#22411;&#30340;&#26816;&#27979;&#21644;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.15133</link><description>&lt;p&gt;
&#20174;&#36164;&#20135;&#27969;&#21040;&#29366;&#24577;&#12289;&#34892;&#21160;&#21644;&#24847;&#22270;&#30340;&#21457;&#29616;&#65306;&#21152;&#23494;&#36135;&#24065;&#20013;&#30340;&#26089;&#26399;&#24694;&#24847;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
From Asset Flow to Status, Action and Intention Discovery: Early Malice Detection in Cryptocurrency. (arXiv:2309.15133v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27604;&#29305;&#24065;&#30340;&#26089;&#26399;&#24694;&#24847;&#26816;&#27979;&#30340;&#24847;&#22270;&#30417;&#25511;&#31995;&#32479;&#65292;&#36890;&#36807;&#23450;&#20041;&#36164;&#20135;&#36716;&#31227;&#36335;&#24452;&#21644;&#25552;&#21462;&#29366;&#24577;&#21644;&#34892;&#21160;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#24694;&#24847;&#31867;&#22411;&#30340;&#26816;&#27979;&#21644;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#20027;&#20307;&#30340;&#20266;&#21311;&#21517;&#24615;&#36136;&#65292;&#21152;&#23494;&#36135;&#24065;&#24448;&#24448;&#27604;&#20256;&#32479;&#37329;&#34701;&#36164;&#20135;&#26356;&#23481;&#26131;&#21463;&#21040;&#38750;&#27861;&#27963;&#21160;&#30340;&#24433;&#21709;&#12290;&#29702;&#24819;&#30340;&#26816;&#27979;&#27169;&#22411;&#24212;&#35813;&#28385;&#36275;&#26089;&#26399;&#26816;&#27979;&#12289;&#33391;&#22909;&#21487;&#35299;&#37322;&#24615;&#21644;&#36866;&#29992;&#20110;&#21508;&#31181;&#38750;&#27861;&#27963;&#21160;&#30340;&#19977;&#20010;&#20851;&#38190;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#28385;&#36275;&#25152;&#26377;&#36825;&#20123;&#35201;&#27714;&#65292;&#22240;&#20026;&#23427;&#20204;&#22823;&#22810;&#20381;&#36182;&#20110;&#28145;&#24230;&#23398;&#20064;&#32780;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#21482;&#36866;&#29992;&#20110;&#29305;&#23450;&#38750;&#27861;&#31867;&#22411;&#30340;&#22238;&#39038;&#24615;&#20998;&#26512;&#12290;&#20026;&#20102;&#35299;&#20915;&#25152;&#26377;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27604;&#29305;&#24065; (BTC) &#30340;&#26089;&#26399;&#24694;&#24847;&#26816;&#27979;&#30340;&#24847;&#22270;&#30417;&#25511;&#31995;&#32479;&#65292;&#20854;&#20013;&#26576;&#20010;&#22320;&#22336;&#30340;&#38142;&#19978;&#35760;&#24405;&#25968;&#25454;&#27604;&#20854;&#20182;&#21152;&#23494;&#36135;&#24065;&#24179;&#21488;&#26356;&#31232;&#32570;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#22522;&#20110;&#20915;&#31574;&#26641;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#34917;&#20805;&#65288;DT-SC&#65289;&#26469;&#23450;&#20041;&#36164;&#20135;&#36716;&#31227;&#36335;&#24452;&#65292;&#20026;&#19981;&#21516;&#30340;&#24694;&#24847;&#31867;&#22411;&#26500;&#24314;&#19981;&#21516;&#30340;&#29305;&#24449;&#38598;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#29366;&#24577;/&#34892;&#21160;&#25552;&#26696;&#27169;&#22359;&#65288;S/A-PM&#65289;&#33719;&#21462;&#21487;&#33021;&#30340;&#29366;&#24577;&#21644;&#34892;&#21160;&#65292;&#36827;&#19968;&#27493;&#21457;&#29616;&#24694;&#24847;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cryptocurrency has been subject to illicit activities probably more often than traditional financial assets due to the pseudo-anonymous nature of its transacting entities. An ideal detection model is expected to achieve all three critical properties of (I) early detection, (II) good interpretability, and (III) versatility for various illicit activities. However, existing solutions cannot meet all these requirements, as most of them heavily rely on deep learning without interpretability and are only available for retrospective analysis of a specific illicit type. To tackle all these challenges, we propose Intention-Monitor for early malice detection in Bitcoin (BTC), where the on-chain record data for a certain address are much scarcer than other cryptocurrency platforms. We first define asset transfer paths with the Decision-Tree based feature Selection and Complement (DT-SC) to build different feature sets for different malice types. Then, the Status/Action Proposal Module (S/A-PM) an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20114;&#20449;&#24687;&#30340;&#35282;&#24230;&#25506;&#32034;&#20102;&#22522;&#22240;&#32452;&#20851;&#32852;&#30740;&#31350;&#20013;&#22270;&#20687;&#36951;&#20256;&#23398;&#30340;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;Genetic InfoMax&#65288;GIM&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#20154;&#33041;&#19977;&#32500;MRI&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15132</link><description>&lt;p&gt;
&#39640;&#32500;&#25104;&#20687;&#36951;&#20256;&#23398;&#30740;&#31350;&#20013;&#30340;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#25506;&#32034;&#8212;&#8212;Genetic InfoMax
&lt;/p&gt;
&lt;p&gt;
Genetic InfoMax: Exploring Mutual Information Maximization in High-Dimensional Imaging Genetics Studies. (arXiv:2309.15132v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20114;&#20449;&#24687;&#30340;&#35282;&#24230;&#25506;&#32034;&#20102;&#22522;&#22240;&#32452;&#20851;&#32852;&#30740;&#31350;&#20013;&#22270;&#20687;&#36951;&#20256;&#23398;&#30340;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;Genetic InfoMax&#65288;GIM&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#20154;&#33041;&#19977;&#32500;MRI&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#22240;&#32452;&#20851;&#32852;&#30740;&#31350;&#20027;&#35201;&#29992;&#20110;&#35782;&#21035;&#22522;&#22240;&#21464;&#24322;&#19982;&#29305;&#23450;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23558;&#20854;&#24212;&#29992;&#20110;&#39640;&#32500;&#21307;&#23398;&#25104;&#20687;&#25968;&#25454;&#26102;&#65292;&#20851;&#38190;&#27493;&#39588;&#26159;&#25552;&#21462;&#20302;&#32500;&#20294;&#26377;&#20449;&#24687;&#37327;&#30340;&#25968;&#25454;&#34920;&#31034;&#20316;&#20026;&#29305;&#24449;&#12290;&#30001;&#20110;&#19982;&#20856;&#22411;&#30340;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#30456;&#27604;&#65292;GWAS&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#20351;&#24471;&#22270;&#20687;&#36951;&#20256;&#23398;&#30340;&#34920;&#31034;&#23398;&#20064;&#23578;&#26410;&#24471;&#21040;&#20840;&#38754;&#25506;&#32034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#20114;&#20449;&#24687;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#25351;&#20986;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20027;&#35201;&#23616;&#38480;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#22240;&#20449;&#24687;&#26368;&#22823;&#21270;&#65288;Genetic InfoMax&#65292;GIM&#65289;&#30340;&#36328;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#27491;&#21017;&#21270;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#22120;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#22240;&#20449;&#24687;&#21551;&#31034;&#30340;&#36716;&#25442;&#22120;&#65292;&#20197;&#24212;&#23545;GWAS&#30340;&#29305;&#23450;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#20154;&#33041;&#19977;&#32500;MRI&#25968;&#25454;&#19978;&#35780;&#20272;&#20102;GIM&#65292;&#24182;&#24314;&#31435;&#20102;&#26631;&#20934;&#21270;&#35780;&#20272;&#21327;&#35758;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;GIM&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;GWAS&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Genome-wide association studies (GWAS) are used to identify relationships between genetic variations and specific traits. When applied to high-dimensional medical imaging data, a key step is to extract lower-dimensional, yet informative representations of the data as traits. Representation learning for imaging genetics is largely under-explored due to the unique challenges posed by GWAS in comparison to typical visual representation learning. In this study, we tackle this problem from the mutual information (MI) perspective by identifying key limitations of existing methods. We introduce a trans-modal learning framework Genetic InfoMax (GIM), including a regularized MI estimator and a novel genetics-informed transformer to address the specific challenges of GWAS. We evaluate GIM on human brain 3D MRI data and establish standardized evaluation protocols to compare it to existing approaches. Our results demonstrate the effectiveness of GIM and a significantly improved performance on GWAS
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30740;&#31350;&#20102;QM7b&#21644;QM9&#37327;&#23376;&#21147;&#23398;&#25968;&#25454;&#38598;&#30340;&#20869;&#37096;&#32467;&#26500;&#21644;&#29305;&#24449;&#65292;&#21457;&#29616;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#20869;&#22312;&#32500;&#24230;&#35201;&#27604;&#25551;&#36848;&#32500;&#24230;&#23567;&#65292;QM7b&#25968;&#25454;&#30001;&#19982;&#21407;&#23376;&#32452;&#25104;&#30456;&#20851;&#30340;&#26126;&#30830;&#23450;&#20041;&#30340;&#31751;&#32452;&#25104;&#65292;&#32780;QM9&#25968;&#25454;&#21253;&#25324;&#19968;&#20010;&#30001;&#24322;&#24120;&#20540;&#32452;&#25104;&#30340;&#22806;&#37096;&#21306;&#22495;&#21644;&#19968;&#20010;&#38598;&#20013;&#20102;&#32858;&#31867;&#21644;&#20869;&#37096;&#23545;&#35937;&#30340;&#20869;&#37096;&#26680;&#24515;&#21306;&#22495;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#20174;&#23646;&#24615;&#39044;&#27979;&#21407;&#23376;&#32452;&#25104;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.15130</link><description>&lt;p&gt;
&#20351;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#29702;&#35299;QM7b&#21644;QM9&#37327;&#23376;&#21147;&#23398;&#25968;&#25454;&#38598;&#30340;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Understanding the Structure of QM7b and QM9 Quantum Mechanical Datasets Using Unsupervised Learning. (arXiv:2309.15130v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30740;&#31350;&#20102;QM7b&#21644;QM9&#37327;&#23376;&#21147;&#23398;&#25968;&#25454;&#38598;&#30340;&#20869;&#37096;&#32467;&#26500;&#21644;&#29305;&#24449;&#65292;&#21457;&#29616;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#20869;&#22312;&#32500;&#24230;&#35201;&#27604;&#25551;&#36848;&#32500;&#24230;&#23567;&#65292;QM7b&#25968;&#25454;&#30001;&#19982;&#21407;&#23376;&#32452;&#25104;&#30456;&#20851;&#30340;&#26126;&#30830;&#23450;&#20041;&#30340;&#31751;&#32452;&#25104;&#65292;&#32780;QM9&#25968;&#25454;&#21253;&#25324;&#19968;&#20010;&#30001;&#24322;&#24120;&#20540;&#32452;&#25104;&#30340;&#22806;&#37096;&#21306;&#22495;&#21644;&#19968;&#20010;&#38598;&#20013;&#20102;&#32858;&#31867;&#21644;&#20869;&#37096;&#23545;&#35937;&#30340;&#20869;&#37096;&#26680;&#24515;&#21306;&#22495;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#20174;&#23646;&#24615;&#39044;&#27979;&#21407;&#23376;&#32452;&#25104;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#20004;&#20010;&#37327;&#23376;&#21147;&#23398;&#25968;&#25454;&#38598;&#65288;QM7b&#12289;QM9&#65289;&#30340;&#20869;&#37096;&#32467;&#26500;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#30001;&#25968;&#21315;&#20010;&#26377;&#26426;&#20998;&#23376;&#32452;&#25104;&#65292;&#24182;&#20197;&#30005;&#23376;&#24615;&#36136;&#25551;&#36848;&#12290;&#20102;&#35299;&#36825;&#31867;&#25968;&#25454;&#30340;&#32467;&#26500;&#21644;&#29305;&#24449;&#22312;&#20174;&#23646;&#24615;&#39044;&#27979;&#21407;&#23376;&#32452;&#25104;&#26041;&#38754;&#26159;&#37325;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20869;&#22312;&#32500;&#24230;&#20998;&#26512;&#12289;&#32858;&#31867;&#21644;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#20869;&#22312;&#32500;&#24230;&#35201;&#27604;&#25551;&#36848;&#32500;&#24230;&#23567;&#20960;&#20493;&#12290;QM7b&#25968;&#25454;&#30001;&#19982;&#21407;&#23376;&#32452;&#25104;&#30456;&#20851;&#30340;&#26126;&#30830;&#23450;&#20041;&#30340;&#31751;&#32452;&#25104;&#12290;QM9&#25968;&#25454;&#21253;&#25324;&#19968;&#20010;&#20027;&#35201;&#30001;&#24322;&#24120;&#20540;&#32452;&#25104;&#30340;&#22806;&#37096;&#21306;&#22495;&#65292;&#20197;&#21450;&#19968;&#20010;&#38598;&#20013;&#20102;&#32858;&#31867;&#21644;&#20869;&#37096;&#23545;&#35937;&#30340;&#20869;&#37096;&#26680;&#24515;&#21306;&#22495;&#12290;&#20998;&#23376;&#20013;&#30340;&#21407;&#23376;&#25968;&#37327;&#19982;&#20854;&#24322;&#24120;/&#20869;&#37096;&#29305;&#24615;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#20851;&#31995;&#12290;&#23613;&#31649;&#32467;&#26500;&#23384;&#22312;&#24046;&#24322;&#65292;&#24863;&#20852;&#36259;&#30340;&#21464;&#37327;&#30340;&#21487;&#39044;&#27979;&#24615;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#26159;&#23384;&#22312;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the internal structure of two quantum mechanics datasets (QM7b, QM9), composed of several thousands of organic molecules and described in terms of electronic properties. Understanding the structure and characteristics of this kind of data is important when predicting the atomic composition from the properties in inverse molecular designs. Intrinsic dimension analysis, clustering, and outlier detection methods were used in the study. They revealed that for both datasets the intrinsic dimensionality is several times smaller than the descriptive dimensions. The QM7b data is composed of well defined clusters related to atomic composition. The QM9 data consists of an outer region predominantly composed of outliers, and an inner core region that concentrates clustered, inliner objects. A significant relationship exists between the number of atoms in the molecule and its outlier/inner nature. Despite the structural differences, the predictability of variables of interest f
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;CogEval&#21327;&#35758;&#65292;&#29992;&#20110;&#31995;&#32479;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#35813;&#21327;&#35758;&#23545;&#20843;&#20010;LLMs&#30340;&#35748;&#30693;&#22320;&#22270;&#21644;&#35268;&#21010;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.15129</link><description>&lt;p&gt;
&#29992;CogEval&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35748;&#30693;&#22320;&#22270;&#21644;&#35268;&#21010;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating Cognitive Maps and Planning in Large Language Models with CogEval. (arXiv:2309.15129v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15129
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;CogEval&#21327;&#35758;&#65292;&#29992;&#20110;&#31995;&#32479;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#35813;&#21327;&#35758;&#23545;&#20843;&#20010;LLMs&#30340;&#35748;&#30693;&#22320;&#22270;&#21644;&#35268;&#21010;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#37327;&#30340;&#30740;&#31350;&#22768;&#31216;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#26032;&#20852;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20381;&#36182;&#20110;&#26696;&#20363;&#65292;&#24573;&#35270;&#20102;&#35757;&#32451;&#38598;&#30340;&#27745;&#26579;&#65292;&#25110;&#32773;&#32570;&#20047;&#28041;&#21450;&#22810;&#20010;&#20219;&#21153;&#12289;&#25511;&#21046;&#26465;&#20214;&#12289;&#22810;&#27425;&#36845;&#20195;&#21644;&#32479;&#35745;&#40065;&#26834;&#24615;&#27979;&#35797;&#30340;&#31995;&#32479;&#35780;&#20272;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#20004;&#20010;&#37325;&#22823;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CogEval&#65292;&#36825;&#26159;&#19968;&#20010;&#21463;&#35748;&#30693;&#31185;&#23398;&#21551;&#21457;&#30340;&#21327;&#35758;&#65292;&#29992;&#20110;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;CogEval&#21327;&#35758;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;CogEval&#21327;&#35758;&#23545;&#20843;&#20010;LLMs&#65288;OpenAI GPT-4&#12289;GPT-3.5-turbo-175B&#12289;davinci-003-175B&#12289;Google Bard&#12289;Cohere-xlarge-52.4B&#12289;Anthropic Claude-1-52B&#12289;LLaMA-13B&#21644;Alpaca-7B&#65289;&#30340;&#35748;&#30693;&#22320;&#22270;&#21644;&#35268;&#21010;&#33021;&#21147;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#20219;&#21153;&#25552;&#31034;&#22522;&#20110;&#20154;&#31867;&#23454;&#39564;&#65292;&#26082;&#20855;&#26377;&#35780;&#20272;&#35268;&#21010;&#30340;&#24050;&#24314;&#31435;&#26500;&#36896;&#25928;&#24230;&#65292;&#21448;&#19981;&#23384;&#22312;&#20110;LLM&#30340;&#35757;&#32451;&#38598;&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;LLMs&#23637;&#31034;&#20102;&#19968;&#20123;
&lt;/p&gt;
&lt;p&gt;
Recently an influx of studies claim emergent cognitive abilities in large language models (LLMs). Yet, most rely on anecdotes, overlook contamination of training sets, or lack systematic Evaluation involving multiple tasks, control conditions, multiple iterations, and statistical robustness tests. Here we make two major contributions. First, we propose CogEval, a cognitive science-inspired protocol for the systematic evaluation of cognitive capacities in Large Language Models. The CogEval protocol can be followed for the evaluation of various abilities. Second, here we follow CogEval to systematically evaluate cognitive maps and planning ability across eight LLMs (OpenAI GPT-4, GPT-3.5-turbo-175B, davinci-003-175B, Google Bard, Cohere-xlarge-52.4B, Anthropic Claude-1-52B, LLaMA-13B, and Alpaca-7B). We base our task prompts on human experiments, which offer both established construct validity for evaluating planning, and are absent from LLM training sets. We find that, while LLMs show a
&lt;/p&gt;</description></item><item><title>DPA-WNO&#26159;&#19968;&#31181;&#23558;&#21487;&#35299;&#37322;&#24615;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#19982;&#23567;&#27874;&#31070;&#32463;&#25805;&#20316;&#31526;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#32416;&#27491;/&#35782;&#21035;&#32570;&#22833;&#30340;&#29289;&#29702;&#65292;&#24182;&#35299;&#20915;&#20102;&#32431;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.15128</link><description>&lt;p&gt;
DPA-WNO&#65306;&#19968;&#31867;&#38543;&#26426;&#21147;&#23398;&#38382;&#39064;&#30340;&#28784;&#31665;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DPA-WNO: A gray box model for a class of stochastic mechanics problem. (arXiv:2309.15128v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15128
&lt;/p&gt;
&lt;p&gt;
DPA-WNO&#26159;&#19968;&#31181;&#23558;&#21487;&#35299;&#37322;&#24615;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#19982;&#23567;&#27874;&#31070;&#32463;&#25805;&#20316;&#31526;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#32416;&#27491;/&#35782;&#21035;&#32570;&#22833;&#30340;&#29289;&#29702;&#65292;&#24182;&#35299;&#20915;&#20102;&#32431;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#65292;&#20247;&#25152;&#21608;&#30693;&#30340;&#29289;&#29702;&#23450;&#24459;&#24120;&#24120;&#22522;&#20110;&#26576;&#20123;&#20551;&#35774;&#21644;&#36817;&#20284;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#36825;&#20123;&#26041;&#31243;&#36827;&#34892;&#30340;&#20998;&#26512;&#21644;&#35774;&#35745;&#20063;&#26159;&#36817;&#20284;&#30340;&#12290;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#20986;&#29616;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#35299;&#20915;&#20102;&#36825;&#20010;&#25361;&#25112;&#65307;&#28982;&#32780;&#65292;&#32431;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#24448;&#24448;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;(a)&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;(b)&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#65292;(c)&#26080;&#27861;&#36229;&#36234;&#35757;&#32451;&#33539;&#22260;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#25805;&#20316;&#31526;&#23398;&#20064;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#20010;&#28508;&#22312;&#30340;&#26367;&#20195;&#26041;&#26696;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#35748;&#20026;&#65292;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#23384;&#22312;&#20110;&#25968;&#25454;&#29289;&#29702;&#34701;&#21512;&#20013;&#65292;&#20854;&#20013;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#29992;&#20110;&#32416;&#27491;/&#35782;&#21035;&#32570;&#22833;&#30340;&#29289;&#29702;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#20998;&#29289;&#29702;&#22686;&#24378;&#23567;&#27874;&#31070;&#32463;&#25805;&#20316;&#31526;(DPA-WNO)&#12290;&#35813;&#25552;&#20986;&#30340;DPA-WNO&#23558;&#21487;&#24494;&#20998;&#29289;&#29702;&#27714;&#35299;&#22120;&#19982;&#23567;&#27874;&#31070;&#32463;&#25805;&#20316;&#31526;(WNO)&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#20854;&#20013;WNO&#30340;&#20316;&#29992;&#26159;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
The well-known governing physics in science and engineering is often based on certain assumptions and approximations. Therefore, analyses and designs carried out based on these equations are also approximate. The emergence of data-driven models has, to a certain degree, addressed this challenge; however, the purely data-driven models often (a) lack interpretability, (b) are data-hungry, and (c) do not generalize beyond the training window. Operator learning has recently been proposed as a potential alternative to address the aforementioned challenges; however, the challenges are still persistent. We here argue that one of the possible solutions resides in data-physics fusion, where the data-driven model is used to correct/identify the missing physics. To that end, we propose a novel Differentiable Physics Augmented Wavelet Neural Operator (DPA-WNO). The proposed DPA-WNO blends a differentiable physics solver with the Wavelet Neural Operator (WNO), where the role of WNO is to model the 
&lt;/p&gt;</description></item><item><title>Grad DFT&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#22686;&#24378;&#30340;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;DFT&#65289;&#36719;&#20214;&#24211;&#65292;&#36890;&#36807;&#20351;&#29992;&#26435;&#37325;&#21644;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#20132;&#25442;&#20851;&#32852;&#33021;&#37327;&#27867;&#20989;&#65292;&#23545;DFT&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.15127</link><description>&lt;p&gt;
Grad DFT&#65306;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#22686;&#24378;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#30340;&#36719;&#20214;&#24211;
&lt;/p&gt;
&lt;p&gt;
Grad DFT: a software library for machine learning enhanced density functional theory. (arXiv:2309.15127v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15127
&lt;/p&gt;
&lt;p&gt;
Grad DFT&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#22686;&#24378;&#30340;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;DFT&#65289;&#36719;&#20214;&#24211;&#65292;&#36890;&#36807;&#20351;&#29992;&#26435;&#37325;&#21644;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#20132;&#25442;&#20851;&#32852;&#33021;&#37327;&#27867;&#20989;&#65292;&#23545;DFT&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;DFT&#65289;&#20316;&#20026;&#35745;&#31639;&#37327;&#23376;&#21270;&#23398;&#21644;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#22522;&#30707;&#26041;&#27861;&#65292;&#22240;&#20854;&#20986;&#33394;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#32780;&#38395;&#21517;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#24378;&#20851;&#32852;&#31995;&#32479;&#26102;&#65292;DFT&#23384;&#22312;&#31934;&#24230;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24320;&#22987;&#25506;&#32034;&#22914;&#20309;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25193;&#23637;DFT&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#20010;&#20805;&#28385;&#35768;&#22810;&#24320;&#25918;&#38382;&#39064;&#21644;&#25216;&#26415;&#25361;&#25112;&#30340;&#21162;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Grad DFT&#65306;&#19968;&#20010;&#23436;&#20840;&#21487;&#24494;&#30340;&#22522;&#20110;JAX&#30340;DFT&#24211;&#65292;&#33021;&#22815;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#21644;&#23454;&#39564;&#26426;&#22120;&#23398;&#20064;&#22686;&#24378;&#30340;&#20132;&#25442;&#20851;&#32852;&#33021;&#37327;&#27867;&#20989;&#12290;Grad DFT&#37319;&#29992;&#20102;&#19968;&#31181;&#20808;&#39537;&#24615;&#30340;&#20132;&#25442;&#20851;&#32852;&#27867;&#20989;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#33021;&#37327;&#23494;&#24230;&#30340;&#21152;&#26435;&#21644;&#26469;&#30830;&#23450;&#26435;&#37325;&#65292;&#26435;&#37325;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30830;&#23450;&#12290;&#27492;&#22806;&#65292;Grad DFT&#21253;&#21547;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#36741;&#21161;&#20989;&#25968;&#65292;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#29305;&#28857;&#26159;&#21487;&#21363;&#26102;&#32534;&#35793;&#21644;&#23436;&#20840;&#21487;&#24494;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Density functional theory (DFT) stands as a cornerstone method in computational quantum chemistry and materials science due to its remarkable versatility and scalability. Yet, it suffers from limitations in accuracy, particularly when dealing with strongly correlated systems. To address these shortcomings, recent work has begun to explore how machine learning can expand the capabilities of DFT; an endeavor with many open questions and technical challenges. In this work, we present Grad DFT: a fully differentiable JAX-based DFT library, enabling quick prototyping and experimentation with machine learning-enhanced exchange-correlation energy functionals. Grad DFT employs a pioneering parametrization of exchange-correlation functionals constructed using a weighted sum of energy densities, where the weights are determined using neural networks. Moreover, Grad DFT encompasses a comprehensive suite of auxiliary functions, notably featuring a just-in-time compilable and fully differentiable s
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SO3krates&#30340;&#27431;&#20960;&#37324;&#24471;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#23427;&#36890;&#36807;&#32452;&#21512;&#31232;&#30095;&#31561;&#21464;&#34920;&#31034;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#21147;&#22330;&#20013;&#23454;&#29616;&#20102;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#36895;&#24230;&#30340;&#29420;&#29305;&#32452;&#21512;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#21069;&#25152;&#26410;&#26377;&#30340;&#26102;&#38388;&#21644;&#31995;&#32479;&#23610;&#24230;&#19978;&#23545;&#29289;&#36136;&#30340;&#37327;&#23376;&#23646;&#24615;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.15126</link><description>&lt;p&gt;
&#20174;&#32957;&#21040;&#32435;&#31859;&#32467;&#26500;&#65306;&#19968;&#31181;&#29992;&#20110;&#24555;&#36895;&#31283;&#23450;&#30340;&#26426;&#22120;&#23398;&#20064;&#21147;&#22330;&#30340;&#27431;&#20960;&#37324;&#24471;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
From Peptides to Nanostructures: A Euclidean Transformer for Fast and Stable Machine Learned Force Fields. (arXiv:2309.15126v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15126
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SO3krates&#30340;&#27431;&#20960;&#37324;&#24471;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#23427;&#36890;&#36807;&#32452;&#21512;&#31232;&#30095;&#31561;&#21464;&#34920;&#31034;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#21147;&#22330;&#20013;&#23454;&#29616;&#20102;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#36895;&#24230;&#30340;&#29420;&#29305;&#32452;&#21512;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#21069;&#25152;&#26410;&#26377;&#30340;&#26102;&#38388;&#21644;&#31995;&#32479;&#23610;&#24230;&#19978;&#23545;&#29289;&#36136;&#30340;&#37327;&#23376;&#23646;&#24615;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#20174;&#22836;&#35745;&#31639;&#30340;&#26426;&#22120;&#23398;&#20064;&#21147;&#22330;&#65288;MLFFs&#65289;&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#23613;&#31649;&#22312;&#27979;&#35797;&#35823;&#24046;&#19978;&#21462;&#24471;&#20102;&#36739;&#20302;&#30340;&#25928;&#26524;&#65292;&#20294;MLFFs&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#20013;&#30340;&#36866;&#29992;&#24615;&#36234;&#26469;&#36234;&#21463;&#21040;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#20854;&#31283;&#23450;&#24615;&#21463;&#21040;&#36136;&#30097;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;MLFFs&#20013;&#30340;&#31561;&#21464;&#34920;&#31034;&#19982;MD&#27169;&#25311;&#31283;&#23450;&#24615;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#28508;&#22312;&#32852;&#31995;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#21487;&#33021;&#38480;&#21046;&#20102;&#23427;&#20204;&#24102;&#26469;&#30340;&#23454;&#38469;&#20248;&#21183;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;SO3krates&#30340;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#23427;&#32467;&#21512;&#20102;&#31232;&#30095;&#31561;&#21464;&#34920;&#31034;&#65288;&#27431;&#20960;&#37324;&#24471;&#21464;&#37327;&#65289;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#21487;&#20197;&#20998;&#31163;&#19981;&#21464;&#21644;&#31561;&#21464;&#20449;&#24687;&#65292;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;&#24352;&#37327;&#31215;&#25805;&#20316;&#12290;SO3krates&#23454;&#29616;&#20102;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#36895;&#24230;&#30340;&#29420;&#29305;&#32452;&#21512;&#65292;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#22312;&#21069;&#25152;&#26410;&#26377;&#30340;&#26102;&#38388;&#21644;&#31995;&#32479;&#23610;&#24230;&#19978;&#23545;&#29289;&#36136;&#30340;&#37327;&#23376;&#23646;&#24615;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen vast progress in the development of machine learned force fields (MLFFs) based on ab-initio reference calculations. Despite achieving low test errors, the suitability of MLFFs in molecular dynamics (MD) simulations is being increasingly scrutinized due to concerns about instability. Our findings suggest a potential connection between MD simulation stability and the presence of equivariant representations in MLFFs, but their computational cost can limit practical advantages they would otherwise bring.  To address this, we propose a transformer architecture called SO3krates that combines sparse equivariant representations (Euclidean variables) with a self-attention mechanism that can separate invariant and equivariant information, eliminating the need for expensive tensor products. SO3krates achieves a unique combination of accuracy, stability, and speed that enables insightful analysis of quantum properties of matter on unprecedented time and system size scales. T
&lt;/p&gt;</description></item><item><title>&#20174;&#25968;&#25454;&#20013;&#24515;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#31070;&#32463;&#32553;&#25918;&#34892;&#20026;&#65292;&#21457;&#29616;&#20102;&#25968;&#25454;&#37327;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#19968;&#33268;&#24130;&#24459;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.15123</link><description>&lt;p&gt;
&#25581;&#31034;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Uncovering Neural Scaling Laws in Molecular Representation Learning. (arXiv:2309.15123v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15123
&lt;/p&gt;
&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#24515;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#31070;&#32463;&#32553;&#25918;&#34892;&#20026;&#65292;&#21457;&#29616;&#20102;&#25968;&#25454;&#37327;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#19968;&#33268;&#24130;&#24459;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#65288;MRL&#65289;&#24050;&#32463;&#25104;&#20026;&#33647;&#29289;&#21644;&#26448;&#26009;&#21457;&#29616;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#22312;&#34394;&#25311;&#31579;&#36873;&#21644;&#21453;&#21521;&#35774;&#35745;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#34429;&#28982;&#23545;&#20110;&#20998;&#23376;&#34920;&#31034;&#30340;&#25968;&#25454;&#25968;&#37327;&#21644;&#36136;&#37327;&#23545;MRL&#30340;&#24433;&#21709;&#30340;&#27169;&#22411;&#20013;&#24515;&#25216;&#26415;&#30340;&#25512;&#36827;&#24341;&#36215;&#20102;&#22823;&#37327;&#20851;&#27880;&#65292;&#20294;&#22312;&#36825;&#20010;&#39046;&#22495;&#20869;&#65292;&#23545;&#20110;&#36825;&#20004;&#20010;&#22240;&#32032;&#30340;&#24433;&#21709;&#36824;&#19981;&#23436;&#20840;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#25968;&#25454;&#20013;&#24515;&#30340;&#35282;&#24230;&#28145;&#20837;&#30740;&#31350;&#20102;MRL&#30340;&#31070;&#32463;&#32553;&#25918;&#34892;&#20026;&#65292;&#32771;&#23519;&#20102;&#22235;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#65288;1&#65289;&#25968;&#25454;&#27169;&#24577;&#65292;&#65288;2&#65289;&#25968;&#25454;&#38598;&#21010;&#20998;&#65292;&#65288;3&#65289;&#39044;&#35757;&#32451;&#30340;&#20316;&#29992;&#65292;&#21644;&#65288;4&#65289;&#27169;&#22411;&#23481;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#35777;&#23454;&#20102;&#25968;&#25454;&#37327;&#21644;MRL&#24615;&#33021;&#20043;&#38388;&#30340;&#19968;&#33268;&#30340;&#24130;&#24459;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#35814;&#32454;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#30340;&#28508;&#22312;&#36884;&#24452;&#12290;&#20026;&#20102;&#25361;&#25112;&#36825;&#20123;&#32553;&#25918;&#23450;&#24459;&#65292;&#25105;&#20204;&#23558;&#19971;&#31181;&#24120;&#35265;&#30340;&#25968;&#25454;&#20462;&#21098;&#31574;&#30053;&#24212;&#29992;&#20110;&#20998;&#23376;&#25968;&#25454;&#24182;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#25913;&#21892;&#23398;&#20064;&#25928;&#29575;&#30340;&#28508;&#22312;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular Representation Learning (MRL) has emerged as a powerful tool for drug and materials discovery in a variety of tasks such as virtual screening and inverse design. While there has been a surge of interest in advancing model-centric techniques, the influence of both data quantity and quality on molecular representations is not yet clearly understood within this field. In this paper, we delve into the neural scaling behaviors of MRL from a data-centric viewpoint, examining four key dimensions: (1) data modalities, (2) dataset splitting, (3) the role of pre-training, and (4) model capacity. Our empirical studies confirm a consistent power-law relationship between data volume and MRL performance across these dimensions. Additionally, through detailed analysis, we identify potential avenues for improving learning efficiency. To challenge these scaling laws, we adapt seven popular data pruning strategies to molecular data and benchmark their performance. Our findings underline the im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;QA-LoRA&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#37327;&#21270;&#24847;&#35782;&#20197;&#21450;&#32452;&#20869;&#36816;&#31639;&#31526;&#26469;&#23454;&#29616;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20302;&#31209;&#36866;&#24212;&#12290;QA-LoRA&#33021;&#22815;&#23558;&#27169;&#22411;&#26435;&#37325;&#37327;&#21270;&#20197;&#20943;&#23569;&#26102;&#38388;&#21644;&#20869;&#23384;&#30340;&#20351;&#29992;&#65292;&#21516;&#26102;&#22312;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#23558;&#27169;&#22411;&#38598;&#25104;&#20026;&#19968;&#20010;&#37327;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.14717</link><description>&lt;p&gt;
QA-LoRA: &#22522;&#20110;&#37327;&#21270;&#24847;&#35782;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#20302;&#31209;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models. (arXiv:2309.14717v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;QA-LoRA&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#37327;&#21270;&#24847;&#35782;&#20197;&#21450;&#32452;&#20869;&#36816;&#31639;&#31526;&#26469;&#23454;&#29616;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20302;&#31209;&#36866;&#24212;&#12290;QA-LoRA&#33021;&#22815;&#23558;&#27169;&#22411;&#26435;&#37325;&#37327;&#21270;&#20197;&#20943;&#23569;&#26102;&#38388;&#21644;&#20869;&#23384;&#30340;&#20351;&#29992;&#65292;&#21516;&#26102;&#22312;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#23558;&#27169;&#22411;&#38598;&#25104;&#20026;&#19968;&#20010;&#37327;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#27785;&#37325;&#30340;&#35745;&#31639;&#36127;&#25285;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#38480;&#21046;&#20102;LLMs&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#24403;&#38656;&#35201;&#23558;&#23427;&#20204;&#37096;&#32626;&#21040;&#36793;&#32536;&#35774;&#22791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#21270;&#24847;&#35782;&#30340;&#20302;&#31209;&#36866;&#24212;&#65288;QA-LoRA&#65289;&#31639;&#27861;&#12290;&#21160;&#26426;&#22312;&#20110;&#37327;&#21270;&#21644;&#36866;&#24212;&#30340;&#33258;&#30001;&#24230;&#19981;&#24179;&#34913;&#65292;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#32452;&#20869;&#36816;&#31639;&#31526;&#65292;&#22686;&#21152;&#37327;&#21270;&#30340;&#33258;&#30001;&#24230;&#65292;&#21516;&#26102;&#20943;&#23569;&#36866;&#24212;&#30340;&#33258;&#30001;&#24230;&#12290;QA-LoRA&#21487;&#20197;&#29992;&#20960;&#34892;&#20195;&#30721;&#36731;&#26494;&#23454;&#29616;&#65292;&#24182;&#20351;&#21407;&#22987;&#30340;LoRA&#20855;&#22791;&#20102;&#20004;&#20010;&#33021;&#21147;&#65306;&#65288;i&#65289;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;LLM&#30340;&#26435;&#37325;&#34987;&#37327;&#21270;&#65288;&#20363;&#22914;&#36716;&#25442;&#20026;INT4&#65289;&#65292;&#20197;&#20943;&#23569;&#26102;&#38388;&#21644;&#20869;&#23384;&#30340;&#20351;&#29992;&#65307;&#65288;ii&#65289;&#32463;&#36807;&#24494;&#35843;&#21518;&#65292;LLM&#21644;&#36741;&#21161;&#26435;&#37325;&#33258;&#28982;&#22320;&#38598;&#25104;&#21040;&#19968;&#20010;&#37327;&#21270;&#27169;&#22411;&#20013;&#65292;&#32780;&#19981;&#20250;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23558;QA-LoRA&#24212;&#29992;&#21040;LLaMA&#21644;LLaMA2&#27169;&#22411;&#23478;&#26063;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently years have witnessed a rapid development of large language models (LLMs). Despite the strong ability in many language-understanding tasks, the heavy computational burden largely restricts the application of LLMs especially when one needs to deploy them onto edge devices. In this paper, we propose a quantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies in the imbalanced degrees of freedom of quantization and adaptation, and the solution is to use group-wise operators which increase the degree of freedom of quantization meanwhile decreasing that of adaptation. QA-LoRA is easily implemented with a few lines of code, and it equips the original LoRA with two-fold abilities: (i) during fine-tuning, the LLM's weights are quantized (e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the LLM and auxiliary weights are naturally integrated into a quantized model without loss of accuracy. We apply QA-LoRA to the LLaMA and LLaMA2 model famil
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#40654;&#26364;&#27969;&#24418;&#19978;&#38543;&#26426;&#20248;&#21270;&#30340;&#38646;&#38454;&#40654;&#26364;&#24179;&#22343;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65288;Zo-RASA&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#40654;&#26364;&#31227;&#21160;&#24179;&#22343;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#22120;&#21644;&#26032;&#39062;&#30340;&#40654;&#26364;-&#26446;&#38597;&#26222;&#35834;&#22827;&#20998;&#26512;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#949;-&#36817;&#20284;&#30340;&#19968;&#38454;&#31283;&#23450;&#35299;&#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#36890;&#36807;&#20351;&#29992;&#22238;&#32553;&#21644;&#21521;&#37327;&#20256;&#36755;&#26367;&#20195;&#25351;&#25968;&#26144;&#23556;&#21644;&#24179;&#34892;&#20256;&#36755;&#38477;&#20302;&#20102;&#31639;&#27861;&#30340;&#27599;&#27425;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.14506</link><description>&lt;p&gt;
&#38646;&#38454;&#40654;&#26364;&#24179;&#22343;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Zeroth-order Riemannian Averaging Stochastic Approximation Algorithms. (arXiv:2309.14506v1 [math.OC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#40654;&#26364;&#27969;&#24418;&#19978;&#38543;&#26426;&#20248;&#21270;&#30340;&#38646;&#38454;&#40654;&#26364;&#24179;&#22343;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65288;Zo-RASA&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#40654;&#26364;&#31227;&#21160;&#24179;&#22343;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#22120;&#21644;&#26032;&#39062;&#30340;&#40654;&#26364;-&#26446;&#38597;&#26222;&#35834;&#22827;&#20998;&#26512;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#949;-&#36817;&#20284;&#30340;&#19968;&#38454;&#31283;&#23450;&#35299;&#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#36890;&#36807;&#20351;&#29992;&#22238;&#32553;&#21644;&#21521;&#37327;&#20256;&#36755;&#26367;&#20195;&#25351;&#25968;&#26144;&#23556;&#21644;&#24179;&#34892;&#20256;&#36755;&#38477;&#20302;&#20102;&#31639;&#27861;&#30340;&#27599;&#27425;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#40654;&#26364;&#27969;&#24418;&#19978;&#38543;&#26426;&#20248;&#21270;&#30340;&#38646;&#38454;&#40654;&#26364;&#24179;&#22343;&#38543;&#26426;&#36924;&#36817;&#65288;Zo-RASA&#65289;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;Zo-RASA&#20165;&#20351;&#29992;&#27599;&#27425;&#36845;&#20195;&#20013;&#30340;&#19968;&#20010;&#26679;&#26412;&#25110;&#24120;&#25968;&#38454;&#30340;&#25209;&#22788;&#29702;&#23601;&#33021;&#23454;&#29616;&#29983;&#25104;&#949;-&#36817;&#20284;&#30340;&#19968;&#38454;&#31283;&#23450;&#35299;&#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#40654;&#26364;&#31227;&#21160;&#24179;&#22343;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#24182;&#21033;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40654;&#26364;-&#26446;&#38597;&#26222;&#35834;&#22827;&#20998;&#26512;&#25216;&#26415;&#36827;&#34892;&#25910;&#25947;&#20998;&#26512;&#12290;&#36890;&#36807;&#20351;&#29992;&#22238;&#32553;&#21644;&#21521;&#37327;&#20256;&#36755;&#20195;&#26367;&#25351;&#25968;&#26144;&#23556;&#21644;&#24179;&#34892;&#20256;&#36755;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#27599;&#27425;&#36845;&#20195;&#30340;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20960;&#20309;&#26465;&#20214;&#65292;&#28385;&#36275;&#26377;&#26377;&#30028;&#31532;&#20108;&#22522;&#26412;&#24418;&#24335;&#30340;&#27969;&#24418;&#65292;&#20174;&#32780;&#20026;&#29992;&#21521;&#37327;&#20256;&#36755;&#36924;&#36817;&#24179;&#34892;&#20256;&#36755;&#25552;&#20379;&#26032;&#30340;&#35823;&#24046;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Zeroth-order Riemannian Averaging Stochastic Approximation (\texttt{Zo-RASA}) algorithms for stochastic optimization on Riemannian manifolds. We show that \texttt{Zo-RASA} achieves optimal sample complexities for generating $\epsilon$-approximation first-order stationary solutions using only one-sample or constant-order batches in each iteration. Our approach employs Riemannian moving-average stochastic gradient estimators, and a novel Riemannian-Lyapunov analysis technique for convergence analysis. We improve the algorithm's practicality by using retractions and vector transport, instead of exponential mappings and parallel transports, thereby reducing per-iteration complexity. Additionally, we introduce a novel geometric condition, satisfied by manifolds with bounded second fundamental form, which enables new error bounds for approximating parallel transport with vector transport.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26102;&#20195;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.14496</link><description>&lt;p&gt;
Era Splitting.&#65288;arXiv:2309.14496v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
Era Splitting. (arXiv:2309.14496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26102;&#20195;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#20250;&#21576;&#29616;&#20986;&#25968;&#25454;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;&#36825;&#31181;&#34892;&#20026;&#36229;&#20986;&#20102;&#20256;&#32479;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#33539;&#24335;&#30340;&#33539;&#22260;&#65292;&#35813;&#33539;&#24335;&#20551;&#35774;&#25968;&#25454;&#22312;&#26102;&#38388;&#21644;&#22320;&#28857;&#19978;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#12290;&#26032;&#20852;&#30340;&#36229;&#20998;&#24067;&#27867;&#21270;&#39046;&#22495;&#36890;&#36807;&#23558;&#29615;&#22659;&#25110;&#26102;&#20195;&#20449;&#24687;&#34701;&#20837;&#31639;&#27861;&#20013;&#65292;&#26469;&#24212;&#23545;&#36825;&#20010;&#29616;&#23454;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#32447;&#24615;&#27169;&#22411;&#21644;/&#25110;&#31070;&#32463;&#32593;&#32476;&#19978;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65292;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#19982;&#27599;&#20010;&#25968;&#25454;&#28857;&#30456;&#20851;&#30340;&#26102;&#20195;&#20449;&#24687;&#65292;&#26469;&#25214;&#21040;&#22312;&#25968;&#25454;&#30340;&#25152;&#26377;&#19981;&#30456;&#20132;&#26102;&#20195;&#20013;&#37117;&#26159;&#26368;&#20248;&#30340;&#20999;&#20998;&#28857;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real life machine learning problems exhibit distributional shifts in the data from one time to another or from on place to another. This behavior is beyond the scope of the traditional empirical risk minimization paradigm, which assumes i.i.d. distribution of data over time and across locations. The emerging field of out-of-distribution (OOD) generalization addresses this reality with new theory and algorithms which incorporate environmental, or era-wise information into the algorithms. So far, most research has been focused on linear models and/or neural networks. In this research we develop two new splitting criteria for decision trees, which allow us to apply ideas from OOD generalization research to decision tree models, including random forest and gradient-boosting decision trees. The new splitting criteria use era-wise information associated with each data point to allow tree-based models to find split points that are optimal across all disjoint eras in the data, instead of optim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#26381;&#21153;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#20219;&#21153;&#21644;&#29615;&#22659;&#30340;&#21464;&#21270;&#36827;&#34892;&#33258;&#36866;&#24212;&#65292;&#24182;&#36890;&#36807;&#33258;&#24674;&#22797;&#26426;&#21046;&#35299;&#20915;&#20449;&#24687;&#19981;&#36275;&#12289;&#35745;&#21010;&#29983;&#25104;&#38169;&#35823;&#21644;&#25191;&#34892;&#22833;&#36133;&#31561;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20219;&#21153;&#30340;&#25104;&#21151;&#23436;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.14425</link><description>&lt;p&gt;
&#33258;&#24674;&#22797;&#25552;&#31034;&#65306;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#21644;&#33258;&#24674;&#22797;&#30340;&#36890;&#29992;&#26381;&#21153;&#26426;&#22120;&#20154;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Self-Recovery Prompting: Promptable General Purpose Service Robot System with Foundation Models and Self-Recovery. (arXiv:2309.14425v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#26381;&#21153;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#20219;&#21153;&#21644;&#29615;&#22659;&#30340;&#21464;&#21270;&#36827;&#34892;&#33258;&#36866;&#24212;&#65292;&#24182;&#36890;&#36807;&#33258;&#24674;&#22797;&#26426;&#21046;&#35299;&#20915;&#20449;&#24687;&#19981;&#36275;&#12289;&#35745;&#21010;&#29983;&#25104;&#38169;&#35823;&#21644;&#25191;&#34892;&#22833;&#36133;&#31561;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20219;&#21153;&#30340;&#25104;&#21151;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#26381;&#21153;&#26426;&#22120;&#20154;&#65288;GPSR&#65289;&#33021;&#22815;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#25191;&#34892;&#22810;&#31181;&#20219;&#21153;&#65292;&#38656;&#35201;&#19968;&#20010;&#20855;&#26377;&#39640;&#36890;&#29992;&#24615;&#21644;&#36866;&#24212;&#24615;&#30340;&#31995;&#32479;&#26469;&#24212;&#23545;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#29615;&#22659;&#12290;&#26412;&#25991;&#39318;&#20808;&#22522;&#20110;&#22810;&#20010;&#22522;&#30784;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#20010;&#39030;&#23618;GPSR&#31995;&#32479;&#65292;&#29992;&#20110;&#20840;&#29699;&#31454;&#36187;&#65288;RoboCup@Home 2023&#65289;&#12290;&#35813;&#31995;&#32479;&#26082;&#21487;&#20197;&#36866;&#24212;&#22810;&#31181;&#21464;&#21270;&#65292;&#21448;&#21487;&#20197;&#36890;&#36807;&#25552;&#31034;&#27599;&#20010;&#27169;&#22411;&#26469;&#23454;&#29616;&#33258;&#36866;&#24212;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20998;&#26512;&#25152;&#24320;&#21457;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#26356;&#21152;&#29616;&#23454;&#30340;GPSR&#24212;&#29992;&#35774;&#32622;&#20013;&#23384;&#22312;&#19977;&#31181;&#22833;&#36133;&#31867;&#22411;&#65306;&#20449;&#24687;&#19981;&#36275;&#12289;&#38169;&#35823;&#30340;&#35745;&#21010;&#29983;&#25104;&#21644;&#35745;&#21010;&#25191;&#34892;&#22833;&#36133;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#24674;&#22797;&#25552;&#31034;&#31649;&#36947;&#65292;&#35813;&#31649;&#36947;&#25506;&#32034;&#24517;&#35201;&#30340;&#20449;&#24687;&#65292;&#24182;&#20462;&#25913;&#20854;&#25552;&#31034;&#26469;&#20174;&#22833;&#36133;&#20013;&#24674;&#22797;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#65292;&#20855;&#26377;&#33258;&#24674;&#22797;&#26426;&#21046;&#30340;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;&#21508;&#31181;&#22833;&#36133;&#26696;&#20363;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#20379;&#34917;&#20805;&#30340;&#35270;&#39057;&#21487;&#22312;https://sites.google.com/view/srgpsr&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
A general-purpose service robot (GPSR), which can execute diverse tasks in various environments, requires a system with high generalizability and adaptability to tasks and environments. In this paper, we first developed a top-level GPSR system for worldwide competition (RoboCup@Home 2023) based on multiple foundation models. This system is both generalizable to variations and adaptive by prompting each model. Then, by analyzing the performance of the developed system, we found three types of failure in more realistic GPSR application settings: insufficient information, incorrect plan generation, and plan execution failure. We then propose the self-recovery prompting pipeline, which explores the necessary information and modifies its prompts to recover from failure. We experimentally confirm that the system with the self-recovery mechanism can accomplish tasks by resolving various failure cases. Supplementary videos are available at https://sites.google.com/view/srgpsr .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#31867;&#22120;&#65292;&#22312;&#21160;&#26426;&#24615;&#35775;&#35848;&#20013;&#20934;&#30830;&#21306;&#20998;&#20102;&#21464;&#21270;&#35805;&#35821;&#12289;&#25345;&#32493;&#35805;&#35821;&#21644;&#36319;&#38543;/&#20013;&#31435;&#35805;&#35821;&#19977;&#31181;&#31867;&#21035;&#12290;&#35813;&#20998;&#31867;&#22120;&#21033;&#29992;&#25991;&#26412;&#12289;&#22768;&#35843;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#36523;&#20307;&#34920;&#29616;&#31561;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#24182;&#23545;AnnoMI&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27880;&#37322;&#21644;&#35757;&#32451;&#12290;&#30740;&#31350;&#36824;&#25214;&#21040;&#20102;&#20915;&#31574;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#30340;&#27169;&#24577;&#65292;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2309.14398</link><description>&lt;p&gt;
&#30475;&#35265;&#21644;&#21548;&#21040;&#27809;&#34987;&#35828;&#30340;&#35805;&#65306;&#21487;&#35299;&#37322;&#34701;&#21512;&#30340;&#22810;&#27169;&#24577;&#21160;&#26426;&#24615;&#35775;&#35848;&#23458;&#25143;&#34892;&#20026;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Seeing and hearing what has not been said; A multimodal client behavior classifier in Motivational Interviewing with interpretable fusion. (arXiv:2309.14398v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#31867;&#22120;&#65292;&#22312;&#21160;&#26426;&#24615;&#35775;&#35848;&#20013;&#20934;&#30830;&#21306;&#20998;&#20102;&#21464;&#21270;&#35805;&#35821;&#12289;&#25345;&#32493;&#35805;&#35821;&#21644;&#36319;&#38543;/&#20013;&#31435;&#35805;&#35821;&#19977;&#31181;&#31867;&#21035;&#12290;&#35813;&#20998;&#31867;&#22120;&#21033;&#29992;&#25991;&#26412;&#12289;&#22768;&#35843;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#36523;&#20307;&#34920;&#29616;&#31561;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#24182;&#23545;AnnoMI&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27880;&#37322;&#21644;&#35757;&#32451;&#12290;&#30740;&#31350;&#36824;&#25214;&#21040;&#20102;&#20915;&#31574;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#30340;&#27169;&#24577;&#65292;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#26426;&#24615;&#35775;&#35848;&#65288;MI&#65289;&#26159;&#19968;&#31181;&#24378;&#35843;&#21512;&#20316;&#24182;&#40723;&#21169;&#34892;&#20026;&#25913;&#21464;&#30340;&#27835;&#30103;&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;MI&#23545;&#35805;&#30340;&#36136;&#37327;&#65292;&#21487;&#20197;&#21033;&#29992;MISC&#20195;&#30721;&#23558;&#23458;&#25143;&#35805;&#35821;&#20998;&#31867;&#20026;&#21464;&#21270;&#35805;&#35821;&#12289;&#25345;&#32493;&#35805;&#35821;&#25110;&#36319;&#38543;/&#20013;&#31435;&#35805;&#35821;&#12290;MI&#23545;&#35805;&#20013;&#21464;&#21270;&#35805;&#35821;&#30340;&#27604;&#20363;&#19982;&#27835;&#30103;&#32467;&#26524;&#21576;&#27491;&#30456;&#20851;&#65292;&#22240;&#27492;&#20934;&#30830;&#20998;&#31867;&#23458;&#25143;&#35805;&#35821;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;&#25991;&#26412;&#12289;&#22768;&#35843;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#36523;&#20307;&#34920;&#29616;&#31561;&#22810;&#27169;&#24577;&#29305;&#24449;&#20934;&#30830;&#21306;&#20998;&#19977;&#20010;MISC&#31867;&#21035;&#65288;&#21464;&#21270;&#35805;&#35821;&#12289;&#25345;&#32493;&#35805;&#35821;&#21644;&#36319;&#38543;/&#20013;&#31435;&#35805;&#35821;&#65289;&#12290;&#20026;&#20102;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#23545;&#20844;&#24320;&#21487;&#29992;&#30340;AnnoMI&#25968;&#25454;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#25910;&#38598;&#20102;&#25991;&#26412;&#12289;&#38899;&#39057;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#36523;&#20307;&#34920;&#29616;&#31561;&#22810;&#27169;&#24577;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#20915;&#31574;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#30340;&#27169;&#24577;&#65292;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivational Interviewing (MI) is an approach to therapy that emphasizes collaboration and encourages behavioral change. To evaluate the quality of an MI conversation, client utterances can be classified using the MISC code as either change talk, sustain talk, or follow/neutral talk. The proportion of change talk in a MI conversation is positively correlated with therapy outcomes, making accurate classification of client utterances essential. In this paper, we present a classifier that accurately distinguishes between the three MISC classes (change talk, sustain talk, and follow/neutral talk) leveraging multimodal features such as text, prosody, facial expressivity, and body expressivity. To train our model, we perform annotations on the publicly available AnnoMI dataset to collect multimodal information, including text, audio, facial expressivity, and body expressivity. Furthermore, we identify the most important modalities in the decision-making process, providing valuable insights i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#38543;&#26426;&#32447;&#24615;Bandit&#31639;&#27861;&#65292;&#21033;&#29992;&#38797;&#28857;&#36793;&#30028;&#30340;&#39532;&#19969;&#26684;&#23572;&#28151;&#21512;&#26500;&#24314;&#20102;&#36866;&#29992;&#20110;&#38543;&#26426;Bandit&#30340;&#32622;&#20449;&#24207;&#21015;&#65292;&#24182;&#35777;&#26126;&#35813;&#31639;&#27861;&#33021;&#22815;&#20197;&#31454;&#20105;&#24615;&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#36951;&#25022;&#20445;&#35777;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.14298</link><description>&lt;p&gt;
&#20351;&#29992;&#38797;&#28857;&#36793;&#30028;&#30340;&#39532;&#19969;&#26684;&#23572;&#28151;&#21512;&#25913;&#36827;&#38543;&#26426;&#32447;&#24615;Bandit&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improved Algorithms for Stochastic Linear Bandits Using Tail Bounds for Martingale Mixtures. (arXiv:2309.14298v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#38543;&#26426;&#32447;&#24615;Bandit&#31639;&#27861;&#65292;&#21033;&#29992;&#38797;&#28857;&#36793;&#30028;&#30340;&#39532;&#19969;&#26684;&#23572;&#28151;&#21512;&#26500;&#24314;&#20102;&#36866;&#29992;&#20110;&#38543;&#26426;Bandit&#30340;&#32622;&#20449;&#24207;&#21015;&#65292;&#24182;&#35777;&#26126;&#35813;&#31639;&#27861;&#33021;&#22815;&#20197;&#31454;&#20105;&#24615;&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#36951;&#25022;&#20445;&#35777;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#38543;&#26426;&#32447;&#24615;Bandit&#38382;&#39064;&#20855;&#26377;&#26368;&#22351;&#24773;&#20917;&#19979;&#36951;&#25022;&#20445;&#35777;&#30340;&#25913;&#36827;&#31639;&#27861;&#12290;&#24191;&#27867;&#20351;&#29992;&#30340;"&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#30340;&#20048;&#35266;&#21407;&#21017;"&#21487;&#20197;&#23558;&#38543;&#26426;Bandit&#38382;&#39064;&#36716;&#21270;&#20026;&#23545;&#26410;&#30693;&#22870;&#21169;&#20989;&#25968;&#26500;&#24314;&#32622;&#20449;&#24207;&#21015;&#30340;&#38382;&#39064;&#12290;&#32467;&#26524;&#31639;&#27861;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#32622;&#20449;&#24207;&#21015;&#30340;&#22823;&#23567;&#65292;&#32622;&#20449;&#38598;&#36739;&#23567;&#21487;&#25552;&#20379;&#26356;&#22909;&#30340;&#32463;&#39564;&#24615;&#33021;&#21644;&#26356;&#24378;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#23545;&#33258;&#36866;&#24212;&#39532;&#19969;&#26684;&#23572;&#28151;&#21512;&#30340;&#23614;&#37096;&#36793;&#30028;&#26469;&#26500;&#24314;&#36866;&#29992;&#20110;&#38543;&#26426;Bandit&#30340;&#32622;&#20449;&#24207;&#21015;&#12290;&#36825;&#20123;&#32622;&#20449;&#24207;&#21015;&#20801;&#35768;&#36890;&#36807;&#20984;&#35268;&#21010;&#36827;&#34892;&#39640;&#25928;&#30340;&#21160;&#20316;&#36873;&#25321;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;&#25105;&#20204;&#30340;&#32622;&#20449;&#24207;&#21015;&#30340;&#32447;&#24615;Bandit&#31639;&#27861;&#33021;&#22815;&#20445;&#35777;&#36798;&#21040;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#36951;&#25022;&#12290;&#25105;&#20204;&#23454;&#35777;&#21644;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#32622;&#20449;&#24207;&#21015;&#27604;&#31454;&#20105;&#23545;&#25163;&#26356;&#32039;&#33268;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#32039;&#33268;&#32622;&#20449;&#24207;&#21015;&#21487;&#20197;&#25552;&#20379;&#21644;&#32622;&#20449;&#38598;&#27604;&#36739;&#23481;&#26131;&#37197;&#32622;&#30340;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present improved algorithms with worst-case regret guarantees for the stochastic linear bandit problem. The widely used "optimism in the face of uncertainty" principle reduces a stochastic bandit problem to the construction of a confidence sequence for the unknown reward function. The performance of the resulting bandit algorithm depends on the size of the confidence sequence, with smaller confidence sets yielding better empirical performance and stronger regret guarantees. In this work, we use a novel tail bound for adaptive martingale mixtures to construct confidence sequences which are suitable for stochastic bandits. These confidence sequences allow for efficient action selection via convex programming. We prove that a linear bandit algorithm based on our confidence sequences is guaranteed to achieve competitive worst-case regret. We show that our confidence sequences are tighter than competitors, both empirically and theoretically. Finally, we demonstrate that our tighter confi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#26684;&#20449;&#24687;&#20256;&#25773;&#30340;&#24819;&#35937;&#26426;&#21046;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#12290;&#36890;&#36807;&#20351;&#20449;&#24687;&#22312;&#19981;&#21516;&#29366;&#24577;&#38388;&#24191;&#25773;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22312;&#21516;&#19968;&#29366;&#24577;&#38598;&#20013;&#20256;&#36755;&#65292;&#36825;&#31181;&#26426;&#21046;&#20419;&#36827;&#20102;&#27169;&#22411;&#23545;&#29366;&#24577;&#38388;&#30456;&#20114;&#20381;&#36182;&#24615;&#30340;&#29702;&#35299;&#65292;&#24182;&#25552;&#39640;&#20102;&#23545;&#26377;&#38480;&#26679;&#26412;&#20449;&#24687;&#30340;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.14243</link><description>&lt;p&gt;
&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#25928;&#29575;&#65306;&#22522;&#20110;&#32593;&#26684;&#20449;&#24687;&#20256;&#25773;&#30340;&#26032;&#22411;&#24819;&#35937;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Enhancing data efficiency in reinforcement learning: a novel imagination mechanism based on mesh information propagation. (arXiv:2309.14243v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14243
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#26684;&#20449;&#24687;&#20256;&#25773;&#30340;&#24819;&#35937;&#26426;&#21046;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#12290;&#36890;&#36807;&#20351;&#20449;&#24687;&#22312;&#19981;&#21516;&#29366;&#24577;&#38388;&#24191;&#25773;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22312;&#21516;&#19968;&#29366;&#24577;&#38598;&#20013;&#20256;&#36755;&#65292;&#36825;&#31181;&#26426;&#21046;&#20419;&#36827;&#20102;&#27169;&#22411;&#23545;&#29366;&#24577;&#38388;&#30456;&#20114;&#20381;&#36182;&#24615;&#30340;&#29702;&#35299;&#65292;&#24182;&#25552;&#39640;&#20102;&#23545;&#26377;&#38480;&#26679;&#26412;&#20449;&#24687;&#30340;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#21644;&#22823;&#35268;&#27169;&#38382;&#39064;&#26102;&#38754;&#20020;&#25968;&#25454;&#25928;&#29575;&#26377;&#38480;&#30340;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#26356;&#26032;&#26234;&#33021;&#20307;&#30340;&#35780;&#35770;&#23478;&#26102;&#24448;&#24448;&#20165;&#20381;&#36182;&#20110;&#21516;&#19968;&#38598;&#20013;&#30340;&#29366;&#24577;&#36716;&#25442;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#25928;&#29575;&#20302;&#21644;&#35757;&#32451;&#26102;&#38388;&#28040;&#32791;&#20122;&#20248;&#12290;&#21463;&#21040;&#20154;&#31867;&#31867;&#27604;&#25512;&#29702;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#24819;&#35937;&#26426;&#21046;&#65288;IM&#65289;&#8221;&#30340;&#26032;&#22411;&#32593;&#26684;&#20449;&#24687;&#20256;&#25773;&#26426;&#21046;&#65292;&#26088;&#22312;&#26174;&#33879;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;IM&#20351;&#24471;&#30001;&#21333;&#20010;&#26679;&#26412;&#29983;&#25104;&#30340;&#20449;&#24687;&#33021;&#22815;&#26377;&#25928;&#22320;&#24191;&#25773;&#21040;&#19981;&#21516;&#30340;&#38598;&#20013;&#29366;&#24577;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22312;&#21516;&#19968;&#38598;&#20013;&#20256;&#36755;&#12290;&#36825;&#31181;&#33021;&#21147;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#29366;&#24577;&#38388;&#30456;&#20114;&#20381;&#36182;&#24615;&#30340;&#29702;&#35299;&#65292;&#24182;&#20419;&#36827;&#20102;&#23545;&#26377;&#38480;&#26679;&#26412;&#20449;&#24687;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;&#20026;&#20102;&#25552;&#39640;&#22810;&#21151;&#33021;&#24615;&#65292;&#25105;&#20204;&#23558;IM&#25193;&#23637;&#20026;&#19968;&#31181;&#21487;&#20197;&#20316;&#20026;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning(RL) algorithms face the challenge of limited data efficiency, particularly when dealing with high-dimensional state spaces and large-scale problems. Most of RL methods often rely solely on state transition information within the same episode when updating the agent's Critic, which can lead to low data efficiency and sub-optimal training time consumption. Inspired by human-like analogical reasoning abilities, we introduce a novel mesh information propagation mechanism, termed the 'Imagination Mechanism (IM)', designed to significantly enhance the data efficiency of RL algorithms. Specifically, IM enables information generated by a single sample to be effectively broadcasted to different states across episodes, instead of simply transmitting in the same episode. This capability enhances the model's comprehension of state interdependencies and facilitates more efficient learning of limited sample information. To promote versatility, we extend the IM to function as a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#19988;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#29992;&#20110;&#22312;&#22810;&#20013;&#24515;&#25968;&#25454;&#24211;&#20013;&#39044;&#27979;&#21152;&#25252;&#30149;&#25151;&#24739;&#32773;&#30340;&#20877;&#20837;&#38498;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.13781</link><description>&lt;p&gt;
ICU &#37325;&#26032;&#20837;&#38498;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Explainable Machine Learning for ICU Readmission Prediction. (arXiv:2309.13781v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#19988;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#29992;&#20110;&#22312;&#22810;&#20013;&#24515;&#25968;&#25454;&#24211;&#20013;&#39044;&#27979;&#21152;&#25252;&#30149;&#25151;&#24739;&#32773;&#30340;&#20877;&#20837;&#38498;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#21307;&#38498;&#29615;&#22659;&#65292;&#21307;&#29983;&#30340;&#20915;&#31574;&#23545;&#24739;&#32773;&#30340;&#29983;&#21629;&#26500;&#25104;&#39640;&#39118;&#38505;&#12290;&#24517;&#39035;&#36981;&#24490;&#19968;&#26465;&#20840;&#38754;&#30340;&#25252;&#29702;&#36335;&#24452;&#26469;&#20943;&#23569;&#24182;&#21457;&#30151;&#12290;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#65292;&#19981;&#30830;&#23450;&#24615;&#12289;&#31454;&#20105;&#24615;&#21644;&#38750;&#35745;&#21010;&#24615;&#30340;&#22240;&#32032;&#22686;&#21152;&#20102;&#32479;&#19968;&#23454;&#26045;&#25252;&#29702;&#36335;&#24452;&#30340;&#22256;&#38590;&#12290;&#20877;&#20837;&#38498;&#26159;&#35813;&#36335;&#24452;&#30340;&#22256;&#38590;&#20043;&#19968;&#65292;&#21363;&#24739;&#32773;&#22312;&#30701;&#26102;&#38388;&#20869;&#20877;&#27425;&#20837;&#20303;ICU&#65292;&#23548;&#33268;&#39640;&#27515;&#20129;&#29575;&#21644;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;&#19968;&#20123;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#24739;&#32773;&#30340;&#21307;&#30103;&#20449;&#24687;&#26469;&#39044;&#27979;&#20877;&#20837;&#38498;&#24773;&#20917;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#39044;&#27979;&#20877;&#20837;&#38498;&#26102;&#26377;&#19968;&#23450;&#30340;&#25104;&#21151;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#24182;&#26410;&#23545;&#20877;&#20837;&#38498;&#39044;&#27979;&#36827;&#34892;&#36866;&#24403;&#30340;&#35780;&#20272;&#12289;&#25551;&#36848;&#21644;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#19988;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#29992;&#20110;&#22312;&#22810;&#20013;&#24515;&#25968;&#25454;&#24211;&#65288;&#21363;&#21253;&#21547;166,355&#21517;&#24739;&#32773;&#30340;eICU&#38431;&#21015;&#65292;200,859&#21517;...&#65289;
&lt;/p&gt;
&lt;p&gt;
The intensive care unit (ICU) comprises a complex hospital environment, where decisions made by clinicians have a high level of risk for the patients' lives. A comprehensive care pathway must then be followed to reduce p complications. Uncertain, competing and unplanned aspects within this environment increase the difficulty in uniformly implementing the care pathway. Readmission contributes to this pathway's difficulty, occurring when patients are admitted again to the ICU in a short timeframe, resulting in high mortality rates and high resource utilisation. Several works have tried to predict readmission through patients' medical information. Although they have some level of success while predicting readmission, those works do not properly assess, characterise and understand readmission prediction. This work proposes a standardised and explainable machine learning pipeline to model patient readmission on a multicentric database (i.e., the eICU cohort with 166,355 patients, 200,859 ad
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#25968;&#25454;&#20998;&#24067;&#19978;&#26159;&#31283;&#23450;&#30340;&#65292;&#24182;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#27169;&#22411;&#31867;&#21644;&#20840;&#23616;&#21464;&#37327;&#37325;&#35201;&#24615;&#25351;&#26631;&#32467;&#21512;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.13775</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;The Rashomon Importance Distribution: &#25670;&#33073;&#19981;&#31283;&#23450;&#30340;&#22522;&#20110;&#21333;&#19968;&#27169;&#22411;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Rashomon Importance Distribution: Getting RID of Unstable, Single Model-based Variable Importance. (arXiv:2309.13775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13775
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#25968;&#25454;&#20998;&#24067;&#19978;&#26159;&#31283;&#23450;&#30340;&#65292;&#24182;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#27169;&#22411;&#31867;&#21644;&#20840;&#23616;&#21464;&#37327;&#37325;&#35201;&#24615;&#25351;&#26631;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#21464;&#37327;&#37325;&#35201;&#24615;&#23545;&#20110;&#22238;&#31572;&#36951;&#20256;&#23398;&#12289;&#20844;&#20849;&#25919;&#31574;&#21644;&#21307;&#23398;&#31561;&#39046;&#22495;&#30340;&#37325;&#22823;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#35745;&#31639;&#32473;&#23450;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32473;&#23450;&#27169;&#22411;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#32473;&#23450;&#25968;&#25454;&#38598;&#65292;&#21487;&#33021;&#26377;&#35768;&#22810;&#27169;&#22411;&#21516;&#26679;&#33021;&#35299;&#37322;&#30446;&#26631;&#32467;&#26524;;&#22914;&#26524;&#19981;&#32771;&#34385;&#25152;&#26377;&#21487;&#33021;&#30340;&#35299;&#37322;&#65292;&#19981;&#21516;&#30340;&#30740;&#31350;&#32773;&#21487;&#33021;&#20250;&#24471;&#20986;&#35768;&#22810;&#20914;&#31361;&#20294;&#21516;&#26679;&#26377;&#25928;&#30340;&#32467;&#35770;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#32771;&#34385;&#20102;&#32473;&#23450;&#25968;&#25454;&#38598;&#30340;&#25152;&#26377;&#21487;&#33021;&#35299;&#37322;&#65292;&#36825;&#20123;&#27934;&#23519;&#21147;&#21487;&#33021;&#19981;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#22240;&#20026;&#24182;&#38750;&#25152;&#26377;&#22909;&#30340;&#35299;&#37322;&#22312;&#21512;&#29702;&#30340;&#25968;&#25454;&#25200;&#21160;&#19979;&#37117;&#26159;&#31283;&#23450;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37327;&#21270;&#20102;&#22312;&#25152;&#26377;&#22909;&#30340;&#27169;&#22411;&#38598;&#21512;&#20013;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#20998;&#24067;&#19978;&#26159;&#31283;&#23450;&#30340;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38750;&#24120;&#28789;&#27963;&#65292;&#21487;&#20197;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#27169;&#22411;&#31867;&#21644;&#20840;&#23616;&#21464;&#37327;&#37325;&#35201;&#24615;&#25351;&#26631;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantifying variable importance is essential for answering high-stakes questions in fields like genetics, public policy, and medicine. Current methods generally calculate variable importance for a given model trained on a given dataset. However, for a given dataset, there may be many models that explain the target outcome equally well; without accounting for all possible explanations, different researchers may arrive at many conflicting yet equally valid conclusions given the same data. Additionally, even when accounting for all possible explanations for a given dataset, these insights may not generalize because not all good explanations are stable across reasonable data perturbations. We propose a new variable importance framework that quantifies the importance of a variable across the set of all good models and is stable across the data distribution. Our framework is extremely flexible and can be integrated with most existing model classes and global variable importance metrics. We d
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#19981;&#21516;&#34701;&#21512;&#31574;&#30053;&#23545;&#22810;&#20809;&#35889;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#65292;&#20381;&#36182;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.12463</link><description>&lt;p&gt;
&#26550;&#26500;&#23545;&#22810;&#20809;&#35889;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#24615;&#21644;&#35299;&#37322;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of architecture on robustness and interpretability of multispectral deep neural networks. (arXiv:2309.12463v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12463
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#19981;&#21516;&#34701;&#21512;&#31574;&#30053;&#23545;&#22810;&#20809;&#35889;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#65292;&#20381;&#36182;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21253;&#21547;&#39069;&#22806;&#20809;&#35889;&#27874;&#27573;&#65288;&#20363;&#22914;&#36817;&#32418;&#22806;&#65289;&#30340;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#26377;&#35768;&#22810;&#21487;&#33021;&#30340;&#26041;&#27861;&#23558;&#36825;&#20123;&#39069;&#22806;&#20449;&#24687;&#32435;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#20294;&#26368;&#20339;&#34701;&#21512;&#31574;&#30053;&#23578;&#26410;&#30830;&#23450;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#20013;&#21487;&#33021;&#26377;&#25152;&#21464;&#21270;&#12290;&#22312;&#20854;&#20013;&#19968;&#20010;&#26497;&#31471;&#65292;&#31216;&#20026;"&#26089;&#26399;&#34701;&#21512;"&#65292;&#39069;&#22806;&#27874;&#27573;&#34987;&#22534;&#21472;&#20026;&#39069;&#22806;&#36890;&#36947;&#65292;&#20197;&#33719;&#24471;&#20855;&#26377;&#36229;&#36807;&#19977;&#20010;&#36890;&#36947;&#30340;&#36755;&#20837;&#22270;&#20687;&#12290;&#22312;&#21478;&#19968;&#20010;&#26497;&#31471;&#65292;&#31216;&#20026;"&#26202;&#26399;&#34701;&#21512;"&#65292;RGB&#21644;&#38750;RGB&#27874;&#27573;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21333;&#29420;&#20998;&#25903;&#20256;&#36882;&#65292;&#24182;&#22312;&#26368;&#32456;&#20998;&#31867;&#25110;&#20998;&#21106;&#23618;&#20043;&#21069;&#31435;&#21363;&#21512;&#24182;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#19968;&#22871;&#19981;&#21516;&#34701;&#21512;&#26041;&#27861;&#30340;&#22810;&#20809;&#35889;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#37327;&#21270;&#20102;&#23427;&#20204;&#23545;&#19981;&#21516;&#36755;&#20837;&#27874;&#27573;&#30340;&#30456;&#23545;&#20381;&#36182;&#24615;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#24433;&#21709;&#19968;&#20010;&#25110;&#22810;&#20010;&#36755;&#20837;&#36890;&#36947;&#30340;&#33258;&#28982;&#22270;&#20687;&#22833;&#30495;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Including information from additional spectral bands (e.g., near-infrared) can improve deep learning model performance for many vision-oriented tasks. There are many possible ways to incorporate this additional information into a deep learning model, but the optimal fusion strategy has not yet been determined and can vary between applications. At one extreme, known as "early fusion," additional bands are stacked as extra channels to obtain an input image with more than three channels. At the other extreme, known as "late fusion," RGB and non-RGB bands are passed through separate branches of a deep learning model and merged immediately before a final classification or segmentation layer. In this work, we characterize the performance of a suite of multispectral deep learning models with different fusion approaches, quantify their relative reliance on different input bands and evaluate their robustness to naturalistic image corruptions affecting one or more input channels.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22534;&#21472;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#24635;&#20307;&#39118;&#38505;&#24182;&#21463;&#38750;&#36127;&#24615;&#32422;&#26463;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#35823;&#24046;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22534;&#21472;&#20272;&#35745;&#22120;&#30456;&#27604;&#20854;&#20013;&#26368;&#20339;&#30340;&#21333;&#20010;&#20272;&#35745;&#22120;&#20855;&#26377;&#26356;&#23567;&#30340;&#24635;&#20307;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2309.09880</link><description>&lt;p&gt;
&#30001;&#22534;&#21472;&#22238;&#24402;&#20943;&#23569;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Error Reduction from Stacked Regressions. (arXiv:2309.09880v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22534;&#21472;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#24635;&#20307;&#39118;&#38505;&#24182;&#21463;&#38750;&#36127;&#24615;&#32422;&#26463;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#35823;&#24046;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22534;&#21472;&#20272;&#35745;&#22120;&#30456;&#27604;&#20854;&#20013;&#26368;&#20339;&#30340;&#21333;&#20010;&#20272;&#35745;&#22120;&#20855;&#26377;&#26356;&#23567;&#30340;&#24635;&#20307;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22534;&#21472;&#22238;&#24402;&#26159;&#19968;&#31181;&#38598;&#25104;&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#24418;&#25104;&#19981;&#21516;&#22238;&#24402;&#20272;&#35745;&#22120;&#30340;&#32447;&#24615;&#32452;&#21512;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#20256;&#32479;&#26041;&#27861;&#20351;&#29992;&#20132;&#21449;&#39564;&#35777;&#25968;&#25454;&#26469;&#29983;&#25104;&#30001;&#26500;&#25104;&#20272;&#35745;&#22120;&#39044;&#27979;&#65292;&#24182;&#20351;&#29992;&#24102;&#38750;&#36127;&#24615;&#32422;&#26463;&#30340;&#26368;&#23567;&#20108;&#20056;&#27861;&#23398;&#20064;&#32452;&#21512;&#26435;&#37325;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31867;&#20284;&#22320;&#36890;&#36807;&#26368;&#23567;&#21270;&#19968;&#31181;&#20272;&#35745;&#30340;&#24635;&#20307;&#39118;&#38505;&#26469;&#23398;&#20064;&#36825;&#20123;&#26435;&#37325;&#65292;&#24182;&#21463;&#21040;&#38750;&#36127;&#24615;&#32422;&#26463;&#12290;&#24403;&#26500;&#25104;&#30340;&#20272;&#35745;&#22120;&#26159;&#36890;&#36807;&#33267;&#23569;&#19977;&#20010;&#32500;&#24230;&#20998;&#38548;&#30340;&#23884;&#22871;&#23376;&#31354;&#38388;&#30340;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#25237;&#24433;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#30001;&#20110;&#25910;&#32553;&#25928;&#24212;&#65292;&#25152;&#24471;&#21040;&#30340;&#22534;&#21472;&#20272;&#35745;&#22120;&#30340;&#24635;&#20307;&#39118;&#38505;&#20005;&#26684;&#23567;&#20110;&#20854;&#20013;&#26368;&#20339;&#30340;&#21333;&#20010;&#20272;&#35745;&#22120;&#12290;&#36825;&#37324;&#30340;&#8220;&#26368;&#20339;&#8221;&#26159;&#25351;&#26368;&#23567;&#21270;&#36873;&#25321;&#20934;&#21017;&#22914;AIC&#25110;BIC&#30340;&#27169;&#22411;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26368;&#20339;&#30340;&#21333;&#20010;&#20272;&#35745;&#22120;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#12290;&#22240;&#20026;&#20248;&#21270;&#38382;&#39064;&#21487;&#20197;&#37325;&#26500;&#20026;&#21516;&#20449;&#24687;&#22238;&#24402;&#65292;&#25152;&#20197;...
&lt;/p&gt;
&lt;p&gt;
Stacking regressions is an ensemble technique that forms linear combinations of different regression estimators to enhance predictive accuracy. The conventional approach uses cross-validation data to generate predictions from the constituent estimators, and least-squares with nonnegativity constraints to learn the combination weights. In this paper, we learn these weights analogously by minimizing an estimate of the population risk subject to a nonnegativity constraint. When the constituent estimators are linear least-squares projections onto nested subspaces separated by at least three dimensions, we show that thanks to a shrinkage effect, the resulting stacked estimator has strictly smaller population risk than best single estimator among them. Here ``best'' refers to a model that minimizes a selection criterion such as AIC or BIC. In other words, in this setting, the best single estimator is inadmissible. Because the optimization problem can be reformulated as isotonic regression, t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20960;&#20309;&#35282;&#24230;&#30740;&#31350;&#20102;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#22810;&#35299;&#21644;&#30072;&#21464;&#34920;&#31034;&#38382;&#39064;&#30340;&#20960;&#20309;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.08247</link><description>&lt;p&gt;
&#23545;&#33258;&#32534;&#30721;&#22120;&#30340;&#20960;&#20309;&#35282;&#24230;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Geometric Perspective on Autoencoders. (arXiv:2309.08247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20960;&#20309;&#35282;&#24230;&#30740;&#31350;&#20102;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#22810;&#35299;&#21644;&#30072;&#21464;&#34920;&#31034;&#38382;&#39064;&#30340;&#20960;&#20309;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#30340;&#20960;&#20309;&#26041;&#38754;&#65292;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#20294;&#34987;&#30456;&#23545;&#36739;&#23569;&#22320;&#35748;&#35782;&#21040;&#12290;&#32473;&#23450;&#19968;&#32452;&#20960;&#20046;&#20301;&#20110;&#26576;&#20010;&#36739;&#20302;&#32500;&#24230;&#27969;&#24418;&#19978;&#30340;&#39640;&#32500;&#25968;&#25454;&#28857;&#65292;&#33258;&#32534;&#30721;&#22120;&#21516;&#26102;&#23398;&#20064;&#27969;&#24418;&#21644;&#20854;&#22352;&#26631;&#22270;&#12290;&#36825;&#31181;&#20960;&#20309;&#35282;&#24230;&#33258;&#28982;&#24341;&#21457;&#20102;&#19968;&#20123;&#38382;&#39064;&#65292;&#27604;&#22914;&#8220;&#26377;&#38480;&#30340;&#25968;&#25454;&#28857;&#23545;&#24212;&#20110;&#21333;&#19968;&#30340;&#27969;&#24418;&#21527;&#65311;&#8221;&#25110;&#32773;&#8220;&#21482;&#26377;&#19968;&#20010;&#22352;&#26631;&#22270;&#21487;&#20197;&#34920;&#31034;&#27969;&#24418;&#21527;&#65311;&#8221;&#23545;&#36825;&#20123;&#38382;&#39064;&#30340;&#22238;&#31572;&#26159;&#21542;&#23450;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#32473;&#23450;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#26377;&#22810;&#20010;&#35299;&#30340;&#33258;&#32534;&#30721;&#22120;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#26377;&#26102;&#20250;&#20135;&#29983;&#20855;&#26377;&#20005;&#37325;&#30072;&#21464;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#30340;&#38169;&#35823;&#27969;&#24418;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26368;&#36817;&#30340;&#20960;&#20309;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the geometric aspect of the autoencoder framework, which, despite its importance, has been relatively less recognized. Given a set of high-dimensional data points that approximately lie on some lower-dimensional manifold, an autoencoder learns the \textit{manifold} and its \textit{coordinate chart}, simultaneously. This geometric perspective naturally raises inquiries like "Does a finite set of data points correspond to a single manifold?" or "Is there only one coordinate chart that can represent the manifold?". The responses to these questions are negative, implying that there are multiple solution autoencoders given a dataset. Consequently, they sometimes produce incorrect manifolds with severely distorted latent space representations. In this paper, we introduce recent geometric approaches that address these issues.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23384;&#22312;&#28151;&#28102;&#25928;&#24212;&#26102;&#30340;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27491;&#20132;&#32467;&#26500;&#21644;&#32447;&#24615;&#25237;&#24433;&#30340;&#32479;&#35745;&#20272;&#35745;&#21644;&#25512;&#26029;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#26410;&#27979;&#28151;&#28102;&#22240;&#32032;&#24341;&#36215;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07261</link><description>&lt;p&gt;
&#20855;&#26377;&#26410;&#27979;&#28151;&#28102;&#22240;&#32032;&#30340;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#21516;&#26102;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Simultaneous inference for generalized linear models with unmeasured confounders. (arXiv:2309.07261v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23384;&#22312;&#28151;&#28102;&#25928;&#24212;&#26102;&#30340;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27491;&#20132;&#32467;&#26500;&#21644;&#32447;&#24615;&#25237;&#24433;&#30340;&#32479;&#35745;&#20272;&#35745;&#21644;&#25512;&#26029;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#26410;&#27979;&#28151;&#28102;&#22240;&#32032;&#24341;&#36215;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#22240;&#32452;&#30740;&#31350;&#20013;&#65292;&#24120;&#24120;&#36827;&#34892;&#25104;&#21315;&#19978;&#19975;&#20010;&#21516;&#26102;&#20551;&#35774;&#26816;&#39564;&#65292;&#20197;&#30830;&#23450;&#24046;&#24322;&#34920;&#36798;&#30340;&#22522;&#22240;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23384;&#22312;&#26410;&#27979;&#28151;&#28102;&#22240;&#32032;&#65292;&#35768;&#22810;&#26631;&#20934;&#32479;&#35745;&#26041;&#27861;&#21487;&#33021;&#23384;&#22312;&#20005;&#37325;&#30340;&#20559;&#24046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23384;&#22312;&#28151;&#28102;&#25928;&#24212;&#26102;&#30340;&#22810;&#20803;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#12290;&#22312;&#20219;&#24847;&#28151;&#28102;&#26426;&#21046;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#32479;&#35745;&#20272;&#35745;&#21644;&#25512;&#26029;&#26041;&#27861;&#65292;&#21033;&#29992;&#27491;&#20132;&#32467;&#26500;&#24182;&#23558;&#32447;&#24615;&#25237;&#24433;&#25972;&#21512;&#21040;&#19977;&#20010;&#20851;&#38190;&#38454;&#27573;&#20013;&#12290;&#39318;&#20808;&#65292;&#21033;&#29992;&#22810;&#20803;&#21709;&#24212;&#21464;&#37327;&#20998;&#31163;&#36793;&#38469;&#21644;&#19981;&#30456;&#20851;&#30340;&#28151;&#28102;&#25928;&#24212;&#65292;&#24674;&#22797;&#28151;&#28102;&#31995;&#25968;&#30340;&#21015;&#31354;&#38388;&#12290;&#38543;&#21518;&#65292;&#21033;&#29992;$\ell_1$&#27491;&#21017;&#21270;&#36827;&#34892;&#31232;&#30095;&#24615;&#20272;&#35745;&#65292;&#24182;&#24378;&#21152;&#27491;&#20132;&#24615;&#38480;&#21046;&#20110;&#28151;&#28102;&#31995;&#25968;&#65292;&#32852;&#21512;&#20272;&#35745;&#28508;&#22312;&#22240;&#23376;&#21644;&#20027;&#35201;&#25928;&#24212;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32467;&#21512;&#25237;&#24433;&#21644;&#21152;&#26435;&#20559;&#24046;&#26657;&#27491;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tens of thousands of simultaneous hypothesis tests are routinely performed in genomic studies to identify differentially expressed genes. However, due to unmeasured confounders, many standard statistical approaches may be substantially biased. This paper investigates the large-scale hypothesis testing problem for multivariate generalized linear models in the presence of confounding effects. Under arbitrary confounding mechanisms, we propose a unified statistical estimation and inference framework that harnesses orthogonal structures and integrates linear projections into three key stages. It first leverages multivariate responses to separate marginal and uncorrelated confounding effects, recovering the confounding coefficients' column space. Subsequently, latent factors and primary effects are jointly estimated, utilizing $\ell_1$-regularization for sparsity while imposing orthogonality onto confounding coefficients. Finally, we incorporate projected and weighted bias-correction steps 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23567;&#22411;&#36816;&#34892;&#26102;&#20462;&#25913;&#24341;&#20837;&#21322;&#32467;&#26500;&#28608;&#27963;&#31232;&#30095;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31232;&#30095;&#35757;&#32451;&#36807;&#31243;&#65292;&#22312;&#20445;&#25345;&#31934;&#24230;&#19979;&#38477;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#22788;&#29702;&#21644;&#25512;&#26029;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2309.06626</link><description>&lt;p&gt;
&#36890;&#36807;&#21322;&#32467;&#26500;&#28608;&#27963;&#31232;&#30095;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Accelerating Deep Neural Networks via Semi-Structured Activation Sparsity. (arXiv:2309.06626v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06626
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23567;&#22411;&#36816;&#34892;&#26102;&#20462;&#25913;&#24341;&#20837;&#21322;&#32467;&#26500;&#28608;&#27963;&#31232;&#30095;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31232;&#30095;&#35757;&#32451;&#36807;&#31243;&#65292;&#22312;&#20445;&#25345;&#31934;&#24230;&#19979;&#38477;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#22788;&#29702;&#21644;&#25512;&#26029;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#39640;&#25928;&#22788;&#29702;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#38656;&#27714;&#26159;&#38480;&#21046;&#20854;&#37096;&#32626;&#30340;&#37325;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#21033;&#29992;&#32593;&#32476;&#29305;&#24449;&#22270;&#20013;&#30340;&#31232;&#30095;&#24615;&#26159;&#20943;&#23569;&#25512;&#26029;&#24310;&#36831;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#24050;&#30693;&#38750;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#19982;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#30456;&#27604;&#23545;&#31934;&#24230;&#19979;&#38477;&#30340;&#24433;&#21709;&#36739;&#23567;&#65292;&#20294;&#21069;&#32773;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#30340;&#25512;&#26029;&#24341;&#25806;&#26356;&#25913;&#20197;&#33719;&#24471;&#24310;&#36831;&#20248;&#21183;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23567;&#22411;&#36816;&#34892;&#26102;&#20462;&#25913;&#24341;&#20837;&#21322;&#32467;&#26500;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#22312;&#25512;&#26029;&#26102;&#33719;&#24471;&#39640;&#21152;&#36895;&#24230;&#27700;&#24179;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31232;&#30095;&#35757;&#32451;&#36807;&#31243;&#65292;&#21516;&#26102;&#22312;&#35745;&#31639;&#24191;&#20041;&#30697;&#38453;&#20056;&#27861;&#65288;GEMM&#65289;&#26102;&#32771;&#34385;&#28608;&#27963;&#30340;&#26368;&#32456;&#20301;&#32622;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#22270;&#20687;&#20998;&#31867;&#21644;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#30340;&#27169;&#22411;&#23545;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#31934;&#24230;&#19979;&#38477;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;1.25&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
The demand for efficient processing of deep neural networks (DNNs) on embedded devices is a significant challenge limiting their deployment. Exploiting sparsity in the network's feature maps is one of the ways to reduce its inference latency. It is known that unstructured sparsity results in lower accuracy degradation with respect to structured sparsity but the former needs extensive inference engine changes to get latency benefits. To tackle this challenge, we propose a solution to induce semi-structured activation sparsity exploitable through minor runtime modifications. To attain high speedup levels at inference time, we design a sparse training procedure with awareness of the final position of the activations while computing the General Matrix Multiplication (GEMM). We extensively evaluate the proposed solution across various models for image classification and object detection tasks. Remarkably, our approach yields a speed improvement of $1.25 \times$ with a minimal accuracy drop 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;DNN&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#21069;&#21015;&#33146;&#32959;&#30244;&#30340;VERDICT&#27169;&#22411;&#21442;&#25968;&#24182;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.06268</link><description>&lt;p&gt;
ssVERDICT&#65306;&#29992;&#20110;&#22686;&#24378;&#21069;&#21015;&#33146;&#32959;&#30244;&#34920;&#24449;&#30340;&#33258;&#30417;&#30563;VERDICT-MRI
&lt;/p&gt;
&lt;p&gt;
ssVERDICT: Self-Supervised VERDICT-MRI for Enhanced Prostate Tumour Characterisation. (arXiv:2309.06268v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;DNN&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#21069;&#21015;&#33146;&#32959;&#30244;&#30340;VERDICT&#27169;&#22411;&#21442;&#25968;&#24182;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MRI&#22312;&#21069;&#21015;&#33146;&#30284;&#65288;PCa&#65289;&#30340;&#35786;&#26029;&#20013;&#36234;&#26469;&#36234;&#34987;&#20351;&#29992;&#65292;&#20854;&#20013;&#25193;&#25955;MRI&#65288;dMRI&#65289;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#24403;&#19982;&#35745;&#31639;&#27169;&#22411;&#32467;&#21512;&#26102;&#65292;dMRI&#21487;&#20197;&#20272;&#35745;&#32454;&#32990;&#22823;&#23567;&#31561;&#24494;&#35266;&#32467;&#26500;&#20449;&#24687;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#38750;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#65288;NLLS&#65289;&#26354;&#32447;&#25311;&#21512;&#26041;&#27861;&#36827;&#34892;&#25311;&#21512;&#65292;&#19982;&#39640;&#35745;&#31639;&#25104;&#26412;&#30456;&#20851;&#12290;&#30417;&#30563;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20294;&#20854;&#24615;&#33021;&#21463;&#21040;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#24213;&#23618;&#20998;&#24067;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#32593;&#32476;&#22312;&#27492;&#26041;&#27861;&#20013;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;&#30340;&#29305;&#24449;&#65292;&#32780;&#19981;&#20351;&#29992;&#21333;&#29420;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36825;&#31181;&#26041;&#27861;&#20165;&#24212;&#29992;&#20110;&#23545;&#24494;&#19981;&#36275;&#36947;&#30340;dMRI&#27169;&#22411;&#30340;&#25311;&#21512;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;DNN&#65292;&#29992;&#20110;&#20272;&#35745;&#21069;&#21015;&#33146;VERDICT&#65288;Tumours&#20013;&#30340;&#34880;&#31649;&#12289;&#32454;&#32990;&#22806;&#21644;&#21463;&#38480;&#25193;&#25955;&#65289;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#21069;&#21015;&#33146;&#32959;&#30244;&#30340;VERDICT&#27169;&#22411;&#30340;&#21442;&#25968;&#21487;&#20197;&#26377;&#25928;&#20272;&#35745;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
MRI is increasingly being used in the diagnosis of prostate cancer (PCa), with diffusion MRI (dMRI) playing an integral role. When combined with computational models, dMRI can estimate microstructural information such as cell size. Conventionally, such models are fit with a nonlinear least squares (NLLS) curve fitting approach, associated with a high computational cost. Supervised deep neural networks (DNNs) are an efficient alternative, however their performance is significantly affected by the underlying distribution of the synthetic training data. Self-supervised learning is an attractive alternative, where instead of using a separate training dataset, the network learns the features of the input data itself. This approach has only been applied to fitting of trivial dMRI models thus far. Here, we introduce a self-supervised DNN to estimate the parameters of the VERDICT (Vascular, Extracellular and Restricted DIffusion for Cytometry in Tumours) model for prostate. We demonstrate, for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CoALA&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#29992;&#20110;&#32452;&#32455;&#35821;&#35328;&#20195;&#29702;&#30340;&#29616;&#26377;&#30740;&#31350;&#24182;&#35268;&#21010;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;CoALA&#25551;&#36848;&#20102;&#19968;&#20010;&#20855;&#26377;&#27169;&#22359;&#21270;&#35760;&#24518;&#32452;&#20214;&#12289;&#32467;&#26500;&#21270;&#34892;&#21160;&#31354;&#38388;&#21644;&#36890;&#29992;&#20915;&#31574;&#36807;&#31243;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;&#36890;&#36807;&#36825;&#19968;&#26694;&#26550;&#65292;&#26377;&#26395;&#21457;&#23637;&#20986;&#26356;&#24378;&#22823;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.02427</link><description>&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#30340;&#35748;&#30693;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Cognitive Architectures for Language Agents. (arXiv:2309.02427v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CoALA&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#29992;&#20110;&#32452;&#32455;&#35821;&#35328;&#20195;&#29702;&#30340;&#29616;&#26377;&#30740;&#31350;&#24182;&#35268;&#21010;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;CoALA&#25551;&#36848;&#20102;&#19968;&#20010;&#20855;&#26377;&#27169;&#22359;&#21270;&#35760;&#24518;&#32452;&#20214;&#12289;&#32467;&#26500;&#21270;&#34892;&#21160;&#31354;&#38388;&#21644;&#36890;&#29992;&#20915;&#31574;&#36807;&#31243;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;&#36890;&#36807;&#36825;&#19968;&#26694;&#26550;&#65292;&#26377;&#26395;&#21457;&#23637;&#20986;&#26356;&#24378;&#22823;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#22686;&#21152;&#20102;&#22806;&#37096;&#36164;&#28304;&#65288;&#20363;&#22914;&#20114;&#32852;&#32593;&#65289;&#25110;&#20869;&#37096;&#25511;&#21046;&#27969;&#65288;&#20363;&#22914;&#25552;&#31034;&#38142;&#65289;&#65292;&#29992;&#20110;&#38656;&#35201;&#22522;&#20110;&#35821;&#22659;&#25110;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#31867;&#26032;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;&#23613;&#31649;&#36825;&#20123;&#20195;&#29702;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#30340;&#26694;&#26550;&#26469;&#32452;&#32455;&#29616;&#26377;&#20195;&#29702;&#24182;&#35268;&#21010;&#26410;&#26469;&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#35748;&#30693;&#31185;&#23398;&#21644;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#20016;&#23500;&#21382;&#21490;&#65292;&#25552;&#20986;&#20102;&#35821;&#35328;&#20195;&#29702;&#30340;&#35748;&#30693;&#26550;&#26500;&#65288;CoALA&#65289;&#12290;CoALA&#25551;&#36848;&#20102;&#19968;&#20010;&#20855;&#26377;&#27169;&#22359;&#21270;&#35760;&#24518;&#32452;&#20214;&#12289;&#29992;&#20110;&#19982;&#20869;&#37096;&#35760;&#24518;&#21644;&#22806;&#37096;&#29615;&#22659;&#20132;&#20114;&#30340;&#32467;&#26500;&#21270;&#34892;&#21160;&#31354;&#38388;&#20197;&#21450;&#36873;&#25321;&#34892;&#21160;&#30340;&#36890;&#29992;&#20915;&#31574;&#36807;&#31243;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;CoALA&#23545;&#26368;&#36817;&#30340;&#22823;&#37327;&#30740;&#31350;&#36827;&#34892;&#20102;&#22238;&#39038;&#21644;&#32452;&#32455;&#65292;&#24182;&#23637;&#26395;&#20102;&#26356;&#24378;&#22823;&#20195;&#29702;&#30340;&#21487;&#34892;&#26041;&#21521;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;CoALA&#23558;&#24403;&#20170;&#30340;&#35821;&#35328;&#20195;&#29702;&#32622;&#20110;&#19978;&#19979;&#25991;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent efforts have augmented large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning, leading to a new class of language agents. While these agents have achieved substantial empirical success, we lack a systematic framework to organize existing agents and plan future developments. In this paper, we draw on the rich history of cognitive science and symbolic artificial intelligence to propose Cognitive Architectures for Language Agents (CoALA). CoALA describes a language agent with modular memory components, a structured action space to interact with internal memory and external environments, and a generalized decision-making process to choose actions. We use CoALA to retrospectively survey and organize a large body of recent work, and prospectively identify actionable directions towards more capable agents. Taken together, CoALA contextualizes today's language agents within th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#28304;&#25968;&#25454;&#30340;&#20998;&#24067;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#32452;&#20998;&#24067;&#40065;&#26834;&#39044;&#27979;&#27169;&#22411;&#26469;&#25552;&#39640;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#30340;&#30446;&#26631;&#20154;&#32676;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.02211</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#28304;&#25968;&#25454;&#30340;&#20998;&#24067;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Machine Learning with Multi-source Data. (arXiv:2309.02211v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#28304;&#25968;&#25454;&#30340;&#20998;&#24067;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#32452;&#20998;&#24067;&#40065;&#26834;&#39044;&#27979;&#27169;&#22411;&#26469;&#25552;&#39640;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#30340;&#30446;&#26631;&#20154;&#32676;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#30446;&#26631;&#20998;&#24067;&#19982;&#28304;&#25968;&#25454;&#38598;&#19981;&#21516;&#26102;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#36739;&#24046;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#26412;&#25991;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#28304;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#20998;&#24067;&#40065;&#26834;&#39044;&#27979;&#27169;&#22411;&#26469;&#20248;&#21270;&#20851;&#20110;&#30446;&#26631;&#20998;&#24067;&#31867;&#30340;&#21487;&#35299;&#37322;&#26041;&#24046;&#30340;&#23545;&#25239;&#24615;&#22870;&#21169;&#12290;&#19982;&#20256;&#32479;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#40065;&#26834;&#39044;&#27979;&#27169;&#22411;&#25913;&#21892;&#20102;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#30340;&#30446;&#26631;&#20154;&#32676;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#32452;&#20998;&#24067;&#40065;&#26834;&#39044;&#27979;&#27169;&#22411;&#26159;&#28304;&#25968;&#25454;&#38598;&#26465;&#20214;&#32467;&#26524;&#27169;&#22411;&#30340;&#21152;&#26435;&#24179;&#22343;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#20851;&#38190;&#37492;&#21035;&#32467;&#26524;&#26469;&#25552;&#39640;&#20219;&#24847;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#21644;&#31070;&#32463;&#32593;&#32476;&#31561;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#24046;&#26657;&#27491;&#20272;&#35745;&#22120;&#26469;&#20272;&#35745;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#20248;&#32858;&#21512;&#26435;&#37325;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;c&#26041;&#38754;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical machine learning methods may lead to poor prediction performance when the target distribution differs from the source populations. This paper utilizes data from multiple sources and introduces a group distributionally robust prediction model defined to optimize an adversarial reward about explained variance with respect to a class of target distributions. Compared to classical empirical risk minimization, the proposed robust prediction model improves the prediction accuracy for target populations with distribution shifts. We show that our group distributionally robust prediction model is a weighted average of the source populations' conditional outcome models. We leverage this key identification result to robustify arbitrary machine learning algorithms, including, for example, random forests and neural networks. We devise a novel bias-corrected estimator to estimate the optimal aggregation weight for general machine-learning algorithms and demonstrate its improvement in the c
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#8212;&#8212;&#32447;&#24615;&#25391;&#21160;&#65288;LoC&#65289;&#28608;&#27963;&#20989;&#25968;&#65292;&#23427;&#36890;&#36807;&#23558;&#32447;&#24615;&#36712;&#36857;&#21644;&#25391;&#33633;&#20559;&#24046;&#26080;&#32541;&#34701;&#21512;&#65292;&#25429;&#25417;&#21040;&#20102;&#8220;&#22256;&#24785;&#30340;&#37325;&#35201;&#24615;&#8221;&#30340;&#26412;&#36136;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#38656;&#35201;&#21306;&#20998;&#24494;&#22937;&#27169;&#24335;&#30340;&#24773;&#22659;&#20013;&#65292;&#20351;&#29992;LoC&#28608;&#27963;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13670</link><description>&lt;p&gt;
&#32447;&#24615;&#25391;&#21160;&#65306;&#35270;&#35273;&#36716;&#25442;&#22120;&#20013;&#30340;&#22256;&#24785;&#32654;&#23398;
&lt;/p&gt;
&lt;p&gt;
Linear Oscillation: The Aesthetics of Confusion for Vision Transformer. (arXiv:2308.13670v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13670
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#8212;&#8212;&#32447;&#24615;&#25391;&#21160;&#65288;LoC&#65289;&#28608;&#27963;&#20989;&#25968;&#65292;&#23427;&#36890;&#36807;&#23558;&#32447;&#24615;&#36712;&#36857;&#21644;&#25391;&#33633;&#20559;&#24046;&#26080;&#32541;&#34701;&#21512;&#65292;&#25429;&#25417;&#21040;&#20102;&#8220;&#22256;&#24785;&#30340;&#37325;&#35201;&#24615;&#8221;&#30340;&#26412;&#36136;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#38656;&#35201;&#21306;&#20998;&#24494;&#22937;&#27169;&#24335;&#30340;&#24773;&#22659;&#20013;&#65292;&#20351;&#29992;LoC&#28608;&#27963;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28608;&#27963;&#20989;&#25968;&#26159;&#28145;&#24230;&#23398;&#20064;&#30340;&#20851;&#38190;&#65292;&#28145;&#21051;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#21644;&#35757;&#32451;&#21160;&#21147;&#23398;&#12290;&#23427;&#20204;&#19981;&#20165;&#22609;&#36896;&#20102;&#34920;&#31034;&#30340;&#24615;&#36136;&#65292;&#36824;&#20248;&#21270;&#20102;&#25910;&#25947;&#36895;&#24230;&#24182;&#22686;&#24378;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#37492;&#20110;&#36825;&#19968;&#20851;&#38190;&#20316;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32447;&#24615;&#25391;&#21160;&#65288;LoC&#65289;&#28608;&#27963;&#20989;&#25968;&#65292;&#23450;&#20041;&#20026;$f(x) = x \times \sin(\alpha x + \beta)$&#12290;&#19982;&#20256;&#32479;&#30340;&#28608;&#27963;&#20989;&#25968;&#19981;&#21516;&#65292;LoC&#23558;&#32447;&#24615;&#36712;&#36857;&#19982;&#25391;&#33633;&#20559;&#24046;&#26080;&#32541;&#34701;&#21512;&#12290;&#21629;&#21517;&#20026;&#8220;&#32447;&#24615;&#25391;&#21160;&#8221;&#26159;&#23545;&#20854;&#29420;&#29305;&#23646;&#24615;&#30340;&#33268;&#25964;&#65292;&#21363;&#36890;&#36807;&#21644;&#35856;&#30340;&#25391;&#21160;&#34701;&#20837;&#32447;&#24615;&#28608;&#27963;&#65292;&#25429;&#25417;&#8220;&#22256;&#24785;&#30340;&#37325;&#35201;&#24615;&#8221;&#30340;&#26412;&#36136;&#12290;&#32593;&#32476;&#28608;&#27963;&#20013;&#30340;&#8220;&#25511;&#21046;&#24615;&#22256;&#24785;&#8221;&#27010;&#24565;&#34987;&#35748;&#20026;&#33021;&#22815;&#20419;&#36827;&#26356;&#31283;&#20581;&#30340;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#21306;&#20998;&#24494;&#22937;&#27169;&#24335;&#30340;&#24773;&#22659;&#20013;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;LoC&#28608;&#27963;&#20989;&#25968;&#26102;&#65292;&#32593;&#32476;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#38544;&#34255;&#30340;&#27169;&#24335;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Activation functions are the linchpins of deep learning, profoundly influencing both the representational capacity and training dynamics of neural networks. They shape not only the nature of representations but also optimize convergence rates and enhance generalization potential. Appreciating this critical role, we present the Linear Oscillation (LoC) activation function, defined as $f(x) = x \times \sin(\alpha x + \beta)$. Distinct from conventional activation functions which primarily introduce non-linearity, LoC seamlessly blends linear trajectories with oscillatory deviations. The nomenclature ``Linear Oscillation'' is a nod to its unique attribute of infusing linear activations with harmonious oscillations, capturing the essence of the 'Importance of Confusion'. This concept of ``controlled confusion'' within network activations is posited to foster more robust learning, particularly in contexts that necessitate discerning subtle patterns. Our empirical studies reveal that, when i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;ResNet&#27169;&#22411;&#21644;&#36731;&#37327;&#32423;&#27880;&#24847;&#26426;&#21046;&#26694;&#26550;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#29305;&#24449;&#34920;&#31034;&#12289;&#22686;&#24378;&#20998;&#31867;&#33021;&#21147;&#21644;&#25913;&#21892;&#29305;&#24449;&#21487;&#36776;&#21035;&#24615;&#65292;&#22312;&#20083;&#33146;&#30284;&#20998;&#31867;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#21331;&#36234;&#24615;&#33021;&#21644;&#28508;&#22312;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2308.13150</link><description>&lt;p&gt;
&#20351;&#29992;&#24102;&#36731;&#37327;&#32423;&#27880;&#24847;&#26426;&#21046;&#30340;&#36801;&#31227;ResNet&#22686;&#24378;&#20083;&#33146;&#30284;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Enhancing Breast Cancer Classification Using Transfer ResNet with Lightweight Attention Mechanism. (arXiv:2308.13150v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;ResNet&#27169;&#22411;&#21644;&#36731;&#37327;&#32423;&#27880;&#24847;&#26426;&#21046;&#26694;&#26550;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#29305;&#24449;&#34920;&#31034;&#12289;&#22686;&#24378;&#20998;&#31867;&#33021;&#21147;&#21644;&#25913;&#21892;&#29305;&#24449;&#21487;&#36776;&#21035;&#24615;&#65292;&#22312;&#20083;&#33146;&#30284;&#20998;&#31867;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#21331;&#36234;&#24615;&#33021;&#21644;&#28508;&#22312;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#21407;&#22987;&#20687;&#32032;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#29305;&#24449;&#23618;&#27425;&#32467;&#26500;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#22270;&#20687;&#20998;&#31867;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;ResNet&#27169;&#22411;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#36731;&#37327;&#32423;&#27880;&#24847;&#26426;&#21046;&#26694;&#26550;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#35813;&#26694;&#26550;&#20248;&#21270;&#29305;&#24449;&#34920;&#31034;&#65292;&#22686;&#24378;&#20998;&#31867;&#33021;&#21147;&#65292;&#25913;&#21892;&#29305;&#24449;&#21487;&#36776;&#21035;&#24615;&#12290;&#25105;&#20204;&#22312;Breakhis&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#35768;&#22810;&#26041;&#38754;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#20256;&#32479;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20248;&#36234;&#65292;&#22312;&#24403;&#20195;&#35270;&#35273;&#21464;&#25442;&#22120;&#31561;&#26368;&#26032;&#26041;&#27861;&#19978;&#20063;&#26174;&#31034;&#20986;&#20248;&#21183;&#12290;&#22312;&#35832;&#22914;&#31934;&#24230;&#12289;&#20934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#12289;F1&#20998;&#25968;&#21644;G-means&#31561;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#21516;&#26102;&#22312;&#25910;&#25947;&#26102;&#38388;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#20123;&#32467;&#26524;&#22686;&#24378;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24041;&#22266;&#20102;&#20854;&#22312;&#23454;&#38469;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have revolutionized image classification by learning complex feature hierarchies in raw pixel data. This paper introduces an image classification method based on the ResNet model, and introduces a lightweight attention mechanism framework to improve performance. The framework optimizes feature representation, enhances classification capabilities, and improves feature discriminativeness. We verified the effectiveness of the algorithm on the Breakhis dataset, showing its superior performance in many aspects. Not only in terms of conventional models, our method also shows advantages on state-of-the-art methods such as contemporary visual transformers. Significant improvements have been achieved in metrics such as precision, accuracy, recall, F1-score, and G-means, while also performing well in terms of convergence time. These results strengthen the performance of the algorithm and solidify its application prospects in practical image classification tasks. Keywords: Re
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#36127;&#36131;&#20219;&#30340;AI&#25968;&#25454;&#25910;&#38598;&#65292;&#38656;&#35201;&#23545;&#25968;&#25454;&#30340;&#36136;&#37327;&#36827;&#34892;&#24443;&#24213;&#30340;&#23457;&#26597;&#65292;&#36991;&#20813;&#19981;&#20844;&#24179;&#12289;&#20559;&#35265;&#25110;&#19981;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.12885</link><description>&lt;p&gt;
&#25910;&#38598;&#65292;&#27979;&#37327;&#65292;&#37325;&#22797;&#65306;&#36127;&#36131;&#20219;&#30340;AI&#25968;&#25454;&#25910;&#38598;&#30340;&#21487;&#38752;&#24615;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Collect, Measure, Repeat: Reliability Factors for Responsible AI Data Collection. (arXiv:2308.12885v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12885
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#36127;&#36131;&#20219;&#30340;AI&#25968;&#25454;&#25910;&#38598;&#65292;&#38656;&#35201;&#23545;&#25968;&#25454;&#30340;&#36136;&#37327;&#36827;&#34892;&#24443;&#24213;&#30340;&#23457;&#26597;&#65292;&#36991;&#20813;&#19981;&#20844;&#24179;&#12289;&#20559;&#35265;&#25110;&#19981;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36805;&#36895;&#36827;&#20837;&#25105;&#20204;&#30340;&#26085;&#24120;&#27963;&#21160;&#21644;&#39640;&#39118;&#38505;&#39046;&#22495;&#65292;&#35201;&#27714;&#23545;&#20854;&#20844;&#24179;&#24615;&#21644;&#21487;&#38752;&#24615;&#36827;&#34892;&#36879;&#26126;&#21644;&#23457;&#26597;&#12290;&#20026;&#20102;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#65292;&#30740;&#31350;&#36890;&#24120;&#20250;&#38598;&#20013;&#22312;&#20854;&#37096;&#32626;&#25152;&#20351;&#29992;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#65292;&#20363;&#22914;&#21019;&#24314;&#21644;&#32500;&#25252;&#25991;&#20214;&#20197;&#20102;&#35299;&#20854;&#26469;&#28304;&#12289;&#24320;&#21457;&#36807;&#31243;&#21644;&#20262;&#29702;&#32771;&#34385;&#12290;&#28982;&#32780;&#65292;AI&#30340;&#25968;&#25454;&#25910;&#38598;&#36890;&#24120;&#20173;&#28982;&#26159;&#19968;&#27425;&#24615;&#30340;&#23454;&#36341;&#65292;&#32780;&#19988;&#32463;&#24120;&#20026;&#29305;&#23450;&#30446;&#30340;&#25110;&#24212;&#29992;&#31243;&#24207;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20250;&#34987;&#37325;&#22797;&#29992;&#20110;&#20854;&#20182;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#38598;&#30340;&#27880;&#37322;&#21487;&#33021;&#38543;&#26102;&#38388;&#19981;&#20855;&#26377;&#20195;&#34920;&#24615;&#65292;&#21253;&#21547;&#27169;&#31946;&#25110;&#38169;&#35823;&#30340;&#27880;&#37322;&#65292;&#25110;&#32773;&#26080;&#27861;&#36328;&#38382;&#39064;&#25110;&#39046;&#22495;&#36827;&#34892;&#27867;&#21270;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#20570;&#27861;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#12289;&#20559;&#35265;&#25110;&#19981;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;AI&#30340;&#25968;&#25454;&#25910;&#38598;&#24212;&#35813;&#20197;&#36127;&#36131;&#20219;&#30340;&#26041;&#24335;&#36827;&#34892;&#65292;&#23545;&#25968;&#25454;&#30340;&#36136;&#37327;&#36827;&#34892;&#24443;&#24213;&#30340;&#23457;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid entry of machine learning approaches in our daily activities and high-stakes domains demands transparency and scrutiny of their fairness and reliability. To help gauge machine learning models' robustness, research typically focuses on the massive datasets used for their deployment, e.g., creating and maintaining documentation for understanding their origin, process of development, and ethical considerations. However, data collection for AI is still typically a one-off practice, and oftentimes datasets collected for a certain purpose or application are reused for a different problem. Additionally, dataset annotations may not be representative over time, contain ambiguous or erroneous annotations, or be unable to generalize across issues or domains. Recent research has shown these practices might lead to unfair, biased, or inaccurate outcomes. We argue that data collection for AI should be performed in a responsible manner where the quality of the data is thoroughly scrutinized
&lt;/p&gt;</description></item><item><title>FOSA&#26159;&#19968;&#31181;&#20840;&#20449;&#24687;&#26368;&#22823;&#20284;&#28982; (FIML) &#20248;&#21270;&#30340;&#33258;&#27880;&#24847;&#21147;&#32570;&#22833;&#25968;&#25454;&#34917;&#20840;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;FIML&#20272;&#35745;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#20934;&#30830;&#24615;&#12289;&#35745;&#31639;&#25928;&#29575;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.12388</link><description>&lt;p&gt;
FOSA: &#20840;&#20449;&#24687;&#26368;&#22823;&#20284;&#28982; (FIML) &#20248;&#21270;&#30340;&#33258;&#27880;&#24847;&#21147;&#32570;&#22833;&#25968;&#25454;&#34917;&#20840;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FOSA: Full Information Maximum Likelihood (FIML) Optimized Self-Attention Imputation for Missing Data. (arXiv:2308.12388v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12388
&lt;/p&gt;
&lt;p&gt;
FOSA&#26159;&#19968;&#31181;&#20840;&#20449;&#24687;&#26368;&#22823;&#20284;&#28982; (FIML) &#20248;&#21270;&#30340;&#33258;&#27880;&#24847;&#21147;&#32570;&#22833;&#25968;&#25454;&#34917;&#20840;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;FIML&#20272;&#35745;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#20934;&#30830;&#24615;&#12289;&#35745;&#31639;&#25928;&#29575;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#34917;&#20840;&#20013;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#32570;&#22833;&#20540;&#23588;&#20026;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#20013;&#12290;&#26412;&#35770;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;FIML&#20248;&#21270;&#33258;&#27880;&#24847;&#21147;&#65288;FOSA&#65289;&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#34701;&#21512;&#20102;&#20840;&#20449;&#24687;&#26368;&#22823;&#20284;&#28982;&#65288;FIML&#65289;&#20272;&#35745;&#21644;&#33258;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#33021;&#21147;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#36890;&#36807;FIML&#23545;&#32570;&#22833;&#20540;&#36827;&#34892;&#21021;&#22987;&#20272;&#35745;&#65292;&#28982;&#21518;&#36890;&#36807;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#36827;&#19968;&#27493;&#25552;&#28860;&#36825;&#20123;&#20272;&#35745;&#20540;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#20840;&#38754;&#23454;&#39564;&#35777;&#26126;&#20102;FOSA&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;FIML&#25216;&#26415;&#22312;&#20934;&#30830;&#24615;&#12289;&#35745;&#31639;&#25928;&#29575;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#21363;&#20351;&#22312;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#65288;SEM&#65289;&#21487;&#33021;&#38169;&#35823;&#35268;&#23450;&#23548;&#33268;&#23376;&#20248;&#30340;FIML&#20272;&#35745;&#30340;&#24773;&#20917;&#19979;&#65292;FOSA&#33258;&#27880;&#24847;&#21147;&#32452;&#20214;&#30340;&#31283;&#20581;&#26550;&#26500;&#33021;&#22815;&#28789;&#27963;&#22320;&#32416;&#27491;&#21644;&#20248;&#21270;&#34917;&#20840;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In data imputation, effectively addressing missing values is pivotal, especially in intricate datasets. This paper delves into the FIML Optimized Self-attention (FOSA) framework, an innovative approach that amalgamates the strengths of Full Information Maximum Likelihood (FIML) estimation with the capabilities of self-attention neural networks. Our methodology commences with an initial estimation of missing values via FIML, subsequently refining these estimates by leveraging the self-attention mechanism. Our comprehensive experiments on both simulated and real-world datasets underscore FOSA's pronounced advantages over traditional FIML techniques, encapsulating facets of accuracy, computational efficiency, and adaptability to diverse data structures. Intriguingly, even in scenarios where the Structural Equation Model (SEM) might be mis-specified, leading to suboptimal FIML estimates, the robust architecture of FOSA's self-attention component adeptly rectifies and optimizes the imputati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#37327;&#23376;&#22122;&#22768;&#39537;&#21160;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#37327;&#23376;&#29305;&#24615;&#20197;&#20811;&#26381;&#20256;&#32479;&#27169;&#22411;&#30340;&#20027;&#35201;&#35745;&#31639;&#22256;&#38590;&#65292;&#24182;&#24314;&#35758;&#23558;&#37327;&#23376;&#22122;&#22768;&#35270;&#20026;&#21487;&#21033;&#29992;&#30340;&#29305;&#24615;&#32780;&#38750;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12013</link><description>&lt;p&gt;
&#37327;&#23376;&#22122;&#22768;&#39537;&#21160;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Quantum-Noise-driven Generative Diffusion Models. (arXiv:2308.12013v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12013
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#37327;&#23376;&#22122;&#22768;&#39537;&#21160;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#37327;&#23376;&#29305;&#24615;&#20197;&#20811;&#26381;&#20256;&#32479;&#27169;&#22411;&#30340;&#20027;&#35201;&#35745;&#31639;&#22256;&#38590;&#65292;&#24182;&#24314;&#35758;&#23558;&#37327;&#23376;&#22122;&#22768;&#35270;&#20026;&#21487;&#21033;&#29992;&#30340;&#29305;&#24615;&#32780;&#38750;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#30340;&#29983;&#25104;&#27169;&#22411;&#26159;&#20174;&#26377;&#38480;&#30340;&#35757;&#32451;&#26679;&#26412;&#20013;&#25512;&#26029;&#20986;&#22797;&#26434;&#21644;&#26410;&#30693;&#25968;&#25454;&#20998;&#24067;&#24182;&#20135;&#29983;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26694;&#26550;&#65292;&#26368;&#36817;&#22312;&#21019;&#24314;&#21512;&#25104;&#25991;&#26412;&#21644;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#24050;&#32463;&#36229;&#36234;&#20102;&#29983;&#25104;&#23545;&#25239;&#24615;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35752;&#35770;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#37327;&#23376;&#25512;generalization&#65292;&#21363;&#19977;&#31181;&#21487;&#33021;&#22312;&#23454;&#38469;&#37327;&#23376;&#31995;&#32479;&#19978;&#36827;&#34892;&#23454;&#39564;&#30340;&#37327;&#23376;&#22122;&#22768;&#39537;&#21160;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#21033;&#29992;&#29420;&#29305;&#30340;&#37327;&#23376;&#29305;&#24615;&#65292;&#29305;&#21035;&#26159;&#30446;&#21069;&#21487;&#29992;&#30340;&#26377;&#22122;&#22768;&#37327;&#23376;&#22788;&#29702;&#22120;&#19981;&#21487;&#36991;&#20813;&#22320;&#21463;&#21040;&#30340;&#30456;&#24178;&#24615;&#12289;&#32416;&#32544;&#24615;&#21644;&#22122;&#22768;&#20043;&#38388;&#30340;&#38750;&#24179;&#20961;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#20811;&#26381;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#20027;&#35201;&#35745;&#31639;&#36127;&#25285;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#37327;&#23376;&#22122;&#22768;&#19981;&#20316;&#20026;&#38656;&#35201;&#26816;&#27979;&#21644;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#32780;&#26159;&#20316;&#20026;&#19968;&#31181;&#21487;&#21033;&#29992;&#30340;&#29305;&#24615;&#65292;&#20351;&#24471;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models realized with machine learning techniques are powerful tools to infer complex and unknown data distributions from a finite number of training samples in order to produce new synthetic data. Diffusion models are an emerging framework that have recently overcome the performance of the generative adversarial networks in creating synthetic text and high-quality images. Here, we propose and discuss the quantum generalization of diffusion models, i.e., three quantum-noise-driven generative diffusion models that could be experimentally tested on real quantum systems. The idea is to harness unique quantum features, in particular the non-trivial interplay among coherence, entanglement and noise that the currently available noisy quantum processors do unavoidably suffer from, in order to overcome the main computational burdens of classical diffusion models during inference. Hence, we suggest to exploit quantum noise not as an issue to be detected and solved but instead as a ver
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#26080;&#33539;&#20363;&#31867;&#22686;&#37327;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#21021;&#22987;&#35757;&#32451;&#31574;&#30053;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21021;&#22987;&#23398;&#20064;&#31574;&#30053;&#30340;&#36873;&#25321;&#20250;&#26174;&#33879;&#24433;&#21709;&#22686;&#37327;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2308.11677</link><description>&lt;p&gt;
&#26080;&#33539;&#20363;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#21021;&#22987;&#35757;&#32451;&#31574;&#30053;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis of Initial Training Strategies for Exemplar-Free Class-Incremental Learning. (arXiv:2308.11677v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#26080;&#33539;&#20363;&#31867;&#22686;&#37327;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#21021;&#22987;&#35757;&#32451;&#31574;&#30053;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21021;&#22987;&#23398;&#20064;&#31574;&#30053;&#30340;&#36873;&#25321;&#20250;&#26174;&#33879;&#24433;&#21709;&#22686;&#37327;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22686;&#37327;&#23398;&#20064;&#26088;&#22312;&#20174;&#25968;&#25454;&#27969;&#20013;&#26500;&#24314;&#20998;&#31867;&#27169;&#22411;&#12290;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#36807;&#31243;&#30340;&#27599;&#19968;&#27493;&#20013;&#65292;&#26032;&#30340;&#31867;&#21035;&#24517;&#39035;&#34987;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#12290;&#30001;&#20110;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24403;&#26080;&#27861;&#23384;&#20648;&#36807;&#21435;&#31867;&#21035;&#30340;&#26679;&#26412;&#26102;&#65292;&#31867;&#22686;&#37327;&#23398;&#20064;&#21464;&#24471;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#27491;&#26159;&#25105;&#20204;&#22312;&#27492;&#30740;&#31350;&#30340;&#23545;&#35937;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20165;&#22522;&#20110;&#31867;&#22686;&#37327;&#23398;&#20064;&#36807;&#31243;&#30340;&#30446;&#26631;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#22312;&#22823;&#37327;&#25968;&#25454;&#19978;&#36890;&#36807;&#33258;&#30417;&#30563;&#26041;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20351;&#29992;&#24050;&#32463;&#36880;&#28176;&#22686;&#22810;&#12290;&#31867;&#22686;&#37327;&#23398;&#20064;&#36807;&#31243;&#30340;&#21021;&#22987;&#27169;&#22411;&#21487;&#33021;&#20165;&#20351;&#29992;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#31532;&#19968;&#25209;&#25968;&#25454;&#65292;&#25110;&#32773;&#36824;&#21487;&#20197;&#20351;&#29992;&#22312;&#36741;&#21161;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#12290;&#36825;&#20004;&#31181;&#21021;&#22987;&#23398;&#20064;&#31574;&#30053;&#30340;&#36873;&#25321;&#21487;&#20197;&#26497;&#22823;&#22320;&#24433;&#21709;&#22686;&#37327;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#12290;&#24615;&#33021;&#36824;&#21463;&#21040;&#31867;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#30340;&#36873;&#25321;&#12289;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12289;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#36136;&#12289;&#31867;&#21035;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-Incremental Learning (CIL) aims to build classification models from data streams. At each step of the CIL process, new classes must be integrated into the model. Due to catastrophic forgetting, CIL is particularly challenging when examples from past classes cannot be stored, the case on which we focus here. To date, most approaches are based exclusively on the target dataset of the CIL process. However, the use of models pre-trained in a self-supervised way on large amounts of data has recently gained momentum. The initial model of the CIL process may only use the first batch of the target dataset, or also use pre-trained weights obtained on an auxiliary dataset. The choice between these two initial learning strategies can significantly influence the performance of the incremental learning model, but has not yet been studied in depth. Performance is also influenced by the choice of the CIL algorithm, the neural architecture, the nature of the target task, the distribution of clas
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34892;&#21160;&#21644;&#35821;&#35328;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#26234;&#33021;&#20307;&#65292;&#20854;&#33021;&#22815;&#21516;&#26102;&#20316;&#20026;&#25945;&#24072;&#21644;&#23398;&#20064;&#32773;&#65292;&#36890;&#36807;&#34892;&#21160;&#28436;&#31034;&#21644;&#35821;&#35328;&#25351;&#20196;&#22686;&#24378;&#20102;&#27807;&#36890;&#25928;&#29575;&#65292;&#24182;&#25506;&#32034;&#20102;&#32467;&#21512;&#34892;&#21160;&#21644;&#35821;&#35328;&#27807;&#36890;&#27169;&#24335;&#23545;&#23398;&#20064;&#32467;&#26524;&#30340;&#31215;&#26497;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.10842</link><description>&lt;p&gt;
&#36890;&#36807;&#34892;&#21160;&#21644;&#35821;&#35328;&#25552;&#21319;&#26234;&#33021;&#20307;&#30340;&#27807;&#36890;&#21644;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Agent Communication and Learning through Action and Language. (arXiv:2308.10842v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10842
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34892;&#21160;&#21644;&#35821;&#35328;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#26234;&#33021;&#20307;&#65292;&#20854;&#33021;&#22815;&#21516;&#26102;&#20316;&#20026;&#25945;&#24072;&#21644;&#23398;&#20064;&#32773;&#65292;&#36890;&#36807;&#34892;&#21160;&#28436;&#31034;&#21644;&#35821;&#35328;&#25351;&#20196;&#22686;&#24378;&#20102;&#27807;&#36890;&#25928;&#29575;&#65292;&#24182;&#25506;&#32034;&#20102;&#32467;&#21512;&#34892;&#21160;&#21644;&#35821;&#35328;&#27807;&#36890;&#27169;&#24335;&#23545;&#23398;&#20064;&#32467;&#26524;&#30340;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;GC&#26234;&#33021;&#20307;&#31867;&#21035;&#65292;&#33021;&#22815;&#21516;&#26102;&#20805;&#24403;&#25945;&#24072;&#21644;&#23398;&#20064;&#32773;&#30340;&#35282;&#33394;&#12290;&#20511;&#21161;&#22522;&#20110;&#34892;&#21160;&#30340;&#28436;&#31034;&#21644;&#22522;&#20110;&#35821;&#35328;&#30340;&#25351;&#20196;&#65292;&#36825;&#20123;&#26234;&#33021;&#20307;&#22686;&#24378;&#20102;&#27807;&#36890;&#25928;&#29575;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20154;&#31867;&#27807;&#36890;&#21644;&#30446;&#26631;&#23454;&#29616;&#20013;&#30340;&#37325;&#35201;&#20803;&#32032;&#8212;&#8212;&#25945;&#32946;&#23398;&#21644;&#23454;&#29992;&#20027;&#20041;&#30340;&#34701;&#20837;&#65292;&#25552;&#21319;&#20102;&#26234;&#33021;&#20307;&#30340;&#25945;&#23398;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#32467;&#21512;&#34892;&#21160;&#21644;&#35821;&#35328;&#27807;&#36890;&#27169;&#24335;&#23545;&#23398;&#20064;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#22810;&#27169;&#24335;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel category of GC-agents capable of functioning as both teachers and learners. Leveraging action-based demonstrations and language-based instructions, these agents enhance communication efficiency. We investigate the incorporation of pedagogy and pragmatism, essential elements in human communication and goal achievement, enhancing the agents' teaching and learning capabilities. Furthermore, we explore the impact of combining communication modes (action and language) on learning outcomes, highlighting the benefits of a multi-modal approach.
&lt;/p&gt;</description></item><item><title>FE-PINN&#26159;&#19968;&#31181;&#23398;&#20064;&#24213;&#23618;&#29289;&#29702;&#29305;&#24449;&#30340;&#26694;&#26550;&#65292;&#22312;&#20027;&#35757;&#32451;&#20043;&#21069;&#20197;&#20302;&#35745;&#31639;&#25104;&#26412;&#35299;&#20915;&#38382;&#39064;&#30340;&#27169;&#24335;&#12290;&#19982;&#20256;&#32479;PINN&#30456;&#27604;&#65292;FE-PINN&#36890;&#36807;&#25191;&#34892;&#19968;&#31995;&#21015;&#23376;&#20219;&#21153;&#26469;&#35299;&#20915;&#25439;&#22833;&#20989;&#25968;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#24555;&#36895;&#35757;&#32451;&#21644;&#26356;&#39640;&#30340;&#27714;&#35299;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.08873</link><description>&lt;p&gt;
&#29305;&#24449;&#24378;&#21270;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;FE-PINN&#65289;&#65306;&#22312;&#30446;&#26631;&#20219;&#21153;&#20043;&#21069;&#23398;&#20064;&#24213;&#23618;&#29289;&#29702;&#29305;&#24449;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Feature Enforcing PINN (FE-PINN): A Framework to Learn the Underlying-Physics Features Before Target Task. (arXiv:2308.08873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08873
&lt;/p&gt;
&lt;p&gt;
FE-PINN&#26159;&#19968;&#31181;&#23398;&#20064;&#24213;&#23618;&#29289;&#29702;&#29305;&#24449;&#30340;&#26694;&#26550;&#65292;&#22312;&#20027;&#35757;&#32451;&#20043;&#21069;&#20197;&#20302;&#35745;&#31639;&#25104;&#26412;&#35299;&#20915;&#38382;&#39064;&#30340;&#27169;&#24335;&#12290;&#19982;&#20256;&#32479;PINN&#30456;&#27604;&#65292;FE-PINN&#36890;&#36807;&#25191;&#34892;&#19968;&#31995;&#21015;&#23376;&#20219;&#21153;&#26469;&#35299;&#20915;&#25439;&#22833;&#20989;&#25968;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#24555;&#36895;&#35757;&#32451;&#21644;&#26356;&#39640;&#30340;&#27714;&#35299;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#29305;&#24449;&#24378;&#21270;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;FE-PINN&#65289;&#30340;&#26032;&#22411;&#26080;&#25968;&#25454;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#20027;&#35757;&#32451;&#24490;&#29615;&#20043;&#21069;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#23398;&#20064;&#20219;&#20309;&#38382;&#39064;&#30340;&#24213;&#23618;&#27169;&#24335;&#12290;&#30001;&#20110;&#23384;&#22312;&#20559;&#24494;&#20998;&#27531;&#24046;&#21644;&#36793;&#30028;&#26465;&#20214;&#22343;&#26041;&#35823;&#24046;&#20004;&#20010;&#39033;&#65292;&#26222;&#36890;PINN&#30340;&#25439;&#22833;&#20989;&#25968;&#19981;&#24179;&#34913;&#12290;FE-PINN&#36890;&#36807;&#21482;&#38656;&#19968;&#20998;&#38047;&#30340;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#32791;&#26102;&#25968;&#23567;&#26102;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;FE-PINN&#36890;&#36807;&#25191;&#34892;&#19968;&#31995;&#21015;&#23376;&#20219;&#21153;&#26469;&#23436;&#25104;&#36825;&#20010;&#36807;&#31243;&#12290;&#31532;&#19968;&#20010;&#23376;&#20219;&#21153;&#23398;&#20064;&#26377;&#20851;&#24213;&#23618;&#29289;&#29702;&#30340;&#26377;&#29992;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#27169;&#22411;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#20197;&#23436;&#21892;&#35745;&#31639;&#12290;FE-PINN&#24212;&#29992;&#20110;&#19977;&#20010;&#22522;&#20934;&#38382;&#39064;&#65306;&#22278;&#26609;&#20307;&#19978;&#30340;&#27969;&#21160;&#12289;&#20108;&#32500;&#28909;&#20256;&#23548;&#20197;&#21450;&#35745;&#31639;&#20837;&#21475;&#36895;&#24230;&#30340;&#36870;&#38382;&#39064;&#12290;FE-PINN&#21487;&#20197;&#20998;&#21035;&#21152;&#36895;15&#20493;&#12289;2&#20493;&#21644;5&#20493;&#22320;&#35299;&#20915;&#27599;&#20010;&#26696;&#20363;&#12290;&#21478;&#22806;
&lt;/p&gt;
&lt;p&gt;
In this work, a new data-free framework called Feature Enforcing Physics Informed Neural Network (FE-PINN) is introduced. This framework is capable of learning the underlying pattern of any problem with low computational cost before the main training loop. The loss function of vanilla PINN due to the existence of two terms of partial differential residuals and boundary condition mean squared error is imbalanced. FE-PINN solves this challenge with just one minute of training instead of time-consuming hyperparameter tuning for loss function that can take hours. The FE-PINN accomplishes this process by performing a sequence of sub-tasks. The first sub-task learns useful features about the underlying physics. Then, the model trains on the target task to refine the calculations. FE-PINN is applied to three benchmarks, flow over a cylinder, 2D heat conduction, and an inverse problem of calculating inlet velocity. FE-PINN can solve each case with, 15x, 2x, and 5x speed up accordingly. Another
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#31070;&#32463;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23384;&#22312;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#21253;&#25324;&#39044;&#27979;&#32467;&#26524;&#35299;&#37322;&#19981;&#36275;&#12289;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#24615;&#19981;&#36275;&#21644;&#19981;&#36866;&#24212;&#19981;&#30830;&#23450;&#29615;&#22659;&#30340;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#21487;&#20449;&#24230;&#32593;&#32476;&#30340;&#35774;&#35745;&#32423;&#21487;&#35299;&#37322;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.03666</link><description>&lt;p&gt;
&#26550;&#36215;&#21487;&#20449;&#24230;&#19982;&#24320;&#25918;&#19990;&#30028;&#23398;&#20064;&#30340;&#26725;&#26753;&#65306;&#19968;&#31181;&#25506;&#32034;&#24615;&#31070;&#32463;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Bridging Trustworthiness and Open-World Learning: An Exploratory Neural Approach for Enhancing Interpretability, Generalization, and Robustness. (arXiv:2308.03666v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03666
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#31070;&#32463;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23384;&#22312;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#21253;&#25324;&#39044;&#27979;&#32467;&#26524;&#35299;&#37322;&#19981;&#36275;&#12289;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#24615;&#19981;&#36275;&#21644;&#19981;&#36866;&#24212;&#19981;&#30830;&#23450;&#29615;&#22659;&#30340;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#21487;&#20449;&#24230;&#32593;&#32476;&#30340;&#35774;&#35745;&#32423;&#21487;&#35299;&#37322;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30740;&#31350;&#20154;&#21592;&#21162;&#21147;&#32553;&#23567;&#26426;&#22120;&#26234;&#33021;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#36890;&#36807;&#21457;&#23637;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25105;&#20204;&#24517;&#39035;&#35748;&#35782;&#21040;&#21487;&#20449;&#24230;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#30340;&#20851;&#38190;&#37325;&#35201;&#24615;&#65292;&#22312;&#26085;&#24120;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#23545;&#27599;&#20010;&#20154;&#37117;&#24050;&#32463;&#26080;&#22788;&#19981;&#22312;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23384;&#22312;&#20960;&#20010;&#25361;&#25112;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#20449;&#20219;&#21361;&#26426;&#65306;1&#65289;&#23545;&#39044;&#27979;&#32467;&#26524;&#30340;&#35299;&#37322;&#19981;&#36275;&#65307;2&#65289;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#19981;&#36275;&#65307;3&#65289;&#23545;&#19981;&#30830;&#23450;&#29615;&#22659;&#30340;&#36866;&#24212;&#33021;&#21147;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#31070;&#32463;&#31243;&#24207;&#65292;&#29992;&#20110;&#26550;&#36215;&#21487;&#20449;&#24230;&#19982;&#24320;&#25918;&#19990;&#30028;&#23398;&#20064;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#20174;&#21333;&#27169;&#24577;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#22330;&#26223;&#65292;&#20197;&#20379;&#35835;&#32773;&#20351;&#29992;&#12290;1&#65289;&#20026;&#20102;&#22686;&#24378;&#35774;&#35745;&#32423;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#21046;&#20102;&#20855;&#26377;&#29305;&#23450;&#29289;&#29702;&#21547;&#20041;&#30340;&#21487;&#20449;&#32593;&#32476;&#65307;2&#65289;&#28982;&#21518;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#23398;&#20064;&#27491;&#21017;&#21270;&#22120;&#35774;&#35745;&#29615;&#22659;&#31119;&#31049;&#20219;&#21153;&#25509;&#21475;&#65292;&#20197;&#25913;&#21892;&#21487;&#20449;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As researchers strive to narrow the gap between machine intelligence and human through the development of artificial intelligence technologies, it is imperative that we recognize the critical importance of trustworthiness in open-world, which has become ubiquitous in all aspects of daily life for everyone. However, several challenges may create a crisis of trust in current artificial intelligence systems that need to be bridged: 1) Insufficient explanation of predictive results; 2) Inadequate generalization for learning models; 3) Poor adaptability to uncertain environments. Consequently, we explore a neural program to bridge trustworthiness and open-world learning, extending from single-modal to multi-modal scenarios for readers. 1) To enhance design-level interpretability, we first customize trustworthy networks with specific physical meanings; 2) We then design environmental well-being task-interfaces via flexible learning regularizers for improving the generalization of trustworthy
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#28155;&#21152;&#40065;&#26834;&#26080;&#30072;&#21464;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26144;&#23556;&#38543;&#26426;&#25968;&#24207;&#21015;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26679;&#26412;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#19981;&#25913;&#21464;&#25991;&#26412;&#20998;&#24067;&#30340;&#21069;&#25552;&#19979;&#23545;&#27700;&#21360;&#25991;&#26412;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#25913;&#20889;&#25915;&#20987;&#19979;&#20381;&#28982;&#20445;&#25345;&#36739;&#39640;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;40-50%&#30340;&#38543;&#26426;&#25200;&#21160;&#19979;&#20173;&#21487;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#27700;&#21360;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.15593</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#26080;&#30072;&#21464;&#27700;&#21360;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Robust Distortion-free Watermarks for Language Models. (arXiv:2307.15593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15593
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#28155;&#21152;&#40065;&#26834;&#26080;&#30072;&#21464;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26144;&#23556;&#38543;&#26426;&#25968;&#24207;&#21015;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26679;&#26412;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#19981;&#25913;&#21464;&#25991;&#26412;&#20998;&#24067;&#30340;&#21069;&#25552;&#19979;&#23545;&#27700;&#21360;&#25991;&#26412;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#25913;&#20889;&#25915;&#20987;&#19979;&#20381;&#28982;&#20445;&#25345;&#36739;&#39640;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;40-50%&#30340;&#38543;&#26426;&#25200;&#21160;&#19979;&#20173;&#21487;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#27700;&#21360;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#28155;&#21152;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#36825;&#20123;&#27700;&#21360;&#23545;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#25991;&#26412;&#30340;&#20998;&#24067;&#65292;&#21516;&#26102;&#20445;&#35777;&#29983;&#25104;&#39044;&#31639;&#22312;&#19968;&#23450;&#33539;&#22260;&#20869;&#12290;&#25105;&#20204;&#29992;&#38543;&#26426;&#27700;&#21360;&#23494;&#38053;&#35745;&#31639;&#30340;&#38543;&#26426;&#25968;&#24207;&#21015;&#26144;&#23556;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26679;&#26412;&#26469;&#29983;&#25104;&#24102;&#27700;&#21360;&#30340;&#25991;&#26412;&#12290;&#35201;&#26816;&#27979;&#27700;&#21360;&#25991;&#26412;&#65292;&#21482;&#35201;&#30693;&#36947;&#23494;&#38053;&#30340;&#20219;&#20309;&#19968;&#26041;&#37117;&#21487;&#20197;&#23558;&#25991;&#26412;&#19982;&#38543;&#26426;&#25968;&#24207;&#21015;&#23545;&#40784;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#37319;&#26679;&#26041;&#26696;&#26469;&#23454;&#20363;&#21270;&#27700;&#21360;&#26041;&#27861;&#65306;&#21453;&#21464;&#25442;&#37319;&#26679;&#21644;&#25351;&#25968;&#26368;&#23567;&#37319;&#26679;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#27700;&#21360;&#24212;&#29992;&#20110;&#19977;&#20010;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;OPT-1.3B&#12289;LLaMA-7B&#21644;Alpaca-7B&#65292;&#20197;&#23454;&#39564;&#35777;&#26126;&#23427;&#20204;&#30340;&#32479;&#35745;&#21151;&#25928;&#21644;&#23545;&#21508;&#31181;&#25913;&#20889;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#20110;OPT-1.3B&#21644;LLaMA-7B&#27169;&#22411;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#25200;&#21160;&#20102;40-50%&#30340;&#35789;&#20803;&#21518;&#65292;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#24102;&#27700;&#21360;&#30340;&#25991;&#26412;&#65288;$p \leq 0.01$&#65289;&#65292;&#21482;&#38656;&#35201;35&#20010;&#35789;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a methodology for planting watermarks in text from an autoregressive language model that are robust to perturbations without changing the distribution over text up to a certain maximum generation budget. We generate watermarked text by mapping a sequence of random numbers -- which we compute using a randomized watermark key -- to a sample from the language model. To detect watermarked text, any party who knows the key can align the text to the random number sequence. We instantiate our watermark methodology with two sampling schemes: inverse transform sampling and exponential minimum sampling. We apply these watermarks to three language models -- OPT-1.3B, LLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power and robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B and LLaMA-7B models, we find we can reliably detect watermarked text ($p \leq 0.01$) from $35$ tokens even after corrupting between $40$-$50$\% of the tokens via random
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;Zonoid&#30340;&#26368;&#20248;&#36924;&#36817;&#21644;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#21248;&#36924;&#36817;&#20004;&#20010;&#38382;&#39064;&#12290;&#23545;&#20110;Zonoid&#30340;&#36924;&#36817;&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#22312;$d=2,3$&#26102;&#30340;&#23545;&#25968;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;&#22312;&#25152;&#26377;&#32500;&#24230;&#19978;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;$k \geq 1$&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#30446;&#21069;&#30340;&#36924;&#36817;&#29575;&#65292;&#24182;&#33021;&#22815;&#22343;&#21248;&#36924;&#36817;&#30446;&#26631;&#20989;&#25968;&#21450;&#20854;&#23548;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.15285</link><description>&lt;p&gt;
Zonoid&#30340;&#26368;&#20248;&#36924;&#36817;&#21644;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#21248;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Optimal Approximation of Zonoids and Uniform Approximation by Shallow Neural Networks. (arXiv:2307.15285v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;Zonoid&#30340;&#26368;&#20248;&#36924;&#36817;&#21644;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#21248;&#36924;&#36817;&#20004;&#20010;&#38382;&#39064;&#12290;&#23545;&#20110;Zonoid&#30340;&#36924;&#36817;&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#22312;$d=2,3$&#26102;&#30340;&#23545;&#25968;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;&#22312;&#25152;&#26377;&#32500;&#24230;&#19978;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;$k \geq 1$&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#30446;&#21069;&#30340;&#36924;&#36817;&#29575;&#65292;&#24182;&#33021;&#22815;&#22343;&#21248;&#36924;&#36817;&#30446;&#26631;&#20989;&#25968;&#21450;&#20854;&#23548;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20197;&#19979;&#20004;&#20010;&#30456;&#20851;&#38382;&#39064;&#12290;&#31532;&#19968;&#20010;&#38382;&#39064;&#26159;&#30830;&#23450;&#19968;&#20010;&#20219;&#24847;&#30340;&#22312;$\mathbb{R}^{d+1}$&#31354;&#38388;&#20013;&#30340;Zonoid&#21487;&#20197;&#36890;&#36807;$n$&#20010;&#32447;&#27573;&#30340;Hausdorff&#36317;&#31163;&#26469;&#36924;&#36817;&#30340;&#35823;&#24046;&#12290;&#31532;&#20108;&#20010;&#38382;&#39064;&#26159;&#30830;&#23450;&#27973;&#23618;ReLU$^k$&#31070;&#32463;&#32593;&#32476;&#22312;&#20854;&#21464;&#20998;&#31354;&#38388;&#20013;&#30340;&#22343;&#21248;&#33539;&#25968;&#30340;&#26368;&#20248;&#36924;&#36817;&#29575;&#12290;&#31532;&#19968;&#20010;&#38382;&#39064;&#24050;&#32463;&#22312;$d \neq 2, 3$&#26102;&#24471;&#21040;&#35299;&#20915;&#65292;&#20294;&#24403;$d = 2, 3$&#26102;&#65292;&#26368;&#20248;&#19978;&#30028;&#21644;&#26368;&#20248;&#19979;&#30028;&#20043;&#38388;&#20173;&#23384;&#22312;&#19968;&#20010;&#23545;&#25968;&#24046;&#36317;&#12290;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#20010;&#24046;&#36317;&#65292;&#23436;&#25104;&#20102;&#25152;&#26377;&#32500;&#24230;&#19978;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#31532;&#20108;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;$k \geq 1$&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#29616;&#26377;&#30340;&#36924;&#36817;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#30446;&#26631;&#20989;&#25968;&#21450;&#20854;&#23548;&#25968;&#30340;&#22343;&#21248;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the following two related problems. The first is to determine to what error an arbitrary zonoid in $\mathbb{R}^{d+1}$ can be approximated in the Hausdorff distance by a sum of $n$ line segments. The second is to determine optimal approximation rates in the uniform norm for shallow ReLU$^k$ neural networks on their variation spaces. The first of these problems has been solved for $d\neq 2,3$, but when $d=2,3$ a logarithmic gap between the best upper and lower bounds remains. We close this gap, which completes the solution in all dimensions. For the second problem, our techniques significantly improve upon existing approximation rates when $k\geq 1$, and enable uniform approximation of both the target function and its derivatives.
&lt;/p&gt;</description></item><item><title>FLuID&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#21464;&#24615;&#20002;&#22833;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#24615;&#33021;&#36739;&#20302;&#35774;&#22791;&#23548;&#33268;&#30340;&#35757;&#32451;&#26102;&#38388;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#35757;&#32451;&#26694;&#26550;&#12290;&#36890;&#36807;&#21160;&#24577;&#24179;&#34913;&#35757;&#32451;&#36127;&#36733;&#65292;FLuID&#33021;&#26377;&#25928;&#22320;&#20943;&#36731;&#38459;&#22622;&#35774;&#22791;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#27169;&#22411;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.02623</link><description>&lt;p&gt;
FLuID: &#20351;&#29992;&#19981;&#21464;&#24615;&#20002;&#22833;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38459;&#22622;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
FLuID: Mitigating Stragglers in Federated Learning using Invariant Dropout. (arXiv:2307.02623v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02623
&lt;/p&gt;
&lt;p&gt;
FLuID&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#21464;&#24615;&#20002;&#22833;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#24615;&#33021;&#36739;&#20302;&#35774;&#22791;&#23548;&#33268;&#30340;&#35757;&#32451;&#26102;&#38388;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#35757;&#32451;&#26694;&#26550;&#12290;&#36890;&#36807;&#21160;&#24577;&#24179;&#34913;&#35757;&#32451;&#36127;&#36733;&#65292;FLuID&#33021;&#26377;&#25928;&#22320;&#20943;&#36731;&#38459;&#22622;&#35774;&#22791;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20801;&#35768;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20010;&#20307;&#31227;&#21160;&#35774;&#22791;&#19978;&#36827;&#34892;&#26412;&#22320;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#20849;&#20139;&#26381;&#21153;&#22120;&#21516;&#27493;&#27169;&#22411;&#26356;&#26032;&#12290;&#36825;&#31181;&#26041;&#27861;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#65292;&#20294;&#20063;&#30001;&#20110;&#19981;&#21516;&#35774;&#22791;&#30340;&#24615;&#33021;&#24046;&#24322;&#32780;&#20135;&#29983;&#20102;&#19968;&#20010;&#24322;&#26500;&#30340;&#35757;&#32451;&#29615;&#22659;&#12290;&#22240;&#27492;&#65292;&#22312;FL&#20013;&#65292;&#24615;&#33021;&#36739;&#20302;&#30340;&#38459;&#22622;&#35774;&#22791;&#32463;&#24120;&#20915;&#23450;&#25972;&#20307;&#35757;&#32451;&#26102;&#38388;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#31995;&#32479;&#21160;&#24577;&#24179;&#34913;&#35757;&#32451;&#36127;&#36733;&#26469;&#20943;&#36731;&#30001;&#20110;&#38459;&#22622;&#22120;&#20135;&#29983;&#30340;&#24615;&#33021;&#29942;&#39048;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#21464;&#24615;&#20002;&#22833;&#65292;&#19968;&#31181;&#22522;&#20110;&#26435;&#37325;&#26356;&#26032;&#38408;&#20540;&#25552;&#21462;&#23376;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#23545;&#20934;&#30830;&#24615;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#22312;&#27492;&#20002;&#22833;&#25216;&#26415;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35757;&#32451;&#26694;&#26550;FLuID&#12290;FLuID&#25552;&#20379;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#23376;&#27169;&#22411;&#25552;&#21462;&#26041;&#27861;&#26469;&#35843;&#33410;&#35745;&#31639;&#24378;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;&#38459;&#22622;&#35774;&#22791;&#30340;&#36127;&#36733;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) allows machine learning models to train locally on individual mobile devices, synchronizing model updates via a shared server. This approach safeguards user privacy; however, it also generates a heterogeneous training environment due to the varying performance capabilities across devices. As a result, straggler devices with lower performance often dictate the overall training time in FL. In this work, we aim to alleviate this performance bottleneck due to stragglers by dynamically balancing the training load across the system. We introduce Invariant Dropout, a method that extracts a sub-model based on the weight update threshold, thereby minimizing potential impacts on accuracy. Building on this dropout technique, we develop an adaptive training framework, Federated Learning using Invariant Dropout (FLuID). FLuID offers a lightweight sub-model extraction to regulate computational intensity, thereby reducing the load on straggler devices without affecting model q
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MoVie&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#35270;&#35273;&#27169;&#22411;&#30340;&#31574;&#30053;&#33258;&#36866;&#24212;&#23454;&#29616;&#20102;&#35270;&#22270;&#27867;&#21270;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#38656;&#35201;&#26174;&#24335;&#22870;&#21169;&#20449;&#21495;&#21644;&#35757;&#32451;&#36807;&#31243;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#22810;&#20010;&#23454;&#38469;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#30456;&#23545;&#25913;&#36827;&#36798;&#21040;&#20102;33%&#33267;152%&#12290;</title><link>http://arxiv.org/abs/2307.00972</link><description>&lt;p&gt;
MoVie: &#22522;&#20110;&#35270;&#35273;&#27169;&#22411;&#30340;&#31574;&#30053;&#33258;&#36866;&#24212;&#29992;&#20110;&#35270;&#22270;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
MoVie: Visual Model-Based Policy Adaptation for View Generalization. (arXiv:2307.00972v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MoVie&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#35270;&#35273;&#27169;&#22411;&#30340;&#31574;&#30053;&#33258;&#36866;&#24212;&#23454;&#29616;&#20102;&#35270;&#22270;&#27867;&#21270;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#38656;&#35201;&#26174;&#24335;&#22870;&#21169;&#20449;&#21495;&#21644;&#35757;&#32451;&#36807;&#31243;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#22810;&#20010;&#23454;&#38469;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#30456;&#23545;&#25913;&#36827;&#36798;&#21040;&#20102;33%&#33267;152%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26377;&#38480;&#30340;&#35270;&#22270;&#19978;&#35757;&#32451;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26234;&#33021;&#20307;&#22312;&#23558;&#20854;&#23398;&#21040;&#30340;&#33021;&#21147;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#35270;&#22270;&#26102;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#36825;&#20010;&#22266;&#26377;&#30340;&#22256;&#38590;&#34987;&#31216;&#20026;$\textit{view generalization}$&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#22522;&#26412;&#38382;&#39064;&#31995;&#32479;&#22320;&#20998;&#20026;&#22235;&#20010;&#19981;&#21516;&#30340;&#65292;&#39640;&#24230;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#26223;&#65292;&#36825;&#20123;&#24773;&#26223;&#19982;&#23454;&#38469;&#24773;&#20917;&#24456;&#30456;&#20284;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22312;&#27979;&#35797;&#26102;&#20351;&#22522;&#20110;&#35270;&#35273;&#30340;$\textbf{Mo}$del-based&#31574;&#30053;&#33021;&#22815;&#25104;&#21151;&#36866;&#24212;$\textbf{Vie}$w generalization ($\textbf{MoVie}$)&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#30340;&#22870;&#21169;&#20449;&#21495;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20219;&#20309;&#20462;&#25913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26469;&#33258;DMControl&#12289;xArm&#21644;Adroit&#30340;&#24635;&#20849;$\textbf{18}$&#20010;&#20219;&#21153;&#20013;&#30340;&#25152;&#26377;&#22235;&#20010;&#24773;&#26223;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#30456;&#23545;&#25913;&#36827;&#20998;&#21035;&#20026;$\mathbf{33}$%&#65292;$\mathbf{86}$%&#21644;$\mathbf{152}$%&#12290;&#20248;&#36234;&#30340;&#32467;&#26524;&#20984;&#26174;&#20986;&#20854;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Reinforcement Learning (RL) agents trained on limited views face significant challenges in generalizing their learned abilities to unseen views. This inherent difficulty is known as the problem of $\textit{view generalization}$. In this work, we systematically categorize this fundamental problem into four distinct and highly challenging scenarios that closely resemble real-world situations. Subsequently, we propose a straightforward yet effective approach to enable successful adaptation of visual $\textbf{Mo}$del-based policies for $\textbf{Vie}$w generalization ($\textbf{MoVie}$) during test time, without any need for explicit reward signals and any modification during training time. Our method demonstrates substantial advancements across all four scenarios encompassing a total of $\textbf{18}$ tasks sourced from DMControl, xArm, and Adroit, with a relative improvement of $\mathbf{33}$%, $\mathbf{86}$%, and $\mathbf{152}$% respectively. The superior results highlight the immens
&lt;/p&gt;</description></item><item><title>SCENEREPLICA&#26159;&#19968;&#20010;&#22522;&#20110;YCB&#23545;&#35937;&#30340;&#21487;&#37325;&#22797;&#24615;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#33021;&#21147;&#12290;&#27492;&#22522;&#20934;&#27979;&#35797;&#26131;&#20110;&#37325;&#22797;&#24182;&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#27604;&#36739;&#19981;&#21516;&#30340;&#25216;&#26415;&#21644;&#31639;&#27861;&#65292;&#26377;&#21161;&#20110;&#21152;&#24555;&#26426;&#22120;&#20154;&#25805;&#32437;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.15620</link><description>&lt;p&gt;
SCENEREPLICA&#65306;&#36890;&#36807;&#21019;&#24314;&#21487;&#37325;&#22797;&#30340;&#22330;&#26223;&#26469;&#35780;&#20272;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
SCENEREPLICA: Benchmarking Real-World Robot Manipulation by Creating Reproducible Scenes. (arXiv:2306.15620v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15620
&lt;/p&gt;
&lt;p&gt;
SCENEREPLICA&#26159;&#19968;&#20010;&#22522;&#20110;YCB&#23545;&#35937;&#30340;&#21487;&#37325;&#22797;&#24615;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#33021;&#21147;&#12290;&#27492;&#22522;&#20934;&#27979;&#35797;&#26131;&#20110;&#37325;&#22797;&#24182;&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#27604;&#36739;&#19981;&#21516;&#30340;&#25216;&#26415;&#21644;&#31639;&#27861;&#65292;&#26377;&#21161;&#20110;&#21152;&#24555;&#26426;&#22120;&#20154;&#25805;&#32437;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#37325;&#22797;&#24615;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#33021;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#25235;&#21462;&#21644;&#25918;&#32622;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20351;&#29992;&#20102;YCB&#23545;&#35937;&#65292;&#36825;&#26159;&#26426;&#22120;&#20154;&#23398;&#30028;&#24120;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#30830;&#20445;&#25105;&#20204;&#30340;&#32467;&#26524;&#21487;&#20197;&#19982;&#20854;&#20182;&#30740;&#31350;&#36827;&#34892;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#27492;&#22522;&#20934;&#27979;&#35797;&#36824;&#34987;&#35774;&#35745;&#20026;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26131;&#20110;&#37325;&#22797;&#65292;&#20351;&#20854;&#21487;&#20379;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#20351;&#29992;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23545;&#22522;&#20934;&#27979;&#35797;&#20013;&#22522;&#20110;&#27169;&#22411;&#21644;&#26080;&#27169;&#22411;&#30340;6D&#26426;&#22120;&#20154;&#25235;&#21462;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#20998;&#26512;&#65292;&#20854;&#20013;&#35780;&#20272;&#20102;&#20195;&#34920;&#24615;&#31639;&#27861;&#22312;&#29289;&#20307;&#24863;&#30693;&#12289;&#25235;&#21462;&#35268;&#21010;&#21644;&#36816;&#21160;&#35268;&#21010;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#23558;&#25104;&#20026;&#25512;&#21160;&#26426;&#22120;&#20154;&#25805;&#32437;&#39046;&#22495;&#21457;&#23637;&#30340;&#23453;&#36149;&#24037;&#20855;&#12290;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#26356;&#23481;&#26131;&#22320;&#27604;&#36739;&#19981;&#21516;&#30340;&#25216;&#26415;&#21644;&#31639;&#27861;&#65292;&#20174;&#32780;&#21152;&#24555;&#21457;&#23637;&#26426;&#22120;&#20154;&#25805;&#32437;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new reproducible benchmark for evaluating robot manipulation in the real world, specifically focusing on pick-and-place. Our benchmark uses the YCB objects, a commonly used dataset in the robotics community, to ensure that our results are comparable to other studies. Additionally, the benchmark is designed to be easily reproducible in the real world, making it accessible to researchers and practitioners. We also provide our experimental results and analyzes for model-based and model-free 6D robotic grasping on the benchmark, where representative algorithms are evaluated for object perception, grasping planning, and motion planning. We believe that our benchmark will be a valuable tool for advancing the field of robot manipulation. By providing a standardized evaluation framework, researchers can more easily compare different techniques and algorithms, leading to faster progress in developing robot manipulation methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#30740;&#31350;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.13840</link><description>&lt;p&gt;
&#36229;&#36234;&#35268;&#27169;&#65306;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;&#25968;&#25454;&#36136;&#37327;&#25351;&#26631;&#35777;&#26126;&#20102;LLMs&#26159;&#22312;&#24418;&#24335;&#22810;&#26679;&#30340;&#25968;&#25454;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;
&lt;/p&gt;
&lt;p&gt;
Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data. (arXiv:2306.13840v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#30740;&#31350;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#65292;&#39044;&#20808;&#35757;&#32451;&#24378;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36235;&#21183;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#25193;&#22823;&#12290;&#28982;&#32780;&#65292;&#39044;&#20808;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#23545;&#20110;&#35757;&#32451;&#24378;&#22823;&#30340;LLMs&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#65292;&#20294;&#23427;&#26159;&#19968;&#20010;&#27169;&#31946;&#30340;&#27010;&#24565;&#65292;&#23578;&#26410;&#23436;&#20840;&#34920;&#24449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;Task2Vec&#22810;&#26679;&#24615;&#31995;&#25968;&#26469;&#22522;&#20110;&#25968;&#25454;&#36136;&#37327;&#30340;&#24418;&#24335;&#26041;&#38754;&#65292;&#36229;&#36234;&#35268;&#27169;&#26412;&#36523;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27979;&#37327;&#20844;&#24320;&#21487;&#29992;&#30340;&#39044;&#20808;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#65292;&#20197;&#35777;&#26126;&#23427;&#20204;&#30340;&#24418;&#24335;&#22810;&#26679;&#24615;&#39640;&#20110;&#29702;&#35770;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#24314;&#31435;&#23545;&#22810;&#26679;&#24615;&#31995;&#25968;&#30340;&#20449;&#24515;&#65292;&#25105;&#20204;&#36827;&#34892;&#21487;&#35299;&#37322;&#24615;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#35813;&#31995;&#25968;&#19982;&#22810;&#26679;&#24615;&#30340;&#30452;&#35266;&#23646;&#24615;&#30456;&#21563;&#21512;&#65292;&#20363;&#22914;&#65292;&#38543;&#30528;&#28508;&#22312;&#27010;&#24565;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#23427;&#22686;&#21152;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#22810;&#26679;&#24615;&#31995;&#25968;&#26159;&#21487;&#38752;&#30340;&#65292;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#65292;&#24182;&#25512;&#27979;&#23427;&#21487;&#20197;&#20316;&#20026;&#39044;&#35757;&#32451;LLMs&#27169;&#22411;&#30340;&#25968;&#25454;&#36136;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current trends to pre-train capable Large Language Models (LLMs) mostly focus on scaling of model and dataset size. However, the quality of pre-training data is an important factor for training powerful LLMs, yet it is a nebulous concept that has not been fully characterized. Therefore, we use the recently proposed Task2Vec diversity coefficient to ground and understand formal aspects of data quality, to go beyond scale alone. Specifically, we measure the diversity coefficient of publicly available pre-training datasets to demonstrate that their formal diversity is high when compared to theoretical lower and upper bounds. In addition, to build confidence in the diversity coefficient, we conduct interpretability experiments and find that the coefficient aligns with intuitive properties of diversity, e.g., it increases as the number of latent concepts increases. We conclude the diversity coefficient is reliable, show it's high for publicly available LLM datasets, and conjecture it can be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21487;&#20197;&#26377;&#25928;&#32416;&#27491;&#25968;&#25454;&#20559;&#24046;&#21644;&#20132;&#21449;&#20559;&#24046;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#20010;&#37325;&#26032;&#21152;&#26435;&#26041;&#26696;&#65292;&#21487;&#20197;&#31934;&#30830;&#35780;&#20272;&#20219;&#20309;&#20551;&#35774;&#22312;&#30495;&#23454;&#20998;&#24067;&#19978;&#30340;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2306.11112</link><description>&lt;p&gt;
&#32416;&#27491;&#20844;&#24179;&#20998;&#31867;&#20013;&#30340;&#20302;&#20272;&#20559;&#24046;&#21644;&#20132;&#21449;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Correcting Underrepresentation and Intersectional Bias for Fair Classification. (arXiv:2306.11112v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21487;&#20197;&#26377;&#25928;&#32416;&#27491;&#25968;&#25454;&#20559;&#24046;&#21644;&#20132;&#21449;&#20559;&#24046;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#20010;&#37325;&#26032;&#21152;&#26435;&#26041;&#26696;&#65292;&#21487;&#20197;&#31934;&#30830;&#35780;&#20272;&#20219;&#20309;&#20551;&#35774;&#22312;&#30495;&#23454;&#20998;&#24067;&#19978;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#23398;&#20064;&#34987;&#20302;&#20272;&#20559;&#24046;&#25439;&#22351;&#30340;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#27491;&#20363;&#22312;&#22266;&#23450;&#25968;&#37327;&#30340;&#25935;&#24863;&#32452;&#20013;&#20197;&#19981;&#21516;&#30340;&#26410;&#30693;&#36895;&#29575;&#20174;&#25968;&#25454;&#20013;&#36807;&#28388;&#25481;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#26377;&#23569;&#37327;&#26080;&#20559;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#20272;&#35745;&#27599;&#20010;&#32452;&#30340;&#20943;&#23569;&#21442;&#25968;&#65292;&#21363;&#20351;&#22312;&#20132;&#21449;&#32452;&#25104;&#21592;&#36164;&#26684;&#20351;&#24471;&#23398;&#20064;&#27599;&#20010;&#20132;&#21449;&#29575;&#21464;&#24471;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#30340;&#24773;&#20917;&#19979;&#12290;&#21033;&#29992;&#36825;&#20010;&#20998;&#32452;&#20002;&#22833;&#29575;&#30340;&#20272;&#35745;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#19968;&#20010;&#37325;&#26032;&#21152;&#26435;&#26041;&#26696;&#65292;&#21487;&#20197;&#20351;&#25105;&#20204;&#36817;&#20284;&#35780;&#20272;&#20219;&#20309;&#20551;&#35774;&#22312;&#30495;&#23454;&#20998;&#24067;&#19978;&#30340;&#25439;&#22833;&#65292;&#21363;&#20351;&#25105;&#20204;&#21482;&#33021;&#22312;&#19968;&#20010;&#26377;&#20559;&#26679;&#26412;&#19978;&#35266;&#23519;&#21040;&#32463;&#39564;&#35823;&#24046;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23553;&#35013;&#20102;&#36825;&#20010;&#23398;&#20064;&#21644;&#37325;&#26032;&#21152;&#26435;&#36807;&#31243;&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;PAC&#39118;&#26684;&#30340;&#20445;&#35777;&#65292;&#21363;&#26377;&#24456;&#39640;&#30340;&#27010;&#29575;&#25105;&#20204;&#23545;&#20551;&#35774;&#22312;&#30495;&#23454;&#20998;&#24067;&#19978;&#30340;&#39118;&#38505;&#30340;&#20272;&#35745;&#23558;&#19982;&#30495;&#23454;&#39118;&#38505;&#20219;&#24847;&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning from data corrupted by underrepresentation bias, where positive examples are filtered from the data at different, unknown rates for a fixed number of sensitive groups. We show that with a small amount of unbiased data, we can efficiently estimate the group-wise drop-out parameters, even in settings where intersectional group membership makes learning each intersectional rate computationally infeasible. Using this estimate for the group-wise drop-out rate, we construct a re-weighting scheme that allows us to approximate the loss of any hypothesis on the true distribution, even if we only observe the empirical error on a biased sample. Finally, we present an algorithm encapsulating this learning and re-weighting process, and we provide strong PAC-style guarantees that, with high probability, our estimate of the risk of the hypothesis over the true distribution will be arbitrarily close to the true risk.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#25506;&#32034;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#25968;&#25454;&#21457;&#29616;&#21644;&#25506;&#32034;&#20219;&#21153;&#20013;&#65292;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#34920;&#26684;&#31867;&#26816;&#27979;&#12289;&#21015;&#31867;&#22411;&#27880;&#37322;&#21644;&#32852;&#25509;&#21015;&#39044;&#27979;&#20013;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#26377;&#26395;&#23558;&#19981;&#21516;&#30340;&#25968;&#25454;&#31649;&#29702;&#20219;&#21153;&#32479;&#19968;&#22312;&#22522;&#30784;&#27169;&#22411;&#19979;&#12290;</title><link>http://arxiv.org/abs/2306.09610</link><description>&lt;p&gt;
CHORUS: &#32479;&#19968;&#25968;&#25454;&#21457;&#29616;&#21644;&#25506;&#32034;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
CHORUS: Foundation Models for Unified Data Discovery and Exploration. (arXiv:2306.09610v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09610
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#25506;&#32034;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#25968;&#25454;&#21457;&#29616;&#21644;&#25506;&#32034;&#20219;&#21153;&#20013;&#65292;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#34920;&#26684;&#31867;&#26816;&#27979;&#12289;&#21015;&#31867;&#22411;&#27880;&#37322;&#21644;&#32852;&#25509;&#21015;&#39044;&#27979;&#20013;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#26377;&#26395;&#23558;&#19981;&#21516;&#30340;&#25968;&#25454;&#31649;&#29702;&#20219;&#21153;&#32479;&#19968;&#22312;&#22522;&#30784;&#27169;&#22411;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#22522;&#30784;&#27169;&#22411;&#24212;&#29992;&#20110;&#25968;&#25454;&#21457;&#29616;&#21644;&#25506;&#32034;&#20219;&#21153;&#20013;&#12290;&#22522;&#30784;&#27169;&#22411;&#26159;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs)&#65292;&#22312;&#21508;&#31181;&#19982;&#20854;&#35757;&#32451;&#26080;&#20851;&#30340;&#19981;&#21516;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#25968;&#25454;&#21457;&#29616;&#21644;&#25968;&#25454;&#25506;&#32034;&#39046;&#22495;&#38750;&#24120;&#36866;&#29992;&#12290;&#22312;&#35880;&#24910;&#20351;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#20855;&#26377;&#20248;&#36234;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#20248;&#21270;&#34920;&#26684;&#31867;&#26816;&#27979;&#12289;&#21015;&#31867;&#22411;&#27880;&#37322;&#21644;&#32852;&#25509;&#21015;&#39044;&#27979;&#36825;&#19977;&#31181;&#20195;&#34920;&#24615;&#20219;&#21153;&#12290;&#22312;&#36825;&#19977;&#20010;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#26041;&#27861;&#20248;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#21644;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#24120;&#36229;&#36807;&#20154;&#31867;&#19987;&#23478;&#30340;&#20219;&#21153;&#34920;&#29616;&#12290;&#36825;&#34920;&#26126;&#20102;&#23558;&#19981;&#21516;&#30340;&#25968;&#25454;&#31649;&#29702;&#20219;&#21153;&#32479;&#19968;&#22312;&#22522;&#30784;&#27169;&#22411;&#19979;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the application of foundation models to data discovery and exploration tasks. Foundation models are large language models (LLMs) that show promising performance on a range of diverse tasks unrelated to their training. We show that these models are highly applicable to the data discovery and data exploration domain. When carefully used, they have superior capability on three representative tasks: table-class detection, column-type annotation and join-column prediction. On all three tasks, we show that a foundation-model-based approach outperforms the task-specific models and so the state of the art. Further, our approach often surpasses human-expert task performance. This suggests a future direction in which disparate data management tasks can be unified under foundation models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21253;&#25324;&#24322;&#24120;&#28857;&#30340;&#39640;&#32500;&#20581;&#22766;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#20351;&#29992;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#30340;&#31934;&#30830;&#28176;&#36817;&#29305;&#24615;&#65292;&#23545;&#27867;&#21270;&#35823;&#24046;&#36827;&#34892;&#20102;&#31616;&#21333;&#26657;&#20934;&#24182;&#35745;&#31639;&#20102;&#25910;&#25947;&#36895;&#29575;&#65292;&#20294;&#30001;&#20110;&#33539;&#25968;&#26657;&#20934;&#19981;&#21305;&#37197;&#65292;&#23545;&#20272;&#35745;&#35823;&#24046;&#30340;&#19968;&#33268;&#24615;&#38656;&#35201;&#19968;&#20010;&#36739;&#24378;&#30340;&#25910;&#25947;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2305.18974</link><description>&lt;p&gt;
&#24322;&#24120;&#28857;&#23384;&#22312;&#26102;&#20581;&#22766;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#24615;&#33021;&#30340;&#28176;&#36827;&#29305;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Asymptotic Characterisation of Robust Empirical Risk Minimisation Performance in the Presence of Outliers. (arXiv:2305.18974v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18974
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21253;&#25324;&#24322;&#24120;&#28857;&#30340;&#39640;&#32500;&#20581;&#22766;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#20351;&#29992;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#30340;&#31934;&#30830;&#28176;&#36817;&#29305;&#24615;&#65292;&#23545;&#27867;&#21270;&#35823;&#24046;&#36827;&#34892;&#20102;&#31616;&#21333;&#26657;&#20934;&#24182;&#35745;&#31639;&#20102;&#25910;&#25947;&#36895;&#29575;&#65292;&#20294;&#30001;&#20110;&#33539;&#25968;&#26657;&#20934;&#19981;&#21305;&#37197;&#65292;&#23545;&#20272;&#35745;&#35823;&#24046;&#30340;&#19968;&#33268;&#24615;&#38656;&#35201;&#19968;&#20010;&#36739;&#24378;&#30340;&#25910;&#25947;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#39640;&#32500;&#20581;&#22766;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#65292;&#24403;&#32500;&#24230;$d$&#21644;&#25968;&#25454;&#28857;&#25968;&#37327;$n$&#20197;&#22266;&#23450;&#27604;&#29575;$\alpha=n/d$&#21457;&#25955;&#65292;&#24182;&#30740;&#31350;&#20102;&#21253;&#25324;&#24322;&#24120;&#28857;&#22312;&#20869;&#30340;&#25968;&#25454;&#27169;&#22411;&#12290;&#25105;&#20204;&#23545;&#20351;&#29992;$\ell_2$ -&#27491;&#21017;&#21270;$\ell_2$&#65292;$\ell_1$&#65292;&#21644; Huber &#25439;&#22833;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#24615;&#33021;&#25552;&#20379;&#20102;&#31934;&#30830;&#30340;&#28176;&#36817;&#29305;&#24615;&#65292;&#36825;&#26159;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#25105;&#20204;&#20851;&#27880;&#24615;&#33021;&#30340;&#20004;&#20010;&#25351;&#26631;&#65306;&#20855;&#26377;&#24322;&#24120;&#28857;&#30340;&#30456;&#20284;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#35823;&#24046;&#21644;&#21407;&#22987;&#26080;&#27745;&#26579;&#20989;&#25968;&#30340;&#20272;&#35745;&#35823;&#24046;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#19982;&#20449;&#24687;&#35770;&#36125;&#21494;&#26031;&#26368;&#20248;&#20272;&#35745;&#30028;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23545;&#20110;&#27867;&#21270;&#35823;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#22914;&#26524;&#36827;&#34892;&#31616;&#21333;&#30340;&#26657;&#20934;&#24182;&#35745;&#31639;&#25910;&#25947;&#36895;&#29575;&#65292;&#21017;&#26368;&#20248;&#27491;&#21017;&#21270;ERM&#22312;&#22823;&#26679;&#26412;&#22797;&#26434;&#24230;&#38480;&#21046;&#19979;&#26159;&#28176;&#36817;&#19968;&#33268;&#30340;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20272;&#35745;&#35823;&#24046;&#65292;&#30001;&#20110;&#33539;&#25968;&#26657;&#20934;&#19981;&#21305;&#37197;&#65292;&#25105;&#20204;&#34920;&#26126;&#20272;&#35745;&#22120;&#30340;&#19968;&#33268;&#24615;&#38656;&#35201;&#19968;&#20010;&#36739;&#24378;&#30340;&#25910;&#25947;&#20551;&#35774;&#65292;&#36825;&#23545;&#38382;&#39064;&#30340;&#35299;&#20915;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study robust linear regression in high-dimension, when both the dimension $d$ and the number of data points $n$ diverge with a fixed ratio $\alpha=n/d$, and study a data model that includes outliers. We provide exact asymptotics for the performances of the empirical risk minimisation (ERM) using $\ell_2$-regularised $\ell_2$, $\ell_1$, and Huber loss, which are the standard approach to such problems. We focus on two metrics for the performance: the generalisation error to similar datasets with outliers, and the estimation error of the original, unpolluted function. Our results are compared with the information theoretic Bayes-optimal estimation bound. For the generalization error, we find that optimally-regularised ERM is asymptotically consistent in the large sample complexity limit if one perform a simple calibration, and compute the rates of convergence. For the estimation error however, we show that due to a norm calibration mismatch, the consistency of the estimator requires an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21327;&#26041;&#24046;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#33041;&#40836;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#30382;&#36136;&#21402;&#24230;&#29305;&#24449;&#25429;&#25417;&#21152;&#36895;&#32769;&#21270;&#65292;&#24182;&#21453;&#26144;&#20986;&#22686;&#21152;&#30340;&#31070;&#32463;&#30142;&#30149;&#25110;&#35748;&#30693;&#38556;&#30861;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2305.18370</link><description>&lt;p&gt;
&#20351;&#29992;&#21327;&#26041;&#24046;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#33041;&#40836;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explainable Brain Age Prediction using coVariance Neural Networks. (arXiv:2305.18370v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21327;&#26041;&#24046;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#33041;&#40836;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#30382;&#36136;&#21402;&#24230;&#29305;&#24449;&#25429;&#25417;&#21152;&#36895;&#32769;&#21270;&#65292;&#24182;&#21453;&#26144;&#20986;&#22686;&#21152;&#30340;&#31070;&#32463;&#30142;&#30149;&#25110;&#35748;&#30693;&#38556;&#30861;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#20013;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34987;&#29992;&#20110;&#21033;&#29992;&#33041;&#25104;&#20687;&#25968;&#25454;&#20026;&#20010;&#20307;&#25552;&#20379;&#8220;&#33041;&#40836;&#8221;&#20272;&#35745;&#12290;&#30001;&#20110;&#33041;&#40836;&#19982;&#23454;&#38469;&#24180;&#40836;&#23384;&#22312;&#24046;&#24322;&#65288;&#31216;&#20026;&#8220;&#33041;&#40836;&#24046;&#8221;&#65289;&#65292;&#22240;&#27492;&#21487;&#20197;&#25429;&#25417;&#30001;&#20110;&#19981;&#33391;&#20581;&#24247;&#29366;&#20917;&#23548;&#33268;&#30340;&#21152;&#36895;&#32769;&#21270;&#65292;&#24182;&#22240;&#27492;&#21453;&#26144;&#20986;&#22686;&#21152;&#30340;&#31070;&#32463;&#30142;&#30149;&#25110;&#35748;&#30693;&#38556;&#30861;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#33041;&#40836;&#39044;&#27979;&#31639;&#27861;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#26041;&#27861;&#35770;&#20381;&#25454;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#26041;&#38754;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#21327;&#26041;&#24046;&#31070;&#32463;&#32593;&#32476; (VNN)&#26469;&#25552;&#20986;&#19968;&#31181;&#35299;&#21078;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#30382;&#36136;&#21402;&#24230;&#29305;&#24449;&#36827;&#34892;&#33041;&#40836;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#33041;&#40836;&#39044;&#27979;&#26694;&#26550;&#19981;&#20165;&#25193;&#23637;&#21040;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149; (AD) &#20013;&#33041;&#40836;&#24046;&#30340;&#31895;&#30053;&#25351;&#26631;&#65292;&#32780;&#19988;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#37325;&#35201;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
In computational neuroscience, there has been an increased interest in developing machine learning algorithms that leverage brain imaging data to provide estimates of "brain age" for an individual. Importantly, the discordance between brain age and chronological age (referred to as "brain age gap") can capture accelerated aging due to adverse health conditions and therefore, can reflect increased vulnerability towards neurological disease or cognitive impairments. However, widespread adoption of brain age for clinical decision support has been hindered due to lack of transparency and methodological justifications in most existing brain age prediction algorithms. In this paper, we leverage coVariance neural networks (VNN) to propose an anatomically interpretable framework for brain age prediction using cortical thickness features. Specifically, our brain age prediction framework extends beyond the coarse metric of brain age gap in Alzheimer's disease (AD) and we make two important obser
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#21644;&#26377;&#25928;&#26816;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#22403;&#22334;&#23454;&#20363;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#28145;&#20837;&#27700;&#19979;&#29615;&#22659;&#20013;&#25552;&#39640;&#20854;&#26816;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.16460</link><description>&lt;p&gt;
&#20248;&#21270;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#29992;&#20110;&#39640;&#25928;&#26816;&#27979;&#27700;&#19979;&#22403;&#22334;
&lt;/p&gt;
&lt;p&gt;
Optimized Custom Dataset for Efficient Detection of Underwater Trash. (arXiv:2305.16460v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#21644;&#26377;&#25928;&#26816;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#22403;&#22334;&#23454;&#20363;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#28145;&#20837;&#27700;&#19979;&#29615;&#22659;&#20013;&#25552;&#39640;&#20854;&#26816;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#35780;&#20272;&#21644;&#28165;&#38500;&#28508;&#22312;&#30340;&#27700;&#19979;&#24223;&#29289;&#23545;&#20110;&#20445;&#25252;&#28023;&#27915;&#29983;&#29289;&#21644;&#29615;&#22659;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#38024;&#23545;&#27700;&#19979;&#22403;&#22334;&#26816;&#27979;&#25152;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#22914;&#20809;&#25240;&#23556;&#12289;&#21560;&#25910;&#12289;&#24748;&#28014;&#39063;&#31890;&#21644;&#33394;&#24425;&#25197;&#26354;&#31561;&#22240;&#32032;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#21644;&#26377;&#25928;&#26816;&#27979;&#26041;&#27861;&#12290;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#22810;&#31181;&#27700;&#19979;&#29615;&#22659;&#65292;&#24182;&#21253;&#25324;&#23545;&#24223;&#24323;&#29289;&#23454;&#20363;&#30340;&#31934;&#30830;&#23450;&#20301;&#26631;&#27880;&#12290;&#26368;&#32456;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#65292;&#30446;&#30340;&#26159;&#36890;&#36807;&#22686;&#21152;&#22403;&#22334;&#23454;&#20363;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#28145;&#20837;&#27700;&#19979;&#29615;&#22659;&#20013;&#25552;&#39640;&#20854;&#26816;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately quantifying and removing submerged underwater waste plays a crucial role in safeguarding marine life and preserving the environment. While detecting floating and surface debris is relatively straightforward, quantifying submerged waste presents significant challenges due to factors like light refraction, absorption, suspended particles, and color distortion. This paper addresses these challenges by proposing the development of a custom dataset and an efficient detection approach for submerged marine debris. The dataset encompasses diverse underwater environments and incorporates annotations for precise labeling of debris instances. Ultimately, the primary objective of this custom dataset is to enhance the diversity of litter instances and improve their detection accuracy in deep submerged environments by leveraging state-of-the-art deep learning architectures.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25439;&#22833;&#40657;&#22622;&#30697;&#38453;&#21644;&#36755;&#20837;-&#36755;&#20986;&#38597;&#20811;&#27604;&#30697;&#38453;&#32852;&#31995;&#36215;&#26469;&#30340;&#20551;&#35774;&#65292;&#37327;&#21270;&#20102;&#27169;&#22411;&#30340;&#36755;&#20837;-&#36755;&#20986;&#38597;&#20811;&#27604;&#30697;&#38453;&#36817;&#20284;&#20854;&#22312;&#25968;&#25454;&#20998;&#24067;&#19978;&#30340;&#21033;&#26222;&#35199;&#33576;&#33539;&#25968;&#30340;&#31243;&#24230;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32463;&#39564;&#38597;&#20811;&#27604;&#30697;&#38453;&#30340;&#26032;&#30340;&#27867;&#21270;&#30028;&#65292;&#32473;&#20986;&#20102;&#20851;&#20110;&#36827;&#21270;&#30952;&#38155;&#21644;&#24179;&#22374;&#26497;&#23567;&#30340;&#27867;&#21270;&#24615;&#36136;&#30340;&#26032;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.14683</link><description>&lt;p&gt;
&#35770;&#36827;&#21270;&#30952;&#38155;&#12289;&#24179;&#22374;&#26497;&#23567;&#21644;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
On progressive sharpening, flat minima and generalisation. (arXiv:2305.14683v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25439;&#22833;&#40657;&#22622;&#30697;&#38453;&#21644;&#36755;&#20837;-&#36755;&#20986;&#38597;&#20811;&#27604;&#30697;&#38453;&#32852;&#31995;&#36215;&#26469;&#30340;&#20551;&#35774;&#65292;&#37327;&#21270;&#20102;&#27169;&#22411;&#30340;&#36755;&#20837;-&#36755;&#20986;&#38597;&#20811;&#27604;&#30697;&#38453;&#36817;&#20284;&#20854;&#22312;&#25968;&#25454;&#20998;&#24067;&#19978;&#30340;&#21033;&#26222;&#35199;&#33576;&#33539;&#25968;&#30340;&#31243;&#24230;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32463;&#39564;&#38597;&#20811;&#27604;&#30697;&#38453;&#30340;&#26032;&#30340;&#27867;&#21270;&#30028;&#65292;&#32473;&#20986;&#20102;&#20851;&#20110;&#36827;&#21270;&#30952;&#38155;&#21644;&#24179;&#22374;&#26497;&#23567;&#30340;&#27867;&#21270;&#24615;&#36136;&#30340;&#26032;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#20013;&#25439;&#22833;&#26354;&#29575;&#19982;&#27867;&#21270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#28145;&#24230;&#32593;&#32476;&#25439;&#22833;&#40657;&#22622;&#30697;&#38453;&#39057;&#35889;&#32463;&#39564;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#25439;&#22833;&#40657;&#22622;&#30697;&#38453;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;-&#36755;&#20986;&#38597;&#20811;&#27604;&#30697;&#38453;&#32852;&#31995;&#36215;&#26469;&#30340;&#20551;&#35774;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31995;&#21015;&#29702;&#35770;&#32467;&#26524;&#65292;&#37327;&#21270;&#20102;&#27169;&#22411;&#30340;&#36755;&#20837;-&#36755;&#20986;&#38597;&#20811;&#27604;&#30697;&#38453;&#36817;&#20284;&#20854;&#22312;&#25968;&#25454;&#20998;&#24067;&#19978;&#30340;&#21033;&#26222;&#35199;&#33576;&#33539;&#25968;&#30340;&#31243;&#24230;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32463;&#39564;&#38597;&#20811;&#27604;&#30697;&#38453;&#30340;&#26032;&#30340;&#27867;&#21270;&#30028;&#12290;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;&#20551;&#35774;&#21644;&#29702;&#35770;&#32467;&#26524;&#65292;&#32473;&#20986;&#20102;&#20851;&#20110;&#26368;&#36817;&#35266;&#23519;&#21040;&#30340;&#36827;&#21270;&#30952;&#38155;&#29616;&#35937;&#20197;&#21450;&#24179;&#22374;&#26497;&#23567;&#30340;&#27867;&#21270;&#24615;&#36136;&#30340;&#26032;&#25551;&#36848;&#12290;&#23454;&#39564;&#35777;&#25454;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20027;&#24352;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new approach to understanding the relationship between loss curvature and generalisation in deep learning. Specifically, we use existing empirical analyses of the spectrum of deep network loss Hessians to ground an ansatz tying together the loss Hessian and the input-output Jacobian of a deep neural network. We then prove a series of theoretical results which quantify the degree to which the input-output Jacobian of a model approximates its Lipschitz norm over a data distribution, and deduce a novel generalisation bound in terms of the empirical Jacobian. We use our ansatz, together with our theoretical results, to give a new account of the recently observed progressive sharpening phenomenon, as well as the generalisation properties of flat minima. Experimental evidence is provided to validate our claims.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diff-Pruning&#30340;&#39640;&#25928;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;Taylor&#23637;&#24320;&#36807;&#31243;&#26469;&#35782;&#21035;&#37325;&#35201;&#26435;&#37325;&#65292;&#20174;&#32780;&#20174;&#39044;&#20808;&#23384;&#22312;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#36731;&#37327;&#32423;&#25193;&#25955;&#27169;&#22411;&#65292;&#24615;&#33021;&#31283;&#23450;&#65292;&#24182;&#22312;&#35757;&#32451;&#25928;&#29575;&#19978;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.10924</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#32467;&#26500;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Structural Pruning for Diffusion Models. (arXiv:2305.10924v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diff-Pruning&#30340;&#39640;&#25928;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;Taylor&#23637;&#24320;&#36807;&#31243;&#26469;&#35782;&#21035;&#37325;&#35201;&#26435;&#37325;&#65292;&#20174;&#32780;&#20174;&#39044;&#20808;&#23384;&#22312;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#36731;&#37327;&#32423;&#25193;&#25955;&#27169;&#22411;&#65292;&#24615;&#33021;&#31283;&#23450;&#65292;&#24182;&#22312;&#35757;&#32451;&#25928;&#29575;&#19978;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24314;&#27169;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPM&#65289;&#30340;&#36716;&#22411;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#36890;&#24120;&#28041;&#21450;&#21040;&#26174;&#33879;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26399;&#38388;&#37117;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Diff-Pruning&#65292;&#19968;&#31181;&#19987;&#20026;&#20174;&#39044;&#20808;&#23384;&#22312;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#36731;&#37327;&#32423;&#25193;&#25955;&#27169;&#22411;&#32780;&#35774;&#35745;&#30340;&#39640;&#25928;&#21387;&#32553;&#26041;&#27861;&#65292;&#26080;&#38656;&#36827;&#34892;&#22823;&#37327;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;Diff-Pruning&#30340;&#26412;&#36136;&#26159;&#36890;&#36807;&#21098;&#26525;&#26102;&#38388;&#27493;&#38271;&#30340;Taylor&#23637;&#24320;&#65292;&#22312;&#36807;&#28388;&#25481;&#26080;&#36129;&#29486;&#25193;&#25955;&#27493;&#39588;&#21644;&#25972;&#21512;&#26377;&#20449;&#24687;&#30340;&#26799;&#24230;&#26469;&#35782;&#21035;&#37325;&#35201;&#26435;&#37325;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#31361;&#20986;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20004;&#20010;&#20027;&#35201;&#20248;&#28857;&#65306;1&#65289;&#25928;&#29575;&#65306;&#23427;&#21487;&#20197;&#20197;&#21407;&#22987;&#35757;&#32451;&#25237;&#20837;&#30340;&#20165;10&#65285;&#21040;20&#65285;&#30340;&#20195;&#20215;&#23454;&#29616;&#32422;50&#65285;&#30340;FLOPs&#20943;&#23569;; 2&#65289;&#19968;&#33268;&#24615;: &#21098;&#26525;&#21518;&#30340;&#25193;&#25955;&#27169;&#22411;&#20135;&#29983;&#30340;&#25928;&#26524;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#24403;&#65292;&#19981;&#20250;&#24433;&#21709;&#29983;&#25104;&#24314;&#27169;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative modeling has recently undergone remarkable advancements, primarily propelled by the transformative implications of Diffusion Probabilistic Models (DPMs). The impressive capability of these models, however, often entails significant computational overhead during both training and inference. To tackle this challenge, we present Diff-Pruning, an efficient compression method tailored for learning lightweight diffusion models from pre-existing ones, without the need for extensive re-training. The essence of Diff-Pruning is encapsulated in a Taylor expansion over pruned timesteps, a process that disregards non-contributory diffusion steps and ensembles informative gradients to identify important weights. Our empirical assessment, undertaken across four diverse datasets highlights two primary benefits of our proposed method: 1) Efficiency: it enables approximately a 50% reduction in FLOPs at a mere 10% to 20% of the original training expenditure; 2) Consistency: the pruned diffusio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;RWSADMM&#65292;&#20197;&#35299;&#20915;&#22312;&#21160;&#24577;&#32852;&#37030;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#19981;&#19968;&#33268;&#21644;&#36890;&#20449;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12534</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#34892;&#36208;&#38543;&#26426;&#20132;&#26367;&#26041;&#21521;&#20056;&#27861;&#31639;&#27861;&#25512;&#21160;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Mobilizing Personalized Federated Learning via Random Walk Stochastic ADMM. (arXiv:2304.12534v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;RWSADMM&#65292;&#20197;&#35299;&#20915;&#22312;&#21160;&#24577;&#32852;&#37030;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#19981;&#19968;&#33268;&#21644;&#36890;&#20449;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26102;&#23384;&#22312;&#30340;&#38556;&#30861;&#65292;&#20854;&#20013;&#19981;&#33021;&#32500;&#25252;&#20013;&#22830;&#26381;&#21153;&#22120;&#19982;&#25152;&#26377;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#19968;&#33268;&#36830;&#25509;&#65292;&#24182;&#19988;&#25968;&#25454;&#20998;&#24067;&#26159;&#24322;&#26500;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#21160;&#24577;&#32852;&#37030;&#23398;&#20064;&#65292;&#20854;&#20013;&#26381;&#21153;&#22120;&#22312;&#30456;&#37051;&#23458;&#25143;&#31471;&#32452;&#20043;&#38388;&#31227;&#21160;&#20197;&#23398;&#20064;&#26412;&#22320;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21363;&#38543;&#26426;&#34892;&#36208;&#38543;&#26426;&#20132;&#26367;&#26041;&#21521;&#20056;&#27861;&#31639;&#27861;&#65288;RWSADMM&#65289;&#65292;&#21482;&#35201;&#26377;&#36275;&#22815;&#25968;&#37327;&#30340;&#36830;&#25509;&#23458;&#25143;&#31471;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#65292;&#23601;&#33021;&#36866;&#24212;&#21160;&#24577;&#21644;&#21363;&#24109;&#32593;&#32476;&#26465;&#20214;&#12290;&#22312;RWSADMM&#20013;&#65292;&#26381;&#21153;&#22120;&#38543;&#26426;&#21521;&#19968;&#32452;&#23458;&#25143;&#31471;&#34892;&#36208;&#12290;&#23427;&#22522;&#20110;&#30828;&#19981;&#31561;&#24335;&#32422;&#26463;&#24418;&#25104;&#30456;&#37051;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#23616;&#37096;&#36817;&#20284;&#65292;&#32780;&#19981;&#26159;&#37319;&#29992;&#19968;&#33268;&#26356;&#26032;&#26469;&#35299;&#20915;&#25968;&#25454;&#24322;&#26500;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#25910;&#25947;&#30340;&#65292;&#21487;&#20197;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#65292;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research, we investigate the barriers associated with implementing Federated Learning (FL) in real-world scenarios, where a consistent connection between the central server and all clients cannot be maintained, and data distribution is heterogeneous. To address these challenges, we focus on mobilizing the federated setting, where the server moves between groups of adjacent clients to learn local models. Specifically, we propose a new algorithm, Random Walk Stochastic Alternating Direction Method of Multipliers (RWSADMM), capable of adapting to dynamic and ad-hoc network conditions as long as a sufficient number of connected clients are available for model training. In RWSADMM, the server walks randomly toward a group of clients. It formulates local proximity among adjacent clients based on hard inequality constraints instead of consensus updates to address data heterogeneity. Our proposed method is convergent, reduces communication costs, and enhances scalability by reducing th
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Cayley&#21464;&#25442;&#22312;&#20219;&#24847;&#32500;&#24230;&#19978;&#23558;&#26925;&#29699;&#25311;&#21512;&#21040;&#22024;&#26434;&#25968;&#25454;&#20013;&#30340;&#26032;&#31639;&#27861;CTEF&#65292;&#21487;&#20197;&#25311;&#21512;&#20219;&#24847;&#30340;&#26925;&#29699;&#65292;&#24182;&#19988;&#33021;&#25552;&#21462;&#20854;&#20182;&#26041;&#27861;&#26080;&#27861;&#35782;&#21035;&#30340;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#38477;&#32500;&#12289;&#25968;&#25454;&#21487;&#35270;&#21270;&#21644;&#32858;&#31867;&#65292;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2304.10630</link><description>&lt;p&gt;
&#29992;Cayley&#21464;&#25442;&#25311;&#21512;&#26925;&#29699;
&lt;/p&gt;
&lt;p&gt;
Ellipsoid fitting with the Cayley transform. (arXiv:2304.10630v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10630
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Cayley&#21464;&#25442;&#22312;&#20219;&#24847;&#32500;&#24230;&#19978;&#23558;&#26925;&#29699;&#25311;&#21512;&#21040;&#22024;&#26434;&#25968;&#25454;&#20013;&#30340;&#26032;&#31639;&#27861;CTEF&#65292;&#21487;&#20197;&#25311;&#21512;&#20219;&#24847;&#30340;&#26925;&#29699;&#65292;&#24182;&#19988;&#33021;&#25552;&#21462;&#20854;&#20182;&#26041;&#27861;&#26080;&#27861;&#35782;&#21035;&#30340;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#38477;&#32500;&#12289;&#25968;&#25454;&#21487;&#35270;&#21270;&#21644;&#32858;&#31867;&#65292;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;Cayley&#21464;&#25442;&#26925;&#29699;&#25311;&#21512;(CTEF)&#65292;&#23427;&#20351;&#29992;Cayley&#21464;&#25442;&#22312;&#20219;&#24847;&#32500;&#24230;&#19978;&#23558;&#26925;&#29699;&#25311;&#21512;&#21040;&#22024;&#26434;&#30340;&#25968;&#25454;&#20013;&#12290;&#19982;&#35768;&#22810;&#26925;&#29699;&#25311;&#21512;&#26041;&#27861;&#19981;&#21516;&#65292;CTEF&#26159;&#26925;&#29699;&#29305;&#23450;&#30340;&#8212;&#8212;&#24847;&#21619;&#30528;&#23427;&#24635;&#26159;&#36820;&#22238;&#26925;&#22278;&#35299;&#8212;&#8212;&#24182;&#19988;&#21487;&#20197;&#25311;&#21512;&#20219;&#24847;&#30340;&#26925;&#29699;&#12290;&#24403;&#25968;&#25454;&#19981;&#22343;&#21248;&#22320;&#20998;&#24067;&#22312;&#26925;&#29699;&#34920;&#38754;&#19978;&#26102;&#65292;&#23427;&#20063;&#20248;&#20110;&#20854;&#20182;&#25311;&#21512;&#26041;&#27861;&#12290;&#21463;&#26426;&#22120;&#23398;&#20064;&#20013;&#21487;&#35299;&#37322;&#21644;&#21487;&#37325;&#22797;&#26041;&#27861;&#30340;&#21628;&#21505;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;CTEF&#24212;&#29992;&#20110;&#38477;&#32500;&#12289;&#25968;&#25454;&#21487;&#35270;&#21270;&#21644;&#32858;&#31867;&#12290;&#30001;&#20110;CTEF&#25429;&#25417;&#20840;&#23616;&#26354;&#29575;&#65292;&#22240;&#27492;&#23427;&#33021;&#22815;&#25552;&#21462;&#20854;&#20182;&#26041;&#27861;&#26080;&#27861;&#35782;&#21035;&#30340;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#12290;&#36825;&#22312;&#20154;&#31867;&#32454;&#32990;&#21608;&#26399;&#25968;&#25454;&#30340;&#38477;&#32500;&#21644;&#22312;&#32463;&#20856;&#29609;&#20855;&#20363;&#23376;&#30340;&#32858;&#31867;&#30340;&#32972;&#26223;&#19979;&#24471;&#21040;&#20102;&#35828;&#26126;&#12290;&#22312;&#21518;&#19968;&#31181;&#24773;&#20917;&#19979;&#65292;CTEF&#20248;&#20110;10&#31181;&#27969;&#34892;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an algorithm, Cayley transform ellipsoid fitting (CTEF), that uses the Cayley transform to fit ellipsoids to noisy data in any dimension. Unlike many ellipsoid fitting methods, CTEF is ellipsoid specific -- meaning it always returns elliptic solutions -- and can fit arbitrary ellipsoids. It also outperforms other fitting methods when data are not uniformly distributed over the surface of an ellipsoid. Inspired by calls for interpretable and reproducible methods in machine learning, we apply CTEF to dimension reduction, data visualization, and clustering. Since CTEF captures global curvature, it is able to extract nonlinear features in data that other methods fail to identify. This is illustrated in the context of dimension reduction on human cell cycle data, and in the context of clustering on classical toy examples. In the latter case, CTEF outperforms 10 popular clustering algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#26377;&#38480;&#26102;&#38388;&#30028;&#38480;&#65292;&#29992;&#20110;&#34987;&#21160;&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#36870;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#31639;&#27861;&#20805;&#24403;&#38543;&#26426;&#37319;&#26679;&#22120;&#65292;&#24674;&#22797;&#29992;&#22806;&#37096;&#36807;&#31243;&#20248;&#21270;&#32780;&#26469;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.09123</link><description>&lt;p&gt;
&#20351;&#29992;&#34987;&#21160; Langevin &#21160;&#21147;&#23398;&#30340;&#33258;&#36866;&#24212;&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#26377;&#38480;&#26679;&#26412;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Finite-Sample Bounds for Adaptive Inverse Reinforcement Learning using Passive Langevin Dynamics. (arXiv:2304.09123v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#26377;&#38480;&#26102;&#38388;&#30028;&#38480;&#65292;&#29992;&#20110;&#34987;&#21160;&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#36870;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#31639;&#27861;&#20805;&#24403;&#38543;&#26426;&#37319;&#26679;&#22120;&#65292;&#24674;&#22797;&#29992;&#22806;&#37096;&#36807;&#31243;&#20248;&#21270;&#32780;&#26469;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398; (SGLD) &#26159;&#20174;&#27010;&#29575;&#20998;&#24067;&#37319;&#26679;&#30340;&#26377;&#29992;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#34987;&#21160;&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398;&#31639;&#27861; (PSGLD) &#30340;&#26377;&#38480;&#26679;&#26412;&#20998;&#26512;&#65292;&#26088;&#22312;&#23454;&#29616;&#36870;&#24378;&#21270;&#23398;&#20064;&#12290;&#27492;&#22788;&#30340;&#8220;&#34987;&#21160;&#8221;&#26159;&#25351; PSGLD &#31639;&#27861;(&#36870;&#23398;&#20064;&#36807;&#31243;)&#21487;&#29992;&#30340;&#22122;&#22768;&#28176;&#21464;&#26159;&#30001;&#22806;&#37096;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;(&#27491;&#21521;&#23398;&#20064;&#22120;)&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#28857;&#19978;&#35780;&#20272;&#30340;&#12290;PSGLD &#31639;&#27861;&#22240;&#27492;&#20805;&#24403;&#19968;&#20010;&#38543;&#26426;&#37319;&#26679;&#22120;&#65292;&#21487;&#24674;&#22797;&#27491;&#22312;&#34987;&#27492;&#22806;&#37096;&#36807;&#31243;&#20248;&#21270;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#20351;&#29992;&#38543;&#26426;&#36924;&#36817;&#25216;&#26415;&#20998;&#26512;&#20102;&#36825;&#20010;&#34987;&#21160;&#31639;&#27861;&#30340;&#28176;&#36817;&#24615;&#33021;&#65307;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23427;&#30340;&#26377;&#38480;&#26102;&#38388;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#34987;&#21160;&#31639;&#27861;&#21644;&#20854;&#31283;&#23450;&#27979;&#24230;&#20043;&#38388;&#30340; 2-Wasserstein &#36317;&#31163;&#19978;&#30340;&#26377;&#38480;&#26102;&#38388;&#30028;&#38480;&#65292;&#20174;&#20013;&#21487;&#20197;&#33719;&#24471;&#37325;&#24314;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic gradient Langevin dynamics (SGLD) are a useful methodology for sampling from probability distributions. This paper provides a finite sample analysis of a passive stochastic gradient Langevin dynamics algorithm (PSGLD) designed to achieve inverse reinforcement learning. By "passive", we mean that the noisy gradients available to the PSGLD algorithm (inverse learning process) are evaluated at randomly chosen points by an external stochastic gradient algorithm (forward learner). The PSGLD algorithm thus acts as a randomized sampler which recovers the cost function being optimized by this external process. Previous work has analyzed the asymptotic performance of this passive algorithm using stochastic approximation techniques; in this work we analyze the non-asymptotic performance. Specifically, we provide finite-time bounds on the 2-Wasserstein distance between the passive algorithm and its stationary measure, from which the reconstructed cost function is obtained.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38376;&#25511;&#22270;&#21367;&#31215;&#32593;&#32476;(AGGCN)&#65292;&#35813;&#32593;&#32476;&#32467;&#21512;&#21367;&#31215;&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#21644;&#21151;&#33021;&#36830;&#25509;&#24230;&#37327;&#33258;&#36866;&#24212;&#23398;&#20064;&#22270;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#65292;&#24182;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#33041;&#21306;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2304.05874</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#38376;&#25511;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;EEG&#25968;&#25454;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21487;&#35299;&#37322;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Adaptive Gated Graph Convolutional Network for Explainable Diagnosis of Alzheimer's Disease using EEG Data. (arXiv:2304.05874v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38376;&#25511;&#22270;&#21367;&#31215;&#32593;&#32476;(AGGCN)&#65292;&#35813;&#32593;&#32476;&#32467;&#21512;&#21367;&#31215;&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#21644;&#21151;&#33021;&#36830;&#25509;&#24230;&#37327;&#33258;&#36866;&#24212;&#23398;&#20064;&#22270;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#65292;&#24182;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#33041;&#21306;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#20998;&#31867;&#33041;&#30005;&#22270;(EEG)&#25968;&#25454;&#65292;&#28982;&#32780;&#65292;&#22522;&#20110;GNN&#30340;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#65292;&#22914;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;(AD)&#30340;&#35786;&#26029;&#20173;&#28982;&#26159;&#30456;&#23545;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#38376;&#25511;&#22270;&#21367;&#31215;&#32593;&#32476;(AGGCN)&#65292;&#35813;&#32593;&#32476;&#21487;&#20197;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;AGGCN&#36890;&#36807;&#23558;&#22522;&#20110;&#21367;&#31215;&#30340;&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#19982;&#22522;&#20110;&#21151;&#33021;&#36830;&#25509;&#24615;&#30340;&#33879;&#21517;&#30456;&#20851;&#24230;&#37327;&#30456;&#32467;&#21512;&#26469;&#33258;&#36866;&#24212;&#23398;&#20064;&#22270;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#38376;&#25511;&#22270;&#21367;&#31215;&#21487;&#20197;&#21160;&#24577;&#22320;&#21152;&#26435;&#32771;&#34385;&#21508;&#31181;&#31354;&#38388;&#23610;&#24230;&#30340;&#36129;&#29486;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#38381;&#30524;&#21644;&#30529;&#30524;&#29366;&#24577;&#19979;&#22343;&#33021;&#21462;&#24471;&#36739;&#39640;&#30340;&#31934;&#24230;&#65292;&#34920;&#26126;&#23398;&#20064;&#21040;&#30340;&#34920;&#24449;&#32467;&#26524;&#30340;&#31283;&#23450;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;AGGCN&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;AD&#26368;&#21463;&#24433;&#21709;&#30340;&#33041;&#21306;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural network (GNN) models are increasingly being used for the classification of electroencephalography (EEG) data. However, GNN-based diagnosis of neurological disorders, such as Alzheimer's disease (AD), remains a relatively unexplored area of research. Previous studies have relied on functional connectivity methods to infer brain graph structures and used simple GNN architectures for the diagnosis of AD. In this work, we propose a novel adaptive gated graph convolutional network (AGGCN) that can provide explainable predictions. AGGCN adaptively learns graph structures by combining convolution-based node feature enhancement with a well-known correlation-based measure of functional connectivity. Furthermore, the gated graph convolution can dynamically weigh the contribution of various spatial scales. The proposed model achieves high accuracy in both eyes-closed and eyes-open conditions, indicating the stability of learned representations. Finally, we demonstrate that the propos
&lt;/p&gt;</description></item><item><title>TraffNet&#26159;&#19968;&#20010;&#23398;&#20064;&#20132;&#36890;&#37327;&#29983;&#25104;&#21407;&#22240;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#34920;&#31034;&#20026;&#24322;&#26500;&#22270;&#65292;&#21033;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#20102;&#23545;&#20132;&#36890;&#29983;&#25104;&#21407;&#22240;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2303.15954</link><description>&lt;p&gt;
TraffNet&#65306;&#23398;&#20064;&#36947;&#36335;&#32593;&#32476;&#25968;&#23383;&#23402;&#29983;&#20132;&#36890;&#29983;&#25104;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins. (arXiv:2303.15954v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15954
&lt;/p&gt;
&lt;p&gt;
TraffNet&#26159;&#19968;&#20010;&#23398;&#20064;&#20132;&#36890;&#37327;&#29983;&#25104;&#21407;&#22240;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#34920;&#31034;&#20026;&#24322;&#26500;&#22270;&#65292;&#21033;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#20102;&#23545;&#20132;&#36890;&#29983;&#25104;&#21407;&#22240;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36947;&#36335;&#32593;&#32476;&#25968;&#23383;&#23402;&#29983;&#65288;RNDT&#65289;&#22312;&#24320;&#21457;&#19979;&#19968;&#20195;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#20132;&#36890;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#20026;&#20102;&#25903;&#25345;&#23454;&#26102;&#20915;&#31574;&#65292;RNDT&#38656;&#35201;&#19968;&#20010;&#27169;&#22411;&#65292;&#20174;&#22312;&#32447;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#21160;&#24577;&#23398;&#20064;&#20132;&#36890;&#27169;&#24335;&#24182;&#29983;&#25104;&#39640;&#20445;&#30495;&#27169;&#25311;&#32467;&#26524;&#12290;&#23613;&#31649;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24403;&#21069;&#20132;&#36890;&#39044;&#27979;&#25216;&#26415;&#24050;&#32463;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#20123;&#25216;&#26415;&#20165;&#36890;&#36807;&#25366;&#25496;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#26469;&#39044;&#27979;&#26410;&#26469;&#20132;&#36890;&#65292;&#32780;&#24573;&#30053;&#20102;&#20132;&#36890;&#29983;&#25104;&#30340;&#21407;&#22240;&#65292;&#20363;&#22914;&#20132;&#36890;&#38656;&#27714;&#21644;&#36335;&#24452;&#36873;&#25321;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#23545;&#20110;&#23454;&#26102;&#20915;&#31574;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#31216;&#20026; TraffNet&#65292;&#35813;&#26694;&#26550;&#20174;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#20013;&#23398;&#20064;&#20132;&#36890;&#37327;&#30340;&#22240;&#26524;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#24322;&#26500;&#22270;&#26469;&#34920;&#31034;&#36947;&#36335;&#32593;&#32476;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24182;&#20837;&#39044;&#27979;&#25152;&#38656;&#30340;&#20854;&#20182;&#25968;&#25454;&#65292;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#33021;&#22815;&#39044;&#27979;&#20132;&#36890;&#37327;&#30340;&#22240;&#26524;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Road network digital twins (RNDTs) play a critical role in the development of next-generation intelligent transportation systems, enabling more precise traffic planning and control. To support just-in-time (JIT) decision making, RNDTs require a model that dynamically learns the traffic patterns from online sensor data and generates high-fidelity simulation results. Although current traffic prediction techniques based on graph neural networks have achieved state-of-the-art performance, these techniques only predict future traffic by mining correlations in historical traffic data, disregarding the causes of traffic generation, such as traffic demands and route selection. Therefore, their performance is unreliable for JIT decision making. To fill this gap, we introduce a novel deep learning framework called TraffNet that learns the causality of traffic volume from vehicle trajectory data. First, we use a heterogeneous graph to represent the road network, allowing the model to incorporate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20248;&#21270;&#20989;&#25968;&#36229;&#27700;&#24179;&#38598;&#22312;&#32593;&#32476;&#31867;&#31574;&#30053;&#21644;&#34920;&#26684;&#24335;&#19979;&#22987;&#32456;&#26159;&#36830;&#36890;&#30340;&#65292;&#24182;&#24212;&#29992;&#27492;&#32467;&#26524;&#23548;&#20986;&#20102;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#30340;&#26497;&#23567;&#26497;&#22823;&#23450;&#29702;&#12290;</title><link>http://arxiv.org/abs/2303.12981</link><description>&lt;p&gt;
(&#28145;&#24230;)&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36830;&#25509;&#36229;&#27700;&#24179;&#38598;&#21450;&#20854;&#22312;&#26497;&#23567;&#26497;&#22823;&#23450;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Connected Superlevel Set in (Deep) Reinforcement Learning and its Application to Minimax Theorems. (arXiv:2303.12981v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20248;&#21270;&#20989;&#25968;&#36229;&#27700;&#24179;&#38598;&#22312;&#32593;&#32476;&#31867;&#31574;&#30053;&#21644;&#34920;&#26684;&#24335;&#19979;&#22987;&#32456;&#26159;&#36830;&#36890;&#30340;&#65292;&#24182;&#24212;&#29992;&#27492;&#32467;&#26524;&#23548;&#20986;&#20102;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#30340;&#26497;&#23567;&#26497;&#22823;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#25913;&#21892;&#24378;&#21270;&#23398;&#20064;&#20013;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#30340;&#20248;&#21270;&#20989;&#25968;&#22270;&#26223;&#29702;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31574;&#30053;&#21442;&#25968;&#30340;&#20248;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#36229;&#27700;&#24179;&#38598;&#65292;&#22312;&#32593;&#32476;&#31867;&#31574;&#30053;&#21644;&#34920;&#26684;&#24335;&#19979;&#22987;&#32456;&#26159;&#36830;&#36890;&#30340;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22870;&#21169;&#20316;&#20026;&#36229;&#27700;&#24179;&#38598;&#30340;&#20989;&#25968;&#28385;&#36275;&#26356;&#24378;&#30340;&#8220;&#31561;&#36830;&#36890;&#8221;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24212;&#29992;&#36825;&#20123;&#36229;&#27700;&#24179;&#38598;&#30340;&#36830;&#36890;&#24615;&#32467;&#26524;&#65292;&#23548;&#20986;&#20102;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#30340;&#26497;&#23567;&#26497;&#22823;&#23450;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20219;&#20309;&#19968;&#20010;&#22312;&#19968;&#20391;&#20026;&#20984;&#30340;&#65292;&#21478;&#19968;&#20391;&#20026;&#31561;&#36830;&#36890;&#30340;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#37117;&#26377;&#32435;&#20160;&#22343;&#34913;&#12290;&#36825;&#20123;&#32467;&#35770;&#26159;&#26032;&#39062;&#32780;&#19988;&#20043;&#21069;&#26410;&#30693;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this paper is to improve the understanding of the optimization landscape for policy optimization problems in reinforcement learning. Specifically, we show that the superlevel set of the objective function with respect to the policy parameter is always a connected set both in the tabular setting and under policies represented by a class of neural networks. In addition, we show that the optimization objective as a function of the policy parameter and reward satisfies a stronger "equiconnectedness" property. To our best knowledge, these are novel and previously unknown discoveries.  We present an application of the connectedness of these superlevel sets to the derivation of minimax theorems for robust reinforcement learning. We show that any minimax optimization program which is convex on one side and is equiconnected on the other side observes the minimax equality (i.e. has a Nash equilibrium). We find that this exact structure is exhibited by an interesting robust reinforceme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#27880;&#37322;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#22768;&#23398;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;ASR&#31995;&#32479;&#65292;&#24182;&#22312;&#39046;&#22495;&#29305;&#23450;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#21830;&#19994;ASR&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.10510</link><description>&lt;p&gt;
&#38754;&#21521;&#39046;&#22495;&#29305;&#23450;&#35821;&#38899;&#35782;&#21035;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning System for Domain-specific speech Recognition. (arXiv:2303.10510v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#27880;&#37322;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#22768;&#23398;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;ASR&#31995;&#32479;&#65292;&#24182;&#22312;&#39046;&#22495;&#29305;&#23450;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#21830;&#19994;ASR&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#26426;&#35821;&#38899;&#25509;&#21475;&#36234;&#26469;&#36234;&#20415;&#25463;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#34987;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#21830;&#19994;ASR&#31995;&#32479;&#36890;&#24120;&#22312;&#39046;&#22495;&#29305;&#23450;&#35821;&#38899;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#36739;&#24046;&#12290;&#20316;&#32773;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;DeepSpeech2&#21644;Wav2Vec2&#22768;&#23398;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#21463;&#30410;&#29305;&#23450;&#30340;ASR&#31995;&#32479;&#12290;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#27880;&#37322;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#65292;&#21482;&#38656;&#23569;&#37327;&#20154;&#24037;&#24178;&#39044;&#21363;&#21487;&#12290;&#26368;&#20339;&#24615;&#33021;&#26469;&#33258;&#19968;&#31181;&#32463;&#36807;&#24494;&#35843;&#30340;Wav2Vec2-Large-LV60&#22768;&#23398;&#27169;&#22411;&#65292;&#24102;&#26377;&#22806;&#37096;KenLM&#65292;&#22312;&#21463;&#30410;&#29305;&#23450;&#35821;&#38899;&#19978;&#36229;&#36234;&#20102;Google&#21644;AWS ASR&#31995;&#32479;&#12290;&#36824;&#30740;&#31350;&#20102;&#23558;&#23481;&#26131;&#20986;&#38169;&#30340;ASR&#36716;&#24405;&#20316;&#20026;&#21475;&#35821;&#29702;&#35299;&#65288;SLU&#65289;&#30340;&#19968;&#37096;&#20998;&#30340;&#21487;&#34892;&#24615;&#12290;&#21463;&#30410;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#39046;&#22495;&#29305;&#23450;&#24494;&#35843;&#30340;ASR&#31995;&#32479;&#21487;&#20197;&#36229;&#36234;&#21830;&#19994;ASR&#31995;&#32479;&#24182;&#25552;&#39640;NLU&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As human-machine voice interfaces provide easy access to increasingly intelligent machines, many state-of-the-art automatic speech recognition (ASR) systems are proposed. However, commercial ASR systems usually have poor performance on domain-specific speech especially under low-resource settings. The author works with pre-trained DeepSpeech2 and Wav2Vec2 acoustic models to develop benefit-specific ASR systems. The domain-specific data are collected using proposed semi-supervised learning annotation with little human intervention. The best performance comes from a fine-tuned Wav2Vec2-Large-LV60 acoustic model with an external KenLM, which surpasses the Google and AWS ASR systems on benefit-specific speech. The viability of using error prone ASR transcriptions as part of spoken language understanding (SLU) is also investigated. Results of a benefit-specific natural language understanding (NLU) task show that the domain-specific fine-tuned ASR system can outperform the commercial ASR sys
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#23616;&#37096;&#36951;&#24536;&#30340;&#26032;&#22411;&#37325;&#25918;&#32531;&#20914;&#21306;&#65292;&#21487;&#20197;&#22312;&#29366;&#24577;&#31354;&#38388;&#30340;&#30456;&#20851;&#37096;&#20998;&#24555;&#36895;&#36951;&#24536;&#36807;&#26102;&#30340;&#32463;&#39564;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#33258;&#36866;&#24212;&#28145;&#24230;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#21152;&#36895;&#20102;&#23398;&#20064;&#36895;&#24230;&#24182;&#25913;&#21892;&#20102;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2303.08690</link><description>&lt;p&gt;
&#24102;&#26377;&#23616;&#37096;&#36951;&#24536;&#30340;&#37325;&#25918;&#32531;&#20914;&#21306;&#29992;&#20110;&#33258;&#36866;&#24212;&#28145;&#24230;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Replay Buffer With Local Forgetting for Adaptive Deep Model-Based Reinforcement Learning. (arXiv:2303.08690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08690
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#23616;&#37096;&#36951;&#24536;&#30340;&#26032;&#22411;&#37325;&#25918;&#32531;&#20914;&#21306;&#65292;&#21487;&#20197;&#22312;&#29366;&#24577;&#31354;&#38388;&#30340;&#30456;&#20851;&#37096;&#20998;&#24555;&#36895;&#36951;&#24536;&#36807;&#26102;&#30340;&#32463;&#39564;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#33258;&#36866;&#24212;&#28145;&#24230;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#21152;&#36895;&#20102;&#23398;&#20064;&#36895;&#24230;&#24182;&#25913;&#21892;&#20102;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31185;&#23398;&#20013;&#29992;&#20110;&#30830;&#23450;&#25152;&#30740;&#31350;&#30340;&#23545;&#35937;&#65288;&#26080;&#35770;&#26159;&#21870;&#40831;&#21160;&#29289;&#36824;&#26159;&#20154;&#31867;&#65289;&#26159;&#21542;&#34920;&#29616;&#20986;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#23398;&#20064;&#30340;&#20851;&#38190;&#34892;&#20026;&#29305;&#24449;&#20043;&#19968;&#26159;&#23545;&#29615;&#22659;&#20013;&#23616;&#37096;&#21464;&#21270;&#30340;&#26377;&#25928;&#36866;&#24212;&#12290;&#28982;&#32780;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;&#28145;&#24230;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#26041;&#27861;&#36739;&#38590;&#36866;&#24212;&#36825;&#31181;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37325;&#25918;&#32531;&#20914;&#21306;&#65292;&#24102;&#26377;&#23616;&#37096;&#36951;&#24536;&#65292;&#21487;&#20197;&#24555;&#36895;&#22320;&#22312;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#30456;&#20851;&#37096;&#20998;&#36951;&#24536;&#36807;&#26102;&#30340;&#32463;&#39564;&#32780;&#22312;&#20854;&#20182;&#22320;&#26041;&#20445;&#30041;&#26087;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23548;&#33322;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#25913;&#21892;&#20102;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#21152;&#24555;&#20102;&#23398;&#20064;&#36895;&#24230;&#24182;&#25913;&#21892;&#20102;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the key behavioral characteristics used in neuroscience to determine whether the subject of study -- be it a rodent or a human -- exhibits model-based learning is effective adaptation to local changes in the environment. In reinforcement learning, however, recent work has shown that modern deep model-based reinforcement-learning (MBRL) methods adapt poorly to such changes. An explanation for this mismatch is that MBRL methods are typically designed with sample-efficiency on a single task in mind and the requirements for effective adaptation are substantially higher, both in terms of the learned world model and the planning routine. One particularly challenging requirement is that the learned world model has to be sufficiently accurate throughout relevant parts of the state-space. This is challenging for deep-learning-based world models due to catastrophic forgetting. And while a replay buffer can mitigate the effects of catastrophic forgetting, the traditional first-in-first-out
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20498;&#25968;&#27010;&#29575;&#20998;&#25968;&#20272;&#35745;&#22120;&#65288;UIPS&#65289;&#65292;&#29992;&#20110;&#25913;&#36827;&#31163;&#32447;&#23398;&#20064;&#65292;&#36890;&#36807;&#26126;&#30830;&#27169;&#25311;&#20272;&#35745;&#30340;&#35760;&#24405;&#31574;&#30053;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#30456;&#23545;&#20110;&#24191;&#27867;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#20855;&#26377;&#20248;&#36234;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.06389</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#31163;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Off-Policy Learning. (arXiv:2303.06389v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20498;&#25968;&#27010;&#29575;&#20998;&#25968;&#20272;&#35745;&#22120;&#65288;UIPS&#65289;&#65292;&#29992;&#20110;&#25913;&#36827;&#31163;&#32447;&#23398;&#20064;&#65292;&#36890;&#36807;&#26126;&#30830;&#27169;&#25311;&#20272;&#35745;&#30340;&#35760;&#24405;&#31574;&#30053;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#30456;&#23545;&#20110;&#24191;&#27867;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#20855;&#26377;&#20248;&#36234;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an Uncertainty-aware Inverse Propensity Score estimator (UIPS) for improved off-policy learning, which explicitly models the uncertainty in the estimated logging policy and demonstrates advantageous sample efficiency against an extensive list of state-of-the-art baselines on synthetic and three real-world recommendation datasets.
&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#23398;&#20064;&#26159;&#25351;&#20165;&#36890;&#36807;&#35760;&#24405;&#30340;&#21453;&#39304;&#25968;&#25454;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#30340;&#36807;&#31243;&#65292;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#37325;&#35201;&#24615;&#65292;&#20363;&#22914;&#25628;&#32034;&#24341;&#25806;&#12289;&#25512;&#33616;&#31995;&#32479;&#31561;&#12290;&#34429;&#28982;&#29983;&#25104;&#35760;&#24405;&#25968;&#25454;&#30340;&#30495;&#23454;&#35760;&#24405;&#31574;&#30053;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#65292;&#20294;&#20197;&#21069;&#30340;&#24037;&#20316;&#20165;&#22312;&#31163;&#32447;&#23398;&#20064;&#20013;&#37319;&#29992;&#20854;&#20272;&#35745;&#20540;&#65292;&#24573;&#30053;&#20102;&#30001;&#20110;&#36825;&#31181;&#20272;&#35745;&#22120;&#23548;&#33268;&#30340;&#39640;&#20559;&#24046;&#21644;&#39640;&#26041;&#24046;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#23567;&#19988;&#20272;&#35745;&#19981;&#20934;&#30830;&#30340;&#35760;&#24405;&#27010;&#29575;&#30340;&#26679;&#26412;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#27169;&#25311;&#20102;&#20272;&#35745;&#30340;&#35760;&#24405;&#31574;&#30053;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20498;&#25968;&#27010;&#29575;&#20998;&#25968;&#20272;&#35745;&#22120;&#65288;UIPS&#65289;&#26469;&#25913;&#36827;&#31163;&#32447;&#23398;&#20064;&#12290;&#22312;&#21512;&#25104;&#21644;&#19977;&#20010;&#30495;&#23454;&#30340;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;UIPS&#20272;&#35745;&#22120;&#30456;&#23545;&#20110;&#24191;&#27867;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#20855;&#26377;&#20248;&#36234;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-policy learning, referring to the procedure of policy optimization with access only to logged feedback data, has shown importance in various real-world applications, such as search engines, recommender systems, and etc. While the ground-truth logging policy, which generates the logged data, is usually unknown, previous work simply takes its estimated value in off-policy learning, ignoring both high bias and high variance resulted from such an estimator, especially on samples with small and inaccurately estimated logging probabilities. In this work, we explicitly model the uncertainty in the estimated logging policy and propose a Uncertainty-aware Inverse Propensity Score estimator (UIPS) for improved off-policy learning. Experiment results on synthetic and three real-world recommendation datasets demonstrate the advantageous sample efficiency of the proposed UIPS estimator against an extensive list of state-of-the-art baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39034;&#24207;&#20272;&#35745;&#22522;&#20110;&#20107;&#20214;&#30340;&#20809;&#27969;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27969;&#31243;&#65292;&#35813;&#27969;&#31243;&#36890;&#36807;&#26032;&#22411;&#30340;&#23545;&#27604;&#24230;&#26368;&#22823;&#21270;&#20844;&#24335;&#35757;&#32451;&#25345;&#32493;&#36816;&#34892;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#20197;&#36866;&#24212;&#38750;&#32447;&#24615;&#21644;&#21464;&#21270;&#32479;&#35745;&#30340;&#36755;&#20837;&#20107;&#20214;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2303.05214</link><description>&lt;p&gt;
&#39535;&#26381;&#23545;&#27604;&#24230;&#26368;&#22823;&#21270;&#20197;&#23398;&#20064;&#39034;&#24207;&#12289;&#20302;&#24310;&#36831;&#12289;&#22522;&#20110;&#20107;&#20214;&#30340;&#20809;&#27969;
&lt;/p&gt;
&lt;p&gt;
Taming Contrast Maximization for Learning Sequential, Low-latency, Event-based Optical Flow. (arXiv:2303.05214v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39034;&#24207;&#20272;&#35745;&#22522;&#20110;&#20107;&#20214;&#30340;&#20809;&#27969;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27969;&#31243;&#65292;&#35813;&#27969;&#31243;&#36890;&#36807;&#26032;&#22411;&#30340;&#23545;&#27604;&#24230;&#26368;&#22823;&#21270;&#20844;&#24335;&#35757;&#32451;&#25345;&#32493;&#36816;&#34892;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#20197;&#36866;&#24212;&#38750;&#32447;&#24615;&#21644;&#21464;&#21270;&#32479;&#35745;&#30340;&#36755;&#20837;&#20107;&#20214;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#30456;&#26426;&#26368;&#36817;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#20026;&#22797;&#26434;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#25552;&#20379;&#20102;&#20302;&#24310;&#36831;&#21644;&#20302;&#21151;&#29575;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#65292;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#21033;&#29992;&#20107;&#20214;&#25968;&#25454;&#30340;&#29420;&#29305;&#24615;&#36136;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#20173;&#28982;&#21463;&#21040;&#22522;&#20110;&#24103;&#30340;&#25991;&#29486;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#24448;&#24448;&#26080;&#27861;&#20817;&#29616;&#36825;&#20123;&#25215;&#35834;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#21040;&#20102;&#36825;&#19968;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27969;&#31243;&#65292;&#29992;&#20110;&#39034;&#24207;&#20272;&#35745;&#22522;&#20110;&#20107;&#20214;&#30340;&#20809;&#27969;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#25193;&#23637;&#21040;&#39640;&#25512;&#29702;&#39057;&#29575;&#12290;&#26680;&#24515;&#26159;&#19968;&#20010;&#25345;&#32493;&#36816;&#34892;&#30340;&#24102;&#29366;&#24577;&#31070;&#32463;&#27169;&#22411;&#65292;&#20351;&#29992;&#23545;&#27604;&#24230;&#26368;&#22823;&#21270;&#30340;&#26032;&#22411;&#20844;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#20854;&#23545;&#36755;&#20837;&#20107;&#20214;&#20013;&#30340;&#38750;&#32447;&#24615;&#21644;&#21464;&#21270;&#32479;&#35745;&#20445;&#25345;&#31283;&#20581;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#22312;&#31934;&#24230;&#26041;&#38754;&#21019;&#36896;&#20102;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event cameras have recently gained significant traction since they open up new avenues for low-latency and low-power solutions to complex computer vision problems. To unlock these solutions, it is necessary to develop algorithms that can leverage the unique nature of event data. However, the current state-of-the-art is still highly influenced by the frame-based literature, and usually fails to deliver on these promises. In this work, we take this into consideration and propose a novel self-supervised learning pipeline for the sequential estimation of event-based optical flow that allows for the scaling of the models to high inference frequencies. At its core, we have a continuously-running stateful neural model that is trained using a novel formulation of contrast maximization that makes it robust to nonlinearities and varying statistics in the input events. Results across multiple datasets confirm the effectiveness of our method, which establishes a new state of the art in terms of ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#31532;&#19968;&#31687;&#30740;&#31350;&#19981;&#21516;&#24212;&#29992;&#65288;&#20559;&#35265;&#28040;&#38500;&#12289;&#28151;&#28102;&#35299;&#20915;&#12289;&#38544;&#31169;&#20445;&#25252;&#65289;&#36951;&#24536;&#38382;&#39064;&#30340;&#35770;&#25991;&#65292;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#24212;&#29992;&#30340;&#36951;&#24536;&#23450;&#20041;&#21644;&#25351;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;SCRUB&#65292;&#19968;&#31181;&#22312;&#19981;&#21516;&#24212;&#29992;&#30340;&#36951;&#24536;&#36136;&#37327;&#24230;&#37327;&#19978;&#22987;&#32456;&#26159;&#39030;&#32423;&#34920;&#29616;&#32773;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.09880</link><description>&lt;p&gt;
&#36808;&#21521;&#26080;&#30028;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Towards Unbounded Machine Unlearning. (arXiv:2302.09880v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#31532;&#19968;&#31687;&#30740;&#31350;&#19981;&#21516;&#24212;&#29992;&#65288;&#20559;&#35265;&#28040;&#38500;&#12289;&#28151;&#28102;&#35299;&#20915;&#12289;&#38544;&#31169;&#20445;&#25252;&#65289;&#36951;&#24536;&#38382;&#39064;&#30340;&#35770;&#25991;&#65292;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#24212;&#29992;&#30340;&#36951;&#24536;&#23450;&#20041;&#21644;&#25351;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;SCRUB&#65292;&#19968;&#31181;&#22312;&#19981;&#21516;&#24212;&#29992;&#30340;&#36951;&#24536;&#36136;&#37327;&#24230;&#37327;&#19978;&#22987;&#32456;&#26159;&#39030;&#32423;&#34920;&#29616;&#32773;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26426;&#22120;&#36951;&#24536;&#26159;&#25351;&#20174;&#35757;&#32451;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#8220;&#31227;&#38500;&#8221;&#20854;&#35757;&#32451;&#38598;&#30340;&#19968;&#20010;&#23376;&#38598;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#38750;&#24120;&#21450;&#26102;&#65292;&#24182;&#19988;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#21253;&#25324;&#35299;&#38500;&#20559;&#35265;&#65288;RB&#65289;&#12289;&#28040;&#38500;&#28151;&#28102;&#65288;RC&#65289;&#65288;&#30001;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#26631;&#31614;&#25968;&#25454;&#24341;&#36215;&#65289;&#65292;&#20197;&#21450;&#20801;&#35768;&#29992;&#25143;&#34892;&#20351;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#20197;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#65288;UP&#65289;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#26412;&#25991;&#26159;&#25105;&#20204;&#25152;&#30693;&#30340;&#31532;&#19968;&#31687;&#30740;&#31350;&#19981;&#21516;&#24212;&#29992;&#65288;RB&#12289;RC&#12289;UP&#65289;&#30340;&#36951;&#24536;&#38382;&#39064;&#30340;&#35770;&#25991;&#65292;&#25105;&#20204;&#35748;&#20026;&#27599;&#20010;&#24212;&#29992;&#37117;&#26377;&#33258;&#24049;&#30340;&#24536;&#35760;&#38656;&#27714;&#12289;&#24536;&#35760;&#23450;&#20041;&#21644;&#19982;&#24536;&#35760;&#36136;&#37327;&#30456;&#20851;&#30340;&#25351;&#26631;&#12290;&#23545;&#20110;UP&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36951;&#24536;&#30340;&#26032;&#39062;&#36866;&#24212;&#24615;&#24378;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;SCRUB&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36951;&#24536;&#31639;&#27861;&#65292;&#22312;RB&#12289;RC&#21644;UP&#30340;&#19981;&#21516;&#24212;&#29992;&#30456;&#20851;&#24230;&#37327;&#25351;&#26631;&#19978;&#22987;&#32456;&#26159;&#24536;&#35760;&#36136;&#37327;&#30340;&#39030;&#32423;&#34920;&#29616;&#32773;&#12290;&#21516;&#26102;&#65292;SCRUB&#36824;&#22312;&#34913;&#37327;&#27169;&#24335;&#30340;&#24230;&#37327;&#25351;&#26631;&#19978;&#22987;&#32456;&#26159;&#39030;&#32423;&#34920;&#29616;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep machine unlearning is the problem of `removing' from a trained neural network a subset of its training set. This problem is very timely and has many applications, including the key tasks of removing biases (RB), resolving confusion (RC) (caused by mislabelled data in trained models), as well as allowing users to exercise their `right to be forgotten' to protect User Privacy (UP). This paper is the first, to our knowledge, to study unlearning for different applications (RB, RC, UP), with the view that each has its own desiderata, definitions for `forgetting' and associated metrics for forget quality. For UP, we propose a novel adaptation of a strong Membership Inference Attack for unlearning. We also propose SCRUB, a novel unlearning algorithm, which is the only method that is consistently a top performer for forget quality across the different application-dependent metrics for RB, RC, and UP. At the same time, SCRUB is also consistently a top performer on metrics that measure mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Thompson-CHM&#30340;&#28176;&#36817;&#26368;&#20248;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20984;&#21253;&#25104;&#21592;&#38382;&#39064;&#65292;&#19988;&#23558;&#31639;&#27861;&#25193;&#23637;&#21040;&#20102;&#19968;&#32500;&#21644;&#22810;&#32500;&#29615;&#22659;&#20013;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21253;&#25324;&#20572;&#27490;&#35268;&#21017;&#21644;&#37319;&#26679;&#35268;&#21017;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.02033</link><description>&lt;p&gt;
&#19968;&#20010;&#28176;&#36817;&#26368;&#20248;&#30340;&#20984;&#21253;&#25104;&#21592;&#38382;&#39064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Asymptotically Optimal Algorithm for the Convex Hull Membership Problem. (arXiv:2302.02033v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Thompson-CHM&#30340;&#28176;&#36817;&#26368;&#20248;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20984;&#21253;&#25104;&#21592;&#38382;&#39064;&#65292;&#19988;&#23558;&#31639;&#27861;&#25193;&#23637;&#21040;&#20102;&#19968;&#32500;&#21644;&#22810;&#32500;&#29615;&#22659;&#20013;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21253;&#25324;&#20572;&#27490;&#35268;&#21017;&#21644;&#37319;&#26679;&#35268;&#21017;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#20984;&#21253;&#25104;&#21592;&#38382;&#39064;&#30340;&#32431;&#25506;&#32034;&#35774;&#32622;&#19982;&#20984;&#21253;&#22343;&#20540;&#30340;&#26377;&#38480;&#20998;&#24067;&#38598;&#21512;&#20013;&#26377;&#25928;&#20934;&#30830;&#22320;&#30830;&#23450;&#32473;&#23450;&#28857;&#26159;&#21542;&#22312;&#20984;&#21253;&#20013;&#30456;&#20851;&#12290;&#25105;&#20204;&#22312;&#19968;&#32500;&#29615;&#22659;&#20013;&#23436;&#20840;&#21051;&#30011;&#20102;&#20984;&#21253;&#25104;&#21592;&#38382;&#39064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#28176;&#36817;&#26368;&#20248;&#31639;&#27861;&#65292;&#21517;&#20026;Thompson-CHM&#65292;&#20854;&#27169;&#22359;&#21270;&#35774;&#35745;&#21253;&#25324;&#20572;&#27490;&#35268;&#21017;&#21644;&#37319;&#26679;&#35268;&#21017;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#31639;&#27861;&#25193;&#23637;&#21040;&#20102;&#19968;&#20123;&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#25991;&#29486;&#20013;&#24191;&#20041;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;Thompson-CHM&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#25193;&#23637;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#20197;&#23637;&#31034;&#31639;&#27861;&#30340;&#32463;&#39564;&#34892;&#20026;&#19982;&#25105;&#20204;&#22312;&#23454;&#38469;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#29702;&#35770;&#32467;&#26524;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies the pure-exploration setting for the convex hull membership (CHM) problem where one aims to efficiently and accurately determine if a given point lies in the convex hull of means of a finite set of distributions. We give a complete characterization of the sample complexity of the CHM problem in the one-dimensional setting. We present the first asymptotically optimal algorithm called Thompson-CHM, whose modular design consists of a stopping rule and a sampling rule. In addition, we extend the algorithm to settings that generalize several important problems in the multi-armed bandit literature. Furthermore, we discuss the extension of Thompson-CHM to higher dimensions. Finally, we provide numerical experiments to demonstrate the empirical behavior of the algorithm matches our theoretical results for realistic time horizons.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#28436;&#21592;-&#35780;&#35770;&#23478;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#33976;&#39311;&#20248;&#21183;&#22312;&#21033;&#29992;&#36807;&#21435;&#32463;&#39564;&#30340;&#21516;&#26102;&#36981;&#24490;&#31283;&#23450;&#30340;&#22312;&#32447;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#23398;&#20064;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#31639;&#27861;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2302.00533</link><description>&lt;p&gt;
&#33976;&#39311;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Distillation Policy Optimization. (arXiv:2302.00533v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#28436;&#21592;-&#35780;&#35770;&#23478;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#33976;&#39311;&#20248;&#21183;&#22312;&#21033;&#29992;&#36807;&#21435;&#32463;&#39564;&#30340;&#21516;&#26102;&#36981;&#24490;&#31283;&#23450;&#30340;&#22312;&#32447;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#23398;&#20064;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#31639;&#27861;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28436;&#21592;-&#35780;&#35770;&#23478;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20511;&#37492;&#20102;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#35270;&#35282;&#21644;&#20004;&#31181;&#31574;&#30053;&#25913;&#36827;&#25968;&#25454;&#30340;&#20132;&#21449;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#23398;&#20064;&#24182;&#21487;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#31639;&#27861;&#31867;&#21035;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#39318;&#20808;&#25552;&#20986;&#20102;&#26041;&#24046;&#20943;&#23569;&#26426;&#21046;&#65292;&#20363;&#22914;&#32479;&#19968;&#20248;&#21183;&#20272;&#35745;&#22120; (UAE) &#21644;&#19968;&#20010;&#23398;&#20064;&#30340;&#22522;&#32447;&#65292;&#19981;&#20165;&#26159;&#36830;&#25509;&#21040;&#21160;&#20316;&#20540;&#20989;&#25968;&#30340;&#26725;&#26753;&#65292;&#36824;&#33021;&#25552;&#28860;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
On-policy algorithms are supposed to be stable, however, sample-intensive yet. Off-policy algorithms utilizing past experiences are deemed to be sample-efficient, nevertheless, unstable in general. Can we design an algorithm that can employ the off-policy data, while exploit the stable learning by sailing along the course of the on-policy walkway? In this paper, we present an actor-critic learning framework that borrows the distributional perspective of interest to evaluate, and cross-breeds two sources of the data for policy improvement, which enables fast learning and can be applied to a wide class of algorithms. In its backbone, the variance reduction mechanisms, such as unified advantage estimator (UAE), that extends generalized advantage estimator (GAE) to be applicable on any state-dependent baseline, and a learned baseline, that is competent to stabilize the policy gradient, are firstly put forward to not merely be a bridge to the action-value function but also distill the advan
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#25105;&#28608;&#21169;&#30340;&#22810;&#26234;&#33021;&#20307;&#25506;&#32034;&#26041;&#27861;&#65292;&#22312;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#25214;&#21040;&#33258;&#25105;&#25506;&#32034;&#21644;&#22242;&#38431;&#21512;&#20316;&#30340;&#24179;&#34913;&#65292;&#20197;&#23454;&#29616;&#22242;&#38431;&#20219;&#21153;&#30340;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2301.02083</link><description>&lt;p&gt;
&#33258;&#25105;&#28608;&#21169;&#30340;&#22810;&#26234;&#33021;&#20307;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Self-Motivated Multi-Agent Exploration. (arXiv:2301.02083v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02083
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#25105;&#28608;&#21169;&#30340;&#22810;&#26234;&#33021;&#20307;&#25506;&#32034;&#26041;&#27861;&#65292;&#22312;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#25214;&#21040;&#33258;&#25105;&#25506;&#32034;&#21644;&#22242;&#38431;&#21512;&#20316;&#30340;&#24179;&#34913;&#65292;&#20197;&#23454;&#29616;&#22242;&#38431;&#20219;&#21153;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26234;&#33021;&#20307;&#22312;&#33258;&#25105;&#25506;&#32034;&#21644;&#22242;&#38431;&#21512;&#20316;&#20043;&#38388;&#23454;&#29616;&#24179;&#34913;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#26234;&#33021;&#20307;&#22312;&#27809;&#26377;&#21327;&#35843;&#30340;&#24773;&#20917;&#19979;&#20960;&#20046;&#26080;&#27861;&#23436;&#25104;&#22242;&#38431;&#20219;&#21153;&#65292;&#24182;&#19988;&#23481;&#26131;&#38519;&#20837;&#21482;&#36827;&#34892;&#31616;&#21333;&#21512;&#20316;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#20013;&#65292;&#26080;&#27861;&#36827;&#34892;&#36275;&#22815;&#30340;&#20010;&#20307;&#25506;&#32034;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#26234;&#33021;&#20307;&#30340;&#21327;&#35843;&#25506;&#32034;&#65292;&#23548;&#33268;&#29366;&#24577;&#31354;&#38388;&#30340;&#25506;&#32034;&#25351;&#25968;&#32423;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#28608;&#21169;&#30340;&#22810;&#26234;&#33021;&#20307;&#25506;&#32034;&#65288;SMMAE&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#25214;&#21040;&#33258;&#25105;&#25506;&#32034;&#21644;&#22242;&#38431;&#21512;&#20316;&#20043;&#38388;&#30340;&#26435;&#34913;&#26469;&#21462;&#24471;&#22242;&#38431;&#20219;&#21153;&#30340;&#25104;&#21151;&#12290;&#22312;SMMAE&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#27599;&#20010;&#26234;&#33021;&#20307;&#35757;&#32451;&#19968;&#20010;&#29420;&#31435;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#20197;&#26368;&#22823;&#21270;&#23427;&#20204;&#33258;&#24049;&#25152;&#35775;&#38382;&#30340;&#29366;&#24577;&#31354;&#38388;&#12290;&#27599;&#20010;&#26234;&#33021;&#20307;&#26681;&#25454;&#32852;&#21512;&#22242;&#38431;&#31574;&#30053;&#30340;&#31283;&#23450;&#24615;&#23398;&#20064;&#21487;&#35843;&#33410;&#30340;&#25506;&#32034;&#27010;&#29575;&#12290;&#22312;StarCraft II&#24494;&#31649;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;SMMAE&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In cooperative multi-agent reinforcement learning (CMARL), it is critical for agents to achieve a balance between self-exploration and team collaboration. However, agents can hardly accomplish the team task without coordination and they would be trapped in a local optimum where easy cooperation is accessed without enough individual exploration. Recent works mainly concentrate on agents' coordinated exploration, which brings about the exponentially grown exploration of the state space. To address this issue, we propose Self-Motivated Multi-Agent Exploration (SMMAE), which aims to achieve success in team tasks by adaptively finding a trade-off between self-exploration and team cooperation. In SMMAE, we train an independent exploration policy for each agent to maximize their own visited state space. Each agent learns an adjustable exploration probability based on the stability of the joint team policy. The experiments on highly cooperative tasks in StarCraft II micromanagement benchmark (
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#20803;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;iMODE&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36712;&#36857;&#20013;&#30340;&#21160;&#21147;&#23398;&#65292;&#21487;&#20197;&#24555;&#36895;&#20934;&#30830;&#22320;&#24314;&#27169;&#21644;&#39044;&#27979;&#22810;&#20010;&#19981;&#21516;&#29289;&#29702;&#21442;&#25968;&#30340;&#21160;&#24577;&#31995;&#32479;&#65292;&#24182;&#33021;&#21453;&#21521;&#25512;&#26029;&#31995;&#32479;&#30340;&#29289;&#29702;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2301.00957</link><description>&lt;p&gt;
&#20174;&#36712;&#36857;&#20013;&#20803;&#23398;&#20064;&#21487;&#27867;&#21270;&#30340;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Metalearning generalizable dynamics from trajectories. (arXiv:2301.00957v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#20803;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;iMODE&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36712;&#36857;&#20013;&#30340;&#21160;&#21147;&#23398;&#65292;&#21487;&#20197;&#24555;&#36895;&#20934;&#30830;&#22320;&#24314;&#27169;&#21644;&#39044;&#27979;&#22810;&#20010;&#19981;&#21516;&#29289;&#29702;&#21442;&#25968;&#30340;&#21160;&#24577;&#31995;&#32479;&#65292;&#24182;&#33021;&#21453;&#21521;&#25512;&#26029;&#31995;&#32479;&#30340;&#29289;&#29702;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#20803;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;iMODE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#23398;&#20064;&#21487;&#27867;&#21270;&#65288;&#21363;&#19981;&#29305;&#23450;&#20110;&#21442;&#25968;&#65289;&#30340;&#22810;&#20010;&#21160;&#24577;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#65292;&#36825;&#20123;&#31995;&#32479;&#22312;&#20854;&#29289;&#29702;&#21442;&#25968;&#19978;&#26377;&#25152;&#21464;&#21270;&#12290;iMODE&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#26469;&#23398;&#20064;&#20803;&#30693;&#35782;&#65292;&#21363;&#21160;&#21147;&#31995;&#32479;&#23454;&#20363;&#30340;&#21147;&#22330;&#30340;&#21151;&#33021;&#21464;&#21270;&#65292;&#32780;&#19981;&#30693;&#36947;&#29289;&#29702;&#21442;&#25968;&#65306;&#22806;&#23618;&#25429;&#25417;&#30740;&#31350;&#30340;&#21160;&#21147;&#31995;&#32479;&#23454;&#20363;&#20043;&#38388;&#30340;&#20849;&#21516;&#21147;&#22330;&#24418;&#24335;&#65292;&#20869;&#23618;&#21017;&#36866;&#24212;&#21508;&#20010;&#31995;&#32479;&#23454;&#20363;&#12290;&#20808;&#39564;&#30340;&#29289;&#29702;&#30693;&#35782;&#21487;&#20197;&#26041;&#20415;&#22320;&#23884;&#20837;&#21040;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;&#65292;&#20363;&#22914;&#20445;&#23432;&#21147;&#22330;&#21644;&#27431;&#20960;&#37324;&#24471;&#23545;&#31216;&#24615;&#12290;&#36890;&#36807;&#23398;&#20064;&#21040;&#30340;&#20803;&#30693;&#35782;&#65292;iMODE&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#23545;&#26410;&#30693;&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#21453;&#21521;&#25581;&#31034;&#20851;&#20110;&#31995;&#32479;&#29289;&#29702;&#21442;&#25968;&#30340;&#30693;&#35782;&#65292;&#25110;&#20316;&#20026;&#8220;&#27979;&#37327;&#8221;&#26410;&#30693;&#31995;&#32479;&#29289;&#29702;&#21442;&#25968;&#30340;&#31070;&#32463;&#37327;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the interpretable meta neural ordinary differential equation (iMODE) method to rapidly learn generalizable (i.e., not parameter-specific) dynamics from trajectories of multiple dynamical systems that vary in their physical parameters. The iMODE method learns meta-knowledge, the functional variations of the force field of dynamical system instances without knowing the physical parameters, by adopting a bi-level optimization framework: an outer level capturing the common force field form among studied dynamical system instances and an inner level adapting to individual system instances. A priori physical knowledge can be conveniently embedded in the neural network architecture as inductive bias, such as conservative force field and Euclidean symmetry. With the learned meta-knowledge, iMODE can model an unseen system within seconds, and inversely reveal knowledge on the physical parameters of a system, or as a Neural Gauge to "measure" the physical parameters of an unseen syste
&lt;/p&gt;</description></item><item><title>UnICLAM&#26159;&#19968;&#31181;&#32479;&#19968;&#21644;&#21487;&#35299;&#37322;&#30340;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#21644;&#23545;&#25239;&#24615;&#23631;&#34109;&#65292;&#23454;&#29616;&#20102;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#23545;&#40784;&#21644;&#35821;&#20041;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2212.10729</link><description>&lt;p&gt;
UnICLAM&#65306;&#23545;&#25239;&#24615;&#23631;&#34109;&#30340;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#32479;&#19968;&#21644;&#21487;&#35299;&#37322;&#30340;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
UnICLAM:Contrastive Representation Learning with Adversarial Masking for Unified and Interpretable Medical Vision Question Answering. (arXiv:2212.10729v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10729
&lt;/p&gt;
&lt;p&gt;
UnICLAM&#26159;&#19968;&#31181;&#32479;&#19968;&#21644;&#21487;&#35299;&#37322;&#30340;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#21644;&#23545;&#25239;&#24615;&#23631;&#34109;&#65292;&#23454;&#29616;&#20102;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#23545;&#40784;&#21644;&#35821;&#20041;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#65288;Medical-VQA&#65289;&#26088;&#22312;&#22238;&#31572;&#26377;&#20851;&#25918;&#23556;&#23398;&#22270;&#20687;&#30340;&#20020;&#24202;&#38382;&#39064;&#65292;&#20026;&#21307;&#29983;&#25552;&#20379;&#20915;&#31574;&#36873;&#39033;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;Medical-VQA&#27169;&#22411;&#36890;&#36807;&#23558;&#35270;&#35273;&#21644;&#32441;&#29702;&#32534;&#30721;&#22120;&#20998;&#21035;&#25918;&#32622;&#22312;&#21452;&#29420;&#31435;&#31354;&#38388;&#20013;&#26469;&#23398;&#20064;&#36328;&#27169;&#24577;&#34920;&#31034;&#65292;&#36825;&#23548;&#33268;&#38388;&#25509;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UnICLAM&#65292;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#21644;&#23545;&#25239;&#24615;&#23631;&#34109;&#23454;&#29616;&#32479;&#19968;&#21644;&#21487;&#35299;&#37322;&#30340;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#23398;&#20064;&#23545;&#40784;&#30340;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21452;&#27969;&#39044;&#35757;&#32451;&#32467;&#26500;&#65292;&#37319;&#29992;&#36880;&#28176;&#36719;&#21442;&#25968;&#20849;&#20139;&#31574;&#30053;&#12290;&#25216;&#26415;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#23398;&#20064;&#20102;&#19968;&#20010;&#32422;&#26463;&#65292;&#20351;&#24471;&#35270;&#35273;&#21644;&#32441;&#29702;&#32534;&#30721;&#22120;&#22312;&#21516;&#19968;&#31354;&#38388;&#20013;&#25509;&#36817;&#65292;&#38543;&#30528;&#23618;&#25968;&#30340;&#22686;&#21152;&#65292;&#36880;&#28176;&#25918;&#26494;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25226;&#25569;&#32479;&#19968;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#25105;&#20204;&#23558;&#23545;&#25239;&#24615;&#23631;&#34109;&#25968;&#25454;&#22686;&#24378;&#25299;&#23637;&#21040;&#23545;&#27604;&#34920;&#31034;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical Visual Question Answering (Medical-VQA) aims to to answer clinical questions regarding radiology images, assisting doctors with decision-making options. Nevertheless, current Medical-VQA models learn cross-modal representations through residing vision and texture encoders in dual separate spaces, which lead to indirect semantic alignment. In this paper, we propose UnICLAM, a Unified and Interpretable Medical-VQA model through Contrastive Representation Learning with Adversarial Masking. Specifically, to learn an aligned image-text representation, we first establish a unified dual-stream pre-training structure with the gradually soft-parameter sharing strategy. Technically, the proposed strategy learns a constraint for the vision and texture encoders to be close in a same space, which is gradually loosened as the higher number of layers. Moreover, for grasping the unified semantic representation, we extend the adversarial masking data augmentation to the contrastive representati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#22823;&#23478;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#20154;&#33080;&#29983;&#25104;&#21644;&#32534;&#36753;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#29305;&#21035;&#20171;&#32461;&#20102;&#22522;&#20110;GAN&#26550;&#26500;&#30340;&#26368;&#26032;&#26041;&#27861;StyleGAN&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20154;&#33080;&#22270;&#20687;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#21487;&#25511;&#35821;&#20041;&#32534;&#36753;&#21644;&#20445;&#25345;&#29031;&#29255;&#36136;&#37327;&#30340;&#25509;&#21475;&#12290;</title><link>http://arxiv.org/abs/2212.09102</link><description>&lt;p&gt;
&#22522;&#20110;StyleGAN&#30340;&#20154;&#33080;&#29983;&#25104;&#21644;&#32534;&#36753;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Face Generation and Editing with StyleGAN: A Survey. (arXiv:2212.09102v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#22823;&#23478;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#20154;&#33080;&#29983;&#25104;&#21644;&#32534;&#36753;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#29305;&#21035;&#20171;&#32461;&#20102;&#22522;&#20110;GAN&#26550;&#26500;&#30340;&#26368;&#26032;&#26041;&#27861;StyleGAN&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20154;&#33080;&#22270;&#20687;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#21487;&#25511;&#35821;&#20041;&#32534;&#36753;&#21644;&#20445;&#25345;&#29031;&#29255;&#36136;&#37327;&#30340;&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30446;&#30340;&#22312;&#20110;&#25552;&#20379;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20154;&#33080;&#29983;&#25104;&#21644;&#32534;&#36753;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#32508;&#36848;&#12290;&#25105;&#20204;&#23558;&#20171;&#32461;&#27969;&#34892;&#30340;&#26368;&#26032;&#26550;&#26500;&#24182;&#35752;&#35770;&#20851;&#38190;&#24605;&#24819;&#65292;&#22914;&#21453;&#28436;&#65292;&#28508;&#22312;&#34920;&#31034;&#65292;&#25439;&#22833;&#20989;&#25968;&#65292;&#35757;&#32451;&#31243;&#24207;&#65292;&#32534;&#36753;&#26041;&#27861;&#21644;&#36328;&#22495;&#26679;&#24335;&#36716;&#31227;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#20197;GAN&#26550;&#26500;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#26368;&#32456;&#38598;&#25104;&#22312;StyleGAN&#20013;&#65292;&#23427;&#20801;&#35768;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20154;&#33080;&#22270;&#20687;&#24182;&#25552;&#20379;&#20016;&#23500;&#30340;&#21487;&#25511;&#35821;&#20041;&#32534;&#36753;&#21644;&#20445;&#25345;&#29031;&#29255;&#36136;&#37327;&#30340;&#25509;&#21475;&#12290;&#25105;&#20204;&#26088;&#22312;&#20026;&#23545;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#26377;&#22522;&#26412;&#20102;&#35299;&#19988;&#23547;&#27714;&#26131;&#20110;&#29702;&#35299;&#30340;&#20171;&#32461;&#21644;&#27010;&#36848;&#30340;&#35835;&#32773;&#25552;&#20379;&#19968;&#20010;&#20837;&#38376;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our goal with this survey is to provide an overview of the state of the art deep learning technologies for face generation and editing. We will cover popular latest architectures and discuss key ideas that make them work, such as inversion, latent representation, loss functions, training procedures, editing methods, and cross domain style transfer. We particularly focus on GAN-based architectures that have culminated in the StyleGAN approaches, which allow generation of high-quality face images and offer rich interfaces for controllable semantics editing and preserving photo quality. We aim to provide an entry point into the field for readers that have basic knowledge about the field of deep learning and are looking for an accessible introduction and overview.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20013;&#25104;&#21151;&#20351;&#29992;&#30340;14&#31181;&#26631;&#20934;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#20174;&#20248;&#21270;&#25991;&#29486;&#30340;&#35282;&#24230;&#23545;&#25968;&#20540;&#20248;&#21270;&#20013;&#30340;&#22256;&#38590;&#36827;&#34892;&#20102;&#29702;&#35770;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2211.15596</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#32508;&#36848; - &#19968;&#38454;&#21644;&#20108;&#38454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A survey of deep learning optimizers -- first and second order methods. (arXiv:2211.15596v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15596
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20013;&#25104;&#21151;&#20351;&#29992;&#30340;14&#31181;&#26631;&#20934;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#20174;&#20248;&#21270;&#25991;&#29486;&#30340;&#35282;&#24230;&#23545;&#25968;&#20540;&#20248;&#21270;&#20013;&#30340;&#22256;&#38590;&#36827;&#34892;&#20102;&#29702;&#35770;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#28041;&#21450;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#26368;&#23567;&#21270;&#39640;&#32500;&#25439;&#22833;&#20989;&#25968;&#65292;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#22256;&#38590;&#65292;&#22914;&#38797;&#28857;&#12289;&#23616;&#37096;&#26368;&#23567;&#20540;&#12289;Hessian&#30697;&#38453;&#30340;&#30149;&#24577;&#21644;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32508;&#21512;&#35780;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20013;&#25104;&#21151;&#20351;&#29992;&#30340;14&#31181;&#26631;&#20934;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#20174;&#20248;&#21270;&#25991;&#29486;&#30340;&#35282;&#24230;&#23545;&#25968;&#20540;&#20248;&#21270;&#30340;&#22256;&#38590;&#36827;&#34892;&#20102;&#29702;&#35770;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning optimization involves minimizing a high-dimensional loss function in the weight space which is often perceived as difficult due to its inherent difficulties such as saddle points, local minima, ill-conditioning of the Hessian and limited compute resources. In this paper, we provide a comprehensive review of $14$ standard optimization methods successfully used in deep learning research and a theoretical assessment of the difficulties in numerical optimization from the optimization literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;GPT-Neo&#27169;&#22411;&#22312;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#20854;&#20182;&#36739;&#22823;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22312;&#36866;&#24403;&#30340;&#36229;&#21442;&#25968;&#35774;&#32622;&#19979;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.15593</link><description>&lt;p&gt;
GPT-Neo&#29992;&#20110;&#24120;&#35782;&#25512;&#29702;--&#29702;&#35770;&#19982;&#23454;&#36341;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
GPT-Neo for commonsense reasoning -- a theoretical and practical lens. (arXiv:2211.15593v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;GPT-Neo&#27169;&#22411;&#22312;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#20854;&#20182;&#36739;&#22823;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22312;&#36866;&#24403;&#30340;&#36229;&#21442;&#25968;&#35774;&#32622;&#19979;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#36827;&#34892;&#26377;&#30417;&#30563;&#24494;&#35843;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;GPT-Neo&#27169;&#22411;&#22312;6&#20010;&#24120;&#35782;&#25512;&#29702;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#26088;&#22312;&#23545;&#20351;&#29992;GPT-Neo&#27169;&#22411;&#30340;&#36739;&#23567;&#27169;&#22411;&#19982;GPT-3&#12289;Llama-2&#12289;MPT&#21644;Falcon&#31561;&#20960;&#20010;&#36739;&#22823;&#27169;&#22411;&#22522;&#20934;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#12290;&#22312;&#20351;&#29992;&#36866;&#24403;&#30340;&#36229;&#21442;&#25968;&#38598;&#36827;&#34892;&#24494;&#35843;&#21518;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#27880;&#24847;&#21147;&#22836;&#21487;&#35270;&#21270;&#26469;&#35843;&#26597;&#21644;&#35777;&#23454;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#22810;&#31181;&#40065;&#26834;&#24615;&#27979;&#35797;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has demonstrated substantial gains in pre-training large-language models (LLMs) followed by supervised fine-tuning on the downstream task. In this paper, we evaluate the performance of the GPT-neo model using $6$ commonsense reasoning benchmark tasks. We aim to examine the performance of smaller models using the GPT-neo models against several larger model baselines such as GPT-$3$, Llama-$2$, MPT and Falcon. Upon fine-tuning with the appropriate set of hyperparameters, our model achieves competitive accuracy on several tasks. We also investigate and substantiate our results using attention-head visualization to better understand the model performance. Finally, we conduct various robustness tests using various methods to gauge the model performance under numerous settings.
&lt;/p&gt;</description></item><item><title>&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#35774;&#32622;&#65292;&#22810;&#20010;&#20855;&#26377;&#20851;&#20110;&#21516;&#19968;&#32452;&#29992;&#25143;&#19981;&#21516;&#29305;&#24449;&#30340;&#21442;&#19982;&#26041;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#20844;&#24320;&#21407;&#22987;&#25968;&#25454;&#25110;&#27169;&#22411;&#21442;&#25968;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;VFL&#27010;&#24565;&#12289;&#31639;&#27861;&#20197;&#21450;&#21508;&#20010;&#26041;&#38754;&#30340;&#36827;&#23637;&#21644;&#25361;&#25112;&#30340;&#32508;&#21512;&#22238;&#39038;&#65292;&#28145;&#20837;&#20998;&#26512;&#20102;&#38544;&#31169;&#20445;&#25252;&#21327;&#35758;&#30340;&#20998;&#31867;&#12289;&#38544;&#31169;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#32771;&#34385;&#22810;&#20010;&#32422;&#26463;&#26465;&#20214;&#30340;&#32479;&#19968;&#26694;&#26550;VFLow&#12290;&#27492;&#22806;&#65292;&#36824;&#22238;&#39038;&#20102;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;VFL&#38754;&#20020;&#30340;&#26410;&#26469;&#25361;&#25112;&#21644;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2211.12814</link><description>&lt;p&gt;
&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65306;&#27010;&#24565;&#12289;&#36827;&#23637;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Learning: Concepts, Advances and Challenges. (arXiv:2211.12814v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12814
&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#35774;&#32622;&#65292;&#22810;&#20010;&#20855;&#26377;&#20851;&#20110;&#21516;&#19968;&#32452;&#29992;&#25143;&#19981;&#21516;&#29305;&#24449;&#30340;&#21442;&#19982;&#26041;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#20844;&#24320;&#21407;&#22987;&#25968;&#25454;&#25110;&#27169;&#22411;&#21442;&#25968;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;VFL&#27010;&#24565;&#12289;&#31639;&#27861;&#20197;&#21450;&#21508;&#20010;&#26041;&#38754;&#30340;&#36827;&#23637;&#21644;&#25361;&#25112;&#30340;&#32508;&#21512;&#22238;&#39038;&#65292;&#28145;&#20837;&#20998;&#26512;&#20102;&#38544;&#31169;&#20445;&#25252;&#21327;&#35758;&#30340;&#20998;&#31867;&#12289;&#38544;&#31169;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#32771;&#34385;&#22810;&#20010;&#32422;&#26463;&#26465;&#20214;&#30340;&#32479;&#19968;&#26694;&#26550;VFLow&#12290;&#27492;&#22806;&#65292;&#36824;&#22238;&#39038;&#20102;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;VFL&#38754;&#20020;&#30340;&#26410;&#26469;&#25361;&#25112;&#21644;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#35774;&#32622;&#65292;&#22810;&#20010;&#20855;&#26377;&#20851;&#20110;&#21516;&#19968;&#32452;&#29992;&#25143;&#19981;&#21516;&#29305;&#24449;&#30340;&#21442;&#19982;&#26041;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#20844;&#24320;&#21407;&#22987;&#25968;&#25454;&#25110;&#27169;&#22411;&#21442;&#25968;&#12290;&#21463;VFL&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#24555;&#36895;&#22686;&#38271;&#30340;&#25512;&#21160;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;VFL&#30340;&#27010;&#24565;&#21644;&#31639;&#27861;&#65292;&#20197;&#21450;&#21508;&#20010;&#26041;&#38754;&#30340;&#24403;&#21069;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#21253;&#25324;&#26377;&#25928;&#24615;&#12289;&#25928;&#29575;&#21644;&#38544;&#31169;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;VFL&#35774;&#32622;&#21644;&#38544;&#31169;&#20445;&#25252;&#21327;&#35758;&#30340;&#35814;&#23613;&#20998;&#31867;&#65292;&#24182;&#23545;&#27599;&#20010;&#21327;&#35758;&#30340;&#38544;&#31169;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;VFLow&#65292;&#23427;&#32771;&#34385;&#20102;VFL&#38382;&#39064;&#22312;&#36890;&#20449;&#12289;&#35745;&#31639;&#12289;&#38544;&#31169;&#20197;&#21450;&#26377;&#25928;&#24615;&#21644;&#20844;&#24179;&#24615;&#32422;&#26463;&#19979;&#30340;&#24773;&#20917;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#24037;&#19994;&#24212;&#29992;&#20013;&#26368;&#26032;&#30340;&#36827;&#23637;&#65292;&#31361;&#20986;&#20102;VFL&#38754;&#20020;&#30340;&#24320;&#25918;&#24615;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Learning (VFL) is a federated learning setting where multiple parties with different features about the same set of users jointly train machine learning models without exposing their raw data or model parameters. Motivated by the rapid growth in VFL research and real-world applications, we provide a comprehensive review of the concept and algorithms of VFL, as well as current advances and challenges in various aspects, including effectiveness, efficiency, and privacy. We provide an exhaustive categorization for VFL settings and privacy-preserving protocols and comprehensively analyze the privacy attacks and defense strategies for each protocol. In the end, we propose a unified framework, termed VFLow, which considers the VFL problem under communication, computation, privacy, as well as effectiveness and fairness constraints. Finally, we review the most recent advances in industrial applications, highlighting open challenges and future directions for VFL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20302;&#31934;&#24230;&#29615;&#22659;&#19979;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;SGD&#21464;&#31181;&#65292;&#24182;&#27979;&#35797;&#20102;&#19981;&#21516;&#21464;&#31181;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;SGD&#26041;&#27861;&#65292;&#22312;&#20302;&#31934;&#24230;&#31639;&#26415;&#29615;&#22659;&#19979;&#20351;&#29992;&#33258;&#36866;&#24212;&#27493;&#38271;&#30340;SGD&#21464;&#31181;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#27979;&#35797;&#38598;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.04655</link><description>&lt;p&gt;
&#20302;&#31934;&#24230;&#29615;&#22659;&#19979;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#25439;&#22833;&#20989;&#25968;&#30340;SGD&#21464;&#31181;
&lt;/p&gt;
&lt;p&gt;
Variants of SGD for Lipschitz Continuous Loss Functions in Low-Precision Environments. (arXiv:2211.04655v4 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20302;&#31934;&#24230;&#29615;&#22659;&#19979;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;SGD&#21464;&#31181;&#65292;&#24182;&#27979;&#35797;&#20102;&#19981;&#21516;&#21464;&#31181;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;SGD&#26041;&#27861;&#65292;&#22312;&#20302;&#31934;&#24230;&#31639;&#26415;&#29615;&#22659;&#19979;&#20351;&#29992;&#33258;&#36866;&#24212;&#27493;&#38271;&#30340;SGD&#21464;&#31181;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#27979;&#35797;&#38598;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20302;&#20301;&#28014;&#28857;&#21644;&#23450;&#28857;&#29615;&#22659;&#19979;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26102;&#20351;&#29992;&#33258;&#36866;&#24212;&#27493;&#38271;&#30340;SGD&#21464;&#31181;&#30340;&#25910;&#25947;&#24615;&#12290;&#32771;&#34385;&#21040;&#19968;&#33324;&#38543;&#26426;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20551;&#35774;&#21482;&#33021;&#35745;&#31639;&#25439;&#22833;&#20989;&#25968;&#38543;&#26426;&#26799;&#24230;&#30340;&#36817;&#20284;&#20540;&#20197;&#21450;&#35745;&#31639;SGD&#27493;&#39588;&#26412;&#36523;&#26102;&#30340;&#35823;&#24046;&#65292;&#32473;&#20986;&#20102;&#19968;&#20010;&#28176;&#36817;&#25910;&#25947;&#21040;Clarke&#31283;&#23450;&#28857;&#30340;&#32467;&#26524;&#21644;&#21040;&#36817;&#20284;&#31283;&#23450;&#28857;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#12290;&#22312;&#21508;&#31181;&#20302;&#31934;&#24230;&#31639;&#26415;&#29615;&#22659;&#19979;&#32463;&#39564;&#22320;&#27979;&#35797;&#20102;&#19981;&#21516;&#30340;SGD&#21464;&#31181;&#65292;&#22312;&#20004;&#20010;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#19982;SGD&#30456;&#27604;&#35266;&#23519;&#21040;&#20102;&#25913;&#36827;&#30340;&#27979;&#35797;&#38598;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by neural network training in low-bit floating and fixed-point environments, this work studies the convergence of variants of SGD using adaptive step sizes with computational error. Considering a general stochastic Lipschitz continuous loss function, an asymptotic convergence result to a Clarke stationary point, and the non-asymptotic convergence to an approximate stationary point are presented assuming that only an approximation of the loss function's stochastic gradient can be computed, as well as error in computing the SGD step itself. Different variants of SGD are tested empirically in a variety of low-precision arithmetic environments, where improved test set accuracy is observed compared to SGD for two image recognition tasks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#38480;&#33258;&#21160;&#26426;&#26469;&#39564;&#35777;&#21644;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#26500;&#24314;&#29305;&#27530;&#30340;&#24369;B&#252;chi&#33258;&#21160;&#26426;&#65292;&#33021;&#22815;&#31934;&#30830;&#22320;&#25429;&#25417;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#36755;&#20986;&#34892;&#20026;&#65292;&#24182;&#29992;&#20110;&#35299;&#20915;DNN&#30340;&#24120;&#35265;&#39564;&#35777;&#21644;&#35299;&#37322;&#20219;&#21153;&#65292;&#22914;&#23545;&#25239;&#40065;&#26834;&#24615;&#25110;&#26368;&#23567;&#20805;&#20998;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2211.01022</link><description>&lt;p&gt;
&#20351;&#29992;&#26377;&#38480;&#33258;&#21160;&#26426;&#39564;&#35777;&#21644;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Verifying And Interpreting Neural Networks using Finite Automata. (arXiv:2211.01022v2 [cs.FL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01022
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#38480;&#33258;&#21160;&#26426;&#26469;&#39564;&#35777;&#21644;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#26500;&#24314;&#29305;&#27530;&#30340;&#24369;B&#252;chi&#33258;&#21160;&#26426;&#65292;&#33021;&#22815;&#31934;&#30830;&#22320;&#25429;&#25417;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#36755;&#20986;&#34892;&#20026;&#65292;&#24182;&#29992;&#20110;&#35299;&#20915;DNN&#30340;&#24120;&#35265;&#39564;&#35777;&#21644;&#35299;&#37322;&#20219;&#21153;&#65292;&#22914;&#23545;&#25239;&#40065;&#26834;&#24615;&#25110;&#26368;&#23567;&#20805;&#20998;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#21253;&#25324;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#30340;&#26222;&#36941;&#20351;&#29992;&#21644;&#20854;&#40657;&#30418;&#29305;&#24615;&#65292;&#39564;&#35777;&#23646;&#24615;&#21644;&#35299;&#37322;DNN&#30340;&#34892;&#20026;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#26426;&#29702;&#35770;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;DNN&#20998;&#26512;&#20013;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DNN&#30340;&#36755;&#20837;&#36755;&#20986;&#34892;&#20026;&#21487;&#20197;&#34987;&#19968;&#20010;&#65288;&#29305;&#27530;&#30340;&#65289;&#24369;B&#252;chi&#33258;&#21160;&#26426;&#31934;&#30830;&#22320;&#25429;&#33719;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#26469;&#35299;&#20915;DNN&#30340;&#24120;&#35265;&#39564;&#35777;&#21644;&#35299;&#37322;&#20219;&#21153;&#65292;&#22914;&#23545;&#25239;&#40065;&#26834;&#24615;&#25110;&#26368;&#23567;&#20805;&#20998;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Verifying properties and interpreting the behaviour of deep neural networks (DNN) is an important task given their ubiquitous use in applications, including safety-critical ones, and their black-box nature. We propose an automata-theoric approach to tackling problems arising in DNN analysis. We show that the input-output behaviour of a DNN can be captured precisely by a (special) weak B\"uchi automaton and we show how these can be used to address common verification and interpretation tasks of DNN like adversarial robustness or minimum sufficient reasons.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#28145;&#24230;&#32593;&#32476;&#31995;&#21015;&#26041;&#27861;&#65292;&#36890;&#36807;&#27531;&#24046;DNN&#36880;&#27493;&#22686;&#21152;&#21160;&#24577;&#33539;&#22260;&#65292;&#36890;&#36807;DNN&#36845;&#20195;&#22320;&#20272;&#35745;&#65292;&#20165;&#29992;&#24456;&#23569;&#30340;&#39033;&#23601;&#33021;&#24471;&#21040;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.16060</link><description>&lt;p&gt;
&#38024;&#23545;&#22823;&#35268;&#27169;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#28145;&#24230;&#32593;&#32476;&#31995;&#21015;
&lt;/p&gt;
&lt;p&gt;
Deep network series for large-scale high-dynamic range imaging. (arXiv:2210.16060v3 [astro-ph.IM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16060
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#28145;&#24230;&#32593;&#32476;&#31995;&#21015;&#26041;&#27861;&#65292;&#36890;&#36807;&#27531;&#24046;DNN&#36880;&#27493;&#22686;&#21152;&#21160;&#24577;&#33539;&#22260;&#65292;&#36890;&#36807;DNN&#36845;&#20195;&#22320;&#20272;&#35745;&#65292;&#20165;&#29992;&#24456;&#23569;&#30340;&#39033;&#23601;&#33021;&#24471;&#21040;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#39640;&#21160;&#24577;&#33539;&#22260;&#35745;&#31639;&#25104;&#20687;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20960;&#20046;&#21487;&#20197;&#30636;&#38388;&#35299;&#20915;&#32447;&#24615;&#21453;&#28436;&#25104;&#20687;&#38382;&#39064;&#12290;&#34429;&#28982;&#23637;&#24320;&#30340;&#26550;&#26500;&#23545;&#20110;&#27979;&#37327;&#35774;&#32622;&#30340;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20294;&#22312;DNN&#26550;&#26500;&#20013;&#23884;&#20837;&#22823;&#35268;&#27169;&#27979;&#37327;&#36816;&#31639;&#31526;&#26159;&#19981;&#23454;&#38469;&#30340;&#12290;&#26367;&#20195;&#30340;&#21363;&#25554;&#21363;&#29992;&#65288;PnP&#65289;&#26041;&#27861;&#23545;&#21487;&#25193;&#23637;&#24615;&#21644;&#39640;&#21160;&#24577;&#33539;&#22260;&#30340;&#25361;&#25112;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#20381;&#36182;&#20110;&#39640;&#24230;&#36845;&#20195;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27531;&#24046;DNN&#31995;&#21015;&#26041;&#27861;&#65292;&#20063;&#21487;&#20197;&#35299;&#37322;&#20026;&#23398;&#20064;&#29256;&#30340;&#21305;&#37197;&#36861;&#36394;&#65292;&#20854;&#20013;&#37325;&#24314;&#30340;&#22270;&#20687;&#26159;&#36880;&#27493;&#22686;&#21152;&#21160;&#24577;&#33539;&#22260;&#30340;&#27531;&#20313;&#22270;&#20687;&#30340;&#21644;&#65292;&#36890;&#36807;DNN&#36845;&#20195;&#22320;&#20272;&#35745;&#65292;&#20197;&#21069;&#19968;&#27425;&#36845;&#20195;&#30340;&#21453;&#25237;&#24433;&#25968;&#25454;&#27531;&#24046;&#20316;&#20026;&#36755;&#20837;&#12290;&#25105;&#20204;&#22312;&#23556;&#30005;&#22825;&#25991;&#25104;&#20687;&#27169;&#25311;&#20013;&#35777;&#26126;&#65292;&#20165;&#26377;&#20960;&#20010;&#39033;&#30340;&#31995;&#21015;&#23601;&#21487;&#20197;&#25552;&#20379;&#36739;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new approach for large-scale high-dynamic range computational imaging. Deep Neural Networks (DNNs) trained end-to-end can solve linear inverse imaging problems almost instantaneously. While unfolded architectures provide robustness to measurement setting variations, embedding large-scale measurement operators in DNN architectures is impractical. Alternative Plug-and-Play (PnP) approaches, where the denoising DNNs are blind to the measurement setting, have proven effective to address scalability and high-dynamic range challenges, but rely on highly iterative algorithms. We propose a residual DNN series approach, also interpretable as a learned version of matching pursuit, where the reconstructed image is a sum of residual images progressively increasing the dynamic range, and estimated iteratively by DNNs taking the back-projected data residual of the previous iteration as input. We demonstrate on radio-astronomical imaging simulations that a series of only few terms provid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#31579;&#36873;&#31574;&#30053;&#23454;&#29616;&#26631;&#31614;&#22122;&#22768;&#40065;&#26834;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#21306;&#20998;&#24178;&#20928;&#26679;&#26412;&#21644;&#26377;&#22122;&#22768;&#30340;&#26679;&#26412;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#30456;&#23545;&#20110;&#26368;&#36817;&#30340;&#30740;&#31350;&#20855;&#26377;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.05330</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#31579;&#36873;&#31574;&#30053;&#23454;&#29616;&#26631;&#31614;&#22122;&#22768;&#40065;&#26834;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Label Noise-Robust Learning using a Confidence-Based Sieving Strategy. (arXiv:2210.05330v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#31579;&#36873;&#31574;&#30053;&#23454;&#29616;&#26631;&#31614;&#22122;&#22768;&#40065;&#26834;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#21306;&#20998;&#24178;&#20928;&#26679;&#26412;&#21644;&#26377;&#22122;&#22768;&#30340;&#26679;&#26412;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#30456;&#23545;&#20110;&#26368;&#36817;&#30340;&#30740;&#31350;&#20855;&#26377;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#30340;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#25913;&#21892;&#27169;&#22411;&#23545;&#36807;&#25311;&#21512;&#30340;&#40065;&#26834;&#24615;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#22240;&#20026;&#27169;&#22411;&#26368;&#32456;&#20250;&#35760;&#20303;&#21253;&#25324;&#22122;&#22768;&#26631;&#31614;&#22312;&#20869;&#30340;&#26631;&#31614;&#12290;&#35782;&#21035;&#20855;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#26679;&#26412;&#24182;&#38450;&#27490;&#27169;&#22411;&#23398;&#20064;&#23427;&#20204;&#26159;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#30340;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#30340;&#27599;&#20010;&#31867;&#21035;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#65292;&#34920;&#31034;&#20026;&#31867;&#21035;&#27010;&#29575;&#65292;&#21487;&#20197;&#20316;&#20026;&#35780;&#20272;&#36755;&#20837;&#26631;&#31614;&#26159;&#21542;&#30495;&#23454;&#26631;&#31614;&#25110;&#32773;&#26159;&#25439;&#22351;&#26631;&#31614;&#30340;&#21487;&#38752;&#26631;&#20934;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21028;&#21035;&#24230;&#37327;&#31216;&#20026;&#32622;&#20449;&#24230;&#35823;&#24046;&#21644;&#19968;&#31181;&#31216;&#20026;CONFES&#30340;&#31579;&#36873;&#31574;&#30053;&#65292;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#24178;&#20928;&#26679;&#26412;&#21644;&#26377;&#22122;&#22768;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#24230;&#37327;&#30340;&#35823;&#24046;&#27010;&#29575;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#65288;&#20363;&#22914;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65289;&#30456;&#23545;&#20110;&#26368;&#36817;&#30340;&#30740;&#31350;&#20855;&#26377;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In learning tasks with label noise, improving model robustness against overfitting is a pivotal challenge because the model eventually memorizes labels, including the noisy ones. Identifying the samples with noisy labels and preventing the model from learning them is a promising approach to address this challenge. When training with noisy labels, the per-class confidence scores of the model, represented by the class probabilities, can be reliable criteria for assessing whether the input label is the true label or the corrupted one. In this work, we exploit this observation and propose a novel discriminator metric called confidence error and a sieving strategy called CONFES to differentiate between the clean and noisy samples effectively. We provide theoretical guarantees on the probability of error for our proposed metric. Then, we experimentally illustrate the superior performance of our proposed approach compared to recent studies on various settings, such as synthetic and real-world
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21160;&#24577;&#23398;&#20064;&#30340;&#31070;&#32463;&#38544;&#24615;&#34920;&#31034;&#23545;&#22810;&#30446;&#26631;&#23548;&#33322;&#36827;&#34892;&#24314;&#27169;&#21644;&#26144;&#23556;&#65292;&#20854;&#20013;&#21253;&#25324;&#35821;&#20041;&#23450;&#20301;&#21644;&#21344;&#29992;&#25506;&#32034;&#38544;&#24615;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2210.05129</link><description>&lt;p&gt;
&#21160;&#24577;&#23398;&#20064;&#30340;&#31070;&#32463;&#38544;&#24615;&#34920;&#31034;&#30340;&#22810;&#30446;&#26631;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Multi-Object Navigation with dynamically learned neural implicit representations. (arXiv:2210.05129v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21160;&#24577;&#23398;&#20064;&#30340;&#31070;&#32463;&#38544;&#24615;&#34920;&#31034;&#23545;&#22810;&#30446;&#26631;&#23548;&#33322;&#36827;&#34892;&#24314;&#27169;&#21644;&#26144;&#23556;&#65292;&#20854;&#20013;&#21253;&#25324;&#35821;&#20041;&#23450;&#20301;&#21644;&#21344;&#29992;&#25506;&#32034;&#38544;&#24615;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21644;&#26144;&#23556;&#26032;&#29615;&#22659;&#26159;&#20219;&#20309;&#33258;&#20027;&#23548;&#33322;&#20195;&#29702;&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#20154;&#23398;&#36890;&#24120;&#20351;&#29992;SLAM&#21464;&#20307;&#20197;&#29420;&#31435;&#30340;&#26041;&#24335;&#20272;&#35745;&#22320;&#22270;&#65292;&#36825;&#20123;&#21464;&#20307;&#32500;&#25252;&#20102;&#25299;&#25169;&#25110;&#24230;&#37327;&#34920;&#31034;&#65292;&#32780;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#23548;&#33322;&#21017;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20445;&#25345;&#20102;&#26576;&#31181;&#24418;&#24335;&#30340;&#35760;&#24518;&#12290;&#32593;&#32476;&#36890;&#24120;&#20855;&#26377;&#24402;&#32435;&#20559;&#24046;&#65292;&#21487;&#20197;&#20174;&#30690;&#37327;&#34920;&#31034;&#21040;&#40479;&#30640;&#24230;&#37327;&#24352;&#37327;&#25110;&#25299;&#25169;&#32467;&#26500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#20004;&#20010;&#31070;&#32463;&#38544;&#24615;&#34920;&#31034;&#26469;&#26500;&#36896;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#27599;&#20010;episode&#26399;&#38388;&#21160;&#24577;&#23398;&#20064;&#24182;&#26144;&#23556;&#22330;&#26223;&#20869;&#23481;&#65306;(i) &#35821;&#20041;&#23450;&#20301;&#22120;&#39044;&#27979;&#20808;&#21069;&#30475;&#21040;&#30340;&#26597;&#35810;&#23545;&#35937;&#30340;&#20301;&#32622;&#65307;(ii) &#21344;&#29992;&#21644;&#25506;&#32034;&#38544;&#24615;&#34920;&#31034;&#23553;&#35013;&#20102;&#24050;&#25506;&#32034;&#21306;&#22495;&#21644;&#38556;&#30861;&#29289;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#26032;&#39062;&#30340;&#20840;&#23616;&#35835;&#21462;&#26426;&#21046;&#36827;&#34892;&#26597;&#35810;&#65292;&#35813;&#26426;&#21046;&#30452;&#25509;&#20174;&#20989;&#25968;&#31354;&#38388;&#26144;&#23556;&#21040;&#21487;&#29992;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding and mapping a new environment are core abilities of any autonomously navigating agent. While classical robotics usually estimates maps in a stand-alone manner with SLAM variants, which maintain a topological or metric representation, end-to-end learning of navigation keeps some form of memory in a neural network. Networks are typically imbued with inductive biases, which can range from vectorial representations to birds-eye metric tensors or topological structures. In this work, we propose to structure neural networks with two neural implicit representations, which are learned dynamically during each episode and map the content of the scene: (i) the Semantic Finder predicts the position of a previously seen queried object; (ii) the Occupancy and Exploration Implicit Representation encapsulates information about explored area and obstacles, and is queried with a novel global read mechanism which directly maps from function space to a usable embedding space. Both representa
&lt;/p&gt;</description></item><item><title>GeONet&#26159;&#19968;&#20010;&#19981;&#21463;&#32593;&#26684;&#24433;&#21709;&#30340;&#28145;&#24230;&#31070;&#32463;&#31639;&#23376;&#32593;&#32476;&#65292;&#23398;&#20064;&#20102;&#20174;&#21021;&#22987;&#21644;&#32456;&#31471;&#20998;&#24067;&#21040;&#36830;&#25509;&#20004;&#20010;&#31471;&#28857;&#20998;&#24067;&#30340;Wasserstein&#27979;&#22320;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#12290;&#36890;&#36807;&#23398;&#20064;&#38797;&#28857;&#20248;&#21270;&#26465;&#20214;&#65292;GeONet&#21487;&#20197;&#24555;&#36895;&#36827;&#34892;&#23454;&#26102;&#39044;&#27979;&#65292;&#24182;&#22312;&#20223;&#30495;&#31034;&#20363;&#21644;&#27979;&#35797;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#19982;&#26631;&#20934;OT&#27714;&#35299;&#22120;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.14440</link><description>&lt;p&gt;
GeONet&#65306;&#19968;&#31181;&#23398;&#20064;Wasserstein&#27979;&#22320;&#30340;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
GeONet: a neural operator for learning the Wasserstein geodesic. (arXiv:2209.14440v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14440
&lt;/p&gt;
&lt;p&gt;
GeONet&#26159;&#19968;&#20010;&#19981;&#21463;&#32593;&#26684;&#24433;&#21709;&#30340;&#28145;&#24230;&#31070;&#32463;&#31639;&#23376;&#32593;&#32476;&#65292;&#23398;&#20064;&#20102;&#20174;&#21021;&#22987;&#21644;&#32456;&#31471;&#20998;&#24067;&#21040;&#36830;&#25509;&#20004;&#20010;&#31471;&#28857;&#20998;&#24067;&#30340;Wasserstein&#27979;&#22320;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#12290;&#36890;&#36807;&#23398;&#20064;&#38797;&#28857;&#20248;&#21270;&#26465;&#20214;&#65292;GeONet&#21487;&#20197;&#24555;&#36895;&#36827;&#34892;&#23454;&#26102;&#39044;&#27979;&#65292;&#24182;&#22312;&#20223;&#30495;&#31034;&#20363;&#21644;&#27979;&#35797;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#19982;&#26631;&#20934;OT&#27714;&#35299;&#22120;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#20256;&#36755;(OT)&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#20960;&#20309;&#19978;&#26377;&#24847;&#20041;&#27604;&#36739;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#20256;&#32479;&#30340;&#35745;&#31639;&#27010;&#29575;&#27979;&#24230;&#30340;Wasserstein&#36317;&#31163;&#21644;&#27979;&#22320;&#30340;&#26041;&#27861;&#38656;&#35201;&#32593;&#26684;&#20381;&#36182;&#30340;&#22495;&#31163;&#25955;&#21270;&#65292;&#21516;&#26102;&#21463;&#21040;&#32500;&#24230;&#28798;&#38590;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GeONet&#65292;&#19968;&#31181;&#19981;&#21463;&#32593;&#26684;&#24433;&#21709;&#30340;&#28145;&#24230;&#31070;&#32463;&#31639;&#23376;&#32593;&#32476;&#65292;&#23427;&#23398;&#20064;&#20102;&#23558;&#36755;&#20837;&#30340;&#21021;&#22987;&#21644;&#32456;&#31471;&#20998;&#24067;&#26144;&#23556;&#21040;&#36830;&#25509;&#20004;&#20010;&#31471;&#28857;&#20998;&#24067;&#30340;Wasserstein&#27979;&#22320;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#12290;&#22312;&#33073;&#26426;&#35757;&#32451;&#38454;&#27573;&#65292;GeONet&#36890;&#36807;&#32806;&#21512;&#30340;PDE&#31995;&#32479;&#34920;&#24449;&#30340;&#21407;&#22987;&#21644;&#23545;&#20598;&#31354;&#38388;&#20013;&#30340;&#21160;&#24577;&#26368;&#20248;&#26465;&#20214;&#23398;&#20064;&#20102;OT&#38382;&#39064;&#30340;&#38797;&#28857;&#20248;&#21270;&#26465;&#20214;&#12290;&#21518;&#32493;&#30340;&#25512;&#29702;&#38454;&#27573;&#26159;&#30636;&#26102;&#23436;&#25104;&#30340;&#65292;&#24182;&#21487;&#20197;&#22312;&#22312;&#32447;&#23398;&#20064;&#35774;&#32622;&#20013;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;GeONet&#22312;&#20223;&#30495;&#31034;&#20363;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Optimal transport (OT) offers a versatile framework to compare complex data distributions in a geometrically meaningful way. Traditional methods for computing the Wasserstein distance and geodesic between probability measures require mesh-dependent domain discretization and suffer from the curse-of-dimensionality. We present GeONet, a mesh-invariant deep neural operator network that learns the non-linear mapping from the input pair of initial and terminal distributions to the Wasserstein geodesic connecting the two endpoint distributions. In the offline training stage, GeONet learns the saddle point optimality conditions for the dynamic formulation of the OT problem in the primal and dual spaces that are characterized by a coupled PDE system. The subsequent inference stage is instantaneous and can be deployed for real-time predictions in the online learning setting. We demonstrate that GeONet achieves comparable testing accuracy to the standard OT solvers on simulation examples and the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#25351;&#23548;&#30340;&#30446;&#26631;-&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25351;&#20195;&#27495;&#20041;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#25945;&#23398;&#27861;&#21644;&#23454;&#29992;&#20027;&#20041;&#30340;&#27010;&#24565;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#27010;&#24565;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#32773;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2209.12758</link><description>&lt;p&gt;
&#20811;&#26381;&#35821;&#35328;&#25351;&#23548;&#30340;&#30446;&#26631;-&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#20013;&#25351;&#20195;&#27495;&#20041;
&lt;/p&gt;
&lt;p&gt;
Overcoming Referential Ambiguity in Language-Guided Goal-Conditioned Reinforcement Learning. (arXiv:2209.12758v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#25351;&#23548;&#30340;&#30446;&#26631;-&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25351;&#20195;&#27495;&#20041;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#25945;&#23398;&#27861;&#21644;&#23454;&#29992;&#20027;&#20041;&#30340;&#27010;&#24565;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#27010;&#24565;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#32773;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25945;&#24072;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#23548;&#19968;&#20010;&#20195;&#29702;&#25191;&#34892;&#26032;&#20219;&#21153;&#26102;&#65292;&#35299;&#37322;&#30340;&#27495;&#20041;&#24456;&#23481;&#26131;&#25104;&#20026;&#38459;&#30861;&#12290;&#24403;&#25945;&#24072;&#36890;&#36807;&#21442;&#32771;&#29289;&#20307;&#30340;&#29305;&#24449;&#21521;&#23398;&#20064;&#32773;&#25552;&#20379;&#25351;&#20196;&#26102;&#65292;&#23398;&#20064;&#32773;&#21487;&#33021;&#20250;&#35823;&#35299;&#25945;&#24072;&#30340;&#24847;&#22270;&#65292;&#29305;&#21035;&#26159;&#24403;&#25351;&#20196;&#27169;&#31946;&#22320;&#28041;&#21450;&#29289;&#20307;&#30340;&#29305;&#24449;&#26102;&#65292;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#25351;&#20195;&#27495;&#20041;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#28304;&#33258;&#35748;&#30693;&#31185;&#23398;&#30340;&#27010;&#24565;&#22914;&#20309;&#24110;&#21161;&#35299;&#20915;&#36825;&#20123;&#25351;&#20195;&#27495;&#20041;&#65306;&#25945;&#23398;&#27861;&#65288;&#36873;&#25321;&#21512;&#36866;&#30340;&#25351;&#20196;&#65289;&#21644;&#23454;&#29992;&#20027;&#20041;&#65288;&#36890;&#36807;&#24402;&#32435;&#25512;&#29702;&#20102;&#35299;&#20854;&#20182;&#20195;&#29702;&#30340;&#20559;&#22909;&#65289;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#24605;&#24819;&#24212;&#29992;&#20110;&#19968;&#20010;&#24102;&#26377;&#20004;&#20010;&#20154;&#24037;&#20195;&#29702;&#30340;&#27169;&#25311;&#26426;&#22120;&#20154;&#20219;&#21153;&#65288;&#22534;&#31215;&#26408;&#22359;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#27010;&#24565;&#22914;&#20309;&#25552;&#39640;&#23398;&#20064;&#32773;&#30340;&#35757;&#32451;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Teaching an agent to perform new tasks using natural language can easily be hindered by ambiguities in interpretation. When a teacher provides an instruction to a learner about an object by referring to its features, the learner can misunderstand the teacher's intentions, for instance if the instruction ambiguously refer to features of the object, a phenomenon called referential ambiguity. We study how two concepts derived from cognitive sciences can help resolve those referential ambiguities: pedagogy (selecting the right instructions) and pragmatism (learning the preferences of the other agents using inductive reasoning). We apply those ideas to a teacher/learner setup with two artificial agents on a simulated robotic task (block-stacking). We show that these concepts improve sample efficiency for training the learner.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31283;&#20581;&#21644;&#32422;&#26463;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#30005;&#21160;&#27773;&#36710;AMoD&#31995;&#32479;&#20013;&#30340;&#20877;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#32771;&#34385;&#20102;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#21644;&#20915;&#31574;&#32422;&#26463;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#35774;&#35745;&#20986;&#31283;&#20581;&#30340;&#30005;&#21160;&#27773;&#36710;&#20877;&#24179;&#34913;&#31574;&#30053;&#65292;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.08230</link><description>&lt;p&gt;
AMoD&#31995;&#32479;&#20013;&#30340;&#31283;&#20581;&#21644;&#32422;&#26463;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30005;&#21160;&#27773;&#36710;&#20877;&#24179;&#34913;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Robust and Constrained Multi-Agent Reinforcement Learning Electric Vehicle Rebalancing Method in AMoD Systems. (arXiv:2209.08230v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08230
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31283;&#20581;&#21644;&#32422;&#26463;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#30005;&#21160;&#27773;&#36710;AMoD&#31995;&#32479;&#20013;&#30340;&#20877;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#32771;&#34385;&#20102;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#21644;&#20915;&#31574;&#32422;&#26463;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#35774;&#35745;&#20986;&#31283;&#20581;&#30340;&#30005;&#21160;&#27773;&#36710;&#20877;&#24179;&#34913;&#31574;&#30053;&#65292;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21160;&#27773;&#36710;&#22312;&#33258;&#20027;&#31227;&#21160;&#20986;&#34892;&#65288;AMoD&#65289;&#31995;&#32479;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#20854;&#29420;&#29305;&#30340;&#20805;&#30005;&#27169;&#24335;&#22686;&#21152;&#20102;AMoD&#31995;&#32479;&#20013;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65288;&#20363;&#22914;&#29366;&#24577;&#36716;&#31227;&#27010;&#29575;&#65289;&#12290;&#30001;&#20110;&#35757;&#32451;&#29615;&#22659;&#21644;&#27979;&#35797;/&#30495;&#23454;&#29615;&#22659;&#20043;&#38388;&#36890;&#24120;&#23384;&#22312;&#19981;&#21305;&#37197;&#24773;&#20917;&#65292;&#23558;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#31995;&#32479;&#35774;&#35745;&#26159;&#23454;&#38469;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#23578;&#26410;&#26126;&#30830;&#32771;&#34385;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#23545;&#30005;&#21160;&#27773;&#36710;AMoD&#31995;&#32479;&#20877;&#24179;&#34913;&#30340;&#24433;&#21709;&#65292;&#32780;&#20915;&#31574;&#38656;&#35201;&#28385;&#36275;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#21644;&#32422;&#26463;&#30340;&#20849;&#23384;&#20351;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20855;&#26377;&#29366;&#24577;&#36716;&#31227;&#26680;&#19981;&#30830;&#23450;&#24615;&#30340;&#31283;&#20581;&#21644;&#32422;&#26463;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#29992;&#20110;&#30005;&#21160;&#27773;&#36710;AMoD&#31995;&#32479;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#31283;&#20581;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#65288;RNPG&#65289;&#35757;&#32451;&#31283;&#20581;&#30340;&#30005;&#21160;&#27773;&#36710;&#20877;&#24179;&#34913;&#31574;&#30053;&#30340;&#31283;&#20581;&#21644;&#32422;&#26463;&#30340;MARL&#31639;&#27861;&#65288;ROCOMA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electric vehicles (EVs) play critical roles in autonomous mobility-on-demand (AMoD) systems, but their unique charging patterns increase the model uncertainties in AMoD systems (e.g. state transition probability). Since there usually exists a mismatch between the training and test/true environments, incorporating model uncertainty into system design is of critical importance in real-world applications. However, model uncertainties have not been considered explicitly in EV AMoD system rebalancing by existing literature yet, and the coexistence of model uncertainties and constraints that the decision should satisfy makes the problem even more challenging. In this work, we design a robust and constrained multi-agent reinforcement learning (MARL) framework with state transition kernel uncertainty for EV AMoD systems. We then propose a robust and constrained MARL algorithm (ROCOMA) with robust natural policy gradients (RNPG) that trains a robust EV rebalancing policy to balance the supply-d
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#25991;&#31456;&#25506;&#35752;&#20102;&#40479;&#30640;&#35270;&#35282;&#24863;&#30693;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26041;&#27861;&#65292;&#20027;&#35201;&#20851;&#27880;&#20102;&#20174;&#36879;&#35270;&#35270;&#22270;&#21040;&#40479;&#30640;&#35270;&#22270;&#30340;&#20449;&#24687;&#36716;&#25442;&#12289;&#22320;&#38754;&#30495;&#20540;&#27880;&#37322;&#33719;&#21462;&#12289;&#29305;&#24449;&#34701;&#21512;&#20197;&#21450;&#25972;&#20307;&#27969;&#31243;&#30340;&#26500;&#24314;&#12290;</title><link>http://arxiv.org/abs/2209.05324</link><description>&lt;p&gt;
&#25506;&#32034;&#40479;&#30640;&#35270;&#35282;&#24863;&#30693;&#20013;&#30340;&#25361;&#25112;&#65306;&#19968;&#39033;&#32508;&#36848;&#12289;&#35780;&#20272;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Delving into the Devils of Bird's-eye-view Perception: A Review, Evaluation and Recipe. (arXiv:2209.05324v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#25991;&#31456;&#25506;&#35752;&#20102;&#40479;&#30640;&#35270;&#35282;&#24863;&#30693;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26041;&#27861;&#65292;&#20027;&#35201;&#20851;&#27880;&#20102;&#20174;&#36879;&#35270;&#35270;&#22270;&#21040;&#40479;&#30640;&#35270;&#22270;&#30340;&#20449;&#24687;&#36716;&#25442;&#12289;&#22320;&#38754;&#30495;&#20540;&#27880;&#37322;&#33719;&#21462;&#12289;&#29305;&#24449;&#34701;&#21512;&#20197;&#21450;&#25972;&#20307;&#27969;&#31243;&#30340;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24863;&#30693;&#20219;&#21153;&#20013;&#65292;&#23398;&#20064;&#40479;&#30640;&#35270;&#35282;&#65288;BEV&#65289;&#30340;&#24378;&#22823;&#34920;&#31034;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#24182;&#24341;&#36215;&#20102;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#20256;&#32479;&#26041;&#27861;&#20013;&#65292;&#22823;&#22810;&#25968;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#22312;&#21069;&#26041;&#25110;&#36879;&#35270;&#35270;&#22270;&#20013;&#36827;&#34892;&#26816;&#27979;&#12289;&#20998;&#21106;&#12289;&#36319;&#36394;&#31561;&#25805;&#20316;&#12290;&#38543;&#30528;&#20256;&#24863;&#22120;&#37197;&#32622;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#20174;&#19981;&#21516;&#20256;&#24863;&#22120;&#20013;&#38598;&#25104;&#22810;&#28304;&#20449;&#24687;&#24182;&#20197;&#32479;&#19968;&#30340;&#35270;&#22270;&#34920;&#31034;&#29305;&#24449;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;BEV&#24863;&#30693;&#20855;&#26377;&#20960;&#20010;&#20248;&#21183;&#65292;&#21363;&#20197;BEV&#34920;&#31034;&#21608;&#22260;&#22330;&#26223;&#30452;&#35266;&#19988;&#26131;&#20110;&#34701;&#21512;&#65307;&#20197;BEV&#34920;&#31034;&#29289;&#20307;&#23545;&#20110;&#21518;&#32493;&#30340;&#35268;&#21010;&#21644;/&#25110;&#25511;&#21046;&#27169;&#22359;&#26159;&#26368;&#29702;&#24819;&#30340;&#12290;BEV&#24863;&#30693;&#30340;&#26680;&#24515;&#38382;&#39064;&#22312;&#20110;&#65306;&#65288;a&#65289;&#22914;&#20309;&#36890;&#36807;&#20174;&#36879;&#35270;&#35270;&#22270;&#21040;BEV&#30340;&#35270;&#35282;&#36716;&#25442;&#37325;&#24314;&#20002;&#22833;&#30340;3D&#20449;&#24687;&#65307;&#65288;b&#65289;&#22914;&#20309;&#22312;BEV&#32593;&#26684;&#20013;&#33719;&#21462;&#22320;&#38754;&#30495;&#20540;&#27880;&#37322;&#65307;&#65288;c&#65289;&#22914;&#20309;&#26500;&#24314;&#21253;&#21547;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#21644;&#35270;&#22270;&#30340;&#29305;&#24449;&#30340;&#27969;&#31243;&#65307;&#20197;&#21450;&#65288;d&#65289;...
&lt;/p&gt;
&lt;p&gt;
Learning powerful representations in bird's-eye-view (BEV) for perception tasks is trending and drawing extensive attention both from industry and academia. Conventional approaches for most autonomous driving algorithms perform detection, segmentation, tracking, etc., in a front or perspective view. As sensor configurations get more complex, integrating multi-source information from different sensors and representing features in a unified view come of vital importance. BEV perception inherits several advantages, as representing surrounding scenes in BEV is intuitive and fusion-friendly; and representing objects in BEV is most desirable for subsequent modules as in planning and/or control. The core problems for BEV perception lie in (a) how to reconstruct the lost 3D information via view transformation from perspective view to BEV; (b) how to acquire ground truth annotations in BEV grid; (c) how to formulate the pipeline to incorporate features from different sources and views; and (d) 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#33041;&#30005;&#22270;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26234;&#33021;&#26694;&#26550;&#65292;&#22312;&#39550;&#39542;&#22330;&#26223;&#20013;&#26816;&#27979;&#39550;&#39542;&#21592;&#21980;&#30561;&#29366;&#24577;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#26159;&#24615;&#33021;&#26368;&#20339;&#30340;&#27169;&#22411;&#65292;&#30456;&#27604;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#26377;&#26356;&#39640;&#30340;f1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2209.04048</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#33041;&#30005;&#22270;&#30740;&#31350;&#39550;&#39542;&#20013;&#30340;&#21980;&#30561;&#26816;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Studying Drowsiness Detection Performance while Driving through Scalable Machine Learning Models using Electroencephalography. (arXiv:2209.04048v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#33041;&#30005;&#22270;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26234;&#33021;&#26694;&#26550;&#65292;&#22312;&#39550;&#39542;&#22330;&#26223;&#20013;&#26816;&#27979;&#39550;&#39542;&#21592;&#21980;&#30561;&#29366;&#24577;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#26159;&#24615;&#33021;&#26368;&#20339;&#30340;&#27169;&#22411;&#65292;&#30456;&#27604;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#26377;&#26356;&#39640;&#30340;f1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223; / &#24341;&#35328;&#65306;&#39550;&#39542;&#21592;&#21980;&#30561;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#20063;&#26159;&#20132;&#36890;&#20107;&#25925;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#12290;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#36827;&#23637;&#20351;&#24471;&#21487;&#20197;&#21033;&#29992;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#65288;BCIs&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26816;&#27979;&#39550;&#39542;&#21592;&#30340;&#21980;&#30561;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#32570;&#20047;&#23545;&#20351;&#29992;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#32508;&#21512;&#35780;&#20272;&#30340;&#21980;&#30561;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#19988;&#26377;&#24517;&#35201;&#30740;&#31350;&#36866;&#29992;&#20110;&#34987;&#35797;&#32676;&#20307;&#30340;&#21487;&#25193;&#23637;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;- &#26041;&#27861;&#65306;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#26694;&#26550;&#65292;&#21033;&#29992;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#21644;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#29305;&#24449;&#65292;&#29992;&#20110;&#26816;&#27979;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#21980;&#30561;&#29366;&#24577;&#12290;&#20351;&#29992;SEED-VIG&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#22312;&#20010;&#20307;&#21644;&#32676;&#20307;&#19978;&#30340;&#34920;&#29616;&#12290;- &#32467;&#26524;&#65306;&#32467;&#26524;&#26174;&#31034;&#65292;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#22312;&#20010;&#20307;&#39550;&#39542;&#21592;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#22312;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#65292;&#22914;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#65292;&#20855;&#26377;78&#65285;&#30340;f1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
- Background / Introduction: Driver drowsiness is a significant concern and one of the leading causes of traffic accidents. Advances in cognitive neuroscience and computer science have enabled the detection of drivers' drowsiness using Brain-Computer Interfaces (BCIs) and Machine Learning (ML). However, the literature lacks a comprehensive evaluation of drowsiness detection performance using a heterogeneous set of ML algorithms, and it is necessary to study the performance of scalable ML models suitable for groups of subjects. - Methods: To address these limitations, this work presents an intelligent framework employing BCIs and features based on electroencephalography for detecting drowsiness in driving scenarios. The SEED-VIG dataset is used to evaluate the best-performing models for individual subjects and groups. - Results: Results show that Random Forest (RF) outperformed other models used in the literature, such as Support Vector Machine (SVM), with a 78% f1-score for individual 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36882;&#24402;&#21487;&#34892;&#30340;&#27010;&#29575;&#23433;&#20840;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#21644;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65292;&#20351;&#24471;&#31995;&#32479;&#22312;&#22312;&#32447;&#35757;&#32451;&#21644;&#25191;&#34892;&#36807;&#31243;&#20013;&#20445;&#25345;&#23433;&#20840;&#12290;</title><link>http://arxiv.org/abs/2208.10733</link><description>&lt;p&gt;
&#36882;&#24402;&#21487;&#34892;&#30340;&#24102;&#26377;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#27010;&#29575;&#23433;&#20840;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Recursively Feasible Probabilistic Safe Online Learning with Control Barrier Functions. (arXiv:2208.10733v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36882;&#24402;&#21487;&#34892;&#30340;&#27010;&#29575;&#23433;&#20840;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#21644;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65292;&#20351;&#24471;&#31995;&#32479;&#22312;&#22312;&#32447;&#35757;&#32451;&#21644;&#25191;&#34892;&#36807;&#31243;&#20013;&#20445;&#25345;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#25511;&#21046;&#26041;&#26696;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#20102;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#30340;&#39640;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#37096;&#32626;&#36825;&#20123;&#26041;&#26696;&#65292;&#20445;&#35777;&#31995;&#32479;&#22312;&#22312;&#32447;&#35757;&#32451;&#21644;&#25191;&#34892;&#36807;&#31243;&#20013;&#20445;&#25345;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#24403;&#21069;&#26368;&#27969;&#34892;&#30340;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26041;&#27861;&#20013;&#65292;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;CBFs&#65289;&#20316;&#20026;&#25968;&#23398;&#24037;&#20855;&#65292;&#20026;&#20855;&#26377;&#24050;&#30693;&#21160;&#21147;&#23398;&#30340;&#31995;&#32479;&#25552;&#20379;&#27491;&#24335;&#30340;&#20445;&#25345;&#23433;&#20840;&#30340;&#25511;&#21046;&#21512;&#25104;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;CBF&#23433;&#20840;&#20851;&#38190;&#25511;&#21046;&#22120;&#30340;&#37325;&#26500;&#65292;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#22238;&#24402;&#26469;&#24314;&#31435;&#36817;&#20284;&#25968;&#23398;&#27169;&#22411;&#19982;&#30495;&#23454;&#31995;&#32479;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24471;&#21040;&#30340;&#40065;&#26834;&#23433;&#20840;&#20851;&#38190;&#25511;&#21046;&#22120;&#30340;&#21487;&#34892;&#24615;&#12290;&#35813;&#21487;&#34892;&#24615;&#20998;&#26512;&#32467;&#26524;&#23548;&#33268;&#20102;&#20851;&#20110;&#31995;&#32479;&#21487;&#29992;&#20449;&#24687;&#24212;&#28385;&#36275;&#30340;&#19968;&#31995;&#21015;&#20016;&#23500;&#26465;&#20214;&#65292;&#20197;&#30830;&#20445;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning-based control schemes have recently shown great efficacy performing complex tasks for a wide variety of applications. However, in order to deploy them in real systems, it is of vital importance to guarantee that the system will remain safe during online training and execution. Among the currently most popular methods to tackle this challenge, Control Barrier Functions (CBFs) serve as mathematical tools that provide a formal safety-preserving control synthesis procedure for systems with known dynamics. In this paper, we first introduce a model-uncertainty-aware reformulation of CBF-based safety-critical controllers using Gaussian Process (GP) regression to bridge the gap between an approximate mathematical model and the real system. Compared to previous approaches, we study the feasibility of the resulting robust safety-critical controller. This feasibility analysis results in a set of richness conditions that the available information about the system should satisfy to guarant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#23398;&#20064;&#20219;&#21153;&#23545;&#25239;&#25915;&#20987;&#30340;&#26131;&#24863;&#24615;&#65292;&#21457;&#29616;&#23398;&#20064;&#20219;&#21153;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#23548;&#33268;&#30340;&#30446;&#26631;&#31867;&#21035;&#38169;&#35823;&#20998;&#31867;&#12290;&#36825;&#23545;&#25968;&#25454;&#23436;&#25972;&#24615;&#21644;&#38544;&#31169;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2207.05225</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#23545;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26131;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
Susceptibility of Continual Learning Against Adversarial Attacks. (arXiv:2207.05225v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.05225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#23398;&#20064;&#20219;&#21153;&#23545;&#25239;&#25915;&#20987;&#30340;&#26131;&#24863;&#24615;&#65292;&#21457;&#29616;&#23398;&#20064;&#20219;&#21153;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#23548;&#33268;&#30340;&#30446;&#26631;&#31867;&#21035;&#38169;&#35823;&#20998;&#31867;&#12290;&#36825;&#23545;&#25968;&#25454;&#23436;&#25972;&#24615;&#21644;&#38544;&#31169;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#26377;&#20004;&#20010;&#20851;&#38190;&#39046;&#22495;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#65306;1&#65289;&#35780;&#20272;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;2&#65289;&#30830;&#20445;&#23398;&#20064;&#20219;&#21153;&#30340;&#23433;&#20840;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#23398;&#20064;&#20219;&#21153;&#65292;&#21253;&#25324;&#24403;&#21069;&#21644;&#20808;&#21069;&#33719;&#21462;&#30340;&#20219;&#21153;&#65292;&#23545;&#25239;&#25915;&#20987;&#30340;&#26131;&#24863;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20219;&#20309;&#23646;&#20110;&#20219;&#20309;&#20219;&#21153;&#30340;&#31867;&#21035;&#37117;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#25104;&#20026;&#20219;&#20309;&#20854;&#20182;&#20219;&#21153;&#25152;&#38656;&#30446;&#26631;&#31867;&#21035;&#30340;&#30446;&#26631;&#65292;&#24182;&#19988;&#34987;&#38169;&#35823;&#20998;&#31867;&#12290;&#36825;&#31181;&#23398;&#20064;&#20219;&#21153;&#23545;&#25239;&#25915;&#20987;&#30340;&#26131;&#24863;&#24615;&#24341;&#21457;&#20102;&#26377;&#20851;&#25968;&#25454;&#23436;&#25972;&#24615;&#21644;&#38544;&#31169;&#30340;&#28145;&#21051;&#20851;&#20999;&#12290;&#20026;&#20102;&#35780;&#20272;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#31181;&#22330;&#26223;&#19979;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#20219;&#21153;&#36882;&#22686;&#23398;&#20064;&#12289;&#39046;&#22495;&#36882;&#22686;&#23398;&#20064;&#21644;&#31867;&#36882;&#22686;&#23398;&#20064;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19977;&#31181;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent continual learning approaches have primarily focused on mitigating catastrophic forgetting. Nevertheless, two critical areas have remained relatively unexplored: 1) evaluating the robustness of proposed methods and 2) ensuring the security of learned tasks. This paper investigates the susceptibility of continually learned tasks, including current and previously acquired tasks, to adversarial attacks. Specifically, we have observed that any class belonging to any task can be easily targeted and misclassified as the desired target class of any other task. Such susceptibility or vulnerability of learned tasks to adversarial attacks raises profound concerns regarding data integrity and privacy. To assess the robustness of continual learning approaches, we consider continual learning approaches in all three scenarios, i.e., task-incremental learning, domain-incremental learning, and class-incremental learning. In this regard, we explore the robustness of three regularization-based me
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedSS&#30340;&#26234;&#33021;&#36873;&#25321;&#23458;&#25143;&#31471;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#23458;&#25143;&#31471;&#36873;&#25321;&#21644;&#35843;&#24230;&#25216;&#26415;&#65292;&#24179;&#34913;&#20102;&#24555;&#36895;&#25910;&#25947;&#21644;&#24322;&#36136;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2207.04569</link><description>&lt;p&gt;
FedSS: &#26234;&#33021;&#36873;&#25321;&#23458;&#25143;&#31471;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedSS: Federated Learning with Smart Selection of clients. (arXiv:2207.04569v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedSS&#30340;&#26234;&#33021;&#36873;&#25321;&#23458;&#25143;&#31471;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#23458;&#25143;&#31471;&#36873;&#25321;&#21644;&#35843;&#24230;&#25216;&#26415;&#65292;&#24179;&#34913;&#20102;&#24555;&#36895;&#25910;&#25947;&#21644;&#24322;&#36136;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#25552;&#20379;&#20102;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#23398;&#20064;&#24322;&#26500;&#29992;&#25143;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20854;&#24403;&#21069;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#25216;&#26415;&#23384;&#22312;&#20559;&#35265;&#65292;&#22240;&#20026;&#23427;&#27495;&#35270;&#24930;&#36895;&#23458;&#25143;&#31471;&#12290;&#39318;&#20808;&#65292;&#23427;&#36873;&#25321;&#28385;&#36275;&#26576;&#20123;&#32593;&#32476;&#21644;&#31995;&#32479;&#29305;&#23450;&#26465;&#20214;&#30340;&#23458;&#25143;&#31471;&#65292;&#22240;&#27492;&#27809;&#26377;&#36873;&#25321;&#24930;&#36895;&#23458;&#25143;&#31471;&#12290;&#21363;&#20351;&#36825;&#20123;&#23458;&#25143;&#31471;&#34987;&#21253;&#25324;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#23427;&#20204;&#35201;&#20040;&#38590;&#20197;&#35757;&#32451;&#65292;&#35201;&#20040;&#22240;&#20026;&#22826;&#24930;&#32780;&#34987;&#25918;&#24323;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#24819;&#27861;&#26159;&#36890;&#36807;&#26234;&#33021;&#23458;&#25143;&#31471;&#36873;&#25321;&#21644;&#35843;&#24230;&#25216;&#26415;&#25214;&#21040;&#24555;&#36895;&#25910;&#25947;&#21644;&#24322;&#36136;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning provides the ability to learn over heterogeneous user data in a distributed manner while preserving user privacy. However, its current client selection technique is a source of bias as it discriminates against slow clients. For starters, it selects clients that satisfy certain network and system-specific criteria, thus not selecting slow clients. Even when such clients are included in the training process, they either struggle with the training or are dropped altogether for being too slow. Our proposed idea looks to find a sweet spot between fast convergence and heterogeneity by looking at smart client selection and scheduling techniques.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31169;&#23494;&#29420;&#31435;&#24615;&#26816;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#31169;&#23494;&#20272;&#35745;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#36317;&#31163;&#30456;&#20851;&#24615;&#36827;&#34892;&#32479;&#35745;&#29420;&#31435;&#24615;&#27979;&#35797;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#24046;&#20998;&#38544;&#31169;&#27979;&#35797;&#30340;&#21152;&#27861;&#21644;&#20056;&#27861;&#35823;&#24046;&#30028;&#38480;&#65292;&#30456;&#20449;&#35813;&#31639;&#27861;&#22312;&#28041;&#21450;&#25935;&#24863;&#25968;&#25454;&#30340;&#20998;&#24067;&#24335;&#20551;&#35774;&#26816;&#39564;&#20013;&#20250;&#26377;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2207.03652</link><description>&lt;p&gt;
&#20004;&#26041;&#31169;&#23494;&#29420;&#31435;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Private independence testing across two parties. (arXiv:2207.03652v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03652
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31169;&#23494;&#29420;&#31435;&#24615;&#26816;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#31169;&#23494;&#20272;&#35745;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#36317;&#31163;&#30456;&#20851;&#24615;&#36827;&#34892;&#32479;&#35745;&#29420;&#31435;&#24615;&#27979;&#35797;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#24046;&#20998;&#38544;&#31169;&#27979;&#35797;&#30340;&#21152;&#27861;&#21644;&#20056;&#27861;&#35823;&#24046;&#30028;&#38480;&#65292;&#30456;&#20449;&#35813;&#31639;&#27861;&#22312;&#28041;&#21450;&#25935;&#24863;&#25968;&#25454;&#30340;&#20998;&#24067;&#24335;&#20551;&#35774;&#26816;&#39564;&#20013;&#20250;&#26377;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; $\pi$-test &#30340;&#20445;&#25252;&#38544;&#31169;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#20010;&#26041;&#20043;&#38388;&#27979;&#35797;&#32479;&#35745;&#29420;&#31435;&#24615;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20381;&#36182;&#20110;&#23545;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#36317;&#31163;&#30456;&#20851;&#24615;&#36827;&#34892;&#31169;&#23494;&#20272;&#35745;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312; Sz\'ekely &#31561;&#20154; [2007] &#20013;&#24341;&#20837;&#30340;&#29420;&#31435;&#24615;&#30340;&#23450;&#37327;&#24230;&#37327;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#24046;&#20998;&#38544;&#31169;&#27979;&#35797;&#30340;&#25928;&#29992;&#24314;&#31435;&#20102;&#21152;&#27861;&#21644;&#20056;&#27861;&#35823;&#24046;&#30028;&#38480;&#65292;&#25105;&#20204;&#30456;&#20449;&#23427;&#23558;&#22312;&#28041;&#21450;&#25935;&#24863;&#25968;&#25454;&#30340;&#21508;&#31181;&#20998;&#24067;&#24335;&#20551;&#35774;&#26816;&#39564;&#22330;&#26223;&#20013;&#25214;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce $\pi$-test, a privacy-preserving algorithm for testing statistical independence between data distributed across multiple parties. Our algorithm relies on privately estimating the distance correlation between datasets, a quantitative measure of independence introduced in Sz\'ekely et al. [2007]. We establish both additive and multiplicative error bounds on the utility of our differentially private test, which we believe will find applications in a variety of distributed hypothesis testing settings involving sensitive data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#32676;&#19981;&#21464;&#24352;&#37327;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#32676;&#30340;&#19981;&#21464;&#24352;&#37327;&#22522;&#24182;&#32467;&#21512;&#32676;&#19981;&#21464;&#24352;&#37327;&#20998;&#35299;&#32593;&#32476;&#65292;&#21487;&#20197;&#33719;&#24471;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#36890;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.15051</link><description>&lt;p&gt;
&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#32676;&#19981;&#21464;&#24352;&#37327;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Group-invariant tensor train networks for supervised learning. (arXiv:2206.15051v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.15051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#32676;&#19981;&#21464;&#24352;&#37327;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#32676;&#30340;&#19981;&#21464;&#24352;&#37327;&#22522;&#24182;&#32467;&#21512;&#32676;&#19981;&#21464;&#24352;&#37327;&#20998;&#35299;&#32593;&#32476;&#65292;&#21487;&#20197;&#33719;&#24471;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#36890;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#19981;&#21464;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#32622;&#12290;&#24352;&#37327;&#32593;&#32476;&#26159;&#19968;&#31181;&#39044;&#27979;&#24615;&#25110;&#29983;&#25104;&#24615;&#27169;&#22411;&#30340;&#31867;&#21035;&#20043;&#19968;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#20540;&#31639;&#27861;&#26469;&#26500;&#24314;&#32676;&#30340;&#27491;&#24120;&#30697;&#38453;&#34920;&#31034;&#19979;&#19981;&#21464;&#30340;&#24352;&#37327;&#22522;&#12290;&#36825;&#31181;&#26041;&#27861;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#24555;&#19978;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#28982;&#21518;&#23558;&#32676;&#19981;&#21464;&#24352;&#37327;&#32452;&#21512;&#25104;&#32676;&#19981;&#21464;&#24352;&#37327;&#20998;&#35299;&#32593;&#32476;&#65292;&#21487;&#20197;&#29992;&#20316;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#27492;&#27169;&#22411;&#24212;&#29992;&#20110;&#34507;&#30333;&#36136;&#32467;&#21512;&#20998;&#31867;&#38382;&#39064;&#65292;&#32771;&#34385;&#20102;&#38382;&#39064;&#29305;&#23450;&#30340;&#19981;&#21464;&#24615;&#65292;&#24182;&#33719;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#31526;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invariance has recently proven to be a powerful inductive bias in machine learning models. One such class of predictive or generative models are tensor networks. We introduce a new numerical algorithm to construct a basis of tensors that are invariant under the action of normal matrix representations of an arbitrary discrete group. This method can be up to several orders of magnitude faster than previous approaches. The group-invariant tensors are then combined into a group-invariant tensor train network, which can be used as a supervised machine learning model. We applied this model to a protein binding classification problem, taking into account problem-specific invariances, and obtained prediction accuracy in line with state-of-the-art deep learning approaches.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22810;&#30446;&#26631;&#29615;&#22659;&#20013;&#20174;&#25945;&#23398;&#31034;&#33539;&#20013;&#23454;&#29992;&#22320;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;&#23454;&#29992;&#20027;&#20041;&#26426;&#21046;&#21644;&#25945;&#23398;&#26426;&#21046;&#65292;&#21033;&#29992;&#36125;&#21494;&#26031;&#27169;&#22411;&#36827;&#34892;&#30446;&#26631;&#25512;&#26029;&#65292;&#21487;&#20197;&#21152;&#24555;&#23398;&#20064;&#36895;&#24230;&#24182;&#20943;&#23569;&#30446;&#26631;&#30340;&#27495;&#20041;&#12290;</title><link>http://arxiv.org/abs/2206.04546</link><description>&lt;p&gt;
&#22312;&#22810;&#30446;&#26631;&#29615;&#22659;&#19979;&#20174;&#25945;&#23398;&#31034;&#33539;&#20013;&#23454;&#29992;&#22320;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pragmatically Learning from Pedagogical Demonstrations in Multi-Goal Environments. (arXiv:2206.04546v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04546
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22810;&#30446;&#26631;&#29615;&#22659;&#20013;&#20174;&#25945;&#23398;&#31034;&#33539;&#20013;&#23454;&#29992;&#22320;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;&#23454;&#29992;&#20027;&#20041;&#26426;&#21046;&#21644;&#25945;&#23398;&#26426;&#21046;&#65292;&#21033;&#29992;&#36125;&#21494;&#26031;&#27169;&#22411;&#36827;&#34892;&#30446;&#26631;&#25512;&#26029;&#65292;&#21487;&#20197;&#21152;&#24555;&#23398;&#20064;&#36895;&#24230;&#24182;&#20943;&#23569;&#30446;&#26631;&#30340;&#27495;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31034;&#33539;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#25509;&#36817;&#26368;&#20339;&#31034;&#33539;&#26469;&#21152;&#24555;&#35757;&#32451;&#36895;&#24230;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#25945;&#24072;&#22312;&#31034;&#33539;&#20219;&#21153;&#26102;&#20250;&#20559;&#31163;&#26368;&#20339;&#31034;&#33539;&#65292;&#24182;&#36890;&#36807;&#32473;&#20986;&#26368;&#33021;&#28040;&#38500;&#30446;&#26631;&#27495;&#20041;&#30340;&#31034;&#33539;&#26469;&#25945;&#23398;&#12290;&#31867;&#20284;&#22320;&#65292;&#20154;&#31867;&#23398;&#20064;&#32773;&#25797;&#38271;&#23454;&#29992;&#22320;&#25512;&#26029;&#25945;&#24072;&#30340;&#24847;&#22270;&#65292;&#20419;&#36827;&#20004;&#20010;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#27969;&#12290;&#36825;&#20123;&#26426;&#21046;&#22312;&#23569;&#31034;&#33539;&#24773;&#26223;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25512;&#26029;&#30446;&#26631;&#26356;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#31034;&#33539;&#30340;&#30446;&#26631;&#25512;&#26029;&#30340;&#36125;&#21494;&#26031;&#27169;&#22411;&#65288;BGI&#65289;&#26469;&#23454;&#29616;&#25945;&#23398;&#21644;&#23454;&#29992;&#20027;&#20041;&#26426;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#20855;&#26377;&#20004;&#20010;&#22522;&#20110;&#30446;&#26631;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#20154;&#24037;&#20195;&#29702;&#30340;&#22810;&#30446;&#26631;&#25945;&#24072;-&#23398;&#20064;&#32773;&#35774;&#32622;&#20013;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23558;BGI&#20195;&#29702;&#65288;&#25945;&#23548;&#32773;&#21644;&#23454;&#29992;&#23398;&#20064;&#32773;&#65289;&#30456;&#32467;&#21512;&#21487;&#20197;&#21152;&#24555;&#23398;&#20064;&#36895;&#24230;&#24182;&#20943;&#23569;&#30446;&#26631;&#30340;&#27495;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from demonstration methods usually leverage close to optimal demonstrations to accelerate training. By contrast, when demonstrating a task, human teachers deviate from optimal demonstrations and pedagogically modify their behavior by giving demonstrations that best disambiguate the goal they want to demonstrate. Analogously, human learners excel at pragmatically inferring the intent of the teacher, facilitating communication between the two agents. These mechanisms are critical in the few demonstrations regime, where inferring the goal is more difficult. In this paper, we implement pedagogy and pragmatism mechanisms by leveraging a Bayesian model of Goal Inference from demonstrations (BGI). We highlight the benefits of this model in multi-goal teacher-learner setups with two artificial agents that learn with goal-conditioned Reinforcement Learning. We show that combining BGI-agents (a pedagogical teacher and a pragmatic learner) results in faster learning and reduced goal ambi
&lt;/p&gt;</description></item><item><title>DIRA&#26159;&#19968;&#20010;&#29992;&#20110;DNN&#20998;&#31867;&#22120;&#30340;&#21160;&#24577;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#23454;&#29616;&#37325;&#26032;&#35757;&#32451;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.00147</link><description>&lt;p&gt;
DIRA: &#19968;&#31181;&#29992;&#20110;&#21160;&#24577;&#39046;&#22495;&#22686;&#37327;&#27491;&#21017;&#21270;&#33258;&#36866;&#24212;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DIRA: A Framework for Dynamic Domain Incremental Regularised Adaptation. (arXiv:2205.00147v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.00147
&lt;/p&gt;
&lt;p&gt;
DIRA&#26159;&#19968;&#20010;&#29992;&#20110;DNN&#20998;&#31867;&#22120;&#30340;&#21160;&#24577;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#23454;&#29616;&#37325;&#26032;&#35757;&#32451;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#31995;&#32479;&#65288;AS&#65289;&#32463;&#24120;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20998;&#31867;&#22120;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#22312;&#22797;&#26434;&#12289;&#39640;&#32500;&#12289;&#38750;&#32447;&#24615;&#21644;&#21160;&#24577;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#36816;&#34892;&#12290;&#30001;&#20110;&#36825;&#20123;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#65292;&#24403;DNN&#20998;&#31867;&#22120;&#38754;&#23545;&#24320;&#21457;&#36807;&#31243;&#20013;&#26410;&#35782;&#21035;&#30340;&#39046;&#22495;&#26102;&#65292;&#21487;&#33021;&#20250;&#22312;&#25805;&#20316;&#36807;&#31243;&#20013;&#36755;&#20986;&#38169;&#35823;&#20998;&#31867;&#12290;&#38543;&#30528;AS&#30340;&#25968;&#37327;&#22686;&#21152;&#65292;&#23558;&#31995;&#32479;&#20174;&#36816;&#34892;&#20013;&#31227;&#38500;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#22686;&#21152;AS&#30340;&#21487;&#38752;&#24615;&#24182;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;DNN&#20998;&#31867;&#22120;&#38656;&#35201;&#22312;&#25805;&#20316;&#36807;&#31243;&#20013;&#36866;&#24212;&#19981;&#21516;&#30340;&#25805;&#20316;&#39046;&#22495;&#65292;&#24182;&#33021;&#22815;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#65288;&#20363;&#22914;100&#20010;&#26679;&#26412;&#65289;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#24050;&#30693;&#22312;&#23569;&#37327;&#26679;&#26412;&#19978;&#37325;&#26032;&#35757;&#32451;DNN&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#22686;&#37327;&#27491;&#21017;&#21270;&#33258;&#36866;&#24212;&#65288;DIRA&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#23454;&#29616;DNN&#20998;&#31867;&#22120;&#30340;&#25805;&#20316;&#39046;&#22495;&#36866;&#24212;&#65292;&#20174;&#32780;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#24182;&#23454;&#29616;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous systems (AS) often use Deep Neural Network (DNN) classifiers to allow them to operate in complex, high dimensional, non-linear, and dynamically changing environments. Due to the complexity of these environments, DNN classifiers may output misclassifications during operation when they face domains not identified during development. Removing a system from operation for retraining becomes impractical as the number of such AS increase. To increase AS reliability and overcome this limitation, DNN classifiers need to have the ability to adapt during operation when faced with different operational domains using a few samples (e.g. 100 samples). However, retraining DNNs on a few samples is known to cause catastrophic forgetting. In this paper, we introduce Dynamic Incremental Regularised Adaptation (DIRA), a framework for operational domain adaption of DNN classifiers using regularisation techniques to overcome catastrophic forgetting and achieve adaptation when retraining using few
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#23548;&#24072;-&#23398;&#20064;&#32773;&#20114;&#21160;&#20013;&#30340;&#25945;&#23398;&#28436;&#31034;&#21644;&#23454;&#29992;&#23398;&#20064;&#26426;&#21046;&#65292;&#24182;&#22312;&#19968;&#20010;&#22810;&#30446;&#26631;&#29615;&#22659;&#20013;&#23454;&#29616;&#12290;&#36890;&#36807;&#20351;&#29992;&#23548;&#24072;&#30340;&#25945;&#23398;&#21644;&#23398;&#20064;&#32773;&#30340;&#23454;&#29992;&#25512;&#29702;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2203.00111</link><description>&lt;p&gt;
&#20154;&#24037;&#23548;&#24072;-&#23398;&#20064;&#32773;&#20114;&#21160;&#20013;&#30340;&#25945;&#23398;&#28436;&#31034;&#21644;&#23454;&#29992;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pedagogical Demonstrations and Pragmatic Learning in Artificial Tutor-Learner Interactions. (arXiv:2203.00111v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#23548;&#24072;-&#23398;&#20064;&#32773;&#20114;&#21160;&#20013;&#30340;&#25945;&#23398;&#28436;&#31034;&#21644;&#23454;&#29992;&#23398;&#20064;&#26426;&#21046;&#65292;&#24182;&#22312;&#19968;&#20010;&#22810;&#30446;&#26631;&#29615;&#22659;&#20013;&#23454;&#29616;&#12290;&#36890;&#36807;&#20351;&#29992;&#23548;&#24072;&#30340;&#25945;&#23398;&#21644;&#23398;&#20064;&#32773;&#30340;&#23454;&#29992;&#25512;&#29702;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23637;&#31034;&#20219;&#21153;&#26102;&#65292;&#20154;&#31867;&#23548;&#24072;&#36890;&#36807;&#8220;&#23637;&#31034;&#8221;&#20219;&#21153;&#32780;&#19981;&#20165;&#20165;&#26159;&#8220;&#23436;&#25104;&#8221;&#20219;&#21153;&#65288;&#22840;&#22823;&#28436;&#31034;&#30340;&#30456;&#20851;&#37096;&#20998;&#65289;&#25110;&#32773;&#32473;&#20986;&#33021;&#26368;&#28165;&#26224;&#34920;&#36798;&#30446;&#26631;&#30340;&#28436;&#31034;&#26469;&#25945;&#23398;&#12290;&#31867;&#20284;&#22320;&#65292;&#20154;&#31867;&#23398;&#20064;&#32773;&#20250;&#25512;&#26029;&#23548;&#24072;&#30340;&#20132;&#38469;&#24847;&#22270;&#65306;&#20182;&#20204;&#35299;&#37322;&#23548;&#24072;&#35797;&#22270;&#25945;&#20182;&#20204;&#20160;&#20040;&#65292;&#24182;&#25512;&#26029;&#20986;&#23398;&#20064;&#25152;&#38656;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#22914;&#26524;&#27809;&#26377;&#36825;&#26679;&#30340;&#26426;&#21046;&#65292;&#20256;&#32479;&#30340;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#65288;Learning from Demonstration&#65292;LfD&#65289;&#31639;&#27861;&#23558;&#35748;&#20026;&#36825;&#26679;&#30340;&#28436;&#31034;&#26159;&#27425;&#20248;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#22312;&#19968;&#20010;&#22810;&#30446;&#26631;&#29615;&#22659;&#20013;&#65292;&#23548;&#24072;&#21644;&#23398;&#20064;&#32773;&#37117;&#26159;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20154;&#30340;&#23398;&#20064;&#32773;-&#23548;&#24072;&#35774;&#32622;&#20013;&#23454;&#29616;&#36825;&#26679;&#30340;&#26426;&#21046;&#12290;&#36890;&#36807;&#23548;&#24072;&#30340;&#25945;&#23398;&#21644;&#23398;&#20064;&#32773;&#30340;&#23454;&#29992;&#25512;&#29702;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#27604;&#26631;&#20934;&#30340;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#26041;&#27861;&#30340;&#22823;&#24133;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
When demonstrating a task, human tutors pedagogically modify their behavior by either "showing" the task rather than just "doing" it (exaggerating on relevant parts of the demonstration) or by giving demonstrations that best disambiguate the communicated goal. Analogously, human learners pragmatically infer the communicative intent of the tutor: they interpret what the tutor is trying to teach them and deduce relevant information for learning. Without such mechanisms, traditional Learning from Demonstration (LfD) algorithms will consider such demonstrations as sub-optimal. In this paper, we investigate the implementation of such mechanisms in a tutor-learner setup where both participants are artificial agents in an environment with multiple goals. Using pedagogy from the tutor and pragmatism from the learner, we show substantial improvements over standard learning from demonstrations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#21487;&#20197;&#26500;&#24314;&#39640;&#25928;&#30340;&#30452;&#36830;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#65292;&#20197;&#20248;&#21270;&#33410;&#28857;&#24310;&#36831;&#21644;&#24102;&#23485;&#26435;&#34913;&#65292;&#36866;&#29992;&#20110;&#38598;&#20307;&#36890;&#20449;&#36127;&#36733;&#12290;</title><link>http://arxiv.org/abs/2202.03356</link><description>&lt;p&gt;
&#39640;&#25928;&#30452;&#36830;&#25299;&#25169;&#32467;&#26500;&#29992;&#20110;&#38598;&#20307;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Efficient Direct-Connect Topologies for Collective Communications. (arXiv:2202.03356v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#21487;&#20197;&#26500;&#24314;&#39640;&#25928;&#30340;&#30452;&#36830;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#65292;&#20197;&#20248;&#21270;&#33410;&#28857;&#24310;&#36831;&#21644;&#24102;&#23485;&#26435;&#34913;&#65292;&#36866;&#29992;&#20110;&#38598;&#20307;&#36890;&#20449;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#26500;&#24314;&#36866;&#29992;&#20110;&#38598;&#20307;&#36890;&#20449;&#30340;&#39640;&#25928;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#38024;&#23545;&#33410;&#28857;&#24310;&#36831;&#19982;&#24102;&#23485;&#26435;&#34913;&#20248;&#21270;&#30340;&#30452;&#36830;&#25299;&#25169;&#32467;&#26500;&#12290;&#36825;&#20010;&#31639;&#27861;&#26694;&#26550;&#20174;&#23567;&#30340;&#22522;&#30784;&#25299;&#25169;&#32467;&#26500;&#21644;&#30456;&#20851;&#30340;&#36890;&#20449;&#36827;&#24230;&#24320;&#22987;&#65292;&#24182;&#20351;&#29992;&#19968;&#32452;&#21487;&#20197;&#36845;&#20195;&#24212;&#29992;&#30340;&#25216;&#26415;&#26469;&#27966;&#29983;&#26356;&#22823;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#36825;&#20123;&#34893;&#29983;&#30340;&#25299;&#25169;&#32467;&#26500;&#30340;&#26102;&#38388;&#34920;&#21487;&#20197;&#19982;&#25193;&#23637;&#19968;&#36215;&#21512;&#25104;&#65292;&#20063;&#21487;&#20197;&#20351;&#29992;&#20248;&#21270;&#20844;&#24335;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#20026;&#32473;&#23450;&#30340;&#38598;&#32676;&#22823;&#23567;&#21644;&#24230;&#25968;&#21512;&#25104;&#35768;&#22810;&#19981;&#21516;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#26102;&#38388;&#34920;&#65292;&#28982;&#21518;&#20026;&#32473;&#23450;&#30340;&#24037;&#20316;&#36127;&#36733;&#30830;&#23450;&#36866;&#24403;&#30340;&#25299;&#25169;&#21644;&#26102;&#38388;&#34920;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992;&#34917;&#19969;&#38754;&#26495;&#37197;&#32622;&#25152;&#38656;&#25299;&#25169;&#32467;&#26500;&#30340;12&#33410;&#28857;&#20809;&#23398;&#23454;&#39564;&#24179;&#21488;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#22686;&#21152;&#20102;&#22522;&#20110;&#20998;&#26512;&#27169;&#22411;&#30340;&#35780;&#20272;&#65292;&#29992;&#20110;&#26356;&#22823;&#30340;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of distilling efficient network topologies for collective communications. We provide an algorithmic framework for constructing direct-connect topologies optimized for the node latency vs bandwidth trade-off given a collective communication workload. Our algorithmic framework allows us to start from small base topologies and associated communication schedules and use a set of techniques that can be iteratively applied to derive much larger topologies. The schedules for these derived topologies are either synthesized along with the expansions or computed using an optimization formulation. Our approach allows us to synthesize many different topologies and schedules for a given cluster size and degree, and then identify the appropriate topology and schedule for a given workload. We evaluate our approach on a 12-node optical testbed that uses patch panels for configuring the desired topology and augment it with an analytical-model-based evaluation for larger deployme
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;STAX&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#23398;&#20064;&#34892;&#20026;&#31354;&#38388;&#26102;&#23454;&#26102;&#25506;&#32034;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#20248;&#21270;&#20219;&#20309;&#21457;&#29616;&#30340;&#22870;&#21169;&#12290;</title><link>http://arxiv.org/abs/2111.01919</link><description>&lt;p&gt;
&#22312;&#23398;&#20064;&#34892;&#20026;&#31354;&#38388;&#20013;&#21457;&#29616;&#21644;&#21033;&#29992;&#31232;&#30095;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Discovering and Exploiting Sparse Rewards in a Learned Behavior Space. (arXiv:2111.01919v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.01919
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;STAX&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#23398;&#20064;&#34892;&#20026;&#31354;&#38388;&#26102;&#23454;&#26102;&#25506;&#32034;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#20248;&#21270;&#20219;&#20309;&#21457;&#29616;&#30340;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31232;&#30095;&#22870;&#21169;&#30340;&#29615;&#22659;&#20013;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#23398;&#20064;&#20195;&#29702;&#20960;&#20046;&#27809;&#26377;&#20851;&#20110;&#34892;&#20026;&#36136;&#37327;&#30340;&#21453;&#39304;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#22909;&#30340;&#31574;&#30053;&#26159;&#19987;&#27880;&#20110;&#25506;&#32034;&#65292;&#24076;&#26395;&#33021;&#21457;&#29616;&#19968;&#20010;&#22870;&#21169;&#20449;&#21495;&#20197;&#36827;&#34892;&#25913;&#36827;&#12290;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#36825;&#31181;&#24773;&#20917;&#30340;&#23398;&#20064;&#31639;&#27861;&#24517;&#39035;&#33021;&#22815;&#65288;1&#65289;&#25506;&#32034;&#21487;&#33021;&#30340;&#20195;&#29702;&#34892;&#20026;&#21644;&#65288;2&#65289;&#21033;&#29992;&#21487;&#33021;&#21457;&#29616;&#30340;&#20219;&#20309;&#22870;&#21169;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#25506;&#32034;&#31639;&#27861;&#65292;&#38656;&#35201;&#23450;&#20041;&#19968;&#20010;&#34892;&#20026;&#31354;&#38388;&#65292;&#23558;&#20195;&#29702;&#19982;&#20854;&#22312;&#21487;&#20197;&#25506;&#32034;&#30340;&#31354;&#38388;&#20013;&#34920;&#29616;&#20986;&#30340;&#34892;&#20026;&#30456;&#20851;&#32852;&#12290;&#38656;&#35201;&#23450;&#20041;&#36825;&#20010;&#31354;&#38388;&#26159;&#36825;&#20123;&#31639;&#27861;&#30340;&#19968;&#20010;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;STAX&#65292;&#19968;&#31181;&#35774;&#35745;&#29992;&#20110;&#23454;&#26102;&#23398;&#20064;&#34892;&#20026;&#31354;&#38388;&#24182;&#22312;&#26377;&#25928;&#22320;&#20248;&#21270;&#20219;&#20309;&#21457;&#29616;&#30340;&#22870;&#21169;&#30340;&#31639;&#27861;&#12290;&#23427;&#36890;&#36807;&#23558;&#34892;&#20026;&#31354;&#38388;&#30340;&#25506;&#32034;&#21644;&#23398;&#20064;&#19982;&#22870;&#21169;&#30340;&#21033;&#29992;&#20998;&#24320;&#65292;&#20197;&#26367;&#20195;&#30340;&#26041;&#24335;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning optimal policies in sparse rewards settings is difficult as the learning agent has little to no feedback on the quality of its actions. In these situations, a good strategy is to focus on exploration, hopefully leading to the discovery of a reward signal to improve on. A learning algorithm capable of dealing with this kind of settings has to be able to (1) explore possible agent behaviors and (2) exploit any possible discovered reward. Efficient exploration algorithms have been proposed that require to define a behavior space, that associates to an agent its resulting behavior in a space that is known to be worth exploring. The need to define this space is a limitation of these algorithms. In this work, we introduce STAX, an algorithm designed to learn a behavior space on-the-fly and to explore it while efficiently optimizing any reward discovered. It does so by separating the exploration and learning of the behavior space from the exploitation of the reward through an alterna
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#32479;&#35745;&#36807;&#31243;&#26469;&#35299;&#20915;&#29983;&#20135;&#36710;&#38388;&#23460;&#20869;&#23450;&#20301;&#31995;&#32479;&#20013;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#32467;&#21512;&#21487;&#35270;&#20998;&#26512;&#21644;&#36816;&#21160;&#26816;&#27979;&#65292;&#19981;&#20165;&#21306;&#20998;&#20572;&#30041;&#19982;&#31227;&#21160;&#65292;&#36824;&#32771;&#34385;&#20102;&#19981;&#26399;&#26395;&#30340;&#21796;&#37266;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#35299;&#37322;&#26041;&#26696;&#65292;&#24182;&#22312;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2109.10757</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#29983;&#20135;&#36710;&#38388;&#23460;&#20869;&#23450;&#20301;&#31995;&#32479;&#20013;&#30340;&#36816;&#21160;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Movement Detection in Indoor Positioning Systems of Production Halls. (arXiv:2109.10757v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.10757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#32479;&#35745;&#36807;&#31243;&#26469;&#35299;&#20915;&#29983;&#20135;&#36710;&#38388;&#23460;&#20869;&#23450;&#20301;&#31995;&#32479;&#20013;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#32467;&#21512;&#21487;&#35270;&#20998;&#26512;&#21644;&#36816;&#21160;&#26816;&#27979;&#65292;&#19981;&#20165;&#21306;&#20998;&#20572;&#30041;&#19982;&#31227;&#21160;&#65292;&#36824;&#32771;&#34385;&#20102;&#19981;&#26399;&#26395;&#30340;&#21796;&#37266;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#35299;&#37322;&#26041;&#26696;&#65292;&#24182;&#22312;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#29983;&#20135;&#36710;&#38388;&#20013;&#30340;&#23460;&#20869;&#23450;&#20301;&#31995;&#32479;&#65292;&#20854;&#20013;&#35013;&#26377;&#20256;&#24863;&#22120;&#30340;&#29289;&#20307;&#21457;&#36865;&#20854;&#24403;&#21069;&#20301;&#32622;&#12290;&#37492;&#20110;&#25968;&#25454;&#23481;&#37327;&#36739;&#22823;&#65292;&#23545;&#20135;&#29983;&#30340;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#26159;&#21463;&#21040;&#22122;&#22768;&#24178;&#25200;&#30340;&#24433;&#21709;&#12290;&#38382;&#39064;&#21253;&#25324;&#31934;&#24230;&#38382;&#39064;&#21644;&#20256;&#24863;&#22120;&#30001;&#20110;&#29289;&#27969;&#36807;&#31243;&#30340;&#21160;&#24577;&#24615;&#65288;&#20363;&#22914;&#32463;&#36807;&#30340;&#21449;&#36710;&#30340;&#25391;&#21160;&#65289;&#32780;&#20135;&#29983;&#30340;&#19981;&#26399;&#26395;&#30340;&#21796;&#37266;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#37327;&#36523;&#23450;&#21046;&#30340;&#32479;&#35745;&#36807;&#31243;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#23558;&#21487;&#35270;&#20998;&#26512;&#19982;&#36816;&#21160;&#26816;&#27979;&#30456;&#32467;&#21512;&#12290;&#19982;&#26222;&#36890;&#30340;&#20572;&#30041;&#28857;&#31639;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#19981;&#20165;&#21306;&#20998;&#20572;&#30041;&#19982;&#31227;&#21160;&#65292;&#36824;&#32771;&#34385;&#20102;&#19981;&#26399;&#26395;&#30340;&#21796;&#37266;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#26356;&#35814;&#32454;&#30340;&#35299;&#37322;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#22312;&#32447;&#65288;&#20363;&#22914;&#35746;&#21333;&#30417;&#25511;&#65289;&#21644;&#31163;&#32447;&#24212;&#29992;&#65288;&#20363;&#22914;&#38382;&#39064;&#21306;&#22495;&#30340;&#26816;&#27979;&#65289;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#38500;&#21407;&#22987;&#23460;&#20869;&#23450;&#20301;&#31995;&#32479;&#36755;&#20986;&#20043;&#22806;&#30340;&#20854;&#20182;&#20449;&#24687;&#65292;&#24182;&#33021;&#36827;&#34892;&#20020;&#26102;&#20998;&#26512;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21253;&#21547;&#30495;&#23454;&#22330;&#26223;&#30340;&#24191;&#27867;&#26696;&#20363;&#30740;&#31350;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider indoor positioning systems (IPS) in production halls where objects equipped with sensors send their current position. Beside its large volume, the analyzation of the resulting raw data is challenging due to the susceptibility towards noise. Reasons are accuracy issues and undesired awakenings of sensors that occur due to the dynamics of logistic processes (e.g.~vibrations of passing forklifts). We propose a tailor-made statistical procedure for these challenges and combine visual analytics with movement detection. Contrary to common stay-point algorithms, we do not only distinguish between stops and moves, but also consider undesired awakenings. This leads to a more detailed interpretation scheme offering usages for online (e.g.~monitoring of orders) and offline applications (e.g.~detection of problematic areas). The approach does not require other information than the raw IPS output and enables an ad-hoc analysis. We underline our findings in an extensive case study with real
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;Frank-Wolfe&#31639;&#27861;&#21464;&#20307;&#65292;&#21033;&#29992;&#24191;&#20041;&#33258;&#21327;&#35843;&#20989;&#25968;&#30340;&#29305;&#24615;&#65292;&#22312;&#19981;&#38656;&#35201;&#20351;&#29992;&#20108;&#38454;&#20449;&#24687;&#25110;&#20272;&#35745;&#23616;&#37096;&#24179;&#28369;&#24230;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;$\mathcal{O}(1/t)$&#30340;&#25910;&#25947;&#36895;&#24230;&#36798;&#21040;&#20102;&#20248;&#21270;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2105.13913</link><description>&lt;p&gt;
&#21482;&#38656;&#31616;&#21333;&#27493;&#39588;&#65306;Frank-Wolfe&#31639;&#27861;&#21644;&#24191;&#20041;&#33258;&#21327;&#35843;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Simple steps are all you need: Frank-Wolfe and generalized self-concordant functions. (arXiv:2105.13913v6 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.13913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;Frank-Wolfe&#31639;&#27861;&#21464;&#20307;&#65292;&#21033;&#29992;&#24191;&#20041;&#33258;&#21327;&#35843;&#20989;&#25968;&#30340;&#29305;&#24615;&#65292;&#22312;&#19981;&#38656;&#35201;&#20351;&#29992;&#20108;&#38454;&#20449;&#24687;&#25110;&#20272;&#35745;&#23616;&#37096;&#24179;&#28369;&#24230;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;$\mathcal{O}(1/t)$&#30340;&#25910;&#25947;&#36895;&#24230;&#36798;&#21040;&#20102;&#20248;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#33258;&#21327;&#35843;&#26159;&#35768;&#22810;&#37325;&#35201;&#23398;&#20064;&#38382;&#39064;&#30340;&#30446;&#26631;&#20989;&#25968;&#20013;&#23384;&#22312;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24615;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;Frank-Wolfe&#21464;&#20307;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#35813;&#21464;&#20307;&#20351;&#29992;&#20102;&#24320;&#29615;&#27493;&#38271;&#31574;&#30053;$\gamma_t=2/(t+2)$&#65292;&#23545;&#20110;&#36825;&#31867;&#20989;&#25968;&#22312;&#21407;&#22987;&#38388;&#38553;&#21644;Frank-Wolfe&#38388;&#38553;&#26041;&#38754;&#33719;&#24471;&#20102;$\mathcal{O}(1/t)$&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20854;&#20013;$t$&#26159;&#36845;&#20195;&#27425;&#25968;&#12290;&#36825;&#36991;&#20813;&#20102;&#20351;&#29992;&#20108;&#38454;&#20449;&#24687;&#25110;&#38656;&#35201;&#20272;&#35745;&#20808;&#21069;&#24037;&#20316;&#30340;&#23616;&#37096;&#24179;&#28369;&#24230;&#21442;&#25968;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19981;&#21516;&#24120;&#35265;&#24773;&#20917;&#19979;&#30340;&#25913;&#36827;&#25910;&#25947;&#36895;&#24230;&#65292;&#20363;&#22914;&#65292;&#24403;&#25152;&#32771;&#34385;&#30340;&#21487;&#34892;&#22495;&#26159;&#22343;&#21248;&#20984;&#30340;&#25110;&#32773;&#26159;&#22810;&#38754;&#20307;&#30340;&#26102;&#20505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalized self-concordance is a key property present in the objective function of many important learning problems. We establish the convergence rate of a simple Frank-Wolfe variant that uses the open-loop step size strategy $\gamma_t = 2/(t+2)$, obtaining a $\mathcal{O}(1/t)$ convergence rate for this class of functions in terms of primal gap and Frank-Wolfe gap, where $t$ is the iteration count. This avoids the use of second-order information or the need to estimate local smoothness parameters of previous work. We also show improved convergence rates for various common cases, e.g., when the feasible region under consideration is uniformly convex or polyhedral.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MimicNorm&#30340;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21270;&#25209;&#24402;&#19968;&#21270;&#65288;BN&#65289;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#24182;&#20445;&#25345;&#20854;&#26680;&#24515;&#24433;&#21709;&#65292;&#21363;&#25968;&#25454;&#21435;&#30456;&#20851;&#24615;&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#26469;&#25552;&#39640;&#32593;&#32476;&#35757;&#32451;&#30340;&#25910;&#25947;&#24615;&#21644;&#25928;&#29575;&#12290;MimicNorm&#20165;&#21253;&#21547;&#20004;&#20010;&#36731;&#37327;&#32423;&#25805;&#20316;&#65292;&#21487;&#19982;BN&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2010.09278</link><description>&lt;p&gt;
MimicNorm: &#26435;&#37325;&#22343;&#20540;&#21644;&#26368;&#21518;&#19968;&#23618;&#25209;&#24402;&#19968;&#21270;&#23618;&#27169;&#20223;&#25209;&#24402;&#19968;&#21270;&#30340;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
MimicNorm: Weight Mean and Last BN Layer Mimic the Dynamic of Batch Normalization. (arXiv:2010.09278v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.09278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MimicNorm&#30340;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21270;&#25209;&#24402;&#19968;&#21270;&#65288;BN&#65289;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#24182;&#20445;&#25345;&#20854;&#26680;&#24515;&#24433;&#21709;&#65292;&#21363;&#25968;&#25454;&#21435;&#30456;&#20851;&#24615;&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#26469;&#25552;&#39640;&#32593;&#32476;&#35757;&#32451;&#30340;&#25910;&#25947;&#24615;&#21644;&#25928;&#29575;&#12290;MimicNorm&#20165;&#21253;&#21547;&#20004;&#20010;&#36731;&#37327;&#32423;&#25805;&#20316;&#65292;&#21487;&#19982;BN&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25209;&#24402;&#19968;&#21270;&#65288;BN&#65289;&#23618;&#22312;&#25910;&#25947;&#21644;&#27867;&#21270;&#25928;&#26524;&#19978;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;BN&#38656;&#35201;&#39069;&#22806;&#30340;&#20869;&#23384;&#21644;&#28014;&#28857;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#22312;&#24494;&#23567;&#25209;&#27425;&#19978;&#65292;BN&#20250;&#21464;&#24471;&#19981;&#20934;&#30830;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#20110;&#25209;&#27425;&#32479;&#35745;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#31616;&#21270;BN&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;BN&#23618;&#30340;&#20004;&#20010;&#22522;&#26412;&#24433;&#21709;&#65292;&#21363;&#25968;&#25454;&#21435;&#30456;&#20851;&#24615;&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;MimicNorm&#65292;&#26469;&#25913;&#21892;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#25910;&#25947;&#24615;&#21644;&#25928;&#29575;&#12290; MimicNorm&#20165;&#21253;&#21547;&#20004;&#20010;&#36731;&#37327;&#32423;&#25805;&#20316;&#65292;&#21253;&#25324;&#20462;&#25913;&#30340;&#26435;&#37325;&#22343;&#20540;&#25805;&#20316;&#65288;&#20174;&#26435;&#37325;&#21442;&#25968;&#24352;&#37327;&#20013;&#20943;&#21435;&#22343;&#20540;&#20540;&#65289;&#21644;&#25439;&#22833;&#20989;&#25968;&#65288;&#26368;&#21518;&#30340;BN&#23618;&#65289;&#20043;&#21069;&#30340;&#19968;&#20010;BN&#23618;&#12290;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#20999;&#32447;&#26680;&#65288;NTK&#65289;&#29702;&#35770;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26435;&#37325;&#22343;&#20540;&#25805;&#20316;&#21487;&#20197;&#30333;&#21270;&#28608;&#27963;&#65292;&#20351;&#32593;&#32476;&#36716;&#21270;&#20026;&#31867;&#20284;BN&#23618;&#30340;&#28151;&#27788;&#29366;&#24577;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#25910;&#25947;&#24615;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Substantial experiments have validated the success of Batch Normalization (BN) Layer in benefiting convergence and generalization. However, BN requires extra memory and float-point calculation. Moreover, BN would be inaccurate on micro-batch, as it depends on batch statistics. In this paper, we address these problems by simplifying BN regularization while keeping two fundamental impacts of BN layers, i.e., data decorrelation and adaptive learning rate. We propose a novel normalization method, named MimicNorm, to improve the convergence and efficiency in network training. MimicNorm consists of only two light operations, including modified weight mean operations (subtract mean values from weight parameter tensor) and one BN layer before loss function (last BN layer). We leverage the neural tangent kernel (NTK) theory to prove that our weight mean operation whitens activations and transits network into the chaotic regime like BN layer, and consequently, leads to an enhanced convergence. T
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#28789;&#27963;&#30340;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#26426;&#26800;&#24314;&#27169;&#20013;&#30340;&#25968;&#25454;&#19968;&#33268;&#21453;&#28436;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#36125;&#21494;&#26031;&#20998;&#26512;&#20013;&#26080;&#20449;&#24687;&#20808;&#39564;&#24341;&#20837;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#22312;&#38543;&#26426;&#36870;&#38382;&#39064;&#26694;&#26550;&#19979;&#25512;&#26029;&#21442;&#25968;&#12290;&#20351;&#29992;&#25298;&#32477;&#37319;&#26679;&#12289;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#31561;&#26032;&#26041;&#27861;&#35299;&#20915;&#20102;&#25968;&#25454;&#19968;&#33268;&#21453;&#28436;&#30340;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#32422;&#26463;&#20248;&#21270;&#21644;&#20808;&#39564;&#36870;&#38382;&#39064;&#20998;&#26512;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2009.08267</link><description>&lt;p&gt;
&#26426;&#26800;&#24314;&#27169;&#20013;&#25968;&#25454;&#19968;&#33268;&#21453;&#28436;&#30340;&#26032;&#39062;&#28789;&#27963;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Novel and flexible parameter estimation methods for data-consistent inversion in mechanistic modeling. (arXiv:2009.08267v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.08267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#28789;&#27963;&#30340;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#26426;&#26800;&#24314;&#27169;&#20013;&#30340;&#25968;&#25454;&#19968;&#33268;&#21453;&#28436;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#36125;&#21494;&#26031;&#20998;&#26512;&#20013;&#26080;&#20449;&#24687;&#20808;&#39564;&#24341;&#20837;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#22312;&#38543;&#26426;&#36870;&#38382;&#39064;&#26694;&#26550;&#19979;&#25512;&#26029;&#21442;&#25968;&#12290;&#20351;&#29992;&#25298;&#32477;&#37319;&#26679;&#12289;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#31561;&#26032;&#26041;&#27861;&#35299;&#20915;&#20102;&#25968;&#25454;&#19968;&#33268;&#21453;&#28436;&#30340;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#32422;&#26463;&#20248;&#21270;&#21644;&#20808;&#39564;&#36870;&#38382;&#39064;&#20998;&#26512;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#31995;&#32479;&#30340;&#39044;&#27979;&#36890;&#24120;&#20381;&#36182;&#20110;&#20174;&#27169;&#25311;&#38598;&#21512;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#20363;&#22914;&#29983;&#29289;&#31185;&#23398;&#20013;&#30340;&#32454;&#32990;&#38598;&#21512;&#12290;&#20026;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#36825;&#20123;&#38598;&#21512;&#20351;&#29992;&#21442;&#25968;&#21270;&#30340;&#26426;&#26800;&#27169;&#22411;&#65288;MM&#65289;&#36827;&#34892;&#27169;&#25311;&#12290;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#27169;&#22411;&#26063;&#26041;&#27861;&#26159;&#30446;&#21069;&#29289;&#29702;&#31995;&#32479;&#21442;&#25968;&#20272;&#35745;&#30340;&#20004;&#31867;&#20027;&#27969;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#36125;&#21494;&#26031;&#20998;&#26512;&#20013;&#65292;&#23545;MM&#21442;&#25968;&#20351;&#29992;&#26080;&#20449;&#24687;&#20808;&#39564;&#20250;&#24341;&#20837;&#19981;&#21487;&#21462;&#30340;&#20559;&#24046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22914;&#20309;&#22312;&#38543;&#26426;&#36870;&#38382;&#39064;&#65288;SIP&#65289;&#26694;&#26550;&#20013;&#25512;&#26029;&#21442;&#25968;&#65292;&#35813;&#26694;&#26550;&#20063;&#34987;&#31216;&#20026;&#25968;&#25454;&#19968;&#33268;&#21453;&#28436;&#65292;&#20854;&#20013;&#20808;&#39564;&#21482;&#20851;&#27880;&#30001;&#20110;MM&#19981;&#21487;&#36870;&#36896;&#25104;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#28436;&#31034;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#25298;&#32477;&#37319;&#26679;&#12289;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;SIP&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20811;&#26381;SIP&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#22522;&#20110;&#32422;&#26463;&#20248;&#21270;&#21644;&#39044;
&lt;/p&gt;
&lt;p&gt;
Predictions for physical systems often rely upon knowledge acquired from ensembles of entities, e.g., ensembles of cells in biological sciences. For qualitative and quantitative analysis, these ensembles are simulated with parametric families of mechanistic models (MM). Two classes of methodologies, based on Bayesian inference and Population of Models, currently prevail in parameter estimation for physical systems. However, in Bayesian analysis, uninformative priors for MM parameters introduce undesirable bias. Here, we propose how to infer parameters within the framework of stochastic inverse problems (SIP), also termed data-consistent inversion, wherein the prior targets only uncertainties that arise due to MM non-invertibility. To demonstrate, we introduce new methods to solve SIP based on rejection sampling, Markov chain Monte Carlo, and generative adversarial networks (GANs). In addition, to overcome limitations of SIP, we reformulate SIP based on constrained optimization and pres
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#36866;&#24212;&#25209;&#22823;&#23567;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21892;SGD&#25910;&#25947;&#24615;&#30340;&#26041;&#27861;&#65292;&#26082;&#20943;&#23569;&#20102;&#39640;&#26041;&#24046;&#26799;&#24230;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#21448;&#20445;&#25345;&#20102;&#36739;&#39640;&#31934;&#24230;&#30340;&#26799;&#24230;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/1910.08222</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#25209;&#22823;&#23567;&#25913;&#21892;SGD&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the convergence of SGD through adaptive batch sizes. (arXiv:1910.08222v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1910.08222
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#25209;&#22823;&#23567;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21892;SGD&#25910;&#25947;&#24615;&#30340;&#26041;&#27861;&#65292;&#26082;&#20943;&#23569;&#20102;&#39640;&#26041;&#24046;&#26799;&#24230;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#21448;&#20445;&#25345;&#20102;&#36739;&#39640;&#31934;&#24230;&#30340;&#26799;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#25209;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21450;&#20854;&#21464;&#31181;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#26469;&#36817;&#20284;&#30446;&#26631;&#20989;&#25968;&#30340;&#26799;&#24230;&#65292;&#20063;&#23601;&#26159;&#25209;&#22823;&#23567;&#12290;&#23567;&#25209;&#37327;&#22823;&#23567;&#22312;&#27599;&#20010;&#27169;&#22411;&#26356;&#26032;&#26102;&#38656;&#35201;&#36739;&#23569;&#30340;&#35745;&#31639;&#37327;&#65292;&#20294;&#21487;&#33021;&#23548;&#33268;&#39640;&#26041;&#24046;&#30340;&#26799;&#24230;&#20272;&#35745;&#65292;&#36825;&#23545;&#20248;&#21270;&#26469;&#35828;&#26159;&#19968;&#20123;&#25361;&#25112;&#12290;&#30456;&#21453;&#65292;&#22823;&#25209;&#37327;&#38656;&#35201;&#26356;&#22810;&#35745;&#31639;&#37327;&#65292;&#20294;&#21487;&#33021;&#20135;&#29983;&#26356;&#39640;&#31934;&#24230;&#30340;&#26799;&#24230;&#20272;&#35745;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25209;&#22823;&#23567;&#35843;&#25972;&#21040;&#27169;&#22411;&#35757;&#32451;&#25439;&#22833;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#21508;&#31181;&#20989;&#25968;&#31867;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#27169;&#22411;&#26356;&#26032;&#26469;&#35828;&#38656;&#35201;&#19982;&#26799;&#24230;&#19979;&#38477;&#30456;&#21516;&#25968;&#37327;&#30340;&#27425;&#25968;&#65292;&#21516;&#26102;&#23545;&#20110;&#26799;&#24230;&#35745;&#31639;&#26469;&#35828;&#38656;&#35201;&#19982;SGD&#30456;&#21516;&#25968;&#37327;&#30340;&#27425;&#25968;&#12290;&#35813;&#26041;&#27861;&#38656;&#35201;&#22312;&#27599;&#20010;&#27169;&#22411;&#26356;&#26032;&#26102;&#35745;&#31639;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25439;&#22833;&#65292;&#20294;&#36890;&#36807;&#36817;&#20284;&#35757;&#32451;&#25439;&#22833;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#25152;&#38656;&#30340;&#35745;&#31639;&#37327;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#27169;&#22411;&#26356;&#26032;&#32780;&#19981;&#22686;&#21152;&#24635;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mini-batch stochastic gradient descent (SGD) and variants thereof approximate the objective function's gradient with a small number of training examples, aka the batch size. Small batch sizes require little computation for each model update but can yield high-variance gradient estimates, which poses some challenges for optimization. Conversely, large batches require more computation but can yield higher precision gradient estimates. This work presents a method to adapt the batch size to the model's training loss. For various function classes, we show that our method requires the same order of model updates as gradient descent while requiring the same order of gradient computations as SGD. This method requires evaluating the model's loss on the entire dataset every model update. However, the required computation is greatly reduced by approximating the training loss. We provide experiments that illustrate our methods require fewer model updates without increasing the total amount of comp
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#20108;&#36827;&#21046;&#21464;&#37327;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#30340;&#23454;&#29992;&#31639;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#30028;&#38480;&#21644;&#29616;&#20195;&#31995;&#32479;&#25216;&#26415;&#30340;&#32467;&#21512;&#26469;&#35299;&#20915;&#20915;&#31574;&#26641;&#20248;&#21270;&#30340;&#22256;&#38590;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#21487;&#25193;&#23637;&#24615;&#12289;&#36895;&#24230;&#21644;&#26368;&#20248;&#24615;&#35777;&#26126;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/1904.12847</link><description>&lt;p&gt;
&#26368;&#20248;&#31232;&#30095;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
Optimal Sparse Decision Trees. (arXiv:1904.12847v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1904.12847
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#20108;&#36827;&#21046;&#21464;&#37327;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#30340;&#23454;&#29992;&#31639;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#30028;&#38480;&#21644;&#29616;&#20195;&#31995;&#32479;&#25216;&#26415;&#30340;&#32467;&#21512;&#26469;&#35299;&#20915;&#20915;&#31574;&#26641;&#20248;&#21270;&#30340;&#22256;&#38590;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#21487;&#25193;&#23637;&#24615;&#12289;&#36895;&#24230;&#21644;&#26368;&#20248;&#24615;&#35777;&#26126;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#31639;&#27861;&#33258;&#20174;1980&#24180;&#20195;&#21021;&#20197;&#26469;&#23601;&#19968;&#30452;&#26159;&#21487;&#35299;&#37322;&#65288;&#36879;&#26126;&#65289;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#31639;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#33258;&#20174;&#23427;&#20204;&#38382;&#19990;&#20197;&#26469;&#65292;&#22256;&#25200;&#20915;&#31574;&#26641;&#31639;&#27861;&#30340;&#38382;&#39064;&#23601;&#26159;&#23427;&#20204;&#30340;&#38750;&#26368;&#20248;&#24615;&#65292;&#25110;&#32773;&#35828;&#32570;&#20047;&#25509;&#36817;&#26368;&#20248;&#30340;&#20445;&#35777;&#65306;&#20915;&#31574;&#26641;&#31639;&#27861;&#24448;&#24448;&#26159;&#36138;&#23146;&#30340;&#25110;&#32773;&#30446;&#20809;&#30701;&#27973;&#30340;&#65292;&#26377;&#26102;&#20250;&#20135;&#29983;&#26126;&#26174;&#38750;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;&#20915;&#31574;&#26641;&#20248;&#21270;&#30340;&#22256;&#38590;&#26082;&#26159;&#19968;&#20010;&#29702;&#35770;&#19978;&#30340;&#38556;&#30861;&#65292;&#20063;&#26159;&#19968;&#20010;&#23454;&#38469;&#19978;&#30340;&#38556;&#30861;&#65292;&#21363;&#20351;&#26159;&#20180;&#32454;&#30340;&#25968;&#23398;&#35268;&#21010;&#26041;&#27861;&#20063;&#26080;&#27861;&#39640;&#25928;&#22320;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#20108;&#36827;&#21046;&#21464;&#37327;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#30340;&#23454;&#29992;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#20998;&#26512;&#30028;&#38480;&#20943;&#23567;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#29616;&#20195;&#31995;&#32479;&#25216;&#26415;&#65292;&#21253;&#25324;&#25968;&#25454;&#32467;&#26500;&#21644;&#33258;&#23450;&#20041;&#20301;&#21521;&#37327;&#24211;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#21487;&#25193;&#23637;&#24615;&#12289;&#36895;&#24230;&#21644;&#26368;&#20248;&#24615;&#35777;&#26126;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/xi&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision tree algorithms have been among the most popular algorithms for interpretable (transparent) machine learning since the early 1980's. The problem that has plagued decision tree algorithms since their inception is their lack of optimality, or lack of guarantees of closeness to optimality: decision tree algorithms are often greedy or myopic, and sometimes produce unquestionably suboptimal models. Hardness of decision tree optimization is both a theoretical and practical obstacle, and even careful mathematical programming approaches have not been able to solve these problems efficiently. This work introduces the first practical algorithm for optimal decision trees for binary variables. The algorithm is a co-design of analytical bounds that reduce the search space and modern systems techniques, including data structures and a custom bit-vector library. Our experiments highlight advantages in scalability, speed, and proof of optimality. The code is available at https://github.com/xi
&lt;/p&gt;</description></item></channel></rss>