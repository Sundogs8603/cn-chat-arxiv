<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#23548;&#21521;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#21487;&#25511;&#38899;&#20048;&#21046;&#20316;&#65292;&#21253;&#25324;&#38899;&#39057;&#30340;&#24310;&#32493;&#12289;&#20462;&#34917;&#21644;&#20877;&#29983;&#65292;&#20197;&#21450;&#39118;&#26684;&#29305;&#24449;&#36716;&#31227;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2311.00613</link><description>&lt;p&gt;
&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#23548;&#21521;&#26799;&#24230;&#23454;&#29616;&#21487;&#25511;&#38899;&#20048;&#21046;&#20316;
&lt;/p&gt;
&lt;p&gt;
Controllable Music Production with Diffusion Models and Guidance Gradients. (arXiv:2311.00613v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#23548;&#21521;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#21487;&#25511;&#38899;&#20048;&#21046;&#20316;&#65292;&#21253;&#25324;&#38899;&#39057;&#30340;&#24310;&#32493;&#12289;&#20462;&#34917;&#21644;&#20877;&#29983;&#65292;&#20197;&#21450;&#39118;&#26684;&#29305;&#24449;&#36716;&#31227;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26465;&#20214;&#29983;&#25104;&#26469;&#22788;&#29702;&#38899;&#20048;&#21046;&#20316;&#20013;&#30340;&#21508;&#31181;&#29616;&#23454;&#20219;&#21153;&#65292;&#21253;&#25324;&#38899;&#20048;&#38899;&#39057;&#30340;&#24310;&#32493;&#12289;&#20462;&#34917;&#21644;&#20877;&#29983;&#12289;&#22312;&#20004;&#20010;&#19981;&#21516;&#38899;&#20048;&#26354;&#30446;&#20043;&#38388;&#21019;&#24314;&#24179;&#28369;&#30340;&#36807;&#28193;&#20197;&#21450;&#23558;&#25152;&#38656;&#30340;&#39118;&#26684;&#29305;&#24449;&#36716;&#31227;&#21040;&#29616;&#26377;&#38899;&#39057;&#29255;&#27573;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#37319;&#26679;&#26102;&#24212;&#29992;&#23548;&#21521;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#20013;&#25903;&#25345;&#37325;&#24314;&#21644;&#20998;&#31867;&#25439;&#22833;&#65292;&#25110;&#32773;&#20004;&#32773;&#30340;&#20219;&#24847;&#32452;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#30830;&#20445;&#29983;&#25104;&#30340;&#38899;&#39057;&#21487;&#20197;&#21305;&#37197;&#20854;&#21608;&#22260;&#30340;&#19978;&#19979;&#25991;&#65292;&#25110;&#32773;&#31526;&#21512;&#30456;&#23545;&#20110;&#20219;&#20309;&#36866;&#24403;&#30340;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#25110;&#23884;&#20837;&#27169;&#22411;&#25351;&#23450;&#30340;&#31867;&#20998;&#24067;&#25110;&#28508;&#22312;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate how conditional generation from diffusion models can be used to tackle a variety of realistic tasks in the production of music in 44.1kHz stereo audio with sampling-time guidance. The scenarios we consider include continuation, inpainting and regeneration of musical audio, the creation of smooth transitions between two different music tracks, and the transfer of desired stylistic characteristics to existing audio clips. We achieve this by applying guidance at sampling time in a simple framework that supports both reconstruction and classification losses, or any combination of the two. This approach ensures that generated audio can match its surrounding context, or conform to a class distribution or latent representation specified relative to any suitable pre-trained classifier or embedding model.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;REMed&#30340;&#26816;&#32034;&#22686;&#24378;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#38480;&#35780;&#20272;&#20020;&#24202;&#20107;&#20214;&#24182;&#33258;&#21160;&#36873;&#25321;&#30456;&#20851;&#20107;&#20214;&#36827;&#34892;&#39044;&#27979;&#65292;&#28040;&#38500;&#20102;&#20154;&#24037;&#29305;&#24449;&#36873;&#25321;&#21644;&#35266;&#23519;&#31383;&#21475;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.20204</link><description>&lt;p&gt;
&#21033;&#29992;&#36817;&#26080;&#38480;&#21382;&#21490;&#30340;&#36890;&#29992;&#26816;&#32034;&#22686;&#24378;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
General-Purpose Retrieval-Enhanced Medical Prediction Model Using Near-Infinite History. (arXiv:2310.20204v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20204
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;REMed&#30340;&#26816;&#32034;&#22686;&#24378;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#38480;&#35780;&#20272;&#20020;&#24202;&#20107;&#20214;&#24182;&#33258;&#21160;&#36873;&#25321;&#30456;&#20851;&#20107;&#20214;&#36827;&#34892;&#39044;&#27979;&#65292;&#28040;&#38500;&#20102;&#20154;&#24037;&#29305;&#24449;&#36873;&#25321;&#21644;&#35266;&#23519;&#31383;&#21475;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#24320;&#21457;&#20020;&#24202;&#39044;&#27979;&#27169;&#22411;&#65288;&#20363;&#22914;&#27515;&#20129;&#39044;&#27979;&#65289;&#36890;&#24120;&#20381;&#36182;&#20110;&#19987;&#23478;&#24847;&#35265;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#21644;&#35843;&#25972;&#35266;&#27979;&#31383;&#21475;&#22823;&#23567;&#12290;&#36825;&#32473;&#19987;&#23478;&#24102;&#26469;&#36127;&#25285;&#24182;&#22312;&#24320;&#21457;&#36807;&#31243;&#20013;&#36896;&#25104;&#29942;&#39048;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;&#65288;REMed&#65289;&#65292;&#20197;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;REMed&#21487;&#20197;&#22522;&#26412;&#35780;&#20272;&#26080;&#38480;&#37327;&#30340;&#20020;&#24202;&#20107;&#20214;&#65292;&#36873;&#25321;&#30456;&#20851;&#30340;&#20107;&#20214;&#24182;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#28040;&#38500;&#20102;&#38656;&#35201;&#25163;&#21160;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#24182;&#23454;&#26102;&#35266;&#23519;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;27&#20010;&#20020;&#24202;&#20219;&#21153;&#21644;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;EHR&#25968;&#25454;&#38598;&#30340;&#29420;&#31435;&#38431;&#21015;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20123;&#29305;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;REMed&#20248;&#20110;&#20854;&#20182;&#29616;&#20195;&#26550;&#26500;&#65292;&#23427;&#20204;&#26088;&#22312;&#22788;&#29702;&#23613;&#21487;&#33021;&#22810;&#30340;&#20107;&#20214;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;REMed&#30340;&#20559;&#22909;&#19982;&#21307;&#23398;&#19987;&#23478;&#30340;&#20559;&#22909;&#23494;&#20999;&#30456;&#20284;&#12290;&#25105;&#20204;&#26399;&#26395;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#26174;&#33879;&#21152;&#36895;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing clinical prediction models (e.g., mortality prediction) based on electronic health records (EHRs) typically relies on expert opinion for feature selection and adjusting observation window size. This burdens experts and creates a bottleneck in the development process. We propose Retrieval-Enhanced Medical prediction model (REMed) to address such challenges. REMed can essentially evaluate an unlimited number of clinical events, select the relevant ones, and make predictions. This approach effectively eliminates the need for manual feature selection and enables an unrestricted observation window. We verified these properties through experiments on 27 clinical tasks and two independent cohorts from publicly available EHR datasets, where REMed outperformed other contemporary architectures that aim to handle as many events as possible. Notably, we found that the preferences of REMed align closely with those of medical experts. We expect our approach to significantly expedite the d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31867;&#38750;&#32447;&#24615;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30340;&#39640;&#27010;&#29575;&#25910;&#25947;&#36793;&#30028;&#12290;&#23545;&#20110;&#20855;&#26377;Lipschitz&#36830;&#32493;&#26799;&#24230;&#30340;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;&#20351;&#22122;&#22768;&#26159;&#37325;&#23614;&#30340;&#65292;&#32467;&#26524;&#35777;&#26126;&#20102;&#23545;&#22833;&#36133;&#27010;&#29575;&#30340;&#23545;&#25968;&#20381;&#36182;&#12290;&#36825;&#20123;&#32467;&#26524;&#36866;&#29992;&#20110;&#21098;&#20999;&#12289;&#24402;&#19968;&#21270;&#21644;&#37327;&#21270;&#31561;&#20219;&#20309;&#20855;&#26377;&#26377;&#30028;&#36755;&#20986;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.18784</link><description>&lt;p&gt;
&#39640;&#27010;&#29575;&#25910;&#25947;&#36793;&#30028;&#19979;&#30340;&#38750;&#32447;&#24615;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#37325;&#23614;&#22122;&#22768;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
High-probability Convergence Bounds for Nonlinear Stochastic Gradient Descent Under Heavy-tailed Noise. (arXiv:2310.18784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31867;&#38750;&#32447;&#24615;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30340;&#39640;&#27010;&#29575;&#25910;&#25947;&#36793;&#30028;&#12290;&#23545;&#20110;&#20855;&#26377;Lipschitz&#36830;&#32493;&#26799;&#24230;&#30340;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;&#20351;&#22122;&#22768;&#26159;&#37325;&#23614;&#30340;&#65292;&#32467;&#26524;&#35777;&#26126;&#20102;&#23545;&#22833;&#36133;&#27010;&#29575;&#30340;&#23545;&#25968;&#20381;&#36182;&#12290;&#36825;&#20123;&#32467;&#26524;&#36866;&#29992;&#20110;&#21098;&#20999;&#12289;&#24402;&#19968;&#21270;&#21644;&#37327;&#21270;&#31561;&#20219;&#20309;&#20855;&#26377;&#26377;&#30028;&#36755;&#20986;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#20010;&#30740;&#31350;&#24037;&#20316;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21450;&#20854;&#21098;&#20999;&#21464;&#20307;&#30340;&#39640;&#27010;&#29575;&#25910;&#25947;&#12290;&#19982;&#26222;&#36890;&#30340;SGD&#30456;&#27604;&#65292;&#21098;&#20999;SGD&#22312;&#23454;&#38469;&#20013;&#26356;&#21152;&#31283;&#23450;&#65292;&#24182;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#23545;&#25968;&#20381;&#36182;&#20110;&#22833;&#36133;&#27010;&#29575;&#30340;&#39069;&#22806;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#20854;&#20182;&#23454;&#38469;&#38750;&#32447;&#24615;SGD&#21464;&#20307;&#65288;&#22914;&#31526;&#21495;SGD&#12289;&#37327;&#21270;SGD&#21644;&#24402;&#19968;&#21270;SGD&#65289;&#30340;&#25910;&#25947;&#24615;&#29702;&#35299;&#35201;&#23569;&#24471;&#22810;&#65292;&#36825;&#20123;&#26041;&#27861;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#36890;&#20449;&#25928;&#29575;&#25110;&#21152;&#36895;&#25910;&#25947;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#24191;&#20041;&#38750;&#32447;&#24615;SGD&#26041;&#27861;&#30340;&#39640;&#27010;&#29575;&#25910;&#25947;&#36793;&#30028;&#12290;&#23545;&#20110;&#20855;&#26377;Lipschitz&#36830;&#32493;&#26799;&#24230;&#30340;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;&#20351;&#22122;&#22768;&#26159;&#37325;&#23614;&#30340;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#22833;&#36133;&#27010;&#29575;&#30340;&#23545;&#25968;&#20381;&#36182;&#12290;&#19982;&#21098;&#20999;SGD&#30340;&#32467;&#26524;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26356;&#20026;&#19968;&#33324;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#26377;&#30028;&#36755;&#20986;&#30340;&#20219;&#20309;&#38750;&#32447;&#24615;&#20989;&#25968;&#65292;&#22914;&#21098;&#20999;&#12289;&#24402;&#19968;&#21270;&#21644;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several recent works have studied the convergence \textit{in high probability} of stochastic gradient descent (SGD) and its clipped variant. Compared to vanilla SGD, clipped SGD is practically more stable and has the additional theoretical benefit of logarithmic dependence on the failure probability. However, the convergence of other practical nonlinear variants of SGD, e.g., sign SGD, quantized SGD and normalized SGD, that achieve improved communication efficiency or accelerated convergence is much less understood. In this work, we study the convergence bounds \textit{in high probability} of a broad class of nonlinear SGD methods. For strongly convex loss functions with Lipschitz continuous gradients, we prove a logarithmic dependence on the failure probability, even when the noise is heavy-tailed. Strictly more general than the results for clipped SGD, our results hold for any nonlinearity with bounded (component-wise or joint) outputs, such as clipping, normalization, and quantizati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38543;&#26426;&#21270;&#39118;&#38505;&#26631;&#20934;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#36991;&#20813;&#22312;&#39118;&#38505;&#19978;&#30340;&#20559;&#21521;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21253;&#25324;Atari 55&#28216;&#25103;&#22312;&#20869;&#30340;&#21508;&#31181;&#29615;&#22659;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.16546</link><description>&lt;p&gt;
&#20048;&#35266;&#20027;&#20041;&#30340;&#38519;&#38449;&#65306;&#36890;&#36807;&#38543;&#26426;&#21270;&#39118;&#38505;&#26631;&#20934;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion. (arXiv:2310.16546v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38543;&#26426;&#21270;&#39118;&#38505;&#26631;&#20934;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#36991;&#20813;&#22312;&#39118;&#38505;&#19978;&#30340;&#20559;&#21521;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21253;&#25324;Atari 55&#28216;&#25103;&#22312;&#20869;&#30340;&#21508;&#31181;&#29615;&#22659;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35797;&#22270;&#21033;&#29992;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#25506;&#32034;&#65292;&#22914;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#30340;&#20048;&#35266;&#20027;&#20041;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20272;&#35745;&#30340;&#26041;&#24046;&#36827;&#34892;&#20048;&#35266;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#25910;&#38598;&#30340;&#20559;&#24046;&#65292;&#38459;&#30861;&#25910;&#25947;&#25110;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#39118;&#38505;&#26631;&#20934;&#26469;&#36873;&#25321;&#21160;&#20316;&#65292;&#36991;&#20813;&#22312;&#39118;&#38505;&#19978;&#30340;&#21333;&#21521;&#20542;&#21521;&#12290;&#25105;&#20204;&#36890;&#36807;&#25197;&#26354;&#39118;&#38505;&#24230;&#37327;&#25552;&#20379;&#20102;&#19968;&#20010;&#25200;&#21160;&#30340;&#20998;&#24067;&#36125;&#23572;&#26364;&#26368;&#20248;&#24615;&#31639;&#23376;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#20855;&#26377;&#36739;&#24369;&#30340;&#25910;&#32553;&#24615;&#36136;&#30340;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#25903;&#25345;&#65292;&#25152;&#25552;&#26041;&#27861;&#19981;&#20250;&#38519;&#20837;&#20559;&#21521;&#24615;&#30340;&#25506;&#32034;&#65292;&#24182;&#30830;&#20445;&#25910;&#25947;&#21040;&#26368;&#20248;&#22238;&#25253;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#21253;&#25324;Atari 55&#28216;&#25103;&#22312;&#20869;&#30340;&#21508;&#31181;&#29615;&#22659;&#20013;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#30340;&#22522;&#20110;&#20998;&#24067;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributional reinforcement learning algorithms have attempted to utilize estimated uncertainty for exploration, such as optimism in the face of uncertainty. However, using the estimated variance for optimistic exploration may cause biased data collection and hinder convergence or performance. In this paper, we present a novel distributional reinforcement learning algorithm that selects actions by randomizing risk criterion to avoid one-sided tendency on risk. We provide a perturbed distributional Bellman optimality operator by distorting the risk measure and prove the convergence and optimality of the proposed method with the weaker contraction property. Our theoretical results support that the proposed method does not fall into biased exploration and is guaranteed to converge to an optimal return. Finally, we empirically show that our method outperforms other existing distribution-based algorithms in various environments including Atari 55 games.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#23398;&#24471;&#34920;&#31034;&#30340;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#25581;&#31034;&#20986;&#21452;&#35895;&#29616;&#35937;&#26159;&#22312;&#20351;&#29992;&#22122;&#22768;&#25968;&#25454;&#35757;&#32451;&#30340;&#19981;&#23436;&#32654;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#21452;&#35895;&#29616;&#35937;&#30340;&#35299;&#37322;&#65306;&#27169;&#22411;&#39318;&#20808;&#23398;&#20064;&#22122;&#22768;&#25968;&#25454;&#30452;&#21040;&#25554;&#20540;&#28857;&#65292;&#28982;&#21518;&#36890;&#36807;&#36229;&#21442;&#25968;&#21270;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#26469;&#23558;&#20449;&#24687;&#19982;&#22122;&#22768;&#20998;&#31163;&#12290;</title><link>http://arxiv.org/abs/2310.13572</link><description>&lt;p&gt;
&#35299;&#24320;&#21452;&#35895;&#20043;&#35868;&#65306;&#36879;&#36807;&#23398;&#24471;&#29305;&#24449;&#31354;&#38388;&#30340;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Unraveling the Enigma of Double Descent: An In-depth Analysis through the Lens of Learned Feature Space. (arXiv:2310.13572v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#23398;&#24471;&#34920;&#31034;&#30340;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#25581;&#31034;&#20986;&#21452;&#35895;&#29616;&#35937;&#26159;&#22312;&#20351;&#29992;&#22122;&#22768;&#25968;&#25454;&#35757;&#32451;&#30340;&#19981;&#23436;&#32654;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#21452;&#35895;&#29616;&#35937;&#30340;&#35299;&#37322;&#65306;&#27169;&#22411;&#39318;&#20808;&#23398;&#20064;&#22122;&#22768;&#25968;&#25454;&#30452;&#21040;&#25554;&#20540;&#28857;&#65292;&#28982;&#21518;&#36890;&#36807;&#36229;&#21442;&#25968;&#21270;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#26469;&#23558;&#20449;&#24687;&#19982;&#22122;&#22768;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#35895;&#29616;&#35937;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#21576;&#29616;&#20986;&#19968;&#31181;&#36870;&#30452;&#35273;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#30740;&#31350;&#20154;&#21592;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#20219;&#21153;&#20013;&#35266;&#23519;&#21040;&#20854;&#34920;&#29616;&#12290;&#34429;&#28982;&#23545;&#35813;&#29616;&#35937;&#22312;&#29305;&#23450;&#32972;&#26223;&#19979;&#25552;&#20986;&#20102;&#19968;&#20123;&#29702;&#35770;&#35299;&#37322;&#65292;&#20294;&#20173;&#27809;&#26377;&#30830;&#31435;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#20013;&#20986;&#29616;&#21452;&#35895;&#30340;&#20844;&#35748;&#29702;&#35770;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#21452;&#35895;&#29616;&#35937;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20986;&#29616;&#21463;&#21040;&#22122;&#22768;&#25968;&#25454;&#24433;&#21709;&#30340;&#24378;&#28872;&#24433;&#21709;&#12290;&#36890;&#36807;&#23545;&#23398;&#24471;&#34920;&#31034;&#30340;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20986;&#22312;&#20351;&#29992;&#22122;&#22768;&#25968;&#25454;&#35757;&#32451;&#30340;&#19981;&#23436;&#32654;&#27169;&#22411;&#20013;&#20250;&#20986;&#29616;&#21452;&#35895;&#29616;&#35937;&#12290;&#25105;&#20204;&#35748;&#20026;&#21452;&#35895;&#26159;&#27169;&#22411;&#39318;&#20808;&#23398;&#20064;&#22122;&#22768;&#25968;&#25454;&#30452;&#21040;&#25554;&#20540;&#28857;&#65292;&#28982;&#21518;&#36890;&#36807;&#36229;&#21442;&#25968;&#21270;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#26469;&#33719;&#21462;&#23558;&#20449;&#24687;&#19982;&#22122;&#22768;&#20998;&#31163;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20551;&#35774;&#21452;&#35895;&#29616;&#35937;&#19981;&#24212;&#35813;&#22312;&#33391;&#22909;&#27491;&#21017;&#21270;&#30340;&#27169;&#22411;&#20013;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Double descent presents a counter-intuitive aspect within the machine learning domain, and researchers have observed its manifestation in various models and tasks. While some theoretical explanations have been proposed for this phenomenon in specific contexts, an accepted theory to account for its occurrence in deep learning remains yet to be established. In this study, we revisit the phenomenon of double descent and demonstrate that its occurrence is strongly influenced by the presence of noisy data. Through conducting a comprehensive analysis of the feature space of learned representations, we unveil that double descent arises in imperfect models trained with noisy data. We argue that double descent is a consequence of the model first learning the noisy data until interpolation and then adding implicit regularization via over-parameterization acquiring therefore capability to separate the information from the noise. We postulate that double descent should never occur in well-regulari
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#23558;&#28201;&#24230;&#21442;&#25968;&#32435;&#20837;&#36125;&#21494;&#26031;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#19968;&#33268;&#39044;&#27979;&#20013;&#30340;&#20248;&#21183;&#65292;&#20197;&#25552;&#20379;&#26377;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.11479</link><description>&lt;p&gt;
&#20851;&#20110;&#36125;&#21494;&#26031;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#19968;&#33268;&#39044;&#27979;&#20013;&#30340;&#28201;&#24230;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On the Temperature of Bayesian Graph Neural Networks for Conformal Prediction. (arXiv:2310.11479v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11479
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#23558;&#28201;&#24230;&#21442;&#25968;&#32435;&#20837;&#36125;&#21494;&#26031;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#19968;&#33268;&#39044;&#27979;&#20013;&#30340;&#20248;&#21183;&#65292;&#20197;&#25552;&#20379;&#26377;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#32463;&#24120;&#20351;&#29992;GNNs&#30340;&#24773;&#20917;&#19979;&#12290;&#19968;&#33268;&#39044;&#27979;(CP)&#20026;&#20219;&#20309;&#40657;&#30418;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#12290;CP&#20445;&#35777;&#20102;&#19968;&#20010;&#39044;&#27979;&#38598;&#20197;&#25152;&#38656;&#30340;&#27010;&#29575;&#21253;&#21547;&#30495;&#23454;&#26631;&#31614;&#30340;&#24418;&#24335;&#30340;&#23448;&#26041;&#27010;&#29575;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#38598;&#30340;&#22823;&#23567;&#65292;&#21363;"&#20302;&#25928;&#29575;"&#65292;&#21463;&#21040;&#24213;&#23618;&#27169;&#22411;&#21644;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36125;&#21494;&#26031;&#23398;&#20064;&#36824;&#22522;&#20110;&#20272;&#35745;&#30340;&#21518;&#39564;&#20998;&#24067;&#25552;&#20379;&#19968;&#20010;&#21487;&#20449;&#21306;&#22495;&#65292;&#20294;&#21482;&#26377;&#22312;&#27169;&#22411;&#27491;&#30830;&#25351;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20010;&#21306;&#22495;&#25165;&#26159;"&#33391;&#22909;&#26657;&#20934;"&#30340;&#12290;&#22312;&#19968;&#20010;&#26368;&#36817;&#30340;&#24037;&#20316;&#30340;&#22522;&#30784;&#19978;&#65292;&#35813;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#20010;&#32553;&#25918;&#21442;&#25968;&#65292;&#29992;&#20110;&#20174;&#21518;&#39564;&#20272;&#35745;&#20013;&#26500;&#24314;&#26377;&#25928;&#30340;&#21487;&#20449;&#21306;&#22495;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;CP&#26694;&#26550;&#20013;&#23558;&#19968;&#20010;&#28201;&#24230;&#21442;&#25968;&#32435;&#20837;&#36125;&#21494;&#26031;GNNs&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate uncertainty quantification in graph neural networks (GNNs) is essential, especially in high-stakes domains where GNNs are frequently employed. Conformal prediction (CP) offers a promising framework for quantifying uncertainty by providing $\textit{valid}$ prediction sets for any black-box model. CP ensures formal probabilistic guarantees that a prediction set contains a true label with a desired probability. However, the size of prediction sets, known as $\textit{inefficiency}$, is influenced by the underlying model and data generating process. On the other hand, Bayesian learning also provides a credible region based on the estimated posterior distribution, but this region is $\textit{well-calibrated}$ only when the model is correctly specified. Building on a recent work that introduced a scaling parameter for constructing valid credible regions from posterior estimate, our study explores the advantages of incorporating a temperature parameter into Bayesian GNNs within CP fra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#23725;&#22238;&#24402;&#22312;&#30149;&#27602;&#20005;&#37325;&#31243;&#24230;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#25968;&#25454;&#32452;&#32455;&#26041;&#38754;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#65292;&#20005;&#37325;&#31243;&#24230;&#25351;&#25968;&#26377;&#21161;&#20110;&#33719;&#24471;&#24739;&#32773;&#25252;&#29702;&#38656;&#27714;&#30340;&#24191;&#27867;&#27010;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.09485</link><description>&lt;p&gt;
&#24212;&#29992;&#36125;&#21494;&#26031;&#23725;&#22238;&#24402;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#30149;&#27602;&#20005;&#37325;&#31243;&#24230;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Applying Bayesian Ridge Regression AI Modeling in Virus Severity Prediction. (arXiv:2310.09485v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#23725;&#22238;&#24402;&#22312;&#30149;&#27602;&#20005;&#37325;&#31243;&#24230;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#25968;&#25454;&#32452;&#32455;&#26041;&#38754;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#65292;&#20005;&#37325;&#31243;&#24230;&#25351;&#25968;&#26377;&#21161;&#20110;&#33719;&#24471;&#24739;&#32773;&#25252;&#29702;&#38656;&#27714;&#30340;&#24191;&#27867;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26159;&#37325;&#22609;&#21307;&#30103;&#31995;&#32479;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#22312;&#21307;&#30103;&#39046;&#22495;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#25968;&#25454;&#22788;&#29702;&#33021;&#21147;&#26080;&#20215;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#21644;&#24555;&#36895;&#30340;&#35786;&#26029;&#65292;&#20174;&#32780;&#20943;&#36731;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#30340;&#24037;&#20316;&#36127;&#25285;&#12290;&#22240;&#27492;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#21508;&#20010;&#34892;&#19994;&#37117;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#31616;&#21270;&#20102;&#22797;&#26434;&#30340;&#20219;&#21153;&#21644;&#27169;&#24335;&#35782;&#21035;&#65292;&#36825;&#20123;&#23545;&#20154;&#31867;&#25110;&#20256;&#32479;&#30340;&#35745;&#31639;&#26426;&#31639;&#27861;&#26469;&#35828;&#26412;&#26469;&#26159;&#38590;&#20197;&#25215;&#21463;&#30340;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#36125;&#21494;&#26031;&#23725;&#22238;&#24402;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#20197;&#23558;&#23574;&#31471;&#30149;&#27602;&#20998;&#26512;&#24102;&#32473;&#20840;&#29699;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#35780;&#20272;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#25968;&#25454;&#32452;&#32455;&#26041;&#38754;&#36824;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#20005;&#37325;&#31243;&#24230;&#25351;&#25968;&#20316;&#20026;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#33719;&#24471;&#20851;&#20110;&#24739;&#32773;&#25252;&#29702;&#38656;&#27714;&#30340;&#24191;&#27867;&#27010;&#36848;&#65292;&#31526;&#21512;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#23545;&#26356;&#24191;&#27867;&#20998;&#31867;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) is a powerful tool for reshaping healthcare systems. In healthcare, AI is invaluable for its capacity to manage vast amounts of data, which can lead to more accurate and speedy diagnoses, ultimately easing the workload on healthcare professionals. As a result, AI has proven itself to be a power tool across various industries, simplifying complex tasks and pattern recognition that would otherwise be overwhelming for humans or traditional computer algorithms. In this paper, we review the strengths and weaknesses of Bayesian Ridge Regression, an AI model that can be used to bring cutting edge virus analysis to healthcare professionals around the world. The model's accuracy assessment revealed promising results, with room for improvement primarily related to data organization. In addition, the severity index serves as a valuable tool to gain a broad overview of patient care needs, aligning with healthcare professionals' preference for broader categorizations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;CloudOps&#39046;&#22495;&#24341;&#20837;&#20102;&#19977;&#20010;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#25317;&#26377;&#25968;&#21313;&#20159;&#20010;&#35266;&#27979;&#28857;&#65292;&#20026;&#30740;&#31350;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21644;&#25193;&#23637;&#24615;&#22880;&#23450;&#20102;&#23454;&#35777;&#22522;&#30784;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#20505;&#36873;&#26550;&#26500;&#20316;&#20026;&#24378;&#22823;&#30340;&#38646;-shot&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2310.05063</link><description>&lt;p&gt;
&#22312;CloudOps&#39046;&#22495;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#25512;&#21160;&#39044;&#35757;&#32451;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Pushing the Limits of Pre-training for Time Series Forecasting in the CloudOps Domain. (arXiv:2310.05063v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;CloudOps&#39046;&#22495;&#24341;&#20837;&#20102;&#19977;&#20010;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#25317;&#26377;&#25968;&#21313;&#20159;&#20010;&#35266;&#27979;&#28857;&#65292;&#20026;&#30740;&#31350;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21644;&#25193;&#23637;&#24615;&#22880;&#23450;&#20102;&#23454;&#35777;&#22522;&#30784;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#20505;&#36873;&#26550;&#26500;&#20316;&#20026;&#24378;&#22823;&#30340;&#38646;-shot&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#26102;&#20195;&#65292;&#26102;&#38388;&#24207;&#21015;&#34987;&#25918;&#22312;&#20102;&#21518;&#38754;&#12290;&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#30740;&#31350;&#27491;&#22312;&#20139;&#21463;&#30528;&#36234;&#26469;&#36234;&#22823;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#24222;&#22823;&#30340;&#27169;&#22411;&#65292;&#20294;&#26368;&#21463;&#27426;&#36814;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#21482;&#21253;&#21547;&#25968;&#19975;&#20010;&#26102;&#38388;&#27493;&#65292;&#38480;&#21046;&#20102;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#21644;&#25193;&#23637;&#24615;&#30340;&#30740;&#31350;&#25928;&#26524;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20063;&#23545;&#34920;&#36798;&#21147;&#27169;&#22411;&#21644;&#35268;&#27169;&#30340;&#24517;&#35201;&#24615;&#20135;&#29983;&#20102;&#24576;&#30097;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26469;&#33258;&#20113;&#25805;&#20316;&#65288;CloudOps&#65289;&#39046;&#22495;&#30340;&#19977;&#20010;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#25317;&#26377;&#25968;&#21313;&#20159;&#20010;&#35266;&#27979;&#28857;&#65292;&#36827;&#19968;&#27493;&#30740;&#31350;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21644;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#30740;&#31350;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21644;&#25193;&#23637;&#24615;&#30340;&#32463;&#39564;&#22522;&#30784;&#65292;&#24182;&#36890;&#36807;&#30830;&#23450;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#20505;&#36873;&#26550;&#26500;&#20026;&#26410;&#26469;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#38646;-shot&#22522;&#32447;&#65292;&#24182;&#20174;&#36827;&#19968;&#27493;&#30340;&#25193;&#23637;&#20013;&#33719;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series has been left behind in the era of pre-training and transfer learning. While research in the fields of natural language processing and computer vision are enjoying progressively larger datasets to train massive models, the most popular time series datasets consist of only tens of thousands of time steps, limiting our ability to study the effectiveness of pre-training and scaling. Recent studies have also cast doubt on the need for expressive models and scale. To alleviate these issues, we introduce three large-scale time series forecasting datasets from the cloud operations (CloudOps) domain, the largest having billions of observations, enabling further study into pre-training and scaling of time series models. We build the empirical groundwork for studying pre-training and scaling of time series models and pave the way for future research by identifying a promising candidate architecture. We show that it is a strong zero-shot baseline and benefits from further scaling, bot
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#65288;LATS&#65289;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#22312;&#35268;&#21010;&#12289;&#34892;&#21160;&#21644;&#25512;&#29702;&#26041;&#38754;&#30456;&#20114;&#21327;&#21516;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#29615;&#22659;&#65292;&#23454;&#29616;&#26356;&#21152;&#28145;&#24605;&#29087;&#34385;&#21644;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#26426;&#21046;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;LATS&#22312;&#22810;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#65292;&#29305;&#21035;&#22312;&#32534;&#31243;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;94.4%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.04406</link><description>&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#32479;&#19968;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25512;&#29702;&#12289;&#34892;&#21160;&#21644;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models. (arXiv:2310.04406v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04406
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#65288;LATS&#65289;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#22312;&#35268;&#21010;&#12289;&#34892;&#21160;&#21644;&#25512;&#29702;&#26041;&#38754;&#30456;&#20114;&#21327;&#21516;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#29615;&#22659;&#65292;&#23454;&#29616;&#26356;&#21152;&#28145;&#24605;&#29087;&#34385;&#21644;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#26426;&#21046;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;LATS&#22312;&#22810;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#65292;&#29305;&#21035;&#22312;&#32534;&#31243;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;94.4%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19968;&#31995;&#21015;&#20915;&#31574;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#34892;&#21160;&#36807;&#31243;&#65292;&#24182;&#26410;&#33021;&#24191;&#27867;&#37096;&#32626;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LATS&#65288;&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;LLMs&#22312;&#35268;&#21010;&#12289;&#34892;&#21160;&#21644;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#30456;&#20114;&#21327;&#21516;&#12290;LATS&#20511;&#37492;&#20102;&#27169;&#22411;&#23548;&#21521;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#30340;&#24605;&#24819;&#65292;&#23558;LLMs&#29992;&#20316;&#20195;&#29702;&#12289;&#20215;&#20540;&#20989;&#25968;&#21644;&#20248;&#21270;&#22120;&#65292;&#37325;&#26032;&#21033;&#29992;&#20854;&#28508;&#22312;&#30340;&#20248;&#21183;&#20197;&#25552;&#21319;&#20915;&#31574;&#33021;&#21147;&#12290;&#20851;&#38190;&#30340;&#19968;&#28857;&#26159;LATS&#20351;&#29992;&#19968;&#20010;&#20855;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#29615;&#22659;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#21152;&#28145;&#24605;&#29087;&#34385;&#21644;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#26426;&#21046;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#22312;&#32534;&#31243;&#12289;HotPotQA&#21644;WebShop&#31561;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;LATS&#22312;&#25512;&#29702;&#21644;&#34892;&#21160;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#32534;&#31243;&#26041;&#38754;&#65292;LATS&#23454;&#29616;&#20102;94.4%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) have demonstrated impressive performance on a range of decision-making tasks, they rely on simple acting processes and fall short of broad deployment as autonomous agents. We introduce LATS (Language Agent Tree Search), a general framework that synergizes the capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration from Monte Carlo tree search in model-based reinforcement learning, LATS employs LLMs as agents, value functions, and optimizers, repurposing their latent strengths for enhanced decision-making. What is crucial in this method is the use of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that moves beyond the limitations of existing techniques. Our experimental evaluation across diverse domains, such as programming, HotPotQA, and WebShop, illustrates the applicability of LATS for both reasoning and acting. In particular, LATS achieves 94.4\% for programming on Hu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30446;&#26631;&#26159;&#23454;&#29616;&#21270;&#23398;&#21160;&#21147;&#23398;&#24615;&#36136;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36229;&#20986;&#20998;&#24067;&#39044;&#27979;&#65292;&#36890;&#36807;&#30740;&#31350;OOD&#21160;&#21147;&#23398;&#24615;&#36136;&#39044;&#27979;&#30340;&#19977;&#20010;&#23618;&#27425;&#65292;&#25581;&#31034;&#38382;&#39064;&#30340;&#29420;&#29305;&#26041;&#38754;&#65292;&#24182;&#21019;&#24314;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#21453;&#24212;&#39044;&#27979;&#26041;&#27861;&#21644;&#21160;&#21147;&#23398;&#24615;&#36136;&#39044;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.03152</link><description>&lt;p&gt;
&#23454;&#29616;&#21270;&#23398;&#21160;&#21147;&#23398;&#24615;&#36136;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36229;&#20986;&#20998;&#24067;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards out-of-distribution generalizable predictions of chemical kinetics properties. (arXiv:2310.03152v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30446;&#26631;&#26159;&#23454;&#29616;&#21270;&#23398;&#21160;&#21147;&#23398;&#24615;&#36136;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36229;&#20986;&#20998;&#24067;&#39044;&#27979;&#65292;&#36890;&#36807;&#30740;&#31350;OOD&#21160;&#21147;&#23398;&#24615;&#36136;&#39044;&#27979;&#30340;&#19977;&#20010;&#23618;&#27425;&#65292;&#25581;&#31034;&#38382;&#39064;&#30340;&#29420;&#29305;&#26041;&#38754;&#65292;&#24182;&#21019;&#24314;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#21453;&#24212;&#39044;&#27979;&#26041;&#27861;&#21644;&#21160;&#21147;&#23398;&#24615;&#36136;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#20272;&#35745;&#21270;&#23398;&#21160;&#21147;&#23398;&#24615;&#36136;&#26041;&#38754;&#25214;&#21040;&#20102;&#24212;&#29992;&#12290;&#38543;&#30528;&#36890;&#36807;&#8220;AI4drug discovery&#8221;&#37492;&#23450;&#30340;&#33647;&#29289;&#20998;&#23376;&#30340;&#32047;&#31215;&#65292;&#19979;&#19968;&#20010;&#24613;&#38656;&#35299;&#20915;&#30340;&#38382;&#39064;&#26159;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#35774;&#35745;&#39640;&#36890;&#37327;&#21270;&#23398;&#21512;&#25104;&#36807;&#31243;&#65292;&#20197;&#20272;&#35745;&#26410;&#35265;&#21453;&#24212;&#20013;&#26410;&#30693;&#20998;&#23376;&#30340;&#24615;&#36136;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#20855;&#22791;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#26222;&#36866;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;OOD&#21160;&#21147;&#23398;&#24615;&#36136;&#39044;&#27979;&#20998;&#20026;&#19977;&#20010;&#23618;&#27425;&#65288;&#32467;&#26500;&#12289;&#26465;&#20214;&#21644;&#26426;&#21046;&#65289;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#38382;&#39064;&#30340;&#29420;&#29305;&#26041;&#38754;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#65292;&#23545;OOD&#26465;&#20214;&#19979;&#30340;&#21453;&#24212;&#39044;&#27979;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20197;&#21450;&#21160;&#21147;&#23398;&#24615;&#36136;&#39044;&#27979;&#38382;&#39064;&#20013;&#30340;&#26368;&#20808;&#36827;&#30340;&#22270;&#24418;OOD&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;OOD&#21160;&#21147;&#23398;&#24615;&#36136;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;...
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) techniques have found applications in estimating chemical kinetics properties. With the accumulated drug molecules identified through "AI4drug discovery", the next imperative lies in AI-driven design for high-throughput chemical synthesis processes, with the estimation of properties of unseen reactions with unexplored molecules. To this end, the existing ML approaches for kinetics property prediction are required to be Out-Of-Distribution (OOD) generalizable. In this paper, we categorize the OOD kinetic property prediction into three levels (structure, condition, and mechanism), revealing unique aspects of such problems. Under this framework, we create comprehensive datasets to benchmark (1) the state-of-the-art ML approaches for reaction prediction in the OOD setting and (2) the state-of-the-art graph OOD methods in kinetics property prediction problems. Our results demonstrated the challenges and opportunities in OOD kinetics property prediction. Our datasets an
&lt;/p&gt;</description></item><item><title>FroSSL&#26159;&#19968;&#31181;&#22522;&#20110;Frobenius&#33539;&#25968;&#26368;&#23567;&#21270;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#21327;&#26041;&#24046;Frobenius&#33539;&#25968;&#26469;&#36991;&#20813;&#20449;&#24687;&#23849;&#28291;&#65292;&#21516;&#26102;&#36890;&#36807;&#26368;&#23567;&#21270;&#22343;&#26041;&#24046;&#26469;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#30340;&#19981;&#21464;&#24615;&#65292;&#30456;&#27604;&#20854;&#20182;SSL&#26041;&#27861;&#65292;FroSSL&#25910;&#25947;&#26356;&#24555;&#65292;&#24182;&#19988;&#36825;&#31181;&#24555;&#36895;&#25910;&#25947;&#26159;&#30001;&#20110;FroSSL&#24433;&#21709;&#23884;&#20837;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#25152;&#33268;&#12290;</title><link>http://arxiv.org/abs/2310.02903</link><description>&lt;p&gt;
FroSSL: &#22522;&#20110;Frobenius&#33539;&#25968;&#26368;&#23567;&#21270;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FroSSL: Frobenius Norm Minimization for Self-Supervised Learning. (arXiv:2310.02903v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02903
&lt;/p&gt;
&lt;p&gt;
FroSSL&#26159;&#19968;&#31181;&#22522;&#20110;Frobenius&#33539;&#25968;&#26368;&#23567;&#21270;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#21327;&#26041;&#24046;Frobenius&#33539;&#25968;&#26469;&#36991;&#20813;&#20449;&#24687;&#23849;&#28291;&#65292;&#21516;&#26102;&#36890;&#36807;&#26368;&#23567;&#21270;&#22343;&#26041;&#24046;&#26469;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#30340;&#19981;&#21464;&#24615;&#65292;&#30456;&#27604;&#20854;&#20182;SSL&#26041;&#27861;&#65292;FroSSL&#25910;&#25947;&#26356;&#24555;&#65292;&#24182;&#19988;&#36825;&#31181;&#24555;&#36895;&#25910;&#25947;&#26159;&#30001;&#20110;FroSSL&#24433;&#21709;&#23884;&#20837;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#25152;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26159;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#34920;&#31034;&#23398;&#20064;&#33539;&#24335;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#21487;&#20998;&#31867;&#20026;&#26679;&#26412;&#23545;&#27604;&#12289;&#32500;&#24230;&#23545;&#27604;&#25110;&#38750;&#23545;&#31216;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#27599;&#20010;&#23478;&#26063;&#37117;&#26377;&#33258;&#24049;&#30340;&#26041;&#27861;&#26469;&#36991;&#20813;&#20449;&#24687;&#23849;&#28291;&#12290;&#34429;&#28982;&#32500;&#24230;&#23545;&#27604;&#26041;&#27861;&#25910;&#25947;&#21040;&#19982;&#26679;&#26412;&#23545;&#27604;&#26041;&#27861;&#30456;&#20284;&#30340;&#35299;&#65292;&#20294;&#21487;&#20197;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#19968;&#20123;&#26041;&#27861;&#38656;&#35201;&#26356;&#22810;&#30340;&#35757;&#32451;&#36845;&#20195;&#25165;&#33021;&#25910;&#25947;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30446;&#26631;&#20989;&#25968;FroSSL&#65292;&#23427;&#22312;&#23884;&#20837;&#24402;&#19968;&#21270;&#26041;&#38754;&#26082;&#26159;&#26679;&#26412;&#23545;&#27604;&#21448;&#26159;&#32500;&#24230;&#23545;&#27604;&#12290;FroSSL&#36890;&#36807;&#26368;&#23567;&#21270;&#21327;&#26041;&#24046;Frobenius&#33539;&#25968;&#26469;&#36991;&#20813;&#23849;&#28291;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#22343;&#26041;&#24046;&#26469;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#30340;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;FroSSL&#27604;&#20854;&#20182;&#21508;&#31181;SSL&#26041;&#27861;&#26356;&#24555;&#22320;&#25910;&#25947;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#25903;&#25345;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26356;&#24555;&#30340;&#25910;&#25947;&#26159;&#30001;&#20110;FroSSL&#23545;&#23884;&#20837;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) is an increasingly popular paradigm for representation learning. Recent methods can be classified as sample-contrastive, dimension-contrastive, or asymmetric network-based, with each family having its own approach to avoiding informational collapse. While dimension-contrastive methods converge to similar solutions as sample-contrastive methods, it can be empirically shown that some methods require more epochs of training to converge. Motivated by closing this divide, we present the objective function FroSSL which is both sample- and dimension-contrastive up to embedding normalization. FroSSL works by minimizing covariance Frobenius norms for avoiding collapse and minimizing mean-squared error for augmentation invariance. We show that FroSSL converges more quickly than a variety of other SSL methods and provide theoretical and empirical support that this faster convergence is due to how FroSSL affects the eigenvalues of the embedding covariance matrices. W
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Stackelberg&#39550;&#39542;&#21592;&#27169;&#22411;&#30340;&#25345;&#32493;&#25919;&#31574;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#38381;&#29615;&#33258;&#21160;&#39550;&#39542;&#20013;&#24341;&#20837;&#32972;&#26223;&#36710;&#36742;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20043;&#38388;&#30340;&#21338;&#24328;&#24335;&#20132;&#20114;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;&#38271;&#23614;&#20998;&#24067;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.14235</link><description>&lt;p&gt;
Stackelberg&#39550;&#39542;&#21592;&#27169;&#22411;&#29992;&#20110;&#22522;&#20110;&#22330;&#26223;&#30340;&#38381;&#29615;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#25345;&#32493;&#25919;&#31574;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Stackelberg Driver Model for Continual Policy Improvement in Scenario-Based Closed-Loop Autonomous Driving. (arXiv:2309.14235v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14235
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Stackelberg&#39550;&#39542;&#21592;&#27169;&#22411;&#30340;&#25345;&#32493;&#25919;&#31574;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#38381;&#29615;&#33258;&#21160;&#39550;&#39542;&#20013;&#24341;&#20837;&#32972;&#26223;&#36710;&#36742;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20043;&#38388;&#30340;&#21338;&#24328;&#24335;&#20132;&#20114;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;&#38271;&#23614;&#20998;&#24067;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#30340;&#37096;&#32626;&#38754;&#20020;&#30528;&#22256;&#38590;&#65292;&#22240;&#20026;&#38271;&#23614;&#20998;&#24067;&#30340;&#39550;&#39542;&#22330;&#26223;&#20013;&#23384;&#22312;&#32597;&#35265;&#20294;&#20851;&#38190;&#30340;&#36793;&#38469;&#24773;&#20917;&#65292;&#36825;&#20250;&#23545;&#23427;&#20204;&#30340;&#25972;&#20307;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#23545;&#25239;&#24615;&#29983;&#25104;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#19968;&#31867;&#26377;&#25928;&#30340;&#36884;&#24452;&#65292;&#29992;&#20110;&#21512;&#25104;AV&#27979;&#35797;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29983;&#25104;&#30340;&#22330;&#26223;&#36890;&#24120;&#34987;&#29992;&#20110;AV&#35757;&#32451;&#30340;&#26426;&#20250;&#26377;&#38480;&#65292;&#36896;&#25104;&#20102;&#25345;&#32493;AV&#25919;&#31574;&#25913;&#36827;&#30340;&#28508;&#21147;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#65292;&#21516;&#26102;&#20063;&#32570;&#20047;&#38381;&#29615;&#35774;&#35745;&#26469;&#23454;&#29616;&#36825;&#19968;&#25913;&#36827;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;Stackelberg&#39550;&#39542;&#21592;&#27169;&#22411;&#65288;SDM&#65289;&#36827;&#34892;&#35843;&#25972;&#65292;&#20197;&#20934;&#30830;&#25551;&#36848;&#36710;&#36742;&#20132;&#20114;&#21160;&#21147;&#23398;&#30340;&#23618;&#27425;&#24615;&#36136;&#65292;&#36890;&#36807;&#23558;&#32972;&#26223;&#36710;&#36742;&#65288;BVs&#65289;&#21644;AV&#22312;&#19968;&#31181;&#39034;&#24207;&#21338;&#24328;&#24335;&#30340;&#20132;&#20114;&#33539; Paradigm&#20869;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;&#36890;&#36807;AV&#20805;&#24403;&#39046;&#23548;&#32773;&#65292;BVs&#20316;&#20026;&#36861;&#38543;&#32773;&#65292;&#36825;&#31181;&#39046;&#23548;&#32773;-&#36861;&#38543;&#32773;&#27169;&#22411;&#30830;&#20445;&#20102;AV&#22987;&#32456;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deployment of autonomous vehicles (AVs) has faced hurdles due to the dominance of rare but critical corner cases within the long-tail distribution of driving scenarios, which negatively affects their overall performance. To address this challenge, adversarial generation methods have emerged as a class of efficient approaches to synthesize safety-critical scenarios for AV testing. However, these generated scenarios are often underutilized for AV training, resulting in the potential for continual AV policy improvement remaining untapped, along with a deficiency in the closed-loop design needed to achieve it. Therefore, we tailor the Stackelberg Driver Model (SDM) to accurately characterize the hierarchical nature of vehicle interaction dynamics, facilitating iterative improvement by engaging background vehicles (BVs) and AV in a sequential game-like interaction paradigm. With AV acting as the leader and BVs as followers, this leader-follower modeling ensures that AV would consistentl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#28145;&#24230;&#30693;&#35782;&#36861;&#36394;&#35270;&#20026;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#21457;&#29616;&#65292;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#35299;&#30721;&#22120;&#21487;&#20197;&#27604;DKT&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.12334</link><description>&lt;p&gt;
&#28145;&#24230;&#30693;&#35782;&#36861;&#36394;&#26159;&#19968;&#20010;&#38544;&#21547;&#30340;&#21160;&#24577;&#22810;&#32500;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deep Knowledge Tracing is an implicit dynamic multidimensional item response theory model. (arXiv:2309.12334v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#28145;&#24230;&#30693;&#35782;&#36861;&#36394;&#35270;&#20026;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#21457;&#29616;&#65292;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#35299;&#30721;&#22120;&#21487;&#20197;&#27604;DKT&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#26159;&#26681;&#25454;&#23398;&#29983;&#20808;&#21069;&#38382;&#39064;&#30340;&#34920;&#29616;&#26469;&#39044;&#27979;&#20854;&#22312;&#26032;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#36825;&#21487;&#20197;&#20316;&#20026;&#20248;&#21270;&#35780;&#20272;&#21644;&#23398;&#20064;&#30340;&#20808;&#21069;&#27493;&#39588;&#12290;&#28145;&#24230;&#30693;&#35782;&#36861;&#36394;&#65288;DKT&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#30693;&#35782;&#36861;&#36394;&#31454;&#20105;&#27169;&#22411;&#65292;&#21363;&#20351;&#19968;&#20123;&#26356;&#31616;&#21333;&#30340;&#27169;&#22411;&#21487;&#33021;&#19982;&#20854;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#20294;&#23545;&#20110;&#20026;&#20160;&#20040;DKT&#33021;&#22815;&#22914;&#27492;&#25104;&#21151;&#30340;&#20102;&#35299;&#36824;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#28145;&#24230;&#30693;&#35782;&#36861;&#36394;&#35270;&#20026;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#12290;&#36825;&#20010;&#35266;&#28857;&#19981;&#20165;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#24615;&#33021;&#12289;&#31616;&#21333;&#24615;&#25110;&#34920;&#36798;&#24615;&#26041;&#38754;&#25552;&#20986;&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#36824;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#25171;&#24320;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#23567;&#22411;&#21644;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#35299;&#30721;&#22120;&#65292;&#21487;&#33021;&#27604;DKT&#20351;&#29992;&#30340;&#35299;&#30721;&#22120;&#21442;&#25968;&#26356;&#23569;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge tracing consists in predicting the performance of some students on new questions given their performance on previous questions, and can be a prior step to optimizing assessment and learning. Deep knowledge tracing (DKT) is a competitive model for knowledge tracing relying on recurrent neural networks, even if some simpler models may match its performance. However, little is known about why DKT works so well. In this paper, we frame deep knowledge tracing as a encoderdecoder architecture. This viewpoint not only allows us to propose better models in terms of performance, simplicity or expressivity but also opens up promising avenues for future research directions. In particular, we show on several small and large datasets that a simpler decoder, with possibly fewer parameters than the one used by DKT, can predict student performance better.
&lt;/p&gt;</description></item><item><title>LongLoRA&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#12290;&#23427;&#36890;&#36807;&#31232;&#30095;&#30340;&#23616;&#37096;&#27880;&#24847;&#21147;&#23454;&#29616;&#31934;&#32454;&#35843;&#25972;&#65292;&#24182;&#20351;&#29992;&#31227;&#21160;&#30701;&#27880;&#24847;&#21147;&#26377;&#25928;&#23454;&#29616;&#19978;&#19979;&#25991;&#25193;&#23637;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12307</link><description>&lt;p&gt;
LongLoRA: &#39640;&#25928;&#30340;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31934;&#32454;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models. (arXiv:2309.12307v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12307
&lt;/p&gt;
&lt;p&gt;
LongLoRA&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#12290;&#23427;&#36890;&#36807;&#31232;&#30095;&#30340;&#23616;&#37096;&#27880;&#24847;&#21147;&#23454;&#29616;&#31934;&#32454;&#35843;&#25972;&#65292;&#24182;&#20351;&#29992;&#31227;&#21160;&#30701;&#27880;&#24847;&#21147;&#26377;&#25928;&#23454;&#29616;&#19978;&#19979;&#25991;&#25193;&#23637;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#8212;&#8212;LongLoRA&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#12290;&#36890;&#24120;&#65292;&#20351;&#29992;&#38271;&#19978;&#19979;&#25991;&#22823;&#23567;&#35757;&#32451;LLM&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;GPU&#36164;&#28304;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#26041;&#38754;&#21152;&#24555;&#20102;LLM&#30340;&#19978;&#19979;&#25991;&#25193;&#23637;&#12290;&#19968;&#26041;&#38754;&#65292;&#23613;&#31649;&#25512;&#29702;&#36807;&#31243;&#20013;&#38656;&#35201;&#31264;&#23494;&#30340;&#20840;&#23616;&#27880;&#24847;&#21147;&#65292;&#20294;&#27169;&#22411;&#30340;&#31934;&#32454;&#35843;&#25972;&#21487;&#20197;&#36890;&#36807;&#31232;&#30095;&#30340;&#23616;&#37096;&#27880;&#24847;&#21147;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#23436;&#25104;&#12290;&#25152;&#25552;&#20986;&#30340;&#31227;&#21160;&#30701;&#27880;&#24847;&#21147;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#19978;&#19979;&#25991;&#30340;&#25193;&#23637;&#65292;&#22312;&#19982;&#20351;&#29992;&#20256;&#32479;&#27880;&#24847;&#21147;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#26102;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21487;&#20197;&#22312;&#35757;&#32451;&#20013;&#21482;&#29992;&#20004;&#34892;&#20195;&#30721;&#23454;&#29616;&#65292;&#22312;&#25512;&#29702;&#20013;&#26159;&#21487;&#36873;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#21442;&#25968;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shift short attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-effici
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;EMT&#26041;&#27861;&#26469;&#35780;&#20272;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;&#27169;&#22411;&#37117;&#26080;&#27861;&#20445;&#25345;&#19982;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#24494;&#35843;&#38454;&#27573;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.10313</link><description>&lt;p&gt;
&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#36827;&#34892;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating the Catastrophic Forgetting in Multimodal Large Language Models. (arXiv:2309.10313v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;EMT&#26041;&#27861;&#26469;&#35780;&#20272;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;&#27169;&#22411;&#37117;&#26080;&#27861;&#20445;&#25345;&#19982;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#24494;&#35843;&#38454;&#27573;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;GPT4&#30340;&#25104;&#21151;&#20043;&#21518;&#65292;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#20391;&#37325;&#20110;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;LLM&#21644;&#35270;&#35273;&#27169;&#22411;&#26469;&#24320;&#21457;&#36890;&#29992;&#30340;LLM&#12290;&#28982;&#32780;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21363;&#24494;&#35843;&#27169;&#22411;&#26080;&#27861;&#20445;&#25345;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#20173;&#28982;&#26159;&#22810;&#27169;&#24577;LLM&#65288;MLLM&#65289;&#20013;&#30340;&#19968;&#20010;&#22266;&#26377;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;EMT&#65306;&#29992;&#20110;&#35780;&#20272;MLLM&#20013;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;MLLM&#20316;&#20026;&#19968;&#20010;&#22270;&#20687;&#20998;&#31867;&#22120;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#24212;&#29992;EMT&#26469;&#35780;&#20272;&#20960;&#20010;&#24320;&#28304;&#30340;&#24494;&#35843;MLLM&#65292;&#24182;&#21457;&#29616;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;MLLM&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#26080;&#27861;&#20445;&#25345;&#19982;&#20182;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32487;&#32493;&#24494;&#35843;LLaVA&#65292;&#19968;&#31181;MLLM&#65292;&#24182;&#21033;&#29992;EMT&#26469;&#35780;&#20272;&#25972;&#20010;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#30340;&#24494;&#35843;&#38454;&#27573;&#26159;&#20851;&#38190;&#30340;&#65292;&#36807;&#26089;&#20572;&#27490;&#24494;&#35843;&#21487;&#33021;&#23548;&#33268;&#20302;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;GPU&#21644;CPU&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;GPU&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#20855;&#26377;&#26356;&#20302;&#30340;&#36816;&#34892;&#26102;&#38388;&#65292;&#23545;&#20110;&#36739;&#31616;&#21333;&#30340;&#32593;&#32476;&#65292;GPU&#24182;&#27809;&#26377;&#22826;&#22810;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.02521</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;CPU&#21644;GPU&#24615;&#33021;&#20998;&#26512;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of CPU and GPU Profiling for Deep Learning Models. (arXiv:2309.02521v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;GPU&#21644;CPU&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;GPU&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#20855;&#26377;&#26356;&#20302;&#30340;&#36816;&#34892;&#26102;&#38388;&#65292;&#23545;&#20110;&#36739;&#31616;&#21333;&#30340;&#32593;&#32476;&#65292;GPU&#24182;&#27809;&#26377;&#22826;&#22810;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#22825;&#65292;&#28145;&#24230;&#23398;&#20064;(DL)&#21644;&#26426;&#22120;&#23398;&#20064;(ML)&#24212;&#29992;&#27491;&#22312;&#24555;&#36895;&#22686;&#21152;&#12290;&#22823;&#37327;&#30340;&#25968;&#25454;&#36890;&#36807;&#20114;&#32852;&#32593;&#29983;&#25104;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;ML&#21644;DL&#31639;&#27861;&#26469;&#24471;&#20986;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#12290;&#30828;&#20214;&#36164;&#28304;&#21644;&#24320;&#28304;&#24211;&#20351;&#24471;&#23454;&#29616;&#36825;&#20123;&#31639;&#27861;&#21464;&#24471;&#23481;&#26131;&#12290;Tensorflow&#21644;PyTorch&#26159;&#23454;&#29616;ML&#39033;&#30446;&#30340;&#39046;&#20808;&#26694;&#26550;&#20043;&#19968;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#36319;&#36394;&#22312;GPU&#21644;CPU&#19978;&#25191;&#34892;&#30340;&#25805;&#20316;&#65292;&#20197;&#20998;&#26512;&#36164;&#28304;&#20998;&#37197;&#21644;&#28040;&#32791;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;PyTorch&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;CPU&#21644;GPU&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#20998;&#37197;&#24773;&#20917;&#12290;&#35813;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;CPU&#30456;&#27604;&#65292;GPU&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#20855;&#26377;&#26356;&#20302;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;&#23545;&#20110;&#19968;&#20010;&#36739;&#31616;&#21333;&#30340;&#32593;&#32476;&#65292;GPU&#22312;CPU&#19978;&#24182;&#27809;&#26377;&#22826;&#22810;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning(DL) and Machine Learning(ML) applications are rapidly increasing in recent days. Massive amounts of data are being generated over the internet which can derive meaningful results by the use of ML and DL algorithms. Hardware resources and open-source libraries have made it easy to implement these algorithms. Tensorflow and Pytorch are one of the leading frameworks for implementing ML projects. By using those frameworks, we can trace the operations executed on both GPU and CPU to analyze the resource allocations and consumption. This paper presents the time and memory allocation of CPU and GPU while training deep neural networks using Pytorch. This paper analysis shows that GPU has a lower running time as compared to CPU for deep neural networks. For a simpler network, there are not many significant improvements in GPU over the CPU.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#32467;&#26500;&#30340;&#22270;&#20449;&#21495;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#22788;&#29702;&#39640;&#38454;&#20132;&#20114;&#12289;&#32858;&#31867;&#21644;&#28040;&#24687;&#20256;&#36882;&#31561;&#27493;&#39588;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#22312;&#21387;&#32553;&#20449;&#21495;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#37325;&#24314;&#35823;&#24046;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#21644;&#21033;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.11068</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#32467;&#26500;&#30340;&#22270;&#20449;&#21495;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Topological Graph Signal Compression. (arXiv:2308.11068v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11068
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#32467;&#26500;&#30340;&#22270;&#20449;&#21495;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#22788;&#29702;&#39640;&#38454;&#20132;&#20114;&#12289;&#32858;&#31867;&#21644;&#28040;&#24687;&#20256;&#36882;&#31561;&#27493;&#39588;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#22312;&#21387;&#32553;&#20449;&#21495;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#37325;&#24314;&#35823;&#24046;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#21644;&#21033;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#30340;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#65288;TDL&#65289;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#22320;&#22788;&#29702;&#39640;&#38454;&#20132;&#20114;&#65292;&#36229;&#36234;&#30001;&#22270;&#34920;&#31034;&#23450;&#20041;&#30340;&#25104;&#23545;&#20851;&#31995;&#21644;&#23616;&#37096;&#37051;&#22495;&#65292;&#20174;&#32780;&#25193;&#23637;&#24403;&#21069;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;TDL&#30340;&#22270;&#20449;&#21495;&#21387;&#32553;&#26041;&#27861;&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#65306;&#39318;&#20808;&#65292;&#22522;&#20110;&#21407;&#22987;&#20449;&#21495;&#25512;&#26029;&#20986;&#19981;&#30456;&#20132;&#30340;&#39640;&#38454;&#32467;&#26500;&#65292;&#36890;&#36807;&#23558;N&#20010;&#25968;&#25454;&#28857;&#32858;&#31867;&#25104;K&#20010;&#38598;&#21512;&#65307;&#28982;&#21518;&#65292;&#22522;&#20110;&#25299;&#25169;&#21551;&#31034;&#30340;&#28040;&#24687;&#20256;&#36882;&#22312;&#36825;&#20123;&#22810;&#20803;&#32032;&#38598;&#21512;&#20013;&#33719;&#24471;&#20449;&#21495;&#30340;&#21387;&#32553;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#21387;&#32553;&#26469;&#33258;&#20004;&#20010;&#30495;&#23454;&#30340;&#20114;&#32852;&#32593;&#26381;&#21153;&#25552;&#20379;&#21830;&#32593;&#32476;&#25968;&#25454;&#38598;&#30340;&#26102;&#38388;&#38142;&#36335;&#20449;&#21495;&#26102;&#65292;&#27604;&#26631;&#20934;&#30340;GNN&#21644;&#21069;&#39304;&#26550;&#26500;&#20855;&#26377;&#26356;&#22909;&#30340;&#37325;&#24314;&#35823;&#24046;&#8212;&#8212;&#22312;&#25152;&#26377;&#35780;&#20272;&#22330;&#26223;&#20013;&#65292;&#37325;&#24314;&#35823;&#24046;&#25552;&#39640;&#20102;&#20174;30%&#21040;90%&#12290;&#36825;&#34920;&#26126;&#23427;&#26356;&#22909;&#22320;&#25429;&#25417;&#21644;&#21033;&#29992;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently emerged Topological Deep Learning (TDL) methods aim to extend current Graph Neural Networks (GNN) by naturally processing higher-order interactions, going beyond the pairwise relations and local neighborhoods defined by graph representations. In this paper we propose a novel TDL-based method for compressing signals over graphs, consisting in two main steps: first, disjoint sets of higher-order structures are inferred based on the original signal --by clustering $N$ datapoints into $K\ll N$ collections; then, a topological-inspired message passing gets a compressed representation of the signal within those multi-element sets. Our results show that our framework improves both standard GNN and feed-forward architectures in compressing temporal link-based signals from two real-word Internet Service Provider Networks' datasets --from $30\%$ up to $90\%$ better reconstruction errors across all evaluation scenarios--, suggesting that it better captures and exploits spatial and tempor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#27979;&#22320;&#32447;&#21453;&#21521;&#20256;&#25773;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24515;&#33039;&#30005;&#29983;&#29702;&#27169;&#22411;&#20013;&#30340;&#36870;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;GPU&#21152;&#36895;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#20248;&#21270;&#26354;&#38754;&#27874;&#26041;&#31243;&#30340;&#21442;&#25968;&#26469;&#37325;&#24314;&#32473;&#23450;&#30340;&#24515;&#30005;&#22270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27979;&#22320;&#32447;&#21453;&#21521;&#20256;&#25773;&#21487;&#20197;&#20197;&#39640;&#20934;&#30830;&#24615;&#37325;&#24314;&#27169;&#25311;&#24515;&#33039;&#28608;&#27963;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#31215;&#26497;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.08410</link><description>&lt;p&gt;
&#20174;&#34920;&#38754;&#24515;&#30005;&#22270;&#20013;&#25968;&#23383;&#21452;&#29983;&#24515;&#33039;&#30005;&#29983;&#29702;&#27169;&#22411;&#65306;&#19968;&#31181;&#27979;&#22320;&#32447;&#21453;&#21521;&#20256;&#25773;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Digital twinning of cardiac electrophysiology models from the surface ECG: a geodesic backpropagation approach. (arXiv:2308.08410v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#27979;&#22320;&#32447;&#21453;&#21521;&#20256;&#25773;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24515;&#33039;&#30005;&#29983;&#29702;&#27169;&#22411;&#20013;&#30340;&#36870;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;GPU&#21152;&#36895;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#20248;&#21270;&#26354;&#38754;&#27874;&#26041;&#31243;&#30340;&#21442;&#25968;&#26469;&#37325;&#24314;&#32473;&#23450;&#30340;&#24515;&#30005;&#22270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27979;&#22320;&#32447;&#21453;&#21521;&#20256;&#25773;&#21487;&#20197;&#20197;&#39640;&#20934;&#30830;&#24615;&#37325;&#24314;&#27169;&#25311;&#24515;&#33039;&#28608;&#27963;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#31215;&#26497;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26354;&#38754;&#27874;&#26041;&#31243;&#24050;&#25104;&#20026;&#31934;&#30830;&#39640;&#25928;&#24314;&#27169;&#24515;&#33039;&#30005;&#28608;&#27963;&#30340;&#19981;&#21487;&#25110;&#32570;&#24037;&#20855;&#12290;&#36890;&#36807;&#21305;&#37197;&#20020;&#24202;&#35760;&#24405;&#30340;&#26354;&#38754;&#27874;&#30005;&#22270;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#65292;&#21407;&#21017;&#19978;&#21487;&#20197;&#20197;&#32431;&#38750;&#20405;&#20837;&#30340;&#26041;&#24335;&#26500;&#24314;&#24739;&#32773;&#29305;&#24322;&#24615;&#30340;&#24515;&#33039;&#30005;&#29983;&#29702;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25311;&#21512;&#36807;&#31243;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#27979;&#22320;&#32447;&#21453;&#21521;&#20256;&#25773;&#65288;Geodesic-BP&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#36870;&#26354;&#38754;&#27874;&#38382;&#39064;&#12290;Geodesic-BP&#38750;&#24120;&#36866;&#29992;&#20110;&#25903;&#25345;GPU&#21152;&#36895;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20248;&#21270;&#26354;&#38754;&#27874;&#26041;&#31243;&#30340;&#21442;&#25968;&#20197;&#37325;&#29616;&#32473;&#23450;&#30340;ECG&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Geodesic-BP&#22312;&#21512;&#25104;&#27979;&#35797;&#26696;&#20363;&#20013;&#65292;&#22312;&#24314;&#27169;&#19981;&#20934;&#30830;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#20197;&#39640;&#20934;&#30830;&#24615;&#37325;&#26500;&#27169;&#25311;&#24515;&#33039;&#28608;&#27963;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#31639;&#27861;&#24212;&#29992;&#20110;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#20820;&#27169;&#22411;&#25968;&#25454;&#38598;&#65292;&#24182;&#21462;&#24471;&#20102;&#38750;&#24120;&#31215;&#26497;&#30340;&#32467;&#26524;&#12290;&#32771;&#34385;&#21040;&#26410;&#26469;&#20010;&#20307;&#21270;&#21307;&#30103;&#30340;&#21457;&#23637;&#65292;Geodesic-BP&#20855;&#26377;&#33391;&#22909;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
The eikonal equation has become an indispensable tool for modeling cardiac electrical activation accurately and efficiently. In principle, by matching clinically recorded and eikonal-based electrocardiograms (ECGs), it is possible to build patient-specific models of cardiac electrophysiology in a purely non-invasive manner. Nonetheless, the fitting procedure remains a challenging task. The present study introduces a novel method, Geodesic-BP, to solve the inverse eikonal problem. Geodesic-BP is well-suited for GPU-accelerated machine learning frameworks, allowing us to optimize the parameters of the eikonal equation to reproduce a given ECG. We show that Geodesic-BP can reconstruct a simulated cardiac activation with high accuracy in a synthetic test case, even in the presence of modeling inaccuracies. Furthermore, we apply our algorithm to a publicly available dataset of a rabbit model, with very positive results. Given the future shift towards personalized medicine, Geodesic-BP has t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;&#22522;&#20110;&#23618;&#27425;&#20869;&#23384;&#22238;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#26088;&#22312;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#12290;&#25552;&#20986;&#20102;Miro&#65292;&#19968;&#20010;&#36890;&#36807;&#21160;&#24577;&#37197;&#32622;&#25345;&#32493;&#23398;&#20064;&#31995;&#32479;&#30340;&#26032;&#39062;&#31995;&#32479;&#36816;&#34892;&#26102;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#26174;&#31034;Miro&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.06053</link><description>&lt;p&gt;
&#22312;&#20869;&#23384;&#23618;&#27425;&#32467;&#26500;&#19978;&#20855;&#26377;MiRo&#30340;&#25104;&#26412;&#25928;&#30410;&#30340;&#35774;&#22791;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cost-effective On-device Continual Learning over Memory Hierarchy with Miro. (arXiv:2308.06053v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06053
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;&#22522;&#20110;&#23618;&#27425;&#20869;&#23384;&#22238;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#26088;&#22312;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#12290;&#25552;&#20986;&#20102;Miro&#65292;&#19968;&#20010;&#36890;&#36807;&#21160;&#24577;&#37197;&#32622;&#25345;&#32493;&#23398;&#20064;&#31995;&#32479;&#30340;&#26032;&#39062;&#31995;&#32479;&#36816;&#34892;&#26102;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#26174;&#31034;Miro&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26159;&#20174;&#25345;&#32493;&#30340;&#20219;&#21153;&#27969;&#20013;&#36880;&#27493;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#20026;&#20102;&#35760;&#20303;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#23558;&#26087;&#26679;&#26412;&#23384;&#20648;&#22312;&#19968;&#20010;&#20869;&#23384;&#23618;&#27425;&#32467;&#26500;&#20013;&#65292;&#24182;&#22312;&#26032;&#20219;&#21153;&#21040;&#26469;&#26102;&#36827;&#34892;&#22238;&#25918;&#12290;&#37319;&#29992;&#25345;&#32493;&#23398;&#20064;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#36793;&#32536;&#35774;&#22791;&#36890;&#24120;&#23545;&#33021;&#28304;&#25935;&#24863;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#19981;&#25439;&#23475;&#33021;&#28304;&#25928;&#29575;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#39640;&#27169;&#22411;&#20934;&#30830;&#24230;&#65292;&#21363;&#25104;&#26412;&#25928;&#30410;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;&#22522;&#20110;&#23618;&#27425;&#20869;&#23384;&#22238;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#20197;&#33719;&#24471;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Miro&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#36816;&#34892;&#26102;&#65292;&#36890;&#36807;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#36164;&#28304;&#29366;&#24577;&#21160;&#24577;&#37197;&#32622;&#25345;&#32493;&#23398;&#20064;&#31995;&#32479;&#65292;&#20174;&#32780;&#23558;&#25105;&#20204;&#30340;&#35265;&#35299;&#31934;&#30830;&#22320;&#25972;&#21512;&#21040;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#25104;&#26412;&#25928;&#30410;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;Miro&#36824;&#23545;&#24102;&#26377;&#26126;&#30830;&#20934;&#30830;&#24230;-&#33021;&#37327;&#24179;&#34913;&#30340;&#21442;&#25968;&#36827;&#34892;&#22312;&#32447;&#20998;&#26512;&#65292;&#24182;&#20197;&#20302;&#24320;&#38144;&#22320;&#36866;&#24212;&#26368;&#20339;&#20540;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#26174;&#31034;Miro&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) trains NN models incrementally from a continuous stream of tasks. To remember previously learned knowledge, prior studies store old samples over a memory hierarchy and replay them when new tasks arrive. Edge devices that adopt CL to preserve data privacy are typically energy-sensitive and thus require high model accuracy while not compromising energy efficiency, i.e., cost-effectiveness. Our work is the first to explore the design space of hierarchical memory replay-based CL to gain insights into achieving cost-effectiveness on edge devices. We present Miro, a novel system runtime that carefully integrates our insights into the CL framework by enabling it to dynamically configure the CL system based on resource states for the best cost-effectiveness. To reach this goal, Miro also performs online profiling on parameters with clear accuracy-energy trade-offs and adapts to optimal values with low overhead. Extensive evaluations show that Miro significantly outperfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25308;&#21344;&#24237;&#25915;&#20987;&#19979;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#23545;&#25163;&#34892;&#20026;&#12290;&#22312;&#25308;&#21344;&#24237;&#21644;&#24694;&#24847;&#23545;&#25163;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#23613;&#31649;&#21487;&#20197;&#25511;&#21046;&#32447;&#24615;&#23545;&#25163;&#36951;&#25022;&#30340;&#24120;&#25968;&#65292;&#20294;&#20998;&#24067;&#24335;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#21482;&#33021;&#23454;&#29616;&#32447;&#24615;&#23545;&#25163;&#36951;&#25022;&#19978;&#30028;&#12290;&#28982;&#32780;&#65292;&#22312;&#29615;&#22659;&#19981;&#23436;&#20840;&#23545;&#25239;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#27425;&#32447;&#24615;&#30340;&#38543;&#26426;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2307.07980</link><description>&lt;p&gt;
&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#65306;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#20013;&#24212;&#23545;&#23545;&#25163;&#30340;&#24694;&#24847;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Byzantine-Robust Distributed Online Learning: Taming Adversarial Participants in An Adversarial Environment. (arXiv:2307.07980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25308;&#21344;&#24237;&#25915;&#20987;&#19979;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#23545;&#25163;&#34892;&#20026;&#12290;&#22312;&#25308;&#21344;&#24237;&#21644;&#24694;&#24847;&#23545;&#25163;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#23613;&#31649;&#21487;&#20197;&#25511;&#21046;&#32447;&#24615;&#23545;&#25163;&#36951;&#25022;&#30340;&#24120;&#25968;&#65292;&#20294;&#20998;&#24067;&#24335;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#21482;&#33021;&#23454;&#29616;&#32447;&#24615;&#23545;&#25163;&#36951;&#25022;&#19978;&#30028;&#12290;&#28982;&#32780;&#65292;&#22312;&#29615;&#22659;&#19981;&#23436;&#20840;&#23545;&#25239;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#27425;&#32447;&#24615;&#30340;&#38543;&#26426;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25308;&#21344;&#24237;&#25915;&#20987;&#19979;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#12290;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#36890;&#24120;&#36890;&#36807;&#65288;&#23545;&#25163;&#30340;&#65289;&#36951;&#25022;&#26469;&#35780;&#20272;&#65292;&#22312;&#29615;&#22659;&#25552;&#20379;&#23545;&#25163;&#25439;&#22833;&#26102;&#35780;&#20272;&#19968;&#27493;&#20915;&#31574;&#30340;&#36136;&#37327;&#65292;&#32780;&#26399;&#26395;&#24471;&#21040;&#19968;&#20010;&#27425;&#32447;&#24615;&#30340;&#19978;&#30028;&#12290;&#20294;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;&#20351;&#29992;&#19968;&#31867;&#26368;&#20808;&#36827;&#30340;&#40065;&#26834;&#32858;&#21512;&#35268;&#21017;&#65292;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#21644;&#23384;&#22312;&#25308;&#21344;&#24237;&#21442;&#19982;&#32773;&#30340;&#24773;&#20917;&#19979;&#65292;&#20998;&#24067;&#24335;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#21482;&#33021;&#23454;&#29616;&#32447;&#24615;&#30340;&#23545;&#25163;&#36951;&#25022;&#19978;&#30028;&#65292;&#36825;&#26159;&#32039;&#23494;&#30340;&#12290;&#36825;&#26159;&#25308;&#21344;&#24237;&#25915;&#20987;&#30340;&#24517;&#28982;&#32467;&#26524;&#65292;&#23613;&#31649;&#25105;&#20204;&#21487;&#20197;&#23558;&#32447;&#24615;&#23545;&#25163;&#36951;&#25022;&#30340;&#24120;&#25968;&#25511;&#21046;&#22312;&#21512;&#29702;&#30340;&#27700;&#24179;&#19978;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#29615;&#22659;&#19981;&#26159;&#23436;&#20840;&#23545;&#25239;&#24615;&#30340;&#65292;&#21363;&#35802;&#23454;&#21442;&#19982;&#32773;&#30340;&#25439;&#22833;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#65288;i.i.d.&#65289;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#19982;&#21069;&#38754;&#25552;&#21040;&#30340;&#23545;&#25163;&#36951;&#25022;&#30456;&#21453;&#65292;&#21487;&#20197;&#23454;&#29616;&#27425;&#32447;&#24615;&#30340;&#38543;&#26426;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies distributed online learning under Byzantine attacks. The performance of an online learning algorithm is often characterized by (adversarial) regret, which evaluates the quality of one-step-ahead decision-making when an environment provides adversarial losses, and a sublinear bound is preferred. But we prove that, even with a class of state-of-the-art robust aggregation rules, in an adversarial environment and in the presence of Byzantine participants, distributed online gradient descent can only achieve a linear adversarial regret bound, which is tight. This is the inevitable consequence of Byzantine attacks, even though we can control the constant of the linear adversarial regret to a reasonable level. Interestingly, when the environment is not fully adversarial so that the losses of the honest participants are i.i.d. (independent and identically distributed), we show that sublinear stochastic regret, in contrast to the aforementioned adversarial regret, is possible
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;OOD&#27979;&#35797;&#22810;&#22270;&#20013;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21452;&#21487;&#20132;&#25442;&#24615;&#27010;&#24565;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#39044;&#27979;&#27169;&#24335;&#19988;&#21487;&#33021;&#20914;&#31361;&#30340;&#20851;&#31995;&#31867;&#22411;&#38598;&#21512;&#30340;&#23646;&#24615;&#22810;&#22270;&#12290;</title><link>http://arxiv.org/abs/2307.06046</link><description>&lt;p&gt;
&#20511;&#21161;&#26032;&#30340;&#20851;&#31995;&#31867;&#22411;&#21644;&#33410;&#28857;&#65292;&#20197;OOD&#22810;&#20219;&#21153;&#35270;&#35282;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
An OOD Multi-Task Perspective for Link Prediction with New Relation Types and Nodes. (arXiv:2307.06046v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;OOD&#27979;&#35797;&#22810;&#22270;&#20013;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21452;&#21487;&#20132;&#25442;&#24615;&#27010;&#24565;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#39044;&#27979;&#27169;&#24335;&#19988;&#21487;&#33021;&#20914;&#31361;&#30340;&#20851;&#31995;&#31867;&#22411;&#38598;&#21512;&#30340;&#23646;&#24615;&#22810;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#25512;&#26029;&#20855;&#26377;&#23646;&#24615;&#30340;&#22810;&#22270;&#20013;&#26032;&#27979;&#35797;&#22810;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#32570;&#22833;&#23646;&#24615;&#38142;&#25509;&#65288;&#20851;&#31995;&#65289;&#12290;&#20256;&#32479;&#30340;&#20851;&#31995;&#23398;&#20064;&#26041;&#27861;&#38754;&#20020;&#30528;&#23545;OOD&#27979;&#35797;&#22810;&#22270;&#30340;&#26377;&#38480;&#27867;&#21270;&#33021;&#21147;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#22810;&#22270;&#21253;&#21547;&#20102;&#35757;&#32451;&#20013;&#26410;&#35265;&#36807;&#30340;&#26032;&#33410;&#28857;&#21644;&#26032;&#20851;&#31995;&#31867;&#22411;&#12290;&#26368;&#36817;&#65292;&#39640;&#31561;&#20154;&#65288;2023&#65289;&#22312;&#25152;&#26377;&#20851;&#31995;&#31867;&#22411;&#20849;&#20139;&#30456;&#21516;&#32467;&#26500;&#39044;&#27979;&#27169;&#24335;&#65288;&#21333;&#20010;&#20219;&#21153;&#65289;&#30340;&#21807;&#19968;&#20551;&#35774;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21452;&#21487;&#20132;&#25442;&#24615;&#29702;&#35770;&#27010;&#24565;&#65288;&#29992;&#20110;&#33410;&#28857;&#21644;&#20851;&#31995;&#31867;&#22411;&#65289;&#26469;&#36827;&#34892;OOD&#38142;&#25509;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#19982;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#35774;&#35745;&#30340;&#65288;&#21333;&#20010;&#65289;&#21487;&#20132;&#25442;&#24615;&#65288;&#20165;&#29992;&#20110;&#33410;&#28857;&#65289;&#30456;&#21453;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#21452;&#21487;&#20132;&#25442;&#24615;&#27010;&#24565;&#25193;&#23637;&#21040;&#22810;&#20219;&#21153;&#21452;&#21487;&#20132;&#25442;&#24615;&#65292;&#20854;&#20013;&#25105;&#20204;&#23450;&#20041;&#20102;&#23646;&#24615;&#22810;&#22270;&#20013;&#30340;&#38142;&#25509;&#39044;&#27979;&#65292;&#36825;&#20123;&#22270;&#21487;&#33021;&#23545;&#19981;&#21516;&#30340;&#20851;&#31995;&#31867;&#22411;&#38598;&#21512;&#20855;&#26377;&#19981;&#21516;&#19988;&#21487;&#33021;&#20914;&#31361;&#30340;&#39044;&#27979;&#27169;&#24335;&#65288;&#22810;&#20010;&#20219;&#21153;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of inductive link prediction in (discrete) attributed multigraphs infers missing attributed links (relations) between nodes in new test multigraphs. Traditional relational learning methods face the challenge of limited generalization to OOD test multigraphs containing both novel nodes and novel relation types not seen in training. Recently, under the only assumption that all relation types share the same structural predictive patterns (single task), Gao et al. (2023) proposed an OOD link prediction method using the theoretical concept of double exchangeability (for nodes &amp; relation types), in contrast to the (single) exchangeability (only for nodes) used to design Graph Neural Networks (GNNs). In this work we further extend the double exchangeability concept to multi-task double exchangeability, where we define link prediction in attributed multigraphs that can have distinct and potentially conflicting predictive patterns for different sets of relation types (multiple tasks). 
&lt;/p&gt;</description></item><item><title>&#36817;&#26399;&#37327;&#23376;&#35774;&#22791;&#19978;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#30740;&#31350;&#20102;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25506;&#31350;&#20102;&#24403;&#21069;&#37327;&#23376;&#30828;&#20214;&#19978;&#30340;QML&#23454;&#29616;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#30340;&#25216;&#26415;&#12290;&#19982;&#32463;&#20856;&#23545;&#24212;&#29289;&#30456;&#27604;&#36739;&#65292;&#36825;&#20123;QML&#23454;&#29616;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.00908</link><description>&lt;p&gt;
&#36817;&#26399;&#37327;&#23376;&#35013;&#32622;&#19978;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;: &#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#25216;&#26415;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#29616;&#29366;
&lt;/p&gt;
&lt;p&gt;
Quantum Machine Learning on Near-Term Quantum Devices: Current State of Supervised and Unsupervised Techniques for Real-World Applications. (arXiv:2307.00908v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00908
&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#37327;&#23376;&#35774;&#22791;&#19978;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#30740;&#31350;&#20102;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25506;&#31350;&#20102;&#24403;&#21069;&#37327;&#23376;&#30828;&#20214;&#19978;&#30340;QML&#23454;&#29616;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#30340;&#25216;&#26415;&#12290;&#19982;&#32463;&#20856;&#23545;&#24212;&#29289;&#30456;&#27604;&#36739;&#65292;&#36825;&#20123;QML&#23454;&#29616;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#37327;&#23376;&#30828;&#20214;&#22312;&#36895;&#24230;&#12289;&#37327;&#23376;&#27604;&#29305;&#25968;&#37327;&#21644;&#37327;&#23376;&#20307;&#31215;&#26041;&#38754;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#37327;&#23376;&#20307;&#31215;&#34987;&#23450;&#20041;&#20026;&#22312;&#36817;&#26399;&#37327;&#23376;&#35774;&#22791;&#19978;&#21487;&#20197;&#26377;&#25928;&#23454;&#29616;&#30340;&#37327;&#23376;&#30005;&#36335;&#30340;&#26368;&#22823;&#35268;&#27169;&#12290;&#22240;&#27492;&#65292;&#22312;&#23454;&#38469;&#30828;&#20214;&#19978;&#24212;&#29992;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;(QML)&#20197;&#23454;&#29616;&#37327;&#23376;&#20248;&#21183;&#24050;&#32463;&#26377;&#20102;&#24456;&#22823;&#30340;&#22686;&#38271;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#22312;&#37327;&#23376;&#30828;&#20214;&#19978;&#23454;&#29616;&#30340;&#36873;&#23450;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#24212;&#29992;&#65292;&#29305;&#21035;&#38024;&#23545;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#12290;&#25105;&#20204;&#25506;&#35752;&#24182;&#24378;&#35843;&#20102;QML&#22312;&#37327;&#23376;&#30828;&#20214;&#19978;&#30340;&#24403;&#21069;&#38480;&#21046;&#12290;&#25105;&#20204;&#28145;&#20837;&#35752;&#35770;&#20102;&#21508;&#31181;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#30340;&#25216;&#26415;&#65292;&#22914;&#32534;&#30721;&#25216;&#26415;&#12289;&#22522;&#24577;&#32467;&#26500;&#12289;&#35823;&#24046;&#34917;&#20607;&#21644;&#26799;&#24230;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20123;QML&#23454;&#29616;&#19982;&#23427;&#20204;&#30340;&#32463;&#20856;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#24615;&#33021;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The past decade has seen considerable progress in quantum hardware in terms of the speed, number of qubits and quantum volume which is defined as the maximum size of a quantum circuit that can be effectively implemented on a near-term quantum device. Consequently, there has also been a rise in the number of works based on the applications of Quantum Machine Learning (QML) on real hardware to attain quantum advantage over their classical counterparts. In this survey, our primary focus is on selected supervised and unsupervised learning applications implemented on quantum hardware, specifically targeting real-world scenarios. Our survey explores and highlights the current limitations of QML implementations on quantum hardware. We delve into various techniques to overcome these limitations, such as encoding techniques, ansatz structure, error mitigation, and gradient methods. Additionally, we assess the performance of these QML implementations in comparison to their classical counterparts
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35782;&#21035;&#24615;&#24418;&#24335;&#65292;&#31216;&#20026;&#37327;&#21270;&#22352;&#26631;&#21487;&#35782;&#21035;&#24615;&#12290;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39640;&#24230;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#19979;&#65292;&#21487;&#20197;&#24674;&#22797;&#31163;&#25955;&#21270;&#30340;&#28508;&#22312;&#22352;&#26631;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#35299;&#32544;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.16334</link><description>&lt;p&gt;
&#36890;&#36807;&#23494;&#24230;&#26631;&#24535;&#26816;&#27979;&#26469;&#35782;&#21035;&#31163;&#25955;&#21270;&#28508;&#22312;&#22352;&#26631;&#31995;&#32479;&#30340;&#21487;&#35782;&#21035;&#24615;
&lt;/p&gt;
&lt;p&gt;
Identifiability of Discretized Latent Coordinate Systems via Density Landmarks Detection. (arXiv:2306.16334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35782;&#21035;&#24615;&#24418;&#24335;&#65292;&#31216;&#20026;&#37327;&#21270;&#22352;&#26631;&#21487;&#35782;&#21035;&#24615;&#12290;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39640;&#24230;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#19979;&#65292;&#21487;&#20197;&#24674;&#22797;&#31163;&#25955;&#21270;&#30340;&#28508;&#22312;&#22352;&#26631;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#35299;&#32544;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#32544;&#26088;&#22312;&#20165;&#20174;&#35266;&#23519;&#21040;&#30340;&#20998;&#24067;&#20013;&#24674;&#22797;&#26377;&#24847;&#20041;&#30340;&#28508;&#22312;&#30495;&#23454;&#22240;&#32032;&#12290; &#21487;&#35782;&#21035;&#24615;&#20026;&#35299;&#32544;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290; &#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#33258;&#36866;&#24212;&#29420;&#31435;&#28508;&#21464;&#37327;&#22240;&#23376;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#20809;&#28369;&#22240;&#23376;&#21040;&#35266;&#27979;&#30340;&#26144;&#23556;&#19979;&#65292;&#26080;&#30417;&#30563;&#30340;&#21487;&#35782;&#21035;&#24615;&#22312;i.i.d.&#35774;&#32622;&#19979;&#26159;&#29702;&#35770;&#19978;&#19981;&#21487;&#33021;&#30340;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38750;&#24120;&#24778;&#20154;&#30340;&#26159;&#65292;&#22312;&#39640;&#24230;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#20809;&#28369;&#26144;&#23556;&#65288;&#19968;&#20010;&#24494;&#20998;&#21516;&#32986;&#65289;&#19979;&#65292;&#21487;&#20197;&#24674;&#22797;&#31163;&#25955;&#21270;&#30340;&#28508;&#22312;&#22352;&#26631;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#26144;&#23556;&#36827;&#34892;&#20219;&#20309;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290; &#36825;&#26159;&#22312;&#20551;&#35774;&#28508;&#22312;&#23494;&#24230;&#20855;&#26377;&#36724;&#23545;&#40784;&#30340;&#19981;&#36830;&#32493;&#26631;&#24535;&#30340;&#24773;&#20917;&#19979;&#65292;&#20294;&#19981;&#20570;&#22240;&#32032;&#30340;&#32479;&#35745;&#29420;&#31435;&#30340;&#19981;&#29616;&#23454;&#30340;&#20551;&#35774;&#12290; &#25105;&#20204;&#24341;&#20837;&#20102;&#36825;&#31181;&#26032;&#39062;&#30340;&#21487;&#35782;&#21035;&#24615;&#24418;&#24335;&#65292;&#31216;&#20026;&#37327;&#21270;&#22352;&#26631;&#21487;&#35782;&#21035;&#24615;&#65292;&#24182;&#23545;&#24674;&#22797;&#31163;&#25955;&#22352;&#26631;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentanglement aims to recover meaningful latent ground-truth factors from only the observed distribution. Identifiability provides the theoretical grounding for disentanglement to be well-founded. Unfortunately, unsupervised identifiability of independent latent factors is a theoretically proven impossibility in the i.i.d. setting under a general nonlinear smooth map from factors to observations. In this work, we show that, remarkably, it is possible to recover discretized latent coordinates under a highly generic nonlinear smooth mapping (a diffeomorphism) without any additional inductive bias on the mapping. This is, assuming that latent density has axis-aligned discontinuity landmarks, but without making the unrealistic assumption of statistical independence of the factors. We introduce this novel form of identifiability, termed quantized coordinate identifiability, and provide a comprehensive proof of the recovery of discretized coordinates.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#37096;&#20998;&#29366;&#24577;&#35266;&#27979;&#30340;&#21160;&#21147;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#37096;&#20998;&#29366;&#24577;&#36755;&#20837;&#21644;&#37096;&#20998;&#25110;&#23436;&#20840;&#29366;&#24577;&#36755;&#20986;&#30340;&#22238;&#38899;&#29366;&#24577;&#32593;&#32476;&#65288;ESN&#65289;&#26694;&#26550;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;Lorenz&#31995;&#32479;&#21644;Chua&#25391;&#33633;&#22120;&#65292;&#23637;&#31034;&#20102;ESN&#30340;&#30701;&#26399;&#39044;&#27979;&#33021;&#21147;&#21644;&#39044;&#27979;&#35270;&#35282;&#30340;&#21464;&#24322;&#24615;&#65292;&#20197;&#21450;ESN&#22312;&#23398;&#20064;&#31995;&#32479;&#21160;&#21147;&#23398;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10797</link><description>&lt;p&gt;
&#37096;&#20998;&#35266;&#27979;&#21160;&#21147;&#31995;&#32479;&#20013;&#22238;&#38899;&#29366;&#24577;&#32593;&#32476;&#39044;&#27979;&#35270;&#35282;&#30340;&#21464;&#24322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Variability of echo state network prediction horizon for partially observed dynamical systems. (arXiv:2306.10797v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#37096;&#20998;&#29366;&#24577;&#35266;&#27979;&#30340;&#21160;&#21147;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#37096;&#20998;&#29366;&#24577;&#36755;&#20837;&#21644;&#37096;&#20998;&#25110;&#23436;&#20840;&#29366;&#24577;&#36755;&#20986;&#30340;&#22238;&#38899;&#29366;&#24577;&#32593;&#32476;&#65288;ESN&#65289;&#26694;&#26550;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;Lorenz&#31995;&#32479;&#21644;Chua&#25391;&#33633;&#22120;&#65292;&#23637;&#31034;&#20102;ESN&#30340;&#30701;&#26399;&#39044;&#27979;&#33021;&#21147;&#21644;&#39044;&#27979;&#35270;&#35282;&#30340;&#21464;&#24322;&#24615;&#65292;&#20197;&#21450;ESN&#22312;&#23398;&#20064;&#31995;&#32479;&#21160;&#21147;&#23398;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#37096;&#20998;&#29366;&#24577;&#35266;&#27979;&#30740;&#31350;&#21160;&#21147;&#31995;&#32479;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#36866;&#29992;&#20110;&#35768;&#22810;&#23454;&#38469;&#31995;&#32479;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#20855;&#26377;&#37096;&#20998;&#29366;&#24577;&#36755;&#20837;&#21644;&#37096;&#20998;&#25110;&#23436;&#20840;&#29366;&#24577;&#36755;&#20986;&#30340;&#22238;&#38899;&#29366;&#24577;&#32593;&#32476;&#65288;ESN&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;Lorenz&#31995;&#32479;&#21644;Chua&#25391;&#33633;&#22120;&#65288;&#21253;&#25324;&#25968;&#20540;&#27169;&#25311;&#21644;&#23454;&#39564;&#31995;&#32479;&#65289;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#26174;&#31034;ESN&#20316;&#20026;&#19968;&#20010;&#33258;&#20027;&#21160;&#21147;&#31995;&#32479;&#65292;&#33021;&#22815;&#20570;&#20986;&#30701;&#26399;&#39044;&#27979;&#65292;&#21487;&#36798;&#20960;&#20010;Lyapunov&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#35270;&#35282;&#30340;&#21464;&#24322;&#24615;&#21462;&#20915;&#20110;&#21021;&#22987;&#26465;&#20214;-&#36825;&#26159;&#25105;&#20204;&#36890;&#36807;&#39044;&#27979;&#35270;&#35282;&#30340;&#20998;&#24067;&#36827;&#34892;&#35814;&#32454;&#30740;&#31350;&#30340;&#19968;&#20010;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#32479;&#35745;&#25351;&#26631;&#23558;ESN&#39044;&#27979;&#30340;&#38271;&#26399;&#21160;&#21147;&#23398;&#19982;&#25968;&#20540;&#27169;&#25311;&#25110;&#23454;&#39564;&#21160;&#21147;&#23398;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35266;&#23519;&#21040;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#34920;&#26126;ESN&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#26102;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Study of dynamical systems using partial state observation is an important problem due to its applicability to many real-world systems. We address the problem by proposing an echo state network (ESN) framework with partial state input with partial or full state output. Application to the Lorenz system and Chua's oscillator (both numerically simulated and experimental systems) demonstrate the effectiveness of our method. We show that the ESN, as an autonomous dynamical system, is capable of making short-term predictions up to a few Lyapunov times. However, the prediction horizon has high variability depending on the initial condition - an aspect that we explore in detail using the distribution of the prediction horizon. Further, using a variety of statistical metrics to compare the long-term dynamics of the ESN predictions with numerically simulated or experimental dynamics and observed similar results, we show that the ESN can effectively learn the system's dynamics even when trained w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#31070;&#32463;&#21551;&#21160;&#25216;&#26415;&#65292;&#29992;&#20110;&#20351;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#20998;&#24067;&#21464;&#21270;&#21644;&#19979;&#28216;&#20219;&#21153;&#65292;&#26080;&#38656;&#36807;&#22810;&#26631;&#35760;&#26679;&#26412;&#12290;&#36890;&#36807;&#22238;&#24518;&#25968;&#25454;&#36827;&#34892;&#36731;&#37327;&#26356;&#26032;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10191</link><description>&lt;p&gt;
&#31070;&#32463;&#21551;&#21160;&#25216;&#26415;&#29992;&#20110;&#23567;&#26679;&#26412;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Neural Priming for Sample-Efficient Adaptation. (arXiv:2306.10191v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#31070;&#32463;&#21551;&#21160;&#25216;&#26415;&#65292;&#29992;&#20110;&#20351;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#20998;&#24067;&#21464;&#21270;&#21644;&#19979;&#28216;&#20219;&#21153;&#65292;&#26080;&#38656;&#36807;&#22810;&#26631;&#35760;&#26679;&#26412;&#12290;&#36890;&#36807;&#22238;&#24518;&#25968;&#25454;&#36827;&#34892;&#36731;&#37327;&#26356;&#26032;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#31070;&#32463;&#21551;&#21160;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#26410;&#32463;&#36807;&#22823;&#37327;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#20998;&#24067;&#21464;&#21270;&#21644;&#19979;&#28216;&#20219;&#21153;&#12290;&#22312;&#32473;&#23450;&#31867;&#21517;&#25110;&#26080;&#26631;&#31614;&#27979;&#35797;&#26679;&#26412;&#26102;&#65292;&#31070;&#32463;&#21551;&#21160;&#21487;&#20197;&#20351;&#27169;&#22411;&#22238;&#24518;&#36215;&#39044;&#35757;&#32451;&#26399;&#38388;&#30475;&#21040;&#30340;&#30456;&#20851;&#25968;&#25454;&#24182;&#20197;&#27492;&#20026;&#22522;&#30784;&#26465;&#20214;&#21270;&#20854;&#21442;&#25968;&#65292;&#20174;&#32780;&#20351;&#20854;&#38024;&#23545;&#27979;&#35797;&#20998;&#24067;&#20570;&#22909;&#20934;&#22791;&#12290;&#31070;&#32463;&#21551;&#21160;&#36824;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#36827;&#34892;&#65292;&#21363;&#20351;&#26159;&#38024;&#23545;&#22914;LAION-2B&#36825;&#26679;&#22823;&#22411;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#22312;&#21508;&#31181;&#20998;&#24067;&#21464;&#21270;&#21644;&#36801;&#31227;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#23545;&#22238;&#24518;&#25968;&#25454;&#36827;&#34892;&#36731;&#37327;&#26356;&#26032;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#30475;&#21040;ImageNet&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;2.45&#65285;&#65292;&#22312;&#26631;&#20934;&#30340;&#36801;&#31227;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#24179;&#22343;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;3.81&#65285;&#12290;&#27492;&#22806;&#65292;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;&#31070;&#32463;&#21551;&#21160;&#26469;&#36866;&#24212;&#20998;&#24067;&#21464;&#21270;&#65292;&#25105;&#20204;&#30475;&#21040;ImageNetV2&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;1.41&#65285;&#12290;&#36825;&#20123;&#32467;&#26524;&#35777;&#26126;&#20102;&#31070;&#32463;&#21551;&#21160;&#22312;&#22788;&#29702;&#23567;&#26679;&#26412;&#33258;&#36866;&#24212;&#25361;&#25112;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Neural Priming, a technique for adapting large pretrained models to distribution shifts and downstream tasks given few or no labeled examples. Presented with class names or unlabeled test samples, Neural Priming enables the model to recall and conditions its parameters on relevant data seen throughout pretraining, thereby priming it for the test distribution. Neural Priming can be performed at test time, even for pretraining datasets as large as LAION-2B. Performing lightweight updates on the recalled data significantly improves accuracy across a variety of distribution shift and transfer learning benchmarks. Concretely, in the zero-shot setting, we see a 2.45% improvement in accuracy on ImageNet and 3.81% accuracy improvement on average across standard transfer learning benchmarks. Further, using Neural Priming at inference to adapt to distribution shift, we see a 1.41% accuracy improvement on ImageNetV2. These results demonstrate the effectiveness of Neural Priming in addr
&lt;/p&gt;</description></item><item><title>Fedstellar&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#65292;&#25903;&#25345;&#29289;&#29702;&#25110;&#34394;&#25311;&#35774;&#22791;&#30340;&#21435;&#20013;&#24515;&#21270;&#12289;&#21322;&#21435;&#20013;&#24515;&#21270;&#21644;&#20013;&#24515;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#24179;&#21488;&#22312;&#22788;&#29702;&#24322;&#26500;&#32852;&#30431;&#32593;&#32476;&#25299;&#25169;&#31561;&#38382;&#39064;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.09750</link><description>&lt;p&gt;
Fedstellar&#65306;&#19968;&#20010;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Fedstellar: A Platform for Decentralized Federated Learning. (arXiv:2306.09750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09750
&lt;/p&gt;
&lt;p&gt;
Fedstellar&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#65292;&#25903;&#25345;&#29289;&#29702;&#25110;&#34394;&#25311;&#35774;&#22791;&#30340;&#21435;&#20013;&#24515;&#21270;&#12289;&#21322;&#21435;&#20013;&#24515;&#21270;&#21644;&#20013;&#24515;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#24179;&#21488;&#22312;&#22788;&#29702;&#24322;&#26500;&#32852;&#30431;&#32593;&#32476;&#25299;&#25169;&#31561;&#38382;&#39064;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2016&#24180;&#65292;&#35895;&#27468;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#36328;&#32852;&#30431;&#21442;&#19982;&#32773;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#34429;&#28982;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;CFL&#65289;&#26159;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#23384;&#22312;&#36890;&#20449;&#29942;&#39048;&#12289;&#21333;&#28857;&#25925;&#38556;&#21644;&#23545;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#20381;&#36182;&#31561;&#23616;&#38480;&#12290;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#36890;&#36807;&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#27169;&#22411;&#32858;&#21512;&#21644;&#26368;&#23567;&#21270;&#23545;&#20013;&#22830;&#23454;&#20307;&#30340;&#20381;&#36182;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#35757;&#32451;DFL&#27169;&#22411;&#30340;&#24179;&#21488;&#22312;&#22788;&#29702;&#24322;&#26500;&#32852;&#30431;&#32593;&#32476;&#25299;&#25169;&#31561;&#20851;&#38190;&#38382;&#39064;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Fedstellar&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#24179;&#21488;&#65292;&#26088;&#22312;&#22312;&#29289;&#29702;&#25110;&#34394;&#25311;&#35774;&#22791;&#30340;&#19981;&#21516;&#32852;&#30431;&#20013;&#20197;&#21435;&#20013;&#24515;&#21270;&#12289;&#21322;&#21435;&#20013;&#24515;&#21270;&#21644;&#20013;&#24515;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;FL&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 2016, Google proposed Federated Learning (FL) as a novel paradigm to train Machine Learning (ML) models across the participants of a federation while preserving data privacy. Since its birth, Centralized FL (CFL) has been the most used approach, where a central entity aggregates participants' models to create a global one. However, CFL presents limitations such as communication bottlenecks, single point of failure, and reliance on a central server. Decentralized Federated Learning (DFL) addresses these issues by enabling decentralized model aggregation and minimizing dependency on a central entity. Despite these advances, current platforms training DFL models struggle with key issues such as managing heterogeneous federation network topologies. To overcome these challenges, this paper presents Fedstellar, a novel platform designed to train FL models in a decentralized, semi-decentralized, and centralized fashion across diverse federations of physical or virtualized devices. The Feds
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23450;&#37327;&#20998;&#26512;&#20102;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25506;&#31350;&#20102;&#19981;&#21516;&#23646;&#24615;&#30340;XAI&#26041;&#27861;&#65292;&#25552;&#20379;&#36873;&#21462;&#21512;&#36866;&#26041;&#27861;&#20197;&#28145;&#20837;&#20102;&#35299;&#27169;&#22411;&#20915;&#31574;&#30340;&#35265;&#35299;&#21644;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2306.04037</link><description>&lt;p&gt;
&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#20027;&#35201;&#36129;&#29486;&#30340;&#23450;&#37327;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Quantitative Analysis of Primary Attribution Explainable Artificial Intelligence Methods for Remote Sensing Image Classification. (arXiv:2306.04037v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04037
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23450;&#37327;&#20998;&#26512;&#20102;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25506;&#31350;&#20102;&#19981;&#21516;&#23646;&#24615;&#30340;XAI&#26041;&#27861;&#65292;&#25552;&#20379;&#36873;&#21462;&#21512;&#36866;&#26041;&#27861;&#20197;&#28145;&#20837;&#20102;&#35299;&#27169;&#22411;&#20915;&#31574;&#30340;&#35265;&#35299;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#20998;&#26512;&#23450;&#37327;&#35780;&#20272;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#22810;&#31181;&#27169;&#24577;&#19979;&#25191;&#34892;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#12290;&#25105;&#20204;&#36890;&#36807;XAI&#26041;&#27861;&#23450;&#24615;&#22320;&#30740;&#31350;&#20102;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#25152;&#38656;&#23646;&#24615;&#30340;&#21508;&#31181;&#31867;&#21035;&#26469;&#23450;&#37327;&#27604;&#36739;XAI&#26041;&#27861;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;XAI&#26041;&#27861;&#20197;&#21152;&#28145;&#23545;&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#29702;&#35299;&#30340;&#35265;&#35299;&#21644;&#24314;&#35758;&#12290;&#27492;&#24037;&#20316;&#30340;&#20195;&#30721;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive analysis of quantitatively evaluating explainable artificial intelligence (XAI) techniques for remote sensing image classification. Our approach leverages state-of-the-art machine learning approaches to perform remote sensing image classification across multiple modalities. We investigate the results of the models qualitatively through XAI methods. Additionally, we compare the XAI methods quantitatively through various categories of desired properties. Through our analysis, we offer insights and recommendations for selecting the most appropriate XAI method(s) to gain a deeper understanding of the models' decision-making processes. The code for this work is publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#39564;&#35777;&#26041;&#27861;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#20197;&#39564;&#35777;&#23433;&#20840;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#24182;&#20195;&#34920;&#20102;&#36890;&#29992;&#12289;&#21487;&#20280;&#32553;&#21644;&#21487;&#39564;&#35777;&#30340;&#25511;&#21046;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2306.04026</link><description>&lt;p&gt;
&#20215;&#20540;&#20989;&#25968;&#21363;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#65306;&#20351;&#29992;&#25511;&#21046;&#29702;&#35770;&#39564;&#35777;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Your Value Function is a Control Barrier Function: Verification of Learned Policies using Control Theory. (arXiv:2306.04026v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#39564;&#35777;&#26041;&#27861;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#20197;&#39564;&#35777;&#23433;&#20840;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#24182;&#20195;&#34920;&#20102;&#36890;&#29992;&#12289;&#21487;&#20280;&#32553;&#21644;&#21487;&#39564;&#35777;&#30340;&#25511;&#21046;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#20855;&#26377;&#39640;&#24230;&#30340;&#36890;&#29992;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#65292;&#20294;&#39564;&#35777;&#31574;&#30053;&#34892;&#20026;&#30340;&#38590;&#24230;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#25511;&#21046;&#29702;&#35770;&#20013;&#20351;&#29992;&#30340;&#39564;&#35777;&#26041;&#27861;&#24212;&#29992;&#20110;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;&#36890;&#36807;&#20998;&#26512;&#23433;&#20840;&#32500;&#25252;&#30340;&#31616;&#21333;&#20219;&#21153;&#32467;&#26500;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#23558;&#20540;&#20989;&#25968;&#19982;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#30456;&#32852;&#31995;&#30340;&#21407;&#22987;&#23450;&#29702;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20197;&#39564;&#35777;&#23433;&#20840;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#21892;&#23398;&#20064;&#30340;&#23454;&#38469;&#23454;&#26045;&#32454;&#33410;&#12290;&#38500;&#20102;&#25552;&#20986;&#35777;&#20070;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#22806;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;RL&#31574;&#30053;&#35299;&#38145;&#20102;&#20016;&#23500;&#30340;&#25511;&#21046;&#29702;&#35770;&#39564;&#35777;&#26041;&#27861;&#65292;&#24182;&#20195;&#34920;&#20102;&#36890;&#29992;&#12289;&#21487;&#20280;&#32553;&#21644;&#21487;&#39564;&#35777;&#30340;&#25511;&#21046;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although RL is highly general and scalable, the difficulty of verifying policy behaviours poses challenges for safety-critical applications. To remedy this, we propose to apply verification methods used in control theory to learned value functions. By analyzing a simple task structure for safety preservation, we derive original theorems linking value functions to control barrier functions. Inspired by this, we propose novel metrics for verification of value functions in safe control tasks, and practical implementation details that improve learning. Besides proposing a novel method for certificate learning, our work unlocks a wealth of verification methods in control theory for RL policies, and represents a first step towards a framework for general, scalable, and verifiable design of control systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#23545;&#31216;&#26680;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21363;Primal-Attention&#65292;&#26469;&#20248;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#39640;&#27880;&#24847;&#21147;&#36755;&#20986;&#30340;&#25237;&#24433;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.19798</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#22987;&#34920;&#36798;&#30340;&#19981;&#23545;&#31216;&#26680;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Primal-Attention: Self-attention through Asymmetric Kernel SVD in Primal Representation. (arXiv:2305.19798v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#23545;&#31216;&#26680;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21363;Primal-Attention&#65292;&#26469;&#20248;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#39640;&#27880;&#24847;&#21147;&#36755;&#20986;&#30340;&#25237;&#24433;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#19968;&#31995;&#21015;&#24037;&#20316;&#23558;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#35270;&#20026;&#26680;&#26426;&#22120;&#65292;&#20197;&#27492;&#26469;&#29702;&#35299;&#21644;&#25913;&#36827;Transformers&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#21482;&#36866;&#29992;&#20110;&#23545;&#31216;&#26680;&#32780;&#19981;&#36866;&#29992;&#20110;&#19981;&#23545;&#31216;&#30340;&#33258;&#27880;&#24847;&#21147;&#65292;&#23548;&#33268;&#20102;&#29702;&#35770;&#21644;&#23454;&#38469;&#30340;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#23545;&#31216;&#26680;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;KSVD&#65289;&#26469;&#34920;&#36798;&#21644;&#20248;&#21270;&#33258;&#27880;&#24847;&#21147;&#30340;&#26032;&#35270;&#35282;&#12290;&#36890;&#36807;&#19981;&#23545;&#31216;KSVD&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#65306;i&#65289;&#33258;&#27880;&#24847;&#21147;&#30340;&#19968;&#31181;&#21407;&#22987;-&#23545;&#20598;&#34920;&#36798;&#65292;&#20854;&#20013;&#20248;&#21270;&#30446;&#26631;&#34987;&#36716;&#21270;&#20026;&#26368;&#22823;&#21270;&#27880;&#24847;&#21147;&#36755;&#20986;&#20013;&#30340;&#25237;&#24433;&#26041;&#24046;&#65307;ii&#65289;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;-Primal-Attention&#65292;&#36890;&#36807;KSVD&#30340;&#21407;&#22987;&#34920;&#36798;&#24335;&#36991;&#20813;&#20102;&#22312;&#23545;&#20598;&#20013;&#26174;&#24335;&#35745;&#31639;&#26680;&#30697;&#38453;&#30340;&#38382;&#39064;&#65307;iii&#65289;&#36890;&#36807;KKT&#26465;&#20214;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Primal-Attention&#30340;&#29366;&#24577;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#35299;&#19982;&#20043;&#21069;&#30340;&#23545;&#20598;&#31639;&#27861;&#20855;&#26377;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, a new line of works has emerged to understand and improve self-attention in Transformers by treating it as a kernel machine. However, existing works apply the methods for symmetric kernels to the asymmetric self-attention, resulting in a nontrivial gap between the analytical understanding and numerical implementation. In this paper, we provide a new perspective to represent and optimize self-attention through asymmetric Kernel Singular Value Decomposition (KSVD), which is also motivated by the low-rank property of self-attention normally observed in deep layers. Through asymmetric KSVD, $i$) a primal-dual representation of self-attention is formulated, where the optimization objective is cast to maximize the projection variances in the attention outputs; $ii$) a novel attention mechanism, i.e., Primal-Attention, is proposed via the primal representation of KSVD, avoiding explicit computation of the kernel matrix in the dual; $iii$) with KKT conditions, we prove that the stati
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20998;&#26512;&#20102;&#26032;&#25552;&#20986;&#30340;&#26435;&#37325;&#24179;&#34913;&#26041;&#27861;&#22312;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#20013;&#26377;&#25928;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#32531;&#35299;&#31070;&#32463;&#23849;&#28291;&#21644;&#22278;&#38181;&#25928;&#24212;&#65292;&#20174;&#32780;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16573</link><description>&lt;p&gt;
&#25506;&#32034;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#20013;&#30340;&#26435;&#37325;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Exploring Weight Balancing on Long-Tailed Recognition Problem. (arXiv:2305.16573v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16573
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20998;&#26512;&#20102;&#26032;&#25552;&#20986;&#30340;&#26435;&#37325;&#24179;&#34913;&#26041;&#27861;&#22312;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#20013;&#26377;&#25928;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#32531;&#35299;&#31070;&#32463;&#23849;&#28291;&#21644;&#22278;&#38181;&#25928;&#24212;&#65292;&#20174;&#32780;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#23614;&#25968;&#25454;&#20013;&#30340;&#35782;&#21035;&#38382;&#39064;&#26368;&#36817;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#20998;&#24067;&#36890;&#24120;&#26159;&#25351;&#25968;&#20998;&#24067;&#65292;&#38500;&#38750;&#26377;&#24847;&#22320;&#35843;&#25972;&#26679;&#26412;&#25968;&#37327;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#26435;&#37325;&#24179;&#34913;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#33879;&#21517;&#30340;&#32463;&#20856;&#27491;&#21017;&#21270;&#25216;&#26415;&#21644;&#20004;&#38454;&#27573;&#35757;&#32451;&#12290;&#23613;&#31649;&#20854;&#31616;&#21333;&#24615;&#65292;&#20294;&#24050;&#30693;&#20854;&#23545;&#29616;&#26377;&#21508;&#31181;&#19981;&#21516;&#26041;&#27861;&#20855;&#26377;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#32570;&#20047;&#20026;&#20160;&#20040;&#36825;&#31181;&#26041;&#27861;&#23545;&#38271;&#23614;&#25968;&#25454;&#26377;&#25928;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#20851;&#27880;&#20102;&#31070;&#32463;&#23849;&#28291;&#21644;&#27599;&#20010;&#35757;&#32451;&#38454;&#27573;&#30340;&#22278;&#38181;&#25928;&#24212;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#20998;&#35299;&#20026;&#30001;&#26435;&#20540;&#34928;&#20943;&#21644;&#20132;&#21449;&#29109;&#25439;&#22833;&#24341;&#36215;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#20013;Fisher&#21028;&#21035;&#27604;&#30340;&#22686;&#21152;&#20197;&#21450;&#30001;&#26435;&#37325;&#34928;&#20943;&#21644;&#31867;&#24179;&#34913;&#27491;&#21017;&#21270;&#24341;&#36215;&#30340;&#38544;&#24335;&#36923;&#36753;&#35843;&#25972;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#26435;&#37325;&#24179;&#34913;&#26041;&#27861;&#25104;&#21151;&#32531;&#35299;&#20102;&#31070;&#32463;&#23849;&#28291;&#21644;&#22278;&#38181;&#25928;&#24212;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#38271;&#23614;&#25968;&#25454;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognition problems in long-tailed data, where the sample size per class is heavily skewed, have recently gained importance because the distribution of the sample size per class in a dataset is generally exponential unless the sample size is intentionally adjusted. Various approaches have been devised to address these problems. Recently, weight balancing, which combines well-known classical regularization techniques with two-stage training, has been proposed. Despite its simplicity, it is known for its high performance against existing methods devised in various ways. However, there is a lack of understanding as to why this approach is effective for long-tailed data. In this study, we analyze the method focusing on neural collapse and cone effect at each training stage and find that it can be decomposed into the increase in Fisher's discriminant ratio of the feature extractor caused by weight decay and cross entropy loss and implicit logit adjustment caused by weight decay and class-b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#27867;&#21270;&#21040;&#23398;&#20064;&#33719;&#21462;&#20989;&#25968;&#30340;&#31070;&#32463;&#36807;&#31243;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20102;&#32570;&#20047;&#26631;&#31614;&#33719;&#21462;&#25968;&#25454;&#20197;&#21450;&#21033;&#29992;&#20195;&#29702;&#27169;&#22411;&#25110;&#33719;&#21462;&#20989;&#25968;&#30340;&#20256;&#32479;Meta-BO&#26041;&#27861;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.15930</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#31070;&#32463;&#36807;&#31243;&#30340;&#31471;&#21040;&#31471;Meta-Bayesian&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
End-to-End Meta-Bayesian Optimisation with Transformer Neural Processes. (arXiv:2305.15930v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#27867;&#21270;&#21040;&#23398;&#20064;&#33719;&#21462;&#20989;&#25968;&#30340;&#31070;&#32463;&#36807;&#31243;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20102;&#32570;&#20047;&#26631;&#31614;&#33719;&#21462;&#25968;&#25454;&#20197;&#21450;&#21033;&#29992;&#20195;&#29702;&#27169;&#22411;&#25110;&#33719;&#21462;&#20989;&#25968;&#30340;&#20256;&#32479;Meta-BO&#26041;&#27861;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;Meta-Bayesian optimization&#65292;Meta-BO&#65289;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#25968;&#25454;&#26469;&#25552;&#39640;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#21151;&#22320;&#29420;&#31435;&#20803;&#23398;&#20064;&#36807;&#20195;&#29702;&#27169;&#22411;&#25110;&#33719;&#21462;&#20989;&#25968;&#65292;&#20294;&#26159;&#21516;&#26102;&#35757;&#32451;&#36825;&#20004;&#20010;&#32452;&#20214;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#30340;Meta-BO&#26694;&#26550;&#65292;&#36890;&#36807;Transformer&#20307;&#31995;&#32467;&#26500;&#23558;&#31070;&#32463;&#36807;&#31243;&#27867;&#21270;&#21040;&#23398;&#20064;&#33719;&#21462;&#20989;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20351;&#36825;&#31181;&#31471;&#21040;&#31471;&#26694;&#26550;&#20855;&#26377;&#22788;&#29702;&#32570;&#20047;&#26631;&#31614;&#33719;&#21462;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-Bayesian optimisation (meta-BO) aims to improve the sample efficiency of Bayesian optimisation by leveraging data from related tasks. While previous methods successfully meta-learn either a surrogate model or an acquisition function independently, joint training of both components remains an open challenge. This paper proposes the first end-to-end differentiable meta-BO framework that generalises neural processes to learn acquisition functions via transformer architectures. We enable this end-to-end framework with reinforcement learning (RL) to tackle the lack of labelled acquisition data. Early on, we notice that training transformer-based neural processes from scratch with RL is challenging due to insufficient supervision, especially when rewards are sparse. We formalise this claim with a combinatorial analysis showing that the widely used notion of regret as a reward signal exhibits a logarithmic sparsity pattern in trajectory lengths. To tackle this problem, we augment the RL 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#32467;&#21512;implicit regularization&#65292;&#20869;&#37096;&#32447;&#24615;&#23618;&#21644;L2&#27491;&#21017;&#21270;&#65288;&#26435;&#37325;&#34928;&#20943;&#65289;&#33258;&#21160;&#20272;&#35745;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#32500;&#24230;&#65292;&#20135;&#29983;&#27491;&#20132;&#30340;&#27969;&#24418;&#22352;&#26631;&#31995;&#65292;&#24182;&#25552;&#20379;&#29615;&#22659;&#31354;&#38388;&#21644;&#27969;&#24418;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#20989;&#25968;&#65292;&#20174;&#32780;&#20801;&#35768;&#36827;&#34892;&#26679;&#26412;&#20043;&#22806;&#30340;&#25237;&#24433;&#12290;&#35813;&#26041;&#27861;&#22312;&#21160;&#21147;&#31995;&#32479;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#20102;&#36739;&#22909;&#30340;&#20302;&#31209;&#34920;&#31034;&#25928;&#26524;&#65292;&#20026;&#24213;&#23618;&#21160;&#24577;&#25552;&#20379;&#20102;&#29289;&#29702;&#27934;&#35265;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#25511;&#21046;&#31574;&#30053;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01090</link><description>&lt;p&gt;
&#33258;&#21160;&#32534;&#30721;&#22120;&#29992;&#20110;&#21457;&#29616;&#22797;&#26434;&#21160;&#21147;&#31995;&#32479;&#25968;&#25454;&#20013;&#30340;&#27969;&#24418;&#32500;&#24230;&#21644;&#22352;&#26631;
&lt;/p&gt;
&lt;p&gt;
Autoencoders for discovering manifold dimension and coordinates in data from complex dynamical systems. (arXiv:2305.01090v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#32467;&#21512;implicit regularization&#65292;&#20869;&#37096;&#32447;&#24615;&#23618;&#21644;L2&#27491;&#21017;&#21270;&#65288;&#26435;&#37325;&#34928;&#20943;&#65289;&#33258;&#21160;&#20272;&#35745;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#32500;&#24230;&#65292;&#20135;&#29983;&#27491;&#20132;&#30340;&#27969;&#24418;&#22352;&#26631;&#31995;&#65292;&#24182;&#25552;&#20379;&#29615;&#22659;&#31354;&#38388;&#21644;&#27969;&#24418;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#20989;&#25968;&#65292;&#20174;&#32780;&#20801;&#35768;&#36827;&#34892;&#26679;&#26412;&#20043;&#22806;&#30340;&#25237;&#24433;&#12290;&#35813;&#26041;&#27861;&#22312;&#21160;&#21147;&#31995;&#32479;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#20102;&#36739;&#22909;&#30340;&#20302;&#31209;&#34920;&#31034;&#25928;&#26524;&#65292;&#20026;&#24213;&#23618;&#21160;&#24577;&#25552;&#20379;&#20102;&#29289;&#29702;&#27934;&#35265;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#25511;&#21046;&#31574;&#30053;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29289;&#29702;&#21644;&#24037;&#31243;&#23398;&#20013;&#30340;&#35768;&#22810;&#29616;&#35937;&#22312;&#24418;&#24335;&#19978;&#26159;&#39640;&#32500;&#30340;&#65292;&#20294;&#23427;&#20204;&#30340;&#38271;&#26102;&#38388;&#21160;&#24577;&#24448;&#24448;&#29983;&#27963;&#22312;&#36739;&#20302;&#32500;&#30340;&#27969;&#24418;&#19978;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#23558;&#38544;&#24335;&#27491;&#21017;&#21270;&#19982;&#20869;&#37096;&#32447;&#24615;&#23618;&#21644;$L_2$&#27491;&#21017;&#21270;&#65288;&#26435;&#37325;&#34928;&#20943;&#65289;&#30456;&#32467;&#21512;&#65292;&#33258;&#21160;&#20272;&#35745;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#32500;&#24230;&#12289;&#20135;&#29983;&#27491;&#20132;&#30340;&#27969;&#24418;&#22352;&#26631;&#31995;&#65292;&#24182;&#25552;&#20379;&#29615;&#22659;&#31354;&#38388;&#21644;&#27969;&#24418;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#20989;&#25968;&#65292;&#20174;&#32780;&#20801;&#35768;&#36827;&#34892;&#26679;&#26412;&#20043;&#22806;&#30340;&#25237;&#24433;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#20272;&#35745;&#27969;&#24418;&#32500;&#24230;&#30340;&#33021;&#21147;&#65292;&#38024;&#23545;&#22810;&#31181;&#22797;&#26434;&#24230;&#30340;&#21160;&#21147;&#31995;&#32479;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#20272;&#35745;&#22120;&#36827;&#34892;&#23545;&#27604;&#12290;&#25105;&#20204;&#20998;&#26512;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#20197;&#20102;&#35299;&#20302;&#31209;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#27599;&#20010;&#38544;&#24335;&#27491;&#21017;&#21270;&#23618;&#20849;&#21516;&#26500;&#25104;&#20102;&#20302;&#31209;&#34920;&#31034;&#65292;&#29978;&#33267;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#20102;&#33258;&#25105;&#32416;&#27491;&#12290;&#23398;&#20064;&#30340;&#22352;&#26631;&#31995;&#30340;&#20998;&#26512;&#21487;&#20197;&#20026;&#24213;&#23618;&#21160;&#24577;&#25552;&#20379;&#29289;&#29702;&#27934;&#35265;&#65292;&#20063;&#21487;&#20197;&#29992;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#25511;&#21046;&#31574;&#30053;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While many phenomena in physics and engineering are formally high-dimensional, their long-time dynamics often live on a lower-dimensional manifold. The present work introduces an autoencoder framework that combines implicit regularization with internal linear layers and $L_2$ regularization (weight decay) to automatically estimate the underlying dimensionality of a data set, produce an orthogonal manifold coordinate system, and provide the mapping functions between the ambient space and manifold space, allowing for out-of-sample projections. We validate our framework's ability to estimate the manifold dimension for a series of datasets from dynamical systems of varying complexities and compare to other state-of-the-art estimators. We analyze the training dynamics of the network to glean insight into the mechanism of low-rank learning and find that collectively each of the implicit regularizing layers compound the low-rank representation and even self-correct during training. Analysis o
&lt;/p&gt;</description></item><item><title>CROP&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32039;&#20945;&#37325;&#22609;&#35266;&#23519;&#22788;&#29702;&#26469;&#20943;&#23569;&#29992;&#20110;&#25919;&#31574;&#20248;&#21270;&#30340;&#29366;&#24577;&#20449;&#24687;&#65292;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#29305;&#23450;&#30340;&#35757;&#32451;&#24067;&#23616;&#65292;&#24182;&#25552;&#39640;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.13616</link><description>&lt;p&gt;
CROP: &#20351;&#29992;&#32039;&#20945;&#37325;&#22609;&#35266;&#23519;&#22788;&#29702;&#23454;&#29616;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CROP: Towards Distributional-Shift Robust Reinforcement Learning using Compact Reshaped Observation Processing. (arXiv:2304.13616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13616
&lt;/p&gt;
&lt;p&gt;
CROP&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32039;&#20945;&#37325;&#22609;&#35266;&#23519;&#22788;&#29702;&#26469;&#20943;&#23569;&#29992;&#20110;&#25919;&#31574;&#20248;&#21270;&#30340;&#29366;&#24577;&#20449;&#24687;&#65292;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#29305;&#23450;&#30340;&#35757;&#32451;&#24067;&#23616;&#65292;&#24182;&#25552;&#39640;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#23433;&#20840;&#24212;&#29992;&#38656;&#35201;&#20174;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#25512;&#24191;&#21040;&#26410;&#30693;&#24773;&#22659;&#12290;&#28982;&#32780;&#65292;&#24212;&#23545;&#19981;&#26029;&#21464;&#21270;&#30340;&#24773;&#20917;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#39033;&#20851;&#38190;&#25361;&#25112;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#36890;&#29992;&#21270;&#26041;&#27861;&#24212;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#12290;&#23613;&#31649;&#36825;&#21487;&#20197;&#38450;&#27490;&#36807;&#24230;&#25311;&#21512;&#35757;&#32451;&#29615;&#22659;&#65292;&#20294;&#20063;&#20250;&#38459;&#30861;&#25919;&#31574;&#20248;&#21270;&#12290;&#35774;&#35745;&#19968;&#20010;&#21512;&#36866;&#30340;&#35266;&#23519;&#20449;&#24687;&#65292;&#21482;&#21253;&#21547;&#20851;&#38190;&#20449;&#24687;&#65292;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32039;&#20945;&#37325;&#22609;&#35266;&#23519;&#22788;&#29702;&#65288;CROP&#65289;&#65292;&#20197;&#20943;&#23569;&#29992;&#20110;&#25919;&#31574;&#20248;&#21270;&#30340;&#29366;&#24577;&#20449;&#24687;&#12290;&#36890;&#36807;&#25552;&#20379;&#21482;&#26377;&#30456;&#20851;&#20449;&#24687;&#65292;&#21487;&#20197;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#29305;&#23450;&#30340;&#35757;&#32451;&#24067;&#23616;&#65292;&#24182;&#25552;&#39640;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19977;&#31181;CROP&#65292;&#21487;&#24212;&#29992;&#20110;&#23436;&#20840;&#21487;&#35266;&#23519;&#30340;&#35266;&#23519;&#21644;&#34892;&#21160;&#31354;&#38388;&#65292;&#24182;&#25552;&#20379;&#26041;&#27861;ologically&#22320;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The safe application of reinforcement learning (RL) requires generalization from limited training data to unseen scenarios. Yet, fulfilling tasks under changing circumstances is a key challenge in RL. Current state-of-the-art approaches for generalization apply data augmentation techniques to increase the diversity of training data. Even though this prevents overfitting to the training environment(s), it hinders policy optimization. Crafting a suitable observation, only containing crucial information, has been shown to be a challenging task itself. To improve data efficiency and generalization capabilities, we propose Compact Reshaped Observation Processing (CROP) to reduce the state information used for policy optimization. By providing only relevant information, overfitting to a specific training layout is precluded and generalization to unseen environments is improved. We formulate three CROPs that can be applied to fully observable observation- and action-spaces and provide methodi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#20845;&#31181;&#19981;&#21516;&#30340;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#20302;&#20998;&#36776;&#29575;&#32418;&#22806;&#38453;&#21015;&#30340;&#20154;&#27969;&#37327;&#35745;&#31639;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#25506;&#32034;&#33719;&#24471;&#20102;&#23500;&#26377;&#25104;&#25928;&#30340; Pareto &#26368;&#20248;&#35299;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#21644;&#20302;&#25104;&#26412;&#21516;&#26102;&#28385;&#36275;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2304.06059</link><description>&lt;p&gt;
&#20302;&#20998;&#36776;&#29575;&#32418;&#22806;&#38453;&#21015;&#19978;&#20445;&#25252;&#38544;&#31169;&#30340;&#20154;&#27969;&#37327;&#35745;&#31639;&#30340;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Efficient Deep Learning Models for Privacy-preserving People Counting on Low-resolution Infrared Arrays. (arXiv:2304.06059v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#20845;&#31181;&#19981;&#21516;&#30340;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#20302;&#20998;&#36776;&#29575;&#32418;&#22806;&#38453;&#21015;&#30340;&#20154;&#27969;&#37327;&#35745;&#31639;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#25506;&#32034;&#33719;&#24471;&#20102;&#23500;&#26377;&#25104;&#25928;&#30340; Pareto &#26368;&#20248;&#35299;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#21644;&#20302;&#25104;&#26412;&#21516;&#26102;&#28385;&#36275;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#20302;&#20998;&#36776;&#29575;&#32418;&#22806;(IR)&#38453;&#21015;&#20256;&#24863;&#22120;&#20026;&#20154;&#27969;&#37327;&#35745;&#31639;&#25552;&#20379;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#12289;&#33021;&#25928;&#39640;&#19988;&#20445;&#25252;&#38544;&#31169;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#29992;&#20110;&#21344;&#29992;&#30417;&#27979;&#31561;&#24212;&#29992;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;(DL)&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#21487;&#20197;&#33719;&#24471;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#32570;&#20047;&#19968;&#20010;&#23545;&#22522;&#20110;IR&#38453;&#21015;&#30340;&#20154;&#27969;&#37327;&#35745;&#31639;&#30340;&#21508;&#31181;&#39640;&#25928;DL&#26550;&#26500;&#36827;&#34892;&#24191;&#27867;&#27604;&#36739;&#20998;&#26512;&#30340;&#30740;&#31350;&#65292;&#36825;&#20123;&#26550;&#26500;&#19981;&#20165;&#32771;&#34385;&#20102;&#20854;&#20934;&#30830;&#24615;&#65292;&#36824;&#32771;&#34385;&#20102;&#23558;&#20854;&#37096;&#32626;&#22312;&#35760;&#24518;&#21644;&#33021;&#37327;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;(IoT)&#36793;&#32536;&#33410;&#28857;&#19978;&#30340;&#25104;&#26412;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;6&#31181;&#19981;&#21516;&#30340;DL&#26550;&#26500;&#65292;&#22312;&#19968;&#20010;&#26032;&#39062;&#30340;&#30001;&#21830;&#19994;8x8&#38453;&#21015;&#25910;&#38598;&#30340;IR&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#65292;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#20844;&#24320;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#12290;&#22312;&#27599;&#31181;&#27169;&#22411;&#31867;&#22411;&#30340;&#24191;&#27867;&#26550;&#26500;&#25506;&#32034;&#19979;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#32452;&#23500;&#26377;&#25104;&#25928;&#30340;Pareto&#26368;&#20248;&#35299;&#65292;&#20854;&#20132;&#21449;&#39564;&#35777;&#24179;&#34913;&#20934;&#30830;&#24615;&#24471;&#20998;&#33539;&#22260;&#20026;55.70%-82.70%&#12290;&#24403;&#22312;S&#21830;&#19994;&#24494;&#25511;&#21046;&#22120;(MCU)&#19978;&#37096;&#32626;&#26102;&#65292;&#25152;&#26377;DL&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#19978;&#22343;&#21487;&#23454;&#29616;&#23454;&#26102;&#24615;&#65292;&#24182;&#22312;&#30465;&#30005;&#27169;&#24335;&#19979;&#28040;&#32791;&#26497;&#23569;&#30340;&#33021;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ultra-low-resolution Infrared (IR) array sensors offer a low-cost, energy-efficient, and privacy-preserving solution for people counting, with applications such as occupancy monitoring. Previous work has shown that Deep Learning (DL) can yield superior performance on this task. However, the literature was missing an extensive comparative analysis of various efficient DL architectures for IR array-based people counting, that considers not only their accuracy, but also the cost of deploying them on memory- and energy-constrained Internet of Things (IoT) edge nodes. In this work, we address this need by comparing 6 different DL architectures on a novel dataset composed of IR images collected from a commercial 8x8 array, which we made openly available. With a wide architectural exploration of each model type, we obtain a rich set of Pareto-optimal solutions, spanning cross-validated balanced accuracy scores in the 55.70-82.70% range. When deployed on a commercial Microcontroller (MCU) by S
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#33539;&#25968;&#27491;&#21017;&#21270;&#30340;&#36890;&#29992;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#38477;&#20302;&#29615;&#22659;&#29305;&#24449;&#30340;&#24433;&#21709;&#24182;&#40723;&#21169;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#30340;&#29305;&#24449;&#65292;&#33021;&#22815;&#22312;&#24191;&#27867;&#30340;&#39046;&#22495;&#36890;&#29992;&#24615;&#20219;&#21153;&#20013;&#33719;&#24471;&#27604;&#22522;&#32447;&#26356;&#24378;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07527</link><description>&lt;p&gt;
&#36890;&#36807;&#26680;&#33539;&#25968;&#27491;&#21017;&#21270;&#23454;&#29616;&#39046;&#22495;&#36890;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization via Nuclear Norm Regularization. (arXiv:2303.07527v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#33539;&#25968;&#27491;&#21017;&#21270;&#30340;&#36890;&#29992;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#38477;&#20302;&#29615;&#22659;&#29305;&#24449;&#30340;&#24433;&#21709;&#24182;&#40723;&#21169;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#30340;&#29305;&#24449;&#65292;&#33021;&#22815;&#22312;&#24191;&#27867;&#30340;&#39046;&#22495;&#36890;&#29992;&#24615;&#20219;&#21153;&#20013;&#33719;&#24471;&#27604;&#22522;&#32447;&#26356;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#20165;&#26377;&#26377;&#38480;&#22495;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20855;&#22791;&#23545;&#26410;&#30693;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#23588;&#20026;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#29305;&#24449;&#26680;&#33539;&#25968;&#30340;&#31616;&#21333;&#26377;&#25928;&#30340;&#39046;&#22495;&#36890;&#29992;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#30452;&#35266;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#20943;&#23569;&#20102;&#29615;&#22659;&#29305;&#24449;&#30340;&#24433;&#21709;&#65292;&#40723;&#21169;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#30340;&#29305;&#24449;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#20851;&#20026;&#20160;&#20040;&#30456;&#27604;&#20110;&#26368;&#23567;&#21270;&#32463;&#39564;&#39118;&#38505;&#25110;&#20854;&#20182;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#26680;&#33539;&#25968;&#27491;&#21017;&#21270;&#26356;&#21152;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;&#23454;&#39564;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#35797;&#39564;&#35777;&#26126;&#65292;&#26680;&#33539;&#25968;&#27491;&#21017;&#21270;&#22312;&#24191;&#27867;&#30340;&#39046;&#22495;&#36890;&#29992;&#24615;&#20219;&#21153;&#20013;&#19982;&#22522;&#32447;&#30456;&#27604;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#26041;&#27861;&#65292;&#22914;ERM&#21644;SWAD&#65292;&#19988;&#34920;&#29616;&#25345;&#32493;&#25552;&#39640;&#65292;&#20363;&#22914;&#27979;&#35797;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;1.7&#65285;&#21644;0.9&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to generalize to unseen domains is crucial for machine learning systems deployed in the real world, especially when we only have data from limited training domains. In this paper, we propose a simple and effective regularization method based on the nuclear norm of the learned features for domain generalization. Intuitively, the proposed regularizer mitigates the impacts of environmental features and encourages learning domain-invariant features. Theoretically, we provide insights into why nuclear norm regularization is more effective compared to ERM and alternative regularization methods. Empirically, we conduct extensive experiments on both synthetic and real datasets. We show that nuclear norm regularization achieves strong performance compared to baselines in a wide range of domain generalization tasks. Moreover, our regularizer is broadly applicable with various methods such as ERM and SWAD with consistently improved performance, e.g., 1.7% and 0.9% test accuracy improv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#32771;&#34385;&#20102;&#21307;&#23398;&#22270;&#20687;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#38544;&#34255;&#34920;&#31034;&#65292;&#36890;&#36807;&#27604;&#36739;&#22312;ImageNet&#21644;RadImageNet&#19978;&#30340;&#21021;&#22987;&#21270;&#65292;&#21457;&#29616;ImageNet&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#21307;&#23398;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;RadImageNet&#19978;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.08272</link><description>&lt;p&gt;
&#37325;&#26032;&#32771;&#34385;&#21307;&#23398;&#22270;&#20687;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#38544;&#34255;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Revisiting Hidden Representations in Transfer Learning for Medical Imaging. (arXiv:2302.08272v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#32771;&#34385;&#20102;&#21307;&#23398;&#22270;&#20687;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#38544;&#34255;&#34920;&#31034;&#65292;&#36890;&#36807;&#27604;&#36739;&#22312;ImageNet&#21644;RadImageNet&#19978;&#30340;&#21021;&#22987;&#21270;&#65292;&#21457;&#29616;ImageNet&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#21307;&#23398;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;RadImageNet&#19978;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#20855;&#26377;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20294;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#36890;&#24120;&#22312;&#22810;&#26679;&#24615;&#21644;&#35268;&#27169;&#19978;&#21463;&#21040;&#38480;&#21046;&#12290;&#36801;&#31227;&#23398;&#20064;&#26377;&#21487;&#33021;&#24357;&#21512;&#30456;&#20851;&#20294;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21307;&#23398;&#24212;&#29992;&#32780;&#35328;&#65292;&#39044;&#35757;&#32451;&#33258;&#28982;&#22270;&#20687;&#36824;&#26159;&#21307;&#23398;&#22270;&#20687;&#26356;&#26377;&#30410;&#20173;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#23545;&#27604;&#22312;ImageNet&#21644;RadImageNet&#19978;&#30340;&#21021;&#22987;&#21270;&#65292;&#22312;&#19971;&#20010;&#21307;&#23398;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#30740;&#31350;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21253;&#25324;&#22797;&#21046;&#24615;&#30740;&#31350;&#65292;&#20854;&#32467;&#26524;&#19982;&#20808;&#21069;&#21457;&#34920;&#30340;&#30740;&#31350;&#30456;&#21453;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;ImageNet&#19978;&#39044;&#35757;&#32451;&#30340;ResNet50&#27169;&#22411;&#24448;&#24448;&#20248;&#20110;&#22312;RadImageNet&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20102;&#35299;&#65292;&#25105;&#20204;&#20351;&#29992;&#20856;&#22411;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#30740;&#31350;&#20102;&#23398;&#20064;&#24471;&#21040;&#30340;&#34920;&#31034;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#30452;&#35273;&#30456;&#21453;&#65292;ImageNet&#21644;RadImageNet&#21487;&#33021;&#20250;&#25910;&#25947;&#21040;&#19981;&#21516;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
While a key component to the success of deep learning is the availability of massive amounts of training data, medical image datasets are often limited in diversity and size. Transfer learning has the potential to bridge the gap between related yet different domains. For medical applications, however, it remains unclear whether it is more beneficial to pre-train on natural or medical images. We aim to shed light on this problem by comparing initialization on ImageNet and RadImageNet on seven medical classification tasks. Our work includes a replication study, which yields results contrary to previously published findings. In our experiments, ResNet50 models pre-trained on ImageNet tend to outperform those trained on RadImageNet. To gain further insights, we investigate the learned representations using Canonical Correlation Analysis (CCA) and compare the predictions of the different models. Our results indicate that, contrary to intuition, ImageNet and RadImageNet may converge to disti
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#23384;&#22312;&#38750;&#39640;&#26031;&#22122;&#22768;&#24773;&#20917;&#19979;&#22833;&#25928;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;&#19968;&#20010;&#33021;&#37327;&#27169;&#22411;&#26469;&#23398;&#20064;&#27491;&#30830;&#30340;&#22122;&#22768;&#20998;&#24067;&#12290;&#36890;&#36807;&#22810;&#20010;&#20363;&#23376;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.15498</link><description>&lt;p&gt;
&#20855;&#26377;&#26410;&#30693;&#27979;&#37327;&#22122;&#22768;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks with unknown measurement noise. (arXiv:2211.15498v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15498
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#23384;&#22312;&#38750;&#39640;&#26031;&#22122;&#22768;&#24773;&#20917;&#19979;&#22833;&#25928;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;&#19968;&#20010;&#33021;&#37327;&#27169;&#22411;&#26469;&#23398;&#20064;&#27491;&#30830;&#30340;&#22122;&#22768;&#20998;&#24067;&#12290;&#36890;&#36807;&#22810;&#20010;&#20363;&#23376;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#26159;&#19968;&#31181;&#26082;&#33021;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#21448;&#33021;&#35782;&#21035;&#20559;&#24494;&#20998;&#26041;&#31243;&#21442;&#25968;&#30340;&#28789;&#27963;&#26041;&#27861;&#12290;&#22823;&#22810;&#25968;&#30456;&#20851;&#30340;&#30740;&#31350;&#37117;&#20551;&#35774;&#25968;&#25454;&#26159;&#26080;&#22122;&#22768;&#30340;&#65292;&#25110;&#32773;&#26159;&#21463;&#24369;&#39640;&#26031;&#22122;&#22768;&#27745;&#26579;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;PINN&#26694;&#26550;&#22312;&#38750;&#39640;&#26031;&#22122;&#22768;&#24773;&#20917;&#19979;&#22833;&#25928;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#20010;&#26681;&#26412;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21363;&#21516;&#26102;&#35757;&#32451;&#19968;&#20010;&#33021;&#37327;&#27169;&#22411;(Energy-Based Model, EBM)&#26469;&#23398;&#20064;&#27491;&#30830;&#30340;&#22122;&#22768;&#20998;&#24067;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#20010;&#20363;&#23376;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) constitute a flexible approach to both finding solutions and identifying parameters of partial differential equations. Most works on the topic assume noiseless data, or data contaminated by weak Gaussian noise. We show that the standard PINN framework breaks down in case of non-Gaussian noise. We give a way of resolving this fundamental issue and we propose to jointly train an energy-based model (EBM) to learn the correct noise distribution. We illustrate the improved performance of our approach using multiple examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Grassmann&#27969;&#24418;&#23398;&#20064;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#31283;&#23450;&#30340;&#24418;&#29366;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2211.02900</link><description>&lt;p&gt;
Grassmann&#27969;&#24418;&#27969;&#29992;&#20110;&#31283;&#23450;&#24418;&#29366;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Grassmann Manifold Flows for Stable Shape Generation. (arXiv:2211.02900v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Grassmann&#27969;&#24418;&#23398;&#20064;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#31283;&#23450;&#30340;&#24418;&#29366;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#21033;&#29992;&#29305;&#23450;&#27969;&#24418;&#20013;&#30340;&#23545;&#31216;&#24615;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;&#30340;&#26041;&#27861;&#19978;&#12290;Grassmann&#27969;&#24418;&#25552;&#20379;&#20102;&#22788;&#29702;&#20197;&#24418;&#29366;&#31354;&#38388;&#34920;&#31034;&#30340;&#22522;&#26412;&#24418;&#29366;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#30340;&#24418;&#29366;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#32493;&#30340;&#24402;&#19968;&#21270;&#27969;&#22312;Grassmann&#27969;&#24418;&#19978;&#24314;&#31435;&#23398;&#20064;&#20998;&#24067;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#26126;&#30830;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#31283;&#23450;&#30340;&#24418;&#29366;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;Grassmann&#27969;&#24418;&#20869;&#23398;&#20064;&#21644;&#29983;&#25104;&#65292;&#26377;&#25928;&#22320;&#28040;&#38500;&#20102;&#26059;&#36716;&#21644;&#32763;&#36716;&#31561;&#22806;&#37096;&#21464;&#25442;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#31283;&#20581;&#30340;&#29983;&#25104;&#65292;&#20197;&#36866;&#24212;&#23545;&#35937;&#30340;&#22522;&#26412;&#24418;&#29366;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#25429;&#25417;&#25968;&#25454;&#32467;&#26500;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;t&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, studies on machine learning have focused on methods that use symmetry implicit in a specific manifold as an inductive bias. Grassmann manifolds provide the ability to handle fundamental shapes represented as shape spaces, enabling stable shape analysis. In this paper, we present a novel approach in which we establish the theoretical foundations for learning distributions on the Grassmann manifold via continuous normalization flows, with the explicit goal of generating stable shapes. Our approach facilitates more robust generation by effectively eliminating the influence of extraneous transformations, such as rotations and inversions, through learning and generating within a Grassmann manifolds designed to accommodate the essential shape information of the object. The experimental results indicated that the proposed method can generate high-quality samples by capturing the data structure. Furthermore, the proposed method significantly outperformed state-of-the-art methods in t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#22120;USR&#65292;&#36890;&#36807;&#26500;&#24314;&#36716;&#25442;&#20989;&#25968;&#21442;&#25968;&#31354;&#38388;&#19978;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#26469;&#25552;&#39640;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#20540;&#20989;&#25968;&#36827;&#34892;&#23545;&#25239;&#29983;&#25104;&#26410;&#30693;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;USR&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#24471;&#21040;&#20102;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.02016</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#27491;&#21017;&#21270;&#30340;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Reinforcement Learning in Continuous Control Tasks with Uncertainty Set Regularization. (arXiv:2207.02016v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02016
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#22120;USR&#65292;&#36890;&#36807;&#26500;&#24314;&#36716;&#25442;&#20989;&#25968;&#21442;&#25968;&#31354;&#38388;&#19978;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#26469;&#25552;&#39640;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#20540;&#20989;&#25968;&#36827;&#34892;&#23545;&#25239;&#29983;&#25104;&#26410;&#30693;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;USR&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#24471;&#21040;&#20102;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#34987;&#35748;&#20026;&#22312;&#29615;&#22659;&#25200;&#21160;&#19979;&#32570;&#20047;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#20854;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#22768;&#31216;&#65292;&#22312;&#20540;&#20989;&#25968;&#20013;&#28155;&#21152;&#27491;&#21017;&#21270;&#31561;&#20215;&#20110;&#23398;&#20064;&#20855;&#26377;&#19981;&#30830;&#23450;&#36716;&#25442;&#30340;&#40065;&#26834;&#31574;&#30053;&#12290;&#23613;&#31649;&#27491;&#21017;&#21270;-&#40065;&#26834;&#24615;&#36716;&#25442;&#22240;&#20854;&#31616;&#21333;&#21644;&#39640;&#25928;&#32780;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#20173;&#28982;&#23384;&#22312;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#21517;&#20026;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#27491;&#21017;&#21270;&#22120;&#65288;USR&#65289;&#65292;&#36890;&#36807;&#22312;&#36716;&#25442;&#20989;&#25968;&#30340;&#21442;&#25968;&#31354;&#38388;&#19978;&#26500;&#24314;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#26469;&#23454;&#29616;&#12290;&#29305;&#21035;&#26159;&#65292;USR&#36275;&#22815;&#28789;&#27963;&#65292;&#21487;&#20197;&#25554;&#20837;&#21040;&#20219;&#20309;&#29616;&#26377;&#30340;RL&#26694;&#26550;&#20013;&#12290;&#20026;&#20102;&#22788;&#29702;&#26410;&#30693;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#20989;&#25968;&#29983;&#25104;&#30340;&#26032;&#39062;&#23545;&#25239;&#26041;&#27861;&#26469;&#29983;&#25104;&#23427;&#20204;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#24378;&#21270;&#23398;&#20064;&#65288;RWRL&#65289;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;USR&#65292;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is recognized as lacking generalization and robustness under environmental perturbations, which excessively restricts its application for real-world robotics. Prior work claimed that adding regularization to the value function is equivalent to learning a robust policy with uncertain transitions. Although the regularization-robustness transformation is appealing for its simplicity and efficiency, it is still lacking in continuous control tasks. In this paper, we propose a new regularizer named $\textbf{U}$ncertainty $\textbf{S}$et $\textbf{R}$egularizer (USR), by formulating the uncertainty set on the parameter space of the transition function. In particular, USR is flexible enough to be plugged into any existing RL framework. To deal with unknown uncertainty sets, we further propose a novel adversarial approach to generate them based on the value function. We evaluate USR on the Real-world Reinforcement Learning (RWRL) benchmark, demonstrating improvements i
&lt;/p&gt;</description></item></channel></rss>