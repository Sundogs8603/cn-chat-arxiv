<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22522;&#20110;&#20302;&#31209;&#21152;&#23545;&#35282;&#32447;&#21442;&#25968;&#21270;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21051;&#30011;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#35823;&#24046;&#30340;&#33258;&#30456;&#20851;&#24615;&#65292;&#24182;&#20855;&#26377;&#22797;&#26434;&#24230;&#20302;&#12289;&#26657;&#20934;&#39044;&#27979;&#20934;&#30830;&#24615;&#39640;&#31561;&#20248;&#28857;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01000</link><description>&lt;p&gt;
&#22810;&#20803;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#19982;&#30456;&#20851;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Multivariate Probabilistic Time Series Forecasting with Correlated Errors
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22522;&#20110;&#20302;&#31209;&#21152;&#23545;&#35282;&#32447;&#21442;&#25968;&#21270;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21051;&#30011;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#35823;&#24046;&#30340;&#33258;&#30456;&#20851;&#24615;&#65292;&#24182;&#20855;&#26377;&#22797;&#26434;&#24230;&#20302;&#12289;&#26657;&#20934;&#39044;&#27979;&#20934;&#30830;&#24615;&#39640;&#31561;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#35823;&#24046;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#19982;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#37327;&#21270;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;&#26368;&#36817;&#30340;&#22810;&#20803;&#27169;&#22411;&#22312;&#32771;&#34385;&#35823;&#24046;&#20043;&#38388;&#30340;&#21516;&#26102;&#30456;&#20851;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#65292;&#23545;&#20110;&#32479;&#35745;&#31616;&#21270;&#30340;&#30446;&#30340;&#65292;&#23545;&#36825;&#20123;&#35823;&#24046;&#30340;&#24120;&#35265;&#20551;&#35774;&#26159;&#23427;&#20204;&#22312;&#26102;&#38388;&#19978;&#26159;&#29420;&#31435;&#30340;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#35266;&#27979;&#24448;&#24448;&#20559;&#31163;&#20102;&#36825;&#20010;&#20551;&#35774;&#65292;&#22240;&#20026;&#35823;&#24046;&#36890;&#24120;&#30001;&#20110;&#21508;&#31181;&#22240;&#32032;&#65288;&#22914;&#25490;&#38500;&#26102;&#38388;&#30456;&#20851;&#30340;&#21327;&#21464;&#37327;&#65289;&#32780;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#33258;&#30456;&#20851;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#21152;&#23545;&#35282;&#32447;&#21442;&#25968;&#21270;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21051;&#30011;&#35823;&#24046;&#30340;&#33258;&#30456;&#20851;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#20960;&#20010;&#21487;&#21462;&#30340;&#29305;&#24615;&#65306;&#22797;&#26434;&#24230;&#19981;&#38543;&#26102;&#38388;&#24207;&#21015;&#25968;&#30446;&#22686;&#21152;&#65292;&#24471;&#21040;&#30340;&#21327;&#26041;&#24046;&#21487;&#20197;&#29992;&#20110;&#26657;&#20934;&#39044;&#27979;&#65292;&#19988;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling the correlations among errors is closely associated with how accurately the model can quantify predictive uncertainty in probabilistic time series forecasting. Recent multivariate models have made significant progress in accounting for contemporaneous correlations among errors, while a common assumption on these errors is that they are temporally independent for the sake of statistical simplicity. However, real-world observations often deviate from this assumption, since errors usually exhibit substantial autocorrelation due to various factors such as the exclusion of temporally correlated covariates. In this work, we propose an efficient method, based on a low-rank-plus-diagonal parameterization of the covariance matrix, which can effectively characterize the autocorrelation of errors. The proposed method possesses several desirable properties: the complexity does not scale with the number of time series, the resulting covariance can be used for calibrating predictions, and i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#34507;&#30333;&#36136;&#21644;&#23567;&#20998;&#23376;&#30456;&#20851;&#20219;&#21153;&#20013;&#36229;&#36234;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#32479;&#19968;&#20998;&#23376;&#24314;&#27169;&#23454;&#29616;&#20102;&#20805;&#20998;&#21033;&#29992;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.12995</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#32479;&#19968;&#20998;&#23376;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Multi-Scale Protein Language Model for Unified Molecular Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12995
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#34507;&#30333;&#36136;&#21644;&#23567;&#20998;&#23376;&#30456;&#20851;&#20219;&#21153;&#20013;&#36229;&#36234;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#32479;&#19968;&#20998;&#23376;&#24314;&#27169;&#23454;&#29616;&#20102;&#20805;&#20998;&#21033;&#29992;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#22312;&#34507;&#30333;&#36136;&#24037;&#31243;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#20027;&#35201;&#22312;&#27531;&#22522;&#32423;&#21035;&#19978;&#36816;&#34892;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21407;&#23376;&#27700;&#24179;&#25552;&#20379;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#36825;&#19968;&#38480;&#21046;&#38459;&#30861;&#20102;&#25105;&#20204;&#20805;&#20998;&#21033;&#29992;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#22312;&#28041;&#21450;&#34507;&#30333;&#36136;&#21644;&#23567;&#20998;&#23376;&#30340;&#24212;&#29992;&#20013;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ms-ESM&#65288;&#22810;&#23610;&#24230;ESM&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#23610;&#24230;&#32479;&#19968;&#20998;&#23376;&#24314;&#27169;&#12290;ms-ESM&#36890;&#36807;&#39044;&#35757;&#32451;&#22810;&#23610;&#24230;&#20195;&#30721;&#20999;&#25442;&#34507;&#30333;&#36136;&#24207;&#21015;&#24182;&#21033;&#29992;&#22810;&#23610;&#24230;&#20301;&#32622;&#32534;&#30721;&#26469;&#25429;&#33719;&#27531;&#22522;&#21644;&#21407;&#23376;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ms-ESM&#22312;&#34507;&#30333;&#36136;-&#20998;&#23376;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#23545;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#25581;&#31034;&#36890;&#36807;&#32479;&#19968;&#20998;&#23376;&#24314;&#27169;&#65292;ms-ESM&#19981;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12995v1 Announce Type: cross  Abstract: Protein language models have demonstrated significant potential in the field of protein engineering. However, current protein language models primarily operate at the residue scale, which limits their ability to provide information at the atom level. This limitation prevents us from fully exploiting the capabilities of protein language models for applications involving both proteins and small molecules. In this paper, we propose ms-ESM (multi-scale ESM), a novel approach that enables multi-scale unified molecular modeling. ms-ESM achieves this by pre-training on multi-scale code-switch protein sequences and utilizing a multi-scale position encoding to capture relationships among residues and atoms. Experimental results indicate that ms-ESM surpasses previous methods in protein-molecule tasks, demonstrating the full utilization of protein language models. Further investigations reveal that through unified molecular modeling, ms-ESM not 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#26680;&#24515;&#23376;&#38598;&#36873;&#25321;&#36827;&#34892;&#25968;&#25454;&#37325;&#26032;&#21152;&#26435;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#20248;&#21270;&#20102;&#35745;&#31639;&#26102;&#38388;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#31361;&#26174;&#20854;&#20316;&#20026;&#27169;&#22411;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#21644;&#31934;&#30830;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.12166</link><description>&lt;p&gt;
&#23569;&#25968;&#20010;&#20307;&#30340;&#21147;&#37327;&#65306;&#21033;&#29992;&#26680;&#24515;&#23376;&#38598;&#36873;&#25321;&#21152;&#36895;&#21644;&#20248;&#21270;&#25968;&#25454;&#37325;&#26032;&#21152;&#26435;
&lt;/p&gt;
&lt;p&gt;
The Power of Few: Accelerating and Enhancing Data Reweighting with Coreset Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12166
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#26680;&#24515;&#23376;&#38598;&#36873;&#25321;&#36827;&#34892;&#25968;&#25454;&#37325;&#26032;&#21152;&#26435;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#20248;&#21270;&#20102;&#35745;&#31639;&#26102;&#38388;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#31361;&#26174;&#20854;&#20316;&#20026;&#27169;&#22411;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#21644;&#31934;&#30830;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#19981;&#26029;&#21457;&#23637;&#65292;&#36235;&#21183;&#26159;&#25910;&#38598;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#24182;&#35757;&#32451;&#35268;&#27169;&#36234;&#26469;&#36234;&#22823;&#30340;&#27169;&#22411;&#12290;&#34429;&#28982;&#36825;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#20294;&#20063;&#23558;&#35745;&#31639;&#25104;&#26412;&#25552;&#39640;&#21040;&#19981;&#21487;&#25345;&#32493;&#30340;&#27700;&#24179;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#24494;&#22937;&#30340;&#24179;&#34913;&#65292;&#36825;&#26159;&#35813;&#39046;&#22495;&#20013;&#19968;&#30452;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#26680;&#24515;&#23376;&#38598;&#36873;&#25321;&#36827;&#34892;&#37325;&#26032;&#21152;&#26435;&#30340;&#26032;&#26041;&#27861;&#65292;&#26377;&#25928;&#20248;&#21270;&#20102;&#35745;&#31639;&#26102;&#38388;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;&#36890;&#36807;&#19987;&#27880;&#20110; strategically selected coreset&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#31283;&#20581;&#30340;&#34920;&#31034;&#65292;&#22240;&#20026;&#23427;&#26377;&#25928;&#22320;&#26368;&#23567;&#21270;&#20102;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#37325;&#26032;&#26657;&#20934;&#30340;&#26435;&#37325;&#34987;&#26144;&#23556;&#22238;&#24182;&#20256;&#25773;&#21040;&#25972;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#31361;&#26174;&#20102;&#23427;&#20316;&#20026;&#27169;&#22411;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#21644;&#31934;&#30830;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12166v1 Announce Type: new  Abstract: As machine learning tasks continue to evolve, the trend has been to gather larger datasets and train increasingly larger models. While this has led to advancements in accuracy, it has also escalated computational costs to unsustainable levels. Addressing this, our work aims to strike a delicate balance between computational efficiency and model accuracy, a persisting challenge in the field. We introduce a novel method that employs core subset selection for reweighting, effectively optimizing both computational time and model performance. By focusing on a strategically selected coreset, our approach offers a robust representation, as it efficiently minimizes the influence of outliers. The re-calibrated weights are then mapped back to and propagated across the entire dataset. Our experimental results substantiate the effectiveness of this approach, underscoring its potential as a scalable and precise solution for model training.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36845;&#20195;&#33258;&#25105;&#35757;&#32451;&#26041;&#27861;&#26469;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#35299;&#20915;&#20108;&#32500;&#26680;&#30913;&#20849;&#25391;&#65288;2D NMR&#65289;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#65292;&#24357;&#34917;&#20102;&#32570;&#20047;&#26631;&#27880;NMR&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.11353</link><description>&lt;p&gt;
&#28342;&#21058;&#24863;&#30693;&#30340;2D&#26680;&#30913;&#20849;&#25391;&#39044;&#27979;&#65306;&#21033;&#29992;&#22810;&#20219;&#21153;&#35757;&#32451;&#21644;&#36845;&#20195;&#33258;&#35757;&#32451;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Solvent-Aware 2D NMR Prediction: Leveraging Multi-Tasking Training and Iterative Self-Training Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11353
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36845;&#20195;&#33258;&#25105;&#35757;&#32451;&#26041;&#27861;&#26469;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#35299;&#20915;&#20108;&#32500;&#26680;&#30913;&#20849;&#25391;&#65288;2D NMR&#65289;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#65292;&#24357;&#34917;&#20102;&#32570;&#20047;&#26631;&#27880;NMR&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#30913;&#20849;&#25391;&#65288;NMR&#65289;&#20809;&#35889;&#22312;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#20998;&#23376;&#30340;&#32467;&#26500;&#20449;&#24687;&#12289;&#30005;&#23376;&#24615;&#36136;&#21644;&#21160;&#24577;&#34892;&#20026;&#30340;&#35265;&#35299;&#12290;&#20934;&#30830;&#30340;NMR&#20809;&#35889;&#39044;&#27979;&#33021;&#22815;&#39640;&#25928;&#22320;&#29983;&#25104;&#20505;&#36873;&#20998;&#23376;&#65292;&#20351;&#21270;&#23398;&#23478;&#33021;&#22815;&#23558;&#23427;&#20204;&#19982;&#23454;&#38469;&#23454;&#39564;&#20809;&#35889;&#36827;&#34892;&#27604;&#36739;&#12290;&#35813;&#36807;&#31243;&#26377;&#21161;&#20110;&#30830;&#35748;&#20998;&#23376;&#32467;&#26500;&#25110;&#25351;&#20986;&#24046;&#24322;&#65292;&#24341;&#23548;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#20998;&#23376;&#32467;&#26500;&#39044;&#27979;&#20998;&#23376;&#30340;&#21407;&#23376;NMR&#21270;&#23398;&#20301;&#31227;&#12290;&#34429;&#28982;&#22312;&#39044;&#27979;&#19968;&#32500;&#65288;1D&#65289;NMR&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#20108;&#32500;&#65288;2D&#65289;NMR&#39044;&#27979;&#20173;&#28982;&#26159;&#19968;&#39033;&#25361;&#25112;&#65292;&#22240;&#20026;&#32570;&#20047;&#29992;&#20110;&#35757;&#32451;&#30340;&#26631;&#27880;&#30340;NMR&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#33258;&#35757;&#32451;&#65288;IST&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#21407;&#23376;2DNMR&#20301;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11353v1 Announce Type: cross  Abstract: Nuclear magnetic resonance (NMR) spectroscopy plays a pivotal role in various scientific fields, offering insights into structural information, electronic properties and dynamic behaviors of molecules. Accurate NMR spectrum prediction efficiently produces candidate molecules, enabling chemists to compare them with actual experimental spectra. This process aids in confirming molecular structures or pinpointing discrepancies, guiding further investigation. Machine Learning (ML) has then emerged as a promising alternative approach for predicting atomic NMR chemical shits of molecules given their structures. Although significant progresses have been made in predicting one-dimensional (1D) NMR, two-dimensional (2D) NMR prediction via ML remains a challenge due to the lack of annotated NMR training datasets. To address this gap, we propose an iterative self-training (IST) approach to train a deep learning model for predicting atomic 2DNMR sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23581;&#35797;&#24635;&#32467;&#21644;&#35780;&#20272;&#30001;&#35813;&#39046;&#22495;&#36804;&#20170;&#36827;&#23637;&#32780;&#24418;&#25104;&#30340;NLP&#39564;&#35777;&#27969;&#31243;&#30340;&#19968;&#33324;&#32452;&#25104;&#37096;&#20998;&#65292;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#23558;&#21477;&#23376;&#23884;&#20837;&#36830;&#32493;&#31354;&#38388;&#24471;&#21040;&#30340;&#21487;&#39564;&#35777;&#23376;&#31354;&#38388;&#30340;&#19968;&#33324;&#25551;&#36848;&#12290;</title><link>https://arxiv.org/abs/2403.10144</link><description>&lt;p&gt;
NLP&#39564;&#35777;&#65306;&#36208;&#21521;&#19968;&#31181;&#36890;&#29992;&#30340;&#29992;&#20110;&#35748;&#35777;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
NLP Verification: Towards a General Methodology for Certifying Robustness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23581;&#35797;&#24635;&#32467;&#21644;&#35780;&#20272;&#30001;&#35813;&#39046;&#22495;&#36804;&#20170;&#36827;&#23637;&#32780;&#24418;&#25104;&#30340;NLP&#39564;&#35777;&#27969;&#31243;&#30340;&#19968;&#33324;&#32452;&#25104;&#37096;&#20998;&#65292;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#23558;&#21477;&#23376;&#23884;&#20837;&#36830;&#32493;&#31354;&#38388;&#24471;&#21040;&#30340;&#21487;&#39564;&#35777;&#23376;&#31354;&#38388;&#30340;&#19968;&#33324;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#30830;&#20445;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#65306;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#24773;&#22659;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#24517;&#39035;&#23545;&#21464;&#21270;&#25110;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#33021;&#23545;&#20854;&#36755;&#20986;&#32473;&#20986;&#20445;&#35777;&#12290;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#19981;&#21516;&#65292;NLP&#32570;&#20047;&#19968;&#20010;&#32479;&#19968;&#30340;&#39564;&#35777;&#26041;&#27861;&#35770;&#65292;&#23613;&#31649;&#36817;&#24180;&#26469;&#25991;&#29486;&#20013;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;NLP&#39564;&#35777;&#30340;&#23454;&#29992;&#38382;&#39064;&#24120;&#24120;&#28041;&#21450;&#19981;&#28145;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#25552;&#28860;&#21644;&#35780;&#20272;&#19968;&#20010;NLP&#39564;&#35777;&#27969;&#31243;&#30340;&#19968;&#33324;&#32452;&#25104;&#37096;&#20998;&#65292;&#35813;&#27969;&#31243;&#26469;&#28304;&#20110;&#36804;&#20170;&#20026;&#27490;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#23558;&#21477;&#23376;&#23884;&#20837;&#36830;&#32493;&#31354;&#38388;&#24471;&#21040;&#30340;&#21487;&#39564;&#35777;&#23376;&#31354;&#38388;&#30340;&#19968;&#33324;&#25551;&#36848;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#21487;&#39564;&#35777;&#23376;&#31354;&#38388;&#30340;&#35821;&#20041;&#27867;&#21270;&#25216;&#26415;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10144v1 Announce Type: cross  Abstract: Deep neural networks have exhibited substantial success in the field of Natural Language Processing (NLP) and ensuring their safety and reliability is crucial: there are safety critical contexts where such models must be robust to variability or attack, and give guarantees over their output. Unlike Computer Vision, NLP lacks a unified verification methodology and, despite recent advancements in literature, they are often light on the pragmatical issues of NLP verification. In this paper, we make an attempt to distil and evaluate general components of an NLP verification pipeline, that emerges from the progress in the field to date. Our contributions are two-fold. Firstly, we give a general characterisation of verifiable subspaces that result from embedding sentences into continuous spaces. We identify, and give an effective method to deal with, the technical challenge of semantic generalisability of verified subspaces; and propose it a
&lt;/p&gt;</description></item><item><title>QDAC&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20215;&#20540;&#20989;&#25968;&#35780;&#35770;&#23478;&#21644;&#32487;&#25215;&#29305;&#24449;&#35780;&#35770;&#23478;&#23398;&#20064;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2403.09930</link><description>&lt;p&gt;
&#36136;&#37327;&#22810;&#26679;&#24615;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65306;&#36890;&#36807;&#20540;&#21644;&#32487;&#25215;&#29305;&#24449;&#35780;&#35770;&#23478;&#23398;&#20064;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Quality-Diversity Actor-Critic: Learning High-Performing and Diverse Behaviors via Value and Successor Features Critics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09930
&lt;/p&gt;
&lt;p&gt;
QDAC&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20215;&#20540;&#20989;&#25968;&#35780;&#35770;&#23478;&#21644;&#32487;&#25215;&#29305;&#24449;&#35780;&#35770;&#23478;&#23398;&#20064;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#34920;&#29616;&#20986;&#36866;&#24212;&#24847;&#22806;&#24773;&#20917;&#30340;&#24191;&#27867;&#34892;&#20026;&#35889;&#12290;&#36807;&#21435;&#21313;&#24180;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36827;&#27493;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#25104;&#23601;&#65292;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#30340;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#21482;&#36820;&#22238;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36136;&#37327;&#22810;&#26679;&#24615;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;QDAC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#20215;&#20540;&#20989;&#25968;&#35780;&#35770;&#23478;&#21644;&#32487;&#25215;&#29305;&#24449;&#35780;&#35770;&#23478;&#23398;&#20064;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#34892;&#20026;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#28436;&#21592;&#36890;&#36807;&#21463;&#38480;&#20248;&#21270;&#26469;&#26368;&#22823;&#21270;&#22238;&#25253;&#24182;&#25191;&#34892;&#22810;&#26679;&#24615;&#25216;&#33021;&#30340;&#23458;&#35266;&#20989;&#25968;&#65292;&#26080;&#32541;&#32479;&#19968;&#20102;&#20004;&#20010;&#35780;&#35770;&#23478;&#12290;&#19982;&#20854;&#20182;&#36136;&#37327;&#22810;&#26679;&#24615;&#26041;&#27861;&#30456;&#27604;&#65292;QDAC&#22312;&#20845;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36830;&#32493;&#25511;&#21046;&#36816;&#21160;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26174;&#30528;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#26356;&#22810;&#26679;&#24615;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09930v1 Announce Type: cross  Abstract: A key aspect of intelligence is the ability to demonstrate a broad spectrum of behaviors for adapting to unexpected situations. Over the past decade, advancements in deep reinforcement learning have led to groundbreaking achievements to solve complex continuous control tasks. However, most approaches return only one solution specialized for a specific problem. We introduce Quality-Diversity Actor-Critic (QDAC), an off-policy actor-critic deep reinforcement learning algorithm that leverages a value function critic and a successor features critic to learn high-performing and diverse behaviors. In this framework, the actor optimizes an objective that seamlessly unifies both critics using constrained optimization to (1) maximize return, while (2) executing diverse skills. Compared with other Quality-Diversity methods, QDAC achieves significantly higher performance and more diverse behaviors on six challenging continuous control locomotion 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36719;&#32422;&#26463;&#21462;&#20195;&#30828;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19987;&#23478;&#28151;&#21512;&#20808;&#39564;&#65292;&#25913;&#21892;&#20102;&#22810;&#27169;&#24577;VAEs&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.05300</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;VAEs&#20013;&#30340;&#32479;&#19968;&#22810;&#26679;&#24615;&#65306;&#25913;&#36827;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unity by Diversity: Improved Representation Learning in Multimodal VAEs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05300
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36719;&#32422;&#26463;&#21462;&#20195;&#30828;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19987;&#23478;&#28151;&#21512;&#20808;&#39564;&#65292;&#25913;&#21892;&#20102;&#22810;&#27169;&#24577;VAEs&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#22312;&#25968;&#25454;&#20998;&#26512;&#30340;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#28508;&#21147;&#65292;&#22914;&#34920;&#31034;&#23398;&#20064;&#12289;&#26377;&#26465;&#20214;&#29983;&#25104;&#21644;&#22635;&#34917;&#12290;&#30446;&#21069;&#30340;&#26550;&#26500;&#35201;&#20040;&#36328;&#27169;&#24577;&#20849;&#20139;&#32534;&#30721;&#22120;&#36755;&#20986;&#12289;&#35299;&#30721;&#22120;&#36755;&#20837;&#65292;&#35201;&#20040;&#20004;&#32773;&#37117;&#35201;&#23398;&#20064;&#20849;&#20139;&#34920;&#31034;&#12290;&#36825;&#26679;&#30340;&#26550;&#26500;&#23545;&#27169;&#22411;&#26045;&#21152;&#20102;&#20005;&#26684;&#32422;&#26463;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#29992;&#36719;&#32422;&#26463;&#21462;&#20195;&#36825;&#20123;&#30828;&#32422;&#26463;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19987;&#23478;&#28151;&#21512;&#20808;&#39564;&#65292;&#36719;&#24615;&#22320;&#24341;&#23548;&#27599;&#20010;&#27169;&#24577;&#30340;&#28508;&#22312;&#34920;&#31034;&#26397;&#30528;&#20849;&#20139;&#30340;&#21518;&#39564;&#12290;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#20102;&#20248;&#31168;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20801;&#35768;&#27599;&#20010;&#32534;&#30721;&#20445;&#30041;&#26469;&#33258;&#20854;&#26410;&#21387;&#32553;&#21407;&#22987;&#29305;&#24449;&#26356;&#22909;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29616;&#23454;&#19990;&#30028;&#31070;&#32463;&#31185;&#23398;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#21644;&#22635;&#34917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05300v1 Announce Type: cross  Abstract: Variational Autoencoders for multimodal data hold promise for many tasks in data analysis, such as representation learning, conditional generation, and imputation. Current architectures either share the encoder output, decoder input, or both across modalities to learn a shared representation. Such architectures impose hard constraints on the model. In this work, we show that a better latent representation can be obtained by replacing these hard constraints with a soft constraint. We propose a new mixture-of-experts prior, softly guiding each modality's latent representation towards a shared aggregate posterior. This approach results in a superior latent representation and allows each encoding to preserve information from its uncompressed original features better. In extensive experiments on multiple benchmark datasets and a challenging real-world neuroscience data set, we show improved learned latent representations and imputation of m
&lt;/p&gt;</description></item><item><title>MedFLIP&#26159;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#20998;&#26512;&#30340;&#24555;&#36895;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;SVD&#25439;&#22833;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#65292;&#39564;&#35777;&#20102;&#29992;&#35821;&#35328;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04626</link><description>&lt;p&gt;
MedFLIP&#65306;&#21307;&#23398;&#35270;&#35273;&#19982;&#35821;&#35328;&#33258;&#30417;&#30563;&#24555;&#36895;&#39044;&#35757;&#32451;&#19982;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
MedFLIP: Medical Vision-and-Language Self-supervised Fast Pre-Training with Masked Autoencoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04626
&lt;/p&gt;
&lt;p&gt;
MedFLIP&#26159;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#20998;&#26512;&#30340;&#24555;&#36895;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;SVD&#25439;&#22833;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#65292;&#39564;&#35777;&#20102;&#29992;&#35821;&#35328;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#20998;&#26512;&#39046;&#22495;&#65292;&#24191;&#27867;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;&#65288;MAEs&#65289;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#20043;&#38388;&#20114;&#30456;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;MAEs&#23545;&#36328;&#27169;&#24577;&#23398;&#20064;&#30340;&#24433;&#21709;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MedFLIP&#65292;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#20998;&#26512;&#30340;&#24555;&#36895;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#32034;&#20351;&#29992;MAEs&#36827;&#34892;&#36328;&#39046;&#22495;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#22312;&#21307;&#23398;&#35786;&#26029;&#20013;&#24120;&#35265;&#30340;&#26377;&#38480;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#23545;&#22270;&#20687;&#36827;&#34892;&#25513;&#34109;&#19981;&#20250;&#24433;&#21709;&#36328;&#27169;&#24577;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SVD&#25439;&#22833;&#20197;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#29305;&#24449;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#36825;&#31867;&#25968;&#25454;&#30340;&#32467;&#26500;&#22797;&#26434;&#24615;&#26469;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#20351;&#29992;&#35821;&#35328;&#23558;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;MedFLIP&#23545;&#25513;&#34109;&#36807;&#31243;&#30340;&#25193;&#23637;&#26631;&#24535;&#30528;&#35813;&#39046;&#22495;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04626v1 Announce Type: cross  Abstract: Within the domain of medical analysis, extensive research has explored the potential of mutual learning between Masked Autoencoders(MAEs) and multimodal data. However, the impact of MAEs on intermodality remains a key challenge. We introduce MedFLIP, a Fast Language-Image Pre-training method for Medical analysis. We explore MAEs for zero-shot learning with crossed domains, which enhances the model ability to learn from limited data, a common scenario in medical diagnostics. We verify that masking an image does not affect intermodal learning. Furthermore, we propose the SVD loss to enhance the representation learning for characteristics of medical images, aiming to improve classification accuracy by leveraging the structural intricacies of such data. Lastly, we validate using language will improve the zero-shot performance for the medical image analysis. MedFLIP scaling of the masking process marks an advancement in the field, offering 
&lt;/p&gt;</description></item><item><title>GUIDE&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#20998;&#31867;&#22120;&#24341;&#23548;&#25216;&#26415;&#65292;&#38024;&#23545;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#36951;&#24536;&#30340;&#20449;&#24687;&#20135;&#29983;&#22797;&#20064;&#31034;&#20363;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>https://arxiv.org/abs/2403.03938</link><description>&lt;p&gt;
GUIDE&#65306;&#22522;&#20110;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GUIDE: Guidance-based Incremental Learning with Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03938
&lt;/p&gt;
&lt;p&gt;
GUIDE&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#20998;&#31867;&#22120;&#24341;&#23548;&#25216;&#26415;&#65292;&#38024;&#23545;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#36951;&#24536;&#30340;&#20449;&#24687;&#20135;&#29983;&#22797;&#20064;&#31034;&#20363;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;GUIDE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#23545;&#26377;&#34987;&#36951;&#24536;&#39118;&#38505;&#30340;&#26679;&#26412;&#36827;&#34892;&#22797;&#20064;&#12290;&#29616;&#26377;&#30340;&#29983;&#25104;&#31574;&#30053;&#36890;&#36807;&#20174;&#29983;&#25104;&#27169;&#22411;&#20013;&#38543;&#26426;&#25277;&#21462;&#22797;&#20064;&#26679;&#26412;&#26469;&#23545;&#25239;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#22522;&#20110;&#32531;&#20914;&#21306;&#30340;&#26041;&#27861;&#30456;&#30683;&#30462;&#65292;&#20854;&#20013;&#37319;&#26679;&#31574;&#30053;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#20998;&#31867;&#22120;&#24341;&#23548;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#20135;&#29983;&#19987;&#38376;&#38024;&#23545;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#36951;&#24536;&#20449;&#24687;&#30340;&#22797;&#20064;&#31034;&#20363;&#65292;&#20197;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;&#33021;&#22815;&#20174;&#20808;&#21069;&#20219;&#21153;&#20998;&#24067;&#20013;&#29983;&#25104;&#26679;&#26412;&#65292;&#36825;&#20123;&#26679;&#26412;&#22312;&#26368;&#36817;&#36935;&#21040;&#30340;&#31867;&#21035;&#24773;&#22659;&#19979;&#26356;&#26377;&#21487;&#33021;&#34987;&#35823;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GUIDE&#26174;&#33879;&#20943;&#23569;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#25277;&#26679;&#26041;&#27861;&#65292;&#24182;&#22312;&#25345;&#32493;&#23398;&#20064;&#26041;&#38754;&#36229;&#36807;&#20102;&#26368;&#36817;&#30340;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03938v1 Announce Type: new  Abstract: We introduce GUIDE, a novel continual learning approach that directs diffusion models to rehearse samples at risk of being forgotten. Existing generative strategies combat catastrophic forgetting by randomly sampling rehearsal examples from a generative model. Such an approach contradicts buffer-based approaches where sampling strategy plays an important role. We propose to bridge this gap by integrating diffusion models with classifier guidance techniques to produce rehearsal examples specifically targeting information forgotten by a continuously trained model. This approach enables the generation of samples from preceding task distributions, which are more likely to be misclassified in the context of recently encountered classes. Our experimental results show that GUIDE significantly reduces catastrophic forgetting, outperforming conventional random sampling approaches and surpassing recent state-of-the-art methods in continual learnin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#39640;&#26031;&#22122;&#22768;&#39537;&#21160;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#21464;&#20998;&#31639;&#27861;&#21644;&#32467;&#26500;&#21270;&#36924;&#36817;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#35780;&#20272;ELBO&#21644;&#33719;&#21462;&#20302;&#26041;&#24046;&#30340;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#65292;&#36890;&#36807;&#21033;&#29992;&#20302;&#31209;&#33945;&#29305;&#21345;&#32599;&#36924;&#36817;&#21644;&#25512;&#26029;&#32593;&#32476;&#30340;&#31934;&#24230;&#30697;&#38453;&#26356;&#26032;&#65292;&#23558;&#36817;&#20284;&#24179;&#28369;&#38382;&#39064;&#36716;&#21270;&#20026;&#36817;&#20284;&#28388;&#27874;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01371</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#21464;&#20998;&#39640;&#26031;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large-scale variational Gaussian state-space models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01371
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#39640;&#26031;&#22122;&#22768;&#39537;&#21160;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#21464;&#20998;&#31639;&#27861;&#21644;&#32467;&#26500;&#21270;&#36924;&#36817;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#35780;&#20272;ELBO&#21644;&#33719;&#21462;&#20302;&#26041;&#24046;&#30340;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#65292;&#36890;&#36807;&#21033;&#29992;&#20302;&#31209;&#33945;&#29305;&#21345;&#32599;&#36924;&#36817;&#21644;&#25512;&#26029;&#32593;&#32476;&#30340;&#31934;&#24230;&#30697;&#38453;&#26356;&#26032;&#65292;&#23558;&#36817;&#20284;&#24179;&#28369;&#38382;&#39064;&#36716;&#21270;&#20026;&#36817;&#20284;&#28388;&#27874;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#23884;&#22871;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#21644;&#32467;&#26500;&#21270;&#21464;&#20998;&#36924;&#36817;&#26041;&#27861;&#65292;&#20854;&#20013;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30001;&#39640;&#26031;&#22122;&#22768;&#39537;&#21160;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20801;&#35768;&#22312;&#27809;&#26377;&#37319;&#29992;&#23545;&#35282;&#39640;&#26031;&#36924;&#36817;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#35780;&#20272;ELBO&#21644;&#20302;&#26041;&#24046;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#65292;&#36890;&#36807;&#21033;&#29992;&#65288;i&#65289;&#36890;&#36807;&#21160;&#21147;&#23398;&#23545;&#38544;&#29366;&#24577;&#36827;&#34892;&#36793;&#32536;&#21270;&#30340;&#33945;&#29305;&#21345;&#32599;&#36924;&#36817;&#30340;&#20302;&#31209;&#32467;&#26500;&#65292;&#65288;ii&#65289;&#19968;&#20010;&#25512;&#26029;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#20302;&#31209;&#31934;&#24230;&#30697;&#38453;&#26356;&#26032;&#26469;&#36817;&#20284;&#26356;&#26032;&#27493;&#39588;&#65292;&#65288;iii&#65289;&#23558;&#24403;&#21069;&#21644;&#26410;&#26469;&#35266;&#27979;&#32534;&#30721;&#20026;&#20266;&#35266;&#27979;--&#23558;&#36817;&#20284;&#24179;&#28369;&#38382;&#39064;&#36716;&#25442;&#20026;&#65288;&#26356;&#31616;&#21333;&#30340;&#65289;&#36817;&#20284;&#28388;&#27874;&#38382;&#39064;&#12290;&#25972;&#20307;&#32780;&#35328;&#65292;&#24517;&#35201;&#30340;&#32479;&#35745;&#20449;&#24687;&#21644;ELBO&#21487;&#20197;&#22312;$O&#65288;TL&#65288;Sr+S^2+r^2&#65289;&#65289;$&#26102;&#38388;&#20869;&#35745;&#31639;&#65292;&#20854;&#20013;$T$&#26159;&#31995;&#21015;&#38271;&#24230;&#65292;$L$&#26159;&#29366;&#24577;&#31354;&#38388;&#32500;&#25968;&#65292;$S$&#26159;&#29992;&#20110;&#36924;&#36817;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01371v1 Announce Type: cross  Abstract: We introduce an amortized variational inference algorithm and structured variational approximation for state-space models with nonlinear dynamics driven by Gaussian noise. Importantly, the proposed framework allows for efficient evaluation of the ELBO and low-variance stochastic gradient estimates without resorting to diagonal Gaussian approximations by exploiting (i) the low-rank structure of Monte-Carlo approximations to marginalize the latent state through the dynamics (ii) an inference network that approximates the update step with low-rank precision matrix updates (iii) encoding current and future observations into pseudo observations -- transforming the approximate smoothing problem into an (easier) approximate filtering problem. Overall, the necessary statistics and ELBO can be computed in $O(TL(Sr + S^2 + r^2))$ time where $T$ is the series length, $L$ is the state-space dimensionality, $S$ are the number of samples used to app
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoMOGA&#30340;&#32422;&#26463;&#22810;&#30446;&#26631;&#26799;&#24230;&#32858;&#21512;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#36716;&#25442;&#20026;&#32422;&#26463;&#65292;&#23454;&#29616;&#20102;&#23545;&#24085;&#32047;&#25176;&#26368;&#20248;&#31574;&#30053;&#30340;&#27714;&#35299;&#65292;&#21516;&#26102;&#28385;&#36275;&#39044;&#23450;&#20041;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2403.00282</link><description>&lt;p&gt;
&#23610;&#24230;&#19981;&#21464;&#26799;&#24230;&#32858;&#21512;&#29992;&#20110;&#21463;&#32422;&#26463;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scale-Invariant Gradient Aggregation for Constrained Multi-Objective Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00282
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoMOGA&#30340;&#32422;&#26463;&#22810;&#30446;&#26631;&#26799;&#24230;&#32858;&#21512;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#36716;&#25442;&#20026;&#32422;&#26463;&#65292;&#23454;&#29616;&#20102;&#23545;&#24085;&#32047;&#25176;&#26368;&#20248;&#31574;&#30053;&#30340;&#27714;&#35299;&#65292;&#21516;&#26102;&#28385;&#36275;&#39044;&#23450;&#20041;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;(MORL)&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#32452;&#24085;&#32047;&#25176;&#26368;&#20248;&#31574;&#30053;&#65292;&#20197;&#28085;&#30422;&#21508;&#31181;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24212;&#29992;MORL&#65292;&#25214;&#21040;&#30340;&#31574;&#30053;&#19981;&#20165;&#35201;&#24085;&#32047;&#25176;&#26368;&#20248;&#65292;&#36824;&#35201;&#28385;&#36275;&#39044;&#23450;&#20041;&#30340;&#23433;&#20840;&#32422;&#26463;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32422;&#26463;&#22810;&#30446;&#26631;&#26799;&#24230;&#32858;&#21512;&#22120;(Constrained Multi-Objective Gradient Aggregator, CoMOGA)&#30340;&#32422;&#26463;MORL(CMORL)&#31639;&#27861;&#12290;CoMOGA&#24847;&#35782;&#21040;&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;&#30446;&#26631;&#21644;&#32422;&#26463;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#36716;&#25442;&#20026;&#39069;&#22806;&#30340;&#32422;&#26463;&#65292;&#23558;&#21407;&#22987;CMORL&#38382;&#39064;&#25918;&#26494;&#25104;&#19968;&#20010;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#36716;&#25442;&#36807;&#31243;&#30830;&#20445;&#36716;&#25442;&#21518;&#30340;&#32422;&#26463;&#23545;&#30446;&#26631;&#23610;&#24230;&#19981;&#21464;&#65292;&#21516;&#26102;&#20855;&#26377;&#19982;&#21407;&#22987;&#30446;&#26631;&#30456;&#21516;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#25910;&#25947;&#21040;&#19968;&#20010;&#23616;&#37096;&#24085;&#32047;&#25176;&#26368;&#20248;&#31574;&#30053;&#65292;&#21516;&#26102;&#28385;&#36275;&#39044;&#23450;&#20041;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00282v1 Announce Type: new  Abstract: Multi-objective reinforcement learning (MORL) aims to find a set of Pareto optimal policies to cover various preferences. However, to apply MORL in real-world applications, it is important to find policies that are not only Pareto optimal but also satisfy pre-defined constraints for safety. To this end, we propose a constrained MORL (CMORL) algorithm called Constrained Multi-Objective Gradient Aggregator (CoMOGA). Recognizing the difficulty of handling multiple objectives and constraints concurrently, CoMOGA relaxes the original CMORL problem into a constrained optimization problem by transforming the objectives into additional constraints. This novel transformation process ensures that the converted constraints are invariant to the objective scales while having the same effect as the original objectives. We show that the proposed method converges to a local Pareto optimal policy while satisfying the predefined constraints. Empirical eva
&lt;/p&gt;</description></item><item><title>BioT5+&#26159;BioT5&#26694;&#26550;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#25972;&#21512;IUPAC&#21517;&#31216;&#12289;&#21253;&#21547;&#24191;&#27867;&#29983;&#29289;&#25991;&#26412;&#21644;&#20998;&#23376;&#25968;&#25454;&#12289;&#22810;&#20219;&#21153;&#25351;&#20196;&#35843;&#25972;&#20197;&#21450;&#26032;&#39062;&#30340;&#25968;&#20540;&#26631;&#35760;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#20998;&#23376;&#34920;&#31034;&#19982;&#25991;&#26412;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.17810</link><description>&lt;p&gt;
BioT5+: &#36890;&#36807;IUPAC&#38598;&#25104;&#21644;&#22810;&#20219;&#21153;&#35843;&#25972;&#23454;&#29616;&#24191;&#20041;&#29983;&#29289;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17810
&lt;/p&gt;
&lt;p&gt;
BioT5+&#26159;BioT5&#26694;&#26550;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#25972;&#21512;IUPAC&#21517;&#31216;&#12289;&#21253;&#21547;&#24191;&#27867;&#29983;&#29289;&#25991;&#26412;&#21644;&#20998;&#23376;&#25968;&#25454;&#12289;&#22810;&#20219;&#21153;&#25351;&#20196;&#35843;&#25972;&#20197;&#21450;&#26032;&#39062;&#30340;&#25968;&#20540;&#26631;&#35760;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#20998;&#23376;&#34920;&#31034;&#19982;&#25991;&#26412;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35745;&#31639;&#29983;&#29289;&#23398;&#30340;&#30740;&#31350;&#36235;&#21183;&#36234;&#26469;&#36234;&#38598;&#20013;&#20110;&#25972;&#21512;&#25991;&#26412;&#21644;&#29983;&#29289;&#23454;&#20307;&#24314;&#27169;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#30340;&#32972;&#26223;&#19979;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#20110;BioT5&#30340;&#20808;&#21069;&#24037;&#20316;&#22312;&#36328;&#36234;&#22810;&#26679;&#21270;&#20219;&#21153;&#21644;&#32570;&#20047;&#23545;&#20998;&#23376;&#32467;&#26500;&#30340;&#32454;&#33268;&#29702;&#35299;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#23427;&#20204;&#30340;&#25991;&#26412;&#34920;&#31034;&#65288;&#20363;&#22914;IUPAC&#65289;&#26041;&#38754;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BioT5+&#65292;&#36825;&#26159;BioT5&#26694;&#26550;&#30340;&#19968;&#20010;&#25193;&#23637;&#65292;&#26088;&#22312;&#22686;&#24378;&#29983;&#29289;&#30740;&#31350;&#21644;&#33647;&#29289;&#21457;&#29616;&#12290; BioT5+&#21253;&#21547;&#20960;&#20010;&#26032;&#39062;&#30340;&#29305;&#24615;&#65306;&#25972;&#21512;IUPAC&#21517;&#31216;&#20197;&#21152;&#28145;&#23545;&#20998;&#23376;&#30340;&#29702;&#35299;&#65292;&#21253;&#25324;&#26469;&#33258;bioRxiv&#21644;PubChem&#31561;&#28304;&#30340;&#24191;&#27867;&#29983;&#29289;&#25991;&#26412;&#21644;&#20998;&#23376;&#25968;&#25454;&#65292;&#22810;&#20219;&#21153;&#25351;&#20196;&#35843;&#25972;&#20197;&#36328;&#36234;&#22810;&#20010;&#20219;&#21153;&#65292;&#20197;&#21450;&#19968;&#31181;&#29992;&#20110;&#25913;&#36827;&#25968;&#23383;&#25968;&#25454;&#22788;&#29702;&#30340;&#26032;&#39062;&#25968;&#20540;&#26631;&#35760;&#25216;&#26415;&#12290; &#36825;&#20123;&#22686;&#24378;&#21151;&#33021;&#20351;BioT5+&#33021;&#22815;&#24357;&#21512;&#20998;&#23376;&#34920;&#31034;&#21644;&#23427;&#20204;&#30340;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17810v1 Announce Type: cross  Abstract: Recent research trends in computational biology have increasingly focused on integrating text and bio-entity modeling, especially in the context of molecules and proteins. However, previous efforts like BioT5 faced challenges in generalizing across diverse tasks and lacked a nuanced understanding of molecular structures, particularly in their textual representations (e.g., IUPAC). This paper introduces BioT5+, an extension of the BioT5 framework, tailored to enhance biological research and drug discovery. BioT5+ incorporates several novel features: integration of IUPAC names for molecular understanding, inclusion of extensive bio-text and molecule data from sources like bioRxiv and PubChem, the multi-task instruction tuning for generality across tasks, and a novel numerical tokenization technique for improved processing of numerical data. These enhancements allow BioT5+ to bridge the gap between molecular representations and their text
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLMs&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#27745;&#26579;&#26816;&#27979;&#30340;&#26041;&#27861;CDD&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;LLMs&#36755;&#20986;&#20462;&#27491;&#30340;&#21487;&#20449;&#35780;&#20272;&#26041;&#27861;TED&#65292;&#20197;&#24212;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#27745;&#26579;&#21644;&#21487;&#20449;&#35780;&#20272;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15938</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#25110;&#35760;&#24518;&#65306;&#25968;&#25454;&#27745;&#26579;&#19982;&#21487;&#20449;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLMs&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#27745;&#26579;&#26816;&#27979;&#30340;&#26041;&#27861;CDD&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;LLMs&#36755;&#20986;&#20462;&#27491;&#30340;&#21487;&#20449;&#35780;&#20272;&#26041;&#27861;TED&#65292;&#20197;&#24212;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#27745;&#26579;&#21644;&#21487;&#20449;&#35780;&#20272;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#33021;&#21147;&#30340;&#35828;&#27861;&#36890;&#24120;&#26159;&#36890;&#36807;&#22312;&#24320;&#25918;&#33719;&#21462;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#26469;&#25903;&#25345;&#30340;&#12290;&#32771;&#34385;&#21040;LLMs&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24222;&#22823;&#35268;&#27169;&#21644;&#24191;&#27867;&#26469;&#28304;&#65292;&#23427;&#21487;&#33021;&#26126;&#30830;&#25110;&#38544;&#21547;&#22320;&#21253;&#21547;&#27979;&#35797;&#25968;&#25454;&#65292;&#23548;&#33268;LLMs&#26356;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#19981;&#36879;&#26126;&#24615;&#12289;&#27169;&#22411;&#30340;&#40657;&#30418;&#35775;&#38382;&#20197;&#21450;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#23545;&#20110;LLMs&#26469;&#35828;&#26816;&#27979;&#21644;&#20943;&#36731;&#25968;&#25454;&#27745;&#26579;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CDD&#65292;&#21363;&#36890;&#36807;LLMs&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#27745;&#26579;&#26816;&#27979;&#30340;CDD&#12290;CDD&#20165;&#38656;&#35201;&#37319;&#26679;&#25991;&#26412;&#26469;&#26816;&#27979;&#25968;&#25454;&#27745;&#26579;&#65292;&#36890;&#36807;&#35782;&#21035;LLMs&#36755;&#20986;&#20998;&#24067;&#30340;&#23792;&#20540;&#26469;&#36827;&#34892;&#26816;&#27979;&#12290;&#20026;&#20102;&#20943;&#36731;&#35780;&#20272;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;TED&#65306;&#22522;&#20110;LLMs&#36755;&#20986;&#20462;&#27491;&#30340;&#21487;&#20449;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15938v1 Announce Type: cross  Abstract: Recent statements about the impressive capabilities of large language models (LLMs) are usually supported by evaluating on open-access benchmarks. Considering the vast size and wide-ranging sources of LLMs' training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination. However, due to the opacity of training data, the black-box access of models, and the rapid growth of synthetic training data, detecting and mitigating data contamination for LLMs faces significant challenges. In this paper, we propose CDD, which stands for Contamination Detection via output Distribution for LLMs. CDD necessitates only the sampled texts to detect data contamination, by identifying the peakedness of LLM's output distribution. To mitigate the impact of data contamination in evaluation, we also present TED: Trustworthy Evaluation via output Distribution, based on the correction of LLM's outp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#37319;&#29992;&#21512;&#20316;&#21338;&#24328;&#35770;&#35299;&#37322;&#24320;&#25918;&#24335;&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;&#20013;&#32852;&#21512;Q&#20540;&#34920;&#31034;&#30340;&#26032;&#29702;&#35770;&#65292;&#20026;&#36827;&#19968;&#27493;&#21457;&#23637;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;</title><link>https://arxiv.org/abs/2402.15259</link><description>&lt;p&gt;
&#37319;&#29992;&#21512;&#20316;&#21338;&#24328;&#35770;&#30340;&#24320;&#25918;&#24335;&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Open Ad Hoc Teamwork with Cooperative Game Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15259
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#37319;&#29992;&#21512;&#20316;&#21338;&#24328;&#35770;&#35299;&#37322;&#24320;&#25918;&#24335;&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;&#20013;&#32852;&#21512;Q&#20540;&#34920;&#31034;&#30340;&#26032;&#29702;&#35770;&#65292;&#20026;&#36827;&#19968;&#27493;&#21457;&#23637;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;&#38754;&#20020;&#30528;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#19982;&#38431;&#21451;&#21327;&#20316;&#20294;&#27809;&#26377;&#20808;&#21069;&#21327;&#35843;&#25110;&#32852;&#21512;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#12290;&#24320;&#25918;&#24335;&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#20102;&#36825;&#19968;&#25361;&#25112;&#65292;&#32771;&#34385;&#20102;&#20855;&#26377;&#19981;&#26029;&#21464;&#21270;&#30340;&#38431;&#21451;&#25968;&#37327;&#30340;&#29615;&#22659;&#65292;&#21363;&#24320;&#25918;&#24335;&#22242;&#38431;&#12290;&#29616;&#26377;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26159;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31574;&#30053;&#23398;&#20064;&#65288;GPL&#65289;&#65292;&#21033;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#26469;&#22788;&#29702;&#26080;&#38480;&#25968;&#37327;&#30340;&#26234;&#33021;&#20307;&#65292;&#26377;&#25928;&#24212;&#23545;&#24320;&#25918;&#24335;&#22242;&#38431;&#12290;GPL&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20294;&#20854;&#32852;&#21512;Q&#20540;&#34920;&#31034;&#23545;&#35299;&#37322;&#36896;&#25104;&#20102;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#21457;&#23637;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#21644;&#24212;&#29992;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#65292;&#20174;&#21512;&#20316;&#21338;&#24328;&#35770;&#30340;&#35282;&#24230;&#20026;GPL&#20013;&#37319;&#29992;&#30340;&#32852;&#21512;Q&#20540;&#34920;&#31034;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#37322;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15259v1 Announce Type: cross  Abstract: Ad hoc teamwork poses a challenging problem, requiring the design of an agent to collaborate with teammates without prior coordination or joint training. Open ad hoc teamwork further complicates this challenge by considering environments with a changing number of teammates, referred to as open teams. The state-of-the-art solution to this problem is graph-based policy learning (GPL), leveraging the generalizability of graph neural networks to handle an unrestricted number of agents and effectively address open teams. GPL's performance is superior to other methods, but its joint Q-value representation presents challenges for interpretation, hindering further development of this research line and applicability. In this paper, we establish a new theory to give an interpretation for the joint Q-value representation employed in GPL, from the perspective of cooperative game theory. Building on our theory, we propose a novel algorithm based on
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#21019;&#30340;&#37327;&#23376;&#35745;&#31639;&#20844;&#24335;&#65292;&#29992;&#20110;&#24773;&#22659;&#21270;&#36755;&#36865;&#35745;&#21010;&#30340;&#25674;&#38144;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#39044;&#27979;&#32972;&#26223;&#24773;&#22659;&#20013;&#33647;&#29289;&#21058;&#37327;&#21442;&#25968;&#21270;&#30340;&#32454;&#32990;&#31867;&#22411;&#20998;&#24067;&#30340;&#21464;&#21270;&#26469;&#39564;&#35777;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#25429;&#25417;&#21058;&#37327;&#24341;&#36215;&#30340;&#32454;&#32990;&#20998;&#24067;&#21464;&#21270;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14991</link><description>&lt;p&gt;
&#37327;&#23376;&#29702;&#35770;&#19982;&#24773;&#22659;&#26368;&#20248;&#36755;&#36816;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Quantum Theory and Application of Contextual Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14991
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#21019;&#30340;&#37327;&#23376;&#35745;&#31639;&#20844;&#24335;&#65292;&#29992;&#20110;&#24773;&#22659;&#21270;&#36755;&#36865;&#35745;&#21010;&#30340;&#25674;&#38144;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#39044;&#27979;&#32972;&#26223;&#24773;&#22659;&#20013;&#33647;&#29289;&#21058;&#37327;&#21442;&#25968;&#21270;&#30340;&#32454;&#32990;&#31867;&#22411;&#20998;&#24067;&#30340;&#21464;&#21270;&#26469;&#39564;&#35777;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#25429;&#25417;&#21058;&#37327;&#24341;&#36215;&#30340;&#32454;&#32990;&#20998;&#24067;&#21464;&#21270;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36755;&#36816;&#65288;Optimal Transport&#65292;OT&#65289;&#25512;&#21160;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#35768;&#22810;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#22312;&#27979;&#37327;&#25968;&#25454;&#65288;$\mu$&#65292;$\nu$&#65289;&#19982;&#19978;&#19979;&#25991;&#21464;&#37327; $p_i$ &#32806;&#21512;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#21162;&#21147;&#23398;&#20064;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#21487;&#33021;&#30475;&#19981;&#35265;&#30340;&#19978;&#19979;&#25991;&#21442;&#25968;&#21270;&#30340;&#20840;&#23616;&#36755;&#36816;&#26144;&#23556;&#12290;&#29616;&#26377;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;&#65292;&#24182;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;Brenier&#23450;&#29702;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#21019;&#30340;&#37327;&#23376;&#35745;&#31639;&#20844;&#24335;&#65292;&#29992;&#20110;&#24773;&#22659;&#21270;&#36755;&#36865;&#35745;&#21010;&#30340;&#25674;&#38144;&#20248;&#21270;&#12290;&#25105;&#20204;&#21033;&#29992;&#21452;&#38543;&#26426;&#30697;&#38453;&#21644;&#37193;&#31639;&#31526;&#20043;&#38388;&#30340;&#30452;&#25509;&#32852;&#31995;&#65292;&#20174;&#32780;&#25214;&#21040;&#20102;&#26368;&#20248;&#36755;&#36816;&#21644;&#37327;&#23376;&#35745;&#31639;&#20043;&#38388;&#30340;&#33258;&#28982;&#32852;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#39564;&#35777;&#65292;&#39044;&#27979;&#36890;&#36807;&#33647;&#29289;&#21058;&#37327;&#21442;&#25968;&#21270;&#30340;&#32454;&#32990;&#31867;&#22411;&#20998;&#24067;&#30340;&#21464;&#21270;&#20316;&#20026;&#32972;&#26223;&#24773;&#22659;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20960;&#20010;&#22522;&#20934;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#21040;&#21058;&#37327;&#24341;&#36215;&#30340;&#32454;&#32990;&#20998;&#24067;&#21464;&#21270;&#65292;&#29978;&#33267;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14991v1 Announce Type: new  Abstract: Optimal Transport (OT) has fueled machine learning (ML) applications across many domains. In cases where paired data measurements ($\mu$, $\nu$) are coupled to a context variable $p_i$ , one may aspire to learn a global transportation map that can be parameterized through a potentially unseen con-text. Existing approaches utilize Neural OT and largely rely on Brenier's theorem. Here, we propose a first-of-its-kind quantum computing formulation for amortized optimization of contextualized transportation plans. We exploit a direct link between doubly stochastic matrices and unitary operators thus finding a natural connection between OT and quantum computation. We verify our method on synthetic and real data, by predicting variations in cell type distributions parameterized through drug dosage as context. Our comparisons to several baselines reveal that our method can capture dose-induced variations in cell distributions, even to some exten
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#23545;&#26356;&#22823;&#31867;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#25552;&#39640;&#20102;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.13901</link><description>&lt;p&gt;
&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#65306;&#26032;&#26041;&#27861;&#21644;&#25913;&#36827;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Non-asymptotic Convergence of Discrete-time Diffusion Models: New Approach and Improved Rate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#23545;&#26356;&#22823;&#31867;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#25552;&#39640;&#20102;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#29983;&#25104;&#25216;&#26415;&#20986;&#29616;&#65292;&#23558;&#22122;&#22768;&#36716;&#21270;&#20026;&#25968;&#25454;&#12290;&#29702;&#35770;&#19978;&#20027;&#35201;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#24182;&#19988;&#20165;&#22312;&#25991;&#29486;&#20013;&#23545;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#30340;&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#33719;&#24471;&#12290;&#26412;&#25991;&#20026;&#26356;&#22823;&#31867;&#30340;&#20998;&#24067;&#24314;&#31435;&#20102;&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#24182;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#23545;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#39318;&#20808;&#20026;&#20855;&#26377;&#26377;&#38480;&#20108;&#38454;&#30697;&#30340;&#24179;&#28369;&#21644;&#19968;&#33324;&#65288;&#21487;&#33021;&#38750;&#20809;&#28369;&#65289;&#20998;&#24067;&#24314;&#31435;&#20102;&#25910;&#25947;&#36895;&#29575;&#12290;&#28982;&#21518;&#23558;&#32467;&#26524;&#19987;&#38376;&#24212;&#29992;&#20110;&#19968;&#20123;&#26377;&#26126;&#30830;&#21442;&#25968;&#20381;&#36182;&#20851;&#31995;&#30340;&#26377;&#36259;&#20998;&#24067;&#31867;&#21035;&#65292;&#21253;&#25324;&#20855;&#26377;Lipschitz&#20998;&#25968;&#12289;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#21644;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13901v1 Announce Type: new  Abstract: The denoising diffusion model emerges recently as a powerful generative technique that converts noise into data. Theoretical convergence guarantee has been mainly studied for continuous-time diffusion models, and has been obtained for discrete-time diffusion models only for distributions with bounded support in the literature. In this paper, we establish the convergence guarantee for substantially larger classes of distributions under discrete-time diffusion models and further improve the convergence rate for distributions with bounded support. In particular, we first establish the convergence rates for both smooth and general (possibly non-smooth) distributions having finite second moment. We then specialize our results to a number of interesting classes of distributions with explicit parameter dependencies, including distributions with Lipschitz scores, Gaussian mixture distributions, and distributions with bounded support. We further 
&lt;/p&gt;</description></item><item><title>&#22810;&#32447;&#24615;&#19987;&#23478;&#28151;&#21512;&#65288;MMoE&#65289;&#23618;&#36890;&#36807;&#22240;&#24335;&#20998;&#35299;&#38024;&#23545;&#35270;&#35273;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#19987;&#23478;&#29305;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#36991;&#20813;&#20102;&#31163;&#25955;&#19987;&#23478;&#36335;&#30001;&#21644;&#36807;&#39640;&#25512;&#29702;&#26102;&#38388;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.12550</link><description>&lt;p&gt;
&#22810;&#32447;&#24615;&#19987;&#23478;&#28151;&#21512;&#65306;&#36890;&#36807;&#22240;&#24335;&#20998;&#35299;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#19987;&#23478;&#29305;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12550
&lt;/p&gt;
&lt;p&gt;
&#22810;&#32447;&#24615;&#19987;&#23478;&#28151;&#21512;&#65288;MMoE&#65289;&#23618;&#36890;&#36807;&#22240;&#24335;&#20998;&#35299;&#38024;&#23545;&#35270;&#35273;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#19987;&#23478;&#29305;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#36991;&#20813;&#20102;&#31163;&#25955;&#19987;&#23478;&#36335;&#30001;&#21644;&#36807;&#39640;&#25512;&#29702;&#26102;&#38388;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#33539;&#24335;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#23558;&#38590;&#20197;&#29702;&#35299;&#30340;&#23494;&#38598;&#23618;&#20998;&#35299;&#20026;&#26356;&#23567;&#12289;&#27169;&#22359;&#21270;&#30340;&#35745;&#31639;&#65292;&#36890;&#24120;&#26356;&#26131;&#20110;&#20154;&#31867;&#35299;&#37322;&#12289;&#35843;&#35797;&#21644;&#32534;&#36753;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#22312;&#20110;&#25193;&#23637;&#19987;&#23478;&#25968;&#37327;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20197;&#23454;&#29616;&#36275;&#22815;&#31934;&#32454;&#30340;&#19987;&#19994;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#32447;&#24615;&#19987;&#23478;&#28151;&#21512;&#65288;MMoE&#65289;&#23618;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#37325;&#28857;&#25918;&#22312;&#35270;&#35273;&#27169;&#22411;&#19978;&#12290;MMoE&#23618;&#23436;&#20840;&#20197;&#22240;&#24335;&#21270;&#24418;&#24335;&#23545;&#24222;&#22823;&#30340;&#26435;&#37325;&#24352;&#37327;&#36827;&#34892;&#38544;&#24335;&#35745;&#31639;&#12290;&#22240;&#27492;&#65292;MMoEs&#26082;&#36991;&#20813;&#20102;&#22312;&#27969;&#34892;&#30340;&#8220;&#31232;&#30095;&#8221;MoE&#27169;&#22411;&#20013;&#31163;&#25955;&#19987;&#23478;&#36335;&#30001;&#25152;&#36896;&#25104;&#30340;&#38382;&#39064;&#65292;&#21448;&#19981;&#20250;&#24341;&#36215;&#8220;&#36719;&#8221;MoE&#26367;&#20195;&#26041;&#26696;&#20013;&#36807;&#39640;&#30340;&#25512;&#29702;&#26102;&#38388;&#25104;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#21487;&#35270;&#21270;&#21644;&#21453;&#20107;&#23454;&#24178;&#39044;&#65292;&#25552;&#20379;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;&#25193;&#23637;MMoE&#23618;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12550v1 Announce Type: cross  Abstract: The Mixture of Experts (MoE) paradigm provides a powerful way to decompose inscrutable dense layers into smaller, modular computations often more amenable to human interpretation, debugging, and editability. A major problem however lies in the computational cost of scaling the number of experts to achieve sufficiently fine-grained specialization. In this paper, we propose the Multilinear Mixutre of Experts (MMoE) layer to address this, focusing on vision models. MMoE layers perform an implicit computation on prohibitively large weight tensors entirely in factorized form. Consequently, MMoEs both (1) avoid the issues incurred through the discrete expert routing in the popular 'sparse' MoE models, yet (2) do not incur the restrictively high inference-time costs of 'soft' MoE alternatives. We present both qualitative and quantitative evidence (through visualization and counterfactual interventions respectively) that scaling MMoE layers wh
&lt;/p&gt;</description></item><item><title>Meta Ranking&#26041;&#27861;&#36890;&#36807;&#27604;&#36739;&#30446;&#26631;&#26597;&#35810;-&#21709;&#24212;&#23545;&#19982;&#21442;&#32771;&#26597;&#35810;-&#21709;&#24212;&#23545;&#26469;&#20351;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#22320;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12146</link><description>&lt;p&gt;
Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement
&lt;/p&gt;
&lt;p&gt;
Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12146
&lt;/p&gt;
&lt;p&gt;
Meta Ranking&#26041;&#27861;&#36890;&#36807;&#27604;&#36739;&#30446;&#26631;&#26597;&#35810;-&#21709;&#24212;&#23545;&#19982;&#21442;&#32771;&#26597;&#35810;-&#21709;&#24212;&#23545;&#26469;&#20351;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#22320;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24191;&#27867;&#20219;&#21153;&#20013;&#23637;&#29616;&#24378;&#22823;&#24615;&#33021;&#65292;&#20294;&#20173;&#38754;&#20020;&#24187;&#35273;&#31561;&#21487;&#38752;&#24615;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20687;GPT-4&#36825;&#26679;&#39640;&#33021;&#21147;&#30340;LLMs&#22312;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#32780;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;LLMs&#36890;&#24120;&#34987;&#35843;&#25972;&#26469;&#35780;&#20272;&#23545;&#30456;&#21516;&#26597;&#35810;&#30340;&#21709;&#24212;&#30340;&#30456;&#23545;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#20351;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;LLMs&#26377;&#25928;&#22320;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\textit{Meta}$ $\textit{Ranking}$&#65288;MR&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20808;&#21069;&#30452;&#25509;&#35780;&#20272;&#21709;&#24212;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#30446;&#26631;&#26597;&#35810;-&#21709;&#24212;&#23545;&#19982;&#21442;&#32771;&#26597;&#35810;-&#21709;&#24212;&#23545;&#36827;&#34892;&#27604;&#36739;&#26469;&#23454;&#29616;&#21028;&#26029;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#25512;&#29702;&#20219;&#21153;&#30340;LLM&#21709;&#24212;&#30340;&#38169;&#35823;&#26816;&#27979;&#20013;&#65292;MR&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;LLMs&#20063;&#33021;&#32988;&#36807;&#24378;&#22522;&#32447;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;MR&#21487;&#20197;&#34987;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12146v1 Announce Type: cross  Abstract: Although Large Language Models (LLMs) have demonstrated strong performance on a wide range of tasks, they still face reliability challenges such as hallucination. Previous studies reveal that highly capable LLMs like GPT-4 are effective in judging the reliability of individual responses, while less capable ones are often tuned to evaluate the relative reliability of responses to the same query. To enable less capable LLMs to effectively judge the reliability of individual responses, we propose a novel method named $\textit{Meta}$ $\textit{Ranking}$ (MR). Unlike previous methods, which assess the response directly, we achieve the judgement by comparing the target query-response pair with reference query-response pairs. We found its remarkable effectiveness in error detection for LLM responses on reasoning tasks, where less capable LLMs could outperform strong baselines, even without fine-tuning. We further demonstrate that MR can be use
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28176;&#21464;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#27169;&#25311;&#37096;&#32626;&#31574;&#30053;&#23545;&#29615;&#22659;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;MDRR&#26469;&#24212;&#23545;&#36825;&#31181;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.09838</link><description>&lt;p&gt;
&#28176;&#21464;&#29615;&#22659;&#20013;&#30340;&#34920;&#28436;&#24615;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Performative Reinforcement Learning in Gradually Shifting Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09838
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28176;&#21464;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#27169;&#25311;&#37096;&#32626;&#31574;&#30053;&#23545;&#29615;&#22659;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;MDRR&#26469;&#24212;&#23545;&#36825;&#31181;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#22312;&#23454;&#36341;&#20013;&#37096;&#32626;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#24433;&#21709;&#29615;&#22659;&#24182;&#25913;&#21464;&#20854;&#21160;&#24577;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#35797;&#22270;&#24418;&#24335;&#21270;&#24314;&#27169;&#36825;&#31181;&#29616;&#35937;&#65292;&#24182;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#20998;&#26512;&#23398;&#20064;&#31639;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20854;&#20013;&#24403;&#21069;&#30340;&#29615;&#22659;&#21462;&#20915;&#20110;&#37096;&#32626;&#31574;&#30053;&#21450;&#20854;&#20808;&#21069;&#30340;&#21160;&#24577;&#12290;&#36825;&#26159;Performative RL&#65288;PRL&#65289;[Mandal et al., 2023]&#30340;&#19968;&#31181;&#27867;&#21270;&#12290;&#19982;PRL&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#23545;&#29615;&#22659;&#36880;&#28176;&#35843;&#25972;&#21040;&#37096;&#32626;&#31574;&#30053;&#30340;&#24773;&#26223;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#23558;&#34920;&#28436;&#24615;&#39044;&#27979;&#25991;&#29486;&#20013;&#30340;&#20004;&#31181;&#31639;&#27861;&#36866;&#24212;&#21040;&#25105;&#20204;&#30340;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#31216;&#20026;&#28151;&#21512;&#24310;&#36831;&#37325;&#22797;&#35757;&#32451;&#65288;MDRR&#65289;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#36825;&#20123;&#31639;&#27861;&#25910;&#25947;&#30340;&#26465;&#20214;&#65292;&#24182;&#20351;&#29992;&#19977;&#20010;&#25351;&#26631;&#36827;&#34892;&#27604;&#36739;&#65306;&#37325;&#35757;&#32451;&#27425;&#25968;&#65292;&#36924;&#36817;&#20445;&#35777;&#21644;&#27599;&#27425;&#37096;&#32626;&#30340;&#26679;&#26412;&#25968;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;MDRR&#32467;&#21512;&#20102;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09838v1 Announce Type: new  Abstract: When Reinforcement Learning (RL) agents are deployed in practice, they might impact their environment and change its dynamics. Ongoing research attempts to formally model this phenomenon and to analyze learning algorithms in these models. To this end, we propose a framework where the current environment depends on the deployed policy as well as its previous dynamics. This is a generalization of Performative RL (PRL) [Mandal et al., 2023]. Unlike PRL, our framework allows to model scenarios where the environment gradually adjusts to a deployed policy. We adapt two algorithms from the performative prediction literature to our setting and propose a novel algorithm called Mixed Delayed Repeated Retraining (MDRR). We provide conditions under which these algorithms converge and compare them using three metrics: number of retrainings, approximation guarantee, and number of samples per deployment. Unlike previous approaches, MDRR combines sample
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#32771;&#34385;&#26377;&#38480;&#39044;&#31639;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#25552;&#31034;&#23398;&#20064;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;TRIPLE&#65292;&#36890;&#36807;&#21033;&#29992;&#32858;&#31867;&#21644;&#23884;&#20837;&#24605;&#24819;&#23454;&#29616;&#20102;&#20004;&#20010;&#22686;&#24378;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09723</link><description>&lt;p&gt;
&#26377;&#38480;&#39044;&#31639;&#19979;&#30340;&#36805;&#36895;&#23398;&#20064;&#26368;&#20339;&#33218;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Best Arm Identification for Prompt Learning under a Limited Budget
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09723
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#32771;&#34385;&#26377;&#38480;&#39044;&#31639;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#25552;&#31034;&#23398;&#20064;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;TRIPLE&#65292;&#36890;&#36807;&#21033;&#29992;&#32858;&#31867;&#21644;&#23884;&#20837;&#24605;&#24819;&#23454;&#29616;&#20102;&#20004;&#20010;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#24341;&#21457;&#20102;&#23545;&#33258;&#21160;&#23398;&#20064;&#21512;&#36866;&#25552;&#31034;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#25552;&#20986;&#20102;&#35768;&#22810;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#25104;&#26412;&#65288;&#20363;&#22914;&#35775;&#38382;LLM&#21644;&#35780;&#20272;&#21709;&#24212;&#65289;&#23578;&#26410;&#24471;&#21040;&#32771;&#34385;&#12290;&#20026;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#24037;&#20316;&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#26126;&#30830;&#24341;&#20837;&#20102;&#26377;&#38480;&#39044;&#31639;&#32422;&#26463;&#12290;&#20026;&#20102;&#24320;&#21457;&#26377;&#21407;&#21017;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26412;&#30740;&#31350;&#22312;&#25552;&#31034;&#23398;&#20064;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#65288;BAI-FB&#65289;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#31995;&#12290;&#22522;&#20110;&#36825;&#31181;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;TRIPLE&#65288;&#29992;&#20110;&#25552;&#31034;&#23398;&#20064;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#65289;&#65292;&#20197;&#31995;&#32479;&#22320;&#21033;&#29992;BAI-FB&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#30340;&#21147;&#37327;&#12290;&#25552;&#31034;&#23398;&#20064;&#30340;&#29420;&#29305;&#29305;&#28857;&#36827;&#19968;&#27493;&#36890;&#36807;&#21033;&#29992;&#32858;&#31867;&#21644;&#23884;&#20837;&#24605;&#24819;&#25552;&#20986;&#20102;TRIPLE&#30340;&#20004;&#20010;&#22522;&#20110;&#23884;&#20837;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09723v1 Announce Type: cross  Abstract: The remarkable instruction-following capability of large language models (LLMs) has sparked a growing interest in automatically learning suitable prompts. However, while many effective methods have been proposed, the cost incurred during the learning process (e.g., accessing LLM and evaluating the responses) has not been considered. To overcome this limitation, this work explicitly incorporates a finite budget constraint into prompt learning. Towards developing principled solutions, a novel connection is established between prompt learning and fixed-budget best arm identification (BAI-FB) in multi-armed bandits (MAB). Based on this connection, a general framework TRIPLE (besT aRm Identification for Prompt LEarning) is proposed to harness the power of BAI-FB in prompt learning systematically. Unique characteristics of prompt learning further lead to two embedding-based enhancements of TRIPLE by exploiting the ideas of clustering and fun
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;</title><link>https://arxiv.org/abs/2402.09615</link><description>&lt;p&gt;
API Pack&#65306;&#19968;&#20010;&#29992;&#20110;API&#35843;&#29992;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
API Pack: A Massive Multilingual Dataset for API Call Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09615
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;API Pack&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#25351;&#20196;-API&#35843;&#29992;&#23545;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;API Pack&#22312;&#25552;&#21319;&#27169;&#22411;&#22312;&#36825;&#19968;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20854;&#22312;&#19968;&#33324;&#32534;&#30721;&#26041;&#38754;&#30340;&#25972;&#20307;&#29087;&#32451;&#31243;&#24230;&#12290;&#20165;&#22312;20,000&#20010;Python&#23454;&#20363;&#19978;&#23545;CodeLlama-13B&#36827;&#34892;&#24494;&#35843;&#65292;&#20854;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#30340;&#20934;&#30830;&#29575;&#27604;GPT-3.5&#21644;GPT-4&#20998;&#21035;&#39640;&#20986;10%&#21644;5%&#12290;&#25193;&#23637;&#21040;100k&#20010;&#20363;&#23376;&#21487;&#20197;&#25552;&#39640;&#23545;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;API&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;&#65292;&#32780;&#26080;&#38656;&#22823;&#37327;&#35821;&#35328;&#29305;&#23450;&#30340;&#25968;&#25454;&#12290;&#25968;&#25454;&#38598;&#12289;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#21644;&#25972;&#20307;&#20195;&#30721;&#24211;&#21487;&#22312;https://github.com/anonymous_url&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09615v1 Announce Type: cross  Abstract: We introduce API Pack, a multilingual dataset featuring over one million instruction-API call pairs aimed at advancing large language models' API call generation capabilities. Through experiments, we demonstrate API Pack's efficacy in enhancing models for this specialized task while maintaining their overall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000 Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4 respectively in generating unseen API calls. Scaling to 100k examples improves generalization to new APIs not seen during training. In addition, cross-lingual API call generation is achieved without needing extensive data per language. The dataset, fine-tuned models, and overall code base are publicly available at https://github.com/anonymous_url.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19982;&#36880;&#23618;&#35757;&#32451;&#30340;&#27604;&#36739;&#65292;&#37325;&#26032;&#32771;&#34385;&#20102;&#20026;&#20160;&#20040;&#31471;&#21040;&#31471;&#35757;&#32451;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#20998;&#26512;&#20102;&#20449;&#24687;&#20256;&#25773;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#24182;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#36136;&#24046;&#24322;&#36827;&#34892;&#20102;&#28145;&#20837;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.09050</link><description>&lt;p&gt;
&#36890;&#36807;&#23618;&#35282;&#33394;&#24046;&#24322;&#21270;&#65292;&#31471;&#21040;&#31471;&#35757;&#32451;&#24341;&#21457;&#20449;&#24687;&#29942;&#39048;&#65306;&#19982;&#36880;&#23618;&#35757;&#32451;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
End-to-End Training Induces Information Bottleneck through Layer-Role Differentiation: A Comparative Analysis with Layer-wise Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19982;&#36880;&#23618;&#35757;&#32451;&#30340;&#27604;&#36739;&#65292;&#37325;&#26032;&#32771;&#34385;&#20102;&#20026;&#20160;&#20040;&#31471;&#21040;&#31471;&#35757;&#32451;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#20998;&#26512;&#20102;&#20449;&#24687;&#20256;&#25773;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#24182;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#36136;&#24046;&#24322;&#36827;&#34892;&#20102;&#28145;&#20837;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#35757;&#32451;&#36890;&#36807;&#35823;&#24046;&#21453;&#21521;&#20256;&#25773;&#20248;&#21270;&#25972;&#20010;&#27169;&#22411;&#65292;&#20174;&#26681;&#26412;&#19978;&#25903;&#25345;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#12290;&#23613;&#31649;&#20854;&#24615;&#33021;&#24456;&#39640;&#65292;&#20294;E2E&#35757;&#32451;&#38754;&#20020;&#20869;&#23384;&#28040;&#32791;&#12289;&#24182;&#34892;&#35745;&#31639;&#21644;&#19982;&#23454;&#38469;&#22823;&#33041;&#21151;&#33021;&#30340;&#19981;&#19968;&#33268;&#31561;&#38382;&#39064;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26367;&#20195;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#22256;&#38590;&#65292;&#28982;&#32780;&#30446;&#21069;&#36824;&#27809;&#26377;&#19968;&#31181;&#33021;&#22815;&#19982;E2E&#35757;&#32451;&#30340;&#24615;&#33021;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#22240;&#27492;&#22312;&#23454;&#29992;&#24615;&#19978;&#23384;&#22312;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#35757;&#32451;&#27169;&#22411;&#24615;&#36136;&#30340;&#24046;&#24322;&#22312;&#24615;&#33021;&#24046;&#36317;&#20043;&#22806;&#32570;&#20047;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19982;&#36880;&#23618;&#35757;&#32451;&#36827;&#34892;&#27604;&#36739;&#65292;&#37325;&#26032;&#32771;&#34385;&#20102;&#20026;&#20160;&#20040;E2E&#35757;&#32451;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#36880;&#23618;&#35757;&#32451;&#26159;&#19968;&#31181;&#23616;&#37096;&#35774;&#32622;&#38169;&#35823;&#30340;&#38750;E2E&#26041;&#27861;&#12290;&#22312;&#35266;&#23519;&#21040;E2E&#35757;&#32451;&#22312;&#20256;&#25773;&#36755;&#20837;&#20449;&#24687;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20013;&#38388;&#34920;&#31034;&#30340;&#20449;&#24687;&#24179;&#38754;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09050v1 Announce Type: new Abstract: End-to-end (E2E) training, optimizing the entire model through error backpropagation, fundamentally supports the advancements of deep learning. Despite its high performance, E2E training faces the problems of memory consumption, parallel computing, and discrepancy with the functionalities of the actual brain. Various alternative methods have been proposed to overcome these difficulties; however, no one can yet match the performance of E2E training, thereby falling short in practicality. Furthermore, there is no deep understanding regarding differences in the trained model properties beyond the performance gap. In this paper, we reconsider why E2E training demonstrates a superior performance through a comparison with layer-wise training, a non-E2E method that locally sets errors. On the basis of the observation that E2E training has an advantage in propagating input information, we analyze the information plane dynamics of intermediate rep
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#26377;&#20984;&#19979;&#23618;&#38382;&#39064;&#30340;&#31616;&#21333;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#23616;&#37096;&#36924;&#36817;&#19979;&#23618;&#38382;&#39064;&#30340;&#35299;&#38598;&#21644;&#21152;&#36895;&#26799;&#24230;&#26356;&#26032;&#26041;&#27861;&#65292;&#22312;&#26377;&#38480;&#27425;&#36845;&#20195;&#20869;&#25214;&#21040;&#19968;&#20010;&#20855;&#26377;&#19968;&#23450;&#31934;&#24230;&#30340;&#26368;&#20248;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.08097</link><description>&lt;p&gt;
&#19968;&#31181;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#27714;&#35299;&#20855;&#26377;&#20984;&#19979;&#23618;&#38382;&#39064;&#30340;&#31616;&#21333;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
An Accelerated Gradient Method for Simple Bilevel Optimization with Convex Lower-level Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#26377;&#20984;&#19979;&#23618;&#38382;&#39064;&#30340;&#31616;&#21333;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#23616;&#37096;&#36924;&#36817;&#19979;&#23618;&#38382;&#39064;&#30340;&#35299;&#38598;&#21644;&#21152;&#36895;&#26799;&#24230;&#26356;&#26032;&#26041;&#27861;&#65292;&#22312;&#26377;&#38480;&#27425;&#36845;&#20195;&#20869;&#25214;&#21040;&#19968;&#20010;&#20855;&#26377;&#19968;&#23450;&#31934;&#24230;&#30340;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#31616;&#21333;&#30340;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#22312;&#21478;&#19968;&#20010;&#20984;&#20809;&#28369;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#38598;&#19978;&#26368;&#23567;&#21270;&#19968;&#20010;&#20984;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#23618;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20999;&#24179;&#38754;&#26041;&#27861;&#23616;&#37096;&#36924;&#36817;&#19979;&#23618;&#38382;&#39064;&#30340;&#35299;&#38598;&#65292;&#24182;&#37319;&#29992;&#21152;&#36895;&#26799;&#24230;&#26356;&#26032;&#26041;&#27861;&#38477;&#20302;&#36817;&#20284;&#35299;&#38598;&#19978;&#30340;&#19978;&#23618;&#30446;&#26631;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#23376;&#26368;&#20248;&#35299;&#21644;&#19981;&#21487;&#34892;&#35823;&#24046;&#24230;&#37327;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20004;&#20010;&#35823;&#24046;&#26631;&#20934;&#30340;&#38750;&#28176;&#36827;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;&#29305;&#21035;&#22320;&#65292;&#24403;&#21487;&#34892;&#38598;&#26159;&#32039;&#33268;&#30340;&#26102;&#20505;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26368;&#22810;&#38656;&#35201;$\mathcal{O}(\max\{1/\sqrt{\epsilon_{f}}, 1/\epsilon_g\})$&#27425;&#36845;&#20195;&#25165;&#33021;&#25214;&#21040;&#19968;&#20010;$\epsilon_f$-&#23376;&#26368;&#20248;&#19988;$\epsilon_g$-&#19981;&#21487;&#34892;&#30340;&#35299;&#12290;&#27492;&#22806;&#65292;&#22312;&#39069;&#22806;&#20551;&#35774;&#19979;&#65292;&#19979;&#23618;&#30446;&#26631;&#28385;&#36275;$r$&#38454;H\"olderian&#35823;&#24046;&#26102;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#35299;&#30340;&#25910;&#25947;&#36895;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on simple bilevel optimization problems, where we minimize a convex smooth objective function over the optimal solution set of another convex smooth constrained optimization problem. We present a novel bilevel optimization method that locally approximates the solution set of the lower-level problem using a cutting plane approach and employs an accelerated gradient-based update to reduce the upper-level objective function over the approximated solution set. We measure the performance of our method in terms of suboptimality and infeasibility errors and provide non-asymptotic convergence guarantees for both error criteria. Specifically, when the feasible set is compact, we show that our method requires at most $\mathcal{O}(\max\{1/\sqrt{\epsilon_{f}}, 1/\epsilon_g\})$ iterations to find a solution that is $\epsilon_f$-suboptimal and $\epsilon_g$-infeasible. Moreover, under the additional assumption that the lower-level objective satisfies the $r$-th H\"olderian err
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#31169;&#26377;&#21464;&#20307;&#30340;&#38750;&#21442;&#25968;bootstrap&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24046;&#20998;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#12290;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#32622;&#20449;&#21306;&#38388;&#38271;&#24230;&#19978;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.07131</link><description>&lt;p&gt;
&#38024;&#23545;&#31169;&#26377;&#32479;&#35745;&#25512;&#26029;&#30340;&#37325;&#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Resampling methods for Private Statistical Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07131
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#31169;&#26377;&#21464;&#20307;&#30340;&#38750;&#21442;&#25968;bootstrap&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24046;&#20998;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#12290;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#32622;&#20449;&#21306;&#38388;&#38271;&#24230;&#19978;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31169;&#26377;&#21464;&#20307;&#30340;&#38750;&#21442;&#25968;bootstrap&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#30340;&#20998;&#21306;&#19978;&#31169;&#19979;&#35745;&#31639;&#22810;&#20010;&#8220;&#23567;&#8221;bootstrap&#30340;&#32467;&#26524;&#30340;&#20013;&#20301;&#25968;&#65292;&#24182;&#32473;&#20986;&#20102;&#24471;&#21040;&#30340;&#32622;&#20449;&#21306;&#38388;&#30340;&#28176;&#36827;&#35206;&#30422;&#35823;&#24046;&#19978;&#30028;&#12290;&#23545;&#20110;&#22266;&#23450;&#30340;&#24046;&#20998;&#38544;&#31169;&#21442;&#25968;&#949;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26679;&#26412;&#22823;&#23567;n&#19978;&#30340;&#35823;&#24046;&#29575;&#19982;&#38750;&#31169;&#26377;bootstrap&#30456;&#24403;&#65292;&#21482;&#26159;&#22312;&#23545;&#25968;&#22240;&#23376;&#20869;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#22312;&#22343;&#20540;&#20272;&#35745;&#12289;&#20013;&#20301;&#25968;&#20272;&#35745;&#21644;&#36923;&#36753;&#22238;&#24402;&#26041;&#38754;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#32463;&#39564;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#20379;&#31867;&#20284;&#30340;&#35206;&#30422;&#31934;&#24230;&#30340;&#21516;&#26102;&#65292;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26174;&#33879;&#32553;&#30701;&#65288;&#22823;&#32422;10&#20493;&#65289;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the task of constructing confidence intervals with differential privacy. We propose two private variants of the non-parametric bootstrap, which privately compute the median of the results of multiple ``little'' bootstraps run on partitions of the data and give asymptotic bounds on the coverage error of the resulting confidence intervals. For a fixed differential privacy parameter $\epsilon$, our methods enjoy the same error rates as that of the non-private bootstrap to within logarithmic factors in the sample size $n$. We empirically validate the performance of our methods for mean estimation, median estimation, and logistic regression with both real and synthetic data. Our methods achieve similar coverage accuracy to existing methods (and non-private baselines) while providing notably shorter ($\gtrsim 10$ times) confidence intervals than previous approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23610;&#24230;&#24459;&#30340;&#35270;&#35282;&#65292;&#30740;&#31350;&#20102;AI&#27169;&#22411;&#22823;&#23567;&#22686;&#38271;&#21644;&#21512;&#25104;&#25968;&#25454;&#24341;&#20837;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#27169;&#22411;&#21487;&#33021;&#20250;&#36973;&#36935;&#24635;&#20307;&#23849;&#28291;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#19968;&#29702;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.07043</link><description>&lt;p&gt;
&#23614;&#24052;&#30340;&#25925;&#20107;&#65306;&#20316;&#20026;&#23610;&#24230;&#24459;&#21464;&#21270;&#30340;&#27169;&#22411;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
A Tale of Tails: Model Collapse as a Change of Scaling Laws
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23610;&#24230;&#24459;&#30340;&#35270;&#35282;&#65292;&#30740;&#31350;&#20102;AI&#27169;&#22411;&#22823;&#23567;&#22686;&#38271;&#21644;&#21512;&#25104;&#25968;&#25454;&#24341;&#20837;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#27169;&#22411;&#21487;&#33021;&#20250;&#36973;&#36935;&#24635;&#20307;&#23849;&#28291;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#19968;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;AI&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#38271;&#65292;&#31070;&#32463;&#23610;&#24230;&#24459;&#24050;&#25104;&#20026;&#39044;&#27979;&#22823;&#27169;&#22411;&#22312;&#25193;&#23481;&#21644;&#21407;&#22987;&#65288;&#20154;&#31867;&#25110;&#33258;&#28982;&#65289;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#22686;&#21152;&#26102;&#25913;&#21892;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#27969;&#34892;&#27169;&#22411;&#30340;&#24191;&#27867;&#20351;&#29992;&#24847;&#21619;&#30528;&#22312;&#32447;&#25968;&#25454;&#21644;&#25991;&#26412;&#30340;&#29983;&#24577;&#31995;&#32479;&#23558;&#36880;&#28176;&#21253;&#21547;&#36234;&#26469;&#36234;&#22810;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38382;&#65306;&#24403;&#21512;&#25104;&#25968;&#25454;&#36827;&#20837;&#35757;&#32451;&#35821;&#26009;&#24211;&#26102;&#65292;&#23610;&#24230;&#24459;&#20250;&#22914;&#20309;&#25913;&#21464;&#65311;&#26410;&#26469;&#30340;&#27169;&#22411;&#20173;&#20250;&#25913;&#21892;&#65292;&#36824;&#26159;&#27880;&#23450;&#20250;&#23436;&#20840;&#23849;&#28291;&#65288;&#27169;&#22411;&#23849;&#28291;&#65289;&#65311;&#36890;&#36807;&#23610;&#24230;&#24459;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#23849;&#28291;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#24191;&#27867;&#30340;&#34928;&#20943;&#29616;&#35937;&#65292;&#20998;&#26512;&#20102;&#23610;&#24230;&#30340;&#20007;&#22833;&#12289;&#19982;&#20195;&#25968;&#30340;&#21464;&#21270;&#23610;&#24230;&#12289;&#25216;&#33021;&#30340;"&#36951;&#24536;"&#20197;&#21450;&#28151;&#21512;&#20154;&#31867;&#21644;&#21512;&#25104;&#25968;&#25454;&#26102;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#36890;&#36807;&#23545;&#19968;&#20010;&#31639;&#26415;&#20219;&#21153;&#21644;&#25991;&#26412;&#29983;&#25104;&#30340;&#36716;&#25442;&#22120;&#36827;&#34892;&#22823;&#35268;&#27169;&#23454;&#39564;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI model size grows, neural scaling laws have become a crucial tool to predict the improvements of large models when increasing capacity and the size of original (human or natural) training data. Yet, the widespread use of popular models means that the ecosystem of online data and text will co-evolve to progressively contain increased amounts of synthesized data. In this paper we ask: How will the scaling laws change in the inevitable regime where synthetic data makes its way into the training corpus? Will future models, still improve, or be doomed to degenerate up to total (model) collapse? We develop a theoretical framework of model collapse through the lens of scaling laws. We discover a wide range of decay phenomena, analyzing loss of scaling, shifted scaling with number of generations, the ''un-learning" of skills, and grokking when mixing human and synthesized data. Our theory is validated by large-scale experiments with a transformer on an arithmetic task and text generation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Dirichlet&#27969;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#22312;DNA&#24207;&#21015;&#35774;&#35745;&#20013;&#21462;&#24471;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#27604;&#25152;&#26377;&#22522;&#32447;&#27169;&#22411;&#22312;&#20998;&#24067;&#24230;&#37327;&#21644;&#29983;&#25104;&#30340;&#24207;&#21015;&#35774;&#35745;&#30446;&#26631;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.05841</link><description>&lt;p&gt;
&#20351;&#29992;Dirichlet&#27969;&#21305;&#37197;&#30340;&#24212;&#29992;&#20110;DNA&#24207;&#21015;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Dirichlet Flow Matching with Applications to DNA Sequence Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Dirichlet&#27969;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#22312;DNA&#24207;&#21015;&#35774;&#35745;&#20013;&#21462;&#24471;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#27604;&#25152;&#26377;&#22522;&#32447;&#27169;&#22411;&#22312;&#20998;&#24067;&#24230;&#37327;&#21644;&#29983;&#25104;&#30340;&#24207;&#21015;&#35774;&#35745;&#30446;&#26631;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#25955;&#25193;&#25955;&#25110;&#27969;&#27169;&#22411;&#21487;&#20197;&#27604;&#33258;&#22238;&#24402;&#27169;&#22411;&#26356;&#24555;&#36895;&#21644;&#26356;&#21487;&#25511;&#22320;&#29983;&#25104;&#24207;&#21015;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#21333;&#32431;&#24418;&#19978;&#30340;&#21407;&#22987;&#32447;&#24615;&#27969;&#21305;&#37197;&#19981;&#36275;&#20197;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#22240;&#20026;&#23427;&#22312;&#35757;&#32451;&#30446;&#26631;&#21644;&#36827;&#19968;&#27493;&#30340;&#36335;&#24452;&#19978;&#23384;&#22312;&#19981;&#36830;&#32493;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;Dirichlet&#20998;&#24067;&#30340;&#28151;&#21512;&#26469;&#24320;&#21457;&#22312;&#21333;&#32431;&#24418;&#19978;&#30340;Dirichlet&#27969;&#21305;&#37197;&#20316;&#20026;&#27010;&#29575;&#36335;&#24452;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#28151;&#21512;&#20998;&#25968;&#21644;&#27969;&#21521;&#37327;&#22330;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20998;&#31867;&#22120;&#21644;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#33976;&#39311;Dirichlet&#27969;&#21305;&#37197;&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#19968;&#27493;&#24207;&#21015;&#29983;&#25104;&#32780;&#21482;&#26377;&#26368;&#23567;&#30340;&#24615;&#33021;&#25439;&#22833;&#65292;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#27604;&#65292;&#36895;&#24230;&#25552;&#39640;&#20102;O(L)&#20493;&#12290;&#22312;&#22797;&#26434;&#30340;DNA&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30456;&#23545;&#20110;&#25152;&#26377;&#22522;&#32447;&#27169;&#22411;&#22312;&#20998;&#24067;&#24230;&#37327;&#21644;&#23454;&#29616;&#29983;&#25104;&#24207;&#21015;&#30340;&#26399;&#26395;&#35774;&#35745;&#30446;&#26631;&#26041;&#38754;&#20855;&#26377;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrete diffusion or flow models could enable faster and more controllable sequence generation than autoregressive models. We show that na\"ive linear flow matching on the simplex is insufficient toward this goal since it suffers from discontinuities in the training target and further pathologies. To overcome this, we develop Dirichlet flow matching on the simplex based on mixtures of Dirichlet distributions as probability paths. In this framework, we derive a connection between the mixtures' scores and the flow's vector field that allows for classifier and classifier-free guidance. Further, we provide distilled Dirichlet flow matching, which enables one-step sequence generation with minimal performance hits, resulting in $O(L)$ speedups compared to autoregressive models. On complex DNA sequence generation tasks, we demonstrate superior performance compared to all baselines in distributional metrics and in achieving desired design targets for generated sequences. Finally, we show that
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#32447;&#32423;&#32852;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#8220;&#32423;&#32852;&#8221;&#27169;&#22411;&#65292;&#20174;&#23481;&#37327;&#36739;&#20302;&#30340;&#27169;&#22411;&#21040;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;&#25512;&#36831;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#27969;&#25968;&#25454;&#22788;&#29702;&#20013;&#21516;&#26102;&#20445;&#35777;&#20934;&#30830;&#24615;&#21644;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.04513</link><description>&lt;p&gt;
&#22312;&#27969;&#25968;&#25454;&#19978;&#36827;&#34892;&#39640;&#25928;&#25512;&#29702;&#30340;&#22312;&#32447;&#32423;&#32852;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Cascade Learning for Efficient Inference over Streams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04513
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#32447;&#32423;&#32852;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#8220;&#32423;&#32852;&#8221;&#27169;&#22411;&#65292;&#20174;&#23481;&#37327;&#36739;&#20302;&#30340;&#27169;&#22411;&#21040;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;&#25512;&#36831;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#27969;&#25968;&#25454;&#22788;&#29702;&#20013;&#21516;&#26102;&#20445;&#35777;&#20934;&#30830;&#24615;&#21644;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#22312;&#22238;&#31572;&#20851;&#20110;&#25968;&#25454;&#27969;&#30340;&#22797;&#26434;&#26597;&#35810;&#26041;&#38754;&#20855;&#26377;&#22825;&#28982;&#30340;&#20248;&#21183;&#65292;&#20294;&#26159; LLM &#25512;&#29702;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#20351;&#24471;&#23427;&#20204;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#19981;&#21487;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#32447;&#32423;&#32852;&#23398;&#20064;&#65292;&#36825;&#26159;&#39318;&#20010;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#36825;&#37324;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#8220;&#32423;&#32852;&#8221;&#27169;&#22411;&#65292;&#20174;&#23481;&#37327;&#36739;&#20302;&#30340;&#27169;&#22411;&#65288;&#22914;&#36923;&#36753;&#22238;&#24402;&#22120;&#65289;&#24320;&#22987;&#65292;&#21040;&#24378;&#22823;&#30340; LLM &#32467;&#26463;&#65292;&#24182;&#37197;&#22791;&#19968;&#20010;&#20915;&#23450;&#22312;&#32473;&#23450;&#36755;&#20837;&#19978;&#20351;&#29992;&#21738;&#20010;&#27169;&#22411;&#30340;&#25512;&#36831;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#22312;&#32447;&#23398;&#20064;&#32423;&#32852;&#30340;&#20219;&#21153;&#20844;&#24335;&#21270;&#20026;&#19968;&#20010;&#27169;&#20223;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#20026;&#35813;&#38382;&#39064;&#25552;&#20379;&#20102;&#26080;&#36951;&#25022;&#31639;&#27861;&#12290;&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982; LLM &#30456;&#24403;&#65292;&#21516;&#26102;&#23558;&#25512;&#29702;&#25104;&#26412;&#21066;&#20943;&#20102;&#22810;&#36798; 90%&#65292;&#31361;&#26174;&#20102;&#23427;&#22312;&#27969;&#22788;&#29702;&#20013;&#30340;&#25928;&#33021;&#21644;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks. We propose online cascade learning, the first approach to addressing this challenge. The objective here is to learn a "cascade" of models, starting with lower-capacity models (such as logistic regressors) and ending with a powerful LLM, along with a deferral policy that determines the model that is used on a given input. We formulate the task of learning cascades online as an imitation-learning problem and give a no-regret algorithm for the problem. Experimental results across four benchmarks show that our method parallels LLMs in accuracy while cutting down inference costs by as much as 90%, underscoring its efficacy and adaptability in stream processing.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#65292;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#21335;&#36981;&#24490;&#24773;&#20917;&#12290;&#31995;&#32479;&#36890;&#36807;&#25910;&#38598;&#29616;&#26377;&#36234;&#29425;&#24182;&#23558;&#20854;&#32452;&#32455;&#25104;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03299</link><description>&lt;p&gt;
GUARD: &#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#26469;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#21335;&#30340;&#21512;&#35268;&#24615;
&lt;/p&gt;
&lt;p&gt;
GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#65292;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#21335;&#36981;&#24490;&#24773;&#20917;&#12290;&#31995;&#32479;&#36890;&#36807;&#25910;&#38598;&#29616;&#26377;&#36234;&#29425;&#24182;&#23558;&#20854;&#32452;&#32455;&#25104;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#32469;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23433;&#20840;&#36807;&#28388;&#21644;&#26377;&#23475;&#22238;&#24212;&#30340;"&#36234;&#29425;"&#24050;&#32463;&#40723;&#21169;&#31038;&#21306;&#37319;&#21462;&#23433;&#20840;&#25514;&#26045;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#30340;&#23433;&#20840;&#25514;&#26045;&#26159;&#22312;&#21457;&#24067;&#20043;&#21069;&#29992;&#36234;&#29425;&#20027;&#21160;&#27979;&#35797;LLM&#12290;&#22240;&#27492;&#65292;&#36825;&#26679;&#30340;&#27979;&#35797;&#23558;&#38656;&#35201;&#19968;&#31181;&#33021;&#22815;&#22823;&#35268;&#27169;&#19988;&#39640;&#25928;&#22320;&#29983;&#25104;&#36234;&#29425;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#22312;&#36861;&#38543;&#19968;&#31181;&#26032;&#39062;&#32780;&#30452;&#35266;&#30340;&#31574;&#30053;&#19979;&#65292;&#20197;&#20154;&#31867;&#29983;&#25104;&#30340;&#26041;&#24335;&#26469;&#29983;&#25104;&#36234;&#29425;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35282;&#33394;&#25198;&#28436;&#31995;&#32479;&#65292;&#23558;&#22235;&#31181;&#19981;&#21516;&#35282;&#33394;&#20998;&#37197;&#32473;&#29992;&#25143;LLM&#65292;&#20197;&#20415;&#21327;&#20316;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25910;&#38598;&#29616;&#26377;&#30340;&#36234;&#29425;&#65292;&#24182;&#36890;&#36807;&#21477;&#23376;&#36880;&#21477;&#36827;&#34892;&#32858;&#31867;&#39057;&#29575;&#21644;&#35821;&#20041;&#27169;&#24335;&#30340;&#21010;&#20998;&#65292;&#23558;&#23427;&#20204;&#20998;&#25104;&#19981;&#21516;&#30340;&#29420;&#31435;&#29305;&#24449;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#29305;&#24449;&#32452;&#32455;&#25104;&#19968;&#20010;&#30693;&#35782;&#22270;&#65292;&#20351;&#20854;&#26356;&#26131;&#20110;&#35775;&#38382;&#21644;&#26816;&#32034;&#12290;&#25105;&#20204;&#30340;&#35282;&#33394;&#31995;&#32479;&#23558;&#21033;&#29992;&#36825;&#20010;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discovery of "jailbreaks" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effec
&lt;/p&gt;</description></item><item><title>&#22270;&#22522;&#30784;&#27169;&#22411;&#26159;&#22270;&#39046;&#22495;&#20013;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#36328;&#19981;&#21516;&#22270;&#21644;&#20219;&#21153;&#36827;&#34892;&#27867;&#21270;&#30340;&#22270;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; GFM &#21457;&#23637;&#35270;&#35282;&#65292;&#36890;&#36807;&#20513;&#23548;&#8220;&#22270;&#35789;&#27719;&#34920;&#8221;&#26469;&#32534;&#30721;&#22270;&#30340;&#19981;&#21464;&#24615;&#65292;&#26377;&#21161;&#20110;&#25512;&#36827;&#26410;&#26469;&#30340;GFM&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.02216</link><description>&lt;p&gt;
&#22270;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Graph Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02216
&lt;/p&gt;
&lt;p&gt;
&#22270;&#22522;&#30784;&#27169;&#22411;&#26159;&#22270;&#39046;&#22495;&#20013;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#36328;&#19981;&#21516;&#22270;&#21644;&#20219;&#21153;&#36827;&#34892;&#27867;&#21270;&#30340;&#22270;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; GFM &#21457;&#23637;&#35270;&#35282;&#65292;&#36890;&#36807;&#20513;&#23548;&#8220;&#22270;&#35789;&#27719;&#34920;&#8221;&#26469;&#32534;&#30721;&#22270;&#30340;&#19981;&#21464;&#24615;&#65292;&#26377;&#21161;&#20110;&#25512;&#36827;&#26410;&#26469;&#30340;GFM&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#22522;&#30784;&#27169;&#22411;&#65288;Graph Foundation Model&#65292;GFM&#65289;&#26159;&#22270;&#39046;&#22495;&#20013;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#36328;&#19981;&#21516;&#22270;&#21644;&#20219;&#21153;&#36827;&#34892;&#27867;&#21270;&#30340;&#22270;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#23454;&#29616;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;GFM&#12290;&#26500;&#24314;GFM&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#33021;&#22312;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#30340;&#22270;&#20043;&#38388;&#23454;&#29616;&#27491;&#21521;&#36801;&#31227;&#12290;&#21463;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; GFM &#21457;&#23637;&#35270;&#35282;&#65292;&#36890;&#36807;&#20513;&#23548;&#8220;&#22270;&#35789;&#27719;&#34920;&#8221;&#65292;&#21363;&#28508;&#34255;&#20110;&#22270;&#20013;&#30340;&#22522;&#26412;&#21487;&#36801;&#31227;&#21333;&#20803;&#26469;&#32534;&#30721;&#22270;&#30340;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#20174;&#32593;&#32476;&#20998;&#26512;&#12289;&#29702;&#35770;&#22522;&#30784;&#21644;&#31283;&#23450;&#24615;&#31561;&#37325;&#35201;&#26041;&#38754;&#26469;&#24314;&#31435;&#22270;&#35789;&#27719;&#34920;&#12290;&#36825;&#31181;&#35789;&#27719;&#34920;&#30340;&#35270;&#35282;&#26377;&#21161;&#20110;&#25353;&#29031;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#25512;&#36827;&#26410;&#26469;&#30340;GFM&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Foundation Model (GFM) is a new trending research topic in the graph domain, aiming to develop a graph model capable of generalizing across different graphs and tasks. However, a versatile GFM has not yet been achieved. The key challenge in building GFM is how to enable positive transfer across graphs with diverse structural patterns. Inspired by the existing foundation models in the CV and NLP domains, we propose a novel perspective for the GFM development by advocating for a ``graph vocabulary'', in which the basic transferable units underlying graphs encode the invariance on graphs. We ground the graph vocabulary construction from essential aspects including network analysis, theoretical foundations, and stability. Such a vocabulary perspective can potentially advance the future GFM design following the neural scaling laws.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#38543;&#26426;&#32447;&#24615;&#25237;&#24433;&#65288;RLP&#65289;&#25439;&#22833;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#20013;&#30340;&#20960;&#20309;&#20851;&#31995;&#26469;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25928;&#29575;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#20351;&#29992;RLP&#25439;&#22833;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20248;&#20110;&#20256;&#32479;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#30340;&#32593;&#32476;&#65292;&#22312;&#26356;&#23569;&#30340;&#25968;&#25454;&#26679;&#26412;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#23545;&#20110;&#28155;&#21152;&#22122;&#22768;&#34920;&#29616;&#26356;&#24378;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.12356</link><description>&lt;p&gt;
&#22522;&#20110;&#36229;&#24179;&#38754;&#20248;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38543;&#26426;&#32447;&#24615;&#25237;&#24433;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Random Linear Projections Loss for Hyperplane-Based Optimization in Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#38543;&#26426;&#32447;&#24615;&#25237;&#24433;&#65288;RLP&#65289;&#25439;&#22833;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#20013;&#30340;&#20960;&#20309;&#20851;&#31995;&#26469;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25928;&#29575;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#20351;&#29992;RLP&#25439;&#22833;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20248;&#20110;&#20256;&#32479;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#30340;&#32593;&#32476;&#65292;&#22312;&#26356;&#23569;&#30340;&#25968;&#25454;&#26679;&#26412;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#23545;&#20110;&#28155;&#21152;&#22122;&#22768;&#34920;&#29616;&#26356;&#24378;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38543;&#26426;&#32447;&#24615;&#25237;&#24433;&#65288;RLP&#65289;&#25439;&#22833;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#20013;&#30340;&#20960;&#20309;&#20851;&#31995;&#26469;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;&#19982;&#20256;&#32479;&#30340;&#26088;&#22312;&#26368;&#23567;&#21270;&#36880;&#28857;&#35823;&#24046;&#30340;&#25439;&#22833;&#20989;&#25968;&#19981;&#21516;&#65292;RLP&#25439;&#22833;&#36890;&#36807;&#26368;&#23567;&#21270;&#36830;&#25509;&#22266;&#23450;&#22823;&#23567;&#30340;&#29305;&#24449;-&#39044;&#27979;&#23545;&#21644;&#29305;&#24449;-&#26631;&#31614;&#23545;&#30340;&#36229;&#24179;&#38754;&#38598;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#25805;&#20316;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#31034;&#20363;&#19978;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#20351;&#29992;RLP&#25439;&#22833;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20248;&#20110;&#20351;&#29992;&#20256;&#32479;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#30340;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#26356;&#23569;&#30340;&#25968;&#25454;&#26679;&#26412;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#23545;&#20110;&#28155;&#21152;&#22122;&#22768;&#34920;&#29616;&#26356;&#24378;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#25903;&#25345;&#25105;&#20204;&#23454;&#35777;&#32467;&#26524;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12356v2 Announce Type: replace  Abstract: Advancing loss function design is pivotal for optimizing neural network training and performance. This work introduces Random Linear Projections (RLP) loss, a novel approach that enhances training efficiency by leveraging geometric relationships within the data. Distinct from traditional loss functions that target minimizing pointwise errors, RLP loss operates by minimizing the distance between sets of hyperplanes connecting fixed-size subsets of feature-prediction pairs and feature-label pairs. Our empirical evaluations, conducted across benchmark datasets and synthetic examples, demonstrate that neural networks trained with RLP loss outperform those trained with traditional loss functions, achieving improved performance with fewer data samples, and exhibiting greater robustness to additive noise. We provide theoretical analysis supporting our empirical findings.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#22312;&#20083;&#33146;MRI&#20013;&#21512;&#25104;&#23545;&#27604;&#22686;&#24378;&#65292;&#24341;&#20837;&#20102;Scaled Aggregate Measure (SAMe)&#36827;&#34892;&#37327;&#21270;&#35780;&#20272;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#20083;&#33146;&#32959;&#30244;&#20998;&#21106;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2311.10879</link><description>&lt;p&gt;
&#25913;&#36827;&#20083;&#33146;MRI&#32959;&#30244;&#20998;&#21106;&#30340;&#21069;&#21518;&#23545;&#27604;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Pre- to Post-Contrast Breast MRI Synthesis for Enhanced Tumour Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10879
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#22312;&#20083;&#33146;MRI&#20013;&#21512;&#25104;&#23545;&#27604;&#22686;&#24378;&#65292;&#24341;&#20837;&#20102;Scaled Aggregate Measure (SAMe)&#36827;&#34892;&#37327;&#21270;&#35780;&#20272;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#20083;&#33146;&#32959;&#30244;&#20998;&#21106;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21160;&#24577;&#22686;&#24378;MRI&#65288;DCE-MRI&#65289;&#20013;&#23545;&#27604;&#21058;&#30340;&#20351;&#29992;&#23545;&#20110;&#32959;&#30244;&#30340;&#26816;&#27979;&#21644;&#27835;&#30103;&#26377;&#30410;&#22788;&#65292;&#20294;&#23384;&#22312;&#19968;&#31995;&#21015;&#38382;&#39064;&#65292;&#21253;&#25324;&#20854;&#20405;&#20837;&#24615;&#12289;&#29983;&#29289;&#31215;&#32047;&#24615;&#21450;&#28508;&#22312;&#30340;&#32958;&#28304;&#24615;&#31995;&#32479;&#32420;&#32500;&#21270;&#39118;&#38505;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;&#23558;&#26415;&#21069;&#23545;&#27604;&#21069;T1&#21152;&#26435;&#33026;&#32938;&#39281;&#21644;&#20083;&#33146;MRI&#36716;&#25442;&#20026;&#20854;&#23545;&#24212;&#30340;&#39318;&#27425;DCE-MRI&#24207;&#21015;&#65292;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#33021;&#21147;&#26469;&#20135;&#29983;&#21512;&#25104;&#23545;&#27604;&#22686;&#24378;&#30340;&#21487;&#34892;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#23450;&#37327;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#30340;&#27604;&#20363;&#32858;&#21512;&#27979;&#37327;&#65288;SAMe&#65289;&#65292;&#24182;&#20316;&#20026;&#36873;&#25321;&#26368;&#20339;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#20351;&#29992;&#23450;&#37327;&#22270;&#20687;&#36136;&#37327;&#25351;&#26631;&#35780;&#20272;&#29983;&#25104;&#30340;DCE-MRI&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;3D&#20083;&#33146;&#32959;&#30244;&#20998;&#21106;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;p&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10879v2 Announce Type: replace-cross  Abstract: Despite its benefits for tumour detection and treatment, the administration of contrast agents in dynamic contrast-enhanced MRI (DCE-MRI) is associated with a range of issues, including their invasiveness, bioaccumulation, and a risk of nephrogenic systemic fibrosis. This study explores the feasibility of producing synthetic contrast enhancements by translating pre-contrast T1-weighted fat-saturated breast MRI to their corresponding first DCE-MRI sequence leveraging the capabilities of a generative adversarial network (GAN). Additionally, we introduce a Scaled Aggregate Measure (SAMe) designed for quantitatively evaluating the quality of synthetic data in a principled manner and serving as a basis for selecting the optimal generative model. We assess the generated DCE-MRI data using quantitative image quality metrics and apply them to the downstream task of 3D breast tumour segmentation. Our results highlight the potential of p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#21270;&#24402;&#19968;&#21270;&#27969;&#30340;&#27010;&#29575;&#22825;&#27668;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#33021;&#22815;&#25429;&#25417;&#21644;&#33391;&#22909;&#22806;&#25512;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.06958</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;&#21270;&#31354;&#38388;-&#26102;&#38388;&#24402;&#19968;&#21270;&#27969;&#30340;&#27010;&#29575;&#22825;&#27668;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards probabilistic Weather Forecasting with Conditioned Spatio-Temporal Normalizing Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06958
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#21270;&#24402;&#19968;&#21270;&#27969;&#30340;&#27010;&#29575;&#22825;&#27668;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#33021;&#22815;&#25429;&#25417;&#21644;&#33391;&#22909;&#22806;&#25512;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#24402;&#19968;&#21270;&#27969;&#33021;&#22815;&#24314;&#27169;&#22810;&#27169;&#24577;&#31354;&#38388;&#20998;&#24067;&#65292;&#24050;&#32463;&#25104;&#21151;&#22320;&#27169;&#25311;&#20102;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#30001;&#20110;&#20854;&#35757;&#32451;&#31283;&#23450;&#24615;&#12289;&#21487;&#36870;&#24615;&#20197;&#21450;&#22312;&#37319;&#26679;&#21644;&#25512;&#26029;&#26041;&#38754;&#30340;&#39640;&#25928;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#27604;&#20854;&#20182;&#31867;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#20102;&#20960;&#39033;&#22909;&#22788;&#12290;&#36825;&#20351;&#23427;&#20204;&#25104;&#20026;&#38543;&#26426;&#31354;&#38388;-&#26102;&#38388;&#39044;&#27979;&#38382;&#39064;&#30340;&#21512;&#36866;&#20505;&#36873;&#32773;&#65292;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20013;&#37117;&#26222;&#36941;&#23384;&#22312;&#65292;&#22914;&#22320;&#29699;&#31185;&#23398;&#12289;&#22825;&#20307;&#29289;&#29702;&#23398;&#25110;&#20998;&#23376;&#31185;&#23398;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#38543;&#26426;&#31354;&#38388;-&#26102;&#38388;&#24314;&#27169;&#30340;&#26465;&#20214;&#21270;&#24402;&#19968;&#21270;&#27969;&#12290;&#35813;&#26041;&#27861;&#22312;&#20174;ERA5&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#26085;&#28201;&#24230;&#21644;&#23567;&#26102;&#31561;&#21387;&#22270;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#33021;&#22815;&#22312;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#30340;&#26102;&#38388;&#33539;&#22260;&#20043;&#22806;&#36827;&#34892;&#33391;&#22909;&#30340;&#22806;&#25512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06958v2 Announce Type: replace-cross  Abstract: Generative normalizing flows are able to model multimodal spatial distributions, and they have been shown to model temporal correlations successfully as well. These models provide several benefits over other types of generative models due to their training stability, invertibility and efficiency in sampling and inference. This makes them a suitable candidate for stochastic spatio-temporal prediction problems, which are omnipresent in many fields of sciences, such as earth sciences, astrophysics or molecular sciences. In this paper, we present conditional normalizing flows for stochastic spatio-temporal modelling. The method is evaluated on the task of daily temperature and hourly geopotential map prediction from ERA5 datasets. Experiments show that our method is able to capture spatio-temporal correlations and extrapolates well beyond the time horizon used during training.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;Meta-Continual Active Learning&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#21644;&#32463;&#39564;&#37325;&#25773;&#35299;&#20915;&#23569;&#26679;&#26412;&#25345;&#32493;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#28151;&#28102;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36827;&#19968;&#27493;&#32467;&#21512;&#25991;&#26412;&#22686;&#24378;&#26469;&#30830;&#20445;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2311.03732</link><description>&lt;p&gt;
&#23398;&#20064;&#23398;&#20064;&#20197;&#36827;&#34892;&#23569;&#26679;&#26412;&#25345;&#20037;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Learn for Few-shot Continual Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;Meta-Continual Active Learning&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#21644;&#32463;&#39564;&#37325;&#25773;&#35299;&#20915;&#23569;&#26679;&#26412;&#25345;&#32493;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#28151;&#28102;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36827;&#19968;&#27493;&#32467;&#21512;&#25991;&#26412;&#22686;&#24378;&#26469;&#30830;&#20445;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#30830;&#20445;&#35299;&#20915;&#20808;&#21069;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#23637;&#31034;&#23545;&#26032;&#39046;&#22495;&#30340;&#21487;&#22609;&#24615;&#12290;&#26368;&#36817;&#22312;&#25345;&#32493;&#23398;&#20064;&#26041;&#38754;&#30340;&#36827;&#23637;&#20027;&#35201;&#23616;&#38480;&#20110;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#65292;&#23588;&#20854;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#25345;&#32493;&#20027;&#21160;&#23398;&#20064;&#65288;CAL&#65289;&#35774;&#32622;&#65292;&#20854;&#20013;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#65292;&#20294;&#26410;&#26631;&#35760;&#25968;&#25454;&#20805;&#36275;&#65292;&#20294;&#26377;&#38480;&#30340;&#27880;&#37322;&#39044;&#31639;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20803;&#25345;&#32493;&#20027;&#21160;&#23398;&#20064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#37319;&#29992;&#20803;&#23398;&#20064;&#21644;&#32463;&#39564;&#37325;&#25773;&#26469;&#35299;&#20915;&#20219;&#21153;&#20043;&#38388;&#30340;&#28151;&#28102;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#32467;&#21512;&#25991;&#26412;&#22686;&#24378;&#26469;&#30830;&#20445;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20197;&#39564;&#35777;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#22312;&#23569;&#26679;&#26412;CAL&#35774;&#32622;&#20013;&#19981;&#21516;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03732v2 Announce Type: replace-cross  Abstract: Continual learning strives to ensure stability in solving previously seen tasks while demonstrating plasticity in a novel domain. Recent advances in CL are mostly confined to a supervised learning setting, especially in NLP domain. In this work, we consider a few-shot continual active learning (CAL) setting where labeled data are inadequate, and unlabeled data are abundant but with a limited annotation budget. We propose a simple but efficient method, called Meta-Continual Active Learning. Specifically, we employ meta-learning and experience replay to address inter-task confusion and catastrophic forgetting. We further incorporate textual augmentations to ensure generalization. We conduct extensive experiments on benchmark text classification datasets to validate the effectiveness of the proposed method and analyze the effect of different active learning strategies in few-shot CAL setting. Our experimental results demonstrate t
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#35856;&#27874;&#33258;&#35843;&#27969;&#21305;&#37197;&#26041;&#27861;&#65292;&#22312;&#22810;&#37197;&#20307;&#23545;&#25509;&#21644;&#32467;&#21512;&#20301;&#28857;&#35774;&#35745;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#29983;&#25104;&#36807;&#31243;&#21644;&#35774;&#35745;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2310.05764</link><description>&lt;p&gt;
&#22810;&#37197;&#20307;&#23545;&#25509;&#21644;&#32467;&#21512;&#20301;&#28857;&#35774;&#35745;&#30340;&#35856;&#27874;&#33258;&#35843;&#27969;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Harmonic Self-Conditioned Flow Matching for Multi-Ligand Docking and Binding Site Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05764
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#35856;&#27874;&#33258;&#35843;&#27969;&#21305;&#37197;&#26041;&#27861;&#65292;&#22312;&#22810;&#37197;&#20307;&#23545;&#25509;&#21644;&#32467;&#21512;&#20301;&#28857;&#35774;&#35745;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#29983;&#25104;&#36807;&#31243;&#21644;&#35774;&#35745;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#34507;&#30333;&#36136;&#21151;&#33021;&#38656;&#35201;&#19982;&#23567;&#20998;&#23376;&#32467;&#21512;&#65292;&#21253;&#25324;&#37238;&#20652;&#21270;&#12290;&#22240;&#27492;&#65292;&#20026;&#23567;&#20998;&#23376;&#35774;&#35745;&#32467;&#21512;&#21475;&#34955;&#20855;&#26377;&#20174;&#33647;&#29289;&#21512;&#25104;&#21040;&#33021;&#37327;&#23384;&#20648;&#31561;&#22810;&#31181;&#24433;&#21709;&#28145;&#36828;&#30340;&#24212;&#29992;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;HarmonicFlow&#65292;&#36825;&#26159;&#19968;&#20010;&#25913;&#36827;&#30340;&#22522;&#20110;&#33258;&#35843;&#27969;&#21305;&#37197;&#30446;&#26631;&#30340;3D&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#32467;&#26500;&#29983;&#25104;&#36807;&#31243;&#12290;FlowSite&#23558;&#36825;&#31181;&#27969;&#27169;&#22411;&#25193;&#23637;&#21040;&#32852;&#21512;&#29983;&#25104;&#34507;&#30333;&#36136;&#21475;&#34955;&#30340;&#31163;&#25955;&#27531;&#22522;&#31867;&#22411;&#21644;&#20998;&#23376;&#30340;&#32467;&#21512;3D&#32467;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;HarmonicFlow&#22312;&#21475;&#34955;&#32423;&#23545;&#25509;&#20013;&#22312;&#31616;&#21333;&#24615;&#12289;&#26222;&#36866;&#24615;&#21644;&#24179;&#22343;&#26679;&#26412;&#36136;&#37327;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;&#20511;&#21161;&#20110;&#36825;&#31181;&#32467;&#26500;&#24314;&#27169;&#65292;FlowSite&#35774;&#35745;&#30340;&#32467;&#21512;&#20301;&#28857;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05764v3 Announce Type: replace-cross  Abstract: A significant amount of protein function requires binding small molecules, including enzymatic catalysis. As such, designing binding pockets for small molecules has several impactful applications ranging from drug synthesis to energy storage. Towards this goal, we first develop HarmonicFlow, an improved generative process over 3D protein-ligand binding structures based on our self-conditioned flow matching objective. FlowSite extends this flow model to jointly generate a protein pocket's discrete residue types and the molecule's binding 3D structure. We show that HarmonicFlow improves upon state-of-the-art generative processes for docking in simplicity, generality, and average sample quality in pocket-level docking. Enabled by this structure modeling, FlowSite designs binding sites substantially better than baseline approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24037;&#20855;&#21464;&#37327;&#27861;&#35782;&#21035;&#21644;&#20272;&#35745;&#36830;&#32493;&#22788;&#29702;&#30340;&#22240;&#26524;&#25928;&#24212;&#24322;&#36136;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19977;&#31867;&#30456;&#24212;&#30340;&#20272;&#35745;&#22120;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#32479;&#35745;&#24615;&#36136;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2401.11130</link><description>&lt;p&gt;
&#36890;&#36807;&#24037;&#20855;&#21464;&#37327;&#27861;&#35782;&#21035;&#21644;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#20559;&#22240;&#26524;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Identification and Estimation of Conditional Average Partial Causal Effects via Instrumental Variable. (arXiv:2401.11130v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24037;&#20855;&#21464;&#37327;&#27861;&#35782;&#21035;&#21644;&#20272;&#35745;&#36830;&#32493;&#22788;&#29702;&#30340;&#22240;&#26524;&#25928;&#24212;&#24322;&#36136;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19977;&#31867;&#30456;&#24212;&#30340;&#20272;&#35745;&#22120;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#32479;&#35745;&#24615;&#36136;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#20272;&#35745;&#24322;&#36136;&#22240;&#26524;&#25928;&#24212;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#21152;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#26465;&#20214;&#24179;&#22343;&#20559;&#22240;&#26524;&#25928;&#24212;&#65288;CAPCE&#65289;&#65292;&#20197;&#25581;&#31034;&#36830;&#32493;&#22788;&#29702;&#30340;&#22240;&#26524;&#25928;&#24212;&#30340;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#22312;&#24037;&#20855;&#21464;&#37327;&#35774;&#32622;&#19979;&#35782;&#21035;CAPCE&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#31867;CAPCE&#20272;&#35745;&#22120;&#65306;&#31579;&#36873;&#12289;&#21442;&#25968;&#21270;&#21644;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;-&#22522;&#30784;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#32479;&#35745;&#24615;&#36136;&#12290;&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#23545;&#25552;&#20986;&#30340;CAPCE&#20272;&#35745;&#22120;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been considerable recent interest in estimating heterogeneous causal effects. In this paper, we introduce conditional average partial causal effects (CAPCE) to reveal the heterogeneity of causal effects with continuous treatment. We provide conditions for identifying CAPCE in an instrumental variable setting. We develop three families of CAPCE estimators: sieve, parametric, and reproducing kernel Hilbert space (RKHS)-based, and analyze their statistical properties. We illustrate the proposed CAPCE estimators on synthetic and real-world data.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#24322;&#26500;&#24615;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#25918;&#26494;&#30340;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#65292;&#38450;&#27490;&#34920;&#31034;&#22349;&#32553;&#65292;&#22686;&#24378;&#29305;&#24449;&#30340;&#21487;&#20256;&#36882;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.04928</link><description>&lt;p&gt;
&#25918;&#26494;&#30340;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Relaxed Contrastive Learning for Federated Learning. (arXiv:2401.04928v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04928
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#24322;&#26500;&#24615;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#25918;&#26494;&#30340;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#65292;&#38450;&#27490;&#34920;&#31034;&#22349;&#32553;&#65292;&#22686;&#24378;&#29305;&#24449;&#30340;&#21487;&#20256;&#36882;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#26377;&#25928;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#24322;&#26500;&#24615;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#26412;&#22320;&#35757;&#32451;&#20013;&#23458;&#25143;&#31471;&#20043;&#38388;&#26799;&#24230;&#26356;&#26032;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#24314;&#31435;&#20854;&#19982;&#29305;&#24449;&#34920;&#31034;&#20998;&#24067;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#23548;&#20986;&#20102;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;SCL&#65289;&#30446;&#26631;&#26469;&#20943;&#36731;&#23616;&#37096;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;SCL&#30340;&#26420;&#32032;&#24212;&#29992;&#20250;&#23548;&#33268;&#34920;&#31034;&#22349;&#32553;&#65292;&#23548;&#33268;&#25910;&#25947;&#32531;&#24930;&#21644;&#24615;&#33021;&#25552;&#21319;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25918;&#26494;&#30340;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#65292;&#23545;&#27599;&#20010;&#31867;&#21035;&#20869;&#36807;&#20110;&#30456;&#20284;&#30340;&#26679;&#26412;&#23545;&#26045;&#21152;&#21457;&#25955;&#24809;&#32602;&#12290;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#38450;&#27490;&#34920;&#31034;&#22349;&#32553;&#65292;&#22686;&#24378;&#29305;&#24449;&#30340;&#21487;&#20256;&#36882;&#24615;&#65292;&#20419;&#36827;&#21327;&#20316;&#35757;&#32451;&#65292;&#24182;&#23548;&#33268;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#25152;&#26377;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#20013;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel contrastive learning framework to effectively address the challenges of data heterogeneity in federated learning. We first analyze the inconsistency of gradient updates across clients during local training and establish its dependence on the distribution of feature representations, leading to the derivation of the supervised contrastive learning (SCL) objective to mitigate local deviations. In addition, we show that a na\"ive adoption of SCL in federated learning leads to representation collapse, resulting in slow convergence and limited performance gains. To address this issue, we introduce a relaxed contrastive learning loss that imposes a divergence penalty on excessively similar sample pairs within each class. This strategy prevents collapsed representations and enhances feature transferability, facilitating collaborative training and leading to significant performance improvements. Our framework outperforms all existing federated learning approaches by huge marg
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#31070;&#32463;&#36864;&#34892;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;AD&#21644;MCI&#30340;&#35782;&#21035;&#12290;&#35813;&#32593;&#32476;&#32771;&#34385;&#20102;&#23616;&#37096;&#32467;&#26500;&#29305;&#24449;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#36827;&#34892;&#26089;&#26399;AD&#30340;&#35786;&#26029;&#12290;</title><link>http://arxiv.org/abs/2401.03922</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#31070;&#32463;&#36864;&#34892;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#24314;&#27169;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Structure-focused Neurodegeneration Convolutional Neural Network for Modeling and Classification of Alzheimer's Disease. (arXiv:2401.03922v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#31070;&#32463;&#36864;&#34892;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;AD&#21644;MCI&#30340;&#35782;&#21035;&#12290;&#35813;&#32593;&#32476;&#32771;&#34385;&#20102;&#23616;&#37096;&#32467;&#26500;&#29305;&#24449;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#36827;&#34892;&#26089;&#26399;AD&#30340;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#20316;&#20026;&#20027;&#35201;&#30340;&#30196;&#21574;&#24418;&#24335;&#65292;&#23545;&#20840;&#29699;&#26500;&#25104;&#20102;&#19968;&#20010;&#19981;&#26029;&#22686;&#38271;&#30340;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#20102;&#20934;&#30830;&#21644;&#26089;&#26399;&#35786;&#26029;&#30340;&#32039;&#36843;&#24615;&#12290;&#20020;&#24202;&#25216;&#26415;&#20013;&#65292;&#25918;&#23556;&#31185;&#21307;&#29983;&#22312;&#20351;&#29992;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#21306;&#20998;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65288;MCI&#65289;&#21644;AD&#26102;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#22240;&#20026;&#36825;&#20123;&#26041;&#27861;&#19981;&#19968;&#33268;&#19988;&#19981;&#21487;&#38752;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#26377;&#26395;&#25552;&#20379;&#26089;&#26399;AD&#35786;&#26029;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#32454;&#31890;&#24230;&#30340;&#23616;&#37096;&#29305;&#24449;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#21040;&#21487;&#20197;&#25552;&#20379;&#22823;&#33041;&#30382;&#23618;&#31070;&#32463;&#36864;&#34892;&#20449;&#24687;&#30340;&#23616;&#37096;&#32467;&#26500;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#38598;&#25104;&#20102;Gamma&#26657;&#27491;&#65288;&#19968;&#31181;&#22270;&#20687;&#22686;&#24378;&#25216;&#26415;&#65289;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;SNeurodCNN&#30340;&#32467;&#26500;&#21270;&#31070;&#32463;&#36864;&#34892;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#21306;&#20998;AD&#21644;MCI&#12290;&#35813;ML&#26694;&#26550;&#21033;&#29992;&#20102;&#20013;&#30690;&#29366;&#38754;&#21644;&#26049;&#30690;&#29366;&#38754;&#30340;&#22823;&#33041;&#22270;&#20687;&#35270;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimer's disease (AD), the predominant form of dementia, poses a growing global challenge and underscores the urgency of accurate and early diagnosis. The clinical technique radiologists adopt for distinguishing between mild cognitive impairment (MCI) and AD using Machine Resonance Imaging (MRI) encounter hurdles because they are not consistent and reliable. Machine learning has been shown to offer promise for early AD diagnosis. However, existing models focused on focal fine-grain features without considerations to focal structural features that give off information on neurodegeneration of the brain cerebral cortex. Therefore, this paper proposes a machine learning (ML) framework that integrates Gamma correction, an image enhancement technique, and includes a structure-focused neurodegeneration convolutional neural network (CNN) architecture called SNeurodCNN for discriminating between AD and MCI. The ML framework leverages the mid-sagittal and para-sagittal brain image viewpoints 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22270;&#21367;&#31215;&#26469;&#25913;&#36827;Transformer&#27169;&#22411;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#22810;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2312.04234</link><description>&lt;p&gt;
&#22270;&#21367;&#31215;&#22312;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#36215;&#21040;&#20102;&#25913;&#36827;&#30340;&#20316;&#29992;&#65281;&#65288;arXiv&#65306;2312.04234v2 [cs.LG]&#24050;&#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
Graph Convolutions Enrich the Self-Attention in Transformers!. (arXiv:2312.04234v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04234
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22270;&#21367;&#31215;&#26469;&#25913;&#36827;Transformer&#27169;&#22411;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#22810;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22240;&#20854;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#32780;&#38395;&#21517;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;Transformer&#27169;&#22411;&#38754;&#20020;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#21363;&#34920;&#31034;&#22312;&#21508;&#20010;&#23618;&#20043;&#38388;&#36235;&#20110;&#26080;&#27861;&#21306;&#20998;&#30340;&#20540;&#65292;&#23548;&#33268;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#12290;&#25105;&#20204;&#23558;&#21407;&#22987;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#35299;&#37322;&#20026;&#19968;&#31181;&#31616;&#21333;&#30340;&#22270;&#28388;&#27874;&#22120;&#65292;&#24182;&#20174;&#22270;&#20449;&#21495;&#22788;&#29702;&#65288;GSP&#65289;&#30340;&#35282;&#24230;&#37325;&#26032;&#35774;&#35745;&#23427;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#28388;&#27874;&#22120;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;GFSA&#65289;&#65292;&#20197;&#23398;&#20064;&#19968;&#31181;&#26082;&#36890;&#29992;&#21448;&#26377;&#25928;&#30340;&#26426;&#21046;&#65292;&#20854;&#22797;&#26434;&#24230;&#30053;&#39640;&#20110;&#21407;&#22987;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;GFSA&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#22270;&#27169;&#24335;&#20998;&#31867;&#12289;&#35821;&#38899;&#35782;&#21035;&#21644;&#20195;&#30721;&#20998;&#31867;&#31561;&#22810;&#20010;&#39046;&#22495;&#20013;&#25913;&#36827;&#20102;Transformer&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers, renowned for their self-attention mechanism, have achieved state-of-the-art performance across various tasks in natural language processing, computer vision, time-series modeling, etc. However, one of the challenges with deep Transformer models is the oversmoothing problem, where representations across layers converge to indistinguishable values, leading to significant performance degradation. We interpret the original self-attention as a simple graph filter and redesign it from a graph signal processing (GSP) perspective. We propose graph-filter-based self-attention (GFSA) to learn a general yet effective one, whose complexity, however, is slightly larger than that of the original self-attention mechanism. We demonstrate that GFSA improves the performance of Transformers in various fields, including computer vision, natural language processing, graph pattern classification, speech recognition, and code classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#32467;&#21512;&#20449;&#21495;&#20256;&#25773;&#29702;&#35770;&#21644;&#23454;&#35777;&#35266;&#23519;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;Transformer&#22359;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#31616;&#21270;&#26041;&#27861;&#21487;&#20197;&#21435;&#38500;&#35768;&#22810;&#19981;&#24433;&#21709;&#35757;&#32451;&#36895;&#24230;&#30340;&#32452;&#20214;&#65292;&#24182;&#19988;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#27604;&#26631;&#20934;Transformer&#22359;&#30340;&#35757;&#32451;&#21534;&#21520;&#37327;&#25552;&#39640;&#20102;15%&#12290;</title><link>http://arxiv.org/abs/2311.01906</link><description>&lt;p&gt;
&#31616;&#21270;Transformer&#22359;
&lt;/p&gt;
&lt;p&gt;
Simplifying Transformer Blocks. (arXiv:2311.01906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#32467;&#21512;&#20449;&#21495;&#20256;&#25773;&#29702;&#35770;&#21644;&#23454;&#35777;&#35266;&#23519;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;Transformer&#22359;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#31616;&#21270;&#26041;&#27861;&#21487;&#20197;&#21435;&#38500;&#35768;&#22810;&#19981;&#24433;&#21709;&#35757;&#32451;&#36895;&#24230;&#30340;&#32452;&#20214;&#65292;&#24182;&#19988;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#27604;&#26631;&#20934;Transformer&#22359;&#30340;&#35757;&#32451;&#21534;&#21520;&#37327;&#25552;&#39640;&#20102;15%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#28145;&#24230;Transformer&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#35774;&#35745;&#26041;&#27861;&#26159;&#32452;&#21512;&#30456;&#21516;&#30340;&#26500;&#24314;&#22359;&#12290;&#20294;&#26159;&#26631;&#20934;&#30340;Transformer&#22359;&#36828;&#38750;&#31616;&#21333;&#65292;&#23427;&#20204;&#23558;&#27880;&#24847;&#21147;&#21644;MLP&#23376;&#22359;&#19982;&#36339;&#36830;&#25509;&#21644;&#26631;&#20934;&#21270;&#23618;&#20197;&#31934;&#30830;&#30340;&#26041;&#24335;&#20132;&#32455;&#22312;&#19968;&#36215;&#12290;&#36825;&#31181;&#22797;&#26434;&#24615;&#23548;&#33268;&#20102;&#33030;&#24369;&#30340;&#26550;&#26500;&#65292;&#21363;&#20284;&#20046;&#24494;&#23567;&#30340;&#25913;&#21464;&#21487;&#33021;&#20250;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#36895;&#24230;&#65292;&#25110;&#20351;&#27169;&#22411;&#26080;&#27861;&#35757;&#32451;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26631;&#20934;Transformer&#22359;&#21487;&#20197;&#34987;&#31616;&#21270;&#21040;&#20160;&#20040;&#31243;&#24230;&#65311;&#32467;&#21512;&#20449;&#21495;&#20256;&#25773;&#29702;&#35770;&#21644;&#23454;&#35777;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#20197;&#21435;&#38500;&#35768;&#22810;&#22359;&#32452;&#20214;&#32780;&#19981;&#25439;&#22833;&#35757;&#32451;&#36895;&#24230;&#30340;&#20462;&#25913;&#65292;&#21253;&#25324;&#36339;&#36830;&#25509;&#12289;&#25237;&#24433;&#25110;&#20540;&#21442;&#25968;&#12289;&#24207;&#21015;&#23376;&#22359;&#21644;&#26631;&#20934;&#21270;&#23618;&#12290;&#22312;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#21644;BERT&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#31616;&#21270;&#30340;Transformer&#22359;&#22312;&#20445;&#25345;&#35757;&#32451;&#36895;&#24230;&#21644;&#24615;&#33021;&#30340;&#22522;&#30784;&#19978;&#65292;&#27604;&#26631;&#20934;Transformer&#22359;&#30340;&#35757;&#32451;&#21534;&#21520;&#37327;&#25552;&#39640;&#20102;15%&#12290;
&lt;/p&gt;
&lt;p&gt;
A simple design recipe for deep Transformers is to compose identical building blocks. But standard transformer blocks are far from simple, interweaving attention and MLP sub-blocks with skip connections &amp; normalisation layers in precise arrangements. This complexity leads to brittle architectures, where seemingly minor changes can significantly reduce training speed, or render models untrainable.  In this work, we ask to what extent the standard transformer block can be simplified? Combining signal propagation theory and empirical observations, we motivate modifications that allow many block components to be removed with no loss of training speed, including skip connections, projection or value parameters, sequential sub-blocks and normalisation layers. In experiments on both autoregressive decoder-only and BERT encoder-only models, our simplified transformers emulate the per-update training speed and performance of standard transformers, while enjoying 15% faster training throughput, 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#32676;&#20869;&#29305;&#24449;&#32858;&#38598;&#21644;&#32676;&#22806;&#29305;&#24449;&#31163;&#25955;&#30340;&#24615;&#36136;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#21644;&#26435;&#37325;&#21521;&#37327;&#25509;&#36817;&#31243;&#24230;&#30340;&#31070;&#32463;&#22349;&#22604;&#65288;NC-OOD&#65289;&#26816;&#27979;&#22120;&#26469;&#25552;&#39640;OAD&#26816;&#27979;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.01479</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#22349;&#22604;&#30340;&#35270;&#35282;&#26816;&#27979;&#21040;&#32676;&#22806;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Detecting Out-of-Distribution Through the Lens of Neural Collapse. (arXiv:2311.01479v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01479
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#32676;&#20869;&#29305;&#24449;&#32858;&#38598;&#21644;&#32676;&#22806;&#29305;&#24449;&#31163;&#25955;&#30340;&#24615;&#36136;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#21644;&#26435;&#37325;&#21521;&#37327;&#25509;&#36817;&#31243;&#24230;&#30340;&#31070;&#32463;&#22349;&#22604;&#65288;NC-OOD&#65289;&#26816;&#27979;&#22120;&#26469;&#25552;&#39640;OAD&#26816;&#27979;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32676;&#22806;&#65288;OOD&#65289;&#26816;&#27979;&#23545;&#20110;&#23433;&#20840;&#37096;&#32626;&#20154;&#24037;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#21035;&#26159;&#65292;OOD&#26816;&#27979;&#22120;&#24212;&#35813;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#26377;&#25928;&#22320;&#27867;&#21270;&#12290;&#20026;&#20102;&#25913;&#36827;&#29616;&#26377;OOD&#26816;&#27979;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#24230;&#28789;&#27963;&#30340;OOD&#26816;&#27979;&#22120;&#65292;&#31216;&#20026;&#31070;&#32463;&#22349;&#22604;&#65288;NC-OOD&#65289;&#26816;&#27979;&#22120;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#26222;&#36941;&#35266;&#23519;&#21040;&#30340;&#32676;&#20869;&#65288;ID&#65289;&#29305;&#24449;&#20542;&#21521;&#20110;&#24418;&#25104;&#31751;&#65292;&#32780;&#32676;&#22806;&#29305;&#24449;&#21017;&#36828;&#31163;&#30340;&#35266;&#23519;&#12290;&#29305;&#21035;&#26159;&#22522;&#20110;&#26368;&#36817;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#31070;&#32463;&#22349;&#22604;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;ID&#29305;&#24449;&#20542;&#21521;&#20110;&#22312;&#25509;&#36817;&#26435;&#37325;&#21521;&#37327;&#30340;&#20301;&#32622;&#32858;&#38598;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#25193;&#23637;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#19982;&#26435;&#37325;&#21521;&#37327;&#30340;&#25509;&#36817;&#31243;&#24230;&#26469;&#26816;&#27979;OOD&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25490;&#38500;OOD&#26679;&#26412;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;OOD&#29305;&#24449;&#20542;&#21521;&#20110;&#27604;ID&#29305;&#24449;&#26356;&#25509;&#36817;&#21407;&#28857;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#29616;&#26377;&#24037;&#20316;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;OOD&#26816;&#27979;&#26041;&#38754;&#22987;&#32456;&#33021;&#22815;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is essential for the safe deployment of AI. Particularly, OOD detectors should generalize effectively across diverse scenarios. To improve upon the generalizability of existing OOD detectors, we introduce a highly versatile OOD detector, called Neural Collapse inspired OOD detector (NC-OOD). We extend the prevalent observation that in-distribution (ID) features tend to form clusters, whereas OOD features are far away. Particularly, based on the recent observation, Neural Collapse, we further demonstrate that ID features tend to cluster in proximity to weight vectors. From our extended observation, we propose to detect OOD based on feature proximity to weight vectors. To further rule out OOD samples, we leverage the observation that OOD features tend to reside closer to the origin than ID features. Extensive experiments show that our approach enhances the generalizability of existing work and can consistently achieve state-of-the-art OOD detection per
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Deep-Align&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#35299;&#20915;&#26435;&#37325;&#23545;&#40784;&#38382;&#39064;&#65292;&#20197;&#21152;&#36895;&#23545;&#40784;&#36807;&#31243;&#24182;&#25552;&#39640;&#20854;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.13397</link><description>&lt;p&gt;
&#31561;&#21464;&#28145;&#24230;&#26435;&#37325;&#31354;&#38388;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Equivariant Deep Weight Space Alignment. (arXiv:2310.13397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Deep-Align&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#35299;&#20915;&#26435;&#37325;&#23545;&#40784;&#38382;&#39064;&#65292;&#20197;&#21152;&#36895;&#23545;&#40784;&#36807;&#31243;&#24182;&#25552;&#39640;&#20854;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#30340;&#25490;&#21015;&#23545;&#31216;&#24615;&#20351;&#24471;&#31616;&#21333;&#25805;&#20316;&#22914;&#27169;&#22411;&#24179;&#22343;&#21644;&#30456;&#20284;&#24230;&#20272;&#35745;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#23545;&#40784;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#21363;&#25214;&#21040;&#23427;&#20204;&#20043;&#38388;&#26368;&#20248;&#25490;&#21015;&#65292;&#26159;&#24517;&#35201;&#30340;&#12290;&#26356;&#19968;&#33324;&#22320;&#35828;&#65292;&#26435;&#37325;&#23545;&#40784;&#23545;&#20110;&#24191;&#27867;&#30340;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#20174;&#27169;&#22411;&#21512;&#24182;&#65292;&#36890;&#36807;&#25506;&#32034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#31354;&#38388;&#65292;&#21040;&#23450;&#20041;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#26377;&#24847;&#20041;&#30340;&#36317;&#31163;&#20989;&#25968;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26435;&#37325;&#23545;&#40784;&#26159;&#19968;&#20010;NP-hard&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35299;&#20915;&#23545;&#40784;&#38382;&#39064;&#30340;&#26494;&#24347;&#29256;&#26412;&#65292;&#23548;&#33268;&#26041;&#27861;&#32791;&#26102;&#25110;&#32773;&#27425;&#20248;&#35299;&#12290;&#20026;&#20102;&#21152;&#36895;&#23545;&#40784;&#36807;&#31243;&#24182;&#25552;&#39640;&#20854;&#36136;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Deep-Align&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#23398;&#20064;&#35299;&#20915;&#26435;&#37325;&#23545;&#40784;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#26435;&#37325;&#23545;&#40784;&#36981;&#24490;&#20004;&#20010;&#22522;&#26412;&#23545;&#31216;&#24615;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Permutation symmetries of deep networks make simple operations like model averaging and similarity estimation challenging. In many cases, aligning the weights of the networks, i.e., finding optimal permutations between their weights, is necessary. More generally, weight alignment is essential for a wide range of applications, from model merging, through exploring the optimization landscape of deep neural networks, to defining meaningful distance functions between neural networks. Unfortunately, weight alignment is an NP-hard problem. Prior research has mainly focused on solving relaxed versions of the alignment problem, leading to either time-consuming methods or sub-optimal solutions. To accelerate the alignment process and improve its quality, we propose a novel framework aimed at learning to solve the weight alignment problem, which we name Deep-Align. To that end, we first demonstrate that weight alignment adheres to two fundamental symmetries and then, propose a deep architecture 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#25506;&#27979;&#22120;&#21644;&#36716;&#25442;&#22120;&#20248;&#21270;&#25351;&#20196;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02905</link><description>&lt;p&gt;
&#20351;&#29992;&#24744;&#30340;&#26412;&#33021;&#65306;&#20351;&#29992;&#31070;&#32463;&#25506;&#27979;&#22120;&#19982;&#36716;&#25442;&#22120;&#36827;&#34892;&#25351;&#20196;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers. (arXiv:2310.02905v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02905
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#25506;&#27979;&#22120;&#21644;&#36716;&#25442;&#22120;&#20248;&#21270;&#25351;&#20196;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#32473;&#20104;&#23427;&#20204;&#30340;&#25351;&#20196;&#65292;&#36825;&#20123;&#25351;&#20196;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#36827;&#34892;&#25163;&#21160;&#35843;&#25972;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#31639;&#27861;&#26469;&#33258;&#21160;&#20248;&#21270;&#32473;&#20104;&#40657;&#30418;LLMs&#30340;&#25351;&#20196;&#12290;&#28982;&#32780;&#65292;&#22312;&#20248;&#21270;&#39640;&#24230;&#22797;&#26434;&#65288;&#20363;&#22914;&#39640;&#32500;&#65289;&#30340;&#30446;&#26631;&#20989;&#25968;&#26102;&#65292;&#22914;&#23558;&#25351;&#20196;&#26144;&#23556;&#21040;LLM&#24615;&#33021;&#30340;&#20989;&#25968;&#65292;BO&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;BO&#20351;&#29992;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#65292;&#35813;&#27169;&#22411;&#34987;&#29992;&#20316;BO&#30340;&#20195;&#29702;&#26469;&#24314;&#27169;&#30446;&#26631;&#20989;&#25968;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24050;&#32463;&#22810;&#27425;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#65292;&#23588;&#20854;&#26159;&#39044;&#35757;&#32451;&#30340;&#36716;&#25442;&#22120;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21487;&#20197;&#24314;&#27169;&#39640;&#24230;&#22797;&#26434;&#30340;&#20989;&#25968;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#31070;&#32463;&#25506;&#27979;&#22120;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown remarkable instruction-following capabilities and achieved impressive performances in various applications. However, the performances of LLMs depend heavily on the instructions given to them, which are typically manually tuned with substantial human efforts. Recent work has used the query-efficient Bayesian optimization (BO) algorithm to automatically optimize the instructions given to black-box LLMs. However, BO usually falls short when optimizing highly sophisticated (e.g., high-dimensional) objective functions, such as the functions mapping an instruction to the performance of an LLM. This is mainly due to the limited expressive power of the Gaussian process (GP) model which is used by BO as a surrogate to model the objective function. Meanwhile, it has been repeatedly shown that neural networks (NNs), especially pre-trained transformers, possess strong expressive power and can model highly complex functions. So, we adopt a neural bandit algor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#25968;&#25454;&#28165;&#27927;&#19982;&#26426;&#22120;&#23398;&#20064;&#30340;&#20851;&#31995;&#65292;&#24182;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#25968;&#25454;&#28165;&#27927;&#26041;&#27861;&#21644;ML&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;&#22312;&#36825;&#20004;&#20010;&#39046;&#22495;&#23384;&#22312;&#30528;&#20114;&#30456;&#20419;&#36827;&#30340;&#20851;&#31995;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2310.01765</link><description>&lt;p&gt;
&#25968;&#25454;&#28165;&#27927;&#19982;&#26426;&#22120;&#23398;&#20064;&#65306;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Data Cleaning and Machine Learning: A Systematic Literature Review. (arXiv:2310.01765v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#25968;&#25454;&#28165;&#27927;&#19982;&#26426;&#22120;&#23398;&#20064;&#30340;&#20851;&#31995;&#65292;&#24182;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#25968;&#25454;&#28165;&#27927;&#26041;&#27861;&#21644;ML&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;&#22312;&#36825;&#20004;&#20010;&#39046;&#22495;&#23384;&#22312;&#30528;&#20114;&#30456;&#20419;&#36827;&#30340;&#20851;&#31995;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#34987;&#25972;&#21512;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#31995;&#32479;&#20013;&#65292;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#12290;&#30001;&#20110;ML&#27169;&#22411;&#30340;&#24615;&#33021;&#39640;&#24230;&#20381;&#36182;&#20110;&#20854;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#22240;&#27492;&#23545;&#20110;&#26816;&#27979;&#21644;&#20462;&#22797;&#25968;&#25454;&#38169;&#35823;&#65288;&#21363;&#25968;&#25454;&#28165;&#27927;&#65289;&#30340;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#30740;&#31350;&#20154;&#21592;&#36824;&#22312;&#25506;&#32034;&#22914;&#20309;&#20351;&#29992;ML&#36827;&#34892;&#25968;&#25454;&#28165;&#27927;&#65292;&#20174;&#32780;&#22312;ML&#21644;&#25968;&#25454;&#28165;&#27927;&#20043;&#38388;&#26500;&#24314;&#20102;&#21452;&#37325;&#20851;&#31995;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23578;&#26080;&#23545;&#36825;&#31181;&#20851;&#31995;&#36827;&#34892;&#20840;&#38754;&#32508;&#36848;&#30340;&#30740;&#31350;&#12290;&#30446;&#26631;&#65306;&#26412;&#25991;&#30340;&#30446;&#26631;&#26377;&#20004;&#20010;&#12290;&#39318;&#20808;&#65292;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#25968;&#25454;&#28165;&#27927;&#26041;&#27861;&#65292;&#21253;&#25324;ML&#29992;&#20110;&#25968;&#25454;&#28165;&#27927;&#21644;&#25968;&#25454;&#28165;&#27927;&#29992;&#20110;ML&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#24037;&#20316;&#24314;&#35758;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#23545;2016&#24180;&#33267;2022&#24180;&#26399;&#38388;&#21457;&#34920;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#28165;&#27927;&#27963;&#21160;&#65292;&#21253;&#25324;&#29305;&#24449;&#28165;&#27927;&#12289;&#26631;&#31614;&#28165;&#27927;&#12289;&#23454;&#20307;&#21305;&#37197;&#12289;&#24322;&#24120;&#20540;&#26816;&#27979;&#12289;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context: Machine Learning (ML) is integrated into a growing number of systems for various applications. Because the performance of an ML model is highly dependent on the quality of the data it has been trained on, there is a growing interest in approaches to detect and repair data errors (i.e., data cleaning). Researchers are also exploring how ML can be used for data cleaning; hence creating a dual relationship between ML and data cleaning. To the best of our knowledge, there is no study that comprehensively reviews this relationship. Objective: This paper's objectives are twofold. First, it aims to summarize the latest approaches for data cleaning for ML and ML for data cleaning. Second, it provides future work recommendations. Method: We conduct a systematic literature review of the papers published between 2016 and 2022 inclusively. We identify different types of data cleaning activities with and for ML: feature cleaning, label cleaning, entity matching, outlier detection, imputati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21407;&#22987;-&#23545;&#20598;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#35299;&#20915;&#21463;&#38480;&#23398;&#20064;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#20998;&#26512;&#20219;&#21153;&#23618;&#38754;&#21644;&#26679;&#26412;&#23618;&#38754;&#30340;&#32422;&#26463;&#65292;&#22312;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#20013;&#20998;&#37197;&#36164;&#28304;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00154</link><description>&lt;p&gt;
&#21407;&#22987;-&#23545;&#20598;&#25345;&#32493;&#23398;&#20064;&#65306;&#36890;&#36807;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#23454;&#29616;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;
&lt;/p&gt;
&lt;p&gt;
Primal-Dual Continual Learning: Stability and Plasticity through Lagrange Multipliers. (arXiv:2310.00154v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21407;&#22987;-&#23545;&#20598;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#35299;&#20915;&#21463;&#38480;&#23398;&#20064;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#20998;&#26512;&#20219;&#21153;&#23618;&#38754;&#21644;&#26679;&#26412;&#23618;&#38754;&#30340;&#32422;&#26463;&#65292;&#22312;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#20013;&#20998;&#37197;&#36164;&#28304;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#22266;&#26377;&#22320;&#26159;&#19968;&#20010;&#21463;&#38480;&#23398;&#20064;&#38382;&#39064;&#12290;&#30446;&#26631;&#26159;&#22312;&#8220;&#26080;&#36951;&#24536;&#8221;&#35201;&#27714;&#19979;&#23398;&#20064;&#19968;&#20010;&#39044;&#27979;&#22120;&#12290;&#23613;&#31649;&#20043;&#21069;&#26377;&#20960;&#39033;&#30740;&#31350;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#36825;&#26679;&#19968;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#27809;&#26377;&#26126;&#30830;&#35299;&#20915;&#36825;&#20010;&#21463;&#38480;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30452;&#25509;&#35299;&#20915;&#36825;&#20010;&#21463;&#38480;&#20248;&#21270;&#38382;&#39064;&#26159;&#21487;&#34892;&#19988;&#26377;&#30410;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#38480;&#21046;&#24615;&#23398;&#20064;&#20013;&#30340;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#21487;&#20197;&#23558;&#20808;&#21069;&#20219;&#21153;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#26679;&#26412;&#23384;&#20648;&#22312;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#30340;&#20004;&#20010;&#29256;&#26412;&#65306;&#19968;&#20010;&#22312;&#20219;&#21153;&#23618;&#38754;&#19978;&#26377;&#32422;&#26463;&#30340;&#31895;&#31961;&#26041;&#27861;&#21644;&#19968;&#20010;&#22312;&#26679;&#26412;&#23618;&#38754;&#19978;&#26377;&#32422;&#26463;&#30340;&#31934;&#32454;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20598;&#21464;&#37327;&#25351;&#31034;&#20102;&#26368;&#20248;&#20540;&#23545;&#20110;&#32422;&#26463;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#32467;&#26524;&#22312;&#31895;&#31961;&#26041;&#27861;&#20013;&#23545;&#32531;&#20914;&#21306;&#36827;&#34892;&#20102;&#21010;&#20998;&#65292;&#23558;&#26356;&#22810;&#36164;&#28304;&#20998;&#37197;&#32473;&#26356;&#38590;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning is inherently a constrained learning problem. The goal is to learn a predictor under a \emph{no-forgetting} requirement. Although several prior studies formulate it as such, they do not solve the constrained problem explicitly. In this work, we show that it is both possible and beneficial to undertake the constrained optimization problem directly. To do this, we leverage recent results in constrained learning through Lagrangian duality. We focus on memory-based methods, where a small subset of samples from previous tasks can be stored in a replay buffer. In this setting, we analyze two versions of the continual learning problem: a coarse approach with constraints at the task level and a fine approach with constraints at the sample level. We show that dual variables indicate the sensitivity of the optimal value with respect to constraint perturbations. We then leverage this result to partition the buffer in the coarse approach, allocating more resources to harder task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39640;&#32500;&#24230;&#21644;&#37325;&#23614;&#24178;&#25200;&#26465;&#20214;&#19979;&#30340;&#40065;&#26834;&#22238;&#24402;&#20272;&#35745;&#22120;&#30340;&#24615;&#36136;&#65292;&#36890;&#36807;&#30740;&#31350;&#19968;&#31867;&#26925;&#22278;&#21327;&#21464;&#37327;&#21644;&#22122;&#22768;&#25968;&#25454;&#20998;&#24067;&#30340;M-&#20272;&#35745;&#22120;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#23384;&#22312;&#37325;&#23614;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;Huber&#25439;&#22833;&#38656;&#35201;&#36827;&#19968;&#27493;&#27491;&#21017;&#21270;&#25165;&#33021;&#36798;&#21040;&#26368;&#20248;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#20102;&#23725;&#22238;&#24402;&#30340;&#36229;&#39069;&#39118;&#38505;&#30340;&#34928;&#20943;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.16476</link><description>&lt;p&gt;
&#39640;&#32500;&#24230;&#19979;&#37325;&#23614;&#25968;&#25454;&#19979;&#30340;&#40065;&#26834;&#22238;&#24402;: &#28176;&#36817;&#24615;&#21644;&#26222;&#36866;&#24615;
&lt;/p&gt;
&lt;p&gt;
High-dimensional robust regression under heavy-tailed data: Asymptotics and Universality. (arXiv:2309.16476v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39640;&#32500;&#24230;&#21644;&#37325;&#23614;&#24178;&#25200;&#26465;&#20214;&#19979;&#30340;&#40065;&#26834;&#22238;&#24402;&#20272;&#35745;&#22120;&#30340;&#24615;&#36136;&#65292;&#36890;&#36807;&#30740;&#31350;&#19968;&#31867;&#26925;&#22278;&#21327;&#21464;&#37327;&#21644;&#22122;&#22768;&#25968;&#25454;&#20998;&#24067;&#30340;M-&#20272;&#35745;&#22120;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#23384;&#22312;&#37325;&#23614;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;Huber&#25439;&#22833;&#38656;&#35201;&#36827;&#19968;&#27493;&#27491;&#21017;&#21270;&#25165;&#33021;&#36798;&#21040;&#26368;&#20248;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#20102;&#23725;&#22238;&#24402;&#30340;&#36229;&#39069;&#39118;&#38505;&#30340;&#34928;&#20943;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21327;&#21464;&#37327;&#21644;&#21709;&#24212;&#20989;&#25968;&#37117;&#21463;&#37325;&#23614;&#24178;&#25200;&#30340;&#24773;&#20917;&#19979;&#65292;&#40065;&#26834;&#22238;&#24402;&#20272;&#35745;&#37327;&#30340;&#39640;&#32500;&#24615;&#36136;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#38024;&#23545;&#19968;&#31867;&#21253;&#21547;&#26925;&#22278;&#21327;&#21464;&#37327;&#21644;&#22122;&#22768;&#25968;&#25454;&#20998;&#24067;&#30340;M-&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#38160;&#21033;&#30340;&#28176;&#36817;&#29305;&#24449;&#21270;&#65292;&#21253;&#25324;&#20108;&#38454;&#21450;&#20197;&#19978;&#30697;&#19981;&#23384;&#22312;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#23384;&#22312;&#37325;&#23614;&#22122;&#22768;&#30340;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#23613;&#31649;Huber&#25439;&#22833;&#36890;&#36807;&#26368;&#20248;&#35843;&#25972;&#30340;&#20301;&#32622;&#21442;&#25968;$\delta$&#26159;&#19968;&#33268;&#30340;&#65292;&#20294;&#20854;&#22312;&#24615;&#33021;&#19978;&#26159;&#27425;&#20248;&#30340;&#65292;&#31361;&#26174;&#20102;&#36827;&#19968;&#27493;&#27491;&#21017;&#21270;&#20197;&#36798;&#21040;&#26368;&#20248;&#24615;&#33021;&#30340;&#24517;&#35201;&#24615;&#12290;&#36825;&#20010;&#32467;&#26524;&#36824;&#25581;&#31034;&#20102;$\delta$&#20316;&#20026;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#27745;&#26579;&#30340;&#20989;&#25968;&#23384;&#22312;&#30340;&#19968;&#20010;&#26377;&#36259;&#30340;&#36716;&#21464;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#23725;&#22238;&#24402;&#20013;&#36229;&#39069;&#39118;&#38505;&#30340;&#34928;&#20943;&#36895;&#29575;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#20855;&#26377;&#26377;&#38480;&#20108;&#38454;&#30697;&#30340;&#22122;&#22768;&#20998;&#24067;&#65292;&#23725;&#22238;&#24402;&#19981;&#20165;&#26159;&#26368;&#20248;&#30340;&#65292;&#32780;&#19988;&#26159;&#26222;&#36866;&#30340;&#65292;&#20294;&#20854;&#34928;&#20943;&#36895;&#29575;&#21487;&#20197;&#26159;...
&lt;/p&gt;
&lt;p&gt;
We investigate the high-dimensional properties of robust regression estimators in the presence of heavy-tailed contamination of both the covariates and response functions. In particular, we provide a sharp asymptotic characterisation of M-estimators trained on a family of elliptical covariate and noise data distributions including cases where second and higher moments do not exist. We show that, despite being consistent, the Huber loss with optimally tuned location parameter $\delta$ is suboptimal in the high-dimensional regime in the presence of heavy-tailed noise, highlighting the necessity of further regularisation to achieve optimal performance. This result also uncovers the existence of a curious transition in $\delta$ as a function of the sample complexity and contamination. Moreover, we derive the decay rates for the excess risk of ridge regression. We show that, while it is both optimal and universal for noise distributions with finite second moment, its decay rate can be consi
&lt;/p&gt;</description></item><item><title>BayOTIDE&#26159;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22312;&#32447;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#19982;&#20989;&#25968;&#20998;&#35299;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#20302;&#31209;&#26102;&#24207;&#22240;&#23376;&#32452;&#30340;&#21152;&#26435;&#32452;&#21512;&#26469;&#36827;&#34892;&#25554;&#34917;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#20840;&#23616;&#36235;&#21183;&#21644;&#21608;&#26399;&#24615;&#27169;&#24335;&#30340;&#24573;&#30053;&#20197;&#21450;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#22788;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.14906</link><description>&lt;p&gt;
BayOTIDE: &#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22312;&#32447;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#19982;&#20989;&#25968;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
BayOTIDE: Bayesian Online Multivariate Time series Imputation with functional decomposition. (arXiv:2308.14906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14906
&lt;/p&gt;
&lt;p&gt;
BayOTIDE&#26159;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22312;&#32447;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#19982;&#20989;&#25968;&#20998;&#35299;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#20302;&#31209;&#26102;&#24207;&#22240;&#23376;&#32452;&#30340;&#21152;&#26435;&#32452;&#21512;&#26469;&#36827;&#34892;&#25554;&#34917;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#20840;&#23616;&#36235;&#21183;&#21644;&#21608;&#26399;&#24615;&#27169;&#24335;&#30340;&#24573;&#30053;&#20197;&#21450;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#22788;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#22914;&#20132;&#36890;&#21644;&#33021;&#28304;&#65292;&#32463;&#24120;&#35266;&#23519;&#21040;&#20855;&#26377;&#32570;&#22833;&#20540;&#21644;&#22122;&#22768;&#30340;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#29978;&#33267;&#26159;&#19981;&#35268;&#21017;&#37319;&#26679;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25554;&#34917;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#25968;&#21482;&#36866;&#29992;&#20110;&#23616;&#37096;&#35270;&#35282;&#65292;&#21363;&#23558;&#38271;&#24207;&#21015;&#25286;&#20998;&#20026;&#36866;&#24403;&#22823;&#23567;&#30340;&#25209;&#27425;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#23616;&#37096;&#35270;&#35282;&#21487;&#33021;&#20351;&#27169;&#22411;&#24573;&#30053;&#20840;&#23616;&#36235;&#21183;&#25110;&#21608;&#26399;&#24615;&#27169;&#24335;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#20960;&#20046;&#25152;&#26377;&#26041;&#27861;&#37117;&#20551;&#35774;&#35266;&#27979;&#20540;&#22312;&#35268;&#21017;&#30340;&#26102;&#38388;&#38388;&#38548;&#36827;&#34892;&#37319;&#26679;&#65292;&#24182;&#19988;&#26080;&#27861;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#24212;&#29992;&#30340;&#22797;&#26434;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#22312;&#31163;&#32447;&#29366;&#24577;&#19979;&#36827;&#34892;&#23398;&#20064;&#30340;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#37027;&#20123;&#26377;&#24555;&#36895;&#21040;&#36798;&#30340;&#27969;&#25968;&#25454;&#30340;&#24212;&#29992;&#26469;&#35828;&#65292;&#23427;&#20204;&#24182;&#19981;&#21512;&#36866;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BayOTIDE&#65306;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22312;&#32447;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#19982;&#20989;&#25968;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world scenarios like traffic and energy, massive time-series data with missing values and noises are widely observed, even sampled irregularly. While many imputation methods have been proposed, most of them work with a local horizon, which means models are trained by splitting the long sequence into batches of fit-sized patches. This local horizon can make models ignore global trends or periodic patterns. More importantly, almost all methods assume the observations are sampled at regular time stamps, and fail to handle complex irregular sampled time series arising from different applications. Thirdly, most existing methods are learned in an offline manner. Thus, it is not suitable for many applications with fast-arriving streaming data. To overcome these limitations, we propose \ours: Bayesian Online Multivariate Time series Imputation with functional decomposition. We treat the multivariate time series as the weighted combination of groups of low-rank temporal factors with dif
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#23398;&#20064;&#23545;&#19990;&#30028;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#24110;&#21161;&#20195;&#29702;&#22120;&#39044;&#27979;&#26410;&#26469;&#24182;&#36827;&#34892;&#34892;&#21160;&#12290;&#36890;&#36807;&#23398;&#20064;&#22810;&#27169;&#24577;&#19990;&#30028;&#27169;&#22411;&#65292;&#20195;&#29702;&#22120;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#22312;&#27169;&#22411;&#22238;&#28378;&#20013;&#36827;&#34892;&#34892;&#21160;&#12290;</title><link>http://arxiv.org/abs/2308.01399</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#23398;&#20064;&#23545;&#19990;&#30028;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Learning to Model the World with Language. (arXiv:2308.01399v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#23398;&#20064;&#23545;&#19990;&#30028;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#24110;&#21161;&#20195;&#29702;&#22120;&#39044;&#27979;&#26410;&#26469;&#24182;&#36827;&#34892;&#34892;&#21160;&#12290;&#36890;&#36807;&#23398;&#20064;&#22810;&#27169;&#24577;&#19990;&#30028;&#27169;&#22411;&#65292;&#20195;&#29702;&#22120;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#22312;&#27169;&#22411;&#22238;&#28378;&#20013;&#36827;&#34892;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#19982;&#20154;&#31867;&#22312;&#19990;&#30028;&#20013;&#30456;&#20114;&#20316;&#29992;&#65292;&#20195;&#29702;&#22120;&#38656;&#35201;&#29702;&#35299;&#20154;&#20204;&#20351;&#29992;&#30340;&#22810;&#26679;&#21270;&#30340;&#35821;&#35328;&#31867;&#22411;&#65292;&#24182;&#23558;&#20854;&#19982;&#35270;&#35273;&#19990;&#30028;&#20851;&#32852;&#36215;&#26469;&#65292;&#24182;&#22522;&#20110;&#35821;&#35328;&#34892;&#21160;&#12290;&#34429;&#28982;&#24403;&#21069;&#30340;&#20195;&#29702;&#22120;&#21487;&#20197;&#36890;&#36807;&#20219;&#21153;&#22870;&#21169;&#23398;&#20064;&#25191;&#34892;&#31616;&#21333;&#30340;&#35821;&#35328;&#25351;&#20196;&#65292;&#20294;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24314;&#31435;&#21487;&#20197;&#21033;&#29992;&#20256;&#36798;&#19968;&#33324;&#30693;&#35782;&#12289;&#25551;&#36848;&#19990;&#30028;&#29366;&#24577;&#12289;&#25552;&#20379;&#20114;&#21160;&#21453;&#39304;&#31561;&#22810;&#26679;&#21270;&#35821;&#35328;&#30340;&#20195;&#29702;&#22120;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#35821;&#35328;&#24110;&#21161;&#20195;&#29702;&#22120;&#39044;&#27979;&#26410;&#26469;&#65306;&#23558;&#20250;&#34987;&#35266;&#23519;&#21040;&#20160;&#20040;&#12289;&#19990;&#30028;&#23558;&#22914;&#20309;&#36816;&#34892;&#20197;&#21450;&#21738;&#20123;&#24773;&#20917;&#23558;&#33719;&#24471;&#22870;&#21169;&#12290;&#36825;&#20010;&#35266;&#28857;&#23558;&#35821;&#35328;&#29702;&#35299;&#19982;&#26410;&#26469;&#39044;&#27979;&#32479;&#19968;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Dynalang&#65292;&#19968;&#31181;&#23398;&#20064;&#22810;&#27169;&#24577;&#19990;&#30028;&#27169;&#22411;&#30340;&#20195;&#29702;&#22120;&#65292;&#23427;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#22312;&#24819;&#20687;&#30340;&#27169;&#22411;&#22238;&#28378;&#20013;&#23398;&#20064;&#34892;&#21160;&#12290;&#19982;&#21482;&#20351;&#29992;&#35821;&#35328;&#39044;&#27979;&#21160;&#20316;&#30340;&#20256;&#32479;&#20195;&#29702;&#22120;&#19981;&#21516;&#65292;Dynalang&#36890;&#36807;&#36807;&#21435;&#30340;&#35821;&#35328;&#36824;&#21487;&#20197;&#33719;&#21462;&#20016;&#23500;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
To interact with humans in the world, agents need to understand the diverse types of language that people use, relate them to the visual world, and act based on them. While current agents learn to execute simple language instructions from task rewards, we aim to build agents that leverage diverse language that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that language helps agents predict the future: what will be observed, how the world will behave, and which situations will be rewarded. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We present Dynalang, an agent that learns a multimodal world model that predicts future text and image representations and learns to act from imagined model rollouts. Unlike traditional agents that use language only to predict actions, Dynalang acquires rich language understanding by using past language also to 
&lt;/p&gt;</description></item><item><title>AMEE&#26159;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#35299;&#37322;&#35780;&#20215;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#21644;&#27604;&#36739;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#22810;&#31181;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#20449;&#24687;&#20215;&#20540;&#65292;&#24110;&#21161;&#35299;&#20915;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#35299;&#37322;&#26041;&#27861;&#36873;&#25321;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.05501</link><description>&lt;p&gt;
AMEE&#65306;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#35299;&#37322;&#35780;&#20215;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AMEE: A Robust Framework for Explanation Evaluation in Time Series Classification. (arXiv:2306.05501v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05501
&lt;/p&gt;
&lt;p&gt;
AMEE&#26159;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#35299;&#37322;&#35780;&#20215;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#21644;&#27604;&#36739;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#22810;&#31181;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#20449;&#24687;&#20215;&#20540;&#65292;&#24110;&#21161;&#35299;&#20915;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#35299;&#37322;&#26041;&#27861;&#36873;&#25321;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#23450;&#37327;&#35780;&#20272;&#21644;&#25490;&#21517;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#35813;&#20219;&#21153;&#28041;&#21450;&#21040;&#21355;&#29983;&#20445;&#20581;&#21644;&#37329;&#34701;&#31561;&#20851;&#38190;&#39046;&#22495;&#30340;&#26222;&#36941;&#25968;&#25454;&#31867;&#22411;&#12290;&#26368;&#36817;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35299;&#37322;&#26041;&#27861;&#30340;&#30740;&#31350;&#20852;&#36259;&#28608;&#22686;&#65292;&#25552;&#20379;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#35299;&#37322;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20123;&#35299;&#37322;&#25216;&#26415;&#22312;&#29305;&#23450;&#38382;&#39064;&#19978;&#20135;&#29983;&#20998;&#27495;&#26102;&#65292;&#20173;&#28982;&#19981;&#28165;&#26970;&#20351;&#29992;&#21738;&#31181;&#25216;&#26415;&#12290;&#27604;&#36739;&#35299;&#37322;&#20197;&#25214;&#21040;&#27491;&#30830;&#31572;&#26696;&#24182;&#19981;&#23481;&#26131;&#12290;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65306;&#22914;&#20309;&#23450;&#37327;&#21644;&#31283;&#20581;&#22320;&#35780;&#20272;&#32473;&#23450;&#35299;&#37322;&#26041;&#27861;&#30340;&#20449;&#24687;&#20215;&#20540;&#65288;&#21363;&#19982;&#20998;&#31867;&#20219;&#21153;&#30456;&#20851;&#24615;&#65289;&#65292;&#20197;&#21450;&#22914;&#20309;&#24182;&#25490;&#27604;&#36739;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AMEE&#65292;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#35299;&#37322;&#35780;&#20215;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#21644;&#27604;&#36739;&#22810;&#31181;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#22312;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#20013;&#22686;&#21152;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
This paper aims to provide a framework to quantitatively evaluate and rank explanation methods for the time series classification task, which deals with a prevalent data type in critical domains such as healthcare and finance. The recent surge of research interest in explanation methods for time series classification has provided a great variety of explanation techniques. Nevertheless, when these explanation techniques disagree on a specific problem, it remains unclear which of them to use. Comparing the explanations to find the right answer is non-trivial. Two key challenges remain: how to quantitatively and robustly evaluate the informativeness (i.e., relevance for the classification task) of a given explanation method, and how to compare explanation methods side-by-side. We propose AMEE, a Model-Agnostic Explanation Evaluation framework for quantifying and comparing multiple saliency-based explanations for time series classification. Perturbation is added to the input time series gu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#32593;&#32476;&#30340;&#38543;&#26426;&#22810;&#23618;&#32452;&#21512;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#31181;&#26032;&#30340;&#20998;&#25955;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#23454;&#29616;&#29420;&#31435;&#20110;&#23618;&#25968;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#29702;&#35770;&#32467;&#26524;&#21644;&#23454;&#39564;&#35777;&#26126;&#23427;&#20204;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.03322</link><description>&lt;p&gt;
&#38754;&#21521;&#32593;&#32476;&#30340;&#38543;&#26426;&#22810;&#23618;&#32452;&#21512;&#20248;&#21270;&#31639;&#27861;&#65292;&#20855;&#26377;&#29420;&#31435;&#20110;&#23618;&#25968;&#30340;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Stochastic Multi-Level Compositional Optimization Algorithms over Networks with Level-Independent Convergence Rate. (arXiv:2306.03322v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#32593;&#32476;&#30340;&#38543;&#26426;&#22810;&#23618;&#32452;&#21512;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#31181;&#26032;&#30340;&#20998;&#25955;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#23454;&#29616;&#29420;&#31435;&#20110;&#23618;&#25968;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#29702;&#35770;&#32467;&#26524;&#21644;&#23454;&#39564;&#35777;&#26126;&#23427;&#20204;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#22810;&#23618;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#28085;&#30422;&#20102;&#24456;&#22810;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#65292;&#22914;&#22810;&#27493;&#39588;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65292;&#38656;&#35201;&#22823;&#35268;&#27169;&#24212;&#29992;&#30340;&#39640;&#25928;&#20248;&#21270;&#31639;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#38543;&#26426;&#22810;&#23618;&#20248;&#21270;&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#22810;&#23618;&#32467;&#26500;&#21644;&#20998;&#25955;&#24335;&#36890;&#35759;&#26041;&#26696;&#21487;&#33021;&#22686;&#21152;&#23618;&#25968;&#23545;&#25910;&#25947;&#36895;&#24230;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#25955;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;&#26469;&#22788;&#29702;&#22810;&#23618;&#20989;&#25968;&#21644;&#20854;&#26799;&#24230;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#21333;&#26426;&#31639;&#27861;&#30456;&#27604;&#65292;&#36825;&#20004;&#31181;&#31639;&#27861;&#22343;&#33021;&#22312;&#38750;&#20984;&#38382;&#39064;&#20013;&#23454;&#29616;&#29420;&#31435;&#20110;&#23618;&#25968;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#26465;&#20214;&#26356;&#21152;&#23485;&#26494;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#22312;&#20998;&#25955;&#24335;&#35774;&#32622;&#19979;&#23454;&#29616;&#29420;&#31435;&#20110;&#23618;&#25968;&#30340;&#25910;&#25947;&#36895;&#24230;&#30340;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic multi-level compositional optimization problems cover many new machine learning paradigms, e.g., multi-step model-agnostic meta-learning, which require efficient optimization algorithms for large-scale applications. This paper studies the decentralized stochastic multi-level optimization algorithm, which is challenging because the multi-level structure and decentralized communication scheme may make the number of levels affect the order of the convergence rate. To this end, we develop two novel decentralized optimization algorithms to deal with the multi-level function and its gradient. Our theoretical results show that both algorithms can achieve the level-independent convergence rate for nonconvex problems under much milder conditions compared with existing single-machine algorithms. To the best of our knowledge, this is the first work that achieves the level-independent convergence rate under the decentralized setting. Moreover, extensive experiments confirm the efficacy 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25490;&#21015;&#20915;&#31574;&#26641;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#25968;&#25454;&#23454;&#20363;&#30340;&#39034;&#24207;&#20381;&#36182;&#24615;&#65292;&#22312;&#19981;&#21516;&#25490;&#21015;&#30340;&#25968;&#25454;&#23454;&#20363;&#19978;&#24471;&#21040;&#19981;&#21516;&#30340;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#20256;&#32479;&#20915;&#31574;&#26641;&#27169;&#22411;&#22312;&#22788;&#29702;&#39034;&#24207;&#30456;&#20851;&#25968;&#25454;&#26102;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.02617</link><description>&lt;p&gt;
&#25490;&#21015;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
Permutation Decision Trees. (arXiv:2306.02617v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02617
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25490;&#21015;&#20915;&#31574;&#26641;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#25968;&#25454;&#23454;&#20363;&#30340;&#39034;&#24207;&#20381;&#36182;&#24615;&#65292;&#22312;&#19981;&#21516;&#25490;&#21015;&#30340;&#25968;&#25454;&#23454;&#20363;&#19978;&#24471;&#21040;&#19981;&#21516;&#30340;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#20256;&#32479;&#20915;&#31574;&#26641;&#27169;&#22411;&#22312;&#22788;&#29702;&#39034;&#24207;&#30456;&#20851;&#25968;&#25454;&#26102;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#26159;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#21270;&#20869;&#37096;&#33410;&#28857;&#20013;&#30340;&#19981;&#32431;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#26368;&#24120;&#35265;&#30340;&#19981;&#32431;&#24230;&#24230;&#37327;&#26159;&#39321;&#20892;&#29109;&#21644;&#22522;&#23612;&#19981;&#32431;&#24230;&#12290;&#36825;&#20123;&#19981;&#32431;&#24230;&#24230;&#37327;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#39034;&#24207;&#19981;&#25935;&#24863;&#65292;&#22240;&#27492;&#24471;&#21040;&#30340;&#26368;&#32456;&#26641;&#23545;&#25968;&#25454;&#30340;&#20219;&#20309;&#25490;&#21015;&#37117;&#26159;&#19981;&#21464;&#30340;&#12290;&#36825;&#23548;&#33268;&#20102;&#22312;&#24314;&#27169;&#23384;&#22312;&#39034;&#24207;&#20381;&#36182;&#24615;&#30340;&#25968;&#25454;&#23454;&#20363;&#26102;&#30340;&#20005;&#37325;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20351;&#29992;&#8220;&#21387;&#32553;&#21162;&#21147;&#8221;(ETC) - &#19968;&#31181;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#20316;&#20026;&#19981;&#32431;&#24230;&#24230;&#37327;&#12290;&#19982;&#39321;&#20892;&#29109;&#21644;&#22522;&#23612;&#19981;&#32431;&#24230;&#19981;&#21516;&#65292;&#22522;&#20110;ETC&#30340;&#32467;&#26500;&#24615;&#19981;&#32431;&#24230;&#33021;&#22815;&#25429;&#25417;&#21040;&#25968;&#25454;&#30340;&#39034;&#24207;&#20381;&#36182;&#24615;&#65292;&#20174;&#32780;&#20026;&#30456;&#21516;&#25968;&#25454;&#23454;&#20363;&#30340;&#19981;&#21516;&#25490;&#21015;&#33719;&#24471;&#28508;&#22312;&#19981;&#21516;&#30340;&#20915;&#31574;&#26641;&#65288;&#25490;&#21015;&#20915;&#31574;&#26641;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20351;&#29992;&#25490;&#21015;&#20915;&#31574;&#26641;&#23454;&#29616;&#25490;&#21015;Bagging&#30340;&#27010;&#24565;&#65292;&#32780;&#26080;&#38656;&#38543;&#26426;&#29305;&#24449;&#36873;&#25321;&#21644;&#23376;&#37319;&#26679;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#32763;&#35793;&#21253;&#8230;(&#20449;&#24687;&#19981;&#20840;)
&lt;/p&gt;
&lt;p&gt;
Decision Tree is a well understood Machine Learning model that is based on minimizing impurities in the internal nodes. The most common impurity measures are Shannon entropy and Gini impurity. These impurity measures are insensitive to the order of training data and hence the final tree obtained is invariant to any permutation of the data. This leads to a serious limitation in modeling data instances that have order dependencies. In this work, we propose the use of Effort-To-Compress (ETC) - a complexity measure, for the first time, as an impurity measure. Unlike Shannon entropy and Gini impurity, structural impurity based on ETC is able to capture order dependencies in the data, thus obtaining potentially different decision trees for different permutations of the same data instances (Permutation Decision Trees). We then introduce the notion of Permutation Bagging achieved using permutation decision trees without the need for random feature selection and sub-sampling. We compare the pe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#19978;&#19979;&#25991;&#21098;&#26525;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#34920;&#29616;&#21147;&#30340;&#21516;&#26102;&#65292;&#21160;&#24577;&#20943;&#23569;&#26080;&#25928;&#20449;&#24687;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#25216;&#26415;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.15805</link><description>&lt;p&gt;
&#21160;&#24577;&#19978;&#19979;&#25991;&#21098;&#26525;&#29992;&#20110;&#39640;&#25928;&#21644;&#21487;&#35299;&#37322;&#30340;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers. (arXiv:2305.15805v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#19978;&#19979;&#25991;&#21098;&#26525;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#34920;&#29616;&#21147;&#30340;&#21516;&#26102;&#65292;&#21160;&#24577;&#20943;&#23569;&#26080;&#25928;&#20449;&#24687;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#25216;&#26415;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37319;&#29992;&#30340;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#38590;&#20197;&#25193;&#23637;&#21040;&#38271;&#24207;&#21015;&#12290;&#23613;&#31649;&#26377;&#20960;&#39033;&#24037;&#20316;&#35797;&#22270;&#20943;&#23569;&#23427;&#20204;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20294;&#22823;&#22810;&#25968;LLM&#20173;&#28982;&#22312;&#25152;&#26377;&#26631;&#35760;&#23545;&#20043;&#38388;&#37319;&#29992;&#27880;&#24847;&#23618;&#65292;&#20174;&#32780;&#20135;&#29983;&#20108;&#27425;&#25104;&#26412;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#30041;&#27169;&#22411;&#30340;&#34920;&#29616;&#21147;&#26469;&#21160;&#24577;&#20462;&#21098;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#21487;&#23398;&#20064;&#26426;&#21046;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#30830;&#23450;&#21738;&#20123;&#26080;&#20851;&#30340;&#26631;&#35760;&#21487;&#20197;&#20174;&#19978;&#19979;&#25991;&#20013;&#21024;&#38500;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#35299;&#20915;&#20102;&#24615;&#33021;&#38382;&#39064;&#65292;&#32780;&#19988;&#22686;&#24378;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#20026;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#21147;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#24494;&#35843;&#36807;&#31243;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#19988;&#21098;&#26525;&#24378;&#24230;&#21487;&#20197;&#30001;&#31232;&#30095;&#24230;&#21442;&#25968;&#25351;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity para
&lt;/p&gt;</description></item><item><title>SPECTRON&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#38899;&#24310;&#32493;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#38899;&#32534;&#30721;&#22120;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#26469;&#29983;&#25104;&#25991;&#26412;&#21644;&#35821;&#38899;&#36755;&#20986;&#65292;&#22312;&#35821;&#20041;&#20869;&#23481;&#21644;&#35762;&#35805;&#32773;&#20445;&#25252;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.15255</link><description>&lt;p&gt;
&#24102;&#26377;&#35821;&#38899;&#30340;LM&#65306;&#36229;&#36234;&#35821;&#38899;&#20196;&#29260;&#30340;&#21475;&#35821;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
LMs with a Voice: Spoken Language Modeling beyond Speech Tokens. (arXiv:2305.15255v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15255
&lt;/p&gt;
&lt;p&gt;
SPECTRON&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#38899;&#24310;&#32493;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#38899;&#32534;&#30721;&#22120;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#26469;&#29983;&#25104;&#25991;&#26412;&#21644;&#35821;&#38899;&#36755;&#20986;&#65292;&#22312;&#35821;&#20041;&#20869;&#23481;&#21644;&#35762;&#35805;&#32773;&#20445;&#25252;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SPECTRON&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20197;&#25191;&#34892;&#35821;&#38899;&#24310;&#32493;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#25991;&#26412;&#21644;&#35821;&#38899;&#36755;&#20986;&#65292;&#25972;&#20010;&#31995;&#32479;&#37117;&#22312;&#39057;&#35889;&#22270;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#12290;&#22312;&#39057;&#35889;&#22270;&#39046;&#22495;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#30456;&#23545;&#20110;&#20351;&#29992;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#30340;&#29616;&#26377;&#32423;&#32852;&#26041;&#27861;&#31616;&#21270;&#20102;&#25105;&#20204;&#30340;&#35821;&#38899;&#24310;&#32493;&#31995;&#32479;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35821;&#20041;&#20869;&#23481;&#21644;&#35762;&#35805;&#32773;&#20445;&#25252;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#20063;&#20174;&#39044;&#20808;&#23384;&#22312;&#30340;&#27169;&#22411;&#20013;&#33719;&#24471;&#20102;&#30693;&#35782;&#20256;&#36882;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#32593;&#31449;https://michelleramanovich.github.io/spectron/spectron&#19978;&#21487;&#20197;&#25214;&#21040;&#38899;&#39057;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SPECTRON, a novel approach to adapting pre-trained language models (LMs) to perform speech continuation. By leveraging pre-trained speech encoders, our model generates both text and speech outputs with the entire system being trained end-to-end operating directly on spectrograms. Training the entire model in the spectrogram domain simplifies our speech continuation system versus existing cascade methods which use discrete speech representations. We further show our method surpasses existing spoken language models both in semantic content and speaker preservation while also benefiting from the knowledge transferred from pre-existing models. Audio samples can be found in our website https://michelleramanovich.github.io/spectron/spectron
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22270;&#19978;&#38271;&#23614;&#20998;&#31867;&#30340;&#31532;&#19968;&#20010;&#27867;&#21270;&#36793;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#34920;&#24449;&#38271;&#23614;&#31867;&#21035;&#30340;&#34892;&#20026;&#24182;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#30340;&#26032;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.09938</link><description>&lt;p&gt;
&#22270;&#20013;&#38271;&#23614;&#31867;&#21035;&#30340;&#29305;&#24449;&#21270;
&lt;/p&gt;
&lt;p&gt;
Characterizing Long-Tail Categories on Graphs. (arXiv:2305.09938v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09938
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22270;&#19978;&#38271;&#23614;&#20998;&#31867;&#30340;&#31532;&#19968;&#20010;&#27867;&#21270;&#36793;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#34920;&#24449;&#38271;&#23614;&#31867;&#21035;&#30340;&#34892;&#20026;&#24182;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#30340;&#26032;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#23614;&#25968;&#25454;&#20998;&#24067;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#32593;&#32476;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#21253;&#25324;&#37329;&#34701;&#20132;&#26131;&#32593;&#32476;&#12289;&#30005;&#23376;&#21830;&#21153;&#32593;&#32476;&#21644;&#21512;&#20316;&#32593;&#32476;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;&#20316;&#21697;&#20027;&#35201;&#38598;&#20013;&#20110;&#36890;&#36807;&#22270;&#22686;&#24378;&#25110;&#30446;&#26631;&#37325;&#26032;&#21152;&#26435;&#28040;&#38500;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#26377;&#38480;&#30340;&#25991;&#29486;&#25552;&#20379;&#29702;&#35770;&#24037;&#20855;&#26469;&#34920;&#24449;&#22270;&#19978;&#38271;&#23614;&#31867;&#21035;&#30340;&#34892;&#20026;&#65292;&#24182;&#29702;&#35299;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#21363;&#27599;&#20010;&#20219;&#21153;&#23545;&#24212;&#20110;&#39044;&#27979;&#19968;&#20010;&#29305;&#23450;&#30340;&#31867;&#21035;&#65292;&#25552;&#20986;&#20102;&#22270;&#19978;&#38271;&#23614;&#20998;&#31867;&#30340;&#31532;&#19968;&#20010;&#27867;&#21270;&#36793;&#30028;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#38271;&#23614;&#20998;&#31867;&#30340;&#27867;&#21270;&#24615;&#33021;&#21463;&#25152;&#26377;&#20219;&#21153;&#20013;&#30340;&#25439;&#22833;&#33539;&#22260;&#21644;&#20219;&#21153;&#24635;&#25968;&#30340;&#25903;&#37197;&#12290;&#22312;&#29702;&#35770;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#34920;&#24449;&#38271;&#23614;&#31867;&#21035;&#30340;&#34892;&#20026;&#65292;&#24182;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-tail data distributions are prevalent in many real-world networks, including financial transaction networks, e-commerce networks, and collaboration networks. Despite the success of recent developments, the existing works mainly focus on debiasing the machine learning models via graph augmentation or objective reweighting. However, there is limited literature that provides a theoretical tool to characterize the behaviors of long-tail categories on graphs and understand the generalization performance in real scenarios. To bridge this gap, we propose the first generalization bound for long-tail classification on graphs by formulating the problem in the fashion of multi-task learning, i.e., each task corresponds to the prediction of one particular category. Our theoretical results show that the generalization performance of long-tail classification is dominated by the range of losses across all tasks and the total number of tasks. Building upon the theoretical findings, we propose a n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#24179;&#22343;&#25216;&#26415;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#25439;&#22833;/&#38169;&#35823;&#26223;&#35266;&#25581;&#31034;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#29615;&#32469;&#20840;&#23616;&#27169;&#22411;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#30406;&#22320;&#20869;&#65292;&#24182;&#19988;&#21457;&#29616;&#20840;&#23616;&#27169;&#22411;&#22312;&#26089;&#26399;&#35757;&#32451;&#21518;&#30340;&#35823;&#24046;&#20027;&#35201;&#26469;&#33258;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#20043;&#38388;&#38750;&#37325;&#21472;&#30340;&#25968;&#25454;&#21450;&#20840;&#23616;&#27169;&#22411;&#19982;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#20004;&#20010;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.07845</link><description>&lt;p&gt;
&#29702;&#35299;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#24179;&#22343;
&lt;/p&gt;
&lt;p&gt;
Understanding Model Averaging in Federated Learning on Heterogeneous Data. (arXiv:2305.07845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#24179;&#22343;&#25216;&#26415;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#25439;&#22833;/&#38169;&#35823;&#26223;&#35266;&#25581;&#31034;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#29615;&#32469;&#20840;&#23616;&#27169;&#22411;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#30406;&#22320;&#20869;&#65292;&#24182;&#19988;&#21457;&#29616;&#20840;&#23616;&#27169;&#22411;&#22312;&#26089;&#26399;&#35757;&#32451;&#21518;&#30340;&#35823;&#24046;&#20027;&#35201;&#26469;&#33258;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#20043;&#38388;&#38750;&#37325;&#21472;&#30340;&#25968;&#25454;&#21450;&#20840;&#23616;&#27169;&#22411;&#19982;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#20004;&#20010;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#24179;&#22343;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#24191;&#27867;&#37319;&#29992;&#30340;&#19968;&#31181;&#25216;&#26415;&#65292;&#23427;&#20250;&#32858;&#38598;&#35757;&#32451;&#20110;&#24322;&#26500;&#25968;&#25454;&#19978;&#30340;&#22810;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#20197;&#33719;&#24471;&#34920;&#29616;&#33391;&#22909;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20854;&#25104;&#21151;&#32972;&#21518;&#30340;&#21407;&#29702;&#23578;&#19981;&#26159;&#24456;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#21487;&#35270;&#21270;&#25439;&#22833;/&#38169;&#35823;&#26223;&#35266;&#26469;&#30740;&#31350;&#27169;&#22411;&#24179;&#22343;&#30340;&#20960;&#20309;&#29305;&#24615;&#65292;&#25581;&#31034;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#29615;&#32469;&#20840;&#23616;&#27169;&#22411;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#30406;&#22320;&#20869;&#65292;&#24182;&#19988;&#21363;&#20351;&#20840;&#23616;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;&#20063;&#21487;&#33021;&#20559;&#31163;&#30406;&#22320;&#24213;&#37096;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20840;&#23616;&#27169;&#22411;&#22312;&#26089;&#26399;&#35757;&#32451;&#21518;&#30340;&#35823;&#24046;&#20027;&#35201;&#26469;&#33258;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#20043;&#38388;&#38750;&#37325;&#21472;&#30340;&#25968;&#25454;&#21450;&#20840;&#23616;&#27169;&#22411;&#19982;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#20004;&#20010;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model averaging, a widely adopted technique in federated learning (FL), aggregates multiple client models trained on heterogeneous data to obtain a well-performed global model. However, the rationale behind its success is not well understood. To shed light on this issue, we investigate the geometric properties of model averaging by visualizing the loss/error landscape. The geometrical visualization shows that the client models surround the global model within a common basin, and the global model may deviate from the bottom of the basin even though it performs better than the client models. To further understand this phenomenon, we decompose the expected prediction error of the global model into five factors related to client models. Specifically, we find that the global-model error after early training mainly comes from i) the client-model error on non-overlapping data between client datasets and the global dataset and ii) the maximal distance between the global and client models. Insp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65306;&#22312;&#21160;&#24577;&#22270;&#24418;&#29615;&#22659;&#19979;&#22914;&#20309;&#26377;&#25928;&#22320;&#36827;&#34892;&#36328;&#22270;&#36801;&#31227;&#23398;&#20064;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#39046;&#22495;&#28436;&#21270;&#23545;&#27867;&#21270;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.00664</link><description>&lt;p&gt;
&#36328;&#22270;&#21160;&#24577;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dynamic Transfer Learning across Graphs. (arXiv:2305.00664v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00664
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65306;&#22312;&#21160;&#24577;&#22270;&#24418;&#29615;&#22659;&#19979;&#22914;&#20309;&#26377;&#25928;&#22320;&#36827;&#34892;&#36328;&#22270;&#36801;&#31227;&#23398;&#20064;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#39046;&#22495;&#28436;&#21270;&#23545;&#27867;&#21270;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#65292;&#36328;&#22270;&#20256;&#36755;&#30693;&#35782;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21253;&#25324;&#36816;&#36755;&#32593;&#32476;&#12289;&#30005;&#23376;&#21830;&#21153;&#32593;&#32476;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#37329;&#34701;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#38382;&#39064;&#65306;&#22312;&#21160;&#24577;&#35774;&#32622;&#19979;&#65292;&#32771;&#34385;&#24050;&#35266;&#23519;&#21040;&#30340;&#20855;&#26377;&#26631;&#31614;&#30340;&#28304;&#22270;&#21644;&#26631;&#31614;&#31232;&#30095;&#30340;&#30446;&#26631;&#22270;&#65292;&#22914;&#20309;&#26377;&#25928;&#22320;&#34920;&#24449;&#19981;&#26029;&#21464;&#21270;&#30340;&#39046;&#22495;&#20559;&#24046;&#65292;&#24182;&#20248;&#21270;&#30446;&#26631;&#22495;&#22312;&#19979;&#19968;&#20010;&#26102;&#38388;&#25139;&#30340;&#27867;&#21270;&#24615;&#33021;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#36328;&#22270;&#21160;&#24577;&#36801;&#31227;&#23398;&#20064;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#21270;&#30028;&#38480;&#65292;&#36825;&#24847;&#21619;&#30528;&#27867;&#21270;&#24615;&#33021;&#30001;&#39046;&#22495;&#28436;&#21270;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transferring knowledge across graphs plays a pivotal role in many high-stake domains, ranging from transportation networks to e-commerce networks, from neuroscience to finance. To date, the vast majority of existing works assume both source and target domains are sampled from a universal and stationary distribution. However, many real-world systems are intrinsically dynamic, where the underlying domains are evolving over time. To bridge the gap, we propose to shift the problem to the dynamic setting and ask: given the label-rich source graphs and the label-scarce target graphs observed in previous T timestamps, how can we effectively characterize the evolving domain discrepancy and optimize the generalization performance of the target domain at the incoming T+1 timestamp? To answer the question, for the first time, we propose a generalization bound under the setting of dynamic transfer learning across graphs, which implies the generalization performance is dominated by domain evolution
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;AUC&#20998;&#25968;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#38543;&#26426;&#32452;&#21512;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#21160;&#37327;&#31639;&#27861;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10101</link><description>&lt;p&gt;
&#32852;&#37030;&#32452;&#21512;&#28145;&#24230;AUC&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Federated Compositional Deep AUC Maximization. (arXiv:2304.10101v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;AUC&#20998;&#25968;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#38543;&#26426;&#32452;&#21512;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#21160;&#37327;&#31639;&#27861;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#30001;&#20110;&#24179;&#34913;&#38544;&#31169;&#21644;&#22823;&#35268;&#27169;&#23398;&#20064;&#30340;&#25215;&#35834;&#32780;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65307;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#22788;&#29702;&#24179;&#34913;&#25968;&#25454;&#38382;&#39064;&#19978;&#65292;&#32780;&#22312;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#19981;&#21516;&#31867;&#21035;&#20013;&#30340;&#26679;&#26412;&#25968;&#37327;&#39640;&#24230;&#19981;&#24179;&#34913;&#65292;&#23548;&#33268;&#39044;&#27979;&#24615;&#33021;&#36828;&#20302;&#20110;&#29702;&#24819;&#27700;&#24179;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38024;&#23545;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#20998;&#25968;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;AUC&#26368;&#22823;&#21270;&#38382;&#39064;&#20316;&#20026;&#32852;&#37030;&#32452;&#21512;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#20102;&#34920;&#36848;&#65292;&#24182;&#24320;&#21457;&#20102;&#26412;&#22320;&#38543;&#26426;&#32452;&#21512;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#21160;&#37327;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#22797;&#26434;&#24230;&#30340;&#30028;&#38480;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#22914;&#27492;&#26377;&#21033;&#29702;&#35770;&#32467;&#26524;&#30340;&#24037;&#20316;&#12290;&#26368;&#21518;&#65292;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has attracted increasing attention due to the promise of balancing privacy and large-scale learning; numerous approaches have been proposed. However, most existing approaches focus on problems with balanced data, and prediction performance is far from satisfactory for many real-world applications where the number of samples in different classes is highly imbalanced. To address this challenging problem, we developed a novel federated learning method for imbalanced data by directly optimizing the area under curve (AUC) score. In particular, we formulate the AUC maximization problem as a federated compositional minimax optimization problem, develop a local stochastic compositional gradient descent ascent with momentum algorithm, and provide bounds on the computational and communication complexities of our algorithm. To the best of our knowledge, this is the first work to achieve such favorable theoretical results. Finally, extensive experimental results confirm the effi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#26469;&#25552;&#21319;&#33945;&#29305;&#21345;&#32599;&#35780;&#20272;&#26041;&#27861;&#65292;&#23454;&#29616;&#22312;&#20445;&#25345;&#30456;&#21516;&#20272;&#35745;&#20934;&#30830;&#24230;&#30340;&#21069;&#25552;&#19979;&#65292;&#20943;&#23569;&#22312;&#32447;&#26679;&#26412;&#25968;&#37327;&#30340;&#30446;&#30340;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#23450;&#21046;&#30340;&#34892;&#20026;&#31574;&#30053;&#65292;&#21487;&#20197;&#27604;&#26222;&#36890;&#30340; MC &#20272;&#35745;&#22120;&#20135;&#29983;&#26356;&#23567;&#30340;&#26041;&#24046;&#12290;&#35813;&#34892;&#20026;&#31574;&#30053;&#21487;&#20197;&#20174;&#29616;&#26377;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#39640;&#25928;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#20351;&#29992;&#23567;&#37096;&#20998;&#22312;&#32447;&#26679;&#26412;&#23601;&#33021;&#23454;&#29616;&#30456;&#21516;&#30340;&#20272;&#35745;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2301.13734</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#32447;&#25968;&#25454;&#25552;&#21319;&#33945;&#29305;&#21345;&#32599;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Monte Carlo Evaluation with Offline Data. (arXiv:2301.13734v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#26469;&#25552;&#21319;&#33945;&#29305;&#21345;&#32599;&#35780;&#20272;&#26041;&#27861;&#65292;&#23454;&#29616;&#22312;&#20445;&#25345;&#30456;&#21516;&#20272;&#35745;&#20934;&#30830;&#24230;&#30340;&#21069;&#25552;&#19979;&#65292;&#20943;&#23569;&#22312;&#32447;&#26679;&#26412;&#25968;&#37327;&#30340;&#30446;&#30340;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#23450;&#21046;&#30340;&#34892;&#20026;&#31574;&#30053;&#65292;&#21487;&#20197;&#27604;&#26222;&#36890;&#30340; MC &#20272;&#35745;&#22120;&#20135;&#29983;&#26356;&#23567;&#30340;&#26041;&#24046;&#12290;&#35813;&#34892;&#20026;&#31574;&#30053;&#21487;&#20197;&#20174;&#29616;&#26377;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#39640;&#25928;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#20351;&#29992;&#23567;&#37096;&#20998;&#22312;&#32447;&#26679;&#26412;&#23601;&#33021;&#23454;&#29616;&#30456;&#21516;&#30340;&#20272;&#35745;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33945;&#29305;&#21345;&#32599; (MC) &#26041;&#27861;&#26159;&#20272;&#35745;&#31574;&#30053;&#34920;&#29616;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#12290;&#32473;&#23450;&#19968;&#20010;&#24863;&#20852;&#36259;&#30340;&#31574;&#30053;&#65292;MC &#26041;&#27861;&#36890;&#36807;&#37325;&#22797;&#36816;&#34892;&#35813;&#31574;&#30053;&#20197;&#25910;&#38598;&#26679;&#26412;&#24182;&#21462;&#20986;&#32467;&#26524;&#24179;&#22343;&#20540;&#26469;&#32473;&#20986;&#20272;&#35745;&#20540;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#25910;&#38598;&#30340;&#26679;&#26412;&#31216;&#20026;&#22312;&#32447;&#26679;&#26412;&#12290;&#20026;&#20102;&#33719;&#24471;&#20934;&#30830;&#30340;&#20272;&#35745;&#20540;&#65292;MC &#26041;&#27861;&#38656;&#35201;&#28040;&#32791;&#22823;&#37327;&#22312;&#32447;&#26679;&#26412;&#12290;&#24403;&#22312;&#32447;&#26679;&#26412;&#26114;&#36149;&#26102;&#65292;&#20363;&#22914;&#22312;&#32447;&#25512;&#33616;&#21644;&#24211;&#23384;&#31649;&#29702;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#23454;&#29616;&#30456;&#21516;&#30340;&#20272;&#35745;&#20934;&#30830;&#24230;&#30340;&#21516;&#26102;&#20943;&#23569;&#22312;&#32447;&#26679;&#26412;&#25968;&#37327;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#31163;&#32447; MC &#26041;&#27861;&#65292;&#36890;&#36807;&#36816;&#34892;&#19981;&#21516;&#30340;&#31574;&#30053;&#65288;&#31216;&#20026;&#34892;&#20026;&#31574;&#30053;&#65289;&#35780;&#20272;&#24863;&#20852;&#36259;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#34892;&#20026;&#31574;&#30053;&#65292;&#20351;&#31163;&#32447; MC &#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#26126;&#26174;&#23567;&#20110;&#26222;&#36890; MC &#20272;&#35745;&#22120;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#35813;&#23450;&#21046;&#34892;&#20026;&#31574;&#30053;&#21487;&#20197;&#20174;&#29616;&#26377;&#30340;&#31163;&#32447;&#25968;&#25454;&#65292;&#21363;&#20808;&#21069;&#35760;&#24405;&#30340;&#25968;&#25454;&#20013;&#39640;&#25928;&#23398;&#20064;&#65292;&#36825;&#27604;&#22312;&#32447;&#26679;&#26412;&#35201;&#20415;&#23452;&#24471;&#22810;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#20351;&#29992;&#23567;&#37096;&#20998;&#22312;&#32447;&#26679;&#26412;&#23601;&#33021;&#23454;&#29616;&#30456;&#21516;&#30340;&#20272;&#35745;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monte Carlo (MC) methods are the most widely used methods to estimate the performance of a policy. Given an interested policy, MC methods give estimates by repeatedly running this policy to collect samples and taking the average of the outcomes. Samples collected during this process are called online samples. To get an accurate estimate, MC methods consume massive online samples. When online samples are expensive, e.g., online recommendations and inventory management, we want to reduce the number of online samples while achieving the same estimate accuracy. To this end, we use off-policy MC methods that evaluate the interested policy by running a different policy called behavior policy. We design a tailored behavior policy such that the variance of the off-policy MC estimator is provably smaller than the ordinary MC estimator. Importantly, this tailored behavior policy can be efficiently learned from existing offline data, i,e., previously logged data, which are much cheaper than onlin
&lt;/p&gt;</description></item><item><title>&#25152;&#25552;&#20986;&#30340;RFold&#26041;&#27861;&#37319;&#29992;&#35299;&#32806;&#20248;&#21270;&#36807;&#31243;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#31616;&#21333;&#21448;&#26377;&#25928;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2212.14041</link><description>&lt;p&gt;
RFold&#65306;&#22522;&#20110;&#35299;&#32806;&#20248;&#21270;&#26041;&#27861;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
RFold: RNA Secondary Structure Prediction with Decoupled Optimization. (arXiv:2212.14041v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14041
&lt;/p&gt;
&lt;p&gt;
&#25152;&#25552;&#20986;&#30340;RFold&#26041;&#27861;&#37319;&#29992;&#35299;&#32806;&#20248;&#21270;&#36807;&#31243;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#31616;&#21333;&#21448;&#26377;&#25928;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#31958;&#26680;&#37240;&#65288;RNA&#65289;&#30340;&#20108;&#32423;&#32467;&#26500;&#27604;&#19977;&#32423;&#32467;&#26500;&#26356;&#31283;&#23450;&#21644;&#26356;&#26131;&#20110;&#22312;&#32454;&#32990;&#20013;&#35775;&#38382;&#65292;&#22240;&#27492;&#23545;&#20110;&#21151;&#33021;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#26174;&#31034;&#20986;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#24403;&#21069;&#30340;&#26041;&#27861;&#23384;&#22312;&#27867;&#21270;&#24615;&#24046;&#21644;&#22797;&#26434;&#24615;&#39640;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;RFold&#12290;RFold&#24341;&#20837;&#20102;&#19968;&#31181;&#35299;&#32806;&#20248;&#21270;&#30340;&#36807;&#31243;&#65292;&#23558;&#20256;&#32479;&#30340;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#20998;&#35299;&#20026;&#36880;&#34892;&#21644;&#36880;&#21015;&#20248;&#21270;&#65292;&#31616;&#21270;&#20102;&#27714;&#35299;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#36755;&#20986;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;RFold&#37319;&#29992;&#27880;&#24847;&#21147;&#22320;&#22270;&#20316;&#20026;&#20449;&#24687;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#35774;&#35745;&#25163;&#24037;&#29305;&#24449;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RFold&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#32422;8&#20493;&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;&#20195;&#30721;&#21644;Colab&#28436;&#31034;&#21487;&#22312;\href{this http URL}{this http UR}&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The secondary structure of ribonucleic acid (RNA) is more stable and accessible in the cell than its tertiary structure, making it essential for functional prediction. Although deep learning has shown promising results in this field, current methods suffer from poor generalization and high complexity. In this work, we present RFold, a simple yet effective RNA secondary structure prediction in an end-to-end manner. RFold introduces a decoupled optimization process that decomposes the vanilla constraint satisfaction problem into row-wise and column-wise optimization, simplifying the solving process while guaranteeing the validity of the output. Moreover, RFold adopts attention maps as informative representations instead of designing hand-crafted features. Extensive experiments demonstrate that RFold achieves competitive performance and about eight times faster inference efficiency than the state-of-the-art method. The code and Colab demo are available in \href{this http URL}{this http UR
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#19979;&#36827;&#34892;&#32479;&#19968;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26399;&#26395;&#33258;&#30001;&#33021;&#20989;&#25968;&#25351;&#23548;&#20195;&#29702;&#36873;&#25321;&#21160;&#20316;&#65292;&#20197;&#23454;&#29616;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;</title><link>http://arxiv.org/abs/2212.07946</link><description>&lt;p&gt;
&#20027;&#21160;&#25512;&#29702;&#21644;&#24378;&#21270;&#23398;&#20064;&#65306;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#19979;&#30340;&#32479;&#19968;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Active Inference and Reinforcement Learning: A unified inference on continuous state and action spaces under partially observability. (arXiv:2212.07946v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#19979;&#36827;&#34892;&#32479;&#19968;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26399;&#26395;&#33258;&#30001;&#33021;&#20989;&#25968;&#25351;&#23548;&#20195;&#29702;&#36873;&#25321;&#21160;&#20316;&#65292;&#20197;&#23454;&#29616;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#23436;&#20840;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#24320;&#21457;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#20197;&#26368;&#22823;&#21270;&#30001;&#22806;&#37096;&#30417;&#30563;&#21592;&#25351;&#23450;&#30340;&#22870;&#21169;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#28041;&#21450;&#37096;&#20998;&#35266;&#27979;&#65292;&#24418;&#24335;&#21270;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#36807;&#23558;&#36807;&#21435;&#30340;&#34892;&#21160;&#21644;&#35266;&#27979;&#35760;&#24518;&#25110;&#36890;&#36807;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#25512;&#26029;&#29615;&#22659;&#30340;&#30495;&#23454;&#29366;&#24577;&#26469;&#35299;&#20915;POMDP&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#38543;&#26102;&#38388;&#32858;&#21512;&#35266;&#27979;&#25968;&#25454;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#25512;&#29702;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#35768;&#22810;&#26679;&#26412;&#25165;&#33021;&#34920;&#29616;&#33391;&#22909;&#65292;&#22240;&#20026;&#23427;&#20204;&#20165;&#20851;&#27880;&#22870;&#21169;&#26368;&#22823;&#21270;&#65292;&#24573;&#35270;&#20102;&#25512;&#26029;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20027;&#21160;&#25512;&#29702;&#65288;AIF&#65289;&#26159;&#22312;POMDP&#20013;&#21046;&#23450;&#30340;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#19968;&#20010;&#31216;&#20026;&#26399;&#26395;&#33258;&#30001;&#33021;&#65288;EFE&#65289;&#30340;&#20989;&#25968;&#25351;&#23548;&#20195;&#29702;&#36873;&#25321;&#21160;&#20316;&#12290;&#36825;&#25552;&#20379;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#65288;&#23500;&#26377;&#24320;&#21457;&#24615;&#65289;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has garnered significant attention for developing decision-making agents that aim to maximize rewards, specified by an external supervisor, within fully observable environments. However, many real-world problems involve partial observations, formulated as partially observable Markov decision processes (POMDPs). Previous studies have tackled RL in POMDPs by either incorporating the memory of past actions and observations or by inferring the true state of the environment from observed data. However, aggregating observed data over time becomes impractical in continuous spaces. Moreover, inference-based RL approaches often require many samples to perform well, as they focus solely on reward maximization and neglect uncertainty in the inferred state. Active inference (AIF) is a framework formulated in POMDPs and directs agents to select actions by minimizing a function called expected free energy (EFE). This supplies reward-maximizing (exploitative) behaviour, as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#20195;&#39537;&#21160;&#30340;&#28151;&#21512;&#23614;&#25968;HBFP&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#21442;&#25968;&#30340;&#25506;&#32034;&#21644;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;DNN&#35757;&#32451;&#20013;&#31639;&#26415;&#25805;&#20316;&#30340;&#26356;&#23567;&#32534;&#30721;&#12290;&#20351;&#29992;&#20998;&#26512;&#27169;&#22411;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;HBFP&#35757;&#32451;&#21152;&#36895;&#22120;&#30340;&#31639;&#26415;&#23494;&#24230;&#22686;&#21152;&#39640;&#36798;$21.3\times$&#12290;</title><link>http://arxiv.org/abs/2211.10737</link><description>&lt;p&gt;
&#25552;&#39640;&#20934;&#30830;&#24615;: &#22522;&#20110;&#26102;&#20195;&#39537;&#21160;&#30340;&#28151;&#21512;&#23614;&#25968;&#22359;&#28014;&#28857;&#26041;&#27861;&#29992;&#20110;DNN&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Accuracy Boosters: Epoch-Driven Mixed-Mantissa Block Floating-Point for DNN Training. (arXiv:2211.10737v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#20195;&#39537;&#21160;&#30340;&#28151;&#21512;&#23614;&#25968;HBFP&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#21442;&#25968;&#30340;&#25506;&#32034;&#21644;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;DNN&#35757;&#32451;&#20013;&#31639;&#26415;&#25805;&#20316;&#30340;&#26356;&#23567;&#32534;&#30721;&#12290;&#20351;&#29992;&#20998;&#26512;&#27169;&#22411;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;HBFP&#35757;&#32451;&#21152;&#36895;&#22120;&#30340;&#31639;&#26415;&#23494;&#24230;&#22686;&#21152;&#39640;&#36798;$21.3\times$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DNN&#27169;&#22411;&#22797;&#26434;&#24615;&#12289;&#35268;&#27169;&#21644;&#35757;&#32451;&#25968;&#25454;&#37327;&#30340;&#21069;&#25152;&#26410;&#26377;&#22686;&#38271;&#23548;&#33268;&#20102;&#23545;&#35745;&#31639;&#30340;&#24040;&#22823;&#38656;&#27714;&#21644;&#23545;&#26368;&#23567;&#32534;&#30721;&#30340;&#25628;&#32034;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#24352;&#20351;&#29992;&#28151;&#21512;&#22359;&#28014;&#28857;(HBFP)&#26469;&#26368;&#23567;&#21270;&#21152;&#36895;&#22120;&#20013;&#30340;&#30789;&#37197;&#22791;&#65292;&#36890;&#36807;&#23558;&#22823;&#37096;&#20998;&#35757;&#32451;&#20013;&#30340;&#31639;&#26415;&#25805;&#20316;&#36716;&#25442;&#20026;8&#20301;&#23450;&#28857;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#23398;&#24037;&#20855;&#23545;HBFP&#35774;&#35745;&#31354;&#38388;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25506;&#32034;&#65292;&#30740;&#31350;&#20102;&#21508;&#31181;&#21442;&#25968;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#30830;&#23450;&#20102;&#22312;&#21508;&#23618;&#21644;&#21508;&#20010;&#26102;&#20195;&#20013;&#26356;&#23567;&#32534;&#30721;&#30340;&#26426;&#20250;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Accuracy Boosters&#65292;&#19968;&#31181;&#22522;&#20110;&#26102;&#20195;&#39537;&#21160;&#30340;&#28151;&#21512;&#23614;&#25968;HBFP&#25216;&#26415;&#65292;&#21482;&#22312;&#26368;&#21518;&#19968;&#20010;&#26102;&#20195;&#21644;&#31532;&#19968;&#20010;/&#26368;&#21518;&#19968;&#23618;&#20013;&#20351;&#29992;6&#20301;&#23614;&#25968;&#65292;&#22312;&#35757;&#32451;&#20013;&#30340;&#20854;&#20182;&#31639;&#26415;&#25805;&#20316;&#20013;&#20351;&#29992;4&#20301;&#23614;&#25968;&#36798;&#21040;$99.7\%$&#12290;&#20351;&#29992;&#20998;&#26512;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Accuracy Boosters&#21487;&#20197;&#20351;HBFP&#35757;&#32451;&#21152;&#36895;&#22120;&#30340;&#31639;&#26415;&#23494;&#24230;&#22686;&#21152;&#39640;&#36798;$21.3\times$&#12290;
&lt;/p&gt;
&lt;p&gt;
The unprecedented growth in DNN model complexity, size, and amount of training data has led to a commensurate increase in demand for computing and a search for minimal encoding. Recent research advocates Hybrid Block Floating Point (HBFP) to minimize silicon provisioning in accelerators by converting the majority of arithmetic operations in training to 8-bit fixed point. In this paper, we perform a full-scale exploration of the HBFP design space using mathematical tools to study the interplay among various parameters and identify opportunities for even smaller encodings across layers and epochs. Based on our findings, we propose Accuracy Boosters, an epoch-driven mixed-mantissa HBFP technique that uses 6-bit mantissas only in the last epoch and first/last layers, and 4-bit mantissas for $99.7\%$ of all other arithmetic operations in training. Using analytic models, we show Accuracy Boosters enable increasing arithmetic density for an HBFP training accelerator by up to $21.3\times$ comp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36816;&#21160;&#24863;&#30693;&#26631;&#35760;&#36873;&#25321;&#26041;&#27861;&#65292;&#38024;&#23545;&#35270;&#39057;&#20013;&#19981;&#21516;&#34917;&#19969;&#30340;&#20449;&#24687;&#23494;&#24230;&#65292;&#36873;&#25321;&#21253;&#21547;&#20016;&#23500;&#21160;&#24577;&#29305;&#24615;&#30340;&#26631;&#35760;&#65292;&#25918;&#24323;&#26080;&#25928;&#30340;&#26631;&#35760;&#65292;&#20174;&#32780;&#22823;&#22823;&#38477;&#20302;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#65292;&#23454;&#29616;&#20102;&#22312;&#21333;&#21488;&#26426;&#22120;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.10636</link><description>&lt;p&gt;
&#36816;&#21160;&#24863;&#30693;&#26631;&#35760;&#36873;&#25321;&#23454;&#29616;&#39640;&#25928;&#35270;&#39057;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Video Representation Learning via Motion-Aware Token Selection. (arXiv:2211.10636v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10636
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36816;&#21160;&#24863;&#30693;&#26631;&#35760;&#36873;&#25321;&#26041;&#27861;&#65292;&#38024;&#23545;&#35270;&#39057;&#20013;&#19981;&#21516;&#34917;&#19969;&#30340;&#20449;&#24687;&#23494;&#24230;&#65292;&#36873;&#25321;&#21253;&#21547;&#20016;&#23500;&#21160;&#24577;&#29305;&#24615;&#30340;&#26631;&#35760;&#65292;&#25918;&#24323;&#26080;&#25928;&#30340;&#26631;&#35760;&#65292;&#20174;&#32780;&#22823;&#22823;&#38477;&#20302;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#65292;&#23454;&#29616;&#20102;&#22312;&#21333;&#21488;&#26426;&#22120;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#30340;&#33945;&#29256;&#35270;&#39057;&#24314;&#27169;&#25216;&#26415;&#36890;&#36807;&#22312;&#35270;&#39057;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#20013;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38543;&#26426;&#33945;&#29256;&#31574;&#30053;&#23548;&#33268;&#39044;&#27979;&#26080;&#25928;&#30340;&#26631;&#35760;/&#24103;&#65292;&#36825;&#20123;&#25216;&#26415;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#65292;&#38656;&#35201;&#26114;&#36149;&#30340;&#35745;&#31639;&#26426;&#21644;&#22823;&#37327;&#26174;&#21345;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#21033;&#29992;&#35270;&#39057;&#34917;&#19969;&#20013;&#30340;&#19981;&#22343;&#21248;&#20449;&#24687;&#23494;&#24230;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26631;&#35760;&#36873;&#25321;&#26041;&#27861;&#65306;MATS&#65306;&#36816;&#21160;&#24863;&#30693;&#26631;&#35760;&#36873;&#25321;&#65292;&#22312;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#36807;&#31243;&#20013;&#25214;&#21040;&#21253;&#21547;&#20016;&#23500;&#21160;&#24577;&#29305;&#24615;&#30340;&#26631;&#35760;&#65292;&#24182;&#25918;&#24323;&#26080;&#25928;&#30340;&#26631;&#35760;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#24103;&#36873;&#25321;&#31574;&#30053;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#20851;&#27880;&#26368;&#37325;&#35201;&#21644;&#22240;&#26524;&#24615;&#30340;&#24103;&#65292;&#24182;&#20351;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#24471;&#21040;&#26174;&#30528;&#38477;&#20302;&#65292;&#20351;&#24471;&#22312;&#21333;&#21488;&#26426;&#22120;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently emerged Masked Video Modeling techniques demonstrated their potential by significantly outperforming previous methods in self-supervised learning for video. However, they require an excessive amount of computations and memory while predicting uninformative tokens/frames due to random masking strategies, requiring excessive computing power for training. (e.g., over 16 nodes with 128 NVIDIA A100 GPUs). To resolve this issue, we exploit the unequal information density among the patches in videos and propose a new token selection method, MATS: Motion-Aware Token Selection, that finds tokens containing rich motion features and drops uninformative ones during both self-supervised pre-training and fine-tuning. We further present an adaptive frame selection strategy that allows the model to focus on informative and causal frames with minimal redundancy. Our method significantly reduces computation and memory requirements, enabling the pre-training and fine-tuning on a single machine w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20256;&#36755;&#23398;&#20064;&#30340;&#22522;&#26412;&#36807;&#31243;&#65292;&#38024;&#23545;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;&#28304;&#20219;&#21153;&#21442;&#25968;&#21644;&#30446;&#26631;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#36755;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#35813;&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#35299;&#20915;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#23792;&#20540;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36275;&#22815;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#65292;&#35813;&#20256;&#36755;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#23725;&#22238;&#24402;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2103.05621</link><description>&lt;p&gt;
&#20256;&#36755;&#23398;&#20064;&#30340;&#20849;&#21516;&#30452;&#35273;&#21487;&#20197;&#24102;&#26469;&#32988;&#21033;&#25110;&#22833;&#36133;: &#32447;&#24615;&#22238;&#24402;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Common Intuition to Transfer Learning Can Win or Lose: Case Studies for Linear Regression. (arXiv:2103.05621v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.05621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20256;&#36755;&#23398;&#20064;&#30340;&#22522;&#26412;&#36807;&#31243;&#65292;&#38024;&#23545;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;&#28304;&#20219;&#21153;&#21442;&#25968;&#21644;&#30446;&#26631;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#36755;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#35813;&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#35299;&#20915;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#23792;&#20540;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36275;&#22815;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#65292;&#35813;&#20256;&#36755;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#23725;&#22238;&#24402;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#28304;&#22238;&#24402;&#20219;&#21153;&#21040;&#30446;&#26631;&#22238;&#24402;&#20219;&#21153;&#30340;&#22522;&#26412;&#20256;&#36755;&#23398;&#20064;&#36807;&#31243;&#65292;&#21253;&#25324;&#22312;&#26377;&#27604;&#25968;&#25454;&#26679;&#26412;&#26356;&#22810;&#30340;&#23398;&#20064;&#21442;&#25968;&#30340;&#36807;&#21442;&#25968;&#21270;&#35774;&#32622;&#20013;&#12290;&#30446;&#26631;&#20219;&#21153;&#30340;&#23398;&#20064;&#36890;&#36807;&#20351;&#29992;&#20854;&#35757;&#32451;&#25968;&#25454;&#21644;&#20808;&#21069;&#35745;&#31639;&#30340;&#28304;&#20219;&#21153;&#21442;&#25968;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#23558;&#30446;&#26631;&#20219;&#21153;&#30340;&#20256;&#36755;&#23398;&#20064;&#26041;&#27861;&#23450;&#20041;&#20026;&#24102;&#26377;&#27491;&#21017;&#21270;&#30340;&#32447;&#24615;&#22238;&#24402;&#20248;&#21270;&#65292;&#20854;&#20013;&#27491;&#21017;&#21270;&#39033;&#26159;&#24453;&#23398;&#20064;&#30340;&#30446;&#26631;&#21442;&#25968;&#19982;&#24050;&#23398;&#20064;&#30340;&#28304;&#21442;&#25968;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#20998;&#26512;&#22320;&#34920;&#24449;&#20102;&#25105;&#20204;&#30340;&#20256;&#36755;&#23398;&#20064;&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#35299;&#20915;&#32447;&#24615;&#22238;&#24402;&#30340;&#26368;&#23567;L2&#33539;&#25968;&#35299;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#23792;&#20540;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#36275;&#22815;&#30456;&#20851;&#30340;&#20219;&#21153;&#26469;&#35828;&#65292;&#32463;&#36807;&#26368;&#20339;&#35843;&#20248;&#30340;&#20256;&#36755;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#26368;&#20339;&#35843;&#20248;&#30340;&#23725;&#22238;&#24402;&#26041;&#27861;&#65292;&#21363;&#20351;&#30495;&#23454;&#21442;&#25968;&#21521;&#37327;&#31526;&#21512;&#26368;&#23567;L2&#33539;&#25968;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a fundamental transfer learning process from source to target linear regression tasks, including overparameterized settings where there are more learned parameters than data samples. The target task learning is addressed by using its training data together with the parameters previously computed for the source task. We define a transfer learning approach to the target task as a linear regression optimization with a regularization on the distance between the to-be-learned target parameters and the already-learned source parameters. We analytically characterize the generalization performance of our transfer learning approach and demonstrate its ability to resolve the peak in generalization errors in double descent phenomena of the minimum L2-norm solution to linear regression. Moreover, we show that for sufficiently related tasks, the optimally tuned transfer learning approach can outperform the optimally tuned ridge regression method, even when the true parameter vector conform
&lt;/p&gt;</description></item></channel></rss>