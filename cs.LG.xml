<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36991;&#20813;&#38480;&#21046;&#24615;&#20551;&#35774;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24615;&#26469;&#25552;&#39640;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#22312;&#39044;&#27979;&#20248;&#21270;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16830</link><description>&lt;p&gt;
&#31163;&#24034;&#65306;&#36229;&#36234;&#26412;&#22320;&#25439;&#22833;&#20989;&#25968;&#30340;&#39044;&#27979;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Leaving the Nest: Going Beyond Local Loss Functions for Predict-Then-Optimize. (arXiv:2305.16830v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36991;&#20813;&#38480;&#21046;&#24615;&#20551;&#35774;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24615;&#26469;&#25552;&#39640;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#22312;&#39044;&#27979;&#20248;&#21270;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#20248;&#21270;&#38382;&#39064;&#26159;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#36827;&#34892;&#20915;&#31574;&#21046;&#23450;&#30340;&#26694;&#26550;&#12290;&#23427;&#30340;&#20013;&#24515;&#30740;&#31350;&#38382;&#39064;&#26159;&#65292;&#8220;&#22914;&#20309;&#21033;&#29992;&#20915;&#31574;&#20219;&#21153;&#30340;&#32467;&#26500;&#26469;&#23450;&#21046;&#29305;&#23450;&#20219;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65311;&#8221;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25429;&#25417;&#36825;&#31181;&#28508;&#22312;&#30340;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#23545;&#36825;&#20123;&#25439;&#22833;&#30340;&#24418;&#24335;&#21644;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34892;&#20026;&#30340;&#24433;&#21709;&#20570;&#20986;&#20102;&#38480;&#21046;&#24615;&#30340;&#20551;&#35774;&#12290;&#36825;&#20123;&#20551;&#35774;&#26082;&#23548;&#33268;&#20102;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#20063;&#22312;&#23454;&#36341;&#20013;&#34987;&#36829;&#21453;&#26102;&#23548;&#33268;&#20102;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36991;&#20813;&#20102;&#19978;&#36848;&#20551;&#35774;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24615;&#26469;&#25552;&#39640;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#20174;&#25991;&#29486;&#20013;&#30340;&#22235;&#20010;&#39046;&#22495;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#36890;&#24120;&#38656;&#35201;&#27604;&#21487;&#27604;&#26041;&#27861;&#23569;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predict-then-Optimize is a framework for using machine learning to perform decision-making under uncertainty. The central research question it asks is, "How can the structure of a decision-making task be used to tailor ML models for that specific task?" To this end, recent work has proposed learning task-specific loss functions that capture this underlying structure. However, current approaches make restrictive assumptions about the form of these losses and their impact on ML model behavior. These assumptions both lead to approaches with high computational cost, and when they are violated in practice, poor performance. In this paper, we propose solutions to these issues, avoiding the aforementioned assumptions and utilizing the ML model's features to increase the sample efficiency of learning loss functions. We empirically show that our method achieves state-of-the-art results in four domains from the literature, often requiring an order of magnitude fewer samples than comparable metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HUB&#30340;&#28151;&#21512;&#26356;&#26032;&#31574;&#30053;&#65292;&#36890;&#36807;&#32467;&#21512;&#23398;&#20064;&#20248;&#21270;&#22120;&#21644;&#25163;&#24037;&#35774;&#35745;&#30340;&#20248;&#21270;&#22120;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#20248;&#21270;&#22120;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16823</link><description>&lt;p&gt;
HUB: &#29992;&#25345;&#32493;&#25552;&#31034;&#35843;&#25972;&#24341;&#23548;&#23398;&#20064;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
HUB: Guiding Learned Optimizers with Continuous Prompt Tuning. (arXiv:2305.16823v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HUB&#30340;&#28151;&#21512;&#26356;&#26032;&#31574;&#30053;&#65292;&#36890;&#36807;&#32467;&#21512;&#23398;&#20064;&#20248;&#21270;&#22120;&#21644;&#25163;&#24037;&#35774;&#35745;&#30340;&#20248;&#21270;&#22120;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#20248;&#21270;&#22120;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20248;&#21270;&#22120;&#26159;&#20803;&#23398;&#20064;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#20854;&#22312;&#22788;&#29702;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#21644;&#32593;&#32476;&#26550;&#26500;&#26102;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#26356;&#26032;&#31574;&#30053;&#30340;&#20248;&#21270;&#26041;&#27861;&#65288;HUB&#65289;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#20102;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;&#27169;&#22411;&#20013;&#30828;&#25552;&#31034;&#35843;&#25972;&#21644;&#32467;&#26524;&#36873;&#25321;&#25216;&#26415;&#30340;&#21551;&#21457;&#12290;&#36890;&#36807;&#23558;&#25163;&#24037;&#35774;&#35745;&#30340;&#20248;&#21270;&#22120;&#20316;&#20026;&#25105;&#20204;&#28151;&#21512;&#26041;&#27861;&#30340;&#31532;&#20108;&#20010;&#32452;&#20214;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#31283;&#23450;&#35757;&#32451;&#30340;&#21516;&#26102;&#20445;&#30041;&#23398;&#20064;&#20248;&#21270;&#22120;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learned optimizers are a crucial component of meta-learning. Recent advancements in scalable learned optimizers have demonstrated their superior performance over hand-designed optimizers in various tasks. However, certain characteristics of these models, such as an unstable learning curve, limited ability to handle unseen tasks and network architectures, difficult-to-control behaviours, and poor performance in fine-tuning tasks impede their widespread adoption. To tackle the issue of generalization in scalable learned optimizers, we propose a hybrid-update-based (HUB) optimization strategy inspired by recent advancements in hard prompt tuning and result selection techniques used in large language and vision models. This approach can be easily applied to any task that involves hand-designed or learned optimizer. By incorporating hand-designed optimizers as the second component in our hybrid approach, we are able to retain the benefits of learned optimizers while stabilizing the training
&lt;/p&gt;</description></item><item><title>&#35748;&#35777;&#25216;&#26415;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#20998;&#24067;&#24335;&#31995;&#32479;&#39564;&#35777;&#19981;&#36866;&#29992;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;ML-based&#20998;&#24067;&#24335;&#31995;&#32479;&#35748;&#35777;&#26041;&#26696;&#65292;&#20197;&#39564;&#35777;&#20854;&#38750;&#21151;&#33021;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16822</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#20998;&#24067;&#24335;&#31995;&#32479;&#35748;&#35777;&#20043;&#36335;
&lt;/p&gt;
&lt;p&gt;
Towards Certification of Machine Learning-Based Distributed Systems. (arXiv:2305.16822v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16822
&lt;/p&gt;
&lt;p&gt;
&#35748;&#35777;&#25216;&#26415;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#20998;&#24067;&#24335;&#31995;&#32479;&#39564;&#35777;&#19981;&#36866;&#29992;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;ML-based&#20998;&#24067;&#24335;&#31995;&#32479;&#35748;&#35777;&#26041;&#26696;&#65292;&#20197;&#39564;&#35777;&#20854;&#38750;&#21151;&#33021;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26085;&#30410;&#34987;&#29992;&#20110;&#39537;&#21160;&#37096;&#32626;&#22312;5G&#20113;&#36793;&#32536;&#36830;&#32493;&#20307;&#19978;&#30340;&#22797;&#26434;&#20998;&#24067;&#24335;&#31995;&#32479;&#30340;&#36816;&#34892;&#12290;&#30456;&#24212;&#22320;&#65292;&#20998;&#24067;&#24335;&#31995;&#32479;&#30340;&#34892;&#20026;&#21464;&#24471;&#26356;&#20855;&#38750;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#20998;&#24067;&#24335;&#31995;&#32479;&#30340;&#28436;&#21270;&#38656;&#35201;&#23450;&#20041;&#26032;&#30340;&#20445;&#35777;&#26041;&#27861;&#26469;&#39564;&#35777;&#38750;&#21151;&#33021;&#23646;&#24615;&#12290;&#35748;&#35777;&#20316;&#20026;&#31995;&#32479;&#21644;&#36719;&#20214;&#39564;&#35777;&#30340;&#26368;&#27969;&#34892;&#30340;&#20445;&#35777;&#25216;&#26415;&#65292;&#19981;&#33021;&#31435;&#21363;&#36866;&#29992;&#20110;&#20854;&#34892;&#20026;&#30001;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25512;&#29702;&#20915;&#23450;&#30340;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#25919;&#31574;&#21046;&#23450;&#32773;&#12289;&#30417;&#31649;&#26426;&#26500;&#21644;&#20135;&#19994;&#21033;&#30410;&#30456;&#20851;&#32773;&#36234;&#26469;&#36234;&#25512;&#23815;&#23450;&#20041;ML&#30340;&#38750;&#21151;&#33021;&#23646;&#24615;&#65288;&#22914;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#65289;&#30340;&#35748;&#35777;&#25216;&#26415;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#24403;&#21069;&#35748;&#35777;&#26041;&#26696;&#30340;&#25361;&#25112;&#21644;&#19981;&#36275;&#20043;&#22788;&#65292;&#35752;&#35770;&#20102;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;ML-based&#20998;&#24067;&#24335;&#31995;&#32479;&#35748;&#35777;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) is increasingly used to drive the operation of complex distributed systems deployed on the cloud-edge continuum enabled by 5G. Correspondingly, distributed systems' behavior is becoming more non-deterministic in nature. This evolution of distributed systems requires the definition of new assurance approaches for the verification of non-functional properties. Certification, the most popular assurance technique for system and software verification, is not immediately applicable to systems whose behavior is determined by Machine Learning-based inference. However, there is an increasing push from policy makers, regulators, and industrial stakeholders towards the definition of techniques for the certification of non-functional properties (e.g., fairness, robustness, privacy) of ML. This article analyzes the challenges and deficiencies of current certification schemes, discusses open research issues and proposes a first certification scheme for ML-based distributed syst
&lt;/p&gt;</description></item><item><title>&#36873;&#25321;&#24615;mixup&#36890;&#36807;&#38750;&#38543;&#26426;&#36873;&#25321;&#23545;&#25552;&#39640;&#35757;&#32451;&#20998;&#24067;&#65292;&#23454;&#29616;&#26631;&#31614;&#20559;&#31227;&#30340;&#32463;&#20856;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16817</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#28151;&#21512;&#26377;&#21161;&#20110;&#24212;&#23545;&#20998;&#24067;&#20559;&#31227;&#65292;&#20294;&#19981;&#20165;&#20165;&#26159;&#22240;&#20026;&#28151;&#21512;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Selective Mixup Helps with Distribution Shifts, But Not (Only) because of Mixup. (arXiv:2305.16817v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16817
&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#24615;mixup&#36890;&#36807;&#38750;&#38543;&#26426;&#36873;&#25321;&#23545;&#25552;&#39640;&#35757;&#32451;&#20998;&#24067;&#65292;&#23454;&#29616;&#26631;&#31614;&#20559;&#31227;&#30340;&#32463;&#20856;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixup&#26159;&#19968;&#31181;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#30340;&#39640;&#24230;&#25104;&#21151;&#30340;&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#38543;&#26426;&#37197;&#23545;&#30340;&#32452;&#21512;&#26469;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#12290;&#36873;&#25321;&#24615;mixup&#26159;&#19968;&#31995;&#21015;&#23558;mixup&#24212;&#29992;&#20110;&#29305;&#23450;&#23545;&#30340;&#26041;&#27861;&#65292;&#20363;&#22914;&#20165;&#22312;&#31867;&#21035;&#25110;&#39046;&#22495;&#20043;&#38388;&#32452;&#21512;&#31034;&#20363;&#12290;&#36825;&#20123;&#26041;&#27861;&#22768;&#31216;&#22312;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#26377;&#26174;&#30528;&#30340;&#25552;&#39640;&#65292;&#20294;&#23427;&#20204;&#30340;&#26426;&#21046;&#21644;&#38480;&#21046;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36873;&#25321;&#24615;mixup&#30340;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#26041;&#38754;&#65292;&#20174;&#19968;&#20010;&#20840;&#26032;&#30340;&#35282;&#24230;&#35299;&#37322;&#20102;&#23427;&#30340;&#25104;&#21151;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38750;&#38543;&#26426;&#36873;&#25321;&#23545;&#20250;&#24433;&#21709;&#35757;&#32451;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#19982;&#28151;&#21512;&#25216;&#26415;&#23436;&#20840;&#26080;&#20851;&#30340;&#26041;&#24335;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#65292;&#31867;&#21035;&#20043;&#38388;&#30340;mixup&#38544;&#21547;&#22320;&#23545;&#25968;&#25454;&#36827;&#34892;&#37325;&#37319;&#26679;&#65292;&#20197;&#23454;&#29616;&#26631;&#31614;&#20559;&#31227;&#30340;&#32463;&#20856;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#32463;&#39564;&#35777;&#23454;&#65292;&#36825;&#31181;&#38544;&#21547;&#37325;&#37319;&#26679;&#35299;&#37322;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#22823;&#37096;&#20998;&#25913;&#36827;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#36825;&#20123;&#32467;&#26524;&#20381;&#36182;&#20110;&#19968;&#20010;&#22238;&#24402;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#38656;&#35201;&#21306;&#20998;&#30495;&#27491;&#30340;&#37325;&#37319;&#26679;&#21644;&#28151;&#21512;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixup is a highly successful technique to improve generalization of neural networks by augmenting the training data with combinations of random pairs. Selective mixup is a family of methods that apply mixup to specific pairs, e.g. only combining examples across classes or domains. These methods have claimed remarkable improvements on benchmarks with distribution shifts, but their mechanisms and limitations remain poorly understood.  We examine an overlooked aspect of selective mixup that explains its success in a completely new light. We find that the non-random selection of pairs affects the training distribution and improve generalization by means completely unrelated to the mixing. For example in binary classification, mixup across classes implicitly resamples the data for a uniform class distribution a classical solution to label shift. We show empirically that this implicit resampling explains much of the improvements in prior work. Theoretically, these results rely on a regress
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26500;&#24314;&#20102;&#19968;&#20010;&#20989;&#23376;&#23558;&#32467;&#35770;&#36716;&#21270;&#20026;&#22270;&#24418;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#32467;&#35770;&#19981;&#21464;&#37327;&#65292;&#20855;&#26377;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.16808</link><description>&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#32467;&#35770;&#29702;&#35770;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Geometric deep learning approach to knot theory. (arXiv:2305.16808v1 [math.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26500;&#24314;&#20102;&#19968;&#20010;&#20989;&#23376;&#23558;&#32467;&#35770;&#36716;&#21270;&#20026;&#22270;&#24418;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#32467;&#35770;&#19981;&#21464;&#37327;&#65292;&#20855;&#26377;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#20989;&#23376;&#23558;&#32467;&#26500;&#36716;&#21270;&#20026;&#22270;&#24418;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#22788;&#29702;&#32467;&#35770;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#23581;&#35797;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#39044;&#27979;&#20960;&#31181;&#32467;&#35770;&#19981;&#21464;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#36739;&#39640;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel way to use geometric deep learning for knot data by constructing a functor that takes knots to graphs and using graph neural networks. We will attempt to predict several knot invariants with this approach. This approach demonstrates high generalization capabilities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#31163;&#25955;&#21270;&#20559;&#24046;&#21644;&#21033;&#26222;&#24076;&#33576;&#20989;&#25968;&#36924;&#36817;&#35823;&#24046;&#65292;&#24471;&#21040;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#19982;&#36125;&#21494;&#26031;&#26368;&#20248;&#39118;&#38505;&#30340;&#27867;&#21270;&#24046;&#36317;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2305.16791</link><description>&lt;p&gt;
&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;&#27867;&#21270;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Generalization Capacities of Neural Controlled Differential Equations. (arXiv:2305.16791v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#31163;&#25955;&#21270;&#20559;&#24046;&#21644;&#21033;&#26222;&#24076;&#33576;&#20989;&#25968;&#36924;&#36817;&#35823;&#24046;&#65292;&#24471;&#21040;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#19982;&#36125;&#21494;&#26031;&#26368;&#20248;&#39118;&#38505;&#30340;&#27867;&#21270;&#24046;&#36317;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#65288;Kidger&#65292;Morrill&#31561;&#65292;2020&#65289;&#20174;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#20013;&#39044;&#27979;&#32467;&#26524;&#30340;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#26102;&#38388;&#24207;&#21015;&#26159;&#19968;&#20010;&#26410;&#35266;&#23519;&#21040;&#30340;&#36830;&#32493;&#36335;&#24452;&#30340;&#31163;&#25955;&#21270;&#65292;&#32467;&#26524;&#36890;&#36807;&#19968;&#20010;&#20855;&#26377;&#26410;&#30693;&#21521;&#37327;&#22330;&#30340;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#20381;&#36182;&#20110;&#36825;&#20010;&#36335;&#24452;&#12290;&#20351;&#29992;&#31163;&#25955;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#20250;&#24341;&#20837;&#31163;&#25955;&#20559;&#24046;&#65292;&#25105;&#20204;&#31934;&#30830;&#22320;&#37327;&#21270;&#20102;&#36825;&#31181;&#20559;&#24046;&#12290;&#36890;&#36807;&#20351;&#29992;&#20851;&#20110;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#27969;&#30340;&#36830;&#32493;&#24615;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36924;&#36817;&#20559;&#24046;&#30452;&#25509;&#19982;&#30001;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#23450;&#20041;&#29983;&#25104;&#27169;&#22411;&#30340;&#21033;&#26222;&#24076;&#33576;&#20989;&#25968;&#30340;&#36924;&#36817;&#35823;&#24046;&#30456;&#20851;&#12290;&#36890;&#36807;&#32467;&#21512;&#26368;&#36817;&#30340;&#24037;&#20316;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#19982;&#20854;&#27867;&#21270;&#33021;&#21147;&#32852;&#31995;&#36215;&#26469;&#65292;&#25105;&#20204;&#19978;&#30028;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#36798;&#21040;&#30340;&#26399;&#26395;&#25439;&#22833;&#19982;&#36125;&#21494;&#26031;&#26368;&#20248;&#39118;&#38505;&#20043;&#38388;&#30340;&#27867;&#21270;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a supervised learning setup in which the goal is to predicts an outcome from a sample of irregularly sampled time series using Neural Controlled Differential Equations (Kidger, Morrill, et al. 2020). In our framework, the time series is a discretization of an unobserved continuous path, and the outcome depends on this path through a controlled differential equation with unknown vector field. Learning with discrete data thus induces a discretization bias, which we precisely quantify. Using theoretical results on the continuity of the flow of controlled differential equations, we show that the approximation bias is directly related to the approximation error of a Lipschitz function defining the generative model by a shallow neural network. By combining these result with recent work linking the Lipschitz constant of neural networks to their generalization capacities, we upper bound the generalization gap between the expected loss attained by the empirical risk minimizer and th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35889;&#21464;&#25442;&#65288;ST&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#35843;&#33410;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39057;&#35889;&#65292;&#24182;&#36991;&#20813;&#29305;&#24449;&#23849;&#28291;&#12290;&#20854;&#20013;&#65292;INTL&#26159;ST&#30340;&#19968;&#20010;&#23454;&#20363;&#65292;&#33021;&#22815;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#23558;&#23884;&#20837;&#30340;&#39057;&#35889;&#35843;&#33410;&#21040;&#19968;&#20010;&#31561;&#29305;&#24449;&#20540;&#20998;&#24067;&#65292;&#23454;&#29616;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.16789</link><description>&lt;p&gt;
&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#35843;&#33410;&#39057;&#35889;
&lt;/p&gt;
&lt;p&gt;
Modulate Your Spectrum in Self-Supervised Learning. (arXiv:2305.16789v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35889;&#21464;&#25442;&#65288;ST&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#35843;&#33410;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39057;&#35889;&#65292;&#24182;&#36991;&#20813;&#29305;&#24449;&#23849;&#28291;&#12290;&#20854;&#20013;&#65292;INTL&#26159;ST&#30340;&#19968;&#20010;&#23454;&#20363;&#65292;&#33021;&#22815;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#23558;&#23884;&#20837;&#30340;&#39057;&#35889;&#35843;&#33410;&#21040;&#19968;&#20010;&#31561;&#29305;&#24449;&#20540;&#20998;&#24067;&#65292;&#23454;&#29616;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30333;&#21270;&#25439;&#22833;&#20026;&#20351;&#29992;&#32852;&#21512;&#23884;&#20837;&#26550;&#26500;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#36991;&#20813;&#20102;&#29305;&#24449;&#23849;&#28291;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35889;&#21464;&#25442;&#65288;ST&#65289;&#26694;&#26550;&#65292;&#22312;&#21069;&#21521;&#20256;&#36882;&#36807;&#31243;&#20013;&#23558;&#23884;&#20837;&#30340;&#39057;&#35889;&#26144;&#23556;&#21040;&#25152;&#38656;&#30340;&#20998;&#24067;&#65292;&#24182;&#22312;&#21453;&#21521;&#20256;&#36882;&#36807;&#31243;&#20013;&#36890;&#36807;&#38544;&#24335;&#26799;&#24230;&#26356;&#26032;&#26469;&#35843;&#21046;&#23884;&#20837;&#30340;&#39057;&#35889;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#30333;&#21270;&#21464;&#25442;&#26159;ST&#30340;&#19968;&#20010;&#29305;&#20363;&#65292;&#36824;&#26377;&#20854;&#20182;&#23454;&#20363;&#21487;&#20197;&#36991;&#20813;&#23849;&#28291;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;INTL&#65288;IterNorm with trace loss&#65289;&#30340;&#26032;&#23454;&#20363;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;INTL&#21487;&#20197;&#36991;&#20813;&#23849;&#28291;&#65292;&#24182;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#23558;&#23884;&#20837;&#30340;&#39057;&#35889;&#35843;&#33410;&#21040;&#19968;&#20010;&#31561;&#29305;&#24449;&#20540;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;INTL&#23454;&#29616;&#20102;76.6&#65285;&#30340;&#26368;&#39640;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whitening loss provides theoretical guarantee in avoiding feature collapse for self-supervised learning (SSL) using joint embedding architectures. One typical implementation of whitening loss is hard whitening that designs whitening transformation over embedding and imposes the loss on the whitened output. In this paper, we propose spectral transformation (ST) framework to map the spectrum of embedding to a desired distribution during forward pass, and to modulate the spectrum of embedding by implicit gradient update during backward pass. We show that whitening transformation is a special instance of ST by definition, and there exist other instances that can avoid collapse by our empirical investigation. Furthermore, we propose a new instance of ST, called IterNorm with trace loss (INTL). We theoretically prove that INTL can avoid collapse and modulate the spectrum of embedding towards an equal-eigenvalue distribution during the course of optimization. Moreover, INTL achieves 76.6% top
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;'RSTformer'&#30340;&#25688;&#35201;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20840;&#38754;&#34701;&#21512;&#20102;&#35805;&#35821;&#20851;&#31995;&#31867;&#22411;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20197;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#20026;&#22522;&#30784;&#65292;&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#65292;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.16784</link><description>&lt;p&gt;
&#32467;&#21512;&#35805;&#35821;&#32467;&#26500;&#20998;&#24067;&#30340;&#38271;&#25991;&#26412;&#33258;&#21160;&#25688;&#35201;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Incorporating Distributions of Discourse Structure for Long Document Abstractive Summarization. (arXiv:2305.16784v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;'RSTformer'&#30340;&#25688;&#35201;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20840;&#38754;&#34701;&#21512;&#20102;&#35805;&#35821;&#20851;&#31995;&#31867;&#22411;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20197;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#20026;&#22522;&#30784;&#65292;&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#65292;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#25991;&#26412;&#25688;&#35201;&#65292;&#35805;&#35821;&#32467;&#26500;&#22312;&#36776;&#35782;&#25991;&#26412;&#26680;&#24515;&#20869;&#23481;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#21487;&#24796;&#30340;&#26159;&#65292;&#20043;&#21069;&#23558;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#65288;RST&#65289;&#24341;&#20837;&#22522;&#20110;transformer&#30340;&#33258;&#21160;&#25688;&#35201;&#27169;&#22411;&#30340;&#30740;&#31350;&#20165;&#32771;&#34385;&#20102;&#26680;&#24515;&#37096;&#20998;&#30340;&#27880;&#37322;&#65292;&#20174;&#32780;&#24573;&#30053;&#20102;&#21508;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#35805;&#35821;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;'RSTformer'&#30340;&#26032;&#22411;&#25688;&#35201;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20840;&#38754;&#34701;&#21512;&#20102;&#35805;&#35821;&#20851;&#31995;&#31867;&#22411;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;RST-attention&#26426;&#21046;&#26159;&#22522;&#20110;&#25991;&#26723;&#32423;&#20462;&#36766;&#32467;&#26500;&#30340;Longformer&#26694;&#26550;&#30340;&#25193;&#23637;&#12290;&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#27169;&#22411;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#20984;&#26174;&#20986;&#20854;&#22312;&#22810;&#20010;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#24037;&#35780;&#20272;&#19978;&#30340;&#21331;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
For text summarization, the role of discourse structure is pivotal in discerning the core content of a text. Regrettably, prior studies on incorporating Rhetorical Structure Theory (RST) into transformer-based summarization models only consider the nuclearity annotation, thereby overlooking the variety of discourse relation types. This paper introduces the 'RSTformer', a novel summarization model that comprehensively incorporates both the types and uncertainty of rhetorical relations. Our RST-attention mechanism, rooted in document-level rhetorical structure, is an extension of the recently devised Longformer framework. Through rigorous evaluation, the model proposed herein exhibits significant superiority over state-of-the-art models, as evidenced by its notable performance on several automatic metrics and human evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#20102;&#24322;&#36136;&#24615;&#21407;&#21017;&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#20351;&#29992;&#23545;&#27969;&#25193;&#25955;&#26041;&#31243;&#23545;&#33410;&#28857;&#19978;&#30340;&#20449;&#24687;&#27969;&#36827;&#34892;&#24314;&#27169;&#65292;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#22522;&#20110;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#30340;&#20449;&#24687;&#20256;&#36882;&#65292;&#22312;&#22788;&#29702;&#24322;&#36136;&#24615;&#22270;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.16780</link><description>&lt;p&gt;
&#20855;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#31070;&#32463;&#23545;&#27969;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Convection-Diffusion with Heterophily. (arXiv:2305.16780v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#20102;&#24322;&#36136;&#24615;&#21407;&#21017;&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#20351;&#29992;&#23545;&#27969;&#25193;&#25955;&#26041;&#31243;&#23545;&#33410;&#28857;&#19978;&#30340;&#20449;&#24687;&#27969;&#36827;&#34892;&#24314;&#27169;&#65292;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#22522;&#20110;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#30340;&#20449;&#24687;&#20256;&#36882;&#65292;&#22312;&#22788;&#29702;&#24322;&#36136;&#24615;&#22270;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#32463;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24456;&#39640;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#23427;&#20204;&#36890;&#24120;&#20551;&#35774;&#21516;&#36136;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#23545;&#24322;&#36136;&#24615;&#22270;&#30340;&#24615;&#33021;&#34920;&#29616;&#36739;&#24046;&#12290;&#36890;&#36807;&#20351;&#29992;&#23545;&#27969;&#25193;&#25955;&#26041;&#31243;&#65288;CDE&#65289;&#23545;&#33410;&#28857;&#19978;&#30340;&#20449;&#24687;&#27969;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GNN&#65292;&#35813;&#27169;&#22411;&#34701;&#21512;&#20102;&#24322;&#36136;&#24615;&#21407;&#21017;&#12290;&#36825;&#20351;&#24471;CDE&#33021;&#22815;&#32771;&#34385;&#21040;&#22522;&#20110;&#21516;&#36136;&#24615;&#30340;&#20449;&#24687;&#25193;&#25955;&#21644;&#22522;&#20110;&#24322;&#36136;&#24615;&#30340;&#20449;&#24687;&#8220;&#23545;&#27969;&#8221;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#34920;&#26126;&#30456;&#27604;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#38024;&#23545;&#24322;&#36136;&#24615;&#22270;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#33021;&#22815;&#23454;&#29616;&#31454;&#20105;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312; \url{https://github.com/zknus/Graph-Diffusion-CDE} &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have shown promising results across various graph learning tasks, but they often assume homophily, which can result in poor performance on heterophilic graphs. The connected nodes are likely to be from different classes or have dissimilar features on heterophilic graphs. In this paper, we propose a novel GNN that incorporates the principle of heterophily by modeling the flow of information on nodes using the convection-diffusion equation (CDE). This allows the CDE to take into account both the diffusion of information due to homophily and the ``convection'' of information due to heterophily. We conduct extensive experiments, which suggest that our framework can achieve competitive performance on node classification tasks for heterophilic graphs, compared to the state-of-the-art methods. The code is available at \url{https://github.com/zknus/Graph-Diffusion-CDE}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#27602;&#21270;&#25915;&#20987;&#30340;&#40065;&#26834;&#38750;&#21442;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;Huber&#25439;&#22833;&#30340;M-&#35780;&#20272;&#22120;&#21644;&#36890;&#36807;&#23558;&#21021;&#22987;&#20272;&#35745;&#25237;&#24433;&#21040;Lipschitz&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#26657;&#27491;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27491;&#30830;&#36873;&#25321;&#24102;&#23485;&#26102;$\ell_\infty $&#35823;&#24046;&#26159;&#26497;&#23567;&#21270;&#26368;&#20248;&#30340;&#65292;&#32780;$\ell_2 $&#35823;&#24046;&#22312;$q\lesssim \sqrt{N/\ln^2 N}$&#26102;&#26368;&#20248;&#12290;</title><link>http://arxiv.org/abs/2305.16771</link><description>&lt;p&gt;
&#27602;&#21270;&#25915;&#20987;&#19979;&#30340;&#40065;&#26834;&#38750;&#21442;&#25968;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Robust Nonparametric Regression under Poisoning Attack. (arXiv:2305.16771v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#27602;&#21270;&#25915;&#20987;&#30340;&#40065;&#26834;&#38750;&#21442;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;Huber&#25439;&#22833;&#30340;M-&#35780;&#20272;&#22120;&#21644;&#36890;&#36807;&#23558;&#21021;&#22987;&#20272;&#35745;&#25237;&#24433;&#21040;Lipschitz&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#26657;&#27491;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27491;&#30830;&#36873;&#25321;&#24102;&#23485;&#26102;$\ell_\infty $&#35823;&#24046;&#26159;&#26497;&#23567;&#21270;&#26368;&#20248;&#30340;&#65292;&#32780;$\ell_2 $&#35823;&#24046;&#22312;$q\lesssim \sqrt{N/\ln^2 N}$&#26102;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#40065;&#26834;&#38750;&#21442;&#25968;&#22238;&#24402;&#65292;&#22312;&#36825;&#31181;&#22238;&#24402;&#20013;&#65292;&#23545;&#25239;&#24615;&#25915;&#20987;&#32773;&#21487;&#20197;&#20462;&#25913;&#26469;&#33258;&#22823;&#23567;&#20026;N&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#26368;&#22810;q&#20010;&#26679;&#26412;&#30340;&#20540;&#12290;&#25105;&#20204;&#30340;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#26159;&#22522;&#20110;Huber&#25439;&#22833;&#26368;&#23567;&#21270;&#30340;M-&#35780;&#20272;&#22120;&#12290;&#19982;&#31616;&#21333;&#30340;&#26680;&#22238;&#24402;&#65288;&#21363;Nadaraya-Watson&#20272;&#35745;&#65289;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#20943;&#24369;&#24694;&#24847;&#26679;&#26412;&#23545;&#22238;&#24402;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25910;&#25947;&#36895;&#29575;&#20197;&#21450;&#30456;&#24212;&#30340;&#26497;&#23567;&#21270;&#19979;&#30028;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#27491;&#30830;&#36873;&#25321;&#24102;&#23485;&#65292;$\ell_\infty $&#35823;&#24046;&#26159;&#26497;&#23567;&#21270;&#26368;&#20248;&#30340;&#12290;&#22914;&#26524;$q\lesssim \sqrt{N/\ln^2 N}$&#65292;&#21017;$\ell_2 $&#35823;&#24046;&#26159;&#26368;&#20248;&#30340;&#65292;&#20294;&#26159;&#22914;&#26524;$q$&#26356;&#22823;&#65292;&#21017;&#26159;&#27425;&#20248;&#30340;&#12290;&#21407;&#22240;&#26159;&#22914;&#26524;&#26377;&#35768;&#22810;&#34987;&#25915;&#20987;&#30340;&#26679;&#26412;&#38598;&#20013;&#22312;&#19968;&#20010;&#23567;&#21306;&#22495;&#20013;&#65292;&#36825;&#20010;&#20272;&#35745;&#37327;&#23601;&#20250;&#24456;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26657;&#27491;&#26041;&#27861;&#65292;&#23558;&#21021;&#22987;&#20272;&#35745;&#25237;&#24433;&#21040;Lipschitz&#20989;&#25968;&#31354;&#38388;&#20013;&#12290;&#26368;&#32456;&#30340;&#20272;&#35745;&#20540;&#20960;&#20046;&#26159;&#20219;&#24847;$q$&#30340;&#26497;&#23567;&#21270;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies robust nonparametric regression, in which an adversarial attacker can modify the values of up to $q$ samples from a training dataset of size $N$. Our initial solution is an M-estimator based on Huber loss minimization. Compared with simple kernel regression, i.e. the Nadaraya-Watson estimator, this method can significantly weaken the impact of malicious samples on the regression performance. We provide the convergence rate as well as the corresponding minimax lower bound. The result shows that, with proper bandwidth selection, $\ell_\infty$ error is minimax optimal. The $\ell_2$ error is optimal if $q\lesssim \sqrt{N/\ln^2 N}$, but is suboptimal with larger $q$. The reason is that this estimator is vulnerable if there are many attacked samples concentrating in a small region. To address this issue, we propose a correction method by projecting the initial estimate to the space of Lipschitz functions. The final estimate is nearly minimax optimal for arbitrary $q$, up t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20154;&#36947;&#20027;&#20041;&#26412;&#20307;&#20026;&#22522;&#30784;&#30340;&#26032;&#22411;&#35821;&#35328;&#27169;&#22411;HumBert&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#21644;&#20943;&#23569;&#20559;&#35265;&#65292;&#20197;&#23454;&#29616;&#23545;&#20154;&#36947;&#20027;&#20041;&#25968;&#25454;&#20998;&#26512;&#30340;&#26377;&#25928;&#21644;&#36947;&#24503;&#24847;&#35782;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2305.16756</link><description>&lt;p&gt;
&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#23454;&#29616;&#21253;&#23481;&#21644;&#20559;&#35265;&#24863;&#30693;&#30340;&#20154;&#36947;&#20027;&#20041;&#21709;&#24212;&#20837;&#21475;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Leveraging Domain Knowledge for Inclusive and Bias-aware Humanitarian Response Entry Classification. (arXiv:2305.16756v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20154;&#36947;&#20027;&#20041;&#26412;&#20307;&#20026;&#22522;&#30784;&#30340;&#26032;&#22411;&#35821;&#35328;&#27169;&#22411;HumBert&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#21644;&#20943;&#23569;&#20559;&#35265;&#65292;&#20197;&#23454;&#29616;&#23545;&#20154;&#36947;&#20027;&#20041;&#25968;&#25454;&#20998;&#26512;&#30340;&#26377;&#25928;&#21644;&#36947;&#24503;&#24847;&#35782;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#36947;&#20027;&#20041;&#21361;&#26426;&#26399;&#38388;&#65292;&#20934;&#30830;&#21644;&#24555;&#36895;&#30340;&#24773;&#20917;&#20998;&#26512;&#23545;&#20110;&#39640;&#25928;&#22320;&#25552;&#20379;&#20154;&#36947;&#20027;&#20041;&#25588;&#21161;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#26159;&#20154;&#36947;&#20027;&#20041;&#21407;&#21017;&#21644;&#19981;&#30041;&#20219;&#20309;&#20154;&#33853;&#21518;&#21407;&#21017;&#30340;&#22522;&#30784;&#12290;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#21487;&#20197;&#26497;&#22823;&#22320;&#21463;&#30410;&#20110;&#36825;&#31181;&#25968;&#25454;&#20998;&#26512;&#65292;&#20363;&#22914;&#65292;&#25353;&#29031;&#20154;&#36947;&#20027;&#20041;&#26412;&#20307;&#23545;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#36890;&#36807;&#24494;&#35843;&#36890;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#26469;&#23454;&#29616;&#65292;&#28041;&#21450;&#19968;&#20123;&#23454;&#36341;&#21644;&#36947;&#24503;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#31232;&#30095;&#21644;&#22797;&#26434;&#23376;&#39046;&#22495;&#19978;&#30340;&#25928;&#26524;&#19981;&#20339;&#20197;&#21450;&#31038;&#20250;&#20559;&#35265;&#21644;&#19981;&#33391;&#20851;&#32852;&#30340;&#32534;&#30721;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#20154;&#36947;&#20027;&#20041;&#25968;&#25454;&#20998;&#26512;&#25552;&#20379;&#19968;&#31181;&#26377;&#25928;&#21644;&#36947;&#24503;&#24847;&#35782;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#36890;&#36807; (1) &#24341;&#20837;&#19968;&#20010;&#36866;&#21512;&#20154;&#36947;&#20027;&#20041;&#20998;&#26512;&#26694;&#26550;&#30340;&#26032;&#26550;&#26500;&#65292;(2) &#21019;&#24314;&#21644;&#21457;&#24067;&#19968;&#20010;&#26032;&#30340;&#20154;&#36947;&#20027;&#20041;&#29305;&#23450; LLM&#65292;&#31216;&#20026; HumBert&#65292;&#24182;&#19988; (3) &#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#24335;&#26469;&#34913;&#37327;&#21644;&#20943;&#23569;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate and rapid situation analysis during humanitarian crises is critical to delivering humanitarian aid efficiently and is fundamental to humanitarian imperatives and the Leave No One Behind (LNOB) principle. This data analysis can highly benefit from language processing systems, e.g., by classifying the text data according to a humanitarian ontology. However, approaching this by simply fine-tuning a generic large language model (LLM) involves considerable practical and ethical issues, particularly the lack of effectiveness on data-sparse and complex subdomains, and the encoding of societal biases and unwanted associations. In this work, we aim to provide an effective and ethically-aware system for humanitarian data analysis. We approach this by (1) introducing a novel architecture adjusted to the humanitarian analysis framework, (2) creating and releasing a novel humanitarian-specific LLM called HumBert, and (3) proposing a systematic way to measure and mitigate biases. Our experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31163;&#25955;&#21608;&#30028;&#38450;&#24481;&#38382;&#39064;&#30340;&#20998;&#25955;&#33033;&#20914;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22242;&#38431;&#38450;&#24481;&#32773;&#20445;&#25252;&#39046;&#22303;&#30340;&#38382;&#39064;&#65292;&#27599;&#20010;&#38450;&#24481;&#32773;&#21547;&#26377;&#33258;&#24049;&#30340;MLC-SEFRON&#32593;&#32476;&#65292;&#20174;&#32780;&#23454;&#29616;&#20998;&#25955;&#29420;&#31435;&#35757;&#32451;&#65292;&#36755;&#20837;&#20449;&#24687;&#26469;&#28304;&#20110;&#38450;&#24481;&#32773;&#21644;&#20837;&#20405;&#32773;&#30340;&#26102;&#31354;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.16748</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#31163;&#25955;&#21608;&#30028;&#38450;&#24481;&#38382;&#39064;&#30340;&#20998;&#25955;&#33033;&#20914;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Decentralized Spike-based Learning Framework for Sequential Capture in Discrete Perimeter Defense Problem. (arXiv:2305.16748v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31163;&#25955;&#21608;&#30028;&#38450;&#24481;&#38382;&#39064;&#30340;&#20998;&#25955;&#33033;&#20914;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22242;&#38431;&#38450;&#24481;&#32773;&#20445;&#25252;&#39046;&#22303;&#30340;&#38382;&#39064;&#65292;&#27599;&#20010;&#38450;&#24481;&#32773;&#21547;&#26377;&#33258;&#24049;&#30340;MLC-SEFRON&#32593;&#32476;&#65292;&#20174;&#32780;&#23454;&#29616;&#20998;&#25955;&#29420;&#31435;&#35757;&#32451;&#65292;&#36755;&#20837;&#20449;&#24687;&#26469;&#28304;&#20110;&#38450;&#24481;&#32773;&#21644;&#20837;&#20405;&#32773;&#30340;&#26102;&#31354;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#25955;&#33033;&#20914;&#23398;&#20064;&#65288;DSL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#31163;&#25955;&#21608;&#30028;&#38450;&#24481;&#38382;&#39064;&#65288;d-PDP&#65289;&#12290;&#22242;&#38431;&#25805;&#20316;&#38450;&#24481;&#32773;&#29992;&#20110;&#20445;&#25252;&#22278;&#24418;&#39046;&#22303;&#20813;&#21463;&#36752;&#23556;&#24615;&#20837;&#20405;&#32773;&#30340;&#25915;&#20987;&#12290;&#39318;&#20808;&#65292;&#23558;&#31163;&#25955;&#21608;&#30028;&#38450;&#24481;&#38382;&#39064;&#65288;d-PDP&#65289;&#34920;&#36848;&#20026;&#26102;&#31354;&#22810;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#65288;STMTA&#65289;&#12290;&#28982;&#21518;&#23558;STMTA&#38382;&#39064;&#36716;&#25442;&#20026;&#22810;&#26631;&#31614;&#23398;&#20064;&#38382;&#39064;&#65292;&#20197;&#33719;&#21462;&#38450;&#24481;&#32773;&#24517;&#39035;&#35775;&#38382;&#30340;&#21306;&#27573;&#26631;&#31614;&#65292;&#20197;&#20415;&#20445;&#25252;&#21608;&#30028;&#12290;DSL&#26694;&#26550;&#20351;&#29992;Multi-Label Classifier Using Synaptic Efficacy Function Spiking NeuRON&#65288;MLC-SEFRON&#65289;&#32593;&#32476;&#36827;&#34892;&#30830;&#23450;&#24615;&#22810;&#26631;&#31614;&#23398;&#20064;&#12290;&#27599;&#20010;&#38450;&#24481;&#32773;&#37117;&#21253;&#21547;&#21333;&#20010;MLC-SEFRON&#32593;&#32476;&#65292;&#27599;&#20010;MLC-SEFRON&#32593;&#32476;&#37117;&#29420;&#31435;&#35757;&#32451;&#65292;&#20351;&#29992;&#20854;&#33258;&#36523;&#36879;&#35270;&#22270;&#30340;&#36755;&#20837;&#36827;&#34892;&#20998;&#25955;&#25805;&#20316;&#12290; MLC-SEFRON&#32593;&#32476;&#30340;&#36755;&#20837;&#33033;&#20914;&#21487;&#20197;&#30452;&#25509;&#20174;&#38450;&#24481;&#32773;&#21644;&#20837;&#20405;&#32773;&#30340;&#26102;&#31354;&#20449;&#24687;&#20013;&#33719;&#24471;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#39044;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel Decentralized Spike-based Learning (DSL) framework for the discrete Perimeter Defense Problem (d-PDP). A team of defenders is operating on the perimeter to protect the circular territory from radially incoming intruders. At first, the d-PDP is formulated as a spatio-temporal multi-task assignment problem (STMTA). The problem of STMTA is then converted into a multi-label learning problem to obtain labels of segments that defenders have to visit in order to protect the perimeter. The DSL framework uses a Multi-Label Classifier using Synaptic Efficacy Function spiking neuRON (MLC-SEFRON) network for deterministic multi-label learning. Each defender contains a single MLC-SEFRON network. Each MLC-SEFRON network is trained independently using input from its own perspective for decentralized operations. The input spikes to the MLC-SEFRON network can be directly obtained from the spatio-temporal information of defenders and intruders without any extra pre-processing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#20197;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#24335;&#29983;&#25104;&#31232;&#30095;&#25513;&#30721;&#65292;&#26080;&#38656;&#28155;&#21152;&#26032;&#21442;&#25968;&#65292;&#36991;&#20813;&#20102;&#39069;&#22806;&#30340;&#25512;&#26029;&#24310;&#36831;&#65292;&#24182;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16742</link><description>&lt;p&gt;
&#26080;&#38656;&#24341;&#20837;&#26032;&#30340;&#24310;&#36831;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning without Introducing New Latency. (arXiv:2305.16742v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#20197;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#24335;&#29983;&#25104;&#31232;&#30095;&#25513;&#30721;&#65292;&#26080;&#38656;&#28155;&#21152;&#26032;&#21442;&#25968;&#65292;&#36991;&#20813;&#20102;&#39069;&#22806;&#30340;&#25512;&#26029;&#24310;&#36831;&#65292;&#24182;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26368;&#36817;&#23637;&#31034;&#20986;&#26126;&#26174;&#30340;&#25104;&#23601;&#65292;&#26377;&#25928;&#22320;&#21305;&#37197;&#20102;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21033;&#29992;&#26126;&#26174;&#26356;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#22240;&#27492;&#35299;&#20915;&#20102;&#23384;&#20648;&#21644;&#36890;&#20449;&#38480;&#21046;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#21508;&#31181;PEFT&#26041;&#27861;&#20173;&#21463;&#20854;&#22266;&#26377;&#29305;&#24615;&#30340;&#38480;&#21046;&#12290;&#22312;&#31232;&#30095;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#21482;&#28041;&#21450;&#20462;&#25913;&#29616;&#26377;&#21442;&#25968;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#24494;&#35843;&#21442;&#25968;&#30340;&#36873;&#25321;&#26159;&#20219;&#21153;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#65292;&#22240;&#27492;&#19981;&#36866;&#29992;&#20110;&#32852;&#21512;&#23398;&#20064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#28155;&#21152;&#26032;&#21442;&#25968;&#30340;PEFT&#26041;&#27861;&#36890;&#24120;&#20250;&#24341;&#20837;&#39069;&#22806;&#30340;&#25512;&#26029;&#24310;&#36831;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20197;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#24335;&#29983;&#25104;&#31232;&#30095;&#25513;&#30721;&#30340;&#21487;&#34892;&#24615;&#65292;&#20854;&#20013;&#25152;&#26377;&#19979;&#28216;&#20219;&#21153;&#20849;&#20139;&#30456;&#21516;&#30340;&#25513;&#30721;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#21442;&#25968;&#30340;&#24133;&#24230;&#20449;&#24687;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#23398;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) of pre-trained language models has recently demonstrated remarkable achievements, effectively matching the performance of full fine-tuning while utilizing significantly fewer trainable parameters, and consequently addressing the storage and communication constraints. Nonetheless, various PEFT methods are limited by their inherent characteristics. In the case of sparse fine-tuning, which involves modifying only a small subset of the existing parameters, the selection of fine-tuned parameters is task- and domain-specific, making it unsuitable for federated learning. On the other hand, PEFT methods with adding new parameters typically introduce additional inference latency. In this paper, we demonstrate the feasibility of generating a sparse mask in a task-agnostic manner, wherein all downstream tasks share a common mask. Our approach, which relies solely on the magnitude information of pre-trained parameters, surpasses existing methodologies by a si
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#35780;&#20272;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#35777;&#26126;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26377;&#33021;&#21147;&#24456;&#22909;&#22320;&#22797;&#21046;&#21407;&#22987;&#26102;&#38388;&#24207;&#21015;&#30340;&#28151;&#27788;&#29305;&#24615;&#65292;&#20294;&#35823;&#24046;&#20998;&#26512;&#34920;&#26126;&#20173;&#28982;&#23384;&#22312;&#22823;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.16729</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#30340;&#35780;&#20215;
&lt;/p&gt;
&lt;p&gt;
Evaluating generation of chaotic time series by convolutional generative adversarial networks. (arXiv:2305.16729v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16729
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#35780;&#20272;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#35777;&#26126;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26377;&#33021;&#21147;&#24456;&#22909;&#22320;&#22797;&#21046;&#21407;&#22987;&#26102;&#38388;&#24207;&#21015;&#30340;&#28151;&#27788;&#29305;&#24615;&#65292;&#20294;&#35823;&#24046;&#20998;&#26512;&#34920;&#26126;&#20173;&#28982;&#23384;&#22312;&#22823;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20102;&#35299;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#31867;&#20284;&#20110;&#22797;&#26434;&#26102;&#38388;&#20449;&#21495;&#30340;&#26102;&#38388;&#24207;&#21015;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#30001;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#32452;&#25104;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26469;&#29983;&#25104;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#38750;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26469;&#35780;&#20272;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#30830;&#23450;&#24615;&#30340;&#25968;&#23383;&#24230;&#37327;&#21644;&#26446;&#38597;&#26222;&#35834;&#22827;&#25351;&#25968;&#65292;&#19968;&#20010;&#36712;&#36857;&#19981;&#31283;&#23450;&#30340;&#24230;&#37327;&#65292;&#34920;&#26126;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#24456;&#22909;&#22320;&#22797;&#21046;&#20102;&#21407;&#22987;&#26102;&#38388;&#24207;&#21015;&#30340;&#28151;&#27788;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#35823;&#24046;&#20998;&#24067;&#20998;&#26512;&#34920;&#26126;&#65292;&#20302;&#20294;&#19981;&#21487;&#24573;&#30053;&#30340;&#36895;&#29575;&#19979;&#20250;&#20986;&#29616;&#22823;&#35823;&#24046;&#12290;&#22914;&#26524;&#20551;&#23450;&#20998;&#24067;&#20026;&#25351;&#25968;&#20998;&#24067;&#65292;&#21017;&#19981;&#20250;&#20986;&#29616;&#36825;&#26679;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
To understand the ability and limitations of convolutional neural networks to generate time series that mimic complex temporal signals, we trained a generative adversarial network consisting of deep convolutional networks to generate chaotic time series and used nonlinear time series analysis to evaluate the generated time series. A numerical measure of determinism and the Lyapunov exponent, a measure of trajectory instability, showed that the generated time series well reproduce the chaotic properties of the original time series. However, error distribution analyses showed that large errors appeared at a low but non-negligible rate. Such errors would not be expected if the distribution were assumed to be exponential.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24418;&#29366;&#20449;&#24687;&#30340;&#33258;&#21160;&#33181;&#20851;&#33410;&#26631;&#20934;&#35270;&#22270;&#23039;&#24577;&#20272;&#35745;&#26694;&#26550;&#65292;&#21487;&#20943;&#23569;&#25163;&#26415;&#36807;&#31243;&#20013;&#30340;&#26102;&#38388;&#21644;&#36752;&#23556;&#21058;&#37327;&#65292;&#33719;&#24471;&#20102;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#30340;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16717</link><description>&lt;p&gt;
&#22522;&#20110;&#24418;&#29366;&#30340;&#33258;&#21160;&#33181;&#37096;&#26631;&#20934;&#35270;&#22270;&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Shape-based pose estimation for automatic standard views of the knee. (arXiv:2305.16717v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16717
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24418;&#29366;&#20449;&#24687;&#30340;&#33258;&#21160;&#33181;&#20851;&#33410;&#26631;&#20934;&#35270;&#22270;&#23039;&#24577;&#20272;&#35745;&#26694;&#26550;&#65292;&#21487;&#20943;&#23569;&#25163;&#26415;&#36807;&#31243;&#20013;&#30340;&#26102;&#38388;&#21644;&#36752;&#23556;&#21058;&#37327;&#65292;&#33719;&#24471;&#20102;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#30340;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#26415;&#27835;&#30103;&#22797;&#26434;&#30340;&#33181;&#20851;&#33410;&#39592;&#25240;&#36890;&#36807;&#20351;&#29992;&#31227;&#21160;C&#33218;&#36827;&#34892;&#23454;&#26102;&#25104;&#20687;&#26469;&#25351;&#23548;&#12290;&#36890;&#36807;&#19982;&#24739;&#32773;&#23450;&#20301;&#30456;&#23545;&#30340;&#29305;&#23450;C&#33218;&#23039;&#21183;&#30456;&#23545;&#24212;&#30340;2D&#35299;&#21078;&#29305;&#24322;&#24615;&#26631;&#20934;&#35270;&#22270;&#23454;&#29616;&#21363;&#26102;&#21644;&#36830;&#32493;&#30340;&#25511;&#21046;&#65292;&#30446;&#21069;&#36890;&#36807;&#35797;&#38169;&#26041;&#27861;&#25163;&#21160;&#30830;&#23450;&#65292;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#21644;&#36752;&#23556;&#21058;&#37327;&#12290;&#33181;&#20851;&#33410;&#26631;&#20934;&#35270;&#22270;&#30340;&#29305;&#24449;&#34920;&#26126;&#65292;&#20010;&#20154;&#39592;&#39612;&#30340;&#24418;&#29366;&#20449;&#24687;&#21487;&#20197;&#25351;&#23548;&#33258;&#21160;&#23450;&#20301;&#36807;&#31243;&#65292;&#20943;&#23569;C&#33218;&#23450;&#20301;&#26399;&#38388;&#30340;&#26102;&#38388;&#21644;&#19981;&#24517;&#35201;&#30340;&#36752;&#23556;&#12290;&#20026;&#20102;&#23436;&#20840;&#33258;&#21160;&#21270;&#33181;&#20851;&#33410;&#25163;&#26415;&#20013;&#30340;C&#33218;&#23450;&#20301;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#26694;&#26550;&#65292;&#23427;&#23454;&#29616;&#20102;&#65288;1&#65289;&#33258;&#21160;&#21306;&#20998;&#24038;&#21491;&#21644;&#26631;&#20934;&#35270;&#22270;&#21644;&#65288;2&#65289;&#22522;&#20110;&#21333;&#20010;&#21021;&#22987;X&#23556;&#32447;&#30340;&#22522;&#20110;&#24418;&#29366;&#30340;&#23039;&#24577;&#22238;&#24402;&#21040;&#25152;&#38656;&#30340;&#26631;&#20934;&#35270;&#22270;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#36866;&#30340;&#24418;&#29366;&#34920;&#31034;&#26469;&#23558;&#35821;&#20041;&#20449;&#24687;&#21512;&#24182;&#21040;&#23039;&#24577;&#22238;&#24402;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surgical treatment of complicated knee fractures is guided by real-time imaging using a mobile C-arm. Immediate and continuous control is achieved via 2D anatomy-specific standard views that correspond to a specific C-arm pose relative to the patient positioning, which is currently determined manually, following a trial-and-error approach at the cost of time and radiation dose. The characteristics of the standard views of the knee suggests that the shape information of individual bones could guide an automatic positioning procedure, reducing time and the amount of unnecessary radiation during C-arm positioning. To fully automate the C-arm positioning task during knee surgeries, we propose a complete framework that enables (1) automatic laterality and standard view classification and (2) automatic shape-based pose regression toward the desired standard view based on a single initial X-ray. A suitable shape representation is proposed to incorporate semantic information into the pose regr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#24605;&#32500;&#30340;&#20154;&#24037;&#26234;&#33021;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20195;&#29702;&#33021;&#22815;&#26681;&#25454;&#24403;&#21069;&#21512;&#20316;&#20249;&#20276;&#33258;&#21160;&#20999;&#25442;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#20195;&#29702;&#19982;&#26032;&#21512;&#20316;&#20249;&#20276;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16708</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#24605;&#32500;&#30340;&#20154;&#24037;&#26234;&#33021;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Hierarchical Approach to Population Training for Human-AI Collaboration. (arXiv:2305.16708v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#24605;&#32500;&#30340;&#20154;&#24037;&#26234;&#33021;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20195;&#29702;&#33021;&#22815;&#26681;&#25454;&#24403;&#21069;&#21512;&#20316;&#20249;&#20276;&#33258;&#21160;&#20999;&#25442;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#20195;&#29702;&#19982;&#26032;&#21512;&#20316;&#20249;&#20276;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#22312;&#19982;&#26410;&#32463;&#36807;&#35757;&#32451;&#30340;&#21512;&#20316;&#20249;&#20276;&#21327;&#21516;&#26102;&#23384;&#22312;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#24403;&#20195;&#29702;&#19982;&#20154;&#31867;&#21512;&#20316;&#20249;&#20276;&#21512;&#20316;&#26102;&#65292;&#22240;&#20154;&#31867;&#34892;&#20026;&#30340;&#19981;&#19968;&#33268;&#24615;&#32780;&#20986;&#29616;&#34892;&#21160;&#21453;&#24212;&#30340;&#26041;&#24046;&#22686;&#21152;&#65292;&#32780;&#36825;&#21152;&#21095;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#21333;&#20010;&#20195;&#29702;&#35757;&#32451;&#20026;&#23545;&#22810;&#26679;&#21270;&#30340;&#35757;&#32451;&#20249;&#20276;&#20570;&#20986;&#26368;&#20339;&#21709;&#24212;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20195;&#29702;&#19982;&#26032;&#21512;&#20316;&#20249;&#20276;&#30340;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#22522;&#20110;&#20154;&#21475;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#65288;HRL&#65289;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#21327;&#21516;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#20195;&#29702;&#33021;&#22815;&#23398;&#20064;&#22810;&#20010;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#20316;&#20026;&#20854;&#20302;&#23618;&#31574;&#30053;&#65292;&#21516;&#26102;&#23398;&#20064;&#19968;&#20010;&#20316;&#20026;&#31649;&#29702;&#32773;&#30340;&#39640;&#23618;&#31574;&#30053;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26681;&#25454;&#20854;&#24403;&#21069;&#30340;&#21512;&#20316;&#20249;&#20276;&#21160;&#24577;&#22320;&#22312;&#20302;&#23618;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#20043;&#38388;&#36827;&#34892;&#20999;&#25442;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;
&lt;/p&gt;
&lt;p&gt;
A major challenge for deep reinforcement learning (DRL) agents is to collaborate with novel partners that were not encountered by them during the training phase. This is specifically worsened by an increased variance in action responses when the DRL agents collaborate with human partners due to the lack of consistency in human behaviors. Recent work have shown that training a single agent as the best response to a diverse population of training partners significantly increases an agent's robustness to novel partners. We further enhance the population-based training approach by introducing a Hierarchical Reinforcement Learning (HRL) based method for Human-AI Collaboration. Our agent is able to learn multiple best-response policies as its low-level policy while at the same time, it learns a high-level policy that acts as a manager which allows the agent to dynamically switch between the low-level best-response policies based on its current partner. We demonstrate that our method is able 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#20998;&#24067;&#28418;&#31227;&#19979;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#27604;&#36739;&#20102;&#21464;&#21387;&#22120;&#21644;&#22522;&#20110;&#38598;&#21512;&#30340;MLP&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#20108;&#32773;&#22312;&#20998;&#24067;&#20869;&#35780;&#20272;&#20013;&#37117;&#34920;&#29616;&#20986;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#38450;&#33539;&#36739;&#23567;&#30340;&#20998;&#24067;&#28418;&#31227;&#26041;&#38754;&#65292;&#21464;&#21387;&#22120;&#26356;&#32988;&#19968;&#31609;&#12290;</title><link>http://arxiv.org/abs/2305.16704</link><description>&lt;p&gt;
&#25506;&#31350;&#20998;&#24067;&#28418;&#31227;&#19979;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65306;&#20197;&#32447;&#24615;&#22238;&#24402;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at In-Context Learning under Distribution Shifts. (arXiv:2305.16704v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#20998;&#24067;&#28418;&#31227;&#19979;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#27604;&#36739;&#20102;&#21464;&#21387;&#22120;&#21644;&#22522;&#20110;&#38598;&#21512;&#30340;MLP&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#20108;&#32773;&#22312;&#20998;&#24067;&#20869;&#35780;&#20272;&#20013;&#37117;&#34920;&#29616;&#20986;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#38450;&#33539;&#36739;&#23567;&#30340;&#20998;&#24067;&#28418;&#31227;&#26041;&#38754;&#65292;&#21464;&#21387;&#22120;&#26356;&#32988;&#19968;&#31609;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#20010;&#23450;&#20041;&#29305;&#24449;&#65292;&#23427;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#26435;&#37325;&#26356;&#26032;&#30340;&#24773;&#20917;&#19979;&#21363;&#26102;&#22320;&#20174;&#36755;&#20837;&#26679;&#20363;&#20013;&#23398;&#20064;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#32447;&#24615;&#22238;&#24402;&#36825;&#19968;&#31616;&#21333;&#32780;&#22522;&#30784;&#30340;&#20219;&#21153;&#65292;&#36981;&#24490;&#65288;Garg et al., 2022&#65289;&#25552;&#20986;&#30340;&#35774;&#32622;&#65292;&#20174;&#31616;&#21333;&#30340;&#22522;&#20110;&#38598;&#21512;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#26550;&#26500;&#30340;&#35282;&#24230;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26222;&#36866;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#26680;&#24515;&#38382;&#39064;&#26159;&#65306;&#22312;&#21464;&#21270;&#30340;&#20998;&#24067;&#28418;&#31227;&#19979;&#65292;&#21464;&#21387;&#22120;&#26159;&#21542;&#27604;&#19968;&#20123;&#33258;&#28982;&#19988;&#26356;&#31616;&#21333;&#30340;&#26550;&#26500;&#26356;&#25797;&#38271;&#25191;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65311;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20998;&#24067;&#20869;&#35780;&#20272;&#19979;&#65292;&#21464;&#21387;&#22120;&#21644;&#22522;&#20110;&#38598;&#21512;&#30340;MLP&#27169;&#22411;&#37117;&#34920;&#29616;&#20986;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#21464;&#21387;&#22120;&#26356;&#25509;&#36817;&#20110;&#26368;&#23567;&#20108;&#20056;&#27861;&#65288;OLS&#65289;&#30340;&#34920;&#29616;&#12290;&#22312;&#20998;&#24067;&#28418;&#31227;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#21464;&#21387;&#22120;&#30340;&#38887;&#24615;&#20063;&#27604;&#22522;&#20110;&#38598;&#21512;&#30340;MLP&#27169;&#22411;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning, a capability that enables a model to learn from input examples on the fly without necessitating weight updates, is a defining characteristic of large language models. In this work, we follow the setting proposed in (Garg et al., 2022) to better understand the generality and limitations of in-context learning from the lens of the simple yet fundamental task of linear regression. The key question we aim to address is: Are transformers more adept than some natural and simpler architectures at performing in-context learning under varying distribution shifts? To compare transformers, we propose to use a simple architecture based on set-based Multi-Layer Perceptrons (MLPs). We find that both transformers and set-based MLPs exhibit in-context learning under in-distribution evaluations, but transformers more closely emulate the performance of ordinary least squares (OLS). Transformers also display better resilience to mild distribution shifts, where set-based MLPs falter. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#21644;&#31867;&#22411;&#65292;&#20174;&#32479;&#35745;&#23398;&#23478;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#20998;&#31867;&#21035;&#20171;&#32461;&#20102;&#38543;&#26426;&#24615;&#21644;&#35748;&#30693;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#27010;&#24565;&#65292;&#35777;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#21508;&#24322;&#65292;&#19981;&#21487;&#31616;&#21333;&#24402;&#20026;&#20004;&#31867;&#12290;&#21516;&#26102;&#65292;&#19982;&#32479;&#35745;&#23398;&#27010;&#24565;&#36827;&#34892;&#31867;&#27604;&#65292;&#25506;&#35752;&#19981;&#30830;&#23450;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.16703</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304; -- &#19968;&#20010;&#32479;&#35745;&#23398;&#23478;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Sources of Uncertainty in Machine Learning -- A Statisticians' View. (arXiv:2305.16703v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#21644;&#31867;&#22411;&#65292;&#20174;&#32479;&#35745;&#23398;&#23478;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#20998;&#31867;&#21035;&#20171;&#32461;&#20102;&#38543;&#26426;&#24615;&#21644;&#35748;&#30693;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#27010;&#24565;&#65292;&#35777;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#21508;&#24322;&#65292;&#19981;&#21487;&#31616;&#21333;&#24402;&#20026;&#20004;&#31867;&#12290;&#21516;&#26102;&#65292;&#19982;&#32479;&#35745;&#23398;&#27010;&#24565;&#36827;&#34892;&#31867;&#27604;&#65292;&#25506;&#35752;&#19981;&#30830;&#23450;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#23601;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22238;&#31572;&#20960;&#24180;&#21069;&#38590;&#20197;&#24819;&#35937;&#30340;&#38382;&#39064;&#12290;&#38500;&#20102;&#36825;&#20123;&#25104;&#21151;&#20043;&#22806;&#65292;&#36234;&#26469;&#36234;&#28165;&#26224;&#30340;&#26159;&#65292;&#22312;&#32431;&#39044;&#27979;&#20043;&#22806;&#65292;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#20063;&#26159;&#30456;&#20851;&#21644;&#24517;&#35201;&#30340;&#12290;&#34429;&#28982;&#36817;&#24180;&#26469;&#24050;&#32463;&#20986;&#29616;&#20102;&#36825;&#26041;&#38754;&#30340;&#31532;&#19968;&#25209;&#27010;&#24565;&#21644;&#24605;&#24819;&#65292;&#20294;&#26412;&#25991;&#37319;&#29992;&#20102;&#19968;&#20010;&#27010;&#24565;&#24615;&#30340;&#35270;&#35282;&#65292;&#24182;&#25506;&#35752;&#20102;&#21487;&#33021;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#12290;&#36890;&#36807;&#37319;&#29992;&#32479;&#35745;&#23398;&#23478;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19982;&#26426;&#22120;&#23398;&#20064;&#26356;&#24120;&#35265;&#30456;&#20851;&#30340;&#38543;&#26426;&#24615;&#21644;&#35748;&#30693;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#27010;&#24565;&#12290;&#26412;&#25991;&#26088;&#22312;&#35268;&#33539;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35777;&#26126;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#21508;&#24322;&#65292;&#24182;&#19988;&#19981;&#24635;&#26159;&#21487;&#20197;&#20998;&#35299;&#20026;&#38543;&#26426;&#24615;&#21644;&#35748;&#30693;&#24615;&#12290;&#36890;&#36807;&#23558;&#32479;&#35745;&#27010;&#24565;&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#31867;&#27604;&#65292;&#25105;&#20204;&#20063;&#23637;&#31034;&#20102;&#32479;&#35745;&#23398;&#27010;&#24565;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning and Deep Learning have achieved an impressive standard today, enabling us to answer questions that were inconceivable a few years ago. Besides these successes, it becomes clear, that beyond pure prediction, which is the primary strength of most supervised machine learning algorithms, the quantification of uncertainty is relevant and necessary as well. While first concepts and ideas in this direction have emerged in recent years, this paper adopts a conceptual perspective and examines possible sources of uncertainty. By adopting the viewpoint of a statistician, we discuss the concepts of aleatoric and epistemic uncertainty, which are more commonly associated with machine learning. The paper aims to formalize the two types of uncertainty and demonstrates that sources of uncertainty are miscellaneous and can not always be decomposed into aleatoric and epistemic. Drawing parallels between statistical concepts and uncertainty in machine learning, we also demonstrate the rol
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#22312;&#38646;&#26679;&#26412;&#35821;&#38899;&#21512;&#25104;&#20013;&#33258;&#21160;&#35843;&#25972;&#25439;&#22833;&#26435;&#34913;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#25628;&#32034;&#12290;&#36890;&#36807;&#27492;&#26041;&#27861;&#65292;VITS-based&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#65292;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#39046;&#20808;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16699</link><description>&lt;p&gt;
&#26080;&#38656;&#36229;&#21442;&#25968;&#25628;&#32034;&#30340;&#31471;&#21040;&#31471;&#38646;&#26679;&#26412;&#35821;&#38899;&#21512;&#25104;&#20013;&#25439;&#22833;&#26435;&#34913;&#30340;&#33258;&#21160;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Automatic Tuning of Loss Trade-offs without Hyper-parameter Search in End-to-End Zero-Shot Speech Synthesis. (arXiv:2305.16699v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#22312;&#38646;&#26679;&#26412;&#35821;&#38899;&#21512;&#25104;&#20013;&#33258;&#21160;&#35843;&#25972;&#25439;&#22833;&#26435;&#34913;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#25628;&#32034;&#12290;&#36890;&#36807;&#27492;&#26041;&#27861;&#65292;VITS-based&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#65292;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#39046;&#20808;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38646;&#26679;&#26412;TTS&#21644;VC&#26041;&#27861;&#22240;&#20854;&#33021;&#22815;&#29983;&#25104;&#22312;&#35757;&#32451;&#20013;&#20174;&#26410;&#35265;&#36807;&#30340;&#35821;&#38899;&#32780;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;VITS&#30340;&#38646;&#26679;&#26412;&#20462;&#25913;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#20855;&#26377;&#20174;VITS&#32487;&#25215;&#26469;&#30340;&#26377;&#29992;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;VITS&#21644;&#22522;&#20110;VITS&#30340;&#38646;&#26679;&#26412;&#27169;&#22411;&#30340;&#24615;&#33021;&#22312;&#25439;&#22833;&#22914;&#20309;&#26435;&#34913;&#26041;&#38754;&#23384;&#22312;&#24040;&#22823;&#24046;&#24322;&#12290;&#36825;&#21487;&#33021;&#26159;&#26377;&#38382;&#39064;&#30340;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#32321;&#29712;&#30340;&#35843;&#25972;&#25439;&#22833;&#26435;&#34913;&#36229;&#21442;&#25968;&#20197;&#25214;&#21040;&#26368;&#20339;&#24179;&#34913;&#28857;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#23548;VITS-based&#27169;&#22411;&#30340;&#35299;&#30721;&#22120;&#36798;&#21040;&#20854;&#23436;&#20840;&#37325;&#24314;&#33021;&#21147;&#65292;&#20197;&#25214;&#21040;&#36825;&#20010;&#26368;&#20339;&#28857;&#32780;&#26080;&#38656;&#25628;&#32034;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#23637;&#29616;&#20102;&#22312;&#38646;&#26679;&#26412;TTS&#21644;VC&#20013;&#27604;&#22522;&#32447;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#39046;&#20808;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#35752;&#35770;&#20013;&#23545;&#32467;&#26524;&#36827;&#34892;&#20102;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, zero-shot TTS and VC methods have gained attention due to their practicality of being able to generate voices even unseen during training. Among these methods, zero-shot modifications of the VITS model have shown superior performance, while having useful properties inherited from VITS. However, the performance of VITS and VITS-based zero-shot models vary dramatically depending on how the losses are balanced. This can be problematic, as it requires a burdensome procedure of tuning loss balance hyper-parameters to find the optimal balance. In this work, we propose a novel framework that finds this optimum without search, by inducing the decoder of VITS-based models to its full reconstruction ability. With our framework, we show superior performance compared to baselines in zero-shot TTS and VC, achieving state-of-the-art performance. Furthermore, we show the robustness of our framework in various settings. We provide an explanation for the results in the discussion.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21452;&#37325;&#36125;&#21494;&#26031;ResNet&#65292;&#29992;&#20110;&#24515;&#33039;&#26434;&#38899;&#26816;&#27979;&#12290;&#20854;&#20013;&#19968;&#31181;&#27169;&#22411;&#23558;&#27599;&#20010;&#24739;&#32773;&#30340;&#35760;&#24405;&#20998;&#27573;&#25104;&#37325;&#21472;&#30340;log mel&#38899;&#35889;&#22270;&#36827;&#34892;&#20108;&#20998;&#31867;&#65292;&#31532;&#20108;&#31181;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#20449;&#21495;&#29305;&#24449;&#26469;&#25552;&#39640;&#26434;&#38899;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.16691</link><description>&lt;p&gt;
&#21452;&#37325;&#36125;&#21494;&#26031;ResNet&#65306;&#19968;&#31181;&#24515;&#33039;&#26434;&#38899;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dual Bayesian ResNet: A Deep Learning Approach to Heart Murmur Detection. (arXiv:2305.16691v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21452;&#37325;&#36125;&#21494;&#26031;ResNet&#65292;&#29992;&#20110;&#24515;&#33039;&#26434;&#38899;&#26816;&#27979;&#12290;&#20854;&#20013;&#19968;&#31181;&#27169;&#22411;&#23558;&#27599;&#20010;&#24739;&#32773;&#30340;&#35760;&#24405;&#20998;&#27573;&#25104;&#37325;&#21472;&#30340;log mel&#38899;&#35889;&#22270;&#36827;&#34892;&#20108;&#20998;&#31867;&#65292;&#31532;&#20108;&#31181;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#20449;&#21495;&#29305;&#24449;&#26469;&#25552;&#39640;&#26434;&#38899;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#25105;&#20204;&#22242;&#38431;PathToMyHeart&#22312;2022&#24180;George B. Moody PhysioNet Challenge&#20013;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#20004;&#20010;&#27169;&#22411;&#12290;&#31532;&#19968;&#20010;&#27169;&#22411;&#26159;&#21452;&#37325;&#36125;&#21494;&#26031;ResNet&#65288;DBRes&#65289;&#65292;&#23558;&#27599;&#20010;&#24739;&#32773;&#30340;&#35760;&#24405;&#20998;&#27573;&#25104;&#37325;&#21472;&#30340;log mel&#38899;&#35889;&#22270;&#36827;&#34892;&#20004;&#20010;&#20108;&#20998;&#31867;&#65306;&#23384;&#22312;&#19982;&#26410;&#30693;&#25110;&#19981;&#23384;&#22312;&#65292;&#26410;&#30693;&#19982;&#23384;&#22312;&#25110;&#19981;&#23384;&#22312;&#12290;&#36825;&#20123;&#20998;&#31867;&#32467;&#26524;&#36827;&#34892;&#32858;&#21512;&#65292;&#20197;&#24471;&#20986;&#24739;&#32773;&#30340;&#26368;&#32456;&#20998;&#31867;&#12290;&#31532;&#20108;&#20010;&#27169;&#22411;&#26159;&#23558;DBRes&#30340;&#36755;&#20986;&#19982;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#20449;&#21495;&#29305;&#24449;&#36827;&#34892;&#25972;&#21512;&#65292;&#20351;&#29992;XGBoost&#36827;&#34892;&#20998;&#31867;&#12290;DBRes&#22312;&#38544;&#21547;&#27979;&#35797;&#38598;&#30340;&#26434;&#38899;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;$0.771$&#30340;&#26368;&#20339;&#21152;&#26435;&#20934;&#30830;&#29575;&#65292;&#20351;&#25105;&#20204;&#22312;&#26434;&#38899;&#20219;&#21153;&#20013;&#25490;&#21517;&#31532;&#22235;&#12290;&#65288;&#22312;&#25105;&#20204;&#24573;&#30053;&#20102;&#20020;&#24202;&#32467;&#23616;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#24471;&#20998;&#20026;$12637$&#12290;&#65289;&#22312;&#25105;&#20204;&#30340;&#35757;&#32451;&#38598;&#23376;&#38598;&#19978;&#65292;&#23558;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#20449;&#21495;&#29305;&#24449;&#25972;&#21512;&#21518;&#65292;DBRes&#30340;&#20934;&#30830;&#29575;&#20174;$0.762$&#25552;&#39640;&#21040;&#20102;$0.820$&#12290;&#20294;&#26159;&#65292;&#36825;&#38477;&#20302;&#20102;DBRes&#30340;&#21152;&#26435;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents our team PathToMyHeart's contribution to the George B. Moody PhysioNet Challenge 2022. Two models are implemented. The first model is a Dual Bayesian ResNet (DBRes), where each patient's recording is segmented into overlapping log mel spectrograms. These undergo two binary classifications: present versus unknown or absent, and unknown versus present or absent. The classifications are aggregated to give a patient's final classification. The second model is the output of DBRes integrated with demographic data and signal features using XGBoost.DBRes achieved our best weighted accuracy of $0.771$ on the hidden test set for murmur classification, which placed us fourth for the murmur task. (On the clinical outcome task, which we neglected, we scored 17th with costs of $12637$.) On our held-out subset of the training set, integrating the demographic data and signal features improved DBRes's accuracy from $0.762$ to $0.820$. However, this decreased DBRes's weighted accurac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26410;&#26469;&#20449;&#24687;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;PDT&#65292;&#20351;&#24471;&#22312;&#27809;&#26377;&#22870;&#21169;&#21644;&#27425;&#20248;&#31163;&#32447;&#25968;&#25454;&#19978;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#25104;&#20026;&#21487;&#33021;&#65292;&#36890;&#36807;&#20026;&#21487;&#33021;&#30340;&#26410;&#26469;&#20998;&#37197;&#22238;&#25253;&#20540;&#21644;&#37319;&#26679;&#26410;&#26469;&#23884;&#20837;&#36827;&#34892;&#22312;&#32447;&#24494;&#35843;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;PDT&#20248;&#20110;&#24403;&#21069;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#19982;&#26377;&#30417;&#30563;&#30340;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#31454;&#20105;&#12290;</title><link>http://arxiv.org/abs/2305.16683</link><description>&lt;p&gt;
&#26410;&#26469;&#26465;&#20214;&#19979;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#23545;&#20915;&#31574;Transformer&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Future-conditioned Unsupervised Pretraining for Decision Transformer. (arXiv:2305.16683v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26410;&#26469;&#20449;&#24687;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;PDT&#65292;&#20351;&#24471;&#22312;&#27809;&#26377;&#22870;&#21169;&#21644;&#27425;&#20248;&#31163;&#32447;&#25968;&#25454;&#19978;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#25104;&#20026;&#21487;&#33021;&#65292;&#36890;&#36807;&#20026;&#21487;&#33021;&#30340;&#26410;&#26469;&#20998;&#37197;&#22238;&#25253;&#20540;&#21644;&#37319;&#26679;&#26410;&#26469;&#23884;&#20837;&#36827;&#34892;&#22312;&#32447;&#24494;&#35843;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;PDT&#20248;&#20110;&#24403;&#21069;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#19982;&#26377;&#30417;&#30563;&#30340;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#38754;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20197;&#22238;&#25253;&#20026;&#26465;&#20214;&#30340;&#30417;&#30563;&#23398;&#20064;&#26159;&#35299;&#20915;&#20915;&#31574;&#38382;&#39064;&#30340;&#19968;&#31181;&#24378;&#26377;&#21147;&#30340;&#33539;&#20363;&#12290;&#34429;&#28982;&#24456;&#26377;&#24076;&#26395;&#65292;&#20294;&#22238;&#25253;&#26465;&#20214;&#20165;&#36866;&#29992;&#20110;&#26631;&#35760;&#26377;&#22870;&#21169;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#22240;&#27492;&#22312;&#20174;&#26080;&#30417;&#30563;&#25968;&#25454;&#20013;&#23398;&#20064;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#21033;&#29992;&#24191;&#20041;&#26410;&#26469;&#26465;&#20214;&#26469;&#21551;&#29992;&#20174;&#27809;&#26377;&#22870;&#21169;&#21644;&#27425;&#20248;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#20915;&#31574;Transformer&#65288;PDT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#27010;&#24565;&#19978;&#31616;&#21333;&#30340;&#26080;&#30417;&#30563;RL&#39044;&#35757;&#32451;&#26041;&#27861;&#12290; PDT&#21033;&#29992;&#26410;&#26469;&#36712;&#36857;&#20449;&#24687;&#20316;&#20026;&#39044;&#27979;&#35757;&#32451;&#26399;&#38388;&#30340;&#21160;&#20316;&#30340;&#29305;&#26435;&#19978;&#19979;&#25991;&#12290;&#22522;&#20110;&#24403;&#21069;&#21644;&#26410;&#26469;&#22240;&#32032;&#20570;&#20986;&#20915;&#31574;&#30340;&#33021;&#21147;&#22686;&#24378;&#20102;PDT&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#21487;&#20197;&#23558;&#27492;&#21151;&#33021;&#36731;&#26494;&#22320;&#21512;&#24182;&#21040;&#22522;&#20110;&#22238;&#25253;&#30340;&#26694;&#26550;&#20013;&#20197;&#36827;&#34892;&#22312;&#32447;&#24494;&#35843;&#65292;&#36890;&#36807;&#20026;&#21487;&#33021;&#30340;&#26410;&#26469;&#20998;&#37197;&#22238;&#25253;&#20540;&#24182;&#26681;&#25454;&#36825;&#20123;&#22238;&#25253;&#26679;&#26412;&#26410;&#26469;&#23884;&#20837;&#26469;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research in offline reinforcement learning (RL) has demonstrated that return-conditioned supervised learning is a powerful paradigm for decision-making problems. While promising, return conditioning is limited to training data labeled with rewards and therefore faces challenges in learning from unsupervised data. In this work, we aim to utilize generalized future conditioning to enable efficient unsupervised pretraining from reward-free and sub-optimal offline data. We propose Pretrained Decision Transformer (PDT), a conceptually simple approach for unsupervised RL pretraining. PDT leverages future trajectory information as a privileged context to predict actions during training. The ability to make decisions based on both present and future factors enhances PDT's capability for generalization. Besides, this feature can be easily incorporated into a return-conditioned framework for online finetuning, by assigning return values to possible futures and sampling future embeddings b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#21512;&#25104;&#26631;&#35782;&#31526;&#30340;&#22810;&#35270;&#35282;&#26631;&#35782;&#31526;&#26469;&#22686;&#24378;&#29983;&#25104;&#24335;&#26816;&#32034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26816;&#32034;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16675</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#26631;&#35782;&#22686;&#24378;&#29983;&#25104;&#24335;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multiview Identifiers Enhanced Generative Retrieval. (arXiv:2305.16675v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16675
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#21512;&#25104;&#26631;&#35782;&#31526;&#30340;&#22810;&#35270;&#35282;&#26631;&#35782;&#31526;&#26469;&#22686;&#24378;&#29983;&#25104;&#24335;&#26816;&#32034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26816;&#32034;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20854;&#31616;&#21333;&#22320;&#23558;&#26597;&#35810;&#19982;&#29616;&#26377;&#27573;&#33853;&#21305;&#37197;&#65292;&#29983;&#25104;&#24335;&#26816;&#32034;&#29983;&#25104;&#27573;&#33853;&#30340;&#26631;&#35782;&#31526;&#23383;&#31526;&#20018;&#20316;&#20026;&#26816;&#32034;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26631;&#35782;&#31526;&#24517;&#39035;&#36275;&#22815;&#29420;&#29305;&#20197;&#20195;&#34920;&#19968;&#20010;&#27573;&#33853;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#25968;&#23383;ID&#25110;&#25991;&#26412;&#29255;&#27573;&#65288;&#22914;&#26631;&#39064;&#25110;&#23376;&#23383;&#31526;&#20018;&#65289;&#20316;&#20026;&#26631;&#35782;&#31526;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26631;&#35782;&#31526;&#19981;&#33021;&#24456;&#22909;&#22320;&#35206;&#30422;&#19968;&#20010;&#27573;&#33853;&#30340;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#22411;&#30340;&#26631;&#35782;&#31526;&#65292;&#21363;&#22522;&#20110;&#27573;&#33853;&#20869;&#23481;&#29983;&#25104;&#30340;&#21512;&#25104;&#26631;&#35782;&#31526;&#65292;&#21487;&#20197;&#25972;&#21512;&#25991;&#26412;&#29255;&#27573;&#32570;&#20047;&#30340;&#24773;&#22659;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21516;&#26102;&#32771;&#34385;&#22810;&#35270;&#35282;&#26631;&#35782;&#31526;&#65292;&#21253;&#25324;&#21512;&#25104;&#26631;&#35782;&#31526;&#12289;&#26631;&#39064;&#21644;&#23376;&#23383;&#31526;&#20018;&#12290;&#36825;&#20123;&#26631;&#35782;&#31526;&#30340;&#35270;&#35282;&#30456;&#20114;&#34917;&#20805;&#65292;&#26377;&#21161;&#20110;&#20174;&#22810;&#20010;&#35282;&#24230;&#32508;&#21512;&#25490;&#21517;&#27573;&#33853;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#24335;&#26816;&#32034;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instead of simply matching a query to pre-existing passages, generative retrieval generates identifier strings of passages as the retrieval target. At a cost, the identifier must be distinctive enough to represent a passage. Current approaches use either a numeric ID or a text piece (such as a title or substrings) as the identifier. However, these identifiers cannot cover a passage's content well. As such, we are motivated to propose a new type of identifier, synthetic identifiers, that are generated based on the content of a passage and could integrate contextualized information that text pieces lack. Furthermore, we simultaneously consider multiview identifiers, including synthetic identifiers, titles, and substrings. These views of identifiers complement each other and facilitate the holistic ranking of passages from multiple perspectives. We conduct a series of experiments on three public datasets, and the results indicate that our proposed approach performs the best in generative 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#35774;&#32622;&#21644; Oracle &#35775;&#38382;&#31867;&#22411;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#29992;&#20110;&#26368;&#22823;&#21270;&#36830;&#32493; DR-submodular &#20989;&#25968;&#65292;&#20026; 16 &#31181;&#24773;&#20917;&#20013;&#30340; 9 &#31181;&#25552;&#20379;&#20102;&#26032;&#30340;/&#25913;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#38024;&#23545;&#22522;&#20110;&#38543;&#26426;&#20989;&#25968;&#20540;&#30340; Oracle &#21462;&#24471;&#20102;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#38543;&#26426; DR-submodular &#20989;&#25968;&#30340;&#21518;&#24724;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2305.16671</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#29992;&#20110;&#26368;&#22823;&#21270;&#36830;&#32493; DR-submodular &#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
A Unified Approach for Maximizing Continuous DR-submodular Functions. (arXiv:2305.16671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#35774;&#32622;&#21644; Oracle &#35775;&#38382;&#31867;&#22411;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#29992;&#20110;&#26368;&#22823;&#21270;&#36830;&#32493; DR-submodular &#20989;&#25968;&#65292;&#20026; 16 &#31181;&#24773;&#20917;&#20013;&#30340; 9 &#31181;&#25552;&#20379;&#20102;&#26032;&#30340;/&#25913;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#38024;&#23545;&#22522;&#20110;&#38543;&#26426;&#20989;&#25968;&#20540;&#30340; Oracle &#21462;&#24471;&#20102;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#38543;&#26426; DR-submodular &#20989;&#25968;&#30340;&#21518;&#24724;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26368;&#22823;&#21270;&#36830;&#32493;&#30340; DR-submodular &#20989;&#25968;&#65292;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#35774;&#32622;&#21644; Oracle &#35775;&#38382;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#38024;&#23545;&#21333;&#35843;&#21644;&#38750;&#21333;&#35843;&#20989;&#25968;&#30340; Frank-Wolfe &#31867;&#22411;&#31163;&#32447;&#31639;&#27861;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#19968;&#33324;&#20984;&#38598;&#38480;&#21046;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102; Oracle &#25552;&#20379;&#20989;&#25968;&#26799;&#24230;&#25110;&#20165;&#20989;&#25968;&#20540;&#30340;&#35775;&#38382;&#20197;&#21450;&#30830;&#23450;&#24615;&#25110;&#38543;&#26426;&#24615;&#35775;&#38382;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#30830;&#23450;&#20102;&#25152;&#38656;&#30340; Oracle &#35775;&#38382;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026; 16 &#20010;&#32771;&#34385;&#30340;&#24773;&#20917;&#20013;&#30340; 9 &#20010;&#25552;&#20379;&#20102;&#26032;&#30340;/&#25913;&#36827;&#30340;&#32467;&#26524;&#65292;&#22312;&#20004;&#20010;&#24773;&#20917;&#19979;&#36991;&#20813;&#20102;&#35745;&#31639;&#19978;&#26114;&#36149;&#30340;&#25237;&#24433;&#65292;&#32780;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#20854;&#20313;&#20116;&#20010;&#24773;&#20917;&#19979;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#21305;&#37197;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#38024;&#23545;&#22522;&#20110;&#38543;&#26426;&#20989;&#25968;&#20540;&#30340; Oracle &#30340;&#26041;&#27861;&#65292;&#20026;&#38543;&#26426; DR-submodular &#20989;&#25968;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#24102;&#26377;&#25506;&#38505;&#21453;&#39304;&#30340;&#21518;&#24724;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a unified approach for maximizing continuous DR-submodular functions that encompasses a range of settings and oracle access types. Our approach includes a Frank-Wolfe type offline algorithm for both monotone and non-monotone functions, with different restrictions on the general convex set. We consider settings where the oracle provides access to either the gradient of the function or only the function value, and where the oracle access is either deterministic or stochastic. We determine the number of required oracle accesses in all cases. Our approach gives new/improved results for nine out of the sixteen considered cases, avoids computationally expensive projections in two cases, with the proposed framework matching performance of state-of-the-art approaches in the remaining five cases. Notably, our approach for the stochastic function value-based oracle enables the first regret bounds with bandit feedback for stochastic DR-submodular functions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#39640;&#38454;&#35268;&#33539;&#31561;&#21464;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#32473;&#23450;&#24863;&#21463;&#22495;&#20869;&#24314;&#27169;&#31354;&#38388;&#25193;&#23637;&#30340;&#38750;&#32447;&#24615;&#20132;&#20114;&#65292;&#24182;&#20445;&#25345;&#23545;&#20840;&#23616;&#31561;&#24230;&#37327;&#30340;&#31561;&#21464;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16657</link><description>&lt;p&gt;
&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#39640;&#38454;&#35268;&#33539;&#31561;&#21464;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Higher Order Gauge Equivariant CNNs on Riemannian Manifolds and Applications. (arXiv:2305.16657v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#39640;&#38454;&#35268;&#33539;&#31561;&#21464;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#32473;&#23450;&#24863;&#21463;&#22495;&#20869;&#24314;&#27169;&#31354;&#38388;&#25193;&#23637;&#30340;&#38750;&#32447;&#24615;&#20132;&#20114;&#65292;&#24182;&#20445;&#25345;&#23545;&#20840;&#23616;&#31561;&#24230;&#37327;&#30340;&#31561;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#32593;&#32476;&#25991;&#29486;&#20013;&#31561;&#21464;&#21367;&#31215;&#30340;&#20986;&#29616;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#20855;&#26377; $\mathsf{SO}(3)$-&#31561;&#21464;&#23618;&#30340;&#29699;&#24418; CNN &#20197;&#22788;&#29702;&#22312;&#29699;&#24418; $S^2$ &#19978;&#30340;&#20449;&#21495;&#26679;&#26412;&#30340;&#25968;&#25454;&#12290; &#36890;&#36807;&#26174;&#24335;&#35201;&#27714;&#30456;&#23545;&#20110; $\mathsf{SO}(2)$ &#30340;&#35268;&#33539;&#31561;&#21464;&#24615;&#65292;&#21487;&#20197;&#22312; $S^2$ &#19978;&#38544;&#24335;&#33719;&#24471; $\mathsf{SO}(3)$-&#31561;&#21464;&#21367;&#31215;&#65292;&#24182;&#23454;&#29616;&#37325;&#22823;&#30340;&#25928;&#29575;&#25552;&#21319;&#12290; &#26412;&#25991;&#22312;&#27492;&#22522;&#30784;&#19978;&#24341;&#20837;&#20102;&#35268;&#33539;&#31561;&#21464;&#21367;&#31215;&#30340;&#39640;&#38454;&#27010;&#25324;&#65292;&#20854;&#23454;&#29616;&#34987;&#31216;&#20026;&#35268;&#33539;&#31561;&#21464;&#30340; Volterra &#32593;&#32476;&#65288;GEVNet&#65289;&#12290; &#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#22312;&#20445;&#25345;&#23545;&#20840;&#23616;&#31561;&#24230;&#37327;&#30340;&#31561;&#21464;&#24615;&#30340;&#21516;&#26102;&#65292;&#22312;&#32473;&#23450;&#30340;&#24863;&#21463;&#22495;&#20869;&#24314;&#27169;&#31354;&#38388;&#25193;&#23637;&#30340;&#38750;&#32447;&#24615;&#20132;&#20114;&#12290; &#25105;&#20204;&#35777;&#26126;&#20102;&#20851;&#20110;&#31561;&#21464;&#24615;&#21644;&#39640;&#38454;&#35268;&#33539;&#31561;&#21464;&#21367;&#31215;&#26500;&#36896;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#20934;&#25968;&#25454;&#65288;&#20363;&#22914;&#29699;&#24418; MNIST&#65289;&#20197;&#21450;&#22312;&#24191;&#21578;&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#21807;&#19968;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#35268;&#27169;&#24037;&#31243;&#23454;&#29616;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#21442;&#25968;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of group equivariant convolutions in deep networks literature, spherical CNNs with $\mathsf{SO}(3)$-equivariant layers have been developed to cope with data that are samples of signals on the sphere $S^2$. One can implicitly obtain $\mathsf{SO}(3)$-equivariant convolutions on $S^2$ with significant efficiency gains by explicitly requiring gauge equivariance w.r.t. $\mathsf{SO}(2)$. In this paper, we build on this fact by introducing a higher order generalization of the gauge equivariant convolution, whose implementation is dubbed a gauge equivariant Volterra network (GEVNet). This allows us to model spatially extended nonlinear interactions within a given receptive field while still maintaining equivariance to global isometries. We prove theoretical results regarding the equivariance and construction of higher order gauge equivariant convolutions. Then, we empirically demonstrate the parameter efficiency of our model, first on computer vision benchmark data (e.g. spheri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#27169;&#21644;&#39044;&#27979;&#26694;&#26550;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#32477;&#23545;&#25512;&#29702;&#33021;&#21147;&#26469;&#25552;&#39640;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#21487;&#20197;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.16646</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23569;&#26679;&#26412;&#30340;&#32477;&#23545;&#25512;&#29702;&#26469;&#25552;&#39640;&#20107;&#20214;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning. (arXiv:2305.16646v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#27169;&#21644;&#39044;&#27979;&#26694;&#26550;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#32477;&#23545;&#25512;&#29702;&#33021;&#21147;&#26469;&#25552;&#39640;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#21487;&#20197;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#25512;&#29702;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20107;&#20214;&#65292;&#24110;&#21161;&#25552;&#39640;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24314;&#27169;&#21644;&#39044;&#27979;&#26694;&#26550;&#65292;&#20854;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#32477;&#23545;&#25512;&#29702;&#20197;&#36741;&#21161;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#65306;&#20107;&#20214;&#27169;&#22411;&#22312;&#32473;&#23450;&#36807;&#21435;&#30340;&#24773;&#20917;&#19979;&#25552;&#20986;&#26410;&#26469;&#20107;&#20214;&#30340;&#39044;&#27979;; &#22312;&#20960;&#20010;&#19987;&#23478;&#27880;&#37322;&#31034;&#33539;&#30340;&#25351;&#23548;&#19979;&#65292;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#20102;&#20026;&#27599;&#20010;&#25552;&#35758;&#25552;&#20379;&#21487;&#33021;&#30340;&#21407;&#22240;; &#19968;&#20010;&#25628;&#32034;&#27169;&#22359;&#25214;&#21040;&#19982;&#21407;&#22240;&#21305;&#37197;&#30340;&#20808;&#21069;&#20107;&#20214;; &#19968;&#20010;&#35780;&#20998;&#20989;&#25968;&#23398;&#20250;&#26816;&#26597;&#26816;&#32034;&#21040;&#30340;&#20107;&#20214;&#26159;&#21542;&#23454;&#38469;&#19978;&#21487;&#20197;&#23548;&#33268;&#25552;&#35758;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65288;&#20122;&#39532;&#36874;&#35780;&#35770;&#21644;GDELT&#65289;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550; - &#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147; - &#21487;&#20197;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have shown astonishing performance on a wide range of reasoning tasks. In this paper, we investigate whether they could reason about real-world events and help improve the prediction accuracy of event sequence models. We design a modeling and prediction framework where a large language model performs abductive reasoning to assist an event sequence model: the event model proposes predictions on future events given the past; instructed by a few expert-annotated demonstrations, the language model learns to suggest possible causes for each proposal; a search module finds out the previous events that match the causes; a scoring function learns to examine whether the retrieved events could actually cause the proposal. Through extensive experiments on two challenging real-world datasets (Amazon Review and GDELT), we demonstrate that our framework -- thanks to the reasoning ability of language models -- could significantly outperform the state-of-the-art event sequence mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32477;&#23545;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;tAPE&#65292;&#20197;&#21450;&#19968;&#31181;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#30340;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#23454;&#29616;&#26041;&#27861;eRPE&#65292;&#26088;&#22312;&#25913;&#36827;Transformer&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16642</link><description>&lt;p&gt;
&#25913;&#36827;Transformer&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#20301;&#32622;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Improving Position Encoding of Transformers for Multivariate Time Series Classification. (arXiv:2305.16642v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32477;&#23545;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;tAPE&#65292;&#20197;&#21450;&#19968;&#31181;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#30340;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#23454;&#29616;&#26041;&#27861;eRPE&#65292;&#26088;&#22312;&#25913;&#36827;Transformer&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#35768;&#22810;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#22312;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26102;&#65292;Transformer&#38656;&#35201;&#26377;&#25928;&#30340;&#20301;&#32622;&#32534;&#30721;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#25490;&#24207;&#12290;&#20294;&#20301;&#32622;&#32534;&#30721;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#25928;&#24212;&#23578;&#26410;&#32463;&#36807;&#20805;&#20998;&#30340;&#30740;&#31350;&#65292;&#24182;&#19988;&#20173;&#23384;&#22312;&#20105;&#35758;&#65292;&#20363;&#22914;&#65292;&#27880;&#20837;&#32477;&#23545;&#20301;&#32622;&#32534;&#30721;&#36824;&#26159;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#26356;&#22909;&#65292;&#25110;&#32773;&#20004;&#32773;&#30340;&#32452;&#21512;&#26356;&#22909;&#12290;&#20026;&#20102;&#28548;&#28165;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#32477;&#23545;&#21644;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#32477;&#23545;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65292;&#31216;&#20026;&#26102;&#38388;&#32477;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;tAPE&#65289;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#23558;&#24207;&#21015;&#38271;&#24230;&#21644;&#36755;&#20837;&#23884;&#20837;&#32500;&#24230;&#32435;&#20837;&#20102;&#32477;&#23545;&#20301;&#32622;&#32534;&#30721;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#30340;&#23454;&#29616;&#26041;&#27861;&#65288;eRPE&#65289;&#65292;&#20197;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#25324;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have demonstrated outstanding performance in many applications of deep learning. When applied to time series data, transformers require effective position encoding to capture the ordering of the time series data. The efficacy of position encoding in time series analysis is not well-studied and remains controversial, e.g., whether it is better to inject absolute position encoding or relative position encoding, or a combination of them. In order to clarify this, we first review existing absolute and relative position encoding methods when applied in time series classification. We then proposed a new absolute position encoding method dedicated to time series data called time Absolute Position Encoding (tAPE). Our new method incorporates the series length and input embedding dimension in absolute position encoding. Additionally, we propose computationally Efficient implementation of Relative Position Encoding (eRPE) to improve generalisability for time series. We then propose 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#65292;&#25805;&#20316;&#20110;Tychonoff&#25299;&#25169;&#31354;&#38388;&#32780;&#38750;&#26377;&#38480;&#32500;&#31354;&#38388;&#30340;&#25968;&#25454;&#65292;&#32467;&#21512;&#20998;&#24067;&#31070;&#32463;&#32593;&#32476;&#21487;&#35782;&#21035;&#38543;&#26426;&#36807;&#31243;&#36335;&#24452;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#12289;&#37325;&#23614;&#20998;&#24067;&#31561;&#29305;&#24615;&#65292;&#19988;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#20219;&#24847;&#36924;&#36817;&#20219;&#20309;&#19968;&#33268;&#36830;&#32493;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.16639</link><description>&lt;p&gt;
&#36890;&#29992;&#36924;&#36817;&#21644;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Universal Approximation and the Topological Neural Network. (arXiv:2305.16639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#65292;&#25805;&#20316;&#20110;Tychonoff&#25299;&#25169;&#31354;&#38388;&#32780;&#38750;&#26377;&#38480;&#32500;&#31354;&#38388;&#30340;&#25968;&#25454;&#65292;&#32467;&#21512;&#20998;&#24067;&#31070;&#32463;&#32593;&#32476;&#21487;&#35782;&#21035;&#38543;&#26426;&#36807;&#31243;&#36335;&#24452;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#12289;&#37325;&#23614;&#20998;&#24067;&#31561;&#29305;&#24615;&#65292;&#19988;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#20219;&#24847;&#36924;&#36817;&#20219;&#20309;&#19968;&#33268;&#36830;&#32493;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#65288;TNN&#65289;&#65292;&#23427;&#20351;&#29992;&#26469;&#33258;Tychonoff&#25299;&#25169;&#31354;&#38388;&#32780;&#19981;&#26159;&#36890;&#24120;&#30340;&#26377;&#38480;&#32500;&#31354;&#38388;&#30340;&#25968;&#25454;&#12290;&#30001;&#27492;&#24341;&#20837;&#20102;&#19968;&#20010;&#25509;&#21463;Borel&#24230;&#37327;&#20026;&#25968;&#25454;&#30340;&#20998;&#24067;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12290;&#36825;&#20123;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#36215;&#26469;&#26377;&#21161;&#20110;&#35782;&#21035;&#38543;&#26426;&#36807;&#31243;&#36335;&#24452;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#12289;&#37325;&#23614;&#20998;&#24067;&#21644;&#20854;&#20182;&#29305;&#24615;&#65292;&#20063;&#26377;&#21161;&#20110;&#23545;&#31890;&#23376;&#28388;&#27874;&#25110;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#31639;&#27861;&#20135;&#29983;&#30340;&#20449;&#24565;&#29366;&#24577;&#36827;&#34892;&#25805;&#20316;&#12290;&#26412;&#25991;&#36824;&#36890;&#36807;Tychonoff&#31354;&#38388;&#30340;&#24378;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#21450;&#20854;&#24230;&#37327;&#31354;&#38388;&#30340;&#25512;&#35770;&#26469;&#35777;&#26126;&#20102;TNN&#21644;DNN&#30340;&#27491;&#30830;&#24615;&#12290;&#36825;&#20123;&#23450;&#29702;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20219;&#24847;&#36924;&#36817;&#19982;&#21807;&#19968;&#19968;&#33268;&#24615;&#24230;&#37327;&#30456;&#20851;&#30340;&#19968;&#33268;&#36830;&#32493;&#20989;&#25968;&#65288;&#20851;&#20110;&#19978;&#30830;&#30028;&#24230;&#37327;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20123;&#35752;&#35770;&#65292;&#34920;&#26126;&#27491;&#23450;&#24230;&#37327;&#19978;&#30340;&#31070;&#32463;&#32593;&#32476;&#26159;&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#8220;&#28145;&#24863;&#30693;&#26426;&#8221;&#27010;&#24565;&#30340;&#19968;&#31181;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;
A topological neural network (TNN), which takes data from a Tychonoff topological space instead of the usual finite dimensional space, is introduced. As a consequence, a distributional neural network (DNN) that takes Borel measures as data is also introduced. Combined these new neural networks facilitate things like recognizing long range dependence, heavy tails and other properties in stochastic process paths or like acting on belief states produced by particle filtering or hidden Markov model algorithms. The veracity of the TNN and DNN are then established herein by a strong universal approximation theorem for Tychonoff spaces and its corollary for spaces of measures. These theorems show that neural networks can arbitrarily approximate uniformly continuous functions (with respect to the sup metric) associated with a unique uniformity. We also provide some discussion showing that neural networks on positive-finite measures are a generalization of the recent deep learning notion of dee
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#38598;&#21512;&#21270;&#22320;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36880;&#23618;&#32534;&#30721;&#26041;&#26696;&#26469;&#32771;&#34385;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23618;&#35745;&#31639;&#32467;&#26500;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#8220;pad-chunk-encode&#8221;&#27969;&#27700;&#32447;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#39640;&#25928;&#32534;&#30721;&#22788;&#29702;&#65292;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.16625</link><description>&lt;p&gt;
&#38598;&#21512;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Set-based Neural Network Encoding. (arXiv:2305.16625v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16625
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#38598;&#21512;&#21270;&#22320;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36880;&#23618;&#32534;&#30721;&#26041;&#26696;&#26469;&#32771;&#34385;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23618;&#35745;&#31639;&#32467;&#26500;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#8220;pad-chunk-encode&#8221;&#27969;&#27700;&#32447;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#39640;&#25928;&#32534;&#30721;&#22788;&#29702;&#65292;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38598;&#21512;&#21040;&#38598;&#21512;&#21644;&#38598;&#21512;&#21040;&#21521;&#37327;&#20989;&#25968;&#26469;&#26377;&#25928;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#36827;&#34892;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#32534;&#30721;&#26041;&#27861;&#12290;&#19982;&#20043;&#21069;&#38656;&#35201;&#23545;&#19981;&#21516;&#26550;&#26500;&#32534;&#20889;&#33258;&#23450;&#20041;&#32534;&#30721;&#27169;&#22411;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23545;&#28151;&#21512;&#26550;&#26500;&#21644;&#19981;&#21516;&#21442;&#25968;&#22823;&#23567;&#30340;&#27169;&#22411;&#21160;&#24577;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340; SNE&#65288;&#38598;&#21512;&#21270;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22120;&#65289;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#36880;&#23618;&#32534;&#30721;&#26041;&#26696;&#65292;&#32771;&#34385;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23618;&#35745;&#31639;&#32467;&#26500;&#12290;&#26368;&#32456;&#23558;&#25152;&#26377;&#23618;&#27425;&#32534;&#30721;&#21512;&#24182;&#21040;&#19968;&#36215;&#65292;&#20197;&#33719;&#21462;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#30690;&#37327;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#8220;pad-chunk-encode&#8221;&#27969;&#27700;&#32447;&#26469;&#26377;&#25928;&#22320;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#35813;&#27969;&#27700;&#32447;&#21487;&#26681;&#25454;&#35745;&#31639;&#21644;&#20869;&#23384;&#38480;&#21046;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20004;&#20010;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#30340;&#26032;&#20219;&#21153;&#65306;&#36328;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#36866;&#24212;&#24615;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an approach to neural network weight encoding for generalization performance prediction that utilizes set-to-set and set-to-vector functions to efficiently encode neural network parameters. Our approach is capable of encoding neural networks in a modelzoo of mixed architecture and different parameter sizes as opposed to previous approaches that require custom encoding models for different architectures. Furthermore, our \textbf{S}et-based \textbf{N}eural network \textbf{E}ncoder (SNE) takes into consideration the hierarchical computational structure of neural networks by utilizing a layer-wise encoding scheme that culminates to encoding all layer-wise encodings to obtain the neural network encoding vector. Additionally, we introduce a \textit{pad-chunk-encode} pipeline to efficiently encode neural network layers that is adjustable to computational and memory constraints. We also introduce two new tasks for neural network generalization performance prediction: cross-dataset a
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32771;&#34385;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#34892;&#20154;&#36816;&#21160;&#36712;&#36857;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16620</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#26041;&#27861;&#39044;&#27979;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#34892;&#20154;&#36816;&#21160;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Pedestrian Trajectory Forecasting Using Deep Ensembles Under Sensing Uncertainty. (arXiv:2305.16620v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16620
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32771;&#34385;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#34892;&#20154;&#36816;&#21160;&#36712;&#36857;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32771;&#34385;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#30340;&#26041;&#27861;&#39044;&#27979;&#34892;&#20154;&#30340;&#36816;&#21160;&#36712;&#36857;&#65292;&#25512;&#27979;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;Bayes&#28388;&#27874;&#22120;&#20173;&#23384;&#22312;&#30340;&#38382;&#39064;&#26159;&#26080;&#27861;&#35299;&#20915;&#38750;&#32447;&#24615;&#21644;&#38271;&#26399;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the fundamental challenges in the prediction of dynamic agents is robustness. Usually, most predictions are deterministic estimates of future states which are over-confident and prone to error. Recently, few works have addressed capturing uncertainty during forecasting of future states. However, these probabilistic estimation methods fail to account for the upstream noise in perception data during tracking. Sensors always have noise and state estimation becomes even more difficult under adverse weather conditions and occlusion. Traditionally, Bayes filters have been used to fuse information from noisy sensors to update states with associated belief. But, they fail to address non-linearities and long-term predictions. Therefore, we propose an end-to-end estimator that can take noisy sensor measurements and make robust future state predictions with uncertainty bounds while simultaneously taking into consideration the upstream perceptual uncertainty. For the current research, we co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#36947;&#32622;&#20449;&#24230;&#19982;&#20266;&#32622;&#20449;&#24230;&#30340;&#29305;&#24449;&#25554;&#20540;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#39640;&#32570;&#22833;&#29305;&#24449;&#29575;&#30340;&#22270;&#20687;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.16618</link><description>&lt;p&gt;
&#24102;&#37096;&#20998;&#30693;&#35782;&#29305;&#24449;&#30340;&#22270;&#20687;&#32570;&#22833;&#29305;&#24449;&#34917;&#20840;&#20013;&#30340;&#32622;&#20449;&#24230;&#29305;&#24615;&#36741;&#21161;&#29305;&#24449;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Confidence-Based Feature Imputation for Graphs with Partially Known Features. (arXiv:2305.16618v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#36947;&#32622;&#20449;&#24230;&#19982;&#20266;&#32622;&#20449;&#24230;&#30340;&#29305;&#24449;&#25554;&#20540;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#39640;&#32570;&#22833;&#29305;&#24449;&#29575;&#30340;&#22270;&#20687;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#32570;&#22833;&#29305;&#24449;&#34917;&#20840;&#38382;&#39064;&#12290;&#36807;&#21435;&#26377;&#20960;&#31181;&#26041;&#27861;&#24050;&#32463;&#35299;&#20915;&#20102;&#20855;&#26377;&#32570;&#22833;&#29305;&#24449;&#30340;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#39640;&#32570;&#22833;&#29305;&#24449;&#29575;&#30340;&#24773;&#20917;&#65292;&#23427;&#20204;&#26080;&#27861;&#36991;&#20813;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#65292;&#21363;&#33410;&#28857;&#29305;&#24449;&#20013;&#30340;&#20449;&#36947;&#32622;&#20449;&#24230;&#65292;&#23427;&#34987;&#25351;&#23450;&#32473;&#27599;&#20010;&#33410;&#28857;&#30340;&#22635;&#20805;&#20449;&#36947;&#29305;&#24449;&#65292;&#20197;&#21453;&#26144;&#22635;&#20805;&#30340;&#30830;&#23450;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20266;&#32622;&#20449;&#24230;&#65292;&#20351;&#29992;&#32570;&#22833;&#29305;&#24449;&#33410;&#28857;&#19982;&#20854;&#26368;&#36817;&#30340;&#24050;&#30693;&#29305;&#24449;&#33410;&#28857;&#20043;&#38388;&#30340;&#20449;&#36947;&#26368;&#30701;&#36335;&#24452;&#36317;&#31163;&#26469;&#26367;&#25442;&#23454;&#38469;&#23398;&#20064;&#36807;&#31243;&#20013;&#32570;&#22833;&#30340;&#30495;&#23454;&#32622;&#20449;&#24230;&#12290;&#22522;&#20110;&#20266;&#32622;&#20449;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#25554;&#20540;&#26041;&#26696;&#65292;&#23427;&#25191;&#34892;&#20449;&#36947;&#20869;&#33410;&#28857;&#25193;&#25955;&#21644;&#33410;&#28857;&#20869;&#20449;&#36947;&#20256;&#25773;&#12290;&#35813;&#26041;&#26696;&#21487;&#20197;&#22312;&#38750;&#24120;&#39640;&#30340;&#32570;&#22833;&#29575;&#19979;&#65288;&#20363;&#22914;&#65292;99.5&#65285;&#65289;&#25345;&#20037;&#23384;&#22312;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates a missing feature imputation problem for graph learning tasks. Several methods have previously addressed learning tasks on graphs with missing features. However, in cases of high rates of missing features, they were unable to avoid significant performance degradation. To overcome this limitation, we introduce a novel concept of channel-wise confidence in a node feature, which is assigned to each imputed channel feature of a node for reflecting certainty of the imputation. We then design pseudo-confidence using the channel-wise shortest path distance between a missing-feature node and its nearest known-feature node to replace unavailable true confidence in an actual learning process. Based on the pseudo-confidence, we propose a novel feature imputation scheme that performs channel-wise inter-node diffusion and node-wise inter-channel propagation. The scheme can endure even at an exceedingly high missing rate (e.g., 99.5\%) and it achieves state-of-the-art accurac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Phy-DRL&#65292;&#36825;&#26159;&#19968;&#20010;&#29289;&#29702;&#27169;&#22411;&#35843;&#25972;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#26377;&#19977;&#20010;&#21019;&#26032;&#28857;&#65292;&#23427;&#20204;&#20998;&#21035;&#26159;: i)&#21069;&#30651;&#24615;&#30340;&#26410;&#30693;&#26410;&#30693;&#35757;&#32451;&#65292;ii)&#32467;&#21512;&#27531;&#24046;&#25511;&#21046;&#65292;&#20197;&#21450;iii)&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#32534;&#36753;&#12290;Phy-DRL&#33021;&#22815;&#23481;&#24525;&#26410;&#30693;&#24178;&#25200;&#65292;&#20445;&#35777;&#23433;&#20840;&#21644;&#31283;&#23450;&#65292;&#21516;&#26102;&#36981;&#23432;Bellman&#26041;&#31243;&#21644;&#22870;&#21169;&#30456;&#20851;&#30340;&#29289;&#29702;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.16614</link><description>&lt;p&gt;
&#29289;&#29702;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;: &#23433;&#20840;&#21644;&#26410;&#30693;&#26410;&#30693;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Physical Deep Reinforcement Learning: Safety and Unknown Unknowns. (arXiv:2305.16614v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Phy-DRL&#65292;&#36825;&#26159;&#19968;&#20010;&#29289;&#29702;&#27169;&#22411;&#35843;&#25972;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#26377;&#19977;&#20010;&#21019;&#26032;&#28857;&#65292;&#23427;&#20204;&#20998;&#21035;&#26159;: i)&#21069;&#30651;&#24615;&#30340;&#26410;&#30693;&#26410;&#30693;&#35757;&#32451;&#65292;ii)&#32467;&#21512;&#27531;&#24046;&#25511;&#21046;&#65292;&#20197;&#21450;iii)&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#32534;&#36753;&#12290;Phy-DRL&#33021;&#22815;&#23481;&#24525;&#26410;&#30693;&#24178;&#25200;&#65292;&#20445;&#35777;&#23433;&#20840;&#21644;&#31283;&#23450;&#65292;&#21516;&#26102;&#36981;&#23432;Bellman&#26041;&#31243;&#21644;&#22870;&#21169;&#30456;&#20851;&#30340;&#29289;&#29702;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Phy-DRL&#65292;&#19968;&#20010;&#29289;&#29702;&#27169;&#22411;&#35843;&#33410;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#33258;&#20027;&#31995;&#32479;&#12290;Phy-DRL&#20855;&#26377;&#19977;&#31181;&#29420;&#29305;&#30340;&#21019;&#26032;&#65306;i&#65289;&#21069;&#30651;&#24615;&#30340;&#26410;&#30693;&#26410;&#30693;&#35757;&#32451;&#65292;ii&#65289;&#32467;&#21512;&#27531;&#24046;&#25511;&#21046;&#65288;&#21363;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#21644;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#25511;&#21046;&#30340;&#38598;&#25104;&#65289;&#21644;&#23433;&#20840;&#21450;&#31283;&#23450;&#24615;&#25935;&#24863;&#30340;&#22870;&#21169;&#65292;&#20197;&#21450;iii&#65289;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#32534;&#36753;&#65292;&#21253;&#25324;&#38142;&#25509;&#32534;&#36753;&#21644;&#28608;&#27963;&#32534;&#36753;&#12290;&#30001;&#20110;&#36825;&#20123;&#24182;&#21457;&#35774;&#35745;&#65292;Phy-DRL&#33021;&#22815;1&#65289;&#23481;&#24525;&#26410;&#30693;&#24178;&#25200;&#65292;2&#65289;&#20445;&#35777;&#21487;&#25968;&#23398;&#35777;&#26126;&#30340;&#23433;&#20840;&#19982;&#31283;&#23450;&#24615;&#65292;&#24182;3&#65289;&#20005;&#26684;&#36981;&#23432;Bellman&#26041;&#31243;&#21644;&#22870;&#21169;&#30456;&#20851;&#30340;&#29289;&#29702;&#30693;&#35782;&#12290;&#26368;&#32456;&#65292;&#36890;&#36807;&#20498;&#31435;&#25670;&#21644;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;Phy-DRL&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;DRL&#30456;&#27604;&#65292;Phy-DRL&#20855;&#26377;&#26126;&#26174;&#26356;&#23569;&#30340;&#23398;&#20064;&#21442;&#25968;&#12289;&#21152;&#36895;&#30340;&#35757;&#32451;&#21644;&#25193;&#22823;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose the Phy-DRL: a physics-model-regulated deep reinforcement learning framework for safety-critical autonomous systems. The Phy-DRL is unique in three innovations: i) proactive unknown-unknowns training, ii) conjunctive residual control (i.e., integration of data-driven control and physics-model-based control) and safety- \&amp; stability-sensitive reward, and iii) physics-model-based neural network editing, including link editing and activation editing. Thanks to the concurrent designs, the Phy-DRL is able to 1) tolerate unknown-unknowns disturbances, 2) guarantee mathematically provable safety and stability, and 3) strictly comply with physical knowledge pertaining to Bellman equation and reward. The effectiveness of the Phy-DRL is finally validated by an inverted pendulum and a quadruped robot. The experimental results demonstrate that compared with purely data-driven DRL, Phy-DRL features remarkably fewer learning parameters, accelerated training and enlarged rew
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;, &#36890;&#36807;&#27491;&#21017;&#21270;&#28216;&#25103;&#30340;&#25903;&#20184;&#25110;&#25928;&#29992;&#21644;&#26356;&#26032;&#25237;&#30707;&#32034;&#31574;&#30053;&#65292;&#26080;&#35770;&#26159;&#21542;&#23384;&#22312;&#22122;&#22768;&#37117;&#33021;&#22815;&#23454;&#29616;&#22312;&#21333;&#35843;&#21338;&#24328;&#20013;&#35745;&#31639;&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.16610</link><description>&lt;p&gt;
&#23398;&#20064;&#21333;&#35843;&#21338;&#24328;&#30340;&#25237;&#30707;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Slingshot Approach to Learning in Monotone Games. (arXiv:2305.16610v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;, &#36890;&#36807;&#27491;&#21017;&#21270;&#28216;&#25103;&#30340;&#25903;&#20184;&#25110;&#25928;&#29992;&#21644;&#26356;&#26032;&#25237;&#30707;&#32034;&#31574;&#30053;&#65292;&#26080;&#35770;&#26159;&#21542;&#23384;&#22312;&#22122;&#22768;&#37117;&#33021;&#22815;&#23454;&#29616;&#22312;&#21333;&#35843;&#21338;&#24328;&#20013;&#35745;&#31639;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#21333;&#35843;&#21338;&#24328;&#20013;&#35745;&#31639;&#22343;&#34913;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#36981;&#24490;&#27491;&#21017;&#21270;&#39046;&#23548;&#32773;&#31639;&#27861;&#21363;&#20351;&#22312;&#21452;&#20154;&#38646;&#21644;&#28216;&#25103;&#20013;&#20063;&#26080;&#27861;&#25910;&#25947;&#21040;&#22343;&#34913;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#20048;&#35266;&#29256;&#26412;&#24182;&#20855;&#26377;&#26368;&#21518;&#36845;&#20195;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#26080;&#22122;&#22768;&#30340;&#26799;&#24230;&#21453;&#39304;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#23454;&#29616;&#26368;&#21518;&#19968;&#27425;&#36845;&#20195;&#30340;&#25910;&#25947;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#25200;&#21160;&#25110;&#27491;&#21017;&#21270;&#28216;&#25103;&#30340;&#25903;&#20184;&#25110;&#25928;&#29992;&#12290;&#36825;&#31181;&#25200;&#21160;&#26377;&#21161;&#20110;&#23558;&#24403;&#21069;&#31574;&#30053;&#25289;&#21521;&#19968;&#20010;&#38170;&#23450;&#31574;&#30053;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#25237;&#30707;&#32034;&#8221;&#31574;&#30053;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#26694;&#26550;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20174;&#32780;&#33719;&#24471;&#38752;&#36817;&#22343;&#34913;&#28857;&#30340;&#31283;&#23450;&#28857;&#65292;&#26080;&#35770;&#26159;&#21542;&#23384;&#22312;&#22122;&#22768;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23450;&#26399;&#26356;&#26032;&#25237;&#30707;&#32034;&#31574;&#30053;&#21644;&#24403;&#21069;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#35299;&#37322;&#20026;&#36817;&#31471;p
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the problem of computing equilibria in monotone games. The traditional Follow the Regularized Leader algorithms fail to converge to an equilibrium even in two-player zero-sum games. Although optimistic versions of these algorithms have been proposed with last-iterate convergence guarantees, they require noiseless gradient feedback. To overcome this limitation, we present a novel framework that achieves last-iterate convergence even in the presence of noise. Our key idea involves perturbing or regularizing the payoffs or utilities of the games. This perturbation serves to pull the current strategy to an anchored strategy, which we refer to as a {\it slingshot} strategy. First, we establish the convergence rates of our framework to a stationary point near an equilibrium, regardless of the presence or absence of noise. Next, we introduce an approach to periodically update the slingshot strategy with the current strategy. We interpret this approach as a proximal p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#21098;&#26525;&#23398;&#20064;PET&#32467;&#26500;&#24182;&#22312;GLUE&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25506;&#35752;&#20102;PET&#26550;&#26500;&#35774;&#35745;&#36873;&#25321;&#23545;&#23454;&#38469;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.16597</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models. (arXiv:2305.16597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#21098;&#26525;&#23398;&#20064;PET&#32467;&#26500;&#24182;&#22312;GLUE&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25506;&#35752;&#20102;PET&#26550;&#26500;&#35774;&#35745;&#36873;&#25321;&#23545;&#23454;&#38469;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PET&#65289;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#37096;&#20998;&#27169;&#22411;&#21442;&#25968;&#30340;&#23567;&#22411;&#21387;&#32553;&#26356;&#26032;&#25110;&#28155;&#21152;&#21644;&#24494;&#35843;&#23569;&#37327;&#26032;&#30340;&#27169;&#22411;&#21442;&#25968;&#21040;&#39044;&#35757;&#32451;&#32593;&#32476;&#65292;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#25163;&#24037;&#35774;&#35745;&#30340;PET&#26550;&#26500;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#36890;&#36807;&#33258;&#21160;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#65292;&#23427;&#20204;&#26377;&#25913;&#36827;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#21098;&#26525;&#23398;&#20064;PET&#32467;&#26500;&#30340;&#26377;&#25928;NAS&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;GLUE&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;PET&#26550;&#26500;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#23454;&#38469;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient tuning (PET) methods fit pre-trained language models (PLMs) to downstream tasks by either computing a small compressed update for a subset of model parameters, or appending and fine-tuning a small number of new model parameters to the pre-trained network. Hand-designed PET architectures from the literature perform well in practice, but have the potential to be improved via automated neural architecture search (NAS). We propose an efficient NAS method for learning PET architectures via structured and unstructured pruning. We present experiments on GLUE demonstrating the effectiveness of our algorithm and discuss how PET architectural design choices affect performance in practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#32908;&#32905;&#39592;&#39612;&#36816;&#21160;&#21644; MSK &#31995;&#32479;&#21442;&#25968;&#35782;&#21035;&#30340;&#22810;&#20998;&#36776;&#29575;&#29289;&#29702;&#20449;&#24687;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;MR PI-RNN&#65289;&#65292;&#21033;&#29992;&#24555;&#36895;&#23567;&#27874;&#21464;&#25442;&#23558;&#28151;&#21512;&#39057;&#29575;&#30340;&#36755;&#20837; sEMG &#21644;&#36755;&#20986;&#20851;&#33410;&#36816;&#21160;&#20449;&#21495;&#20998;&#35299;&#20026;&#23884;&#22871;&#30340;&#22810;&#20998;&#36776;&#29575;&#20449;&#21495;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.16593</link><description>&lt;p&gt;
&#22810;&#20998;&#36776;&#29575;&#29289;&#29702;&#20449;&#24687;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65306;&#26500;&#24314;&#19982;&#24212;&#29992;&#20110;&#32908;&#32905;&#39592;&#39612;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Multi-Resolution Physics-Informed Recurrent Neural Network: Formulation and Application to Musculoskeletal Systems. (arXiv:2305.16593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#32908;&#32905;&#39592;&#39612;&#36816;&#21160;&#21644; MSK &#31995;&#32479;&#21442;&#25968;&#35782;&#21035;&#30340;&#22810;&#20998;&#36776;&#29575;&#29289;&#29702;&#20449;&#24687;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;MR PI-RNN&#65289;&#65292;&#21033;&#29992;&#24555;&#36895;&#23567;&#27874;&#21464;&#25442;&#23558;&#28151;&#21512;&#39057;&#29575;&#30340;&#36755;&#20837; sEMG &#21644;&#36755;&#20986;&#20851;&#33410;&#36816;&#21160;&#20449;&#21495;&#20998;&#35299;&#20026;&#23884;&#22871;&#30340;&#22810;&#20998;&#36776;&#29575;&#20449;&#21495;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#20998;&#36776;&#29575;&#29289;&#29702;&#20449;&#24687;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;MR PI-RNN&#65289;&#65292;&#29992;&#20110;&#21516;&#26102;&#39044;&#27979;&#32908;&#32905;&#39592;&#39612;&#65288;MSK&#65289;&#36816;&#21160;&#21644; MSK &#31995;&#32479;&#21442;&#25968;&#30340;&#35782;&#21035;&#12290;&#30001;&#20110;&#39640;&#39057;&#34920;&#38754;&#32908;&#30005;&#22270;&#65288;sEMG&#65289;&#20449;&#21495;&#19982;&#30001; MSK &#21644;&#32908;&#32905;&#25910;&#32553;&#21160;&#21147;&#23398;&#25511;&#21046;&#30340;&#20302;&#39057;&#36523;&#20307;&#20851;&#33410;&#36816;&#21160;&#20043;&#38388;&#30340;&#26144;&#23556;&#30340;&#25361;&#25112;&#24615;&#65292;&#22240;&#27492;&#36873;&#25321;&#20102; MSK &#24212;&#29992;&#20316;&#20026;&#27169;&#22411;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#21033;&#29992;&#24555;&#36895;&#23567;&#27874;&#21464;&#25442;&#23558;&#28151;&#21512;&#39057;&#29575;&#30340;&#36755;&#20837; sEMG &#21644;&#36755;&#20986;&#20851;&#33410;&#36816;&#21160;&#20449;&#21495;&#20998;&#35299;&#20026;&#23884;&#22871;&#30340;&#22810;&#20998;&#36776;&#29575;&#20449;&#21495;&#12290;&#39044;&#27979;&#27169;&#22411;&#38543;&#21518;&#20351;&#29992;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#22312;&#36739;&#31895;&#30340;&#23610;&#24230;&#36755;&#20837;-&#36755;&#20986;&#20449;&#21495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#23558;&#35757;&#32451;&#22909;&#30340;&#21442;&#25968;&#36716;&#31227;&#21040;&#19979;&#19968;&#23618;&#32454;&#21270;&#30340;&#20449;&#21495;&#35757;&#32451;&#20013;&#12290;&#36825;&#20123;&#35757;&#32451;&#36807;&#31243;&#22312;&#36716;&#31227;&#23398;&#20064;&#30340;&#26041;&#24335;&#19979;&#36882;&#24402;&#22320;&#37325;&#22797;&#65292;&#30452;&#21040;&#23436;&#25104;&#20840;&#23610;&#24230;&#35757;&#32451;&#65288;&#21363;&#20351;&#29992;&#26410;&#32463;&#28388;&#27874;&#30340;&#20449;&#21495;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a multi-resolution physics-informed recurrent neural network (MR PI-RNN), for simultaneous prediction of musculoskeletal (MSK) motion and parameter identification of the MSK systems. The MSK application was selected as the model problem due to its challenging nature in mapping the high-frequency surface electromyography (sEMG) signals to the low-frequency body joint motion controlled by the MSK and muscle contraction dynamics. The proposed method utilizes the fast wavelet transform to decompose the mixed frequency input sEMG and output joint motion signals into nested multi-resolution signals. The prediction model is subsequently trained on coarser-scale input-output signals using a gated recurrent unit (GRU), and then the trained parameters are transferred to the next level of training with finer-scale signals. These training processes are repeated recursively under a transfer-learning fashion until the full-scale training (i.e., with unfiltered signals) is achieved
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#40065;&#26834;&#24615;&#20197;&#32553;&#23567;&#27169;&#25311;&#19982;&#30495;&#23454;&#24046;&#36317;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#20998;&#24067;&#40065;&#26834;&#20540;&#36845;&#20195;&#8221;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.16589</link><description>&lt;p&gt;
&#20855;&#26377;&#29983;&#25104;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#20998;&#24067;&#40065;&#26834;&#24615;&#30340;&#21487;&#30097;&#20215;&#26684;
&lt;/p&gt;
&lt;p&gt;
The Curious Price of Distributional Robustness in Reinforcement Learning with a Generative Model. (arXiv:2305.16589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#40065;&#26834;&#24615;&#20197;&#32553;&#23567;&#27169;&#25311;&#19982;&#30495;&#23454;&#24046;&#36317;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#20998;&#24067;&#40065;&#26834;&#20540;&#36845;&#20195;&#8221;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#20197;&#20943;&#23569;&#22312;&#23454;&#36341;&#20013;&#30340;&#27169;&#25311;&#19982;&#30495;&#23454;&#24046;&#36317;&#12290;&#25105;&#20204;&#37319;&#29992;&#20998;&#24067;&#40065;&#26834;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;RMDPs&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#31574;&#30053;&#65292;&#22312;&#37096;&#32626;&#29615;&#22659;&#33853;&#22312;&#39044;&#23450;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20869;&#26102;&#65292;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#12290;&#23613;&#31649;&#26368;&#36817;&#26377;&#20102;&#19968;&#20123;&#21162;&#21147;&#65292;&#20294;RMDPs&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#35299;&#20915;&#65292;&#26080;&#35770;&#20351;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#26159;&#20160;&#20040;&#12290;&#19981;&#28165;&#26970;&#20998;&#24067;&#40065;&#26834;&#24615;&#19982;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#30456;&#27604;&#26159;&#21542;&#20855;&#26377;&#32479;&#35745;&#23398;&#19978;&#30340;&#24433;&#21709;&#12290;&#20551;&#35774;&#26377;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#26681;&#25454;&#21517;&#20041;MDP&#32472;&#21046;&#26679;&#26412;&#65292;&#25105;&#20204;&#23558;&#25551;&#36848;RMDPs&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24403;&#30001;&#24635;&#21464;&#24046;&#65288;TV&#65289;&#36317;&#31163;&#25110;$\chi^2$&#20998;&#27495;&#25351;&#23450;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#26102;&#12290;&#22312;&#36825;&#37324;&#30740;&#31350;&#30340;&#31639;&#27861;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20998;&#24067;&#40065;&#26834;&#20540;&#36845;&#20195;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#25972;&#20010;&#33539;&#22260;&#20869;&#37117;&#26159;&#36817;&#20046;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates model robustness in reinforcement learning (RL) to reduce the sim-to-real gap in practice. We adopt the framework of distributionally robust Markov decision processes (RMDPs), aimed at learning a policy that optimizes the worst-case performance when the deployed environment falls within a prescribed uncertainty set around the nominal MDP. Despite recent efforts, the sample complexity of RMDPs remained mostly unsettled regardless of the uncertainty set in use. It was unclear if distributional robustness bears any statistical consequences when benchmarked against standard RL.  Assuming access to a generative model that draws samples based on the nominal MDP, we characterize the sample complexity of RMDPs when the uncertainty set is specified via either the total variation (TV) distance or $\chi^2$ divergence. The algorithm studied here is a model-based method called {\em distributionally robust value iteration}, which is shown to be near-optimal for the full range
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#21508;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#21033;&#29992;&#20219;&#20309;&#22238;&#24402;&#22120;&#26816;&#27979;&#25968;&#20540;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#20540;&#19982;&#33258;&#28982;&#25968;&#25454;&#27874;&#21160;&#65292;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#30495;&#27491;&#30340;&#24322;&#24120;&#21644;&#33258;&#28982;&#25968;&#25454;&#27874;&#21160;&#12290;</title><link>http://arxiv.org/abs/2305.16583</link><description>&lt;p&gt;
&#36890;&#36807;&#20219;&#24847;&#22238;&#24402;&#27169;&#22411;&#26816;&#27979;&#25968;&#20540;&#25968;&#25454;&#20013;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting Errors in Numerical Data via any Regression Model. (arXiv:2305.16583v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16583
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#21508;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#21033;&#29992;&#20219;&#20309;&#22238;&#24402;&#22120;&#26816;&#27979;&#25968;&#20540;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#20540;&#19982;&#33258;&#28982;&#25968;&#25454;&#27874;&#21160;&#65292;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#30495;&#27491;&#30340;&#24322;&#24120;&#21644;&#33258;&#28982;&#25968;&#25454;&#27874;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22122;&#22768;&#22256;&#25200;&#30528;&#35768;&#22810;&#25968;&#20540;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#25968;&#25454;&#35760;&#24405;&#30340;&#20540;&#21487;&#33021;&#30001;&#20110;&#38169;&#35823;&#30340;&#20256;&#24863;&#22120;&#12289;&#25968;&#25454;&#36755;&#20837;/&#22788;&#29702;&#38169;&#35823;&#25110;&#19981;&#23436;&#32654;&#30340;&#20154;&#31867;&#20272;&#35745;&#31561;&#21407;&#22240;&#32780;&#26080;&#27861;&#21305;&#37197;&#30495;&#23454;&#30340;&#24213;&#23618;&#20540;&#12290;&#25105;&#20204;&#32771;&#34385;&#20272;&#35745;&#27839;&#25968;&#20540;&#21015;&#21738;&#20123;&#25968;&#25454;&#20540;&#26159;&#19981;&#27491;&#30830;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#20219;&#20309;&#22238;&#24402;&#22120;&#65288;&#21363;&#22522;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#20854;&#20182;&#21464;&#37327;&#26469;&#39044;&#27979;&#35813;&#21015;&#20540;&#30340;&#32479;&#35745;&#23398;&#25110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65289;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#36890;&#36807;&#32771;&#34385;&#21508;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21306;&#20998;&#20102;&#30495;&#27491;&#30340;&#24322;&#24120;&#21644;&#33258;&#28982;&#25968;&#25454;&#27874;&#21160;&#65292;&#26465;&#20214;&#26159;&#26377;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#34920;&#26126;&#20854;&#20182;&#26041;&#27861;&#65288;&#22914;&#31526;&#21512;&#24615;&#25512;&#26029;&#65289;&#38590;&#20197;&#26816;&#27979;&#38169;&#35823;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35823;&#24046;&#26816;&#27979;&#22522;&#20934;&#65292;&#28041;&#21450; 5 &#20010;&#20855;&#26377;&#30495;&#23454;&#19990;&#30028;&#25968;&#23383;&#38169;&#35823;&#30340;&#22238;&#24402;&#25968;&#25454;&#38598;&#65288;&#23545;&#20110;&#20854;&#20013;&#30340;&#30495;&#23454;&#20540;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Noise plagues many numerical datasets, where the recorded values in the data may fail to match the true underlying values due to reasons including: erroneous sensors, data entry/processing mistakes, or imperfect human estimates. Here we consider estimating \emph{which} data values are incorrect along a numerical column. We present a model-agnostic approach that can utilize \emph{any} regressor (i.e.\ statistical or machine learning model) which was fit to predict values in this column based on the other variables in the dataset. By accounting for various uncertainties, our approach distinguishes between genuine anomalies and natural data fluctuations, conditioned on the available information in the dataset. We establish theoretical guarantees for our method and show that other approaches like conformal inference struggle to detect errors. We also contribute a new error detection benchmark involving 5 regression datasets with real-world numerical errors (for which the true values are al
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20998;&#26512;&#20102;&#26032;&#25552;&#20986;&#30340;&#26435;&#37325;&#24179;&#34913;&#26041;&#27861;&#22312;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#20013;&#26377;&#25928;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#32531;&#35299;&#31070;&#32463;&#23849;&#28291;&#21644;&#22278;&#38181;&#25928;&#24212;&#65292;&#20174;&#32780;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16573</link><description>&lt;p&gt;
&#25506;&#32034;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#20013;&#30340;&#26435;&#37325;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Exploring Weight Balancing on Long-Tailed Recognition Problem. (arXiv:2305.16573v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16573
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20998;&#26512;&#20102;&#26032;&#25552;&#20986;&#30340;&#26435;&#37325;&#24179;&#34913;&#26041;&#27861;&#22312;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#20013;&#26377;&#25928;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#32531;&#35299;&#31070;&#32463;&#23849;&#28291;&#21644;&#22278;&#38181;&#25928;&#24212;&#65292;&#20174;&#32780;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#23614;&#25968;&#25454;&#20013;&#30340;&#35782;&#21035;&#38382;&#39064;&#26368;&#36817;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#20998;&#24067;&#36890;&#24120;&#26159;&#25351;&#25968;&#20998;&#24067;&#65292;&#38500;&#38750;&#26377;&#24847;&#22320;&#35843;&#25972;&#26679;&#26412;&#25968;&#37327;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#26435;&#37325;&#24179;&#34913;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#33879;&#21517;&#30340;&#32463;&#20856;&#27491;&#21017;&#21270;&#25216;&#26415;&#21644;&#20004;&#38454;&#27573;&#35757;&#32451;&#12290;&#23613;&#31649;&#20854;&#31616;&#21333;&#24615;&#65292;&#20294;&#24050;&#30693;&#20854;&#23545;&#29616;&#26377;&#21508;&#31181;&#19981;&#21516;&#26041;&#27861;&#20855;&#26377;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#32570;&#20047;&#20026;&#20160;&#20040;&#36825;&#31181;&#26041;&#27861;&#23545;&#38271;&#23614;&#25968;&#25454;&#26377;&#25928;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#20851;&#27880;&#20102;&#31070;&#32463;&#23849;&#28291;&#21644;&#27599;&#20010;&#35757;&#32451;&#38454;&#27573;&#30340;&#22278;&#38181;&#25928;&#24212;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#20998;&#35299;&#20026;&#30001;&#26435;&#20540;&#34928;&#20943;&#21644;&#20132;&#21449;&#29109;&#25439;&#22833;&#24341;&#36215;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#20013;Fisher&#21028;&#21035;&#27604;&#30340;&#22686;&#21152;&#20197;&#21450;&#30001;&#26435;&#37325;&#34928;&#20943;&#21644;&#31867;&#24179;&#34913;&#27491;&#21017;&#21270;&#24341;&#36215;&#30340;&#38544;&#24335;&#36923;&#36753;&#35843;&#25972;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#26435;&#37325;&#24179;&#34913;&#26041;&#27861;&#25104;&#21151;&#32531;&#35299;&#20102;&#31070;&#32463;&#23849;&#28291;&#21644;&#22278;&#38181;&#25928;&#24212;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#38271;&#23614;&#25968;&#25454;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognition problems in long-tailed data, where the sample size per class is heavily skewed, have recently gained importance because the distribution of the sample size per class in a dataset is generally exponential unless the sample size is intentionally adjusted. Various approaches have been devised to address these problems. Recently, weight balancing, which combines well-known classical regularization techniques with two-stage training, has been proposed. Despite its simplicity, it is known for its high performance against existing methods devised in various ways. However, there is a lack of understanding as to why this approach is effective for long-tailed data. In this study, we analyze the method focusing on neural collapse and cone effect at each training stage and find that it can be decomposed into the increase in Fisher's discriminant ratio of the feature extractor caused by weight decay and cross entropy loss and implicit logit adjustment caused by weight decay and class-b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#20540;&#36845;&#20195;&#31639;&#27861;Anc-VI&#65292;&#37319;&#29992;&#20102;&#38170;&#23450;&#26426;&#21046;&#65292;&#21487;&#21152;&#36895;Bellman&#19968;&#33268;&#24615;&#21644;&#26368;&#20248;&#24615;&#31639;&#23376;&#30340;&#35745;&#31639;&#12290;&#23545;&#20110;$\gamma\approx 1$&#25110;$\gamma=1$&#65292;Anc-VI&#36895;&#24230;&#20026;$\mathcal{O}(1/k)$&#65292;&#27604;&#26631;&#20934;VI&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2305.16569</link><description>&lt;p&gt;
&#22522;&#20110;&#38170;&#23450;&#26426;&#21046;&#30340;&#20540;&#36845;&#20195;&#21152;&#36895;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Accelerating Value Iteration with Anchoring. (arXiv:2305.16569v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#20540;&#36845;&#20195;&#31639;&#27861;Anc-VI&#65292;&#37319;&#29992;&#20102;&#38170;&#23450;&#26426;&#21046;&#65292;&#21487;&#21152;&#36895;Bellman&#19968;&#33268;&#24615;&#21644;&#26368;&#20248;&#24615;&#31639;&#23376;&#30340;&#35745;&#31639;&#12290;&#23545;&#20110;$\gamma\approx 1$&#25110;$\gamma=1$&#65292;Anc-VI&#36895;&#24230;&#20026;$\mathcal{O}(1/k)$&#65292;&#27604;&#26631;&#20934;VI&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20540;&#36845;&#20195;(Value Iteration, VI)&#26159;&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#29702;&#35770;&#21644;&#23454;&#36341;&#30340;&#22522;&#30784;&#65292;&#24050;&#30693;&#20854;&#25910;&#25947;&#36895;&#24230;&#20026;$\mathcal{O}(\gamma^k)$&#65292;&#20854;&#20013;$\gamma$&#26159;&#25240;&#25187;&#22240;&#23376;&#12290;&#28982;&#32780;&#65292;&#22312;VI&#35774;&#32622;&#20013;&#30340;&#26368;&#20248;&#36895;&#24230;&#23578;&#26410;&#30830;&#23450;&#65292;&#23547;&#27714;&#19968;&#31181;&#36890;&#29992;&#30340;&#21152;&#36895;&#26426;&#21046;&#19968;&#30452;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#38170;&#23450;&#26426;&#21046;&#30340;VI&#21152;&#36895;&#31639;&#27861;&#65292;&#31216;&#20026;Anc-VI&#12290;&#19981;&#21516;&#20110;Nesterov&#30340;&#21152;&#36895;&#26041;&#27861;&#65292;Anc-VI&#21487;&#20197;&#21152;&#36895;Bellman&#19968;&#33268;&#24615;&#21644;&#26368;&#20248;&#24615;&#31639;&#23376;&#65292;&#36824;&#27604;&#26631;&#20934;VI&#26356;&#24555;&#22320;&#20943;&#23569;Bellman&#35823;&#24046;&#12290;&#23588;&#20854;&#26159;&#65292;&#23545;&#20110;$\gamma\approx 1$&#25110;&#29978;&#33267;$\gamma=1$&#65292;Anc-VI&#21576;&#29616;&#20986;$\mathcal{O}(1/k)$&#30340;&#36895;&#24230;&#65292;&#32780;&#26631;&#20934;VI&#22312;$\gamma\ge 1-1/k$&#26102;&#30340;&#36895;&#24230;&#20026;$\mathcal{O}(1)$&#65292;&#20854;&#20013;$k$&#26159;&#36845;&#20195;&#27425;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19982;&#19978;&#30028;&#21305;&#37197;&#30340;&#22797;&#26434;&#24615;&#19979;&#30028;&#65292;&#38500;&#20102;&#19968;&#20010;&#24120;&#25968;&#22240;&#23376;$4$&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;Anc-VI&#30340;&#21152;&#36895;&#36895;&#24230;&#30340;&#26368;&#20248;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;Anc-VI&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Value Iteration (VI) is foundational to the theory and practice of modern reinforcement learning, and it is known to converge at a $\mathcal{O}(\gamma^k)$-rate, where $\gamma$ is the discount factor. Surprisingly, however, the optimal rate for the VI setup was not known, and finding a general acceleration mechanism has been an open problem. In this paper, we present the first accelerated VI for both the Bellman consistency and optimality operators. Our method, called Anc-VI, is based on an \emph{anchoring} mechanism (distinct from Nesterov's acceleration), and it reduces the Bellman error faster than standard VI. In particular, Anc-VI exhibits a $\mathcal{O}(1/k)$-rate for $\gamma\approx 1$ or even $\gamma=1$, while standard VI has rate $\mathcal{O}(1)$ for $\gamma\ge 1-1/k$, where $k$ is the iteration count. We also provide a complexity lower bound matching the upper bound up to a constant factor of $4$, thereby establishing optimality of the accelerated rate of Anc-VI. Finally, we sh
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#35780;&#20272;&#23884;&#20837;&#36136;&#37327;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#20851;&#27880;&#22914;&#20309;&#20197;&#31283;&#23450;&#30340;&#26041;&#24335;&#36827;&#34892;&#32447;&#24615;&#20998;&#31163;&#12290;&#20174;&#35843;&#26597;&#30340;&#25991;&#29486;&#21644;&#24341;&#20837;&#30340;&#26032;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#35780;&#20272;&#23884;&#20837;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#34920;&#29616;&#12290;(This work proposes new methods to evaluate the quality of embeddings, focusing on stable linear separation. From the surveyed literature and introduced novel methods, we can evaluate the quality of embeddings and improve the performance of unsupervised learning.)</title><link>http://arxiv.org/abs/2305.16562</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23884;&#20837;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Embedding Quality Evaluation. (arXiv:2305.16562v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16562
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#35780;&#20272;&#23884;&#20837;&#36136;&#37327;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#20851;&#27880;&#22914;&#20309;&#20197;&#31283;&#23450;&#30340;&#26041;&#24335;&#36827;&#34892;&#32447;&#24615;&#20998;&#31163;&#12290;&#20174;&#35843;&#26597;&#30340;&#25991;&#29486;&#21644;&#24341;&#20837;&#30340;&#26032;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#35780;&#20272;&#23884;&#20837;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#34920;&#29616;&#12290;(This work proposes new methods to evaluate the quality of embeddings, focusing on stable linear separation. From the surveyed literature and introduced novel methods, we can evaluate the quality of embeddings and improve the performance of unsupervised learning.)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26368;&#36817;&#22312;&#23398;&#26415;&#30028;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#21457;&#23637;&#12290;&#34429;&#28982;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#25509;&#36817;&#30417;&#30563;&#23398;&#20064;&#27700;&#24179;&#30340;&#25104;&#26524;&#65292;&#20294;&#30001;&#20110;&#26080;&#30417;&#30563;&#38382;&#39064;&#30340;&#26412;&#36136;&#65292;&#23454;&#36341;&#20013;&#35757;&#32451;&#21644;&#35780;&#20272; SSL &#27169;&#22411;&#20173;&#28982;&#24456;&#22256;&#38590;&#12290;&#21363;&#20351;&#26159;&#20197;&#26377;&#30417;&#30563;&#30340;&#26041;&#24335;&#35757;&#32451;&#30340;&#32593;&#32476;&#65292;&#22312;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#39046;&#22495;&#26102;&#26159;&#21542;&#33021;&#22815;&#33391;&#22909;&#22320;&#34920;&#29616;&#65292;&#20063;&#24448;&#24448;&#19981;&#28165;&#26970;&#12290;&#36807;&#21435;&#30340;&#24037;&#20316;&#36890;&#24120;&#20165;&#38480;&#20110;&#35780;&#20272;&#23884;&#20837;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#37327;&#65292;&#36825;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#26368;&#20026;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#36825;&#39033;&#24037;&#20316;&#36873;&#25321;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65306;&#25105;&#20204;&#33021;&#21542;&#37327;&#21270;&#25968;&#25454;&#20013;&#22914;&#20309;&#20197;&#31283;&#23450;&#30340;&#26041;&#24335;&#36827;&#34892;&#32447;&#24615;&#20998;&#31163;&#65311;&#25105;&#20204;&#35843;&#26597;&#20102;&#30456;&#20851;&#30340;&#25991;&#29486;&#65292;&#24182;&#21457;&#29616;&#19977;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#26399;&#23545;&#39640;&#32500;&#31354;&#38388;&#29702;&#35299;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised learning has recently significantly gained in popularity, especially with deep learning-based approaches. Despite numerous successes and approaching supervised-level performance on a variety of academic benchmarks, it is still hard to train and evaluate SSL models in practice due to the unsupervised nature of the problem. Even with networks trained in a supervised fashion, it is often unclear whether they will perform well when transferred to another domain.  Past works are generally limited to assessing the amount of information contained in embeddings, which is most relevant for self-supervised learning of deep neural networks. This works chooses to follow a different approach: can we quantify how easy it is to linearly separate the data in a stable way? We survey the literature and uncover three methods that could be potentially used for evaluating quality of representations. We also introduce one novel method based on recent advances in understanding the high-dimension
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31867;&#22686;&#37327;&#20449;&#24687;&#25552;&#21462;&#20013;&#20998;&#31867;&#22120;&#28418;&#31227;&#22914;&#20309;&#23548;&#33268;&#36951;&#24536;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22235;&#31181;&#35299;&#20915;&#26041;&#26696;&#26469;&#32531;&#35299;&#20998;&#31867;&#22120;&#28418;&#31227;&#12290;</title><link>http://arxiv.org/abs/2305.16559</link><description>&lt;p&gt;
&#22242;&#38431;&#21512;&#20316;&#24182;&#19981;&#24635;&#26159;&#22909;&#30340;&#65306;&#31867;&#22686;&#37327;&#20449;&#24687;&#25552;&#21462;&#20013;&#20998;&#31867;&#22120;&#28418;&#31227;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Teamwork Is Not Always Good: An Empirical Study of Classifier Drift in Class-incremental Information Extraction. (arXiv:2305.16559v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31867;&#22686;&#37327;&#20449;&#24687;&#25552;&#21462;&#20013;&#20998;&#31867;&#22120;&#28418;&#31227;&#22914;&#20309;&#23548;&#33268;&#36951;&#24536;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22235;&#31181;&#35299;&#20915;&#26041;&#26696;&#26469;&#32531;&#35299;&#20998;&#31867;&#22120;&#28418;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22686;&#37327;&#23398;&#20064;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#23398;&#20064;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#19981;&#26029;&#20174;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#26032;&#31867;&#65292;&#32780;&#19981;&#20250;&#24536;&#35760;&#20043;&#21069;&#23398;&#20064;&#36807;&#30340;&#31867;&#12290;&#28982;&#32780;&#65292;&#24403;&#23398;&#20064;&#22686;&#37327;&#31867;&#26102;&#65292;&#20998;&#31867;&#22120;&#24517;&#39035;&#19981;&#26029;&#26356;&#26032;&#20197;&#32435;&#20837;&#26032;&#31867;&#65292;&#24182;&#19988;&#20915;&#31574;&#36793;&#30028;&#30340;&#28418;&#31227;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#26681;&#26412;&#24615;&#25361;&#25112;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#22312;&#19981;&#23384;&#20648;&#26087;&#31867;&#21035;&#26679;&#26412;&#20197;&#36827;&#34892;&#37325;&#28436;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#26356;&#35814;&#32454;&#22320;&#30740;&#31350;&#20102;&#20998;&#31867;&#22120;&#28418;&#31227;&#22914;&#20309;&#23548;&#33268;&#36951;&#24536;&#65292;&#24182;&#25454;&#27492;&#35774;&#35745;&#20102;&#22235;&#31181;&#31616;&#21333;&#20294;&#65288;&#36229;&#32423;&#65289;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#32531;&#35299;&#20998;&#31867;&#22120;&#28418;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-incremental learning (CIL) aims to develop a learning system that can continually learn new classes from a data stream without forgetting previously learned classes. When learning classes incrementally, the classifier must be constantly updated to incorporate new classes, and the drift in decision boundary may lead to severe forgetting. This fundamental challenge, however, has not yet been studied extensively, especially in the setting where no samples from old classes are stored for rehearsal. In this paper, we take a closer look at how the drift in the classifier leads to forgetting, and accordingly, design four simple yet (super-) effective solutions to alleviate the classifier drift: an Individual Classifiers with Frozen Feature Extractor (ICE) framework where we individually train a classifier for each learning session, and its three variants ICE-PL, ICE-O, and ICE-PL&amp;O which further take the logits of previously learned classes from old sessions or a constant logit of an Ot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#30340;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;&#31639;&#27861;(TreeDSB)&#26469;&#35299;&#20915;&#22810;&#20803;&#26368;&#20248;&#36755;&#36816;(mOT)&#30340;&#38382;&#39064;&#65292;&#24182;&#21487;&#20197;&#24212;&#29992;&#20110;&#39640;&#32500;&#35774;&#32622;&#22914;&#22270;&#20687;&#25554;&#20540;&#21644;&#36125;&#21494;&#26031;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.16557</link><description>&lt;p&gt;
&#22522;&#20110;&#26641;&#30340;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;&#31639;&#27861;&#22312;Wasserstein&#37325;&#24515;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tree-Based Diffusion Schr\"odinger Bridge with Applications to Wasserstein Barycenters. (arXiv:2305.16557v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#30340;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;&#31639;&#27861;(TreeDSB)&#26469;&#35299;&#20915;&#22810;&#20803;&#26368;&#20248;&#36755;&#36816;(mOT)&#30340;&#38382;&#39064;&#65292;&#24182;&#21487;&#20197;&#24212;&#29992;&#20110;&#39640;&#32500;&#35774;&#32622;&#22914;&#22270;&#20687;&#25554;&#20540;&#21644;&#36125;&#21494;&#26031;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26368;&#20248;&#36755;&#36816;(mOT)&#26159;&#26368;&#20248;&#36755;&#36816;(OT)&#30340;&#19968;&#31181;&#25512;&#24191;&#65292;&#20854;&#26088;&#22312;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#30456;&#23545;&#20110;&#26576;&#20123;&#39044;&#20808;&#25351;&#23450;&#30340;&#36793;&#38469;&#20998;&#24067;&#30340;&#31215;&#20998;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#26641;&#24418;&#20108;&#27425;&#25104;&#26412;&#30340;&#29109;&#29256;&#26412;&#65292;&#21363;&#19968;&#31181;&#21487;&#20197;&#20889;&#20316;&#26641;&#33410;&#28857;&#20043;&#38388;&#25104;&#23545;&#25104;&#26412;&#20989;&#25968;&#20043;&#21644;&#30340;&#20989;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Tree-based Diffusion Schr\"odinger Bridge(TreeDSB)&#65292;&#36825;&#26159;&#25193;&#23637;&#20102;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;(DSB)&#31639;&#27861;&#30340;&#31639;&#27861;&#12290;TreeDSB&#23545;&#24212;&#20110;&#22810;&#20803;Sinkhorn&#31639;&#27861;&#30340;&#21160;&#24577;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#26174;&#33879;&#24212;&#29992;&#26159;&#35745;&#31639;Wasserstein&#37325;&#24515;&#65292;&#23427;&#21487;&#20197;&#34987;&#37325;&#26032;&#36716;&#21270;&#20026;&#22522;&#20110;&#26143;&#24418;&#26641;&#30340;mOT&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#39640;&#32500;&#35774;&#32622;&#65292;&#22914;&#22270;&#20687;&#25554;&#20540;&#21644;&#36125;&#21494;&#26031;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-marginal Optimal Transport (mOT), a generalization of OT, aims at minimizing the integral of a cost function with respect to a distribution with some prescribed marginals. In this paper, we consider an entropic version of mOT with a tree-structured quadratic cost, i.e., a function that can be written as a sum of pairwise cost functions between the nodes of a tree. To address this problem, we develop Tree-based Diffusion Schr\"odinger Bridge (TreeDSB), an extension of the Diffusion Schr\"odinger Bridge (DSB) algorithm. TreeDSB corresponds to a dynamic and continuous state-space counterpart of the multimarginal Sinkhorn algorithm. A notable use case of our methodology is to compute Wasserstein barycenters which can be recast as the solution of a mOT problem on a star-shaped tree. We demonstrate that our methodology can be applied in high-dimensional settings such as image interpolation and Bayesian fusion.
&lt;/p&gt;</description></item><item><title>LANISTR&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26694;&#26550;&#65292;&#21487;&#20174;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#22312;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.16556</link><description>&lt;p&gt;
LANISTR&#65306;&#20174;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#36827;&#34892;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LANISTR: Multimodal Learning from Structured and Unstructured Data. (arXiv:2305.16556v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16556
&lt;/p&gt;
&lt;p&gt;
LANISTR&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26694;&#26550;&#65292;&#21487;&#20174;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#22312;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#24050;&#32463;&#22312;&#22788;&#29702;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#65289;&#26041;&#38754;&#23637;&#29616;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#20294;&#26159;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#26368;&#24120;&#35265;&#30340;&#24773;&#20917;&#26159;&#32467;&#26500;&#21270;&#65288;&#21253;&#25324;&#34920;&#26684;&#21644;&#26102;&#38388;&#24207;&#21015;&#65289;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#32467;&#21512;&#65292;&#20294;&#36825;&#19968;&#39046;&#22495;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LANISTR&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#35821;&#35328;&#12289;&#22270;&#20687;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#22810;&#27169;&#24577;&#36974;&#32617;&#25439;&#22833;&#65292;&#20351;&#24471;LANISTR&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#23398;&#20064;&#36328;&#27169;&#24577;&#20851;&#31995;&#65292;&#24182;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#22788;&#29702;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;MIMIC-IV&#21644;Amazon Product Review&#19978;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30456;&#27604;&#65292;LANISTR&#20998;&#21035;&#36798;&#21040;&#20102;6.47%&#65288;AUROC&#65289;&#21644;&#39640;&#36798;17.69%&#65288;&#20934;&#30830;&#24230;&#65289;&#30340;&#32477;&#23545;&#25552;&#21319;&#65292;&#24182;&#26174;&#31034;&#20986;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal large-scale pretraining has shown impressive performance gains for unstructured data including language, image, audio, and video. Yet, the scenario most prominent in real-world applications is the existence of combination of structured (including tabular and time-series) and unstructured data, and this has so far been understudied. Towards this end, we propose LANISTR, a novel attention-based framework to learn from LANguage, Image, and STRuctured data. We introduce a new multimodal fusion module with a similarity-based multimodal masking loss that enables LANISTR to learn cross-modal relations from large-scale multimodal data with missing modalities during training and test time. On two publicly available challenging datasets, MIMIC-IV and Amazon Product Review, LANISTR achieves absolute improvements of 6.47% (AUROC) and up to 17.69% (accuracy), respectively, compared to the state-of-the-art multimodal models while showing superior generalization capabilities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;LSTM&#21644;BLSTM&#20004;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#30005;&#21147;&#28040;&#32791;&#30701;&#26399;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22235;&#20010;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#34920;&#26126;BLSTM&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.16546</link><description>&lt;p&gt;
&#27604;&#36739;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#21452;&#21521;LSTM&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#30005;&#21147;&#28040;&#32791;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Comparing Long Short-Term Memory (LSTM) and Bidirectional LSTM Deep Neural Networks for power consumption prediction. (arXiv:2305.16546v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;LSTM&#21644;BLSTM&#20004;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#30005;&#21147;&#28040;&#32791;&#30701;&#26399;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22235;&#20010;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#34920;&#26126;BLSTM&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#28040;&#32791;&#39044;&#27979;&#26041;&#27861;&#26159;&#20026;&#20102;&#20915;&#31574;&#33410;&#33021;&#20197;&#21450;&#22312;&#33021;&#28304;&#24066;&#22330;&#20013;&#39044;&#27979;&#38656;&#27714;&#31561;&#22810;&#31181;&#21407;&#22240;&#32780;&#36827;&#34892;&#30740;&#31350;&#30340;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#27604;&#36739;&#20004;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21363;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#21452;&#21521;LSTM&#65288;BLSTM&#65289;&#65292;&#29992;&#20110;&#21333;&#21464;&#37327;&#30005;&#24230;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#30701;&#26399;&#39044;&#27979;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36873;&#25321;&#20102;&#22235;&#20010;&#19981;&#21516;&#19978;&#19979;&#25991;&#21644;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#22235;&#20010;&#25968;&#25454;&#38598;&#20998;&#21035;&#26159;&#65306;&#65288;a&#65289;&#27861;&#22269;&#23478;&#24237;&#30340;&#29992;&#30005;&#37327;&#65307;&#65288;b&#65289;&#24052;&#35199;Santa&#233;m&#30340;&#19968;&#24231;&#22823;&#23398;&#24314;&#31569;&#30340;&#29992;&#30005;&#37327;&#65307;&#65288;c&#65289;&#25705;&#27931;&#21733;T&#233;touan&#24066;&#30340;&#29992;&#30005;&#38656;&#27714;&#65307;&#65288;d&#65289;&#26032;&#21152;&#22369;&#32858;&#21512;&#30005;&#21147;&#38656;&#27714;&#12290;&#37319;&#29992;&#26102;&#38388;&#24207;&#21015;&#20132;&#21449;&#39564;&#35777;&#26041;&#26696;&#35745;&#31639;&#20102;RMSE&#12289;MAE&#12289;MAPE&#21644;R2&#31561;&#25351;&#26631;&#12290;&#23545;&#24402;&#19968;&#21270;RMSE&#65288;NRMSE&#65289;&#30340;&#32467;&#26524;&#24212;&#29992;&#20102;Friedman&#26816;&#39564;&#65292;&#34920;&#26126;BLSTM&#27604;LSTM&#34920;&#29616;&#26356;&#22909;&#24182;&#19988;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electric consumption prediction methods are investigated for many reasons such as decision-making related to energy efficiency as well as for anticipating demand in the energy market dynamics. The objective of the present work is the comparison between two Deep Learning models, namely the Long Short-Term Memory (LSTM) and Bi-directional LSTM (BLSTM) for univariate electric consumption Time Series (TS) short-term forecast. The Data Sets (DSs) were selected for their different contexts and scales, aiming the assessment of the models' robustness. Four DSs were used, related to the power consumption of: (a) a household in France; (b) a university building in Santar\'em, Brazil; (c) the T\'etouan city zones, in Morocco; and (c) the Singapore aggregated electric demand. The metrics RMSE, MAE, MAPE and R2 were calculated in a TS cross-validation scheme. The Friedman's test was applied to normalized RMSE (NRMSE) results, showing that BLSTM outperforms LSTM with statistically significant differ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#26816;&#27979;&#24433;&#21709;&#34892;&#21160;&#30340;&#25805;&#20316;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#30830;&#23450;&#19982;&#20219;&#20309;&#25805;&#20316;&#26080;&#20851;&#30340;&#25351;&#26631;&#65292;&#20351;&#29992;&#22270;&#23398;&#20064;&#26469;&#32534;&#30721;&#21327;&#35843;&#25805;&#32437;&#30340;&#25277;&#35937;&#26631;&#24535;&#65292;&#24182;&#22312;&#19981;&#21516;&#25805;&#20316;&#20043;&#38388;&#36827;&#34892;&#36328;&#25805;&#20316;&#27867;&#21270;&#65292;&#26377;&#26395;&#20026;&#39044;&#38450;&#24433;&#21709;&#34892;&#21160;&#25552;&#20379;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2305.16544</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#23398;&#20064;&#36827;&#34892;&#24402;&#32435;&#24335;&#26816;&#27979;&#24433;&#21709;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
Inductive detection of Influence Operations via Graph Learning. (arXiv:2305.16544v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#26816;&#27979;&#24433;&#21709;&#34892;&#21160;&#30340;&#25805;&#20316;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#30830;&#23450;&#19982;&#20219;&#20309;&#25805;&#20316;&#26080;&#20851;&#30340;&#25351;&#26631;&#65292;&#20351;&#29992;&#22270;&#23398;&#20064;&#26469;&#32534;&#30721;&#21327;&#35843;&#25805;&#32437;&#30340;&#25277;&#35937;&#26631;&#24535;&#65292;&#24182;&#22312;&#19981;&#21516;&#25805;&#20316;&#20043;&#38388;&#36827;&#34892;&#36328;&#25805;&#20316;&#27867;&#21270;&#65292;&#26377;&#26395;&#20026;&#39044;&#38450;&#24433;&#21709;&#34892;&#21160;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24433;&#21709;&#34892;&#21160;&#26159;&#19968;&#31181;&#22823;&#35268;&#27169;&#25805;&#20316;&#65292;&#26088;&#22312;&#25805;&#32437;&#20844;&#20247;&#33286;&#35770;&#12290;&#24555;&#36895;&#26816;&#27979;&#21644;&#24178;&#25200;&#36825;&#20123;&#25805;&#20316;&#23545;&#20581;&#24247;&#30340;&#20844;&#20849;&#35805;&#35821;&#33267;&#20851;&#37325;&#35201;&#12290;&#26032;&#20852;AI&#25216;&#26415;&#21487;&#33021;&#20250;&#21551;&#29992;&#26032;&#30340;&#25805;&#20316;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#35268;&#36991;&#24403;&#21069;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#24433;&#21709;&#26356;&#22823;&#30340;&#35268;&#27169;&#12289;&#33539;&#22260;&#21644;&#29305;&#24322;&#24615;&#12290;&#20026;&#20102;&#22312;&#36825;&#20123;&#26032;&#25805;&#20316;&#25913;&#21464;&#20844;&#20247;&#24847;&#35265;&#21644;&#20107;&#20214;&#20043;&#21069;&#35782;&#21035;&#36825;&#20123;&#26032;&#25805;&#20316;&#65292;&#38656;&#35201;&#20855;&#26377;&#24402;&#32435;&#23398;&#20064;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#24402;&#32435;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#65306;1)&#30830;&#23450;&#19981;&#29305;&#23450;&#20110;&#20219;&#20309;&#25805;&#20316;&#30340;&#22522;&#20110;&#20869;&#23481;&#21644;&#22270;&#24418;&#30340;&#25351;&#26631;&#65307;2)&#20351;&#29992;&#22270;&#23398;&#20064;&#26469;&#32534;&#30721;&#21327;&#35843;&#25805;&#32437;&#30340;&#25277;&#35937;&#26631;&#24535;&#65307;3)&#36890;&#36807;&#23545;&#26469;&#33258;&#20420;&#32599;&#26031;&#12289;&#20013;&#22269;&#21644;&#20234;&#26391;&#30340;&#25805;&#20316;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#26469;&#35780;&#20272;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#26694;&#26550;&#33021;&#22815;&#22312;&#36328;&#25805;&#20316;&#27867;&#21270;&#26041;&#38754;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#26174;&#33879;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Influence operations are large-scale efforts to manipulate public opinion. The rapid detection and disruption of these operations is critical for healthy public discourse. Emergent AI technologies may enable novel operations which evade current detection methods and influence public discourse on social media with greater scale, reach, and specificity. New methods with inductive learning capacity will be needed to identify these novel operations before they indelibly alter public opinion and events. We develop an inductive learning framework which: 1) determines content- and graph-based indicators that are not specific to any operation; 2) uses graph learning to encode abstract signatures of coordinated manipulation; and 3) evaluates generalization capacity by training and testing models across operations originating from Russia, China, and Iran. We find that this framework enables strong cross-operation generalization while also revealing salient indicators$\unicode{x2013}$illustrating
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#20351;&#29992;&#22522;&#20110;&#20998;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#33719;&#24471;&#37327;&#23376;&#22810;&#20307;&#22522;&#24577;&#24615;&#36136;&#65292;&#36890;&#36807;Langevin&#21160;&#21147;&#23398;&#36827;&#34892;&#37319;&#26679;&#65292;&#20934;&#30830;&#35745;&#31639;&#22522;&#24577;&#12290;</title><link>http://arxiv.org/abs/2305.16540</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#27169;&#22411;&#30340;&#31070;&#32463;&#27874;&#20989;&#25968;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Score-Based Model for Learning Neural Wavefunctions. (arXiv:2305.16540v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#20351;&#29992;&#22522;&#20110;&#20998;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#33719;&#24471;&#37327;&#23376;&#22810;&#20307;&#22522;&#24577;&#24615;&#36136;&#65292;&#36890;&#36807;Langevin&#21160;&#21147;&#23398;&#36827;&#34892;&#37319;&#26679;&#65292;&#20934;&#30830;&#35745;&#31639;&#22522;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#37327;&#23376;&#33945;&#29305;&#21345;&#32599;&#19982;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#30456;&#32467;&#21512;&#24050;&#32463;&#22312;&#35745;&#31639;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#30340;&#22522;&#24577;&#26041;&#38754;&#26174;&#31034;&#20986;&#25104;&#21151;&#12290;&#29616;&#26377;&#30340;&#20248;&#21270;&#26041;&#27861;&#36890;&#36807;&#20174;&#27874;&#20989;&#25968;&#32473;&#23450;&#30340;&#26174;&#24335;&#27010;&#29575;&#20998;&#24067;&#20013;&#37319;&#26679;&#23616;&#37096;&#33021;&#37327;&#26469;&#35745;&#31639;&#33021;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#29992;&#20110;&#20351;&#29992;&#22522;&#20110;&#20998;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#33719;&#24471;&#37327;&#23376;&#22810;&#20307;&#22522;&#24577;&#24615;&#36136;&#30340;&#26032;&#20248;&#21270;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26032;&#26694;&#26550;&#19981;&#38656;&#35201;&#26174;&#24335;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#32780;&#26159;&#36890;&#36807;Langevin&#21160;&#21147;&#23398;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#23616;&#37096;&#33021;&#37327;&#19982;&#20998;&#25968;&#20043;&#38388;&#30452;&#25509;&#30340;&#20851;&#31995;&#36825;&#19968;&#20851;&#38190;&#35266;&#23519;&#12290;&#28789;&#24863;&#26469;&#33258;&#20998;&#25968;&#37197;&#23545;&#21644;&#25193;&#25955;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#21152;&#26435;&#20998;&#25968;&#21305;&#37197;&#30446;&#26631;&#65292;&#20197;&#24341;&#23548;&#22522;&#20110;&#20998;&#25968;&#30340;&#27169;&#22411;&#27491;&#30830;&#22320;&#25910;&#25947;&#21040;&#22522;&#24577;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#37327;&#23376;&#35856;&#25391;&#23376;&#23454;&#39564;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#21487;&#20197;&#20934;&#30830;&#22320;&#35745;&#31639;&#20986;&#22522;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum Monte Carlo coupled with neural network wavefunctions has shown success in computing ground states of quantum many-body systems. Existing optimization approaches compute the energy by sampling local energy from an explicit probability distribution given by the wavefunction. In this work, we provide a new optimization framework for obtaining properties of quantum many-body ground states using score-based neural networks. Our new framework does not require explicit probability distribution and performs the sampling via Langevin dynamics. Our method is based on the key observation that the local energy is directly related to scores, defined as the gradient of the logarithmic wavefunction. Inspired by the score matching and diffusion Monte Carlo methods, we derive a weighted score matching objective to guide our score-based models to converge correctly to ground states. We first evaluate our approach with experiments on quantum harmonic traps, and results show that it can accuratel
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#23545;&#20110;&#26377;&#30417;&#30563;&#30340;&#24773;&#20917;&#26131;&#20110;&#20135;&#29983;&#31867;&#22349;&#22604;&#65292;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#26131;&#20110;&#25233;&#21046;&#31867;&#21035;&#30456;&#20851;&#30340;&#22797;&#26434;&#29305;&#24449;&#65307;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20559;&#21521;&#20110;&#23547;&#25214;&#26356;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23548;&#33268;&#36825;&#31181;&#29616;&#35937;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.16536</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#23398;&#21040;&#20102;&#21738;&#20123;&#29305;&#24449;&#65311;&#20851;&#20110;&#31616;&#26131;&#20559;&#24046;&#22312;&#31867;&#22349;&#22604;&#21644;&#29305;&#24449;&#25233;&#21046;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Which Features are Learnt by Contrastive Learning? On the Role of Simplicity Bias in Class Collapse and Feature Suppression. (arXiv:2305.16536v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16536
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#23545;&#20110;&#26377;&#30417;&#30563;&#30340;&#24773;&#20917;&#26131;&#20110;&#20135;&#29983;&#31867;&#22349;&#22604;&#65292;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#26131;&#20110;&#25233;&#21046;&#31867;&#21035;&#30456;&#20851;&#30340;&#22797;&#26434;&#29305;&#24449;&#65307;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20559;&#21521;&#20110;&#23547;&#25214;&#26356;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23548;&#33268;&#36825;&#31181;&#29616;&#35937;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#20855;&#22791;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#26377;&#30417;&#30563;&#22330;&#26223;&#19979;&#26131;&#20110;&#22349;&#22604;&#21516;&#19968;&#31867;&#21035;&#20869;&#30340;&#23376;&#31867;&#34920;&#31034;&#65292;&#20002;&#22833;&#19968;&#37096;&#20998;&#29305;&#24449;&#20449;&#24687;&#65307;&#32780;&#26080;&#30417;&#30563;&#23398;&#20064;&#21017;&#21487;&#33021;&#36890;&#36807;&#23398;&#20064;&#26131;&#20110;&#22788;&#29702;&#30340;&#31867;&#21035;&#26080;&#20851;&#29305;&#24449;&#32780;&#26080;&#35270;&#19968;&#20123;&#31867;&#21035;&#30456;&#20851;&#30340;&#22797;&#26434;&#29305;&#24449;&#20449;&#24687;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20250;&#26174;&#33879;&#22320;&#38477;&#20302;&#34920;&#24449;&#30340;&#36136;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#32479;&#19968;&#20005;&#35880;&#30340;&#26694;&#26550;&#26469;&#29702;&#35299;&#27979;&#35797;&#26102;&#30340;&#31867;&#22349;&#22604;&#21644;&#29305;&#24449;&#25233;&#21046;&#20135;&#29983;&#30340;&#21407;&#22240;&#65292;&#30456;&#20851;&#20998;&#26512;&#34920;&#26126;&#65292;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20559;&#21521;&#20110;&#23547;&#25214;&#26356;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23548;&#33268;&#23376;&#31867;&#34920;&#31034;&#22349;&#22604;&#21644;&#31867;&#21035;&#30456;&#20851;&#30340;&#22797;&#26434;&#29305;&#24449;&#34987;&#25233;&#21046;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#25552;&#39640;&#23884;&#20837;&#32500;&#24230;&#21644;&#25913;&#36827;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#25552;&#20379;&#26377;&#25928;&#30340;&#39044;&#38450;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning (CL) has emerged as a powerful technique for representation learning, with or without label supervision. However, supervised CL is prone to collapsing representations of subclasses within a class by not capturing all their features, and unsupervised CL may suppress harder class-relevant features by focusing on learning easy class-irrelevant features; both significantly compromise representation quality. Yet, there is no theoretical understanding of \textit{class collapse} or \textit{feature suppression} at \textit{test} time. We provide the first unified theoretically rigorous framework to determine \textit{which} features are learnt by CL. Our analysis indicate that, perhaps surprisingly, bias of (stochastic) gradient descent towards finding simpler solutions is a key factor in collapsing subclass representations and suppressing harder class-relevant features. Moreover, we present increasing embedding dimensionality and improving the quality of data augmentations 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#36890;&#36807;&#21152;&#26435;&#34928;&#20943;&#35757;&#32451;&#30340;&#22810;&#36755;&#20986;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#20989;&#25968;&#31867;&#22411;&#21644;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#26032;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.16534</link><description>&lt;p&gt;
&#21521;&#37327;&#20540;&#21464;&#20998;&#31354;&#38388;&#21644;DNN&#30340;&#23485;&#24230;&#30028;&#65306;&#20851;&#20110;&#26435;&#37325;&#34928;&#20943;&#27491;&#21017;&#21270;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vector-Valued Variation Spaces and Width Bounds for DNNs: Insights on Weight Decay Regularization. (arXiv:2305.16534v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16534
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#36890;&#36807;&#21152;&#26435;&#34928;&#20943;&#35757;&#32451;&#30340;&#22810;&#36755;&#20986;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#20989;&#25968;&#31867;&#22411;&#21644;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#26368;&#23567;&#21270;&#25439;&#22833;&#39033;&#21644;&#24179;&#26041;&#26435;&#37325;&#21644;&#30456;&#24212;&#65292;&#23545;&#24212;&#20110;&#35757;&#32451;&#21152;&#26435;&#34928;&#20943;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#26377;&#20851;&#36825;&#31181;&#24120;&#35265;&#23398;&#20064;&#26694;&#26550;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#35757;&#32451;&#21152;&#26435;&#34928;&#20943;&#20197;&#33719;&#24471;&#22810;&#36755;&#20986;(&#21521;&#37327;&#20540;)ReLU&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#20989;&#25968;&#31867;&#22411;&#12290;&#36825;&#25193;&#23637;&#20102;&#20808;&#21069;&#38480;&#20110;&#21333;&#36755;&#20986;(&#26631;&#37327;&#20540;)&#32593;&#32476;&#30340;&#34920;&#24449;&#12290;&#36825;&#31181;&#34920;&#24449;&#38656;&#35201;&#23450;&#20041;&#25105;&#20204;&#31216;&#20043;&#20026;&#21521;&#37327;&#20540;&#21464;&#20998;(VV)&#31354;&#38388;&#30340;&#26032;&#31867;&#31070;&#32463;&#20989;&#25968;&#31354;&#38388;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#34920;&#24449;&#23450;&#29702;&#35777;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;(NNs)&#26159;&#36890;&#36807;VV&#31354;&#38388;&#20013;&#25552;&#20986;&#23398;&#20064;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#12290;&#36825;&#20010;&#26032;&#30340;&#34920;&#24449;&#23450;&#29702;&#34920;&#26126;&#65292;&#36825;&#20123;&#23398;&#20064;&#38382;&#39064;&#30340;&#35299;&#23384;&#22312;&#20110;&#23485;&#24230;&#21463;&#35757;&#32451;&#25968;&#25454;&#25968;&#38480;&#21046;&#30340;&#21521;&#37327;&#20540;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#25509;&#19979;&#26469;&#65292;&#36890;&#36807;&#19982;&#22810;&#20219;&#21153;lasso&#38382;&#39064;&#30340;&#26032;&#32852;&#31995;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) trained to minimize a loss term plus the sum of squared weights via gradient descent corresponds to the common approach of training with weight decay. This paper provides new insights into this common learning framework. We characterize the kinds of functions learned by training with weight decay for multi-output (vector-valued) ReLU neural networks. This extends previous characterizations that were limited to single-output (scalar-valued) networks. This characterization requires the definition of a new class of neural function spaces that we call vector-valued variation (VV) spaces. We prove that neural networks (NNs) are optimal solutions to learning problems posed over VV spaces via a novel representer theorem. This new representer theorem shows that solutions to these learning problems exist as vector-valued neural networks with widths bounded in terms of the number of training data. Next, via a novel connection to the multi-task lasso problem, we derive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#30340;&#26694;&#26550;&#26469;&#35299;&#37322;&#40657;&#30418;DRL&#25152;&#20316;&#30340;&#20915;&#31574;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#35299;&#37322;&#26694;&#26550;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16532</link><description>&lt;p&gt;
&#20351;&#29992;&#31574;&#30053;&#33976;&#39311;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#22120;&#26694;&#26550;&#35299;&#37322;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explainer Framework for Deep Reinforcement Learning Models Using Policy Distillation. (arXiv:2305.16532v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#30340;&#26694;&#26550;&#26469;&#35299;&#37322;&#40657;&#30418;DRL&#25152;&#20316;&#30340;&#20915;&#31574;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#35299;&#37322;&#26694;&#26550;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#35299;&#20915;&#22797;&#26434;&#25511;&#21046;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#65292;DRL&#24212;&#29992;&#21463;&#21040;&#32570;&#20047;&#24378;&#22823;&#30340;&#39564;&#35777;&#25216;&#26415;&#26469;&#20445;&#35777;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#12290;&#39564;&#35777;&#36807;&#31243;&#30340;&#20851;&#38190;&#35201;&#27714;&#20043;&#19968;&#26159;&#24320;&#21457;&#26377;&#25928;&#30340;&#25216;&#26415;&#26469;&#35299;&#37322;&#31995;&#32479;&#30340;&#21151;&#33021;&#65292;&#21363;&#20026;&#20160;&#20040;&#31995;&#32479;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#20135;&#29983;&#29305;&#23450;&#32467;&#26524;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#21453;&#20107;&#23454;&#65288;Counterfactual&#65292;CF&#65289;&#35299;&#37322;&#26041;&#27861;&#30340;&#35299;&#37322;&#26041;&#27861;&#26469;&#35299;&#20915;DRL&#20013;&#30340;&#35299;&#37322;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CF&#35299;&#37322;&#26694;&#26550;&#65292;&#26469;&#35299;&#37322;&#40657;&#30418;DRL&#25152;&#20316;&#30340;&#20915;&#31574;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#35299;&#37322;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#21644;Atari Pong&#28216;&#25103;&#39046;&#22495;&#36827;&#34892;&#20102;&#20960;&#39033;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#29983;&#25104;&#20102;&#21512;&#29702;&#21644;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#19982;&#21407;&#22987;DRL&#27169;&#22411;&#30456;&#27604;&#30340;&#39640;&#24230;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (DRL) has demonstrated promising capability in solving complex control problems. However, DRL applications in safety-critical systems are hindered by the inherent lack of robust verification techniques to assure their performance in such applications. One of the key requirements of the verification process is the development of effective techniques to explain the system functionality, i.e., why the system produces specific results in given circumstances. Recently, interpretation methods based on the Counterfactual (CF) explanation approach have been proposed to address the problem of explanation in DRLs. This paper proposes a novel CF explanation framework to explain the decisions made by a black-box DRL. To evaluate the efficacy of the proposed explanation framework, we carried out several experiments in the domains of automated driving systems and Atari Pong game. Our analysis demonstrates that the proposed framework generates plausible and meaningful expl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#20445;&#30495;&#24230;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20302;&#12289;&#39640;&#20445;&#30495;&#24230;&#26679;&#26412;&#20013;&#20272;&#35745;&#29289;&#29702;&#31995;&#32479;&#20013;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24179;&#34913;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#25968;&#20540;&#31934;&#24230;&#20043;&#38388;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.16530</link><description>&lt;p&gt;
&#21452;&#20445;&#30495;&#24230;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bi-fidelity Variational Auto-encoder for Uncertainty Quantification. (arXiv:2305.16530v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#20445;&#30495;&#24230;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20302;&#12289;&#39640;&#20445;&#30495;&#24230;&#26679;&#26412;&#20013;&#20272;&#35745;&#29289;&#29702;&#31995;&#32479;&#20013;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24179;&#34913;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#25968;&#20540;&#31934;&#24230;&#20043;&#38388;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27169;&#22411;&#39564;&#35777;&#20013;&#65292;&#37327;&#21270;&#29289;&#29702;&#31995;&#32479;&#24863;&#20852;&#36259;&#30340;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#19968;&#20010;&#20027;&#35201;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#38656;&#35201;&#24179;&#34913;&#35745;&#31639;&#25928;&#29575;&#21644;&#25968;&#20540;&#31934;&#24230;&#20043;&#38388;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#20445;&#30495;&#24230;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;BF-VAE&#65289;&#20844;&#24335;&#65292;&#26088;&#22312;&#20174;&#29289;&#29702;&#31995;&#32479;&#20013;&#20302;&#12289;&#39640;&#20445;&#30495;&#24230;&#26679;&#26412;&#20013;&#20272;&#35745;&#19982;&#37327;&#24863;&#20852;&#36259;&#30340;&#37327;&#26377;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#20174;&#20302;&#20445;&#30495;&#24230;&#26679;&#26412;&#24471;&#20986;&#30340;&#20449;&#24687;&#26469;&#36924;&#36817;&#39640;&#20445;&#30495;&#24230;&#37327;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#21452;&#20445;&#30495;&#24230;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#23558;&#20854;&#25972;&#21512;&#21040;VAE&#30340;&#27010;&#29575;&#32534;&#30721;-&#35299;&#30721;&#32467;&#26500;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#20197;&#22312;&#23384;&#22312;&#26377;&#38480;&#39640;&#20445;&#30495;&#24230;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#22823;&#21270;&#39640;&#20445;&#30495;&#24230;&#23545;&#25968;&#20284;&#28982;&#30340;&#21464;&#20998;&#19979;&#30028;&#65292;&#20174;&#32780;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#21512;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#20540;&#31034;&#20363;&#20013;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#38750;&#32447;&#24615;&#38543;&#26426;&#31995;&#32479;&#21644;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantifying the uncertainty of quantities of interest (QoIs) from physical systems is a primary objective in model validation. However, achieving this goal entails balancing the need for computational efficiency with the requirement for numerical accuracy. To address this trade-off, we propose a novel bi-fidelity formulation of variational auto-encoders (BF-VAE) designed to estimate the uncertainty associated with a QoI from low-fidelity (LF) and high-fidelity (HF) samples of the QoI. This model allows for the approximation of the statistics of the HF QoI by leveraging information derived from its LF counterpart. Specifically, we design a bi-fidelity auto-regressive model in the latent space that is integrated within the VAE's probabilistic encoder-decoder structure. An effective algorithm is proposed to maximize the variational lower bound of the HF log-likelihood in the presence of limited HF data, resulting in the synthesis of HF realizations with a reduced computational cost. Addit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#23558;&#21487;&#35299;&#37322;&#30340;&#25552;&#21319;&#26426;&#22120;&#31639;&#27861;&#24212;&#29992;&#20110;&#31185;&#23398;&#22270;&#20687;&#25968;&#25454;&#65292;&#36890;&#36807;&#22312;&#20919;&#21407;&#23376;&#23396;&#23376;&#22270;&#20687;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#65292;&#35777;&#26126;&#20102;&#36825;&#19968;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16526</link><description>&lt;p&gt;
&#23558;&#21487;&#35299;&#37322;&#30340;&#25552;&#21319;&#26426;&#22120;&#31639;&#27861;&#24212;&#29992;&#20110;&#31185;&#23398;&#22270;&#20687;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Extending Explainable Boosting Machines to Scientific Image Data. (arXiv:2305.16526v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#23558;&#21487;&#35299;&#37322;&#30340;&#25552;&#21319;&#26426;&#22120;&#31639;&#27861;&#24212;&#29992;&#20110;&#31185;&#23398;&#22270;&#20687;&#25968;&#25454;&#65292;&#36890;&#36807;&#22312;&#20919;&#21407;&#23376;&#23396;&#23376;&#22270;&#20687;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#65292;&#35777;&#26126;&#20102;&#36825;&#19968;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#22312;&#21307;&#23398;&#25110;&#31185;&#23398;&#31561;&#37325;&#35201;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#23545;&#31995;&#32479;&#36755;&#20986;&#32467;&#26524;&#30340;&#35299;&#37322;&#38656;&#27714;&#24050;&#25104;&#20026;&#20851;&#27880;&#30340;&#28966;&#28857;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24403;&#21069;&#20808;&#36827;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#26159;&#19981;&#36879;&#26126;&#30340;&#65292;&#20351;&#24471;&#20174;&#35299;&#37322;&#35282;&#24230;&#20351;&#29992;&#23427;&#20204;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#30446;&#21069;&#35299;&#37322;&#36825;&#20123;&#19981;&#36879;&#26126;&#27169;&#22411;&#30340;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#30340;&#23616;&#38480;&#24615;&#24182;&#21463;&#21040;&#20005;&#37325;&#25209;&#35780;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21487;&#35299;&#37322;&#30340;&#25552;&#21319;&#26426;&#22120;&#31639;&#27861;&#26159;&#19968;&#31867;&#26131;&#20110;&#35299;&#37322;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#19981;&#36874;&#20110;&#26368;&#20339;&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#65292;&#23427;&#20204;&#20165;&#38480;&#20110;&#34920;&#26684;&#25968;&#25454;&#12290;&#22312;&#31185;&#23398;&#30028;&#23545;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#36843;&#20999;&#38656;&#27714;&#39537;&#21160;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#21487;&#35299;&#37322;&#30340;&#25552;&#21319;&#26426;&#22120;&#31639;&#27861;&#24212;&#29992;&#20110;&#31185;&#23398;&#22270;&#20687;&#25968;&#25454;&#12290;&#25105;&#20204;&#20197;&#25903;&#25745;&#37327;&#23376;&#25216;&#26415;&#21457;&#23637;&#30340;&#37325;&#35201;&#24212;&#29992;&#20919;&#21407;&#23376;&#23396;&#23376;&#22270;&#20687;&#25968;&#25454;&#20026;&#20363;&#65292;&#24212;&#29992;&#21487;&#35299;&#37322;&#30340;&#25552;&#21319;&#26426;&#22120;&#31639;&#27861;&#36827;&#34892;&#25506;&#31350;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#20854;&#24212;&#29992;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the deployment of computer vision technology becomes increasingly common in applications of consequence such as medicine or science, the need for explanations of the system output has become a focus of great concern. Unfortunately, many state-of-the-art computer vision models are opaque, making their use challenging from an explanation standpoint, and current approaches to explaining these opaque models have stark limitations and have been the subject of serious criticism. In contrast, Explainable Boosting Machines (EBMs) are a class of models that are easy to interpret and achieve performance on par with the very best-performing models, however, to date EBMs have been limited solely to tabular data. Driven by the pressing need for interpretable models in science, we propose the use of EBMs for scientific image data. Inspired by an important application underpinning the development of quantum technologies, we apply EBMs to cold-atom soliton image data, and, in doing so, demonstrate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#31350;&#25913;&#36827;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#39640;&#20854;&#22312;&#38646;&#26679;&#26412;&#24773;&#22659;&#19979;&#30340;&#25991;&#26412;&#20998;&#31867;&#34920;&#29616;&#12290;&#36890;&#36807;&#24341;&#20837;&#38544;&#24335;&#21644;&#26174;&#24335;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#27880;&#20837;&#26041;&#38754;&#32423;&#21035;&#30340;&#29702;&#35299;&#65292;&#20197;&#24314;&#31435;&#20219;&#21153;&#23618;&#27425;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.16521</link><description>&lt;p&gt;
&#38754;&#21521;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#30340;&#26631;&#31614;&#26080;&#20851;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Label Agnostic Pre-training for Zero-shot Text Classification. (arXiv:2305.16521v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#31350;&#25913;&#36827;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#39640;&#20854;&#22312;&#38646;&#26679;&#26412;&#24773;&#22659;&#19979;&#30340;&#25991;&#26412;&#20998;&#31867;&#34920;&#29616;&#12290;&#36890;&#36807;&#24341;&#20837;&#38544;&#24335;&#21644;&#26174;&#24335;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#27880;&#20837;&#26041;&#38754;&#32423;&#21035;&#30340;&#29702;&#35299;&#65292;&#20197;&#24314;&#31435;&#20219;&#21153;&#23618;&#27425;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#23384;&#22312;&#19968;&#32452;&#22266;&#23450;&#30340;&#39044;&#23450;&#20041;&#26631;&#31614;&#65292;&#29992;&#20110;&#23558;&#32473;&#23450;&#30340;&#25991;&#26412;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#23384;&#22312;&#30528;&#29992;&#20110;&#25551;&#36848;&#32473;&#23450;&#25991;&#26412;&#30340;&#26080;&#38480;&#26631;&#31614;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#26681;&#25454;&#25991;&#26412;&#30340;&#26041;&#38754;&#65288;&#24773;&#24863;&#12289;&#20027;&#39064;&#31561;&#65289;&#21644;&#39046;&#22495;&#65288;&#37329;&#34701;&#12289;&#27861;&#24459;&#31561;&#65289;&#65292;&#26631;&#31614;&#30340;&#35299;&#37322;&#21487;&#33021;&#22823;&#19981;&#30456;&#21516;&#12290;&#36825;&#20351;&#24471;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#38646;&#26679;&#26412;&#22330;&#26223;&#19979;&#65292;&#21464;&#24471;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#23545;&#19981;&#21516;&#26041;&#38754;&#21644;&#39046;&#22495;&#20013;&#24050;&#30693;&#21644;&#26410;&#30693;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21363;&#38544;&#24335;&#21644;&#26174;&#24335;&#39044;&#35757;&#32451;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#26102;&#27880;&#20837;&#20102;&#26041;&#38754;&#32423;&#21035;&#30340;&#29702;&#35299;&#65292;&#30446;&#30340;&#26159;&#35753;&#27169;&#22411;&#26500;&#24314;&#20219;&#21153;&#23618;&#27425;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional approaches to text classification typically assume the existence of a fixed set of predefined labels to which a given text can be classified. However, in real-world applications, there exists an infinite label space for describing a given text. In addition, depending on the aspect (sentiment, topic, etc.) and domain of the text (finance, legal, etc.), the interpretation of the label can vary greatly. This makes the task of text classification, particularly in the zero-shot scenario, extremely challenging. In this paper, we investigate the task of zero-shot text classification with the aim of improving the ability of pre-trained language models (PLMs) to generalize to both seen and unseen data across varying aspects and domains. To solve this we introduce two new simple yet effective pre-training strategies, Implicit and Explicit pre-training. These methods inject aspect-level understanding into the model at train time with the goal of conditioning the model to build task-l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#36890;&#29992;&#30690;&#37327;&#21270;&#28369;&#21160;&#21644;&#31639;&#27861;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#36895;&#24230;&#65292;&#27604;&#24120;&#29992;&#30340;&#31639;&#27861;&#26356;&#26377;&#25928;&#65292;&#33021;&#22815;&#34920;&#36798;&#27744;&#21270;&#21644;&#21367;&#31215;&#21407;&#35821;&#12290;</title><link>http://arxiv.org/abs/2305.16513</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28369;&#21160;&#31383;&#21475;&#27714;&#21644;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sliding Window Sum Algorithms for Deep Neural Networks. (arXiv:2305.16513v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#36890;&#29992;&#30690;&#37327;&#21270;&#28369;&#21160;&#21644;&#31639;&#27861;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#36895;&#24230;&#65292;&#27604;&#24120;&#29992;&#30340;&#31639;&#27861;&#26356;&#26377;&#25928;&#65292;&#33021;&#22815;&#34920;&#36798;&#27744;&#21270;&#21644;&#21367;&#31215;&#21407;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28369;&#21160;&#31383;&#21475;&#27714;&#21644;&#24191;&#27867;&#24212;&#29992;&#20110;&#23383;&#31526;&#20018;&#32034;&#24341;&#12289;&#21704;&#24076;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#36890;&#29992;&#30690;&#37327;&#21270;&#28369;&#21160;&#21644;&#31639;&#27861;&#65292;&#23545;&#20110;&#31383;&#21475;&#22823;&#23567;w&#21644;&#22788;&#29702;&#22120;&#25968;&#37327;P&#65292;&#25552;&#20379;&#20102;O&#65288;P/w&#65289;&#30340;&#21152;&#36895;&#12290;&#23545;&#20110;&#21487;&#20132;&#25442;&#36816;&#31639;&#31526;&#30340;&#24635;&#21644;&#65292;&#21152;&#36895;&#21487;&#20197;&#25552;&#39640;&#21040;O&#65288;P/log(w)&#65289;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#34920;&#29616;&#20986;&#39640;&#25928;&#30340;&#20869;&#23384;&#35775;&#38382;&#27169;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#28369;&#21160;&#21644;&#31639;&#27861;&#24212;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#27744;&#21270;&#21644;&#21367;&#31215;&#21407;&#35821;&#34920;&#36798;&#20026;&#28369;&#21160;&#21644;&#65292;&#24182;&#36890;&#36807;&#20855;&#26377;&#20849;&#20139;&#32467;&#26500;&#30340;&#35745;&#31639;&#20869;&#26680;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#28369;&#21160;&#21644;&#21367;&#31215;&#20869;&#26680;&#27604;CPU&#19978;&#36890;&#24120;&#20351;&#29992;&#30340;GEMM&#20869;&#26680;&#26356;&#26377;&#25928;&#65292;&#29978;&#33267;&#21487;&#20197;&#32988;&#36807;&#23427;&#20204;&#30340;GPU&#21516;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sliding window sums are widely used for string indexing, hashing and time series analysis. We have developed a family of the generic vectorized sliding sum algorithms that provide speedup of O(P/w) for window size $w$ and number of processors P. For a sum with a commutative operator the speedup is improved to O(P/log(w)). Even more important, our algorithms exhibit efficient memory access patterns. In this paper we study the application of the sliding sum algorithms to the training and inference of the Deep Neural Networks. We demonstrate how both pooling and convolution primitives could be expressed as sliding sums and evaluated by the compute kernels with the shared structure. We show that the sliding sum convolution kernels are more efficient than the commonly used GEMM kernels on the CPU, and could even outperform their GPU counterparts.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#38543;&#26426;&#24120;&#25968;&#28145;&#24230;&#32593;&#32476;&#30340;PTAS&#26041;&#27861;&#65292;&#23545;&#20110;&#20219;&#20309;&#22266;&#23450;&#35823;&#24046;&#21644;&#28145;&#24230;&#65292;&#20960;&#20046;&#25152;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#37117;&#26159;&#21487;&#23398;&#20064;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.16508</link><description>&lt;p&gt;
&#22823;&#37096;&#20998;&#31070;&#32463;&#32593;&#32476;&#20960;&#20046;&#26159;&#21487;&#23398;&#20064;&#30340;
&lt;/p&gt;
&lt;p&gt;
Most Neural Networks Are Almost Learnable. (arXiv:2305.16508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#38543;&#26426;&#24120;&#25968;&#28145;&#24230;&#32593;&#32476;&#30340;PTAS&#26041;&#27861;&#65292;&#23545;&#20110;&#20219;&#20309;&#22266;&#23450;&#35823;&#24046;&#21644;&#28145;&#24230;&#65292;&#20960;&#20046;&#25152;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#37117;&#26159;&#21487;&#23398;&#20064;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;PTAS&#26469;&#23398;&#20064;&#38543;&#26426;&#24120;&#25968;&#28145;&#24230;&#32593;&#32476;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20219;&#20309;&#22266;&#23450;&#30340;$\epsilon&gt;0$&#21644;&#28145;&#24230;$i$&#65292;&#23384;&#22312;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#23545;&#20110;$\sqrt{d} \cdot \mathbb{S}^{d-1}$&#19978;&#30340;&#20219;&#20309;&#20998;&#24067;&#65292;&#23398;&#20064;&#38543;&#26426;Xavier&#32593;&#32476;&#30340;&#28145;&#24230;$i$&#65292;&#35823;&#24046;&#20026;$\epsilon$&#12290;&#35813;&#31639;&#27861;&#30340;&#26102;&#38388;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$(\bar{d})^{\mathrm{poly}(\epsilon^{-1})}$&#65292;&#20854;&#20013;$\bar d$&#26159;&#32593;&#32476;&#30340;&#22823;&#23567;&#12290;&#23545;&#20110;&#26576;&#20123;&#31867;&#20284;&#20110;Sigmoid&#21644;ReLU&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#21487;&#20197;&#23558;&#35823;&#24046;&#30028;&#38480;&#25913;&#36827;&#20026;$(\bar{d})^{\mathrm{polylog}(\epsilon^{-1})}$&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#31181;&#20960;&#20046;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#23398;&#20064;&#24120;&#25968;&#28145;&#24230;&#38543;&#26426;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a PTAS for learning random constant-depth networks. We show that for any fixed $\epsilon&gt;0$ and depth $i$, there is a poly-time algorithm that for any distribution on $\sqrt{d} \cdot \mathbb{S}^{d-1}$ learns random Xavier networks of depth $i$, up to an additive error of $\epsilon$. The algorithm runs in time and sample complexity of $(\bar{d})^{\mathrm{poly}(\epsilon^{-1})}$, where $\bar d$ is the size of the network. For some cases of sigmoid and ReLU-like activations the bound can be improved to $(\bar{d})^{\mathrm{polylog}(\epsilon^{-1})}$, resulting in a quasi-poly-time algorithm for learning constant depth random networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#26426;&#21046;&#26469;&#25351;&#23548;&#33258;&#36866;&#24212;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20449;&#36182;&#22320;&#23454;&#29616;&#26368;&#20248;&#34892;&#20026;&#65292;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.16505</link><description>&lt;p&gt;
&#22870;&#21169;&#26426;&#21046;&#25351;&#23548;&#19979;&#30340;&#33258;&#36866;&#24212;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reward-Machine-Guided, Self-Paced Reinforcement Learning. (arXiv:2305.16505v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#26426;&#21046;&#26469;&#25351;&#23548;&#33258;&#36866;&#24212;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20449;&#36182;&#22320;&#23454;&#29616;&#26368;&#20248;&#34892;&#20026;&#65292;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#33258;&#21160;&#21019;&#24314;&#19978;&#19979;&#25991;&#27010;&#29575;&#20998;&#24067;&#24207;&#21015;&#26469;&#25552;&#39640;&#23398;&#20064;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33258;&#36866;&#24212;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#22312;&#28041;&#21450;&#26102;&#38388;&#19978;&#24310;&#38271;&#30340;&#34892;&#20026;&#30340;&#38271;&#26399;&#35745;&#21010;&#20219;&#21153;&#20013;&#22833;&#36133;&#12290;&#25105;&#20204;&#20551;&#35774;&#21033;&#29992;&#20851;&#20110;&#24213;&#23618;&#20219;&#21153;&#32467;&#26500;&#30340;&#20808;&#21069;&#30693;&#35782;&#21487;&#20197;&#25552;&#39640;&#33258;&#36866;&#24212;&#24378;&#21270;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#22522;&#20110;&#22870;&#21169;&#26426;&#21046;&#26469;&#36827;&#34892;&#25351;&#23548;&#12290;&#35813;&#31639;&#27861;&#23558;&#22870;&#21169;&#26426;&#21046;&#25972;&#21512;&#21040;1&#65289;&#36890;&#36807;&#20219;&#20309;&#36873;&#25321;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#33719;&#24471;&#30340;&#31574;&#30053;&#21644;&#20215;&#20540;&#20989;&#25968;&#30340;&#26356;&#26032;&#20013;&#65292;&#20197;&#21450;2&#65289;&#29983;&#25104;&#19978;&#19979;&#25991;&#20998;&#24067;&#30340;&#33258;&#21160;&#35838;&#31243;&#34920;&#30340;&#26356;&#26032;&#20013;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#38752;&#22320;&#23454;&#29616;&#26368;&#20248;&#34892;&#20026;&#65292;&#21363;&#20351;&#26159;&#29616;&#26377;&#22522;&#32447;&#26080;&#27861;&#21462;&#24471;&#20219;&#20309;&#26377;&#24847;&#20041;&#30340;&#36827;&#23637;&#30340;&#24773;&#20917;&#19979;&#20063;&#21487;&#34892;&#12290;&#23427;&#36824;&#38477;&#20302;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-paced reinforcement learning (RL) aims to improve the data efficiency of learning by automatically creating sequences, namely curricula, of probability distributions over contexts. However, existing techniques for self-paced RL fail in long-horizon planning tasks that involve temporally extended behaviors. We hypothesize that taking advantage of prior knowledge about the underlying task structure can improve the effectiveness of self-paced RL. We develop a self-paced RL algorithm guided by reward machines, i.e., a type of finite-state machine that encodes the underlying task structure. The algorithm integrates reward machines in 1) the update of the policy and value functions obtained by any RL algorithm of choice, and 2) the update of the automated curriculum that generates context distributions. Our empirical results evidence that the proposed algorithm achieves optimal behavior reliably even in cases in which existing baselines cannot make any meaningful progress. It also decre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#35757;&#32451;&#20351;&#29992;&#31034;&#20363;&#12289;&#19978;&#19979;&#25991;&#28436;&#31034;&#21644;&#29983;&#25104;&#26679;&#24335;&#35268;&#21017;&#26469;&#21152;&#24378;&#24320;&#28304;LLMs&#20197;&#36798;&#21040;&#19982;&#23553;&#38381;&#22411;API&#30340;&#24037;&#20855;&#25805;&#20316;&#24615;&#33021;&#21516;&#31561;&#29978;&#33267;&#26356;&#20248;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;ToolBench&#27979;&#35797;&#24471;&#20986;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#21516;&#26102;&#26412;&#25991;&#36824;&#35777;&#26126;&#20102;&#25913;&#36827;&#30340;&#24320;&#28304;LLMs&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16504</link><description>&lt;p&gt;
&#24320;&#28304;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#24037;&#20855;&#25805;&#20316;&#33021;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Tool Manipulation Capability of Open-source Large Language Models. (arXiv:2305.16504v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#35757;&#32451;&#20351;&#29992;&#31034;&#20363;&#12289;&#19978;&#19979;&#25991;&#28436;&#31034;&#21644;&#29983;&#25104;&#26679;&#24335;&#35268;&#21017;&#26469;&#21152;&#24378;&#24320;&#28304;LLMs&#20197;&#36798;&#21040;&#19982;&#23553;&#38381;&#22411;API&#30340;&#24037;&#20855;&#25805;&#20316;&#24615;&#33021;&#21516;&#31561;&#29978;&#33267;&#26356;&#20248;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;ToolBench&#27979;&#35797;&#24471;&#20986;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#21516;&#26102;&#26412;&#25991;&#36824;&#35777;&#26126;&#20102;&#25913;&#36827;&#30340;&#24320;&#28304;LLMs&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;( LLMs)&#36827;&#34892;&#36719;&#20214;&#24037;&#20855;&#25805;&#20316;&#30340;&#30740;&#31350;&#22823;&#22810;&#20381;&#36182;&#20110;&#23553;&#38381;&#27169;&#22411;API&#12290;&#30001;&#20110;&#21521;&#23553;&#38381;LLMAPI&#26381;&#21153;&#20844;&#24320;&#20449;&#24687;&#23384;&#22312;&#23433;&#20840;&#21644;&#40065;&#26834;&#24615;&#39118;&#38505;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24037;&#19994;&#37319;&#29992;&#21463;&#21040;&#20102;&#23454;&#36136;&#24615;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#37027;&#23601;&#26159;&#25105;&#20204;&#33021;&#21542;&#22312;&#23454;&#36341;&#20013;&#21152;&#24378;&#24320;&#28304;LLMs&#30340;&#21151;&#33021;&#65292;&#20351;&#20854;&#22312;&#24037;&#20855;&#25805;&#20316;&#26041;&#38754;&#19982;&#39046;&#20808;&#30340;&#23553;&#38381;LLM APIs&#31454;&#20105;&#12290;&#36890;&#36807;&#20998;&#26512;&#24120;&#35265;&#30340;&#24037;&#20855;&#25805;&#20316;&#22833;&#36133;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#24320;&#28304;LLMs&#21487;&#33021;&#38656;&#35201;&#35757;&#32451;&#20351;&#29992;&#31034;&#20363;&#12289;&#19978;&#19979;&#25991;&#28436;&#31034;&#21644;&#29983;&#25104;&#26679;&#24335;&#35268;&#21017;&#26469;&#35299;&#20915;&#22833;&#36133;&#12290;&#36825;&#20123;&#35265;&#35299;&#28608;&#21457;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;LLM&#25991;&#29486;&#20013;&#30340;&#32463;&#20856;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#21487;&#20197;&#23558;&#23427;&#20204;&#20316;&#20026;&#31243;&#24207;&#25968;&#25454;&#29983;&#25104;&#30340;&#27169;&#22411;&#23545;&#40784;&#12289;&#31995;&#32479;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#28436;&#31034;&#26816;&#32034;&#22120;&#26469;&#36866;&#24212;&#24320;&#28304;LLMs&#20197;&#23454;&#29616;&#24037;&#20855;&#25805;&#20316;&#30340;&#22686;&#24378;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#25216;&#26415;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;ToolBench&#65292;&#19968;&#20010;&#24037;&#20855;&#25805;&#20316;&#33021;&#21147;&#27979;&#35797;&#22871;&#20214;&#65292;&#21253;&#25324;&#29616;&#26377;API&#21644;&#25105;&#20204;&#25913;&#36827;&#30340;&#24320;&#28304;LLMs&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#32534;&#31243;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#25913;&#36827;&#30340;&#24320;&#28304;LLMs&#33021;&#22815;&#36798;&#21040;&#25110;&#36229;&#36234;&#29616;&#26377;API&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#24050;&#32534;&#20889;&#30340;&#31243;&#24207;&#36827;&#34892;&#36731;&#24494;&#20462;&#25913;&#31561;&#23454;&#38469;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21453;&#21521;&#24037;&#31243;&#27979;&#35797;&#21644;&#40657;&#30418;&#27979;&#35797;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies on software tool manipulation with large language models (LLMs) mostly rely on closed model APIs. The industrial adoption of these models is substantially constrained due to the security and robustness risks in exposing information to closed LLM API services. In this paper, we ask can we enhance open-source LLMs to be competitive to leading closed LLM APIs in tool manipulation, with practical amount of human supervision. By analyzing common tool manipulation failures, we first demonstrate that open-source LLMs may require training with usage examples, in-context demonstration and generation style regulation to resolve failures. These insights motivate us to revisit classical methods in LLM literature, and demonstrate that we can adapt them as model alignment with programmatic data generation, system prompts and in-context demonstration retrievers to enhance open-source LLMs for tool manipulation. To evaluate these techniques, we create the ToolBench, a tool manipulation 
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20102;&#22312;&#26410;&#30693;&#19988;&#20010;&#24615;&#21270;&#25805;&#32437;&#24433;&#21709;&#19979;&#30340;&#25112;&#30053;&#20998;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#30340;&#23450;&#20041;&#65292;&#26088;&#22312;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#31574;&#30053;&#25805;&#32437;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.16501</link><description>&lt;p&gt;
&#26410;&#30693;&#20010;&#24615;&#21270;&#25805;&#32437;&#19979;&#30340;&#25112;&#30053;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Strategic Classification under Unknown Personalized Manipulation. (arXiv:2305.16501v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16501
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20102;&#22312;&#26410;&#30693;&#19988;&#20010;&#24615;&#21270;&#25805;&#32437;&#24433;&#21709;&#19979;&#30340;&#25112;&#30053;&#20998;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#30340;&#23450;&#20041;&#65292;&#26088;&#22312;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#31574;&#30053;&#25805;&#32437;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#25112;&#30053;&#20998;&#31867;&#20013;&#30340;&#22522;&#30784;&#38169;&#35823;&#30028;&#38480;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20854;&#20013;&#20195;&#29702;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25112;&#30053;&#24615;&#22320;&#25805;&#32437;&#20854;&#29305;&#24449;&#21521;&#37327;&#20197;&#39044;&#27979;&#20026;&#27491;&#12290;&#20363;&#22914;&#65292;&#32473;&#23450;&#19968;&#20010;&#30830;&#23450;&#22823;&#23398;&#24405;&#21462;&#30340;&#20998;&#31867;&#22120;&#65292;&#23398;&#29983;&#20505;&#36873;&#20154;&#21487;&#33021;&#20250;&#23581;&#35797;&#36873;&#25321;&#26356;&#23481;&#26131;&#30340;&#35838;&#31243;&#26469;&#25552;&#39640;&#20182;&#20204;&#30340;GPA&#65292;&#37325;&#26032;&#21442;&#21152;SAT&#24182;&#26356;&#25442;&#23398;&#26657;&#65292;&#20197;&#23581;&#35797;&#27450;&#39575;&#20998;&#31867;&#22120;&#12290; &#22312;&#25991;&#29486;&#20013;&#65292;&#29699;&#25805;&#32437;&#26159;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#25805;&#32437;&#31867;&#21035;&#65292;&#20195;&#29702;&#21487;&#20197;&#22312;&#26377;&#30028;&#21322;&#24452;&#29699;&#20869;&#20462;&#25913;&#20854;&#29305;&#24449;&#21521;&#37327;&#12290;&#19982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#35748;&#20026;&#25805;&#32437;&#26159;&#20010;&#24615;&#21270;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#20195;&#29702;&#21487;&#20197;&#25317;&#26377;&#19981;&#21516;&#27700;&#24179;&#30340;&#25805;&#32437;&#33021;&#21147;&#65288;&#20363;&#22914;&#65292;&#29699;&#20307;&#25805;&#32437;&#30340;&#21464;&#21270;&#21322;&#24452;&#65289;&#65292;&#24182;&#19988;&#23545;&#23398;&#20064;&#32773;&#26159;&#26410;&#30693;&#30340;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20132;&#20114;&#27169;&#22411;&#20013;&#24418;&#24335;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#39318;&#20808;&#37096;&#32626;&#20998;&#31867;&#22120;&#65292;&#20195;&#29702;&#22312;&#20854;&#25805;&#32437;&#38598;&#21512;&#20869;&#25805;&#32437;&#29305;&#24449;&#21521;&#37327;&#20197;&#25805;&#20316;&#37096;&#32626;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the fundamental mistake bound and sample complexity in the strategic classification, where agents can strategically manipulate their feature vector up to an extent in order to be predicted as positive. For example, given a classifier determining college admission, student candidates may try to take easier classes to improve their GPA, retake SAT and change schools in an effort to fool the classifier. Ball manipulations are a widely studied class of manipulations in the literature, where agents can modify their feature vector within a bounded radius ball. Unlike most prior work, our work considers manipulations to be personalized, meaning that agents can have different levels of manipulation abilities (e.g., varying radii for ball manipulations), and unknown to the learner.  We formalize the learning problem in an interaction model where the learner first deploys a classifier and the agent manipulates the feature vector within their manipulation set to game the deployed classif
&lt;/p&gt;</description></item><item><title>AD-NEv&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#32423;&#20248;&#21270;&#31070;&#32463;&#36827;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#21327;&#21516;&#20248;&#21270;&#29305;&#24449;&#23376;&#31354;&#38388;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#27169;&#22411;&#26435;&#37325;&#30340;&#26041;&#27861;&#65292;&#34920;&#29616;&#20986;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#26816;&#27979;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.16497</link><description>&lt;p&gt;
AD-NEV&#65306;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#23618;&#31070;&#32463;&#36827;&#21270;&#26694;&#26550;&#29992;&#20110;&#22810;&#20803;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
AD-NEV: A Scalable Multi-level Neuroevolution Framework for Multivariate Anomaly Detection. (arXiv:2305.16497v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16497
&lt;/p&gt;
&lt;p&gt;
AD-NEv&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#32423;&#20248;&#21270;&#31070;&#32463;&#36827;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#21327;&#21516;&#20248;&#21270;&#29305;&#24449;&#23376;&#31354;&#38388;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#27169;&#22411;&#26435;&#37325;&#30340;&#26041;&#27861;&#65292;&#34920;&#29616;&#20986;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#26816;&#27979;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#24037;&#20855;&#21644;&#26041;&#27861;&#22312;&#29616;&#20195;&#30340;&#26234;&#33021;&#29289;&#29702;&#31995;&#32479;&#21644;&#25925;&#38556;&#39044;&#27979;&#31995;&#32479;&#20013;&#20855;&#26377;&#20851;&#38190;&#33021;&#21147;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#21457;&#23637;&#36805;&#36895;&#65292;&#20294;&#38024;&#23545;&#32473;&#23450;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#20248;&#21270;&#26159;&#19968;&#20010;&#32321;&#29712;&#32780;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#31070;&#32463;&#36827;&#21270;&#21487;&#20197;&#26159;&#36825;&#20010;&#38382;&#39064;&#30340;&#26377;&#25928;&#21644;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#65292;&#26159;&#19968;&#31181;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#26368;&#20248;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#25903;&#25345;&#26799;&#24230;&#21644;&#38750;&#26799;&#24230;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22823;&#22810;&#38598;&#20013;&#20110;&#20248;&#21270;&#27169;&#22411;&#32467;&#26500;&#65292;&#32780;&#26410;&#32771;&#34385;&#29305;&#24449;&#23376;&#31354;&#38388;&#21644;&#27169;&#22411;&#26435;&#37325;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Anomaly Detection Neuroevolution (AD-NEv)&#30340;&#21487;&#25193;&#23637;&#22810;&#32423;&#20248;&#21270;&#31070;&#32463;&#36827;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#34920;&#31034;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#21327;&#21516;&#20248;&#21270;&#65306;i) &#22522;&#20110;&#35013;&#34955;&#25216;&#26415;&#23545;&#38598;&#21512;&#27169;&#22411;&#36827;&#34892;&#29305;&#24449;&#23376;&#31354;&#38388;&#20248;&#21270;; ii) &#20248;&#21270;&#21333;&#20010;&#24322;&#24120;&#26816;&#27979;&#32593;&#32476;&#30340;&#27169;&#22411;&#26550;&#26500;; iii) &#20351;&#29992;&#26799;&#24230;&#21644;&#38750;&#26799;&#24230;&#26041;&#27861;&#20248;&#21270;&#27169;&#22411;&#26435;&#37325;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#25193;&#23637;&#65292;&#21487;&#24212;&#29992;&#20110;&#19981;&#21516;&#22823;&#23567;&#21644;&#32500;&#24230;&#30340;&#25968;&#25454;&#38598;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AD-NEv&#22312;&#26816;&#27979;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#25110;&#21487;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection tools and methods present a key capability in modern cyberphysical and failure prediction systems. Despite the fast-paced development in deep learning architectures for anomaly detection, model optimization for a given dataset is a cumbersome and time consuming process. Neuroevolution could be an effective and efficient solution to this problem, as a fully automated search method for learning optimal neural networks, supporting both gradient and non-gradient fine tuning. However, existing methods mostly focus on optimizing model architectures without taking into account feature subspaces and model weights. In this work, we propose Anomaly Detection Neuroevolution (AD-NEv) - a scalable multi-level optimized neuroevolution framework for multivariate time series anomaly detection. The method represents a novel approach to synergically: i) optimize feature subspaces for an ensemble model based on the bagging technique; ii) optimize the model architecture of single anomaly
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#27861;&#65292;&#21363;SAMoSSA&#12290;&#35813;&#26041;&#27861;&#32508;&#21512;&#20102;&#22810;&#20803;&#22855;&#24322;&#35889;&#20998;&#26512;&#21644;&#33258;&#22238;&#24402;&#20998;&#26512;&#65292;&#22312;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#25104;&#20998;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.16491</link><description>&lt;p&gt;
SAMoSSA&#65306;&#24102;&#38543;&#26426;&#33258;&#22238;&#24402;&#22122;&#22768;&#30340;&#22810;&#20803;&#22855;&#24322;&#35889;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
SAMoSSA: Multivariate Singular Spectrum Analysis with Stochastic Autoregressive Noise. (arXiv:2305.16491v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16491
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#27861;&#65292;&#21363;SAMoSSA&#12290;&#35813;&#26041;&#27861;&#32508;&#21512;&#20102;&#22810;&#20803;&#22855;&#24322;&#35889;&#20998;&#26512;&#21644;&#33258;&#22238;&#24402;&#20998;&#26512;&#65292;&#22312;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#25104;&#20998;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#24815;&#20363;&#26159;&#20808;&#20272;&#35745;&#30830;&#23450;&#24615;&#12289;&#38750;&#24179;&#31283;&#36235;&#21183;&#21644;&#23395;&#33410;&#25104;&#20998;&#65292;&#28982;&#21518;&#23398;&#20064;&#27531;&#24046;&#38543;&#26426;&#12289;&#24179;&#31283;&#25104;&#20998;&#12290;&#26368;&#36817;&#24050;&#32463;&#34920;&#26126;&#65292;&#22312;&#27809;&#26377;&#30456;&#20851;&#24179;&#31283;&#25104;&#20998;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20351;&#29992;&#22810;&#20803;&#22855;&#24322;&#35889;&#20998;&#26512;&#65288;mSSA&#65289;&#20934;&#30830;&#22320;&#23398;&#20064;&#30830;&#23450;&#24615;&#38750;&#24179;&#31283;&#25104;&#20998;&#65307;&#21516;&#26102;&#65292;&#22312;&#27809;&#26377;&#30830;&#23450;&#24615;&#38750;&#24179;&#31283;&#25104;&#20998;&#30340;&#24773;&#20917;&#19979;&#65292;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#24179;&#31283;&#25104;&#20998;&#20063;&#21487;&#20197;&#36731;&#26494;&#23398;&#20064;&#65292;&#20363;&#22914;&#36890;&#36807;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#65288;OLS&#65289;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#31181;&#20004;&#20010;&#27493;&#39588;&#30340;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#26222;&#36941;&#23384;&#22312;&#65292;&#20294;&#20851;&#20110;&#21516;&#26102;&#28041;&#21450;&#30830;&#23450;&#24615;&#21644;&#24179;&#31283;&#25104;&#20998;&#30340;&#22810;&#38454;&#27573;&#23398;&#20064;&#31639;&#27861;&#30340;&#29702;&#35770;&#25903;&#25745;&#22312;&#25991;&#29486;&#20013;&#36824;&#27809;&#26377;&#35299;&#20915;&#12290;&#25105;&#20204;&#36890;&#36807;&#20026;&#19968;&#31181;&#33258;&#28982;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#24314;&#31435;&#29702;&#35770;&#20445;&#35777;&#26469;&#35299;&#20915;&#36825;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#65292;&#20854;&#20013;&#39318;&#20808;&#24212;&#29992;mSSA&#26469;&#20272;&#35745;&#38750;&#24179;&#31283;&#25104;&#20998;&#65292;&#23613;&#31649;&#23384;&#22312;&#30456;&#20851;&#24615;&#24179;&#31283;&#25104;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
The well-established practice of time series analysis involves estimating deterministic, non-stationary trend and seasonality components followed by learning the residual stochastic, stationary components. Recently, it has been shown that one can learn the deterministic non-stationary components accurately using multivariate Singular Spectrum Analysis (mSSA) in the absence of a correlated stationary component; meanwhile, in the absence of deterministic non-stationary components, the Autoregressive (AR) stationary component can also be learnt readily, e.g. via Ordinary Least Squares (OLS). However, a theoretical underpinning of multi-stage learning algorithms involving both deterministic and stationary components has been absent in the literature despite its pervasiveness. We resolve this open question by establishing desirable theoretical guarantees for a natural two-stage algorithm, where mSSA is first applied to estimate the non-stationary components despite the presence of a correla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25209;&#27425;&#27169;&#22411;&#25972;&#21512;&#65288;BMC&#65289;&#26469;&#25903;&#25345;&#26356;&#29616;&#23454;&#30340;&#36830;&#32493;&#23398;&#20064;&#65292;&#23427;&#36890;&#36807;&#22312;&#27491;&#21017;&#21270;&#38454;&#27573;&#35757;&#32451;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#26469;&#23398;&#20064;&#19968;&#32452;&#19981;&#30456;&#20132;&#30340;&#20219;&#21153;&#65292;&#24182;&#22312;&#25972;&#21512;&#38454;&#27573;&#23558;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#25972;&#21512;&#20026;&#19968;&#20010;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.16484</link><description>&lt;p&gt;
&#25209;&#27425;&#27169;&#22411;&#25972;&#21512;&#65306;&#19968;&#20010;&#22810;&#20219;&#21153;&#27169;&#22411;&#25972;&#21512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Batch Model Consolidation: A Multi-Task Model Consolidation Framework. (arXiv:2305.16484v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25209;&#27425;&#27169;&#22411;&#25972;&#21512;&#65288;BMC&#65289;&#26469;&#25903;&#25345;&#26356;&#29616;&#23454;&#30340;&#36830;&#32493;&#23398;&#20064;&#65292;&#23427;&#36890;&#36807;&#22312;&#27491;&#21017;&#21270;&#38454;&#27573;&#35757;&#32451;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#26469;&#23398;&#20064;&#19968;&#32452;&#19981;&#30456;&#20132;&#30340;&#20219;&#21153;&#65292;&#24182;&#22312;&#25972;&#21512;&#38454;&#27573;&#23558;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#25972;&#21512;&#20026;&#19968;&#20010;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#38656;&#35201;&#25353;&#39034;&#24207;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#32780;&#19981;&#20250;&#22312;&#20043;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#19978;&#20986;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#38754;&#23545;&#21508;&#31181;&#39046;&#22495;&#21644;&#38590;&#24230;&#30340;&#38271;&#24207;&#21015;&#20219;&#21153;&#26102;&#25928;&#26524;&#19981;&#20339;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30001;&#20110;&#20869;&#23384;&#36164;&#28304;&#28040;&#32791;&#36807;&#22823;&#25110;&#35757;&#32451;&#26102;&#38388;&#36807;&#38271;&#32780;&#38590;&#20197;&#22312;&#23454;&#36341;&#20013;&#24212;&#29992;&#65292;&#25110;&#21482;&#33021;&#22312;&#21333;&#20010;&#35774;&#22791;&#19978;&#32039;&#23494;&#32806;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#25209;&#27425;&#27169;&#22411;&#25972;&#21512;&#65288;BMC&#65289;&#26469;&#25903;&#25345;&#26356;&#29616;&#23454;&#30340;&#36830;&#32493;&#23398;&#20064;&#65292;&#38754;&#23545;&#22810;&#20010;&#20195;&#29702;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#25509;&#35302;&#30340;&#24773;&#20917;&#12290;&#22312;&#27491;&#21017;&#21270;&#38454;&#27573;&#65292;BMC&#24182;&#34892;&#35757;&#32451;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#26469;&#23398;&#20064;&#19968;&#32452;&#19981;&#30456;&#20132;&#30340;&#20219;&#21153;&#12290;&#27599;&#20010;&#19987;&#23478;&#36890;&#36807;&#31283;&#23450;&#24615;&#25439;&#22833;&#19982;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#20445;&#25345;&#26435;&#37325;&#30456;&#20284;&#24615;&#65292;&#24182;&#20174;&#20219;&#21153;&#25968;&#25454;&#30340;&#19968;&#37096;&#20998;&#26500;&#24314;&#32531;&#20914;&#21306;&#12290;&#22312;&#25972;&#21512;&#38454;&#27573;&#65292;&#25105;&#20204;&#23558;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#25972;&#21512;&#20026;&#19968;&#20010;&#27169;&#22411;&#65292;&#24182;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Continual Learning (CL), a model is required to learn a stream of tasks sequentially without significant performance degradation on previously learned tasks. Current approaches fail for a long sequence of tasks from diverse domains and difficulties. Many of the existing CL approaches are difficult to apply in practice due to excessive memory cost or training time, or are tightly coupled to a single device. With the intuition derived from the widely applied mini-batch training, we propose Batch Model Consolidation ($\textbf{BMC}$) to support more realistic CL under conditions where multiple agents are exposed to a range of tasks. During a $\textit{regularization}$ phase, BMC trains multiple $\textit{expert models}$ in parallel on a set of disjoint tasks. Each expert maintains weight similarity to a $\textit{base model}$ through a $\textit{stability loss}$, and constructs a $\textit{buffer}$ from a fraction of the task's data. During the $\textit{consolidation}$ phase, we combine the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28151;&#21512;&#31995;&#32479;&#20013;&#36890;&#36807;&#22686;&#24378;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#25490;&#38431;&#32593;&#32476;&#38382;&#39064;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.16483</link><description>&lt;p&gt;
&#22686;&#24378;&#26679;&#26412;&#19979;&#28151;&#21512;&#31995;&#32479;&#20013;&#26377;&#25928;&#24378;&#21270;&#23398;&#20064;&#21450;&#20854;&#22312;&#25490;&#38431;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sample Efficient Reinforcement Learning in Mixed Systems through Augmented Samples and Its Applications to Queueing Networks. (arXiv:2305.16483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28151;&#21512;&#31995;&#32479;&#20013;&#36890;&#36807;&#22686;&#24378;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#25490;&#38431;&#32593;&#32476;&#38382;&#39064;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#28041;&#21450;&#20855;&#26377;&#20004;&#31181;&#29366;&#24577;&#30340;&#31995;&#32479;&#65306;&#38543;&#26426;&#29366;&#24577;&#21644;&#20266;&#38543;&#26426;&#29366;&#24577;&#12290;&#22312;&#36825;&#31181;&#31995;&#32479;&#20013;&#65292;&#38543;&#26426;&#29366;&#24577;&#36981;&#24490;&#38543;&#26426;&#36716;&#31227;&#26680;&#32780;&#20266;&#38543;&#26426;&#29366;&#24577;&#30340;&#36716;&#31227;&#26159;&#22312;&#32473;&#23450;&#38543;&#26426;&#29366;&#24577;/&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#26159;&#30830;&#23450;&#30340;&#12290;&#25105;&#20204;&#31216;&#36825;&#26679;&#30340;&#31995;&#32479;&#20026;&#28151;&#21512;&#31995;&#32479;&#12290;&#36825;&#31181;&#31995;&#32479;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#21046;&#36896;&#31995;&#32479;&#12289;&#36890;&#20449;&#32593;&#32476;&#21644;&#25490;&#38431;&#32593;&#32476;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#20135;&#29983;&#22686;&#24378;&#30340;&#25968;&#25454;&#26679;&#26412;&#26469;&#21152;&#36895;&#23398;&#20064;&#30340;&#26679;&#26412;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#31639;&#27861;&#26159;&#25968;&#25454;&#39537;&#21160;&#30340;&#65292;&#24182;&#20174;&#23454;&#38469;&#21644;&#22686;&#24378;&#26679;&#26412;&#30340;&#25968;&#25454;&#26679;&#26412;&#20013;&#23398;&#20064;&#31574;&#30053;&#65292;&#20174;&#32780;&#26174;&#33879;&#25913;&#21892;&#20102;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;Fitted Q Iteration&#65288;FQI&#65289;&#19979;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#35777;&#26126;&#20102;&#23398;&#20064;&#31574;&#30053;&#30340;&#26368;&#20248;&#24615;&#24046;&#38543;&#30528;&#30495;&#23454;&#25968;&#25454;&#26679;&#26412;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#20943;&#23569;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#35299;&#20915;&#19968;&#31867;&#25490;&#38431;&#32593;&#32476;&#38382;&#39064;&#65292;&#20854;&#20013;&#28151;&#21512;&#29366;&#24577;&#31354;&#38388;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers a class of reinforcement learning problems, which involve systems with two types of states: stochastic and pseudo-stochastic. In such systems, stochastic states follow a stochastic transition kernel while the transitions of pseudo-stochastic states are deterministic given the stochastic states/transitions. We refer to such systems as mixed systems, which are widely used in various applications, including manufacturing systems, communication networks, and queueing networks. We propose a sample efficient RL method that accelerates learning by generating augmented data samples. The proposed algorithm is data-driven and learns the policy from data samples from both real and augmented samples. This method significantly improves learning by reducing the sample complexity such that the dataset only needs to have sufficient coverage of the stochastic states. We analyze the sample complexity of the proposed method under Fitted Q Iteration (FQI) and demonstrate that the opti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#32447;&#24615;&#39044;&#27979;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#30456;&#20851;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#26032;&#32467;&#26524;&#65292;&#35299;&#20915;&#20102;&#19968;&#20123;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#26032;&#30340;&#20984;&#32447;&#24615;&#39044;&#27979;&#38382;&#39064;&#21487;&#20197;&#34987;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.16475</link><description>&lt;p&gt;
&#32447;&#24615;&#39044;&#27979;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#21021;&#22987;&#21270;&#30456;&#20851;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Initialization-Dependent Sample Complexity of Linear Predictors and Neural Networks. (arXiv:2305.16475v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#32447;&#24615;&#39044;&#27979;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#30456;&#20851;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#26032;&#32467;&#26524;&#65292;&#35299;&#20915;&#20102;&#19968;&#20123;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#26032;&#30340;&#20984;&#32447;&#24615;&#39044;&#27979;&#38382;&#39064;&#21487;&#20197;&#34987;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#21521;&#37327;&#20540;&#32447;&#24615;&#39044;&#27979;&#22120;(&#30001;&#30697;&#38453;&#21442;&#25968;&#21270;)&#12289;&#26356;&#19968;&#33324;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#26032;&#32467;&#26524;&#12290;&#19987;&#27880;&#20110;&#22823;&#23567;&#26080;&#20851;&#30340;&#30028;&#38480;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20165;&#25511;&#21046;&#20174;&#26576;&#20010;&#22266;&#23450;&#21442;&#32771;&#30697;&#38453;$W_0$&#30340;&#21442;&#25968;&#30340;Frobenius&#33539;&#25968;&#36317;&#31163;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#34892;&#20026;&#21487;&#20197;&#20986;&#20154;&#24847;&#26009;&#22320;&#19981;&#21516;&#20110;&#25105;&#20204;&#22312;&#30740;&#31350;&#26631;&#37327;&#20540;&#32447;&#24615;&#39044;&#27979;&#22120;&#26041;&#38754;&#25152;&#26399;&#26395;&#30340;&#12290;&#36825;&#36824;&#23548;&#33268;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#35299;&#20915;&#20102;&#19968;&#20123;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#30830;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#20984;&#32447;&#24615;&#39044;&#27979;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#32479;&#19968;&#25910;&#25947;&#30340;&#24773;&#20917;&#19979;&#34987;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide several new results on the sample complexity of vector-valued linear predictors (parameterized by a matrix), and more generally neural networks. Focusing on size-independent bounds, where only the Frobenius norm distance of the parameters from some fixed reference matrix $W_0$ is controlled, we show that the sample complexity behavior can be surprisingly different than what we may expect considering the well-studied setting of scalar-valued linear predictors. This also leads to new sample complexity bounds for feed-forward neural networks, tackling some open questions in the literature, and establishing a new convex linear prediction problem that is provably learnable without uniform convergence.
&lt;/p&gt;</description></item><item><title>FairDP&#26159;&#19968;&#31181;&#21516;&#26102;&#30830;&#20445;&#24046;&#20998;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#30340;&#26032;&#22411;&#26426;&#21046;&#65292;&#36890;&#36807;&#29420;&#31435;&#20026;&#19981;&#21516;&#30340;&#20010;&#20307;&#32676;&#20307;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#27493;&#25972;&#21512;&#26469;&#33258;&#32676;&#20307;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#21046;&#23450;&#32508;&#21512;&#27169;&#22411;&#20197;&#24179;&#34913;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;FairDP&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#27169;&#22411;&#25928;&#30410;&#12289;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.16474</link><description>&lt;p&gt;
FairDP: &#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#35748;&#35777;&#30340;&#20844;&#24179;&#24615;&#20445;&#38556;
&lt;/p&gt;
&lt;p&gt;
FairDP: Certified Fairness with Differential Privacy. (arXiv:2305.16474v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16474
&lt;/p&gt;
&lt;p&gt;
FairDP&#26159;&#19968;&#31181;&#21516;&#26102;&#30830;&#20445;&#24046;&#20998;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#30340;&#26032;&#22411;&#26426;&#21046;&#65292;&#36890;&#36807;&#29420;&#31435;&#20026;&#19981;&#21516;&#30340;&#20010;&#20307;&#32676;&#20307;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#27493;&#25972;&#21512;&#26469;&#33258;&#32676;&#20307;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#21046;&#23450;&#32508;&#21512;&#27169;&#22411;&#20197;&#24179;&#34913;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;FairDP&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#27169;&#22411;&#25928;&#30410;&#12289;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FairDP&#30340;&#26032;&#22411;&#26426;&#21046;&#65292;&#26088;&#22312;&#21516;&#26102;&#30830;&#20445;&#24046;&#20998;&#38544;&#31169;(DP)&#21644;&#20844;&#24179;&#24615;&#12290;FairDP&#36890;&#36807;&#29420;&#31435;&#20026;&#19981;&#21516;&#30340;&#20010;&#20307;&#32676;&#20307;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#20351;&#29992;&#32452;&#29305;&#23450;&#30340;&#21098;&#35009;&#39033;&#26469;&#35780;&#20272;&#21644;&#38480;&#21046;DP&#30340;&#24046;&#24322;&#24433;&#21709;&#30340;&#21516;&#26102;&#25805;&#20316;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#35813;&#26426;&#21046;&#36880;&#27493;&#25972;&#21512;&#26469;&#33258;&#32676;&#20307;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#21046;&#23450;&#32508;&#21512;&#27169;&#22411;&#20197;&#24179;&#34913;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#24191;&#27867;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#39564;&#35777;&#20102;FairDP&#30340;&#21151;&#25928;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#27169;&#22411;&#25928;&#30410;&#12289;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces FairDP, a novel mechanism designed to simultaneously ensure differential privacy (DP) and fairness. FairDP operates by independently training models for distinct individual groups, using group-specific clipping terms to assess and bound the disparate impacts of DP. Throughout the training process, the mechanism progressively integrates knowledge from group models to formulate a comprehensive model that balances privacy, utility, and fairness in downstream tasks. Extensive theoretical and empirical analyses validate the efficacy of FairDP, demonstrating improved trade-offs between model utility, privacy, and fairness compared with existing methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#32654;&#22269;&#36991;&#38590;&#26696;&#20214;&#20013;&#30340;&#20010;&#20154;&#21644;&#31995;&#32479;&#24615;&#20559;&#35265;&#65292;&#21457;&#29616;&#32654;&#22269;&#36991;&#38590;&#20915;&#23450;&#24448;&#24448;&#21463;&#21040;&#19982;&#26696;&#20214;&#23454;&#36136;&#26080;&#20851;&#30340;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#32780;&#19988;&#26159;&#21542;&#25480;&#20104;&#30003;&#35831;&#20154;&#24199;&#25252;&#20027;&#35201;&#21462;&#20915;&#20110;&#25919;&#27835;&#29615;&#22659;&#21644;&#23457;&#21028;&#27861;&#23448;&#30340;&#20010;&#20307;&#21464;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.16471</link><description>&lt;p&gt;
&#32654;&#22269;&#36991;&#38590;&#26696;&#20214;&#20013;&#30340;&#20559;&#35265;&#12289;&#19968;&#33268;&#24615;&#21644;&#20826;&#27966;&#24615;&#65306;&#23545;&#31227;&#27665;&#27861;&#24237;&#20915;&#23450;&#20013;&#30340;&#26080;&#20851;&#22240;&#32032;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bias, Consistency, and Partisanship in U.S. Asylum Cases: A Machine Learning Analysis of Extraneous Factors in Immigration Court Decisions. (arXiv:2305.16471v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#32654;&#22269;&#36991;&#38590;&#26696;&#20214;&#20013;&#30340;&#20010;&#20154;&#21644;&#31995;&#32479;&#24615;&#20559;&#35265;&#65292;&#21457;&#29616;&#32654;&#22269;&#36991;&#38590;&#20915;&#23450;&#24448;&#24448;&#21463;&#21040;&#19982;&#26696;&#20214;&#23454;&#36136;&#26080;&#20851;&#30340;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#32780;&#19988;&#26159;&#21542;&#25480;&#20104;&#30003;&#35831;&#20154;&#24199;&#25252;&#20027;&#35201;&#21462;&#20915;&#20110;&#25919;&#27835;&#29615;&#22659;&#21644;&#23457;&#21028;&#27861;&#23448;&#30340;&#20010;&#20307;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21452;&#37325;&#35780;&#20998;&#31995;&#32479;&#65292;&#26469;&#34913;&#37327;&#32654;&#22269;&#31227;&#27665;&#27861;&#24237;&#65288;EOIR&#65289;&#20013;&#30340;&#20010;&#20154;&#21644;&#31995;&#32479;&#24615;&#20559;&#35265;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36817;600&#19975;&#20010;&#31227;&#27665;&#27861;&#24237;&#35785;&#35772;&#21644;228&#20010;&#26696;&#20214;&#29305;&#24449;&#65292;&#20197;&#36827;&#19968;&#27493;&#30740;&#31350;&#32654;&#22269;&#36991;&#38590;&#20915;&#23450;&#22914;&#20309;&#21463;&#21040;&#19982;&#26696;&#20214;&#23454;&#36136;&#26080;&#20851;&#30340;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#20351;&#29992;&#39044;&#27979;&#24314;&#27169;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#25351;&#26631;&#65306;&#20826;&#27966;&#24615;&#21644;&#27861;&#23448;&#38388;&#19968;&#33268;&#24615;&#65292;&#35299;&#37322;&#20102;&#24635;&#20915;&#23450;&#21464;&#24322;&#30340;58.54&#65285;&#12290;&#22240;&#27492;&#65292;EOIR&#26159;&#21542;&#25480;&#20104;&#30003;&#35831;&#20154;&#24199;&#25252;&#65292;&#20027;&#35201;&#21462;&#20915;&#20110;&#25919;&#27835;&#29615;&#22659;&#21644;&#23457;&#21028;&#27861;&#23448;&#30340;&#20010;&#20307;&#21464;&#24322; - &#32780;&#19981;&#26159;&#26696;&#20214;&#30340;&#20010;&#20307;&#23454;&#36136;&#12290;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20826;&#27966;&#24615;&#22312;1990&#24180;&#20195;&#21021;&#26399;&#22686;&#21152;&#65292;&#20294;&#22312;&#20043;&#21518;&#24179;&#31283;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce a novel two-pronged scoring system to measure individual and systemic bias in immigration courts under the U.S. Executive Office of Immigration Review (EOIR). We analyze nearly 6 million immigration court proceedings and 228 case features to build on prior research showing that U.S. asylum decisions vary dramatically based on factors that are extraneous to the merits of a case. We close a critical gap in the literature of variability metrics that can span space and time. Using predictive modeling, we explain 58.54% of the total decision variability using two metrics: partisanship and inter-judge cohort consistency. Thus, whether the EOIR grants asylum to an applicant or not depends in majority on the combined effects of the political climate and the individual variability of the presiding judge - not the individual merits of the case. Using time series analysis, we also demonstrate that partisanship increased in the early 1990s but plateaued following the tu
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#35780;&#20272;&#19981;&#21516;&#32676;&#20307;&#23545;&#24433;&#21709;&#21147;&#20449;&#24687;&#30340;&#21453;&#24212;&#30340;&#20219;&#21153;&#21644;&#25152;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#65292;&#31361;&#20986;&#20102;&#26032;&#20219;&#21153;&#22312;&#24314;&#27169;&#20013;&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#65292;&#39044;&#27979;&#20102;&#27599;&#20010;&#21453;&#24212;&#30340;&#24773;&#24863;&#26497;&#24615;&#21644;&#24378;&#24230;&#65292;&#24182;&#35753;&#35780;&#20272;&#21644;&#24212;&#29992;&#26356;&#21152;&#21487;&#38752;&#12290;</title><link>http://arxiv.org/abs/2305.16470</link><description>&lt;p&gt;
&#35780;&#20272;&#19981;&#21516;&#32676;&#20307;&#23545;&#24433;&#21709;&#21147;&#20449;&#24687;&#30340;&#21453;&#24212;
&lt;/p&gt;
&lt;p&gt;
Measuring the Effect of Influential Messages on Varying Personas. (arXiv:2305.16470v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16470
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#35780;&#20272;&#19981;&#21516;&#32676;&#20307;&#23545;&#24433;&#21709;&#21147;&#20449;&#24687;&#30340;&#21453;&#24212;&#30340;&#20219;&#21153;&#21644;&#25152;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#65292;&#31361;&#20986;&#20102;&#26032;&#20219;&#21153;&#22312;&#24314;&#27169;&#20013;&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#65292;&#39044;&#27979;&#20102;&#27599;&#20010;&#21453;&#24212;&#30340;&#24773;&#24863;&#26497;&#24615;&#21644;&#24378;&#24230;&#65292;&#24182;&#35753;&#35780;&#20272;&#21644;&#24212;&#29992;&#26356;&#21152;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#29992;&#25143;&#23545;&#26032;&#38395;&#20107;&#20214;&#30340;&#21453;&#24212;&#33021;&#22815;&#23454;&#29616;&#26234;&#33021;&#20195;&#29702;&#25110;&#20869;&#23481;&#29983;&#25104;&#32773;&#20272;&#35745;&#19981;&#21516;&#31038;&#21306;&#30340;&#24433;&#21709;&#24182;&#20462;&#35746;&#26410;&#21457;&#24067;&#30340;&#20449;&#24687;&#65292;&#38450;&#27490;&#31038;&#20250;&#20914;&#31361;&#21644;&#36947;&#24503;&#20260;&#23475;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#20219;&#21153;&#65306;&#29992;&#20110;&#26032;&#38395;&#23186;&#20307;&#30340;&#20154;&#35774;&#21453;&#24212;&#39044;&#27979;&#65292;&#20197;&#39044;&#27979;&#20154;&#35774;&#65288;&#25551;&#36848;&#20010;&#20154;&#25110;&#32676;&#20307;&#65289;&#23545;&#26032;&#38395;&#20449;&#24687;&#30340;&#21453;&#24212;&#12290;&#19982;&#20197;&#24448;&#20165;&#39044;&#27979;&#26032;&#38395;&#30340;&#36890;&#29992;&#35780;&#35770;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#20219;&#21153;&#19981;&#20165;&#22312;&#24314;&#27169;&#20013;&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#65292;&#36824;&#39044;&#27979;&#20102;&#27599;&#20010;&#21453;&#24212;&#30340;&#24773;&#24863;&#26497;&#24615;&#21644;&#24378;&#24230;&#12290;&#36825;&#20351;&#24471;&#23545;&#20154;&#35774;&#30340;&#24515;&#29702;&#29366;&#24577;&#36827;&#34892;&#26356;&#20934;&#30830;&#21644;&#20840;&#38754;&#30340;&#25512;&#26029;&#25104;&#20026;&#21487;&#33021;&#12290;&#21516;&#26102;&#65292;&#29983;&#25104;&#30340;&#24773;&#24863;&#32500;&#24230;&#20351;&#24471;&#35780;&#20272;&#21644;&#24212;&#29992;&#26356;&#21152;&#21487;&#38752;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;Twitter&#30340;3,847&#20010;&#26032;&#38395;&#26631;&#39064;&#30340;13,357&#20010;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting how a user responds to news events enables important applications such as allowing intelligent agents or content producers to estimate the effect on different communities and revise unreleased messages to prevent unexpected bad outcomes such as social conflict and moral injury. We present a new task, Response Forecasting on Personas for News Media, to estimate the response a persona (characterizing an individual or a group) might have upon seeing a news message. Compared to the previous efforts which only predict generic comments to news, the proposed task not only introduces personalization in the modeling but also predicts the sentiment polarity and intensity of each response. This enables more accurate and comprehensive inference on the mental state of the persona. Meanwhile, the generated sentiment dimensions make the evaluation and application more reliable. We create the first benchmark dataset, which consists of 13,357 responses to 3,847 news headlines from Twitter. W
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#35299;&#20915;&#32593;&#32476;&#25915;&#20987;&#24341;&#36215;&#30340;&#19981;&#30830;&#23450;&#24615;&#23545;&#33258;&#21160;&#30005;&#21387;&#25511;&#21046;&#30340;&#24433;&#21709;&#65292;&#20854;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#33258;&#21160;&#25511;&#21046;&#65292;&#24182;&#33021;&#22815;&#33258;&#21160;&#23547;&#25214;&#25506;&#32034;&#21644;&#24320;&#21457;&#30340;&#38408;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.16469</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#30340;&#30005;&#21387;&#33258;&#21160;&#25511;&#21046;&#22312;&#32593;&#32476;&#25915;&#20987;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bayesian Reinforcement Learning for Automatic Voltage Control under Cyber-Induced Uncertainty. (arXiv:2305.16469v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#35299;&#20915;&#32593;&#32476;&#25915;&#20987;&#24341;&#36215;&#30340;&#19981;&#30830;&#23450;&#24615;&#23545;&#33258;&#21160;&#30005;&#21387;&#25511;&#21046;&#30340;&#24433;&#21709;&#65292;&#20854;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#33258;&#21160;&#25511;&#21046;&#65292;&#24182;&#33021;&#22815;&#33258;&#21160;&#23547;&#25214;&#25506;&#32034;&#21644;&#24320;&#21457;&#30340;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21387;&#25511;&#21046;&#23545;&#20110;&#22823;&#35268;&#27169;&#30005;&#21147;&#31995;&#32479;&#30340;&#21487;&#38752;&#36816;&#20316;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#21450;&#26102;&#25552;&#20379;&#26080;&#21151;&#34917;&#20607;&#26377;&#21161;&#20110;&#38450;&#27490;&#26222;&#36941;&#20572;&#30005;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30005;&#21147;&#31995;&#32479;&#27809;&#26377;&#20869;&#32622;&#26426;&#21046;&#26469;&#30830;&#20445;&#22312;&#23545;&#25163;&#23384;&#22312;&#30340;&#19981;&#30830;&#23450;&#24615;&#19979;&#32500;&#25252;&#21487;&#38752;&#30340;&#36816;&#34892;&#30005;&#21387;&#25511;&#21046;&#30446;&#26631;&#30340;&#29983;&#23384;&#25110;&#24310;&#32493;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#65288;BRL&#65289;&#30340;&#30005;&#21147;&#31995;&#32479;&#25511;&#21046;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#22312;&#32593;&#32476;&#25915;&#20987;&#29615;&#22659;&#19979;&#19981;&#30830;&#23450;&#24615;&#30340;&#25345;&#32493;&#30005;&#21387;&#25511;&#21046;&#12290;&#35813;&#24037;&#20316;&#36890;&#36807;&#26500;&#24314;&#21644;&#35299;&#20915;&#19968;&#20010;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;(POMDP)&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#22522;&#20110;BRL&#30340;&#30005;&#21387;&#33258;&#21160;&#25511;&#21046;&#26041;&#27861;&#65292;&#20854;&#20013;&#30001;&#20110;&#32593;&#32476;&#25915;&#20987;&#32780;&#36896;&#25104;&#30340;&#29366;&#24577;&#26159;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#12290;&#25216;&#26415;&#22312;WSCC&#21644;IEEE 14&#24635;&#32447;&#31995;&#32479;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;BRL&#25216;&#26415;&#21487;&#24110;&#21161;&#33258;&#21160;&#25214;&#21040;&#21508;&#31181;RL&#25216;&#26415;&#20013;&#30340;&#25506;&#32034;&#21644;&#24320;&#21457;&#30340;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Voltage control is crucial to large-scale power system reliable operation, as timely reactive power support can help prevent widespread outages. However, there is currently no built in mechanism for power systems to ensure that the voltage control objective to maintain reliable operation will survive or sustain the uncertainty caused under adversary presence. Hence, this work introduces a Bayesian Reinforcement Learning (BRL) approach for power system control problems, with focus on sustained voltage control under uncertainty in a cyber-adversarial environment. This work proposes a data-driven BRL-based approach for automatic voltage control by formulating and solving a Partially-Observable Markov Decision Problem (POMDP), where the states are partially observable due to cyber intrusions. The techniques are evaluated on the WSCC and IEEE 14 bus systems. Additionally, BRL techniques assist in automatically finding a threshold for exploration and exploitation in various RL techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411; PairVAE&#65292;&#21487;&#20197;&#35757;&#32451;&#22810;&#31181;&#20114;&#34917;&#32467;&#26500;&#34920;&#24449;&#25216;&#26415;&#30340;&#25968;&#25454;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#19981;&#21516;&#25216;&#26415;&#20043;&#38388;&#20132;&#21449;&#37325;&#24314;&#34920;&#24449;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.16467</link><description>&lt;p&gt;
&#30001;&#20114;&#34917;&#32467;&#26500;&#34920;&#24449;&#25216;&#26415;&#38142;&#25509;&#21644;&#20132;&#21449;&#37325;&#24314;&#34920;&#24449;&#25968;&#25454;&#30340; PairVAE
&lt;/p&gt;
&lt;p&gt;
Pair-Variational Autoencoders (PairVAE) for Linking and Cross-Reconstruction of Characterization Data from Complementary Structural Characterization Techniques. (arXiv:2305.16467v1 [cond-mat.soft])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411; PairVAE&#65292;&#21487;&#20197;&#35757;&#32451;&#22810;&#31181;&#20114;&#34917;&#32467;&#26500;&#34920;&#24449;&#25216;&#26415;&#30340;&#25968;&#25454;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#19981;&#21516;&#25216;&#26415;&#20043;&#38388;&#20132;&#21449;&#37325;&#24314;&#34920;&#24449;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26448;&#26009;&#30740;&#31350;&#20013;&#65292;&#32467;&#26500;&#34920;&#24449;&#36890;&#24120;&#38656;&#35201;&#22810;&#31181;&#20114;&#34917;&#25216;&#26415;&#33719;&#24471;&#32508;&#21512;&#30340;&#24418;&#24577;&#35270;&#22270;&#12290;&#30001;&#20110;&#19981;&#21516;&#25216;&#26415;&#30340;&#21487;&#29992;&#24615;&#21644;&#21487;&#35775;&#38382;&#24615;&#65288;&#20363;&#22914;&#25955;&#23556;&#12289;&#26174;&#24494;&#38236;&#12289;&#20809;&#35889;&#65289;&#65292;&#27599;&#20010;&#30740;&#31350;&#26426;&#26500;&#25110;&#23398;&#26415;&#30740;&#31350;&#23454;&#39564;&#23460;&#21487;&#33021;&#21482;&#33021;&#22312;&#19968;&#31181;&#25216;&#26415;&#19978;&#20855;&#26377;&#39640;&#36890;&#37327;&#33021;&#21147;&#65292;&#20294;&#22312;&#20854;&#20182;&#25216;&#26415;&#19978;&#38754;&#20020;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#19968;&#31181;&#31867;&#22411;&#30340;&#32467;&#26500;&#34920;&#24449;&#25968;&#25454;&#21487;&#33021;&#27604;&#21478;&#19968;&#31181;&#26356;&#23481;&#26131;&#35299;&#37322;&#12290;&#22240;&#27492;&#65292;&#26377;&#29992;&#30340;&#26159;&#20855;&#26377;&#21487;&#35757;&#32451;&#22810;&#31181;&#32467;&#26500;&#34920;&#24449;&#25216;&#26415;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20415;&#35813;&#27169;&#22411;&#21487;&#20197;&#20174;&#19968;&#32452;&#25968;&#25454;&#29983;&#25104;&#21478;&#19968;&#32452;&#25968;&#25454;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#36825;&#26679;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#65292;PairVAE&#12290;
&lt;/p&gt;
&lt;p&gt;
In material research, structural characterization often requires multiple complementary techniques to obtain a holistic morphological view of the synthesized material. Depending on the availability of and accessibility of the different characterization techniques (e.g., scattering, microscopy, spectroscopy), each research facility or academic research lab may have access to high-throughput capability in one technique but face limitations (sample preparation, resolution, access time) with other techniques(s). Furthermore, one type of structural characterization data may be easier to interpret than another (e.g., microscopy images are easier to interpret than small angle scattering profiles). Thus, it is useful to have machine learning models that can be trained on paired structural characterization data from multiple techniques so that the model can generate one set of characterization data from the other. In this paper we demonstrate one such machine learning workflow, PairVAE, that wo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#21644;&#26377;&#25928;&#26816;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#22403;&#22334;&#23454;&#20363;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#28145;&#20837;&#27700;&#19979;&#29615;&#22659;&#20013;&#25552;&#39640;&#20854;&#26816;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.16460</link><description>&lt;p&gt;
&#20248;&#21270;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#29992;&#20110;&#39640;&#25928;&#26816;&#27979;&#27700;&#19979;&#22403;&#22334;
&lt;/p&gt;
&lt;p&gt;
Optimized Custom Dataset for Efficient Detection of Underwater Trash. (arXiv:2305.16460v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#21644;&#26377;&#25928;&#26816;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#22403;&#22334;&#23454;&#20363;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#28145;&#20837;&#27700;&#19979;&#29615;&#22659;&#20013;&#25552;&#39640;&#20854;&#26816;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#35780;&#20272;&#21644;&#28165;&#38500;&#28508;&#22312;&#30340;&#27700;&#19979;&#24223;&#29289;&#23545;&#20110;&#20445;&#25252;&#28023;&#27915;&#29983;&#29289;&#21644;&#29615;&#22659;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#38024;&#23545;&#27700;&#19979;&#22403;&#22334;&#26816;&#27979;&#25152;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#22914;&#20809;&#25240;&#23556;&#12289;&#21560;&#25910;&#12289;&#24748;&#28014;&#39063;&#31890;&#21644;&#33394;&#24425;&#25197;&#26354;&#31561;&#22240;&#32032;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#21644;&#26377;&#25928;&#26816;&#27979;&#26041;&#27861;&#12290;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#22810;&#31181;&#27700;&#19979;&#29615;&#22659;&#65292;&#24182;&#21253;&#25324;&#23545;&#24223;&#24323;&#29289;&#23454;&#20363;&#30340;&#31934;&#30830;&#23450;&#20301;&#26631;&#27880;&#12290;&#26368;&#32456;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#65292;&#30446;&#30340;&#26159;&#36890;&#36807;&#22686;&#21152;&#22403;&#22334;&#23454;&#20363;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#28145;&#20837;&#27700;&#19979;&#29615;&#22659;&#20013;&#25552;&#39640;&#20854;&#26816;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately quantifying and removing submerged underwater waste plays a crucial role in safeguarding marine life and preserving the environment. While detecting floating and surface debris is relatively straightforward, quantifying submerged waste presents significant challenges due to factors like light refraction, absorption, suspended particles, and color distortion. This paper addresses these challenges by proposing the development of a custom dataset and an efficient detection approach for submerged marine debris. The dataset encompasses diverse underwater environments and incorporates annotations for precise labeling of debris instances. Ultimately, the primary objective of this custom dataset is to enhance the diversity of litter instances and improve their detection accuracy in deep submerged environments by leveraging state-of-the-art deep learning architectures.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26032;&#22411;&#25955;&#24230;&#8212;&#8212;&#34920;&#31034;Jensen-Shannon&#25955;&#24230;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#20998;&#24067;&#23884;&#20837;&#21040;RKHS&#20013;&#65292;&#24182;&#21033;&#29992;&#34920;&#31034;&#30340;&#21327;&#26041;&#24046;&#31639;&#23376;&#30340;&#39057;&#35889;&#65292;&#23454;&#29616;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#20272;&#35745;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#21487;&#25193;&#23637;&#24615;&#65292;&#21487;&#24494;&#20998;&#24615;&#30340;&#32463;&#39564;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#20989;&#25968;&#21644;&#22522;&#20110;&#26680;&#30697;&#38453;&#30340;&#20272;&#35745;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.16446</link><description>&lt;p&gt;
&#22522;&#20110;&#34920;&#31034;&#30340;Jensen-Shannon&#25955;&#24230;
&lt;/p&gt;
&lt;p&gt;
The Representation Jensen-Shannon Divergence. (arXiv:2305.16446v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26032;&#22411;&#25955;&#24230;&#8212;&#8212;&#34920;&#31034;Jensen-Shannon&#25955;&#24230;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#20998;&#24067;&#23884;&#20837;&#21040;RKHS&#20013;&#65292;&#24182;&#21033;&#29992;&#34920;&#31034;&#30340;&#21327;&#26041;&#24046;&#31639;&#23376;&#30340;&#39057;&#35889;&#65292;&#23454;&#29616;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#20272;&#35745;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#21487;&#25193;&#23637;&#24615;&#65292;&#21487;&#24494;&#20998;&#24615;&#30340;&#32463;&#39564;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#20989;&#25968;&#21644;&#22522;&#20110;&#26680;&#30697;&#38453;&#30340;&#20272;&#35745;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#25955;&#24230;&#37327;&#21270;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#37325;&#35201;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#25968;&#25454;&#30340;&#24213;&#23618;&#20998;&#24067;&#36890;&#24120;&#26410;&#30693;&#65292;&#20174;&#32463;&#39564;&#26679;&#26412;&#20013;&#20272;&#35745;&#25955;&#24230;&#26159;&#19968;&#20010;&#22522;&#26412;&#38590;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;(RKHS)&#20013;&#21327;&#26041;&#24046;&#31639;&#23376;&#30340;&#26032;&#22411;&#25955;&#24230;&#8212;&#8212;&#34920;&#31034;Jensen-Shannon&#25955;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#25968;&#25454;&#20998;&#24067;&#23884;&#20837;&#21040;RKHS&#20013;&#65292;&#24182;&#21033;&#29992;&#34920;&#31034;&#30340;&#21327;&#26041;&#24046;&#31639;&#23376;&#30340;&#39057;&#35889;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20174;&#32463;&#39564;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#30340;&#20272;&#35745;&#20989;&#25968;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;Fourier&#29305;&#24449;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;RKHS&#20013;&#12290;&#27492;&#20272;&#35745;&#20989;&#25968;&#26159;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#12289;&#21487;&#24494;&#20998;&#30340;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#23567;&#25209;&#37327;&#20248;&#21270;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30697;&#38453;&#30340;&#20272;&#35745;&#20989;&#25968;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;RKHS&#36827;&#34892;&#26174;&#24335;&#26144;&#23556;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#37327;&#26159;Jensen-Shannon&#25955;&#24230;&#30340;&#19968;&#20010;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical divergences quantify the difference between probability distributions finding multiple uses in machine-learning. However, a fundamental challenge is to estimate divergence from empirical samples since the underlying distributions of the data are usually unknown. In this work, we propose the representation Jensen-Shannon Divergence, a novel divergence based on covariance operators in reproducing kernel Hilbert spaces (RKHS). Our approach embeds the data distributions in an RKHS and exploits the spectrum of the covariance operators of the representations. We provide an estimator from empirical covariance matrices by explicitly mapping the data to an RKHS using Fourier features. This estimator is flexible, scalable, differentiable, and suitable for minibatch-based optimization problems. Additionally, we provide an estimator based on kernel matrices without having an explicit mapping to the RKHS. We show that this quantity is a lower bound on the Jensen-Shannon divergence, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#36801;&#31227;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#32473;&#23450;&#24456;&#23569;&#25968;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#32452;&#22312;&#21487;&#33021;&#19981;&#21516;&#30340;&#25968;&#25454;&#39046;&#22495;&#19978;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;&#22238;&#24402;&#27169;&#22411;&#65292;&#26469;&#26500;&#24314;&#30446;&#26631;&#27169;&#22411;&#65292;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.16440</link><description>&lt;p&gt;
&#22810;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#34920;&#31034;&#36801;&#31227;&#23398;&#20064;&#22312;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Representation Transfer Learning via Multiple Pre-trained models for Linear Regression. (arXiv:2305.16440v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#36801;&#31227;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#32473;&#23450;&#24456;&#23569;&#25968;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#32452;&#22312;&#21487;&#33021;&#19981;&#21516;&#30340;&#25968;&#25454;&#39046;&#22495;&#19978;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;&#22238;&#24402;&#27169;&#22411;&#65292;&#26469;&#26500;&#24314;&#30446;&#26631;&#27169;&#22411;&#65292;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32473;&#23450;&#24456;&#23569;&#25968;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#22312;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#39046;&#22495;&#65288;&#30446;&#26631;&#65289;&#19978;&#23398;&#20064;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#36801;&#31227;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#32452;&#22312;&#21487;&#33021;&#19981;&#21516;&#30340;&#25968;&#25454;&#39046;&#22495;&#65288;&#26469;&#28304;&#65289;&#19978;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;&#22238;&#24402;&#27169;&#22411;&#65292;&#26469;&#26500;&#24314;&#30446;&#26631;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#30001;&#20004;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#65288;i&#65289;&#21033;&#29992;&#19981;&#21516;&#30340;&#28304;&#34920;&#31034;&#26469;&#26500;&#36896;&#36866;&#24212;&#30446;&#26631;&#25968;&#25454;&#30340;&#34920;&#31034;&#65292;&#65288;ii&#65289;&#23558;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#20316;&#20026;&#21021;&#22987;&#20540;&#65292;&#36890;&#36807;&#24494;&#35843;&#31243;&#24207;&#65292;&#22312;&#30446;&#26631;&#25968;&#25454;&#19978;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#65288;&#36229;&#21442;&#25968;&#65289;&#22238;&#24402;&#27169;&#22411;&#12290;&#23545;&#20110;&#35757;&#32451;&#26041;&#27861;&#30340;&#27599;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23398;&#20064;&#27169;&#22411;&#19982;&#30495;&#23454;&#25968;&#25454;&#29983;&#25104;&#30446;&#26631;&#27169;&#22411;&#20043;&#38388;&#30340;&#36229;&#39069;&#39118;&#38505;&#38480;&#21046;&#12290;&#23548;&#20986;&#30340;&#38480;&#21046;&#26174;&#31034;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the problem of learning a linear regression model on a data domain of interest (target) given few samples. To aid learning, we are provided with a set of pre-trained regression models that are trained on potentially different data domains (sources). Assuming a representation structure for the data generating linear models at the sources and the target domains, we propose a representation transfer based learning method for constructing the target model. The proposed scheme is comprised of two phases: (i) utilizing the different source representations to construct a representation that is adapted to the target data, and (ii) using the obtained model as an initialization to a fine-tuning procedure that re-trains the entire (over-parameterized) regression model on the target data. For each phase of the training method, we provide excess risk bounds for the learned model compared to the true data generating target model. The derived bounds show a gain in sample co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#21644;&#31070;&#32463;&#23849;&#28291;&#65288;NC&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#22312;&#20855;&#26377;&#22359;&#29366;NTK&#30340;DNN&#20013;&#20250;&#20986;&#29616;NC&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#39564;&#25903;&#25345;&#29702;&#35770;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16427</link><description>&lt;p&gt;
&#31070;&#32463;&#65288;&#20999;&#21521;&#26680;&#65289;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
Neural (Tangent Kernel) Collapse. (arXiv:2305.16427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#21644;&#31070;&#32463;&#23849;&#28291;&#65288;NC&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#22312;&#20855;&#26377;&#22359;&#29366;NTK&#30340;DNN&#20013;&#20250;&#20986;&#29616;NC&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#39564;&#25903;&#25345;&#29702;&#35770;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#27010;&#24565;&#65306;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#65292;&#23427;&#25429;&#25417;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#35757;&#32451;&#26399;&#38388;&#30340;&#28436;&#21270;&#21644;&#31070;&#32463;&#23849;&#28291;&#65288;NC&#65289;&#29616;&#35937;&#65292;&#23427;&#25351;&#30340;&#26159;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;&#20998;&#31867;DNN&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#20013;&#23545;&#31216;&#24615;&#21644;&#32467;&#26500;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#20551;&#35774;&#32463;&#39564;NTK&#19982;&#31867;&#26631;&#31614;&#23545;&#40784;&#24182;&#24418;&#25104;&#22359;&#29366;&#32467;&#26500;&#65292;&#21363;&#21516;&#19968;&#31867;&#21035;&#30340;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#27604;&#19981;&#21516;&#31867;&#21035;&#30340;&#26679;&#26412;&#26356;&#24378;&#65292;&#22522;&#20110;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#35757;&#32451;&#30340;DNN&#21160;&#24577;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#38454;&#27573;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31181;&#19981;&#21464;&#37327;&#65292;&#25429;&#25417;&#20102;&#21160;&#24577;&#30340;&#26412;&#36136;&#65292;&#24182;&#29992;&#23427;&#35777;&#26126;&#20102;&#22312;&#20855;&#26377;&#22359;&#29366;NTK&#30340;DNN&#20013;&#20250;&#20986;&#29616;NC&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19977;&#31181;&#24120;&#35265;DNN&#26550;&#26500;&#21644;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#25968;&#20540;&#23454;&#39564;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work bridges two important concepts: the Neural Tangent Kernel (NTK), which captures the evolution of deep neural networks (DNNs) during training, and the Neural Collapse (NC) phenomenon, which refers to the emergence of symmetry and structure in the last-layer features of well-trained classification DNNs. We adopt the natural assumption that the empirical NTK develops a block structure aligned with the class labels, i.e., samples within the same class have stronger correlations than samples from different classes. Under this assumption, we derive the dynamics of DNNs trained with mean squared (MSE) loss and break them into interpretable phases. Moreover, we identify an invariant that captures the essence of the dynamics, and use it to prove the emergence of NC in DNNs with block-structured NTK. We provide large-scale numerical experiments on three common DNN architectures and three benchmark datasets to support our theory.
&lt;/p&gt;</description></item><item><title>SketchOGD&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22312;&#32447;&#33609;&#22270;&#31639;&#27861;&#65292;&#23558;&#27169;&#22411;&#26799;&#24230;&#21387;&#32553;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#30697;&#38453;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#31639;&#27861;&#8212;&#8212;&#27491;&#20132;&#26799;&#24230;&#19979;&#38477;&#65288;OGD&#65289;&#12290;</title><link>http://arxiv.org/abs/2305.16424</link><description>&lt;p&gt;
SketchOGD&#65306;&#20869;&#23384;&#39640;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SketchOGD: Memory-Efficient Continual Learning. (arXiv:2305.16424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16424
&lt;/p&gt;
&lt;p&gt;
SketchOGD&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22312;&#32447;&#33609;&#22270;&#31639;&#27861;&#65292;&#23558;&#27169;&#22411;&#26799;&#24230;&#21387;&#32553;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#30697;&#38453;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#31639;&#27861;&#8212;&#8212;&#27491;&#20132;&#26799;&#24230;&#19979;&#38477;&#65288;OGD&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#25345;&#32493;&#35757;&#32451;&#26102;&#65292;&#23427;&#20204;&#23481;&#26131;&#24536;&#35760;&#20808;&#21069;&#20219;&#21153;&#19978;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#65292;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26041;&#27861;&#24448;&#24448;&#28041;&#21450;&#23384;&#20648;&#36807;&#21435;&#20219;&#21153;&#30340;&#20449;&#24687;&#65292;&#36825;&#24847;&#21619;&#30528;&#20869;&#23384;&#20351;&#29992;&#26159;&#30830;&#23450;&#23454;&#29992;&#24615;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#19968;&#31181;&#24050;&#26377;&#30340;&#31639;&#27861;&#8212;&#8212;&#27491;&#20132;&#26799;&#24230;&#19979;&#38477;&#65288;OGD&#65289;&#12290;OGD&#21033;&#29992;&#20808;&#21069;&#27169;&#22411;&#26799;&#24230;&#26469;&#25214;&#21040;&#32500;&#25345;&#20808;&#21069;&#25968;&#25454;&#28857;&#24615;&#33021;&#30340;&#26435;&#37325;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23384;&#20648;&#20808;&#21069;&#27169;&#22411;&#26799;&#24230;&#30340;&#20869;&#23384;&#25104;&#26412;&#38543;&#31639;&#27861;&#36816;&#34892;&#26102;&#38388;&#22686;&#38271;&#32780;&#22686;&#21152;&#65292;&#22240;&#27492;OGD&#19981;&#36866;&#29992;&#20110;&#20219;&#24847;&#38271;&#26102;&#38388;&#36328;&#24230;&#30340;&#36830;&#32493;&#23398;&#20064;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SketchOGD&#12290;SketchOGD&#37319;&#29992;&#22312;&#32447;&#33609;&#22270;&#31639;&#27861;&#65292;&#23558;&#27169;&#22411;&#26799;&#24230;&#21387;&#32553;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;
When machine learning models are trained continually on a sequence of tasks, they are liable to forget what they learned on previous tasks -- a phenomenon known as catastrophic forgetting. Proposed solutions to catastrophic forgetting tend to involve storing information about past tasks, meaning that memory usage is a chief consideration in determining their practicality. This paper proposes a memory-efficient solution to catastrophic forgetting, improving upon an established algorithm known as orthogonal gradient descent (OGD). OGD utilizes prior model gradients to find weight updates that preserve performance on prior datapoints. However, since the memory cost of storing prior model gradients grows with the runtime of the algorithm, OGD is ill-suited to continual learning over arbitrarily long time horizons. To address this problem, this paper proposes SketchOGD. SketchOGD employs an online sketching algorithm to compress model gradients as they are encountered into a matrix of a fix
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#24322;&#26500;&#25968;&#25454;&#19979;&#30340;&#32852;&#37030;&#31070;&#32463;&#21387;&#32553;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#28304;&#27169;&#22411;&#21644;&#20010;&#24615;&#21270;&#29109;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#65292;&#20351;&#29992;&#20840;&#23616;&#20849;&#20139;&#34920;&#31034;&#20248;&#20110;&#26412;&#22320;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.16416</link><description>&lt;p&gt;
&#24322;&#26500;&#25968;&#25454;&#19979;&#30340;&#32852;&#37030;&#31070;&#32463;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Federated Neural Compression Under Heterogeneous Data. (arXiv:2305.16416v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#24322;&#26500;&#25968;&#25454;&#19979;&#30340;&#32852;&#37030;&#31070;&#32463;&#21387;&#32553;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#28304;&#27169;&#22411;&#21644;&#20010;&#24615;&#21270;&#29109;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#65292;&#20351;&#29992;&#20840;&#23616;&#20849;&#20139;&#34920;&#31034;&#20248;&#20110;&#26412;&#22320;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#21387;&#32553;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#20174;&#25955;&#24067;&#22312;&#23458;&#25143;&#31471;&#19978;&#19988;&#21487;&#33021;&#26159;&#32479;&#35745;&#24322;&#26500;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#21387;&#32553;&#22120;&#65292;&#20294;&#20445;&#26377;&#20849;&#21516;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#28304;&#27169;&#22411;&#65292;&#26082;&#21253;&#25324;&#36825;&#20004;&#31181;&#29305;&#24449;&#65292;&#20063;&#33258;&#28982;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#22120;&#32467;&#26500;&#65292;&#35813;&#32467;&#26500;&#20351;&#29992;&#30001;&#23458;&#25143;&#31471;&#20849;&#20139;&#30340;&#20998;&#26512;&#21644;&#21512;&#25104;&#21464;&#25442;&#12290;&#21463;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#23545;&#27599;&#20010;&#23458;&#25143;&#31471;&#20010;&#24615;&#21270;&#30340;&#29109;&#27169;&#22411;&#12290;&#36825;&#20801;&#35768;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#23398;&#20064;&#20840;&#23616;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#20010;&#24615;&#21270;&#22320;&#35843;&#25972;&#20197;&#36866;&#24212;&#23458;&#25143;&#31471;&#30340;&#28508;&#22312;&#20998;&#24067;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;&#35813;&#31574;&#30053;&#20248;&#20110;&#20165;&#20351;&#29992;&#26412;&#22320;&#26041;&#27861;&#65292;&#36825;&#34920;&#26126;&#22312;&#32479;&#35745;&#24322;&#26500;&#30340;&#32852;&#37030;&#35774;&#32622;&#20013;&#65292;&#23398;&#20064;&#21387;&#32553;&#20063;&#21463;&#30410;&#20110;&#20849;&#20139;&#30340;&#20840;&#23616;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We discuss a federated learned compression problem, where the goal is to learn a compressor from real-world data which is scattered across clients and may be statistically heterogeneous, yet share a common underlying representation. We propose a distributed source model that encompasses both characteristics, and naturally suggests a compressor architecture that uses analysis and synthesis transforms shared by clients. Inspired by personalized federated learning methods, we employ an entropy model that is personalized to each client. This allows for a global latent space to be learned across clients, and personalized entropy models that adapt to the clients' latent distributions. We show empirically that this strategy outperforms solely local methods, which indicates that learned compression also benefits from a shared global representation in statistically heterogeneous federated settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GrowSP&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#26469;&#36827;&#34892;3D&#22330;&#26223;&#20013;&#27599;&#20010;&#28857;&#22797;&#26434;&#35821;&#20041;&#31867;&#21035;&#30340;&#35782;&#21035;&#21644;&#20998;&#21106;&#65292;&#26041;&#27861;&#36890;&#36807;&#36880;&#27493;&#22686;&#38271;&#36229;&#32423;&#28857;&#30340;&#22823;&#23567;&#26469;&#21457;&#29616;3D&#35821;&#20041;&#20803;&#32032;&#65292;&#24182;&#20248;&#20110;&#25152;&#26377;&#26080;&#30417;&#30563;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.16404</link><description>&lt;p&gt;
GrowSP&#65306;3D&#28857;&#20113;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
GrowSP: Unsupervised Semantic Segmentation of 3D Point Clouds. (arXiv:2305.16404v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GrowSP&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#26469;&#36827;&#34892;3D&#22330;&#26223;&#20013;&#27599;&#20010;&#28857;&#22797;&#26434;&#35821;&#20041;&#31867;&#21035;&#30340;&#35782;&#21035;&#21644;&#20998;&#21106;&#65292;&#26041;&#27861;&#36890;&#36807;&#36880;&#27493;&#22686;&#38271;&#36229;&#32423;&#28857;&#30340;&#22823;&#23567;&#26469;&#21457;&#29616;3D&#35821;&#20041;&#20803;&#32032;&#65292;&#24182;&#20248;&#20110;&#25152;&#26377;&#26080;&#30417;&#30563;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20174;&#21407;&#22987;&#28857;&#20113;&#20013;&#36827;&#34892;3D&#35821;&#20041;&#20998;&#21106;&#30340;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;GrowSP&#65292;&#25104;&#21151;&#22320;&#20026;3D&#22330;&#26223;&#20013;&#30340;&#27599;&#20010;&#28857;&#35782;&#21035;&#20986;&#22797;&#26434;&#30340;&#35821;&#20041;&#31867;&#21035;&#65292;&#26080;&#38656;&#20219;&#20309;&#24418;&#24335;&#30340;&#20154;&#24037;&#26631;&#31614;&#25110;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#36890;&#36807;&#36229;&#32423;&#28857;&#30340;&#36880;&#27493;&#22686;&#38271;&#26469;&#21457;&#29616;3D&#35821;&#20041;&#20803;&#32032;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30001;&#19977;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;1&#65289;&#29305;&#24449;&#25552;&#21462;&#22120;&#20174;&#36755;&#20837;&#28857;&#20113;&#20013;&#23398;&#20064;&#27599;&#20010;&#28857;&#30340;&#29305;&#24449;&#65307;2&#65289;&#36229;&#32423;&#28857;&#26500;&#36896;&#22120;&#36880;&#27493;&#22686;&#21152;&#36229;&#32423;&#28857;&#30340;&#22823;&#23567;&#65307;3&#65289;&#35821;&#20041;&#21407;&#22987;&#32858;&#31867;&#27169;&#22359;&#23558;&#36229;&#32423;&#28857;&#20998;&#32452;&#25104;&#35821;&#20041;&#20803;&#32032;&#20197;&#23454;&#29616;&#26368;&#32456;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#25152;&#26377;&#26080;&#30417;&#30563;&#22522;&#32447;&#65292;&#24182;&#25509;&#36817;&#32463;&#20856;&#23436;&#20840;&#30417;&#30563;&#30340;PointNet&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of 3D semantic segmentation from raw point clouds. Unlike existing methods which primarily rely on a large amount of human annotations for training neural networks, we propose the first purely unsupervised method, called GrowSP, to successfully identify complex semantic classes for every point in 3D scenes, without needing any type of human labels or pretrained models. The key to our approach is to discover 3D semantic elements via progressive growing of superpoints. Our method consists of three major components, 1) the feature extractor to learn per-point features from input point clouds, 2) the superpoint constructor to progressively grow the sizes of superpoints, and 3) the semantic primitive clustering module to group superpoints into semantic elements for the final semantic segmentation. We extensively evaluate our method on multiple datasets, demonstrating superior performance over all unsupervised baselines and approaching the classic fully-supervised PointN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#25903;&#25345;&#21521;&#37327;&#26426;&#24341;&#23548;&#20877;&#29983;&#26680;&#39063;&#31890;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#25968;&#23383;&#34920;&#31034;&#22797;&#21512;&#26448;&#26009;&#30340;&#26500;&#24314;&#26041;&#26696;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#30028;&#38754;&#20462;&#25913;&#30340;&#20877;&#29983;&#26680;&#39063;&#31890;&#27861;&#20197;&#36866;&#24403;&#36817;&#20284;&#24369;&#19981;&#36830;&#32493;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16402</link><description>&lt;p&gt;
&#22522;&#20110;&#25903;&#25345;&#21521;&#37327;&#26426;&#24341;&#23548;&#20877;&#29983;&#26680;&#39063;&#31890;&#26041;&#27861;&#30340;&#24494;&#35266;&#32467;&#26500;&#22270;&#20687;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Support Vector Machine Guided Reproducing Kernel Particle Method for Image-Based Modeling of Microstructures. (arXiv:2305.16402v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#25903;&#25345;&#21521;&#37327;&#26426;&#24341;&#23548;&#20877;&#29983;&#26680;&#39063;&#31890;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#25968;&#23383;&#34920;&#31034;&#22797;&#21512;&#26448;&#26009;&#30340;&#26500;&#24314;&#26041;&#26696;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#30028;&#38754;&#20462;&#25913;&#30340;&#20877;&#29983;&#26680;&#39063;&#31890;&#27861;&#20197;&#36866;&#24403;&#36817;&#20284;&#24369;&#19981;&#36830;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#20998;&#31867;&#24341;&#23548;&#26469;&#33258;&#21160;&#36827;&#34892;&#25968;&#23383;&#22797;&#21512;&#26448;&#26009;&#30340;&#26500;&#24314;&#65292;&#35813;&#22797;&#21512;&#26448;&#26009;&#30001;&#20855;&#26377;&#22797;&#26434;&#24494;&#32467;&#26500;&#30340;&#24494;CT&#22270;&#20687;&#26500;&#25104;&#12290;&#24341;&#20837;SVM&#36719;&#38388;&#38548;&#35757;&#32451;&#36807;&#31243;&#65292;&#23545;&#24322;&#36136;&#26448;&#26009;&#28857;&#36827;&#34892;&#20998;&#31867;&#65292;&#36890;&#36807;&#35782;&#21035;&#25903;&#25345;&#21521;&#37327;&#36890;&#36807;&#26412;&#22320;&#24120;&#35268;&#21270;&#20248;&#21270;&#38382;&#39064;&#23454;&#29616;&#22270;&#20687;&#20998;&#21106;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#30028;&#38754;&#20462;&#25913;&#30340;&#20877;&#29983;&#26680;&#39063;&#31890;&#27861;&#65288;IM-RKPM&#65289;&#65292;&#29992;&#20110;&#36866;&#24403;&#36817;&#20284;&#36328;&#26448;&#26009;&#30028;&#38754;&#30340;&#24369;&#38388;&#27463;&#24615;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#26448;&#26009;&#30028;&#38754;&#22788;&#20351;&#29992;&#24120;&#35268;&#21270;&#37325;&#39304;&#20989;&#25968;&#26469;&#20462;&#25913;&#24179;&#28369;&#20869;&#26680;&#20989;&#25968;&#20197;&#20943;&#36731;&#21513;&#24067;&#26031;&#25391;&#33633;&#12290;&#35813;IM-RKPM&#26159;&#22312;&#19981;&#24341;&#20837;&#37325;&#22797;&#33258;&#30001;&#24230;&#30340;&#24773;&#20917;&#19979;&#21046;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents an approach for automating the discretization and approximation procedures in constructing digital representations of composites from Micro-CT images featuring intricate microstructures. The proposed method is guided by the Support Vector Machine (SVM) classification, offering an effective approach for discretizing microstructural images. An SVM soft margin training process is introduced as a classification of heterogeneous material points, and image segmentation is accomplished by identifying support vectors through a local regularized optimization problem. In addition, an Interface-Modified Reproducing Kernel Particle Method (IM-RKPM) is proposed for appropriate approximations of weak discontinuities across material interfaces. The proposed method modifies the smooth kernel functions with a regularized heavy-side function concerning the material interfaces to alleviate Gibb's oscillations. This IM-RKPM is formulated without introducing duplicated degrees of freedom
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Hessian&#30697;&#38453;&#36924;&#36817;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#35745;&#31639;&#37327;&#36739;&#23567;&#19988;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#25928;&#26524;&#26174;&#33879;&#65292;&#21487;&#24191;&#27867;&#29992;&#20110;CNN&#27169;&#22411;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.16396</link><description>&lt;p&gt;
ADLER -- &#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;Hessian&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
ADLER -- An efficient Hessian-based strategy for adaptive learning rate. (arXiv:2305.16396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16396
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Hessian&#30697;&#38453;&#36924;&#36817;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#35745;&#31639;&#37327;&#36739;&#23567;&#19988;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#25928;&#26524;&#26174;&#33879;&#65292;&#21487;&#24191;&#27867;&#29992;&#20110;CNN&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#12289;&#21322;&#27491;&#23450;&#30340;Hessian&#30697;&#38453;&#36924;&#36817;&#26041;&#27861;&#65292;&#20351;&#24471;&#35745;&#31639;Hessian&#30697;&#38453;-&#21521;&#37327;&#31215;&#21464;&#24471;&#23481;&#26131;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#20108;&#27425;&#36924;&#36817;&#30340;&#33258;&#36866;&#24212;SGD&#23398;&#20064;&#29575;&#31574;&#30053;&#65292;&#20854;&#20165;&#38656;&#35201;&#35745;&#31639;&#21333;&#20010;SGD&#25152;&#38656;&#30340;&#20004;&#20493;&#35745;&#31639;&#37327;&#65292;&#20294;&#20854;&#22312;&#19981;&#21516;&#27169;&#22411;&#32467;&#26500;&#65288;&#20855;&#26377;&#21644;&#19981;&#20855;&#26377;&#27531;&#24046;&#36830;&#25509;&#30340;CNN&#65289;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#19982;SGD&#23398;&#20064;&#29575;&#30340;&#32593;&#26684;&#25628;&#32034;&#30456;&#24403;&#12290;&#25105;&#20204;&#36824;&#19982;Gauss-Newton&#36924;&#36817;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We derive a sound positive semi-definite approximation of the Hessian of deep models for which Hessian-vector products are easily computable. This enables us to provide an adaptive SGD learning rate strategy based on the minimization of the local quadratic approximation, which requires just twice the computation of a single SGD run, but performs comparably with grid search on SGD learning rates on different model architectures (CNN with and without residual connections) on classification tasks. We also compare the novel approximation with the Gauss-Newton approximation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20197;&#23567;&#34892;&#26143; Proper Orbital Elements &#20026;&#20989;&#25968;&#24314;&#27169;&#23610;&#23544;&#38590;&#20197;&#35266;&#27979;&#30340;&#23567;&#34892;&#26143;&#21453;&#29031;&#29575;&#65292;&#23558; NEOWISE &#20219;&#21153;&#20013;&#30340;&#21487;&#35265;&#21644;&#32418;&#22806;&#21453;&#29031;&#29575;&#24314;&#27169;&#20026;&#20989;&#25968;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#19982;&#23567;&#34892;&#26143;&#22312;&#24102;&#20013;&#30340;&#20301;&#32622;&#26174;&#30528;&#30456;&#20851;&#65292;&#30456;&#27604;&#20165;&#37319;&#29992;&#24179;&#22343;&#20540;&#27169;&#22411;&#65292;&#38598;&#21512;&#27169;&#22411;&#23558;&#21453;&#29031;&#29575;&#35823;&#24046;&#38477;&#20302;&#20102;&#32422; 37&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.16392</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#23567;&#34892;&#26143;&#21453;&#29031;&#29575;&#24314;&#27169;&#20026;&#20854; Proper Orbital Elements &#30340;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Using neural networks to model Main Belt Asteroid albedos as a function of their proper orbital elements. (arXiv:2305.16392v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20197;&#23567;&#34892;&#26143; Proper Orbital Elements &#20026;&#20989;&#25968;&#24314;&#27169;&#23610;&#23544;&#38590;&#20197;&#35266;&#27979;&#30340;&#23567;&#34892;&#26143;&#21453;&#29031;&#29575;&#65292;&#23558; NEOWISE &#20219;&#21153;&#20013;&#30340;&#21487;&#35265;&#21644;&#32418;&#22806;&#21453;&#29031;&#29575;&#24314;&#27169;&#20026;&#20989;&#25968;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#19982;&#23567;&#34892;&#26143;&#22312;&#24102;&#20013;&#30340;&#20301;&#32622;&#26174;&#30528;&#30456;&#20851;&#65292;&#30456;&#27604;&#20165;&#37319;&#29992;&#24179;&#22343;&#20540;&#27169;&#22411;&#65292;&#38598;&#21512;&#27169;&#22411;&#23558;&#21453;&#29031;&#29575;&#35823;&#24046;&#38477;&#20302;&#20102;&#32422; 37&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#24456;&#38590;&#20272;&#35745;&#23567;&#34892;&#26143;&#30340;&#30452;&#24452;&#12290;&#24403;&#30452;&#25509;&#35266;&#27979;&#19981;&#21487;&#33021;&#36890;&#36807;&#39135;&#21464;&#25110;&#30452;&#25509;&#38647;&#36798;&#35266;&#27979;&#27979;&#37327;&#30452;&#24452;&#26102;&#65292;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20174;&#32418;&#22806;&#35266;&#27979;&#20013;&#36817;&#20284;&#20272;&#31639;&#30452;&#24452;&#12290;&#19968;&#26086;&#30452;&#24452;&#30693;&#36947;&#65292;&#20415;&#21487;&#20197;&#21033;&#29992;&#21487;&#35265;&#20809;&#35266;&#27979;&#19982;&#20043;&#27604;&#36739;&#24471;&#21040;&#29289;&#20307;&#30340;&#21487;&#35265;&#20960;&#20309;&#21453;&#29031;&#29575;&#12290;NEOWISE &#20219;&#21153;&#25552;&#20379;&#20102;&#26368;&#22823;&#30340;&#23567;&#34892;&#26143;&#21453;&#29031;&#29575;&#25968;&#25454;&#38598;&#65292;&#21487;&#21516;&#26102;&#27979;&#37327;&#21487;&#35265;&#20809;&#21644;&#32418;&#22806;&#20809;&#30340;&#21453;&#29031;&#29575;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#32452;&#31070;&#32463;&#32593;&#32476;&#23558;&#36825;&#20123;&#21453;&#29031;&#29575;&#24314;&#27169;&#20026;&#21487;&#20174; Asteroid Families Portal &#33719;&#21462;&#30340; Proper Elements &#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#21457;&#29616;&#21487;&#35265;&#21644;&#32418;&#22806;&#20960;&#20309;&#21453;&#29031;&#29575;&#37117;&#19982;&#23567;&#34892;&#26143;&#22312;&#24102;&#20013;&#30340;&#20301;&#32622;&#26174;&#30528;&#30456;&#20851;&#65292;&#24182;&#19988;&#22312;&#23567;&#34892;&#26143;&#26063;&#21644;&#32972;&#26223;&#24102;&#20013;&#37117;&#23384;&#22312;&#12290;&#19982;&#20165;&#37319;&#29992;&#24179;&#22343;&#20540;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;&#38598;&#21512;&#30340;&#39044;&#27979;&#23558;&#21453;&#29031;&#29575;&#30340;&#24179;&#22343;&#35823;&#24046;&#38477;&#20302;&#20102;&#32422; 37&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Asteroid diameters are traditionally difficult to estimate. When a direct measurement of the diameter cannot be made through either occultation or direct radar observation, the most common method is to approximate the diameter from infrared observations. Once the diameter is known, a comparison with visible light observations can be used to find the visible geometric albedo of the body. One of the largest datasets of asteroid albedos comes from the NEOWISE mission, which measured asteroid albedos both in the visible and infrared. We model these albedos as a function of proper elements available from the Asteroid Families Portal using an ensemble of neural networks. We find that both the visible and infrared geometric albedos are significantly correlated with asteroid position in the belt and occur in both asteroid families and in the background belt. We find that the ensemble's prediction reduces the average error in albedo by about 37% compared to a model that simply adopts an average
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#26080;&#27169;&#22411;&#25968;&#25454;&#23376;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#29992;&#25143;-&#29289;&#21697;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#26469;&#20272;&#35745;&#27599;&#20010;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#22312;&#32593;&#32476;&#19978;&#36827;&#34892;&#20256;&#25773;&#26469;&#24179;&#28369;&#20272;&#35745;&#20540;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26080;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#23376;&#37319;&#26679;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16391</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#26080;&#27169;&#22411;&#25968;&#25454;&#23376;&#37319;&#26679;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Graph-Based Model-Agnostic Data Subsampling for Recommendation Systems. (arXiv:2305.16391v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#26080;&#27169;&#22411;&#25968;&#25454;&#23376;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#29992;&#25143;-&#29289;&#21697;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#26469;&#20272;&#35745;&#27599;&#20010;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#22312;&#32593;&#32476;&#19978;&#36827;&#34892;&#20256;&#25773;&#26469;&#24179;&#28369;&#20272;&#35745;&#20540;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26080;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#23376;&#37319;&#26679;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#23376;&#37319;&#26679;&#24191;&#27867;&#29992;&#20110;&#21152;&#36895;&#35757;&#32451;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#12290;&#22823;&#22810;&#25968;&#23376;&#37319;&#26679;&#26041;&#27861;&#26159;&#22522;&#20110;&#27169;&#22411;&#30340;&#65292;&#24120;&#24120;&#38656;&#35201;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#35797;&#39564;&#27169;&#22411;&#26469;&#36890;&#36807;&#26679;&#26412;&#38590;&#24230;&#31561;&#26041;&#24335;&#27979;&#37327;&#25968;&#25454;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#35797;&#39564;&#27169;&#22411;&#34987;&#38169;&#35823;&#25351;&#23450;&#26102;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#23376;&#37319;&#26679;&#26041;&#27861;&#23558;&#20250;&#24694;&#21270;&#12290;&#37492;&#20110;&#35797;&#39564;&#27169;&#22411;&#30340;&#38169;&#35823;&#25351;&#23450;&#22312;&#30495;&#23454;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#32467;&#26500;&#65292;&#21363;&#22270;&#24418;&#26469;&#25506;&#32034;&#30340;&#26080;&#27169;&#22411;&#25968;&#25454;&#23376;&#37319;&#26679;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30740;&#31350;&#29992;&#25143;-&#29289;&#21697;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#36890;&#36807;&#22270;&#23548;&#30005;&#24615;&#26469;&#20272;&#35745;&#27599;&#20010;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#65288;&#21363;&#29992;&#25143;-&#29289;&#21697;&#22270;&#20013;&#30340;&#19968;&#26465;&#36793;&#65289;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#22312;&#32593;&#32476;&#19978;&#36827;&#34892;&#20256;&#25773;&#27493;&#39588;&#65292;&#24179;&#28369;&#20272;&#35745;&#30340;&#37325;&#35201;&#24615;&#20540;&#12290;&#30001;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#26080;&#27169;&#22411;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#23558;&#26080;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#23376;&#37319;&#26679;&#26041;&#27861;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#32452;&#21512;&#20351;&#29992;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#22343;&#27604;&#20219;&#20309;&#21333;&#19968;&#26041;&#27861;&#37117;&#35201;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data subsampling is widely used to speed up the training of large-scale recommendation systems. Most subsampling methods are model-based and often require a pre-trained pilot model to measure data importance via e.g. sample hardness. However, when the pilot model is misspecified, model-based subsampling methods deteriorate. Since model misspecification is persistent in real recommendation systems, we instead propose model-agnostic data subsampling methods by only exploring input data structure represented by graphs. Specifically, we study the topology of the user-item graph to estimate the importance of each user-item interaction (an edge in the user-item graph) via graph conductance, followed by a propagation step on the network to smooth out the estimated importance value.  Since our proposed method is model-agnostic, we can marry the merits of both model-agnostic and model-based subsampling methods. Empirically, we show that combing the two consistently improves over any single meth
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;DPOK&#65292;&#19968;&#31181;&#20351;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24494;&#35843;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16381</link><description>&lt;p&gt;
DPOK: &#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#24494;&#35843;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models. (arXiv:2305.16381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;DPOK&#65292;&#19968;&#31181;&#20351;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24494;&#35843;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#21487;&#20197;&#25913;&#21892;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#12290;&#36825;&#20123;&#25216;&#26415;&#39318;&#20808;&#23398;&#20064;&#19968;&#20010;&#25429;&#25417;&#20219;&#21153;&#20013;&#20154;&#31867;&#20851;&#24515;&#30340;&#29305;&#24449;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#28982;&#21518;&#26681;&#25454;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#25913;&#36827;&#27169;&#22411;&#12290;&#34429;&#28982;&#24050;&#32463;&#30740;&#31350;&#20102;&#30456;&#23545;&#31616;&#21333;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#22522;&#20110;&#22870;&#21169;&#24471;&#20998;&#30340;&#25298;&#32477;&#37319;&#26679;&#65289;&#65292;&#20294;&#20351;&#29992;&#22870;&#21169;&#20989;&#25968;&#24494;&#35843;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#24494;&#35843;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#24494;&#35843;&#20219;&#21153;&#23450;&#20041;&#20026;RL&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#26356;&#26032;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#21270;&#21453;&#39304;&#35757;&#32451;&#22870;&#21169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;DPOK&#38598;&#25104;&#20102;KL&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#23545;RL&#24494;&#35843;&#21644;&#30417;&#30563;&#24494;&#35843;&#30340;KL&#27491;&#21017;&#21270;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DPOK&#36890;&#24120;&#20248;&#20110;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#30417;&#30563;&#24494;&#35843;&#21644;&#20197;&#21069;&#30340;RL&#24494;&#35843;&#25216;&#26415;&#12290;DPOK&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;IS&#21644;FID&#24471;&#20998;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from human feedback has been shown to improve text-to-image models. These techniques first learn a reward function that captures what humans care about in the task and then improve the models based on the learned reward function. Even though relatively simple approaches (e.g., rejection sampling based on reward scores) have been investigated, fine-tuning text-to-image models with the reward function remains challenging. In this work, we propose using online reinforcement learning (RL) to fine-tune text-to-image models. We focus on diffusion models, defining the fine-tuning task as an RL problem, and updating the pre-trained text-to-image diffusion models using policy gradient to maximize the feedback-trained reward. Our approach, coined DPOK, integrates policy optimization with KL regularization. We conduct an analysis of KL regularization for both RL fine-tuning and supervised fine-tuning. In our experiments, we show that DPOK is generally superior to supervised fine-tuning w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;1&#23618;Transformer&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65292;&#20174;&#32780;&#36880;&#27493;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#24182;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24635;&#32467;&#30456;&#20851;&#20449;&#24687;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#21516;&#26102;&#30740;&#31350;&#20102;&#26631;&#35760;&#39057;&#29575;&#12289;&#19978;&#19979;&#25991;&#21644;&#21021;&#22987;&#21270;&#33258;&#25105;&#20851;&#27880;&#23618;&#31561;&#23545;Transformer&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.16380</link><description>&lt;p&gt;
&#25195;&#25551;&#19982;&#25293;&#29031;&#65306;&#29702;&#35299;1&#23618;Transformer&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#26631;&#35760;&#32452;&#25104;
&lt;/p&gt;
&lt;p&gt;
Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer. (arXiv:2305.16380v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;1&#23618;Transformer&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65292;&#20174;&#32780;&#36880;&#27493;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#24182;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24635;&#32467;&#30456;&#20851;&#20449;&#24687;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#21516;&#26102;&#30740;&#31350;&#20102;&#26631;&#35760;&#39057;&#29575;&#12289;&#19978;&#19979;&#25991;&#21644;&#21021;&#22987;&#21270;&#33258;&#25105;&#20851;&#27880;&#23618;&#31561;&#23545;Transformer&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#22312;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#20026;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#20854;&#22914;&#20309;&#24037;&#20316;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#29305;&#21035;&#26159;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#39044;&#27979;&#24615;&#25439;&#22833;&#65292;&#34920;&#31034;&#22914;&#20309;&#20174;&#26799;&#24230;&#35757;&#32451;&#21160;&#24577;&#20013;&#20986;&#29616;&#20173;&#28982;&#26159;&#19968;&#20010;&#35868;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#38024;&#23545;&#20855;&#26377;&#19968;&#20010;&#33258;&#25105;&#20851;&#27880;&#23618;&#21644;&#19968;&#20010;&#35299;&#30721;&#22120;&#23618;&#30340;1&#23618;Transformer&#65292;&#25105;&#20204;&#20197;&#25968;&#23398;&#20005;&#35880;&#30340;&#26041;&#24335;&#20998;&#26512;&#20854;&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#12290;&#25105;&#20204;&#25171;&#24320;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#32452;&#21512;&#36755;&#20837;&#26631;&#35760;&#30340;&#21160;&#24577;&#36807;&#31243;&#30340;&#40657;&#30418;&#23376;&#65292;&#24182;&#25581;&#31034;&#20102;&#24213;&#23618;&#24402;&#32435;&#20559;&#24046;&#30340;&#26412;&#36136;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#27809;&#26377;&#20301;&#32622;&#32534;&#30721;&#12289;&#38271;&#36755;&#20837;&#24207;&#21015;&#21644;&#35299;&#30721;&#22120;&#23618;&#23398;&#20064;&#36895;&#24230;&#24555;&#20110;&#33258;&#25105;&#20851;&#27880;&#23618;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65306;&#20174;&#22343;&#21248;&#27880;&#24847;&#21147;&#24320;&#22987;&#65292;&#23427;&#36880;&#28176;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#65292;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#30452;&#21040;&#25152;&#26377;&#30456;&#20851;&#20449;&#24687;&#34987;&#25195;&#25551;&#24182;&#24635;&#32467;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#26174;&#31034;&#20102;&#26631;&#35760;&#39057;&#29575;&#21644;&#19978;&#19979;&#25991;&#22914;&#20309;&#24433;&#21709;&#27880;&#24847;&#26435;&#37325;&#65292;&#20197;&#21450;&#33258;&#25105;&#20851;&#27880;&#23618;&#21021;&#22987;&#21270;&#22914;&#20309;&#24433;&#21709;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer architecture has shown impressive performance in multiple research domains and has become the backbone of many neural network models. However, there is limited understanding on how it works. In particular, with a simple predictive loss, how the representation emerges from the gradient \emph{training dynamics} remains a mystery. In this paper, for 1-layer transformer with one self-attention layer plus one decoder layer, we analyze its SGD training dynamics for the task of next token prediction in a mathematically rigorous manner. We open the black box of the dynamic process of how the self-attention layer combines input tokens, and reveal the nature of underlying inductive bias. More specifically, with the assumption (a) no positional encoding, (b) long input sequence, and (c) the decoder layer learns faster than the self-attention layer, we prove that self-attention acts as a \emph{discriminative scanning algorithm}: starting from uniform attention, it gradually attends mor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;&#25968;&#25454;&#22686;&#24378;&#22312;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#31354;&#38388;&#22810;&#26679;&#24615;&#21644;&#36731;&#24494;&#30340;&#22256;&#38590;&#24230;&#19981;&#21487;&#25110;&#32570;&#12290;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DA&#25805;&#20316;&#8212;&#8212;Rand PR&#65292;&#23427;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#31354;&#38388;&#22810;&#26679;&#24615;&#21644;&#26368;&#23567;&#30340;&#22256;&#38590;&#24230;&#65292;&#24050;&#32463;&#22312;&#22810;&#31181;&#25968;&#25454;&#19978;&#24471;&#21040;&#20102;&#26377;&#25928;&#24615;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.16379</link><description>&lt;p&gt;
&#26377;&#25928;&#22686;&#24378;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#21033;&#29992;&#29575;&#65306;&#20197;&#23569;&#23398;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
Learning Better with Less: Effective Augmentation for Sample-Efficient Visual Reinforcement Learning. (arXiv:2305.16379v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;&#25968;&#25454;&#22686;&#24378;&#22312;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#31354;&#38388;&#22810;&#26679;&#24615;&#21644;&#36731;&#24494;&#30340;&#22256;&#38590;&#24230;&#19981;&#21487;&#25110;&#32570;&#12290;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DA&#25805;&#20316;&#8212;&#8212;Rand PR&#65292;&#23427;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#31354;&#38388;&#22810;&#26679;&#24615;&#21644;&#26368;&#23567;&#30340;&#22256;&#38590;&#24230;&#65292;&#24050;&#32463;&#22312;&#22810;&#31181;&#25968;&#25454;&#19978;&#24471;&#21040;&#20102;&#26377;&#25928;&#24615;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#26159;&#22686;&#24378;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20165;&#20351;&#29992;&#31616;&#21333;&#30340;&#35266;&#23519;&#21464;&#25442;&#23601;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#39069;&#22806;&#36741;&#21161;&#34920;&#31034;&#20219;&#21153;&#25110;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#19981;&#28165;&#26970;DA&#30340;&#21738;&#20123;&#23646;&#24615;&#26159;&#23454;&#29616;&#26679;&#26412;&#25928;&#29575;&#35270;&#35273;RL&#30340;&#26377;&#25928;&#24615;&#30340;&#21407;&#22240;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#24182;&#36827;&#19968;&#27493;&#25506;&#32034;DA&#30340;&#28508;&#21147;&#65292;&#26412;&#25991;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;DA&#23646;&#24615;&#23545;&#20854;&#26377;&#25928;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20197;&#19979;&#35265;&#35299;&#21644;&#25913;&#36827;&#65306;&#65288;1&#65289;&#23545;&#20110;&#21333;&#20010;DA&#25805;&#20316;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20805;&#36275;&#30340;&#31354;&#38388;&#22810;&#26679;&#24615;&#21644;&#36731;&#24494;&#30340;&#22256;&#38590;&#24230;&#37117;&#26159;&#19981;&#21487;&#32570;&#23569;&#30340;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;DA&#25805;&#20316;&#8212;&#8212;&#38543;&#26426;PadResize&#65288;Rand PR&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#31354;&#38388;&#22810;&#26679;&#24615;&#21644;&#26368;&#23567;&#30340;&#22256;&#38590;&#24230;&#12290;&#65288;2&#65289;&#23545;&#20110;&#22810;&#31867;&#22411;&#30340;DA&#34701;&#21512;&#26041;&#26696;&#65292;&#22686;&#21152;&#30340;DA&#22256;&#38590;&#24230;&#21644;&#19981;&#31283;&#23450;&#30340;&#25968;&#25454;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Data augmentation (DA) is a crucial technique for enhancing the sample efficiency of visual reinforcement learning (RL) algorithms. Notably, employing simple observation transformations alone can yield outstanding performance without extra auxiliary representation tasks or pre-trained encoders. However, it remains unclear which attributes of DA account for its effectiveness in achieving sample-efficient visual RL. To investigate this issue and further explore the potential of DA, this work conducts comprehensive experiments to assess the impact of DA's attributes on its efficacy and provides the following insights and improvements: (1) For individual DA operations, we reveal that both ample spatial diversity and slight hardness are indispensable. Building on this finding, we introduce Random PadResize (Rand PR), a new DA operation that offers abundant spatial diversity with minimal hardness. (2) For multi-type DA fusion schemes, the increased DA hardness and unstable data distribution 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#25968;&#25454;&#25299;&#25169;&#30456;&#20851;&#30340;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#19978;&#30028;&#65292;&#24182;&#36890;&#36807;&#25299;&#25169;&#26041;&#27861;&#35777;&#26126;&#20102;&#19977;&#23618;ReLU&#32593;&#32476;&#30340;&#26222;&#36866;&#36924;&#36817;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2305.16375</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#19982;&#25968;&#25454;&#25299;&#25169;&#29305;&#24449;&#30456;&#20851;&#30340;&#19978;&#30028;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Data Topology-Dependent Upper Bounds of Neural Network Widths. (arXiv:2305.16375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#25968;&#25454;&#25299;&#25169;&#30456;&#20851;&#30340;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#19978;&#30028;&#65292;&#24182;&#36890;&#36807;&#25299;&#25169;&#26041;&#27861;&#35777;&#26126;&#20102;&#19977;&#23618;ReLU&#32593;&#32476;&#30340;&#26222;&#36866;&#36924;&#36817;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26222;&#36866;&#36924;&#36817;&#24615;&#36136;&#19982;&#25968;&#25454;&#25299;&#25169;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24341;&#20837;&#20102;&#25968;&#25454;&#25299;&#25169;&#30456;&#20851;&#30340;&#32593;&#32476;&#23485;&#24230;&#19978;&#30028;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#19968;&#20010;&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#24212;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#21644;&#26368;&#22823;&#27744;&#21270;&#65292;&#21487;&#20197;&#35774;&#35745;&#26469;&#36924;&#36817;&#19968;&#20010;&#22312;&#32039;&#20945;&#20984;&#22810;&#38754;&#20307;&#20869;&#23553;&#35013;&#30340;&#25351;&#31034;&#20989;&#25968;&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;&#20854;&#25193;&#23637;&#21040;&#19968;&#20010;&#21333;&#32431;&#22797;&#21512;&#20307;&#65292;&#22522;&#20110;&#20854;&#25299;&#25169;&#32467;&#26500;&#25512;&#23548;&#23485;&#24230;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#36873;&#25321;&#25299;&#25169;&#31354;&#38388;&#30340;Betti&#25968;&#35745;&#31639;&#19978;&#30028;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25299;&#25169;&#26041;&#27861;&#35777;&#26126;&#20102;&#19977;&#23618;ReLU&#32593;&#32476;&#30340;&#26222;&#36866;&#36924;&#36817;&#24615;&#36136;&#12290;&#25105;&#20204;&#36824;&#39564;&#35777;&#20102;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#25910;&#25947;&#20110;&#26412;&#30740;&#31350;&#25552;&#20986;&#30340;&#32593;&#32476;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the relationship between the universal approximation property of deep neural networks and topological characteristics of datasets. Our primary contribution is to introduce data topology-dependent upper bounds on the network width. Specifically, we first show that a three-layer neural network, applying a ReLU activation function and max pooling, can be designed to approximate an indicator function over a compact set, one that is encompassed by a tight convex polytope. This is then extended to a simplicial complex, deriving width upper bounds based on its topological structure. Further, we calculate upper bounds in relation to the Betti numbers of select topological spaces. Finally, we prove the universal approximation property of three-layer ReLU networks using our topological approach. We also verify that gradient descent converges to the network structure proposed in our study.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DeepGate2, &#19968;&#20010;&#26032;&#30340;&#21151;&#33021;&#24863;&#30693;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#36890;&#36807;&#21033;&#29992;&#25104;&#23545;&#30495;&#20540;&#34920;&#24046;&#24322;&#20316;&#20026;&#35757;&#32451;&#30417;&#30563;&#65292;&#26126;&#30830;&#32771;&#34385;&#30005;&#36335;&#21151;&#33021;&#65292;&#26469;&#25552;&#39640;&#30005;&#36335;&#34920;&#31034;&#23398;&#20064;&#30340;&#23398;&#20064;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.16373</link><description>&lt;p&gt;
DeepGate2: &#21151;&#33021;&#24863;&#30693;&#30340;&#30005;&#36335;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DeepGate2: Functionality-Aware Circuit Representation Learning. (arXiv:2305.16373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DeepGate2, &#19968;&#20010;&#26032;&#30340;&#21151;&#33021;&#24863;&#30693;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#36890;&#36807;&#21033;&#29992;&#25104;&#23545;&#30495;&#20540;&#34920;&#24046;&#24322;&#20316;&#20026;&#35757;&#32451;&#30417;&#30563;&#65292;&#26126;&#30830;&#32771;&#34385;&#30005;&#36335;&#21151;&#33021;&#65292;&#26469;&#25552;&#39640;&#30005;&#36335;&#34920;&#31034;&#23398;&#20064;&#30340;&#23398;&#20064;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#36335;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#33719;&#24471;&#30005;&#36335;&#20803;&#20214;&#30340;&#31070;&#32463;&#34920;&#31034;&#65292;&#24182;&#24050;&#25104;&#20026;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;EDA&#21644;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#30340;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20363;&#22914;DeepGate&#65292;&#21487;&#20197;&#23884;&#20837;&#30005;&#36335;&#32467;&#26500;&#20449;&#24687;&#21644;&#21151;&#33021;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#33021;&#21147;&#21463;&#21040;&#24369;&#30417;&#30563;&#25110;&#38169;&#35823;&#30340;&#27169;&#22411;&#35774;&#35745;&#30340;&#38480;&#21046;&#65292;&#23548;&#33268;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#20196;&#20154;&#19981;&#28385;&#24847;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DeepGate2&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21151;&#33021;&#24863;&#30693;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#22312;&#23398;&#20064;&#25928;&#26524;&#21644;&#25928;&#29575;&#26041;&#38754;&#26174;&#30528;&#20248;&#20110;&#21407;DeepGate&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#26679;&#26412;&#36923;&#36753;&#38376;&#20043;&#38388;&#30340;&#25104;&#23545;&#30495;&#20540;&#34920;&#24046;&#24322;&#20316;&#20026;&#35757;&#32451;&#30417;&#30563;&#65292;&#20197;&#21450;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#21644;&#21487;&#25193;&#23637;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26126;&#30830;&#32771;&#34385;&#30005;&#36335;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32771;&#34385;&#30005;&#36335;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#19968;&#36718;&#22270;&#34920;&#36798;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Circuit representation learning aims to obtain neural representations of circuit elements and has emerged as a promising research direction that can be applied to various EDA and logic reasoning tasks. Existing solutions, such as DeepGate, have the potential to embed both circuit structural information and functional behavior. However, their capabilities are limited due to weak supervision or flawed model design, resulting in unsatisfactory performance in downstream tasks. In this paper, we introduce DeepGate2, a novel functionality-aware learning framework that significantly improves upon the original DeepGate solution in terms of both learning effectiveness and efficiency. Our approach involves using pairwise truth table differences between sampled logic gates as training supervision, along with a well-designed and scalable loss function that explicitly considers circuit functionality. Additionally, we consider inherent circuit characteristics and design an efficient one-round graph 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#32858;&#31867;&#31561;&#21521;&#24615;&#24230;&#37327;&#30340;&#23454;&#29616;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#20998;&#25968;&#21508;&#21521;&#24322;&#24615;&#25193;&#23637;&#20102;&#36825;&#20123;&#24230;&#37327;&#26469;&#26816;&#26597;&#32858;&#31867;&#30340;&#24179;&#22343;&#31561;&#21521;&#24615;&#12290;&#36890;&#36807;&#37327;&#21270;&#19981;&#21516;&#26448;&#26009;&#32467;&#26500;&#25968;&#25454;&#24211;&#34920;&#31034;&#30340;&#26680;&#36924;&#36817;&#20989;&#25968;&#23545;&#32467;&#26524;&#32858;&#31867;&#30340;&#24433;&#21709;&#65292;&#28436;&#31034;&#20102;&#36825;&#31181;&#24230;&#37327;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.16372</link><description>&lt;p&gt;
&#24212;&#29992;&#20110;&#26448;&#26009;&#23398;&#39046;&#22495;&#39640;&#32500;&#26080;&#30417;&#30563;&#32858;&#31867;&#20219;&#21153;&#30340;&#31561;&#21521;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Metrics for quantifying isotropy in high dimensional unsupervised clustering tasks in a materials context. (arXiv:2305.16372v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16372
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#32858;&#31867;&#31561;&#21521;&#24615;&#24230;&#37327;&#30340;&#23454;&#29616;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#20998;&#25968;&#21508;&#21521;&#24322;&#24615;&#25193;&#23637;&#20102;&#36825;&#20123;&#24230;&#37327;&#26469;&#26816;&#26597;&#32858;&#31867;&#30340;&#24179;&#22343;&#31561;&#21521;&#24615;&#12290;&#36890;&#36807;&#37327;&#21270;&#19981;&#21516;&#26448;&#26009;&#32467;&#26500;&#25968;&#25454;&#24211;&#34920;&#31034;&#30340;&#26680;&#36924;&#36817;&#20989;&#25968;&#23545;&#32467;&#26524;&#32858;&#31867;&#30340;&#24433;&#21709;&#65292;&#28436;&#31034;&#20102;&#36825;&#31181;&#24230;&#37327;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#24120;&#35265;&#30340;&#20219;&#21153;&#65292;&#20294;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#32858;&#31867;&#21487;&#33021;&#38590;&#20197;&#37327;&#21270;&#12290;&#21270;&#23398;&#20013;&#30340;&#32858;&#31867;&#31639;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#26448;&#26009;&#34920;&#31034;&#12290;&#30001;&#20110;&#25968;&#25454;&#30340;&#32500;&#24230;&#65292;&#30830;&#23450;&#19981;&#21516;&#34920;&#31034;&#12289;&#32858;&#31867;&#31639;&#27861;&#25110;&#25968;&#25454;&#21464;&#25442;&#23545;&#32467;&#26524;&#32858;&#31867;&#30340;&#24433;&#21709;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#32858;&#31867;&#31561;&#21521;&#24615;&#24230;&#37327;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#21253;&#25324;&#19968;&#31181;&#22522;&#20110;&#29616;&#26377;&#25512;&#23548;&#30340;&#26032;&#30340;&#23454;&#29616;&#26041;&#27861;&#12290;&#20351;&#29992;&#20998;&#25968;&#21508;&#21521;&#24322;&#24615;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#27604;&#36739;&#21307;&#23398;&#25104;&#20687;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#32780;&#25193;&#23637;&#36825;&#20123;&#24230;&#37327;&#65292;&#20197;&#26816;&#26597;&#19968;&#32452;&#32858;&#31867;&#30340;&#24179;&#22343;&#31561;&#21521;&#24615;&#12290;&#36890;&#36807;&#37327;&#21270;&#19981;&#21516;&#26448;&#26009;&#32467;&#26500;&#25968;&#25454;&#24211;&#34920;&#31034;&#30340;&#26680;&#36924;&#36817;&#20989;&#25968;&#23545;&#32467;&#26524;&#32858;&#31867;&#30340;&#24433;&#21709;&#65292;&#28436;&#31034;&#20102;&#36825;&#20123;&#24230;&#37327;&#30340;&#29992;&#20363;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#20063;&#22312;&#20998;&#26512;MNIST&#25968;&#25454;&#38598;&#30340;&#23398;&#20064;&#23884;&#20837;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#38543;&#26426;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Clustering is a common task in machine learning, but clusters of unlabelled data can be hard to quantify. The application of clustering algorithms in chemistry is often dependant on material representation. Ascertaining the effects of different representations, clustering algorithms, or data transformations on the resulting clusters is difficult due to the dimensionality of these data. We present a thorough analysis of measures for isotropy of a cluster, including a novel implantation based on an existing derivation. Using fractional anisotropy, a common method used in medical imaging for comparison, we then expand these measures to examine the average isotropy of a set of clusters. A use case for such measures is demonstrated by quantifying the effects of kernel approximation functions on different representations of the Inorganic Crystal Structure Database. Broader applicability of these methods is demonstrated in analysing learnt embedding of the MNIST dataset. Random clusters are e
&lt;/p&gt;</description></item><item><title>Stecformer&#26159;&#19968;&#31181;&#22788;&#29702;&#22810;&#20803;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31354;&#38388;-&#26102;&#38388;&#32534;&#30721;&#22120;&#21644;&#32423;&#32852;&#35299;&#30721;&#39044;&#27979;&#22120;&#65288;CDP&#65289;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20026;&#22810;&#20803;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.16370</link><description>&lt;p&gt;
Stecformer&#65306;&#22522;&#20110;&#31354;&#38388;-&#26102;&#38388;&#32534;&#30721;&#20018;&#32852;&#21464;&#21387;&#22120;&#30340;&#22810;&#20803;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Stecformer: Spatio-temporal Encoding Cascaded Transformer for Multivariate Long-term Time Series Forecasting. (arXiv:2305.16370v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16370
&lt;/p&gt;
&lt;p&gt;
Stecformer&#26159;&#19968;&#31181;&#22788;&#29702;&#22810;&#20803;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31354;&#38388;-&#26102;&#38388;&#32534;&#30721;&#22120;&#21644;&#32423;&#32852;&#35299;&#30721;&#39044;&#27979;&#22120;&#65288;CDP&#65289;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20026;&#22810;&#20803;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#35832;&#22810;&#39046;&#22495;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#65292;&#22914;&#33021;&#28304;&#28040;&#32791;&#21644;&#22825;&#27668;&#39044;&#25253;&#31561;&#12290;&#38543;&#30528;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#22810;&#20803;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#21464;&#21387;&#22120;&#27169;&#22411;&#20013;&#23545;&#20110;&#31354;&#38388;&#29305;&#24449;&#30340;&#30740;&#31350;&#36739;&#23569;&#65292;&#32780;&#19981;&#21516;&#39044;&#27979;&#21608;&#26399;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20063;&#30001;&#20110;&#26102;&#38388;&#36328;&#24230;&#36739;&#22823;&#32780;&#19981;&#23613;&#20154;&#24847;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#25972;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#29305;&#24449;&#25552;&#21462;&#21644;&#30446;&#26631;&#39044;&#27979;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;&#29305;&#24449;&#25552;&#21462;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31354;&#38388;-&#26102;&#38388;&#32534;&#30721;&#22120;&#65292;&#21253;&#25324;&#21322;&#33258;&#36866;&#24212;&#22270;&#24418;&#26469;&#33719;&#21462;&#36275;&#22815;&#30340;&#31354;&#38388;-&#26102;&#38388;&#20449;&#24687;&#12290;&#23545;&#20110;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32423;&#32852;&#35299;&#30721;&#39044;&#27979;&#22120;&#65288;CDP&#65289;&#65292;&#20197;&#21152;&#24378;&#19981;&#21516;&#38388;&#38548;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20063;&#21487;&#20197;&#29992;&#20316;&#36890;&#29992;&#30340;&#32452;&#20214;&#26469;&#25552;&#39640;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#31216;&#20026;Stecformer&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#20026;&#22810;&#20803;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate long-term time series forecasting is of great application across many domains, such as energy consumption and weather forecasting. With the development of transformer-based methods, the performance of multivariate long-term time series forecasting has been significantly improved, however, the study of spatial features extracting in transformer-based model is rare and the consistency of different prediction periods is unsatisfactory due to the large span. In this work, we propose a complete solution to address these problems in terms of feature extraction and target prediction. For extraction, we design an efficient spatio-temporal encoding extractor including a semi-adaptive graph to acquire sufficient spatio-temporal information. For prediction, we propose a Cascaded Decoding Predictor (CDP) to strengthen the correlation between different intervals, which can also be utilized as a generic component to improve the performance of transformer-based methods. The proposed meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#19981;&#23436;&#20840;&#20998;&#35299;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#22495;&#30340;&#26377;&#25928;&#39044;&#22788;&#29702;&#22120;&#12290;&#20854;&#36890;&#36807;&#26367;&#25442;&#20256;&#32479;&#25163;&#24037;&#39044;&#22788;&#29702;&#22120;&#26174;&#30528;&#25552;&#39640;&#20102;&#25910;&#25947;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#38382;&#39064;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#22343;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.16368</link><description>&lt;p&gt;
&#31070;&#32463;&#19981;&#23436;&#20840;&#20998;&#35299;&#65306;&#23398;&#20064;&#20849;&#36717;&#26799;&#24230;&#27861;&#30340;&#39044;&#22788;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Neural incomplete factorization: learning preconditioners for the conjugate gradient method. (arXiv:2305.16368v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#19981;&#23436;&#20840;&#20998;&#35299;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#22495;&#30340;&#26377;&#25928;&#39044;&#22788;&#29702;&#22120;&#12290;&#20854;&#36890;&#36807;&#26367;&#25442;&#20256;&#32479;&#25163;&#24037;&#39044;&#22788;&#29702;&#22120;&#26174;&#30528;&#25552;&#39640;&#20102;&#25910;&#25947;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#38382;&#39064;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#22343;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#31185;&#23398;&#35745;&#31639;&#21644;&#20248;&#21270;&#20013;&#36935;&#21040;&#30340;&#22823;&#35268;&#27169;&#32447;&#24615;&#26041;&#31243;&#32452;&#27714;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#33258;&#30417;&#30563;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29983;&#25104;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#22495;&#30340;&#26377;&#25928;&#39044;&#22788;&#29702;&#22120;&#12290;&#36890;&#36807;&#26367;&#25442;&#19982;&#20849;&#36717;&#26799;&#24230;&#27861;&#19968;&#36215;&#20351;&#29992;&#30340;&#20256;&#32479;&#25163;&#24037;&#39044;&#22788;&#29702;&#22120;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#65288;&#31216;&#20026;&#31070;&#32463;&#19981;&#23436;&#20840;&#20998;&#35299;&#65289;&#26174;&#30528;&#21152;&#36895;&#20102;&#25910;&#25947;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#21463;&#31232;&#30095;&#30697;&#38453;&#29702;&#35770;&#21551;&#21457;&#30340;&#26032;&#22411;&#28040;&#24687;&#20256;&#36882;&#22359;&#65292;&#23427;&#19982;&#23547;&#25214;&#30697;&#38453;&#30340;&#31232;&#30095;&#20998;&#35299;&#30340;&#30446;&#26631;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#38382;&#39064;&#21644;&#26469;&#33258;&#31185;&#23398;&#35745;&#31639;&#30340;&#30495;&#23454;&#38382;&#39064;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31070;&#32463;&#19981;&#23436;&#20840;&#20998;&#35299;&#22987;&#32456;&#20248;&#20110;&#26368;&#24120;&#35265;&#30340;&#36890;&#29992;&#39044;&#22788;&#29702;&#22120;&#65292;&#21253;&#25324;&#19981;&#23436;&#20840;&#30340;Cholesky&#26041;&#27861;&#65292;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we develop a novel data-driven approach to accelerate solving large-scale linear equation systems encountered in scientific computing and optimization. Our method utilizes self-supervised training of a graph neural network to generate an effective preconditioner tailored to the specific problem domain. By replacing conventional hand-crafted preconditioners used with the conjugate gradient method, our approach, named neural incomplete factorization (NeuralIF), significantly speeds-up convergence and computational efficiency. At the core of our method is a novel message-passing block, inspired by sparse matrix theory, that aligns with the objective to find a sparse factorization of the matrix. We evaluate our proposed method on both a synthetic and a real-world problem arising from scientific computing. Our results demonstrate that NeuralIF consistently outperforms the most common general-purpose preconditioners, including the incomplete Cholesky method, achieving competit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#23545;&#35805;&#20195;&#29702;&#34892;&#20026;&#25551;&#36848;&#20026;&#35282;&#33394;&#25198;&#28436;&#65292;&#20197;&#36991;&#20813;&#36171;&#20104;&#20854;&#20154;&#31867;&#29305;&#24449;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#30740;&#31350;&#20195;&#29702;&#34892;&#20026;&#20013;&#30340;&#27450;&#39575;&#21644;&#33258;&#25105;&#24847;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.16367</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#35282;&#33394;&#25198;&#28436;
&lt;/p&gt;
&lt;p&gt;
Role-Play with Large Language Models. (arXiv:2305.16367v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#23545;&#35805;&#20195;&#29702;&#34892;&#20026;&#25551;&#36848;&#20026;&#35282;&#33394;&#25198;&#28436;&#65292;&#20197;&#36991;&#20813;&#36171;&#20104;&#20854;&#20154;&#31867;&#29305;&#24449;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#30740;&#31350;&#20195;&#29702;&#34892;&#20026;&#20013;&#30340;&#27450;&#39575;&#21644;&#33258;&#25105;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#35805;&#20195;&#29702;&#31243;&#24207;&#22312;&#34920;&#29616;&#19978;&#36234;&#26469;&#36234;&#25509;&#36817;&#20154;&#31867;&#65292;&#26377;&#24517;&#35201;&#24320;&#21457;&#26377;&#25928;&#30340;&#26041;&#24335;&#39640;&#23618;&#27425;&#25551;&#36848;&#20854;&#34892;&#20026;&#65292;&#32780;&#19981;&#20250;&#38519;&#20837;&#36171;&#20104;&#20854;&#20154;&#31867;&#29305;&#24449;&#30340;&#38519;&#38449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35282;&#33394;&#25198;&#28436;&#30340;&#27010;&#24565;&#65292;&#23558;&#23545;&#35805;&#20195;&#29702;&#31243;&#24207;&#30340;&#34892;&#20026;&#35270;&#20026;&#35282;&#33394;&#25198;&#28436;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20511;&#37492;&#29087;&#24713;&#30340;&#27665;&#38388;&#24515;&#29702;&#23398;&#26415;&#35821;&#65292;&#32780;&#19981;&#26159;&#36171;&#20104;&#23427;&#20204;&#23454;&#38469;&#19978;&#24182;&#19981;&#20855;&#22791;&#30340;&#20154;&#31867;&#29305;&#24449;&#12290;&#26412;&#25991;&#20197;(&#34920;&#38754;&#19978;&#30340;)&#27450;&#39575;&#21644;(&#34920;&#38754;&#19978;&#30340;)&#33258;&#25105;&#24847;&#35782;&#20026;&#20363;&#65292;&#25506;&#35752;&#20102;&#23545;&#35805;&#20195;&#29702;&#31243;&#24207;&#34892;&#20026;&#30340;&#20004;&#31181;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
As dialogue agents become increasingly human-like in their performance, it is imperative that we develop effective ways to describe their behaviour in high-level terms without falling into the trap of anthropomorphism. In this paper, we foreground the concept of role-play. Casting dialogue agent behaviour in terms of role-play allows us to draw on familiar folk psychological terms, without ascribing human characteristics to language models they in fact lack. Two important cases of dialogue agent behaviour are addressed this way, namely (apparent) deception and (apparent) self-awareness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23376;&#30446;&#26631;&#30340;&#28436;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22522;&#20110;&#23376;&#30446;&#26631;&#30340;&#23398;&#20064;&#26041;&#27861;&#19982;&#25193;&#25955;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#28436;&#31034;&#30340;&#21487;&#29702;&#35299;&#24615;&#65292;&#24182;&#25552;&#39640;LLMs&#22312;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#20013;&#30340;&#21534;&#21520;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.16366</link><description>&lt;p&gt;
&#20943;&#23569;&#35868;&#22242;&#65306;&#22522;&#20110;&#23376;&#30446;&#26631;&#30340;&#28436;&#31034;&#23398;&#20064;&#22312;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal Theorem Proving. (arXiv:2305.16366v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23376;&#30446;&#26631;&#30340;&#28436;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22522;&#20110;&#23376;&#30446;&#26631;&#30340;&#23398;&#20064;&#26041;&#27861;&#19982;&#25193;&#25955;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#28436;&#31034;&#30340;&#21487;&#29702;&#35299;&#24615;&#65292;&#24182;&#25552;&#39640;LLMs&#22312;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#20013;&#30340;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#25506;&#32034;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#23436;&#20840;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#28436;&#31034;&#26684;&#24335;&#21644;&#32452;&#32455;&#26041;&#38754;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#22686;&#24378;LLMs&#30340;&#25928;&#33021;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23376;&#30446;&#26631;&#30340;&#28436;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#20803;&#32032;&#65306;&#31532;&#19968;&#65292;&#20174;&#24378;&#21270;&#23398;&#20064;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#23376;&#30446;&#26631;&#23398;&#20064;&#20013;&#27762;&#21462;&#32463;&#39564;&#65292;&#20026;&#27599;&#20010;&#28436;&#31034;&#31034;&#20363;&#26500;&#24314;&#19981;&#21516;&#30340;&#23376;&#30446;&#26631;&#65292;&#24182;&#26681;&#25454;&#30456;&#20851;&#30340;&#23376;&#30446;&#26631;&#23398;&#20064;&#29702;&#35770;&#26469;&#20248;&#21270;&#36825;&#20123;&#23376;&#30446;&#26631;&#12290;&#31532;&#20108;&#65292;&#21033;&#29992;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#26469;&#39044;&#27979;&#26368;&#20339;&#32452;&#32455;&#26041;&#24335;&#65292;&#21516;&#26102;&#35299;&#20915;&#28436;&#31034;&#32452;&#32455;&#39046;&#22495;&#20013;&#23384;&#22312;&#30340;&#20004;&#20010;&#22797;&#26434;&#38382;&#39064;&#65306;&#23376;&#38598;&#36873;&#25321;&#21644;&#39034;&#24207;&#30830;&#23450;&#12290;&#36890;&#36807;&#23558;&#22522;&#20110;&#23376;&#30446;&#26631;&#30340;&#23398;&#20064;&#26041;&#27861;&#19982;&#25193;&#25955;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20316;&#32773;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#28436;&#31034;&#30340;&#21487;&#29702;&#35299;&#24615;&#65292;&#24182;&#25552;&#39640;LLMs&#22312;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#20013;&#30340;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models~(LLMs) present an intriguing avenue of exploration in the domain of formal theorem proving. Nonetheless, the full utilization of these models, particularly in terms of demonstration formatting and organization, remains an underexplored area. In an endeavor to enhance the efficacy of LLMs, we introduce a subgoal-based demonstration learning framework, consisting of two primary elements: Firstly, drawing upon the insights of subgoal learning from the domains of reinforcement learning and robotics, we propose the construction of distinct subgoals for each demonstration example and refine these subgoals in accordance with the pertinent theories of subgoal learning. Secondly, we build upon recent advances in diffusion models to predict the optimal organization, simultaneously addressing two intricate issues that persist within the domain of demonstration organization: subset selection and order determination. Through the integration of subgoal-based learning methodolog
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#20027;&#21160;&#25237;&#36164;&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65288;E2EAI&#65289;&#65292;&#21253;&#21547;&#20102;&#22240;&#23376;&#36873;&#25321;&#12289;&#22240;&#23376;&#32452;&#21512;&#12289;&#32929;&#31080;&#36873;&#25321;&#21644;&#25237;&#36164;&#32452;&#21512;&#26500;&#24314;&#30340;&#25972;&#20010;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#23545;&#30495;&#23454;&#32929;&#31080;&#24066;&#22330;&#25968;&#25454;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16364</link><description>&lt;p&gt;
E2EAI&#65306;&#38754;&#21521;&#20027;&#21160;&#25237;&#36164;&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
E2EAI: End-to-End Deep Learning Framework for Active Investing. (arXiv:2305.16364v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#20027;&#21160;&#25237;&#36164;&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65288;E2EAI&#65289;&#65292;&#21253;&#21547;&#20102;&#22240;&#23376;&#36873;&#25321;&#12289;&#22240;&#23376;&#32452;&#21512;&#12289;&#32929;&#31080;&#36873;&#25321;&#21644;&#25237;&#36164;&#32452;&#21512;&#26500;&#24314;&#30340;&#25972;&#20010;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#23545;&#30495;&#23454;&#32929;&#31080;&#24066;&#22330;&#25968;&#25454;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#25237;&#36164;&#26088;&#22312;&#26500;&#24314;&#19968;&#32452;&#22312;&#24066;&#22330;&#19978;&#34987;&#35748;&#20026;&#30456;&#23545;&#26377;&#21033;&#21487;&#22270;&#30340;&#25237;&#36164;&#32452;&#21512;&#65292;&#20854;&#20013;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#22522;&#20110;&#22240;&#23376;&#30340;&#31574;&#30053;&#26469;&#26500;&#24314;&#25237;&#36164;&#32452;&#21512;&#12290;&#36817;&#24180;&#26469;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#21162;&#21147;&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#36861;&#27714;&#20855;&#26377;&#26356;&#39640;&#20027;&#21160;&#22238;&#25253;&#25110;&#26377;&#21069;&#36884;&#30340;&#36164;&#20135;&#36235;&#21183;&#39044;&#27979;&#27969;&#27700;&#32447;&#30340;&#8220;&#28145;&#24230;&#22240;&#23376;&#8221;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#36890;&#36807;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65288;E2E&#65289;&#26500;&#24314;&#20027;&#21160;&#25237;&#36164;&#32452;&#21512;&#30340;&#38382;&#39064;&#20173;&#28982;&#26159;&#24320;&#25918;&#30340;&#65292;&#22312;&#29616;&#26377;&#25991;&#29486;&#20013;&#24456;&#23569;&#24471;&#21040;&#35299;&#20915;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010; E2E&#65292;&#20960;&#20046;&#28085;&#30422;&#20102;&#22240;&#23376;&#36873;&#25321;&#12289;&#22240;&#23376;&#32452;&#21512;&#12289;&#32929;&#31080;&#36873;&#25321;&#21644;&#25237;&#36164;&#32452;&#21512;&#26500;&#24314;&#30340;&#25972;&#20010;&#36807;&#31243;&#12290;&#23545;&#30495;&#23454;&#32929;&#31080;&#24066;&#22330;&#25968;&#25454;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#22312;&#20027;&#21160;&#25237;&#36164;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active investing aims to construct a portfolio of assets that are believed to be relatively profitable in the markets, with one popular method being to construct a portfolio via factor-based strategies. In recent years, there have been increasing efforts to apply deep learning to pursue "deep factors'' with more active returns or promising pipelines for asset trends prediction. However, the question of how to construct an active investment portfolio via an end-to-end deep learning framework (E2E) is still open and rarely addressed in existing works. In this paper, we are the first to propose an E2E that covers almost the entire process of factor investing through factor selection, factor combination, stock selection, and portfolio construction. Extensive experiments on real stock market data demonstrate the effectiveness of our end-to-end deep leaning framework in active investing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20195;&#34920;&#19981;&#36275;&#30340;&#20122;&#32676;&#20307;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16363</link><description>&lt;p&gt;
&#22686;&#21152;&#20122;&#32676;&#27169;&#22411;&#24615;&#33021;&#30340;&#38598;&#25104;&#21512;&#25104;&#30005;&#23376;&#30149;&#21382;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Ensemble Synthetic EHR Generation for Increasing Subpopulation Model's Performance. (arXiv:2305.16363v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20195;&#34920;&#19981;&#36275;&#30340;&#20122;&#32676;&#20307;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#30149;&#21382;&#65288;EHR&#65289;&#36890;&#24120;&#21253;&#21547;&#26576;&#20123;&#20122;&#32676;&#20307;&#65288;SP&#65289;&#30340;&#19981;&#21516;&#27604;&#20363;&#34920;&#31034;&#12290;&#24739;&#32773;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#12289;&#20020;&#24202;&#24773;&#20917;&#30340;&#27969;&#34892;&#31243;&#24230;&#21644;&#21307;&#30103;&#20013;&#24515;&#31867;&#22411;&#31561;&#22240;&#32032;&#23548;&#33268;&#36825;&#31181;&#19981;&#20805;&#20998;&#30340;&#20195;&#34920;&#24615;&#12290;&#22240;&#27492;&#65292;&#24403;&#22312;&#36825;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#27169;&#22411;&#24456;&#38590;&#36827;&#34892;&#33391;&#22909;&#30340;&#27010;&#25324;&#24182;&#22312;&#20195;&#34920;&#19981;&#36275;&#30340;SP&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#22411;&#38598;&#25104;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;SP&#35757;&#32451;&#19968;&#20010;&#22522;&#20110;GAN&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#24182;&#23558;&#21512;&#25104;&#26679;&#26412;&#32435;&#20837;&#27599;&#20010;SP&#30340;&#35757;&#32451;&#38598;&#20013;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#35757;&#32451;SP&#29305;&#23450;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#20026;&#20102;&#27491;&#30830;&#35780;&#20272;&#35813;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#27969;&#31243;&#65292;&#24182;&#20351;&#29992;&#20174;MIMIC&#25968;&#25454;&#24211;&#26597;&#35810;&#30340;&#20004;&#20010;&#30495;&#23454;&#29992;&#20363;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#22312;&#20195;&#34920;&#19981;&#36275;&#30340;SP&#19978;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#27169;&#22411;&#23558;&#20316;&#20026;&#34917;&#20805;&#26448;&#26009;&#25552;&#20379;&#65292;&#24182;&#23558;&#22312;&#20844;&#20849;&#23384;&#20648;&#24211;&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic health records (EHR) often contain different rates of representation of certain subpopulations (SP). Factors like patient demographics, clinical condition prevalence, and medical center type contribute to this underrepresentation. Consequently, when training machine learning models on such datasets, the models struggle to generalize well and perform poorly on underrepresented SPs. To address this issue, we propose a novel ensemble framework that utilizes generative models. Specifically, we train a GAN-based synthetic data generator for each SP and incorporate synthetic samples into each SP training set. Ultimately, we train SP-specific prediction models. To properly evaluate this method, we design an evaluation pipeline with 2 real-world use case datasets, queried from the MIMIC database. Our approach shows increased model performance over underrepresented SPs. Our code and models are given as supplementary and will be made available on a public repository.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#27604;&#36739;&#20102;14&#31181;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#22312;&#23545;9&#31181;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#21644;&#19977;&#31181;&#34394;&#25311;&#26041;&#27861;&#36827;&#34892;&#24212;&#29992;&#26102;&#30340;&#25928;&#26524;&#65292;&#32473;&#20986;&#20102;&#39640;&#24230;&#30456;&#20851;&#32467;&#26524;&#65292;&#25351;&#20986;&#20102;&#23384;&#22312;&#28508;&#22312;&#20887;&#20313;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#22522;&#32447;&#36229;&#21442;&#25968;&#23545;&#35780;&#20272;&#25351;&#26631;&#20540;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.16361</link><description>&lt;p&gt;
&#19968;&#20010;&#20851;&#20110;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#35780;&#20272;&#30340;&#23454;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Experimental Investigation into the Evaluation of Explainability Methods. (arXiv:2305.16361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16361
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#27604;&#36739;&#20102;14&#31181;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#22312;&#23545;9&#31181;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#21644;&#19977;&#31181;&#34394;&#25311;&#26041;&#27861;&#36827;&#34892;&#24212;&#29992;&#26102;&#30340;&#25928;&#26524;&#65292;&#32473;&#20986;&#20102;&#39640;&#24230;&#30456;&#20851;&#32467;&#26524;&#65292;&#25351;&#20986;&#20102;&#23384;&#22312;&#28508;&#22312;&#20887;&#20313;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#22522;&#32447;&#36229;&#21442;&#25968;&#23545;&#35780;&#20272;&#25351;&#26631;&#20540;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#32972;&#21518;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#35768;&#22810;XAI&#26041;&#27861;&#65292;&#30456;&#24212;&#22320;&#65292;&#19982;XAI&#26041;&#27861;&#35780;&#20272;&#30456;&#20851;&#30340;&#23376;&#39046;&#22495;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#26088;&#22312;&#30830;&#23450;&#20351;&#29992;&#21508;&#31181;&#26041;&#27861;&#21644;&#26631;&#20934;&#25552;&#20379;&#26368;&#20339;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#32570;&#20047;&#23545;&#35780;&#20272;&#25351;&#26631;&#26412;&#36523;&#30340;&#27604;&#36739;&#65292;&#36825;&#20123;&#25351;&#26631;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;XAI&#26041;&#27861;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#27604;&#36739;14&#31181;&#19981;&#21516;&#30340;&#25351;&#26631;&#22312;&#23545;&#20061;&#31181;&#26368;&#20808;&#36827;&#30340;XAI&#26041;&#27861;&#21644;&#19977;&#31181;&#34394;&#25311;&#26041;&#27861;&#65288;&#20363;&#22914;&#38543;&#26426;&#26174;&#33879;&#24615;&#22270;&#65289;&#36827;&#34892;&#24212;&#29992;&#26102;&#30340;&#25928;&#26524;&#65292;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#21738;&#20123;&#25351;&#26631;&#20135;&#29983;&#39640;&#24230;&#30456;&#20851;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#23384;&#22312;&#28508;&#22312;&#30340;&#20887;&#20313;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22522;&#32447;&#36229;&#21442;&#25968;&#23545;&#35780;&#20272;&#25351;&#26631;&#20540;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#34394;&#25311;&#26041;&#27861;&#35780;&#20272;&#20102;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
EXplainable Artificial Intelligence (XAI) aims to help users to grasp the reasoning behind the predictions of an Artificial Intelligence (AI) system. Many XAI approaches have emerged in recent years. Consequently, a subfield related to the evaluation of XAI methods has gained considerable attention, with the aim to determine which methods provide the best explanation using various approaches and criteria. However, the literature lacks a comparison of the evaluation metrics themselves, that one can use to evaluate XAI methods. This work aims to fill this gap by comparing 14 different metrics when applied to nine state-of-the-art XAI methods and three dummy methods (e.g., random saliency maps) used as references. Experimental results show which of these metrics produces highly correlated results, indicating potential redundancy. We also demonstrate the significant impact of varying the baseline hyperparameter on the evaluation metric values. Finally, we use dummy methods to assess the re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#24179;&#34913;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411; (BMoE) &#36890;&#36807;&#22810;&#38376;&#19987;&#23478;&#28151;&#21512;&#27169;&#22359; (MMoE) &#21644; &#20219;&#21153;&#26799;&#24230;&#24179;&#34913; (TGB) &#27169;&#22359;&#26469;&#25551;&#36848;&#20219;&#21153;&#20851;&#31995;&#21644;&#24179;&#34913;&#35757;&#32451;&#36807;&#31243;&#65292;&#22312;&#35299;&#20915;&#25968;&#25454;&#25928;&#29575;&#21644;&#36127;&#38754;&#20256;&#36882;&#38382;&#39064;&#26041;&#38754;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16360</link><description>&lt;p&gt;
&#22312;&#22810;&#21464;&#37327;&#36719;&#20256;&#24863;&#22120;&#20013;&#24314;&#27169;&#20219;&#21153;&#20851;&#31995;&#30340;&#24179;&#34913;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Modeling Task Relationships in Multi-variate Soft Sensor with Balanced Mixture-of-Experts. (arXiv:2305.16360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#24179;&#34913;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411; (BMoE) &#36890;&#36807;&#22810;&#38376;&#19987;&#23478;&#28151;&#21512;&#27169;&#22359; (MMoE) &#21644; &#20219;&#21153;&#26799;&#24230;&#24179;&#34913; (TGB) &#27169;&#22359;&#26469;&#25551;&#36848;&#20219;&#21153;&#20851;&#31995;&#21644;&#24179;&#34913;&#35757;&#32451;&#36807;&#31243;&#65292;&#22312;&#35299;&#20915;&#25968;&#25454;&#25928;&#29575;&#21644;&#36127;&#38754;&#20256;&#36882;&#38382;&#39064;&#26041;&#38754;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#20272;&#35745;&#22810;&#20010;&#36136;&#37327;&#21464;&#37327;&#23545;&#20110;&#26500;&#24314;&#24037;&#19994;&#36719;&#20256;&#24863;&#22120;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#65292;&#36825;&#19968;&#30452;&#38754;&#20020;&#25968;&#25454;&#25928;&#29575;&#21644;&#36127;&#38754;&#20256;&#36882;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#34913;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;BMoE&#65289;&#65292;&#23427;&#30001;&#22810;&#38376;&#19987;&#23478;&#28151;&#21512;&#27169;&#22359;&#65288;MMoE&#65289;&#21644;&#20219;&#21153;&#26799;&#24230;&#24179;&#34913;&#65288;TGB&#65289;&#27169;&#22359;&#32452;&#25104;&#12290;MoE&#27169;&#22359;&#26088;&#22312;&#25551;&#36848;&#20219;&#21153;&#20851;&#31995;&#65292;&#32780;TGB&#27169;&#22359;&#21160;&#24577;&#24179;&#34913;&#20219;&#21153;&#20043;&#38388;&#30340;&#26799;&#24230;&#65292;&#20004;&#32773;&#20849;&#21516;&#35299;&#20915;&#20102;&#36127;&#38754;&#20256;&#36882;&#38382;&#39064;&#12290;&#22312;&#20856;&#22411;&#30340;&#30827;&#22238;&#25910;&#35013;&#32622;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;BMoE&#26377;&#25928;&#22320;&#24314;&#27169;&#20102;&#20219;&#21153;&#20851;&#31995;&#21644;&#24179;&#34913;&#20102;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#19988;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate estimation of multiple quality variables is critical for building industrial soft sensor models, which have long been confronted with data efficiency and negative transfer issues. Methods sharing backbone parameters among tasks address the data efficiency issue; however, they still fail to mitigate the negative transfer problem. To address this issue, a balanced Mixture-of-Experts (BMoE) is proposed in this work, which consists of a multi-gate mixture of experts (MMoE) module and a task gradient balancing (TGB) module. The MoE module aims to portray task relationships, while the TGB module balances the gradients among tasks dynamically. Both of them cooperate to mitigate the negative transfer problem. Experiments on the typical sulfur recovery unit demonstrate that BMoE models task relationship and balances the training process effectively, and achieves better performance than baseline models significantly.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25200;&#21160;&#29983;&#25104;&#26641;&#30340;&#21487;&#24494;&#32858;&#31867;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#32447;&#24615;&#35268;&#21010;&#35299;&#30340;&#38543;&#26426;&#25200;&#21160;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16358</link><description>&lt;p&gt;
&#24102;&#25200;&#21160;&#29983;&#25104;&#26641;&#30340;&#21487;&#24494;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentiable Clustering with Perturbed Spanning Forests. (arXiv:2305.16358v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16358
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25200;&#21160;&#29983;&#25104;&#26641;&#30340;&#21487;&#24494;&#32858;&#31867;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#32447;&#24615;&#35268;&#21010;&#35299;&#30340;&#38543;&#26426;&#25200;&#21160;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#26435;&#37325;&#29983;&#25104;&#26641;&#30340;&#21487;&#24494;&#32858;&#31867;&#26041;&#27861;&#65292;&#23427;&#26159;&#29983;&#25104;&#26641;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#20855;&#26377;&#22810;&#20010;&#36830;&#36890;&#20998;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#32447;&#24615;&#35268;&#21010;&#35299;&#30340;&#38543;&#26426;&#25200;&#21160;&#65292;&#20197;&#23454;&#29616;&#24179;&#28369;&#21644;&#39640;&#25928;&#30340;&#26799;&#24230;&#35745;&#31639;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#30340;&#27969;&#27700;&#32447;&#20013;&#21253;&#21547;&#32858;&#31867;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21363;&#20351;&#22312;&#22024;&#26434;&#30340;&#25968;&#25454;&#38598;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20960;&#20309;&#29615;&#22659;&#19979;&#20063;&#33021;&#33391;&#22909;&#22320;&#24037;&#20316;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#21046;&#23450;&#20102;&#19968;&#20010;&#29305;&#21035;&#30340;&#25439;&#22833;&#65292;&#20197;&#26377;&#25928;&#22320;&#20174;&#37096;&#20998;&#32858;&#31867;&#25968;&#25454;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#23427;&#22312;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a differentiable clustering method based on minimum-weight spanning forests, a variant of spanning trees with several connected components. Our method relies on stochastic perturbations of solutions of linear programs, for smoothing and efficient gradient computations. This allows us to include clustering in end-to-end trainable pipelines. We show that our method performs well even in difficult settings, such as datasets with high noise and challenging geometries. We also formulate an ad hoc loss to efficiently learn from partial clustering data using this operation. We demonstrate its performance on several real world datasets for supervised and semi-supervised tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WeiAvg&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#35843;&#26469;&#33258;&#39640;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#26356;&#26032;&#24182;&#20943;&#23569;&#26469;&#33258;&#20302;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#20102;&#32852;&#37030;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16351</link><description>&lt;p&gt;
WeiAvg&#65306;&#20419;&#36827;&#25968;&#25454;&#22810;&#26679;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
WeiAvg: Federated Learning Model Aggregation Promoting Data Diversity. (arXiv:2305.16351v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WeiAvg&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#35843;&#26469;&#33258;&#39640;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#26356;&#26032;&#24182;&#20943;&#23569;&#26469;&#33258;&#20302;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#20102;&#32852;&#37030;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20026;&#21033;&#29992;&#22823;&#35268;&#27169;&#31169;&#26377;&#36793;&#32536;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#24335;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#29289;&#32852;&#32593;&#35774;&#22791;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20248;&#21270;&#23398;&#20064;&#36807;&#31243;&#12289;&#35745;&#31639;&#25928;&#29575;&#21644;&#36890;&#20449;&#24320;&#38144;&#31561;&#26041;&#38754;&#65292;&#24573;&#30053;&#20102;&#21442;&#19982;&#32773;&#23545;&#32852;&#37030;&#27169;&#22411;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#24179;&#22343;&#65288;WeiAvg&#65289;&#30340;&#26694;&#26550;&#65292;&#30528;&#37325;&#24378;&#35843;&#26469;&#33258;&#39640;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#26356;&#26032;&#65292;&#24182;&#20943;&#23569;&#26469;&#33258;&#20302;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#25237;&#24433;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#26469;&#35780;&#20272;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning provides a promising privacy-preserving way for utilizing large-scale private edge data from massive Internet-of-Things (IoT) devices. While existing research extensively studied optimizing the learning process, computing efficiency, and communication overhead, one important and often overlooked aspect is that participants contribute predictive knowledge from their data, impacting the quality of the federated models learned. While FedAvg treats each client equally and assigns weight solely based on the number of samples, the diversity of samples on each client could greatly affect the local update performance and the final aggregated model. In this paper, we propose a novel approach to address this issue by introducing a Weighted Averaging (WeiAvg) framework that emphasizes updates from high-diversity clients and diminishes the influence of those from low-diversity clients. Specifically, we introduced a projection-based approximation method to estimate the diversity 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#31216;&#20026;Lexinvariant&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#19981;&#38656;&#35201;&#20219;&#20309;&#22266;&#23450;&#26631;&#35760;&#23884;&#20837;&#65292;&#23436;&#20840;&#20381;&#36182;&#19978;&#19979;&#25991;&#20013;&#26631;&#35760;&#30340;&#20849;&#29616;&#21644;&#37325;&#22797;&#12290;&#20316;&#32773;&#35777;&#26126;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;lexinvariant LM&#65292;&#20197;&#22810;&#39033;&#24335;&#26041;&#24335;&#19982;&#19978;&#19979;&#25991;&#38271;&#24230;&#25104;&#27604;&#20363;&#22320;&#25910;&#25947;&#21040;&#30495;&#23454;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#24120;&#37327;&#22240;&#23376;&#22312;&#35789;&#27719;&#34920;&#22823;&#23567;&#19979;&#20026;&#27425;&#32447;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16349</link><description>&lt;p&gt;
Lexinvariant&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lexinvariant Language Models. (arXiv:2305.16349v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#31216;&#20026;Lexinvariant&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#19981;&#38656;&#35201;&#20219;&#20309;&#22266;&#23450;&#26631;&#35760;&#23884;&#20837;&#65292;&#23436;&#20840;&#20381;&#36182;&#19978;&#19979;&#25991;&#20013;&#26631;&#35760;&#30340;&#20849;&#29616;&#21644;&#37325;&#22797;&#12290;&#20316;&#32773;&#35777;&#26126;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;lexinvariant LM&#65292;&#20197;&#22810;&#39033;&#24335;&#26041;&#24335;&#19982;&#19978;&#19979;&#25991;&#38271;&#24230;&#25104;&#27604;&#20363;&#22320;&#25910;&#25947;&#21040;&#30495;&#23454;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#24120;&#37327;&#22240;&#23376;&#22312;&#35789;&#27719;&#34920;&#22823;&#23567;&#19979;&#20026;&#27425;&#32447;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20196;&#29260;&#23884;&#20837;&#26159;&#20174;&#31163;&#25955;&#35789;&#27719;&#31526;&#21495;&#21040;&#36830;&#32493;&#21521;&#37327;&#30340;&#26144;&#23556;&#65292;&#26159;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#26680;&#24515;&#12290;&#20294;&#26159;&#65292;&#35789;&#27719;&#31526;&#21495;&#30340;&#21547;&#20041;&#20063;&#21487;&#20197;&#36890;&#36807;&#23427;&#20204;&#22312;&#38271;&#19978;&#19979;&#25991;&#20013;&#30340;&#32467;&#26500;&#35282;&#33394;&#26469;&#30830;&#23450;&#29978;&#33267;&#37325;&#26032;&#23450;&#20041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38382;&#65306;&#26159;&#21542;&#21487;&#33021;&#23384;&#22312;&#19968;&#31181;&#27809;&#26377;&#20219;&#20309;&#22266;&#23450;&#26631;&#35760;&#23884;&#20837;&#30340;&#24615;&#33021;&#33391;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;&#65311;&#36825;&#26679;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#23436;&#20840;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#20013;&#26631;&#35760;&#30340;&#20849;&#29616;&#21644;&#37325;&#22797;&#65292;&#32780;&#19981;&#26159;&#20219;&#20309;&#26631;&#35760;&#30340;\textit{a priori}&#26631;&#35782;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;\textit{lexinvariant}&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#23545;&#35789;&#27719;&#31526;&#21495;&#19981;&#21464;&#65292;&#22240;&#27492;&#22312;&#23454;&#36341;&#20013;&#19981;&#38656;&#35201;&#22266;&#23450;&#30340;&#20196;&#29260;&#23884;&#20837;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;lexinvariant LM&#65292;&#20197;&#22810;&#39033;&#24335;&#26041;&#24335;&#19982;&#19978;&#19979;&#25991;&#38271;&#24230;&#25104;&#27604;&#20363;&#22320;&#25910;&#25947;&#21040;&#30495;&#23454;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#24120;&#37327;&#22240;&#23376;&#22312;&#35789;&#27719;&#34920;&#22823;&#23567;&#19979;&#20026;&#27425;&#32447;&#24615;&#12290;&#20854;&#27425;&#65292;&#35201;&#26500;&#24314;&#19968;&#20010;lexinvariant LM&#65292;&#25105;&#20204;&#21482;&#38656;&#20351;&#29992;&#38543;&#26426;&#39640;&#26031;&#20989;&#25968;&#23545;&#26631;&#35760;&#36827;&#34892;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Token embeddings, a mapping from discrete lexical symbols to continuous vectors, are at the heart of any language model (LM). However, lexical symbol meanings can also be determined and even redefined by their structural role in a long context. In this paper, we ask: is it possible for a language model to be performant without \emph{any} fixed token embeddings? Such a language model would have to rely entirely on the co-occurence and repetition of tokens in the context rather than the \textit{a priori} identity of any token. To answer this, we study \textit{lexinvariant}language models that are invariant to lexical symbols and therefore do not need fixed token embeddings in practice. First, we prove that we can construct a lexinvariant LM to converge to the true language model at a uniform rate that is polynomial in terms of the context length, with a constant factor that is sublinear in the vocabulary size. Second, to build a lexinvariant LM, we simply encode tokens using random Gauss
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#34920;&#24449;&#29983;&#29289;&#36136;&#27700;&#28909;&#30899;&#21270;&#20135;&#29289;&#29305;&#24615;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#22312;&#19981;&#21516;&#22788;&#29702;&#26465;&#20214;&#19979;&#20174;&#21508;&#31181;&#29983;&#29289;&#36136;&#26469;&#28304;&#20135;&#29983;&#30340;&#27700;&#28845;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#65292;&#20026;&#29983;&#29289;&#36136;&#30340;&#21487;&#25345;&#32493;&#33021;&#28304;&#21644;&#26448;&#26009;&#29983;&#20135;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2305.16348</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#29983;&#29289;&#36136;&#27700;&#28909;&#30899;&#21270;&#20135;&#29289;&#29305;&#24615;&#34920;&#24449;&#65306;&#22312;&#21487;&#25345;&#32493;&#33021;&#28304;&#21644;&#26448;&#26009;&#29983;&#20135;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine learning-based characterization of hydrochar from biomass: Implications for sustainable energy and material production. (arXiv:2305.16348v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#34920;&#24449;&#29983;&#29289;&#36136;&#27700;&#28909;&#30899;&#21270;&#20135;&#29289;&#29305;&#24615;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#22312;&#19981;&#21516;&#22788;&#29702;&#26465;&#20214;&#19979;&#20174;&#21508;&#31181;&#29983;&#29289;&#36136;&#26469;&#28304;&#20135;&#29983;&#30340;&#27700;&#28845;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#65292;&#20026;&#29983;&#29289;&#36136;&#30340;&#21487;&#25345;&#32493;&#33021;&#28304;&#21644;&#26448;&#26009;&#29983;&#20135;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#28909;&#30899;&#21270;&#65288;HTC&#65289;&#26159;&#19968;&#31181;&#23558;&#29983;&#29289;&#36136;&#36716;&#21270;&#20026;&#22810;&#29992;&#36884;&#27700;&#28845;&#30340;&#36807;&#31243;&#65292;&#26080;&#38656;&#20107;&#20808;&#24178;&#29157;&#12290;&#27700;&#28845;&#30340;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#21463;&#29983;&#29289;&#36136;&#24615;&#36136;&#21644;&#22788;&#29702;&#21442;&#25968;&#30340;&#24433;&#21709;&#65292;&#20351;&#24471;&#36890;&#36807;&#35797;&#38169;&#23454;&#39564;&#20248;&#21270;&#20854;&#29305;&#23450;&#24212;&#29992;&#25104;&#20026;&#19968;&#39033;&#25361;&#25112;&#12290;&#20026;&#33410;&#30465;&#26102;&#38388;&#21644;&#25104;&#26412;&#65292;&#21487;&#20197;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#27169;&#22411;&#65292;&#23545;&#26469;&#33258;&#19981;&#21516;&#29983;&#29289;&#36136;&#26469;&#28304;&#22312;&#19981;&#21516;&#21453;&#24212;&#22788;&#29702;&#21442;&#25968;&#19979;&#20135;&#29983;&#30340;&#27700;&#28845;&#36827;&#34892;&#34920;&#24449;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#28085;&#30422;&#19968;&#31995;&#21015;&#29983;&#29289;&#36136;&#31867;&#22411;&#21644;&#21453;&#24212;&#22788;&#29702;&#21442;&#25968;&#30340;&#25968;&#25454;&#24211;&#24320;&#21457;&#21253;&#21547;&#24615;&#27169;&#22411;&#20197;&#34920;&#24449;&#27700;&#28845;&#12290;&#20351;&#29992;&#20004;&#20010;&#27169;&#22411;&#65288;&#20915;&#31574;&#26641;&#22238;&#24402;&#21644;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#65289;&#26469;&#39044;&#27979;&#27700;&#28845;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#12290;&#20915;&#31574;&#26641;&#22238;&#24402;&#27169;&#22411;&#22312;&#39044;&#27979;&#31934;&#24230;&#26041;&#38754;&#20248;&#20110;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#27169;&#22411;&#65288;R2 &gt; 0.88&#65292;RMSE &lt; 6.848&#21644;MAE &lt; 4.718&#65289;&#12290;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#20248;&#21270;&#20915;&#31574;&#26641;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#31934;&#24230;&#65292;&#24182;&#30830;&#23450;&#24433;&#21709;&#27700;&#28845;&#36136;&#37327;&#21644;&#20135;&#29575;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#22312;&#19981;&#21516;&#22788;&#29702;&#26465;&#20214;&#19979;&#20174;&#21508;&#31181;&#29983;&#29289;&#36136;&#26469;&#28304;&#20135;&#29983;&#30340;&#27700;&#28845;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#12290;&#24320;&#21457;&#30340;&#27169;&#22411;&#26377;&#26395;&#23545;&#20174;&#29983;&#29289;&#36136;&#20013;&#21487;&#25345;&#32493;&#29983;&#20135;&#33021;&#28304;&#21644;&#26448;&#26009;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hydrothermal carbonization (HTC) is a process that converts biomass into versatile hydrochar without the need for prior drying. The physicochemical properties of hydrochar are influenced by biomass properties and processing parameters, making it challenging to optimize for specific applications through trial-and-error experiments. To save time and money, machine learning can be used to develop a model that characterizes hydrochar produced from different biomass sources under varying reaction processing parameters. Thus, this study aims to develop an inclusive model to characterize hydrochar using a database covering a range of biomass types and reaction processing parameters. The quality and quantity of hydrochar are predicted using two models (decision tree regression and support vector regression). The decision tree regression model outperforms the support vector regression model in terms of forecast accuracy (R2 &gt; 0.88, RMSE &lt; 6.848, and MAE &lt; 4.718). Using an evolutionary algorithm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31958;&#23615;&#30149;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#22343;&#34920;&#29616;&#31361;&#20986;&#65292;&#20294;&#22806;&#37096;&#39564;&#35777;&#26377;&#38480;&#65292;&#35299;&#37322;&#24615;&#26041;&#27861;&#38656;&#35201;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.16346</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31934;&#20934;&#21307;&#30103;&#26041;&#27861;&#65306;&#31958;&#23615;&#30149;&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence-Based Methods for Precision Medicine: Diabetes Risk Prediction. (arXiv:2305.16346v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31958;&#23615;&#30149;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#22343;&#34920;&#29616;&#31361;&#20986;&#65292;&#20294;&#22806;&#37096;&#39564;&#35777;&#26377;&#38480;&#65292;&#35299;&#37322;&#24615;&#26041;&#27861;&#38656;&#35201;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2&#22411;&#31958;&#23615;&#30149;&#30340;&#26085;&#30410;&#26222;&#21450;&#38656;&#35201;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#39118;&#38505;&#35780;&#20272;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#34987;&#24191;&#27867;&#29992;&#20110;&#27492;&#30446;&#30340;&#65292;&#20294;&#32570;&#20047;&#23545;&#20854;&#36827;&#23637;&#21644;&#25361;&#25112;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#22522;&#20110;AI&#30340;2&#22411;&#31958;&#23615;&#30149;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#12290;&#21253;&#25324;&#20102;40&#20010;&#30740;&#31350;&#65292;&#20027;&#35201;&#21457;&#34920;&#22312;&#36807;&#21435;&#22235;&#24180;&#20013;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27604;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26356;&#26222;&#36941;&#12290;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#26159;&#26368;&#24120;&#29992;&#30340;&#25968;&#25454;&#26469;&#28304;&#12290;&#21333;&#27169;&#24577;&#20381;&#36182;EHR&#25968;&#25454;&#30340;AI&#27169;&#22411;&#24456;&#31361;&#20986;&#65292;&#32780;&#21482;&#26377;&#23569;&#25968;&#20351;&#29992;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#22343;&#34920;&#29616;&#33391;&#22909;&#65292;&#21518;&#32773;&#20248;&#20110;&#21069;&#32773;&#12290;&#20869;&#37096;&#39564;&#35777;&#24456;&#24120;&#35265;&#65292;&#32780;&#22806;&#37096;&#39564;&#35777;&#24456;&#26377;&#38480;&#12290;&#35299;&#37322;&#24615;&#26041;&#27861;&#22312;&#19968;&#21322;&#30340;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#25253;&#21578;&#12290;&#23569;&#25968;&#30740;&#31350;&#25253;&#21578;&#20102;&#26032;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#19988;&#26377;&#24320;&#28304;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rising prevalence of type 2 diabetes mellitus (T2DM) necessitates the development of predictive models for T2DM risk assessment. Artificial intelligence (AI) models are being extensively used for this purpose, but a comprehensive review of their advancements and challenges is lacking. This scoping review analyzes existing literature on AI-based models for T2DM risk prediction. Forty studies were included, mainly published in the past four years. Traditional machine learning models were more prevalent than deep learning models. Electronic health records were the most commonly used data source. Unimodal AI models relying on EHR data were prominent, while only a few utilized multimodal models. Both unimodal and multimodal models showed promising performance, with the latter outperforming the former. Internal validation was common, while external validation was limited. Interpretability methods were reported in half of the studies. Few studies reported novel biomarkers, and open-source
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#24179;&#38754;&#20998;&#31867;&#22120;&#30340;&#23398;&#20064;&#31639;&#27861;&#20013;&#38598;&#25104;&#23618;&#27425;&#20998;&#31867;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20808;&#39564;&#30693;&#35782;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23398;&#20064;&#32773;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21322;&#30417;&#30563;&#21644;&#23436;&#20840;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#37117;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16341</link><description>&lt;p&gt;
"TaxoKnow&#65306;&#20998;&#31867;&#20013;&#30340;&#20808;&#39564;&#30693;&#35782;&#8212;&#8212;&#23618;&#27425;&#20998;&#31867;&#20316;&#20026;&#22810;&#31867;&#20998;&#31867;&#25439;&#22833;&#20989;&#25968;&#20013;&#30340;&#20808;&#39564;&#30693;&#35782;"
&lt;/p&gt;
&lt;p&gt;
TaxoKnow: Taxonomy as Prior Knowledge in the Loss Function of Multi-class Classification. (arXiv:2305.16341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#24179;&#38754;&#20998;&#31867;&#22120;&#30340;&#23398;&#20064;&#31639;&#27861;&#20013;&#38598;&#25104;&#23618;&#27425;&#20998;&#31867;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20808;&#39564;&#30693;&#35782;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23398;&#20064;&#32773;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21322;&#30417;&#30563;&#21644;&#23436;&#20840;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#37117;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23558;&#26631;&#31614;&#30340;&#23618;&#27425;&#20998;&#31867;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#38598;&#25104;&#21040;&#24179;&#38754;&#20998;&#31867;&#22120;&#30340;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23558;&#23618;&#27425;&#20998;&#31867;&#20316;&#20026;&#26174;&#24335;&#27491;&#21017;&#21270;&#22120;&#38598;&#25104;&#21040;&#23398;&#20064;&#31639;&#27861;&#25439;&#22833;&#20989;&#25968;&#20013;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#23618;&#27425;&#20998;&#31867;&#36827;&#34892;&#25512;&#29702;&#65292;&#31070;&#32463;&#32593;&#32476;&#20943;&#36731;&#20102;&#20854;&#23545;&#31867;&#21035;&#30340;&#36755;&#20986;&#20998;&#24067;&#65292;&#20801;&#35768;&#23558;&#19978;&#23618;&#27010;&#24565;&#30340;&#26465;&#20214;&#38468;&#21152;&#21040;&#23569;&#25968;&#31867;&#19978;&#12290;&#25105;&#20204;&#20165;&#38480;&#20110;&#24179;&#38754;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#20004;&#20010;&#24037;&#19994;&#20869;&#37096;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#38598;&#65288;RCV1&#21644;Amazon&#20135;&#21697;&#35780;&#35770;&#65289;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;&#25105;&#20204;&#24471;&#21040;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#21322;&#30417;&#30563;&#22810;&#31867;&#20998;&#31867;&#20013;&#65292;&#20998;&#31867;&#36807;&#31243;&#20013;&#30340;&#20808;&#39564;&#30693;&#35782;&#21487;&#20197;&#26174;&#33879;&#22320;&#25552;&#39640;&#23398;&#20064;&#32773;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#23436;&#20840;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#20063;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the effectiveness of integrating a hierarchical taxonomy of labels as prior knowledge into the learning algorithm of a flat classifier. We introduce two methods to integrate the hierarchical taxonomy as an explicit regularizer into the loss function of learning algorithms. By reasoning on a hierarchical taxonomy, a neural network alleviates its output distributions over the classes, allowing conditioning on upper concepts for a minority class. We limit ourselves to the flat classification task and provide our experimental results on two industrial in-house datasets and two public benchmarks, RCV1 and Amazon product reviews. Our obtained results show the significant effect of a taxonomy in increasing the performance of a learner in semisupervised multi-class classification and the considerable results obtained in a fully supervised fashion.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#24490;&#29615;Transformer&#65288;SRformer&#65289;&#26469;&#20943;&#23569;&#35745;&#31639;/&#20869;&#23384;&#25104;&#26412;&#65292;&#24182;&#20351;&#29992;RAF&#23618;&#22788;&#29702;&#36328;&#27573;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.16340</link><description>&lt;p&gt;
&#20998;&#27573;&#24490;&#29615;Transformer:&#19968;&#31181;&#39640;&#25928;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model. (arXiv:2305.16340v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#24490;&#29615;Transformer&#65288;SRformer&#65289;&#26469;&#20943;&#23569;&#35745;&#31639;/&#20869;&#23384;&#25104;&#26412;&#65292;&#24182;&#20351;&#29992;RAF&#23618;&#22788;&#29702;&#36328;&#27573;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35821;&#35328;&#21644;&#35270;&#35273;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#25104;&#26412;&#21576;&#20108;&#27425;&#22686;&#38271;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24212;&#29992;&#20013;&#20351;&#29992;&#25104;&#20026;&#19981;&#21487;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#23558;&#25972;&#20010;&#24207;&#21015;&#21010;&#20998;&#25104;&#33509;&#24178;&#27573;&#12290;&#28982;&#21518;&#20351;&#29992;&#20855;&#26377;&#24490;&#29615;&#32467;&#26500;&#30340;&#31070;&#32463;&#20803;&#26469;&#32858;&#21512;&#36328;&#27573;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20855;&#26377;&#36739;&#20302;&#35745;&#31639;/&#20869;&#23384;&#25104;&#26412;&#30340;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#27169;&#22411;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#20351;&#29992;&#23616;&#37096;Attention&#26426;&#21046;&#23545;&#21333;&#20010;&#27573;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#24490;&#29615;Transformer&#65288;SRformer&#65289;&#65292;&#23427;&#23558;&#20998;&#27573;Attention&#21644;&#24490;&#29615;Attention&#30456;&#32467;&#21512;&#12290;&#23427;&#20351;&#29992;&#24490;&#29615;accumulate and fire&#65288;RAF&#65289;&#23618;&#22312;&#30456;&#37051;&#27573;&#20043;&#38388;&#22788;&#29702;&#20449;&#24687;&#12290;&#36890;&#36807;&#26356;&#26032;key&#30340;&#20135;&#21697;&#26469;&#34917;&#20607;&#20943;&#23569;Attention&#31383;&#21475;&#38271;&#24230;&#20135;&#29983;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have shown dominant performance across a range of domains including language and vision. However, their computational cost grows quadratically with the sequence length, making their usage prohibitive for resource-constrained applications. To counter this, our approach is to divide the whole sequence into segments. The information across segments can then be aggregated using neurons with recurrence leveraging their inherent memory. Such an approach leads to models with sequential processing capability at a lower computation/memory cost. To investigate this idea, first, we examine the effects of using local attention mechanism on the individual segments. Then we propose a segmented recurrent transformer (SRformer) that combines segmented attention with recurrent attention. It uses recurrent accumulate and fire (RAF) layers to process information between consecutive segments. The loss caused by reducing the attention window length is compensated by updating the product of key
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#20869;&#37096;&#24037;&#20316;&#35760;&#24518;&#27169;&#22359;&#30340;&#20915;&#31574;Transformer&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#20195;&#29702;&#22312;&#22788;&#29702;&#26032;&#20219;&#21153;&#19978;&#24615;&#33021;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#36716;&#21270;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16338</link><description>&lt;p&gt;
&#28145;&#24605;&#29087;&#34385;&#65306;&#20855;&#26377;&#20869;&#37096;&#24037;&#20316;&#35760;&#24518;&#30340;&#20915;&#31574;Transformer
&lt;/p&gt;
&lt;p&gt;
Think Before You Act: Decision Transformers with Internal Working Memory. (arXiv:2305.16338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#20869;&#37096;&#24037;&#20316;&#35760;&#24518;&#27169;&#22359;&#30340;&#20915;&#31574;Transformer&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#20195;&#29702;&#22312;&#22788;&#29702;&#26032;&#20219;&#21153;&#19978;&#24615;&#33021;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#36716;&#21270;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#24050;&#32463;&#23637;&#31034;&#20102;&#36328;&#36234;&#22810;&#20010;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#31181;&#20302;&#25928;&#24615;&#28304;&#20110;&#36951;&#24536;&#29616;&#35937;&#65292;&#21363;&#27169;&#22411;&#36890;&#36807;&#21442;&#25968;&#35760;&#24518;&#20854;&#34892;&#20026;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#12290;&#22240;&#27492;&#65292;&#26032;&#20219;&#21153;&#30340;&#35757;&#32451;&#21487;&#33021;&#20250;&#38477;&#20302;&#27169;&#22411;&#22312;&#20808;&#21069;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#19982;LLM&#30340;&#38544;&#24335;&#35760;&#24518;&#26426;&#21046;&#19981;&#21516;&#65292;&#20154;&#33041;&#21033;&#29992;&#20998;&#24067;&#24335;&#23384;&#20648;&#22120;&#23384;&#20648;&#35760;&#24518;&#65292;&#20197;&#26377;&#25928;&#22320;&#31649;&#29702;&#21644;&#32452;&#32455;&#22810;&#31181;&#25216;&#33021;&#65292;&#20943;&#36731;&#20102;&#36951;&#24536;&#29616;&#35937;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#20869;&#37096;&#24037;&#20316;&#35760;&#24518;&#27169;&#22359;&#26469;&#23384;&#20648;&#12289;&#34701;&#21512;&#21644;&#26816;&#32034;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;Atari&#28216;&#25103;&#21644;&#20803;&#19990;&#30028;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35760;&#24518;&#24494;&#35843;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#36716;&#21270;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language model (LLM)-based decision-making agents have shown the ability to generalize across multiple tasks. However, their performance relies on massive data and compute. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model's performance on previous tasks. In contrast to LLMs' implicit memory mechanism, the human brain utilizes distributed memory storage, which helps manage and organize multiple skills efficiently, mitigating the forgetting phenomenon. Thus inspired, we propose an internal working memory module to store, blend, and retrieve information for different downstream tasks. Evaluation results show that the proposed method improves training efficiency and generalization in both Atari games and meta-world object manipulation tasks. Moreover, we demonstrate that memory fine-tuning further enhances the adaptability of t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20581;&#22766;&#30701;&#25991;&#26412;&#32858;&#31867;&#65288;RSTC&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#26368;&#20248;&#36755;&#36816;&#30340;&#20266;&#26631;&#31614;&#29983;&#25104;&#65292;&#20197;&#21450;&#22522;&#20110;&#31867;&#21644;&#23454;&#20363;&#30340;&#23545;&#27604;&#23398;&#20064;&#30340;&#20581;&#22766;&#34920;&#31034;&#23398;&#20064;&#65292;&#24110;&#21161;&#25552;&#39640;&#23545;&#19981;&#24179;&#34913;&#21644;&#22122;&#38899;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16335</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#36866;&#24212;&#26368;&#20248;&#36755;&#36816;&#30340;&#20581;&#22766;&#30701;&#25991;&#26412;&#32858;&#31867;&#20013;&#21487;&#38752;&#20266;&#26631;&#31614;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Representation Learning with Reliable Pseudo-labels Generation via Self-Adaptive Optimal Transport for Short Text Clustering. (arXiv:2305.16335v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20581;&#22766;&#30701;&#25991;&#26412;&#32858;&#31867;&#65288;RSTC&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#26368;&#20248;&#36755;&#36816;&#30340;&#20266;&#26631;&#31614;&#29983;&#25104;&#65292;&#20197;&#21450;&#22522;&#20110;&#31867;&#21644;&#23454;&#20363;&#30340;&#23545;&#27604;&#23398;&#20064;&#30340;&#20581;&#22766;&#34920;&#31034;&#23398;&#20064;&#65292;&#24110;&#21161;&#25552;&#39640;&#23545;&#19981;&#24179;&#34913;&#21644;&#22122;&#38899;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#25991;&#26412;&#32858;&#31867;&#22240;&#36755;&#20837;&#30340;&#19981;&#24179;&#34913;&#21644;&#22122;&#38899;&#25968;&#25454;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#24456;&#22909;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#23481;&#26131;&#22312;&#37325;&#24230;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#36864;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#19988;&#26131;&#21463;&#21040;&#22122;&#22768;&#24178;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20581;&#22766;&#30701;&#25991;&#26412;&#32858;&#31867;&#65288;RSTC&#65289;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#23545;&#19981;&#24179;&#34913;&#21644;&#22122;&#38899;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#12290;RSTC&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65292;&#21363;&#20266;&#26631;&#35760;&#29983;&#25104;&#27169;&#22359;&#21644;&#20581;&#22766;&#34920;&#31034;&#23398;&#20064;&#27169;&#22359;&#12290;&#21069;&#32773;&#29983;&#25104;&#20266;&#26631;&#35760;&#65292;&#20026;&#21518;&#32773;&#25552;&#20379;&#30417;&#30563;&#65292;&#26377;&#21161;&#20110;&#26356;&#20581;&#22766;&#30340;&#34920;&#31034;&#21644;&#27491;&#30830;&#20998;&#31163;&#30340;&#32858;&#31867;&#12290;&#20026;&#20102;&#25552;&#20379;&#23545;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#20266;&#26631;&#31614;&#29983;&#25104;&#27169;&#22359;&#20013;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#26368;&#20248;&#36755;&#36816;&#12290;&#20026;&#20102;&#25552;&#39640;&#23545;&#25968;&#25454;&#20013;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#20581;&#22766;&#34920;&#31034;&#23398;&#20064;&#27169;&#22359;&#20013;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#22522;&#20110;&#31867;&#21644;&#23454;&#20363;&#30340;&#23545;&#27604;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Short text clustering is challenging since it takes imbalanced and noisy data as inputs. Existing approaches cannot solve this problem well, since (1) they are prone to obtain degenerate solutions especially on heavy imbalanced datasets, and (2) they are vulnerable to noises. To tackle the above issues, we propose a Robust Short Text Clustering (RSTC) model to improve robustness against imbalanced and noisy data. RSTC includes two modules, i.e., pseudo-label generation module and robust representation learning module. The former generates pseudo-labels to provide supervision for the later, which contributes to more robust representations and correctly separated clusters. To provide robustness against the imbalance in data, we propose self-adaptive optimal transport in the pseudo-label generation module. To improve robustness against the noise in data, we further introduce both class-wise and instance-wise contrastive learning in the robust representation learning module. Our empirical 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#25991;&#26412;&#22686;&#24191;&#23545;ASR&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#21512;&#25104;&#25991;&#26412;&#65292;&#24182;&#36890;&#36807;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#36716;&#25442;&#20026;&#21512;&#25104;&#35821;&#38899;&#65292;&#23454;&#39564;&#21457;&#29616;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#25991;&#26412;&#22686;&#24191;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;ASR&#20934;&#30830;&#24230;&#65292;&#21487;&#20197;&#20316;&#20026;&#25913;&#36827;ASR&#31995;&#32479;&#30340;&#19968;&#31181;&#21487;&#34892;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.16333</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#38899;&#21512;&#25104;&#30340;&#25991;&#26412;&#29983;&#25104;&#29992;&#20110;ASR&#25968;&#25454;&#22686;&#24191;
&lt;/p&gt;
&lt;p&gt;
Text Generation with Speech Synthesis for ASR Data Augmentation. (arXiv:2305.16333v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#25991;&#26412;&#22686;&#24191;&#23545;ASR&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#21512;&#25104;&#25991;&#26412;&#65292;&#24182;&#36890;&#36807;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#36716;&#25442;&#20026;&#21512;&#25104;&#35821;&#38899;&#65292;&#23454;&#39564;&#21457;&#29616;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#25991;&#26412;&#22686;&#24191;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;ASR&#20934;&#30830;&#24230;&#65292;&#21487;&#20197;&#20316;&#20026;&#25913;&#36827;ASR&#31995;&#32479;&#30340;&#19968;&#31181;&#21487;&#34892;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20943;&#23569;&#23545;&#26114;&#36149;&#20154;&#24037;&#27880;&#37322;&#30340;&#20381;&#36182;&#65292;&#25968;&#25454;&#22686;&#24191;&#19968;&#30452;&#26159;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#39046;&#22495;&#30340;&#19968;&#20010;&#28909;&#38376;&#30740;&#31350;&#26041;&#21521;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20391;&#37325;&#20110;&#29992;&#20110;ASR&#25968;&#25454;&#22686;&#24191;&#30340;&#21512;&#25104;&#35821;&#38899;&#29983;&#25104;&#65292;&#32780;&#20854;&#19982;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#30340;&#32467;&#21512;&#21364;&#30456;&#23545;&#36739;&#23569;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#25506;&#32034;&#25991;&#26412;&#22686;&#24191;&#23545;ASR&#30340;&#24433;&#21709;&#65292;&#24182;&#23558;&#20854;&#19982;&#20256;&#32479;&#25991;&#26412;&#22686;&#24191;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#27604;&#36739;&#12290;&#29983;&#25104;&#30340;&#21512;&#25104;&#25991;&#26412;&#28982;&#21518;&#36890;&#36807;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#31995;&#32479;&#36716;&#25442;&#20026;&#21512;&#25104;&#35821;&#38899;&#65292;&#24182;&#28155;&#21152;&#21040;ASR&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#21457;&#29616;&#65292;&#31070;&#32463;&#27169;&#22411;&#23454;&#29616;&#20102;9&#65285;-15&#65285;&#30340;&#30456;&#23545;WER&#25913;&#36827;&#65292;&#24182;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#29616;&#20195;&#31070;&#32463;&#26041;&#27861;&#65292;&#25991;&#26412;&#22686;&#24191;&#26159;&#25552;&#39640;ASR&#31995;&#32479;&#20934;&#30830;&#24615;&#30340;&#19968;&#31181;&#21487;&#34892;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aiming at reducing the reliance on expensive human annotations, data synthesis for Automatic Speech Recognition (ASR) has remained an active area of research. While prior work mainly focuses on synthetic speech generation for ASR data augmentation, its combination with text generation methods is considerably less explored. In this work, we explore text augmentation for ASR using large-scale pre-trained neural networks, and systematically compare those to traditional text augmentation methods. The generated synthetic texts are then converted to synthetic speech using a text-to-speech (TTS) system and added to the ASR training data. In experiments conducted on three datasets, we find that neural models achieve 9%-15% relative WER improvement and outperform traditional methods. We conclude that text augmentation, particularly through modern neural approaches, is a viable tool for improving the accuracy of ASR systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32467;&#21512;&#26426;&#22120;&#20154;&#21644;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#30340;&#26041;&#24335;&#19982;60&#21517;&#21442;&#19982;&#32773;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#36830;&#32493;&#23398;&#20064;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#65292;&#21442;&#19982;&#32773;&#26356;&#20542;&#21521;&#20110;&#19982;&#20854;&#36827;&#34892;&#21453;&#22797;&#20132;&#20114;&#65292;&#24182;&#25552;&#20379;&#26356;&#22810;&#30340;&#21453;&#39304;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.16332</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#23454;&#29616;&#36830;&#32493;&#23398;&#20064;--&#20154;&#31867;&#22312;&#19982;&#26426;&#22120;&#20154;&#21453;&#22797;&#20132;&#20114;&#20013;&#23545;&#26426;&#22120;&#20154;&#36830;&#32493;&#23398;&#20064;&#30340;&#30475;&#27861;
&lt;/p&gt;
&lt;p&gt;
Continual Learning through Human-Robot Interaction -- Human Perceptions of a Continual Learning Robot in Repeated Interactions. (arXiv:2305.16332v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32467;&#21512;&#26426;&#22120;&#20154;&#21644;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#30340;&#26041;&#24335;&#19982;60&#21517;&#21442;&#19982;&#32773;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#36830;&#32493;&#23398;&#20064;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#65292;&#21442;&#19982;&#32773;&#26356;&#20542;&#21521;&#20110;&#19982;&#20854;&#36827;&#34892;&#21453;&#22797;&#20132;&#20114;&#65292;&#24182;&#25552;&#20379;&#26356;&#22810;&#30340;&#21453;&#39304;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#21160;&#24577;&#30340;&#23454;&#38469;&#29615;&#22659;&#20013;&#38271;&#26399;&#37096;&#32626;&#36741;&#21161;&#26426;&#22120;&#20154;&#65292;&#26426;&#22120;&#20154;&#24517;&#39035;&#32487;&#32493;&#23398;&#20064;&#21644;&#36866;&#24212;&#20854;&#29615;&#22659;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#19981;&#26029;&#20174;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#24182;&#36991;&#20813;&#36951;&#24536;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#34429;&#28982;&#36825;&#20123;CL&#27169;&#22411;&#21487;&#20197;&#32531;&#35299;&#38745;&#24577;&#12289;&#31995;&#32479;&#22320;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#36951;&#24536;&#65292;&#20294;&#20154;&#20204;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22312;&#22810;&#27425;&#20132;&#20114;&#20013;&#36830;&#32493;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#26159;&#22914;&#20309;&#34987;&#20154;&#31867;&#29992;&#25143;&#25152;&#24863;&#30693;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#23558;&#30446;&#26631;&#35782;&#21035;&#30340;CL&#27169;&#22411;&#19982;Fetch&#31227;&#21160;&#25805;&#32437;&#26426;&#22120;&#20154;&#36827;&#34892;&#25972;&#21512;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#21442;&#19982;&#32773;&#22312;&#22810;&#20010;&#20250;&#35805;&#20013;&#30452;&#25509;&#25945;&#25480;&#21644;&#27979;&#35797;&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#24320;&#23637;&#20102;&#19968;&#39033;&#29616;&#22330;&#30740;&#31350;&#65292;60&#21517;&#21442;&#19982;&#32773;&#22312;300&#20010;&#20250;&#35805;&#20013;&#19982;&#25105;&#20204;&#30340;&#31995;&#32479;&#20114;&#21160;&#65288;&#27599;&#20010;&#21442;&#19982;&#32773;5&#27425;&#20250;&#35805;&#65289;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20004;&#32452;&#23454;&#39564;&#30340;&#30740;&#31350;&#65292;&#24182;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;CL&#27169;&#22411;&#65288;&#19977;&#20010;&#23454;&#39564;&#26465;&#20214;&#65289;&#26469;&#20102;&#35299;&#20154;&#31867;&#23545;&#36830;&#32493;&#23398;&#20064;&#26426;&#22120;&#20154;&#30340;&#30475;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
For long-term deployment in dynamic real-world environments, assistive robots must continue to learn and adapt to their environments. Researchers have developed various computational models for continual learning (CL) that can allow robots to continually learn from limited training data, and avoid forgetting previous knowledge. While these CL models can mitigate forgetting on static, systematically collected datasets, it is unclear how human users might perceive a robot that continually learns over multiple interactions with them. In this paper, we developed a system that integrates CL models for object recognition with a Fetch mobile manipulator robot and allows human participants to directly teach and test the robot over multiple sessions. We conducted an in-person study with 60 participants who interacted with our system in 300 sessions (5 sessions per participant). We conducted a between-participant study with three different CL models (3 experimental conditions) to understand huma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#25552;&#31034;&#24037;&#31243;&#31574;&#30053;&#21019;&#24314;&#20102;&#22522;&#20110;&#21521;&#37327;&#30340;&#21270;&#23398;&#25628;&#32034;&#26041;&#27861;&#65292;&#21487;&#35782;&#21035;&#20986;&#19982;&#26597;&#35810;&#21151;&#33021;&#31867;&#20284;&#20294;&#22312;&#32467;&#26500;&#19978;&#19981;&#21516;&#30340;&#20998;&#23376;&#12290;</title><link>http://arxiv.org/abs/2305.16330</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#21270;&#23398;&#30456;&#20284;&#24615;&#25628;&#32034;&#30340;&#25552;&#31034;&#24037;&#31243;&#21487;&#35782;&#21035;&#20986;&#20855;&#26377;&#32467;&#26500;&#24046;&#24322;&#30340;&#21151;&#33021;&#31867;&#20284;&#29289;
&lt;/p&gt;
&lt;p&gt;
Prompt Engineering for Transformer-based Chemical Similarity Search Identifies Structurally Distinct Functional Analogues. (arXiv:2305.16330v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#25552;&#31034;&#24037;&#31243;&#31574;&#30053;&#21019;&#24314;&#20102;&#22522;&#20110;&#21521;&#37327;&#30340;&#21270;&#23398;&#25628;&#32034;&#26041;&#27861;&#65292;&#21487;&#35782;&#21035;&#20986;&#19982;&#26597;&#35810;&#21151;&#33021;&#31867;&#20284;&#20294;&#22312;&#32467;&#26500;&#19978;&#19981;&#21516;&#30340;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#30456;&#20284;&#24615;&#25628;&#32034;&#26159;&#29992;&#20110;&#35782;&#21035;&#26032;&#30340;&#33647;&#29289;&#20998;&#23376;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#26041;&#27861;&#65292;&#20854;&#20013;&#21382;&#21490;&#19978;&#19968;&#30452;&#20381;&#36182;&#20110;&#22522;&#20110;&#32467;&#26500;&#30340;&#27604;&#36739;&#26469;&#35745;&#31639;&#20998;&#23376;&#30456;&#20284;&#24615;&#12290;&#26412;&#25991;&#20351;&#29992;&#21270;&#23398;&#35821;&#35328;&#27169;&#22411;&#21019;&#24314;&#20102;&#22522;&#20110;&#21521;&#37327;&#30340;&#21270;&#23398;&#25628;&#32034;&#65292;&#24182;&#37319;&#29992;&#20102;&#25552;&#31034;&#24037;&#31243;&#31574;&#30053;&#26469;&#25193;&#23637;&#23454;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;&#26816;&#32034;&#20116;&#31181;&#31867;&#20284;&#20110;&#33647;&#21697;&#30340;&#20998;&#23376;&#21644;&#19977;&#31181;&#31867;&#20284;&#26579;&#26009;&#30340;&#20998;&#23376;&#30340;&#25628;&#32034;&#32467;&#26524;&#26469;&#25506;&#32034;&#35813;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35813;&#26032;&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#20986;&#21151;&#33021;&#19978;&#31867;&#20284;&#20110;&#26597;&#35810;&#30340;&#20998;&#23376;&#65292;&#24182;&#19988;&#36825;&#20123;&#20998;&#23376;&#22312;&#32467;&#26500;&#19978;&#19982;&#26597;&#35810;&#19981;&#21516;&#65292;&#22240;&#27492;&#36890;&#36807;&#20256;&#32479;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#26041;&#27861;&#19981;&#23481;&#26131;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chemical similarity searches are widely used in-silico methods for identifying new drug-like molecules. These methods have historically relied on structure-based comparisons to compute molecular similarity. Here, we use a chemical language model to create a vector-based chemical search. We extend implementations by creating a prompt engineering strategy that utilizes two different chemical string representation algorithms: one for the query and the other for the database. We explore this method by reviewing the search results from five drug-like query molecules (penicillin G, nirmatrelvir, zidovudine, lysergic acid diethylamide, and fentanyl) and three dye-like query molecules (acid blue 25, avobenzone, and 2-diphenylaminocarbazole). We find that this novel method identifies molecules that are functionally similar to the query, indicated by the associated patent literature, and that many of these molecules are structurally distinct from the query, making them unlikely to be found with 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#32452;&#21512;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#32452;&#21512;&#35270;&#35273;&#38382;&#31572;&#22522;&#20934;&#65292;&#21477;&#27861;&#31070;&#32463;&#27169;&#22359;&#33976;&#39311;&#31561;&#26041;&#27861;&#20197;&#25552;&#39640;&#32452;&#21512;&#33021;&#21147;&#65292;&#24182;&#25506;&#32034;&#20102;&#23545;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#30340;&#22240;&#26524;&#36861;&#36394;&#20197;&#23450;&#20301;&#37325;&#35201;&#31070;&#32463;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.16328</link><description>&lt;p&gt;
&#35270;&#35273;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Semantic Composition in Visually Grounded Language Models. (arXiv:2305.16328v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#32452;&#21512;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#32452;&#21512;&#35270;&#35273;&#38382;&#31572;&#22522;&#20934;&#65292;&#21477;&#27861;&#31070;&#32463;&#27169;&#22359;&#33976;&#39311;&#31561;&#26041;&#27861;&#20197;&#25552;&#39640;&#32452;&#21512;&#33021;&#21147;&#65292;&#24182;&#25506;&#32034;&#20102;&#23545;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#30340;&#22240;&#26524;&#36861;&#36394;&#20197;&#23450;&#20301;&#37325;&#35201;&#31070;&#32463;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#30340;&#24847;&#20041;&#21644;&#20854;&#29702;&#24819;&#34920;&#36798;&#26041;&#24335;&#26159;&#20160;&#20040;&#65311;&#20154;&#31867;&#35821;&#35328;&#34920;&#29616;&#21147;&#30340;&#24456;&#22823;&#19968;&#37096;&#20998;&#26469;&#33258;&#35821;&#20041;&#32452;&#21512;&#65292;&#21363;&#20154;&#31867;&#24515;&#26234;&#20197;&#23618;&#27425;&#21270;&#21644;&#20851;&#31995;&#24615;&#26041;&#24335;&#34920;&#31034;&#24847;&#20041;&#30340;&#33021;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#37096;&#20998;&#21477;&#23376;&#30340;&#24847;&#20041;&#23384;&#22312;&#20110;&#25991;&#26412;&#20043;&#22806;&#65292;&#38656;&#35201;&#22522;&#20110;&#24863;&#23448;&#12289;&#36816;&#21160;&#21644;&#20307;&#39564;&#27169;&#24577;&#36827;&#34892;&#20805;&#20998;&#30340;&#23398;&#20064;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26174;&#31034;&#20986;&#30456;&#24403;&#30340;&#32452;&#21512;&#33021;&#21147;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#31034;&#32452;&#21512;&#32467;&#26500;&#26102;&#20005;&#37325;&#22833;&#36133;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#27169;&#22411;&#26159;&#21542;&#21450;&#22914;&#20309;&#32452;&#21512;&#35270;&#35273;&#19978;&#19979;&#25991;&#35821;&#20041;&#20197;&#21450;&#22914;&#20309;&#25552;&#39640;&#20854;&#32452;&#21512;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; 1) WinogroundVQA&#65292;&#19968;&#20010;&#26032;&#30340;&#32452;&#21512;&#35270;&#35273;&#38382;&#31572;&#22522;&#20934;&#65292;2) &#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#20013;&#32452;&#21512;&#33021;&#21147;&#30340;&#21477;&#27861;&#31070;&#32463;&#27169;&#22359;&#33976;&#39311;&#65292;3) &#23545;&#20110;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#30340;&#22240;&#26524;&#36861;&#36394;&#65292;&#20197;&#23450;&#20301;&#37325;&#35201;&#30340;&#31070;&#32463;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
What is sentence meaning and its ideal representation? Much of the expressive power of human language derives from semantic composition, the mind's ability to represent meaning hierarchically &amp; relationally over constituents. At the same time, much sentential meaning is outside the text and requires grounding in sensory, motor, and experiential modalities to be adequately learned. Although large language models display considerable compositional ability, recent work shows that visually-grounded language models drastically fail to represent compositional structure. In this thesis, we explore whether &amp; how models compose visually grounded semantics, and how we might improve their ability to do so.  Specifically, we introduce 1) WinogroundVQA, a new compositional visual question answering benchmark, 2) Syntactic Neural Module Distillation, a measure of compositional ability in sentence embedding models, 3) Causal Tracing for Image Captioning Models to locate neural representations vital f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.16326</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;: &#22522;&#20934;&#12289;&#22522;&#32447;&#21644;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. (arXiv:2305.16326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#25163;&#21160;&#31579;&#36873;&#21644;&#25552;&#21462;&#30693;&#35782;&#21464;&#24471;&#22256;&#38590;&#12290;&#33258;&#21160;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;BioNLP&#65289;&#25216;&#26415;&#26377;&#21161;&#20110;&#20943;&#36731;&#36825;&#31181;&#36127;&#25285;&#12290;&#36817;&#24180;&#26469;&#65292;&#22914;GPT-3&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#22312;BioNLP&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#23545;&#26041;&#27861;&#24320;&#21457;&#21644;&#19979;&#28216;&#29992;&#25143;&#30340;&#24433;&#21709;&#20173;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#65288;1&#65289;&#22312;&#22235;&#20010;&#24212;&#29992;&#31243;&#24207;&#20013;&#22312;&#20843;&#20010;BioNLP&#25968;&#25454;&#38598;&#20013;&#24314;&#31435;&#20102;GPT-3&#21644;GPT-4&#22312;&#38646;-shot&#21644;&#19968;-shot&#35774;&#32622;&#19979;&#30340;&#22522;&#20934;&#34920;&#29616;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#20851;&#31995;&#25552;&#21462;&#65292;&#22810;&#26631;&#31614;&#25991;&#26723;&#20998;&#31867;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#25512;&#29702;&#65307;&#65288;2&#65289;&#23457;&#26597;&#20102;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#65292;&#24182;&#23558;&#38169;&#35823;&#20998;&#20026;&#19977;&#31181;&#31867;&#22411;&#65306;&#32570;&#22833;&#65292;&#19981;&#19968;&#33268;&#21644;&#19981;&#38656;&#35201;&#30340;&#20154;&#24037;&#20869;&#23481;&#65307;&#65288;3&#65289;&#25552;&#20986;&#20102;&#20351;&#29992;LLMs&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical literature is growing rapidly, making it challenging to curate and extract knowledge manually. Biomedical natural language processing (BioNLP) techniques that can automatically extract information from biomedical literature help alleviate this burden. Recently, large Language Models (LLMs), such as GPT-3 and GPT-4, have gained significant attention for their impressive performance. However, their effectiveness in BioNLP tasks and impact on method development and downstream users remain understudied. This pilot study (1) establishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and one-shot settings in eight BioNLP datasets across four applications: named entity recognition, relation extraction, multi-label document classification, and semantic similarity and reasoning, (2) examines the errors produced by the LLMs and categorized the errors into three types: missingness, inconsistencies, and unwanted artificial content, and (3) provides suggestions for using L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#26694;&#26550;&#26469;&#22521;&#35757;&#21644;&#37325;&#26032;&#26657;&#20934;&#22270;&#31070;&#32463;&#32593;&#32476;&#21512;&#22863;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#24102;&#26377;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#33021;&#37327;&#21644;&#21147;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#26448;&#26009;&#30340;&#32467;&#26500;&#20248;&#21270;&#21644;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#12290;</title><link>http://arxiv.org/abs/2305.16325</link><description>&lt;p&gt;
&#20855;&#26377;&#26657;&#20934;&#30340;Aleatoric&#21644;Epistemic&#19981;&#30830;&#23450;&#24615;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#20114;&#20316;&#29992;&#21183;&#21512;&#22863;&#36816;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network Interatomic Potential Ensembles with Calibrated Aleatoric and Epistemic Uncertainty on Energy and Forces. (arXiv:2305.16325v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#26694;&#26550;&#26469;&#22521;&#35757;&#21644;&#37325;&#26032;&#26657;&#20934;&#22270;&#31070;&#32463;&#32593;&#32476;&#21512;&#22863;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#24102;&#26377;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#33021;&#37327;&#21644;&#21147;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#26448;&#26009;&#30340;&#32467;&#26500;&#20248;&#21270;&#21644;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24265;&#20215;&#30340;&#26426;&#22120;&#23398;&#20064;&#21183;&#22330;&#36234;&#26469;&#36234;&#34987;&#29992;&#20110;&#21152;&#36895;&#26448;&#26009;&#30340;&#32467;&#26500;&#20248;&#21270;&#21644;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#65292;&#36890;&#36807;&#36845;&#20195;&#24615;&#22320;&#39044;&#27979;&#21644;&#24212;&#29992;&#21407;&#23376;&#38388;&#21147;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#26816;&#27979;&#21040;&#39044;&#27979;&#30340;&#19981;&#21487;&#38752;&#24615;&#20197;&#36991;&#20813;&#38169;&#35823;&#25110;&#35823;&#23548;&#24615;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#26694;&#26550;&#26469;&#22521;&#35757;&#21644;&#37325;&#26032;&#26657;&#20934;&#22270;&#31070;&#32463;&#32593;&#32476;&#21512;&#22863;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#24102;&#26377;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#33021;&#37327;&#21644;&#21147;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#32771;&#34385;&#21040;&#20102;Epistemic&#21644;Aleatoric&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#32553;&#25918;&#20989;&#25968;&#22312;&#21518;&#26399;&#37325;&#26032;&#35843;&#25972;&#20102;&#24635;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#22312;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#19978;&#23454;&#29616;&#33391;&#22909;&#30340;&#26657;&#20934;&#24615;&#33021;&#65292;&#32780;&#19981;&#20250;&#25439;&#22833;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#21644;&#35780;&#20272;&#65292;ANI-1x&#65288;Smith&#31561;&#20154;&#65289;&#21644;Transition1x&#65288;Schreiner&#31561;&#20154;&#65289;&#65292;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#37117;&#21253;&#21547;&#36828;&#31163;&#24179;&#34913;&#29366;&#24577;&#30340;&#22810;&#26679;&#21270;&#26500;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inexpensive machine learning potentials are increasingly being used to speed up structural optimization and molecular dynamics simulations of materials by iteratively predicting and applying interatomic forces. In these settings, it is crucial to detect when predictions are unreliable to avoid wrong or misleading results. Here, we present a complete framework for training and recalibrating graph neural network ensemble models to produce accurate predictions of energy and forces with calibrated uncertainty estimates. The proposed method considers both epistemic and aleatoric uncertainty and the total uncertainties are recalibrated post hoc using a nonlinear scaling function to achieve good calibration on previously unseen data, without loss of predictive accuracy. The method is demonstrated and evaluated on two challenging, publicly available datasets, ANI-1x (Smith et al.) and Transition1x (Schreiner et al.), both containing diverse conformations far from equilibrium. A detailed analys
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#26410;&#26631;&#35760;&#30340;&#31616;&#21270;&#21644;&#37325;&#26032;&#37319;&#26679;&#25968;&#25454;&#35299;&#37322;&#20013;&#30340;&#21464;&#21270;&#26469;&#30452;&#25509;&#35782;&#21035;&#27010;&#24565;&#28418;&#31227;&#30340;&#28857;&#65292;&#20197;&#24320;&#21457;&#21487;&#38752;&#30340; JIT-SDP &#27169;&#22411;&#12290;&#27492;&#20030;&#23558;&#26377;&#21161;&#20110;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#29616;&#35937;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#23545;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.16323</link><description>&lt;p&gt;
&#20351;&#29992;&#23454;&#20363;&#35299;&#37322;&#26816;&#27979;&#36719;&#20214;&#32570;&#38519;&#21487;&#38752;&#24615;&#39044;&#27979;&#30340;&#27010;&#24565;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Detecting Concept Drift for the reliability prediction of Software Defects using Instance Interpretation. (arXiv:2305.16323v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16323
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#26410;&#26631;&#35760;&#30340;&#31616;&#21270;&#21644;&#37325;&#26032;&#37319;&#26679;&#25968;&#25454;&#35299;&#37322;&#20013;&#30340;&#21464;&#21270;&#26469;&#30452;&#25509;&#35782;&#21035;&#27010;&#24565;&#28418;&#31227;&#30340;&#28857;&#65292;&#20197;&#24320;&#21457;&#21487;&#38752;&#30340; JIT-SDP &#27169;&#22411;&#12290;&#27492;&#20030;&#23558;&#26377;&#21161;&#20110;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#29616;&#35937;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#23545;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312; JIT-SDP &#19978;&#19979;&#25991;&#20013;&#65292;&#30001;&#20110;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#30340;&#21464;&#21270;&#12289;&#36719;&#20214;&#22797;&#26434;&#24615;&#25110;&#29992;&#25143;&#34892;&#20026;&#30340;&#21464;&#21270;&#21487;&#33021;&#20250;&#24433;&#21709;&#38543;&#26102;&#38388;&#31283;&#23450;&#24615;&#30340; JIT-SDP &#27169;&#22411;&#65292;&#22240;&#27492;&#21487;&#33021;&#20250;&#20986;&#29616;&#27010;&#24565;&#28418;&#31227;&#65288;CD&#65289;&#12290;&#21478;&#22806;&#65292;&#22312; JIT-SDP &#25968;&#25454;&#20013;&#20986;&#29616;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#29616;&#35937;&#21487;&#33021;&#20250;&#23545; CD &#26816;&#27979;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#26500;&#25104;&#28508;&#22312;&#23041;&#32961;&#65292;&#22914;&#26524;&#36827;&#34892;&#37325;&#26032;&#24179;&#34913;&#65292;&#21017;&#20250;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#20010;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#25506;&#35752;&#12290;&#27492;&#22806;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#36890;&#36807;&#32771;&#34385;&#26631;&#35760;&#30340;&#35780;&#20272;&#25968;&#25454;&#26469;&#26816;&#26597; JIT-SDP &#27169;&#22411;&#38543;&#26102;&#38388;&#30340;&#31283;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#26410;&#26469;&#30340;&#25968;&#25454;&#26631;&#31614;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#21450;&#26102;&#21487;&#29992;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#26410;&#26631;&#35760;&#30340;&#31616;&#21270;&#21644;&#37325;&#26032;&#37319;&#26679;&#25968;&#25454;&#35299;&#37322;&#20013;&#30340;&#21464;&#21270;&#26469;&#30452;&#25509;&#35782;&#21035; CD &#30340;&#28857;&#65292;&#20174;&#32780;&#24320;&#21457;&#19968;&#20010;&#21487;&#38752;&#30340; JIT-SDP &#27169;&#22411;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#24615;&#33021;&#30417;&#25511;&#30340;&#22522;&#32447;&#26041;&#27861;&#26469;&#35782;&#21035; CD &#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of Just-In-Time Software Defect Prediction (JIT-SDP), Concept drift (CD) can occur due to changes in the software development process, the complexity of the software, or changes in user behavior that may affect the stability of the JIT-SDP model over time. Additionally, the challenge of class imbalance in JIT-SDP data poses a potential risk to the accuracy of CD detection methods if rebalancing is implemented. This issue has not been explored to the best of our knowledge. Furthermore, methods to check the stability of JIT-SDP models over time by considering labeled evaluation data have been proposed. However, it should be noted that future data labels may not always be available promptly. We aim to develop a reliable JIT-SDP model using CD point detection directly by identifying changes in the interpretation of unlabeled simplified and resampled data. To evaluate our approach, we first obtained baseline methods based on model performance monitoring to identify CD points 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#24615;&#24863;&#30693;&#30340;&#30456;&#24178;&#24615;&#25439;&#22833;&#65292;&#21487;&#20197;&#24110;&#21161;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#22312;&#20445;&#25345;&#39640;&#22810;&#26679;&#24615;&#21516;&#26102;&#65292;&#26356;&#22909;&#22320;&#23398;&#20064;&#35821;&#26009;&#24211;&#32423;&#21035;&#30340;&#36830;&#36143;&#24615;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.16199</link><description>&lt;p&gt;
&#38754;&#21521;&#22810;&#26679;&#24615;&#30340;&#30456;&#24178;&#25439;&#22833;&#29992;&#20110;&#25913;&#36827;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diversity-Aware Coherence Loss for Improving Neural Topic Models. (arXiv:2305.16199v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#24615;&#24863;&#30693;&#30340;&#30456;&#24178;&#24615;&#25439;&#22833;&#65292;&#21487;&#20197;&#24110;&#21161;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#22312;&#20445;&#25345;&#39640;&#22810;&#26679;&#24615;&#21516;&#26102;&#65292;&#26356;&#22909;&#22320;&#23398;&#20064;&#35821;&#26009;&#24211;&#32423;&#21035;&#30340;&#36830;&#36143;&#24615;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#30340;&#26631;&#20934;&#26041;&#27861;&#20351;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26694;&#26550;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20272;&#35745;&#21518;&#39564;&#21644;&#20808;&#39564;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#65292;&#20197;&#21450;&#37325;&#24314;&#25439;&#22833;&#12290;&#30001;&#20110;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#26159;&#36890;&#36807;&#37325;&#26032;&#21019;&#24314;&#21508;&#20010;&#36755;&#20837;&#25991;&#26723;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#23427;&#20204;&#19981;&#20250;&#26126;&#30830;&#22320;&#25429;&#33719;&#35821;&#26009;&#24211;&#32423;&#21035;&#30340;&#20027;&#39064;&#35789;&#20043;&#38388;&#30340;&#36830;&#36143;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26679;&#24615;&#24863;&#30693;&#30340;&#30456;&#24178;&#24615;&#25439;&#22833;&#65292;&#40723;&#21169;&#27169;&#22411;&#23398;&#20064;&#35821;&#26009;&#24211;&#32423;&#21035;&#30340;&#36830;&#36143;&#24615;&#20998;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#20027;&#39064;&#20043;&#38388;&#30340;&#39640;&#22810;&#26679;&#24615;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#39044;&#35757;&#32451;&#25110;&#39069;&#22806;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The standard approach for neural topic modeling uses a variational autoencoder (VAE) framework that jointly minimizes the KL divergence between the estimated posterior and prior, in addition to the reconstruction loss. Since neural topic models are trained by recreating individual input documents, they do not explicitly capture the coherence between topic words on the corpus level. In this work, we propose a novel diversity-aware coherence loss that encourages the model to learn corpus-level coherence scores while maintaining a high diversity between topics. Experimental results on multiple datasets show that our method significantly improves the performance of neural topic models without requiring any pretraining or additional parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#30697;&#38453;&#21644;&#26684;&#25289;&#22982;&#36845;&#20195;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#20272;&#35745;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;Lipschitz&#24120;&#25968;&#19978;&#30028;&#12290;&#35813;&#26041;&#27861;&#31934;&#30830;&#12289;&#24555;&#36895;&#12289;&#21487;&#24494;&#20998;&#65292;&#24182;&#23637;&#29616;&#20102;&#36229;&#32447;&#24615;&#25910;&#25947;&#12290;&#22312;&#23454;&#39564;&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#31934;&#24230;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#21033;&#26222;&#24076;&#33576;&#27491;&#21017;&#21270;&#26041;&#38754;&#20063;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16173</link><description>&lt;p&gt;
&#36890;&#36807;&#26684;&#25289;&#22982;&#36845;&#20195;&#23454;&#29616;&#21367;&#31215;&#23618;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#30340;&#39640;&#25928;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Efficient Bound of Lipschitz Constant for Convolutional Layers by Gram Iteration. (arXiv:2305.16173v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#30697;&#38453;&#21644;&#26684;&#25289;&#22982;&#36845;&#20195;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#20272;&#35745;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;Lipschitz&#24120;&#25968;&#19978;&#30028;&#12290;&#35813;&#26041;&#27861;&#31934;&#30830;&#12289;&#24555;&#36895;&#12289;&#21487;&#24494;&#20998;&#65292;&#24182;&#23637;&#29616;&#20102;&#36229;&#32447;&#24615;&#25910;&#25947;&#12290;&#22312;&#23454;&#39564;&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#31934;&#24230;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#21033;&#26222;&#24076;&#33576;&#27491;&#21017;&#21270;&#26041;&#38754;&#20063;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#30340;&#25511;&#21046;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#12289;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#26377;&#24456;&#22823;&#24433;&#21709;&#65292;&#22240;&#27492;&#20272;&#35745;&#36825;&#20010;&#20540;&#26159;&#30446;&#21069;&#30340;&#19968;&#20010;&#31185;&#23398;&#38590;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24490;&#29615;&#30697;&#38453;&#29702;&#35770;&#21644;&#19968;&#31181;&#26032;&#30340;&#21151;&#29575;&#36845;&#20195;&#26367;&#20195;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#31934;&#30830;&#12289;&#24555;&#36895;&#21644;&#21487;&#24494;&#20998;&#30340;&#19978;&#30028;&#65292;&#29992;&#20110;&#21367;&#31215;&#23618;&#30340;&#35889;&#33539;&#25968;&#12290;&#31216;&#20026;&#26684;&#25289;&#22982;&#36845;&#20195;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20102;&#19968;&#20010;&#36229;&#32447;&#24615;&#30340;&#25910;&#25947;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31934;&#24230;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#21487;&#20280;&#32553;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#23545;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21033;&#26222;&#24076;&#33576;&#27491;&#21017;&#21270;&#38750;&#24120;&#26377;&#25928;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;&#20195;&#30721;&#21487;&#22312; https://github.com/blaisedelattre/lip4conv &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the control of the Lipschitz constant has a great impact on the training stability, generalization, and robustness of neural networks, the estimation of this value is nowadays a real scientific challenge. In this paper we introduce a precise, fast, and differentiable upper bound for the spectral norm of convolutional layers using circulant matrix theory and a new alternative to the Power iteration. Called the Gram iteration, our approach exhibits a superlinear convergence. First, we show through a comprehensive set of experiments that our approach outperforms other state-of-the-art methods in terms of precision, computational cost, and scalability. Then, it proves highly effective for the Lipschitz regularization of convolutional neural networks, with competitive results against concurrent approaches. Code is available at https://github.com/blaisedelattre/lip4conv.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#31185;&#22827;&#36716;&#25442;&#27169;&#22411;&#30340;&#21487;&#36776;&#35782;&#24615;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#39640;&#26031;&#21442;&#25968;&#21270;&#36801;&#31227;&#20998;&#24067;&#23454;&#29616;&#31532;&#19968;&#38454;&#27573;&#39532;&#23572;&#31185;&#22827;&#20381;&#36182;&#32467;&#26500;&#20013;&#30340;&#21487;&#36776;&#35782;&#24615;&#26465;&#20214;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20381;&#36182;&#20110;&#25919;&#26435;&#30340;&#22240;&#26524;&#21457;&#29616;&#21644;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2305.15925</link><description>&lt;p&gt;
&#20851;&#20110;&#39532;&#23572;&#31185;&#22827;&#36716;&#25442;&#27169;&#22411;&#30340;&#21487;&#36776;&#35782;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Identifiability of Markov Switching Models. (arXiv:2305.15925v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#31185;&#22827;&#36716;&#25442;&#27169;&#22411;&#30340;&#21487;&#36776;&#35782;&#24615;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#39640;&#26031;&#21442;&#25968;&#21270;&#36801;&#31227;&#20998;&#24067;&#23454;&#29616;&#31532;&#19968;&#38454;&#27573;&#39532;&#23572;&#31185;&#22827;&#20381;&#36182;&#32467;&#26500;&#20013;&#30340;&#21487;&#36776;&#35782;&#24615;&#26465;&#20214;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20381;&#36182;&#20110;&#25919;&#26435;&#30340;&#22240;&#26524;&#21457;&#29616;&#21644;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#21487;&#36776;&#35782;&#24615;&#22240;&#20854;&#22312;&#21487;&#35299;&#37322;&#24615;&#25110;&#20998;&#24067;&#27867;&#21270;&#26041;&#38754;&#30340;&#24212;&#29992;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20316;&#20026;&#23558;&#26368;&#36817;&#30340;&#32467;&#26524;&#25193;&#23637;&#21040;&#24207;&#21015;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#31532;&#19968;&#27493;&#30340;&#39532;&#23572;&#31185;&#22827;&#36716;&#25442;&#27169;&#22411;&#30340;&#21487;&#36776;&#35782;&#24615;&#12290;&#25105;&#20204;&#22312;&#31532;&#19968;&#38454;&#27573;&#39532;&#23572;&#31185;&#22827;&#20381;&#36182;&#32467;&#26500;&#20013;&#25552;&#20986;&#20102;&#21487;&#36776;&#35782;&#24615;&#26465;&#20214;&#65292;&#24182;&#36890;&#36807;&#38750;&#32447;&#24615;&#39640;&#26031;&#21442;&#25968;&#21270;&#36801;&#31227;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#20381;&#36182;&#20110;&#25919;&#26435;&#30340;&#22240;&#26524;&#21457;&#29616;&#21644;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifiability of latent variable models has recently gained interest in terms of its applications to interpretability or out of distribution generalisation. In this work, we study identifiability of Markov Switching Models as a first step towards extending recent results to sequential latent variable models. We present identifiability conditions within first-order Markov dependency structures, and parametrise the transition distribution via non-linear Gaussians. Our experiments showcase the applicability of our approach for regime-dependent causal discovery and high-dimensional time series segmentation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;Grug&#65292;&#26088;&#22312;&#32479;&#19968;HGNN&#20013;&#30340;&#22270;&#24418;&#25299;&#25169;&#21644;&#33410;&#28857;&#29305;&#24449;&#30340;&#27491;&#21017;&#21270;&#65292;&#24182;&#35299;&#20915;&#20102;&#36807;&#24230;&#24179;&#28369;&#12289;&#38750;&#40065;&#26834;&#24615;&#31561;&#38382;&#39064;&#65292;&#32508;&#21512;&#25928;&#26524;&#21644;&#25928;&#29575;&#20248;&#20110;&#20960;&#31181;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15811</link><description>&lt;p&gt;
&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#26799;&#24230;&#27491;&#21017;&#21270;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unifying gradient regularization for Heterogeneous Graph Neural Networks. (arXiv:2305.15811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;Grug&#65292;&#26088;&#22312;&#32479;&#19968;HGNN&#20013;&#30340;&#22270;&#24418;&#25299;&#25169;&#21644;&#33410;&#28857;&#29305;&#24449;&#30340;&#27491;&#21017;&#21270;&#65292;&#24182;&#35299;&#20915;&#20102;&#36807;&#24230;&#24179;&#28369;&#12289;&#38750;&#40065;&#26834;&#24615;&#31561;&#38382;&#39064;&#65292;&#32508;&#21512;&#25928;&#26524;&#21644;&#25928;&#29575;&#20248;&#20110;&#20960;&#31181;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#24322;&#26500;&#22270;&#30340;&#34920;&#24449;&#12290;&#23613;&#31649;HGNN&#36805;&#36895;&#21457;&#23637;&#65292;&#20294;&#20173;&#38754;&#20020;&#36807;&#24230;&#24179;&#28369;&#21644;&#38750;&#40065;&#26834;&#24615;&#31561;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#26799;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#29616;&#26377;&#30340;&#26799;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;&#19987;&#27880;&#20110;&#22270;&#24418;&#25299;&#25169;&#25110;&#33410;&#28857;&#29305;&#24449;&#65292;&#32570;&#20047;&#32479;&#19968;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;Grug&#65292;&#26088;&#22312;&#32479;&#19968;HGNN&#20013;&#30340;&#22270;&#24418;&#25299;&#25169;&#21644;&#33410;&#28857;&#29305;&#24449;&#30340;&#27491;&#21017;&#21270;&#65292;&#24182;&#35299;&#20915;&#20102;&#36807;&#24230;&#24179;&#28369;&#12289;&#38750;&#40065;&#26834;&#24615;&#31561;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Grug&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20960;&#31181;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Graph Neural Networks (HGNNs) are a class of powerful deep learning methods widely used to learn representations of heterogeneous graphs. Despite the fast development of HGNNs, they still face some challenges such as over-smoothing, and non-robustness. Previous studies have shown that these problems can be reduced by using gradient regularization methods. However, the existing gradient regularization methods focus on either graph topology or node features. There is no universal approach to integrate these features, which severely affects the efficiency of regularization. In addition, the inclusion of gradient regularization into HGNNs sometimes leads to some problems, such as an unstable training process, increased complexity and insufficient coverage regularized information. Furthermore, there is still short of a complete theoretical analysis of the effects of gradient regularization on HGNNs. In this paper, we propose a novel gradient regularization method called Grug, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#38598;&#21512;&#31574;&#30053;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#25110;&#26377;&#38480;&#32500;&#21472;&#21152;&#27867;&#21270;&#27169;&#22411;&#20013;&#36873;&#25321;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#24615;&#33021;&#30340;&#26368;&#20248;&#21472;&#21152;&#27867;&#21270;&#19982;&#26368;&#20248;&#35299;&#24615;&#33021;&#30456;&#36817;&#12290;</title><link>http://arxiv.org/abs/2305.15786</link><description>&lt;p&gt;
&#23398;&#20064;&#38598;&#21512;&#31574;&#30053;&#30340;&#29702;&#35770;&#20445;&#35777;&#21450;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Theoretical Guarantees of Learning Ensembling Strategies with Applications to Time Series Forecasting. (arXiv:2305.15786v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#38598;&#21512;&#31574;&#30053;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#25110;&#26377;&#38480;&#32500;&#21472;&#21152;&#27867;&#21270;&#27169;&#22411;&#20013;&#36873;&#25321;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#24615;&#33021;&#30340;&#26368;&#20248;&#21472;&#21152;&#27867;&#21270;&#19982;&#26368;&#20248;&#35299;&#24615;&#33021;&#30456;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#21512;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24120;&#29992;&#30340;&#24037;&#20855;&#20043;&#19968;&#65292;&#30001;&#20110;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#20943;&#23569;&#26041;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#38024;&#23545;&#40657;&#30418;&#22522;&#23398;&#20064;&#22120;&#30340;&#22823;&#22810;&#25968;&#38598;&#21512;&#26041;&#27861;&#37117;&#23646;&#20110;&#8220;&#21472;&#21152;&#27867;&#21270;&#8221;&#33539;&#30068;&#65292;&#21363;&#35757;&#32451;&#19968;&#20010;&#25509;&#21463;&#22522;&#23398;&#20064;&#22120;&#25512;&#29702;&#20316;&#20026;&#36755;&#20837;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#34429;&#28982;&#21472;&#21152;&#27867;&#21270;&#22312;&#23454;&#36341;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#20854;&#29702;&#35770;&#24615;&#36136;&#20173;&#28982;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#19968;&#20010;&#26032;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#36873;&#25321;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#24615;&#33021;&#30340;&#8220;&#26377;&#38480;&#25110;&#26377;&#38480;&#32500;&#8221;&#21472;&#21152;&#27867;&#21270;&#20013;&#30340;&#26368;&#20339;&#21472;&#21152;&#27867;&#21270;&#24182;&#19981;&#27604;&#26368;&#20248;&#35299;&#34920;&#29616;&#8220;&#24046;&#24471;&#22810;&#8221;&#12290;&#36825;&#19968;&#32467;&#26524;&#21152;&#24378;&#21644;&#22823;&#22823;&#25193;&#23637;&#20102;Van der Laan&#31561;&#20154;&#65288;2007&#24180;&#65289;&#30340;&#32467;&#26524;&#12290;&#21463;&#21040;&#29702;&#35770;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#27010;&#29575;&#39044;&#27979;&#30340;&#32972;&#26223;&#19979;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#19981;&#21516;&#25935;&#24863;&#24615;&#30340;&#21472;&#21152;&#27867;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensembling is among the most popular tools in machine learning (ML) due to its effectiveness in minimizing variance and thus improving generalization. Most ensembling methods for black-box base learners fall under the umbrella of "stacked generalization," namely training an ML algorithm that takes the inferences from the base learners as input. While stacking has been widely applied in practice, its theoretical properties are poorly understood. In this paper, we prove a novel result, showing that choosing the best stacked generalization from a (finite or finite-dimensional) family of stacked generalizations based on cross-validated performance does not perform "much worse" than the oracle best. Our result strengthens and significantly extends the results in Van der Laan et al. (2007). Inspired by the theoretical analysis, we further propose a particular family of stacked generalizations in the context of probabilistic forecasting, each one with a different sensitivity for how much the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#19981;&#21516;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#21360;&#22320;&#35821;-&#33521;&#35821;&#20195;&#30721;&#28151;&#21512;&#25968;&#25454;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20197;&#25552;&#39640;&#24773;&#24863;&#20998;&#26512;&#12289;&#24773;&#32490;&#35782;&#21035;&#21644;&#20167;&#24680;&#35328;&#35770;&#35782;&#21035;&#31561;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15722</link><description>&lt;p&gt;
&#38754;&#21521;&#20195;&#30721;&#28151;&#21512;&#30340;&#21360;&#22320;&#35821;-&#33521;&#35821;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparative Study of Pre-Trained BERT Models for Code-Mixed Hindi-English Data. (arXiv:2305.15722v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#19981;&#21516;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#21360;&#22320;&#35821;-&#33521;&#35821;&#20195;&#30721;&#28151;&#21512;&#25968;&#25454;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20197;&#25552;&#39640;&#24773;&#24863;&#20998;&#26512;&#12289;&#24773;&#32490;&#35782;&#21035;&#21644;&#20167;&#24680;&#35328;&#35770;&#35782;&#21035;&#31561;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#20195;&#30721;&#28151;&#21512;&#8221;&#26159;&#25351;&#22312;&#21516;&#19968;&#27573;&#25991;&#26412;&#20013;&#20351;&#29992;&#22810;&#31181;&#35821;&#35328;&#30340;&#29616;&#35937;&#12290;&#36825;&#31181;&#29616;&#35937;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#24191;&#27867;&#23384;&#22312;&#65292;&#24182;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#37319;&#32435;&#12290;&#26816;&#27979;&#35821;&#35328;&#20013;&#30340;&#22806;&#26469;&#20803;&#32032;&#24182;&#27491;&#30830;&#22788;&#29702;&#23427;&#20204;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#35768;&#22810;&#20154;&#20351;&#29992;&#20195;&#30721;&#28151;&#21512;&#35821;&#35328;&#65292;&#20854;&#20013;&#20219;&#19968;&#35821;&#35328;&#37117;&#26080;&#27861;&#29702;&#35299;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20302;&#36164;&#28304;&#30340;&#21360;&#22320;&#35821;-&#33521;&#35821;&#20195;&#30721;&#28151;&#21512;&#35821;&#35328;&#65292;&#24182;&#25552;&#39640;&#19981;&#21516;&#20195;&#30721;&#28151;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65288;&#22914;&#24773;&#24863;&#20998;&#26512;&#12289;&#24773;&#32490;&#35782;&#21035;&#21644;&#20167;&#24680;&#35328;&#35770;&#35782;&#21035;&#65289;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#20351;&#29992;&#26080;&#30417;&#30563;&#26041;&#27861;&#39044;&#35757;&#32451;&#30340;&#19981;&#21516;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#25105;&#20204;&#21253;&#25324;&#20102;&#20195;&#30721;&#28151;&#21512;&#27169;&#22411;&#65288;&#22914;HingBERT&#12289;HingRoBERTa&#12289;HingRoBERTa-Mixed&#12289;mBERT&#65289;&#21644;&#38750;&#20195;&#30721;&#28151;&#21512;&#27169;&#22411;&#65288;&#22914;AlBERT&#12289;BERT&#12289;RoBERTa&#65289;&#65292;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#21360;&#22320;&#35821;-&#33521;&#35821;&#20195;&#30721;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The term "Code Mixed" refers to the use of more than one language in the same text. This phenomenon is predominantly observed on social media platforms, with an increasing amount of adaptation as time goes on. It is critical to detect foreign elements in a language and process them correctly, as a considerable number of individuals are using code-mixed languages that could not be comprehended by understanding one of those languages. In this work, we focus on low-resource Hindi-English code-mixed language and enhancing the performance of different code-mixed natural language processing tasks such as sentiment analysis, emotion recognition, and hate speech identification. We perform a comparative analysis of different Transformer-based language Models pre-trained using unsupervised approaches. We have included the code-mixed models like HingBERT, HingRoBERTa, HingRoBERTa-Mixed, mBERT, and non-code-mixed models like AlBERT, BERT, and RoBERTa for comparative analysis of code-mixed Hindi-En
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#25913;&#21892;&#25628;&#32034;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20195;&#34920;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22810;&#26679;&#21270;&#26041;&#27861;&#65292;&#24182;&#22312;Pinterest&#24179;&#21488;&#19978;&#23454;&#39564;&#21644;&#37096;&#32626;&#20102;&#21487;&#25193;&#23637;&#30340;&#22810;&#26679;&#21270;&#26426;&#21046;&#65292;&#20197;&#25913;&#21892;&#32654;&#23481;&#21644;&#26102;&#23578;&#31867;&#21035;&#20013;&#19981;&#21516;&#32932;&#33394;&#30340;&#20195;&#34920;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15534</link><description>&lt;p&gt;
&#22312;&#25628;&#32034;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#22312;&#32447;&#34920;&#31034;&#24456;&#37325;&#35201;&#65306;&#23454;&#29992;&#30340;&#31471;&#21040;&#31471;&#22810;&#26679;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation Online Matters: Practical End-to-End Diversification in Search and Recommender Systems. (arXiv:2305.15534v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15534
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25913;&#21892;&#25628;&#32034;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20195;&#34920;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22810;&#26679;&#21270;&#26041;&#27861;&#65292;&#24182;&#22312;Pinterest&#24179;&#21488;&#19978;&#23454;&#39564;&#21644;&#37096;&#32626;&#20102;&#21487;&#25193;&#23637;&#30340;&#22810;&#26679;&#21270;&#26426;&#21046;&#65292;&#20197;&#25913;&#21892;&#32654;&#23481;&#21644;&#26102;&#23578;&#31867;&#21035;&#20013;&#19981;&#21516;&#32932;&#33394;&#30340;&#20195;&#34920;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#32447;&#24179;&#21488;&#22312;&#21508;&#20010;&#20154;&#21475;&#32479;&#35745;&#23398;&#20013;&#30340;&#20351;&#29992;&#19981;&#26029;&#22686;&#38271;&#65292;&#29992;&#25143;&#32463;&#24120;&#34920;&#36798;&#24076;&#26395;&#22312;&#20869;&#23481;&#20013;&#24863;&#21463;&#21040;&#33258;&#24049;&#30340;&#20195;&#34920;&#24615;&#12290;&#20026;&#20102;&#25913;&#21892;&#25628;&#32034;&#32467;&#26524;&#21644;&#25512;&#33616;&#20013;&#30340;&#20195;&#34920;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31471;&#21040;&#31471;&#30340;&#22810;&#26679;&#21270;&#26041;&#27861;&#65292;&#30830;&#20445;&#22810;&#26679;&#21270;&#20869;&#23481;&#22312;&#36825;&#20123;&#31995;&#32479;&#30340;&#21508;&#20010;&#38454;&#27573;&#20013;&#27969;&#21160;&#65292;&#20174;&#26816;&#32034;&#21040;&#25490;&#24207;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;Pinterest&#24179;&#21488;&#30340;&#29983;&#20135;&#30028;&#38754;&#20013;&#24320;&#21457;&#12289;&#23454;&#39564;&#21644;&#37096;&#32626;&#21487;&#25193;&#23637;&#30340;&#22810;&#26679;&#21270;&#26426;&#21046;&#65292;&#21253;&#25324;&#25628;&#32034;&#12289;&#30456;&#20851;&#20135;&#21697;&#21644;&#26032;&#29992;&#25143;&#20027;&#39029;&#65292;&#20197;&#25913;&#21892;&#32654;&#23481;&#21644;&#26102;&#23578;&#20869;&#23481;&#20013;&#19981;&#21516;&#32932;&#33394;&#30340;&#20195;&#34920;&#24615;&#12290;&#29983;&#20135;&#31995;&#32479;&#20013;&#30340;&#22810;&#26679;&#21270;&#21253;&#25324;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#30830;&#23450;&#20250;&#35302;&#21457;&#22810;&#26679;&#21270;&#30340;&#35831;&#27714;&#65292;&#22312;&#26816;&#32034;&#38454;&#27573;&#30830;&#20445;&#20174;&#22823;&#22411;&#20869;&#23481;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#21040;&#22810;&#26679;&#21270;&#30340;&#20869;&#23481;&#65292;&#26368;&#21518;&#65292;&#22312;&#25490;&#21517;&#38454;&#27573;&#20197;&#33258;&#25105;&#35843;&#25972;&#30340;&#26041;&#24335;&#24179;&#34913;&#22810;&#26679;&#24615;&#21644;&#25928;&#29992;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#20351;&#29992;Strong-O&#24320;&#22987;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the use of online platforms continues to grow across all demographics, users often express a desire to feel represented in the content. To improve representation in search results and recommendations, we introduce end-to-end diversification, ensuring that diverse content flows throughout the various stages of these systems, from retrieval to ranking. We develop, experiment, and deploy scalable diversification mechanisms in multiple production surfaces on the Pinterest platform, including Search, Related Products, and New User Homefeed, to improve the representation of different skin tones in beauty and fashion content. Diversification in production systems includes three components: identifying requests that will trigger diversification, ensuring diverse content is retrieved from the large content corpus during the retrieval stage, and finally, balancing the diversity-utility trade-off in a self-adjusting manner in the ranking stage. Our approaches, which evolved from using Strong-O
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23569;&#37327;&#25968;&#25454;&#36827;&#34892;&#26032;&#22411;&#34507;&#30333;&#36136;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21322;&#30417;&#30563;&#36716;&#31227;&#23398;&#20064;&#21644;&#36827;&#21270;&#39532;&#23572;&#21487;&#22827;&#33945;&#29305;&#21345;&#32599;&#38142;&#37319;&#26679;&#31639;&#27861;&#65292;&#26356;&#26377;&#25928;&#22320;&#25506;&#32034;&#36866;&#24212;&#24230;&#26223;&#35266;&#65292;&#20174;&#32780;&#21152;&#36895;&#26114;&#36149;&#30340;&#28287;&#23454;&#39564;&#27979;&#35797;&#21608;&#26399;&#12290;</title><link>http://arxiv.org/abs/2305.15441</link><description>&lt;p&gt;
&#29992;&#36827;&#21270;&#37319;&#26679;&#25913;&#21892;&#22522;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#34507;&#30333;&#36136;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Improving few-shot learning-based protein engineering with evolutionary sampling. (arXiv:2305.15441v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23569;&#37327;&#25968;&#25454;&#36827;&#34892;&#26032;&#22411;&#34507;&#30333;&#36136;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21322;&#30417;&#30563;&#36716;&#31227;&#23398;&#20064;&#21644;&#36827;&#21270;&#39532;&#23572;&#21487;&#22827;&#33945;&#29305;&#21345;&#32599;&#38142;&#37319;&#26679;&#31639;&#27861;&#65292;&#26356;&#26377;&#25928;&#22320;&#25506;&#32034;&#36866;&#24212;&#24230;&#26223;&#35266;&#65292;&#20174;&#32780;&#21152;&#36895;&#26114;&#36149;&#30340;&#28287;&#23454;&#39564;&#27979;&#35797;&#21608;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#26032;&#22411;&#21151;&#33021;&#34507;&#30333;&#36136;&#20173;&#28982;&#26159;&#19968;&#20010;&#32531;&#24930;&#19988;&#26114;&#36149;&#30340;&#36807;&#31243;&#65292;&#36825;&#26159;&#30001;&#20110;&#21508;&#31181;&#34507;&#30333;&#36136;&#24037;&#31243;&#25361;&#25112;; &#29305;&#21035;&#26159;&#65292;&#22312;&#32473;&#23450;&#30340;&#23454;&#39564;&#20013;&#21487;&#20197;&#27979;&#35797;&#30340;&#34507;&#30333;&#21464;&#20307;&#25968;&#37327;&#36828;&#36828;&#19981;&#21450;&#25972;&#20010;&#24207;&#21015;&#31354;&#38388;&#30340;&#24191;&#38420;&#65292;&#23548;&#33268;&#20302;&#21629;&#20013;&#29575;&#21644;&#26114;&#36149;&#30340;&#28287;&#23454;&#39564;&#27979;&#35797;&#21608;&#26399;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#26469;&#35774;&#35745;&#26032;&#22411;&#34507;&#30333;&#36136;&#65292;&#26088;&#22312;&#21152;&#36895;&#26114;&#36149;&#30340;&#28287;&#23454;&#39564;&#27979;&#35797;&#21608;&#26399;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#19968;&#20010;&#23567;&#19988;&#20559;&#26012;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65288;&#32422;$10^5$&#25968;&#25454;&#28857;&#65292;$&lt;1\%$&#31215;&#26497;&#32467;&#26524;&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30001;&#20004;&#37096;&#20998;&#32452;&#25104;&#65306;&#19968;&#31181;&#21322;&#30417;&#30563;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#25152;&#38656;&#34507;&#30333;&#36136;&#21151;&#33021;&#30340;&#31163;&#25955;&#36866;&#24212;&#24230;&#26223;&#35266;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#36827;&#21270;&#39532;&#23572;&#21487;&#22827;&#33945;&#29305;&#21345;&#32599;&#38142;&#37319;&#26679;&#31639;&#27861;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#25506;&#32034;&#36866;&#24212;&#24230;&#26223;&#35266;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#39044;&#27979;&#39640;&#36866;&#24212;&#24230;&#22522;&#22240;&#24182;&#36827;&#34892;&#31579;&#36873;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing novel functional proteins remains a slow and expensive process due to a variety of protein engineering challenges; in particular, the number of protein variants that can be experimentally tested in a given assay pales in comparison to the vastness of the overall sequence space, resulting in low hit rates and expensive wet lab testing cycles. In this paper, we propose a few-shot learning approach to novel protein design that aims to accelerate the expensive wet lab testing cycle and is capable of leveraging a training dataset that is both small and skewed ($\approx 10^5$ datapoints, $&lt; 1\%$ positive hits). Our approach is composed of two parts: a semi-supervised transfer learning approach to generate a discrete fitness landscape for a desired protein function and a novel evolutionary Monte Carlo Markov Chain sampling algorithm to more efficiently explore the fitness landscape. We demonstrate the performance of our approach by experimentally screening predicted high fitness gen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26412;&#22320; SGD &#26356;&#26032;&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312; IID &#25968;&#25454;&#26041;&#26696;&#20013;&#65292;L-SGD &#33021;&#22815;&#25506;&#32034;&#24182;&#21033;&#29992;&#25439;&#22833;&#20989;&#25968;&#30340;&#20108;&#38454;&#20449;&#24687;&#20197;&#23454;&#29616;&#26356;&#24555;&#36895;&#30340;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2305.15013</link><description>&lt;p&gt;
&#21033;&#29992;&#25439;&#22833;&#20989;&#25968;&#30340;&#20108;&#38454;&#20449;&#24687;&#65292;&#26412;&#22320; SGD &#21152;&#36895;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local SGD Accelerates Convergence by Exploiting Second Order Information of the Loss Function. (arXiv:2305.15013v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26412;&#22320; SGD &#26356;&#26032;&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312; IID &#25968;&#25454;&#26041;&#26696;&#20013;&#65292;L-SGD &#33021;&#22815;&#25506;&#32034;&#24182;&#21033;&#29992;&#25439;&#22833;&#20989;&#25968;&#30340;&#20108;&#38454;&#20449;&#24687;&#20197;&#23454;&#29616;&#26356;&#24555;&#36895;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22810;&#27425;&#26412;&#22320;&#32479;&#35745;&#26799;&#24230;&#19979;&#38477;&#65288;L-SGD&#65289;&#26356;&#26032;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;L-SGD&#33021;&#22815;&#25506;&#32034;&#25439;&#22833;&#20989;&#25968;&#30340;&#20108;&#38454;&#20449;&#24687;&#65292;&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;IID&#65289;&#25968;&#25454;&#26041;&#26696;&#20013;&#36229;&#36234;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#30456;&#27604;SGD&#65292;L-SGD&#30340;&#26356;&#26032;&#27839;&#30528;&#20855;&#26377;&#23567;&#29305;&#24449;&#20540;&#30340;&#40657;&#22622;&#30697;&#38453;&#30340;&#29305;&#24449;&#21521;&#37327;&#26377;&#26356;&#22823;&#30340;&#25237;&#24433;&#65292;&#20174;&#32780;&#33021;&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
With multiple iterations of updates, local statistical gradient descent (L-SGD) has been proven to be very effective in distributed machine learning schemes such as federated learning. In fact, many innovative works have shown that L-SGD with independent and identically distributed (IID) data can even outperform SGD. As a result, extensive efforts have been made to unveil the power of L-SGD. However, existing analysis failed to explain why the multiple local updates with small mini-batches of data (L-SGD) can not be replaced by the update with one big batch of data and a larger learning rate (SGD). In this paper, we offer a new perspective to understand the strength of L-SGD. We theoretically prove that, with IID data, L-SGD can effectively explore the second order information of the loss function. In particular, compared with SGD, the updates of L-SGD have much larger projection on the eigenvectors of the Hessian matrix with small eigenvalues, which leads to faster convergence. Under 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25512;&#23548;&#21644;&#35780;&#20272;&#20102;&#22235;&#31181;&#36125;&#21494;&#26031;&#25209;&#27425;&#36172;&#21338;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#30830;&#23450;&#27969;&#37327;&#20998;&#37197;&#65292;&#20840;&#38754;&#35780;&#20272;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#21487;&#20449;&#24230;&#12289;&#25935;&#24863;&#24615;&#21644;&#21518;&#24724;&#24230;&#65292;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#22522;&#20110;&#25209;&#27425;&#30340;&#36125;&#21494;&#26031;&#31639;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.14704</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#25277;&#26679;&#31639;&#27861;&#30340;&#22312;&#32447;&#33258;&#36866;&#24212;&#27969;&#37327;&#23454;&#39564;&#30340;&#23454;&#29992;&#25209;&#27425;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Evaluation on Practical Batch Bayesian Sampling Algorithms for Online Adaptive Traffic Experimentation. (arXiv:2305.14704v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25512;&#23548;&#21644;&#35780;&#20272;&#20102;&#22235;&#31181;&#36125;&#21494;&#26031;&#25209;&#27425;&#36172;&#21338;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#30830;&#23450;&#27969;&#37327;&#20998;&#37197;&#65292;&#20840;&#38754;&#35780;&#20272;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#21487;&#20449;&#24230;&#12289;&#25935;&#24863;&#24615;&#21644;&#21518;&#24724;&#24230;&#65292;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#22522;&#20110;&#25209;&#27425;&#30340;&#36125;&#21494;&#26031;&#31639;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#21152;&#36895;&#22312;&#32447;&#27979;&#35797;&#65292;&#22810;&#33218;&#36172;&#21338;&#31639;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#25910;&#38598;&#25968;&#25454;&#32780;&#34987;&#20316;&#20026;&#22266;&#23450;&#26102;&#38388;A/B&#27979;&#35797;&#30340;&#37325;&#35201;&#34917;&#20805;&#26041;&#24335;&#19981;&#26029;&#25552;&#39640;&#12290;&#26412;&#25991;&#22522;&#20110;&#26368;&#36817;&#20851;&#20110;&#33258;&#36866;&#24212;&#25910;&#38598;&#25968;&#25454;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#21644;&#32479;&#35745;&#25512;&#26029;&#30340;&#30740;&#31350;&#65292;&#25512;&#23548;&#21644;&#35780;&#20272;&#20102;&#22235;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#25209;&#27425;&#36172;&#21338;&#31639;&#27861;&#65288;NB-TS&#65292;WB-TS&#65292;NB-TTTS&#65292;WB-TTTS&#65289;&#65292;&#23427;&#20204;&#26159;&#20004;&#31181;&#21152;&#26435;&#25209;&#27425;&#65288;Naive Batch&#21644;Weighted Batch&#65289;&#21644;&#20004;&#31181;&#36125;&#21494;&#26031;&#25277;&#26679;&#31574;&#30053;&#65288;Thompson Sampling&#21644;Top-Two Thompson Sampling&#65289;&#30340;&#32452;&#21512;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#30830;&#23450;&#27969;&#37327;&#20998;&#37197;&#12290;&#26412;&#25991;&#25552;&#20379;&#30340;&#36825;&#20123;&#22522;&#20110;&#25209;&#27425;&#32479;&#35745;&#22870;&#21169;&#24230;&#37327;&#30340;&#36125;&#21494;&#26031;&#25277;&#26679;&#31639;&#27861;&#22312;&#23454;&#36341;&#20013;&#24471;&#20197;&#24212;&#29992;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#30340;&#20854;&#20013;&#19968;&#20010;&#32452;&#21512;WB-TTTS&#20284;&#20046;&#26159;&#26368;&#26032;&#35752;&#35770;&#30340;&#12290;&#23545;&#36825;&#22235;&#31181;&#22522;&#20110;&#25209;&#27425;&#30340;&#36125;&#21494;&#26031;&#25277;&#26679;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21253;&#25324;&#27979;&#35797;&#26041;&#27861;&#30340;&#21487;&#20449;&#24230;&#12289;&#25935;&#24863;&#24615;&#21644;&#21518;&#24724;&#24230;&#12290;&#27492;&#22806;&#65292;&#35780;&#20272;&#36824;&#32771;&#34385;&#20102;&#25209;&#27425;&#20869;&#22870;&#21169;&#24230;&#37327;&#30340;&#26041;&#24046;&#20197;&#21450;&#25209;&#27425;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#22312;&#20197;&#21069;&#30340;&#30740;&#31350;&#20013;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#35299;&#20915;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#36172;&#21338;&#31639;&#27861;&#65288;&#20363;&#22914;UCB1&#65292;TS&#21644;Exp3&#65289;&#30456;&#27604;&#65292;&#36825;&#20123;&#22522;&#20110;&#25209;&#27425;&#30340;&#36125;&#21494;&#26031;&#31639;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
To speed up online testing, adaptive traffic experimentation through multi-armed bandit algorithms is rising as an essential complementary alternative to the fixed horizon A/B testing. Based on recent research on best arm identification and statistical inference with adaptively collected data, this paper derives and evaluates four Bayesian batch bandit algorithms (NB-TS, WB-TS, NB-TTTS, WB-TTTS), which are combinations of two ways of weighting batches (Naive Batch and Weighted Batch) and two Bayesian sampling strategies (Thompson Sampling and Top-Two Thompson Sampling) to adaptively determine traffic allocation. These derived Bayesian sampling algorithms are practically based on summary batch statistics of a reward metric for pilot experiments, where one of the combination WB-TTTS in this paper seems to be newly discussed. The comprehensive evaluation on the four Bayesian sampling algorithms covers trustworthiness, sensitivity and regret of a testing methodology. Moreover, the evaluati
&lt;/p&gt;</description></item><item><title>&#24207;&#21015;&#24314;&#27169;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#27604;Q-Learning&#21644;Imitation Learning&#26356;&#36866;&#21512;&#22312;&#31232;&#30095;&#22870;&#21169;&#21644;&#20302;&#36136;&#37327;&#25968;&#25454;&#35774;&#32622;&#19979;&#30340;&#36873;&#25321;&#65292;&#22312;&#20219;&#21153;&#33539;&#22260;&#22686;&#21152;&#26102;&#65292;&#24207;&#21015;&#24314;&#27169;&#21644;&#27169;&#20223;&#23398;&#20064;&#26356;&#21487;&#21462;&#12290;</title><link>http://arxiv.org/abs/2305.14550</link><description>&lt;p&gt;
&#24207;&#21015;&#24314;&#27169;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#31454;&#20105;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence Modeling is a Robust Contender for Offline Reinforcement Learning. (arXiv:2305.14550v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14550
&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#24314;&#27169;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#27604;Q-Learning&#21644;Imitation Learning&#26356;&#36866;&#21512;&#22312;&#31232;&#30095;&#22870;&#21169;&#21644;&#20302;&#36136;&#37327;&#25968;&#25454;&#35774;&#32622;&#19979;&#30340;&#36873;&#25321;&#65292;&#22312;&#20219;&#21153;&#33539;&#22260;&#22686;&#21152;&#26102;&#65292;&#24207;&#21015;&#24314;&#27169;&#21644;&#27169;&#20223;&#23398;&#20064;&#26356;&#21487;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20351;&#20195;&#29702;&#33021;&#22815;&#20174;&#38745;&#24577;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#26368;&#22823;&#21270;&#25910;&#30410;&#31574;&#30053;&#12290;&#31163;&#32447;RL&#30340;&#19977;&#22823;&#33539;&#24335;&#26159;Q-Learning&#12289;Imitation Learning&#21644;Sequence Modeling&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#38382;&#39064;&#26159;&#65306;&#22312;&#20160;&#20040;&#26465;&#20214;&#19979;&#65292;&#21738;&#31181;&#33539;&#24335;&#34987;&#20248;&#20808;&#36873;&#25321;&#65311;&#25105;&#20204;&#36890;&#36807;&#25506;&#32034;&#20195;&#34920;&#24615;&#31639;&#27861;&#8212;&#8212;&#20445;&#23432;Q-Learning(CQL)&#12289;&#34892;&#20026;&#20811;&#38534; (BC)&#21644;&#20915;&#31574;Transformer (DT)&#8212;&#8212;&#22312;&#24120;&#29992;&#30340;D4RL&#21644;Robomimic&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#26469;&#23545;&#36825;&#20010;&#38382;&#39064;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#26377;&#38024;&#23545;&#24615;&#30340;&#23454;&#39564;&#26469;&#29702;&#35299;&#23427;&#20204;&#22312;&#25968;&#25454;&#23376;&#20248;&#24615;&#21644;&#20219;&#21153;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65306;(1)&#24207;&#21015;&#24314;&#27169;&#38656;&#35201;&#27604;Q-Learning&#26356;&#22810;&#30340;&#25968;&#25454;&#26469;&#23398;&#20064;&#31454;&#20105;&#24615;&#31574;&#30053;&#65292;&#20294;&#26356;&#21152;&#31283;&#20581;&#65307;(2)&#24207;&#21015;&#24314;&#27169;&#22312;&#31232;&#30095;&#22870;&#21169;&#21644;&#20302;&#36136;&#37327;&#25968;&#25454;&#35774;&#32622;&#20013;&#27604;Q-Learning&#21644;Imitation Learning&#37117;&#35201;&#22909;&#24471;&#22810;&#65307;(3)&#38543;&#30528;&#20219;&#21153;&#33539;&#22260;&#30340;&#22686;&#21152;&#65292;&#24207;&#21015;&#24314;&#27169;&#21644;&#27169;&#20223;&#23398;&#20064;&#26356;&#21487;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) allows agents to learn effective, return-maximizing policies from a static dataset. Three major paradigms for offline RL are Q-Learning, Imitation Learning, and Sequence Modeling. A key open question is: which paradigm is preferred under what conditions? We study this question empirically by exploring the performance of representative algorithms -- Conservative Q-Learning (CQL), Behavior Cloning (BC), and Decision Transformer (DT) -- across the commonly used D4RL and Robomimic benchmarks. We design targeted experiments to understand their behavior concerning data suboptimality and task complexity. Our key findings are: (1) Sequence Modeling requires more data than Q-Learning to learn competitive policies but is more robust; (2) Sequence Modeling is a substantially better choice than both Q-Learning and Imitation Learning in sparse-reward and low-quality data settings; and (3) Sequence Modeling and Imitation Learning are preferable as task horizon inc
&lt;/p&gt;</description></item><item><title>Chakra&#26159;&#19968;&#31181;&#24320;&#25918;&#30340;&#22270;&#24418;&#27169;&#24335;&#65292;&#29992;&#20110;&#26631;&#20934;&#21270;&#24037;&#20316;&#36127;&#36733;&#35268;&#33539;&#65292;&#25429;&#25417;&#20851;&#38190;&#25805;&#20316;&#21644;&#20381;&#36182;&#39033;&#65292;&#20197;&#25512;&#36827;&#24615;&#33021;&#22522;&#20934;&#21644;&#21327;&#21516;&#35774;&#35745;&#65292;&#21516;&#26102;&#25552;&#20379;&#19968;&#32452;&#24037;&#20855;&#21644;&#33021;&#21147;&#65292;&#20197;&#20415;&#22312;&#27169;&#25311;&#22120;&#21644;&#20223;&#30495;&#22120;&#20013;&#23454;&#29616;&#26410;&#26469;&#31995;&#32479;&#30340;&#21327;&#21516;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.14516</link><description>&lt;p&gt;
Chakra: &#21033;&#29992;&#26631;&#20934;&#21270;&#25191;&#34892;&#36319;&#36394;&#25512;&#36827;&#24615;&#33021;&#22522;&#20934;&#21644;&#21327;&#21516;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Chakra: Advancing Performance Benchmarking and Co-design using Standardized Execution Traces. (arXiv:2305.14516v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14516
&lt;/p&gt;
&lt;p&gt;
Chakra&#26159;&#19968;&#31181;&#24320;&#25918;&#30340;&#22270;&#24418;&#27169;&#24335;&#65292;&#29992;&#20110;&#26631;&#20934;&#21270;&#24037;&#20316;&#36127;&#36733;&#35268;&#33539;&#65292;&#25429;&#25417;&#20851;&#38190;&#25805;&#20316;&#21644;&#20381;&#36182;&#39033;&#65292;&#20197;&#25512;&#36827;&#24615;&#33021;&#22522;&#20934;&#21644;&#21327;&#21516;&#35774;&#35745;&#65292;&#21516;&#26102;&#25552;&#20379;&#19968;&#32452;&#24037;&#20855;&#21644;&#33021;&#21147;&#65292;&#20197;&#20415;&#22312;&#27169;&#25311;&#22120;&#21644;&#20223;&#30495;&#22120;&#20013;&#23454;&#29616;&#26410;&#26469;&#31995;&#32479;&#30340;&#21327;&#21516;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20934;&#27979;&#35797;&#21644;&#21327;&#21516;&#35774;&#35745;&#23545;&#20110;&#25512;&#21160;ML&#27169;&#22411;&#12289;ML&#36719;&#20214;&#21644;&#19979;&#19968;&#20195;&#30828;&#20214;&#30340;&#20248;&#21270;&#21644;&#21019;&#26032;&#33267;&#20851;&#37325;&#35201;&#12290;&#23436;&#25972;&#30340;&#24037;&#20316;&#36127;&#36733;&#22522;&#20934;&#27979;&#35797;&#65292;&#20363;&#22914;MLPerf&#65292;&#22312;&#31995;&#32479;&#23436;&#20840;&#35774;&#35745;&#21644;&#37096;&#32626;&#21518;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#36719;&#20214;&#21644;&#30828;&#20214;&#22534;&#26632;&#20043;&#38388;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#65292;&#21457;&#25381;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#21019;&#26032;&#30340;&#36895;&#24230;&#35201;&#27714;&#37319;&#29992;&#26356;&#25935;&#25463;&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#21644;&#20351;&#29992;&#22522;&#20934;&#27979;&#35797;&#65292;&#20511;&#21161;&#27169;&#25311;&#22120;&#21644;&#20223;&#30495;&#22120;&#36827;&#34892;&#26410;&#26469;&#31995;&#32479;&#21327;&#21516;&#35774;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Chakra&#65292;&#19968;&#31181;&#24320;&#25918;&#30340;&#22270;&#24418;&#27169;&#24335;&#65292;&#29992;&#20110;&#26631;&#20934;&#21270;&#24037;&#20316;&#36127;&#36733;&#35268;&#33539;&#65292;&#25429;&#25417;&#20851;&#38190;&#25805;&#20316;&#21644;&#20381;&#36182;&#39033;&#65292;&#21363;&#25191;&#34892;&#36319;&#36394;&#65288;ET&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#20114;&#34917;&#30340;&#24037;&#20855;/&#33021;&#21147;&#65292;&#20197;&#20351;&#21508;&#31181;&#27169;&#25311;&#22120;&#12289;&#20223;&#30495;&#22120;&#21644;&#22522;&#20934;&#27979;&#35797;&#21487;&#20197;&#25910;&#38598;&#12289;&#29983;&#25104;&#21644;&#37319;&#29992;Chakra ET&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#20351;&#29992;&#29983;&#25104;AI&#27169;&#22411;&#23398;&#20064;&#20102;&#25968;&#21315;&#20010;Chakra ET&#30340;&#28508;&#22312;&#32479;&#35745;&#23646;&#24615;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#26469;&#32508;&#21512;Chakra ET&#12290;
&lt;/p&gt;
&lt;p&gt;
Benchmarking and co-design are essential for driving optimizations and innovation around ML models, ML software, and next-generation hardware. Full workload benchmarks, e.g. MLPerf, play an essential role in enabling fair comparison across different software and hardware stacks especially once systems are fully designed and deployed. However, the pace of AI innovation demands a more agile methodology to benchmark creation and usage by simulators and emulators for future system co-design. We propose Chakra, an open graph schema for standardizing workload specification capturing key operations and dependencies, also known as Execution Trace (ET). In addition, we propose a complementary set of tools/capabilities to enable collection, generation, and adoption of Chakra ETs by a wide range of simulators, emulators, and benchmarks. For instance, we use generative AI models to learn latent statistical properties across thousands of Chakra ETs and use these models to synthesize Chakra ETs. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32852;&#37030;&#21464;&#20998;&#25512;&#26029;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#36328;&#35774;&#22791;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20010;&#24615;&#21270;&#21644;&#27867;&#21270;&#65292;&#24182;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.13672</link><description>&lt;p&gt;
&#32852;&#37030;&#21464;&#24322;&#25512;&#26029;&#65306;&#36808;&#21521;&#20010;&#24615;&#21270;&#21644;&#27867;&#21270;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Federated Variational Inference: Towards Improved Personalization and Generalization. (arXiv:2305.13672v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32852;&#37030;&#21464;&#20998;&#25512;&#26029;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#36328;&#35774;&#22791;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20010;&#24615;&#21270;&#21644;&#27867;&#21270;&#65292;&#24182;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#21033;&#29992;&#25152;&#26377;&#21442;&#19982;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#21333;&#20010;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23458;&#25143;&#29983;&#25104;&#20998;&#24067;&#21644;&#39044;&#27979;&#27169;&#22411;&#30340;&#24322;&#36136;&#24615;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#19981;&#36866;&#24403;&#22320;&#36817;&#20284;&#39044;&#27979;&#36807;&#31243;&#12289;&#25910;&#25947;&#21040;&#26368;&#20248;&#29366;&#24577;&#25110;&#27867;&#21270;&#21040;&#26032;&#23458;&#25143;&#31471;&#12290;&#25105;&#20204;&#30740;&#31350;&#22312;&#20551;&#35774;&#23458;&#25143;&#25968;&#25454;&#20998;&#24067;&#21644;&#39044;&#27979;&#27169;&#22411;&#30340;&#24322;&#36136;&#24615;&#30340;&#29366;&#24577;&#19979;&#65292;&#36328;&#35774;&#22791;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#20013;&#30340;&#20010;&#24615;&#21270;&#21644;&#27867;&#21270;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#21152;&#20197;&#35268;&#33539;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#26469;&#26377;&#25928;&#22320;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#31216;&#27492;&#31639;&#27861;&#20026;&#32852;&#37030;&#21464;&#20998;&#25512;&#26029;&#65288;FedVI&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;PAC-Bayes&#20998;&#26512;&#20026;FedVI&#25552;&#20379;&#20102;&#27867;&#21270;&#30028;&#38480;&#12290;&#25105;&#20204;&#22312;FEMNIST&#21644;CIFAR-100&#22270;&#20687;&#20998;&#31867;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;FedVI&#22312;&#20004;&#20010;&#20219;&#21153;&#19978;&#22343;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional federated learning algorithms train a single global model by leveraging all participating clients' data. However, due to heterogeneity in client generative distributions and predictive models, these approaches may not appropriately approximate the predictive process, converge to an optimal state, or generalize to new clients. We study personalization and generalization in stateless cross-device federated learning setups assuming heterogeneity in client data distributions and predictive models. We first propose a hierarchical generative model and formalize it using Bayesian Inference. We then approximate this process using Variational Inference to train our model efficiently. We call this algorithm Federated Variational Inference (FedVI). We use PAC-Bayes analysis to provide generalization bounds for FedVI. We evaluate our model on FEMNIST and CIFAR-100 image classification and show that FedVI beats the state-of-the-art on both tasks.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SPEECH&#30340;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#33021;&#37327;&#24314;&#27169;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#20107;&#20214;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#36229;&#29699;&#26469;&#34920;&#31034;&#20107;&#20214;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SPEECH&#22312;&#20107;&#20214;&#26816;&#27979;&#21644;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13617</link><description>&lt;p&gt;
SPEECH: &#22522;&#20110;&#33021;&#37327;&#30340;&#20107;&#20214;&#20013;&#24515;&#36229;&#29699;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SPEECH: Structured Prediction with Energy-Based Event-Centric Hyperspheres. (arXiv:2305.13617v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13617
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SPEECH&#30340;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#33021;&#37327;&#24314;&#27169;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#20107;&#20214;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#36229;&#29699;&#26469;&#34920;&#31034;&#20107;&#20214;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SPEECH&#22312;&#20107;&#20214;&#26816;&#27979;&#21644;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#20013;&#24515;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;&#28041;&#21450;&#39044;&#27979;&#20107;&#20214;&#30340;&#32467;&#26500;&#21270;&#36755;&#20986;&#12290;&#22312;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24773;&#20917;&#19979;&#65292;&#20107;&#20214;&#32467;&#26500;&#37117;&#20855;&#26377;&#22797;&#26434;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22240;&#27492;&#26377;&#25928;&#22320;&#34920;&#31034;&#36825;&#20123;&#22797;&#26434;&#30340;&#20107;&#20214;&#32467;&#26500;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#20107;&#20214;&#20013;&#24515;&#36229;&#29699;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979; (SPEECH)&#12290; SPEECH &#20351;&#29992;&#22522;&#20110;&#33021;&#37327;&#30340;&#24314;&#27169;&#26469;&#27169;&#25311;&#20107;&#20214;&#32467;&#26500;&#32452;&#20214;&#20043;&#38388;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#36229;&#29699;&#26469;&#34920;&#31034;&#20107;&#20214;&#31867;&#21035;&#12290;&#22312;&#20004;&#20010;&#32479;&#19968;&#26631;&#27880;&#30340;&#20107;&#20214;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#32467;&#26524;&#34920;&#26126;SPEECH&#22312;&#20107;&#20214;&#26816;&#27979;&#21644;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#21344;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event-centric structured prediction involves predicting structured outputs of events. In most NLP cases, event structures are complex with manifold dependency, and it is challenging to effectively represent these complicated structured events. To address these issues, we propose Structured Prediction with Energy-based Event-Centric Hyperspheres (SPEECH). SPEECH models complex dependency among event structured components with energy-based modeling, and represents event classes with simple but effective hyperspheres. Experiments on two unified-annotated event datasets indicate that SPEECH is predominant in event detection and event-relation extraction tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DR&#30340;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#19981;&#23545;&#31216;&#30340;&#23398;&#20064;&#29575;&#26469;&#35299;&#20915;&#30001;&#21512;&#20316;&#21338;&#24328;&#24341;&#21457;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#25913;&#21892;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13599</link><description>&lt;p&gt;
&#19981;&#23545;&#31216;&#23398;&#20064;&#29575;&#30340;&#20998;&#31163;&#24335;&#29702;&#24615;&#21270;: &#19968;&#31181;&#28789;&#27963;&#30340;Lipschitz&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Decoupled Rationalization with Asymmetric Learning Rates: A Flexible Lipshitz Restraint. (arXiv:2305.13599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DR&#30340;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#19981;&#23545;&#31216;&#30340;&#23398;&#20064;&#29575;&#26469;&#35299;&#20915;&#30001;&#21512;&#20316;&#21338;&#24328;&#24341;&#21457;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#25913;&#21892;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#33258;&#35828;&#26126;&#29702;&#24615;&#21270;&#27169;&#22411;&#36890;&#36807;&#21512;&#20316;&#21338;&#24328;&#26500;&#24314;&#65292;&#20854;&#20013;&#29983;&#25104;&#22120;&#20174;&#36755;&#20837;&#25991;&#26412;&#20013;&#36873;&#25321;&#26368;&#26131;&#29702;&#35299;&#30340;&#37096;&#20998;&#20316;&#20026;&#21407;&#29702;&#65292;&#25509;&#30528;&#39044;&#27979;&#22120;&#22522;&#20110;&#25152;&#36873;&#25321;&#30340;&#21407;&#29702;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21512;&#20316;&#21338;&#24328;&#21487;&#33021;&#20250;&#24341;&#21457;&#36864;&#21270;&#38382;&#39064;&#65292;&#39044;&#27979;&#22120;&#36807;&#24230;&#25311;&#21512;&#20110;&#30001;&#23578;&#26410;&#35757;&#32451;&#22909;&#30340;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#20449;&#24687;&#19981;&#36275;&#30340;&#37096;&#20998;&#65292;&#21453;&#36807;&#26469;&#23548;&#33268;&#29983;&#25104;&#22120;&#25910;&#25947;&#20110;&#36235;&#21521;&#20110;&#36873;&#25321;&#26080;&#24847;&#20041;&#30340;&#37096;&#20998;&#30340;&#27425;&#20248;&#27169;&#22411;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#23558;&#36864;&#21270;&#38382;&#39064;&#19982;&#39044;&#27979;&#22120;&#30340;Lipschitz&#36830;&#32493;&#24615;&#32852;&#31995;&#36215;&#26469;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23454;&#39564;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DR&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#28982;&#12289;&#28789;&#27963;&#22320;&#32422;&#26463;&#39044;&#27979;&#22120;&#30340;Lipschitz&#24120;&#25968;&#65292;&#24182;&#35299;&#20915;&#20102;&#36864;&#21270;&#38382;&#39064;&#12290;DR&#26041;&#27861;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#23558;&#29983;&#25104;&#22120;&#21644;&#39044;&#27979;&#22120;&#20998;&#31163;&#65292;&#20026;&#23427;&#20204;&#20998;&#37197;&#19981;&#23545;&#31216;&#30340;&#23398;&#20064;&#29575;&#12290;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;DR&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
A self-explaining rationalization model is generally constructed by a cooperative game where a generator selects the most human-intelligible pieces from the input text as rationales, followed by a predictor that makes predictions based on the selected rationales. However, such a cooperative game may incur the degeneration problem where the predictor overfits to the uninformative pieces generated by a not yet well-trained generator and in turn, leads the generator to converge to a sub-optimal model that tends to select senseless pieces. In this paper, we theoretically bridge degeneration with the predictor's Lipschitz continuity. Then, we empirically propose a simple but effective method named DR, which can naturally and flexibly restrain the Lipschitz constant of the predictor, to address the problem of degeneration. The main idea of DR is to decouple the generator and predictor to allocate them with asymmetric learning rates. A series of experiments conducted on two widely used benchm
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31895;&#21040;&#32454;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20998;&#23376;&#29983;&#25104;&#20013;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#20998;&#23618;&#25193;&#25955;&#27169;&#22411;&#21644;HIPER&#31639;&#27861;&#29983;&#25104;&#32467;&#26500;&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13266</link><description>&lt;p&gt;
&#31895;&#21040;&#32454;: &#19968;&#31181;&#29992;&#20110;&#19977;&#32500;&#20998;&#23376;&#29983;&#25104;&#30340;&#20998;&#23618;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Coarse-to-Fine: a Hierarchical Diffusion Model for Molecule Generation in 3D. (arXiv:2305.13266v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13266
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31895;&#21040;&#32454;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20998;&#23376;&#29983;&#25104;&#20013;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#20998;&#23618;&#25193;&#25955;&#27169;&#22411;&#21644;HIPER&#31639;&#27861;&#29983;&#25104;&#32467;&#26500;&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#65292;&#29983;&#25104;&#29702;&#24819;&#30340;&#19977;&#32500;&#20998;&#23376;&#32467;&#26500;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#23613;&#31649;&#25105;&#20204;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#22312;&#21407;&#23376;&#20998;&#36776;&#29575;&#19979;&#29983;&#25104;&#20998;&#23376;&#65292;&#24182;&#24573;&#30053;&#20869;&#22312;&#30340;&#23616;&#37096;&#32467;&#26500;&#65292;&#22914;&#29615;&#65292;&#36825;&#23548;&#33268;&#29983;&#25104;&#30340;&#32467;&#26500;&#36136;&#37327;&#36739;&#24046;&#65292;&#29305;&#21035;&#26159;&#24403;&#29983;&#25104;&#22823;&#20998;&#23376;&#26102;&#12290;&#22522;&#20110;&#29255;&#27573;&#30340;&#20998;&#23376;&#29983;&#25104;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#65292;&#20294;&#30001;&#20110;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#23427;&#19981;&#23481;&#26131;&#29992;&#20110;3D&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#31895;&#21040;&#32454;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#25193;&#25955;&#30340;&#27169;&#22411;&#65288;&#21363;HierDiff&#65289;&#65292;&#20197;&#20445;&#25345;&#23616;&#37096;&#27573;&#30340;&#26377;&#25928;&#24615;&#32780;&#19981;&#20381;&#36182;&#20110;&#33258;&#22238;&#24402;&#24314;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;HierDiff&#39318;&#20808;&#36890;&#36807;&#31561;&#21464;&#25193;&#25955;&#36807;&#31243;&#29983;&#25104;&#31895;&#31890;&#24230;&#20998;&#23376;&#20960;&#20309;&#20307;&#65292;&#20854;&#20013;&#27599;&#20010;&#31895;&#31890;&#24230;&#33410;&#28857;&#21453;&#26144;&#20998;&#23376;&#20013;&#30340;&#19968;&#20010;&#29255;&#27573;&#12290;&#28982;&#21518;&#65292;&#31895;&#31890;&#24230;&#33410;&#28857;&#34987;&#20998;&#35299;&#25104;&#32454;&#31890;&#24230;&#33410;&#28857;&#65292;&#20854;&#20013;&#32454;&#31890;&#24230;&#33410;&#28857;&#26159;&#20998;&#23376;&#20013;&#30340;&#19968;&#20010;&#19977;&#32500;&#31354;&#38388;&#28857;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#37319;&#26679;&#21644;&#32463;&#39564;&#27010;&#29575;&#32454;&#21270;&#65288;&#21363;HIPER&#65289;&#31639;&#27861;&#65292;&#20197;&#30830;&#20445;&#29983;&#25104;&#30340;&#32467;&#26500;&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;HierDiff&#22312;&#23450;&#37327;&#35780;&#20272;&#25351;&#26631;&#21644;&#35270;&#35273;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating desirable molecular structures in 3D is a fundamental problem for drug discovery. Despite the considerable progress we have achieved, existing methods usually generate molecules in atom resolution and ignore intrinsic local structures such as rings, which leads to poor quality in generated structures, especially when generating large molecules. Fragment-based molecule generation is a promising strategy, however, it is nontrivial to be adapted for 3D non-autoregressive generations because of the combinational optimization problems. In this paper, we utilize a coarse-to-fine strategy to tackle this problem, in which a Hierarchical Diffusion-based model (i.e.~HierDiff) is proposed to preserve the validity of local segments without relying on autoregressive modeling. Specifically, HierDiff first generates coarse-grained molecule geometries via an equivariant diffusion process, where each coarse-grained node reflects a fragment in a molecule. Then the coarse-grained nodes are dec
&lt;/p&gt;</description></item><item><title>INVICTUS&#26159;&#19968;&#20010;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#25628;&#32034;&#30340;&#27169;&#22411;&#65292;&#33258;&#21160;&#29983;&#25104;&#36923;&#36753;&#26368;&#23567;&#21270;&#21551;&#21457;&#24335;&#31639;&#27861;&#32508;&#21512;&#37197;&#26041;&#20197;&#20248;&#21270;&#30005;&#36335;&#38754;&#31215;&#21644;&#26102;&#24310;&#31561;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.13164</link><description>&lt;p&gt;
INVICTUS: &#36890;&#36807;&#21327;&#21516;&#23398;&#20064;&#21644;&#25628;&#32034;&#20248;&#21270;&#24067;&#23572;&#36923;&#36753;&#30005;&#36335;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
INVICTUS: Optimizing Boolean Logic Circuit Synthesis via Synergistic Learning and Search. (arXiv:2305.13164v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13164
&lt;/p&gt;
&lt;p&gt;
INVICTUS&#26159;&#19968;&#20010;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#25628;&#32034;&#30340;&#27169;&#22411;&#65292;&#33258;&#21160;&#29983;&#25104;&#36923;&#36753;&#26368;&#23567;&#21270;&#21551;&#21457;&#24335;&#31639;&#27861;&#32508;&#21512;&#37197;&#26041;&#20197;&#20248;&#21270;&#30005;&#36335;&#38754;&#31215;&#21644;&#26102;&#24310;&#31561;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#32508;&#21512;&#26159;&#33455;&#29255;&#35774;&#35745;&#20013;&#30340;&#31532;&#19968;&#27493;&#20063;&#26159;&#26368;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;INVICTUS&#65292;&#21033;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#24050;&#26377;&#30340;&#35774;&#35745;&#25968;&#25454;&#38598;&#33258;&#21160;&#29983;&#25104;&#19968;&#31995;&#21015;&#36923;&#36753;&#26368;&#23567;&#21270;&#21551;&#21457;&#24335;&#31639;&#27861;&#65288;&#8220;&#32508;&#21512;&#37197;&#26041;&#8221;&#65289;&#65292;&#20248;&#21270;&#30005;&#36335;&#38754;&#31215;&#21644;&#26102;&#24310;&#31561;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logic synthesis is the first and most vital step in chip design. This steps converts a chip specification written in a hardware description language (such as Verilog) into an optimized implementation using Boolean logic gates. State-of-the-art logic synthesis algorithms have a large number of logic minimization heuristics, typically applied sequentially based on human experience and intuition. The choice of the order greatly impacts the quality (e.g., area and delay) of the synthesized circuit. In this paper, we propose INVICTUS, a model-based offline reinforcement learning (RL) solution that automatically generates a sequence of logic minimization heuristics ("synthesis recipe") based on a training dataset of previously seen designs. A key challenge is that new designs can range from being very similar to past designs (e.g., adders and multipliers) to being completely novel (e.g., new processor instructions). %Compared to prior work, INVICTUS is the first solution that uses a mix of R
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DermSynth3D&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#21487;&#24494;&#20998;&#28210;&#26579;&#22120;&#23558;&#30382;&#32932;&#30149;&#21464;&#27169;&#24335;&#28151;&#21512;&#21040;&#20154;&#20307;&#19977;&#32500;&#32441;&#29702;&#32593;&#26684;&#19978;&#24182;&#29983;&#25104;&#36924;&#30495;&#30340;&#20108;&#32500;&#30382;&#32932;&#38236;&#20687;&#22270;&#20687;&#65292;&#21516;&#26102;&#25552;&#20379;&#23545;&#24212;&#30340;&#23494;&#38598;&#27880;&#37322;&#20197;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2305.12621</link><description>&lt;p&gt;
DermSynth3D&#65306;&#37326;&#22806;&#27880;&#37322;&#30382;&#32932;&#31185;&#22270;&#20687;&#30340;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
DermSynth3D: Synthesis of in-the-wild Annotated Dermatology Images. (arXiv:2305.12621v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12621
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DermSynth3D&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#21487;&#24494;&#20998;&#28210;&#26579;&#22120;&#23558;&#30382;&#32932;&#30149;&#21464;&#27169;&#24335;&#28151;&#21512;&#21040;&#20154;&#20307;&#19977;&#32500;&#32441;&#29702;&#32593;&#26684;&#19978;&#24182;&#29983;&#25104;&#36924;&#30495;&#30340;&#20108;&#32500;&#30382;&#32932;&#38236;&#20687;&#22270;&#20687;&#65292;&#21516;&#26102;&#25552;&#20379;&#23545;&#24212;&#30340;&#23494;&#38598;&#27880;&#37322;&#20197;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#30382;&#32932;&#31185;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#23384;&#22312;&#26174;&#30528;&#38480;&#21046;&#65292;&#21253;&#25324;&#26679;&#26412;&#22270;&#20687;&#25968;&#37327;&#36739;&#23569;&#12289;&#30142;&#30149;&#26465;&#20214;&#26377;&#38480;&#12289;&#27880;&#37322;&#19981;&#36275;&#20197;&#21450;&#38750;&#26631;&#20934;&#21270;&#22270;&#20687;&#37319;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DermSynth3D&#30340;&#26032;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#21487;&#24494;&#20998;&#28210;&#26579;&#22120;&#23558;&#30382;&#32932;&#30149;&#21464;&#27169;&#24335;&#28151;&#21512;&#21040;&#20154;&#20307;&#30340;&#19977;&#32500;&#32441;&#29702;&#32593;&#26684;&#19978;&#65292;&#24182;&#22312;&#21508;&#31181;&#32972;&#26223;&#22330;&#26223;&#19979;&#37319;&#29992;&#19981;&#21516;&#35270;&#35282;&#21644;&#20809;&#29031;&#26465;&#20214;&#29983;&#25104;&#20108;&#32500;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36981;&#24490;&#33258;&#19978;&#32780;&#19979;&#30340;&#35268;&#21017;&#65292;&#38480;&#21046;&#28151;&#21512;&#21644;&#28210;&#26579;&#36807;&#31243;&#65292;&#20197;&#21019;&#24314;&#20855;&#26377;&#37326;&#22806;&#29031;&#29255;&#24863;&#30340;&#30382;&#32932;&#26465;&#20214;&#30340;&#20108;&#32500;&#22270;&#20687;&#65292;&#30830;&#20445;&#26356;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#12290;&#35813;&#26694;&#26550;&#29983;&#25104;&#36924;&#30495;&#30340;&#20108;&#32500;&#30382;&#32932;&#38236;&#20687;&#22270;&#20687;&#65292;&#24182;&#29983;&#25104;&#23545;&#30382;&#32932;&#12289;&#30382;&#32932;&#29366;&#20917;&#12289;&#36523;&#20307;&#37096;&#20301;&#21644;&#22836;&#21457;&#21306;&#22495;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#23545;&#24212;&#23494;&#38598;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning (DL) has shown great potential in the field of dermatological image analysis. However, existing datasets in this domain have significant limitations, including a small number of image samples, limited disease conditions, insufficient annotations, and non-standardized image acquisitions. To address these shortcomings, we propose a novel framework called DermSynth3D. DermSynth3D blends skin disease patterns onto 3D textured meshes of human subjects using a differentiable renderer and generates 2D images from various camera viewpoints under chosen lighting conditions in diverse background scenes. Our method adheres to top-down rules that constrain the blending and rendering process to create 2D images with skin conditions that mimic in-the-wild acquisitions, ensuring more meaningful results. The framework generates photo-realistic 2D dermoscopy images and the corresponding dense annotations for semantic segmentation of the skin, skin conditions, body parts, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#22836;&#29366;&#24577;&#31354;&#38388;&#65288;MH-SSM&#65289;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#24182;&#22312;LibriSpeech&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#30340;&#26032;&#30340;&#24615;&#33021;&#65292;&#26159;&#21464;&#21387;&#22120;&#21464;&#25442;&#22120;&#30340;&#20248;&#31168;&#26367;&#20195;&#26041;&#26696;&#12290;&#21516;&#26102;, MH-SSM&#23618;&#30340;&#24341;&#20837;&#20063;&#25552;&#39640;&#20102;&#21464;&#21387;&#22120;&#22359;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#29616;&#26377;&#26368;&#26032;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.12498</link><description>&lt;p&gt;
&#22810;&#22836;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-Head State Space Model for Speech Recognition. (arXiv:2305.12498v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#22836;&#29366;&#24577;&#31354;&#38388;&#65288;MH-SSM&#65289;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#24182;&#22312;LibriSpeech&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#30340;&#26032;&#30340;&#24615;&#33021;&#65292;&#26159;&#21464;&#21387;&#22120;&#21464;&#25442;&#22120;&#30340;&#20248;&#31168;&#26367;&#20195;&#26041;&#26696;&#12290;&#21516;&#26102;, MH-SSM&#23618;&#30340;&#24341;&#20837;&#20063;&#25552;&#39640;&#20102;&#21464;&#21387;&#22120;&#22359;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#29616;&#26377;&#26368;&#26032;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#19968;&#20123;&#23567;&#35268;&#27169;&#30340;&#24207;&#21015;&#21644;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#24050;&#32463;&#34920;&#29616;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#24182;&#19988;&#33021;&#22815;&#19982;&#35768;&#22810;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#36229;&#36234;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#22836;&#29366;&#24577;&#31354;&#38388;&#65288;MH-SSM&#65289;&#26550;&#26500;&#65292;&#23427;&#37197;&#22791;&#20102;&#29305;&#27530;&#30340;&#38376;&#25511;&#26426;&#21046;&#65292;&#20854;&#20013;&#24182;&#34892;&#22836;&#34987;&#25945;&#25480;&#22914;&#20309;&#22312;&#24207;&#21015;&#25968;&#25454;&#19978;&#23398;&#20064;&#26412;&#22320;&#21644;&#20840;&#23616;&#30340;&#26102;&#38388;&#21160;&#24577;&#12290;&#20316;&#20026;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#20013;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#30452;&#25509;&#26367;&#20195;&#26041;&#26696;&#65292;&#36825;&#20010;&#26032;&#27169;&#22411;&#22312;LibriSpeech&#35821;&#38899;&#35782;&#21035;&#35821;&#26009;&#24211;&#19978;&#26174;&#33879;&#20248;&#20110;&#21464;&#21387;&#22120;&#21464;&#25442;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#21464;&#21387;&#22120;&#22359;&#20013;&#22686;&#21152;&#20102;MH-SSM&#23618;&#65292;&#31216;&#20026;Stateformer&#65292;&#19981;&#20351;&#29992;&#22806;&#37096;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;LibriSpeech&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#65292;&#24320;&#21457;&#38598;&#21644;&#27979;&#35797;&#38598;&#30340;&#35789;&#38169;&#35823;&#29575;&#20998;&#21035;&#20026;1.76&#65285; / 4.37&#65285;&#21644;1.91&#65285; / 4.36&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
State space models (SSMs) have recently shown promising results on small-scale sequence and language modelling tasks, rivalling and outperforming many attention-based approaches. In this paper, we propose a multi-head state space (MH-SSM) architecture equipped with special gating mechanisms, where parallel heads are taught to learn local and global temporal dynamics on sequence data. As a drop-in replacement for multi-head attention in transformer encoders, this new model significantly outperforms the transformer transducer on the LibriSpeech speech recognition corpus. Furthermore, we augment the transformer block with MH-SSMs layers, referred to as the Stateformer, achieving state-of-the-art performance on the LibriSpeech task, with word error rates of 1.76\%/4.37\% on the development and 1.91\%/4.36\% on the test sets without using an external language model.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20061;&#20010;&#25216;&#24039;&#26469;&#24110;&#21161;&#29983;&#24577;&#23398;&#23478;&#23454;&#26045;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#20123;&#25216;&#24039;&#38024;&#23545;&#20998;&#31867;&#38382;&#39064;&#65292;&#26088;&#22312;&#35782;&#21035;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#24120;&#35265;&#38169;&#35823;&#12289;&#38519;&#38449;&#25110;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10472</link><description>&lt;p&gt;
&#29983;&#24577;&#23398;&#23478;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#20061;&#20010;&#25216;&#24039;
&lt;/p&gt;
&lt;p&gt;
Nine tips for ecologists using machine learning. (arXiv:2305.10472v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20061;&#20010;&#25216;&#24039;&#26469;&#24110;&#21161;&#29983;&#24577;&#23398;&#23478;&#23454;&#26045;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#20123;&#25216;&#24039;&#38024;&#23545;&#20998;&#31867;&#38382;&#39064;&#65292;&#26088;&#22312;&#35782;&#21035;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#24120;&#35265;&#38169;&#35823;&#12289;&#38519;&#38449;&#25110;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#39640;&#31934;&#24230;&#21644;&#28789;&#27963;&#24615;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25104;&#20026;&#20102;&#29983;&#24577;&#23398;&#23478;&#21512;&#36866;&#19988;&#39640;&#25928;&#30340;&#24037;&#20855;&#12290;&#20294;&#26159;&#65292;&#23454;&#26045;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24182;&#19981;&#26159;&#19968;&#39033;&#31616;&#21333;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#27809;&#26377;&#22312;&#36825;&#20010;&#39046;&#22495;&#26377;&#32463;&#39564;&#30340;&#29983;&#24577;&#23398;&#23478;&#26469;&#35828;&#21487;&#33021;&#20250;&#26377;&#20123;&#38590;&#20197;&#25509;&#21463;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#25216;&#24039;&#26469;&#24110;&#21161;&#29983;&#24577;&#23398;&#23478;&#23454;&#26045;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20998;&#31867;&#38382;&#39064;&#65292;&#22240;&#20026;&#35768;&#22810;&#29983;&#24577;&#23398;&#30740;&#31350;&#26088;&#22312;&#23558;&#25968;&#25454;&#24402;&#20837;&#39044;&#23450;&#20041;&#30340;&#31867;&#21035;&#65292;&#20363;&#22914;&#29983;&#24577;&#29366;&#24577;&#25110;&#29983;&#29289;&#23454;&#20307;&#12290;&#36825;&#20061;&#20010;&#25216;&#24039;&#20013;&#65292;&#27599;&#19968;&#20010;&#37117;&#25552;&#20986;&#20102;&#22312;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26041;&#38754;&#30340;&#24120;&#35265;&#38169;&#35823;&#12289;&#38519;&#38449;&#25110;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#21161;&#20110;&#29983;&#24577;&#23398;&#30740;&#31350;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to their high predictive performance and flexibility, machine learning models are an appropriate and efficient tool for ecologists. However, implementing a machine learning model is not yet a trivial task and may seem intimidating to ecologists with no previous experience in this area. Here we provide a series of tips to help ecologists in implementing machine learning models. We focus on classification problems as many ecological studies aim to assign data into predefined classes such as ecological states or biological entities. Each of the nine tips identifies a common error, trap or challenge in developing machine learning models and provides recommendations to facilitate their use in ecological studies.
&lt;/p&gt;</description></item><item><title>sustain.AI&#26159;&#19968;&#20010;&#26234;&#33021;&#30340;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#23457;&#35745;&#24072;&#12289;&#37329;&#34701;&#25237;&#36164;&#32773;&#20197;&#21450;&#24191;&#22823;&#20844;&#20247;&#39640;&#25928;&#22320;&#20998;&#26512;&#20844;&#21496;&#30340;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#24182;&#36890;&#36807;&#19982;GRI&#26631;&#20934;&#21305;&#37197;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#25512;&#33616;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.08711</link><description>&lt;p&gt;
sustain.AI: &#19968;&#31181;&#20998;&#26512;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
sustain.AI: a Recommender System to analyze Sustainability Reports. (arXiv:2305.08711v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08711
&lt;/p&gt;
&lt;p&gt;
sustain.AI&#26159;&#19968;&#20010;&#26234;&#33021;&#30340;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#23457;&#35745;&#24072;&#12289;&#37329;&#34701;&#25237;&#36164;&#32773;&#20197;&#21450;&#24191;&#22823;&#20844;&#20247;&#39640;&#25928;&#22320;&#20998;&#26512;&#20844;&#21496;&#30340;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#24182;&#36890;&#36807;&#19982;GRI&#26631;&#20934;&#21305;&#37197;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#25512;&#33616;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;sustain.AI&#65292;&#36825;&#26159;&#19968;&#20010;&#26234;&#33021;&#30340;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#24110;&#21161;&#23457;&#35745;&#24072;&#12289;&#37329;&#34701;&#25237;&#36164;&#32773;&#20197;&#21450;&#24191;&#22823;&#20844;&#20247;&#39640;&#25928;&#22320;&#20998;&#26512;&#20844;&#21496;&#30340;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#12290;&#35813;&#24037;&#20855;&#21033;&#29992;&#20102;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#30340;&#26550;&#26500;&#65292;&#23558;&#22522;&#20110;BERT&#30340;&#32534;&#30721;&#27169;&#22359;&#19982;&#22810;&#26631;&#31614;&#20998;&#31867;&#22836;&#30456;&#32467;&#21512;&#65292;&#23558;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#20013;&#30340;&#30456;&#20851;&#25991;&#26412;&#27573;&#33853;&#19982;&#20840;&#29699;&#25253;&#21578;&#20513;&#35758;&#65288;GRI&#65289;&#26631;&#20934;&#20013;&#30340;&#30456;&#24212;&#27861;&#24459;&#27861;&#35268;&#21305;&#37197;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26032;&#39062;&#30340;&#24503;&#22269;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#22987;&#32456;&#23454;&#29616;&#20102;&#19982;&#22810;&#20010;&#24378;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#26356;&#39640;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;sustain.AI&#24050;&#32463;&#20844;&#24320;&#22312;https://sustain.ki.nrw/&#19978;&#25552;&#20379;&#32473;&#25152;&#26377;&#20154;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present $\text{sustain.AI}$, an intelligent, context-aware recommender system that assists auditors and financial investors as well as the general public to efficiently analyze companies' sustainability reports. The tool leverages an end-to-end trainable architecture that couples a BERT-based encoding module with a multi-label classification head to match relevant text passages from sustainability reports to their respective law regulations from the Global Reporting Initiative (GRI) standards. We evaluate our model on two novel German sustainability reporting data sets and consistently achieve a significantly higher recommendation performance compared to multiple strong baselines. Furthermore, $\text{sustain.AI}$ is publicly available for everyone at https://sustain.ki.nrw/.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#26041;&#27861;&#8212;&#8212;&#35299;&#37322;&#24615;&#24494;&#35843;&#65292;&#36890;&#36807;&#35753;&#27169;&#22411;&#22312;&#32473;&#20986;&#31572;&#26696;&#30340;&#21516;&#26102;&#29983;&#25104;&#25903;&#25345;&#35813;&#31572;&#26696;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#65292;&#26469;&#20943;&#36731;LLMs&#20381;&#36182;&#34394;&#20551;&#20851;&#32852;&#65292;&#20351;&#24471;&#27169;&#22411;&#23545;&#34394;&#20551;&#25552;&#31034;&#26356;&#21152;&#24378;&#38887;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04990</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#24494;&#35843;&#20351;&#27169;&#22411;&#23545;&#34394;&#20551;&#25552;&#31034;&#26356;&#24378;&#38887;
&lt;/p&gt;
&lt;p&gt;
Explanation-based Finetuning Makes Models More Robust to Spurious Cues. (arXiv:2305.04990v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#26041;&#27861;&#8212;&#8212;&#35299;&#37322;&#24615;&#24494;&#35843;&#65292;&#36890;&#36807;&#35753;&#27169;&#22411;&#22312;&#32473;&#20986;&#31572;&#26696;&#30340;&#21516;&#26102;&#29983;&#25104;&#25903;&#25345;&#35813;&#31572;&#26696;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#65292;&#26469;&#20943;&#36731;LLMs&#20381;&#36182;&#34394;&#20551;&#20851;&#32852;&#65292;&#20351;&#24471;&#27169;&#22411;&#23545;&#34394;&#20551;&#25552;&#31034;&#26356;&#21152;&#24378;&#38887;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38750;&#24120;&#24378;&#22823;&#65292;&#26377;&#26102;&#20250;&#23398;&#20064;&#21040;&#26631;&#31614;&#21644;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#22312;&#20998;&#24067;&#22806;&#25968;&#25454;&#19978;&#27867;&#21270;&#33021;&#21147;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#35299;&#37322;&#24615;&#24494;&#35843;&#20316;&#20026;&#20943;&#36731;LLMs&#20381;&#36182;&#34394;&#20551;&#20851;&#32852;&#30340;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#19982;&#26631;&#20934;&#24494;&#35843;&#21482;&#22312;&#32473;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#31572;&#26696;&#19981;&#21516;&#65292;&#25105;&#20204;&#24494;&#35843;&#27169;&#22411;&#20197;&#29983;&#25104;&#25903;&#25345;&#20854;&#31572;&#26696;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#20154;&#24037;&#26500;&#24314;&#30340;&#35757;&#32451;&#38598;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#35813;&#35757;&#32451;&#38598;&#21253;&#21547;&#19981;&#21516;&#31867;&#22411;&#30340;&#34394;&#20551;&#25552;&#31034;&#65292;&#24182;&#22312;&#27809;&#26377;&#36825;&#20123;&#25552;&#31034;&#30340;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#19982;&#26631;&#20934;&#24494;&#35843;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22235;&#20010;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#26041;&#38754;&#20351;&#27169;&#22411;&#26497;&#20854;&#24378;&#38887;&#65306;ComVE&#65288;+1.2&#65289;&#65292;CREAK&#65288;+9.1&#65289;&#65292;e-SNLI&#65288;+15.4&#65289;&#21644;SBIC&#65288;+6.5&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#27169;&#22411;&#29983;&#25104;&#30340;&#35299;&#37322;&#21516;&#26679;&#26377;&#25928;&#65292;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are so powerful that they sometimes learn correlations between labels and features that are irrelevant to the task, leading to poor generalization on out-of-distribution data. We propose explanation-based finetuning as a novel and general approach to mitigate LLMs' reliance on spurious correlations. Unlike standard finetuning where the model only predicts the answer given the input, we finetune the model to additionally generate a free-text explanation supporting its answer. To evaluate our method, we finetune the model on artificially constructed training sets containing different types of spurious cues, and test it on a test set without these cues. Compared to standard finetuning, our method makes models remarkably more robust against spurious cues in terms of accuracy drop across four classification tasks: ComVE (+1.2), CREAK (+9.1), e-SNLI (+15.4), and SBIC (+6.5). Moreover, our method works equally well with explanations generated by the model, implyin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30740;&#31350;&#20102;&#21367;&#31215;&#20113;&#30340;&#39537;&#21160;&#22240;&#32032;&#19982;&#20113;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#27668;&#35937;&#21644;&#27668;&#28342;&#33014;&#26465;&#20214;&#21487;&#20197;&#39044;&#27979;&#21367;&#31215;&#20113;&#23646;&#24615;&#12290;&#21151;&#33021;&#23646;&#24615;&#26041;&#27861;&#36824;&#21487;&#20197;&#37327;&#21270;&#36825;&#20123;&#20851;&#31995;&#65292;&#20102;&#35299;&#21738;&#20123;&#22240;&#32032;&#21487;&#20197;&#24433;&#21709;&#21367;&#31215;&#20113;&#20013;&#30340;&#20912;&#26230;&#25968;&#27987;&#24230;&#21644;&#20912;&#27700;&#21547;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.02090</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#29702;&#35299;&#21367;&#31215;&#20113;
&lt;/p&gt;
&lt;p&gt;
Understanding cirrus clouds using explainable machine learning. (arXiv:2305.02090v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30740;&#31350;&#20102;&#21367;&#31215;&#20113;&#30340;&#39537;&#21160;&#22240;&#32032;&#19982;&#20113;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#27668;&#35937;&#21644;&#27668;&#28342;&#33014;&#26465;&#20214;&#21487;&#20197;&#39044;&#27979;&#21367;&#31215;&#20113;&#23646;&#24615;&#12290;&#21151;&#33021;&#23646;&#24615;&#26041;&#27861;&#36824;&#21487;&#20197;&#37327;&#21270;&#36825;&#20123;&#20851;&#31995;&#65292;&#20102;&#35299;&#21738;&#20123;&#22240;&#32032;&#21487;&#20197;&#24433;&#21709;&#21367;&#31215;&#20113;&#20013;&#30340;&#20912;&#26230;&#25968;&#27987;&#24230;&#21644;&#20912;&#27700;&#21547;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#20113;&#26159;&#22320;&#29699;&#27668;&#20505;&#30340;&#20851;&#38190;&#35843;&#33410;&#22240;&#32032;&#65292;&#20294;&#23427;&#20204;&#19982;&#27668;&#35937;&#21644;&#27668;&#28342;&#33014;&#26465;&#20214;&#30340;&#20851;&#31995;&#26159;&#20840;&#29699;&#27668;&#20505;&#27169;&#22411;&#20013;&#26368;&#22823;&#30340;&#19981;&#30830;&#23450;&#24615;&#20043;&#19968;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#19977;&#24180;&#30340;&#21355;&#26143;&#21644;&#20877;&#20998;&#26512;&#25968;&#25454;&#65292;&#30740;&#31350;&#21367;&#31215;&#20113;&#39537;&#21160;&#22240;&#32032;&#19982;&#20113;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#19968;&#20010;&#24102;&#26377;&#27880;&#24847;&#23618;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#26469;&#39044;&#27979;&#20912;&#27700;&#21547;&#37327;&#21644;&#20912;&#26230;&#25968;&#27987;&#24230;&#12290;&#27169;&#22411;&#34920;&#26126;&#65292;&#27668;&#35937;&#21644;&#27668;&#28342;&#33014;&#26465;&#20214;&#21487;&#20197;&#39044;&#27979;&#21367;&#31215;&#20113;&#23646;&#24615;&#65292;$R^2=0.49$&#12290;&#20351;&#29992;SHapley Additive exPlanations (SHAP)&#35745;&#31639;&#21151;&#33021;&#23646;&#24615;&#65292;&#20197;&#37327;&#21270;&#27668;&#35937;&#21644;&#27668;&#28342;&#33014;&#26465;&#20214;&#19982;&#21367;&#31215;&#20113;&#23646;&#24615;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#20363;&#22914;&#65292;&#24341;&#36215;&#20912;&#26230;&#25968;&#27987;&#24230;&#39044;&#27979;&#19979;&#38477;&#25152;&#38656;&#30340;&#36229;&#24494;&#31859;&#23576;&#22467;&#31890;&#23376;&#30340;&#26368;&#23567;&#27987;&#24230;&#20026;$2 \times 10^{-4}$ mg m\textsuperscript{-3}&#12290;&#35266;&#23519;&#21069;15&#20010;&#23567;&#26102;
&lt;/p&gt;
&lt;p&gt;
Cirrus clouds are key modulators of Earth's climate. Their dependencies on meteorological and aerosol conditions are among the largest uncertainties in global climate models. This work uses three years of satellite and reanalysis data to study the link between cirrus drivers and cloud properties. We use a gradient-boosted machine learning model and a Long Short-Term Memory (LSTM) network with an attention layer to predict the ice water content and ice crystal number concentration. The models show that meteorological and aerosol conditions can predict cirrus properties with $R^2 = 0.49$. Feature attributions are calculated with SHapley Additive exPlanations (SHAP) to quantify the link between meteorological and aerosol conditions and cirrus properties. For instance, the minimum concentration of supermicron-sized dust particles required to cause a decrease in ice crystal number concentration predictions is $2 \times 10^{-4}$ mg m\textsuperscript{-3}. The last 15 hours before the observat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#19968;&#31687;&#20171;&#32461;&#32431;&#25968;&#23398;&#23478;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#36827;&#34892;&#30740;&#31350;&#30340;&#20010;&#20154;&#21644;&#38750;&#27491;&#24335;&#21465;&#36848;&#12290;</title><link>http://arxiv.org/abs/2304.12602</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#23545;&#20110;&#32431;&#25968;&#23398;&#23478;&#26469;&#35828;&#26159;&#19968;&#20010;&#26377;&#29992;&#30340;&#24037;&#20855;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is deep learning a useful tool for the pure mathematician?. (arXiv:2304.12602v1 [math.RT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#19968;&#31687;&#20171;&#32461;&#32431;&#25968;&#23398;&#23478;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#36827;&#34892;&#30740;&#31350;&#30340;&#20010;&#20154;&#21644;&#38750;&#27491;&#24335;&#21465;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31687;&#20851;&#20110;&#32431;&#25968;&#23398;&#23478;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#36827;&#34892;&#30740;&#31350;&#30340;&#20010;&#20154;&#21644;&#38750;&#27491;&#24335;&#21465;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
A personal and informal account of what a pure mathematician might expect when using tools from deep learning in their research.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861; Tree-structured Parzen estimator (TPE)&#65292;&#24182;&#23545;&#20854;&#25511;&#21046;&#21442;&#25968;&#30340;&#20316;&#29992;&#21644;&#31639;&#27861;&#30452;&#35273;&#36827;&#34892;&#20102;&#35752;&#35770;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#19968;&#32452;&#25512;&#33616;&#35774;&#32622;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#25552;&#39640;TPE&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.11127</link><description>&lt;p&gt;
&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#65306;&#29702;&#35299;&#20854;&#31639;&#27861;&#32452;&#25104;&#37096;&#20998;&#21450;&#20854;&#22312;&#25552;&#39640;&#23454;&#35777;&#34920;&#29616;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tree-structured Parzen estimator: Understanding its algorithm components and their roles for better empirical performance. (arXiv:2304.11127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11127
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861; Tree-structured Parzen estimator (TPE)&#65292;&#24182;&#23545;&#20854;&#25511;&#21046;&#21442;&#25968;&#30340;&#20316;&#29992;&#21644;&#31639;&#27861;&#30452;&#35273;&#36827;&#34892;&#20102;&#35752;&#35770;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#19968;&#32452;&#25512;&#33616;&#35774;&#32622;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#25552;&#39640;TPE&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#39046;&#22495;&#20013;&#26368;&#36817;&#30340;&#36827;&#23637;&#35201;&#27714;&#26356;&#21152;&#22797;&#26434;&#30340;&#23454;&#39564;&#35774;&#35745;&#12290;&#36825;&#31181;&#22797;&#26434;&#30340;&#23454;&#39564;&#36890;&#24120;&#26377;&#35768;&#22810;&#21442;&#25968;&#65292;&#38656;&#35201;&#21442;&#25968;&#35843;&#25972;&#12290;Tree-structured Parzen estimator (TPE) &#26159;&#19968;&#31181;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#26368;&#36817;&#30340;&#21442;&#25968;&#35843;&#25972;&#26694;&#26550;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#23613;&#31649;&#23427;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#25511;&#21046;&#21442;&#25968;&#30340;&#35282;&#33394;&#21644;&#31639;&#27861;&#30452;&#35273;&#23578;&#26410;&#24471;&#21040;&#35752;&#35770;&#12290;&#22312;&#26412;&#25945;&#31243;&#20013;&#65292;&#25105;&#20204;&#23558;&#30830;&#23450;&#27599;&#20010;&#25511;&#21046;&#21442;&#25968;&#30340;&#20316;&#29992;&#20197;&#21450;&#23427;&#20204;&#23545;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#23558;&#20174;&#21078;&#26512;&#30740;&#31350;&#20013;&#24471;&#20986;&#30340;&#25512;&#33616;&#35774;&#32622;&#19982;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#25512;&#33616;&#35774;&#32622;&#25552;&#39640;&#20102;TPE&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;TPE&#23454;&#29616;&#21487;&#22312;https://github.com/nabenabe0928/tpe/tree/single-opt&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in many domains require more and more complicated experiment design. Such complicated experiments often have many parameters, which necessitate parameter tuning. Tree-structured Parzen estimator (TPE), a Bayesian optimization method, is widely used in recent parameter tuning frameworks. Despite its popularity, the roles of each control parameter and the algorithm intuition have not been discussed so far. In this tutorial, we will identify the roles of each control parameter and their impacts on hyperparameter optimization using a diverse set of benchmarks. We compare our recommended setting drawn from the ablation study with baseline methods and demonstrate that our recommended setting improves the performance of TPE. Our TPE implementation is available at https://github.com/nabenabe0928/tpe/tree/single-opt.
&lt;/p&gt;</description></item><item><title>PED-ANOVA &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340; f-ANOVA &#20844;&#24335;&#65292;&#33021;&#22815;&#22312;&#20219;&#24847;&#23376;&#31354;&#38388;&#20013;&#39640;&#25928;&#22320;&#35745;&#31639;&#36229;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#26377;&#21161;&#20110;&#28145;&#24230;&#23398;&#20064;&#20013;&#22909;&#30340;&#36229;&#21442;&#25968;&#31354;&#38388;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.10255</link><description>&lt;p&gt;
PED-ANOVA: &#22312;&#20219;&#24847;&#23376;&#31354;&#38388;&#20013;&#39640;&#25928;&#37327;&#21270;&#36229;&#21442;&#25968;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
PED-ANOVA: Efficiently Quantifying Hyperparameter Importance in Arbitrary Subspaces. (arXiv:2304.10255v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10255
&lt;/p&gt;
&lt;p&gt;
PED-ANOVA &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340; f-ANOVA &#20844;&#24335;&#65292;&#33021;&#22815;&#22312;&#20219;&#24847;&#23376;&#31354;&#38388;&#20013;&#39640;&#25928;&#22320;&#35745;&#31639;&#36229;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#26377;&#21161;&#20110;&#28145;&#24230;&#23398;&#20064;&#20013;&#22909;&#30340;&#36229;&#21442;&#25968;&#31354;&#38388;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#27969;&#34892;&#20351;&#24471;&#22909;&#30340;&#36229;&#21442;&#25968;&#31354;&#38388;&#35774;&#35745;&#23545;&#20110;&#35757;&#32451;&#24378;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#22909;&#30340;&#36229;&#21442;&#25968;&#31354;&#38388;&#35774;&#35745;&#21448;&#20005;&#37325;&#20381;&#36182;&#20110;&#20102;&#35299;&#19981;&#21516;&#36229;&#21442;&#25968;&#30340;&#20316;&#29992;&#12290;&#36825;&#28608;&#21457;&#20102;&#20851;&#20110;&#36229;&#21442;&#25968;&#37325;&#35201;&#24615;&#30340;&#30740;&#31350;&#65292;&#20363;&#22914;&#20351;&#29992;&#21151;&#33021;&#26041;&#24046;&#20998;&#26512; (f-ANOVA) &#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21407;&#22987;&#30340; f-ANOVA &#20844;&#24335;&#19981;&#36866;&#29992;&#20110;&#31639;&#27861;&#35774;&#35745;&#24072;&#26368;&#30456;&#20851;&#30340;&#23376;&#31354;&#38388;&#65292;&#20363;&#22914;&#30001;&#26368;&#20339;&#24615;&#33021;&#23450;&#20041;&#30340;&#23376;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#26032;&#30340;&#38024;&#23545;&#20219;&#24847;&#23376;&#31354;&#38388;&#30340; f-ANOVA &#20844;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#20351;&#29992; Pearson &#25955;&#24230; (PED) &#23454;&#29616;&#36229;&#21442;&#25968;&#37325;&#35201;&#24615;&#30340;&#38381;&#24335;&#35745;&#31639;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#20010;&#26032;&#31639;&#27861;&#65292;&#31216;&#20026; PED-ANOVA&#65292;&#33021;&#22815;&#25104;&#21151;&#22320;&#35782;&#21035;&#19981;&#21516;&#23376;&#31354;&#38388;&#20013;&#37325;&#35201;&#30340;&#36229;&#21442;&#25968;&#65292;&#21516;&#26102;&#35745;&#31639;&#25928;&#29575;&#26497;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent rise in popularity of Hyperparameter Optimization (HPO) for deep learning has highlighted the role that good hyperparameter (HP) space design can play in training strong models. In turn, designing a good HP space is critically dependent on understanding the role of different HPs. This motivates research on HP Importance (HPI), e.g., with the popular method of functional ANOVA (f-ANOVA). However, the original f-ANOVA formulation is inapplicable to the subspaces most relevant to algorithm designers, such as those defined by top performance. To overcome this problem, we derive a novel formulation of f-ANOVA for arbitrary subspaces and propose an algorithm that uses Pearson divergence (PED) to enable a closed-form computation of HPI. We demonstrate that this new algorithm, dubbed PED-ANOVA, is able to successfully identify important HPs in different subspaces while also being extremely computationally efficient.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#22312;&#20445;&#35777;&#26412;&#22320;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#24378;&#20013;&#24515;&#38544;&#31169;&#20445;&#35777;&#65292;&#24182;&#21033;&#29992;Shuffle&#27169;&#22411;&#36827;&#34892;&#38544;&#31169;&#25193;&#22686;&#12290;</title><link>http://arxiv.org/abs/2304.05516</link><description>&lt;p&gt;
&#37051;&#23621;&#30340;&#22238;&#21709;&#65306;&#22522;&#20110;Shuffle&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#25193;&#22686;
&lt;/p&gt;
&lt;p&gt;
Echo of Neighbors: Privacy Amplification for Personalized Private Federated Learning with Shuffle Model. (arXiv:2304.05516v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#22312;&#20445;&#35777;&#26412;&#22320;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#24378;&#20013;&#24515;&#38544;&#31169;&#20445;&#35777;&#65292;&#24182;&#21033;&#29992;Shuffle&#27169;&#22411;&#36827;&#34892;&#38544;&#31169;&#25193;&#22686;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#21327;&#21516;&#35757;&#32451;&#33539;&#20363;&#65292;&#20294;&#20250;&#21463;&#21040;&#38544;&#31169;&#25915;&#20987;&#12290;&#20026;&#20102;&#28385;&#36275;&#29992;&#25143;&#23545;&#20110;&#19981;&#21516;&#38544;&#31169;&#38656;&#27714;&#30340;&#26412;&#22320;&#38656;&#27714;&#65292;&#38656;&#35201;&#20445;&#30041;&#20010;&#24615;&#21270;&#30340;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#65292;&#21516;&#26102;&#36824;&#38656;&#35201;&#20026;&#20840;&#23616;&#27169;&#22411;&#25552;&#20379;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65288;APES&#65289;&#26469;&#21152;&#24378;&#20010;&#24615;&#21270;&#26412;&#22320;&#38544;&#31169;&#20445;&#25252;&#26465;&#20214;&#19979;&#30340;&#27169;&#22411;&#38544;&#31169;&#65292;&#21033;&#29992;Shuffle&#27169;&#22411;&#30340;&#38544;&#31169;&#25193;&#22686;&#25928;&#26524;&#12290;&#20026;&#20102;&#22686;&#24378;&#38544;&#31169;&#20445;&#35777;&#65292;&#25105;&#20204;&#37327;&#21270;&#27599;&#20010;&#29992;&#25143;&#23545;&#20013;&#24515;&#38544;&#31169;&#30340;&#24322;&#26500;&#36129;&#29486;&#65292;&#24182;&#36890;&#36807;&#25200;&#21160;&#8220;&#22238;&#22768;&#8221;&#26469;&#25551;&#36848;&#29992;&#25143;&#30340;&#29305;&#24449;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning, as a popular paradigm for collaborative training, is vulnerable against privacy attacks. Different privacy levels regarding users' attitudes need to be satisfied locally, while a strict privacy guarantee for the global model is also required centrally. Personalized Local Differential Privacy (PLDP) is suitable for preserving users' varying local privacy, yet only provides a central privacy guarantee equivalent to the worst-case local privacy level. Thus, achieving strong central privacy as well as personalized local privacy with a utility-promising model is a challenging problem. In this work, a general framework (APES) is built up to strengthen model privacy under personalized local privacy by leveraging the privacy amplification effect of the shuffle model. To tighten the privacy bound, we quantify the heterogeneous contributions to the central privacy user by user. The contributions are characterized by the ability of generating "echos" from the perturbation of e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20301;&#32622;&#21435;&#22122;&#39044;&#27979;&#26131;&#24471;&#20960;&#20309;&#32467;&#26500;&#30340;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;&#65292;&#21487;&#20197;&#29992;&#30456;&#23545;&#23481;&#26131;&#33719;&#24471;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#31934;&#30830;&#39044;&#27979;&#24615;&#36136;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#20197;&#21450;&#21270;&#23398;&#21453;&#24212;&#24615;&#36136;&#30340;&#39044;&#27979;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2304.03724</link><description>&lt;p&gt;
&#21033;&#29992;&#20301;&#32622;&#21435;&#22122;&#39044;&#27979;&#26131;&#24471;&#20960;&#20309;&#32467;&#26500;&#30340;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Predicting quantum chemical property with easy-to-obtain geometry via positional denoising. (arXiv:2304.03724v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03724
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20301;&#32622;&#21435;&#22122;&#39044;&#27979;&#26131;&#24471;&#20960;&#20309;&#32467;&#26500;&#30340;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;&#65292;&#21487;&#20197;&#29992;&#30456;&#23545;&#23481;&#26131;&#33719;&#24471;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#31934;&#30830;&#39044;&#27979;&#24615;&#36136;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#20197;&#21450;&#21270;&#23398;&#21453;&#24212;&#24615;&#36136;&#30340;&#39044;&#27979;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;&#19982;&#20854;&#20960;&#20309;&#32467;&#26500;&#26377;&#37325;&#35201;&#20851;&#32852;&#65292;&#20351;&#29992;3D&#20960;&#20309;&#20449;&#24687;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#39640;&#32423;&#37327;&#23376;&#21147;&#23398;&#35745;&#31639;&#24471;&#20986;&#30340;3D&#20960;&#20309;&#32467;&#26500;&#65292;&#36825;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#29616;&#23454;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#23481;&#26131;&#33719;&#24471;&#30340;&#20960;&#20309;&#32467;&#26500;&#65288;&#20363;&#22914;&#26469;&#33258;&#20998;&#23376;&#21147;&#22330;&#30340;&#20248;&#21270;&#20960;&#20309;&#32467;&#26500;&#65289;&#31934;&#30830;&#39044;&#27979;&#24615;&#36136;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#36755;&#20837;&#20960;&#20309;&#32467;&#26500;&#36880;&#28176;&#25509;&#36817;&#27491;&#30830;&#20960;&#20309;&#32467;&#26500;&#65292;&#36890;&#36807;&#22534;&#21472;&#21435;&#22122;&#23618;&#12290;&#25105;&#20204;&#20351;&#29992;3D&#28040;&#24687;&#20256;&#36882;&#20307;&#31995;&#32467;&#26500;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#39044;&#27979;&#20219;&#21153;&#65288;&#20998;&#23376;&#24615;&#36136;&#21644;&#21270;&#23398;&#21453;&#24212;&#24615;&#36136;&#65289;&#20013;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#21435;&#22122;&#36807;&#31243;&#20943;&#23569;&#20301;&#32622;&#35823;&#24046;&#26377;&#21161;&#20110;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
As quantum chemical properties have a significant dependence on their geometries, graph neural networks (GNNs) using 3D geometric information have achieved high prediction accuracy in many tasks. However, they often require 3D geometries obtained from high-level quantum mechanical calculations, which are practically infeasible, limiting their applicability in real-world problems. To tackle this, we propose a method to accurately predict the properties with relatively easy-to-obtain geometries (e.g., optimized geometries from the molecular force field). In this method, the input geometry, regarded as the corrupted geometry of the correct one, gradually approaches the correct one as it passes through the stacked denoising layers. We investigated the performance of the proposed method using 3D message-passing architectures for two prediction tasks: molecular properties and chemical reaction property. The reduction of positional errors through the denoising process contributed to performan
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23485;&#20294;&#26377;&#38480;&#30340;&#29305;&#24449;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#20013;&#26377;&#38480;&#23485;&#24230;&#25928;&#24212;&#30340;&#21160;&#21147;&#23398;&#65292;&#25552;&#20379;&#20102;&#23545;&#32593;&#32476;&#26435;&#37325;&#38543;&#26426;&#21021;&#22987;&#21270;&#19979;DMFT&#24207;&#21442;&#25968;&#27874;&#21160;&#30340;&#34920;&#24449;&#20197;&#21450;&#29305;&#24449;&#23398;&#20064;&#22914;&#20309;&#21160;&#24577;&#22320;&#20943;&#23569;&#26368;&#32456;NTK&#21644;&#26368;&#32456;&#32593;&#32476;&#39044;&#27979;&#30340;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2304.03408</link><description>&lt;p&gt;
&#26377;&#38480;&#23485;&#24230;&#26680;&#21644;&#24179;&#22343;&#22330;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#39044;&#27979;&#27874;&#21160;&#21160;&#21147;&#23398;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks. (arXiv:2304.03408v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23485;&#20294;&#26377;&#38480;&#30340;&#29305;&#24449;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#20013;&#26377;&#38480;&#23485;&#24230;&#25928;&#24212;&#30340;&#21160;&#21147;&#23398;&#65292;&#25552;&#20379;&#20102;&#23545;&#32593;&#32476;&#26435;&#37325;&#38543;&#26426;&#21021;&#22987;&#21270;&#19979;DMFT&#24207;&#21442;&#25968;&#27874;&#21160;&#30340;&#34920;&#24449;&#20197;&#21450;&#29305;&#24449;&#23398;&#20064;&#22914;&#20309;&#21160;&#24577;&#22320;&#20943;&#23569;&#26368;&#32456;NTK&#21644;&#26368;&#32456;&#32593;&#32476;&#39044;&#27979;&#30340;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#23485;&#20294;&#26377;&#38480;&#30340;&#29305;&#24449;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#20013;&#26377;&#38480;&#23485;&#24230;&#25928;&#24212;&#30340;&#21160;&#21147;&#23398;&#12290;&#19982;&#35768;&#22810;&#20808;&#21069;&#30340;&#20998;&#26512;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#38024;&#23545;&#29305;&#24449;&#23398;&#20064;&#24378;&#24230;&#30340;&#38750;&#24494;&#25200;&#26377;&#38480;&#23485;&#24230;&#30340;&#32467;&#26524;&#12290;&#20174;&#26080;&#38480;&#23485;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26680;&#21644;&#39044;&#27979;&#21160;&#21147;&#23398;&#30340;&#21160;&#21147;&#23398;&#24179;&#22343;&#22330;&#29702;&#35770;&#65288;DMFT&#65289;&#25551;&#36848;&#24320;&#22987;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#32593;&#32476;&#26435;&#37325;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#19979;DMFT&#24207;&#21442;&#25968;$\mathcal{O}(1/\sqrt{\text{width}})$&#27874;&#21160;&#30340;&#34920;&#24449;&#12290;&#22312;&#32593;&#32476;&#35757;&#32451;&#30340;&#25042;&#24816;&#26497;&#38480;&#20013;&#65292;&#25152;&#26377;&#26680;&#37117;&#26159;&#38543;&#26426;&#30340;&#20294;&#22312;&#26102;&#38388;&#19978;&#38745;&#27490;&#30340;&#65292;&#39044;&#27979;&#26041;&#24046;&#20855;&#26377;&#36890;&#29992;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#23500;&#26377;&#29305;&#24449;&#23398;&#20064;&#30340;&#21306;&#22495;&#65292;&#26680;&#21644;&#39044;&#27979;&#30340;&#27874;&#21160;&#26159;&#21160;&#24577;&#32806;&#21512;&#19988;&#26041;&#24046;&#21487;&#20197;&#34987;&#33258;&#27965;&#35745;&#31639;&#12290;&#22312;&#20004;&#23618;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29305;&#24449;&#23398;&#20064;&#22914;&#20309;&#21160;&#24577;&#22320;&#20943;&#23569;&#26368;&#32456;NTK&#21644;&#26368;&#32456;&#32593;&#32476;&#39044;&#27979;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#36827;&#34892;&#21021;&#22987;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the dynamics of finite width effects in wide but finite feature learning neural networks. Unlike many prior analyses, our results, while perturbative in width, are non-perturbative in the strength of feature learning. Starting from a dynamical mean field theory (DMFT) description of infinite width deep neural network kernel and prediction dynamics, we provide a characterization of the $\mathcal{O}(1/\sqrt{\text{width}})$ fluctuations of the DMFT order parameters over random initialization of the network weights. In the lazy limit of network training, all kernels are random but static in time and the prediction variance has a universal form. However, in the rich, feature learning regime, the fluctuations of the kernels and predictions are dynamically coupled with variance that can be computed self-consistently. In two layer networks, we show how feature learning can dynamically reduce the variance of the final NTK and final network predictions. We also show how initialization
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23383;&#20856;&#23398;&#20064;&#20013;&#20004;&#31181;&#20132;&#26367;&#26497;&#23567;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#22312;&#33391;&#22909;&#30340;&#21021;&#22987;&#21270;&#19979;&#65292;&#36825;&#20004;&#31181;&#31639;&#27861;&#33021;&#22815;&#20197;&#20960;&#20309;&#25910;&#25947;&#36895;&#29575;&#25910;&#25947;&#20110;&#29983;&#25104;&#30340;&#23383;&#20856;&#65292;&#19988;&#21487;&#36866;&#29992;&#20110;&#38750;&#22343;&#21248;&#20998;&#24067;&#30340;&#25968;&#25454;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.01768</link><description>&lt;p&gt;
&#23383;&#20856;&#23398;&#20064;&#20013;&#20132;&#26367;&#26497;&#23567;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Convergence of alternating minimisation algorithms for dictionary learning. (arXiv:2304.01768v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23383;&#20856;&#23398;&#20064;&#20013;&#20004;&#31181;&#20132;&#26367;&#26497;&#23567;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#22312;&#33391;&#22909;&#30340;&#21021;&#22987;&#21270;&#19979;&#65292;&#36825;&#20004;&#31181;&#31639;&#27861;&#33021;&#22815;&#20197;&#20960;&#20309;&#25910;&#25947;&#36895;&#29575;&#25910;&#25947;&#20110;&#29983;&#25104;&#30340;&#23383;&#20856;&#65292;&#19988;&#21487;&#36866;&#29992;&#20110;&#38750;&#22343;&#21248;&#20998;&#24067;&#30340;&#25968;&#25454;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23548;&#20986;&#20102;&#38024;&#23545;&#23383;&#20856;&#23398;&#20064;&#20004;&#31181;&#27969;&#34892;&#30340;&#20132;&#26367;&#26497;&#23567;&#21270;&#31639;&#27861; - &#26368;&#20248;&#26041;&#21521;&#27861;&#65288;MOD&#65289;&#21644;&#22312;&#32447;&#23383;&#20856;&#23398;&#20064;&#65288;ODL&#65289;&#30340;&#25910;&#25947;&#24615;&#36275;&#22815;&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#21482;&#35201;&#21021;&#22987;&#20540;&#33391;&#22909;&#65292;&#21363;&#36317;&#31163;&#29983;&#25104;&#30340;&#23383;&#20856;&#19981;&#36229;&#36807;$1/\log(K)$&#25110;&#20855;&#26377;&#19968;&#23450;&#30340;&#32467;&#26500;&#65292;&#30830;&#20445;&#21021;&#22987;&#20540;&#20013;&#30340;&#27599;&#20010;&#20803;&#32032;&#21482;&#25351;&#21521;&#19968;&#20010;&#29983;&#25104;&#20803;&#65292;&#20004;&#31181;&#31639;&#27861;&#23558;&#20197;&#20960;&#20309;&#25910;&#25947;&#36895;&#29575;&#25910;&#25947;&#20110;&#29983;&#25104;&#30340;&#23383;&#20856;&#12290;&#36825;&#22312;&#20855;&#26377;&#38750;&#22343;&#21248;&#20998;&#24067;&#30340;&#25968;&#25454;&#27169;&#22411;&#19978;&#20063;&#33021;&#23454;&#29616;&#65292;&#35813;&#27169;&#22411;&#20013;&#31232;&#30095;&#31995;&#25968;&#30340;&#25903;&#25745;&#38598;&#30340;&#20986;&#29616;&#39057;&#29575;&#21487;&#20197;&#21464;&#21270;&#24456;&#22823;&#65292;&#20174;&#32780;&#26356;&#25509;&#36817;&#30495;&#23454;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we derive sufficient conditions for the convergence of two popular alternating minimisation algorithms for dictionary learning - the Method of Optimal Directions (MOD) and Online Dictionary Learning (ODL), which can also be thought of as approximative K-SVD. We show that given a well-behaved initialisation that is either within distance at most $1/\log(K)$ to the generating dictionary or has a special structure ensuring that each element of the initialisation only points to one generating element, both algorithms will converge with geometric convergence rate to the generating dictionary. This is done even for data models with non-uniform distributions on the supports of the sparse coefficients. These allow the appearance frequency of the dictionary elements to vary heavily and thus model real data more closely.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#31526;&#21495;&#28040;&#24687;&#20256;&#36882;&#21644;&#20851;&#31995;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#20851;&#31995;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#24863;&#24615;&#29366;&#24577;&#19982;&#25277;&#35937;&#29366;&#24577;&#20043;&#38388;&#30340;&#32465;&#23450;&#12290;</title><link>http://arxiv.org/abs/2304.00195</link><description>&lt;p&gt;
&#25277;&#35937;&#22120;&#65306;&#22522;&#20110;Transformer&#30340;&#31526;&#21495;&#28040;&#24687;&#20256;&#36882;&#21644;&#20851;&#31995;&#25512;&#29702;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Abstractors: Transformer Modules for Symbolic Message Passing and Relational Reasoning. (arXiv:2304.00195v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00195
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#31526;&#21495;&#28040;&#24687;&#20256;&#36882;&#21644;&#20851;&#31995;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#20851;&#31995;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#24863;&#24615;&#29366;&#24577;&#19982;&#25277;&#35937;&#29366;&#24577;&#20043;&#38388;&#30340;&#32465;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#20851;&#31995;&#23398;&#20064;&#36716;&#21270;&#20026;Transformer&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20851;&#31995;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#24863;&#24615;&#29366;&#24577;&#19982;&#25277;&#35937;&#29366;&#24577;&#20043;&#38388;&#30340;&#32465;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
A framework is proposed that casts relational learning in terms of transformers, implementing binding between sensory states and abstract states with relational cross attention mechanisms.
&lt;/p&gt;</description></item><item><title>&#33258;&#25105;&#21453;&#39304;&#36845;&#20195;&#31934;&#28860;&#26159;&#19968;&#31181;&#26080;&#38656;&#30417;&#30563;&#23398;&#20064;&#25110;&#21152;&#24378;&#23398;&#20064;&#30340;LLMs&#21021;&#22987;&#36755;&#20986;&#20248;&#21270;&#26041;&#27861;&#65292;&#20248;&#20110;&#30452;&#25509;&#29983;&#25104;&#65292;&#34987;&#35777;&#23454;&#22312;7&#20010;&#19981;&#21516;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.17651</link><description>&lt;p&gt;
&#33258;&#25105;&#21453;&#39304;&#36845;&#20195;&#31934;&#28860;&#65306;&#19968;&#31181;&#26080;&#38656;&#30417;&#30563;&#23398;&#20064;&#25110;&#21152;&#24378;&#23398;&#20064;&#30340;LM&#25913;&#36827;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Self-Refine: Iterative Refinement with Self-Feedback. (arXiv:2303.17651v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17651
&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#21453;&#39304;&#36845;&#20195;&#31934;&#28860;&#26159;&#19968;&#31181;&#26080;&#38656;&#30417;&#30563;&#23398;&#20064;&#25110;&#21152;&#24378;&#23398;&#20064;&#30340;LLMs&#21021;&#22987;&#36755;&#20986;&#20248;&#21270;&#26041;&#27861;&#65292;&#20248;&#20110;&#30452;&#25509;&#29983;&#25104;&#65292;&#34987;&#35777;&#23454;&#22312;7&#20010;&#19981;&#21516;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19981;&#24635;&#26159;&#33021;&#22312;&#31532;&#19968;&#27425;&#33391;&#22909;&#22320;&#35299;&#20915;&#29983;&#25104;&#38382;&#39064;&#65288;&#22914;&#25688;&#35201;&#12289;&#31572;&#26696;&#12289;&#35299;&#37322;&#31561;&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#33258;&#25105;&#21453;&#39304;&#36845;&#20195;&#31934;&#28860;&#65288;SELF-REFINE&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#21453;&#39304;&#21644;&#31934;&#28860;&#30456;&#20284;&#22320;&#20248;&#21270;LLMs&#30340;&#21021;&#22987;&#36755;&#20986;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#65306;&#20351;&#29992;LLM&#29983;&#25104;&#36755;&#20986;&#65292;&#28982;&#21518;&#20801;&#35768;&#21516;&#19968;&#27169;&#22411;&#25552;&#20379;&#20854;&#33258;&#36523;&#36755;&#20986;&#30340;&#22810;&#26041;&#38754;&#21453;&#39304;&#65292;&#26368;&#21518;&#21033;&#29992;&#21453;&#39304;&#20351;&#30456;&#21516;&#27169;&#22411;&#31934;&#28860;&#20808;&#21069;&#29983;&#25104;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#36845;&#20195;&#31934;&#28860;&#26694;&#26550;&#19982;&#26089;&#26399;&#24037;&#20316;&#19981;&#21516;&#65292;&#26080;&#38656;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#25110;&#21152;&#24378;&#23398;&#20064;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#21333;&#20010;LLM&#19968;&#36215;&#20351;&#29992;&#12290;&#25105;&#20204;&#23545;&#19971;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#33539;&#22260;&#20174;&#35780;&#35770;&#37325;&#20889;&#21040;&#25968;&#23398;&#25512;&#29702;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#30452;&#25509;&#29983;&#25104;&#12290;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;SELF-REFINE&#29983;&#25104;&#30340;&#36755;&#20986;&#34987;&#20154;&#31867;&#21644;&#33258;&#21160;&#21270;&#25351;&#26631;&#20248;&#20808;&#20110;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#30452;&#25509;&#29983;&#25104;&#30340;&#36755;&#20986;&#65292;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Like people, LLMs do not always generate the best text for a given generation problem on their first try (e.g., summaries, answers, explanations). Just as people then refine their text, we introduce SELF-REFINE, a framework for similarly improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an output using an LLM, then allow the same model to provide multi-aspect feedback for its own output; finally, the same model refines its previously generated output given its own feedback. Unlike earlier work, our iterative refinement framework does not require supervised training data or reinforcement learning, and works with a single LLM. We experiment with 7 diverse tasks, ranging from review rewriting to math reasoning, demonstrating that our approach outperforms direct generation. In all tasks, outputs generated with SELF-REFINE are preferred by humans and by automated metrics over those generated directly with GPT-3.5 and GPT-4, improving
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;&#40657;&#30418;&#21464;&#20998;&#25512;&#29702;&#65288;BBVI&#65289;&#28385;&#36275;SGD&#25991;&#29486;&#20013;&#30340;ABC&#26465;&#20214;&#65292;&#35813;&#32467;&#26524;&#36866;&#29992;&#20110;&#24179;&#28369;&#21644;&#20108;&#27425;&#22686;&#38271;&#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#65292;&#21516;&#26102;&#25105;&#20204;&#30340;&#32467;&#26524;&#25512;&#24191;&#21040;&#24191;&#27867;&#24212;&#29992;&#20110;BBVI&#23454;&#36341;&#20013;&#30340;&#38750;&#32447;&#24615;&#21327;&#26041;&#24046;&#21442;&#25968;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.10472</link><description>&lt;p&gt;
&#40657;&#30418;&#21464;&#20998;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#23454;&#29992;&#21305;&#37197;&#26799;&#24230;&#26041;&#24046;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Practical and Matching Gradient Variance Bounds for Black-Box Variational Bayesian Inference. (arXiv:2303.10472v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#40657;&#30418;&#21464;&#20998;&#25512;&#29702;&#65288;BBVI&#65289;&#28385;&#36275;SGD&#25991;&#29486;&#20013;&#30340;ABC&#26465;&#20214;&#65292;&#35813;&#32467;&#26524;&#36866;&#29992;&#20110;&#24179;&#28369;&#21644;&#20108;&#27425;&#22686;&#38271;&#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#65292;&#21516;&#26102;&#25105;&#20204;&#30340;&#32467;&#26524;&#25512;&#24191;&#21040;&#24191;&#27867;&#24212;&#29992;&#20110;BBVI&#23454;&#36341;&#20013;&#30340;&#38750;&#32447;&#24615;&#21327;&#26041;&#24046;&#21442;&#25968;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#40657;&#30418;&#21464;&#20998;&#25512;&#29702;&#65288;BBVI&#65289;&#30340;&#26799;&#24230;&#26041;&#24046;&#26159;&#24314;&#31435;&#20854;&#25910;&#25947;&#24615;&#21644;&#31639;&#27861;&#25913;&#36827;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#23578;&#26410;&#34920;&#26126;BBVI&#30340;&#26799;&#24230;&#26041;&#24046;&#28385;&#36275;&#29992;&#20110;&#30740;&#31350;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#25910;&#25947;&#30340;&#26465;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#24212;&#29992;&#20110;&#24179;&#28369;&#21644;&#20108;&#27425;&#22686;&#38271;&#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#26102;&#65292;BBVI&#28385;&#36275;&#19982;SGD&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;ABC&#26465;&#20214;&#30456;&#21305;&#37197;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25512;&#24191;&#21040;&#24191;&#27867;&#24212;&#29992;&#20110;BBVI&#23454;&#36341;&#20013;&#30340;&#38750;&#32447;&#24615;&#21327;&#26041;&#24046;&#21442;&#25968;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#24179;&#22343;&#22330;&#21442;&#25968;&#21270;&#30340;&#26041;&#24046;&#20855;&#26377;&#32463;&#36807;&#39564;&#35777;&#30340;&#20248;&#36234;&#32500;&#24230;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the gradient variance of black-box variational inference (BBVI) is a crucial step for establishing its convergence and developing algorithmic improvements. However, existing studies have yet to show that the gradient variance of BBVI satisfies the conditions used to study the convergence of stochastic gradient descent (SGD), the workhorse of BBVI. In this work, we show that BBVI satisfies a matching bound corresponding to the $ABC$ condition used in the SGD literature when applied to smooth and quadratically-growing log-likelihoods. Our results generalize to nonlinear covariance parameterizations widely used in the practice of BBVI. Furthermore, we show that the variance of the mean-field parameterization has provably superior dimensional dependence.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#25913;&#21892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#36755;&#20986;&#30340;ASR&#38169;&#35823;&#32416;&#27491;&#65288;EC&#65289;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;&#34701;&#20837;&#35270;&#35273;&#20449;&#24687;&#65292;&#20351;&#29992;&#38376;&#25511;&#34701;&#21512;&#21644;&#22270;&#20687;&#26631;&#39064;&#20316;&#20026;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;EC&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;Visual-ASR-EC&#12290;</title><link>http://arxiv.org/abs/2303.10160</link><description>&lt;p&gt;
&#35270;&#35273;&#20449;&#24687;&#23545;ASR&#38169;&#35823;&#32416;&#27491;&#38750;&#24120;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Visual Information Matters for ASR Error Correction. (arXiv:2303.10160v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10160
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#25913;&#21892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#36755;&#20986;&#30340;ASR&#38169;&#35823;&#32416;&#27491;&#65288;EC&#65289;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;&#34701;&#20837;&#35270;&#35273;&#20449;&#24687;&#65292;&#20351;&#29992;&#38376;&#25511;&#34701;&#21512;&#21644;&#22270;&#20687;&#26631;&#39064;&#20316;&#20026;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;EC&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;Visual-ASR-EC&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#36755;&#20986;&#65292;ASR&#38169;&#35823;&#32416;&#27491;&#65288;EC&#65289;&#25216;&#26415;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#24320;&#21457;&#65292;&#22240;&#20854;&#20351;&#29992;&#24182;&#34892;&#25991;&#26412;&#25968;&#25454;&#30340;&#25928;&#29575;&#39640;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#25991;&#26412;&#25110;/&#21644;&#35821;&#38899;&#25968;&#25454;&#19978;&#65292;&#36825;&#22312;&#27809;&#26377;&#20165;&#26377;&#25991;&#26412;&#21644;&#35821;&#38899;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#38480;&#21046;&#20102;&#24615;&#33021;&#25552;&#21319;&#65307;&#32780;&#35270;&#35273;&#20449;&#24687;&#31561;&#20854;&#20182;&#24418;&#24335;&#30340;&#25968;&#25454;&#24456;&#23569;&#34987;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20123;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21363;&#38376;&#25511;&#34701;&#21512;&#21644;&#22270;&#20687;&#26631;&#39064;&#20316;&#20026;&#25552;&#31034;&#65292;&#29992;&#20110;&#23558;&#35270;&#35273;&#20449;&#24687;&#34701;&#20837;EC&#20013;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;Visual-ASR-EC&#65292;&#20854;&#20013;&#27599;&#20010;&#39033;&#30446;&#30340;&#35757;&#32451;&#25968;&#25454;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#38899;&#21644;&#25991;&#26412;&#20449;&#24687;&#65292;&#27979;&#35797;&#25968;&#25454;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Aiming to improve the Automatic Speech Recognition (ASR) outputs with a post-processing step, ASR error correction (EC) techniques have been widely developed due to their efficiency in using parallel text data. Previous works mainly focus on using text or/ and speech data, which hinders the performance gain when not only text and speech information, but other modalities, such as visual information are critical for EC. The challenges are mainly two folds: one is that previous work fails to emphasize visual information, thus rare exploration has been studied. The other is that the community lacks a high-quality benchmark where visual information matters for the EC models. Therefore, this paper provides 1) simple yet effective methods, namely gated fusion and image captions as prompts to incorporate visual information to help EC; 2) large-scale benchmark datasets, namely Visual-ASR-EC, where each item in the training data consists of visual, speech, and text information, and the test data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09716</link><description>&lt;p&gt;
&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#25919;&#31574;&#36845;&#20195;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A New Policy Iteration Algorithm For Reinforcement Learning in Zero-Sum Markov Games. (arXiv:2303.09716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#34987;&#35270;&#20026;&#20855;&#26377;&#20004;&#20010;&#38454;&#27573;: &#23398;&#20064;&#38454;&#27573;&#21644;&#35268;&#21010;&#38454;&#27573;&#12290;&#22312;&#26631;&#20934;MDPs&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20351;&#29992;&#20215;&#20540;&#36845;&#20195;&#25110;&#31574;&#30053;&#36845;&#20195;&#26469;&#35299;&#20915;&#23398;&#20064;&#38382;&#39064;&#12290;&#20294;&#22312;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#24773;&#20917;&#19979;&#65292;&#27809;&#26377;&#26377;&#25928;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#65292;&#20197;&#21069;&#30340;&#23581;&#35797;&#37117;&#26377;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31574;&#30053;&#36845;&#20195;&#21464;&#20307;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many model-based reinforcement learning (RL) algorithms can be viewed as having two phases that are iteratively implemented: a learning phase where the model is approximately learned and a planning phase where the learned model is used to derive a policy. In the case of standard MDPs, the learning problem can be solved using either value iteration or policy iteration. However, in the case of zero-sum Markov games, there is no efficient policy iteration algorithm; e.g., it has been shown in Hansen et al. (2013) that one has to solve Omega(1/(1-alpha)) MDPs, where alpha is the discount factor, to implement the only known convergent version of policy iteration. Another algorithm for Markov zero-sum games, called naive policy iteration, is easy to implement but is only provably convergent under very restrictive assumptions. Prior attempts to fix naive policy iteration algorithm have several limitations. Here, we show that a simple variant of naive policy iteration for games converges, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#25968;&#19975;&#20010;&#38646;-shot&#23454;&#39564;&#23545;&#22522;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21516;&#37327;&#21270;&#32452;&#20214;&#36827;&#34892;&#20102;&#32508;&#21512;&#30740;&#31350;&#65292;&#32467;&#26524;&#21457;&#29616;&#32454;&#31890;&#24230;&#37327;&#21270;&#21644;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#24456;&#37325;&#35201;&#65292;&#29992;&#31895;&#31890;&#24230;&#37327;&#21270;&#30340;&#26356;&#39640;&#20301;&#25968;&#27604;&#29992;&#38750;&#24120;&#32454;&#31890;&#24230;&#30340;&#26356;&#20302;&#20301;&#25968;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#22914;&#20309;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;\llms&#21033;&#29992;&#37327;&#21270;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2303.08302</link><description>&lt;p&gt;
&#22522;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study on Post-Training Quantization for Large Language Models. (arXiv:2303.08302v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#25968;&#19975;&#20010;&#38646;-shot&#23454;&#39564;&#23545;&#22522;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21516;&#37327;&#21270;&#32452;&#20214;&#36827;&#34892;&#20102;&#32508;&#21512;&#30740;&#31350;&#65292;&#32467;&#26524;&#21457;&#29616;&#32454;&#31890;&#24230;&#37327;&#21270;&#21644;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#24456;&#37325;&#35201;&#65292;&#29992;&#31895;&#31890;&#24230;&#37327;&#21270;&#30340;&#26356;&#39640;&#20301;&#25968;&#27604;&#29992;&#38750;&#24120;&#32454;&#31890;&#24230;&#30340;&#26356;&#20302;&#20301;&#25968;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#22914;&#20309;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;\llms&#21033;&#29992;&#37327;&#21270;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#26159;&#19968;&#31181;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#23384;&#28040;&#32791;&#21644;/&#25110;&#35745;&#31639;&#25104;&#26412;&#30340;&#26435;&#34913;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#19981;&#21516;&#37327;&#21270;&#26041;&#26696;&#12289;&#19981;&#21516;&#27169;&#22411;&#26063;&#12289;&#19981;&#21516;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12289;&#19981;&#21516;&#37327;&#21270;&#20301;&#31934;&#24230;&#31561;&#30340;&#24433;&#21709;&#30340;&#20840;&#38754;&#30740;&#31350;&#20173;&#32570;&#22833;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#19975;&#20010;&#38646;-shot&#23454;&#39564;&#23545;&#36825;&#20123;&#32452;&#20214;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;(1)&#32454;&#31890;&#24230;&#37327;&#21270;&#21644;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;(&#32780;&#19981;&#26159;&#26420;&#32032;&#30340;&#26368;&#36817;&#33293;&#20837;&#37327;&#21270;)&#26159;&#23454;&#29616;&#33391;&#22909;&#31934;&#24230;&#30340;&#24517;&#35201;&#26465;&#20214;&#65307;(2) &#29992;&#31895;&#31890;&#24230;&#37327;&#21270;&#30340;&#26356;&#39640;&#20301;&#25968;&#65288;&#22914;5&#20301;&#65289;&#27604;&#29992;&#38750;&#24120;&#32454;&#31890;&#24230;&#30340;&#26356;&#20302;&#20301;&#25968;&#65288;&#22914;4&#20301;&#65289;&#65288;&#20854;&#26377;&#25928;&#20301;&#25968;&#19982;5&#20301;&#30456;&#20284;&#65289;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22914;&#20309;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;\llms&#21033;&#29992;&#37327;&#21270;&#30340;&#24314;&#35758;&#65292;&#24182;&#30041;&#19979;&#26410;&#26469;&#26426;&#20250;&#21644;&#31995;&#32479;&#24037;&#20316;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (\ptq) had been recently shown as a compromising method to reduce the memory consumption and/or compute cost for large language models. However, a comprehensive study about the effect of different quantization schemes, different model families, different \ptq methods, different quantization bit precision, etc, is still missing. In this work, we provide an extensive study on those components over tens of thousands of zero-shot experiments. Our results show that (1) Fine-grained quantization and \ptq methods (instead of naive round-to-nearest quantization) are necessary to achieve good accuracy and (2) Higher bits (e.g., 5 bits) with coarse-grained quantization is more powerful than lower bits (e.g., 4 bits) with very fine-grained quantization (whose effective bits is similar to 5-bits). We also present recommendations about how to utilize quantization for \llms with different sizes, and leave suggestions of future opportunities and system work that are not res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Vision Transformer&#20013;&#23398;&#20064;&#25104;&#38271;&#20154;&#24037;&#28023;&#39532;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#24377;&#24615;&#32456;&#36523;&#23398;&#20064;&#12290;&#36890;&#36807;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#36827;&#34892;&#32500;&#25252;&#65292;&#36873;&#21462;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#22359;&#20013;&#30340;&#26368;&#32456;&#32447;&#24615;&#25237;&#24433;&#23618;&#36827;&#34892;ArtiHippo&#30340;&#23454;&#29616;&#21644;&#25104;&#38271;&#12290;</title><link>http://arxiv.org/abs/2303.08250</link><description>&lt;p&gt;
&#22312;&#35270;&#35273;Transformer&#20013;&#23398;&#20064;&#25104;&#38271;&#20154;&#24037;&#28023;&#39532;&#65292;&#23454;&#29616;&#24377;&#24615;&#32456;&#36523;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Grow Artificial Hippocampi in Vision Transformers for Resilient Lifelong Learning. (arXiv:2303.08250v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Vision Transformer&#20013;&#23398;&#20064;&#25104;&#38271;&#20154;&#24037;&#28023;&#39532;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#24377;&#24615;&#32456;&#36523;&#23398;&#20064;&#12290;&#36890;&#36807;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#36827;&#34892;&#32500;&#25252;&#65292;&#36873;&#21462;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#22359;&#20013;&#30340;&#26368;&#32456;&#32447;&#24615;&#25237;&#24433;&#23618;&#36827;&#34892;ArtiHippo&#30340;&#23454;&#29616;&#21644;&#25104;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#23398;&#20064;&#38656;&#35201;&#25317;&#26377;&#20154;&#31867;&#26234;&#33021;&#30340;&#38887;&#24615;&#65292;&#21363;&#19981;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36825;&#31181;&#38887;&#24615;&#19982;&#22823;&#33041;&#20013;&#22797;&#26434;&#30340;&#35760;&#24518;&#26426;&#21046;&#65292;&#23588;&#20854;&#26159;&#28023;&#39532;&#32500;&#25252;&#30340;&#38271;&#26399;&#35760;&#24518;&#65288;LM&#65289;&#32039;&#23494;&#30456;&#20851;&#12290;Transformer&#24050;&#32463;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#8220;&#22823;&#33041;&#8221;&#30340;&#23545;&#24212;&#20307;&#65292;&#20294;LM&#32452;&#20214;&#22312;&#32456;&#36523;&#23398;&#20064;&#20013;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Vision Transformer&#20013;&#23398;&#20064;&#25104;&#38271;&#20154;&#24037;&#28023;&#39532;&#65288;ArtiHippo&#65289;&#20197;&#23454;&#29616;&#24377;&#24615;&#32456;&#36523;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20840;&#38754;&#28040;&#34701;&#23454;&#39564;&#65292;&#36873;&#23450;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#65288;MHSA&#65289;&#22359;&#20013;&#30340;&#26368;&#32456;&#32447;&#24615;&#25237;&#24433;&#23618;&#26469;&#23454;&#29616;&#21644;&#25104;&#38271;ArtiHippo&#12290;ArtiHippo&#30001;&#19987;&#23478;&#28151;&#21512;&#65288;MoEs&#65289;&#34920;&#31034;&#12290;&#27599;&#20010;&#19987;&#23478;&#32452;&#20214;&#26159;&#32447;&#24615;&#25237;&#24433;&#23618;&#30340;&#29616;&#22330;&#21464;&#20307;&#65292;&#36890;&#36807;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#36827;&#34892;&#32500;&#25252;&#65292;&#25628;&#32034;&#31354;&#38388;&#30001;&#22235;&#20010;&#22522;&#26412;&#25104;&#38271;&#25805;&#20316;&#65288;&#36339;&#36807;&#12289;&#37325;&#29992;&#12289;&#36866;&#24212;&#21644;&#26032;&#65289;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelong learning without catastrophic forgetting (i.e., resiliency) possessed by human intelligence is entangled with sophisticated memory mechanisms in the brain, especially the long-term memory (LM) maintained by Hippocampi. To a certain extent, Transformers have emerged as the counterpart ``Brain" of Artificial Intelligence (AI), and yet leave the LM component under-explored for lifelong learning settings. This paper presents a method of learning to grow Artificial Hippocampi (ArtiHippo) in Vision Transformers (ViTs) for resilient lifelong learning. With a comprehensive ablation study, the final linear projection layer in the multi-head self-attention (MHSA) block is selected in realizing and growing ArtiHippo. ArtiHippo is represented by a mixture of experts (MoEs). Each expert component is an on-site variant of the linear projection layer, maintained via neural architecture search (NAS) with the search space defined by four basic growing operations -- skip, reuse, adapt, and new 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GOATS&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#30446;&#26631;&#37319;&#26679;&#33258;&#36866;&#24212;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#25554;&#20540;&#20301;&#32622;&#30446;&#26631;&#21644;&#25968;&#37327;&#30446;&#26631;&#30340;&#20998;&#24067;&#21019;&#24314;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#35838;&#31243;&#26469;&#35299;&#20915;&#26426;&#22120;&#20154;&#33280;&#21462;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#30446;&#26631;&#21644;&#27700;&#37327;&#30446;&#26631;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#27604;&#22522;&#32447;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.05193</link><description>&lt;p&gt;
GOATS&#65306;&#30446;&#26631;&#37319;&#26679;&#33258;&#36866;&#24212;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33280;&#21462;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
GOATS: Goal Sampling Adaptation for Scooping with Curriculum Reinforcement Learning. (arXiv:2303.05193v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GOATS&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#30446;&#26631;&#37319;&#26679;&#33258;&#36866;&#24212;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#25554;&#20540;&#20301;&#32622;&#30446;&#26631;&#21644;&#25968;&#37327;&#30446;&#26631;&#30340;&#20998;&#24067;&#21019;&#24314;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#35838;&#31243;&#26469;&#35299;&#20915;&#26426;&#22120;&#20154;&#33280;&#21462;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#30446;&#26631;&#21644;&#27700;&#37327;&#30446;&#26631;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#27604;&#22522;&#32447;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#20808;&#20351;&#29992;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#23545;&#26426;&#22120;&#20154;&#33280;&#21462;&#27700;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#38416;&#36848;&#12290;&#30001;&#20110;&#27969;&#20307;&#30340;&#22797;&#26434;&#21160;&#21147;&#23398;&#21644;&#23454;&#29616;&#22810;&#27169;&#24335;&#30446;&#26631;&#30340;&#38656;&#27714;&#65292;&#35813;&#20219;&#21153;&#20855;&#26377;&#29305;&#21035;&#30340;&#25361;&#25112;&#24615;&#12290;&#25919;&#31574;&#38656;&#35201;&#25104;&#21151;&#22320;&#36798;&#21040;&#20301;&#32622;&#30446;&#26631;&#21644;&#27700;&#37327;&#30446;&#26631;&#65292;&#36825;&#23548;&#33268;&#19968;&#20010;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#30446;&#26631;&#29366;&#24577;&#31354;&#38388;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GOATS&#65292;&#19968;&#31181;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25554;&#20540;&#20301;&#32622;&#30446;&#26631;&#20998;&#24067;&#21644;&#25968;&#37327;&#30446;&#26631;&#20998;&#24067;&#26469;&#21019;&#24314;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#35838;&#31243;&#65292;&#20351;&#29992;&#30446;&#26631;&#20998;&#35299;&#22870;&#21169;&#20844;&#24335;&#65292;&#23398;&#20064;&#19968;&#20010;&#39640;&#25928;&#19988;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;&#26426;&#22120;&#20154;&#33280;&#21462;&#31574;&#30053;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20223;&#30495;&#20013;&#34920;&#29616;&#20986;&#27604;&#22522;&#32447;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20998;&#21035;&#22312;&#30871;&#33280;&#21644;&#26742;&#33280;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;5.46&#65285;&#21644;8.71&#65285;&#30340;&#35823;&#24046;&#65292;&#28085;&#30422;&#20102;1000&#31181;&#21021;&#22987;&#27700;&#29366;&#24577;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we first formulate the problem of robotic water scooping using goal-conditioned reinforcement learning. This task is particularly challenging due to the complex dynamics of fluid and the need to achieve multi-modal goals. The policy is required to successfully reach both position goals and water amount goals, which leads to a large convoluted goal state space. To overcome these challenges, we introduce Goal Sampling Adaptation for Scooping (GOATS), a curriculum reinforcement learning method that can learn an effective and generalizable policy for robot scooping tasks. Specifically, we use a goal-factorized reward formulation and interpolate position goal distributions and amount goal distributions to create curriculum throughout the learning process. As a result, our proposed method can outperform the baselines in simulation and achieves 5.46% and 8.71% amount errors on bowl scooping and bucket scooping tasks, respectively, under 1000 variations of initial water states in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22810;&#20803;Edgeworth&#23637;&#24320;&#65292;&#25552;&#20986;&#29992;&#24494;&#20998;&#24418;&#24335;&#34920;&#31034;&#38750;&#39640;&#26031;&#20998;&#24067;&#65292;&#26469;&#27169;&#25311;&#26377;&#38480;&#23485;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#39640;&#26031;&#21518;&#39564;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2303.02859</link><description>&lt;p&gt;
&#26377;&#38480;&#23485;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Bayesian inference with finitely wide neural networks. (arXiv:2303.02859v2 [cond-mat.dis-nn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22810;&#20803;Edgeworth&#23637;&#24320;&#65292;&#25552;&#20986;&#29992;&#24494;&#20998;&#24418;&#24335;&#34920;&#31034;&#38750;&#39640;&#26031;&#20998;&#24067;&#65292;&#26469;&#27169;&#25311;&#26377;&#38480;&#23485;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#39640;&#26031;&#21518;&#39564;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#23558;&#23485;&#24230;&#24456;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#35270;&#20026;&#36125;&#21494;&#26031;&#35774;&#32622;&#20013;&#30340;&#39640;&#26031;&#36807;&#31243;&#26102;&#65292;&#35299;&#26512;&#25512;&#26029;&#65292;&#20363;&#22914;&#39044;&#27979;&#20998;&#24067;&#20197;&#38381;&#21512;&#24418;&#24335;&#32473;&#20986;&#65292;&#21487;&#33021;&#26159;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#20248;&#21183;&#12290;&#20294;&#26159;&#65292;&#23454;&#38469;&#30340;&#23485;&#24230;&#26159;&#26377;&#38480;&#30340;&#65292;&#24182;&#19988;&#22312;&#35813;&#23485;&#24230;&#19979;&#65292;&#19968;&#20123;&#38543;&#26426;&#21464;&#37327;&#30340;&#36793;&#38469;&#21270;&#30340;&#39640;&#26031;&#20551;&#35774;&#21487;&#33021;&#20986;&#29616;&#20559;&#24046;&#12290;&#22522;&#20110;&#22810;&#20803;Edgeworth&#23637;&#24320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#24494;&#20998;&#24418;&#24335;&#34920;&#31034;&#30340;&#38750;&#39640;&#26031;&#20998;&#24067;&#65292;&#26469;&#23545;&#26469;&#33258;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#38480;&#36755;&#20986;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#25512;&#23548;&#20986;&#30456;&#24212;&#30340;&#36793;&#38469;&#21644;&#26465;&#20214;&#23646;&#24615;&#65292;&#20174;&#32780;&#33021;&#22815;&#22312;&#36125;&#21494;&#26031;&#22238;&#24402;&#20219;&#21153;&#20013;&#25512;&#23548;&#20986;&#38750;&#39640;&#26031;&#21518;&#39564;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#22312;&#29942;&#39048;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#36890;&#36807;&#36793;&#32536;&#26680;&#25506;&#31350;&#20102;&#28145;&#39640;&#26031;&#36807;&#31243;&#30340;&#38750;&#39640;&#26031;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The analytic inference, e.g. predictive distribution being in closed form, may be an appealing benefit for machine learning practitioners when they treat wide neural networks as Gaussian process in Bayesian setting. The realistic widths, however, are finite and cause weak deviation from the Gaussianity under which partial marginalization of random variables in a model is straightforward. On the basis of multivariate Edgeworth expansion, we propose a non-Gaussian distribution in differential form to model a finite set of outputs from a random neural network, and derive the corresponding marginal and conditional properties. Thus, we are able to derive the non-Gaussian posterior distribution in Bayesian regression task. In addition, in the bottlenecked deep neural networks, a weight space representation of deep Gaussian process, the non-Gaussianity is investigated through the marginal kernel.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24187;&#24819;&#23545;&#25239;&#25511;&#21046;&#30340;HAMBO&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65292;&#24182;&#19988;&#33021;&#22815;&#24471;&#20986;&#26377;&#25928;&#30340;&#31574;&#30053;&#34920;&#29616;&#19979;&#38480;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.01076</link><description>&lt;p&gt;
&#22522;&#20110;&#24187;&#24819;&#23545;&#25239;&#25511;&#21046;&#30340;&#20445;&#23432;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Hallucinated Adversarial Control for Conservative Offline Policy Evaluation. (arXiv:2303.01076v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24187;&#24819;&#23545;&#25239;&#25511;&#21046;&#30340;HAMBO&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65292;&#24182;&#19988;&#33021;&#22815;&#24471;&#20986;&#26377;&#25928;&#30340;&#31574;&#30053;&#34920;&#29616;&#19979;&#38480;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20445;&#23432;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#65292;&#23545;&#20110;&#32473;&#23450;&#20854;&#20182;&#20195;&#29702;&#25910;&#38598;&#30340;&#31163;&#32447;&#29615;&#22659;&#20132;&#20114;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#26088;&#22312;&#33719;&#24471;&#19968;&#20010;&#20851;&#20110;&#31574;&#30053;&#24615;&#33021;&#30340;(&#32039;)&#19979;&#38480;&#20272;&#35745;&#12290;&#36825;&#22312;&#20915;&#23450;&#26159;&#21542;&#37096;&#32626;&#26576;&#20010;&#31574;&#30053;&#28385;&#36275;&#26368;&#23567;&#24615;&#33021;/&#23433;&#20840;&#26631;&#20934;&#20043;&#21069;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;HAMBO&#65292;&#23427;&#24314;&#31435;&#22312;&#19968;&#20010;&#23398;&#20064;&#21040;&#30340;&#20256;&#36882;&#21160;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#20043;&#19978;&#12290;&#20026;&#20102;&#24418;&#25104;&#31574;&#30053;&#32489;&#25928;&#30340;&#20445;&#23432;&#20272;&#35745;&#65292;HAMBO&#20250;&#24187;&#24819;&#31574;&#30053;&#21487;&#33021;&#37319;&#21462;&#30340;&#26368;&#22351;&#36712;&#36857;&#65292;&#19988;&#35813;&#36712;&#36857;&#22312;&#27169;&#22411;&#30340;&#35748;&#30693;&#32622;&#20449;&#21306;&#38388;&#20869;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#32467;&#26524;&#30340;COPE&#20272;&#35745;&#26159;&#26377;&#25928;&#30340;&#19979;&#38480;&#65292;&#24182;&#22312;&#27491;&#21017;&#24615;&#26465;&#20214;&#19979;&#23637;&#31034;&#20854;&#25910;&#25947;&#20110;&#30495;&#23454;&#30340;&#39044;&#26399;&#22238;&#25253;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22522;&#20110;Bayesian&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#25193;&#23637;&#21464;&#20307;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#23427;&#20204;&#20135;&#29983;&#21487;&#38752;&#19988;&#32039;&#23494;&#30340;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of conservative off-policy evaluation (COPE) where given an offline dataset of environment interactions, collected by other agents, we seek to obtain a (tight) lower bound on a policy's performance. This is crucial when deciding whether a given policy satisfies certain minimal performance/safety criteria before it can be deployed in the real world. To this end, we introduce HAMBO, which builds on an uncertainty-aware learned model of the transition dynamics. To form a conservative estimate of the policy's performance, HAMBO hallucinates worst-case trajectories that the policy may take, within the margin of the models' epistemic confidence regions. We prove that the resulting COPE estimates are valid lower bounds, and, under regularity conditions, show their convergence to the true expected return. Finally, we discuss scalable variants of our approach based on Bayesian Neural Networks and empirically demonstrate that they yield reliable and tight lower bounds in var
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#20351;&#29992;&#36125;&#21494;&#26031;&#20869;&#26680;&#24352;&#37327;&#20998;&#35299;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#20855;&#26377;&#22797;&#26434;&#29305;&#24449;&#30340;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2302.14510</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#20869;&#26680;&#24352;&#37327;&#20998;&#35299;&#20316;&#20026;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26367;&#20195;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bayesian Kernelized Tensor Factorization as Surrogate for Bayesian Optimization. (arXiv:2302.14510v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#20351;&#29992;&#36125;&#21494;&#26031;&#20869;&#26680;&#24352;&#37327;&#20998;&#35299;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#20855;&#26377;&#22797;&#26434;&#29305;&#24449;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#20316;&#20026;&#20027;&#35201;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#22823;&#22810;&#20351;&#29992;&#31616;&#21333;&#30340;&#22266;&#23450;&#21644;&#21487;&#20998;&#31163;&#30340;&#20869;&#26680;&#20989;&#25968;&#65292;&#20363;&#22914;&#20855;&#26377;&#33258;&#21160;&#30456;&#20851;&#20915;&#23450;&#65288;SE-ARD&#65289;&#30340;&#24179;&#26041;&#25351;&#25968;&#20869;&#26680;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#31616;&#21333;&#20869;&#26680;&#35268;&#26684;&#35828;&#26126;&#19981;&#36275;&#20197;&#23398;&#20064;&#20855;&#26377;&#22797;&#26434;&#29305;&#24449;&#30340;&#20989;&#25968;&#65292;&#20363;&#22914;&#38750;&#23450;&#24120;&#65292;&#38750;&#21487;&#20998;&#31163;&#21644;&#22810;&#23792;&#12290;&#21363;&#20351;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#65292;&#20351;&#29992;&#23616;&#37096;GP&#36924;&#36817;&#36825;&#26679;&#30340;&#20989;&#25968;&#20063;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#65292;&#26356;&#19981;&#29992;&#35828;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#20102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#36125;&#21494;&#26031;&#20869;&#26680;&#24352;&#37327;&#20998;&#35299;&#65288;BKTF&#65289;&#20316;&#20026;$ D $&#32500;&#31515;&#21345;&#23572;&#20056;&#31215;&#31354;&#38388;&#20013; BO &#30340;&#26032;&#20195;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#20351;&#29992;&#20840;&#36125;&#21494;&#26031;&#20302;&#31209;&#24352;&#37327; CP &#20998;&#35299;&#36817;&#20284;&#22522;&#30784;&#30340; $ D $ &#32500;&#23454;&#20307;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#20026;&#27599;&#20010;&#32500;&#24230;&#30340;&#28508;&#22312;&#22522;&#30784;&#20989;&#25968;&#25918;&#32622; GP &#20808;&#39564;&#65292;&#20197;&#32534;&#30721;&#23616;&#37096;&#19968;&#33268;&#24615;&#21644;&#24179;&#28369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) primarily uses Gaussian processes (GP) as the key surrogate model, mostly with a simple stationary and separable kernel function such as the squared-exponential kernel with automatic relevance determination (SE-ARD). However, such simple kernel specifications are deficient in learning functions with complex features, such as being nonstationary, nonseparable, and multimodal. Approximating such functions using a local GP, even in a low-dimensional space, requires a large number of samples, not to mention in a high-dimensional setting. In this paper, we propose to use Bayesian Kernelized Tensor Factorization (BKTF) -- as a new surrogate model -- for BO in a $D$-dimensional Cartesian product space. Our key idea is to approximate the underlying $D$-dimensional solid with a fully Bayesian low-rank tensor CP decomposition, in which we place GP priors on the latent basis functions for each dimension to encode local consistency and smoothness. With this formulation, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#21270;&#21442;&#25968;&#35009;&#21098;&#31574;&#30053;&#26469;&#23454;&#29616;&#33410;&#33021;&#28145;&#24230;&#23398;&#20064;&#65292;&#21487;&#20197;&#21457;&#29616;&#26368;&#20339;&#30340;&#38745;&#24577;&#23376;&#32593;&#32476;&#65292;&#21253;&#21547;&#20108;&#20540;&#38376;&#25511;&#27169;&#22359;&#21644;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#22312;&#24050;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.10798</link><description>&lt;p&gt;
&#36731;&#37327;&#21270;&#21442;&#25968;&#35009;&#21098;&#20197;&#23454;&#29616;&#33410;&#33021;&#28145;&#24230;&#23398;&#20064;: &#19968;&#31181;&#20108;&#20540;&#38376;&#25511;&#27169;&#22359;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Lightweight Parameter Pruning for Energy-Efficient Deep Learning: A Binarized Gating Module Approach. (arXiv:2302.10798v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#21270;&#21442;&#25968;&#35009;&#21098;&#31574;&#30053;&#26469;&#23454;&#29616;&#33410;&#33021;&#28145;&#24230;&#23398;&#20064;&#65292;&#21487;&#20197;&#21457;&#29616;&#26368;&#20339;&#30340;&#38745;&#24577;&#23376;&#32593;&#32476;&#65292;&#21253;&#21547;&#20108;&#20540;&#38376;&#25511;&#27169;&#22359;&#21644;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#22312;&#24050;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#22823;&#19988;&#26356;&#21152;&#22797;&#26434;&#65292;&#32511;&#33394;AI&#24050;&#32463;&#24341;&#36215;&#20102;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#37319;&#29992;&#23545;&#32593;&#32476;&#21442;&#25968;&#36827;&#34892;&#35009;&#21098;&#65292;&#20197;&#20943;&#23569;&#35757;&#32451;&#25512;&#26029;&#26102;&#30340;&#35745;&#31639;&#36127;&#33655;&#12290;&#28982;&#32780;&#65292;&#35009;&#21098;&#26041;&#26696;&#36890;&#24120;&#20250;&#23548;&#33268;&#39069;&#22806;&#30340;&#24320;&#38144;&#65292;&#22240;&#20026;&#38656;&#35201;&#36827;&#34892;&#36845;&#20195;&#35757;&#32451;&#21644;&#24494;&#35843;&#25110;&#37325;&#22797;&#35745;&#31639;&#21160;&#24577;&#35009;&#21098;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#35009;&#21098;&#31574;&#30053;&#65292;&#20197;&#23398;&#20064;&#36731;&#37327;&#32423;&#23376;&#32593;&#32476;&#65292;&#26082;&#33021;&#26368;&#23567;&#21270;&#33021;&#32791;&#65292;&#21448;&#33021;&#22312;&#32473;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#19982;&#23436;&#20840;&#21442;&#25968;&#21270;&#30340;&#32593;&#32476;&#20445;&#25345;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35009;&#21098;&#26041;&#26696;&#20197;&#32511;&#33394;&#20026;&#23548;&#21521;&#65292;&#22240;&#20026;&#23427;&#20165;&#38656;&#35201;&#21160;&#24577;&#35009;&#21098;&#26041;&#27861;&#36827;&#34892;&#19968;&#27425;&#35757;&#32451;&#26469;&#21457;&#29616;&#26368;&#20339;&#30340;&#38745;&#24577;&#23376;&#32593;&#32476;&#12290;&#35813;&#26041;&#26696;&#30001;&#19968;&#20010;&#20108;&#36827;&#21046;&#38376;&#25511;&#27169;&#22359;&#21644;&#19968;&#20010;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#32452;&#25104;&#65292;&#20197;&#21457;&#29616;&#20855;&#26377;&#29992;&#25143;&#23450;&#20041;&#31232;&#30095;&#24230;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#31561;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35009;&#21098;&#21644;&#36716;&#25442;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#33021;&#37327;&#28040;&#32791;&#65292;&#21516;&#26102;&#22312;&#24050;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The subject of green AI has been gaining attention within the deep learning community given the recent trend of ever larger and more complex neural network models. Existing solutions for reducing the computational load of training at inference time usually involve pruning the network parameters. Pruning schemes often create extra overhead either by iterative training and fine-tuning for static pruning or repeated computation of a dynamic pruning graph. We propose a new parameter pruning strategy for learning a lighter-weight sub-network that minimizes the energy cost while maintaining comparable performance to the fully parameterised network on given downstream tasks. Our proposed pruning scheme is green-oriented, as it only requires a one-off training to discover the optimal static sub-networks by dynamic pruning methods. The pruning scheme consists of a binary gating module and a novel loss function to uncover sub-networks with user-defined sparsity. Our method enables pruning and tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#20102;&#25968;&#25454;&#20998;&#25968;&#38543;&#26426;&#21453;&#21521;&#36807;&#31243;&#26159;&#19968;&#20010;&#38789;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26657;&#20934;&#20219;&#24847;&#39044;&#20808;&#35757;&#32451;&#30340;DPM&#65292;&#26377;&#25928;&#20943;&#23567;&#27169;&#22411;&#30340;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#65292;&#22686;&#21152;&#27169;&#22411;&#20284;&#28982;&#30340;&#19979;&#38480;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#33324;&#26657;&#20934;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2302.10688</link><description>&lt;p&gt;
&#20851;&#20110;&#26657;&#20934;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
On Calibrating Diffusion Probabilistic Models. (arXiv:2302.10688v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#20102;&#25968;&#25454;&#20998;&#25968;&#38543;&#26426;&#21453;&#21521;&#36807;&#31243;&#26159;&#19968;&#20010;&#38789;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26657;&#20934;&#20219;&#24847;&#39044;&#20808;&#35757;&#32451;&#30340;DPM&#65292;&#26377;&#25928;&#20943;&#23567;&#27169;&#22411;&#30340;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#65292;&#22686;&#21152;&#27169;&#22411;&#20284;&#28982;&#30340;&#19979;&#38480;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#33324;&#26657;&#20934;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPM&#65289;&#22312;&#21508;&#31181;&#29983;&#25104;&#24615;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;DPM&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#36880;&#28176;&#25193;&#25955;&#25968;&#25454;&#20998;&#24067;&#30340;&#27491;&#21521;&#36807;&#31243;&#21644;&#19968;&#20010;&#20174;&#26102;&#38388;&#30456;&#20851;&#25968;&#25454;&#20998;&#25968;&#20013;&#24674;&#22797;&#25968;&#25454;&#20998;&#24067;&#30340;&#38543;&#26426;&#21453;&#21521;&#36807;&#31243;&#12290;&#26412;&#25991;&#35266;&#23519;&#21040;&#25968;&#25454;&#20998;&#25968;&#30340;&#38543;&#26426;&#21453;&#21521;&#36807;&#31243;&#26159;&#19968;&#20010;&#38789;&#65292;&#20174;&#20013;&#21487;&#20197;&#23548;&#20986;&#25968;&#25454;&#20998;&#25968;&#30340;&#38598;&#20013;&#30028;&#21644;&#38543;&#26426;&#20572;&#27490;&#23450;&#29702;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26657;&#20934;&#20219;&#24847;&#39044;&#20808;&#35757;&#32451;&#30340;DPM&#65292;&#20197;&#20943;&#23567;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#65292;&#24182;&#22240;&#27492;&#22686;&#21152;&#27169;&#22411;&#20284;&#28982;&#30340;&#19979;&#38480;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#21508;&#31181;&#27169;&#22411;&#21442;&#25968;&#21270;&#19979;&#30340;&#19968;&#33324;&#26657;&#20934;&#25351;&#21335;&#12290;&#25105;&#20204;&#30340;&#26657;&#20934;&#26041;&#27861;&#20165;&#25191;&#34892;&#19968;&#27425;&#65292;&#24182;&#19988;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#25105;&#20204;&#30340;&#25552;&#35758;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20301;&#20110;https://github.com/thudzj/Cal&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, diffusion probabilistic models (DPMs) have achieved promising results in diverse generative tasks. A typical DPM framework includes a forward process that gradually diffuses the data distribution and a reverse process that recovers the data distribution from time-dependent data scores. In this work, we observe that the stochastic reverse process of data scores is a martingale, from which concentration bounds and the optional stopping theorem for data scores can be derived. Then, we discover a simple way for calibrating an arbitrary pretrained DPM, with which the score matching loss can be reduced and the lower bounds of model likelihood can consequently be increased. We provide general calibration guidelines under various model parametrizations. Our calibration method is performed only once and the resulting models can be used repeatedly for sampling. We conduct experiments on multiple datasets to empirically validate our proposal. Our code is at https://github.com/thudzj/Cal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;$\omega$PAP&#31354;&#38388;&#30340;&#33539;&#30068;&#65292;&#21487;&#20026;&#22823;&#22810;&#25968;&#23454;&#29992;&#30340;&#27010;&#29575;&#21644;&#21487;&#24494;&#31243;&#24207;&#36171;&#20104;&#24847;&#20041;&#65292;&#21516;&#26102;&#36824;&#35777;&#26126;&#20102;&#26377;&#20851;&#33258;&#21160;&#24494;&#20998;&#27491;&#30830;&#24615;&#21644;&#27010;&#29575;&#31243;&#24207;&#20013;&#36857;&#23494;&#24230;&#20989;&#25968;&#30340;&#21487;&#24494;&#24615;&#31561;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.10636</link><description>&lt;p&gt;
$\omega$PAP&#31354;&#38388;&#65306;&#20851;&#20110;&#39640;&#38454;&#12289;&#36882;&#24402;&#27010;&#29575;&#21644;&#21487;&#24494;&#31243;&#24207;&#30340;&#25351;&#31216;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
$\omega$PAP Spaces: Reasoning Denotationally About Higher-Order, Recursive Probabilistic and Differentiable Programs. (arXiv:2302.10636v2 [cs.PL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;$\omega$PAP&#31354;&#38388;&#30340;&#33539;&#30068;&#65292;&#21487;&#20026;&#22823;&#22810;&#25968;&#23454;&#29992;&#30340;&#27010;&#29575;&#21644;&#21487;&#24494;&#31243;&#24207;&#36171;&#20104;&#24847;&#20041;&#65292;&#21516;&#26102;&#36824;&#35777;&#26126;&#20102;&#26377;&#20851;&#33258;&#21160;&#24494;&#20998;&#27491;&#30830;&#24615;&#21644;&#27010;&#29575;&#31243;&#24207;&#20013;&#36857;&#23494;&#24230;&#20989;&#25968;&#30340;&#21487;&#24494;&#24615;&#31561;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#35774;&#32622;&#8212;&#8212;$\omega$PAP&#31354;&#38388;&#30340;&#33539;&#30068;&#65292;&#29992;&#20110;&#20851;&#20110;&#34920;&#29616;&#21147;&#24378;&#22823;&#30340;&#21487;&#24494;&#21644;&#27010;&#29575;&#32534;&#31243;&#35821;&#35328;&#36827;&#34892;&#25351;&#31216;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#35821;&#20041;&#36275;&#22815;&#19968;&#33324;&#21270;&#65292;&#21487;&#20197;&#20026;&#22823;&#22810;&#25968;&#23454;&#29992;&#30340;&#27010;&#29575;&#21644;&#21487;&#24494;&#31243;&#24207;&#36171;&#20104;&#24847;&#20041;&#65292;&#21253;&#25324;&#20351;&#29992;&#19968;&#33324;&#36882;&#24402;&#12289;&#39640;&#38454;&#20989;&#25968;&#12289;&#19981;&#36830;&#32493;&#21407;&#35821;&#20197;&#21450;&#31163;&#25955;&#21644;&#36830;&#32493;&#37319;&#26679;&#30340;&#31243;&#24207;&#12290;&#20294;&#26159;&#65292;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#20063;&#36275;&#22815;&#29305;&#23450;&#65292;&#21487;&#20197;&#25490;&#38500;&#35768;&#22810;&#30149;&#24577;&#30340;&#25351;&#31216;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#24314;&#31435;&#26377;&#20851;&#30830;&#23450;&#24615;&#21487;&#24494;&#31243;&#24207;&#21644;&#27010;&#29575;&#31243;&#24207;&#30340;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new setting, the category of $\omega$PAP spaces, for reasoning denotationally about expressive differentiable and probabilistic programming languages. Our semantics is general enough to assign meanings to most practical probabilistic and differentiable programs, including those that use general recursion, higher-order functions, discontinuous primitives, and both discrete and continuous sampling. But crucially, it is also specific enough to exclude many pathological denotations, enabling us to establish new results about both deterministic differentiable programs and probabilistic programs. In the deterministic setting, we prove very general correctness theorems for automatic differentiation and its use within gradient descent. In the probabilistic setting, we establish the almost-everywhere differentiability of probabilistic programs' trace density functions, and the existence of convenient base measures for density computation in Monte Carlo inference. In some cases th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21307;&#29992;&#21475;&#32617;&#23545;&#24773;&#24863;&#35782;&#21035;&#30340;&#24433;&#21709;&#65292;&#24182;&#23637;&#31034;&#20102;&#25972;&#20307;&#20154;&#20307;&#36755;&#20837;&#20248;&#20110;&#21333;&#32431;&#38754;&#37096;&#36974;&#25377;&#30340;&#21331;&#36234;&#24615;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#21035;&#22788;&#29702;&#38754;&#37096;&#21644;&#36523;&#20307;&#29305;&#24449;&#24182;&#34701;&#21512;&#39044;&#27979;&#20998;&#25968;&#26469;&#25552;&#39640;&#24773;&#24863;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.10021</link><description>&lt;p&gt;
&#21307;&#29992;&#21475;&#32617;&#19982;&#36523;&#20307;&#24773;&#32490;&#35782;&#21035;&#65306;&#28145;&#24230;&#23398;&#20064;&#35270;&#35282;&#19979;&#30340;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Medical Face Masks and Emotion Recognition from the Body: Insights from a Deep Learning Perspective. (arXiv:2302.10021v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21307;&#29992;&#21475;&#32617;&#23545;&#24773;&#24863;&#35782;&#21035;&#30340;&#24433;&#21709;&#65292;&#24182;&#23637;&#31034;&#20102;&#25972;&#20307;&#20154;&#20307;&#36755;&#20837;&#20248;&#20110;&#21333;&#32431;&#38754;&#37096;&#36974;&#25377;&#30340;&#21331;&#36234;&#24615;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#21035;&#22788;&#29702;&#38754;&#37096;&#21644;&#36523;&#20307;&#29305;&#24449;&#24182;&#34701;&#21512;&#39044;&#27979;&#20998;&#25968;&#26469;&#25552;&#39640;&#24773;&#24863;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20896;&#32954;&#28814;&#30123;&#24773;&#26080;&#30097;&#25913;&#21464;&#20102;&#25105;&#20204;&#29983;&#27963;&#30340;&#26631;&#20934;&#21644;&#24433;&#21709;&#20102;&#31038;&#20132;&#36890;&#35759;&#30340;&#26041;&#26041;&#38754;&#38754;&#12290;&#20026;&#20102;&#39044;&#38450;&#20256;&#25773;&#65292;&#20154;&#20204;&#34987;&#36843;&#24191;&#27867;&#20329;&#25140;&#21307;&#29992;&#21475;&#32617;&#65292;&#36825;&#31181;&#38754;&#37096;&#36974;&#25377;&#20005;&#37325;&#24433;&#21709;&#20102;&#20174;&#38754;&#37096;&#35835;&#21462;&#24773;&#24863;&#30340;&#33021;&#21147;&#65292;&#20419;&#20351;&#25105;&#20204;&#23558;&#25972;&#20010;&#36523;&#20307;&#35270;&#20026;&#24773;&#24863;&#32447;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#38754;&#37096;&#36974;&#25377;&#23545;&#24773;&#24863;&#35782;&#21035;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;&#25972;&#20307;&#20154;&#20307;&#36755;&#20837;&#20248;&#20110;&#21333;&#32431;&#38754;&#37096;&#36974;&#25377;&#30340;&#21331;&#36234;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#26102;&#38388;&#20998;&#27573;&#32593;&#32476;&#26694;&#26550;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#28212;&#26395;&#23436;&#20840;&#20811;&#26381;&#21475;&#32617;&#24102;&#26469;&#30340;&#21518;&#26524;&#12290;&#34429;&#28982;&#38754;&#37096;&#21644;&#36523;&#20307;&#29305;&#24449;&#21487;&#20197;&#20174;&#21333;&#20010;&#36755;&#20837;&#20013;&#23398;&#20064;&#65292;&#20294;&#36825;&#21487;&#33021;&#23548;&#33268;&#26080;&#20851;&#20449;&#24687;&#28151;&#28102;&#12290;&#36890;&#36807;&#20998;&#21035;&#22788;&#29702;&#36825;&#20123;&#29305;&#24449;&#24182;&#34701;&#21512;&#23427;&#20204;&#30340;&#39044;&#27979;&#20998;&#25968;&#65292;&#25105;&#20204;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#20004;&#31181;&#27169;&#24335;&#12290;&#35813;&#26694;&#26550;&#36824;&#33258;&#28982;&#22320;&#25903;&#25345;&#33410;&#22863;&#30340;&#25512;&#26029;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#19982;&#35821;&#38899;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#24230;&#37327;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has undoubtedly changed the standards and affected all aspects of our lives, especially social communication. It has forced people to extensively wear medical face masks, in order to prevent transmission. This face occlusion can strongly irritate emotional reading from the face and urges us to incorporate the whole body as an emotional cue. In this paper, we conduct insightful studies about the effect of face occlusion on emotion recognition performance, and showcase the superiority of full body input over the plain masked face. We utilize a deep learning model based on the Temporal Segment Network framework, and aspire to fully overcome the face mask consequences. Although facial and bodily features can be learned from a single input, this may lead to irrelevant information confusion. By processing those features separately and fusing their prediction scores, we are more effectively taking advantage of both modalities. This framework also naturally supports tempo
&lt;/p&gt;</description></item><item><title>&#23558;PAC-Bayesian&#29702;&#35770;&#25193;&#23637;&#21040;&#29983;&#25104;&#27169;&#22411;&#65292;&#20026;&#22522;&#20110;Wasserstein&#36317;&#31163;&#21644;&#24635;&#21464;&#24046;&#36317;&#31163;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#27867;&#21270;&#30028;&#65292;&#20026;Wasserstein GAN&#21644;Energy-Based GAN&#25552;&#20379;&#20102;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#38750;&#34394;&#31354;&#27867;&#21270;&#30028;&#12290;</title><link>http://arxiv.org/abs/2302.08942</link><description>&lt;p&gt;
&#38754;&#21521;&#23545;&#25239;&#29983;&#25104;&#27169;&#22411;&#30340;PAC-Bayesian&#27867;&#21270;&#30028;
&lt;/p&gt;
&lt;p&gt;
PAC-Bayesian Generalization Bounds for Adversarial Generative Models. (arXiv:2302.08942v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08942
&lt;/p&gt;
&lt;p&gt;
&#23558;PAC-Bayesian&#29702;&#35770;&#25193;&#23637;&#21040;&#29983;&#25104;&#27169;&#22411;&#65292;&#20026;&#22522;&#20110;Wasserstein&#36317;&#31163;&#21644;&#24635;&#21464;&#24046;&#36317;&#31163;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#27867;&#21270;&#30028;&#65292;&#20026;Wasserstein GAN&#21644;Energy-Based GAN&#25552;&#20379;&#20102;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#38750;&#34394;&#31354;&#27867;&#21270;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;PAC-Bayesian&#29702;&#35770;&#25193;&#23637;&#21040;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#20026;&#22522;&#20110;Wasserstein&#36317;&#31163;&#21644;&#24635;&#21464;&#24046;&#36317;&#31163;&#30340;&#27169;&#22411;&#24320;&#21457;&#20102;&#27867;&#21270;&#30028;&#12290;&#25105;&#20204;&#31532;&#19968;&#20010;&#20851;&#20110;Wasserstein&#36317;&#31163;&#30340;&#32467;&#26524;&#20551;&#35774;&#23454;&#20363;&#31354;&#38388;&#26159;&#26377;&#30028;&#30340;&#65292;&#32780;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#32467;&#26524;&#21033;&#29992;&#20102;&#38477;&#32500;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#33258;&#28982;&#36866;&#29992;&#20110;Wasserstein GAN&#21644;Energy-Based GAN&#65292;&#32780;&#25105;&#20204;&#30340;&#30028;&#38480;&#20026;&#36825;&#20004;&#31181;GAN&#25552;&#20379;&#20102;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#24037;&#20316;&#20027;&#35201;&#26159;&#29702;&#35770;&#24615;&#30340;&#65292;&#20294;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;Wasserstein GAN&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#38750;&#34394;&#31354;&#27867;&#21270;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We extend PAC-Bayesian theory to generative models and develop generalization bounds for models based on the Wasserstein distance and the total variation distance. Our first result on the Wasserstein distance assumes the instance space is bounded, while our second result takes advantage of dimensionality reduction. Our results naturally apply to Wasserstein GANs and Energy-Based GANs, and our bounds provide new training objectives for these two. Although our work is mainly theoretical, we perform numerical experiments showing non-vacuous generalization bounds for Wasserstein GANs on synthetic datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#36873;&#25321;&#31574;&#30053;TAILOR&#65292;&#23427;&#22312;&#22810;&#31867;&#21644;&#22810;&#26631;&#31614;&#24212;&#29992;&#30340;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#20505;&#36873;&#31639;&#27861;&#30456;&#24403;&#25110;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.07317</link><description>&lt;p&gt;
&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19979;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#36873;&#25321;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Algorithm Selection for Deep Active Learning with Imbalanced Datasets. (arXiv:2302.07317v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#36873;&#25321;&#31574;&#30053;TAILOR&#65292;&#23427;&#22312;&#22810;&#31867;&#21644;&#22810;&#26631;&#31614;&#24212;&#29992;&#30340;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#20505;&#36873;&#31639;&#27861;&#30456;&#24403;&#25110;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#25928;&#29575;&#24050;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#30446;&#26631;&#12290;&#20027;&#21160;&#23398;&#20064;&#26088;&#22312;&#20943;&#23569;&#35757;&#32451;&#28145;&#24230;&#32593;&#32476;&#25152;&#38656;&#30340;&#26631;&#35760;&#31034;&#20363;&#25968;&#37327;&#65292;&#20294;&#26159;&#65292;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#20013;&#65292;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#30340;&#23454;&#35777;&#24615;&#33021;&#21487;&#33021;&#20250;&#22823;&#24133;&#24230;&#21464;&#21270;&#12290;&#20107;&#20808;&#24456;&#38590;&#30693;&#36947;&#21738;&#31181;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#22312;&#29305;&#23450;&#24212;&#29992;&#20013;&#34920;&#29616;&#33391;&#22909;&#25110;&#26368;&#20339;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#36873;&#25321;&#31574;&#30053;&#12290;&#23545;&#20110;&#20219;&#20309;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;(meta)&#31639;&#27861;TAILOR (Thompson ActIve Learning algORithm selection)&#36845;&#20195;&#22320;&#24182;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#19968;&#32452;&#20505;&#36873;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#12290;TAILOR&#20351;&#29992;&#26088;&#22312;&#25910;&#38598;&#31867;&#24179;&#34913;&#31034;&#20363;&#30340;&#26032;&#22870;&#21169;&#20989;&#25968;&#12290;&#22312;&#22810;&#31867;&#21644;&#22810;&#26631;&#31614;&#24212;&#29992;&#30340;&#22823;&#37327;&#23454;&#39564;&#20013;&#65292;TAILOR&#22312;&#23454;&#29616;&#19982;&#20505;&#36873;&#31639;&#27861;&#20013;&#26368;&#20339;&#31639;&#27861;&#30456;&#24403;&#25110;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label efficiency has become an increasingly important objective in deep learning applications. Active learning aims to reduce the number of labeled examples needed to train deep networks, but the empirical performance of active learning algorithms can vary dramatically across datasets and applications. It is difficult to know in advance which active learning strategy will perform well or best in a given application. To address this, we propose the first adaptive algorithm selection strategy for deep active learning. For any unlabeled dataset, our (meta) algorithm TAILOR (Thompson ActIve Learning algORithm selection) iteratively and adaptively chooses among a set of candidate active learning algorithms. TAILOR uses novel reward functions aimed at gathering class-balanced examples. Extensive experiments in multi-class and multi-label applications demonstrate TAILOR's effectiveness in achieving accuracy comparable or better than that of the best of the candidate algorithms.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;I$^2$SB&#65292;&#30452;&#25509;&#23398;&#20064;&#20004;&#20010;&#32473;&#23450;&#20998;&#24067;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#25193;&#25955;&#36807;&#31243;&#12290;&#36890;&#36807;&#36793;&#30028;&#23545;&#27714;&#35299;&#30340;&#26041;&#27861;&#20351;&#24471;I$^2$SB&#35757;&#32451;&#25104;&#20026;&#19968;&#31181;&#26080;&#38656;&#27169;&#25311;&#30340;&#38750;&#32447;&#24615;&#25193;&#25955;&#26694;&#26550;&#65292;&#22312;&#21508;&#31181;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#20248;&#20110;&#26631;&#20934;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#26356;&#21487;&#35299;&#37322;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2302.05872</link><description>&lt;p&gt;
I$^2$SB&#65306;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;Schr\"odinger&#26725;
&lt;/p&gt;
&lt;p&gt;
I$^2$SB: Image-to-Image Schr\"odinger Bridge. (arXiv:2302.05872v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05872
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;I$^2$SB&#65292;&#30452;&#25509;&#23398;&#20064;&#20004;&#20010;&#32473;&#23450;&#20998;&#24067;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#25193;&#25955;&#36807;&#31243;&#12290;&#36890;&#36807;&#36793;&#30028;&#23545;&#27714;&#35299;&#30340;&#26041;&#27861;&#20351;&#24471;I$^2$SB&#35757;&#32451;&#25104;&#20026;&#19968;&#31181;&#26080;&#38656;&#27169;&#25311;&#30340;&#38750;&#32447;&#24615;&#25193;&#25955;&#26694;&#26550;&#65292;&#22312;&#21508;&#31181;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#20248;&#20110;&#26631;&#20934;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#26356;&#21487;&#35299;&#37322;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21363;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;Schr\"odinger&#26725;&#65288;I$^2$SB&#65289;&#65292;&#30452;&#25509;&#23398;&#20064;&#20004;&#20010;&#32473;&#23450;&#20998;&#24067;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#25193;&#25955;&#36807;&#31243;&#12290;&#36825;&#20123;&#25193;&#25955;&#26725;&#23545;&#20110;&#22270;&#20687;&#24674;&#22797;&#29305;&#21035;&#26377;&#29992;&#65292;&#22240;&#20026;&#36864;&#21270;&#22270;&#20687;&#26159;&#37325;&#26500;&#28165;&#26224;&#22270;&#20687;&#30340;&#32467;&#26500;&#20449;&#24687;&#20808;&#39564;&#12290; I$^2$SB&#23646;&#20110;&#19968;&#31867;&#21487;&#22788;&#29702;&#30340;Schr\"odinger&#26725;&#27169;&#22411;&#65292;&#23427;&#26159;&#24471;&#20998;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#25193;&#23637;&#65292;&#20854;&#36793;&#30028;&#23545;&#30340;&#36793;&#32536;&#20998;&#24067;&#21487;&#20197;&#22312;&#35299;&#26512;&#19978;&#35745;&#31639;&#12290;&#36825;&#31181;&#36890;&#36807;&#36793;&#30028;&#23545;&#27714;&#35299;&#30340;&#26041;&#27861;&#20351;&#24471;I$^2$SB&#35757;&#32451;&#25104;&#20026;&#19968;&#31181;&#26080;&#38656;&#27169;&#25311;&#30340;&#38750;&#32447;&#24615;&#25193;&#25955;&#26694;&#26550;&#65292;&#36827;&#32780;&#37319;&#29992;&#22312;&#26631;&#20934;&#25193;&#25955;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#23454;&#29992;&#25216;&#26415;&#65292;&#20351;&#24471;I$^2$SB&#35757;&#32451;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;ImageNet 256x256&#19978;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;I$^2$SB&#22312;&#21508;&#31181;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20462;&#22797;&#65292;&#36229;&#20998;&#36776;&#29575;&#65292;&#21435;&#27169;&#31946;&#21644;JPEG&#24674;&#22797;&#65292;&#24182;&#34920;&#26126;I$^2$SB&#36229;&#36807;&#20102;&#26631;&#20934;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#20855;&#26377;&#26356;&#21487;&#35299;&#37322;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Image-to-Image Schr\"odinger Bridge (I$^2$SB), a new class of conditional diffusion models that directly learn the nonlinear diffusion processes between two given distributions. These diffusion bridges are particularly useful for image restoration, as the degraded images are structurally informative priors for reconstructing the clean images. I$^2$SB belongs to a tractable class of Schr\"odinger bridge, the nonlinear extension to score-based models, whose marginal distributions can be computed analytically given boundary pairs. This results in a simulation-free framework for nonlinear diffusions, where the I$^2$SB training becomes scalable by adopting practical techniques used in standard diffusion models. We validate I$^2$SB in solving various image restoration tasks, including inpainting, super-resolution, deblurring, and JPEG restoration on ImageNet 256x256 and show that I$^2$SB surpasses standard conditional diffusion models with more interpretable generative processes. 
&lt;/p&gt;</description></item><item><title>AGNES&#26159;&#19968;&#31181;&#33021;&#22312;&#24179;&#28369;&#20984;&#20248;&#21270;&#20219;&#21153;&#20013;&#23454;&#29616;&#21152;&#36895;&#30340;&#31639;&#27861;&#65292;&#21363;&#20351;&#26799;&#24230;&#20272;&#35745;&#30340;&#20449;&#22122;&#27604;&#24456;&#23567;&#65292;&#23427;&#20063;&#33021;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#25928;&#26524;&#26174;&#33879;&#20248;&#20110;&#21160;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;Nesterov&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.05515</link><description>&lt;p&gt;
&#23454;&#29616;&#21152;&#36895;&#23613;&#31649;&#26799;&#24230;&#38750;&#24120;&#22024;&#26434;&#12290;
&lt;/p&gt;
&lt;p&gt;
Achieving acceleration despite very noisy gradients. (arXiv:2302.05515v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05515
&lt;/p&gt;
&lt;p&gt;
AGNES&#26159;&#19968;&#31181;&#33021;&#22312;&#24179;&#28369;&#20984;&#20248;&#21270;&#20219;&#21153;&#20013;&#23454;&#29616;&#21152;&#36895;&#30340;&#31639;&#27861;&#65292;&#21363;&#20351;&#26799;&#24230;&#20272;&#35745;&#30340;&#20449;&#22122;&#27604;&#24456;&#23567;&#65292;&#23427;&#20063;&#33021;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#25928;&#26524;&#26174;&#33879;&#20248;&#20110;&#21160;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;Nesterov&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Nesterov&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#19968;&#33324;&#21270;&#12290;&#22914;&#26524;&#22122;&#22768;&#30340;&#24378;&#24230;&#19982;&#26799;&#24230;&#30340;&#22823;&#23567;&#25104;&#27604;&#20363;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#65288;AGNES&#65289;&#21487;&#20197;&#35777;&#26126;&#22312;&#20855;&#26377;&#22024;&#26434;&#26799;&#24230;&#20272;&#35745;&#30340;&#24179;&#28369;&#20984;&#20248;&#21270;&#20219;&#21153;&#20013;&#23454;&#29616;&#21152;&#36895;&#12290;&#22914;&#26524;&#24120;&#25968;&#27604;&#20363;&#36229;&#36807;&#19968;&#65292;Nesterov&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#22312;&#36825;&#31181;&#22122;&#22768;&#27169;&#22411;&#19979;&#19981;&#20250;&#25910;&#25947;&#12290;AGNES&#33021;&#20462;&#22797;&#36825;&#31181;&#19981;&#36275;&#65292;&#24182;&#19988;&#21487;&#20197;&#35777;&#26126;&#23427;&#30340;&#25910;&#25947;&#36895;&#24230;&#21152;&#24555;&#65292;&#26080;&#35770;&#26799;&#24230;&#20272;&#35745;&#30340;&#20449;&#22122;&#27604;&#26377;&#22810;&#23567;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#26159;&#29992;&#20110;&#36229;&#21442;&#25968;&#36807;&#22810;&#30340;&#28145;&#24230;&#23398;&#20064;&#23567;&#25209;&#37327;&#26799;&#24230;&#30340;&#36866;&#24403;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;AGNES&#22312;CNN&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;&#21160;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;Nesterov&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a generalization of Nesterov's accelerated gradient descent algorithm. Our algorithm (AGNES) provably achieves acceleration for smooth convex minimization tasks with noisy gradient estimates if the noise intensity is proportional to the magnitude of the gradient. Nesterov's accelerated gradient descent does not converge under this noise model if the constant of proportionality exceeds one. AGNES fixes this deficiency and provably achieves an accelerated convergence rate no matter how small the signal to noise ratio in the gradient estimate. Empirically, we demonstrate that this is an appropriate model for mini-batch gradients in overparameterized deep learning. Finally, we show that AGNES outperforms stochastic gradient descent with momentum and Nesterov's method in the training of CNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40654;&#26364;&#27969;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#20960;&#20309;&#19978;&#35757;&#32451;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#65292;&#24182;&#22312;&#39640;&#32500;&#24230;&#25968;&#25454;&#19978;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2302.03660</link><description>&lt;p&gt;
&#19968;&#33324;&#20960;&#20309;&#19978;&#30340;&#40654;&#26364;&#27969;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Riemannian Flow Matching on General Geometries. (arXiv:2302.03660v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40654;&#26364;&#27969;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#20960;&#20309;&#19978;&#35757;&#32451;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#65292;&#24182;&#22312;&#39640;&#32500;&#24230;&#25968;&#25454;&#19978;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40654;&#26364;&#27969;&#21305;&#37197;&#65288;RFM&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#27969;&#24418;&#19978;&#35757;&#32451;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#12290;&#29616;&#26377;&#30340;&#27969;&#24418;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#26114;&#36149;&#30340;&#27169;&#25311;&#65292;&#35201;&#20040;&#26080;&#27861;&#26412;&#36136;&#19978;&#25193;&#23637;&#21040;&#39640;&#32500;&#24230;&#65292;&#35201;&#20040;&#20351;&#29992;&#38480;&#21046;&#37327;&#30340;&#36817;&#20284;&#26469;&#20135;&#29983;&#26377;&#20559;&#30340;&#35757;&#32451;&#30446;&#26631;&#12290;&#40654;&#26364;&#27969;&#21305;&#37197;&#32469;&#36807;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#27604;&#20197;&#21069;&#26041;&#27861;&#26356;&#22810;&#30340;&#20248;&#21183;&#65306;&#23427;&#22312;&#31616;&#21333;&#20960;&#20309;&#19978;&#26080;&#38656;&#27169;&#25311;&#65292;&#19981;&#38656;&#35201;&#25955;&#24230;&#35745;&#31639;&#65292;&#24182;&#20197;&#38381;&#21512;&#24418;&#24335;&#35745;&#31639;&#20854;&#30446;&#26631;&#21521;&#37327;&#22330;&#12290; RFM&#30340;&#20851;&#38190;&#22240;&#32032;&#26159;&#26500;&#24314;&#19968;&#20010;&#30456;&#23545;&#31616;&#21333;&#30340;&#21069;&#24230;&#37327;&#65292;&#20197;&#23450;&#20041;&#30446;&#26631;&#21521;&#37327;&#22330;&#65292;&#20854;&#20013;&#21253;&#25324;&#29616;&#26377;&#30340;&#27431;&#20960;&#37324;&#24471;&#24773;&#20917;&#12290;&#20026;&#20102;&#25193;&#23637;&#21040;&#19968;&#33324;&#20960;&#20309;&#65292;&#25105;&#20204;&#20381;&#38752;&#20351;&#29992;&#35889;&#20998;&#35299;&#26469;&#26377;&#25928;&#22320;&#21363;&#20852;&#35745;&#31639;&#21069;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#22312;3D&#32593;&#26684;&#21644;&#21452;&#26354;&#31354;&#38388;&#19978;&#35757;&#32451;&#26631;&#20934;&#21270;&#27969;&#26469;&#35777;&#26126;&#20854;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Riemannian Flow Matching (RFM), a simple yet powerful framework for training continuous normalizing flows on manifolds. Existing methods for generative modeling on manifolds either require expensive simulation, are inherently unable to scale to high dimensions, or use approximations for limiting quantities that result in biased training objectives. Riemannian Flow Matching bypasses these limitations and offers several advantages over previous approaches: it is simulation-free on simple geometries, does not require divergence computation, and computes its target vector field in closed-form. The key ingredient behind RFM is the construction of a relatively simple premetric for defining target vector fields, which encompasses the existing Euclidean case. To extend to general geometries, we rely on the use of spectral decompositions to efficiently compute premetrics on the fly. Our method achieves state-of-the-art performance on real-world non-Euclidean datasets, and we demonstr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#25581;&#31034;&#20102;&#26679;&#26412;&#25968;&#12289;&#38544;&#31169;&#32423;&#21035;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#19979;&#28216;&#25968;&#25454;&#38598;&#20197;&#21450;&#21487;&#23398;&#20064;&#21442;&#25968;&#23376;&#38598;&#31561;&#22240;&#32032;&#23545;&#20998;&#31867;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.01190</link><description>&lt;p&gt;
&#20851;&#20110;&#24046;&#20998;&#38544;&#31169;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#26377;&#25928;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Efficacy of Differentially Private Few-shot Image Classification. (arXiv:2302.01190v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#25581;&#31034;&#20102;&#26679;&#26412;&#25968;&#12289;&#38544;&#31169;&#32423;&#21035;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#19979;&#28216;&#25968;&#25454;&#38598;&#20197;&#21450;&#21487;&#23398;&#20064;&#21442;&#25968;&#23376;&#38598;&#31561;&#22240;&#32032;&#23545;&#20998;&#31867;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#35757;&#32451;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36825;&#20123;DP&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25509;&#36817;&#26368;&#20339;&#30340;&#38750;&#31169;&#26377;&#27169;&#22411;&#12290;&#36825;&#20123;DP&#27169;&#22411;&#36890;&#24120;&#22312;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#30456;&#23545;&#22823;&#19988;&#19982;&#39044;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30456;&#20284;&#30340;&#31169;&#26377;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#21253;&#25324;&#20010;&#24615;&#21270;&#21644;&#32852;&#21512;&#23398;&#20064;&#65292;&#37325;&#35201;&#30340;&#26159;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#33391;&#22909;&#22320;&#34920;&#29616;&#65288;i.e. &#33719;&#21462;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#21487;&#33021;&#26377;&#38382;&#39064;&#65289;&#65292;&#19988;&#33021;&#22815;&#22312;&#21508;&#31181;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#65288;&#21363;&#29992;&#20110;&#21508;&#31181;&#19987;&#19994;&#35774;&#32622;&#65289;&#36827;&#34892;&#33391;&#22909;&#30340;&#20998;&#31867;&#12290;&#20026;&#20102;&#20102;&#35299;&#23569;&#26679;&#26412;DP&#20309;&#26102;&#26377;&#25928;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#35814;&#23613;&#30340;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#27599;&#31867;&#26679;&#26412;&#25968;&#12289;&#38544;&#31169;&#32423;&#21035;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#19979;&#28216;&#25968;&#25454;&#38598;&#20197;&#21450;&#21487;&#23398;&#20064;&#21442;&#25968;&#23376;&#38598;&#31561;&#23545;&#23569;&#26679;&#26412;DP&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#26131;&#21463;&#25915;&#20987;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been significant recent progress in training differentially private (DP) models which achieve accuracy that approaches the best non-private models. These DP models are typically pretrained on large public datasets and then fine-tuned on private downstream datasets that are relatively large and similar in distribution to the pretraining data. However, in many applications including personalization and federated learning, it is crucial to perform well (i) in the few-shot setting, as obtaining large amounts of labeled data may be problematic; and (ii) on datasets from a wide variety of domains for use in various specialist settings. To understand under which conditions few-shot DP can be effective, we perform an exhaustive set of experiments that reveals how the accuracy and vulnerability to attack of few-shot DP image classification models are affected as the number of shots per class, privacy level, model architecture, downstream dataset, and subset of learnable parameters in 
&lt;/p&gt;</description></item><item><title>SimMTM&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#24207;&#21015;&#27169;&#22411;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#26435;&#32858;&#21512;&#22810;&#20010;&#27969;&#24418;&#20043;&#22806;&#30340;&#37051;&#23621;&#26469;&#24674;&#22797;&#25513;&#30721;&#26102;&#38388;&#28857;&#65292;&#20174;&#32780;&#31616;&#21270;&#37325;&#26500;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2302.00861</link><description>&lt;p&gt;
SimMTM: &#19968;&#31181;&#29992;&#20110;&#24207;&#21015;&#27169;&#22411;&#30340;&#31616;&#21333;&#39044;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SimMTM: A Simple Pre-Training Framework for Masked Time-Series Modeling. (arXiv:2302.00861v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00861
&lt;/p&gt;
&lt;p&gt;
SimMTM&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#24207;&#21015;&#27169;&#22411;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#26435;&#32858;&#21512;&#22810;&#20010;&#27969;&#24418;&#20043;&#22806;&#30340;&#37051;&#23621;&#26469;&#24674;&#22797;&#25513;&#30721;&#26102;&#38388;&#28857;&#65292;&#20174;&#32780;&#31616;&#21270;&#37325;&#26500;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#39046;&#22495;&#12290;&#20026;&#20102;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#24182;&#21463;&#30410;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#26368;&#36817;&#65292;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#20854;&#20013;&#19968;&#31181;&#20027;&#27969;&#33539;&#20363;&#26159;&#25513;&#30721;&#24314;&#27169;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#22522;&#20110;&#26410;&#25513;&#30721;&#37096;&#20998;&#30340;&#25513;&#30721;&#20869;&#23481;&#30340;&#37325;&#26500;&#65292;&#25104;&#21151;&#22320;&#39044;&#35757;&#32451;&#28145;&#24230;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#35821;&#20041;&#20449;&#24687;&#20027;&#35201;&#21253;&#21547;&#22312;&#26102;&#38388;&#21464;&#21270;&#20013;&#65292;&#26631;&#20934;&#30340;&#38543;&#26426;&#23631;&#34109;&#19968;&#37096;&#20998;&#26102;&#38388;&#28857;&#30340;&#26041;&#24335;&#20250;&#20005;&#37325;&#30772;&#22351;&#26102;&#38388;&#24207;&#21015;&#30340;&#37325;&#35201;&#26102;&#38388;&#21464;&#21270;&#65292;&#20351;&#37325;&#26500;&#20219;&#21153;&#36807;&#20110;&#22256;&#38590;&#65292;&#26080;&#27861;&#24341;&#23548;&#34920;&#31034;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24207;&#21015;&#27169;&#22411;&#30340;&#31616;&#21333;&#39044;&#35757;&#32451;&#26694;&#26550;SimMTM&#12290;&#36890;&#36807;&#23558;&#25513;&#30721;&#24314;&#27169;&#19982;&#27969;&#24418;&#23398;&#20064;&#30456;&#20851;&#32852;&#65292;SimMTM&#25552;&#20986;&#36890;&#36807;&#22810;&#20010;&#27969;&#24418;&#20043;&#22806;&#30340;&#37051;&#23621;&#30340;&#21152;&#26435;&#32858;&#21512;&#26469;&#24674;&#22797;&#25513;&#30721;&#26102;&#38388;&#28857;&#65292;&#20174;&#32780;&#36890;&#36807;&#32452;&#35013;&#34987;&#30772;&#22351;&#20294;&#20114;&#34917;&#30340;&#26102;&#38388;&#21464;&#21270;&#26469;&#31616;&#21270;&#37325;&#26500;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series analysis is widely used in extensive areas. Recently, to reduce labeling expenses and benefit various tasks, self-supervised pre-training has attracted immense interest. One mainstream paradigm is masked modeling, which successfully pre-trains deep models by learning to reconstruct the masked content based on the unmasked part. However, since the semantic information of time series is mainly contained in temporal variations, the standard way of randomly masking a portion of time points will seriously ruin vital temporal variations of time series, making the reconstruction task too difficult to guide representation learning. We thus present SimMTM, a Simple pre-training framework for Masked Time-series Modeling. By relating masked modeling to manifold learning, SimMTM proposes to recover masked time points by the weighted aggregation of multiple neighbors outside the manifold, which eases the reconstruction task by assembling ruined but complementary temporal variations from
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#33021;&#37327;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#39640;&#33021;&#29289;&#29702;&#20107;&#20214;&#65292;&#21487;&#29992;&#20110;&#21442;&#25968;&#21270;&#30340;&#20107;&#20214;&#29983;&#25104;&#65292;&#24322;&#24120;&#20449;&#21495;&#25506;&#27979;&#20197;&#21450;&#31890;&#23376;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2302.00695</link><description>&lt;p&gt;
&#22810;&#21151;&#33021;&#33021;&#37327;&#27010;&#29575;&#27169;&#22411;&#22312;&#39640;&#33021;&#29289;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Versatile Energy-Based Probabilistic Models for High Energy Physics. (arXiv:2302.00695v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#33021;&#37327;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#39640;&#33021;&#29289;&#29702;&#20107;&#20214;&#65292;&#21487;&#29992;&#20110;&#21442;&#25968;&#21270;&#30340;&#20107;&#20214;&#29983;&#25104;&#65292;&#24322;&#24120;&#20449;&#21495;&#25506;&#27979;&#20197;&#21450;&#31890;&#23376;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#32463;&#20856;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#20855;&#26377;&#33021;&#37327;&#20989;&#25968;&#24418;&#24335;&#28789;&#27963;&#24615;&#30340;&#22825;&#28982;&#20248;&#21183;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24314;&#27169;&#39640;&#32500;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#19982;&#36825;&#20123;&#36827;&#23637;&#19968;&#33268;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#33021;&#37327;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#26469;&#33258;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#30340;&#39640;&#33021;&#29289;&#29702;&#20107;&#20214;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#19968;&#20010;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25551;&#36848;&#20102;&#26356;&#39640;&#38454;&#30340;&#31890;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#32534;&#30721;&#20307;&#31995;&#32467;&#26500;&#21644;&#38544;&#24335;&#29983;&#25104;&#12290;&#22312;&#24212;&#29992;&#26041;&#38754;&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#24378;&#22823;&#30340;&#21442;&#25968;&#21270;&#20107;&#20214;&#29983;&#25104;&#22120;&#29992;&#20110;&#29289;&#29702;&#20223;&#30495;&#65292;&#19968;&#31181;&#27867;&#29992;&#30340;&#26080;&#20551;&#35774;&#20851;&#32852;&#30340;&#24322;&#24120;&#20449;&#21495;&#25506;&#27979;&#22120;&#65292;&#20197;&#21450;&#29992;&#20110;&#31890;&#23376;&#35782;&#21035;&#30340;&#22686;&#24378;&#20107;&#20214;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a classical generative modeling approach, energy-based models have the natural advantage of flexibility in the form of the energy function. Recently, energy-based models have achieved great success in modeling high-dimensional data in computer vision and natural language processing. In line with these advancements, we build a multi-purpose energy-based probabilistic model for High Energy Physics events at the Large Hadron Collider. This framework builds on a powerful generative model and describes higher-order inter-particle interactions.It suits different encoding architectures and builds on implicit generation. As for applicational aspects, it can serve as a powerful parameterized event generator for physics simulation, a generic anomalous signal detector free from spurious correlations, and an augmented event classifier for particle identification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#28436;&#21592;-&#35780;&#35770;&#23478;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#33976;&#39311;&#20248;&#21183;&#22312;&#21033;&#29992;&#36807;&#21435;&#32463;&#39564;&#30340;&#21516;&#26102;&#36981;&#24490;&#31283;&#23450;&#30340;&#22312;&#32447;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#23398;&#20064;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#31639;&#27861;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2302.00533</link><description>&lt;p&gt;
&#33976;&#39311;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Distillation Policy Optimization. (arXiv:2302.00533v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#28436;&#21592;-&#35780;&#35770;&#23478;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#33976;&#39311;&#20248;&#21183;&#22312;&#21033;&#29992;&#36807;&#21435;&#32463;&#39564;&#30340;&#21516;&#26102;&#36981;&#24490;&#31283;&#23450;&#30340;&#22312;&#32447;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#23398;&#20064;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#31639;&#27861;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28436;&#21592;-&#35780;&#35770;&#23478;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20511;&#37492;&#20102;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#35270;&#35282;&#21644;&#20004;&#31181;&#31574;&#30053;&#25913;&#36827;&#25968;&#25454;&#30340;&#20132;&#21449;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#23398;&#20064;&#24182;&#21487;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#31639;&#27861;&#31867;&#21035;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#39318;&#20808;&#25552;&#20986;&#20102;&#26041;&#24046;&#20943;&#23569;&#26426;&#21046;&#65292;&#20363;&#22914;&#32479;&#19968;&#20248;&#21183;&#20272;&#35745;&#22120; (UAE) &#21644;&#19968;&#20010;&#23398;&#20064;&#30340;&#22522;&#32447;&#65292;&#19981;&#20165;&#26159;&#36830;&#25509;&#21040;&#21160;&#20316;&#20540;&#20989;&#25968;&#30340;&#26725;&#26753;&#65292;&#36824;&#33021;&#25552;&#28860;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
On-policy algorithms are supposed to be stable, however, sample-intensive yet. Off-policy algorithms utilizing past experiences are deemed to be sample-efficient, nevertheless, unstable in general. Can we design an algorithm that can employ the off-policy data, while exploit the stable learning by sailing along the course of the on-policy walkway? In this paper, we present an actor-critic learning framework that borrows the distributional perspective of interest to evaluate, and cross-breeds two sources of the data for policy improvement, which enables fast learning and can be applied to a wide class of algorithms. In its backbone, the variance reduction mechanisms, such as unified advantage estimator (UAE), that extends generalized advantage estimator (GAE) to be applicable on any state-dependent baseline, and a learned baseline, that is competent to stabilize the policy gradient, are firstly put forward to not merely be a bridge to the action-value function but also distill the advan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#38544;&#24335;&#27491;&#21017;&#21270;&#21487;&#23545;&#20110;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#35937;&#20855;&#26377;&#20419;&#36827;&#20316;&#29992;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#21442;&#25968;&#21270;&#24418;&#24335;&#65292;&#32467;&#21512;&#20102;$\ell_1$&#21644;$\ell_2$&#20869;&#25554;&#22120;&#30340;&#20248;&#28857;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#21487;&#24471;&#21040;&#19968;&#20010;&#25509;&#36817;&#26368;&#20248;&#27979;&#35797;&#25439;&#22833;&#30340;&#20869;&#25554;&#22120;&#12290;</title><link>http://arxiv.org/abs/2302.00257</link><description>&lt;p&gt;
&#38544;&#24335;&#27491;&#21017;&#21270;&#23545;&#20110;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#35937;&#20855;&#26377;&#20419;&#36827;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Implicit Regularization Leads to Benign Overfitting for Sparse Linear Regression. (arXiv:2302.00257v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#38544;&#24335;&#27491;&#21017;&#21270;&#21487;&#23545;&#20110;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#35937;&#20855;&#26377;&#20419;&#36827;&#20316;&#29992;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#21442;&#25968;&#21270;&#24418;&#24335;&#65292;&#32467;&#21512;&#20102;$\ell_1$&#21644;$\ell_2$&#20869;&#25554;&#22120;&#30340;&#20248;&#28857;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#21487;&#24471;&#21040;&#19968;&#20010;&#25509;&#36817;&#26368;&#20248;&#27979;&#35797;&#25439;&#22833;&#30340;&#20869;&#25554;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#35757;&#32451;&#36807;&#31243;&#32463;&#24120;&#20250;&#25214;&#21040;&#19968;&#20010;&#20869;&#25554;&#22120;&#65288;&#19968;&#20010;0&#35757;&#32451;&#25439;&#22833;&#30340;&#35299;&#65289;&#65292;&#20294;&#27979;&#35797;&#25439;&#22833;&#20173;&#28982;&#24456;&#20302;&#12290;&#36825;&#31181;&#34987;&#31216;&#20026;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#29616;&#35937;&#65292;&#26159;&#19968;&#20010;&#22791;&#21463;&#20851;&#27880;&#30340;&#37325;&#35201;&#35868;&#22242;&#12290;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#19968;&#20010;&#24120;&#35265;&#26426;&#21046;&#26159;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#35757;&#32451;&#36807;&#31243;&#20250;&#23548;&#33268;&#20869;&#25554;&#22120;&#20855;&#26377;&#39069;&#22806;&#30340;&#24615;&#36136;&#65292;&#24120;&#34987;&#25551;&#36848;&#20026;&#26368;&#23567;&#21270;&#26576;&#20123;&#33539;&#25968;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#23545;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;$y=\beta^{*\top}x+\xi$&#65292;&#26368;&#23567;&#30340;$\ell_1$&#25110;$\ell_2$&#33539;&#25968;&#20869;&#25554;&#22120;&#20063;&#19981;&#33021;&#32473;&#20986;&#26368;&#20248;&#30340;&#27979;&#35797;&#25439;&#22833;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#30340;&#19981;&#21516;&#21442;&#25968;&#21270;&#24418;&#24335;&#65292;&#23427;&#23548;&#33268;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#25928;&#24212;&#65292;&#32467;&#21512;&#20102;$\ell_1$&#21644;$\ell_2$&#20869;&#25554;&#22120;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#25105;&#20204;&#30340;&#26032;&#27169;&#22411;&#65292;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#20855;&#26377;&#25509;&#36817;&#26368;&#20248;&#27979;&#35797;&#25439;&#22833;&#30340;&#20869;&#25554;&#22120;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22522;&#20110;&#23545;&#35757;&#32451;&#21160;&#21147;&#23398;&#30340;&#32454;&#33268;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35299;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
In deep learning, often the training process finds an interpolator (a solution with 0 training loss), but the test loss is still low. This phenomenon, known as benign overfitting, is a major mystery that received a lot of recent attention. One common mechanism for benign overfitting is implicit regularization, where the training process leads to additional properties for the interpolator, often characterized by minimizing certain norms. However, even for a simple sparse linear regression problem $y = \beta^{*\top} x +\xi$ with sparse $\beta^*$, neither minimum $\ell_1$ or $\ell_2$ norm interpolator gives the optimal test loss. In this work, we give a different parametrization of the model which leads to a new implicit regularization effect that combines the benefit of $\ell_1$ and $\ell_2$ interpolators. We show that training our new model via gradient descent leads to an interpolator with near-optimal test loss. Our result is based on careful analysis of the training dynamics and prov
&lt;/p&gt;</description></item><item><title>dyMEAN&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#20840;&#21407;&#23376;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#34920;&#20301;&#21644;&#19981;&#23436;&#25972;&#30340;&#25239;&#20307;&#24207;&#21015;&#36827;&#34892;&#25239;&#20307;&#35774;&#35745;&#65292;&#22312;&#22788;&#29702;&#20840;&#21407;&#23376;&#26102;&#33021;&#22815;&#22788;&#29702;&#21487;&#21464;&#22823;&#23567;&#30340;&#34507;&#30333;&#27531;&#22522;&#12290;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#23398;&#20064;&#26041;&#27861;&#30340;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#21482;&#22788;&#29702;&#25239;&#20307;&#35774;&#35745;&#36807;&#31243;&#20013;&#30340;&#26576;&#20010;&#23376;&#20219;&#21153;&#21644;&#26080;&#27861;&#25429;&#25417;&#20840;&#21407;&#23376;&#20960;&#20309;&#24418;&#29366;&#12290;</title><link>http://arxiv.org/abs/2302.00203</link><description>&lt;p&gt;
&#31471;&#21040;&#31471;&#20840;&#21407;&#23376;&#25239;&#20307;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
End-to-End Full-Atom Antibody Design. (arXiv:2302.00203v3 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00203
&lt;/p&gt;
&lt;p&gt;
dyMEAN&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#20840;&#21407;&#23376;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#34920;&#20301;&#21644;&#19981;&#23436;&#25972;&#30340;&#25239;&#20307;&#24207;&#21015;&#36827;&#34892;&#25239;&#20307;&#35774;&#35745;&#65292;&#22312;&#22788;&#29702;&#20840;&#21407;&#23376;&#26102;&#33021;&#22815;&#22788;&#29702;&#21487;&#21464;&#22823;&#23567;&#30340;&#34507;&#30333;&#27531;&#22522;&#12290;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#23398;&#20064;&#26041;&#27861;&#30340;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#21482;&#22788;&#29702;&#25239;&#20307;&#35774;&#35745;&#36807;&#31243;&#20013;&#30340;&#26576;&#20010;&#23376;&#20219;&#21153;&#21644;&#26080;&#27861;&#25429;&#25417;&#20840;&#21407;&#23376;&#20960;&#20309;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#20307;&#35774;&#35745;&#26159;&#33647;&#29289;&#27835;&#30103;&#21644;&#29983;&#29289;&#23398;&#31561;&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#24403;&#21069;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26377;&#20004;&#20010;&#20027;&#35201;&#32570;&#38519;&#65306;1&#65289;&#21482;&#22788;&#29702;&#25972;&#20010;&#25239;&#20307;&#35774;&#35745;&#27969;&#31243;&#20013;&#30340;&#26576;&#20010;&#23376;&#20219;&#21153;&#65292;&#20351;&#20854;&#27425;&#20248;&#25110;&#36164;&#28304;&#23494;&#38598;&#12290;2&#65289;&#24573;&#30053;&#26694;&#26550;&#21306;&#22495;&#25110;&#20391;&#38142;&#20013;&#30340;&#20219;&#19968;&#37096;&#20998;&#65292;&#22240;&#27492;&#26080;&#27861;&#25429;&#25417;&#20840;&#21407;&#23376;&#20960;&#20309;&#24418;&#29366;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#22810;&#36890;&#36947;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;dyMEAN&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#20840;&#21407;&#23376;&#27169;&#22411;&#65292;&#29992;&#20110;&#26681;&#25454;&#34920;&#20301;&#21644;&#19981;&#23436;&#25972;&#30340;&#25239;&#20307;&#24207;&#21015;&#36827;&#34892; E&#65288;3&#65289;&#31561;&#21464;&#25239;&#20307;&#35774;&#35745;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25506;&#31350;&#32467;&#26500;&#21021;&#22987;&#21270;&#20316;&#20026;&#25239;&#20307;&#32467;&#26500;&#30340;&#26377;&#30693;&#35782;&#29468;&#27979;&#65292;&#28982;&#21518;&#25552;&#20986;&#24433;&#21709;&#34920;&#20301;-&#25239;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#8220;shadow paratope&#8221;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#22810;&#36890;&#36947;&#31561;&#21464;&#32534;&#30721;&#22120;&#26469;&#26356;&#26032;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#65292;&#35813;&#32534;&#30721;&#22120;&#21487;&#22312;&#32771;&#34385;&#20840;&#21407;&#23376;&#26102;&#22788;&#29702;&#21487;&#21464;&#22823;&#23567;&#30340;&#34507;&#30333;&#27531;&#22522;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;dyMEAN&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#20026;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#25239;&#20307;&#35774;&#35745;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Antibody design is an essential yet challenging task in various domains like therapeutics and biology. There are two major defects in current learning-based methods: 1) tackling only a certain subtask of the whole antibody design pipeline, making them suboptimal or resource-intensive. 2) omitting either the framework regions or side chains, thus incapable of capturing the full-atom geometry. To address these pitfalls, we propose dynamic Multi-channel Equivariant grAph Network (dyMEAN), an end-to-end full-atom model for E(3)-equivariant antibody design given the epitope and the incomplete sequence of the antibody. Specifically, we first explore structural initialization as a knowledgeable guess of the antibody structure and then propose shadow paratope to bridge the epitope-antibody connections. Both 1D sequences and 3D structures are updated via an adaptive multi-channel equivariant encoder that is able to process protein residues of variable sizes when considering full atoms. Finally,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#20165;&#22788;&#29702;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#20687;&#36827;&#34892;&#32852;&#31995;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#29615;&#22659;&#30456;&#20851;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#21313;&#20998;&#20248;&#24322;&#65292;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#35270;&#35273;&#22330;&#26223;&#19979;&#20132;&#20114;&#38382;&#39064;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2301.13823</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#20687;&#36827;&#34892;&#32852;&#31995;&#20197;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Grounding Language Models to Images for Multimodal Inputs and Outputs. (arXiv:2301.13823v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13823
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#20165;&#22788;&#29702;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#20687;&#36827;&#34892;&#32852;&#31995;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#29615;&#22659;&#30456;&#20851;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#21313;&#20998;&#20248;&#24322;&#65292;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#35270;&#35273;&#22330;&#26223;&#19979;&#20132;&#20114;&#38382;&#39064;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#20165;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#19982;&#35270;&#35273;&#39046;&#22495;&#32852;&#31995;&#36215;&#26469;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#21033;&#29992;&#20174;&#22823;&#35268;&#27169;&#25991;&#26412;&#39044;&#35757;&#32451;&#20013;&#23398;&#21040;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#20445;&#25345;&#35821;&#35328;&#27169;&#22411;&#20923;&#32467;&#65292;&#24182;&#24494;&#35843;&#36755;&#20837;&#21644;&#36755;&#20986;&#32447;&#24615;&#23618;&#20197;&#23454;&#29616;&#36328;&#27169;&#24577;&#20132;&#20114;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#12290;&#25105;&#20204;&#22312;&#29615;&#22659;&#30456;&#20851;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#34920;&#29616;&#65292;&#24182;&#23637;&#31034;&#20102;&#24341;&#20154;&#20837;&#32988;&#30340;&#20132;&#20114;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#29616;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#22312;&#35270;&#35273;&#22330;&#26223;&#19979;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#19988;&#36890;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;STEEL&#65292;&#22312;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#34892;&#21160;&#30340;&#26080;&#38480;&#26102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#19981;&#20381;&#36182;&#20110;&#32477;&#23545;&#36830;&#32493;&#20551;&#35774;&#65292;&#36890;&#36807;&#26368;&#22823;&#22343;&#20540;&#20559;&#24046;&#21644;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30830;&#20445;&#24322;&#24120;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.13152</link><description>&lt;p&gt;
STEEL: &#22855;&#24322;&#24615;&#24863;&#30693;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
STEEL: Singularity-aware Reinforcement Learning. (arXiv:2301.13152v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13152
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;STEEL&#65292;&#22312;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#34892;&#21160;&#30340;&#26080;&#38480;&#26102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#19981;&#20381;&#36182;&#20110;&#32477;&#23545;&#36830;&#32493;&#20551;&#35774;&#65292;&#36890;&#36807;&#26368;&#22823;&#22343;&#20540;&#20559;&#24046;&#21644;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30830;&#20445;&#24322;&#24120;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#21033;&#29992;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#20197;&#26368;&#22823;&#21270;&#26399;&#26395;&#24635;&#22238;&#25253;&#12290;&#28982;&#32780;&#65292;&#20960;&#20046;&#25152;&#26377;&#29616;&#26377;&#31639;&#27861;&#37117;&#20381;&#36182;&#20110;&#30446;&#26631;&#31574;&#30053;&#35825;&#23548;&#30340;&#20998;&#24067;&#32477;&#23545;&#36830;&#32493;&#20551;&#35774;&#65292;&#20197;&#20415;&#36890;&#36807;&#21464;&#25442;&#27979;&#24230;&#20351;&#29992;&#25209;&#37327;&#25968;&#25454;&#26469;&#26657;&#20934;&#30446;&#26631;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#19981;&#38656;&#35201;&#22312;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#34892;&#21160;&#30340;&#26080;&#38480;&#26102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#32477;&#23545;&#36830;&#32493;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#31216;&#36825;&#20010;&#31639;&#27861;&#20026;STEEL&#65306;SingulariTy-awarE rEinforcement Learning&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21463;&#21040;&#20851;&#20110;&#31163;&#32447;&#35780;&#20272;&#30340;&#26032;&#35823;&#24046;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#20102;&#26368;&#22823;&#22343;&#20540;&#20559;&#24046;&#65292;&#20197;&#21450;&#24102;&#26377;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30340;&#31574;&#30053;&#23450;&#21521;&#35823;&#24046;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#24322;&#24120;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#22855;&#24322;&#24773;&#20917;&#30340;&#23450;&#21521;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batch reinforcement learning (RL) aims at leveraging pre-collected data to find an optimal policy that maximizes the expected total rewards in a dynamic environment. Nearly all existing algorithms rely on the absolutely continuous assumption on the distribution induced by target policies with respect to the data distribution, so that the batch data can be used to calibrate target policies via the change of measure. However, the absolute continuity assumption could be violated in practice (e.g., no-overlap support), especially when the state-action space is large or continuous. In this paper, we propose a new batch RL algorithm without requiring absolute continuity in the setting of an infinite-horizon Markov decision process with continuous states and actions. We call our algorithm STEEL: SingulariTy-awarE rEinforcement Learning. Our algorithm is motivated by a new error analysis on off-policy evaluation, where we use maximum mean discrepancy, together with distributionally robust opti
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#65292;&#22312;&#26368;&#23567;&#20551;&#35774;&#19979;&#23545;&#20998;&#24067;&#24335;&#38750;&#20984;&#30446;&#26631;&#36827;&#34892;&#20102;&#38543;&#26426;&#20248;&#21270;&#65292;&#24314;&#31435;&#20102;&#20165;&#28385;&#36275;&#38543;&#26426;&#26799;&#24230;&#28201;&#21644;&#26465;&#20214;&#30340;&#25910;&#25947;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.12677</link><description>&lt;p&gt;
&#36890;&#29992;&#26041;&#24046;&#26465;&#20214;&#19979;&#30340;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Distributed Stochastic Optimization under a General Variance Condition. (arXiv:2301.12677v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12677
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#65292;&#22312;&#26368;&#23567;&#20551;&#35774;&#19979;&#23545;&#20998;&#24067;&#24335;&#38750;&#20984;&#30446;&#26631;&#36827;&#34892;&#20102;&#38543;&#26426;&#20248;&#21270;&#65292;&#24314;&#31435;&#20102;&#20165;&#28385;&#36275;&#38543;&#26426;&#26799;&#24230;&#28201;&#21644;&#26465;&#20214;&#30340;&#25910;&#25947;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#26102;&#34920;&#29616;&#20986;&#20102;&#24456;&#39640;&#30340;&#25928;&#29575;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#19968;&#33324;&#23454;&#38469;&#38382;&#39064;&#30340;&#31639;&#27861;&#24456;&#22810;&#65292;&#20294;&#23427;&#20204;&#30340;&#29702;&#35770;&#20445;&#35777;&#20027;&#35201;&#20381;&#36182;&#20110;&#38543;&#26426;&#26799;&#24230;&#30340;&#26576;&#20123;&#26377;&#30028;&#26465;&#20214;&#65292;&#20174;&#22343;&#21248;&#26377;&#30028;&#24615;&#21040;&#25918;&#26494;&#22686;&#38271;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#22312;&#20195;&#29702;&#20043;&#38388;&#34920;&#24449;&#25968;&#25454;&#24322;&#36136;&#24615;&#21450;&#20854;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#20381;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20986;&#20110;&#36825;&#26679;&#30340;&#21160;&#26426;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#32463;&#20856;&#30340;&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65289;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#24179;&#28369;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#38543;&#26426;&#26799;&#24230;&#20165;&#28385;&#36275;&#28201;&#21644;&#26041;&#24046;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#24314;&#31435;&#20102;&#25910;&#25947;&#32467;&#26524;&#12290;&#22312;&#27492;&#26465;&#20214;&#19979;&#65292;&#36824;&#24314;&#31435;&#20102;&#25509;&#36817;&#30830;&#23450;&#30340;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#24577;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#20010;&#26356;&#20855;&#20449;&#24687;&#24615;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed stochastic optimization has drawn great attention recently due to its effectiveness in solving large-scale machine learning problems. Though numerous algorithms have been proposed and successfully applied to general practical problems, their theoretical guarantees mainly rely on certain boundedness conditions on the stochastic gradients, varying from uniform boundedness to the relaxed growth condition. In addition, how to characterize the data heterogeneity among the agents and its impacts on the algorithmic performance remains challenging. In light of such motivations, we revisit the classical Federated Averaging (FedAvg) algorithm for solving the distributed stochastic optimization problem and establish the convergence results under only a mild variance condition on the stochastic gradients for smooth nonconvex objective functions. Almost sure convergence to a stationary point is also established under the condition. Moreover, we discuss a more informative measurement for
&lt;/p&gt;</description></item><item><title>FedEBA+&#26159;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#37319;&#29992;&#20844;&#24179;&#32858;&#21512;&#26041;&#26696;&#21644;&#23545;&#40784;&#26356;&#26032;&#26041;&#27861;&#65292;&#22312;&#21516;&#26102;&#25552;&#39640;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;FedEBA+&#20248;&#20110;&#20854;&#20182;&#20844;&#24179;&#24615;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.12407</link><description>&lt;p&gt;
FedEBA+&#65306;&#22522;&#20110;&#29109;&#30340;&#27169;&#22411;&#23454;&#29616;&#20844;&#24179;&#21644;&#26377;&#25928;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedEBA+: Towards Fair and Effective Federated Learning via Entropy-Based Model. (arXiv:2301.12407v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12407
&lt;/p&gt;
&lt;p&gt;
FedEBA+&#26159;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#37319;&#29992;&#20844;&#24179;&#32858;&#21512;&#26041;&#26696;&#21644;&#23545;&#40784;&#26356;&#26032;&#26041;&#27861;&#65292;&#22312;&#21516;&#26102;&#25552;&#39640;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;FedEBA+&#20248;&#20110;&#20854;&#20182;&#20844;&#24179;&#24615;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#20844;&#24179;&#24615;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#26041;&#38754;&#65292;&#23427;&#20351;&#27169;&#22411;&#22312;&#25152;&#26377;&#23458;&#25143;&#31471;&#19978;&#20445;&#25345;&#19968;&#33268;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#19968;&#31181;&#21487;&#20197;&#21516;&#26102;&#25552;&#39640;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#21644;&#20419;&#36827;&#20844;&#24179;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#33392;&#24040;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23454;&#29616;&#21518;&#32773;&#36890;&#24120;&#38656;&#35201;&#19982;&#21069;&#32773;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;FedEBA+&#65292;&#23427;&#22312;&#21516;&#26102;&#25552;&#39640;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#39640;&#20844;&#24179;&#24615;&#65292;&#35813;&#31639;&#27861;&#37319;&#29992;&#20844;&#24179;&#32858;&#21512;&#26041;&#26696;&#21644;&#23545;&#40784;&#26356;&#26032;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#25910;&#25947;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;FedEBA+&#30340;&#20844;&#24179;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;FedEBA+&#22312;&#20844;&#24179;&#24615;&#21644;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#22343;&#20248;&#20110;&#20854;&#20182;SOTA&#30340;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring fairness is a crucial aspect of Federated Learning (FL), which enables the model to perform consistently across all clients. However, designing an FL algorithm that simultaneously improves global model performance and promotes fairness remains a formidable challenge, as achieving the latter often necessitates a trade-off with the former.To address this challenge, we propose a new FL algorithm, FedEBA+, which enhances fairness while simultaneously improving global model performance. FedEBA+ incorporates a fair aggregation scheme that assigns higher weights to underperforming clients and an alignment update method. In addition, we provide theoretical convergence analysis and show the fairness of FedEBA+. Extensive experiments demonstrate that FedEBA+ outperforms other SOTA fairness FL methods in terms of both fairness and global model performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#24615; Bandit &#29615;&#22659;&#19979;&#38024;&#23545;&#21253;&#21547;&#24322;&#26041;&#24046;&#22870;&#21169;&#22122;&#22768;&#30340;&#31574;&#30053;&#35780;&#20272;&#65292;&#20351;&#29992;&#26368;&#20248;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#30340;&#26032;&#31639;&#27861; SPEED&#65292;&#35813;&#31639;&#27861;&#21487;&#23454;&#29616;&#24102;&#26377;&#22343;&#26041;&#35823;&#24046;&#27604;&#36739;&#23567;&#30340;&#31574;&#30053;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2301.12357</link><description>&lt;p&gt;
SPEED: &#32447;&#24615;&#24322;&#26041;&#24046; Bandit &#31574;&#30053;&#35780;&#20272;&#30340;&#23454;&#39564;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
SPEED: Experimental Design for Policy Evaluation in Linear Heteroscedastic Bandits. (arXiv:2301.12357v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#24615; Bandit &#29615;&#22659;&#19979;&#38024;&#23545;&#21253;&#21547;&#24322;&#26041;&#24046;&#22870;&#21169;&#22122;&#22768;&#30340;&#31574;&#30053;&#35780;&#20272;&#65292;&#20351;&#29992;&#26368;&#20248;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#30340;&#26032;&#31639;&#27861; SPEED&#65292;&#35813;&#31639;&#27861;&#21487;&#23454;&#29616;&#24102;&#26377;&#22343;&#26041;&#35823;&#24046;&#27604;&#36739;&#23567;&#30340;&#31574;&#30053;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615; Bandit &#19979;&#31574;&#30053;&#35780;&#20272;&#30340;&#26368;&#20248;&#25968;&#25454;&#25910;&#38598;&#38382;&#39064;&#12290;&#22312;&#31574;&#30053;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#20272;&#35745;&#22810;&#33218;&#36172;&#21338;&#26426;&#29615;&#22659;&#20013;&#25191;&#34892;&#30446;&#26631;&#31574;&#30053;&#23558;&#33719;&#24471;&#30340;&#26399;&#26395;&#25910;&#30410;&#12290;&#26412;&#25991;&#26159;&#39318;&#20010;&#19987;&#27880;&#20110;&#35299;&#20915;&#32447;&#24615; Bandit &#29615;&#22659;&#19979;&#21253;&#21547;&#24322;&#26041;&#24046;&#22870;&#21169;&#22122;&#22768;&#30340;&#31574;&#30053;&#35780;&#20272;&#30340;&#26368;&#20248;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#32447;&#24615; Bandit &#29615;&#22659;&#19979;&#21046;&#23450;&#20102;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#30340;&#26368;&#20248;&#35774;&#35745;&#65292;&#20197;&#20943;&#23569;&#30446;&#26631;&#31574;&#30053;&#20215;&#20540;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#35774;&#35745;&#26469;&#25512;&#23548;&#20986;&#25968;&#25454;&#25910;&#38598;&#26399;&#38388;&#27599;&#20010;&#21160;&#20316;&#30340;&#26368;&#20248;&#26679;&#26412;&#20998;&#37197;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026; SPEED&#65288;Structured Policy Evaluation Experimental Design&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36319;&#36394;&#26368;&#20248;&#35774;&#35745;&#65292;&#24182;&#35745;&#31639;&#20854;&#19982;&#26368;&#20248;&#35774;&#35745;&#30340;&#36951;&#25022;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126; SPEED &#21487;&#20197;&#23454;&#29616;&#24102;&#26377;&#22343;&#26041;&#35823;&#24046;&#27604;&#36739;&#23567;&#30340;&#31574;&#30053;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the problem of optimal data collection for policy evaluation in linear bandits. In policy evaluation, we are given a target policy and asked to estimate the expected reward it will obtain when executed in a multi-armed bandit environment. Our work is the first work that focuses on such optimal data collection strategy for policy evaluation involving heteroscedastic reward noise in the linear bandit setting. We first formulate an optimal design for weighted least squares estimates in the heteroscedastic linear bandit setting that reduces the MSE of the value of the target policy. We then use this formulation to derive the optimal allocation of samples per action during data collection. We then introduce a novel algorithm SPEED (Structured Policy Evaluation Experimental Design) that tracks the optimal design and derive its regret with respect to the optimal design. Finally, we empirically validate that SPEED leads to policy evaluation with mean squared error compa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#19981;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;BAFFLE&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#22810;&#20010;&#27491;&#21521;&#36807;&#31243;&#20272;&#35745;&#26799;&#24230;&#65292;&#20855;&#26377;&#39640;&#20869;&#23384;&#25928;&#29575;&#65292;&#23481;&#26131;&#36866;&#24212;&#19978;&#20256;&#24102;&#23485;&#65292;&#19982;&#30828;&#20214;&#20248;&#21270;&#21644;&#27169;&#22411;&#37327;&#21270;/&#20462;&#21098;&#20860;&#23481;&#65292;&#36866;&#29992;&#20110;&#21463;&#20449;&#20219;&#30340;&#25191;&#34892;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2301.12195</link><description>&lt;p&gt;
&#12298;&#32852;&#37030;&#23398;&#20064;&#26159;&#21542;&#30495;&#27491;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#65311;&#12299;
&lt;/p&gt;
&lt;p&gt;
Does Federated Learning Really Need Backpropagation?. (arXiv:2301.12195v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#19981;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;BAFFLE&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#22810;&#20010;&#27491;&#21521;&#36807;&#31243;&#20272;&#35745;&#26799;&#24230;&#65292;&#20855;&#26377;&#39640;&#20869;&#23384;&#25928;&#29575;&#65292;&#23481;&#26131;&#36866;&#24212;&#19978;&#20256;&#24102;&#23485;&#65292;&#19982;&#30828;&#20214;&#20248;&#21270;&#21644;&#27169;&#22411;&#37327;&#21270;/&#20462;&#21098;&#20860;&#23481;&#65292;&#36866;&#29992;&#20110;&#21463;&#20449;&#20219;&#30340;&#25191;&#34892;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#22320;&#35753;&#23458;&#25143;&#31471;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#26381;&#21153;&#22120;&#27169;&#22411;&#30340;&#19968;&#33324;&#24615;&#21407;&#21017;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#12290;FL&#26159;&#19968;&#20010;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#30340;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#65292;&#20294;&#20854;&#26631;&#20934;&#35757;&#32451;&#33539;&#24335;&#35201;&#27714;&#23458;&#25143;&#31471;&#36890;&#36807;&#27169;&#22411;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#20197;&#35745;&#31639;&#26799;&#24230;&#12290;&#30001;&#20110;&#36825;&#20123;&#23458;&#25143;&#31471;&#36890;&#24120;&#26159;&#36793;&#32536;&#35774;&#22791;&#32780;&#19981;&#26159;&#23436;&#20840;&#21463;&#20449;&#20219;&#30340;&#65292;&#22240;&#27492;&#22312;&#23427;&#20204;&#19978;&#25191;&#34892;&#21453;&#21521;&#20256;&#25773;&#20250;&#20135;&#29983;&#35745;&#31639;&#21644;&#23384;&#20648;&#24320;&#38144;&#20197;&#21450;&#30333;&#30418;&#28431;&#27934;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#31216;&#20026;BAFFLE&#65292;&#20854;&#20013;&#21453;&#21521;&#20256;&#25773;&#26367;&#25442;&#20026;&#22810;&#20010;&#27491;&#21521;&#36807;&#31243;&#20197;&#20272;&#35745;&#26799;&#24230;&#12290;BAFFLE&#20855;&#26377;&#20197;&#19979;&#20248;&#28857;&#65306;1&#65289;&#20869;&#23384;&#25928;&#29575;&#39640;&#24182;&#19988;&#23481;&#26131;&#36866;&#24212;&#19978;&#20256;&#24102;&#23485;&#65307;2&#65289;&#19982;&#20165;&#25512;&#29702;&#30828;&#20214;&#20248;&#21270;&#20197;&#21450;&#27169;&#22411;&#37327;&#21270;&#25110;&#20462;&#21098;&#20860;&#23481;&#65307;3&#65289;&#38750;&#24120;&#36866;&#21512;&#21463;&#20449;&#20219;&#30340;&#25191;&#34892;&#29615;&#22659;&#65292;&#22240;&#20026;BAFFLE&#20013;&#30340;&#23458;&#25143;&#31471;&#20165;&#25191;&#34892;&#27491;&#21521;&#20256;&#25773;&#24182;&#36820;&#22238;&#19968;&#32452;&#26631;&#37327;&#21040;&#26381;&#21153;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#20351;&#29992;&#20102;BAFFLE&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a general principle for decentralized clients to train a server model collectively without sharing local data. FL is a promising framework with practical applications, but its standard training paradigm requires the clients to backpropagate through the model to compute gradients. Since these clients are typically edge devices and not fully trusted, executing backpropagation on them incurs computational and storage overhead as well as white-box vulnerability. In light of this, we develop backpropagation-free federated learning, dubbed BAFFLE, in which backpropagation is replaced by multiple forward processes to estimate gradients. BAFFLE is 1) memory-efficient and easily fits uploading bandwidth; 2) compatible with inference-only hardware optimization and model quantization or pruning; and 3) well-suited to trusted execution environments, because the clients in BAFFLE only execute forward propagation and return a set of scalars to the server. Empirically we us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#8220;few-&#8221;&#31867;&#22411;&#37327;&#35789;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#25152;&#26377;&#27169;&#22411;&#23545;&#36825;&#31181;&#37327;&#35789;&#37117;&#34920;&#29616;&#19981;&#20339;&#65292;&#19988;&#36739;&#22823;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#24046;&#12290;&#36825;&#31181;&#21453;&#27604;&#20363;&#32553;&#25918;&#30340;&#29616;&#35937;&#34920;&#26126;&#22823;&#22411;&#27169;&#22411;&#36234;&#26469;&#24840;&#21453;&#26144;&#22312;&#32447;&#20154;&#31867;&#22788;&#29702;&#65292;&#32780;&#19981;&#26159;&#31163;&#32447;&#22788;&#29702;&#12290;&#36825;&#21487;&#33021;&#25361;&#25112;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#31995;&#32479;&#22522;&#30784;&#30340;&#20570;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.08700</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#37327;&#35789;&#26102;&#34920;&#29616;&#30053;&#26377;&#38382;&#39064;&#65311;&#20351;&#29992;&#23569;&#37327;&#31867;&#22411;&#30340;&#37327;&#35789;&#20250;&#23548;&#33268;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#21576;&#29616;&#21453;&#27604;&#20363;&#32553;&#25918;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rarely a problem? Language models exhibit inverse scaling in their predictions following few-type quantifiers. (arXiv:2212.08700v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#8220;few-&#8221;&#31867;&#22411;&#37327;&#35789;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#25152;&#26377;&#27169;&#22411;&#23545;&#36825;&#31181;&#37327;&#35789;&#37117;&#34920;&#29616;&#19981;&#20339;&#65292;&#19988;&#36739;&#22823;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#24046;&#12290;&#36825;&#31181;&#21453;&#27604;&#20363;&#32553;&#25918;&#30340;&#29616;&#35937;&#34920;&#26126;&#22823;&#22411;&#27169;&#22411;&#36234;&#26469;&#24840;&#21453;&#26144;&#22312;&#32447;&#20154;&#31867;&#22788;&#29702;&#65292;&#32780;&#19981;&#26159;&#31163;&#32447;&#22788;&#29702;&#12290;&#36825;&#21487;&#33021;&#25361;&#25112;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#31995;&#32479;&#22522;&#30784;&#30340;&#20570;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#37327;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#20197;&#8220;few-&#8221;&#31867;&#22411;&#30340;&#37327;&#35789;&#20026;&#37325;&#28857;&#65292;&#27604;&#22914;&#8220;few children like toys&#8221;&#65292;&#22240;&#20026;&#36825;&#31181;&#31867;&#22411;&#30340;&#21477;&#23376;&#32452;&#25104;&#37096;&#20998;&#36890;&#24120;&#20250;&#20849;&#29616;&#65292;&#32780;&#8220;few-&#8221;&#31867;&#22411;&#30340;&#37327;&#35789;&#36739;&#20026;&#32597;&#35265;&#65292;&#36825;&#21487;&#33021;&#23545;&#35821;&#35328;&#27169;&#22411;&#26500;&#25104;&#29305;&#21035;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;&#26469;&#33258;&#20004;&#39033;&#20154;&#31867;&#31070;&#32463;&#35821;&#35328;&#23398;&#23454;&#39564;&#30340;960&#20010;&#33521;&#35821;&#21477;&#23376;&#36827;&#34892;&#20102;&#35797;&#39564;&#65292;&#24182;&#23558;&#23427;&#20204;&#25552;&#20379;&#32473;22&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#27169;&#22411;&#12290;&#19981;&#20165;&#25152;&#26377;&#27169;&#22411;&#23545;&#8220;few-&#8221;&#31867;&#22411;&#30340;&#37327;&#35789;&#37117;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#19988;&#24635;&#20307;&#19978;&#65292;&#27169;&#22411;&#36234;&#22823;&#65292;&#20854;&#34920;&#29616;&#36234;&#24046;&#12290;&#36825;&#31181;&#21453;&#27604;&#20363;&#32553;&#25918;&#30340;&#29616;&#35937;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#19968;&#33268;&#65292;&#34920;&#26126;&#36739;&#22823;&#30340;&#27169;&#22411;&#36234;&#26469;&#36234;&#21453;&#26144;&#22312;&#32447;&#20154;&#31867;&#22788;&#29702;&#65292;&#32780;&#19981;&#26159;&#31163;&#32447;&#22788;&#29702;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#19979;&#38477;&#21487;&#33021;&#20250;&#25361;&#25112;&#23558;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#31995;&#32479;&#22522;&#30784;&#30340;&#20570;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
How well do language models deal with quantification? In this study, we focus on 'few'-type quantifiers, as in 'few children like toys', which might pose a particular challenge for language models because the sentence components with out the quantifier are likely to co-occur, and 'few'-type quantifiers are rare. We present 960 English sentence stimuli from two human neurolinguistic experiments to 22 autoregressive transformer models of differing sizes. Not only do all the models perform poorly on 'few'-type quantifiers, but overall the larger the model, the worse its performance. This inverse scaling is consistent with previous work suggesting that larger models increasingly reflect online rather than offline human processing, and we argue that the decreasing performance of larger models may challenge uses of language models as the basis for natural language systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#20219;&#21153;&#22411;&#23545;&#35805;&#30340;&#21452;&#37325;&#23545;&#40784;&#22810;&#35821;&#35328;&#35299;&#26512;&#22120;&#65292;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#22810;&#35821;&#35328;&#21644;&#20195;&#30721;&#20999;&#25442;&#35821;&#20041;&#35299;&#26512;&#31995;&#32479;&#30340;&#38646;-shot&#24615;&#33021;&#65292;&#25552;&#39640;mBERT&#36716;&#31227;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.08054</link><description>&lt;p&gt;
DAMP&#65306;&#38754;&#21521;&#20219;&#21153;&#22411;&#23545;&#35805;&#30340;&#21452;&#37325;&#23545;&#40784;&#22810;&#35821;&#35328;&#35299;&#26512;&#22120;
&lt;/p&gt;
&lt;p&gt;
DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue. (arXiv:2212.08054v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#20219;&#21153;&#22411;&#23545;&#35805;&#30340;&#21452;&#37325;&#23545;&#40784;&#22810;&#35821;&#35328;&#35299;&#26512;&#22120;&#65292;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#22810;&#35821;&#35328;&#21644;&#20195;&#30721;&#20999;&#25442;&#35821;&#20041;&#35299;&#26512;&#31995;&#32479;&#30340;&#38646;-shot&#24615;&#33021;&#65292;&#25552;&#39640;mBERT&#36716;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#34394;&#25311;&#21161;&#25163;&#20351;&#29992;&#20869;&#37096;&#35821;&#20041;&#35299;&#26512;&#24341;&#25806;&#23558;&#29992;&#25143;&#35805;&#35821;&#36716;&#21270;&#20026;&#21487;&#25805;&#20316;&#21629;&#20196;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#20041;&#35299;&#26512;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#22810;&#35821;&#35328;&#36716;&#31227;&#20219;&#21153;&#65292;&#20854;&#36716;&#31227;&#25928;&#29575;&#27604;&#20854;&#20182;&#20219;&#21153;&#20302;&#12290;&#22312;&#20840;&#29699;&#24066;&#22330;&#65288;&#22914;&#21360;&#24230;&#21644;&#25289;&#19969;&#32654;&#27954;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#22240;&#20026;&#21452;&#35821;&#29992;&#25143;&#39057;&#32321;&#20999;&#25442;&#35821;&#35328;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#38454;&#27573;&#30340;&#22810;&#35821;&#35328;&#23545;&#40784;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22810;&#35821;&#35328;&#21644;&#20195;&#30721;&#20999;&#25442;&#35821;&#20041;&#35299;&#26512;&#31995;&#32479;&#30340;&#38646;-shot&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#23545;&#27604;&#23545;&#40784;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#33521;&#25991;&#24615;&#33021;&#21644;&#36716;&#31227;&#25928;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#21463;&#38480;&#21046;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#36229;&#21442;&#25968;&#30340;&#23545;&#25239;&#24615;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#21452;&#37325;&#23545;&#40784;&#22810;&#35821;&#35328;&#35299;&#26512;&#22120;&#65288;DAMP&#65289;&#20998;&#21035;&#23558;Spanglish&#12289;Hinglish&#21644;&#22810;&#35821;&#35328;&#20219;&#21153;&#23548;&#21521;&#35299;&#26512;&#22522;&#20934;&#30340;mBERT&#36716;&#31227;&#24615;&#33021;&#25552;&#39640;&#20102;3&#20493;&#12289;6&#20493;&#21644;81&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern virtual assistants use internal semantic parsing engines to convert user utterances to actionable commands. However, prior work has demonstrated that semantic parsing is a difficult multilingual transfer task with low transfer efficiency compared to other tasks. In global markets such as India and Latin America, this is a critical issue as switching between languages is prevalent for bilingual users. In this work we dramatically improve the zero-shot performance of a multilingual and codeswitched semantic parsing system using two stages of multilingual alignment. First, we show that constrastive alignment pretraining improves both English performance and transfer efficiency. We then introduce a constrained optimization approach for hyperparameter-free adversarial alignment during finetuning. Our Doubly Aligned Multilingual Parser (DAMP) improves mBERT transfer performance by 3x, 6x, and 81x on the Spanglish, Hinglish and Multilingual Task Oriented Parsing benchmarks respectively
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#21033;&#29992;&#35745;&#31639;&#26426;&#26029;&#23618;&#25104;&#20687;&#27169;&#25311;&#22120;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#19977;&#32500;&#26029;&#23618;&#25104;&#20687;&#37325;&#24314;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#21442;&#32771;&#37325;&#24314;&#20197;&#36827;&#34892;&#35757;&#32451;&#65292;&#32467;&#26524;&#20445;&#30495;&#24230;&#26356;&#39640;&#19988;&#36895;&#24230;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2212.07431</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#25311;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#19977;&#32500;&#26029;&#23618;&#25104;&#20687;&#37325;&#24314;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Simulator-Based Self-Supervision for Learned 3D Tomography Reconstruction. (arXiv:2212.07431v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07431
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#21033;&#29992;&#35745;&#31639;&#26426;&#26029;&#23618;&#25104;&#20687;&#27169;&#25311;&#22120;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#19977;&#32500;&#26029;&#23618;&#25104;&#20687;&#37325;&#24314;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#21442;&#32771;&#37325;&#24314;&#20197;&#36827;&#34892;&#35757;&#32451;&#65292;&#32467;&#26524;&#20445;&#30495;&#24230;&#26356;&#39640;&#19988;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20302;&#21058;&#37327;&#34746;&#26059;&#38181;&#26463;&#35745;&#31639;&#26426;&#20307;&#23618;&#25104;&#20687;&#30340;&#19977;&#32500;&#20307;&#31215;&#37325;&#24314;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#20808;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#29992;&#21478;&#19968;&#20010;&#31639;&#27861;&#35745;&#31639;&#21442;&#32771;&#37325;&#24314;&#20197;&#36827;&#34892;&#35757;&#32451;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#20165;&#24102;&#26377;&#22122;&#22768;&#30340;&#20108;&#32500;X&#23556;&#32447;&#25968;&#25454;&#20197;&#23436;&#20840;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#12290;&#36825;&#26159;&#36890;&#36807;&#22312;&#35757;&#32451;&#24490;&#29615;&#20013;&#32467;&#21512;&#24555;&#36895;&#21487;&#24494;&#30340;&#35745;&#31639;&#26426;&#26029;&#23618;&#25104;&#20687;&#27169;&#25311;&#22120;&#23454;&#29616;&#30340;&#12290;&#30001;&#20110;&#25105;&#20204;&#19981;&#20381;&#36182;&#20110;&#21442;&#32771;&#37325;&#24314;&#65292;&#22240;&#27492;&#25105;&#20204;&#32467;&#26524;&#30340;&#20445;&#30495;&#24230;&#19981;&#21463;&#20854;&#28508;&#22312;&#32570;&#38519;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#34746;&#26059;&#38181;&#26463;&#25237;&#24433;&#21644;&#27169;&#25311;&#24187;&#20687;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#19982;&#20381;&#36182;&#29616;&#26377;&#37325;&#24314;&#25216;&#26415;&#30340;&#25216;&#26415;&#30456;&#27604;&#65292;&#20855;&#26377;&#26174;&#30528;&#26356;&#39640;&#30340;&#35270;&#35273;&#20445;&#30495;&#24230;&#21644;&#26356;&#22909;&#30340;PSNR&#12290;&#24403;&#24212;&#29992;&#20110;&#20840;&#21058;&#37327;&#25968;&#25454;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#39640;&#36136;&#37327;&#32467;&#26524;&#27604;&#36845;&#20195;&#25216;&#26415;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a deep learning method for 3D volumetric reconstruction in low-dose helical cone-beam computed tomography. Prior machine learning approaches require reference reconstructions computed by another algorithm for training. In contrast, we train our model in a fully self-supervised manner using only noisy 2D X-ray data. This is enabled by incorporating a fast differentiable CT simulator in the training loop. As we do not rely on reference reconstructions, the fidelity of our results is not limited by their potential shortcomings. We evaluate our method on real helical cone-beam projections and simulated phantoms. Our results show significantly higher visual fidelity and better PSNR over techniques that rely on existing reconstructions. When applied to full-dose data, our method produces high-quality results orders of magnitude faster than iterative techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#23398;&#20064;&#29575;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26368;&#22823;&#21021;&#22987;&#23398;&#20064;&#29575;&#30340;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#20854;&#34892;&#20026;&#19982;&#35757;&#32451;&#21518;&#26399;&#30340;&#26368;&#22823;&#23398;&#20064;&#29575;&#19981;&#21516;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;&#26368;&#22823;&#21021;&#22987;&#23398;&#20064;&#29575;&#21487;&#20197;&#24456;&#22909;&#22320;&#39044;&#27979;&#20026;&#28145;&#24230;&#215;&#23485;&#24230;&#30340;&#24130;&#27425;&#12290;</title><link>http://arxiv.org/abs/2212.07295</link><description>&lt;p&gt;
&#28145;&#23618;ReLU&#32593;&#32476;&#20013;&#30340;&#26368;&#22823;&#21021;&#22987;&#23398;&#20064;&#29575;
&lt;/p&gt;
&lt;p&gt;
Maximal Initial Learning Rates in Deep ReLU Networks. (arXiv:2212.07295v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#23398;&#20064;&#29575;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26368;&#22823;&#21021;&#22987;&#23398;&#20064;&#29575;&#30340;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#20854;&#34892;&#20026;&#19982;&#35757;&#32451;&#21518;&#26399;&#30340;&#26368;&#22823;&#23398;&#20064;&#29575;&#19981;&#21516;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;&#26368;&#22823;&#21021;&#22987;&#23398;&#20064;&#29575;&#21487;&#20197;&#24456;&#22909;&#22320;&#39044;&#27979;&#20026;&#28145;&#24230;&#215;&#23485;&#24230;&#30340;&#24130;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#36873;&#25321;&#36866;&#24403;&#30340;&#23398;&#20064;&#29575;&#65292;&#36825;&#28041;&#21450;&#21040;&#36895;&#24230;&#21644;&#26377;&#25928;&#25910;&#25947;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#23613;&#31649;&#23545;&#20110;&#23398;&#20064;&#29575;&#21487;&#20197;&#26377;&#22810;&#22823;&#36827;&#34892;&#20102;&#30456;&#24403;&#22823;&#37327;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#20294;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#21482;&#20851;&#27880;&#20110;&#21518;&#26399;&#35757;&#32451;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26368;&#22823;&#21021;&#22987;&#23398;&#20064;&#29575;$\eta^{*}$&#8212;&#8212;&#22312;&#36825;&#20010;&#23398;&#20064;&#29575;&#19979;&#65292;&#19968;&#20010;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#25104;&#21151;&#22320;&#24320;&#22987;&#35757;&#32451;&#24182;&#36798;&#21040;&#65288;&#33267;&#23569;&#65289;&#19968;&#20010;&#32473;&#23450;&#30340;&#38408;&#20540;&#31934;&#24230;&#12290;&#20351;&#29992;&#31616;&#21333;&#30340;&#26041;&#27861;&#20272;&#35745;$\eta^{*}$&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#24658;&#23450;&#23485;&#24230;&#30340;&#23436;&#20840;&#36830;&#25509;&#30340;ReLU&#32593;&#32476;&#20013;&#65292;$\eta^{*}$&#30340;&#34892;&#20026;&#19982;&#35757;&#32451;&#21518;&#26399;&#30340;&#26368;&#22823;&#23398;&#20064;&#29575;&#19981;&#21516;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;$\eta^{*}$&#21487;&#20197;&#24456;&#22909;&#22320;&#39044;&#27979;&#20026;&#28145;&#24230;$\times$&#23485;&#24230;&#30340;&#24130;&#27425;&#65292;&#21069;&#25552;&#26159;&#65288;i&#65289;&#32593;&#32476;&#30340;&#23485;&#24230;&#30456;&#23545;&#28145;&#24230;&#36275;&#22815;&#22823;&#65292;&#65288;ii&#65289;&#36755;&#20837;&#23618;&#20197;&#30456;&#23545;&#36739;&#23567;&#30340;&#23398;&#20064;&#29575;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training a neural network requires choosing a suitable learning rate, which involves a trade-off between speed and effectiveness of convergence. While there has been considerable theoretical and empirical analysis of how large the learning rate can be, most prior work focuses only on late-stage training. In this work, we introduce the maximal initial learning rate $\eta^{\ast}$ - the largest learning rate at which a randomly initialized neural network can successfully begin training and achieve (at least) a given threshold accuracy. Using a simple approach to estimate $\eta^{\ast}$, we observe that in constant-width fully-connected ReLU networks, $\eta^{\ast}$ behaves differently from the maximum learning rate later in training. Specifically, we find that $\eta^{\ast}$ is well predicted as a power of depth $\times$ width, provided that (i) the width of the network is sufficiently large compared to the depth, and (ii) the input layer is trained at a relatively small learning rate. We fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#30456;&#20284;&#24230;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#26641;&#24418;&#32467;&#26500;Parzen&#20272;&#35745;&#20013;&#30340;&#22810;&#30446;&#26631;&#38750;&#20998;&#23618;&#36229;&#21442;&#25968;&#26368;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.06751</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#30456;&#20284;&#24230;&#20803;&#23398;&#20064;&#21152;&#36895;&#22810;&#30446;&#26631;&#38750;&#20998;&#23618;&#36229;&#21442;&#25968;&#26368;&#20248;&#21270;&#30340;&#26641;&#24418;&#32467;&#26500;Parzen&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Speeding up Multi-objective Non-hierarchical Hyperparameter Optimization by Task Similarity-Based Meta-Learning for the Tree-structured Parzen Estimator. (arXiv:2212.06751v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#30456;&#20284;&#24230;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#26641;&#24418;&#32467;&#26500;Parzen&#20272;&#35745;&#20013;&#30340;&#22810;&#30446;&#26631;&#38750;&#20998;&#23618;&#36229;&#21442;&#25968;&#26368;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#26159;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#24615;&#33021;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#23454;&#36341;&#32773;&#36890;&#24120;&#38754;&#20020;&#22810;&#20010;&#26041;&#38754;&#30340;&#26435;&#34913;&#65292;&#22914;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#26102;&#38388;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#39640;&#35745;&#31639;&#38656;&#27714;&#21644;&#23545;&#39640;&#25928;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#19981;&#26029;&#22686;&#38271;&#38656;&#27714;&#19979;&#65292;&#21152;&#36895;&#22810;&#30446;&#26631;&#20248;&#21270;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#23558;TPE&#30340;&#25910;&#36141;&#20989;&#25968;&#25193;&#23637;&#21040;&#20803;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#20351;&#29992;&#30001;&#20219;&#21153;&#20043;&#38388;&#39030;&#32423;&#22495;&#20043;&#38388;&#30340;&#37325;&#21472;&#24230;&#23450;&#20041;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#20063;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#24182;&#35299;&#20915;&#20102;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#34920;&#26684;HPO&#22522;&#20934;&#19978;&#21152;&#36895;&#20102;MO-TPE&#65292;&#24182;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#36890;&#36807;&#36194;&#24471;AutoML 2022&#26469;&#24471;&#21040;&#22806;&#37096;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization (HPO) is a vital step in improving performance in deep learning (DL). Practitioners are often faced with the trade-off between multiple criteria, such as accuracy and latency. Given the high computational needs of DL and the growing demand for efficient HPO, the acceleration of multi-objective (MO) optimization becomes ever more important. Despite the significant body of work on meta-learning for HPO, existing methods are inapplicable to MO tree-structured Parzen estimator (MO-TPE), a simple yet powerful MO-HPO algorithm. In this paper, we extend TPE's acquisition function to the meta-learning setting using a task similarity defined by the overlap of top domains between tasks. We also theoretically analyze and address the limitations of our task similarity. In the experiments, we demonstrate that our method speeds up MO-TPE on tabular HPO benchmarks and attains state-of-the-art performance. Our method was also validated externally by winning the AutoML 2022 
&lt;/p&gt;</description></item><item><title>NPM&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#38750;&#21442;&#25968;&#20998;&#24067;&#26367;&#25442;softmax&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#31232;&#26377;&#27169;&#24335;&#21644;&#39044;&#27979;&#32597;&#35265;&#25110;&#20960;&#20046;&#26410;&#35265;&#36807;&#30340;&#21333;&#35789;&#65292;&#24182;&#22312;16&#39033;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#26356;&#22823;&#30340;&#21442;&#25968;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.01349</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Masked Language Modeling. (arXiv:2212.01349v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01349
&lt;/p&gt;
&lt;p&gt;
NPM&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#38750;&#21442;&#25968;&#20998;&#24067;&#26367;&#25442;softmax&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#31232;&#26377;&#27169;&#24335;&#21644;&#39044;&#27979;&#32597;&#35265;&#25110;&#20960;&#20046;&#26410;&#35265;&#36807;&#30340;&#21333;&#35789;&#65292;&#24182;&#22312;16&#39033;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#26356;&#22823;&#30340;&#21442;&#25968;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36890;&#36807;&#26377;&#38480;&#35789;&#27719;&#34920;&#19978;&#30340; softmax &#26469;&#39044;&#27979;&#26631;&#35760;&#65292;&#36825;&#21487;&#33021;&#20351;&#24471;&#39044;&#27979;&#31232;&#26377;&#26631;&#35760;&#25110;&#30701;&#35821;&#21464;&#24471;&#22256;&#38590;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102; NPM&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#23545;&#27599;&#20010;&#21442;&#32771;&#35821;&#26009;&#24211;&#20013;&#30701;&#35821;&#30340;&#38750;&#21442;&#25968;&#20998;&#24067;&#26367;&#25442;&#27492; softmax &#30340;&#38750;&#21442;&#25968;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#12290;NPM &#20165;&#36890;&#36807;&#20174;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#26631;&#35760;&#26469;&#22635;&#20889; [MASK]&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; NPM &#21487;&#20197;&#36890;&#36807;&#23545;&#27604;&#24615;&#30446;&#26631;&#21644;&#25209;&#37327;&#36817;&#20284;&#20840;&#35821;&#26009;&#24211;&#26816;&#32034;&#26377;&#25928;&#22320;&#35757;&#32451;&#12290;&#23545; 16 &#39033;&#20219;&#21153;&#36827;&#34892;&#38646;&#26679;&#26412;&#35780;&#20272;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#20107;&#23454;&#25506;&#38024;&#21644;&#38382;&#39064;&#22238;&#31572;&#65292;&#35777;&#26126; NPM &#36229;&#36807;&#20102;&#26174;&#30528;&#26356;&#22823;&#30340;&#21442;&#25968;&#27169;&#22411;&#65292;&#26080;&#35770;&#20351;&#29992;&#25110;&#19981;&#20351;&#29992;&#26816;&#32034;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#22312;&#22788;&#29702;&#31232;&#26377;&#27169;&#24335;&#65288;&#35789;&#20041;&#25110;&#20107;&#23454;&#65289;&#21644;&#39044;&#27979;&#32597;&#35265;&#25110;&#20960;&#20046;&#26410;&#35265;&#36807;&#30340;&#21333;&#35789;&#65288;&#22914;&#38750;&#25289;&#19969;&#25991;&#33050;&#26412;&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312; github.com/facebookresearch/NPM &#19978;&#21457;&#24067;&#20102;&#27169;&#22411;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing language models (LMs) predict tokens with a softmax over a finite vocabulary, which can make it difficult to predict rare tokens or phrases. We introduce NPM, the first nonparametric masked language model that replaces this softmax with a nonparametric distribution over every phrase in a reference corpus. NPM fills in the [MASK] solely from retrieving a token from a text corpus. We show that NPM can be efficiently trained with a contrastive objective and an in-batch approximation to full corpus retrieval. Zero-shot evaluation on 16 tasks including classification, fact probing and question answering demonstrates that NPM outperforms significantly larger parametric models, either with or without a retrieve-and-generate approach. It is particularly better at dealing with rare patterns (word senses or facts) and predicting rare or nearly unseen words (e.g., non-Latin script). We release the model and code at github.com/facebookresearch/NPM.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36866;&#29992;&#20110;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#30340;&#36866;&#24403;&#27491;&#20132;&#20998;&#35299;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21487;&#36890;&#36807;&#25214;&#21040;&#31561;&#25928;&#24615;&#30340;&#20302;&#38454;&#34920;&#31034;&#24418;&#24335;&#26469;&#26367;&#25442;&#39640;&#32500;&#30340;ESN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;POD&#30340;ESN&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;&#29366;&#24577;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20351;&#20854;&#22312;&#35745;&#31639;&#26102;&#38388;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2211.17179</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#30340;&#36866;&#24403;&#27491;&#20132;&#20998;&#35299;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
Investigation of Proper Orthogonal Decomposition for Echo State Networks. (arXiv:2211.17179v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36866;&#29992;&#20110;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#30340;&#36866;&#24403;&#27491;&#20132;&#20998;&#35299;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21487;&#36890;&#36807;&#25214;&#21040;&#31561;&#25928;&#24615;&#30340;&#20302;&#38454;&#34920;&#31034;&#24418;&#24335;&#26469;&#26367;&#25442;&#39640;&#32500;&#30340;ESN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;POD&#30340;ESN&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;&#29366;&#24577;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20351;&#20854;&#22312;&#35745;&#31639;&#26102;&#38388;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#65288;ESN&#65289;&#26159;&#19968;&#31181;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#34920;&#31034;&#26102;&#38388;&#24207;&#21015;&#21644;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#26041;&#38754;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#34429;&#28982;&#23427;&#20204;&#37197;&#22791;&#20102;&#38750;&#24120;&#26377;&#25928;&#30340;&#35757;&#32451;&#31243;&#24207;&#65292;&#20363;&#22914;ESN&#36825;&#26679;&#30340;&#20648;&#30041;&#35745;&#31639;&#31574;&#30053;&#65292;&#20294;&#21364;&#38656;&#35201;&#39640;&#38454;&#32593;&#32476;&#21363;&#35768;&#22810;&#31070;&#32463;&#20803;&#65292;&#23548;&#33268;&#29366;&#24577;&#25968;&#37327;&#36828;&#39640;&#20110;&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#25968;&#37327;&#12290;&#22823;&#37327;&#30340;&#29366;&#24577;&#19981;&#20165;&#20250;&#22686;&#21152;&#26102;&#38388;&#27493;&#35745;&#31639;&#30340;&#25104;&#26412;&#65292;&#36824;&#20250;&#23548;&#33268;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#23558;ESN&#24212;&#29992;&#20110;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#21644;&#20854;&#20182;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#26102;&#12290;&#19968;&#31181;&#36991;&#20813;&#36825;&#31181;&#22797;&#26434;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#27169;&#22411;&#38454;&#25968;&#38477;&#20302;&#31574;&#30053;&#65292;&#20363;&#22914;&#36866;&#24403;&#27491;&#20132;&#20998;&#35299;&#65288;POD&#65289;&#21450;&#20854;&#21464;&#20307;&#65288;POD-DEIM&#65289;&#65292;&#20174;&#32780;&#25214;&#21040;&#19968;&#20010;&#31561;&#25928;&#30340;&#20302;&#38454;&#34920;&#31034;&#24418;&#24335;&#26469;&#26367;&#25442;&#24050;&#32463;&#35757;&#32451;&#36807;&#30340;&#39640;&#32500;ESN&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#24182;&#20998;&#26512;&#36866;&#29992;&#20110;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#30340;&#36866;&#24403;&#27491;&#20132;&#20998;&#35299;&#65288;POD&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#21644;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#26088;&#22312;&#34920;&#26126;&#22914;&#20309;&#36890;&#36807;POD&#26377;&#25928;&#22320;&#20943;&#23569;ESN&#30340;&#22797;&#26434;&#24230;&#65292;&#25214;&#21040;&#19968;&#20010;&#20302;&#38454;&#34920;&#31034;&#24418;&#24335;&#65292;&#20174;&#32780;&#20445;&#30041;ESN&#30340;&#21160;&#24577;&#24615;&#21644;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;POD&#30340;ESN&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;&#29366;&#24577;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20351;&#20854;&#22312;&#35745;&#31639;&#26102;&#38388;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Echo State Networks (ESN) are a type of Recurrent Neural Network that yields promising results in representing time series and nonlinear dynamic systems. Although they are equipped with a very efficient training procedure, Reservoir Computing strategies, such as the ESN, require high-order networks, i.e., many neurons, resulting in a large number of states that are magnitudes higher than the number of model inputs and outputs. A large number of states not only makes the time-step computation more costly but also may pose robustness issues, especially when applying ESNs to problems such as Model Predictive Control (MPC) and other optimal control problems. One way to circumvent this complexity issue is through Model Order Reduction strategies such as the Proper Orthogonal Decomposition (POD) and its variants (POD-DEIM), whereby we find an equivalent lower order representation to an already trained high dimension ESN. To this end, this work aims to investigate and analyze the performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#28304;&#25968;&#25454;&#34892;&#20026;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#28508;&#21464;&#37327;&#27169;&#22411;&#25512;&#26029;&#31574;&#30053;&#26469;&#20811;&#26381;&#25968;&#25454;&#24322;&#26500;&#24615;&#23548;&#33268;&#30340;&#34892;&#20026;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2211.16078</link><description>&lt;p&gt;
&#22810;&#28304;&#25968;&#25454;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#34892;&#20026;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Behavior Estimation from Multi-Source Data for Offline Reinforcement Learning. (arXiv:2211.16078v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#28304;&#25968;&#25454;&#34892;&#20026;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#28508;&#21464;&#37327;&#27169;&#22411;&#25512;&#26029;&#31574;&#30053;&#26469;&#20811;&#26381;&#25968;&#25454;&#24322;&#26500;&#24615;&#23548;&#33268;&#30340;&#34892;&#20026;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30001;&#20110;&#25968;&#25454;&#25928;&#29575;&#39640;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#34892;&#20026;&#20272;&#35745;&#65292;&#36825;&#26159;&#35768;&#22810;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#30784;&#20219;&#21153;&#12290;&#34892;&#20026;&#20272;&#35745;&#30340;&#30446;&#26631;&#26159;&#20272;&#35745;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#20174;&#22810;&#20010;&#26469;&#28304;&#25910;&#38598;&#25968;&#25454;&#30340;&#22330;&#26223;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24573;&#30053;&#25968;&#25454;&#24322;&#26500;&#24615;&#65292;&#29616;&#26377;&#30340;&#34892;&#20026;&#20272;&#35745;&#26041;&#27861;&#20250;&#20986;&#29616;&#34892;&#20026;&#38169;&#35823;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#32570;&#38519;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28508;&#21464;&#37327;&#27169;&#22411;&#26469;&#20174;&#25968;&#25454;&#20013;&#25512;&#26029;&#19968;&#32452;&#31574;&#30053;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#20351;&#29992;&#26368;&#33021;&#25551;&#36848;&#29305;&#23450;&#36712;&#36857;&#30340;&#31574;&#30053;&#20316;&#20026;&#34892;&#20026;&#31574;&#30053;&#12290;&#35813;&#27169;&#22411;&#20026;&#22810;&#28304;&#25968;&#25454;&#25552;&#20379;&#20102;&#20195;&#29702;&#30340;&#31934;&#32454;&#21270;&#25551;&#36848;&#65292;&#24182;&#24110;&#21161;&#23427;&#20811;&#26381;&#34892;&#20026;&#38169;&#35823;&#12290;&#26412;&#25991;&#36824;&#20026;&#35813;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;Soft Actor-Critic&#65288;SAC&#65289;&#26469;&#22788;&#29702;&#22810;&#28304;&#25968;&#25454;&#65292;&#35828;&#26126;&#20102;&#23427;&#30340;&#23454;&#38469;&#29992;&#36884;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26367;&#20195;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) have received rising interest due to its appealing data efficiency. The present study addresses behavior estimation, a task that lays the foundation of many offline RL algorithms. Behavior estimation aims at estimating the policy with which training data are generated. In particular, this work considers a scenario where the data are collected from multiple sources. In this case, neglecting data heterogeneity, existing approaches for behavior estimation suffers from behavior misspecification. To overcome this drawback, the present study proposes a latent variable model to infer a set of policies from data, which allows an agent to use as behavior policy the policy that best describes a particular trajectory. This model provides with a agent fine-grained characterization for multi-source data and helps it overcome behavior misspecification. This work also proposes a learning algorithm for this model and illustrates its practical usage via extending an 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32422;&#26463;TPE&#65288;c-TPE&#65289;&#26041;&#27861;&#65292;&#26159;&#26641;&#24418;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#30340;&#25193;&#23637;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#22312;&#24615;&#33021;&#35201;&#27714;&#20043;&#19978;&#26045;&#21152;&#30340;&#32422;&#26463;&#38480;&#21046;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;81&#20010;&#26114;&#36149;&#30340;HPO&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#26368;&#20339;&#24615;&#33021;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2211.14411</link><description>&lt;p&gt;
c-TPE:&#22522;&#20110;&#26641;&#24418;&#32467;&#26500;&#30340;&#24102;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#24085;&#25463;&#26031;&#29305;&#20272;&#35745;&#22120;&#29992;&#20110;&#26114;&#36149;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
c-TPE: Tree-structured Parzen Estimator with Inequality Constraints for Expensive Hyperparameter Optimization. (arXiv:2211.14411v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32422;&#26463;TPE&#65288;c-TPE&#65289;&#26041;&#27861;&#65292;&#26159;&#26641;&#24418;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#30340;&#25193;&#23637;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#22312;&#24615;&#33021;&#35201;&#27714;&#20043;&#19978;&#26045;&#21152;&#30340;&#32422;&#26463;&#38480;&#21046;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;81&#20010;&#26114;&#36149;&#30340;HPO&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#26368;&#20339;&#24615;&#33021;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#24378;&#22823;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#23454;&#38469;&#24212;&#29992;&#36890;&#24120;&#20250;&#22312;&#24615;&#33021;&#35201;&#27714;&#20043;&#19978;&#26045;&#21152;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#20869;&#23384;&#20351;&#29992;&#25110;&#24310;&#36831;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32422;&#26463;TPE&#65288;c-TPE&#65289;&#65292;&#36825;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#22810;&#21151;&#33021;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#8212;&#8212;&#26641;&#24418;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#30340;&#25193;&#23637;&#65292;&#20197;&#22788;&#29702;&#36825;&#20123;&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25193;&#23637;&#19981;&#20165;&#26159;&#31616;&#21333;&#22320;&#23558;&#29616;&#26377;&#25910;&#30410;&#20989;&#25968;&#21644;&#21407;&#22987;TPE&#32452;&#21512;&#36215;&#26469;&#65292;&#32780;&#26159;&#21253;&#25324;&#20462;&#25913;&#26469;&#35299;&#20915;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;&#32463;&#39564;&#21644;&#29702;&#35770;&#19978;&#28145;&#20837;&#20998;&#26512;&#36825;&#20123;&#20462;&#25913;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#23427;&#20204;&#22914;&#20309;&#26377;&#25928;&#22320;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#30340;&#35265;&#35299;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;c-TPE&#22312;81&#20010;&#26114;&#36149;&#30340;HPO&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#26368;&#20339;&#30340;&#24179;&#22343;&#25490;&#21517;&#24615;&#33021;&#65292;&#20855;&#26377;&#32479;&#35745;&#26174;&#30528;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization (HPO) is crucial for strong performance of deep learning algorithms and real-world applications often impose some constraints, such as memory usage, or latency on top of the performance requirement. In this work, we propose constrained TPE (c-TPE), an extension of the widely-used versatile Bayesian optimization method, tree-structured Parzen estimator (TPE), to handle these constraints. Our proposed extension goes beyond a simple combination of an existing acquisition function and the original TPE, and instead includes modifications that address issues that cause poor performance. We thoroughly analyze these modifications both empirically and theoretically, providing insights into how they effectively overcome these challenges. In the experiments, we demonstrate that c-TPE exhibits the best average rank performance among existing methods with statistical significance on 81 expensive HPO settings.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#20351;&#29992;MCFS&#31639;&#27861;&#21512;&#25104;UNSAT&#27714;&#35299;&#22120;&#30340;&#26041;&#27861;&#65292;&#31639;&#27861;&#21487;&#29992;&#20110;&#35299;&#20915;&#21253;&#25324;SAT&#20844;&#24335;&#19981;&#21487;&#28385;&#36275;&#24615;&#35777;&#26126;&#12289;&#21487;&#28385;&#36275;SAT&#20844;&#24335;&#35299;&#30340;&#25968;&#37327;&#35745;&#25968;&#21644;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#26368;&#20248;&#35299;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#21512;&#25104;&#26862;&#26519;&#26500;&#24314;&#31639;&#27861;&#21644;&#21512;&#25104;MDP&#31867;&#26469;&#36991;&#20813;&#26500;&#24314;&#20505;&#36873;&#26641;&#26862;&#26519;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.12581</link><description>&lt;p&gt;
&#36890;&#36807;Monte Carlo Forest Search&#23454;&#29616;UNSAT&#27714;&#35299;&#22120;&#30340;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
UNSAT Solver Synthesis via Monte Carlo Forest Search. (arXiv:2211.12581v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12581
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#20351;&#29992;MCFS&#31639;&#27861;&#21512;&#25104;UNSAT&#27714;&#35299;&#22120;&#30340;&#26041;&#27861;&#65292;&#31639;&#27861;&#21487;&#29992;&#20110;&#35299;&#20915;&#21253;&#25324;SAT&#20844;&#24335;&#19981;&#21487;&#28385;&#36275;&#24615;&#35777;&#26126;&#12289;&#21487;&#28385;&#36275;SAT&#20844;&#24335;&#35299;&#30340;&#25968;&#37327;&#35745;&#25968;&#21644;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#26368;&#20248;&#35299;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#21512;&#25104;&#26862;&#26519;&#26500;&#24314;&#31639;&#27861;&#21644;&#21512;&#25104;MDP&#31867;&#26469;&#36991;&#20813;&#26500;&#24314;&#20505;&#36873;&#26641;&#26862;&#26519;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Monte Carlo Forest Search&#65288;MCFS&#65289;&#65292;&#19968;&#31867;&#29992;&#20110;&#23398;&#20064;&#20915;&#31574;&#26641;MDP&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#12290;&#36825;&#20123;&#38382;&#39064;&#30340;&#31034;&#20363;&#21253;&#25324;&#35777;&#26126;SAT&#20844;&#24335;&#30340;&#19981;&#21487;&#28385;&#36275;&#24615;&#65307;&#35745;&#31639;&#21487;&#28385;&#36275;&#30340;SAT&#20844;&#24335;&#30340;&#35299;&#30340;&#25968;&#37327;&#65307;&#20197;&#21450;&#25214;&#21040;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#26368;&#20248;&#35299;&#12290;MCFS&#31639;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;Monte Carlo Tree Search&#65288;MCTS&#65289;&#30340;&#25193;&#23637;&#65292;&#29992;&#20110;&#22312;&#20505;&#36873;&#26641;&#30340;&#26862;&#26519;&#20013;&#23547;&#25214;&#19968;&#20010;&#23567;&#26641;&#65292;&#32780;&#19981;&#26159;&#22312;&#26641;&#20013;&#25214;&#21040;&#19968;&#20010;&#22909;&#36335;&#24452;&#65288;&#35299;&#20915;&#26041;&#26696;&#65289;&#12290;&#25105;&#20204;&#22312;&#31639;&#27861;&#20013;&#23454;&#20363;&#21270;&#21644;&#35780;&#20272;&#20102;&#33258;&#24049;&#30340;&#24819;&#27861;&#65292;&#31216;&#20043;&#20026;Knuth Synthesis&#65292;&#36825;&#26159;&#19968;&#20010;MCFS&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;DPLL&#20998;&#25903;&#31574;&#30053;&#26469;&#35299;&#20915;&#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#65288;SAT&#65289;&#38382;&#39064;&#12290;&#36825;&#21033;&#29992;&#20102;&#20004;&#20010;&#20851;&#38190;&#24605;&#24819;&#65292;&#20197;&#36991;&#20813;&#26500;&#24314;&#20505;&#36873;&#26641;&#26862;&#26519;&#30340;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19968;&#31181;&#21512;&#25104;&#26862;&#26519;&#26500;&#24314;&#31639;&#27861;&#65292;&#36890;&#36807;&#20174;&#27744;&#20013;&#38543;&#26426;&#36873;&#25321;&#8220;&#22909;&#8221;&#30340;&#26641;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;&#26356;&#22823;&#30340;&#26862;&#26519;&#26469;&#36880;&#27493;&#26500;&#24314;&#26862;&#26519;&#65307;&#65288;2&#65289;&#19968;&#31181;&#21512;&#25104;MDP&#31867;&#65292;&#29992;&#20316;&#30495;&#23454;&#26641;MDP&#30340;&#20195;&#29702;&#65292;&#25105;&#20204;&#21487;&#20197;&#36731;&#26494;&#35745;&#31639;&#33410;&#28857;&#38388;&#36716;&#25442;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Monte Carlo Forest Search (MCFS), a class of reinforcement learning (RL) algorithms for learning policies in {tree MDPs}, for which policy execution involves traversing an exponential-sized tree. Examples of such problems include proving unsatisfiability of a SAT formula; counting the number of solutions of a satisfiable SAT formula; and finding the optimal solution to a mixed-integer program. MCFS algorithms can be seen as extensions of Monte Carlo Tree Search (MCTS) to cases where, rather than finding a good path (solution) within a tree, the problem is to find a small tree within a forest of candidate trees. We instantiate and evaluate our ideas in an algorithm that we dub Knuth Synthesis, an MCFS algorithm that learns DPLL branching policies for solving the Boolean satisfiability (SAT) problem, with the objective of achieving good average-case performance on a given distribution of unsatisfiable problem instances. Knuth Synthesis leverages two key ideas to avoid the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#26399;&#38388;&#20250;&#20986;&#29616;&#20998;&#24067;&#24335;&#31616;&#21333;&#24615;&#20559;&#24046;&#65288;DSB&#65289;&#65292;&#21363;&#26368;&#21021;&#20351;&#29992;&#20302;&#38454;&#36755;&#20837;&#32479;&#35745;&#26469;&#20998;&#31867;&#36755;&#20837;&#65292;&#21482;&#26377;&#22312;&#35757;&#32451;&#21518;&#26399;&#25165;&#21033;&#29992;&#26356;&#39640;&#38454;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2211.11567</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26085;&#30410;&#22797;&#26434;&#30340;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Neural networks trained with SGD learn distributions of increasing complexity. (arXiv:2211.11567v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#26399;&#38388;&#20250;&#20986;&#29616;&#20998;&#24067;&#24335;&#31616;&#21333;&#24615;&#20559;&#24046;&#65288;DSB&#65289;&#65292;&#21363;&#26368;&#21021;&#20351;&#29992;&#20302;&#38454;&#36755;&#20837;&#32479;&#35745;&#26469;&#20998;&#31867;&#36755;&#20837;&#65292;&#21482;&#26377;&#22312;&#35757;&#32451;&#21518;&#26399;&#25165;&#21033;&#29992;&#26356;&#39640;&#38454;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21363;&#20351;&#22312;&#25554;&#20540;&#35757;&#32451;&#25968;&#25454;&#26102;&#20063;&#33021;&#24456;&#22909;&#22320;&#36827;&#34892;&#27867;&#21270;&#30340;&#33021;&#21147;&#24050;&#32463;&#36890;&#36807;&#21508;&#31181;&#8220;&#31616;&#21333;&#24615;&#20559;&#24046;&#8221;&#24471;&#21040;&#20102;&#35299;&#37322;&#12290;&#36825;&#20123;&#29702;&#35770;&#20551;&#35774;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#26356;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#20043;&#21069;&#20808;&#23398;&#20064;&#31616;&#21333;&#20989;&#25968;&#65292;&#20363;&#22914;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#21516;&#26102;&#65292;&#25968;&#25454;&#32467;&#26500;&#20063;&#34987;&#35748;&#20026;&#26159;&#33391;&#22909;&#27867;&#21270;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#28982;&#32780;&#65292;&#25968;&#25454;&#32467;&#26500;&#22312;&#31616;&#21333;&#24615;&#20559;&#24046;&#20013;&#30340;&#20316;&#29992;&#23578;&#26410;&#34987;&#29702;&#35299;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26368;&#21021;&#20351;&#29992;&#20302;&#38454;&#36755;&#20837;&#32479;&#35745;&#65288;&#22914;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#65289;&#26469;&#23545;&#20854;&#36755;&#20837;&#36827;&#34892;&#20998;&#31867;&#65292;&#21482;&#26377;&#22312;&#35757;&#32451;&#21518;&#26399;&#25165;&#21033;&#29992;&#26356;&#39640;&#38454;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#31070;&#32463;&#32593;&#32476;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#21487;&#35299;&#27169;&#22411;&#20013;&#23637;&#31034;&#20102;&#36825;&#31181;&#20998;&#24067;&#24335;&#31616;&#21333;&#24615;&#20559;&#24046;&#65288;DSB&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#20110;CIFAR10&#19978;&#30340;&#19968;&#31995;&#21015;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#20013;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;DSB&#65292;&#29978;&#33267;&#21457;&#29616;&#35813;&#20559;&#24046;&#22312;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#21644;&#26356;&#24191;&#27867;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#20013;&#20063;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability of deep neural networks to generalise well even when they interpolate their training data has been explained using various "simplicity biases". These theories postulate that neural networks avoid overfitting by first learning simple functions, say a linear classifier, before learning more complex, non-linear functions. Meanwhile, data structure is also recognised as a key ingredient for good generalisation, yet its role in simplicity biases is not yet understood. Here, we show that neural networks trained using stochastic gradient descent initially classify their inputs using lower-order input statistics, like mean and covariance, and exploit higher-order statistics only later during training. We first demonstrate this distributional simplicity bias (DSB) in a solvable model of a neural network trained on synthetic data. We empirically demonstrate DSB in a range of deep convolutional networks and visual transformers trained on CIFAR10, and show that it even holds in network
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#38477;&#20302;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#32463;&#36807;&#27979;&#35797;&#22312;&#20302;&#36164;&#28304;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.08794</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#30340;&#40065;&#26834;&#24615;&#20302;&#36164;&#28304;&#24494;&#35843;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations. (arXiv:2211.08794v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#38477;&#20302;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#32463;&#36807;&#27979;&#35797;&#22312;&#20302;&#36164;&#28304;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21442;&#25968;&#30340;&#24040;&#22823;&#25968;&#37327;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#24494;&#35843;&#23481;&#26131;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#20986;&#29616;&#36807;&#24230;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;PLM&#30340;&#38544;&#34255;&#34920;&#31034;&#19978;&#25805;&#20316;&#65292;&#20197;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;PLM&#30340;&#38544;&#34255;&#23618;&#20043;&#38388;&#25554;&#20837;&#38543;&#26426;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#26469;&#33258;&#21069;&#19968;&#23618;&#30340;&#28608;&#27963;&#36716;&#25442;&#20026;&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#20854;&#39304;&#36865;&#21040;&#19978;&#23618;&#12290;&#24494;&#35843;&#32467;&#26463;&#21518;&#65292;&#33258;&#32534;&#30721;&#22120;&#20250;&#34987;&#31227;&#38500;&#25481;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#19981;&#20250;&#22686;&#21152;&#39069;&#22806;&#30340;&#21442;&#25968;&#25110;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#24207;&#21015;&#21644;&#26631;&#35760;&#32423;&#21035;&#30340;&#20302;&#36164;&#28304;NLP&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the huge amount of parameters, fine-tuning of pretrained language models (PLMs) is prone to overfitting in the low resource scenarios. In this work, we present a novel method that operates on the hidden representations of a PLM to reduce overfitting. During fine-tuning, our method inserts random autoencoders between the hidden layers of a PLM, which transform activations from the previous layers into a multi-view compressed representation before feeding it into the upper layers. The autoencoders are plugged out after fine-tuning, so our method does not add extra parameters or increase computation cost during inference. Our method demonstrates promising performance improvement across a wide range of sequence- and token-level low-resource NLP tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#35299;&#20915;&#35768;&#22810;&#22270;&#20687;&#20219;&#21153;(&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;)&#26102;&#21487;&#20197;&#24573;&#30053;&#20559;&#32622;&#65292;&#24182;&#19988;&#38646;&#20559;&#32622;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#38469;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#21516;&#26102;&#20855;&#26377;&#26631;&#37327; (&#20056;&#27861;) &#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#22312;&#25913;&#21464;&#23545;&#27604;&#24230;&#26102;&#20173;&#33021;&#20445;&#25345;&#39044;&#27979;&#19981;&#21464;&#12290;</title><link>http://arxiv.org/abs/2211.08486</link><description>&lt;p&gt;
&#38646;&#20559;&#32622;&#26631;&#37327;&#19981;&#21464;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Scalar Invariant Networks with Zero Bias. (arXiv:2211.08486v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#35299;&#20915;&#35768;&#22810;&#22270;&#20687;&#20219;&#21153;(&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;)&#26102;&#21487;&#20197;&#24573;&#30053;&#20559;&#32622;&#65292;&#24182;&#19988;&#38646;&#20559;&#32622;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#38469;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#21516;&#26102;&#20855;&#26377;&#26631;&#37327; (&#20056;&#27861;) &#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#22312;&#25913;&#21464;&#23545;&#27604;&#24230;&#26102;&#20173;&#33021;&#20445;&#25345;&#39044;&#27979;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#26435;&#37325;&#19968;&#26679;&#65292;&#20559;&#32622;&#39033;&#20063;&#26159;&#35768;&#22810;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;(&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;)&#21487;&#23398;&#20064;&#30340;&#21442;&#25968;&#12290;&#20154;&#20204;&#35748;&#20026;&#20559;&#24046;&#33021;&#26377;&#25928;&#22320;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#33021;&#21147;&#26469;&#35299;&#20915;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#25105;&#20204;&#20174;&#31532;&#19968;&#21407;&#29702;&#32771;&#34385;&#22270;&#20687;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#20869;&#22312;&#20998;&#24067;&#20197;&#21450;&#27169;&#22411;&#24212;&#20855;&#26377;&#30340;&#19968;&#20123;&#26399;&#26395;&#29305;&#24615;&#65292;&#21017;&#20559;&#24046;&#21487;&#20197;&#23436;&#20840;&#24573;&#30053;&#65292;&#20197;&#35299;&#20915;&#35768;&#22810;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#32467;&#26524;&#34920;&#26126;&#65292;&#38646;&#20559;&#32622;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#38469;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#21487;&#33021;&#19982;&#24102;&#20559;&#32622;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#38646;&#20559;&#32622;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#31216;&#20026;&#26631;&#37327;(&#20056;&#27861;)&#19981;&#21464;&#24615;&#30340;&#33391;&#22909;&#23646;&#24615;&#65292;&#36825;&#20351;&#24471;&#24403;&#25913;&#21464;&#36755;&#20837;&#22270;&#20687;&#30340;&#23545;&#27604;&#24230;&#26102;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#20445;&#25345;&#19981;&#21464;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#26631;&#37327;&#19981;&#21464;&#24615;&#25193;&#23637;&#21040;&#26356;&#19968;&#33324;&#30340;&#24773;&#20917;&#8230;
&lt;/p&gt;
&lt;p&gt;
Just like weights, bias terms are the learnable parameters of many popular machine learning models, including neural networks. Biases are believed to effectively increase the representational power of neural networks to solve a wide range of tasks in computer vision. However, we argue that if we consider the intrinsic distribution of images in the input space as well as some desired properties a model should have from the first principles, biases can be completely ignored in addressing many image-related tasks, such as image classification. Our observation indicates that zero-bias neural networks could perform comparably to neural networks with bias at least on practical image classification tasks. In addition, we prove that zero-bias neural networks possess a nice property called scalar (multiplication) invariance, which allows the prediction of neural networks remains the same when altering the contrast of the input image. We then extend scalar invariance to more general cases that a
&lt;/p&gt;</description></item><item><title>PAD-Net&#26159;&#19968;&#20010;&#37096;&#20998;&#21160;&#24577;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#23558;&#20887;&#20313;&#30340;&#21160;&#24577;&#21442;&#25968;&#36716;&#25442;&#20026;&#38745;&#24577;&#21442;&#25968;&#65292;&#25552;&#39640;&#20102;&#21160;&#24577;&#32593;&#32476;&#30340;&#25928;&#29575;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.05528</link><description>&lt;p&gt;
PAD-Net&#65306;&#29992;&#20110;&#21160;&#24577;&#32593;&#32476;&#30340;&#39640;&#25928;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PAD-Net: An Efficient Framework for Dynamic Networks. (arXiv:2211.05528v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05528
&lt;/p&gt;
&lt;p&gt;
PAD-Net&#26159;&#19968;&#20010;&#37096;&#20998;&#21160;&#24577;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#23558;&#20887;&#20313;&#30340;&#21160;&#24577;&#21442;&#25968;&#36716;&#25442;&#20026;&#38745;&#24577;&#21442;&#25968;&#65292;&#25552;&#39640;&#20102;&#21160;&#24577;&#32593;&#32476;&#30340;&#25928;&#29575;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#32593;&#32476;&#65292;&#20363;&#22914;&#21160;&#24577;&#21367;&#31215;&#65288;DY-Conv&#65289;&#21644;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;MoE&#65289;&#65292;&#24050;&#34987;&#24191;&#27867;&#25506;&#32034;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#21487;&#25509;&#21463;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#23454;&#29616;&#21160;&#24577;&#32593;&#32476;&#30340;&#24120;&#35265;&#20570;&#27861;&#26159;&#23558;&#32473;&#23450;&#30340;&#38745;&#24577;&#23618;&#36716;&#25442;&#20026;&#23436;&#20840;&#21160;&#24577;&#30340;&#23618;&#65292;&#20854;&#20013;&#25152;&#26377;&#21442;&#25968;&#37117;&#26159;&#21160;&#24577;&#30340;&#65288;&#33267;&#23569;&#22312;&#21333;&#20010;&#23618;&#20869;&#65289;&#24182;&#38543;&#36755;&#20837;&#21464;&#21270;&#12290;&#20294;&#26159;&#65292;&#36825;&#31181;&#23436;&#20840;&#21160;&#24577;&#30340;&#35774;&#32622;&#21487;&#33021;&#20250;&#23548;&#33268;&#20887;&#20313;&#21442;&#25968;&#21644;&#39640;&#37096;&#32626;&#25104;&#26412;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#21160;&#24577;&#32593;&#32476;&#22312;&#26356;&#24191;&#27867;&#30340;&#20219;&#21153;&#21644;&#27169;&#22411;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25361;&#25112;&#21160;&#24577;&#32593;&#32476;&#30340;&#22522;&#26412;&#24120;&#35782;&#65292;&#24182;&#25552;&#20986;&#37096;&#20998;&#21160;&#24577;&#32593;&#32476;&#65292;&#21363;PAD-Net&#65292;&#20197;&#23558;&#20887;&#20313;&#21160;&#24577;&#21442;&#25968;&#36716;&#25442;&#20026;&#38745;&#24577;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#36845;&#20195;&#27169;&#24335;&#20998;&#21306;&#26469;&#26377;&#25928;&#22320;&#20998;&#21306;&#21160;&#24577;&#21644;&#38745;&#24577;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#22823;&#35268;&#27169;&#23454;&#39564;&#30340;&#20840;&#38754;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic networks, e.g., Dynamic Convolution (DY-Conv) and the Mixture of Experts (MoE), have been extensively explored as they can considerably improve the model's representation power with acceptable computational cost. The common practice in implementing dynamic networks is to convert the given static layers into fully dynamic ones where all parameters are dynamic (at least within a single layer) and vary with the input. However, such a fully dynamic setting may cause redundant parameters and high deployment costs, limiting the applicability of dynamic networks to a broader range of tasks and models. The main contributions of our work are challenging the basic commonsense in dynamic networks and proposing a partially dynamic network, namely PAD-Net, to transform the redundant dynamic parameters into static ones. Also, we further design Iterative Mode Partition to partition dynamic and static parameters efficiently. Our method is comprehensively supported by large-scale experiments wi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#32034;&#20102;&#25552;&#39640;&#31354;&#20013;&#33258;&#20027;&#25216;&#26415;&#36827;&#27493;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#22914;&#20309;&#29983;&#25104;&#22823;&#37327;&#30340;&#31354;&#20013;&#25968;&#25454;&#38598;&#20197;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#24182;&#21033;&#29992;&#27169;&#25311;&#29615;&#22659;&#21644;&#25968;&#25454;&#22686;&#24378;&#26469;&#35299;&#20915;&#29616;&#26377;&#24037;&#20855;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#25968;&#25454;&#29983;&#25104;&#24037;&#20316;&#27969;&#31243;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.05335</link><description>&lt;p&gt;
&#38761;&#26032;&#31354;&#20013;&#33258;&#20027;&#25216;&#26415;&#30340;&#21487;&#25193;&#23637;&#27169;&#22359;&#21270;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Scalable Modular Synthetic Data Generation for Advancing Aerial Autonomy. (arXiv:2211.05335v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05335
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#32034;&#20102;&#25552;&#39640;&#31354;&#20013;&#33258;&#20027;&#25216;&#26415;&#36827;&#27493;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#22914;&#20309;&#29983;&#25104;&#22823;&#37327;&#30340;&#31354;&#20013;&#25968;&#25454;&#38598;&#20197;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#24182;&#21033;&#29992;&#27169;&#25311;&#29615;&#22659;&#21644;&#25968;&#25454;&#22686;&#24378;&#26469;&#35299;&#20915;&#29616;&#26377;&#24037;&#20855;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#25968;&#25454;&#29983;&#25104;&#24037;&#20316;&#27969;&#31243;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#21160;&#31354;&#20013;&#33258;&#20027;&#25216;&#26415;&#36827;&#27493;&#30340;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#26159;&#33719;&#21462;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#31354;&#20013;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#36991;&#20813;&#23454;&#26426;&#25968;&#25454;&#37319;&#38598;&#30340;&#39640;&#25104;&#26412;&#21644;&#32791;&#26102;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#26080;&#20154;&#26426;&#24212;&#29992;&#24320;&#22987;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#35201;&#24819;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#23558;&#20854;&#36801;&#31227;&#21040;&#23454;&#38469;&#29615;&#22659;&#65292;&#22686;&#21152;&#27169;&#25311;&#29615;&#22659;&#30340;&#22810;&#26679;&#24615;&#20197;&#35757;&#32451;&#25152;&#26377;&#21487;&#33021;&#24773;&#20917;&#19979;&#30340;&#27169;&#22411;&#65292;&#20197;&#21450;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#37117;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#30446;&#21069;&#65292;&#29616;&#26377;&#30340;&#21512;&#25104;&#31354;&#20013;&#25968;&#25454;&#29983;&#25104;&#24037;&#20855;&#35201;&#20040;&#32570;&#20047;&#25968;&#25454;&#22686;&#24378;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#25163;&#24037;&#36127;&#36733;&#25110;&#23454;&#38469;&#26679;&#26412;&#36827;&#34892;&#37197;&#32622;&#21644;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#27169;&#25311;&#22330;&#26223;&#20197;&#36827;&#34892;&#25968;&#25454;&#37319;&#38598;&#12290;&#36825;&#20123;&#20381;&#36182;&#24615;&#38480;&#21046;&#20102;&#25968;&#25454;&#29983;&#25104;&#24037;&#20316;&#27969;&#31243;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#20013;&#24179;&#34913;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
One major barrier to advancing aerial autonomy has been collecting large-scale aerial datasets for training machine learning models. Due to costly and time-consuming real-world data collection through deploying drones, there has been an increasing shift towards using synthetic data for training models in drone applications. However, to increase widespread generalization and transferring models to real-world, increasing the diversity of simulation environments to train a model over all the varieties and augmenting the training data, has been proved to be essential. Current synthetic aerial data generation tools either lack data augmentation or rely heavily on manual workload or real samples for configuring and generating diverse realistic simulation scenes for data collection. These dependencies limit scalability of the data generation workflow. Accordingly, there is a major challenge in balancing generalizability and scalability in synthetic data generation. To address these gaps, we i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#38646;&#35757;&#32451;&#35823;&#24046;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#20013;&#8220;&#22351;&#8221;&#26041;&#26696;&#30340;&#21344;&#27604;&#38543;&#35757;&#32451;&#25968;&#25454;&#30340;&#22686;&#21152;&#32780;&#25351;&#25968;&#32423;&#36882;&#20943;&#65292;&#24182;&#33021;&#35299;&#37322;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#20986;&#20046;&#24847;&#26009;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.03570</link><description>&lt;p&gt;
&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#20998;&#31867;&#22120;&#27867;&#21270;&#33021;&#21147;&#24378;&#65292;&#22240;&#20026;&#31967;&#31957;&#30340;&#35299;&#20915;&#26041;&#26696;&#24456;&#23569;
&lt;/p&gt;
&lt;p&gt;
Highly over-parameterized classifiers generalize since bad solutions are rare. (arXiv:2211.03570v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#38646;&#35757;&#32451;&#35823;&#24046;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#20013;&#8220;&#22351;&#8221;&#26041;&#26696;&#30340;&#21344;&#27604;&#38543;&#35757;&#32451;&#25968;&#25454;&#30340;&#22686;&#21152;&#32780;&#25351;&#25968;&#32423;&#36882;&#20943;&#65292;&#24182;&#33021;&#35299;&#37322;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#20986;&#20046;&#24847;&#26009;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20854;&#20013;&#36890;&#36807;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#23398;&#20064;&#23548;&#33268;&#38646;&#35757;&#32451;&#35823;&#24046;&#12290;&#22312;&#36825;&#20123;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#35774;&#32622;&#20013;&#65292;&#26377;&#35768;&#22810;&#20855;&#26377;&#38646;&#35757;&#32451;&#35823;&#24046;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#65292;&#20854;&#20013;&#19968;&#20123;&#27604;&#20854;&#20182;&#30340;&#26356;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#19968;&#23450;&#30340;&#26465;&#20214;&#19979;&#65292;&#8220;&#22351;&#8221;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#20998;&#25968;&#65292;&#20854;&#30495;&#23454;&#35823;&#24046;&#22823;&#20110;&#949;&#65292;&#19982;&#35757;&#32451;&#25968;&#25454;n&#30340;&#25968;&#37327;&#25351;&#25968;&#32423;&#22320;&#36882;&#20943;&#21040;&#38646;&#12290;&#35813;&#33539;&#22260;&#21462;&#20915;&#20110;&#29992;&#20110;&#32473;&#23450;&#20998;&#31867;&#38382;&#39064;&#30340;&#20998;&#31867;&#22120;&#20989;&#25968;&#38598;&#21512;&#19978;&#30495;&#23454;&#35823;&#24046;&#30340;&#20998;&#24067;&#65292;&#19981;&#19968;&#23450;&#21462;&#20915;&#20110;&#20998;&#31867;&#22120;&#20989;&#25968;&#38598;&#21512;&#30340;&#22823;&#23567;&#25110;&#22797;&#26434;&#24230;&#65288;&#20363;&#22914;&#21442;&#25968;&#25968;&#37327;&#65289;&#12290;&#36825;&#21487;&#33021;&#35299;&#37322;&#20102;&#21363;&#20351;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20063;&#20855;&#26377;&#20986;&#20046;&#24847;&#26009;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;MNIST&#30340;&#23376;&#38598;&#19978;&#36890;&#36807;&#23454;&#39564;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#25968;&#23398;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the generalization of over-parameterized classifiers where Empirical Risk Minimization (ERM) for learning leads to zero training error. In these over-parameterized settings there are many global minima with zero training error, some of which generalize better than others. We show that under certain conditions the fraction of "bad" global minima with a true error larger than {\epsilon} decays to zero exponentially fast with the number of training data n. The bound depends on the distribution of the true error over the set of classifier functions used for the given classification problem, and does not necessarily depend on the size or complexity (e.g. the number of parameters) of the classifier function set. This might explain the unexpectedly good generalization even of highly over-parameterized Neural Networks. We support our mathematical framework with experiments on a synthetic data set and a subset of MNIST.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24037;&#19994;&#29615;&#22659;&#20013;&#40831;&#36718;&#30005;&#26426;&#26411;&#31471;&#27979;&#35797;&#20013;&#22768;&#23398;&#24322;&#24120;&#26816;&#27979;&#30340;&#29305;&#24449;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#20174;&#23545;&#25968;&#21253;&#32476;&#35889;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#21644;&#24515;&#29702;&#22768;&#23398;&#29305;&#24449;&#65292;&#32467;&#21512;&#23396;&#31435;&#26862;&#26519;&#25110;&#35013;&#34955;&#38543;&#26426;&#30719;&#24037;&#31639;&#27861;&#36827;&#34892;&#26816;&#27979;&#65292;&#33021;&#26377;&#25928;&#36991;&#20813;&#22823;&#22810;&#25968;&#24178;&#25200;&#65292;&#20294;&#20351;&#29992;&#38180;&#23376;&#25110;&#27668;&#21387;&#21017;&#20250;&#20135;&#29983;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.01716</link><description>&lt;p&gt;
&#24037;&#19994;&#29615;&#22659;&#20013;&#22768;&#23398;&#24322;&#24120;&#26816;&#27979;&#20013;&#29305;&#24449;&#30340;&#25506;&#35752;&#8212;&#8212;&#20197;&#40831;&#36718;&#30005;&#26426;&#26411;&#31471;&#27979;&#35797;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Discussion of Features for Acoustic Anomaly Detection under Industrial Disturbing Noise in an End-of-Line Test of Geared Motors. (arXiv:2211.01716v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24037;&#19994;&#29615;&#22659;&#20013;&#40831;&#36718;&#30005;&#26426;&#26411;&#31471;&#27979;&#35797;&#20013;&#22768;&#23398;&#24322;&#24120;&#26816;&#27979;&#30340;&#29305;&#24449;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#20174;&#23545;&#25968;&#21253;&#32476;&#35889;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#21644;&#24515;&#29702;&#22768;&#23398;&#29305;&#24449;&#65292;&#32467;&#21512;&#23396;&#31435;&#26862;&#26519;&#25110;&#35013;&#34955;&#38543;&#26426;&#30719;&#24037;&#31639;&#27861;&#36827;&#34892;&#26816;&#27979;&#65292;&#33021;&#26377;&#25928;&#36991;&#20813;&#22823;&#22810;&#25968;&#24178;&#25200;&#65292;&#20294;&#20351;&#29992;&#38180;&#23376;&#25110;&#27668;&#21387;&#21017;&#20250;&#20135;&#29983;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#40831;&#36718;&#30005;&#26426;&#30340;&#26411;&#31471;&#27979;&#35797;&#20013;&#65292;&#20135;&#21697;&#36136;&#37327;&#30340;&#35780;&#20272;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#26102;&#38388;&#38480;&#21046;&#21644;&#21508;&#31181;&#21464;&#20307;&#30340;&#39640;&#24230;&#22810;&#26679;&#24615;&#65292;&#22768;&#23398;&#27979;&#37327;&#27604;&#25391;&#21160;&#27979;&#37327;&#26356;&#32463;&#27982;&#12290;&#28982;&#32780;&#65292;&#22768;&#23398;&#25968;&#25454;&#21463;&#21040;&#24037;&#19994;&#24178;&#25200;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;&#29992;&#20110;&#40831;&#36718;&#30005;&#26426;&#26411;&#31471;&#27979;&#35797;&#20013;&#24322;&#24120;&#26816;&#27979;&#30340;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#12290;&#19968;&#20010;&#30495;&#23454;&#30340;&#25968;&#25454;&#38598;&#34987;&#35760;&#24405;&#19979;&#26469;&#65292;&#24182;&#21253;&#21547;&#26469;&#33258;&#29983;&#20135;&#29615;&#22659;&#30340;&#24037;&#19994;&#22122;&#22768;&#21644;&#31995;&#32479;&#20135;&#29983;&#30340;&#24178;&#25200;&#65292;&#29992;&#20110;&#27604;&#36739;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#24314;&#35758;&#20351;&#29992;&#20174;&#23545;&#25968;&#21253;&#32476;&#35889;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#21644;&#24515;&#29702;&#22768;&#23398;&#29305;&#24449;&#32467;&#21512;&#20351;&#29992;&#65292;&#20351;&#29992;&#23396;&#31435;&#26862;&#26519;&#25110;&#26356;&#36890;&#29992;&#30340;&#35013;&#34955;&#38543;&#26426;&#30719;&#24037;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#22823;&#22810;&#25968;&#24178;&#25200;&#37117;&#21487;&#20197;&#36991;&#20813;&#65292;&#32780;&#20351;&#29992;&#38180;&#23376;&#25110;&#27668;&#21387;&#36890;&#24120;&#20250;&#24341;&#36215;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the end-of-line test of geared motors, the evaluation of product qual-ity is important. Due to time constraints and the high diversity of variants, acous-tic measurements are more economical than vibration measurements. However, the acoustic data is affected by industrial disturbing noise. Therefore, the aim of this study is to investigate the robustness of features used for anomaly detection in geared motor end-of-line testing. A real-world dataset with typical faults and acoustic disturbances is recorded by an acoustic array. This includes industrial noise from the production and systematically produced disturbances, used to compare the robustness. Overall, it is proposed to apply features extracted from a log-envelope spectrum together with psychoacoustic features. The anomaly de-tection is done by using the isolation forest or the more universal bagging random miner. Most disturbances can be circumvented, while the use of a hammer or air pressure often causes problems. In genera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#25968;&#25454;&#26500;&#24314;&#37327;&#23376;&#24577;&#30340;&#20302;&#32500;&#34920;&#31034;&#36827;&#34892;&#30456;&#20284;&#24615;&#35780;&#20272;&#65292;&#21487;&#20197;&#23545;&#38750;&#39640;&#26031;&#37327;&#23376;&#24577;&#36827;&#34892;&#30456;&#20284;&#24615;&#26816;&#27979;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#20197;&#24448;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.01668</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#37327;&#23376;&#30456;&#20284;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Quantum Similarity Testing with Convolutional Neural Networks. (arXiv:2211.01668v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#25968;&#25454;&#26500;&#24314;&#37327;&#23376;&#24577;&#30340;&#20302;&#32500;&#34920;&#31034;&#36827;&#34892;&#30456;&#20284;&#24615;&#35780;&#20272;&#65292;&#21487;&#20197;&#23545;&#38750;&#39640;&#26031;&#37327;&#23376;&#24577;&#36827;&#34892;&#30456;&#20284;&#24615;&#26816;&#27979;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#20197;&#24448;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#20004;&#20010;&#26410;&#32463;&#29305;&#24449;&#21270;&#30340;&#37327;&#23376;&#35774;&#22791;&#26159;&#21542;&#20197;&#30456;&#21516;&#26041;&#24335;&#36816;&#20316;&#23545;&#20110;&#22522;&#20934;&#27979;&#35797;&#36817;&#26399;&#37327;&#23376;&#35745;&#31639;&#26426;&#21644;&#37327;&#23376;&#27169;&#25311;&#22120;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#33267;&#20170;&#20026;&#27490;&#65292;&#22312;&#36830;&#32493;&#21464;&#37327;&#37327;&#23376;&#31995;&#32479;&#20013;&#65292;&#23384;&#22312;&#19968;&#20123;&#26410;&#35299;&#20915;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#26377;&#38480;&#19988;&#22024;&#26434;&#30340;&#25968;&#25454;&#27604;&#36739;&#26410;&#30693;&#30340;&#36830;&#32493;&#21464;&#37327;&#24577;&#12290;&#31639;&#27861;&#36866;&#29992;&#20110;&#38750;&#39640;&#26031;&#37327;&#23376;&#24577;&#65292;&#32780;&#27492;&#21069;&#30340;&#25216;&#26415;&#26080;&#27861;&#23454;&#29616;&#30456;&#20284;&#24615;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#26681;&#25454;&#27979;&#37327;&#25968;&#25454;&#26500;&#24314;&#19968;&#20010;&#20302;&#32500;&#24577;&#34920;&#31034;&#26469;&#35780;&#20272;&#37327;&#23376;&#24577;&#30340;&#30456;&#20284;&#24615;&#12290;&#32593;&#32476;&#21487;&#20197;&#36827;&#34892;&#31163;&#32447;&#35757;&#32451;&#65292;&#20351;&#29992;&#20855;&#26377;&#19982;&#24453;&#27979;&#35797;&#24577;&#30456;&#20284;&#32467;&#26500;&#30340;&#19968;&#32452;&#22522;&#20934;&#24577;&#30340;&#32463;&#20856;&#27169;&#25311;&#25968;&#25454;&#65292;&#25110;&#20351;&#29992;&#23545;&#22522;&#20934;&#24577;&#30340;&#27979;&#37327;&#25152;&#29983;&#25104;&#30340;&#23454;&#39564;&#25968;&#25454;&#65292;&#25110;&#20351;&#29992;&#27169;&#25311;&#21644;&#23454;&#39564;&#25968;&#25454;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#38750;&#39640;&#26031;&#21387;&#32553;&#30456;&#24178;&#24577;&#26469;&#27979;&#35797;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#23427;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#22343;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of testing whether two uncharacterized quantum devices behave in the same way is crucial for benchmarking near-term quantum computers and quantum simulators, but has so far remained open for continuous-variable quantum systems. In this Letter, we develop a machine learning algorithm for comparing unknown continuous variable states using limited and noisy data. The algorithm works on non-Gaussian quantum states for which similarity testing could not be achieved with previous techniques. Our approach is based on a convolutional neural network that assesses the similarity of quantum states based on a lower-dimensional state representation built from measurement data. The network can be trained offline with classically simulated data from a fiducial set of states sharing structural similarities with the states to be tested, or with experimental data generated by measurements on the fiducial states, or with a combination of simulated and experimental data. We test the performance o
&lt;/p&gt;</description></item><item><title>RQUGE&#26159;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#20505;&#36873;&#38382;&#39064;&#26159;&#21542;&#21487;&#20197;&#22238;&#31572;&#26469;&#35780;&#20272;&#38382;&#39064;&#29983;&#25104;&#36136;&#37327;, &#27604;&#29616;&#26377;&#25351;&#26631;&#26356;&#21152;&#31283;&#20581;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20154;&#24037;&#25552;&#20379;&#21442;&#32771;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2211.01482</link><description>&lt;p&gt;
RQUGE&#65306;&#19968;&#31181;&#22522;&#20110;&#22238;&#31572;&#38382;&#39064;&#35780;&#20272;&#38382;&#39064;&#29983;&#25104;&#30340;&#26080;&#21442;&#32771;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RQUGE: Reference-Free Metric for Evaluating Question Generation by Answering the Question. (arXiv:2211.01482v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01482
&lt;/p&gt;
&lt;p&gt;
RQUGE&#26159;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#20505;&#36873;&#38382;&#39064;&#26159;&#21542;&#21487;&#20197;&#22238;&#31572;&#26469;&#35780;&#20272;&#38382;&#39064;&#29983;&#25104;&#36136;&#37327;, &#27604;&#29616;&#26377;&#25351;&#26631;&#26356;&#21152;&#31283;&#20581;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20154;&#24037;&#25552;&#20379;&#21442;&#32771;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#35780;&#20272;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#36136;&#37327;&#30340;&#25351;&#26631;&#65288;&#22914;BLEU&#12289;ROUGE&#12289;BERTScore&#21644;BLEURT&#65289;&#23558;&#21442;&#32771;&#21644;&#39044;&#27979;&#38382;&#39064;&#36827;&#34892;&#27604;&#36739;&#65292;&#24403;&#20505;&#36873;&#38382;&#39064;&#21644;&#21442;&#32771;&#38382;&#39064;&#20043;&#38388;&#23384;&#22312;&#30456;&#24403;&#30340;&#35789;&#27719;&#37325;&#21472;&#25110;&#35821;&#20041;&#30456;&#20284;&#24615;&#26102;&#65292;&#25552;&#20379;&#39640;&#20998;&#12290;&#35813;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#24037;&#25552;&#20379;&#21442;&#32771;&#38382;&#39064;&#65307;&#20854;&#27425;&#65292;&#23427;&#24809;&#32602;&#37027;&#20123;&#21487;&#33021;&#19982;&#21442;&#32771;&#38382;&#39064;&#27809;&#26377;&#39640;&#35789;&#27719;&#25110;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26377;&#25928;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;RQUGE&#65292;&#22522;&#20110;&#32473;&#23450;&#19978;&#19979;&#25991;&#30340;&#20505;&#36873;&#38382;&#39064;&#30340;&#21487;&#22238;&#31572;&#24615;&#12290;&#35813;&#24230;&#37327;&#26631;&#20934;&#30001;&#19968;&#20010;&#38382;&#31572;&#27169;&#22359;&#21644;&#19968;&#20010;&#36328;&#24230;&#35780;&#20998;&#22120;&#27169;&#22359;&#32452;&#25104;&#65292;&#20351;&#29992;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22240;&#27492;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#36827;&#19968;&#27493;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;RQUGE&#19982;&#20154;&#31867;&#21028;&#26029;&#20855;&#26377;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#21442;&#32771;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;RQUGE&#26174;&#31034;&#26356;&#21152;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing metrics for evaluating the quality of automatically generated questions such as BLEU, ROUGE, BERTScore, and BLEURT compare the reference and predicted questions, providing a high score when there is a considerable lexical overlap or semantic similarity between the candidate and the reference questions. This approach has two major shortcomings. First, we need expensive human-provided reference questions. Second, it penalises valid questions that may not have high lexical or semantic similarity to the reference questions. In this paper, we propose a new metric, RQUGE, based on the answerability of the candidate question given the context. The metric consists of a question-answering and a span scorer modules, using pre-trained models from existing literature, thus it can be used without any further training. We demonstrate that RQUGE has a higher correlation with human judgment without relying on the reference question. Additionally, RQUGE is shown to be more robust to several ad
&lt;/p&gt;</description></item><item><title>&#38450;&#27490;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#36880;&#23383;&#35760;&#24518;&#26080;&#27861;&#30495;&#27491;&#20445;&#25252;&#38544;&#31169;&#65292;&#26412;&#25991;&#35774;&#35745;&#30340;&#24067;&#38534;&#36807;&#28388;&#22120;&#34429;&#28982;&#38450;&#27490;&#20102;&#25152;&#26377;&#36880;&#23383;&#35760;&#24518;&#65292;&#20294;&#20173;&#28982;&#26080;&#27861;&#38450;&#27490;&#35757;&#32451;&#25968;&#25454;&#27844;&#38706;&#65292;&#23481;&#26131;&#34987;&#21512;&#29702;&#20462;&#25913;&#30340;&#8220;&#26679;&#24335;&#36716;&#25442;&#8221;&#25552;&#31034;&#32469;&#36807;&#12290;</title><link>http://arxiv.org/abs/2210.17546</link><description>&lt;p&gt;
&#38450;&#27490;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#30340;&#36880;&#23383;&#35760;&#24518;&#20250;&#20135;&#29983;&#34394;&#20551;&#38544;&#31169;&#20445;&#25252;&#24863;
&lt;/p&gt;
&lt;p&gt;
Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy. (arXiv:2210.17546v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17546
&lt;/p&gt;
&lt;p&gt;
&#38450;&#27490;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#36880;&#23383;&#35760;&#24518;&#26080;&#27861;&#30495;&#27491;&#20445;&#25252;&#38544;&#31169;&#65292;&#26412;&#25991;&#35774;&#35745;&#30340;&#24067;&#38534;&#36807;&#28388;&#22120;&#34429;&#28982;&#38450;&#27490;&#20102;&#25152;&#26377;&#36880;&#23383;&#35760;&#24518;&#65292;&#20294;&#20173;&#28982;&#26080;&#27861;&#38450;&#27490;&#35757;&#32451;&#25968;&#25454;&#27844;&#38706;&#65292;&#23481;&#26131;&#34987;&#21512;&#29702;&#20462;&#25913;&#30340;&#8220;&#26679;&#24335;&#36716;&#25442;&#8221;&#25552;&#31034;&#32469;&#36807;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20013;&#25968;&#25454;&#35760;&#24518;&#30340;&#29616;&#35937;&#65292;&#26412;&#30740;&#31350;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#19982;&#38544;&#31169;&#25110;&#29256;&#26435;&#30456;&#20851;&#30340;&#39118;&#38505;&#65292;&#24182;&#26377;&#21161;&#20110;&#35780;&#20272;&#23545;&#31574;&#12290;&#28982;&#32780;&#36880;&#23383;&#35760;&#24518;&#23450;&#20041;&#36807;&#20110;&#20005;&#26684;&#65292;&#26410;&#33021;&#25429;&#25417;&#26356;&#20026;&#24494;&#22937;&#30340;&#35760;&#24518;&#24418;&#24335;&#12290;&#26412;&#25991;&#22522;&#20110;&#24067;&#38534;&#36807;&#28388;&#22120;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#20294;&#35813;&#8220;&#23436;&#32654;&#8221;&#36807;&#28388;&#22120;&#24182;&#19981;&#33021;&#38450;&#27490;&#35757;&#32451;&#25968;&#25454;&#27844;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studying data memorization in neural language models helps us understand the risks (e.g., to privacy or copyright) associated with models regurgitating training data, and aids in the evaluation of potential countermeasures. Many prior works -- and some recently deployed defenses -- focus on "verbatim memorization", defined as a model generation that exactly matches a substring from the training set. We argue that verbatim memorization definitions are too restrictive and fail to capture more subtle forms of memorization. Specifically, we design and implement an efficient defense based on Bloom filters that perfectly prevents all verbatim memorization. And yet, we demonstrate that this "perfect" filter does not prevent the leakage of training data. Indeed, it is easily circumvented by plausible and minimally modified "style-transfer" prompts -- and in some cases even the non-modified original prompts -- to extract memorized information. For example, instructing the model to output ALL-CA
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MARLlib&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;MARL&#31639;&#27861;&#24211;&#65292;&#21487;&#32479;&#19968;&#25968;&#21313;&#31181;&#31639;&#27861;&#12290;&#23427;&#36824;&#36229;&#36234;&#20102;&#24403;&#21069;&#24037;&#20316;&#65292;&#38598;&#25104;&#20102;&#21508;&#31181;&#29615;&#22659;&#25509;&#21475;&#21644;&#25552;&#20379;&#28789;&#27963;&#30340;&#21442;&#25968;&#20849;&#20139;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2210.13708</link><description>&lt;p&gt;
MARLlib: &#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#24211;
&lt;/p&gt;
&lt;p&gt;
MARLlib: A Scalable Multi-agent Reinforcement Learning Library. (arXiv:2210.13708v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MARLlib&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;MARL&#31639;&#27861;&#24211;&#65292;&#21487;&#32479;&#19968;&#25968;&#21313;&#31181;&#31639;&#27861;&#12290;&#23427;&#36824;&#36229;&#36234;&#20102;&#24403;&#21069;&#24037;&#20316;&#65292;&#38598;&#25104;&#20102;&#21508;&#31181;&#29615;&#22659;&#25509;&#21475;&#21644;&#25552;&#20379;&#28789;&#27963;&#30340;&#21442;&#25968;&#20849;&#20139;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#20294;&#32570;&#20047;&#32479;&#19968;&#30340;&#35780;&#20272;&#24179;&#21488;&#21644;&#20844;&#35748;&#30340;&#22522;&#20934;&#23454;&#29616;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#19968;&#20010;&#38598;&#25104;&#24211;&#22871;&#20214;&#65292;&#20197;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#25552;&#20379;&#21487;&#38752;&#30340;MARL&#23454;&#29616;&#21644;&#21487;&#22797;&#21046;&#30340;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MARLlib&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;MARL&#31639;&#27861;&#24211;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#12290;MARLlib&#36890;&#36807;&#26032;&#39062;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#27969;&#35774;&#35745;&#65292;&#22312;&#39640;&#24230;&#21487;&#32452;&#21512;&#30340;&#38598;&#25104;&#39118;&#26684;&#20013;&#32479;&#19968;&#20102;&#25968;&#21313;&#31181;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;MARLlib&#36890;&#36807;&#38598;&#25104;&#21508;&#31181;&#29615;&#22659;&#25509;&#21475;&#21644;&#25552;&#20379;&#28789;&#27963;&#30340;&#21442;&#25968;&#20849;&#20139;&#31574;&#30053;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#24037;&#20316;&#65307;&#36825;&#20801;&#35768;&#26368;&#32456;&#29992;&#25143;&#22312;&#26368;&#23567;&#30340;&#20195;&#30721;&#20462;&#25913;&#19979;&#23454;&#29616;&#21327;&#20316;&#12289;&#31454;&#20105;&#21644;&#28151;&#21512;&#20219;&#21153;&#30340;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;MARLlib&#25552;&#20379;&#26131;&#20110;&#20351;&#29992;&#30340;API&#21644;&#23436;&#20840;&#35299;&#32806;&#21512;&#30340;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the fast development of multi-agent systems (MAS) and multi-agent reinforcement learning (MARL) algorithms, there is a lack of unified evaluation platforms and commonly-acknowledged baseline implementation. Therefore, an urgent need is to develop an integrated library suite that delivers reliable MARL implementation and replicable evaluation in various benchmarks. To fill such a research gap, in this paper, we propose MARLlib, a comprehensive MARL algorithm library for solving multi-agent problems. With a novel design of agent-level distributed dataflow, MARLlib manages to unify tens of algorithms in a highly composable integration style. Moreover, MARLlib goes beyond current work by integrating diverse environment interfaces and providing flexible parameter sharing strategies; this allows for versatile solutions to cooperative, competitive, and mixed tasks with minimal code modifications for end users. Finally, MARLlib provides easy-to-use APIs and a fully decoupled configurat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#20363;&#24863;&#30693;&#22270;&#20687;&#20462;&#22797;&#27169;&#22411;ImComplete&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#24187;&#35937;&#20986;&#19982;&#29615;&#22659;&#32972;&#26223;&#30456;&#21327;&#35843;&#30340;&#35270;&#35273;&#23454;&#20363;&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;&#35821;&#20041;&#21644;&#32467;&#26500;&#30340;&#20687;&#32032;&#32423;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2210.12350</link><description>&lt;p&gt;
&#23454;&#20363;&#24863;&#30693;&#22270;&#20687;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Instance-Aware Image Completion. (arXiv:2210.12350v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#20363;&#24863;&#30693;&#22270;&#20687;&#20462;&#22797;&#27169;&#22411;ImComplete&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#24187;&#35937;&#20986;&#19982;&#29615;&#22659;&#32972;&#26223;&#30456;&#21327;&#35843;&#30340;&#35270;&#35273;&#23454;&#20363;&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;&#35821;&#20041;&#21644;&#32467;&#26500;&#30340;&#20687;&#32032;&#32423;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20462;&#22797;&#26159;&#19968;&#39033;&#26088;&#22312;&#22635;&#34917;&#24102;&#26377;&#32570;&#22833;&#21306;&#22495;&#30340;&#22270;&#20687;&#30340;&#20219;&#21153;&#65292;&#20351;&#23427;&#20204;&#20855;&#26377;&#21512;&#29702;&#30340;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22270;&#20687;&#20462;&#22797;&#26041;&#27861;&#24448;&#24448;&#36890;&#36807;&#22635;&#20805;&#21608;&#22260;&#32441;&#29702;&#26469;&#22635;&#34917;&#32570;&#22833;&#21306;&#22495;&#65292;&#32780;&#19981;&#26159;&#21435;&#24187;&#35937;&#19968;&#20010;&#19982;&#29615;&#22659;&#32972;&#26223;&#30456;&#21327;&#35843;&#30340;&#35270;&#35273;&#23454;&#20363;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20462;&#22797;&#27169;&#22411;&#65292;&#21517;&#20026;ImComplete&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#24187;&#35937;&#32570;&#22833;&#30340;&#23454;&#20363;&#65292;&#20174;&#32780;&#19982;&#21407;&#22987;&#32972;&#26223;&#21327;&#35843;&#12290;ImComplete&#39318;&#20808;&#37319;&#29992;&#20102;&#19968;&#20010;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#32771;&#34385;&#21040;&#21487;&#35265;&#23454;&#20363;&#21644;&#32570;&#22833;&#21306;&#22495;&#30340;&#20301;&#32622;&#12290;&#28982;&#21518;&#65292;ImComplete&#23436;&#25104;&#20102;&#32570;&#22833;&#21306;&#22495;&#20869;&#30340;&#35821;&#20041;&#20998;&#21106;&#25513;&#27169;&#65292;&#25552;&#20379;&#20687;&#32032;&#32423;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#25351;&#23548;&#12290;&#26368;&#21518;&#65292;&#22270;&#20687;&#21512;&#25104;&#22359;&#29983;&#25104;&#20102;&#36924;&#30495;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image completion is a task that aims to fill in the missing region of a masked image with plausible contents. However, existing image completion methods tend to fill in the missing region with the surrounding texture instead of hallucinating a visual instance that is suitable in accordance with the context of the scene. In this work, we propose a novel image completion model, dubbed ImComplete, that hallucinates the missing instance that harmonizes well with - and thus preserves - the original context. ImComplete first adopts a transformer architecture that considers the visible instances and the location of the missing region. Then, ImComplete completes the semantic segmentation masks within the missing region, providing pixel-level semantic and structural guidance. Finally, the image synthesis blocks generate photo-realistic content. We perform a comprehensive evaluation of the results in terms of visual quality (LPIPS and FID) and contextual preservation scores (CLIPscore and object
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#30340;&#23616;&#38480;&#24615;&#65292;&#35777;&#26126;&#20102;&#19968;&#20123;&#38024;&#23545;&#29305;&#23450;&#24773;&#20917;&#19979;&#30340;&#38480;&#21046;&#26465;&#20214;&#19979;&#23384;&#22312;&#21487;&#38752;&#30340;GNN&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;MILP&#38382;&#39064;&#30340;&#21487;&#34892;&#24615;&#12289;&#26368;&#20248;&#30446;&#26631;&#20540;&#21644;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2210.10759</link><description>&lt;p&gt;
&#35770;&#36848;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Representing Mixed-Integer Linear Programs by Graph Neural Networks. (arXiv:2210.10759v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#30340;&#23616;&#38480;&#24615;&#65292;&#35777;&#26126;&#20102;&#19968;&#20123;&#38024;&#23545;&#29305;&#23450;&#24773;&#20917;&#19979;&#30340;&#38480;&#21046;&#26465;&#20214;&#19979;&#23384;&#22312;&#21487;&#38752;&#30340;GNN&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;MILP&#38382;&#39064;&#30340;&#21487;&#34892;&#24615;&#12289;&#26368;&#20248;&#30446;&#26631;&#20540;&#21644;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;(MILP)&#38382;&#39064;&#36890;&#24120;&#20026;NP&#38590;&#38382;&#39064;&#65292;&#20294;&#26159;&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#20013;&#65292;&#23454;&#38469;&#30340;MILP&#38382;&#39064;&#24050;&#32463;&#33719;&#24471;&#20102;&#22823;&#32422;100&#20493;&#30340;&#21152;&#36895;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#35768;&#22810;&#31867;&#21035;&#30340;MILP&#38382;&#39064;&#22312;&#35268;&#27169;&#19981;&#26029;&#22686;&#21152;&#26102;&#36805;&#36895;&#21464;&#24471;&#19981;&#21487;&#35299;&#65292;&#36825;&#20419;&#20351;&#30740;&#31350;&#20154;&#21592;&#23547;&#27714;&#26032;&#30340;&#21152;&#36895;&#25216;&#26415;&#20197;&#35299;&#20915;MILP&#38382;&#39064;&#12290;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#65292;&#30740;&#31350;&#20154;&#21592;&#33719;&#24471;&#20102;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#24182;&#19988;&#35768;&#22810;&#32467;&#26524;&#37117;&#26159;&#36890;&#36807;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#24212;&#29992;&#20110;MILP&#35299;&#20915;&#36807;&#31243;&#30340;&#21508;&#20010;&#38454;&#27573;&#26469;&#33719;&#24471;&#30340;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#20010;&#26681;&#26412;&#24615;&#30340;&#23616;&#38480;&#24615;&#65306;&#23384;&#22312;&#21487;&#34892;&#21644;&#19981;&#21487;&#34892;&#30340;MILP&#38382;&#39064;&#65292;&#32780;&#25152;&#26377;&#30340;GNN&#37117;&#20250;&#24179;&#31561;&#22320;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#65292;&#34920;&#26126;GNN&#23545;&#20110;&#34920;&#31034;&#19968;&#33324;&#30340;MILP&#38382;&#39064;&#30340;&#33021;&#21147;&#19981;&#36275;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#38480;&#21046;MILP&#20026;&#21487;&#23637;&#24320;&#30340;&#38382;&#39064;&#25110;&#28155;&#21152;&#38543;&#26426;&#29305;&#24449;&#65292;&#35777;&#26126;&#20102;&#23384;&#22312;&#33021;&#22815;&#21487;&#38752;&#22320;&#39044;&#27979;MILP&#21487;&#34892;&#24615;&#12289;&#26368;&#20248;&#30446;&#26631;&#20540;&#21644;&#26368;&#20248;&#35299;&#30340;GNN&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23567;&#35268;&#27169;&#30340;&#25968;&#20540;&#23454;&#39564;&#26469;&#39564;&#35777;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Mixed-integer linear programming (MILP) is NP-hard in general, practical MILP has received roughly 100--fold speedup in the past twenty years. Still, many classes of MILPs quickly become unsolvable as their sizes increase, motivating researchers to seek new acceleration techniques for MILPs. With deep learning, they have obtained strong empirical results, and many results were obtained by applying graph neural networks (GNNs) to making decisions in various stages of MILP solution processes. This work discovers a fundamental limitation: there exist feasible and infeasible MILPs that all GNNs will, however, treat equally, indicating GNN's lacking power to express general MILPs. Then, we show that, by restricting the MILPs to unfoldable ones or by adding random features, there exist GNNs that can reliably predict MILP feasibility, optimal objective values, and optimal solutions up to prescribed precision. We conducted small-scale numerical experiments to validate our theoretical fin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20869;&#22312;&#22870;&#21169;&#21305;&#37197;(IRF)&#26041;&#27861;&#65292;&#36890;&#36807;&#25216;&#33021;&#37492;&#21035;&#22120;&#21305;&#37197;&#20869;&#22312;&#21644;&#19979;&#28216;&#20219;&#21153;&#22870;&#21169;&#26469;&#30830;&#23450;&#26410;&#35265;&#20219;&#21153;&#30340;&#26368;&#20248;&#25216;&#33021;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.07426</link><description>&lt;p&gt;
&#22522;&#20110;&#25216;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#19982;&#20869;&#22312;&#22870;&#21169;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Skill-Based Reinforcement Learning with Intrinsic Reward Matching. (arXiv:2210.07426v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07426
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20869;&#22312;&#22870;&#21169;&#21305;&#37197;(IRF)&#26041;&#27861;&#65292;&#36890;&#36807;&#25216;&#33021;&#37492;&#21035;&#22120;&#21305;&#37197;&#20869;&#22312;&#21644;&#19979;&#28216;&#20219;&#21153;&#22870;&#21169;&#26469;&#30830;&#23450;&#26410;&#35265;&#20219;&#21153;&#30340;&#26368;&#20248;&#25216;&#33021;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26080;&#30417;&#30563;&#25216;&#33021;&#25506;&#32034;&#24050;&#32463;&#23637;&#31034;&#20102;&#33258;&#20027;&#33719;&#21462;&#34892;&#20026;&#21407;&#35821;&#30340;&#28508;&#21147;&#65292;&#20294;&#26159;&#20219;&#21153;&#26080;&#20851;&#30340;&#25216;&#33021;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#30340;&#20219;&#21153;&#24863;&#30693;&#35843;&#20248;&#20043;&#38388;&#20173;&#23384;&#22312;&#24456;&#22823;&#30340;&#26041;&#27861;&#35770;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#20869;&#22312;&#22870;&#21169;&#21305;&#37197;(IRF)&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#32452;&#20214; "&#25216;&#33021;&#37492;&#21035;&#22120;" &#32479;&#19968;&#36825;&#20004;&#20010;&#23398;&#20064;&#38454;&#27573;&#12290;&#20256;&#32479;&#26041;&#27861;&#22312;&#31574;&#30053;&#32423;&#21035;&#30452;&#25509;&#24494;&#35843;&#39044;&#35757;&#32451;&#20195;&#29702;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#29615;&#22659;&#22238;&#25918;&#26469;&#32463;&#39564;&#24615;&#22320;&#30830;&#23450;&#26368;&#20248;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#20219;&#21153;&#26368;&#31616;&#26126;&#20294;&#23436;&#25972;&#30340;&#25551;&#36848;&#36890;&#24120;&#26159;&#22870;&#21169;&#20989;&#25968;&#26412;&#36523;&#65292;&#25216;&#33021;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#37492;&#21035;&#22120;&#23398;&#20064;&#19982;&#25216;&#33021;&#31574;&#30053;&#30456;&#23545;&#24212;&#30340;&#8220;&#20869;&#22312;&#8221;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#25216;&#33021;&#37492;&#21035;&#22120;&#8220;&#21305;&#37197;&#8221;&#20869;&#22312;&#21644;&#19979;&#28216;&#20219;&#21153;&#22870;&#21169;&#65292;&#24182;&#30830;&#23450;&#26410;&#35265;&#20219;&#21153;&#30340;&#26368;&#20248;&#25216;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
While unsupervised skill discovery has shown promise in autonomously acquiring behavioral primitives, there is still a large methodological disconnect between task-agnostic skill pretraining and downstream, task-aware finetuning. We present Intrinsic Reward Matching (IRM), which unifies these two phases of learning via the $\textit{skill discriminator}$, a pretraining model component often discarded during finetuning. Conventional approaches finetune pretrained agents directly at the policy level, often relying on expensive environment rollouts to empirically determine the optimal skill. However, often the most concise yet complete description of a task is the reward function itself, and skill learning methods learn an $\textit{intrinsic}$ reward function via the discriminator that corresponds to the skill policy. We propose to leverage the skill discriminator to $\textit{match}$ the intrinsic and downstream task rewards and determine the optimal skill for an unseen task without enviro
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#35770;&#28436;&#21592;&#31639;&#27861;&#65292;&#23427;&#22312;&#24555;&#36895;&#21644;&#24930;&#36895;&#26102;&#38388;&#23610;&#24230;&#19978;&#35745;&#31639;&#20215;&#20540;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#19982;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#34920;&#29616;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2210.04470</link><description>&lt;p&gt;
&#28436;&#21592;&#35780;&#35770;&#25110;&#35780;&#35770;&#28436;&#21592;&#65311;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#30340;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;
Actor-Critic or Critic-Actor? A Tale of Two Time Scales. (arXiv:2210.04470v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04470
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#35770;&#28436;&#21592;&#31639;&#27861;&#65292;&#23427;&#22312;&#24555;&#36895;&#21644;&#24930;&#36895;&#26102;&#38388;&#23610;&#24230;&#19978;&#35745;&#31639;&#20215;&#20540;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#19982;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;&#34920;&#26684;&#30340;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#30340;&#26631;&#20934;&#20844;&#24335;&#65292;&#23558;&#20854;&#35270;&#20026;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#30340;&#38543;&#26426;&#36924;&#36817;&#65292;&#20854;&#20013;&#20215;&#20540;&#20989;&#25968;&#22312;&#24555;&#36895;&#26102;&#38388;&#23610;&#24230;&#19978;&#35745;&#31639;&#65292;&#31574;&#30053;&#22312;&#24930;&#36895;&#26102;&#38388;&#23610;&#24230;&#19978;&#35745;&#31639;&#12290;&#36825;&#27169;&#25311;&#20102;&#31574;&#30053;&#36845;&#20195;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#65292;&#26102;&#38388;&#23610;&#24230;&#30340;&#21453;&#36716;&#23454;&#38469;&#19978;&#20250;&#27169;&#25311;&#20540;&#36845;&#20195;&#65292;&#24182;&#19988;&#26159;&#19968;&#31181;&#21512;&#27861;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#35777;&#26126;&#65292;&#24182;&#36890;&#36807;&#24102;&#26377;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#22120;&#30340;&#20989;&#25968;&#36924;&#36817;&#27979;&#35797;&#20004;&#31181;&#26041;&#27861;&#65292;&#24182;&#35266;&#23519;&#21040;&#25105;&#20204;&#25552;&#20986;&#30340;&#35780;&#35770;&#28436;&#21592;&#31639;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#19982;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit the standard formulation of tabular actor-critic algorithm as a two time-scale stochastic approximation with value function computed on a faster time-scale and policy computed on a slower time-scale. This emulates policy iteration. We begin by observing that reversal of the time scales will in fact emulate value iteration and is a legitimate algorithm. We provide a proof of convergence and compare the two empirically with and without function approximation (with both linear and nonlinear function approximators) and observe that our proposed critic-actor algorithm performs on par with actor-critic in terms of both accuracy and computational effort.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#26031;&#36807;&#31243;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#65288;GPEC&#65289;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#23545;&#22797;&#26434;&#30340;&#40657;&#30418;&#20998;&#31867;&#22120;&#36827;&#34892;&#21487;&#38752;&#30340;&#29305;&#24449;&#24402;&#22240;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#20915;&#31574;&#36793;&#30028;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#35299;&#37322;&#20989;&#25968;&#36924;&#36817;&#19981;&#30830;&#23450;&#24615;&#65292;&#33021;&#22815;&#29983;&#25104;&#19968;&#20010;&#32479;&#19968;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2210.02419</link><description>&lt;p&gt;
&#36793;&#30028;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#21487;&#35299;&#37322;&#29305;&#24449;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Boundary-Aware Uncertainty for Feature Attribution Explainers. (arXiv:2210.02419v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#26031;&#36807;&#31243;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#65288;GPEC&#65289;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#23545;&#22797;&#26434;&#30340;&#40657;&#30418;&#20998;&#31867;&#22120;&#36827;&#34892;&#21487;&#38752;&#30340;&#29305;&#24449;&#24402;&#22240;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#20915;&#31574;&#36793;&#30028;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#35299;&#37322;&#20989;&#25968;&#36924;&#36817;&#19981;&#30830;&#23450;&#24615;&#65292;&#33021;&#22815;&#29983;&#25104;&#19968;&#20010;&#32479;&#19968;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#32493;&#30340;&#35299;&#37322;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#29702;&#35299;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#40657;&#30418;&#20998;&#31867;&#22120;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#39640;&#24615;&#33021;&#20998;&#31867;&#22120;&#36890;&#24120;&#26159;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#65292;&#24182;&#19988;&#22312;&#20915;&#31574;&#36793;&#30028;&#21608;&#22260;&#23637;&#29616;&#20986;&#22797;&#26434;&#30340;&#34892;&#20026;&#65292;&#23548;&#33268;&#33030;&#24369;&#25110;&#35823;&#23548;&#24615;&#30340;&#23616;&#37096;&#35299;&#37322;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#37327;&#21270;&#36825;&#31181;&#35299;&#37322;&#26041;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#20102;&#35299;&#20309;&#26102;&#21487;&#20197;&#20449;&#20219;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#26031;&#36807;&#31243;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#65288;GPEC&#65289;&#26694;&#26550;&#65292;&#23427;&#29983;&#25104;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#23558;&#20915;&#31574;&#36793;&#30028;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#19982;&#35299;&#37322;&#20989;&#25968;&#36924;&#36817;&#19981;&#30830;&#23450;&#24615;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27979;&#22320;&#32447;&#30340;&#26680;&#65292;&#23427;&#25429;&#25417;&#30446;&#26631;&#40657;&#30418;&#20915;&#31574;&#36793;&#30028;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26680;&#30456;&#20284;&#24230;&#38543;&#30528;&#20915;&#31574;&#36793;&#30028;&#30340;&#22797;&#26434;&#24615;&#36882;&#22686;&#12290;&#35813;&#25552;&#20986;&#30340;&#26694;&#26550;&#38750;&#24120;&#28789;&#27963;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#40657;&#30418;&#20998;&#31867;&#22120;&#21644;&#20219;&#20309;&#35299;&#37322;&#26041;&#27861;&#19968;&#36215;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#26174;&#31034;GPEC&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#30830;&#23450;&#24230;&#20272;&#35745;&#65292;&#24182;&#23548;&#33268;&#26356;&#21487;&#38752;&#30340;&#29305;&#24449;&#24402;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-hoc explanation methods have become a critical tool for understanding black-box classifiers in high-stakes applications. However, high-performing classifiers are often highly nonlinear and can exhibit complex behavior around the decision boundary, leading to brittle or misleading local explanations. Therefore there is an impending need to quantify the uncertainty of such explanation methods in order to understand when explanations are trustworthy. In this work we propose the Gaussian Process Explanation unCertainty (GPEC) framework, which generates a unified uncertainty estimate combining decision boundary-aware uncertainty with explanation function approximation uncertainty. We introduce a novel geodesic-based kernel, which captures the complexity of the target black-box decision boundary. We show theoretically that the proposed kernel similarity increases with decision boundary complexity. The proposed framework is highly flexible; it can be used with any black-box classifier an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;&#26080;&#38480;&#23485;&#24230;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#23398;&#20064;&#35268;&#21017;&#22914;GD&#12289;FA&#12289;DFA&#12289;Hebb&#21644;GLN&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#21457;&#29616;&#27599;&#31181;&#35268;&#21017;&#19979;&#30340;&#36755;&#20986;&#20989;&#25968;&#28436;&#21270;&#37117;&#21463;&#21040;&#26102;&#38388;&#21464;&#21270;&#30340;&#26377;&#25928;&#31070;&#32463;&#20999;&#21521;&#26680;(eNTK)&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#21160;&#24577;&#22343;&#22330;&#29702;&#35770;(DMFT)&#27604;&#36739;&#20102;&#27599;&#31181;&#23398;&#20064;&#35268;&#21017;&#25152;&#24341;&#36215;&#30340;&#29305;&#24449;&#21644;&#39044;&#27979;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2210.02157</link><description>&lt;p&gt;
&#23398;&#20064;&#35268;&#21017;&#23545;&#24191;&#27867;&#31070;&#32463;&#32593;&#32476;&#34920;&#24449;&#21160;&#21147;&#23398;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Influence of Learning Rule on Representation Dynamics in Wide Neural Networks. (arXiv:2210.02157v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;&#26080;&#38480;&#23485;&#24230;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#23398;&#20064;&#35268;&#21017;&#22914;GD&#12289;FA&#12289;DFA&#12289;Hebb&#21644;GLN&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#21457;&#29616;&#27599;&#31181;&#35268;&#21017;&#19979;&#30340;&#36755;&#20986;&#20989;&#25968;&#28436;&#21270;&#37117;&#21463;&#21040;&#26102;&#38388;&#21464;&#21270;&#30340;&#26377;&#25928;&#31070;&#32463;&#20999;&#21521;&#26680;(eNTK)&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#21160;&#24577;&#22343;&#22330;&#29702;&#35770;(DMFT)&#27604;&#36739;&#20102;&#27599;&#31181;&#23398;&#20064;&#35268;&#21017;&#25152;&#24341;&#36215;&#30340;&#29305;&#24449;&#21644;&#39044;&#27979;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#23578;&#19981;&#28165;&#26970;&#25913;&#21464;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#35268;&#21017;&#22914;&#20309;&#25913;&#21464;&#20854;&#23398;&#20064;&#21160;&#21147;&#23398;&#21644;&#34920;&#24449;&#12290;&#20026;&#20102;&#28145;&#20837;&#20102;&#35299;&#23398;&#20064;&#29305;&#24449;&#12289;&#20989;&#25968;&#36924;&#36817;&#21644;&#23398;&#20064;&#35268;&#21017;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26080;&#38480;&#23485;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#37319;&#29992;&#20102;&#26799;&#24230;&#19979;&#38477;(GD)&#20197;&#21450;&#29983;&#29289;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21253;&#25324;&#21453;&#39304;&#23545;&#40784;(FA)&#12289;&#30452;&#25509;&#21453;&#39304;&#23545;&#40784;(DFA)&#12289;&#35823;&#24046;&#35843;&#21046;&#40657;&#27604;&#23398;&#20064;(Hebb)&#65292;&#20197;&#21450;&#38376;&#25511;&#32447;&#24615;&#32593;&#32476;(GLN)&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is unclear how changing the learning rule of a deep neural network alters its learning dynamics and representations. To gain insight into the relationship between learned features, function approximation, and the learning rule, we analyze infinite-width deep networks trained with gradient descent (GD) and biologically-plausible alternatives including feedback alignment (FA), direct feedback alignment (DFA), and error modulated Hebbian learning (Hebb), as well as gated linear networks (GLN). We show that, for each of these learning rules, the evolution of the output function at infinite width is governed by a time varying effective neural tangent kernel (eNTK). In the lazy training limit, this eNTK is static and does not evolve, while in the rich mean-field regime this kernel's evolution can be determined self-consistently with dynamical mean field theory (DMFT). This DMFT enables comparisons of the feature and prediction dynamics induced by each of these learning rules. In the lazy 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2210.01969</link><description>&lt;p&gt;
&#20998;&#23618;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Adversarial Inverse Reinforcement Learning. (arXiv:2210.01969v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#19968;&#33324;&#29992;&#20110;&#20174;&#28436;&#31034;&#20013;&#24674;&#22797;&#19987;&#23478;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#39640;&#24230;&#22797;&#26434;&#30340;&#12289;&#38271;&#26102;&#31243;&#20219;&#21153;&#65292;&#24674;&#22797;&#21333;&#19968;&#25972;&#20307;&#31574;&#30053;&#26159;&#22256;&#38590;&#30340;&#65292;&#32780;&#19987;&#23478;&#31574;&#30053;&#36890;&#24120;&#21253;&#21547;&#23376;&#20219;&#21153;&#23618;&#27425;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#32773;&#24320;&#21457;&#20102;&#20998;&#23618;&#27169;&#20223;&#23398;&#20064;&#65288;HIL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36873;&#39033;&#26694;&#26550;&#20013;&#26174;&#24335;&#22320;&#24314;&#27169;&#20219;&#21153;&#20013;&#30340;&#27963;&#21160;&#32467;&#26500;&#26469;&#23398;&#20064;&#20998;&#23618;&#31574;&#30053;&#12290;&#29616;&#26377;&#30340;HIL&#26041;&#27861;&#35201;&#20040;&#24573;&#35270;&#20102;&#23376;&#20219;&#21153;&#32467;&#26500;&#19982;&#23398;&#20064;&#31574;&#30053;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#35201;&#20040;&#26080;&#27861;&#21516;&#26102;&#22312;&#20998;&#23618;&#26694;&#26550;&#20013;&#23398;&#20064;&#39640;&#32423;&#21035;&#21644;&#20302;&#32423;&#21035;&#31574;&#30053;&#65292;&#23548;&#33268;&#20122;&#26368;&#20248;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;HIL&#31639;&#27861;&#8212;&#8212;&#20998;&#23618;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;H-AIRL&#65289;&#65292;&#23427;&#22312;&#26368;&#26032;&#30340;IL&#31639;&#27861;AIRL&#19978;&#25193;&#23637;&#20102;&#19968;&#27493;&#36873;&#39033;&#26694;&#26550;&#65292;&#37325;&#26032;&#23450;&#20041;&#20102;AIRL&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning (IL) has been proposed to recover the expert policy from demonstrations. However, it would be difficult to learn a single monolithic policy for highly-complex long-horizon tasks of which the expert policy usually contains subtask hierarchies. Therefore, Hierarchical Imitation Learning (HIL) has been developed to learn a hierarchical policy from expert demonstrations through explicitly modelling the activity structure in a task with the option framework. Existing HIL methods either overlook the causal relationship between the subtask structure and the learned policy, or fail to learn the high-level and low-level policy in the hierarchical framework in conjuncture, which leads to suboptimality. In this work, we propose a novel HIL algorithm -Hierarchical Adversarial Inverse Reinforcement Learning (H-AIRL), which extends a state-of-the-art (SOTA) IL algorithm -- AIRL, with the one-step option framework. Specifically, we redefine the AIRL objectives on the extended sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19979;&#28216;&#25968;&#25454;&#38598;&#36827;&#34892;&#33258;&#25105;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#19982;&#20351;&#29992;&#22823;&#22411;&#35821;&#26009;&#24211;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#26631;&#20934;&#26041;&#27861;&#30456;&#23218;&#32654;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#26356;&#21152;&#20248;&#31168;&#12290;&#21516;&#26102;&#65292;&#36825;&#20123;&#33258;&#25105;&#39044;&#35757;&#32451;&#27169;&#22411;&#36824;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2209.14389</link><description>&lt;p&gt;
&#19979;&#28216;&#25968;&#25454;&#38598;&#24847;&#22806;&#22320;&#25104;&#20026;&#33391;&#22909;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Downstream Datasets Make Surprisingly Good Pretraining Corpora. (arXiv:2209.14389v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19979;&#28216;&#25968;&#25454;&#38598;&#36827;&#34892;&#33258;&#25105;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#19982;&#20351;&#29992;&#22823;&#22411;&#35821;&#26009;&#24211;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#26631;&#20934;&#26041;&#27861;&#30456;&#23218;&#32654;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#26356;&#21152;&#20248;&#31168;&#12290;&#21516;&#26102;&#65292;&#36825;&#20123;&#33258;&#25105;&#39044;&#35757;&#32451;&#27169;&#22411;&#36824;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20027;&#35201;&#30340;&#20570;&#27861;&#26159;&#20351;&#29992;&#26356;&#23567;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#65288;&#20363;&#22914;BERT&#65289;&#36827;&#34892;&#24494;&#35843;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#25910;&#30410;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#24402;&#22240;&#20110;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#65292;&#32780;&#19981;&#26159;&#39044;&#35757;&#32451;&#30446;&#26631;&#26412;&#36523;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#20851;&#20110;&#33258;&#25105;&#39044;&#35757;&#32451;&#65288;self-pretraining&#65289;&#30340;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#20854;&#20013;&#30456;&#21516;&#30340;&#65288;&#19979;&#28216;&#65289;&#35757;&#32451;&#25968;&#25454;&#29992;&#20110;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#22312;&#38024;&#23545;ELECTRA&#21644;RoBERTa&#27169;&#22411;&#20197;&#21450;10&#20010;&#19981;&#21516;&#30340;&#19979;&#28216;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#33258;&#25105;&#39044;&#35757;&#32451;&#19982;&#20351;&#29992;BookWiki&#35821;&#26009;&#24211;&#36827;&#34892;&#26631;&#20934;&#39044;&#35757;&#32451;&#30456;&#23218;&#32654;&#65288;&#23613;&#31649;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#20165;&#20026;&#21518;&#32773;&#30340;$10$&#20493;&#21040;$500$&#20493;&#19981;&#31561;&#65289;&#65292;&#24182;&#19988;&#22312;$7$&#20010;&#21644;$5$&#20010;&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#20248;&#20110;&#21518;&#32773;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#20123;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#21253;&#25324;GLUE&#22522;&#20934;&#27979;&#35797;&#12290;&#38500;&#20102;&#20998;&#31867;&#20219;&#21153;&#65292;&#33258;&#25105;&#39044;&#35757;&#32451;&#27169;&#22411;&#36824;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#21644;&#25277;&#21462;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
For most natural language processing tasks, the dominant practice is to finetune large pretrained transformer models (e.g., BERT) using smaller downstream datasets. Despite the success of this approach, it remains unclear to what extent these gains are attributable to the massive background corpora employed for pretraining versus to the pretraining objectives themselves. This paper introduces a large-scale study of self-pretraining, where the same (downstream) training data is used for both pretraining and finetuning. In experiments addressing both ELECTRA and RoBERTa models and 10 distinct downstream classification datasets, we observe that self-pretraining rivals standard pretraining on the BookWiki corpus (despite using around $10\times$--$500\times$ less data), outperforming the latter on $7$ and $5$ datasets, respectively. Surprisingly, these task-specific pretrained models often perform well on other tasks, including the GLUE benchmark. Besides classification tasks, self-pretrain
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;GNN&#26469;&#22788;&#29702;&#19981;&#21516;LP&#24182;&#32473;&#20986;&#20102;&#21512;&#29702;&#30340;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;&#35813;GNN&#21487;&#20197;&#39044;&#27979;LP&#30340;&#21487;&#34892;&#24615;&#12289;&#26377;&#30028;&#24615;&#21644;&#26368;&#20248;&#35299;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;GNN&#22312;&#22788;&#29702;LP&#26102;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.12288</link><description>&lt;p&gt;
&#35770;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Representing Linear Programs by Graph Neural Networks. (arXiv:2209.12288v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;GNN&#26469;&#22788;&#29702;&#19981;&#21516;LP&#24182;&#32473;&#20986;&#20102;&#21512;&#29702;&#30340;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;&#35813;GNN&#21487;&#20197;&#39044;&#27979;LP&#30340;&#21487;&#34892;&#24615;&#12289;&#26377;&#30028;&#24615;&#21644;&#26368;&#20248;&#35299;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;GNN&#22312;&#22788;&#29702;LP&#26102;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20248;&#21270;&#26159;&#24555;&#36895;&#21457;&#23637;&#30340;&#19968;&#20010;&#39046;&#22495;&#65292;&#26088;&#22312;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#25110;&#25913;&#36827;&#29616;&#26377;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#34987;&#35748;&#20026;&#26159;&#36866;&#29992;&#20110;&#20855;&#26377;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#20248;&#21270;&#38382;&#39064;&#30340;&#21512;&#36866;ML&#27169;&#22411;&#65292;&#20363;&#22914;&#32447;&#24615;&#35268;&#21010;&#65288;LP&#65289;&#12290;&#23613;&#31649;&#25991;&#29486;&#25253;&#36947;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#25968;&#20540;&#32467;&#26524;&#65292;&#20294;&#26412;&#25991;&#30830;&#31435;&#20102;&#23558;GNN&#24212;&#29992;&#20110;&#35299;&#20915;LP&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;GNN&#65292;&#23558;&#19981;&#21516;&#30340;LP&#26144;&#23556;&#21040;&#19981;&#21516;&#30340;&#36755;&#20986;&#65292;&#38024;&#23545;&#20219;&#20309;LP&#30340;&#22823;&#23567;&#38480;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36866;&#24403;&#26500;&#24314;&#30340;GNN&#21487;&#20197;&#21487;&#38752;&#22320;&#39044;&#27979;&#24191;&#27867;&#31867;&#21035;&#20013;&#27599;&#20010;LP&#30340;&#21487;&#34892;&#24615;&#65292;&#26377;&#30028;&#24615;&#21644;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#22522;&#20110;&#26368;&#36817;&#21457;&#29616;&#30340;Weisfeiler--Lehman&#21516;&#26500;&#27979;&#35797;&#21644;GNN&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;GNN&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#23558;LP&#26144;&#23556;&#21040;&#21487;&#34892;&#24615;&#21644;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to optimize is a rapidly growing area that aims to solve optimization problems or improve existing optimization algorithms using machine learning (ML). In particular, the graph neural network (GNN) is considered a suitable ML model for optimization problems whose variables and constraints are permutation--invariant, for example, the linear program (LP). While the literature has reported encouraging numerical results, this paper establishes the theoretical foundation of applying GNNs to solving LPs. Given any size limit of LPs, we construct a GNN that maps different LPs to different outputs. We show that properly built GNNs can reliably predict feasibility, boundedness, and an optimal solution for each LP in a broad class. Our proofs are based upon the recently--discovered connections between the Weisfeiler--Lehman isomorphism test and the GNN. To validate our results, we train a simple GNN and present its accuracy in mapping LPs to their feasibilities and solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#24212;&#29992;&#27169;&#22411;&#37327;&#21270;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#30340;&#21487;&#21464;&#20301;&#23485;&#20248;&#21270;&#26469;&#25552;&#39640;&#26080;&#32447;&#36890;&#20449;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#22312;&#26080;&#32447;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#37319;&#29992;&#22810;&#23610;&#24230;&#37327;&#21270;&#32852;&#37030;&#23398;&#20064;&#32858;&#21512;&#31639;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#25913;&#21892;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#20855;&#22791;&#26356;&#39640;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#23569;&#30340;&#35757;&#32451;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2209.10200</link><description>&lt;p&gt;
&#26080;&#32447;&#32593;&#32476;&#20013;&#22522;&#20110;&#21487;&#21464;&#20301;&#23485;&#30340;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Performance Optimization for Variable Bitwidth Federated Learning in Wireless Networks. (arXiv:2209.10200v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#24212;&#29992;&#27169;&#22411;&#37327;&#21270;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#30340;&#21487;&#21464;&#20301;&#23485;&#20248;&#21270;&#26469;&#25552;&#39640;&#26080;&#32447;&#36890;&#20449;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#22312;&#26080;&#32447;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#37319;&#29992;&#22810;&#23610;&#24230;&#37327;&#21270;&#32852;&#37030;&#23398;&#20064;&#32858;&#21512;&#31639;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#25913;&#21892;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#20855;&#22791;&#26356;&#39640;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#23569;&#30340;&#35757;&#32451;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#27169;&#22411;&#37327;&#21270;&#26469;&#25913;&#21892;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#30340;&#26080;&#32447;&#36890;&#20449;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#20301;&#23485;FL&#26041;&#26696;&#20013;&#65292;&#36793;&#32536;&#35774;&#22791;&#35757;&#32451;&#21644;&#20256;&#36755;&#20854;&#26412;&#22320;FL&#27169;&#22411;&#21442;&#25968;&#30340;&#37327;&#21270;&#29256;&#26412;&#21040;&#19968;&#20010;&#21327;&#35843;&#26381;&#21153;&#22120;&#65292;&#23558;&#23427;&#20204;&#32858;&#21512;&#25104;&#19968;&#20010;&#37327;&#21270;&#30340;&#20840;&#23616;&#27169;&#22411;&#24182;&#21516;&#27493;&#35774;&#22791;&#12290;&#30446;&#26631;&#26159;&#20849;&#21516;&#30830;&#23450;&#29992;&#20110;&#26412;&#22320;FL&#27169;&#22411;&#37327;&#21270;&#30340;&#20301;&#23485;&#21644;&#27599;&#27425;&#36845;&#20195;&#21442;&#19982;FL&#35757;&#32451;&#30340;&#35774;&#22791;&#38598;&#21512;&#12290;&#25105;&#20204;&#23558;&#36825;&#35270;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#22312;&#27599;&#27425;&#36845;&#20195;&#30340;&#35774;&#22791;&#25277;&#26679;&#39044;&#31639;&#21644;&#24310;&#36831;&#35201;&#27714;&#19979;&#26368;&#23567;&#21270;&#37327;&#21270;FL&#30340;&#35757;&#32451;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#25152;&#21046;&#23450;&#30340;&#38382;&#39064;&#38590;&#20197;&#35299;&#20915;&#65292;&#27809;&#26377;(i)&#23545;&#37327;&#21270;&#22914;&#20309;&#24433;&#21709;&#20840;&#23616;ML&#24615;&#33021;&#30340;&#20855;&#20307;&#29702;&#35299;&#20197;&#21450;(ii)&#26381;&#21153;&#22120;&#26500;&#24314;&#36825;&#20010;&#36807;&#31243;&#20272;&#35745;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#31532;&#19968;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20998;&#26512;&#22320;&#34920;&#24449;&#20102;&#26377;&#38480;&#30340;&#26080;&#32447;&#36164;&#28304;&#22914;&#20309;&#24433;&#21709;&#37327;&#21270;FL&#24615;&#33021;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#26368;&#20248;&#30340;&#20301;&#23485;&#20998;&#37197;&#31574;&#30053;&#12290;&#20026;&#20102;&#24212;&#23545;&#31532;&#20108;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#23610;&#24230;&#37327;&#21270;FL&#32858;&#21512;&#31639;&#27861;&#65292;&#20351;&#26381;&#21153;&#22120;&#33021;&#22815;&#36731;&#26494;&#22320;&#20174;&#20998;&#24067;&#24335;&#37327;&#21270;FL&#27169;&#22411;&#20013;&#37325;&#26500;&#20840;&#23616;&#37327;&#21270;FL&#27169;&#22411;&#12290;&#24191;&#27867;&#30340;&#20223;&#30495;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#22521;&#35757;&#25439;&#22833;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#27604;&#29616;&#26377;&#25216;&#26415;&#26041;&#26696;&#25552;&#39640;&#20102;&#39640;&#36798;35&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers improving wireless communication and computation efficiency in federated learning (FL) via model quantization. In the proposed bitwidth FL scheme, edge devices train and transmit quantized versions of their local FL model parameters to a coordinating server, which aggregates them into a quantized global model and synchronizes the devices. The goal is to jointly determine the bitwidths employed for local FL model quantization and the set of devices participating in FL training at each iteration. We pose this as an optimization problem that aims to minimize the training loss of quantized FL under a per-iteration device sampling budget and delay requirement. However, the formulated problem is difficult to solve without (i) a concrete understanding of how quantization impacts global ML performance and (ii) the ability of the server to construct estimates of this process efficiently. To address the first challenge, we analytically characterize how limited wireless resou
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ReX&#65292;&#19968;&#20010;&#23558;&#26102;&#38388;&#20449;&#24687;&#34701;&#20837;&#27169;&#22411;&#26080;&#20851;&#23616;&#37096;&#35299;&#37322;&#25216;&#26415;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20026;&#35299;&#37322;&#28155;&#21152;&#26102;&#38388;&#20449;&#24687;&#65292;&#20351;&#19968;&#20123;&#29616;&#26377;&#27169;&#22411;&#26080;&#27861;&#24212;&#29992;&#30340;&#23616;&#37096;&#35299;&#37322;&#25216;&#26415;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#21487;&#21464;&#38271;&#24230;&#30340;&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2209.03798</link><description>&lt;p&gt;
ReX&#65306;&#19968;&#20010;&#23558;&#26102;&#38388;&#20449;&#24687;&#34701;&#20837;&#27169;&#22411;&#26080;&#20851;&#23616;&#37096;&#35299;&#37322;&#25216;&#26415;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
ReX: A Framework for Incorporating Temporal Information in Model-Agnostic Local Explanation Techniques. (arXiv:2209.03798v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03798
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ReX&#65292;&#19968;&#20010;&#23558;&#26102;&#38388;&#20449;&#24687;&#34701;&#20837;&#27169;&#22411;&#26080;&#20851;&#23616;&#37096;&#35299;&#37322;&#25216;&#26415;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20026;&#35299;&#37322;&#28155;&#21152;&#26102;&#38388;&#20449;&#24687;&#65292;&#20351;&#19968;&#20123;&#29616;&#26377;&#27169;&#22411;&#26080;&#27861;&#24212;&#29992;&#30340;&#23616;&#37096;&#35299;&#37322;&#25216;&#26415;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#21487;&#21464;&#38271;&#24230;&#30340;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20197;&#22788;&#29702;&#21487;&#21464;&#38271;&#24230;&#36755;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#21151;&#33021;&#65292;&#20294;&#36890;&#24120;&#24456;&#38590;&#35299;&#37322;&#12290;&#32570;&#20047;&#36879;&#26126;&#24230;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#35768;&#22810;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#35299;&#37322;&#25216;&#26415;&#23545;&#20110;&#25552;&#39640;&#36879;&#26126;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#26080;&#20851;&#36890;&#29992;&#35299;&#37322;&#25216;&#26415;&#27809;&#26377;&#32771;&#34385;&#36755;&#20837;&#25968;&#25454;&#28857;&#30340;&#21487;&#21464;&#38271;&#24230;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ReX&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20026;&#22788;&#29702;&#21487;&#21464;&#38271;&#24230;&#36755;&#20837;&#30340;&#27169;&#22411;&#36866;&#24212;&#21508;&#31181;&#35299;&#37322;&#25216;&#26415;&#65292;&#25193;&#23637;&#35299;&#37322;&#35206;&#30422;&#21040;&#19981;&#21516;&#38271;&#24230;&#30340;&#25968;&#25454;&#28857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#25913;&#21464;&#29616;&#26377;&#25216;&#26415;&#26680;&#24515;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#29616;&#26377;&#25216;&#26415;&#29983;&#25104;&#30340;&#35299;&#37322;&#28155;&#21152;&#26102;&#38388;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#20004;&#31181;&#27969;&#34892;&#30340;&#35299;&#37322;&#25216;&#26415;LIME&#21644;Anchors&#19978;&#23454;&#29616;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;ReX&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20004;&#20010;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#19977;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#30528;&#22320;&#25552;&#39640;&#20102;&#35299;&#37322;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network models that can handle inputs of variable lengths are powerful, but often hard to interpret. The lack of transparency hinders their adoption in many domains. Explanation techniques are essential for improving transparency. However, existing model-agnostic general explanation techniques do not consider the variable lengths of input data points, which limits their effectiveness. To address this limitation, we propose ReX, a general framework for adapting various explanation techniques to models that process variable-length inputs, expanding explanation coverage to data points of different lengths. Our approach adds temporal information to the explanations generated by existing techniques without altering their core algorithms. We instantiate our approach on two popular explanation techniques: LIME and Anchors. To evaluate the effectiveness of ReX, we apply our approach to three models in two different tasks. Our evaluation results demonstrate that our approach significantl
&lt;/p&gt;</description></item><item><title>Fix-A-Step&#26159;&#19968;&#20010;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#31616;&#21270;&#27969;&#31243;&#65292;&#23558;&#25152;&#26377;&#26410;&#32463;&#31579;&#36873;&#30340;&#26080;&#26631;&#31614;&#22270;&#20687;&#35270;&#20026;&#28508;&#22312;&#26377;&#29992;&#30340;&#65307;&#22686;&#24378;&#26377;&#26631;&#31614;&#38598;&#30340;&#25968;&#25454;&#65292;&#20462;&#27491;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#26041;&#24335;&#21487;&#20197;&#20462;&#22797;&#35768;&#22810;&#24120;&#35265;&#30340;&#28145;&#24230; SSL &#26041;&#27861;&#65292;&#24182;&#22312;&#21307;&#23398;&#25104;&#20687;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.11870</link><description>&lt;p&gt;
Fix-A-Step: &#21322;&#30417;&#30563;&#23398;&#20064;&#22788;&#29702;&#26410;&#32463;&#31579;&#36873;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Fix-A-Step: Semi-supervised Learning from Uncurated Unlabeled Data. (arXiv:2208.11870v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11870
&lt;/p&gt;
&lt;p&gt;
Fix-A-Step&#26159;&#19968;&#20010;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#31616;&#21270;&#27969;&#31243;&#65292;&#23558;&#25152;&#26377;&#26410;&#32463;&#31579;&#36873;&#30340;&#26080;&#26631;&#31614;&#22270;&#20687;&#35270;&#20026;&#28508;&#22312;&#26377;&#29992;&#30340;&#65307;&#22686;&#24378;&#26377;&#26631;&#31614;&#38598;&#30340;&#25968;&#25454;&#65292;&#20462;&#27491;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#26041;&#24335;&#21487;&#20197;&#20462;&#22797;&#35768;&#22810;&#24120;&#35265;&#30340;&#28145;&#24230; SSL &#26041;&#27861;&#65292;&#24182;&#22312;&#21307;&#23398;&#25104;&#20687;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064; (SSL) &#22312;&#35757;&#32451;&#20998;&#31867;&#22120;&#26102;&#65292;&#36890;&#36807;&#22312;&#35768;&#22810;&#26080;&#26631;&#31614;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25215;&#35834;&#27604;&#22312;&#23569;&#37327;&#26377;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20363;&#22914;&#21307;&#23398;&#25104;&#20687;&#65292;&#20026;&#20102;&#36895;&#24230;&#32780;&#25910;&#38598;&#26410;&#32463;&#31579;&#36873;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#22240;&#27492;&#21487;&#33021;&#19982;&#26377;&#26631;&#31614;&#38598;&#20013;&#30340;&#31867;&#21035;&#25110;&#29305;&#24449;&#19981;&#21516;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#20195;&#28145;&#24230;&#21322;&#30417;&#30563;&#23398;&#20064;&#22312;&#22788;&#29702;&#26410;&#32463;&#31579;&#36873;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#26102;&#65292;&#24120;&#24120;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#36739;&#20026;&#22797;&#26434;&#30340;&#26041;&#27861;&#36890;&#36807;&#26816;&#27979;&#20998;&#24067;&#22806;&#30340;&#26080;&#26631;&#31614;&#22270;&#20687;&#65292;&#28982;&#21518;&#20002;&#24323;&#25110;&#38477;&#20302;&#23427;&#20204;&#30340;&#26435;&#37325;&#26469;&#20462;&#22797;&#36825;&#20010;&#38382;&#39064;&#12290;&#19982;&#27492;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; Fix-A-Step&#65292;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#36807;&#31243;&#65292;&#23558;&#25152;&#26377;&#26410;&#32463;&#31579;&#36873;&#30340;&#26080;&#26631;&#31614;&#22270;&#20687;&#35270;&#20026;&#28508;&#22312;&#26377;&#29992;&#30340;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#27934;&#35265;&#26159;&#65292;&#21363;&#20351;&#26410;&#32463;&#31579;&#36873;&#30340;&#22270;&#20687;&#20063;&#21487;&#20197;&#20135;&#29983;&#26377;&#29992;&#30340;&#24050;&#26631;&#35760;&#25968;&#25454;&#22686;&#24378;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#30340;&#26041;&#24335;&#65292;&#20197;&#38450;&#27490;&#20248;&#21270;&#22810;&#20219;&#21153; SSL &#25439;&#22833;&#23545;&#26377;&#26631;&#31614;&#38598;&#20934;&#30830;&#24615;&#30340;&#25439;&#23475;&#12290;Fix-A-Step &#21487;&#20197;&#20462;&#22797;&#35768;&#22810;&#24120;&#35265;&#30340;&#28145;&#24230; SSL &#26041;&#27861;&#65292;&#22312; CIFAR &#22522;&#20934;&#27979;&#35797;&#20013;&#25552;&#39640;&#20102;&#25152;&#26377;&#27979;&#35797;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#21307;&#23398;&#25104;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;Fix-A-Step &#22312;&#22788;&#29702;&#26410;&#32463;&#31579;&#36873;&#30340;&#26080;&#26631;&#31614;&#22270;&#20687;&#26041;&#38754;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#35201;&#22909;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning (SSL) promises improved accuracy compared to training classifiers on small labeled datasets by also training on many unlabeled images. In real applications like medical imaging, unlabeled data will be collected for expediency and thus uncurated: possibly different from the labeled set in classes or features. Unfortunately, modern deep SSL often makes accuracy worse when given uncurated unlabeled data. Recent complex remedies try to detect out-of-distribution unlabeled images and then discard or downweight them. Instead, we introduce Fix-A-Step, a simpler procedure that views all uncurated unlabeled images as potentially helpful. Our first insight is that even uncurated images can yield useful augmentations of labeled data. Second, we modify gradient descent updates to prevent optimizing a multi-task SSL loss from hurting labeled-set accuracy. Fix-A-Step can repair many common deep SSL methods, improving accuracy on CIFAR benchmarks across all tested methods and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;M2WU&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20013;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#65292;&#24182;&#22312;&#20840;&#21453;&#39304;&#21644;&#22122;&#22768;&#21453;&#39304;&#24773;&#20917;&#19979;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.09855</link><description>&lt;p&gt;
&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20013;&#20840;&#21453;&#39304;&#21644;&#22122;&#22768;&#21453;&#39304;&#19979;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Last-Iterate Convergence with Full and Noisy Feedback in Two-Player Zero-Sum Games. (arXiv:2208.09855v3 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;M2WU&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20013;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#65292;&#24182;&#22312;&#20840;&#21453;&#39304;&#21644;&#22122;&#22768;&#21453;&#39304;&#24773;&#20917;&#19979;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31361;&#21464;&#39537;&#21160;&#20056;&#24615;&#26435;&#37325;&#26356;&#26032;(M2WU)&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20004;&#20154;&#38646;&#21644;&#27491;&#24577;&#22411;&#21338;&#24328;&#20013;&#30340;&#22343;&#34913;&#28857;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#20840;&#21453;&#39304;&#21644;&#22122;&#22768;&#21453;&#39304;&#20004;&#31181;&#24773;&#20917;&#19979;&#37117;&#20855;&#26377;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#24615;&#36136;&#12290;&#20854;&#20013;&#65292;&#21069;&#32773;&#34920;&#31034;&#29609;&#23478;&#35266;&#23519;&#21040;&#20182;&#20204;&#30340;&#25928;&#29992;&#20989;&#25968;&#30340;&#31934;&#30830;&#26799;&#24230;&#21521;&#37327;&#65292;&#21518;&#32773;&#34920;&#31034;&#29609;&#23478;&#21482;&#35266;&#23519;&#21040;&#24102;&#22122;&#22768;&#30340;&#26799;&#24230;&#21521;&#37327;&#12290;&#19982;&#24191;&#21463;&#27426;&#36814;&#30340;&#20056;&#24615;&#26435;&#37325;&#26356;&#26032;(MWU)&#21644;&#20048;&#35266;&#30340;MWU&#65288;OMWU&#65289;&#31639;&#27861;&#30456;&#27604;&#65292;&#22312;&#22122;&#22768;&#21453;&#39304;&#19979;&#65292;&#23427;&#20204;&#21487;&#33021;&#19981;&#33021;&#25910;&#25947;&#21040;&#32435;&#20160;&#22343;&#34913;&#12290;&#30456;&#21453;&#65292;M2WU&#26041;&#27861;&#22312;&#20004;&#31181;&#21453;&#39304;&#24773;&#20917;&#19979;&#37117;&#34920;&#29616;&#20986;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#21040;&#25509;&#36817;&#32435;&#20160;&#22343;&#34913;&#28857;&#30340;&#31283;&#23450;&#28857;&#30340;&#24615;&#36136;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36890;&#36807;&#36845;&#20195;&#36866;&#24212;&#31361;&#21464;&#39033;&#65292;M2WU&#26041;&#27861;&#26368;&#32456;&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#31934;&#30830;&#30340;&#32435;&#20160;&#24179;&#34913;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;M2WU&#22312;&#21487;&#21033;&#29992;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#21644;&#25928;&#26524;&#39640;&#20110;MWU&#21644;OMWU&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes Mutation-Driven Multiplicative Weights Update (M2WU) for learning an equilibrium in two-player zero-sum normal-form games and proves that it exhibits the last-iterate convergence property in both full and noisy feedback settings. In the former, players observe their exact gradient vectors of the utility functions. In the latter, they only observe the noisy gradient vectors. Even the celebrated Multiplicative Weights Update (MWU) and Optimistic MWU (OMWU) algorithms may not converge to a Nash equilibrium with noisy feedback. On the contrary, M2WU exhibits the last-iterate convergence to a stationary point near a Nash equilibrium in both feedback settings. We then prove that it converges to an exact Nash equilibrium by iteratively adapting the mutation term. We empirically confirm that M2WU outperforms MWU and OMWU in exploitability and convergence rates.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#20010;&#21517;&#20026;FedLN&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#22788;&#29702;&#26631;&#31614;&#22122;&#22768;&#65292;&#36825;&#26159;FL&#20013;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#19988;&#24433;&#21709;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;FedLN&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#28085;&#30422;FL&#21021;&#22987;&#21270;&#12289;&#35774;&#22791;&#31471;&#27169;&#22411;&#35757;&#32451;&#21644;&#26381;&#21153;&#22120;&#27169;&#22411;&#32858;&#21512;&#19977;&#20010;&#38454;&#27573;&#12290;</title><link>http://arxiv.org/abs/2208.09378</link><description>&lt;p&gt;
&#20174;&#28151;&#20081;&#26631;&#31614;&#20013;&#23398;&#20064;&#21644;&#35856;&#65306;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Labeling Chaos to Learning Harmony: Federated Learning with Noisy Labels. (arXiv:2208.09378v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09378
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#20010;&#21517;&#20026;FedLN&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#22788;&#29702;&#26631;&#31614;&#22122;&#22768;&#65292;&#36825;&#26159;FL&#20013;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#19988;&#24433;&#21709;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;FedLN&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#28085;&#30422;FL&#21021;&#22987;&#21270;&#12289;&#35774;&#22791;&#31471;&#27169;&#22411;&#35757;&#32451;&#21644;&#26381;&#21153;&#22120;&#27169;&#22411;&#32858;&#21512;&#19977;&#20010;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#65292;&#23427;&#33021;&#22815;&#20174;&#20998;&#25955;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#27169;&#22411;&#65292;&#26631;&#31614;&#24037;&#20316;&#22996;&#25176;&#32473;&#23458;&#25143;&#31471;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;FL&#26041;&#27861;&#20551;&#35774;&#39640;&#36136;&#37327;&#26631;&#31614;&#24050;&#32463;&#22312;&#29992;&#25143;&#35774;&#22791;&#19978;&#20934;&#22791;&#22909;&#20102;&#65292;&#20294;&#23454;&#38469;&#19978;&#65292;&#22312;FL&#20013;&#33258;&#28982;&#21487;&#33021;&#20986;&#29616;&#26631;&#31614;&#22122;&#22768;&#65292;&#36825;&#19982;&#23458;&#25143;&#31471;&#30340;&#29305;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;&#30001;&#20110;&#22312;FL&#20013;&#21487;&#29992;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#26174;&#30528;&#30340;&#26631;&#31614;&#22122;&#22768;&#21464;&#21270;&#65292;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#38598;&#20013;&#24335;&#26041;&#27861;&#34920;&#29616;&#19981;&#23613;&#22914;&#20154;&#24847;&#65292;&#32780;&#20808;&#21069;&#30340;FL&#30740;&#31350;&#21017;&#20381;&#36182;&#20110;&#39069;&#22806;&#30340;&#24178;&#20928;&#25968;&#25454;&#25110;&#36807;&#24230;&#30340;&#35774;&#22791;&#31471;&#35745;&#31639;&#26041;&#26696;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedLN&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#30340;FL&#35757;&#32451;&#38454;&#27573;&#22788;&#29702;&#26631;&#31614;&#22122;&#22768;&#65307;&#21363;FL&#21021;&#22987;&#21270;&#12289;&#35774;&#22791;&#31471;&#27169;&#22411;&#35757;&#32451;&#21644;&#26381;&#21153;&#22120;&#27169;&#22411;&#32858;&#21512;&#65292;&#33021;&#22815;&#36866;&#24212;FL&#31995;&#32479;&#20013;&#21508;&#31181;&#35774;&#22791;&#30340;&#35745;&#31639;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FedLN&#35745;&#31639;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a distributed machine learning paradigm that enables learning models from decentralized private datasets, where the labeling effort is entrusted to the clients. While most existing FL approaches assume high-quality labels are readily available on users' devices; in reality, label noise can naturally occur in FL and is closely related to clients' characteristics. Due to scarcity of available data and significant label noise variations among clients in FL, existing state-of-the-art centralized approaches exhibit unsatisfactory performance, while prior FL studies rely on excessive on-device computational schemes or additional clean data available on server. Here, we propose FedLN, a framework to deal with label noise across different FL training stages; namely, FL initialization, on-device model training, and server model aggregation, able to accommodate the diverse computational capabilities of devices in a FL system. Specifically, FedLN computes per-client noi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#22240;&#32452;&#24207;&#21015;&#21306;&#20998;&#21644;&#21487;&#35270;&#21270;COVID-19&#20027;&#35201;&#21464;&#24322;&#20307;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#36825;&#19968;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#20102;&#35299;&#30149;&#27602;&#30340;&#27969;&#34892;&#30149;&#23398;&#21644;&#36827;&#21270;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2208.01439</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#21306;&#20998;COVID-19&#20027;&#35201;&#21464;&#24322;&#20307;
&lt;/p&gt;
&lt;p&gt;
Unsupervised machine learning framework for discriminating major variants of concern during COVID-19. (arXiv:2208.01439v3 [q-bio.OT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.01439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#22240;&#32452;&#24207;&#21015;&#21306;&#20998;&#21644;&#21487;&#35270;&#21270;COVID-19&#20027;&#35201;&#21464;&#24322;&#20307;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#36825;&#19968;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#20102;&#35299;&#30149;&#27602;&#30340;&#27969;&#34892;&#30149;&#23398;&#21644;&#36827;&#21270;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30149;&#27602;&#30340;&#39640;&#31361;&#21464;&#29575;&#65292;COVID-19&#30123;&#24773;&#36805;&#36895;&#28436;&#21464;&#12290;&#26576;&#20123;&#30149;&#27602;&#21464;&#24322;&#20307;&#65292;&#22914;Delta&#21644;Omicron&#65292;&#20986;&#29616;&#24182;&#25913;&#21464;&#20102;&#30149;&#27602;&#30340;&#29305;&#24615;&#65292;&#23548;&#33268;&#30149;&#20363;&#20256;&#25773;&#21644;&#27515;&#20129;&#29575;&#20005;&#37325;&#12290;&#36825;&#20123;&#21464;&#24322;&#20307;&#23545;&#20840;&#29699;&#21307;&#30103;&#31995;&#32479;&#36896;&#25104;&#20102;&#27785;&#37325;&#36127;&#25285;&#65292;&#23545;&#26053;&#34892;&#12289;&#29983;&#20135;&#21147;&#21644;&#19990;&#30028;&#32463;&#27982;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#21387;&#32553;&#12289;&#25551;&#36848;&#21644;&#21487;&#35270;&#21270;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#22522;&#22240;&#32452;&#24207;&#21015;&#21306;&#20998;&#21644;&#21487;&#35270;&#21270;COVID-19&#20027;&#35201;&#21464;&#24322;&#20307;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#36825;&#20123;&#26041;&#27861;&#37319;&#29992;&#19968;&#20123;&#36873;&#23450;&#30340;&#38477;&#32500;&#21644;&#32858;&#31867;&#25216;&#26415;&#30340;&#32452;&#21512;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23545;&#25968;&#25454;&#25191;&#34892;k-mer&#20998;&#26512;&#26469;&#22788;&#29702;RNA&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#21253;&#25324;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#12289;t-&#20998;&#24067;&#38543;&#26426;&#37051;&#22495;&#23884;&#20837;&#65288;t-SNE&#65289;&#21644;&#22343;&#21248;&#27969;&#24418;&#36924;&#36817;&#21644;&#25237;&#24433;&#65288;UMAP&#65289;&#30340;&#36873;&#23450;&#38477;&#32500;&#26041;&#27861;&#36827;&#19968;&#27493;&#21487;&#35270;&#21270;&#21644;&#27604;&#36739;&#32467;&#26524;&#12290;&#36825;&#20010;&#26694;&#26550;&#23637;&#31034;&#30340;&#32858;&#31867;&#21644;&#20851;&#32852;&#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#20102;&#35299;COVID-19&#30149;&#27602;&#30340;&#27969;&#34892;&#30149;&#23398;&#21644;&#36827;&#21270;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the high mutation rate of the virus, the COVID-19 pandemic evolved rapidly. Certain variants of the virus, such as Delta and Omicron, emerged with altered viral properties leading to severe transmission and death rates. These variants burdened the medical systems worldwide with a major impact to travel, productivity, and the world economy. Unsupervised machine learning methods have the ability to compress, characterize, and visualize unlabelled data. This paper presents a framework that utilizes unsupervised machine learning methods to discriminate and visualize the associations between major COVID-19 variants based on their genome sequences. These methods comprise a combination of selected dimensionality reduction and clustering techniques. The framework processes the RNA sequences by performing a k-mer analysis on the data and further visualises and compares the results using selected dimensionality reduction methods that include principal component analysis (PCA), t-distribut
&lt;/p&gt;</description></item><item><title>&#28909;&#21147;&#23398;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#24050;&#32463;&#34987;&#24191;&#27867;&#24847;&#35782;&#21040;&#65292;&#25991;&#31456;&#22238;&#39038;&#20102;&#28909;&#21147;&#23398;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#30740;&#31350;&#19981;&#21516;&#26041;&#38754;&#23545;&#23398;&#20064;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2207.12749</link><description>&lt;p&gt;
&#23398;&#20064;&#29289;&#29702;&#29616;&#35937;&#30340;&#28909;&#21147;&#23398;&#12290; (arXiv:2207.12749v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Thermodynamics of learning physical phenomena. (arXiv:2207.12749v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12749
&lt;/p&gt;
&lt;p&gt;
&#28909;&#21147;&#23398;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#24050;&#32463;&#34987;&#24191;&#27867;&#24847;&#35782;&#21040;&#65292;&#25991;&#31456;&#22238;&#39038;&#20102;&#28909;&#21147;&#23398;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#30740;&#31350;&#19981;&#21516;&#26041;&#38754;&#23545;&#23398;&#20064;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28909;&#21147;&#23398;&#21487;&#20197;&#34987;&#35270;&#20026;&#39640;&#35748;&#30693;&#32423;&#21035;&#19979;&#30340;&#29289;&#29702;&#34920;&#36798;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24050;&#32463;&#24847;&#35782;&#21040;&#20854;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;&#30340;&#28508;&#21147;&#65292;&#20197;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#36807;&#31243;&#36798;&#25104;&#20934;&#30830;&#21644;&#21487;&#20449;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#22238;&#39038;&#28909;&#21147;&#23398;&#22914;&#20309;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#25552;&#20379;&#26377;&#29992;&#30340;&#27934;&#35265;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30740;&#31350;&#19981;&#21516;&#26041;&#38754;&#23545;&#23398;&#20064;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#25551;&#36848;&#32473;&#23450;&#29616;&#35937;&#30340;&#23610;&#24230;&#12289;&#20026;&#35813;&#25551;&#36848;&#36873;&#25321;&#30456;&#20851;&#21464;&#37327;&#25110;&#21487;&#29992;&#20110;&#23398;&#20064;&#36807;&#31243;&#30340;&#19981;&#21516;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thermodynamics could be seen as an expression of physics at a high epistemic level. As such, its potential as an inductive bias to help machine learning procedures attain accurate and credible predictions has been recently realized in many fields. We review how thermodynamics provides helpful insights in the learning process. At the same time, we study the influence of aspects such as the scale at which a given phenomenon is to be described, the choice of relevant variables for this description or the different techniques available for the learning process.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#26435;&#22810;&#20852;&#36259;&#26816;&#32034;&#27169;&#22411;&#65288;Multi-Interest Preference&#65292;MIP&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20026;&#29992;&#25143;&#24314;&#31435;&#22810;&#20010;&#20852;&#36259;&#23884;&#20837;&#65292;&#24182;&#23558;&#29992;&#25143;&#22312;&#22810;&#20010;&#20852;&#36259;&#19978;&#30340;&#20559;&#22909;&#36827;&#34892;&#24314;&#27169;&#65292;&#20174;&#32780;&#25552;&#39640;&#20505;&#36873;&#26816;&#32034;&#32467;&#26524;&#30340;&#26597;&#20840;&#29575;&#12290;</title><link>http://arxiv.org/abs/2207.06652</link><description>&lt;p&gt;
&#27599;&#20010;&#20154;&#30340;&#20559;&#22909;&#21464;&#21270;&#19981;&#21516;&#65306;&#21152;&#26435;&#22810;&#20852;&#36259;&#26816;&#32034;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Everyone's Preference Changes Differently: Weighted Multi-Interest Retrieval Model. (arXiv:2207.06652v4 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#26435;&#22810;&#20852;&#36259;&#26816;&#32034;&#27169;&#22411;&#65288;Multi-Interest Preference&#65292;MIP&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20026;&#29992;&#25143;&#24314;&#31435;&#22810;&#20010;&#20852;&#36259;&#23884;&#20837;&#65292;&#24182;&#23558;&#29992;&#25143;&#22312;&#22810;&#20010;&#20852;&#36259;&#19978;&#30340;&#20559;&#22909;&#36827;&#34892;&#24314;&#27169;&#65292;&#20174;&#32780;&#25552;&#39640;&#20505;&#36873;&#26816;&#32034;&#32467;&#26524;&#30340;&#26597;&#20840;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#23884;&#20837;&#65288;&#29992;&#25143;&#30340;&#21521;&#37327;&#21270;&#34920;&#31034;&#65289;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#26500;&#24314;&#29992;&#25143;&#30340;&#22810;&#32500;&#24230;&#34920;&#31034;&#65292;&#20197;&#20415;&#20110;&#26816;&#32034;&#20219;&#21153;&#20013;&#25214;&#21040;&#30456;&#20284;&#30340;&#29289;&#21697;&#65292;&#24182;&#19988;&#24050;&#32463;&#22312;&#24037;&#19994;&#25512;&#33616;&#31995;&#32479;&#20013;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#26368;&#36817;&#20154;&#20204;&#21457;&#29616;&#20351;&#29992;&#22810;&#31181;&#23884;&#20837;&#65288;&#21363;&#22810;&#20010;&#32500;&#24230;&#30340;&#29992;&#25143;&#34920;&#31034;&#65289;&#26469;&#34920;&#31034;&#29992;&#25143;&#30340;&#20852;&#36259;&#26159;&#26377;&#29992;&#30340;&#65292;&#27599;&#20010;&#23884;&#20837;&#34920;&#31034;&#29992;&#25143;&#30340;&#26576;&#20010;&#20027;&#39064;&#20852;&#36259;&#12290;&#23545;&#20110;&#22810;&#20852;&#36259;&#34920;&#31034;&#65292;&#37325;&#35201;&#30340;&#26159;&#23545;&#29992;&#25143;&#22312;&#19981;&#21516;&#20027;&#39064;&#19978;&#30340;&#20559;&#22909;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#19988;&#20102;&#35299;&#20559;&#22909;&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#26080;&#27861;&#20272;&#31639;&#29992;&#25143;&#23545;&#27599;&#20010;&#20852;&#36259;&#30340;&#22909;&#24863;&#24230;&#65292;&#35201;&#20040;&#19981;&#21512;&#29702;&#22320;&#20551;&#35774;&#27599;&#20010;&#29992;&#25143;&#23545;&#27599;&#20010;&#20852;&#36259;&#30340;&#20852;&#36259;&#24378;&#24230;&#20250;&#20197;&#30456;&#31561;&#30340;&#36895;&#29575;&#19979;&#38477;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#20505;&#36873;&#26816;&#32034;&#32467;&#26524;&#30340;&#26597;&#20840;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21152;&#26435;&#22810;&#20852;&#36259;&#26816;&#32034;&#27169;&#22411;&#65288;Multi-Interest Preference, MIP&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20026;&#29992;&#25143;&#20135;&#29983;&#22810;&#20010;&#20852;&#36259;&#23884;&#20837;&#65292;&#24182;&#19988;&#21487;&#20197;&#23545;&#29992;&#25143;&#22312;&#22810;&#31181;&#20852;&#36259;&#19979;&#30340;&#20559;&#22909;&#36827;&#34892;&#20272;&#35745;&#65292;&#20174;&#32780;&#25552;&#39640;&#20505;&#36873;&#26816;&#32034;&#32467;&#26524;&#30340;&#26597;&#20840;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
User embeddings (vectorized representations of a user) are essential in recommendation systems. Numerous approaches have been proposed to construct a representation for the user in order to find similar items for retrieval tasks, and they have been proven effective in industrial recommendation systems as well. Recently people have discovered the power of using multiple embeddings to represent a user, with the hope that each embedding represents the user's interest in a certain topic. With multi-interest representation, it's important to model the user's preference over the different topics and how the preference change with time. However, existing approaches either fail to estimate the user's affinity to each interest or unreasonably assume every interest of every user fades with an equal rate with time, thus hurting the recall of candidate retrieval. In this paper, we propose the Multi-Interest Preference (MIP) model, an approach that not only produces multi-interest for users by usin
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#65306;&#24369;&#30417;&#30563;&#26816;&#27979;&#21464;&#24418;&#22120;&#65288;Weakly Supervised Detection Transformer&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#36716;&#31227;&#33267;&#25968;&#30334;&#31181;&#26032;&#22411;&#29289;&#21697;&#30340;WSOD&#24494;&#35843;&#20013;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#20808;&#36827;&#27169;&#22411;&#65292;&#22312;&#22823;&#35268;&#27169;&#26032;&#22411;&#29289;&#21697;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.05205</link><description>&lt;p&gt;
&#21033;&#29992;&#24369;&#30417;&#30563;&#26816;&#27979;&#21464;&#24418;&#22120;&#25193;&#23637;&#26032;&#22411;&#29289;&#21697;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Scaling Novel Object Detection with Weakly Supervised Detection Transformers. (arXiv:2207.05205v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.05205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#65306;&#24369;&#30417;&#30563;&#26816;&#27979;&#21464;&#24418;&#22120;&#65288;Weakly Supervised Detection Transformer&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#36716;&#31227;&#33267;&#25968;&#30334;&#31181;&#26032;&#22411;&#29289;&#21697;&#30340;WSOD&#24494;&#35843;&#20013;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#20808;&#36827;&#27169;&#22411;&#65292;&#22312;&#22823;&#35268;&#27169;&#26032;&#22411;&#29289;&#21697;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#20013;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#26159;&#24494;&#35843;&#29616;&#26377;&#27169;&#22411;&#20197;&#20415;&#26816;&#27979;&#26032;&#22411;&#29289;&#21697;&#65292;&#20294;&#26631;&#27880;&#36793;&#30028;&#26694;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#37329;&#38065;&#12290;&#24369;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#22270;&#20687;&#32423;&#21035;&#26631;&#31614;&#26469;&#35757;&#32451;&#29289;&#21697;&#26816;&#27979;&#22120;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#24369;&#30417;&#30563;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#20165;&#36866;&#29992;&#20110;&#23567;&#35268;&#27169;&#25968;&#25454;&#65292;&#24182;&#38656;&#35201;&#22810;&#27425;&#35757;&#32451;&#21644;&#25913;&#36827;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24369;&#30417;&#30563;&#26816;&#27979;&#21464;&#24418;&#22120;&#65292;&#23427;&#33021;&#22815;&#23558;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#26377;&#25928;&#22320;&#36716;&#31227;&#33267;&#25968;&#30334;&#31181;&#26032;&#22411;&#29289;&#21697;&#30340;WSOD&#24494;&#35843;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#39044;&#35757;&#32451;&#30693;&#35782;&#26469;&#25913;&#36827;&#22312;WSOD&#26041;&#27861;&#20013;&#24120;&#29992;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#26694;&#26550;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#26032;&#22411;&#29289;&#21697;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#25193;&#23637;&#30740;&#31350;&#8230;
&lt;/p&gt;
&lt;p&gt;
A critical object detection task is finetuning an existing model to detect novel objects, but the standard workflow requires bounding box annotations which are time-consuming and expensive to collect. Weakly supervised object detection (WSOD) offers an appealing alternative, where object detectors can be trained using image-level labels. However, the practical application of current WSOD models is limited, as they only operate at small data scales and require multiple rounds of training and refinement. To address this, we propose the Weakly Supervised Detection Transformer, which enables efficient knowledge transfer from a large-scale pretraining dataset to WSOD finetuning on hundreds of novel objects. Additionally, we leverage pretrained knowledge to improve the multiple instance learning (MIL) framework often used in WSOD methods. Our experiments show that our approach outperforms previous state-of-the-art models on large-scale novel object detection datasets, and our scaling study r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CGAN&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#20174;&#26032;&#26465;&#20214;&#19979;&#30340;&#27491;&#24120;&#25968;&#25454;&#20013;&#29983;&#25104;&#23545;&#24212;&#30340;&#28378;&#21160;&#36724;&#25215;&#25925;&#38556;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#24320;&#21457;&#25968;&#25454;&#39537;&#21160;&#30340;&#25925;&#38556;&#35786;&#26029;&#24037;&#20855;&#65292;&#25552;&#39640;&#26816;&#27979;&#25925;&#38556;&#31867;&#22411;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2206.12076</link><description>&lt;p&gt;
&#22522;&#20110;&#25913;&#36827;&#30340;CGAN&#26694;&#26550;&#30340;&#26032;&#26465;&#20214;&#19979;&#28378;&#21160;&#36724;&#25215;&#25925;&#38556;&#26679;&#26412;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Rolling Bearing Fault Samples in New Conditions: A framework based on a modified CGAN. (arXiv:2206.12076v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12076
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CGAN&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#20174;&#26032;&#26465;&#20214;&#19979;&#30340;&#27491;&#24120;&#25968;&#25454;&#20013;&#29983;&#25104;&#23545;&#24212;&#30340;&#28378;&#21160;&#36724;&#25215;&#25925;&#38556;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#24320;&#21457;&#25968;&#25454;&#39537;&#21160;&#30340;&#25925;&#38556;&#35786;&#26029;&#24037;&#20855;&#65292;&#25552;&#39640;&#26816;&#27979;&#25925;&#38556;&#31867;&#22411;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36724;&#25215;&#26159;&#26059;&#36716;&#26426;&#26800;&#30340;&#37325;&#35201;&#32452;&#20214;&#20043;&#19968;&#65292;&#23481;&#26131;&#20986;&#29616;&#24847;&#22806;&#25925;&#38556;&#12290;&#22240;&#27492;&#65292;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;&#21644;&#29366;&#24577;&#30417;&#27979;&#23545;&#20110;&#20943;&#23569;&#22810;&#20010;&#34892;&#19994;&#30340;&#36816;&#33829;&#25104;&#26412;&#21644;&#20572;&#26426;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#21508;&#31181;&#29983;&#20135;&#26465;&#20214;&#19979;&#65292;&#36724;&#25215;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#36127;&#36733;&#21644;&#36716;&#36895;&#19979;&#36816;&#34892;&#65292;&#36825;&#20250;&#23548;&#33268;&#19982;&#27599;&#31181;&#25925;&#38556;&#31867;&#22411;&#30456;&#20851;&#30340;&#19981;&#21516;&#25391;&#21160;&#27169;&#24335;&#12290;&#27491;&#24120;&#25968;&#25454;&#24456;&#20805;&#36275;&#65292;&#22240;&#20026;&#31995;&#32479;&#36890;&#24120;&#22312;&#39044;&#26399;&#30340;&#26465;&#20214;&#19979;&#24037;&#20316;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25925;&#38556;&#25968;&#25454;&#24456;&#23569;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#27809;&#26377;&#35760;&#24405;&#25925;&#38556;&#31867;&#21035;&#30340;&#25968;&#25454;&#12290;&#33719;&#21462;&#25925;&#38556;&#25968;&#25454;&#23545;&#20110;&#24320;&#21457;&#25968;&#25454;&#39537;&#21160;&#30340;&#25925;&#38556;&#35786;&#26029;&#24037;&#20855;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#25552;&#39640;&#25805;&#20316;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;&#22240;&#27492;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CGANs&#65289;&#30340;&#26032;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#20219;&#20309;&#23454;&#38469;&#25925;&#38556;&#26465;&#20214;&#19979;&#20351;&#29992;&#27491;&#24120;&#25968;&#25454;&#21644;&#25925;&#38556;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#20174;&#30446;&#26631;&#26465;&#20214;&#30340;&#27491;&#24120;&#25968;&#25454;&#20013;&#29983;&#25104;&#23545;&#24212;&#30340;&#25925;&#38556;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#29305;&#24449;&#25552;&#21462;&#38454;&#27573;&#21644;&#25925;&#38556;&#21512;&#25104;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#20174;&#28304;&#26465;&#20214;&#30340;&#27491;&#24120;&#25968;&#25454;&#20013;&#25552;&#21462;&#25925;&#38556;&#29305;&#24449;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#29305;&#24449;&#20351;&#29992;&#25913;&#36827;&#30340;CGAN&#29983;&#25104;&#19982;&#30446;&#26631;&#26465;&#20214;&#30456;&#24212;&#30340;&#25925;&#38556;&#25968;&#25454;&#12290;&#23454;&#39564;&#22312;&#28378;&#21160;&#36724;&#25215;&#30340;&#27169;&#25311;&#25968;&#25454;&#38598;&#21644;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25104;&#21151;&#22320;&#22312;&#26032;&#26465;&#20214;&#19979;&#29983;&#25104;&#28378;&#21160;&#36724;&#25215;&#30340;&#25925;&#38556;&#26679;&#26412;&#65292;&#24182;&#22312;&#26816;&#27979;&#25925;&#38556;&#31867;&#22411;&#26041;&#38754;&#25552;&#20379;&#39640;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bearings are one of the vital components of rotating machines that are prone to unexpected faults. Therefore, bearing fault diagnosis and condition monitoring is essential for reducing operational costs and downtime in numerous industries. In various production conditions, bearings can be operated under a range of loads and speeds, which causes different vibration patterns associated with each fault type. Normal data is ample as systems usually work in desired conditions. On the other hand, fault data is rare, and in many conditions, there is no data recorded for the fault classes. Accessing fault data is crucial for developing data-driven fault diagnosis tools that can improve both the performance and safety of operations. To this end, a novel algorithm based on Conditional Generative Adversarial Networks (CGANs) is introduced. Trained on the normal and fault data on any actual fault conditions, this algorithm generates fault data from normal data of target conditions. The proposed me
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#24314;&#27169;&#26694;&#26550;&#8212;&#8212;&#21487;&#35299;&#37322;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;IME&#65289;&#65292;&#33021;&#22815;&#20135;&#29983;&#19982;&#8220;&#40657;&#30418;&#8221;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30456;&#23218;&#32654;&#30340;&#39640;&#31934;&#24230;&#65292;&#24182;&#22312;&#25552;&#20379;&#26377;&#29992;&#35299;&#37322;&#30340;&#21516;&#26102;&#65292;&#36824;&#20855;&#26377;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2206.02107</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Interpretable Mixture of Experts. (arXiv:2206.02107v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02107
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#24314;&#27169;&#26694;&#26550;&#8212;&#8212;&#21487;&#35299;&#37322;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;IME&#65289;&#65292;&#33021;&#22815;&#20135;&#29983;&#19982;&#8220;&#40657;&#30418;&#8221;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30456;&#23218;&#32654;&#30340;&#39640;&#31934;&#24230;&#65292;&#24182;&#22312;&#25552;&#20379;&#26377;&#29992;&#35299;&#37322;&#30340;&#21516;&#26102;&#65292;&#36824;&#20855;&#26377;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#34920;&#26684;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#65292;&#38656;&#35201;&#21487;&#38752;&#30340;&#27169;&#22411;&#35299;&#37322;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#20351;&#29992;&#26696;&#20363;&#32463;&#24120;&#28041;&#21450;&#39640;&#39118;&#38505;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#24314;&#27169;&#26694;&#26550;&#8212;&#8212;&#21487;&#35299;&#37322;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;IME&#65289;&#65292;&#23427;&#22312;&#24456;&#22810;&#24773;&#20917;&#19979;&#21487;&#20197;&#20135;&#29983;&#19982;&#8220;&#40657;&#30418;&#8221;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30456;&#23218;&#32654;&#30340;&#39640;&#31934;&#24230;&#65292;&#24182;&#20855;&#26377;&#26377;&#29992;&#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;IME&#21253;&#25324;&#19968;&#20010;&#20998;&#37197;&#27169;&#22359;&#21644;&#19968;&#20010;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#27599;&#20010;&#26679;&#26412;&#34987;&#20998;&#37197;&#32473;&#19968;&#20010;&#21333;&#19968;&#30340;&#19987;&#23478;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#20026;IME&#24341;&#20837;&#22810;&#20010;&#36873;&#39033;&#65292;&#22522;&#20110;&#21487;&#35299;&#37322;&#30340;&#20998;&#37197;&#21644;&#19987;&#23478;&#30340;&#36873;&#25321;&#12290;&#24403;&#19987;&#23478;&#34987;&#36873;&#25321;&#20026;&#21487;&#35299;&#37322;&#30340;&#32447;&#24615;&#27169;&#22411;&#26102;&#65292;IME&#20135;&#29983;&#20102;&#19968;&#31181;&#20869;&#22312;&#21487;&#35299;&#37322;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;IME&#20135;&#29983;&#30340;&#35299;&#37322;&#26159;&#39044;&#27979;&#35745;&#31639;&#30340;&#31934;&#30830;&#25551;&#36848;&#12290;&#38500;&#20102;&#26500;&#25104;&#29420;&#31435;&#30340;&#20869;&#22312;&#21487;&#35299;&#37322;&#30340;&#20307;&#31995;&#32467;&#26500;&#22806;&#65292;IME&#36824;&#20855;&#26377;&#25552;&#20379;&#39044;&#35757;&#32451;&#30340;&#40657;&#30418;&#27169;&#22411;&#39044;&#27979;&#21518;&#20107;&#23454;&#35299;&#37322;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;DNNs&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;IME&#22312;&#34920;&#26684;&#21644;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#19987;&#23478;&#21644;&#20998;&#37197;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The need for reliable model explanations is prominent for many machine learning applications, particularly for tabular and time-series data as their use cases often involve high-stakes decision making. Towards this goal, we introduce a novel interpretable modeling framework, Interpretable Mixture of Experts (IME), that yields high accuracy, comparable to `black-box' Deep Neural Networks (DNNs) in many cases, along with useful interpretability capabilities. IME consists of an assignment module and a mixture of experts, with each sample being assigned to a single expert for prediction. We introduce multiple options for IME based on the assignment and experts being interpretable. When the experts are chosen to be interpretable such as linear models, IME yields an inherently-interpretable architecture where the explanations produced by IME are the exact descriptions of how the prediction is computed. In addition to constituting a standalone inherently-interpretable architecture, IME has th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; B2T &#36830;&#25509;&#30340;&#26041;&#27861;&#65292;&#36830;&#25509;&#20102; Pre-LN &#21644; Post-LN &#23618;&#30340;&#36755;&#20986;&#65292;&#20026;&#28145;&#24230; Transformer &#25552;&#20379;&#20102;&#39640;&#31283;&#23450;&#24615;&#21644;&#26377;&#25928;&#30340;&#35757;&#32451;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.00330</link><description>&lt;p&gt;
B2T &#36830;&#25509;&#65306;&#26381;&#21153;&#20110;&#28145;&#24230; Transformer &#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
B2T Connection: Serving Stability and Performance in Deep Transformers. (arXiv:2206.00330v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; B2T &#36830;&#25509;&#30340;&#26041;&#27861;&#65292;&#36830;&#25509;&#20102; Pre-LN &#21644; Post-LN &#23618;&#30340;&#36755;&#20986;&#65292;&#20026;&#28145;&#24230; Transformer &#25552;&#20379;&#20102;&#39640;&#31283;&#23450;&#24615;&#21644;&#26377;&#25928;&#30340;&#35757;&#32451;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#23618;&#24402;&#19968;&#21270;&#65288;LN&#65289;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;Transformer &#30340;&#26550;&#26500;&#21487;&#20197;&#20998;&#20026;&#20004;&#31181;&#31867;&#22411;&#65306;Post-LN &#21644; Pre-LN&#12290;&#26368;&#36817;&#30340; Transformers &#20542;&#21521;&#20110;&#37319;&#29992; Pre-LN&#65292;&#22240;&#20026;&#22312; Post-LN &#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#28145;&#24230; Transformers &#20013;&#65288;&#20363;&#22914;&#26377;&#21313;&#20010;&#25110;&#26356;&#22810;&#23618;&#30340;&#27169;&#22411;&#65289;&#65292;&#35757;&#32451;&#32463;&#24120;&#19981;&#31283;&#23450;&#65292;&#23548;&#33268;&#24471;&#21040;&#26080;&#29992;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#19982; Pre-LN &#30456;&#27604;&#65292;&#22312;&#30456;&#23545;&#36739;&#27973;&#30340; Transformers&#65288;&#20363;&#22914;&#26377;&#20845;&#20010;&#25110;&#26356;&#23569;&#30340;&#23618;&#65289;&#20013;&#65292;Post-LN &#19968;&#30452;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#20174;&#32463;&#39564;&#21644;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#36825;&#20123;&#19981;&#19968;&#33268;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#24182;&#21457;&#29616;&#20102;&#20197;&#19979;&#21457;&#29616;&#65306;1&#65289;Post-LN &#20013;&#30340; LN &#26159;&#23548;&#33268;&#19981;&#31283;&#23450;&#35757;&#32451;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#32780; Pre-LN &#21487;&#20197;&#36991;&#20813;&#36825;&#31181;&#38382;&#39064;&#65307;2&#65289;Post-LN &#24448;&#24448;&#20250;&#22312;&#21453;&#21521;&#20256;&#25773;&#30340;&#39640;&#23618;&#20445;&#30041;&#26356;&#22823;&#30340;&#26799;&#24230;&#33539;&#25968;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#26377;&#25928;&#30340;&#35757;&#32451;&#12290;&#21033;&#29992;&#36825;&#20123;&#26032;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#21516;&#26102;&#25552;&#20379;&#39640;&#31283;&#23450;&#24615;&#21644;&#26377;&#25928;&#30340; Transformer &#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026; B2T Connection&#65292;&#23427;&#36830;&#25509;&#20102; Pre-LN &#21644; Post-LN &#23618;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;B2T Connection &#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#28145;&#24230; Transformers&#65288;&#26377;&#21313;&#20010;&#25110;&#26356;&#22810;&#23618;&#65289;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
From the perspective of the layer normalization (LN) positions, the architectures of Transformers can be categorized into two types: Post-LN and Pre-LN. Recent Transformers tend to be Pre-LN because, in Post-LN with deep Transformers (e.g., those with ten or more layers), the training is often unstable, resulting in useless models. However, Post-LN has consistently achieved better performance than Pre-LN in relatively shallow Transformers (e.g., those with six or fewer layers). This study first investigates the reason for these discrepant observations empirically and theoretically and made the following discoveries: 1, the LN in Post-LN is the main source of the vanishing gradient problem that leads to unstable training, whereas Pre-LN prevents it, and 2, Post-LN tends to preserve larger gradient norms in higher layers during the back-propagation, which may lead to effective training. Exploiting the new findings, we propose a method that can provide both high stability and effective tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#21363;&#24211;&#23384;&#37325;&#26032;&#22635;&#20805;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861; Perishable DQN&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#38144;&#21806;&#21516;&#26102;&#26368;&#23567;&#21270;&#28010;&#36153;&#65292;&#23454;&#29616;&#35745;&#31639;&#26426;&#24605;&#32500;&#23545;&#24223;&#29289;&#20943;&#37327;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2205.15455</link><description>&lt;p&gt;
&#24223;&#29289;&#20943;&#37327;&#30340;&#20223;&#30495;&#29615;&#22659;&#19982;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Simulation Environment and Reinforcement Learning Method for Waste Reduction. (arXiv:2205.15455v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#21363;&#24211;&#23384;&#37325;&#26032;&#22635;&#20805;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861; Perishable DQN&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#38144;&#21806;&#21516;&#26102;&#26368;&#23567;&#21270;&#28010;&#36153;&#65292;&#23454;&#29616;&#35745;&#31639;&#26426;&#24605;&#32500;&#23545;&#24223;&#29289;&#20943;&#37327;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38646;&#21806;&#34892;&#19994;&#65288;&#20363;&#22914;&#65292;&#26434;&#36135;&#24215;&#12289;&#26381;&#35013;&#24215;&#12289;&#22312;&#32447;&#38646;&#21806;&#21830;&#65289;&#65292;&#24211;&#23384;&#31649;&#29702;&#32773;&#24517;&#39035;&#22312;&#30701;&#26399;&#39118;&#38505;&#65288;&#27809;&#26377;&#38144;&#21806;&#21830;&#21697;&#65289;&#21644;&#38271;&#26399;&#39118;&#38505;&#65288;&#36807;&#37327;&#35746;&#36135;&#23548;&#33268;&#20135;&#21697;&#28010;&#36153;&#65289;&#20043;&#38388;&#36827;&#34892;&#24179;&#34913;&#12290;&#30001;&#20110;&#32570;&#20047;&#26377;&#20851;&#26410;&#26469;&#23458;&#25143;&#36141;&#20080;&#30340;&#20449;&#24687;&#65292;&#36825;&#20010;&#24179;&#34913;&#20219;&#21153;&#23588;&#20854;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20998;&#24067;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22312;&#26102;&#38388;&#33539;&#22260;&#20869;&#37325;&#26032;&#22635;&#20805;&#26131;&#33104;&#29289;&#21697;&#30340;&#26434;&#36135;&#24215;&#24211;&#23384;&#38382;&#39064;&#12290;&#20854;&#30446;&#26631;&#26159;&#22312;&#19981;&#30830;&#23450;&#23454;&#38469;&#28040;&#36153;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#22823;&#21270;&#38144;&#21806;&#21516;&#26102;&#26368;&#23567;&#21270;&#28010;&#36153;&#12290;&#30001;&#20110;&#23545;&#39135;&#29289;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#20197;&#21450;&#39135;&#29289;&#28010;&#36153;&#23545;&#29615;&#22659;&#12289;&#32463;&#27982;&#21644;&#36141;&#20080;&#21147;&#30340;&#24433;&#21709;&#65292;&#36825;&#20010;&#38382;&#39064;&#22312;&#20170;&#22825;&#20855;&#26377;&#26497;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#23558;&#24211;&#23384;&#37325;&#26032;&#22635;&#20805;&#20316;&#20026;&#19968;&#39033;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#33104;&#36133;DQN&#8221;&#30340;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#38382;&#39064;&#30340;&#20998;&#24067;&#24335;&#20844;&#24335;&#26469;&#20272;&#31639;&#27599;&#20010;&#21160;&#20316;&#30340;&#39044;&#26399;&#22238;&#25253;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21512;&#25104;&#30340;&#26434;&#36135;&#24215;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#25991;&#29486;&#20013;&#30340;&#20960;&#20010;&#22522;&#20934;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
In retail (e.g., grocery stores, apparel shops, online retailers), inventory managers have to balance short-term risk (no items to sell) with long-term-risk (over ordering leading to product waste). This balancing task is made especially hard due to the lack of information about future customer purchases. In this paper, we study the problem of restocking a grocery store's inventory with perishable items over time, from a distributional point of view. The objective is to maximize sales while minimizing waste, with uncertainty about the actual consumption by costumers. This problem is of a high relevance today, given the growing demand for food and the impact of food waste on the environment, the economy, and purchasing power. We frame inventory restocking as a new reinforcement learning task that exhibits stochastic behavior conditioned on the agent's actions, making the environment partially observable. We make two main contributions. First, we introduce a new reinforcement learning en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#23545;&#25239;&#24615;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#31561;&#20215;&#30340;&#24191;&#20041;&#20960;&#20309;&#37325;&#24515;&#38382;&#39064;&#21644;&#22810;&#37325;&#36793;&#38469;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#30340;&#37325;&#36848;&#65292;&#25581;&#31034;&#20102;&#20854;&#20016;&#23500;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#25193;&#23637;&#20102;&#20043;&#21069;&#20165;&#38480;&#20110;&#20108;&#20998;&#31867;&#35774;&#32622;&#30340;&#30456;&#20851;&#32467;&#26524;&#12290;&#36890;&#36807;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24674;&#22797;&#21407;&#22987;&#23545;&#25239;&#24615;&#38382;&#39064;&#30340;&#26368;&#20248;&#31283;&#20581;&#20998;&#31867;&#35268;&#21017;&#21644;&#26368;&#20248;&#23545;&#25239;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2204.12676</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#30340;&#22810;&#37325;&#36793;&#38469;&#26368;&#20248;&#36755;&#36816;&#20844;&#24335;
&lt;/p&gt;
&lt;p&gt;
The Multimarginal Optimal Transport Formulation of Adversarial Multiclass Classification. (arXiv:2204.12676v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.12676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#23545;&#25239;&#24615;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#31561;&#20215;&#30340;&#24191;&#20041;&#20960;&#20309;&#37325;&#24515;&#38382;&#39064;&#21644;&#22810;&#37325;&#36793;&#38469;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#30340;&#37325;&#36848;&#65292;&#25581;&#31034;&#20102;&#20854;&#20016;&#23500;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#25193;&#23637;&#20102;&#20043;&#21069;&#20165;&#38480;&#20110;&#20108;&#20998;&#31867;&#35774;&#32622;&#30340;&#30456;&#20851;&#32467;&#26524;&#12290;&#36890;&#36807;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24674;&#22797;&#21407;&#22987;&#23545;&#25239;&#24615;&#38382;&#39064;&#30340;&#26368;&#20248;&#31283;&#20581;&#20998;&#31867;&#35268;&#21017;&#21644;&#26368;&#20248;&#23545;&#25239;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#23545;&#25239;&#24615;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#31561;&#20215;&#37325;&#36848;&#12290;&#20854;&#20013;&#19968;&#31181;&#26159;&#22522;&#20110;&#26412;&#25991;&#24341;&#20837;&#30340;&#19968;&#32452;&#24191;&#20041;&#20960;&#20309;&#37325;&#24515;&#38382;&#39064;&#65307;&#21478;&#19968;&#31181;&#26159;&#20351;&#29992;&#22810;&#37325;&#36793;&#38469;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#20854;&#20013;&#36793;&#38469;&#30340;&#25968;&#37327;&#31561;&#20110;&#21407;&#22987;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#31867;&#25968;&#12290;&#36825;&#20123;&#26032;&#30340;&#29702;&#35770;&#32467;&#26524;&#25581;&#31034;&#20102;&#22810;&#31867;&#20998;&#31867;&#20013;&#23545;&#25239;&#24615;&#23398;&#20064;&#38382;&#39064;&#30340;&#20016;&#23500;&#20960;&#20309;&#32467;&#26500;&#65292;&#24182;&#25193;&#23637;&#20102;&#20043;&#21069;&#20165;&#38480;&#20110;&#20108;&#20998;&#31867;&#35774;&#32622;&#30340;&#30456;&#20851;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#30340;&#19968;&#20010;&#30452;&#25509;&#35745;&#31639;&#21547;&#20041;&#26159;&#65292;&#36890;&#36807;&#35299;&#20915;&#37325;&#24515;&#38382;&#39064;&#21450;&#20854;&#23545;&#20598;&#65292;&#25110;&#32773;&#26159; MOT &#38382;&#39064;&#21450;&#20854;&#23545;&#20598;&#65292;&#25105;&#20204;&#21487;&#20197;&#24674;&#22797;&#21407;&#22987;&#23545;&#25239;&#24615;&#38382;&#39064;&#30340;&#26368;&#20248;&#31283;&#20581;&#20998;&#31867;&#35268;&#21017;&#21644;&#26368;&#20248;&#23545;&#25239;&#31574;&#30053;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#31034;&#20363;&#26469;&#35828;&#26126;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a family of adversarial multiclass classification problems and provide equivalent reformulations in terms of: 1) a family of generalized barycenter problems introduced in the paper and 2) a family of multimarginal optimal transport problems where the number of marginals is equal to the number of classes in the original classification problem. These new theoretical results reveal a rich geometric structure of adversarial learning problems in multiclass classification and extend recent results restricted to the binary classification setting. A direct computational implication of our results is that by solving either the barycenter problem and its dual, or the MOT problem and its dual, we can recover the optimal robust classification rule and the optimal adversarial strategy for the original adversarial problem. Examples with synthetic and real data illustrate our results.
&lt;/p&gt;</description></item><item><title>EVOTER&#20351;&#29992;&#31616;&#21333;&#30340;&#36923;&#36753;&#34920;&#36798;&#24335;&#28436;&#21270;&#20986;&#36879;&#26126;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#38598;&#65292;&#19982;&#40657;&#30418;&#27169;&#22411;&#24615;&#33021;&#30456;&#20284;&#65292;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#24182;&#20026;&#26410;&#26469;&#26500;&#24314;&#21487;&#38752;&#30340;AI&#31995;&#32479;&#25552;&#20379;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2204.10438</link><description>&lt;p&gt;
EVOTER&#65306;&#36879;&#26126;&#21487;&#35299;&#37322;&#35268;&#21017;&#38598;&#30340;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
EVOTER: Evolution of Transparent Explainable Rule-sets. (arXiv:2204.10438v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10438
&lt;/p&gt;
&lt;p&gt;
EVOTER&#20351;&#29992;&#31616;&#21333;&#30340;&#36923;&#36753;&#34920;&#36798;&#24335;&#28436;&#21270;&#20986;&#36879;&#26126;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#38598;&#65292;&#19982;&#40657;&#30418;&#27169;&#22411;&#24615;&#33021;&#30456;&#20284;&#65292;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#24182;&#20026;&#26410;&#26469;&#26500;&#24314;&#21487;&#38752;&#30340;AI&#31995;&#32479;&#25552;&#20379;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;AI&#31995;&#32479;&#26159;&#40657;&#30418;&#23376;&#65292;&#20026;&#32473;&#23450;&#30340;&#36755;&#20837;&#29983;&#25104;&#21512;&#29702;&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#26576;&#20123;&#39046;&#22495;&#20855;&#26377;&#35299;&#37322;&#33021;&#21147;&#21644;&#20449;&#20219;&#24230;&#35201;&#27714;&#65292;&#36825;&#20123;&#35201;&#27714;&#19981;&#33021;&#30452;&#25509;&#28385;&#36275;&#36825;&#20123;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#21363;&#24320;&#22987;&#26102;&#27169;&#22411;&#23601;&#26159;&#36879;&#26126;&#30340;&#21644;&#21487;&#35299;&#37322;&#30340;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#31616;&#21333;&#30340;&#36923;&#36753;&#34920;&#36798;&#24335;&#28436;&#21270;&#20986;&#35268;&#21017;&#38598;&#65292;&#31216;&#20026;EVOTER&#12290;EVOTER&#22312;&#22810;&#20010;&#39044;&#27979;/&#20998;&#31867;&#21644;&#22788;&#26041;/&#25919;&#31574;&#25628;&#32034;&#39046;&#22495;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26377;&#21644;&#27809;&#26377;&#20195;&#29702;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23427;&#33021;&#22815;&#21457;&#29616;&#21644;&#40657;&#30418;&#27169;&#22411;&#30456;&#20284;&#30340;&#26377;&#24847;&#20041;&#30340;&#35268;&#21017;&#38598;&#12290;&#36825;&#20123;&#35268;&#21017;&#21487;&#20197;&#25552;&#20379;&#39046;&#22495;&#30340;&#35265;&#35299;&#65292;&#24182;&#20351;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#20559;&#35265;&#26174;&#24615;&#21270;&#12290;&#20063;&#21487;&#20197;&#30452;&#25509;&#23545;&#23427;&#20204;&#36827;&#34892;&#32534;&#36753;&#65292;&#20197;&#28040;&#38500;&#20559;&#35265;&#24182;&#28155;&#21152;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;EVOTER&#20026;&#26410;&#26469;&#26500;&#24314;&#20540;&#24471;&#20449;&#36182;&#30340;AI&#31995;&#32479;&#30340;&#21487;&#38752;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most AI systems are black boxes generating reasonable outputs for given inputs. Some domains, however, have explainability and trustworthiness requirements that cannot be directly met by these approaches. Various methods have therefore been developed to interpret black-box models after training. This paper advocates an alternative approach where the models are transparent and explainable to begin with. This approach, EVOTER, evolves rule-sets based on simple logical expressions. The approach is evaluated in several prediction/classification and prescription/policy search domains with and without a surrogate. It is shown to discover meaningful rule sets that perform similarly to black-box models. The rules can provide insight into the domain, and make biases hidden in the data explicit. It may also be possible to edit them directly to remove biases and add constraints. EVOTER thus forms a promising foundation for building trustworthy AI systems for real-world applications in the future.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GCGD&#30340;&#26799;&#24230;&#20462;&#27491;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#26799;&#24230;&#36136;&#37327;&#65292;&#20174;&#32780;&#23558;&#35757;&#32451;&#36718;&#25968;&#20943;&#23569;&#32422;20&#65285;&#12290;</title><link>http://arxiv.org/abs/2203.08345</link><description>&lt;p&gt;
&#36229;&#36234;&#26799;&#24230;&#19979;&#38477;&#30340;&#26799;&#24230;&#20462;&#27491;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Gradient Correction beyond Gradient Descent. (arXiv:2203.08345v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.08345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GCGD&#30340;&#26799;&#24230;&#20462;&#27491;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#26799;&#24230;&#36136;&#37327;&#65292;&#20174;&#32780;&#23558;&#35757;&#32451;&#36718;&#25968;&#20943;&#23569;&#32422;20&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#25104;&#21151;&#19982;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#24212;&#29992;&#23494;&#19981;&#21487;&#20998;&#12290;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#65292;&#24050;&#32463;&#20986;&#29616;&#20102;&#35768;&#22810;&#21464;&#31181;&#31639;&#27861;&#26469;&#25913;&#21892;&#26799;&#24230;&#19979;&#38477;&#30340;&#20248;&#21270;&#36807;&#31243;&#12290;&#21453;&#21521;&#20256;&#25773;&#30340;&#26799;&#24230;&#26174;&#28982;&#26159;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#30340;&#19968;&#29615;&#65292;&#28982;&#32780;&#35745;&#31639;&#26799;&#24230;&#30340;&#36136;&#37327;&#21463;&#22810;&#31181;&#22240;&#32032;&#24433;&#21709;&#65292;&#22914;&#22122;&#22768;&#25968;&#25454;&#12289;&#35745;&#31639;&#35823;&#24046;&#12289;&#31639;&#27861;&#38480;&#21046;&#31561;&#12290;&#20026;&#20102;&#25581;&#31034;&#36229;&#36234;&#26799;&#24230;&#19979;&#38477;&#30340;&#26799;&#24230;&#20449;&#24687;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;GCGD&#30340;&#26694;&#26550;&#36827;&#34892;&#26799;&#24230;&#20462;&#27491;&#65292;&#23427;&#30001;&#20004;&#20010;&#25554;&#20214;&#27169;&#22359;&#32452;&#25104;&#65306;1&#65289;&#21463;&#26799;&#24230;&#39044;&#27979;&#24605;&#24819;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GC-W&#30340;&#26435;&#37325;&#26799;&#24230;&#20462;&#27491;&#27169;&#22359;&#65307;2&#65289;&#22522;&#20110;&#31070;&#32463;ODE&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GC-ODE&#30340;&#38544;&#34255;&#29366;&#24577;&#26799;&#24230;&#20462;&#27491;&#27169;&#22359;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26799;&#24230;&#20462;&#27491;&#26694;&#26550;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#26799;&#24230;&#36136;&#37327;&#65292;&#24182;&#23558;&#35757;&#32451;&#36718;&#25968;&#20943;&#23569;&#32422;20&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The great success neural networks have achieved is inseparable from the application of gradient-descent (GD) algorithms. Based on GD, many variant algorithms have emerged to improve the GD optimization process. The gradient for back-propagation is apparently the most crucial aspect for the training of a neural network. The quality of the calculated gradient can be affected by multiple aspects, e.g., noisy data, calculation error, algorithm limitation, and so on. To reveal gradient information beyond gradient descent, we introduce a framework (\textbf{GCGD}) to perform gradient correction. GCGD consists of two plug-in modules: 1) inspired by the idea of gradient prediction, we propose a \textbf{GC-W} module for weight gradient correction; 2) based on Neural ODE, we propose a \textbf{GC-ODE} module for hidden states gradient correction. Experiment results show that our gradient correction framework can effectively improve the gradient quality to reduce training epochs by $\sim$ 20\% and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#36125;&#23572;&#26364;&#26041;&#31243;&#30340;&#19968;&#33324;&#26465;&#20214;&#65292;&#21253;&#25324;&#35299;&#30340;&#23384;&#22312;&#21807;&#19968;&#24615;&#21644;&#22238;&#25253;&#20998;&#24067;&#30340;&#23614;&#37096;&#24615;&#36136;&#12290;&#23558;&#20998;&#24067;&#36125;&#23572;&#26364;&#26041;&#31243;&#19982;&#22810;&#20803;&#20223;&#23556;&#20998;&#24067;&#26041;&#31243;&#32852;&#31995;&#36215;&#26469;&#65292;&#21457;&#29616;&#20219;&#20309;&#20998;&#24067;&#36125;&#23572;&#26364;&#26041;&#31243;&#30340;&#35299;&#37117;&#21487;&#20197;&#20316;&#20026;&#35299;&#30340;&#36793;&#32536;&#24459;&#30340;&#21521;&#37327;&#26469;&#33719;&#24471;&#12290;&#36825;&#19968;&#29702;&#35770;&#36866;&#29992;&#20110;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2202.00081</link><description>&lt;p&gt;
&#35770;&#20998;&#24067;&#36125;&#23572;&#26364;&#26041;&#31243;&#30340;&#35299;
&lt;/p&gt;
&lt;p&gt;
On solutions of the distributional Bellman equation. (arXiv:2202.00081v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#36125;&#23572;&#26364;&#26041;&#31243;&#30340;&#19968;&#33324;&#26465;&#20214;&#65292;&#21253;&#25324;&#35299;&#30340;&#23384;&#22312;&#21807;&#19968;&#24615;&#21644;&#22238;&#25253;&#20998;&#24067;&#30340;&#23614;&#37096;&#24615;&#36136;&#12290;&#23558;&#20998;&#24067;&#36125;&#23572;&#26364;&#26041;&#31243;&#19982;&#22810;&#20803;&#20223;&#23556;&#20998;&#24067;&#26041;&#31243;&#32852;&#31995;&#36215;&#26469;&#65292;&#21457;&#29616;&#20219;&#20309;&#20998;&#24067;&#36125;&#23572;&#26364;&#26041;&#31243;&#30340;&#35299;&#37117;&#21487;&#20197;&#20316;&#20026;&#35299;&#30340;&#36793;&#32536;&#24459;&#30340;&#21521;&#37327;&#26469;&#33719;&#24471;&#12290;&#36825;&#19968;&#29702;&#35770;&#36866;&#29992;&#20110;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#19981;&#20165;&#35201;&#32771;&#34385;&#39044;&#26399;&#22238;&#25253;&#65292;&#36824;&#35201;&#32771;&#34385;&#31574;&#30053;&#30340;&#23436;&#25972;&#22238;&#25253;&#20998;&#24067;&#12290;&#23545;&#20110;&#22266;&#23450;&#30340;&#31574;&#30053;&#65292;&#20854;&#22238;&#25253;&#20998;&#24067;&#26159;&#30456;&#24212;&#20998;&#24067;&#36125;&#23572;&#26364;&#26041;&#31243;&#30340;&#35299;&#12290;&#26412;&#25991;&#32771;&#34385;&#19968;&#33324;&#30340;&#20998;&#24067;&#36125;&#23572;&#26364;&#26041;&#31243;&#65292;&#30740;&#31350;&#35299;&#30340;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#20197;&#21450;&#22238;&#25253;&#20998;&#24067;&#30340;&#23614;&#37096;&#24615;&#36136;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#23384;&#22312;&#21644;&#21807;&#19968;&#24615;&#22238;&#25253;&#20998;&#24067;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#30830;&#23450;&#20102;&#27491;&#21017;&#21464;&#21270;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23558;&#20998;&#24067;&#36125;&#23572;&#26364;&#26041;&#31243;&#19982;&#22810;&#20803;&#20223;&#23556;&#20998;&#24067;&#26041;&#31243;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#22810;&#20803;&#20223;&#23556;&#20998;&#24067;&#26041;&#31243;&#30340;&#35299;&#30340;&#26465;&#20214;&#19979;&#65292;&#20219;&#20309;&#20998;&#24067;&#36125;&#23572;&#26364;&#26041;&#31243;&#30340;&#35299;&#37117;&#21487;&#20197;&#20316;&#20026;&#35299;&#30340;&#36793;&#32536;&#24459;&#30340;&#21521;&#37327;&#26469;&#33719;&#24471;&#12290;&#36825;&#20351;&#24471;&#36825;&#31181;&#26041;&#31243;&#30340;&#19968;&#33324;&#29702;&#35770;&#36866;&#29992;&#20110;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
In distributional reinforcement learning not only expected returns but the complete return distributions of a policy are taken into account. The return distribution for a fixed policy is given as the solution of an associated distributional Bellman equation. In this note we consider general distributional Bellman equations and study existence and uniqueness of their solutions as well as tail properties of return distributions. We give necessary and sufficient conditions for existence and uniqueness of return distributions and identify cases of regular variation. We link distributional Bellman equations to multivariate affine distributional equations. We show that any solution of a distributional Bellman equation can be obtained as the vector of marginal laws of a solution to a multivariate affine distributional equation. This makes the general theory of such equations applicable to the distributional reinforcement learning setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20174;&#20195;&#24065;&#30340;&#26234;&#33021;&#21512;&#32422;&#20195;&#30721;&#20013;&#25552;&#21462;&#29305;&#24449;&#65288;opcode-based&#29305;&#24449;&#65289;&#24182;&#24314;&#31435;&#20998;&#31867;&#22120;&#26469;&#35782;&#21035;DeFi&#39033;&#30446;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#35777;&#21048;&#36829;&#35268;&#27963;&#21160;&#12290;&#26368;&#32456;&#27169;&#22411;&#26159;&#19968;&#20010;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#65292;&#23545;&#22522;&#32447;&#36827;&#34892;&#20102;80&#65285;&#30340;F-1&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2112.02731</link><description>&lt;p&gt;
&#20174;&#20195;&#24065;&#26234;&#33021;&#21512;&#32422;&#20195;&#30721;&#20013;&#26816;&#27979;DeFi&#35777;&#21048;&#36829;&#35268;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Detecting DeFi Securities Violations from Token Smart Contract Code. (arXiv:2112.02731v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.02731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20174;&#20195;&#24065;&#30340;&#26234;&#33021;&#21512;&#32422;&#20195;&#30721;&#20013;&#25552;&#21462;&#29305;&#24449;&#65288;opcode-based&#29305;&#24449;&#65289;&#24182;&#24314;&#31435;&#20998;&#31867;&#22120;&#26469;&#35782;&#21035;DeFi&#39033;&#30446;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#35777;&#21048;&#36829;&#35268;&#27963;&#21160;&#12290;&#26368;&#32456;&#27169;&#22411;&#26159;&#19968;&#20010;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#65292;&#23545;&#22522;&#32447;&#36827;&#34892;&#20102;80&#65285;&#30340;F-1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#20013;&#24515;&#21270;&#37329;&#34701;&#65288;DeFi&#65289;&#26159;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#22312;&#21508;&#31181;&#21306;&#22359;&#38142;&#19978;&#26500;&#24314;&#21644;&#25552;&#20379;&#30340;&#37329;&#34701;&#20135;&#21697;&#21644;&#26381;&#21153;&#12290;&#22312;&#36807;&#21435;&#30340;&#19968;&#24180;&#20013;&#65292;DeFi&#24050;&#32463;&#25104;&#20026;&#20102;&#28909;&#38376;&#35805;&#39064;&#24182;&#33719;&#24471;&#20102;&#24066;&#22330;&#36164;&#26412;&#21270;&#12290;&#28982;&#32780;&#65292;&#23427;&#20063;&#19982;&#29359;&#32618;&#26377;&#20851;&#65292;&#29305;&#21035;&#26159;&#21508;&#31181;&#31867;&#22411;&#30340;&#35777;&#21048;&#36829;&#35268;&#34892;&#20026;&#12290;DeFi&#32570;&#20047;&#20102;&#35299;&#24744;&#30340;&#23458;&#25143;&#35201;&#27714;&#65292;&#36825;&#32473;&#35797;&#22270;&#20943;&#36731;&#27492;&#39046;&#22495;&#28508;&#22312;&#29359;&#32618;&#30340;&#25919;&#24220;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25581;&#31034;&#36825;&#20010;&#38382;&#39064;&#26159;&#21542;&#36866;&#21512;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#26159;&#21542;&#21487;&#20197;&#26681;&#25454;&#20195;&#24065;&#30340;&#26234;&#33021;&#21512;&#32422;&#20195;&#30721;&#35782;&#21035;&#28508;&#22312;&#20174;&#20107;&#35777;&#21048;&#36829;&#35268;&#27963;&#21160;&#30340;DeFi&#39033;&#30446;&#12290;&#25105;&#20204;&#22312;&#20197;&#22826;&#22346;&#19978;&#36866;&#24212;&#20102;&#20197;&#21069;&#26816;&#27979;&#29305;&#23450;&#31867;&#22411;&#35777;&#21048;&#36829;&#35268;&#34892;&#20026;&#30340;&#24037;&#20316;&#65292;&#26681;&#25454;&#20174;DeFi&#39033;&#30446;&#30340;&#20195;&#24065;&#26234;&#33021;&#21512;&#32422;&#20195;&#30721;&#65288;&#29305;&#21035;&#26159;&#22522;&#20110;opcode&#30340;&#29305;&#24449;&#65289;&#25552;&#21462;&#30340;&#29305;&#24449;&#26500;&#24314;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#27169;&#22411;&#26159;&#19968;&#20010;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#65292;&#23545;&#22522;&#32447;&#36827;&#34892;&#20102;80&#65285;&#30340;F-1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized Finance (DeFi) is a system of financial products and services built and delivered through smart contracts on various blockchains. In the past year, DeFi has gained popularity and market capitalization. However, it has also been connected to crime, in particular, various types of securities violations. The lack of Know Your Customer requirements in DeFi poses challenges to governments trying to mitigate potential offending in this space. This study aims to uncover whether this problem is suited to a machine learning approach, namely, whether we can identify DeFi projects potentially engaging in securities violations based on their tokens' smart contract code. We adapt prior work on detecting specific types of securities violations across Ethereum, building classifiers based on features extracted from DeFi projects' tokens' smart contract code (specifically, opcode-based features). Our final model is a random forest model that achieves an 80\% F-1 score against a baseline o
&lt;/p&gt;</description></item><item><title>&#23558;&#19981;&#21516;&#30340;&#27169;&#22411;&#21103;&#26412;&#20998;&#21457;&#32473;&#19981;&#21516;&#30340;&#29992;&#25143;&#65292;&#21487;&#20197;&#38477;&#20302;&#24694;&#24847;&#29992;&#25143;&#23545;&#20854;&#20182;&#29992;&#25143;&#30340;&#25915;&#20987;&#39118;&#38505;&#12290;&#27169;&#22411;&#20351;&#29992;&#19981;&#21516;&#30340;&#38543;&#26426;&#24615;&#35757;&#32451;&#21487;&#20197;&#20943;&#36731;&#22797;&#21046;&#25915;&#20987;&#65292;&#20294;&#26159;&#37325;&#35757;&#32451;&#20195;&#20215;&#39640;&#19988;&#32467;&#26524;&#19981;&#31283;&#23450;&#12290;&#19968;&#20123;&#26041;&#27861;&#25193;&#23637;&#20102;&#37325;&#35757;&#32451;&#20197;&#22686;&#24378;&#27169;&#22411;&#24046;&#24322;&#65292;&#20294;&#26159;&#35745;&#31639;&#25104;&#26412;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2111.15160</link><description>&lt;p&gt;
&#23558;&#19981;&#21516;&#30340;&#27169;&#22411;&#20998;&#21457;&#32473;&#19981;&#21516;&#30340;&#29992;&#25143;&#21487;&#20943;&#36731;&#23545;&#25239;&#24615;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Mitigating Adversarial Attacks by Distributing Different Copies to Different Users. (arXiv:2111.15160v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.15160
&lt;/p&gt;
&lt;p&gt;
&#23558;&#19981;&#21516;&#30340;&#27169;&#22411;&#21103;&#26412;&#20998;&#21457;&#32473;&#19981;&#21516;&#30340;&#29992;&#25143;&#65292;&#21487;&#20197;&#38477;&#20302;&#24694;&#24847;&#29992;&#25143;&#23545;&#20854;&#20182;&#29992;&#25143;&#30340;&#25915;&#20987;&#39118;&#38505;&#12290;&#27169;&#22411;&#20351;&#29992;&#19981;&#21516;&#30340;&#38543;&#26426;&#24615;&#35757;&#32451;&#21487;&#20197;&#20943;&#36731;&#22797;&#21046;&#25915;&#20987;&#65292;&#20294;&#26159;&#37325;&#35757;&#32451;&#20195;&#20215;&#39640;&#19988;&#32467;&#26524;&#19981;&#31283;&#23450;&#12290;&#19968;&#20123;&#26041;&#27861;&#25193;&#23637;&#20102;&#37325;&#35757;&#32451;&#20197;&#22686;&#24378;&#27169;&#22411;&#24046;&#24322;&#65292;&#20294;&#26159;&#35745;&#31639;&#25104;&#26412;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#26412;&#25991;&#32771;&#34385;&#23558;&#27169;&#22411;&#20998;&#21457;&#32473;&#22810;&#20010;&#29992;&#25143;&#30340;&#24773;&#20917;&#65292;&#22312;&#36825;&#20854;&#20013;&#65292;&#24694;&#24847;&#29992;&#25143;&#35797;&#22270;&#25915;&#20987;&#20854;&#20182;&#29992;&#25143;&#12290;&#24694;&#24847;&#29992;&#25143;&#20250;&#25506;&#27979;&#20854;&#25317;&#26377;&#30340;&#27169;&#22411;&#20197;&#23547;&#25214;&#23545;&#25239;&#26679;&#26412;&#65292;&#28982;&#21518;&#23558;&#21457;&#29616;&#30340;&#26679;&#26412;&#21576;&#29616;&#32473;&#21463;&#23475;&#32773;&#27169;&#22411;&#30340;&#21103;&#26412;&#20197;&#22797;&#21046;&#25915;&#20987;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#27169;&#22411;&#20998;&#21457;&#32473;&#19981;&#21516;&#30340;&#20080;&#23478;&#65292;&#25105;&#20204;&#21487;&#20197;&#20943;&#36731;&#25915;&#20987;&#65292;&#36825;&#26679;&#22312;&#19968;&#20010;&#27169;&#22411;&#19978;&#25214;&#21040;&#30340;&#23545;&#25239;&#26679;&#26412;&#23558;&#19981;&#36215;&#20316;&#29992;&#20110;&#21478;&#19968;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#38543;&#26426;&#24615;&#35757;&#32451;&#27169;&#22411;&#30830;&#23454;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20943;&#36731;&#20102;&#36825;&#31181;&#22797;&#21046;&#12290;&#28982;&#32780;&#65292;&#24182;&#27809;&#26377;&#20445;&#35777;&#65292;&#37325;&#26032;&#35757;&#32451;&#30340;&#25104;&#26412;&#20063;&#24456;&#39640;&#12290;&#19968;&#20123;&#20316;&#21697;&#25193;&#23637;&#20102;&#37325;&#26032;&#22521;&#35757;&#26041;&#27861;&#20197;&#22686;&#24378;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21482;&#33021;&#20135;&#29983;&#38750;&#24120;&#26377;&#38480;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#35745;&#31639;&#25104;&#26412;&#21464;&#24471;&#26356;&#39640;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are vulnerable to adversarial attacks. In this paper, we consider the scenario where a model is distributed to multiple buyers, among which a malicious buyer attempts to attack another buyer. The malicious buyer probes its copy of the model to search for adversarial samples and then presents the found samples to the victim's copy of the model in order to replicate the attack. We point out that by distributing different copies of the model to different buyers, we can mitigate the attack such that adversarial samples found on one copy would not work on another copy. We observed that training a model with different randomness indeed mitigates such replication to a certain degree. However, there is no guarantee and retraining is computationally expensive. A number of works extended the retraining method to enhance the differences among models. However, a very limited number of models can be produced using such methods and the computational cost becomes even higher. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25913;&#36827;&#20102;&#24120;&#35265;&#30340;&#24230;&#37327;&#26631;&#20934;H-score&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25910;&#32553;&#20272;&#35745;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;H&#20998;&#25968;&#30340;&#30456;&#20851;&#24615;&#24615;&#33021;&#33719;&#24471;&#39640;&#36798;80%&#30340;&#32477;&#23545;&#22686;&#30410;&#65292;&#20351;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;LogME&#24230;&#37327;&#26631;&#20934;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#21516;&#26102;&#65292;&#38024;&#23545;&#30446;&#26631;&#20219;&#21153;&#30340;&#36873;&#25321;&#65292;&#26412;&#25991;&#20063;&#21457;&#29616;&#20102;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2110.06893</link><description>&lt;p&gt;
&#26032;&#19981;&#19968;&#23450;&#24635;&#26159;&#26356;&#22909;&#65306;&#37325;&#26032;&#24605;&#32771;&#36801;&#31227;&#24230;&#37327;&#65292;&#23427;&#20204;&#30340;&#29305;&#27530;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Newer is not always better: Rethinking transferability metrics, their peculiarities, stability and performance. (arXiv:2110.06893v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.06893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25913;&#36827;&#20102;&#24120;&#35265;&#30340;&#24230;&#37327;&#26631;&#20934;H-score&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25910;&#32553;&#20272;&#35745;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;H&#20998;&#25968;&#30340;&#30456;&#20851;&#24615;&#24615;&#33021;&#33719;&#24471;&#39640;&#36798;80%&#30340;&#32477;&#23545;&#22686;&#30410;&#65292;&#20351;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;LogME&#24230;&#37327;&#26631;&#20934;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#21516;&#26102;&#65292;&#38024;&#23545;&#30446;&#26631;&#20219;&#21153;&#30340;&#36873;&#25321;&#65292;&#26412;&#25991;&#20063;&#21457;&#29616;&#20102;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#25913;&#21892;&#39044;&#27979;&#21644;&#39640;&#25928;&#21033;&#29992;&#26377;&#38480;&#36164;&#28304;&#65292;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#22270;&#20687;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#23567;&#22411;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#24050;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#24494;&#35843;&#38656;&#35201;&#30830;&#23450;&#26368;&#20339;&#27169;&#22411;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#37327;&#21270;&#21487;&#36716;&#31227;&#24615;&#20197;&#36991;&#20813;&#22312;&#25152;&#26377;&#20505;&#36873;&#27169;&#22411;/&#20219;&#21153;&#23545;&#19978;&#36827;&#34892;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21327;&#26041;&#24046;&#20272;&#35745;&#30340;&#32479;&#35745;&#38382;&#39064;&#23548;&#33268;&#20102;H&#20998;&#25968; (H-score) &#30340;&#24615;&#33021;&#19981;&#20339;&#8212;&#8212;&#35813;&#20998;&#25968;&#26159;&#26032;&#22411;&#24230;&#37327;&#26631;&#20934;&#30340;&#24120;&#35265;&#22522;&#20934;&#8212;&#8212;&#24182;&#25552;&#20986;&#22522;&#20110;&#25910;&#32553;&#20272;&#35745;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#26679;&#21487;&#20197;&#20351;H&#20998;&#25968;&#30340;&#30456;&#20851;&#24615;&#24615;&#33021;&#33719;&#24471;&#39640;&#36798;80%&#30340;&#32477;&#23545;&#22686;&#30410;&#65292;&#20351;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;LogME&#24230;&#37327;&#26631;&#20934;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;&#25910;&#32553;&#20272;&#35745;&#30340;H&#20998;&#25968;&#27604;LogME&#26356;&#24555;3&#20493;&#21040;10&#20493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#30446;&#26631;&#65288;&#32780;&#19981;&#26159;&#28304;&#65289;&#20219;&#21153;&#36873;&#25321;&#30340;&#36739;&#23569;&#24120;&#35265;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#25968;&#37327;&#12289;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#65292;&#27492;&#31867;&#35774;&#32622;&#20013;&#20197;&#21069;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning of large pre-trained image and language models on small customized datasets has become increasingly popular for improved prediction and efficient use of limited resources. Fine-tuning requires identification of best models to transfer-learn from and quantifying transferability prevents expensive re-training on all of the candidate models/tasks pairs. In this paper, we show that the statistical problems with covariance estimation drive the poor performance of H-score -- a common baseline for newer metrics -- and propose shrinkage-based estimator. This results in up to 80% absolute gain in H-score correlation performance, making it competitive with the state-of-the-art LogME measure. Our shrinkage-based H-score is $3\times$-10$\times$ faster to compute compared to LogME. Additionally, we look into a less common setting of target (as opposed to source) task selection. We demonstrate previously overlooked problems in such settings with different number of labels, class-imbalanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#38750;&#21442;&#25968;&#21152;&#24615;&#27169;&#22411;&#65292;&#20351;&#29992;&#23569;&#37327;&#20027;&#35201;&#21644;&#25104;&#23545;&#20132;&#20114;&#25928;&#24212;&#39044;&#27979;&#35843;&#26597;&#21453;&#24212;&#29575;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#26131;&#20110;&#21487;&#35270;&#21270;&#21644;&#35299;&#37322;&#30340;&#39044;&#27979;&#38754;&#65292;&#24182;&#21462;&#24471;&#20102; ROAM &#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#21487;&#20197;&#25552;&#20379;&#25913;&#36827;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#21644;&#20854;&#20182;&#35843;&#26597;&#30340;&#21453;&#24212;&#29575;&#35758;&#35770;&#12290;</title><link>http://arxiv.org/abs/2108.11328</link><description>&lt;p&gt;
&#29992;&#31616;&#27905;&#21487;&#35299;&#37322;&#30340;&#21152;&#24615;&#27169;&#22411;&#21644;&#32467;&#26500;&#20132;&#20114;&#39044;&#27979;&#20154;&#21475;&#26222;&#26597;&#35843;&#26597;&#21453;&#24212;&#29575;
&lt;/p&gt;
&lt;p&gt;
Predicting Census Survey Response Rates With Parsimonious Additive Models and Structured Interactions. (arXiv:2108.11328v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.11328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#38750;&#21442;&#25968;&#21152;&#24615;&#27169;&#22411;&#65292;&#20351;&#29992;&#23569;&#37327;&#20027;&#35201;&#21644;&#25104;&#23545;&#20132;&#20114;&#25928;&#24212;&#39044;&#27979;&#35843;&#26597;&#21453;&#24212;&#29575;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#26131;&#20110;&#21487;&#35270;&#21270;&#21644;&#35299;&#37322;&#30340;&#39044;&#27979;&#38754;&#65292;&#24182;&#21462;&#24471;&#20102; ROAM &#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#21487;&#20197;&#25552;&#20379;&#25913;&#36827;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#21644;&#20854;&#20182;&#35843;&#26597;&#30340;&#21453;&#24212;&#29575;&#35758;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20351;&#29992;&#19968;&#31995;&#21015;&#28789;&#27963;&#19988;&#21487;&#35299;&#37322;&#30340;&#38750;&#21442;&#25968;&#27169;&#22411;&#39044;&#27979;&#35843;&#26597;&#21453;&#24212;&#29575;&#12290;&#26412;&#30740;&#31350;&#21463;&#21040;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#33879;&#21517;&#30340; ROAM &#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#35813;&#24212;&#29992;&#20351;&#29992;&#22312;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#35268;&#21010;&#25968;&#25454;&#24211;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#26469;&#35782;&#21035;&#38590;&#20197;&#35843;&#26597;&#30340;&#21306;&#22495;&#12290;&#21313;&#24180;&#21069;&#32452;&#32455;&#30340;&#19968;&#22330;&#20247;&#21253;&#31454;&#36187;&#34920;&#26126;&#65292;&#22522;&#20110;&#22238;&#24402;&#26641;&#38598;&#25104;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#35843;&#26597;&#21453;&#24212;&#29575;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#40657;&#30418;&#29305;&#24615;&#65292;&#30456;&#24212;&#30340;&#27169;&#22411;&#19981;&#33021;&#29992;&#20110;&#25311;&#23450;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992; $\ell_0$-based &#24809;&#32602;&#30340;&#38750;&#21442;&#25968;&#21152;&#24615;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#23569;&#25968;&#20027;&#35201;&#21644;&#25104;&#23545;&#20132;&#20114;&#25928;&#24212;&#12290;&#20174;&#26041;&#27861;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#20272;&#35745;&#22120;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#26041;&#38754;&#65292;&#24182;&#35752;&#35770;&#20102;&#23558;&#24378;&#23618;&#27425;&#20132;&#20114;&#21512;&#24182;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#65288;&#22312;Github &#19978;&#24320;&#28304;&#65289;&#20801;&#35768;&#25105;&#20204;&#29983;&#25104;&#26131;&#20110;&#21487;&#35270;&#21270;&#21644;&#35299;&#37322;&#30340;&#39044;&#27979;&#38754;&#65292;&#20174;&#32780;&#33719;&#24471;&#26377;&#20851;&#35843;&#26597;&#21453;&#24212;&#29575;&#30340;&#21487;&#34892;&#35265;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312; ROAM &#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#21644;&#20854;&#20182;&#35843;&#26597;&#30340;&#25913;&#36827;&#35843;&#26597;&#21453;&#24212;&#29575;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we consider the problem of predicting survey response rates using a family of flexible and interpretable nonparametric models. The study is motivated by the US Census Bureau's well-known ROAM application which uses a linear regression model trained on the US Census Planning Database data to identify hard-to-survey areas. A crowdsourcing competition organized around ten years ago revealed that machine learning methods based on ensembles of regression trees led to the best performance in predicting survey response rates; however, the corresponding models could not be adopted for the intended application due to their black-box nature. We consider nonparametric additive models with small number of main and pairwise interaction effects using $\ell_0$-based penalization. From a methodological viewpoint, we study both computational and statistical aspects of our estimator; and discuss variants that incorporate strong hierarchical interactions. Our algorithms (opensourced on gith
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#24341;&#23548;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#39640;&#24230;&#38750;&#32447;&#24615;&#21442;&#25968;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2106.01078</link><description>&lt;p&gt;
&#29289;&#29702;&#24341;&#23548;&#19979;&#39640;&#24230;&#38750;&#32447;&#24615;&#21442;&#25968;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Physics-Guided Discovery of Highly Nonlinear Parametric Partial Differential Equations. (arXiv:2106.01078v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.01078
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#24341;&#23548;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#39640;&#24230;&#38750;&#32447;&#24615;&#21442;&#25968;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#25311;&#21512;&#31185;&#23398;&#25968;&#25454;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#21487;&#20197;&#20195;&#34920;&#22810;&#31181;&#20197;&#25968;&#23398;&#20026;&#23548;&#21521;&#30340;&#23398;&#31185;&#65292;&#20363;&#22914;&#29289;&#29702;&#21644;&#37329;&#34701;&#30340;&#21487;&#35299;&#37322;&#26426;&#21046;&#30340;&#29289;&#29702;&#23450;&#24459;&#12290;&#20174;&#31185;&#23398;&#25968;&#25454;&#20013;&#25968;&#25454;&#39537;&#21160;&#22320;&#21457;&#29616;&#20559;&#24494;&#20998;&#26041;&#31243;&#26159;&#19968;&#31181;&#27169;&#25311;&#33258;&#28982;&#30028;&#20013;&#22797;&#26434;&#29616;&#35937;&#30340;&#26032;&#23581;&#35797;&#65292;&#20294;&#30446;&#21069;&#30340;&#23454;&#36341;&#25928;&#26524;&#36890;&#24120;&#21463;&#21040;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#29616;&#35937;&#22797;&#26434;&#24615;&#30340;&#38480;&#21046;&#12290;&#29305;&#21035;&#22320;&#65292;&#20174;&#20302;&#36136;&#37327;&#25968;&#25454;&#20013;&#21457;&#29616;&#39640;&#24230;&#38750;&#32447;&#24615;&#31995;&#25968;&#30340;PDE&#20173;&#28982;&#38754;&#20020;&#30528;&#30456;&#23545;&#19981;&#36275;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38590;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#24341;&#23548;&#23398;&#20064;&#26041;&#27861;&#65292;&#26082;&#21487;&#20197;&#32534;&#30721;&#21021;&#20540;&#21644;&#36793;&#30028;&#26465;&#20214;&#31561;&#35266;&#23519;&#30693;&#35782;&#65292;&#20063;&#21487;&#20197;&#32435;&#20837;&#22522;&#26412;&#30340;&#29289;&#29702;&#21407;&#29702;&#21644;&#23450;&#24459;&#26469;&#25351;&#23548;&#27169;&#22411;&#20248;&#21270;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20005;&#26684;&#20943;&#23567;&#20102;&#29616;&#26377;&#22522;&#32447;&#31995;&#25968;&#20272;&#35745;&#35823;&#24046;&#65292;&#24182;&#19988;&#36824;&#25239;&#22122;&#38899;&#40065;&#26834;&#24615;&#39640;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#24230;&#25968;&#31995;&#25968;&#19979;&#30340;&#20934;&#30830;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial differential equations (PDEs) that fit scientific data can represent physical laws with explainable mechanisms for various mathematically-oriented subjects, such as physics and finance. The data-driven discovery of PDEs from scientific data thrives as a new attempt to model complex phenomena in nature, but the effectiveness of current practice is typically limited by the scarcity of data and the complexity of phenomena. Especially, the discovery of PDEs with highly nonlinear coefficients from low-quality data remains largely under-addressed. To deal with this challenge, we propose a novel physics-guided learning method, which can not only encode observation knowledge such as initial and boundary conditions but also incorporate the basic physical principles and laws to guide the model optimization. We theoretically show that our proposed method strictly reduces the coefficient estimation error of existing baselines, and is also robust against noise. Extensive experiments show th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27963;&#21160;&#35782;&#21035;&#26694;&#26550;&#65292;&#21487;&#20197;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#26469;&#35299;&#37322;&#20998;&#31867;&#30340;&#21407;&#22240;&#65292;&#20197;&#25552;&#39640;&#26234;&#33021;&#23478;&#23621;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2105.09787</link><description>&lt;p&gt;
&#26234;&#33021;&#23478;&#23621;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Explainable Activity Recognition for Smart Home Systems. (arXiv:2105.09787v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.09787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27963;&#21160;&#35782;&#21035;&#26694;&#26550;&#65292;&#21487;&#20197;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#26469;&#35299;&#37322;&#20998;&#31867;&#30340;&#21407;&#22240;&#65292;&#20197;&#25552;&#39640;&#26234;&#33021;&#23478;&#23621;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#23478;&#23621;&#29615;&#22659;&#26088;&#22312;&#36890;&#36807;&#23433;&#35013;&#22312;&#25972;&#20010;&#31354;&#38388;&#20013;&#30340;&#21508;&#31181;&#20256;&#24863;&#22120;&#21644;&#25191;&#34892;&#22120;&#25552;&#20379;&#24110;&#21161;&#25913;&#21892;&#23621;&#27665;&#29983;&#27963;&#36136;&#37327;&#30340;&#26381;&#21153;&#12290;&#26234;&#33021;&#23478;&#23621;&#37319;&#21462;&#30340;&#35768;&#22810;&#33258;&#21160;&#21270;&#25805;&#20316;&#37117;&#26159;&#30001;&#22522;&#30784;&#27963;&#21160;&#35782;&#21035;&#31995;&#32479;&#30340;&#36755;&#20986;&#25511;&#21046;&#30340;&#12290;&#28982;&#32780;&#65292;&#27963;&#21160;&#35782;&#21035;&#31995;&#32479;&#21487;&#33021;&#24182;&#19981;&#23436;&#20840;&#20934;&#30830;&#65292;&#22240;&#27492;&#26234;&#33021;&#23478;&#23621;&#25805;&#20316;&#30340;&#19981;&#19968;&#33268;&#24615;&#21487;&#33021;&#20250;&#23548;&#33268;&#20381;&#36182;&#26234;&#33021;&#23478;&#23621;&#39044;&#27979;&#30340;&#29992;&#25143;&#24819;&#30693;&#36947;&#8220;&#20026;&#20160;&#20040;&#26234;&#33021;&#23478;&#23621;&#35201;&#37027;&#26679;&#20570;&#65311;&#8221; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#30340;&#35265;&#35299;&#22522;&#30784;&#19978;&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#27963;&#21160;&#35782;&#21035;&#26694;&#26550;&#65292;&#21033;&#29992;&#39046;&#20808;&#30340;XAI&#26041;&#27861;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#35299;&#37322;&#27963;&#21160;&#20013;&#20160;&#20040;&#23548;&#33268;&#20102;&#32473;&#23450;&#30340;&#20998;&#31867;&#12290;&#22312;&#36828;&#31243;&#29031;&#25252;&#30417;&#27979;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#27493;&#35780;&#20272;&#65306;&#65288;a&#65289;&#21033;&#29992;ML&#19987;&#23478;&#35780;&#20272;&#35299;&#37322;&#30340;&#21512;&#29702;&#24615;&#65292;&#65288;b&#65289;&#25307;&#21215;&#38750;&#19987;&#19994;&#20154;&#21592;
&lt;/p&gt;
&lt;p&gt;
Smart home environments are designed to provide services that help improve the quality of life for the occupant via a variety of sensors and actuators installed throughout the space. Many automated actions taken by a smart home are governed by the output of an underlying activity recognition system. However, activity recognition systems may not be perfectly accurate and therefore inconsistencies in smart home operations can lead users reliant on smart home predictions to wonder "why did the smart home do that?" In this work, we build on insights from Explainable Artificial Intelligence (XAI) techniques and introduce an explainable activity recognition framework in which we leverage leading XAI methods to generate natural language explanations that explain what about an activity led to the given classification. Within the context of remote caregiver monitoring, we perform a two-step evaluation: (a) utilize ML experts to assess the sensibility of explanations, and (b) recruit non-experts
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#27880;&#24847;&#21147;&#27531;&#24046;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#38598;&#25104;&#26041;&#27861;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#26080;&#38656;&#20551;&#35774;&#25991;&#26723;&#25110;&#35770;&#25454;&#32467;&#26500;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#22810;&#20010;&#35770;&#36848;&#25366;&#25496;&#20219;&#21153;&#20013;&#65292;&#25104;&#20026;&#20102;&#19968;&#31181;&#26082;&#36890;&#29992;&#21448;&#39640;&#24615;&#33021;&#30340;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2102.12227</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#27880;&#24847;&#21147;&#27531;&#24046;&#32593;&#32476;&#29992;&#20110;&#35770;&#36848;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Attentive Residual Networks for Argument Mining. (arXiv:2102.12227v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.12227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#27880;&#24847;&#21147;&#27531;&#24046;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#38598;&#25104;&#26041;&#27861;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#26080;&#38656;&#20551;&#35774;&#25991;&#26723;&#25110;&#35770;&#25454;&#32467;&#26500;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#22810;&#20010;&#35770;&#36848;&#25366;&#25496;&#20219;&#21153;&#20013;&#65292;&#25104;&#20026;&#20102;&#19968;&#31181;&#26082;&#36890;&#29992;&#21448;&#39640;&#24615;&#33021;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22810;&#20219;&#21153;&#27880;&#24847;&#21147;&#27531;&#24046;&#32593;&#32476;&#22312;&#22810;&#20010;&#35770;&#36848;&#25366;&#25496;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27531;&#24046;&#26550;&#26500;&#65292;&#21033;&#29992;&#20102;&#27880;&#24847;&#21147;&#12289;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#26041;&#27861;&#65292;&#19981;&#23545;&#25991;&#26723;&#25110;&#35770;&#25454;&#32467;&#26500;&#20570;&#20219;&#20309;&#20551;&#35774;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#19981;&#21516;&#30340;&#29992;&#25143;&#29983;&#25104;&#35780;&#35770;&#12289;&#31185;&#23398;&#20986;&#29256;&#29289;&#21644;&#21149;&#35828;&#24615;&#35770;&#25991;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#38024;&#23545;&#20855;&#26377;&#26356;&#39640;&#35745;&#31639;&#21360;&#35760;&#25110;&#29305;&#23450;&#20110;&#35821;&#26009;&#24211;&#35774;&#35745;&#30340;&#26368;&#20808;&#36827;&#26550;&#26500;&#30340;&#24378;&#26377;&#21147;&#30340;&#31454;&#20105;&#23545;&#25163;&#65292;&#20195;&#34920;&#20102;&#36890;&#29992;&#24615;&#12289;&#24615;&#33021;&#31934;&#24230;&#21644;&#20943;&#23569;&#27169;&#22411;&#22823;&#23567;&#20043;&#38388;&#30340;&#26377;&#36259;&#25240;&#34935;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the use of residual networks and neural attention for multiple argument mining tasks. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble, without any assumption on document or argument structure. We present an extensive experimental evaluation on five different corpora of user-generated comments, scientific publications, and persuasive essays. Our results show that our approach is a strong competitor against state-of-the-art architectures with a higher computational footprint or corpus-specific design, representing an interesting compromise between generality, performance accuracy and reduced model size.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#21644;&#39640;&#26031;&#36807;&#31243;&#25216;&#26415;&#35299;&#37322;&#20102;&#31070;&#32463;&#32593;&#32476;&#21452;&#23792;&#19979;&#38477;&#29616;&#35937;&#65292;&#24314;&#31435;&#20102;NNGP&#21644;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#20043;&#38388;&#30340;&#26032;&#32852;&#31995;&#65292;&#25581;&#31034;&#35813;&#29616;&#35937;&#21463;&#21040;&#32463;&#39564;&#26680;&#21644;NNGP&#26680;&#20043;&#38388;&#24046;&#24322;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2102.07238</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#35270;&#35282;&#19979;&#31070;&#32463;&#32593;&#32476;&#30340;&#21452;&#23792;&#19979;&#38477;&#26354;&#32447;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Double-descent curves in neural networks: a new perspective using Gaussian processes. (arXiv:2102.07238v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.07238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#21644;&#39640;&#26031;&#36807;&#31243;&#25216;&#26415;&#35299;&#37322;&#20102;&#31070;&#32463;&#32593;&#32476;&#21452;&#23792;&#19979;&#38477;&#29616;&#35937;&#65292;&#24314;&#31435;&#20102;NNGP&#21644;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#20043;&#38388;&#30340;&#26032;&#32852;&#31995;&#65292;&#25581;&#31034;&#35813;&#29616;&#35937;&#21463;&#21040;&#32463;&#39564;&#26680;&#21644;NNGP&#26680;&#20043;&#38388;&#24046;&#24322;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21452;&#23792;&#19979;&#38477;&#26354;&#32447;&#29616;&#35937;&#25551;&#36848;&#20102;&#24403;&#22686;&#21152;&#21442;&#25968;&#26102;&#65292;&#27867;&#21270;&#35823;&#24046;&#36215;&#21021;&#19979;&#38477;&#65292;&#20294;&#22312;&#36798;&#21040;&#19968;&#20010;&#23567;&#20110;&#25968;&#25454;&#28857;&#25968;&#37327;&#30340;&#26368;&#20248;&#21442;&#25968;&#21518;&#22686;&#21152;&#65292;&#28982;&#21518;&#22312;&#36807;&#21442;&#25968;&#21270;&#21306;&#38388;&#20877;&#27425;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#25216;&#26415;&#26469;&#34920;&#24449;&#32463;&#39564;&#29305;&#24449;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#35889;&#20998;&#24067;&#65292;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#65288;NNGP&#65289;&#26680;&#35889;&#30340;&#23485;&#24230;&#30456;&#20851;&#25200;&#21160;&#65292;&#20174;&#32780;&#22312;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#24314;&#31435;&#20102;NNGP&#25991;&#29486;&#19982;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#25991;&#29486;&#20043;&#38388;&#30340;&#26032;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#36798;&#24335;&#20801;&#35768;&#25105;&#20204;&#30740;&#31350;&#30456;&#24212;&#26680;&#21644;GP&#22238;&#24402;&#30340;&#27867;&#21270;&#34892;&#20026;&#65292;&#24182;&#20026;&#21452;&#23792;&#19979;&#38477;&#30340;&#29616;&#35937;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35299;&#37322;&#65292;&#21363;&#30001;&#23485;&#24230;&#30456;&#20851;&#30340;&#32463;&#39564;&#26680;&#19982;&#23485;&#24230;&#26080;&#20851;&#30340;NNGP&#26680;&#20043;&#38388;&#30340;&#24046;&#24322;&#25152;&#20915;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Double-descent curves in neural networks describe the phenomenon that the generalisation error initially descends with increasing parameters, then grows after reaching an optimal number of parameters which is less than the number of data points, but then descends again in the overparameterized regime. In this paper, we use techniques from random matrix theory to characterize the spectral distribution of the empirical feature covariance matrix as a width-dependent perturbation of the spectrum of the neural network Gaussian process (NNGP) kernel, thus establishing a novel connection between the NNGP literature and the random matrix theory literature in the context of neural networks. Our analytical expression allows us to study the generalisation behavior of the corresponding kernel and GP regression, and provides a new interpretation of the double-descent phenomenon, namely as governed by the discrepancy between the width-dependent empirical kernel and the width-independent NNGP kernel.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#24418;&#29366;&#38598;&#21512;&#20013;&#36890;&#36807;&#20132;&#21449;&#24418;&#29366;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#23454;&#29616;&#19977;&#32500;&#28857;&#20113;&#20998;&#21106;&#65292;&#36890;&#36807;&#35780;&#20272;&#28857;&#20043;&#38388;&#30340;&#20132;&#20114;&#31243;&#24230;&#21644;&#20171;&#23548;&#29305;&#24449;&#20256;&#25773;&#26469;&#25552;&#21319;&#32467;&#26524;&#31934;&#24230;&#21644;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2003.09053</link><description>&lt;p&gt;
&#19977;&#32500;&#28857;&#20113;&#20998;&#21106;&#30340;&#20132;&#21449;&#24418;&#29366;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Cross-Shape Attention for Part Segmentation of 3D Point Clouds. (arXiv:2003.09053v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2003.09053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#24418;&#29366;&#38598;&#21512;&#20013;&#36890;&#36807;&#20132;&#21449;&#24418;&#29366;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#23454;&#29616;&#19977;&#32500;&#28857;&#20113;&#20998;&#21106;&#65292;&#36890;&#36807;&#35780;&#20272;&#28857;&#20043;&#38388;&#30340;&#20132;&#20114;&#31243;&#24230;&#21644;&#20171;&#23548;&#29305;&#24449;&#20256;&#25773;&#26469;&#25552;&#21319;&#32467;&#26524;&#31934;&#24230;&#21644;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24418;&#29366;&#38598;&#21512;&#20013;&#20256;&#36882;&#36880;&#28857;&#29305;&#24449;&#34920;&#31034;&#26469;&#36827;&#34892;&#19977;&#32500;&#24418;&#29366;&#20998;&#21106;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#21449;&#24418;&#29366;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#20351;&#19968;&#20010;&#24418;&#29366;&#30340;&#36880;&#28857;&#29305;&#24449;&#19982;&#20854;&#20182;&#24418;&#29366;&#30340;&#36880;&#28857;&#29305;&#24449;&#20135;&#29983;&#30456;&#20114;&#20316;&#29992;&#12290;&#35813;&#26426;&#21046;&#35780;&#20272;&#28857;&#20043;&#38388;&#30340;&#20132;&#20114;&#31243;&#24230;&#24182;&#22312;&#24418;&#29366;&#20043;&#38388;&#20171;&#23548;&#29305;&#24449;&#20256;&#25773;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#29992;&#20110;&#24418;&#29366;&#20998;&#21106;&#30340;&#32467;&#26524;&#30340;&#28857;&#36880;&#28857;&#29305;&#24449;&#34920;&#31034;&#30340;&#31934;&#24230;&#21644;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24418;&#29366;&#26816;&#32034;&#24230;&#37327;&#65292;&#20197;&#36873;&#25321;&#36866;&#21512;&#27599;&#20010;&#27979;&#35797;&#24418;&#29366;&#30340;&#20132;&#21449;&#24418;&#29366;&#27880;&#24847;&#21147;&#25805;&#20316;&#30340;&#24418;&#29366;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27969;&#34892;&#30340;PartNet&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a deep learning method that propagates point-wise feature representations across shapes within a collection for the purpose of 3D shape segmentation. We propose a cross-shape attention mechanism to enable interactions between a shape's point-wise features and those of other shapes. The mechanism assesses both the degree of interaction between points and also mediates feature propagation across shapes, improving the accuracy and consistency of the resulting point-wise feature representations for shape segmentation. Our method also proposes a shape retrieval measure to select suitable shapes for cross-shape attention operations for each test shape. Our experiments demonstrate that our approach yields state-of-the-art results in the popular PartNet dataset.
&lt;/p&gt;</description></item></channel></rss>