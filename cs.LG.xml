<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Moreau&#21253;&#32476;&#20803;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;MEMRL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;Moreau&#21253;&#32476;&#20195;&#29702;&#27491;&#21017;&#21270;&#22120;&#65292;&#21487;&#20197;&#23398;&#20064;&#19968;&#20010;&#21487;&#20197;&#36866;&#24212;&#20219;&#21153;&#20998;&#24067;&#30340;&#20803;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.12216</link><description>&lt;p&gt;
&#20851;&#20110;Moreau&#21253;&#32476;&#30340;&#19968;&#38454;&#20803;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
On First-Order Meta-Reinforcement Learning with Moreau Envelopes. (arXiv:2305.12216v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Moreau&#21253;&#32476;&#20803;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;MEMRL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;Moreau&#21253;&#32476;&#20195;&#29702;&#27491;&#21017;&#21270;&#22120;&#65292;&#21487;&#20197;&#23398;&#20064;&#19968;&#20010;&#21487;&#20197;&#36866;&#24212;&#20219;&#21153;&#20998;&#24067;&#30340;&#20803;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#24378;&#21270;&#23398;&#20064;(MRL)&#26159;&#19968;&#31181;&#35757;&#32451;&#26234;&#33021;&#20307;&#22312;&#26032;&#29615;&#22659;&#21644;&#20219;&#21153;&#20013;&#24555;&#36895;&#36866;&#24212;&#30340;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;MRL&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Moreau&#21253;&#32476;&#20195;&#29702;&#27491;&#21017;&#21270;&#22120;&#26469;&#20849;&#21516;&#23398;&#20064;&#21487;&#20197;&#36866;&#24212;&#27599;&#20010;&#20219;&#21153;&#29615;&#22659;&#30340;&#20803;&#31574;&#30053;&#30340;&#26032;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#31216;&#20026;Moreau&#21253;&#32476;&#20803;&#24378;&#21270;&#23398;&#20064;(MEMRL)&#65292;&#23427;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#21644;Moreau&#21253;&#32476;&#27491;&#21017;&#21270;&#30340;&#32452;&#21512;&#26377;&#25928;&#22320;&#26356;&#26032;&#31574;&#30053;&#21442;&#25968;&#65292;&#23398;&#20064;&#21487;&#20197;&#36866;&#24212;&#20219;&#21153;&#20998;&#24067;&#30340;&#20803;&#31574;&#30053;&#12290;Moreau&#21253;&#32476;&#25552;&#20379;&#20102;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#30340;&#24179;&#28369;&#36817;&#20284;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#24212;&#29992;&#26631;&#20934;&#20248;&#21270;&#25216;&#26415;&#24182;&#25910;&#25947;&#21040;&#36866;&#24403;&#30340;&#31283;&#23450;&#28857;&#12290;&#25105;&#20204;&#23545;MEMRL&#31639;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#23545;&#20110;&#38750;&#20984;&#38382;&#39064;&#65292;&#23427;&#21487;&#20197;&#20197;&#20122;&#32447;&#24615;&#30340;&#25910;&#25947;&#36895;&#24230;&#36798;&#21040;&#19968;&#38454;&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-Reinforcement Learning (MRL) is a promising framework for training agents that can quickly adapt to new environments and tasks. In this work, we study the MRL problem under the policy gradient formulation, where we propose a novel algorithm that uses Moreau envelope surrogate regularizers to jointly learn a meta-policy that is adjustable to the environment of each individual task. Our algorithm, called Moreau Envelope Meta-Reinforcement Learning (MEMRL), learns a meta-policy that can adapt to a distribution of tasks by efficiently updating the policy parameters using a combination of gradient-based optimization and Moreau Envelope regularization. Moreau Envelopes provide a smooth approximation of the policy optimization problem, which enables us to apply standard optimization techniques and converge to an appropriate stationary point. We provide a detailed analysis of the MEMRL algorithm, where we show a sublinear convergence rate to a first-order stationary point for non-convex p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21160;&#24577;&#20998;&#25209;&#25216;&#26415;&#65292;&#29992;&#20110;&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#26681;&#25454;&#27599;&#20010;&#24037;&#20316;&#33410;&#28857;&#30340;&#36164;&#28304;&#21487;&#29992;&#24615;&#21644;&#21534;&#21520;&#37327;&#35843;&#25972;&#23567;&#25209;&#37327;&#22823;&#23567;&#65292;&#20174;&#32780;&#24179;&#34913;&#25152;&#26377;&#24037;&#20316;&#33410;&#28857;&#30340;&#36845;&#20195;&#26102;&#38388;&#65292;&#20943;&#23569;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.12213</link><description>&lt;p&gt;
&#29992;&#21160;&#24577;&#20998;&#25209;&#25216;&#26415;&#24179;&#34913;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#22521;&#35757;&#20013;&#30340;&#36164;&#28304;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Taming Resource Heterogeneity In Distributed ML Training With Dynamic Batching. (arXiv:2305.12213v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21160;&#24577;&#20998;&#25209;&#25216;&#26415;&#65292;&#29992;&#20110;&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#26681;&#25454;&#27599;&#20010;&#24037;&#20316;&#33410;&#28857;&#30340;&#36164;&#28304;&#21487;&#29992;&#24615;&#21644;&#21534;&#21520;&#37327;&#35843;&#25972;&#23567;&#25209;&#37327;&#22823;&#23567;&#65292;&#20174;&#32780;&#24179;&#34913;&#25152;&#26377;&#24037;&#20316;&#33410;&#28857;&#30340;&#36845;&#20195;&#26102;&#38388;&#65292;&#20943;&#23569;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20998;&#24067;&#24335;&#27169;&#22411;&#35757;&#32451;&#30340;&#25216;&#26415;&#21644;&#31995;&#32479;&#22823;&#22810;&#37117;&#20551;&#35774;&#38598;&#32676;&#30001;&#20855;&#26377;&#24658;&#23450;&#36164;&#28304;&#21487;&#29992;&#24615;&#30340;&#21516;&#26500;&#26381;&#21153;&#22120;&#32452;&#25104;&#65292;&#28982;&#32780;&#38598;&#32676;&#24322;&#36136;&#24615;&#22312;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#20013;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#24182;&#19988;&#26159;&#20302;&#24265;&#30340;&#30636;&#24577;&#36164;&#28304;&#65288;&#20363;&#22914; EC2 spot &#23454;&#20363;&#65289;&#30340;&#22522;&#26412;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21160;&#24577;&#20998;&#25209;&#25216;&#26415;&#65292;&#29992;&#20110;&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#35757;&#32451;&#65292;&#26681;&#25454;&#27599;&#20010;&#24037;&#20316;&#33410;&#28857;&#30340;&#36164;&#28304;&#21487;&#29992;&#24615;&#21644;&#21534;&#21520;&#37327;&#35843;&#25972;&#23567;&#25209;&#37327;&#22823;&#23567;&#12290;&#25105;&#20204;&#30340;&#23567;&#25209;&#37327;&#25511;&#21046;&#22120;&#26088;&#22312;&#24179;&#34913;&#25152;&#26377;&#24037;&#20316;&#33410;&#28857;&#30340;&#36845;&#20195;&#26102;&#38388;&#65292;&#24182;&#20419;&#36827;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#25968;&#37327; CPU &#21644; GPU &#36164;&#28304;&#30340;&#26381;&#21153;&#22120;&#32452;&#25104;&#30340;&#38598;&#32676;&#36827;&#34892;&#35757;&#32451;&#12290;&#35813;&#21464;&#37327;&#23567;&#25209;&#37327;&#25216;&#26415;&#20351;&#29992;&#27604;&#20363;&#25511;&#21046;&#21644; PID &#25511;&#21046;&#22120;&#30340;&#24605;&#24819;&#26469;&#25214;&#21040;&#31283;&#23450;&#30340;&#23567;&#25209;&#37327;&#22823;&#23567;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#21160;&#24577;&#20998;&#25209;&#21487;&#20197;&#23558;&#24322;&#26500;&#38598;&#32676;&#19978;&#30340;&#27169;&#22411;&#35757;&#32451;&#26102;&#38388;&#32553;&#30701;&#36229;&#36807; 4 &#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current techniques and systems for distributed model training mostly assume that clusters are comprised of homogeneous servers with a constant resource availability. However, cluster heterogeneity is pervasive in computing infrastructure, and is a fundamental characteristic of low-cost transient resources (such as EC2 spot instances). In this paper, we develop a dynamic batching technique for distributed data-parallel training that adjusts the mini-batch sizes on each worker based on its resource availability and throughput. Our mini-batch controller seeks to equalize iteration times on all workers, and facilitates training on clusters comprised of servers with different amounts of CPU and GPU resources. This variable mini-batch technique uses proportional control and ideas from PID controllers to find stable mini-batch sizes. Our empirical evaluation shows that dynamic batching can reduce model training times by more than 4x on heterogeneous clusters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#29992;&#36924;&#36817;&#30340;&#35789;&#27719;&#65292;&#35777;&#26126;&#20102;&#26377;&#38480;&#8220;&#35789;&#27719;&#8221;&#23384;&#22312;&#24182;&#21487;&#29992;&#20110;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#26144;&#23556;$f$&#21644;&#32039;&#33268;&#22495;$\Omega$&#20013;&#30340;&#27599;&#20010;&#28857;&#65292;&#35823;&#24046;&#23567;&#20110;$\varepsilon$&#12290;</title><link>http://arxiv.org/abs/2305.12205</link><description>&lt;p&gt;
&#36890;&#29992;&#36924;&#36817;&#30340;&#35789;&#27719;&#65306;&#19968;&#31181;&#23558;&#26144;&#23556;&#32452;&#21512;&#30475;&#20316;&#35821;&#35328;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Vocabulary for Universal Approximation: A Linguistic Perspective of Mapping Compositions. (arXiv:2305.12205v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#29992;&#36924;&#36817;&#30340;&#35789;&#27719;&#65292;&#35777;&#26126;&#20102;&#26377;&#38480;&#8220;&#35789;&#27719;&#8221;&#23384;&#22312;&#24182;&#21487;&#29992;&#20110;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#26144;&#23556;$f$&#21644;&#32039;&#33268;&#22495;$\Omega$&#20013;&#30340;&#27599;&#20010;&#28857;&#65292;&#35823;&#24046;&#23567;&#20110;$\varepsilon$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24207;&#21015;&#24314;&#27169;&#65292;&#22914;&#35821;&#35328;&#27169;&#22411;&#65292;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#21644;&#25104;&#21151;&#30340;&#24212;&#29992;&#65292;&#36825;&#20419;&#20351;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#23558;&#38750;&#36830;&#32493;&#38382;&#39064;&#36716;&#21270;&#20026;&#36830;&#32493;&#24418;&#24335;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#27839;&#30528;&#36825;&#20010;&#24605;&#36335;&#65292;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#19968;&#31995;&#21015;&#26144;&#23556;&#20989;&#25968;&#30340;&#32452;&#21512;&#65292;&#20854;&#20013;&#27599;&#20010;&#32452;&#21512;&#21487;&#35270;&#20026;&#19968;&#20010;&#8220;&#21333;&#35789;&#8221;&#12290;&#28982;&#32780;&#65292;&#32447;&#24615;&#26144;&#23556;&#30340;&#26435;&#37325;&#26159;&#26410;&#30830;&#23450;&#30340;&#65292;&#22240;&#27492;&#38656;&#35201;&#26080;&#38480;&#25968;&#37327;&#30340;&#21333;&#35789;&#12290;&#26412;&#25991;&#30740;&#31350;&#26377;&#38480;&#24773;&#20917;&#65292;&#26500;&#24314;&#24615;&#22320;&#35777;&#26126;&#20102;&#36890;&#29992;&#36924;&#36817;&#30340;&#26377;&#38480;&#8220;&#35789;&#27719;&#8221;$V=\{\phi_i: \mathbb{R}^d \to \mathbb{R}^d | i=1,...,n\}$&#23384;&#22312;&#65292;&#20854;&#20013;$n = O(d^2)$&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#23545;&#20110;&#20219;&#20309;&#36830;&#32493;&#26144;&#23556;$f: \mathbb{R}^d \to \mathbb{R}^d$&#12289;&#32039;&#33268;&#22495;$\Omega$&#21644;$\varepsilon&gt;0$&#65292;&#23384;&#22312;&#26144;&#23556;&#24207;&#21015;$\phi_{i_1}, ..., \phi_{i_m} \in V, m \in \mathbb{Z}_+$&#65292;&#20351;&#24471;&#32452;&#21512;$\phi_{i_m}$&#33021;&#22815;&#36924;&#36817;$f$&#21644;$\Omega$&#20013;&#30340;&#27599;&#20010;&#28857;&#65292;&#19988;&#35823;&#24046;&#23567;&#20110;$\varepsilon$&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning-based sequence modelings, such as language models, have received much attention and success, which pushes researchers to explore the possibility of transforming non-sequential problems into a sequential form. Following this thought, deep neural networks can be represented as composite functions of a sequence of mappings, linear or nonlinear, where each composition can be viewed as a \emph{word}. However, the weights of linear mappings are undetermined and hence require an infinite number of words. In this article, we investigate the finite case and constructively prove the existence of a finite \emph{vocabulary} $V=\{\phi_i: \mathbb{R}^d \to \mathbb{R}^d | i=1,...,n\}$ with $n=O(d^2)$ for the universal approximation. That is, for any continuous mapping $f: \mathbb{R}^d \to \mathbb{R}^d$, compact domain $\Omega$ and $\varepsilon&gt;0$, there is a sequence of mappings $\phi_{i_1}, ..., \phi_{i_m} \in V, m \in \mathbb{Z}_+$, such that the composition $\phi_{i_m
&lt;/p&gt;</description></item><item><title>GraVAC&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#24577;&#35843;&#25972;&#21387;&#32553;&#22240;&#23376;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#36827;&#23637;&#21644;&#35780;&#20272;&#19982;&#21387;&#32553;&#30456;&#20851;&#30340;&#26799;&#24230;&#20449;&#24687;&#25439;&#22833;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;GraVAC&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#20851;&#20110;&#27169;&#22411;&#25110;&#20854;&#36229;&#21442;&#25968;&#30340;&#20808;&#21069;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#36798;&#21040;&#19982;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#21387;&#32553;&#26041;&#27861;&#30456;&#21516;&#25110;&#26356;&#22909;&#30340;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;&#22312;CIFAR-10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#65292;&#30456;&#23545;&#20110;&#38745;&#24577;&#21387;&#32553;&#23545;&#24212;&#29289;&#65292;GraVAC&#21487;&#20197;&#23558;&#36890;&#20449;&#20943;&#23569;&#39640;&#36798;87&#65285;&#21644;75&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.12201</link><description>&lt;p&gt;
GraVAC&#65306;&#36866;&#24212;&#24615;&#21387;&#32553;&#29992;&#20110;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
GraVAC: Adaptive Compression for Communication-Efficient Distributed DL Training. (arXiv:2305.12201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12201
&lt;/p&gt;
&lt;p&gt;
GraVAC&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#24577;&#35843;&#25972;&#21387;&#32553;&#22240;&#23376;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#36827;&#23637;&#21644;&#35780;&#20272;&#19982;&#21387;&#32553;&#30456;&#20851;&#30340;&#26799;&#24230;&#20449;&#24687;&#25439;&#22833;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;GraVAC&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#20851;&#20110;&#27169;&#22411;&#25110;&#20854;&#36229;&#21442;&#25968;&#30340;&#20808;&#21069;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#36798;&#21040;&#19982;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#21387;&#32553;&#26041;&#27861;&#30456;&#21516;&#25110;&#26356;&#22909;&#30340;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;&#22312;CIFAR-10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#65292;&#30456;&#23545;&#20110;&#38745;&#24577;&#21387;&#32553;&#23545;&#24212;&#29289;&#65292;GraVAC&#21487;&#20197;&#23558;&#36890;&#20449;&#20943;&#23569;&#39640;&#36798;87&#65285;&#21644;75&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#65288;DDP&#65289;&#35757;&#32451;&#36890;&#36807;&#22810;&#20010;&#35774;&#22791;&#22312;&#25968;&#25454;&#23376;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#24182;&#32858;&#21512;&#26356;&#26032;&#26469;&#25552;&#39640;&#24212;&#29992;&#31243;&#24207;&#30340;&#25972;&#20307;&#21534;&#21520;&#37327;&#12290;&#27599;&#27425;&#36845;&#20195;&#30340;&#21608;&#26399;&#24615;&#21516;&#27493;&#20135;&#29983;&#20102;&#30456;&#24403;&#22823;&#30340;&#24320;&#38144;&#65292;&#21463;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#22823;&#21644;&#22797;&#26434;&#30340;&#24433;&#21709;&#26356;&#21152;&#20005;&#37325;&#12290;&#23613;&#31649;&#35768;&#22810;&#26799;&#24230;&#21387;&#32553;&#25216;&#26415;&#25552;&#20986;&#20102;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#30340;&#26041;&#24335;&#65292;&#20294;&#26368;&#20339;&#21387;&#32553;&#22240;&#23376;&#23548;&#33268;&#26368;&#22823;&#36895;&#24230;&#25552;&#21319;&#25110;&#26368;&#23567;&#25968;&#25454;&#20132;&#25442;&#30340;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24335;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#19982;&#21387;&#32553;&#36136;&#37327;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#32467;&#26500;&#12289;&#30828;&#20214;&#12289;&#32593;&#32476;&#25299;&#25169;&#21644;&#24102;&#23485;&#26377;&#20851;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GraVAC&#65292;&#19968;&#20010;&#21160;&#24577;&#35843;&#25972;&#21387;&#32553;&#22240;&#23376;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#36827;&#23637;&#21644;&#35780;&#20272;&#19982;&#21387;&#32553;&#30456;&#20851;&#30340;&#26799;&#24230;&#20449;&#24687;&#25439;&#22833;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;GraVAC&#20197;&#22312;&#32447;&#30340;&#40657;&#30418;&#26041;&#24335;&#24037;&#20316;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#20851;&#20110;&#27169;&#22411;&#25110;&#20854;&#36229;&#21442;&#25968;&#30340;&#20808;&#21069;&#20551;&#35774;&#65292;&#21516;&#26102;&#23454;&#29616;&#19982;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#21387;&#32553;&#26041;&#27861;&#30456;&#21516;&#25110;&#26356;&#22909;&#30340;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;&#23545;&#20110;CIFAR-10&#21644;ImageNet&#25968;&#25454;&#38598;&#65292;&#30456;&#23545;&#20110;&#20854;&#38745;&#24577;&#21387;&#32553;&#23545;&#24212;&#29289;&#65292;GraVAC&#23558;&#36890;&#20449;&#20943;&#23569;&#20102;&#39640;&#36798;87&#65285;&#21644;75&#65285;&#65292;&#32780;&#25910;&#25947;&#31934;&#24230;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed data-parallel (DDP) training improves overall application throughput as multiple devices train on a subset of data and aggregate updates to produce a globally shared model. The periodic synchronization at each iteration incurs considerable overhead, exacerbated by the increasing size and complexity of state-of-the-art neural networks. Although many gradient compression techniques propose to reduce communication cost, the ideal compression factor that leads to maximum speedup or minimum data exchange remains an open-ended problem since it varies with the quality of compression, model size and structure, hardware, network topology and bandwidth. We propose GraVAC, a framework to dynamically adjust compression factor throughout training by evaluating model progress and assessing gradient information loss associated with compression. GraVAC works in an online, black-box manner without any prior assumptions about a model or its hyperparameters, while achieving the same or better
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#20351;&#29992;&#28508;&#22312;&#23884;&#20837;&#26469;&#27169;&#25311;&#32593;&#32476;&#21160;&#21147;&#31995;&#32479;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#23884;&#20837;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#19977;&#20010;&#27491;&#30830;&#38271;&#26399;&#34892;&#20026;&#27979;&#35797;&#30340;&#39564;&#35777;&#34920;&#26126;&#35813;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12185</link><description>&lt;p&gt;
&#25105;&#20204;&#38656;&#35201;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26469;&#27169;&#25311;&#32593;&#32476;&#19978;&#30340;&#21160;&#21147;&#31995;&#32479;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do We Need an Encoder-Decoder to Model Dynamical Systems on Networks?. (arXiv:2305.12185v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#20351;&#29992;&#28508;&#22312;&#23884;&#20837;&#26469;&#27169;&#25311;&#32593;&#32476;&#21160;&#21147;&#31995;&#32479;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#23884;&#20837;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#19977;&#20010;&#27491;&#30830;&#38271;&#26399;&#34892;&#20026;&#27979;&#35797;&#30340;&#39564;&#35777;&#34920;&#26126;&#35813;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#22312;&#27169;&#25311;&#21160;&#21147;&#31995;&#32479;&#26041;&#38754;&#30340;&#26222;&#21450;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#19982;&#32593;&#32476;&#21160;&#21147;&#23398;&#24314;&#27169;&#30456;&#20851;&#19988;&#26410;&#34987;&#37325;&#35270;&#30340;&#35823;&#35299;&#12290;&#21463;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#28872;&#24433;&#21709;&#65292;&#28508;&#22312;&#39030;&#28857;&#23884;&#20837;&#34987;&#33258;&#28982;&#22320;&#37319;&#29992;&#22312;&#35768;&#22810;&#31070;&#32463;&#21160;&#21147;&#32593;&#32476;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23884;&#20837;&#20542;&#21521;&#20110;&#20135;&#29983;&#36866;&#24212;&#35266;&#27979;&#33391;&#22909;&#20294;&#21516;&#26102;&#20855;&#26377;&#38169;&#35823;&#21160;&#21147;&#34892;&#20026;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#27491;&#30830;&#38271;&#26399;&#34892;&#20026;&#30340;&#27979;&#35797;&#65292;&#24182;&#35828;&#26126;&#20102;&#23884;&#20837;&#24335;&#21160;&#21147;&#27169;&#22411;&#22914;&#20309;&#26410;&#36890;&#36807;&#36825;&#20123;&#27979;&#35797;&#65292;&#24182;&#20998;&#26512;&#20854;&#20013;&#30340;&#21407;&#22240;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#25299;&#25169;&#20849;&#36717;&#30340;&#35282;&#24230;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#34920;&#26126;&#36991;&#20813;&#20351;&#29992;&#23884;&#20837;&#21487;&#20197;&#36991;&#20813;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#21442;&#25968;&#21270;&#20004;&#20010;&#21152;&#24615;&#30690;&#37327;&#22330;&#20998;&#37327;&#30340;&#26080;&#23884;&#20837;&#26367;&#20195;&#26041;&#26696;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#26367;&#20195;&#26041;&#26696;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep learning gains popularity in modelling dynamical systems, we expose an underappreciated misunderstanding relevant to modelling dynamics on networks. Strongly influenced by graph neural networks, latent vertex embeddings are naturally adopted in many neural dynamical network models. However, we show that embeddings tend to induce a model that fits observations well but simultaneously has incorrect dynamical behaviours. Recognising that previous studies narrowly focus on short-term predictions during the transient phase of a flow, we propose three tests for correct long-term behaviour, and illustrate how an embedding-based dynamical model fails these tests, and analyse the causes, particularly through the lens of topological conjugacy. In doing so, we show that the difficulties can be avoided by not using embedding. We propose a simple embedding-free alternative based on parametrising two additive vector-field components. Through extensive experiments, we verify that the proposed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#24615;&#26694;&#26550;&#65292;&#36890;&#36807;&#26799;&#24230;&#35828;&#26126;&#25214;&#21040;&#20004;&#20010;&#27169;&#22411;&#28966;&#28857;&#36827;&#34892;&#21435;&#20559;&#35265;&#22788;&#29702;&#65292;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12178</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#35828;&#26126;&#30340;&#34920;&#31034;&#27861;&#21435;&#20559;&#35265;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Model Debiasing via Gradient-based Explanation on Representation. (arXiv:2305.12178v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#24615;&#26694;&#26550;&#65292;&#36890;&#36807;&#26799;&#24230;&#35828;&#26126;&#25214;&#21040;&#20004;&#20010;&#27169;&#22411;&#28966;&#28857;&#36827;&#34892;&#21435;&#20559;&#35265;&#22788;&#29702;&#65292;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20250;&#23545;&#26576;&#20123;&#20154;&#21475;&#32479;&#35745;&#23398;&#32676;&#20307;&#20135;&#29983;&#20559;&#35265;&#65292;&#21363;&#19981;&#20844;&#24179;&#29616;&#35937;&#12290;&#36817;&#26399;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#36890;&#36807;&#20998;&#31163;&#24335;&#34920;&#31034;&#23398;&#20064;&#23398;&#20064;&#28508;&#22312;&#30721;&#65288;&#21363;&#34920;&#31034;&#27861;&#65289;&#65292;&#28982;&#21518;&#20002;&#24323;&#19982;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#65289;&#30456;&#20851;&#30340;&#30721;&#12290;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#22788;&#29702;&#29616;&#23454;&#25968;&#25454;&#65288;&#29305;&#21035;&#26159;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65289;&#26102;&#65292;&#21487;&#33021;&#20250;&#36951;&#28431;&#20195;&#29702;&#23646;&#24615;&#65288;&#25935;&#24863;&#23646;&#24615;&#30340;&#20195;&#29702;&#65289;&#65292;&#24182;&#19988;&#21463;&#21040;&#19981;&#23436;&#20840;&#20998;&#31163;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#20844;&#24179;&#24615;&#33021;&#19979;&#38477;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#25439;&#22833;&#26377;&#29992;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#24615;&#26694;&#26550;&#65292;&#38024;&#23545;&#25935;&#24863;&#23646;&#24615;&#21644;&#20195;&#29702;&#23646;&#24615;&#36827;&#34892;&#21435;&#20559;&#35265;&#22788;&#29702;&#65292;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#32780;&#19981;&#38656;&#35201;&#23436;&#20840;&#20998;&#31163;&#12290;&#20027;&#35201;&#24605;&#36335;&#26159;&#21033;&#29992;&#26799;&#24230;&#35828;&#26126;&#25214;&#21040;&#20004;&#20010;&#27169;&#22411;&#28966;&#28857;&#65306;1&#65289;&#20854;&#20013;&#19968;&#20010;&#28966;&#28857;&#29992;&#20110;&#39044;&#27979;&#20540;&#65292;2&#65289;&#21478;&#19968;&#20010;&#28966;&#28857;&#29992;&#20110;&#20195;&#29702;&#23646;&#24615;&#65292;&#28982;&#21518;&#23545;&#28508;&#22312;&#30721;&#36827;&#34892;&#20462;&#27491;&#20197;&#20943;&#36731;&#36825;&#20123;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning systems produce biased results towards certain demographic groups, known as the fairness problem. Recent approaches to tackle this problem learn a latent code (i.e., representation) through disentangled representation learning and then discard the latent code dimensions correlated with sensitive attributes (e.g., gender). Nevertheless, these approaches may suffer from incomplete disentanglement and overlook proxy attributes (proxies for sensitive attributes) when processing real-world data, especially for unstructured data, causing performance degradation in fairness and loss of useful information for downstream tasks. In this paper, we propose a novel fairness framework that performs debiasing with regard to both sensitive attributes and proxy attributes, which boosts the prediction performance of downstream task models without complete disentanglement. The main idea is to, first, leverage gradient-based explanation to find two model focuses, 1) one focus for predicti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;AMenuNet&#26469;&#26500;&#36896;AMAs&#21442;&#25968;&#21644;&#29983;&#25104;&#20505;&#36873;&#20998;&#37197;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#21344;&#20248;&#31574;&#30053;&#28608;&#21169;&#20860;&#23481;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#20854;&#22312;&#21327;&#21830;&#19968;&#33268;&#30340;&#20215;&#20540;&#21644;&#31038;&#20250;&#27531;&#20313;&#20215;&#20540;&#26041;&#38754;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12162</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;DSIC&#20223;&#23556;&#26497;&#22823;&#20215;&#25293;&#21334;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
A Scalable Neural Network for DSIC Affine Maximizer Auction Design. (arXiv:2305.12162v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12162
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;AMenuNet&#26469;&#26500;&#36896;AMAs&#21442;&#25968;&#21644;&#29983;&#25104;&#20505;&#36873;&#20998;&#37197;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#21344;&#20248;&#31574;&#30053;&#28608;&#21169;&#20860;&#23481;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#20854;&#22312;&#21327;&#21830;&#19968;&#33268;&#30340;&#20215;&#20540;&#21644;&#31038;&#20250;&#27531;&#20313;&#20215;&#20540;&#26041;&#38754;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25293;&#21334;&#35774;&#35745;&#26088;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#23547;&#25214;&#32463;&#39564;&#19978;&#39640;&#25910;&#20837;&#30340;&#26426;&#21046;&#12290;&#29616;&#26377;&#30340;&#22810;&#29289;&#21697;&#25293;&#21334;&#24773;&#26223;&#30340;&#24037;&#20316;&#21487;&#20197;&#31895;&#30053;&#22320;&#20998;&#20026;RegretNet&#31867;&#21644;&#20223;&#23556;&#26497;&#22823;&#20215;&#65288;AMAs&#65289;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#19981;&#33021;&#20005;&#26684;&#20445;&#35777;&#21344;&#20248;&#31574;&#30053;&#28608;&#21169;&#20860;&#23481;&#24615;&#65288;DSIC&#65289;&#65292;&#32780;&#21518;&#32773;&#22240;&#20026;&#20998;&#37197;&#20505;&#36873;&#20154;&#25968;&#36807;&#22810;&#32780;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AMenuNet&#65292;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#20174;&#20986;&#20215;&#20154;&#21644;&#29289;&#21697;&#34920;&#31034;&#20013;&#26500;&#36896;AMA&#21442;&#25968;&#65288;&#29978;&#33267;&#21253;&#25324;&#20998;&#37197;&#33756;&#21333;&#65289;&#12290;&#30001;&#20110;AMA&#30340;&#23646;&#24615;&#65292;AMenuNet&#22987;&#32456;&#26159;DSIC&#21644;&#20010;&#20154;&#29702;&#24615;&#65288;IR&#65289;&#30340;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#20505;&#36873;&#20998;&#37197;&#26469;&#22686;&#24378;&#21487;&#20280;&#32553;&#24615;&#12290;&#27492;&#22806;&#65292;AMenuNet&#26159;&#32622;&#25442;&#31561;&#21464;&#30340;&#65292;&#20854;&#21442;&#25968;&#25968;&#37327;&#19981;&#21463;&#25293;&#21334;&#35268;&#27169;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;AMenuNet&#22312;&#21327;&#21830;&#19968;&#33268;&#30340;&#20215;&#20540;&#21644;&#31038;&#20250;&#27531;&#20313;&#20215;&#20540;&#26041;&#38754;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated auction design aims to find empirically high-revenue mechanisms through machine learning. Existing works on multi item auction scenarios can be roughly divided into RegretNet-like and affine maximizer auctions (AMAs) approaches. However, the former cannot strictly ensure dominant strategy incentive compatibility (DSIC), while the latter faces scalability issue due to the large number of allocation candidates. To address these limitations, we propose AMenuNet, a scalable neural network that constructs the AMA parameters (even including the allocation menu) from bidder and item representations. AMenuNet is always DSIC and individually rational (IR) due to the properties of AMAs, and it enhances scalability by generating candidate allocations through a neural network. Additionally, AMenuNet is permutation equivariant, and its number of parameters is independent of auction scale. We conduct extensive experiments to demonstrate that AMenuNet outperforms strong baselines in both co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#26679;&#26412;&#26377;&#25928;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#27169;&#22411;&#36716;&#25442;&#23454;&#29616;&#27491;&#21521;&#36801;&#31227;&#65292;&#24182;&#29992;&#20316;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#30340;&#21021;&#22987;&#21270;&#20197;&#36798;&#21040;&#26032;&#30340;&#26368;&#20248;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.12158</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#22312;&#21442;&#25968;&#21464;&#21270;&#31995;&#32479;&#30340;&#26679;&#26412;&#26377;&#25928;&#36801;&#31227;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Model-based adaptation for sample efficient transfer in reinforcement learning control of parameter-varying systems. (arXiv:2305.12158v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#26679;&#26412;&#26377;&#25928;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#27169;&#22411;&#36716;&#25442;&#23454;&#29616;&#27491;&#21521;&#36801;&#31227;&#65292;&#24182;&#29992;&#20316;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#30340;&#21021;&#22987;&#21270;&#20197;&#36798;&#21040;&#26032;&#30340;&#26368;&#20248;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#27169;&#22411;&#30340;&#25511;&#21046;&#26041;&#27861;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26679;&#26412;&#26377;&#25928;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#27169;&#22411;&#36716;&#25442;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25511;&#21046;&#31574;&#30053;&#24212;&#29992;&#20110;&#30446;&#26631;&#31995;&#32479;&#65292;&#23454;&#29616;&#27491;&#21521;&#36801;&#31227;&#65292;&#20174;&#32780;&#20026;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#25552;&#20379;&#21021;&#22987;&#21270;&#65292;&#20197;&#20415;&#20854;&#25910;&#25947;&#21040;&#26032;&#30340;&#26368;&#20248;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we leverage ideas from model-based control to address the sample efficiency problem of reinforcement learning (RL) algorithms. Accelerating learning is an active field of RL highly relevant in the context of time-varying systems. Traditional transfer learning methods propose to use prior knowledge of the system behavior to devise a gradual or immediate data-driven transformation of the control policy obtained through RL. Such transformation is usually computed by estimating the performance of previous control policies based on measurements recently collected from the system. However, such retrospective measures have debatable utility with no guarantees of positive transfer in most cases. Instead, we propose a model-based transformation, such that when actions from a control policy are applied to the target system, a positive transfer is achieved. The transformation can be used as an initialization for the reinforcement learning process to converge to a new optimum. We va
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35748;&#20026;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#38656;&#35201;&#25509;&#21463;&#31639;&#27861;&#25945;&#32946;&#65292;&#20197;&#25913;&#36827;&#20854;&#20915;&#31574;&#36947;&#24503;&#65292;&#32780;&#38750;&#20165;&#20165;&#20572;&#30041;&#22312;&#35757;&#32451;&#19978;&#12290;&#23545;&#20110;AI&#20262;&#29702;&#20915;&#31574;&#65292;&#35299;&#20915;&#26041;&#26696;&#22312;&#20110;&#23545;ML&#36827;&#34892;&#31639;&#27861;&#25945;&#32946;&#12290;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24182;&#19981;&#26159;&#19982;&#25105;&#20204;&#30340;&#20154;&#31867;&#29305;&#36136;&#20998;&#24320;&#32780;&#23384;&#22312;&#30340;&#65292;&#32780;&#26159;&#25105;&#20204;&#26368;&#20869;&#22312;&#30340;&#20559;&#35265;&#21644;&#20559;&#35265;&#30340;&#19968;&#31181;&#20307;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.12157</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#21463;&#25945;&#32946;&#21527;&#65311;&#8212;&#8212;&#35770;&#31639;&#27861;&#25945;&#32946;&#32780;&#38750;&#35757;&#32451; (arXiv:2305.12157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
(Machine) Learning to Be Like Thee? For Algorithm Education, Not Training. (arXiv:2305.12157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35748;&#20026;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#38656;&#35201;&#25509;&#21463;&#31639;&#27861;&#25945;&#32946;&#65292;&#20197;&#25913;&#36827;&#20854;&#20915;&#31574;&#36947;&#24503;&#65292;&#32780;&#38750;&#20165;&#20165;&#20572;&#30041;&#22312;&#35757;&#32451;&#19978;&#12290;&#23545;&#20110;AI&#20262;&#29702;&#20915;&#31574;&#65292;&#35299;&#20915;&#26041;&#26696;&#22312;&#20110;&#23545;ML&#36827;&#34892;&#31639;&#27861;&#25945;&#32946;&#12290;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24182;&#19981;&#26159;&#19982;&#25105;&#20204;&#30340;&#20154;&#31867;&#29305;&#36136;&#20998;&#24320;&#32780;&#23384;&#22312;&#30340;&#65292;&#32780;&#26159;&#25105;&#20204;&#26368;&#20869;&#22312;&#30340;&#20559;&#35265;&#21644;&#20559;&#35265;&#30340;&#19968;&#31181;&#20307;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35748;&#20026;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#24517;&#39035;&#35201;&#21463;&#25945;&#32946;&#12290;ML&#35757;&#32451;&#30340;&#31639;&#27861;&#30340;&#36947;&#24503;&#20915;&#31574;&#22312;&#20154;&#31867;&#31038;&#20250;&#20013;&#38543;&#22788;&#21487;&#35265;&#65292;&#26377;&#26102;&#20250;&#36870;&#36716;&#25919;&#24220;&#12289;&#38750;&#25919;&#24220;&#32452;&#32455;&#21644;&#20844;&#27665;&#31038;&#20250;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#21462;&#24471;&#30340;&#31038;&#20250;&#36827;&#27493;&#65292;&#25110;&#32773;&#20173;&#22312;&#21162;&#21147;&#23454;&#29616;&#36825;&#20123;&#36827;&#27493;&#12290;&#23613;&#31649;&#36825;&#20123;&#31639;&#27861;&#30340;&#20915;&#31574;&#23545;&#20154;&#31867;&#31038;&#20250;&#20855;&#26377;&#19981;&#21487;&#20272;&#37327;&#30340;&#24433;&#21709;&#65292;&#20294;&#23427;&#20204;&#21364;&#26159;&#24050;&#30693;&#30340;&#26368;&#26080;&#30693;&#30340;&#20195;&#29702;&#26426;&#26500;&#20043;&#19968;&#65288;&#25968;&#25454;&#19981;&#23436;&#25972;&#12289;&#19981;&#21253;&#23481;&#25110;&#26377;&#20559;&#35265;&#65289;&#12290;ML&#31639;&#27861;&#24182;&#19981;&#26159;&#19982;&#25105;&#20204;&#30340;&#20154;&#31867;&#29305;&#36136;&#20998;&#24320;&#32780;&#23384;&#22312;&#30340;&#65292;&#32780;&#26159;&#25105;&#20204;&#26368;&#20869;&#22312;&#30340;&#20559;&#35265;&#21644;&#20559;&#35265;&#30340;&#19968;&#31181;&#20307;&#29616;&#12290;&#19968;&#20123;&#30740;&#31350;&#33268;&#21147;&#20110;&#36131;&#20219;&#20998;&#37197;&#20316;&#20026;&#35299;&#20915;&#38750;&#36947;&#24503;AI&#34892;&#20026;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#35748;&#20026;&#65292;AI&#20262;&#29702;&#20915;&#31574;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#20110;&#23545;ML&#36827;&#34892;&#31639;&#27861;&#25945;&#32946;&#65288;&#32780;&#38750;&#35757;&#32451;&#65289;&#12290;&#20174;ML&#21644;&#20799;&#31461;&#31038;&#20250;&#36131;&#20219;&#25945;&#32946;&#20043;&#38388;&#30340;&#31867;&#27604;&#20013;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#36127;&#36131;&#20219;&#21644;&#21487;&#25345;&#32493;&#21457;&#23637;&#30340;&#26126;&#30830;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper argues that Machine Learning (ML) algorithms must be educated. ML-trained algorithms moral decisions are ubiquitous in human society. Sometimes reverting the societal advances governments, NGOs and civil society have achieved with great effort in the last decades or are yet on the path to be achieved. While their decisions have an incommensurable impact on human societies, these algorithms are within the least educated agents known (data incomplete, un-inclusive, or biased). ML algorithms are not something separate from our human idiosyncrasy but an enactment of our most implicit prejudices and biases. Some research is devoted to responsibility assignment as a strategy to tackle immoral AI behaviour. Yet this paper argues that the solution for AI ethical decision-making resides in algorithm education (as opposed to the training) of ML. Drawing from an analogy between ML and child education for social responsibility, the paper offers clear directions for responsible and susta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#20351;&#29992; Langevin &#21160;&#21147;&#23398;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#24402;&#19968;&#21270;&#27969;&#21487;&#33021;&#38754;&#20020;&#30340;&#22797;&#26434;&#20998;&#24067;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#36731;&#26494;&#22320;&#34701;&#20837;&#20219;&#20309; NF &#32467;&#26500;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.12149</link><description>&lt;p&gt;
&#22312;&#28508;&#31354;&#38388;&#20013;&#20351;&#29992; Langevin &#21160;&#21147;&#23398;&#30340;&#24402;&#19968;&#21270;&#27969;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Normalizing flow sampling with Langevin dynamics in the latent space. (arXiv:2305.12149v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#20351;&#29992; Langevin &#21160;&#21147;&#23398;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#24402;&#19968;&#21270;&#27969;&#21487;&#33021;&#38754;&#20020;&#30340;&#22797;&#26434;&#20998;&#24067;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#36731;&#26494;&#22320;&#34701;&#20837;&#20219;&#20309; NF &#32467;&#26500;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#19968;&#21270;&#27969;&#65288;NF&#65289;&#20351;&#29992;&#36830;&#32493;&#29983;&#25104;&#22120;&#23558;&#31616;&#21333;&#30340;&#28508;&#22312;&#20998;&#24067;&#65288;&#20363;&#22914;&#39640;&#26031;&#20998;&#24067;&#65289;&#26144;&#23556;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#38598;&#30456;&#20851;&#32852;&#30340;&#32463;&#39564;&#30446;&#26631;&#20998;&#24067;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#21464;&#20998;&#30446;&#26631;&#26469;&#35757;&#32451;&#21518;&#65292;&#23398;&#20064;&#21040;&#30340;&#26144;&#23556;&#25552;&#20379;&#20102;&#30446;&#26631;&#20998;&#24067;&#30340;&#36817;&#20284;&#29983;&#25104;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#31639;&#27861;&#65292;&#23558;&#30446;&#26631;&#20998;&#24067;&#22312;&#28508;&#22495;&#20013;&#37319;&#26679;&#65292;&#28982;&#21518;&#23558;&#20854;&#20256;&#36755;&#22238;&#30446;&#26631;&#22495;&#65292;&#20197;&#20811;&#26381;&#24403;&#24212;&#23545;&#22797;&#26434;&#20998;&#24067;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#28508;&#31354;&#38388;&#20013;&#30340; Metropolis &#35843;&#25972; Langevin &#21160;&#21147;&#23398;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#22320;&#34701;&#20837;&#20219;&#20309; NF &#32467;&#26500;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29609;&#20855;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalizing flows (NF) use a continuous generator to map a simple latent (e.g. Gaussian) distribution, towards an empirical target distribution associated with a training data set. Once trained by minimizing a variational objective, the learnt map provides an approximate generative model of the target distribution. Since standard NF implement differentiable maps, they may suffer from pathological behaviors when targeting complex distributions. For instance, such problems may appear for distributions on multi-component topologies or characterized by multiple modes with high probability regions separated by very unlikely areas. A typical symptom is the explosion of the Jacobian norm of the transformation in very low probability areas. This paper proposes to overcome this issue thanks to a new Markov chain Monte Carlo algorithm to sample from the target distribution in the latent domain before transporting it back to the target domain. The approach relies on a Metropolis adjusted Langevin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20013;&#24425;&#31080;&#20551;&#35774;&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#25351;&#20986;&#26681;&#25454;&#29616;&#26377;SNN&#20013;&#30340;&#26435;&#37325;&#22823;&#23567;&#36827;&#34892;&#20462;&#21098;&#19981;&#26159;&#26368;&#20248;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.12148</link><description>&lt;p&gt;
&#27010;&#29575;&#24314;&#27169;&#65306;&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#35777;&#26126;&#20013;&#24425;&#31080;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Modeling: Proving the Lottery Ticket Hypothesis in Spiking Neural Network. (arXiv:2305.12148v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20013;&#24425;&#31080;&#20551;&#35774;&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#25351;&#20986;&#26681;&#25454;&#29616;&#26377;SNN&#20013;&#30340;&#26435;&#37325;&#22823;&#23567;&#36827;&#34892;&#20462;&#21098;&#19981;&#26159;&#26368;&#20248;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#24425;&#31080;&#20551;&#35774;&#65288;LTH&#65289;&#25351;&#20986;&#65292;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#21253;&#21547;&#19968;&#20010;&#23567;&#30340;&#23376;&#32593;&#32476;&#65288;&#21363;&#20013;&#22870;&#21048;&#65289; &#65292;&#24403;&#35813;&#23376;&#32593;&#32476;&#21333;&#29420;&#35757;&#32451;&#26102;&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;&#22823;&#22411;&#32593;&#32476;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290; LTH&#20026;&#32593;&#32476;&#20462;&#21098;&#24320;&#36767;&#20102;&#19968;&#26465;&#26032;&#36335;&#12290;&#29616;&#26377;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#20013;&#30340;LTH&#35777;&#26126;&#26159;&#22522;&#20110;&#36830;&#32493;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;ReLU&#65289;&#65292;&#36825;&#20123;&#20989;&#25968;&#28385;&#36275;Lipschitz&#26465;&#20214;&#12290; &#28982;&#32780;&#65292;&#30001;&#20110;&#33033;&#20914;&#20989;&#25968;&#30340;&#19981;&#36830;&#32493;&#24615;&#65292;&#36825;&#20123;&#29702;&#35770;&#26041;&#27861;&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#20013;&#19981;&#36866;&#29992;&#12290;&#25105;&#20204;&#35748;&#20026;&#21487;&#20197;&#36890;&#36807;&#28040;&#38500;Lipschitz&#26465;&#20214;&#26469;&#25193;&#23637;LTH&#30340;&#33539;&#22260;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#22797;&#26434;&#26102;&#31354;&#21160;&#24577;&#30340;&#33033;&#20914;&#31070;&#32463;&#20803;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#19978;&#35777;&#26126;&#20102;LTH&#22312;SNN&#20013;&#25104;&#31435;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#23450;&#29702;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#26681;&#25454;&#29616;&#26377;SNN&#20013;&#30340;&#26435;&#37325;&#22823;&#23567;&#36827;&#34892;&#20462;&#21098;&#26174;&#28982;&#19981;&#26159;&#26368;&#20248;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Lottery Ticket Hypothesis (LTH) states that a randomly-initialized large neural network contains a small sub-network (i.e., winning tickets) which, when trained in isolation, can achieve comparable performance to the large network. LTH opens up a new path for network pruning. Existing proofs of LTH in Artificial Neural Networks (ANNs) are based on continuous activation functions, such as ReLU, which satisfying the Lipschitz condition. However, these theoretical methods are not applicable in Spiking Neural Networks (SNNs) due to the discontinuous of spiking function. We argue that it is possible to extend the scope of LTH by eliminating Lipschitz condition. Specifically, we propose a novel probabilistic modeling approach for spiking neurons with complicated spatio-temporal dynamics. Then we theoretically and experimentally prove that LTH holds in SNNs. According to our theorem, we conclude that pruning directly in accordance with the weight size in existing SNNs is clearly not optim
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#26368;&#32039;Horn&#36924;&#36817;&#30446;&#26631;&#29702;&#35770;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#20110;&#25581;&#31034;&#22522;&#20110;&#32844;&#19994;&#30340;&#24615;&#21035;&#20559;&#35265;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2305.12143</link><description>&lt;p&gt;
&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#23398;&#20064;Horn&#21253;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learning Horn Envelopes via Queries from Large Language Models. (arXiv:2305.12143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12143
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#26368;&#32039;Horn&#36924;&#36817;&#30446;&#26631;&#29702;&#35770;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#20110;&#25581;&#31034;&#22522;&#20110;&#32844;&#19994;&#30340;&#24615;&#21035;&#20559;&#35265;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;Angluin&#30340;&#31934;&#30830;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#25104;&#21592;&#21644;&#31561;&#20215;&#24615;&#26597;&#35810;&#21040;&#19968;&#20010;oracle&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;oracle&#26159;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;Angluin&#29992;&#20110;&#23398;&#20064;Horn&#29702;&#35770;&#30340;&#32463;&#20856;&#31639;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#24517;&#35201;&#30340;&#21464;&#21270;&#65292;&#20197;&#20351;&#20854;&#36866;&#29992;&#20110;&#20174;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24517;&#39035;&#32771;&#34385;&#21040;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#19981;&#20250;&#20687;Horn oracle&#37027;&#26679;&#34892;&#20107;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#30340;&#28508;&#22312;&#30446;&#26631;&#29702;&#35770;&#21487;&#33021;&#19981;&#26159;Horn&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26088;&#22312;&#25552;&#21462;&#30446;&#26631;&#29702;&#35770;&#8220;&#26368;&#32039;Horn&#36924;&#36817;&#8221;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#20445;&#35777;&#22312;&#25351;&#25968;&#26102;&#38388;&#65288;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#65289;&#20869;&#32456;&#27490;&#65292;&#22312;&#30446;&#26631;&#20855;&#26377;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#38750;Horn&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#32456;&#27490;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25552;&#21462;&#25581;&#31034;&#22522;&#20110;&#32844;&#19994;&#30340;&#24615;&#21035;&#20559;&#35265;&#30340;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate an approach for extracting knowledge from trained neural networks based on Angluin's exact learning model with membership and equivalence queries to an oracle. In this approach, the oracle is a trained neural network. We consider Angluin's classical algorithm for learning Horn theories and study the necessary changes to make it applicable to learn from neural networks. In particular, we have to consider that trained neural networks may not behave as Horn oracles, meaning that their underlying target theory may not be Horn. We propose a new algorithm that aims at extracting the ``tightest Horn approximation'' of the target theory and that is guaranteed to terminate in exponential time (in the worst case) and in polynomial time if the target has polynomially many non-Horn examples. To showcase the applicability of the approach, we perform experiments on pre-trained language models and extract rules that expose occupation-based gender biases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24335;&#32852;&#37030;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#36890;&#36807;&#19968;&#20010;&#29305;&#23450;&#30340;&#31995;&#32479;&#65292;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#22833;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12134</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#32852;&#37030;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Privacy in Multimodal Federated Human Activity Recognition. (arXiv:2305.12134v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24335;&#32852;&#37030;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#36890;&#36807;&#19968;&#20010;&#29305;&#23450;&#30340;&#31995;&#32479;&#65292;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#22833;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#30340;&#35757;&#32451;&#25968;&#25454;&#24448;&#24448;&#21253;&#21547;&#38544;&#31169;&#20449;&#24687;&#25110;&#30001;&#19981;&#21512;&#20316;&#23454;&#20307;&#25345;&#26377;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29992;&#25143;&#12289;&#29615;&#22659;&#21644;&#20256;&#24863;&#22120;&#32423;&#21035;&#19978;&#38544;&#31169;&#23545;&#32852;&#37030;HAR&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;FL&#23545;HAR&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;FL&#31995;&#32479;&#30340;&#38544;&#31169;&#20445;&#25252;&#31243;&#24230;&#65292;&#24182;&#19988;&#20027;&#35201;&#21462;&#20915;&#20110;&#26469;&#33258;&#19981;&#21516;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#30340;&#37197;&#32622;&#12290;&#23613;&#31649;&#36991;&#20813;&#25968;&#25454;&#20849;&#20139;&#24182;&#22312;&#20154;&#31867;&#25110;&#29615;&#22659;&#32423;&#21035;&#19978;&#20551;&#35774;&#38544;&#31169;&#65292;&#22914;&#20043;&#21069;&#30340;&#24037;&#20316;&#25152;&#20570;&#30340;&#37027;&#26679;&#65292;&#31934;&#24230;&#20250;&#38477;&#20302;5-7&#65285;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#31181;&#38544;&#31169;&#24310;&#20280;&#21040;&#27169;&#24577;&#32423;&#21035;&#24182;&#20005;&#26684;&#20998;&#31163;&#22810;&#20010;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#21487;&#33021;&#20250;&#23548;&#33268;&#31934;&#24230;&#38477;&#20302;19-42&#65285;&#12290;&#30001;&#20110;&#36825;&#31181;&#24418;&#24335;&#30340;&#38544;&#31169;&#26159;HAR&#20013;&#34987;&#35201;&#27714;&#30340;&#36947;&#24503;&#21033;&#29992;&#34987;&#21160;&#20256;&#24863;&#26041;&#27861;&#25152;&#24517;&#38656;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#22312;&#35813;&#31995;&#32479;&#20013;&#23458;&#25143;&#31471;&#30456;&#20114;&#35757;&#32451;&#19968;&#20010;&#36890;&#29992;&#30340;FL&#27169;&#22411;&#21644;&#19968;&#20010;&#27599;&#31181;&#27169;&#24577;&#19968;&#20010;&#30340;&#32452;&#32423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;HAR&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human Activity Recognition (HAR) training data is often privacy-sensitive or held by non-cooperative entities. Federated Learning (FL) addresses such concerns by training ML models on edge clients. This work studies the impact of privacy in federated HAR at a user, environment, and sensor level. We show that the performance of FL for HAR depends on the assumed privacy level of the FL system and primarily upon the colocation of data from different sensors. By avoiding data sharing and assuming privacy at the human or environment level, as prior works have done, the accuracy decreases by 5-7%. However, extending this to the modality level and strictly separating sensor data between multiple clients may decrease the accuracy by 19-42%. As this form of privacy is necessary for the ethical utilisation of passive sensing methods in HAR, we implement a system where clients mutually train both a general FL model and a group-level one per modality. Our evaluation shows that this method leads to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#25439;&#22833;&#20540;&#23792;&#20540;&#29616;&#35937;&#30340;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#26368;&#22823;&#29305;&#24449;&#20540;&#30340;&#31532;&#19968;&#20010;&#29305;&#24449;&#21521;&#37327;&#30340;&#20559;&#24046;&#20027;&#35201;&#21463;&#20302;&#39057;&#25104;&#20998;&#21344;&#25454;&#12290;&#20302;&#39057;&#25104;&#20998;&#21487;&#20197;&#34987;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#24456;&#22909;&#22320;&#25429;&#33719;&#65292;&#25152;&#20197;&#23548;&#33268;&#20855;&#26377;&#33391;&#22909;&#21644;&#21155;&#36136;&#27867;&#21270;&#33021;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#37117;&#21487;&#20197;&#24456;&#22909;&#22320;&#23398;&#20064;&#20302;&#39057;&#25104;&#20998;&#65292;&#20294;&#21155;&#36136;&#27867;&#21270;&#33021;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#20250;&#36807;&#24230;&#25311;&#21512;&#39640;&#39057;&#25104;&#20998;&#65292;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#26356;&#24179;&#28369;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.12133</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#25439;&#22833;&#23792;&#20540;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Loss Spike in Training Neural Networks. (arXiv:2305.12133v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#25439;&#22833;&#20540;&#23792;&#20540;&#29616;&#35937;&#30340;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#26368;&#22823;&#29305;&#24449;&#20540;&#30340;&#31532;&#19968;&#20010;&#29305;&#24449;&#21521;&#37327;&#30340;&#20559;&#24046;&#20027;&#35201;&#21463;&#20302;&#39057;&#25104;&#20998;&#21344;&#25454;&#12290;&#20302;&#39057;&#25104;&#20998;&#21487;&#20197;&#34987;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#24456;&#22909;&#22320;&#25429;&#33719;&#65292;&#25152;&#20197;&#23548;&#33268;&#20855;&#26377;&#33391;&#22909;&#21644;&#21155;&#36136;&#27867;&#21270;&#33021;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#37117;&#21487;&#20197;&#24456;&#22909;&#22320;&#23398;&#20064;&#20302;&#39057;&#25104;&#20998;&#65292;&#20294;&#21155;&#36136;&#27867;&#21270;&#33021;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#20250;&#36807;&#24230;&#25311;&#21512;&#39640;&#39057;&#25104;&#20998;&#65292;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#26356;&#24179;&#28369;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#25439;&#22833;&#20540;&#23792;&#20540;&#29616;&#35937;&#25152;&#32972;&#21518;&#30340;&#26426;&#21046;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20855;&#26377;&#36739;&#23567;&#25439;&#22833;&#30340;&#21306;&#22495;&#65292;&#19968;&#26086;&#35757;&#32451;&#36827;&#20837;&#35813;&#21306;&#22495;&#65292;&#35757;&#32451;&#23601;&#20250;&#21464;&#24471;&#19981;&#31283;&#23450;&#65292;&#25439;&#22833;&#20540;&#21576;&#25351;&#25968;&#24335;&#22686;&#38271;&#65292;&#21363;&#20986;&#29616;&#25439;&#22833;&#23792;&#20540;&#29616;&#35937;&#12290;&#24403;&#35757;&#32451;&#36827;&#20837;&#24179;&#22374;&#21306;&#22495;&#26102;&#65292;&#35757;&#32451;&#20250;&#21464;&#24471;&#31283;&#23450;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#25439;&#22833;Hessian&#30697;&#38453;&#30340;&#26368;&#22823;&#29305;&#24449;&#20540;&#65288;$\lambda_{\mathrm{max}}$&#65289;&#30340;&#31532;&#19968;&#20010;&#29305;&#24449;&#21521;&#37327;&#30340;&#20559;&#24046;&#20027;&#35201;&#30001;&#20302;&#39057;&#25104;&#20998;&#21344;&#25454;&#12290;&#30001;&#20110;&#20302;&#39057;&#25104;&#20998;&#21487;&#20197;&#38750;&#24120;&#24555;&#36895;&#22320;&#34987;&#25429;&#33719;&#65288;&#39057;&#29575;&#21407;&#29702;&#65289;&#65292;&#22240;&#27492;&#20250;&#20986;&#29616;&#24613;&#21095;&#19979;&#38477;&#30340;&#29616;&#35937;&#12290;&#22312;&#20998;&#26512;&#25439;&#22833;&#23792;&#20540;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;$\lambda_{\mathrm{max}}$&#24179;&#22374;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23545;&#20110;&#23454;&#38469;&#25968;&#25454;&#38598;&#65292;&#20302;&#39057;&#24448;&#24448;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#24182;&#19988;&#21487;&#20197;&#34987;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#25152;&#24456;&#22909;&#22320;&#25429;&#33719;&#12290;&#22240;&#27492;&#65292;&#20855;&#26377;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#21644;&#20855;&#26377;&#21155;&#36136;&#27867;&#21270;&#33021;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#37117;&#21487;&#20197;&#24456;&#22909;&#22320;&#23398;&#20064;&#20302;&#39057;&#25104;&#20998;&#65292;&#22240;&#27492;&#23427;&#20204;&#22312;&#25439;&#22833;&#20989;&#25968;&#19978;&#30340;&#24615;&#36136;&#30456;&#20284;&#12290;&#20294;&#26159;&#65292;&#21155;&#36136;&#27867;&#21270;&#33021;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#20250;&#36807;&#24230;&#25311;&#21512;&#39640;&#39057;&#25104;&#20998;&#65292;&#32780;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#26356;&#24179;&#28369;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study the mechanism underlying loss spikes observed during neural network training. When the training enters a region, which has a smaller-loss-as-sharper (SLAS) structure, the training becomes unstable and loss exponentially increases once it is too sharp, i.e., the rapid ascent of the loss spike. The training becomes stable when it finds a flat region. The deviation in the first eigen direction (with maximum eigenvalue of the loss Hessian ($\lambda_{\mathrm{max}}$) is found to be dominated by low-frequency. Since low-frequency is captured very fast (frequency principle), the rapid descent is then observed. Inspired by our analysis of loss spikes, we revisit the link between $\lambda_{\mathrm{max}}$ flatness and generalization. For real datasets, low-frequency is often dominant and well-captured by both the training data and the test data. Then, a solution with good generalization and a solution with bad generalization can both learn low-frequency well, thus, they hav
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#22312;&#24046;&#20998;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#20013;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#20844;&#20849;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#38544;&#31169;&#21644;&#25928;&#29992;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#20998;&#24067;&#21305;&#37197;&#31639;&#27861;&#25552;&#39640;&#20844;&#20849;&#25968;&#25454;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#38544;&#31169;&#24615;&#65292;&#20026;&#35757;&#32451;&#31169;&#26377;&#27169;&#22411;&#25552;&#20379;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.12132</link><description>&lt;p&gt;
&#20844;&#20849;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#24110;&#21161;&#31169;&#26377;&#20132;&#21449;&#35774;&#22791;&#32852;&#37030;&#23398;&#20064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Public Large Language Models Help Private Cross-device Federated Learning?. (arXiv:2305.12132v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#22312;&#24046;&#20998;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#20013;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#20844;&#20849;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#38544;&#31169;&#21644;&#25928;&#29992;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#20998;&#24067;&#21305;&#37197;&#31639;&#27861;&#25552;&#39640;&#20844;&#20849;&#25968;&#25454;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#38544;&#31169;&#24615;&#65292;&#20026;&#35757;&#32451;&#31169;&#26377;&#27169;&#22411;&#25552;&#20379;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#65288;&#24046;&#20998;&#65289;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#20132;&#21449;&#35774;&#22791;FL&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#23545;&#36739;&#23567;&#65292;&#22312;&#35757;&#32451;&#20013;&#30340;&#22823;&#35268;&#27169;&#24182;&#34892;&#24615;&#21442;&#19982;&#19979;&#21487;&#20197;&#20351;&#29992;&#26377;&#24847;&#20041;&#30340;&#24418;&#24335;&#21270;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#20445;&#35777;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#36817;&#65292;&#20844;&#20849;&#25968;&#25454;&#24050;&#29992;&#20110;&#25913;&#21892;&#22823;&#22411;&#21644;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#21644;LLMs&#26469;&#24110;&#21161;&#35774;&#22791;&#19978;&#30340;FL&#27169;&#22411;&#36827;&#34892;&#24046;&#20998;&#31169;&#26377;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#33976;&#39311;&#25216;&#26415;&#36827;&#19968;&#27493;&#25913;&#21892;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#21305;&#37197;&#31639;&#27861;&#65292;&#36890;&#36807;&#29702;&#35770;&#22522;&#30784;&#23545;&#20844;&#20849;&#25968;&#25454;&#36827;&#34892;&#25509;&#36817;&#31169;&#26377;&#25968;&#25454;&#20998;&#24067;&#30340;&#37319;&#26679;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#65288;&#39044;&#65289;&#35757;&#32451;&#20844;&#20849;&#25968;&#25454;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#21033;&#29992;&#20844;&#20849;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#31169;&#26377;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study (differentially) private federated learning (FL) of language models. The language models in cross-device FL are relatively small, which can be trained with meaningful formal user-level differential privacy (DP) guarantees when massive parallelism in training is enabled by the participation of a moderate size of users. Recently, public data has been used to improve privacy-utility trade-offs for both large and small language models. In this work, we provide a systematic study of using large-scale public data and LLMs to help differentially private training of on-device FL models, and further improve the privacy-utility tradeoff by techniques of distillation. Moreover, we propose a novel distribution matching algorithm with theoretical grounding to sample public data close to private data distribution, which significantly improves the sample efficiency of (pre-)training on public data. The proposed method is efficient and effective for training private model by taking advantage 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#24847;&#26102;&#24310;&#30340;&#38750;&#31283;&#24577;&#22312;&#32447;&#20984;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;DOGD&#65292;&#24182;&#35777;&#26126;&#23427;&#33021;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#33719;&#24471;$O(\sqrt{dT}(P_T+1))$&#30340;&#21160;&#24577;&#36951;&#25022;&#30028;&#65292;&#21516;&#26102;&#24403;&#24310;&#36831;&#19981;&#25913;&#21464;&#26799;&#24230;&#21040;&#36798;&#39034;&#24207;&#26102;&#65292;&#33258;&#21160;&#23558;&#21160;&#24577;&#36951;&#25022;&#20943;&#23569;&#21040;$O(\sqrt{S}(1+P_T))$&#12290;</title><link>http://arxiv.org/abs/2305.12131</link><description>&lt;p&gt;
&#20219;&#24847;&#26102;&#24310;&#30340;&#38750;&#31283;&#24577;&#22312;&#32447;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Non-stationary Online Convex Optimization with Arbitrary Delays. (arXiv:2305.12131v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#24847;&#26102;&#24310;&#30340;&#38750;&#31283;&#24577;&#22312;&#32447;&#20984;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;DOGD&#65292;&#24182;&#35777;&#26126;&#23427;&#33021;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#33719;&#24471;$O(\sqrt{dT}(P_T+1))$&#30340;&#21160;&#24577;&#36951;&#25022;&#30028;&#65292;&#21516;&#26102;&#24403;&#24310;&#36831;&#19981;&#25913;&#21464;&#26799;&#24230;&#21040;&#36798;&#39034;&#24207;&#26102;&#65292;&#33258;&#21160;&#23558;&#21160;&#24577;&#36951;&#25022;&#20943;&#23569;&#21040;$O(\sqrt{S}(1+P_T))$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20197;&#26799;&#24230;&#25110;&#20854;&#20182;&#20989;&#25968;&#20449;&#24687;&#21487;&#20197;&#20219;&#24847;&#24310;&#36831;&#20026;&#29305;&#28857;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#65288;OCO&#65289;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#19982;&#20043;&#21069;&#30740;&#31350;&#31283;&#24577;&#29615;&#22659;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#31283;&#24577;&#29615;&#22659;&#19979;&#30340;&#24310;&#36831;OCO&#65292;&#24182;&#26088;&#22312;&#26368;&#23567;&#21270;&#19982;&#20219;&#20309;&#27604;&#36739;&#22120;&#24207;&#21015;&#30456;&#20851;&#30340;&#21160;&#24577;&#36951;&#25022;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#21363;DOGD&#65292;&#35813;&#31639;&#27861;&#26681;&#25454;&#20854;&#21040;&#36798;&#39034;&#24207;&#20026;&#27599;&#20010;&#24310;&#36831;&#26799;&#24230;&#25191;&#34892;&#28176;&#21464;&#19979;&#38477;&#27493;&#39588;&#12290;&#23613;&#31649;&#23427;&#24456;&#31616;&#21333;&#65292;&#20294;&#25105;&#20204;&#30340;&#26032;&#22411;&#20998;&#26512;&#34920;&#26126;&#65292;DOGD&#21487;&#20197;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#33719;&#24471;$O(\sqrt{dT}(P_T+1))$&#30340;&#21160;&#24577;&#36951;&#25022;&#30028;&#65292;&#20854;&#20013;$d$&#26159;&#26368;&#22823;&#24310;&#36831;&#65292;$T$&#26159;&#26102;&#38388;&#36328;&#24230;&#65292;$P_T$&#26159;&#27604;&#36739;&#22120;&#30340;&#36335;&#24452;&#38271;&#24230;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#24310;&#36831;&#19981;&#25913;&#21464;&#28176;&#21464;&#30340;&#21040;&#36798;&#39034;&#24207;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#23558;&#21160;&#24577;&#36951;&#25022;&#20943;&#23569;&#21040;$O(\sqrt{S}(1+P_T))$&#65292;&#20854;&#20013;$S$&#26159;&#24310;&#36831;&#20043;&#21644;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;DOGD&#25193;&#23637;&#20026;&#26356;&#36890;&#29992;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#23454;&#29616;&#20102;&#19982;DOGD&#30456;&#21516;&#30340;&#36951;&#25022;&#30028;&#12290;&#24191;&#27867;&#30340;&#27169;&#25311;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online convex optimization (OCO) with arbitrary delays, in which gradients or other information of functions could be arbitrarily delayed, has received increasing attention recently. Different from previous studies that focus on stationary environments, this paper investigates the delayed OCO in non-stationary environments, and aims to minimize the dynamic regret with respect to any sequence of comparators. To this end, we first propose a simple algorithm, namely DOGD, which performs a gradient descent step for each delayed gradient according to their arrival order. Despite its simplicity, our novel analysis shows that DOGD can attain an $O(\sqrt{dT}(P_T+1)$ dynamic regret bound in the worst case, where $d$ is the maximum delay, $T$ is the time horizon, and $P_T$ is the path length of comparators. More importantly, in case delays do not change the arrival order of gradients, it can automatically reduce the dynamic regret to $O(\sqrt{S}(1+P_T))$, where $S$ is the sum of delays. Furtherm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65288;MiniMoE&#65289;&#65292;&#36890;&#36807;&#22686;&#21152;&#23398;&#29983;&#30340;&#23481;&#37327;&#32780;&#19981;&#26126;&#26174;&#22686;&#21152;&#25512;&#29702;&#35745;&#31639;&#35299;&#38500;&#23481;&#37327;&#24046;&#24322;&#35781;&#21650;&#65292;&#24182;&#22312;GLUE&#21644;CoNLL&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.12129</link><description>&lt;p&gt;
&#35299;&#38500;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23481;&#37327;&#24046;&#24322;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
Lifting the Curse of Capacity Gap in Distilling Language Models. (arXiv:2305.12129v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65288;MiniMoE&#65289;&#65292;&#36890;&#36807;&#22686;&#21152;&#23398;&#29983;&#30340;&#23481;&#37327;&#32780;&#19981;&#26126;&#26174;&#22686;&#21152;&#25512;&#29702;&#35745;&#31639;&#35299;&#38500;&#23481;&#37327;&#24046;&#24322;&#35781;&#21650;&#65292;&#24182;&#22312;GLUE&#21644;CoNLL&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#19981;&#24184;&#30340;&#26159;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#25512;&#29702;&#35745;&#31639;&#12290;&#30693;&#35782;&#33976;&#39311;&#36890;&#36807;&#24072;&#29983;&#33539;&#24335;&#20026;&#23567;&#22411;&#27169;&#22411;&#21387;&#32553;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#24403;&#24072;&#29983;&#20043;&#38388;&#30340;&#23481;&#37327;&#24046;&#36317;&#24456;&#22823;&#26102;&#65292;&#23481;&#37327;&#24046;&#36317;&#35781;&#21650;&#20250;&#20986;&#29616;&#65292;&#23548;&#33268;&#33976;&#39311;&#35821;&#35328;&#27169;&#22411;&#19981;&#36275;&#12290;&#34429;&#28982;&#24050;&#26377;&#20960;&#39033;&#30740;&#31350;&#22635;&#34917;&#20102;&#36825;&#19968;&#24046;&#36317;&#65292;&#20294;&#35781;&#21650;&#20173;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#35299;&#20915;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#23398;&#29983;&#30340;&#23481;&#37327;&#32780;&#19981;&#26126;&#26174;&#22686;&#21152;&#25512;&#29702;&#35745;&#31639;&#26469;&#35299;&#38500;&#23481;&#37327;&#24046;&#24322;&#35781;&#21650;&#12290;&#21463;&#28151;&#21512;&#19987;&#23478;(Sparse Activation Regime of Mixture of Experts (MoE))&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#23567;&#19987;&#23478;(MiniMoE)&#30340;&#28151;&#21512;&#29289;&#65292;&#36825;&#20026;&#23398;&#29983;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#21442;&#25968;&#65292;&#20294;&#20960;&#20046;&#27809;&#26377;&#24341;&#20837;&#20219;&#20309;&#39069;&#22806;&#30340;&#25512;&#29702;&#35745;&#31639;&#12290;&#22312;GLUE&#21644;CoNLL&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MiniMoE&#30340;&#39764;&#21147;&#28040;&#38500;&#20102;&#23481;&#37327;&#24046;&#36317;&#35781;&#21650;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models (LMs) have shown compelling performance on various downstream tasks, but unfortunately they require a tremendous amount of inference compute. Knowledge distillation finds a path to compress LMs to small ones with a teacher-student paradigm. However, when the capacity gap between the teacher and the student is large, a curse of capacity gap appears, invoking a deficiency in distilling LMs. While a few studies have been carried out to fill the gap, the curse is not yet well tackled. In this paper, we aim at lifting the curse of capacity gap via enlarging the capacity of the student without notably increasing the inference compute. Largely motivated by sparse activation regime of mixture of experts (MoE), we propose a mixture of minimal experts (MiniMoE), which imposes extra parameters to the student but introduces almost no additional inference compute. Experimental results on GLUE and CoNLL demonstrate the curse of capacity gap is lifted by the magic of MiniMo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#23450;&#19968;&#33268;&#35757;&#32451;&#26694;&#26550;&#65292;&#20854;&#26680;&#24515;&#22312;&#20110;&#32467;&#21512;&#20102;&#26631;&#20934;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#26799;&#24230;&#35009;&#21098;&#26041;&#27861;&#20197;&#21450;&#20351;&#29992;&#33258;&#21160;&#31283;&#23450;&#30340;&#21387;&#32553;&#28608;&#27963;&#20989;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#21644;&#35757;&#32451;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.12125</link><description>&lt;p&gt;
&#28145;&#24230;&#21069;&#39304;&#32593;&#32476;&#30340;&#31283;&#23450;&#19968;&#33268;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Provably Stable and Consistent Training of Deep Feedforward Networks. (arXiv:2305.12125v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12125
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#23450;&#19968;&#33268;&#35757;&#32451;&#26694;&#26550;&#65292;&#20854;&#26680;&#24515;&#22312;&#20110;&#32467;&#21512;&#20102;&#26631;&#20934;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#26799;&#24230;&#35009;&#21098;&#26041;&#27861;&#20197;&#21450;&#20351;&#29992;&#33258;&#21160;&#31283;&#23450;&#30340;&#21387;&#32553;&#28608;&#27963;&#20989;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#21644;&#35757;&#32451;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#30417;&#30563;&#65288;&#20998;&#31867;&#21644;&#22238;&#24402;&#65289;&#21644;&#38750;&#30417;&#30563;&#65288;&#24378;&#21270;&#23398;&#20064;&#65289;&#22330;&#26223;&#19979;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;&#26631;&#20934;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#26799;&#24230;&#35009;&#21098;&#26041;&#27861;&#12290;&#36755;&#20986;&#23618;&#20351;&#29992;&#35009;&#21098;&#26799;&#24230;&#26356;&#26032;&#65292;&#20854;&#20313;&#31070;&#32463;&#32593;&#32476;&#20351;&#29992;&#26631;&#20934;&#26799;&#24230;&#26356;&#26032;&#12290;&#20351;&#29992;&#35009;&#21098;&#26799;&#24230;&#26356;&#26032;&#36755;&#20986;&#23618;&#21487;&#20197;&#20351;&#20854;&#31283;&#23450;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#21482;&#35201;&#31070;&#32463;&#32593;&#32476;&#20165;&#30001;&#21387;&#32553;&#65288;&#32039;&#20945;&#33539;&#22260;&#65289;&#28608;&#27963;&#20989;&#25968;&#32452;&#25104;&#65292;&#20854;&#20313;&#23618;&#23558;&#33258;&#21160;&#34987;&#31283;&#23450;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21387;&#32553;&#28608;&#27963;&#20989;&#25968; - &#36890;&#36807;&#20462;&#25913;&#39640;&#26031;&#35823;&#24046;&#32447;&#24615;&#21333;&#20803;&#65288;GELU&#65289;&#26469;&#33719;&#24471; - &#25105;&#20204;&#31216;&#20043;&#20026;Truncated GELU&#65288;tGELU&#65289;&#12290;&#19982;&#20854;&#20182;&#21387;&#32553;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;Sigmoid&#65289;&#19981;&#21516;&#65292;tGELU&#30340;&#33539;&#22260;&#21487;&#20197;&#26126;&#30830;&#25351;&#23450;&#12290;&#22240;&#27492;&#65292;&#22312;&#19968;&#20123;&#28608;&#27963;&#20989;&#25968;&#65288;&#20363;&#22914;Sigmod&#65289;&#30340;&#33539;&#22260;&#36739;&#23567;&#26102;&#20250;&#20986;&#29616;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#21487;&#20197;&#36991;&#20813;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel algorithm for training deep neural networks in supervised (classification and regression) and unsupervised (reinforcement learning) scenarios. This algorithm combines the standard stochastic gradient descent and the gradient clipping method. The output layer is updated using clipped gradients, the rest of the neural network is updated using standard gradients. Updating the output layer using clipped gradient stabilizes it. We show that the remaining layers are automatically stabilized provided the neural network is only composed of squashing (compact range) activations. We also present a novel squashing activation function - it is obtained by modifying a Gaussian Error Linear Unit (GELU) to have compact range - we call it Truncated GELU (tGELU). Unlike other squashing activations, such as sigmoid, the range of tGELU can be explicitly specified. As a consequence, the problem of vanishing gradients that arise due to a small range, e.g., in the case of a sigmoid activat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#24863;&#30693;&#35828;&#35805;&#20154;&#23884;&#20837;&#25552;&#21462;&#22120;&#65292;&#20351;&#29992;&#19981;&#23545;&#31216;&#20132;&#21449;&#27880;&#24847;&#21147;(ACA)&#20195;&#26367;&#26102;&#38388;&#27744;&#21270;&#65292;&#20855;&#26377;&#39640;&#25928;&#30340;&#20840;&#23616;&#29305;&#24449;&#25552;&#21462;&#21644;&#36866;&#24212;&#26102;&#38388;&#21464;&#21270;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.12121</link><description>&lt;p&gt;
ACA-Net: &#37319;&#29992;&#19981;&#23545;&#31216;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#36731;&#37327;&#32423;&#35828;&#35805;&#20154;&#39564;&#35777;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ACA-Net: Towards Lightweight Speaker Verification using Asymmetric Cross Attention. (arXiv:2305.12121v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#24863;&#30693;&#35828;&#35805;&#20154;&#23884;&#20837;&#25552;&#21462;&#22120;&#65292;&#20351;&#29992;&#19981;&#23545;&#31216;&#20132;&#21449;&#27880;&#24847;&#21147;(ACA)&#20195;&#26367;&#26102;&#38388;&#27744;&#21270;&#65292;&#20855;&#26377;&#39640;&#25928;&#30340;&#20840;&#23616;&#29305;&#24449;&#25552;&#21462;&#21644;&#36866;&#24212;&#26102;&#38388;&#21464;&#21270;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;ACA-Net&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#24863;&#30693;&#35828;&#35805;&#20154;&#23884;&#20837;&#25552;&#21462;&#22120;&#65292;&#29992;&#20110;&#35828;&#35805;&#20154;&#39564;&#35777;&#12290;ACA-Net&#20351;&#29992;&#19981;&#23545;&#31216;&#20132;&#21449;&#27880;&#24847;&#21147;(ACA)&#20195;&#26367;&#26102;&#38388;&#27744;&#21270;&#65292;&#23558;&#22823;&#30340;&#21464;&#38271;&#24207;&#21015;&#21387;&#32553;&#25104;&#23567;&#30340;&#22266;&#23450;&#22823;&#23567;&#30340;&#21521;&#37327;&#12290;&#22312;ACA-Net&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#23618;&#27719;&#32858;&#22359;&#65292;&#20351;&#29992;ACA&#20174;&#21464;&#38271;&#24230;&#36755;&#20837;&#20013;&#29983;&#25104;&#22266;&#23450;&#38271;&#24230;&#30340;&#36523;&#20221;&#21521;&#37327;&#12290;&#19982;&#29616;&#26377;&#30340;SV&#27169;&#22411;&#19981;&#21516;&#65292;ACA-Net&#36890;&#36807;&#20840;&#23616;&#27880;&#24847;&#21147;&#20316;&#20026;&#26377;&#25928;&#30340;&#20840;&#23616;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#36866;&#24212;&#26102;&#38388;&#21464;&#21270;&#32780;&#19981;&#26159;&#24212;&#29992;&#20110;&#26102;&#38388;&#32500;&#24230;&#30340;&#22266;&#23450;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;WSJ0-1talker&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;1/5&#30340;&#21442;&#25968;&#65292;ACA-Net&#22312;EER&#26041;&#38754;&#30456;&#23545;&#20110;&#24378;&#22522;&#32447;&#25552;&#39640;&#20102;5%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose ACA-Net, a lightweight, global context-aware speaker embedding extractor for Speaker Verification (SV) that improves upon existing work by using Asymmetric Cross Attention (ACA) to replace temporal pooling. ACA is able to distill large, variable-length sequences into small, fixed-sized latents by attending a small query to large key and value matrices. In ACA-Net, we build a Multi-Layer Aggregation (MLA) block using ACA to generate fixed-sized identity vectors from variable-length inputs. Through global attention, ACA-Net acts as an efficient global feature extractor that adapts to temporal variability unlike existing SV models that apply a fixed function for pooling over the temporal dimension which may obscure information about the signal's non-stationary temporal variability. Our experiments on the WSJ0-1talker show ACA-Net outperforms a strong baseline by 5\% relative improvement in EER using only 1/5 of the parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36864;&#28779;&#33258;&#33976;&#39311;&#26657;&#27491;(ADR)&#26041;&#27861;&#65292;&#20854;&#33021;&#29983;&#25104;&#36719;&#26631;&#31614;&#29992;&#20316;&#26356;&#22909;&#30340;&#25351;&#23548;&#26426;&#21046;&#65292;&#20934;&#30830;&#21453;&#26144;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#25915;&#20987;&#19979;&#30340;&#20998;&#24067;&#21464;&#21270;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#24179;&#28369;&#30340;&#25554;&#20837;&#24615;&#25972;&#21512;&#21040;&#20854;&#20182;&#23545;&#25239;&#24615;&#35757;&#32451;&#25216;&#26415;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.12118</link><description>&lt;p&gt;
&#36864;&#28779;&#33258;&#33976;&#39311;&#26657;&#27491;&#25913;&#36827;&#20102;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Annealing Self-Distillation Rectification Improves Adversarial Training. (arXiv:2305.12118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36864;&#28779;&#33258;&#33976;&#39311;&#26657;&#27491;(ADR)&#26041;&#27861;&#65292;&#20854;&#33021;&#29983;&#25104;&#36719;&#26631;&#31614;&#29992;&#20316;&#26356;&#22909;&#30340;&#25351;&#23548;&#26426;&#21046;&#65292;&#20934;&#30830;&#21453;&#26144;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#25915;&#20987;&#19979;&#30340;&#20998;&#24067;&#21464;&#21270;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#24179;&#28369;&#30340;&#25554;&#20837;&#24615;&#25972;&#21512;&#21040;&#20854;&#20182;&#23545;&#25239;&#24615;&#35757;&#32451;&#25216;&#26415;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#23545;&#25239;&#35757;&#32451;&#20013;&#65292;&#27169;&#22411;&#34987;&#20248;&#21270;&#20197;&#36866;&#24212;&#21487;&#25509;&#21463;&#30340;&#23545;&#25239;&#25200;&#21160;&#39044;&#31639;&#20869;&#30340;&#19968;&#28909;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#24573;&#30053;&#30001;&#25200;&#21160;&#24102;&#26469;&#30340;&#22522;&#30784;&#20998;&#24067;&#21464;&#21270;&#65292;&#23548;&#33268;&#20102;&#24378;&#20581;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22686;&#24378;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#24378;&#20581;&#27169;&#22411;&#30340;&#29305;&#24449;&#65292;&#24182;&#30830;&#23450;&#24378;&#20581;&#27169;&#22411;&#20542;&#21521;&#20110;&#29983;&#25104;&#26356;&#24179;&#28369;&#21644;&#26356;&#33391;&#22909;&#26657;&#20934;&#30340;&#36755;&#20986;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#27979;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#8212;&#8212;&#36864;&#28779;&#33258;&#33976;&#39311;&#26657;&#27491;(ADR)&#65292;&#35813;&#26041;&#27861;&#29983;&#25104;&#36719;&#26631;&#31614;&#20316;&#20026;&#26356;&#22909;&#30340;&#25351;&#23548;&#26426;&#21046;&#65292;&#33021;&#20934;&#30830;&#21453;&#26144;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#25915;&#20987;&#19979;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;ADR&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#20462;&#27491;&#30340;&#20998;&#24067;&#65292;&#26174;&#33879;&#25913;&#21892;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#19981;&#38656;&#35201;&#39044;&#35757;&#32451;&#27169;&#22411;&#25110;&#39069;&#22806;&#30340;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26367;&#25442;&#21367;&#31215;&#23618;&#20197;&#23454;&#29616;&#24179;&#28369;&#30340;&#25554;&#20837;&#24615;&#25972;&#21512;&#21040;&#20854;&#20182;&#23545;&#25239;&#24615;&#35757;&#32451;&#25216;&#26415;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In standard adversarial training, models are optimized to fit one-hot labels within allowable adversarial perturbation budgets. However, the ignorance of underlying distribution shifts brought by perturbations causes the problem of robust overfitting. To address this issue and enhance adversarial robustness, we analyze the characteristics of robust models and identify that robust models tend to produce smoother and well-calibrated outputs. Based on the observation, we propose a simple yet effective method, Annealing Self-Distillation Rectification (ADR), which generates soft labels as a better guidance mechanism that accurately reflects the distribution shift under attack during adversarial training. By utilizing ADR, we can obtain rectified distributions that significantly improve model robustness without the need for pre-trained models or extensive extra computation. Moreover, our method facilitates seamless plug-and-play integration with other adversarial training techniques by repl
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#25512;&#29702;&#30340;&#39063;&#31890;&#32858;&#31867;&#31639;&#27861;GFDC&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20811;&#26381;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#22312;&#34913;&#37327;&#20840;&#23616;&#23494;&#24230;&#12289;&#30830;&#23450;&#21512;&#29702;&#30340;&#32858;&#31867;&#20013;&#24515;&#25110;&#32467;&#26500;&#12289;&#20934;&#30830;&#22320;&#20998;&#37197;&#26679;&#26412;&#20197;&#21450;&#22788;&#29702;&#20855;&#26377;&#22823;&#23494;&#24230;&#24046;&#24322;&#30340;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#30340;&#32570;&#28857;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;GFDC &#37319;&#29992;&#20102;&#19977;&#31181;&#26032;&#39062;&#30340;&#39063;&#31890;&#34701;&#21512;&#31574;&#30053;&#26469;&#23558;&#39063;&#31890;&#32452;&#21512;&#25104;&#31283;&#23450;&#30340;&#32858;&#31867;&#32467;&#26500;&#65292;&#26377;&#21161;&#20110;&#26816;&#27979;&#20855;&#26377;&#20219;&#24847;&#24418;&#29366;&#30340;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2305.12114</link><description>&lt;p&gt;
GFDC&#65306;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#25512;&#29702;&#30340;&#39063;&#31890;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
GFDC: A Granule Fusion Density-Based Clustering with Evidential Reasoning. (arXiv:2305.12114v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12114
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#25512;&#29702;&#30340;&#39063;&#31890;&#32858;&#31867;&#31639;&#27861;GFDC&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20811;&#26381;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#22312;&#34913;&#37327;&#20840;&#23616;&#23494;&#24230;&#12289;&#30830;&#23450;&#21512;&#29702;&#30340;&#32858;&#31867;&#20013;&#24515;&#25110;&#32467;&#26500;&#12289;&#20934;&#30830;&#22320;&#20998;&#37197;&#26679;&#26412;&#20197;&#21450;&#22788;&#29702;&#20855;&#26377;&#22823;&#23494;&#24230;&#24046;&#24322;&#30340;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#30340;&#32570;&#28857;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;GFDC &#37319;&#29992;&#20102;&#19977;&#31181;&#26032;&#39062;&#30340;&#39063;&#31890;&#34701;&#21512;&#31574;&#30053;&#26469;&#23558;&#39063;&#31890;&#32452;&#21512;&#25104;&#31283;&#23450;&#30340;&#32858;&#31867;&#32467;&#26500;&#65292;&#26377;&#21161;&#20110;&#26816;&#27979;&#20855;&#26377;&#20219;&#24847;&#24418;&#29366;&#30340;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#22240;&#20854;&#33021;&#22815;&#26816;&#27979;&#20855;&#26377;&#20219;&#24847;&#24418;&#29366;&#30340;&#31867;&#32676;&#32780;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#34913;&#37327;&#20840;&#23616;&#23494;&#24230;&#12289;&#30830;&#23450;&#21512;&#29702;&#30340;&#32858;&#31867;&#20013;&#24515;&#25110;&#32467;&#26500;&#12289;&#20934;&#30830;&#22320;&#20998;&#37197;&#26679;&#26412;&#20197;&#21450;&#22788;&#29702;&#20855;&#26377;&#22823;&#23494;&#24230;&#24046;&#24322;&#30340;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#20811;&#26381;&#23427;&#20204;&#30340;&#32570;&#28857;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#25512;&#29702;&#30340;&#39063;&#31890;&#32858;&#31867;&#31639;&#27861;&#65288;GFDC&#65289;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#19968;&#31181;&#31232;&#30095;&#24230;&#37327;&#25351;&#26631;&#26469;&#27979;&#37327;&#26679;&#26412;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#23494;&#24230;&#12290;&#28982;&#21518;&#22312;&#39640;&#23494;&#24230;&#21644;&#20302;&#23494;&#24230;&#21306;&#22495;&#20013;&#29983;&#25104;&#20449;&#24687;&#39063;&#31890;&#65292;&#24110;&#21161;&#22788;&#29702;&#20855;&#26377;&#26174;&#33879;&#23494;&#24230;&#24046;&#24322;&#30340;&#32858;&#31867;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#20102;&#19977;&#31181;&#26032;&#39062;&#30340;&#39063;&#31890;&#34701;&#21512;&#31574;&#30053;&#26469;&#23558;&#39063;&#31890;&#32452;&#21512;&#25104;&#31283;&#23450;&#30340;&#32858;&#31867;&#32467;&#26500;&#65292;&#26377;&#21161;&#20110;&#26816;&#27979;&#20855;&#26377;&#20219;&#24847;&#24418;&#29366;&#30340;&#32858;&#31867;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#22522;&#20110;Dempster-Shafer&#29702;&#35770;&#24320;&#21457;&#30340;&#20998;&#37197;&#26041;&#27861;&#26469;&#20998;&#37197;&#19981;&#31283;&#23450;&#30340;&#26679;&#26412;&#12290;&#20351;&#29992;GFDC&#21518;&#65292;&#21487;&#20197;&#24471;&#21040;&#21512;&#29702;&#30340;&#32858;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, density-based clustering algorithms are widely applied because they can detect clusters with arbitrary shapes. However, they perform poorly in measuring global density, determining reasonable cluster centers or structures, assigning samples accurately and handling data with large density differences among clusters. To overcome their drawbacks, this paper proposes a granule fusion density-based clustering with evidential reasoning (GFDC). Both local and global densities of samples are measured by a sparse degree metric first. Then information granules are generated in high-density and low-density regions, assisting in processing clusters with significant density differences. Further, three novel granule fusion strategies are utilized to combine granules into stable cluster structures, helping to detect clusters with arbitrary shapes. Finally, by an assignment method developed from Dempster-Shafer theory, unstable samples are assigned. After using GFDC, a reasonable clustering
&lt;/p&gt;</description></item><item><title>&#20803;&#31070;&#32463;&#21327;&#35843;&#26159;&#20026;&#20102;&#35299;&#20915;&#20803;&#23398;&#20064;&#20013;&#22914;&#20309;&#34920;&#31034;&#21644;&#25512;&#29702;&#20854;&#20182;&#23398;&#20064;&#31639;&#27861;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#25805;&#20316;&#21644;&#34920;&#29616;&#20197;&#21450;&#20256;&#32479;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#32780;&#25552;&#20986;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.12109</link><description>&lt;p&gt;
&#20803;&#31070;&#32463;&#21327;&#35843;
&lt;/p&gt;
&lt;p&gt;
Meta Neural Coordination. (arXiv:2305.12109v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12109
&lt;/p&gt;
&lt;p&gt;
&#20803;&#31070;&#32463;&#21327;&#35843;&#26159;&#20026;&#20102;&#35299;&#20915;&#20803;&#23398;&#20064;&#20013;&#22914;&#20309;&#34920;&#31034;&#21644;&#25512;&#29702;&#20854;&#20182;&#23398;&#20064;&#31639;&#27861;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#25805;&#20316;&#21644;&#34920;&#29616;&#20197;&#21450;&#20256;&#32479;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#32780;&#25552;&#20986;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#26088;&#22312;&#24320;&#21457;&#33021;&#22815;&#20174;&#20854;&#20182;&#23398;&#20064;&#31639;&#27861;&#20013;&#23398;&#20064;&#20197;&#36866;&#24212;&#26032;&#30340;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#30340;&#31639;&#27861;&#12290;&#36825;&#38656;&#35201;&#27169;&#22411;&#26469;&#25551;&#36848;&#20854;&#20182;&#23398;&#20064;&#31639;&#27861;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#25805;&#20316;&#21644;&#34920;&#29616;&#65292;&#31867;&#20284;&#20110;&#22312;&#24515;&#29702;&#29702;&#35770;&#20013;&#34920;&#36798;&#21644;&#25512;&#29702;&#24515;&#29702;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#20984;&#26174;&#20102;&#19990;&#30028;&#30340;&#23616;&#37096;&#21487;&#39044;&#30693;&#24615;&#65292;&#38656;&#35201;&#23545;&#22810;&#20010;&#39044;&#27979;&#21516;&#26102;&#36827;&#34892;&#34920;&#31034;&#12290;&#31070;&#32463;&#27169;&#22359;&#20043;&#38388;&#30340;&#21327;&#35843;&#20419;&#36827;&#20102;&#19981;&#21516;&#30340;&#27169;&#22359;&#20043;&#38388;&#30456;&#20114;&#35748;&#30693;&#20854;&#20449;&#24565;&#21644;&#28212;&#26395;&#30340;&#34920;&#31034;&#12290;&#27169;&#22359;&#21270;&#21644;&#20998;&#25955;&#30340;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#31070;&#32463;&#21327;&#35843;&#26159;&#26500;&#24314;&#33021;&#22815;&#28789;&#27963;&#21644;&#36866;&#24212;&#24615;&#22320;&#20132;&#20114;&#30340;&#33258;&#20027;&#26234;&#33021;&#26426;&#22120;&#30340;&#22522;&#26412;&#20808;&#20915;&#26465;&#20214;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#20010;&#35777;&#25454;&#65292;&#35777;&#26126;&#19968;&#31181;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#30340;&#26032;&#36884;&#24452;&#65292;&#31216;&#20026;&#20803;&#31070;&#32463;&#21327;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-learning aims to develop algorithms that can learn from other learning algorithms to adapt to new and changing environments. This requires a model of how other learning algorithms operate and perform in different contexts, which is similar to representing and reasoning about mental states in the theory of mind. Furthermore, the problem of uncertainty in the predictions of conventional deep neural networks highlights the partial predictability of the world, requiring the representation of multiple predictions simultaneously. This is facilitated by coordination among neural modules, where different modules' beliefs and desires are attributed to others. The neural coordination among modular and decentralized neural networks is a fundamental prerequisite for building autonomous intelligence machines that can interact flexibly and adaptively. In this work, several pieces of evidence demonstrate a new avenue for tackling the problems above, termed Meta Neural Coordination. We discuss th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#29305;&#24449;&#22797;&#29992;&#8221;&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#21333;&#19968;&#30340;&#34920;&#31034;&#31354;&#38388; &#33021;&#22815;&#39640;&#25928;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#29305;&#24449;&#23884;&#20837;&#65292;&#21516;&#26102;&#21306;&#20998;&#19981;&#21516;&#30340;&#20998;&#31867;&#29305;&#24449;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#26032;&#25968;&#25454;&#38598;&#8220;Web-Available Image Search (WAIS)&#8221;&#19978;&#30340;&#27979;&#35797;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.12102</link><description>&lt;p&gt;
&#32479;&#19968;&#23884;&#20837;&#65306;&#38754;&#21521; Web &#35268;&#27169; ML &#31995;&#32479;&#30340;&#32463;&#36807;&#39564;&#35777;&#30340;&#29305;&#24449;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Unified Embedding: Battle-Tested Feature Representations for Web-Scale ML Systems. (arXiv:2305.12102v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#29305;&#24449;&#22797;&#29992;&#8221;&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#21333;&#19968;&#30340;&#34920;&#31034;&#31354;&#38388; &#33021;&#22815;&#39640;&#25928;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#29305;&#24449;&#23884;&#20837;&#65292;&#21516;&#26102;&#21306;&#20998;&#19981;&#21516;&#30340;&#20998;&#31867;&#29305;&#24449;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#26032;&#25968;&#25454;&#38598;&#8220;Web-Available Image Search (WAIS)&#8221;&#19978;&#30340;&#27979;&#35797;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#12289;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#29305;&#24449;&#23884;&#20837;&#23545;&#20110; Web &#35268;&#27169;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#26631;&#20934;&#26041;&#27861;&#26159;&#23558;&#27599;&#20010;&#29305;&#24449;&#20540;&#34920;&#31034;&#20026;&#19968;&#20010; d &#32500;&#23884;&#20837;&#65292;&#24341;&#20837;&#25968;&#30334;&#20159;&#20010;&#21442;&#25968;&#65292;&#32780;&#36825;&#20123;&#29305;&#24449;&#30340;&#22522;&#25968;&#38750;&#24120;&#39640;&#12290;&#36825;&#20010;&#29942;&#39048;&#23548;&#33268;&#20102;&#22791;&#36873;&#23884;&#20837;&#31639;&#27861;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21363;&#8220;&#29305;&#24449;&#22797;&#29992;&#8221;&#65292;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#20998;&#31867;&#29305;&#24449;&#20043;&#38388;&#20351;&#29992;&#19968;&#20010;&#21333;&#19968;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#22797;&#29992;&#30340;&#23884;&#20837;&#21487;&#20197;&#20998;&#35299;&#20026;&#27599;&#20010;&#32452;&#25104;&#29305;&#24449;&#30340;&#32452;&#20214;&#65292;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#21306;&#20998;&#29305;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22797;&#29992;&#30340;&#23884;&#20837;&#22312;&#20960;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;Web-Available Image Search (WAIS)&#8221;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#20197;&#20005;&#26684;&#35780;&#20272; Web &#35268;&#27169;&#19979;&#30340;&#26032;&#23884;&#20837;&#31639;&#27861;&#12290;&#25105;&#20204;&#36992;&#35831;&#31038;&#21306;&#36890;&#36807;&#25552;&#20986;&#21487;&#20197;&#20934;&#30830;&#12289;&#39640;&#25928;&#22320;&#23558;&#25968;&#30334;&#19975;&#24352;&#22270;&#20687;&#23884;&#20837;&#21644;&#20998;&#31867;&#21040;&#25104;&#21315;&#19978;&#19975;&#20010;&#31867;&#21035;&#30340;&#26032;&#27169;&#22411;&#26469;&#36129;&#29486; WAIS &#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning high-quality feature embeddings efficiently and effectively is critical for the performance of web-scale machine learning systems. A typical model ingests hundreds of features with vocabularies on the order of millions to billions of tokens. The standard approach is to represent each feature value as a d-dimensional embedding, introducing hundreds of billions of parameters for extremely high-cardinality features. This bottleneck has led to substantial progress in alternative embedding algorithms. Many of these methods, however, make the assumption that each feature uses an independent embedding table. This work introduces a simple yet highly effective framework, Feature Multiplexing, where one single representation space is used across many different categorical features. Our theoretical and empirical analysis reveals that multiplexed embeddings can be decomposed into components from each constituent feature, allowing models to distinguish between features. We show that multip
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;ERM&#35757;&#32451;&#27169;&#22411;&#23545;&#25239;&#24378;&#22823;&#40657;&#30418;&#25915;&#20987;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#25351;&#26631;&#37327;&#21270;&#27169;&#22411;&#23433;&#20840;&#24615;&#65306;&#21333;&#20010;&#26679;&#26412;&#30340;&#31283;&#23450;&#24615;&#21644;&#26597;&#35810;&#19982;&#21407;&#22987;&#25968;&#25454;&#29305;&#24449;&#30340;&#23545;&#40784;&#12290;&#22312;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#30740;&#31350;RF&#21644;NTK&#22238;&#24402;&#65292;&#35777;&#26126;&#38543;&#30528;&#27867;&#21270;&#33021;&#21147;&#30340;&#25552;&#39640;&#65292;&#38544;&#31169;&#20445;&#25252;&#21487;&#20197;&#24471;&#21040;&#21152;&#24378;&#12290;</title><link>http://arxiv.org/abs/2305.12100</link><description>&lt;p&gt;
&#31283;&#23450;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#65306;&#23545;&#20110;&#38543;&#26426;&#29305;&#24449;&#21644;NTK&#29305;&#24449;&#30340;&#31934;&#30830;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Stability, Generalization and Privacy: Precise Analysis for Random and NTK Features. (arXiv:2305.12100v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;ERM&#35757;&#32451;&#27169;&#22411;&#23545;&#25239;&#24378;&#22823;&#40657;&#30418;&#25915;&#20987;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#25351;&#26631;&#37327;&#21270;&#27169;&#22411;&#23433;&#20840;&#24615;&#65306;&#21333;&#20010;&#26679;&#26412;&#30340;&#31283;&#23450;&#24615;&#21644;&#26597;&#35810;&#19982;&#21407;&#22987;&#25968;&#25454;&#29305;&#24449;&#30340;&#23545;&#40784;&#12290;&#22312;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#30740;&#31350;RF&#21644;NTK&#22238;&#24402;&#65292;&#35777;&#26126;&#38543;&#30528;&#27867;&#21270;&#33021;&#21147;&#30340;&#25552;&#39640;&#65292;&#38544;&#31169;&#20445;&#25252;&#21487;&#20197;&#24471;&#21040;&#21152;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#24674;&#22797;&#25915;&#20987;&#65292;&#24341;&#36215;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#30340;&#25285;&#24551;&#12290;&#38024;&#23545;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#31561;&#24120;&#35265;&#31639;&#27861;&#36890;&#24120;&#19981;&#33021;&#30452;&#25509;&#23454;&#26045;&#23433;&#20840;&#20445;&#38556;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;ERM&#35757;&#32451;&#27169;&#22411;&#23545;&#25239;&#29305;&#23450;&#24378;&#22823;&#40657;&#30418;&#23376;&#25915;&#20987;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36890;&#36807;&#20004;&#20010;&#30475;&#20284;&#19981;&#21516;&#20294;&#26377;&#32852;&#31995;&#30340;&#25351;&#26631;&#26469;&#37327;&#21270;&#27169;&#22411;&#23433;&#20840;&#24615;&#65306;&#19968;&#26159;&#30456;&#23545;&#20110;&#21333;&#20010;&#35757;&#32451;&#26679;&#26412;&#30340;&#27169;&#22411;&#31283;&#23450;&#24615;&#65292;&#21478;&#19968;&#20010;&#26159;&#25915;&#20987;&#26597;&#35810;&#21644;&#21407;&#22987;&#25968;&#25454;&#29305;&#24449;&#30340;&#29305;&#24449;&#23545;&#40784;&#12290;&#34429;&#28982;&#21069;&#32773;&#22312;&#23398;&#20064;&#29702;&#35770;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#38416;&#36848;&#65292;&#24182;&#19982;&#32463;&#20856;&#24037;&#20316;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#30456;&#20851;&#65292;&#20294;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#31532;&#20108;&#31181;&#29305;&#24615;&#26159;&#26032;&#39062;&#30340;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#25216;&#26415;&#32467;&#26524;&#20026;&#20004;&#31181;&#21407;&#22411;&#35774;&#32622;&#25552;&#20379;&#20102;&#29305;&#24449;&#23545;&#40784;&#30340;&#31934;&#30830;&#21051;&#30011;&#65306;&#38543;&#26426;&#29305;&#24449;&#65288;RF&#65289;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#22238;&#24402;&#12290;&#36825;&#35777;&#26126;&#65292;&#38543;&#30528;&#27867;&#21270;&#33021;&#21147;&#30340;&#25552;&#39640;&#65292;&#38544;&#31169;&#20445;&#25252;&#33021;&#22815;&#24471;&#21040;&#21152;&#24378;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20854;&#20182;&#26377;&#36259;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models can be vulnerable to recovery attacks, raising privacy concerns to users, and widespread algorithms such as empirical risk minimization (ERM) often do not directly enforce safety guarantees. In this paper, we study the safety of ERM-trained models against a family of powerful black-box attacks. Our analysis quantifies this safety via two separate terms: (i) the model stability with respect to individual training samples, and (ii) the feature alignment between the attacker query and the original data. While the first term is well established in learning theory and it is connected to the generalization error in classical work, the second one is, to the best of our knowledge, novel. Our key technical result provides a precise characterization of the feature alignment for the two prototypical settings of random features (RF) and neural tangent kernel (NTK) regression. This proves that privacy strengthens with an increase in the generalization capability, unveiling also
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36947;&#23545;&#40784;&#40065;&#26834;&#21452;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21452;Transformer&#32467;&#26500;&#21644;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#30340;&#24341;&#20837;&#65292;&#35299;&#20915;&#20102;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20851;&#38190;&#32570;&#28857;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.12095</link><description>&lt;p&gt;
&#20351;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#20877;&#27425;&#21331;&#36234;&#65306;&#36890;&#36947;&#23545;&#40784;&#40065;&#26834;&#21452;Transformer
&lt;/p&gt;
&lt;p&gt;
Make Transformer Great Again for Time Series Forecasting: Channel Aligned Robust Dual Transformer. (arXiv:2305.12095v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36947;&#23545;&#40784;&#40065;&#26834;&#21452;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21452;Transformer&#32467;&#26500;&#21644;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#30340;&#24341;&#20837;&#65292;&#35299;&#20915;&#20102;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20851;&#38190;&#32570;&#28857;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;Transformer&#21644;MLP&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#20248;&#21183;&#12290;&#23613;&#31649;&#22312;NLP&#21644;CV&#26041;&#38754;&#33719;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#35768;&#22810;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;MLP&#30456;&#27604;&#65292;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#30340;&#25928;&#26524;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;Transformer&#65292;&#21363;&#36890;&#36947;&#23545;&#40784;&#40065;&#26834;&#21452;Transformer&#65288;CARD&#65289;&#65292;&#20197;&#35299;&#20915;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20851;&#38190;&#32570;&#28857;&#12290;&#39318;&#20808;&#65292;CARD&#24341;&#20837;&#20102;&#21452;Transformer&#32467;&#26500;&#65292;&#20351;&#20854;&#33021;&#22815;&#25429;&#25417;&#20449;&#21495;&#20043;&#38388;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#21644;&#22810;&#20010;&#21464;&#37327;&#22312;&#26102;&#38388;&#19978;&#30340;&#21160;&#24577;&#20381;&#36182;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#20943;&#36731;&#28508;&#22312;&#30340;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#12290;&#36825;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#22522;&#20110;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21152;&#26435;&#39044;&#27979;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#23545;&#22810;&#20010;&#38271;&#26399;&#21644;&#30701;&#26399;&#39044;&#27979;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;CARD&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have demonstrated the great power of deep learning methods, particularly Transformer and MLP, for time series forecasting. Despite its success in NLP and CV, many studies found that Transformer is less effective than MLP for time series forecasting. In this work, we design a special Transformer, i.e., channel-aligned robust dual Transformer (CARD for short), that addresses key shortcomings of Transformer in time series forecasting. First, CARD introduces a dual Transformer structure that allows it to capture both temporal correlations among signals and dynamical dependence among multiple variables over time. Second, we introduce a robust loss function for time series forecasting to alleviate the potential overfitting issue. This new loss function weights the importance of forecasting over a finite horizon based on prediction uncertainties. Our evaluation of multiple long-term and short-term forecasting datasets demonstrates that CARD significantly outperforms state-of-th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#30784;&#27169;&#22411;UP5&#65292;&#23427;&#37319;&#29992;&#21453;&#20107;&#23454;&#20844;&#24179;&#20419;&#36827;&#25216;&#26415;&#26469;&#28040;&#38500;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#20174;&#32780;&#23454;&#29616;&#38754;&#21521;&#20844;&#24179;&#24615;&#30340;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2305.12090</link><description>&lt;p&gt;
UP5: &#38754;&#21521;&#20844;&#24179;&#24615;&#25512;&#33616;&#30340;&#26080;&#20559;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UP5: Unbiased Foundation Model for Fairness-aware Recommendation. (arXiv:2305.12090v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#30784;&#27169;&#22411;UP5&#65292;&#23427;&#37319;&#29992;&#21453;&#20107;&#23454;&#20844;&#24179;&#20419;&#36827;&#25216;&#26415;&#26469;&#28040;&#38500;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#20174;&#32780;&#23454;&#29616;&#38754;&#21521;&#20844;&#24179;&#24615;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#31561;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24050;&#23558;&#23427;&#20204;&#25512;&#21040;&#20102;&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#30340;&#21069;&#27839;&#12290;&#27492;&#22806;&#65292;RS&#20013;&#30340;&#20844;&#24179;&#24615;&#24456;&#20851;&#38190;&#65292;&#22240;&#20026;&#35768;&#22810;&#29992;&#25143;&#23558;&#20854;&#29992;&#20110;&#20915;&#31574;&#21644;&#38656;&#27714;&#23653;&#34892;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#32570;&#20047;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#23637;&#31034;&#20844;&#24179;&#24615;&#27700;&#24179;&#21644;&#20844;&#24179;&#22788;&#29702;&#19981;&#21516;&#29992;&#25143;&#32676;&#32452;&#30340;&#36866;&#24403;&#26041;&#27861;&#30340;&#29702;&#35299;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#29992;&#25143;&#26041;&#38754;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24443;&#24213;&#26816;&#26597;&#34920;&#26126;&#65292;LLMs&#20013;&#23384;&#22312;&#19981;&#20844;&#24179;&#24615;&#65292;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#25512;&#33616;&#32467;&#26524;&#12290;&#20026;&#20102;&#28040;&#38500;LLM&#20013;&#30340;&#20559;&#24046;&#20197;&#23454;&#29616;&#38754;&#21521;&#20844;&#24179;&#24615;&#30340;&#25512;&#33616;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#20844;&#24179;&#20419;&#36827;&#25216;&#26415;&#30340;&#26032;&#22411;&#26080;&#20559;P5&#65288;UP5&#65289;&#22522;&#30784;&#27169;&#22411;&#12290;CFP&#21253;&#25324;&#20004;&#20010;&#23376;&#27169;&#22359;&#65306;&#20010;&#24615;&#21270;&#21069;&#32512;&#25552;&#31034;&#21644;Prompt&#28151;&#21512;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#20010;&#20307;&#25935;&#24863;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in foundation models such as large language models (LLM) have propelled them to the forefront of recommender systems (RS). Moreover, fairness in RS is critical since many users apply it for decision-making and demand fulfillment. However, at present, there is a lack of understanding regarding the level of fairness exhibited by recommendation foundation models and the appropriate methods for equitably treating different groups of users in foundation models. In this paper, we focus on user-side unfairness problem and show through a thorough examination that there is unfairness involved in LLMs that lead to unfair recommendation results. To eliminate bias from LLM for fairness-aware recommendation, we introduce a novel Unbiased P5 (UP5) foundation model based on Counterfactually-Fair-Prompting (CFP) techniques. CFP includes two sub-modules: a personalized prefix prompt that enhances fairness with respect to individual sensitive attributes, and a Prompt Mixture that int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#21516;&#34892;&#35780;&#23457;&#26399;&#21002;&#20013;&#20195;&#37329;&#21048;&#22870;&#21169;&#21046;&#24230;&#21487;&#33021;&#20250;&#23548;&#33268;&#35780;&#23457;&#20154;&#21592;&#20108;&#20803;&#20915;&#31574;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#22870;&#21169;&#31995;&#32479;&#65292;&#26377;&#25928;&#22320;&#20419;&#36827;&#20102;&#26356;&#20840;&#38754;&#30340;&#23457;&#26597;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#26356;&#21152;&#24179;&#34913;&#19988;&#31283;&#23450;&#12290;</title><link>http://arxiv.org/abs/2305.12088</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21516;&#34892;&#35780;&#23457;&#26399;&#21002;&#31995;&#32479;&#20013;&#35780;&#23457;&#22870;&#21169;&#30340;&#21338;&#24328;&#35770;&#20998;&#26512;&#19982;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Game-Theoretical Analysis of Reviewer Rewards in Peer-Review Journal Systems: Analysis and Experimental Evaluation using Deep Reinforcement Learning. (arXiv:2305.12088v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#21516;&#34892;&#35780;&#23457;&#26399;&#21002;&#20013;&#20195;&#37329;&#21048;&#22870;&#21169;&#21046;&#24230;&#21487;&#33021;&#20250;&#23548;&#33268;&#35780;&#23457;&#20154;&#21592;&#20108;&#20803;&#20915;&#31574;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#22870;&#21169;&#31995;&#32479;&#65292;&#26377;&#25928;&#22320;&#20419;&#36827;&#20102;&#26356;&#20840;&#38754;&#30340;&#23457;&#26597;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#26356;&#21152;&#24179;&#34913;&#19988;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#25968;&#23398;&#20934;&#30830;&#24615;&#21644;&#21338;&#24328;&#35770;&#31574;&#30053;&#27934;&#23519;&#21147;&#65292;&#25506;&#35752;&#20102;&#24320;&#25918;&#33719;&#21462;&#23398;&#26415;&#20986;&#29256;&#20013;&#30340;&#35780;&#23457;&#22870;&#21169;&#39046;&#22495;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#20195;&#37329;&#21048;&#22870;&#21169;&#31995;&#32479;&#27010;&#24565;&#21270;&#20026;&#19968;&#20010;&#20004;&#20010;&#29609;&#23478;&#30340;&#21338;&#24328;&#65292;&#24182;&#35782;&#21035;&#20102;&#21487;&#33021;&#23548;&#33268;&#35780;&#23457;&#20154;&#21592;&#20542;&#21521;&#20110;&#20108;&#20803;&#20915;&#31574;&#30340;&#28508;&#22312;&#32570;&#28857;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#25968;&#23398;&#19978;&#24418;&#24335;&#21270;&#20102;&#19968;&#31181;&#26367;&#20195;&#22870;&#21169;&#31995;&#32479;&#65292;&#26088;&#22312;&#20943;&#36731;&#36825;&#31181;&#20559;&#35265;&#65292;&#20419;&#36827;&#26356;&#20840;&#38754;&#30340;&#23457;&#26597;&#12290;&#25105;&#20204;&#36816;&#29992;&#20005;&#26684;&#30340;&#21338;&#24328;&#35770;&#20998;&#26512;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#25311;&#65292;&#23545;&#20004;&#31181;&#31995;&#32479;&#30340;&#23646;&#24615;&#21644;&#32467;&#26524;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;&#20004;&#31181;&#31995;&#32479;&#20043;&#38388;&#30340;&#26174;&#30528;&#24046;&#24322;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31995;&#32479;&#23637;&#29616;&#20986;&#20102;&#26356;&#24179;&#34913;&#30340;&#20915;&#31574;&#20998;&#37197;&#21644;&#26356;&#21152;&#31283;&#23450;&#30340;&#29305;&#28857;&#12290;&#36825;&#39033;&#30740;&#31350;&#19981;&#20165;&#25193;&#20805;&#20102;&#26377;&#20851;&#35780;&#23457;&#22870;&#21169;&#31995;&#32479;&#30340;&#25968;&#23398;&#29702;&#35299;&#65292;&#32780;&#19988;&#25552;&#20379;&#20102;&#35299;&#20915;&#24403;&#21069;&#20195;&#37329;&#21048;&#22870;&#21169;&#31995;&#32479;&#20559;&#35265;&#38382;&#39064;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we navigate the intricate domain of reviewer rewards in open-access academic publishing, leveraging the precision of mathematics and the strategic acumen of game theory. We conceptualize the prevailing voucher-based reviewer reward system as a two-player game, subsequently identifying potential shortcomings that may incline reviewers towards binary decisions. To address this issue, we propose and mathematically formalize an alternative reward system with the objective of mitigating this bias and promoting more comprehensive reviews. We engage in a detailed investigation of the properties and outcomes of both systems, employing rigorous game-theoretical analysis and deep reinforcement learning simulations. Our results underscore a noteworthy divergence between the two systems, with our proposed system demonstrating a more balanced decision distribution and enhanced stability. This research not only augments the mathematical understanding of reviewer reward systems, but it
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#26694;&#26550;&#26469;&#35299;&#20915;&#22270;&#24418;&#22238;&#24402;&#20219;&#21153;&#20013;&#31232;&#26377;&#26631;&#31614;&#20540;&#31034;&#20363;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#33258;&#25105;&#35757;&#32451;&#65292;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#24182;&#20943;&#23569;&#27169;&#22411;&#20559;&#24046;&#12290;&#20854;&#20013;&#65292;&#37319;&#29992;&#19968;&#31181;&#26032;&#30340;&#22238;&#24402;&#32622;&#20449;&#24230;&#27979;&#37327;&#26041;&#27861;&#20026;&#20195;&#34920;&#31232;&#26377;&#26631;&#31614;&#30340;&#26356;&#22810;&#22270;&#24418;&#20266;&#26631;&#27880;&#65292;&#24182;&#22312;&#25968;&#25454;&#24179;&#34913;&#21518;&#20351;&#29992;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#22686;&#21152;&#22270;&#24418;&#20197;&#29983;&#25104;&#26356;&#22810;&#30340;&#31232;&#26377;&#26631;&#31614;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.12087</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#22270;&#24418;&#19981;&#24179;&#34913;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Graph Imbalanced Regression. (arXiv:2305.12087v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#26694;&#26550;&#26469;&#35299;&#20915;&#22270;&#24418;&#22238;&#24402;&#20219;&#21153;&#20013;&#31232;&#26377;&#26631;&#31614;&#20540;&#31034;&#20363;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#33258;&#25105;&#35757;&#32451;&#65292;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#24182;&#20943;&#23569;&#27169;&#22411;&#20559;&#24046;&#12290;&#20854;&#20013;&#65292;&#37319;&#29992;&#19968;&#31181;&#26032;&#30340;&#22238;&#24402;&#32622;&#20449;&#24230;&#27979;&#37327;&#26041;&#27861;&#20026;&#20195;&#34920;&#31232;&#26377;&#26631;&#31614;&#30340;&#26356;&#22810;&#22270;&#24418;&#20266;&#26631;&#27880;&#65292;&#24182;&#22312;&#25968;&#25454;&#24179;&#34913;&#21518;&#20351;&#29992;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#22686;&#21152;&#22270;&#24418;&#20197;&#29983;&#25104;&#26356;&#22810;&#30340;&#31232;&#26377;&#26631;&#31614;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#65292;&#30001;&#20110;&#26576;&#20123;&#36830;&#32493;&#26631;&#31614;&#20540;&#30340;&#35266;&#27979;&#24456;&#38590;&#25910;&#38598;&#65292;&#22240;&#27492;&#27880;&#37322;&#25968;&#25454;&#20013;&#24456;&#23481;&#26131;&#20986;&#29616;&#25968;&#25454;&#19981;&#24179;&#34913;&#12290;&#22312;&#20998;&#23376;&#21644;&#32858;&#21512;&#29289;&#23646;&#24615;&#39044;&#27979;&#20013;&#65292;&#27880;&#37322;&#30340;&#22270;&#24418;&#25968;&#25454;&#38598;&#36890;&#24120;&#24456;&#23567;&#65292;&#22240;&#20026;&#26631;&#27880;&#23427;&#20204;&#38656;&#35201;&#26114;&#36149;&#30340;&#35774;&#22791;&#21644;&#24037;&#20316;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#22270;&#24418;&#22238;&#24402;&#20219;&#21153;&#20013;&#31232;&#26377;&#26631;&#31614;&#20540;&#31034;&#20363;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#35757;&#32451;&#36880;&#27493;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#24182;&#20943;&#23569;&#27169;&#22411;&#20559;&#24046;&#12290;&#35757;&#32451;&#25968;&#25454;&#30340;&#24179;&#34913;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#23454;&#29616;&#65306;&#65288;1&#65289;&#29992;&#19968;&#31181;&#26032;&#30340;&#22238;&#24402;&#32622;&#20449;&#24230;&#27979;&#37327;&#26041;&#27861;&#20026;&#20195;&#34920;&#31232;&#26377;&#26631;&#31614;&#30340;&#26356;&#22810;&#22270;&#24418;&#20266;&#26631;&#27880;&#65292;&#24182;&#20174;&#19981;&#24179;&#34913;&#30340;&#27880;&#37322;&#25968;&#25454;&#20013;&#36870;&#20998;&#24067;&#37319;&#26679;&#23427;&#20204;&#30340;&#23376;&#38598;&#20197;&#35782;&#21035;&#26410;&#26631;&#27880;&#25968;&#25454;&#20013;&#30340;&#39640;&#36136;&#37327;&#26679;&#20363;&#65307;(2)&#22312;&#25968;&#25454;&#24179;&#34913;&#21518;&#65292;&#20351;&#29992;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#22686;&#21152;&#22270;&#24418;&#65292;&#20197;&#29983;&#25104;&#26356;&#22810;&#30340;&#31232;&#26377;&#26631;&#31614;&#31034;&#20363;&#65292;&#32780;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#12290;&#21518;&#32773;&#26159;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#29983;&#25104;&#27169;&#22411;&#26469;&#20154;&#24037;&#29983;&#25104;&#26356;&#22810;&#30340;&#31232;&#26377;&#26631;&#31614;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data imbalance is easily found in annotated data when the observations of certain continuous label values are difficult to collect for regression tasks. When they come to molecule and polymer property predictions, the annotated graph datasets are often small because labeling them requires expensive equipment and effort. To address the lack of examples of rare label values in graph regression tasks, we propose a semi-supervised framework to progressively balance training data and reduce model bias via self-training. The training data balance is achieved by (1) pseudo-labeling more graphs for under-represented labels with a novel regression confidence measurement and (2) augmenting graph examples in latent space for remaining rare labels after data balancing with pseudo-labels. The former is to identify quality examples from unlabeled data whose labels are confidently predicted and sample a subset of them with a reverse distribution from the imbalanced annotated data. The latter collabor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33258;&#21160;&#25915;&#20987;&#26694;&#26550;SneakyPrompt&#65292;&#20197;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#35813;&#26694;&#26550;&#30340;&#20851;&#38190;&#27934;&#35265;&#26159;&#25628;&#32034;&#22791;&#36873;&#20196;&#29260;&#26469;&#32469;&#36807;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;</title><link>http://arxiv.org/abs/2305.12082</link><description>&lt;p&gt;
SneakyPrompt&#65306;&#35780;&#20272;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
SneakyPrompt: Evaluating Robustness of Text-to-image Generative Models' Safety Filters. (arXiv:2305.12082v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33258;&#21160;&#25915;&#20987;&#26694;&#26550;SneakyPrompt&#65292;&#20197;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#35813;&#26694;&#26550;&#30340;&#20851;&#38190;&#27934;&#35265;&#26159;&#25628;&#32034;&#22791;&#36873;&#20196;&#29260;&#26469;&#32469;&#36807;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#65292;&#22914;Stable Diffusion&#21644;DALL$\cdot$E 2&#31561;&#65292;&#30001;&#20110;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#20010;&#25361;&#25112;&#24615;&#38382;&#39064;&#26159;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#65292;&#20363;&#22914;&#19982;&#26292;&#21147;&#21644;&#25104;&#20154;&#30456;&#20851;&#30340;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;&#24120;&#35265;&#20570;&#27861;&#26159;&#37096;&#32626;&#25152;&#35859;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#65292;&#22522;&#20110;&#25991;&#26412;&#25110;&#22270;&#20687;&#29305;&#24449;&#38459;&#27490;&#19981;&#23433;&#20840;&#20869;&#23481;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#27492;&#31867;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#21487;&#33021;&#32469;&#36807;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#25163;&#21160;&#23436;&#25104;&#24182;&#19987;&#38376;&#38024;&#23545;Stable Diffusion&#23448;&#26041;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#25105;&#20204;&#30340;&#35780;&#20272;&#65292;Stable Diffusion&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#32469;&#36807;&#27604;&#29575;&#20165;&#20026;23.51&#65285;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33258;&#21160;&#25915;&#20987;&#26694;&#26550;SneakyPrompt&#65292;&#20197;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#29616;&#23454;&#19990;&#30028;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#35265;&#26159;&#25628;&#32034;&#22791;&#36873;&#20196;&#29260;&#26469;&#32469;&#36807;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generative models such as Stable Diffusion and DALL$\cdot$E 2 have attracted much attention since their publication due to their wide application in the real world. One challenging problem of text-to-image generative models is the generation of Not-Safe-for-Work (NSFW) content, e.g., those related to violence and adult. Therefore, a common practice is to deploy a so-called safety filter, which blocks NSFW content based on either text or image features. Prior works have studied the possible bypass of such safety filters. However, existing works are largely manual and specific to Stable Diffusion's official safety filter. Moreover, the bypass ratio of Stable Diffusion's safety filter is as low as 23.51% based on our evaluation.  In this paper, we propose the first automated attack framework, called SneakyPrompt, to evaluate the robustness of real-world safety filters in state-of-the-art text-to-image generative models. Our key insight is to search for alternative tokens in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; AnyPredict &#30340;&#34920;&#26684;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#20351;&#29992;&#25968;&#25454;&#24341;&#25806;&#25972;&#21512;&#39046;&#22495;&#20869;&#21644;&#24191;&#27867;&#30340;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#65292;&#20197;&#20811;&#26381;&#27169;&#24335;&#19981;&#21305;&#37197;&#21644;&#39044;&#27979;&#30446;&#26631;&#24322;&#36136;&#24615;&#31561;&#26041;&#38754;&#30340;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2305.12081</link><description>&lt;p&gt;
AnyPredict: &#34920;&#26684;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AnyPredict: Foundation Model for Tabular Prediction. (arXiv:2305.12081v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; AnyPredict &#30340;&#34920;&#26684;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#20351;&#29992;&#25968;&#25454;&#24341;&#25806;&#25972;&#21512;&#39046;&#22495;&#20869;&#21644;&#24191;&#27867;&#30340;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#65292;&#20197;&#20811;&#26381;&#27169;&#24335;&#19981;&#21305;&#37197;&#21644;&#39044;&#27979;&#30446;&#26631;&#24322;&#36136;&#24615;&#31561;&#26041;&#38754;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#26159;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#27169;&#22411;&#22312;&#34920;&#26684;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#21463;&#21040;&#38480;&#21046;&#65292;&#20027;&#35201;&#38382;&#39064;&#21253;&#25324; (1) &#32570;&#20047;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#30340;&#24102;&#26377;&#26631;&#20934;&#26631;&#31614;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450; (2) &#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#27169;&#24335;&#19981;&#21305;&#37197;&#21644;&#39044;&#27979;&#30446;&#26631;&#30340;&#24322;&#36136;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#22522;&#20110; AnyPredict &#30340;&#34920;&#26684;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#65292;&#21253;&#25324;&#39046;&#22495;&#20869;&#21644;&#24191;&#27867;&#30340;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#25968;&#25454;&#24341;&#25806;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#26469;&#25972;&#21512;&#34920;&#26684;&#26679;&#26412;&#65292;&#20811;&#26381;&#20102;&#19981;&#21516;&#27169;&#24335;&#34920;&#26684;&#20043;&#38388;&#30340;&#38556;&#30861;&#65292;&#24182;&#20351;&#29992;&#8220;&#23398;&#20064;&#65292;&#27880;&#37322;&#21644;&#23457;&#35745;&#8221;&#27969;&#31243;&#23558;&#39046;&#22495;&#22806;&#25968;&#25454;&#19982;&#30446;&#26631;&#20219;&#21153;&#23545;&#40784;&#12290;&#25193;&#23637;&#30340;&#35757;&#32451;&#25968;&#25454;&#20351;&#39044;&#35757;&#32451;&#30340; AnyPredict &#33021;&#22815;&#25903;&#25345;&#27599;&#20010;&#34920;&#26684;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models are pre-trained on massive data to perform well across many downstream tasks. They have demonstrated significant success in natural language processing and computer vision. Nonetheless, the use of such models in tabular prediction tasks has been limited, with the main hurdles consisting of (1) the lack of large-scale and diverse tabular datasets with standardized labels and (2) the schema mismatch and predictive target heterogeneity across domains.  This paper proposes a method for building training data at scale for tabular prediction foundation models (AnyPredict) using both in-domain and a wide range of out-domain datasets. The method uses a data engine that leverages large language models (LLMs) to consolidate tabular samples to overcome the barrier across tables with varying schema and align out-domain data with the target task using a ``learn, annotate, and audit'' pipeline. The expanded training data enables the pre-trained AnyPredict to support every tabular d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;GELU&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25968;&#23398;&#20998;&#26512;&#21644;&#24191;&#27867;&#30340;&#23454;&#39564;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12073</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;GELU&#28608;&#27963;&#20989;&#25968;&#65306;&#20840;&#38754;&#30340;&#25968;&#23398;&#20998;&#26512;&#21644;&#24615;&#33021;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
GELU Activation Function in Deep Learning: A Comprehensive Mathematical Analysis and Performance. (arXiv:2305.12073v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;GELU&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25968;&#23398;&#20998;&#26512;&#21644;&#24191;&#27867;&#30340;&#23454;&#39564;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#28608;&#27963;&#20989;&#25968;&#26159;&#24433;&#21709;&#20854;&#23398;&#20064;&#33021;&#21147;&#12289;&#31283;&#23450;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36817;&#24180;&#26469;&#65292;&#39640;&#26031;&#35823;&#24046;&#32447;&#24615;&#21333;&#20803;&#65288;GELU&#65289;&#28608;&#27963;&#20989;&#25968;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#20027;&#27969;&#26041;&#27861;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#22914;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#12290;&#26412;&#25991;&#23545;GELU&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#25968;&#23398;&#20998;&#26512;&#65292;&#35814;&#32454;&#25506;&#35752;&#20102;&#20854;&#21487;&#24494;&#24615;&#12289;&#26377;&#30028;&#24615;&#12289;&#24179;&#31283;&#24615;&#21644;&#20809;&#28369;&#24615;&#31561;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;GELU&#20989;&#25968;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#27604;&#36739;&#65292;&#21033;&#29992;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;STL-10&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27531;&#24046;&#21367;&#31215;&#32593;&#32476;&#20316;&#20026;&#23454;&#35777;&#27979;&#35797;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;GELU&#30456;&#23545;&#20110;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#30830;&#31435;&#20102;&#23427;&#22312;&#24191;&#27867;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selecting the most suitable activation function is a critical factor in the effectiveness of deep learning models, as it influences their learning capacity, stability, and computational efficiency. In recent years, the Gaussian Error Linear Unit (GELU) activation function has emerged as a dominant method, surpassing traditional functions such as the Rectified Linear Unit (ReLU) in various applications. This study presents a rigorous mathematical investigation of the GELU activation function, exploring its differentiability, boundedness, stationarity, and smoothness properties in detail. Additionally, we conduct an extensive experimental comparison of the GELU function against a broad range of alternative activation functions, utilizing a residual convolutional network trained on the CIFAR-10, CIFAR-100, and STL-10 datasets as the empirical testbed. Our results demonstrate the superior performance of GELU compared to other activation functions, establishing its suitability for a wide ra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#24577;&#26799;&#24230;&#24179;&#34913;&#25915;&#20987;&#65288;DGBA&#65289;&#26694;&#26550;&#26469;&#25915;&#20987;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#22238;&#31572;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;&#12289;&#22810;&#20219;&#21153;&#25915;&#20987;&#21644;&#23545;&#25239;&#35757;&#32451;&#26159;&#21542;&#22686;&#24378;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#31561;&#23433;&#20840;&#30740;&#31350;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.12066</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#21160;&#24577;&#26799;&#24230;&#24179;&#34913;&#22686;&#24378;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Dynamic Gradient Balancing for Enhanced Adversarial Attacks on Multi-Task Models. (arXiv:2305.12066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#24577;&#26799;&#24230;&#24179;&#34913;&#25915;&#20987;&#65288;DGBA&#65289;&#26694;&#26550;&#26469;&#25915;&#20987;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#22238;&#31572;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;&#12289;&#22810;&#20219;&#21153;&#25915;&#20987;&#21644;&#23545;&#25239;&#35757;&#32451;&#26159;&#21542;&#22686;&#24378;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#31561;&#23433;&#20840;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064; (MTL) &#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#21333;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#22810;&#20010;&#20219;&#21153;&#12290;&#34429;&#28982;&#21333;&#20219;&#21153;&#20998;&#31867;&#22120;&#30340;&#23433;&#20840;&#24615;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#23384;&#22312;&#30528;&#20960;&#20010;&#20851;&#38190;&#30340;&#23433;&#20840;&#24615;&#30740;&#31350;&#38382;&#39064;&#65292;&#21253;&#25324;: 1&#65289;&#22810;&#20219;&#21153;&#27169;&#22411;&#23545;&#21333;&#20219;&#21153;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;&#22914;&#20309;&#65311;2&#65289;&#33021;&#21542;&#35774;&#35745;&#23545;&#25239;&#24615;&#25915;&#20987;&#26469;&#21516;&#26102;&#25915;&#20987;&#22810;&#20010;&#20219;&#21153;&#65311; 3&#65289;&#20219;&#21153;&#20849;&#20139;&#21644;&#23545;&#25239;&#35757;&#32451;&#26159;&#21542;&#22686;&#21152;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65311;&#26412;&#25991;&#36890;&#36807;&#20180;&#32454;&#20998;&#26512;&#21644;&#20005;&#26684;&#30340;&#23454;&#39564;&#22238;&#31572;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#21333;&#20219;&#21153;&#30333;&#30418;&#25915;&#20987;&#30340;&#21021;&#32423;&#36716;&#21270;&#24182;&#20998;&#26512;&#20102;&#20854;&#22266;&#26377;&#32570;&#38519;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26694;&#26550;&#65292;&#21160;&#24577;&#26799;&#24230;&#24179;&#34913;&#25915;&#20987;&#65288;DGBA&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25226;&#25915;&#20987;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#38382;&#39064;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#24179;&#22343;&#30456;&#23545;&#25439;&#22833;&#21464;&#21270;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) creates a single machine learning model called multi-task model to simultaneously perform multiple tasks. Although the security of single task classifiers has been extensively studied, there are several critical security research questions for multi-task models including 1) How secure are multi-task models to single task adversarial machine learning attacks, 2) Can adversarial attacks be designed to attack multiple tasks simultaneously, and 3) Does task sharing and adversarial training increase multi-task model robustness to adversarial attacks? In this paper, we answer these questions through careful analysis and rigorous experimentation. First, we develop na\"ive adaptation of single-task white-box attacks and analyze their inherent drawbacks. We then propose a novel attack framework, Dynamic Gradient Balancing Attack (DGBA). Our framework poses the problem of attacking a multi-task model as an optimization problem based on averaged relative loss change, whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#38899;&#39057;-&#25163;&#21183;&#22810;&#27169;&#34701;&#21512;&#31995;&#32479;&#26469;&#23454;&#29616;&#26080;&#38656;&#35302;&#21457;&#22120;&#30340;&#35821;&#38899;&#21161;&#25163;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#38899;&#39057;&#21644;&#25163;&#21183;&#25968;&#25454;&#20043;&#38388;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#36890;&#29992;&#24615;&#24378;&#65292;&#24182;&#21487;&#20197;&#24555;&#36895;&#21551;&#21160;&#65292;&#25552;&#39640;&#36164;&#20135;&#24320;&#21457;&#27969;&#31243;&#30340;&#29983;&#20135;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.12063</link><description>&lt;p&gt;
&#26080;&#38656;&#35302;&#21457;&#22120;&#35821;&#38899;&#21161;&#25163;&#30340;&#39640;&#25928;&#22810;&#27169;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Efficient Multimodal Neural Networks for Trigger-less Voice Assistants. (arXiv:2305.12063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#38899;&#39057;-&#25163;&#21183;&#22810;&#27169;&#34701;&#21512;&#31995;&#32479;&#26469;&#23454;&#29616;&#26080;&#38656;&#35302;&#21457;&#22120;&#30340;&#35821;&#38899;&#21161;&#25163;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#38899;&#39057;&#21644;&#25163;&#21183;&#25968;&#25454;&#20043;&#38388;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#36890;&#29992;&#24615;&#24378;&#65292;&#24182;&#21487;&#20197;&#24555;&#36895;&#21551;&#21160;&#65292;&#25552;&#39640;&#36164;&#20135;&#24320;&#21457;&#27969;&#31243;&#30340;&#29983;&#20135;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21161;&#25163;&#65288;VA&#65289;&#37319;&#29992;&#22810;&#27169;&#20114;&#21160;&#20197;&#22686;&#24378;&#20154;&#26426;&#20132;&#20114;&#30340;&#26041;&#24335;&#27491;&#22312;&#36805;&#36895;&#22686;&#38271;&#12290;&#26234;&#33021;&#25163;&#34920;&#29616;&#22312;&#24050;&#32463;&#34701;&#21512;&#20102;&#26080;&#38656;&#26174;&#24335;&#35302;&#21457;&#22120;&#30340;VA&#35843;&#29992;&#26041;&#27861;&#65292;&#22914;Raise To Speak&#65288;RTS&#65289;&#65292;&#29992;&#25143;&#23558;&#25163;&#34920;&#20030;&#36215;&#24182;&#21521;VA&#35828;&#35805;&#32780;&#26080;&#38656;&#26174;&#24335;&#35302;&#21457;&#22120;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;RTS&#31995;&#32479;&#20381;&#38752;&#21551;&#21457;&#24335;&#21644;&#35774;&#35745;&#30340;&#26377;&#38480;&#29366;&#24577;&#26426;&#26469;&#34701;&#21512;&#25163;&#21183;&#21644;&#38899;&#39057;&#25968;&#25454;&#20197;&#36827;&#34892;&#22810;&#27169;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#36866;&#24212;&#24615;&#26377;&#38480;&#12289;&#21487;&#25193;&#23637;&#24615;&#19981;&#36275;&#21644;&#20154;&#31867;&#20135;&#29983;&#30340;&#20559;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#38899;&#39057;-&#25163;&#21183;&#22810;&#27169;&#34701;&#21512;&#31995;&#32479;&#65292;&#20854;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;&#65288;1&#65289;&#26356;&#22909;&#22320;&#29702;&#35299;&#38899;&#39057;&#21644;&#25163;&#21183;&#25968;&#25454;&#20043;&#38388;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#36827;&#34892;&#31934;&#30830;&#30340;&#35843;&#29992;&#65288;2&#65289;&#22312;&#24191;&#27867;&#30340;&#29615;&#22659;&#21644;&#22330;&#26223;&#19979;&#20855;&#26377;&#36890;&#29992;&#24615;&#65288;3&#65289;&#36731;&#20415;&#19988;&#21487;&#22312;&#20302;&#21151;&#29575;&#35774;&#22791;&#19978;&#37096;&#32626;&#65292;&#22914;&#26234;&#33021;&#25163;&#34920;&#65292;&#24182;&#20855;&#26377;&#24555;&#36895;&#21551;&#21160;&#26102;&#38388;&#65288;4&#65289;&#25552;&#39640;&#36164;&#20135;&#24320;&#21457;&#27969;&#31243;&#30340;&#29983;&#20135;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The adoption of multimodal interactions by Voice Assistants (VAs) is growing rapidly to enhance human-computer interactions. Smartwatches have now incorporated trigger-less methods of invoking VAs, such as Raise To Speak (RTS), where the user raises their watch and speaks to VAs without an explicit trigger. Current state-of-the-art RTS systems rely on heuristics and engineered Finite State Machines to fuse gesture and audio data for multimodal decision-making. However, these methods have limitations, including limited adaptability, scalability, and induced human biases. In this work, we propose a neural network based audio-gesture multimodal fusion system that (1) Better understands temporal correlation between audio and gesture data, leading to precise invocations (2) Generalizes to a wide range of environments and scenarios (3) Is lightweight and deployable on low-power devices, such as smartwatches, with quick launch times (4) Improves productivity in asset development processes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#29983;&#29289;&#30456;&#23481;&#24615;&#38209;&#21512;&#37329;&#30340;&#23624;&#26381;&#24378;&#24230;&#65292;&#36890;&#36807;&#36951;&#20256;&#31639;&#27861;&#30340;&#20248;&#21270;&#65292;&#24179;&#34913;&#20102;&#20854;&#21147;&#23398;&#24378;&#24230;&#21644;&#29983;&#29289;&#30456;&#23481;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12060</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#35774;&#35745;&#29983;&#29289;&#30456;&#23481;&#24615;&#38209;&#21512;&#37329;&#30340;&#21147;&#23398;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Mechanical Property Design of Bio-compatible Mg alloys using Machine-Learning Algorithms. (arXiv:2305.12060v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#29983;&#29289;&#30456;&#23481;&#24615;&#38209;&#21512;&#37329;&#30340;&#23624;&#26381;&#24378;&#24230;&#65292;&#36890;&#36807;&#36951;&#20256;&#31639;&#27861;&#30340;&#20248;&#21270;&#65292;&#24179;&#34913;&#20102;&#20854;&#21147;&#23398;&#24378;&#24230;&#21644;&#29983;&#29289;&#30456;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38209;&#21512;&#37329;&#22240;&#20854;&#29983;&#29289;&#30456;&#23481;&#24615;&#12289;&#21487;&#25511;&#33104;&#34432;&#36895;&#29575;&#20197;&#21450;&#19982;&#33258;&#28982;&#39592;&#39612;&#22312;&#21018;&#24230;&#21644;&#23494;&#24230;&#26041;&#38754;&#30340;&#30456;&#20284;&#24615;&#32780;&#25104;&#20026;&#26242;&#26102;&#24615;&#29983;&#29289;&#26893;&#20837;&#29289;&#30340;&#26377;&#21560;&#24341;&#21147;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#20302;&#30340;&#26426;&#26800;&#24378;&#24230;&#38459;&#30861;&#20102;&#23427;&#20204;&#20316;&#20026;&#24515;&#34880;&#31649;&#25903;&#26550;&#21644;&#39592;&#26367;&#20195;&#21697;&#30340;&#20351;&#29992;&#12290;&#34429;&#28982;&#21487;&#20197;&#36827;&#34892;&#21512;&#37329;&#24037;&#31243;&#65292;&#20197;&#23454;&#29616;&#25152;&#38656;&#30340;&#26426;&#26800;&#24378;&#24230;&#65292;&#20294;&#20351;&#29992;&#24120;&#35268;&#23454;&#39564;&#26041;&#27861;&#20248;&#21270;&#29983;&#29289;&#30456;&#23481;&#24615;&#38209;&#21512;&#37329;&#30340;&#21147;&#23398;&#24615;&#33021;&#38750;&#24120;&#32791;&#26102;&#21644;&#26114;&#36149;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#26469;&#31616;&#21270;&#21512;&#37329;&#35774;&#35745;&#27969;&#31243;&#24182;&#32553;&#30701;&#25152;&#38656;&#26102;&#38388;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#39044;&#27979;&#29983;&#29289;&#30456;&#23481;&#24615;&#38209;&#21512;&#37329;&#30340;&#23624;&#26381;&#24378;&#24230;&#65292;$R^2$&#30340;&#20934;&#30830;&#24230;&#20026;91&#65285;&#12290;&#25509;&#19979;&#26469;&#65292;&#23558;&#39044;&#27979;&#27169;&#22411;&#29992;&#20316;&#36951;&#20256;&#31639;&#27861;&#30340;&#36866;&#24212;&#24230;&#20989;&#25968;&#65292;&#20197;&#20248;&#21270;&#21512;&#37329;&#20013;&#20803;&#32032;&#30340;&#36136;&#37327;&#30334;&#20998;&#27604;&#65292;&#24182;&#22312;&#21147;&#23398;&#24378;&#24230;&#21644;&#29983;&#29289;&#30456;&#23481;&#24615;&#20043;&#38388;&#33719;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Magnesium alloys are attractive options for temporary bio-implants because of their biocompatibility, controlled corrosion rate, and similarity to natural bone in terms of stiffness and density. Nevertheless, their low mechanical strength hinders their use as cardiovascular stents and bone substitutes. While it is possible to engineer alloys with the desired mechanical strength, optimizing the mechanical properties of biocompatible magnesium alloys using conventional experimental methods is time-consuming and expensive. Therefore, Artificial Intelligence (AI) can be leveraged to streamline the alloy design process and reduce the required time. In this study, a machine learning model was developed to predict the yield strength (YS) of biocompatible magnesium alloys with an $R^2$ accuracy of 91\%. The predictive model was then validated using the CALPHAD technique and thermodynamics calculations. Next, the predictive model was employed as the fitness function of a genetic algorithm to op
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#24615;&#30340;&#28145;&#24230;&#36328;&#39046;&#22495;&#28857;&#20987;&#29575;&#39044;&#27979;&#27169;&#22411;&#8212;&#8212;&#39046;&#22495;&#23545;&#25239;&#28145;&#24230;&#20852;&#36259;&#32593;&#32476;&#65288;DADIN&#65289;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#19981;&#21487;&#30693;&#23618;&#21644;&#29305;&#21035;&#35774;&#35745;&#30340;&#25439;&#22833;&#65292;&#21019;&#26032;&#22320;&#23454;&#29616;&#20102;&#20004;&#20010;&#39046;&#22495;&#30340;&#32852;&#21512;&#20998;&#24067;&#23545;&#40784;&#65292;&#24182;&#37319;&#29992;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#24335;&#19982;&#28857;&#20987;&#29575;&#39044;&#27979;&#25439;&#22833;&#19968;&#36215;&#36827;&#34892;&#20248;&#21270;&#65292;&#30456;&#27604;&#31454;&#20105;&#22522;&#32447;&#31639;&#27861;&#25552;&#21319;&#26126;&#26174;&#12290;</title><link>http://arxiv.org/abs/2305.12058</link><description>&lt;p&gt;
DADIN: &#38754;&#21521;&#36328;&#22495;&#25512;&#33616;&#31995;&#32479;&#30340;&#39046;&#22495;&#23545;&#25239;&#28145;&#24230;&#20852;&#36259;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DADIN: Domain Adversarial Deep Interest Network for Cross Domain Recommender Systems. (arXiv:2305.12058v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12058
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#24615;&#30340;&#28145;&#24230;&#36328;&#39046;&#22495;&#28857;&#20987;&#29575;&#39044;&#27979;&#27169;&#22411;&#8212;&#8212;&#39046;&#22495;&#23545;&#25239;&#28145;&#24230;&#20852;&#36259;&#32593;&#32476;&#65288;DADIN&#65289;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#19981;&#21487;&#30693;&#23618;&#21644;&#29305;&#21035;&#35774;&#35745;&#30340;&#25439;&#22833;&#65292;&#21019;&#26032;&#22320;&#23454;&#29616;&#20102;&#20004;&#20010;&#39046;&#22495;&#30340;&#32852;&#21512;&#20998;&#24067;&#23545;&#40784;&#65292;&#24182;&#37319;&#29992;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#24335;&#19982;&#28857;&#20987;&#29575;&#39044;&#27979;&#25439;&#22833;&#19968;&#36215;&#36827;&#34892;&#20248;&#21270;&#65292;&#30456;&#27604;&#31454;&#20105;&#22522;&#32447;&#31639;&#27861;&#25552;&#21319;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#39044;&#27979;&#26159;&#25512;&#33616;&#31995;&#32479;&#30340;&#20027;&#35201;&#20219;&#21153;&#20043;&#19968;&#65292;&#29992;&#25143;&#38024;&#23545;&#19981;&#21516;&#39033;&#30446;&#36827;&#34892;&#28857;&#20987;&#20197;&#33719;&#21462;&#25512;&#33616;&#32467;&#26524;&#12290;&#38024;&#23545;&#25968;&#25454;&#31232;&#30095;&#24615;&#12289;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30340;&#38271;&#23614;&#20998;&#24067;&#21644;&#39033;&#30446;&#25110;&#29992;&#25143;&#30340;&#20919;&#21551;&#21160;&#31561;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36328;&#39046;&#22495;&#28857;&#20987;&#29575;&#39044;&#27979;&#27169;&#22411;&#12290;&#20026;&#20102;&#20351;&#28304;&#22495;&#21040;&#30446;&#26631;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#26356;&#21152;&#39034;&#30021;&#65292;&#25552;&#20986;&#20102;&#21019;&#26032;&#24615;&#30340;&#28145;&#24230;&#36328;&#39046;&#22495;&#28857;&#20987;&#29575;&#39044;&#27979;&#27169;&#22411;&#8212;&#8212;&#39046;&#22495;&#23545;&#25239;&#28145;&#24230;&#20852;&#36259;&#32593;&#32476; (DADIN)&#65292;&#23558;&#36328;&#22495;&#25512;&#33616;&#20219;&#21153;&#36716;&#21270;&#20026;&#39046;&#22495;&#36866;&#24212;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#19981;&#21487;&#30693;&#23618;&#21644;&#29305;&#21035;&#35774;&#35745;&#30340;&#25439;&#22833;&#65292;&#21019;&#26032;&#22320;&#23454;&#29616;&#20102;&#20004;&#20010;&#39046;&#22495;&#30340;&#32852;&#21512;&#20998;&#24067;&#23545;&#40784;&#65292;&#24182;&#37319;&#29992;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#24335;&#19982;&#28857;&#20987;&#29575;&#39044;&#27979;&#25439;&#22833;&#19968;&#36215;&#36827;&#34892;&#20248;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21326;&#20026;&#25968;&#25454;&#38598;&#19978;&#65292;DADIN &#30340;&#26354;&#32447;&#19979;&#38754;&#31215; (AUC) &#27604;&#26368;&#20855;&#31454;&#20105;&#21147;&#30340;&#22522;&#32447;&#39640;&#20986;0.08&#65285;&#65292;&#39640;&#20986;0.7&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Click-Through Rate (CTR) prediction is one of the main tasks of the recommendation system, which is conducted by a user for different items to give the recommendation results. Cross-domain CTR prediction models have been proposed to overcome problems of data sparsity, long tail distribution of user-item interactions, and cold start of items or users. In order to make knowledge transfer from source domain to target domain more smoothly, an innovative deep learning cross-domain CTR prediction model, Domain Adversarial Deep Interest Network (DADIN) is proposed to convert the cross-domain recommendation task into a domain adaptation problem. The joint distribution alignment of two domains is innovatively realized by introducing domain agnostic layers and specially designed loss, and optimized together with CTR prediction loss in a way of adversarial training. It is found that the Area Under Curve (AUC) of DADIN is 0.08% higher than the most competitive baseline on Huawei dataset and is 0.7
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24314;&#31435;&#23398;&#20064;&#29702;&#35770;&#21644;&#24212;&#29992;&#27010;&#29575;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35777;&#26126;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;Wasserstein&#31283;&#23450;&#24615;&#30028;&#38480;&#30340;&#32479;&#19968;&#25351;&#21335;&#65292;&#24182;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#24378;&#20984;&#25439;&#22833;&#21644;&#24102;&#28155;&#21152;&#22122;&#22768;&#30340;&#38750;&#20984;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2305.12056</link><description>&lt;p&gt;
&#65288;&#24102;&#22122;&#22768;&#30340;&#65289;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#26102;&#38388;&#22343;&#21248;Wasserstein&#31283;&#23450;&#24615;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Uniform-in-Time Wasserstein Stability Bounds for (Noisy) Stochastic Gradient Descent. (arXiv:2305.12056v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24314;&#31435;&#23398;&#20064;&#29702;&#35770;&#21644;&#24212;&#29992;&#27010;&#29575;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35777;&#26126;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;Wasserstein&#31283;&#23450;&#24615;&#30028;&#38480;&#30340;&#32479;&#19968;&#25351;&#21335;&#65292;&#24182;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#24378;&#20984;&#25439;&#22833;&#21644;&#24102;&#28155;&#21152;&#22122;&#22768;&#30340;&#38750;&#20984;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#31283;&#23450;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#27010;&#24565;&#65292;&#23545;&#20110;&#25512;&#23548;&#23454;&#36341;&#31639;&#27861;&#30340;&#27867;&#21270;&#30028;&#38480;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#29992;&#30340;&#12290;&#36807;&#21435;&#21313;&#24180;&#24050;&#32463;&#35265;&#35777;&#20102;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#25152;&#24212;&#29992;&#30340;&#19981;&#21516;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#30028;&#38480;&#30340;&#22686;&#21152;&#12290;&#34429;&#28982;&#36825;&#20123;&#30028;&#38480;&#29031;&#20142;&#20102;&#20248;&#21270;&#31639;&#27861;&#30340;&#21508;&#31181;&#23646;&#24615;&#65292;&#20294;&#27599;&#20010;&#26696;&#20363;&#30340;&#20998;&#26512;&#36890;&#24120;&#38656;&#35201;&#19981;&#21516;&#30340;&#35777;&#26126;&#25216;&#26415;&#21644;&#26174;&#33879;&#19981;&#21516;&#30340;&#25968;&#23398;&#24037;&#20855;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#23398;&#20064;&#29702;&#35770;&#21644;&#24212;&#29992;&#27010;&#29575;&#20043;&#38388;&#24314;&#31435;&#20102;&#26032;&#30340;&#32852;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#35777;&#26126;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;Wasserstein&#31283;&#23450;&#24615;&#30028;&#38480;&#30340;&#32479;&#19968;&#25351;&#21335;&#12290;&#25105;&#20204;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#19978;&#38416;&#36848;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#33719;&#24471;&#20102;&#24378;&#20984;&#25439;&#22833;&#21644;&#24102;&#28155;&#21152;&#22122;&#22768;&#30340;&#38750;&#20984;&#25439;&#22833;&#30340;&#26102;&#38388;&#22343;&#21248;&#31283;&#23450;&#24615;&#30028;&#38480;&#65288;&#21363;&#65292;&#30028;&#38480;&#19981;&#38543;&#36845;&#20195;&#27425;&#25968;&#22686;&#21152;&#32780;&#22686;&#21152;&#65289;&#65292;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24674;&#22797;&#20102;&#19982;&#20808;&#21069;&#25991;&#29486;&#30456;&#20284;&#30340;&#32467;&#26524;&#25110;&#23558;&#23427;&#20204;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic stability is an important notion that has proven powerful for deriving generalization bounds for practical algorithms. The last decade has witnessed an increasing number of stability bounds for different algorithms applied on different classes of loss functions. While these bounds have illuminated various properties of optimization algorithms, the analysis of each case typically required a different proof technique with significantly different mathematical tools. In this study, we make a novel connection between learning theory and applied probability and introduce a unified guideline for proving Wasserstein stability bounds for stochastic optimization algorithms. We illustrate our approach on stochastic gradient descent (SGD) and we obtain time-uniform stability bounds (i.e., the bound does not increase with the number of iterations) for strongly convex losses and non-convex losses with additive noise, where we recover similar results to the prior art or extend them to mor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#27700;&#21160;&#21147;&#27169;&#22411;&#65292;&#25104;&#21151;&#21152;&#36895;&#24182;&#25552;&#21319;&#20102;&#27946;&#27700;&#28145;&#24230;&#21644;&#36895;&#24230;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.12052</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#27946;&#27700;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;DL Hydro-FRAN
&lt;/p&gt;
&lt;p&gt;
Deep Learning Hydrodynamic Forecasting for Flooded Region Assessment in Near-Real-Time (DL Hydro-FRAN). (arXiv:2305.12052v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#27700;&#21160;&#21147;&#27169;&#22411;&#65292;&#25104;&#21151;&#21152;&#36895;&#24182;&#25552;&#21319;&#20102;&#27946;&#27700;&#28145;&#24230;&#21644;&#36895;&#24230;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#21160;&#21147;&#27946;&#27700;&#24314;&#27169;&#25552;&#39640;&#20102;&#26292;&#38632;&#20107;&#20214;&#30340;&#27700;&#25991;&#21644;&#27700;&#21147;&#39044;&#27979;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#39640;&#20998;&#36776;&#29575;&#27700;&#21160;&#21147;&#23398;&#25152;&#38656;&#30340;&#35745;&#31639;&#23494;&#38598;&#22411;&#25968;&#20540;&#35299;&#36890;&#24120;&#20250;&#38459;&#30861;&#22312;&#36817;&#23454;&#26102;&#27946;&#27700;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#29992;&#20110;&#20248;&#21270;&#27946;&#27700;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#20351;&#29992;2D HEC-RAS&#27700;&#21160;&#21147;&#27169;&#22411;&#22312;&#20302;&#28023;&#25300;&#12289;&#39640;&#20998;&#36776;&#29575;&#22478;&#24066;&#29615;&#22659;&#20013;&#27169;&#25311;&#20102;&#20960;&#27425;&#26292;&#38632;&#20107;&#20214;&#12290;&#23558;&#36825;&#20123;&#27169;&#25311;&#25968;&#25454;&#29992;&#20110;DNN&#35757;&#32451;&#38598;&#65292;&#39044;&#27979;&#27946;&#27700;&#28145;&#24230;&#21644;&#36895;&#24230;&#12290;&#19982;&#27700;&#21160;&#21147;&#27946;&#27700;&#27169;&#22411;&#30456;&#27604;&#65292;DNN&#27169;&#22411;&#30340;&#39044;&#27979;&#20855;&#26377;&#33391;&#22909;&#30340;&#19968;&#33268;&#24615;&#65292;&#22312;&#30740;&#31350;&#21306;&#22495;&#20869;&#32454;&#32990;&#27946;&#27700;&#28145;&#24230;&#20013;&#65292;&#20013;&#20301;&#25968;RMSE&#32422;&#20026;2&#27627;&#31859;&#12290;&#21516;&#26102;&#65292;DNN&#27169;&#22411;&#39044;&#27979;&#30340;&#35745;&#31639;&#26102;&#38388;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#39044;&#27979;&#26102;&#38388;&#27604;&#20256;&#32479;&#26041;&#27861;&#24555;34.2&#21040;72.4&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hydrodynamic flood modeling improves hydrologic and hydraulic prediction of storm events. However, the computationally intensive numerical solutions required for high-resolution hydrodynamics have historically prevented their implementation in near-real-time flood forecasting. This study examines whether several Deep Neural Network (DNN) architectures are suitable for optimizing hydrodynamic flood models. Several pluvial flooding events were simulated in a low-relief high-resolution urban environment using a 2D HEC-RAS hydrodynamic model. These simulations were assembled into a training set for the DNNs, which were then used to forecast flooding depths and velocities. The DNNs' forecasts were compared to the hydrodynamic flood models, and showed good agreement, with a median RMSE of around 2 mm for cell flooding depths in the study area. The DNNs also improved forecast computation time significantly, with the DNNs providing forecasts between 34.2 and 72.4 times faster than conventional
&lt;/p&gt;</description></item><item><title>SIDAR&#26159;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;3D&#28210;&#26579;&#29983;&#25104;&#22810;&#35270;&#35282;&#12289;&#22810;&#20809;&#29031;&#12289;&#22810;&#38452;&#24433;&#12289;&#22810;&#36974;&#25377;&#31561;&#30495;&#23454;&#22330;&#26223;&#30340;&#26041;&#24335;&#26469;&#35299;&#20915;&#22270;&#20687;&#23545;&#40784;&#21644;&#22270;&#20687;&#20462;&#22797;&#38382;&#39064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.12036</link><description>&lt;p&gt;
SIDAR&#65306;&#29992;&#20110;&#23545;&#40784;&#21644;&#20462;&#22797;&#30340;&#21512;&#25104;&#22270;&#20687;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SIDAR: Synthetic Image Dataset for Alignment &amp; Restoration. (arXiv:2305.12036v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12036
&lt;/p&gt;
&lt;p&gt;
SIDAR&#26159;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;3D&#28210;&#26579;&#29983;&#25104;&#22810;&#35270;&#35282;&#12289;&#22810;&#20809;&#29031;&#12289;&#22810;&#38452;&#24433;&#12289;&#22810;&#36974;&#25377;&#31561;&#30495;&#23454;&#22330;&#26223;&#30340;&#26041;&#24335;&#26469;&#35299;&#20915;&#22270;&#20687;&#23545;&#40784;&#21644;&#22270;&#20687;&#20462;&#22797;&#38382;&#39064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23545;&#40784;&#21644;&#22270;&#20687;&#20462;&#22797;&#26159;&#32463;&#20856;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20173;&#32570;&#20047;&#36275;&#22815;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#21644;&#35780;&#20272;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#33719;&#21462;&#29992;&#20110;&#22270;&#20687;&#23545;&#40784;&#30340;&#22522;&#20934;&#25968;&#25454;&#38656;&#35201;&#31934;&#32454;&#30340;&#36816;&#21160;&#32467;&#26500;&#26041;&#27861;&#25110;&#20809;&#27969;&#31995;&#32479;&#65292;&#36890;&#24120;&#21482;&#25552;&#20379;&#22823;&#37327;&#30340;&#22270;&#20687;&#23545;&#24212;&#20851;&#31995;&#65292;&#32780;&#22312;&#22522;&#30784;&#22270;&#20687;&#24207;&#21015;&#20013;&#20165;&#24341;&#20837;&#23569;&#37327;&#30340;&#22330;&#26223;&#21464;&#21270;&#12290;&#32780;&#26367;&#20195;&#26041;&#27861;&#21017;&#21033;&#29992;&#29616;&#26377;&#22270;&#20687;&#25968;&#25454;&#19978;&#30340;&#38543;&#26426;&#36879;&#35270;&#25197;&#26354;&#12290;&#28982;&#32780;&#65292;&#36825;&#21482;&#25552;&#20379;&#20102;&#24179;&#20961;&#30340;&#25197;&#26354;&#65292;&#32570;&#20047;&#30495;&#23454;&#22330;&#26223;&#30340;&#22797;&#26434;&#24615;&#21644;&#21464;&#21270;&#24615;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;3D&#28210;&#26579;&#26469;&#20811;&#26381;&#25968;&#25454;&#32570;&#20047;&#30340;&#38382;&#39064;&#65306;&#23558;&#22270;&#20687;&#28155;&#21152;&#20026;&#24179;&#38754;&#30340;&#32441;&#29702;&#65292;&#28982;&#21518;&#28155;&#21152;&#19981;&#21516;&#30340;&#20809;&#29031;&#26465;&#20214;&#12289;&#38452;&#24433;&#21644;&#36974;&#25377;&#21040;&#22330;&#26223;&#20013;&#12290;&#22330;&#26223;&#20174;&#22810;&#20010;&#35270;&#35282;&#28210;&#26579;&#65292;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#38598;&#65292;&#20854;&#20013;&#30340;&#23545;&#40784;&#21644;/&#25110;&#36864;&#21270;&#24471;&#21040;&#20102;&#24050;&#30693;&#30340;&#22522;&#20934;&#25968;&#25454;&#65292;&#22240;&#27492;&#20026;&#22270;&#20687;&#23545;&#40784;&#21644;&#20462;&#22797;&#20219;&#21153;&#25552;&#20379;&#20102;&#36866;&#24403;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image alignment and image restoration are classical computer vision tasks. However, there is still a lack of datasets that provide enough data to train and evaluate end-to-end deep learning models. Obtaining ground-truth data for image alignment requires sophisticated structure-from-motion methods or optical flow systems that often do not provide enough data variance, i.e., typically providing a high number of image correspondences, while only introducing few changes of scenery within the underlying image sequences. Alternative approaches utilize random perspective distortions on existing image data. However, this only provides trivial distortions, lacking the complexity and variance of real-world scenarios. Instead, our proposed data augmentation helps to overcome the issue of data scarcity by using 3D rendering: images are added as textures onto a plane, then varying lighting conditions, shadows, and occlusions are added to the scene. The scene is rendered from multiple viewpoints, g
&lt;/p&gt;</description></item><item><title>Waymo&#24320;&#25918;&#27169;&#25311;&#20195;&#29702;&#25361;&#25112;&#36187;&#25552;&#20986;&#20351;&#29992;&#30495;&#23454;&#12289;&#20114;&#21160;&#30340;&#26234;&#33021;&#20307;&#20223;&#30495;&#20197;&#20419;&#36827;&#33258;&#21160;&#39550;&#39542;&#34892;&#20026;&#27169;&#22411;&#30340;&#35780;&#20272;&#21644;&#35757;&#32451;&#65292;&#26159;&#35813;&#39046;&#22495;&#30340;&#39318;&#20010;&#20844;&#24320;&#25361;&#25112;&#36187;&#65292;&#26088;&#22312;&#25512;&#21160;&#36924;&#30495;&#27169;&#25311;&#22120;&#30340;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.12032</link><description>&lt;p&gt;
Waymo&#24320;&#25918;&#27169;&#25311;&#20195;&#29702;&#25361;&#25112;&#36187;
&lt;/p&gt;
&lt;p&gt;
The Waymo Open Sim Agents Challenge. (arXiv:2305.12032v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12032
&lt;/p&gt;
&lt;p&gt;
Waymo&#24320;&#25918;&#27169;&#25311;&#20195;&#29702;&#25361;&#25112;&#36187;&#25552;&#20986;&#20351;&#29992;&#30495;&#23454;&#12289;&#20114;&#21160;&#30340;&#26234;&#33021;&#20307;&#20223;&#30495;&#20197;&#20419;&#36827;&#33258;&#21160;&#39550;&#39542;&#34892;&#20026;&#27169;&#22411;&#30340;&#35780;&#20272;&#21644;&#35757;&#32451;&#65292;&#26159;&#35813;&#39046;&#22495;&#30340;&#39318;&#20010;&#20844;&#24320;&#25361;&#25112;&#36187;&#65292;&#26088;&#22312;&#25512;&#21160;&#36924;&#30495;&#27169;&#25311;&#22120;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23450;&#20041;&#20102;Waymo&#24320;&#25918;&#27169;&#25311;&#20195;&#29702;&#25361;&#25112;&#36187;(WOSAC)&#12290;&#36890;&#36807;&#19982;&#30495;&#23454;&#12289;&#20114;&#21160;&#30340;&#26234;&#33021;&#20307;&#36827;&#34892;&#20223;&#30495;&#26159;&#33258;&#21160;&#39550;&#39542;&#36719;&#20214;&#24320;&#21457;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;WOSAC&#26159;&#31532;&#19968;&#20010;&#20844;&#24320;&#30340;&#25361;&#25112;&#36187;&#65292;&#26088;&#22312;&#35299;&#20915;&#35813;&#20219;&#21153;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#35813;&#25361;&#25112;&#30340;&#30446;&#26631;&#26159;&#28608;&#21457;&#35774;&#35745;&#36924;&#30495;&#27169;&#25311;&#22120;&#30340;&#20852;&#36259;&#65292;&#20197;&#29992;&#20110;&#35780;&#20272;&#21644;&#35757;&#32451;&#33258;&#21160;&#39550;&#39542;&#30340;&#34892;&#20026;&#27169;&#22411;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20960;&#31181;&#22522;&#20934;&#20223;&#30495;&#20195;&#29702;&#26041;&#27861;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we define the Waymo Open Sim Agents Challenge (WOSAC). Simulation with realistic, interactive agents represents a key task for autonomous vehicle software development. WOSAC is the first public challenge to tackle this task and propose corresponding metrics. The goal of the challenge is to stimulate the design of realistic simulators that can be used to evaluate and train a behavior model for autonomous driving. We outline our evaluation methodology and present preliminary results for a number of different baseline simulation agent methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#30340;&#26159;&#22914;&#20309;&#36827;&#34892;&#22270;&#25968;&#25454;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#38754;&#20020;&#30528;&#30001;&#20110;&#25968;&#25454;&#22522;&#30784;&#29305;&#24615;&#25152;&#23548;&#33268;&#30340;&#29702;&#35770;&#19982;&#26041;&#27861;&#19978;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.12030</link><description>&lt;p&gt;
&#23398;&#20064;&#36830;&#32493;&#30340;&#22270;&#24207;&#21015; -- &#21160;&#21147;&#31995;&#32479;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Continually on a Sequence of Graphs -- The Dynamical System Way. (arXiv:2305.12030v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12030
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#30340;&#26159;&#22914;&#20309;&#36827;&#34892;&#22270;&#25968;&#25454;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#38754;&#20020;&#30528;&#30001;&#20110;&#25968;&#25454;&#22522;&#30784;&#29305;&#24615;&#25152;&#23548;&#33268;&#30340;&#29702;&#35770;&#19982;&#26041;&#27861;&#19978;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;(CL)&#26159;&#19968;&#20010;&#39046;&#22495;&#65292;&#20851;&#27880;&#20110;&#23398;&#20064;&#19968;&#31995;&#21015;&#30456;&#20114;&#20851;&#32852;&#30340;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#36890;&#24120;&#26159;&#20197;&#22238;&#24402;&#25110;&#20998;&#31867;&#30340;&#26041;&#24335;&#23450;&#20041;&#30340;&#12290;&#36817;&#24180;&#26469;&#65292;&#24403;&#36825;&#20123;&#20219;&#21153;&#26159;&#20351;&#29992;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#23450;&#20041;&#30340;&#26102;&#65292;&#22914;&#22270;&#20687;&#65292;CL&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#24403;&#19982;CL&#20219;&#21153;&#30456;&#23545;&#24212;&#30340;&#25968;&#25454;&#26159;&#38750;&#27431;&#20960;&#37324;&#24503;&#30340;&#26102;&#65292;&#22914;&#22270;&#24418;&#12289;&#28857;&#20113;&#25110;&#27969;&#24418;&#65292;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#24847;&#20041;&#19979;&#30340;&#30456;&#20284;&#24615;&#27010;&#24565;&#24182;&#19981;&#36866;&#29992;&#12290;&#22240;&#27492;&#65292;&#20026;&#38750;&#27431;&#20960;&#37324;&#24503;&#25968;&#25454;&#24320;&#21457;CL&#38754;&#20020;&#30528;&#20960;&#20010;&#29702;&#35770;&#21644;&#26041;&#27861;&#19978;&#30340;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#65292;&#23545;&#20110;&#22270;&#20013;&#30340;CL&#38656;&#35201;&#26174;&#24335;&#22320;&#27169;&#25311;&#33410;&#28857;&#21644;&#36793;&#30340;&#38750;&#24179;&#31283;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning~(CL) is a field concerned with learning a series of inter-related task with the tasks typically defined in the sense of either regression or classification. In recent years, CL has been studied extensively when these tasks are defined using Euclidean data-- data, such as images, that can be described by a set of vectors in an n-dimensional real space. However, the literature is quite sparse, when the data corresponding to a CL task is nonEuclidean-- data , such as graphs, point clouds or manifold, where the notion of similarity in the sense of Euclidean metric does not hold. For instance, a graph is described by a tuple of vertices and edges and similarities between two graphs is not well defined through a Euclidean metric. Due to this fundamental nature of the data, developing CL for nonEuclidean data presents several theoretical and methodological challenges. In particular, CL for graphs requires explicit modelling of nonstationary behavior of vertices and edges an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MultiTurnCleanup&#20219;&#21153;&#65292;&#25910;&#38598;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;MultiTurnCleanup1&#65292;&#38024;&#23545;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#20013;&#30340;&#19981;&#36830;&#32493;&#29616;&#35937;&#36827;&#34892;&#25506;&#35752;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#21487;&#29992;&#20110;&#26410;&#26469;&#30740;&#31350;&#30340;&#22522;&#20934;&#27979;&#35797;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12029</link><description>&lt;p&gt;
MultiTurnCleanup&#65306;&#29992;&#20110;&#22810;&#36718;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#28165;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup. (arXiv:2305.12029v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MultiTurnCleanup&#20219;&#21153;&#65292;&#25910;&#38598;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;MultiTurnCleanup1&#65292;&#38024;&#23545;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#20013;&#30340;&#19981;&#36830;&#32493;&#29616;&#35937;&#36827;&#34892;&#25506;&#35752;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#21487;&#29992;&#20110;&#26410;&#26469;&#30740;&#31350;&#30340;&#22522;&#20934;&#27979;&#35797;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35821;&#35843;&#19981;&#36830;&#32493;&#26816;&#27979;&#27169;&#22411;&#20391;&#37325;&#20110;&#21333;&#20010;&#35828;&#35805;&#32773;&#30340;&#27599;&#20010;&#35805;&#35821;&#12290;&#28982;&#32780;&#65292;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#20013;&#30340;&#35768;&#22810;&#19981;&#36830;&#32493;&#29616;&#35937;&#37117;&#21457;&#29983;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#65292;&#36825;&#24433;&#21709;&#20102;&#20154;&#31867;&#30340;&#21487;&#35835;&#24615;&#21644;&#19979;&#28216; NLP &#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21019;&#26032;&#30340;&#8220;MultiTurnCleanup&#8221;&#20219;&#21153;&#65292;&#38024;&#23545;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#20013;&#30340;&#19981;&#36830;&#32493;&#29616;&#35937;&#36827;&#34892;&#25506;&#35752;&#65292;&#24182;&#25910;&#38598;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;MultiTurnCleanup1&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#25968;&#25454;&#26631;&#27880;&#27169;&#24335;&#20197;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#25968;&#25454;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20004;&#31181;&#24314;&#27169;&#26041;&#27861;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#65292;&#20316;&#20026;&#26410;&#26469;&#30740;&#31350;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current disfluency detection models focus on individual utterances each from a single speaker. However, numerous discontinuity phenomena in spoken conversational transcripts occur across multiple turns, hampering human readability and the performance of downstream NLP tasks. This study addresses these phenomena by proposing an innovative Multi-Turn Cleanup task for spoken conversational transcripts and collecting a new dataset, MultiTurnCleanup1. We design a data labeling schema to collect the high-quality dataset and provide extensive data analysis. Furthermore, we leverage two modeling approaches for experimental evaluation as benchmarks for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#25928;&#38543;&#24577;&#30005;&#23481;&#22120;&#30340;&#29289;&#29702;&#20648;&#23618;&#35745;&#31639;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#25968;&#25454;&#30340;&#20998;&#31867;&#20219;&#21153;&#21644;&#20998;&#26512;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#31995;&#32479;&#33021;&#22815;&#23454;&#29616;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#36739;&#23567;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.12025</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#25928;&#38543;&#24577;&#30005;&#23481;&#22120;&#30340;&#29289;&#29702;&#20648;&#23618;&#35745;&#31639;&#31995;&#32479;&#29992;&#20110;&#26102;&#38388;&#25968;&#25454;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Energy-efficient memcapacitive physical reservoir computing system for temporal data processing. (arXiv:2305.12025v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#25928;&#38543;&#24577;&#30005;&#23481;&#22120;&#30340;&#29289;&#29702;&#20648;&#23618;&#35745;&#31639;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#25968;&#25454;&#30340;&#20998;&#31867;&#20219;&#21153;&#21644;&#20998;&#26512;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#31995;&#32479;&#33021;&#22815;&#23454;&#29616;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#36739;&#23567;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20648;&#23618;&#35745;&#31639;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#36755;&#20837;&#20449;&#21495;&#20013;&#25552;&#21462;&#29305;&#24449;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#39640;&#32500;&#31354;&#38388;&#26469;&#22788;&#29702;&#26102;&#38388;&#25968;&#25454;&#12290;&#29289;&#29702;&#20648;&#23618;&#21487;&#20351;&#29992;&#30913;&#26059;&#30005;&#23376;&#12289;&#21407;&#23376;&#24320;&#20851;&#32593;&#32476;&#12289;&#30789;&#20809;&#23398;&#27169;&#22359;&#12289;&#38081;&#30005;&#26230;&#20307;&#31649;&#21644;&#26131;&#22833;&#24615;&#23384;&#20648;&#22120;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35774;&#22791;&#30001;&#20110;&#20854;&#30005;&#38459;&#24615;&#36136;&#26412;&#36136;&#19978;&#23384;&#22312;&#33021;&#37327;&#32791;&#25955;&#38382;&#39064;&#65292;&#23548;&#33268;&#21151;&#32791;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#37319;&#29992;&#30005;&#23481;&#23384;&#20648;&#22120;&#35774;&#22791;&#21487;&#25552;&#20379;&#26356;&#20026;&#33021;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21033;&#29992;&#27169;&#25311;&#21644;&#23454;&#39564;&#20013;&#36817;&#20284;&#26576;&#20123;&#30701;&#26399;&#31361;&#35302;&#21487;&#22609;&#24615;&#21151;&#33021;&#30340;&#26131;&#22833;&#29983;&#29289;&#33180;&#22522;&#36136;&#37327;&#20316;&#20026;&#20648;&#23618;&#65292;&#35299;&#20915;&#20998;&#31867;&#20219;&#21153;&#21644;&#20998;&#26512;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#21475;&#38899;&#25968;&#23383;&#20998;&#31867;&#20013;&#23454;&#29616;&#20102;98&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#20108;&#38454;&#38750;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;0.0012&#30340;&#24402;&#19968;&#21270;&#22343;&#26041;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reservoir computing is a highly efficient machine learning framework for processing temporal data by extracting features from the input signal and mapping them into higher dimensional spaces. Physical reservoir layers have been realized using spintronic oscillators, atomic switch networks, silicon photonic modules, ferroelectric transistors, and volatile memristors. However, these devices are intrinsically energy-dissipative due to their resistive nature, which leads to increased power consumption. Therefore, capacitive memory devices can provide a more energy-efficient approach. Here, we leverage volatile biomembrane-based memcapacitors that closely mimic certain short-term synaptic plasticity functions as reservoirs to solve classification tasks and analyze time-series data in simulation and experimentally. Our system achieves a 98% accuracy rate for spoken digit classification and a normalized mean square error of 0.0012 in a second-order non-linear regression task. Further, to demo
&lt;/p&gt;</description></item><item><title>Chemellia&#26159;&#19968;&#20010;&#24320;&#28304;&#21407;&#23376;&#23618;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#24211;&#35774;&#35745;&#20851;&#27880;&#28857;&#20998;&#31163;&#12289;&#20114;&#25805;&#20316;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#20854;&#37325;&#35201;&#36129;&#29486;&#26159;&#23454;&#29616;&#19968;&#31181;&#29992;&#20110;&#26448;&#26009;&#23646;&#24615;&#39044;&#27979;&#30340;&#26230;&#20307;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2305.12010</link><description>&lt;p&gt;
Chemellia: &#29992;&#20110;&#21407;&#23376;&#23618;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#30340;&#29983;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Chemellia: An Ecosystem for Atomistic Scientific Machine Learning. (arXiv:2305.12010v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12010
&lt;/p&gt;
&lt;p&gt;
Chemellia&#26159;&#19968;&#20010;&#24320;&#28304;&#21407;&#23376;&#23618;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#24211;&#35774;&#35745;&#20851;&#27880;&#28857;&#20998;&#31163;&#12289;&#20114;&#25805;&#20316;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#20854;&#37325;&#35201;&#36129;&#29486;&#26159;&#23454;&#29616;&#19968;&#31181;&#29992;&#20110;&#26448;&#26009;&#23646;&#24615;&#39044;&#27979;&#30340;&#26230;&#20307;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Chemellia&#26159;&#19968;&#20010;&#22522;&#20110;Julia&#32534;&#31243;&#35821;&#35328;&#30340;&#21407;&#23376;&#23618;&#26426;&#22120;&#23398;&#20064;&#30340;&#24320;&#28304;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#20102;Julia&#30340;&#39640;&#36895;&#24230;&#20197;&#21450;&#36890;&#36807;&#22810;&#37325;&#27966;&#29983;&#33539;&#24335;&#20849;&#20139;&#21644;&#37325;&#22797;&#20351;&#29992;&#20195;&#30721;&#21644;&#25509;&#21475;&#30340;&#33021;&#21147;&#12290;Chemellia&#26088;&#22312;&#21033;&#29992;&#29616;&#26377;&#30340;&#25509;&#21475;&#65292;&#24182;&#22312;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#36991;&#20813;&#8220;&#37325;&#22797;&#36896;&#36718;&#23376;&#8221;&#12290;Chemellia&#29983;&#24577;&#31995;&#32479;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;ChemistryFeaturization&#30028;&#38754;&#65292;&#29992;&#20110;&#23450;&#20041;&#21644;&#32534;&#30721;&#29305;&#24449;&#8212;&#8212;&#26088;&#22312;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#29305;&#24449;&#21270;&#26041;&#26696;&#21450;&#20854;&#20803;&#32032;&#20043;&#38388;&#30340;&#20114;&#25805;&#20316;&#24615;&#65292;&#20445;&#25345;&#32534;&#30721;&#29305;&#24449;&#30340;&#26469;&#28304;&#65292;&#24182;&#30830;&#20445;&#26131;&#20110;&#35299;&#30721;&#21644;&#37325;&#26032;&#37197;&#32622;&#65292;&#20197;&#21551;&#29992;&#29305;&#24449;&#24037;&#31243;&#23454;&#39564;&#12290;&#36825;&#20307;&#29616;&#20102;Chemellia&#29983;&#24577;&#31995;&#32479;&#30340;&#24635;&#20307;&#35774;&#35745;&#21407;&#21017;&#65306;&#20851;&#27880;&#28857;&#20998;&#31163;&#12289;&#20114;&#25805;&#20316;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#35752;&#35770;&#29992;&#20110;&#26448;&#26009;&#23646;&#24615;&#39044;&#27979;&#30340;&#26230;&#20307;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#29616;&#26469;&#35828;&#26126;&#36825;&#20123;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chemellia is an open-source framework for atomistic machine learning in the Julia programming language. The framework takes advantage of Julia's high speed as well as the ability to share and reuse code and interfaces through the paradigm of multiple dispatch. Chemellia is designed to make use of existing interfaces and avoid ``reinventing the wheel'' wherever possible. A key aspect of the Chemellia ecosystem is the ChemistryFeaturization interface for defining and encoding features -- it is designed to maximize interoperability between featurization schemes and elements thereof, to maintain provenance of encoded features, and to ensure easy decodability and reconfigurability to enable feature engineering experiments. This embodies the overall design principles of the Chemellia ecosystem: separation of concerns, interoperability, and transparency. We illustrate these principles by discussing the implementation of crystal graph convolutional neural networks for material property predict
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OMPify&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#20018;&#34892;&#20195;&#30721;&#30340;&#20998;&#26512;&#65292;&#33258;&#21160;&#26816;&#27979;&#21644;&#39044;&#27979;&#24182;&#34892;&#20195;&#30721;&#20013;&#30340;OpenMP&#32534;&#35793;&#25351;&#31034;&#31526;&#21644;&#20849;&#20139;&#20869;&#23384;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11999</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#22270;&#24418;&#26041;&#27861;&#20026;OpenMP&#24182;&#34892;&#21270;&#25552;&#20379;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Advising OpenMP Parallelization via a Graph-Based Approach with Transformers. (arXiv:2305.11999v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OMPify&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#20018;&#34892;&#20195;&#30721;&#30340;&#20998;&#26512;&#65292;&#33258;&#21160;&#26816;&#27979;&#21644;&#39044;&#27979;&#24182;&#34892;&#20195;&#30721;&#20013;&#30340;OpenMP&#32534;&#35793;&#25351;&#31034;&#31526;&#21644;&#20849;&#20139;&#20869;&#23384;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22810;&#26680;&#26550;&#26500;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#38656;&#35201;&#20849;&#20139;&#20869;&#23384;&#24182;&#34892;&#21270;&#26041;&#26696;&#12290;&#24403;&#21069;&#26368;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;OpenMP&#24182;&#34892;&#32534;&#31243;&#25509;&#21475;&#12290;&#34429;&#28982;&#25163;&#21160;&#32534;&#20889;&#24182;&#34892;&#20195;&#30721;&#26159;&#22797;&#26434;&#21644;&#36153;&#21147;&#30340;&#65292;&#20294;&#26159;&#35768;&#22810;&#30830;&#23450;&#24615;&#28304;&#21040;&#28304;&#65288;S2S&#65289;&#32534;&#35793;&#22120;&#24050;&#32463;&#28044;&#29616;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#23558;&#20018;&#34892;&#20195;&#30721;&#36716;&#25442;&#20026;&#24182;&#34892;&#20195;&#30721;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#36825;&#20123;&#32534;&#35793;&#22120;&#26159;&#19981;&#23454;&#38469;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;AI&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#19982;&#22823;&#37327;&#30340;&#24320;&#28304;&#20195;&#30721;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#33258;&#21160;&#24182;&#34892;&#21270;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;OMPify&#65292;&#36890;&#36807;&#20018;&#34892;&#20195;&#30721;&#26469;&#26816;&#27979;&#21644;&#39044;&#27979;&#24182;&#34892;&#20195;&#30721;&#20013;&#30340;OpenMP&#32534;&#35793;&#25351;&#31034;&#31526;&#21644;&#20849;&#20139;&#20869;&#23384;&#23646;&#24615;&#65292;OMPify&#22522;&#20110;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#28304;&#20195;&#30721;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an ever-present need for shared memory parallelization schemes to exploit the full potential of multi-core architectures. The most common parallelization API addressing this need today is OpenMP. Nevertheless, writing parallel code manually is complex and effort-intensive. Thus, many deterministic source-to-source (S2S) compilers have emerged, intending to automate the process of translating serial to parallel code. However, recent studies have shown that these compilers are impractical in many scenarios. In this work, we combine the latest advancements in the field of AI and natural language processing (NLP) with the vast amount of open-source code to address the problem of automatic parallelization. Specifically, we propose a novel approach, called OMPify, to detect and predict the OpenMP pragmas and shared-memory attributes in parallel code, given its serial version. OMPify is based on a Transformer-based model that leverages a graph-based representation of source code that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#31070;&#32463;&#32593;&#32476;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#38024;&#23545;&#33258;&#28982;&#21457;&#29983;&#30340;&#27169;&#22411;&#21464;&#21270;&#25552;&#20379;&#39640;&#27010;&#29575;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11997</link><description>&lt;p&gt;
&#20855;&#26377;&#27010;&#29575;&#20445;&#35777;&#30340;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Robust Counterfactual Explanations for Neural Networks With Probabilistic Guarantees. (arXiv:2305.11997v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#31070;&#32463;&#32593;&#32476;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#38024;&#23545;&#33258;&#28982;&#21457;&#29983;&#30340;&#27169;&#22411;&#21464;&#21270;&#25552;&#20379;&#39640;&#27010;&#29575;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#21457;&#29616;&#20559;&#31227;&#65292;&#36890;&#36807;&#20351;&#29992;&#31283;&#23450;&#24615;&#24230;&#37327;&#26469;&#37327;&#21270;&#21453;&#20107;&#23454;&#35299;&#37322;&#23545;&#21487;&#33021;&#30340;&#27169;&#22411;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#22312;&#21453;&#20107;&#23454;&#35299;&#37322;&#20248;&#21270;&#20013;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#26469;&#23558;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#38752;&#36817;&#25968;&#25454;&#27969;&#24418;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#33258;&#28982;&#21457;&#29983;&#30340;&#27169;&#22411;&#21464;&#21270;&#30340;&#39640;&#27010;&#29575;&#40065;&#26834;&#24615;&#12290;&#26032;&#30340;&#31639;&#27861;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an emerging interest in generating robust counterfactual explanations that would remain valid if the model is updated or changed even slightly. Towards finding robust counterfactuals, existing literature often assumes that the original model $m$ and the new model $M$ are bounded in the parameter space, i.e., $\|\text{Params}(M){-}\text{Params}(m)\|{&lt;}\Delta$. However, models can often change significantly in the parameter space with little to no change in their predictions or accuracy on the given dataset. In this work, we introduce a mathematical abstraction termed \emph{naturally-occurring} model change, which allows for arbitrary changes in the parameter space such that the change in predictions on points that lie on the data manifold is limited. Next, we propose a measure -- that we call \emph{Stability} -- to quantify the robustness of counterfactuals to potential model changes for differentiable models, e.g., neural networks. Our main contribution is to show that counter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36719;&#20214;&#22270;&#20687;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#65292;&#21253;&#25324;&#21435;&#39532;&#36187;&#20811;&#12289;&#38477;&#22122;&#21644;&#22686;&#24378;&#31561;&#22810;&#20010;&#36807;&#31243;&#65292;&#30740;&#31350;&#24182;&#20998;&#26512;&#20102;&#26368;&#26032;&#30340;&#20960;&#39033;&#30740;&#31350;&#65292;&#24182;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#25913;&#36827;&#28857;&#30340;&#25506;&#35752;&#12290;</title><link>http://arxiv.org/abs/2305.11994</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36719;&#20214;&#22270;&#20687;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Survey on software ISP methods based on Deep Learning. (arXiv:2305.11994v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36719;&#20214;&#22270;&#20687;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#65292;&#21253;&#25324;&#21435;&#39532;&#36187;&#20811;&#12289;&#38477;&#22122;&#21644;&#22686;&#24378;&#31561;&#22810;&#20010;&#36807;&#31243;&#65292;&#30740;&#31350;&#24182;&#20998;&#26512;&#20102;&#26368;&#26032;&#30340;&#20960;&#39033;&#30740;&#31350;&#65292;&#24182;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#25913;&#36827;&#28857;&#30340;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#26426;&#30340;&#25972;&#20010;&#22270;&#20687;&#20449;&#21495;&#22788;&#29702;&#22120;&#65288;ISP&#65289;&#20381;&#38752;&#22810;&#20010;&#36807;&#31243;&#23558;&#26469;&#33258;&#24425;&#33394;&#28388;&#27874;&#38453;&#21015;&#65288;CFA&#65289;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#36716;&#25442;&#65292;&#20363;&#22914;&#21435;&#39532;&#36187;&#20811;&#12289;&#38477;&#22122;&#21644;&#22686;&#24378;&#12290;&#36825;&#20123;&#36807;&#31243;&#21487;&#20197;&#36890;&#36807;&#26576;&#20123;&#30828;&#20214;&#25110;&#36719;&#20214;&#26469;&#25191;&#34892;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#20102;&#20854;&#20013;&#19968;&#20123;&#36807;&#31243;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29978;&#33267;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#25972;&#20010;ISP&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#35813;&#39046;&#22495;&#20869;&#30340;&#20960;&#39033;&#26368;&#26032;&#30740;&#31350;&#65292;&#24182;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#21644;&#27604;&#36739;&#65292;&#21253;&#25324;&#32467;&#26524;&#21450;&#26410;&#26469;&#30740;&#31350;&#30340;&#21487;&#33021;&#25913;&#36827;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The entire Image Signal Processor (ISP) of a camera relies on several processes to transform the data from the Color Filter Array (CFA) sensor, such as demosaicing, denoising, and enhancement. These processes can be executed either by some hardware or via software. In recent years, Deep Learning has emerged as one solution for some of them or even to replace the entire ISP using a single neural network for the task. In this work, we investigated several recent pieces of research in this area and provide deeper analysis and comparison among them, including results and possible points of improvement for future researchers.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#21322;&#30417;&#30563;&#21644;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#33258;&#21160;&#26816;&#27979;&#39640;&#20135;&#20892;&#30000;&#65292;&#33719;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#26377;&#26395;&#20026;&#20892;&#27665;&#25552;&#20379;&#24110;&#21161;.</title><link>http://arxiv.org/abs/2305.11990</link><description>&lt;p&gt;
&#39640;&#20135;&#20892;&#30000;&#26816;&#27979;&#65306;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#28145;&#24230;&#23398;&#20064;&#22522;&#20934;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Productive Crop Field Detection: A New Dataset and Deep Learning Benchmark Results. (arXiv:2305.11990v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#21322;&#30417;&#30563;&#21644;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#33258;&#21160;&#26816;&#27979;&#39640;&#20135;&#20892;&#30000;&#65292;&#33719;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#26377;&#26395;&#20026;&#20892;&#27665;&#25552;&#20379;&#24110;&#21161;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31934;&#20934;&#20892;&#19994;&#20013;&#65292;&#26816;&#27979;&#39640;&#20135;&#20892;&#30000;&#26159;&#19968;&#39033;&#24517;&#35201;&#30340;&#23454;&#36341;&#65292;&#20351;&#24471;&#20892;&#27665;&#21487;&#20197;&#21333;&#29420;&#35780;&#20272;&#25805;&#20316;&#32489;&#25928;&#24182;&#27604;&#36739;&#19981;&#21516;&#30340;&#31181;&#23376;&#21697;&#31181;&#12289;&#20892;&#33647;&#21644;&#32933;&#26009;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#35782;&#21035;&#39640;&#20135;&#20892;&#30000;&#24448;&#24448;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#20219;&#21153;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#23581;&#35797;&#20351;&#29992;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26816;&#27979;&#20892;&#30000;&#65292;&#20294;&#24448;&#24448;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#23427;&#26159;&#36890;&#36807;&#26426;&#22120;&#25805;&#20316;&#32467;&#21512;&#38543;&#30528;&#26102;&#38388;&#25512;&#31227;&#32780;&#36319;&#36394;&#30340;Sentinel-2&#22270;&#20687;&#29983;&#25104;&#30340;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#25216;&#26415;&#20811;&#26381;&#26631;&#35760;&#26679;&#26412;&#19981;&#36275;&#30340;&#25968;&#25454;&#38598;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#24212;&#29992;&#21322;&#30417;&#30563;&#26080;&#26631;&#31614;&#25968;&#25454;&#20998;&#31867;&#21644;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#33258;&#21160;&#26816;&#27979;&#39640;&#20135;&#20892;&#30000;&#12290;&#26368;&#32456;&#32467;&#26524;&#34920;&#26126;&#22312;&#27491;&#26080;&#26631;&#35760;&#23398;&#20064;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#38750;&#24120;&#36866;&#21512;
&lt;/p&gt;
&lt;p&gt;
In precision agriculture, detecting productive crop fields is an essential practice that allows the farmer to evaluate operating performance separately and compare different seed varieties, pesticides, and fertilizers. However, manually identifying productive fields is often a time-consuming and error-prone task. Previous studies explore different methods to detect crop fields using advanced machine learning algorithms, but they often lack good quality labeled data. In this context, we propose a high-quality dataset generated by machine operation combined with Sentinel-2 images tracked over time. As far as we know, it is the first one to overcome the lack of labeled samples by using this technique. In sequence, we apply a semi-supervised classification of unlabeled data and state-of-the-art supervised and self-supervised deep learning methods to detect productive crop fields automatically. Finally, the results demonstrate high accuracy in Positive Unlabeled learning, which perfectly fi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;OL-Transformer&#29992;&#20110;&#20809;&#23398;&#22810;&#23618;&#34180;&#33180;&#32467;&#26500;&#65292;&#21487;&#20197;&#39044;&#27979;&#22810;&#36798;$10^{25}$&#31181;&#19981;&#21516;&#22810;&#23618;&#32467;&#26500;&#30340;&#31934;&#30830;&#21453;&#23556;&#21644;&#36879;&#23556;&#20809;&#35889;&#65292;&#21516;&#26102;&#20855;&#26377;&#24555;&#36895;&#30340;&#35745;&#31639;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.11984</link><description>&lt;p&gt;
OL-Transformer&#65306;&#29992;&#20110;&#20809;&#23398;&#22810;&#23618;&#34180;&#33180;&#32467;&#26500;&#30340;&#24555;&#36895;&#36890;&#29992;&#20195;&#29702;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
OL-Transformer: A Fast and Universal Surrogate Simulator for Optical Multilayer Thin Film Structures. (arXiv:2305.11984v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11984
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;OL-Transformer&#29992;&#20110;&#20809;&#23398;&#22810;&#23618;&#34180;&#33180;&#32467;&#26500;&#65292;&#21487;&#20197;&#39044;&#27979;&#22810;&#36798;$10^{25}$&#31181;&#19981;&#21516;&#22810;&#23618;&#32467;&#26500;&#30340;&#31934;&#30830;&#21453;&#23556;&#21644;&#36879;&#23556;&#20809;&#35889;&#65292;&#21516;&#26102;&#20855;&#26377;&#24555;&#36895;&#30340;&#35745;&#31639;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26368;&#36817;&#34987;&#35777;&#26126;&#26159;&#29992;&#20110;&#20809;&#23398;&#22810;&#23618;&#34180;&#33180;&#32467;&#26500;&#30340;&#24555;&#36895;&#20934;&#30830;&#30340;&#20195;&#29702;&#27169;&#25311;&#22120;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20165;&#36866;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#26448;&#26009;&#25490;&#21015;&#26041;&#24335;&#30340;&#26377;&#38480;&#31867;&#22411;&#30340;&#32467;&#26500;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#21521;&#22810;&#26679;&#21270;&#21644;&#36890;&#29992;&#21270;&#32467;&#26500;&#30340;&#24212;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Opto-Layer&#65288;OL&#65289;Transformer&#20316;&#20026;&#24040;&#37327;&#32467;&#26500;&#30340;&#36890;&#29992;&#26367;&#20195;&#27169;&#25311;&#22120;&#12290;&#32467;&#21512;&#32467;&#26500;&#24207;&#21015;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#39044;&#27979;&#22810;&#36798;$10^{25}$&#31181;&#19981;&#21516;&#22810;&#23618;&#32467;&#26500;&#30340;&#31934;&#30830;&#21453;&#23556;&#21644;&#36879;&#23556;&#20809;&#35889;&#65292;&#21516;&#26102;&#30456;&#36739;&#20110;&#29289;&#29702;&#27714;&#35299;&#22120;&#20173;&#28982;&#23454;&#29616;&#20102;6&#20493;&#26102;&#38388;&#21152;&#36895;&#12290;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26222;&#36941;&#30340;&#23398;&#20064;&#33021;&#21147;&#26469;&#33258;&#20110;&#25105;&#20204;&#30340;&#27169;&#22411;&#39318;&#20808;&#23398;&#20064;&#29289;&#29702;&#23884;&#20837;&#65292;&#28982;&#21518;&#20351;&#29992;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#25417;&#27599;&#23618;&#20043;&#38388;&#30340;&#20809;&#29289;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#38544;&#34255;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based methods have recently been established as fast and accurate surrogate simulators for optical multilayer thin film structures. However, existing methods only work for limited types of structures with different material arrangements, preventing their applications towards diverse and universal structures. Here, we propose the Opto-Layer (OL) Transformer to act as a universal surrogate simulator for enormous types of structures. Combined with the technique of structure serialization, our model can predict accurate reflection and transmission spectra for up to $10^{25}$ different multilayer structures, while still achieving a six-fold time speedup compared to physical solvers. Further investigation reveals that the general learning ability comes from the fact that our model first learns the physical embeddings and then uses the self-attention mechanism to capture the hidden relationship of light-matter interaction between each layer.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;PC&#30340;&#26032;&#22411;&#26102;&#24207;&#35760;&#24518;&#27169;&#22411;&#65292;&#31216;&#20026;&#26102;&#38388;&#39044;&#27979;&#32534;&#30721;&#65288;tPC&#65289;&#65292;&#21487;&#20197;&#36890;&#36807;&#29983;&#29289;&#21487;&#34892;&#30340;&#31070;&#32463;&#23454;&#29616;&#20934;&#30830;&#22320;&#35760;&#24518;&#21644;&#26816;&#32034;&#36830;&#32493;&#36755;&#20837;&#12290;&#20854;&#20013;tPC&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#31181;&#32463;&#20856;&#24322;&#21521;&#24615;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#65288;AHN&#65289;&#65292;&#20855;&#26377;&#26356;&#31283;&#23450;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#32534;&#30721;&#19978;&#19979;&#25991;&#30456;&#20851;&#20449;&#24687;&#65292;&#21306;&#20998;&#22312;&#24207;&#21015;&#20013;&#20986;&#29616;&#30340;&#37325;&#22797;&#20803;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.11982</link><description>&lt;p&gt;
&#24102;&#26377;&#26102;&#38388;&#39044;&#27979;&#32534;&#30721;&#30340;&#26102;&#24207;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Sequential Memory with Temporal Predictive Coding. (arXiv:2305.11982v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11982
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;PC&#30340;&#26032;&#22411;&#26102;&#24207;&#35760;&#24518;&#27169;&#22411;&#65292;&#31216;&#20026;&#26102;&#38388;&#39044;&#27979;&#32534;&#30721;&#65288;tPC&#65289;&#65292;&#21487;&#20197;&#36890;&#36807;&#29983;&#29289;&#21487;&#34892;&#30340;&#31070;&#32463;&#23454;&#29616;&#20934;&#30830;&#22320;&#35760;&#24518;&#21644;&#26816;&#32034;&#36830;&#32493;&#36755;&#20837;&#12290;&#20854;&#20013;tPC&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#31181;&#32463;&#20856;&#24322;&#21521;&#24615;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#65288;AHN&#65289;&#65292;&#20855;&#26377;&#26356;&#31283;&#23450;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#32534;&#30721;&#19978;&#19979;&#25991;&#30456;&#20851;&#20449;&#24687;&#65292;&#21306;&#20998;&#22312;&#24207;&#21015;&#20013;&#20986;&#29616;&#30340;&#37325;&#22797;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29983;&#29289;&#20307;&#23384;&#20648;&#20107;&#20214;&#24207;&#21015;&#30340;&#26102;&#38388;&#39034;&#24207;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#22823;&#33041;&#20013;&#25903;&#37197;&#26102;&#24207;&#35760;&#24518;&#30340;&#35745;&#31639;&#26426;&#21046;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#29702;&#35770;&#21644;&#39044;&#27979;&#32534;&#30721;&#65288;PC&#65289;&#22312;&#38745;&#24577;&#23384;&#20648;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#21551;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;PC&#30340;&#26032;&#22411;&#26102;&#24207;&#35760;&#24518;&#27169;&#22411;&#65292;&#31216;&#20026;&#26102;&#38388;&#39044;&#27979;&#32534;&#30721;&#65288;tPC&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;tPC&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#29983;&#29289;&#21487;&#34892;&#30340;&#31070;&#32463;&#23454;&#29616;&#20934;&#30830;&#22320;&#35760;&#24518;&#21644;&#26816;&#32034;&#36830;&#32493;&#36755;&#20837;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#30740;&#31350;&#34920;&#26126;&#65292;tPC&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#31181;&#20855;&#26377;&#38544;&#24335;&#32479;&#35745;&#30333;&#21270;&#36807;&#31243;&#30340;&#32463;&#20856;&#24322;&#21521;&#24615;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#65288;AHN&#65289;&#65292;&#36825;&#20250;&#22312;&#32467;&#26500;&#21270;&#36755;&#20837;&#30340;&#26102;&#24207;&#35760;&#24518;&#20219;&#21153;&#20013;&#23548;&#33268;&#26356;&#31283;&#23450;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20855;&#26377;&#22810;&#23618;&#32467;&#26500;&#30340;tPC&#21487;&#20197;&#32534;&#30721;&#19978;&#19979;&#25991;&#30456;&#20851;&#20449;&#24687;&#65292;&#22240;&#27492;&#21487;&#20197;&#21306;&#20998;&#22312;&#24207;&#21015;&#20013;&#20986;&#29616;&#30340;&#37325;&#22797;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memorizing the temporal order of event sequences is critical for the survival of biological agents. However, the computational mechanism underlying sequential memory in the brain remains unclear. Inspired by neuroscience theories and recent successes in applying predictive coding (PC) to static memory tasks, in this work we propose a novel PC-based model for sequential memory, called temporal predictive coding (tPC). We show that our tPC models can memorize and retrieve sequential inputs accurately with a biologically plausible neural implementation. Importantly, our analytical study reveals that tPC can be viewed as a classical Asymmetric Hopfield Network (AHN) with an implicit statistical whitening process, which leads to more stable performance in sequential memory tasks of structured inputs. Moreover, we find that tPC with a multi-layer structure can encode context-dependent information, thus distinguishing between repeating elements appearing in a sequence, a computation attribute
&lt;/p&gt;</description></item><item><title>AutoCoreset&#26159;&#19968;&#20010;&#33258;&#21160;&#26500;&#24314;&#39640;&#36136;&#37327;Coreset&#30340;&#36890;&#29992;&#19988;&#23454;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#25143;&#21482;&#38656;&#25552;&#20379;&#25968;&#25454;&#21644;&#25104;&#26412;&#20989;&#25968;&#21363;&#21487;&#65292;&#26080;&#38656;&#20854;&#20182;&#35745;&#31639;&#65292;&#21487;&#29992;&#20110;&#20219;&#20309;&#25968;&#25454;&#21644;&#25104;&#26412;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.11980</link><description>&lt;p&gt;
AutoCoreset&#65306;&#19968;&#20010;&#33258;&#21160;&#23454;&#29992;&#30340;Coreset&#26500;&#24314;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
AutoCoreset: An Automatic Practical Coreset Construction Framework. (arXiv:2305.11980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11980
&lt;/p&gt;
&lt;p&gt;
AutoCoreset&#26159;&#19968;&#20010;&#33258;&#21160;&#26500;&#24314;&#39640;&#36136;&#37327;Coreset&#30340;&#36890;&#29992;&#19988;&#23454;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#25143;&#21482;&#38656;&#25552;&#20379;&#25968;&#25454;&#21644;&#25104;&#26412;&#20989;&#25968;&#21363;&#21487;&#65292;&#26080;&#38656;&#20854;&#20182;&#35745;&#31639;&#65292;&#21487;&#29992;&#20110;&#20219;&#20309;&#25968;&#25454;&#21644;&#25104;&#26412;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Coreset&#26159;&#19968;&#20010;&#19982;&#36755;&#20837;&#38598;&#21512;&#32039;&#23494;&#30456;&#20284;&#20110;&#26576;&#31181;&#29305;&#23450;&#26597;&#35810;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#21152;&#26435;&#23376;&#38598;&#12290;&#30001;&#20110;Coreset&#23545;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#65292;&#22240;&#27492;Coreset&#25104;&#20026;&#20102;&#26222;&#36941;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;Coreset&#30340;&#26500;&#24314;&#24448;&#24448;&#26159;&#38382;&#39064;&#20381;&#36182;&#30340;&#65292;&#21363;&#23545;&#20110;&#27599;&#20010;&#38382;&#39064;&#65292;&#36890;&#24120;&#37117;&#20250;&#24314;&#35758;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;Coreset&#26500;&#24314;&#31639;&#27861;&#65292;&#36825;&#19968;&#36807;&#31243;&#21487;&#33021;&#38656;&#35201;&#26102;&#38388;&#65292;&#25110;&#32773;&#23545;&#20110;&#35813;&#39046;&#22495;&#30340;&#26032;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#21487;&#33021;&#24456;&#38590;&#12290;&#21363;&#20351;&#26159;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#25143;&#20063;&#38656;&#35201;&#23436;&#25104;&#20854;&#20182;&#65288;&#38382;&#39064;&#30456;&#20851;&#30340;&#65289;&#35745;&#31639;&#25110;&#35777;&#26126;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#38382;&#39064;&#24182;&#27809;&#26377;&#65288;&#21487;&#35777;&#26126;&#30340;&#65289;&#23567;&#30340;Coreset&#65292;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#23454;&#29992;&#30340;&#26694;&#26550;&#26469;&#26500;&#24314;Coreset&#65292;&#23427;&#21482;&#38656;&#35201;&#29992;&#25143;&#36755;&#20837;&#25968;&#25454;&#21644;&#26399;&#26395;&#30340;&#25104;&#26412;&#20989;&#25968;&#65292;&#26080;&#38656;&#29992;&#25143;&#23436;&#25104;&#20219;&#20309;&#20854;&#20182;&#20219;&#21153;&#30456;&#20851;&#30340;&#35745;&#31639;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#23558;&#22823;&#25968;&#25454;&#38598;&#36716;&#25442;&#20026;&#23567;Coreset&#31616;&#21270;&#20026;&#19968;&#20010;&#31616;&#21333;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#30001;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#33258;&#21160;&#35299;&#20915;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26159;&#36890;&#29992;&#30340;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#25968;&#25454;&#21644;&#25104;&#26412;&#20989;&#25968;&#65292;&#24182;&#19988;&#20063;&#26159;&#23454;&#29992;&#30340;&#65292;&#22240;&#20026;&#23427;&#33021;&#24555;&#36895;&#26500;&#24314;Coreset&#65292;&#21516;&#26102;&#20445;&#35777;&#39640;&#36136;&#37327;&#30340;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
A coreset is a tiny weighted subset of an input set, that closely resembles the loss function, with respect to a certain set of queries. Coresets became prevalent in machine learning as they have shown to be advantageous for many applications. While coreset research is an active research area, unfortunately, coresets are constructed in a problem-dependent manner, where for each problem, a new coreset construction algorithm is usually suggested, a process that may take time or may be hard for new researchers in the field. Even the generic frameworks require additional (problem-dependent) computations or proofs to be done by the user. Besides, many problems do not have (provable) small coresets, limiting their applicability. To this end, we suggest an automatic practical framework for constructing coresets, which requires (only) the input data and the desired cost function from the user, without the need for any other task-related computation to be done by the user. To do so, we reduce t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#21464;&#28857;&#26816;&#27979;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#28789;&#27963;&#36866;&#29992;&#20110;&#21508;&#31181;&#25968;&#25454;&#28304;&#65292;&#26080;&#38656;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#21644;&#37325;&#26032;&#26657;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.11976</link><description>&lt;p&gt;
&#24322;&#26500;&#20256;&#24863;&#22120;&#20449;&#21495;&#30340;&#26080;&#30417;&#30563;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Change Point Detection for heterogeneous sensor signals. (arXiv:2305.11976v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#21464;&#28857;&#26816;&#27979;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#28789;&#27963;&#36866;&#29992;&#20110;&#21508;&#31181;&#25968;&#25454;&#28304;&#65292;&#26080;&#38656;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#21644;&#37325;&#26032;&#26657;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#28857;&#26816;&#27979;&#26159;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#19968;&#20010;&#26041;&#38754;&#65292;&#22240;&#20026;&#21464;&#28857;&#30340;&#23384;&#22312;&#34920;&#26126;&#29983;&#25104;&#25968;&#25454;&#30340;&#36807;&#31243;&#21457;&#29983;&#20102;&#31361;&#28982;&#32780;&#26174;&#33879;&#30340;&#21464;&#21270;&#12290;&#34429;&#28982;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#35768;&#22810;&#21464;&#28857;&#26816;&#27979;&#31639;&#27861;&#24050;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#20294;&#22312;&#29305;&#23450;&#38382;&#39064;&#20013;&#36873;&#25321;&#21512;&#36866;&#30340;&#31639;&#27861;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#31639;&#27861;&#30340;&#36873;&#25321;&#20005;&#37325;&#20381;&#36182;&#20110;&#38382;&#39064;&#30340;&#24615;&#36136;&#21644;&#24213;&#23618;&#25968;&#25454;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#19987;&#38376;&#32771;&#23519;&#26080;&#30417;&#30563;&#25216;&#26415;&#65292;&#22240;&#20026;&#23427;&#20204;&#36866;&#29992;&#20110;&#21508;&#31181;&#25968;&#25454;&#28304;&#65292;&#32780;&#26080;&#38656;&#22823;&#37327;&#26631;&#27880;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#37325;&#26032;&#26657;&#20934;&#12290;&#20171;&#32461;&#24182;&#35780;&#20272;&#20102;&#20960;&#20010;&#26631;&#20934;&#26469;&#27604;&#36739;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Change point detection is a crucial aspect of analyzing time series data, as the presence of a change point indicates an abrupt and significant change in the process generating the data. While many algorithms for the problem of change point detection have been developed over time, it can be challenging to select the appropriate algorithm for a specific problem. The choice of the algorithm heavily depends on the nature of the problem and the underlying data source. In this paper, we will exclusively examine unsupervised techniques due to their flexibility in the application to various data sources without the requirement for abundant annotated training data and the re-calibration of the model. The examined methods will be introduced and evaluated based on several criteria to compare the algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20010;&#24615;&#21270;&#28201;&#24230;&#30340;&#23545;&#27604;&#25439;&#22833;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#26681;&#25454;&#25968;&#25454;&#20998;&#24067;&#33258;&#21160;&#35843;&#25972;&#28201;&#24230;&#20197;&#20351;&#24471;&#35757;&#32451;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.11965</link><description>&lt;p&gt;
&#19981;&#26159;&#25152;&#26377;&#30340;&#35821;&#20041;&#37117;&#26159;&#24179;&#31561;&#30340;&#65306;&#20855;&#26377;&#33258;&#23450;&#20041;&#28201;&#24230;&#30340;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Not All Semantics are Created Equal: Contrastive Self-supervised Learning with Automatic Temperature Individualization. (arXiv:2305.11965v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20010;&#24615;&#21270;&#28201;&#24230;&#30340;&#23545;&#27604;&#25439;&#22833;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#26681;&#25454;&#25968;&#25454;&#20998;&#24067;&#33258;&#21160;&#35843;&#25972;&#28201;&#24230;&#20197;&#20351;&#24471;&#35757;&#32451;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21407;&#21017;&#24615;&#21644;&#31995;&#32479;&#24615;&#30340;&#26041;&#24335;&#65292;&#20248;&#21270;&#20855;&#26377;&#20010;&#24615;&#21270;&#28201;&#24230;&#30340;&#23545;&#27604;&#25439;&#22833;&#65292;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#26222;&#36941;&#20570;&#27861;&#26159;&#23558;&#20840;&#23616;&#28201;&#24230;&#21442;&#25968;&#964;&#29992;&#20110;&#25152;&#26377;&#25968;&#25454;&#65292;&#24573;&#30053;&#20102;&#8220;&#19981;&#26159;&#25152;&#26377;&#30340;&#35821;&#20041;&#37117;&#26159;&#24179;&#31561;&#30340;&#8221;&#36825;&#20010;&#20107;&#23454;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#23637;&#31034;&#38271;&#23614;&#20998;&#24067;&#26102;&#65292;&#19981;&#21516;&#30340;&#38170;&#28857;&#25968;&#25454;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#25968;&#37327;&#30340;&#31867;&#20284;&#35821;&#20041;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#40065;&#26834;&#24615;&#20248;&#21270;&#65288;DRO&#65289;&#30340;&#26032;&#22411;&#40065;&#26834;&#23545;&#27604;&#25439;&#22833;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#20851;&#964;&#30340;&#24433;&#21709;&#30340;&#30452;&#35273;&#21644;&#33258;&#21160;&#28201;&#24230;&#20010;&#24615;&#21270;&#30340;&#26426;&#21046;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#38543;&#26426;&#31639;&#27861;&#26469;&#20248;&#21270;&#40065;&#26834;&#24615;&#23545;&#27604;&#25439;&#22833;&#65292;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#22823;&#22411;&#23567;&#25209;&#37327;&#22823;&#23567;&#12290;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#33258;&#21160;&#23398;&#20064;&#27599;&#20010;&#26679;&#26412;&#30340;&#21512;&#36866;&#964;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20855;&#26377;&#39057;&#32321;&#35821;&#20041;&#30340;&#26679;&#26412;&#20351;&#29992;&#36739;&#22823;&#28201;&#24230;&#20197;&#20445;&#25345;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we aim to optimize a contrastive loss with individualized temperatures in a principled and systematic manner for self-supervised learning. The common practice of using a global temperature parameter $\tau$ ignores the fact that ``not all semantics are created equal", meaning that different anchor data may have different numbers of samples with similar semantics, especially when data exhibits long-tails. First, we propose a new robust contrastive loss inspired by distributionally robust optimization (DRO), providing us an intuition about the effect of $\tau$ and a mechanism for automatic temperature individualization. Then, we propose an efficient stochastic algorithm for optimizing the robust contrastive loss with a provable convergence guarantee without using large mini-batch sizes. Theoretical and experimental results show that our algorithm automatically learns a suitable $\tau$ for each sample. Specifically, samples with frequent semantics use large temperatures to k
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#24314;&#27169;&#20026;&#20449;&#24687;&#29942;&#39048;&#38382;&#39064;&#65292;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#23548;&#33268;&#33391;&#22909;&#30340;&#27867;&#21270;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#25509;&#36817;&#20998;&#31867;&#38382;&#39064;&#30340;&#26368;&#20248;&#20449;&#24687;&#29942;&#39048;&#35299;&#26102;&#12290;</title><link>http://arxiv.org/abs/2305.11957</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#26041;&#27861;&#25506;&#32034;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20013;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards understanding neural collapse in supervised contrastive learning with the information bottleneck method. (arXiv:2305.11957v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#24314;&#27169;&#20026;&#20449;&#24687;&#29942;&#39048;&#38382;&#39064;&#65292;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#23548;&#33268;&#33391;&#22909;&#30340;&#27867;&#21270;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#25509;&#36817;&#20998;&#31867;&#38382;&#39064;&#30340;&#26368;&#20248;&#20449;&#24687;&#29942;&#39048;&#35299;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#26159;&#25351;&#22312;&#36229;&#20986;&#24615;&#33021;&#24179;&#21488;&#35757;&#32451;&#26102;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26368;&#21518;&#19968;&#23618;&#28608;&#27963;&#30340;&#20960;&#20309;&#23398;&#34920;&#29616;&#12290;&#30446;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#26159;&#21542;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#27867;&#21270;&#65292;&#22914;&#26524;&#26159;&#65292;&#36229;&#20986;&#24615;&#33021;&#24179;&#21488;&#30340;&#35757;&#32451;&#22914;&#20309;&#24110;&#21161;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#12290;&#26412;&#25991;&#23558;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#24314;&#27169;&#20026;&#20449;&#24687;&#29942;&#39048;&#38382;&#39064;&#65292;&#20197;&#25506;&#31350;&#26159;&#21542;&#23384;&#22312;&#36825;&#26679;&#19968;&#31181;&#32039;&#20945;&#30340;&#34920;&#31034;&#65292;&#24182;&#21457;&#29616;&#20854;&#19982;&#27867;&#21270;&#24615;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#23548;&#33268;&#33391;&#22909;&#30340;&#27867;&#21270;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#25509;&#36817;&#20998;&#31867;&#38382;&#39064;&#30340;&#26368;&#20248;&#20449;&#24687;&#29942;&#39048;&#35299;&#26102;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#30456;&#21516;&#30340;&#23545;&#27604;&#25439;&#22833;&#30446;&#26631;&#29420;&#31435;&#35757;&#32451;&#30340;&#20004;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#32447;&#24615;&#21487;&#35782;&#21035;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#24471;&#21040;&#30340;&#34920;&#31034;&#31561;&#25928;&#20110;&#30697;&#38453;&#21464;&#25442;&#12290;&#25105;&#20204;&#21033;&#29992;&#32447;&#24615;&#21487;&#35782;&#21035;&#24615;&#26469;&#36817;&#20284;&#20449;&#24687;&#29942;&#39048;&#38382;&#39064;&#30340;&#35299;&#26512;&#35299;&#12290;&#36825;&#20010;&#36817;&#20284;&#34920;&#26126;&#65292;&#24403;&#31867;&#24179;&#22343;&#20540;&#30456;&#31561;&#26102;&#65292;&#26368;&#20248;&#35299;&#38750;&#24120;&#25509;&#36817;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural collapse describes the geometry of activation in the final layer of a deep neural network when it is trained beyond performance plateaus. Open questions include whether neural collapse leads to better generalization and, if so, why and how training beyond the plateau helps. We model neural collapse as an information bottleneck (IB) problem in order to investigate whether such a compact representation exists and discover its connection to generalization. We demonstrate that neural collapse leads to good generalization specifically when it approaches an optimal IB solution of the classification problem. Recent research has shown that two deep neural networks independently trained with the same contrastive loss objective are linearly identifiable, meaning that the resulting representations are equivalent up to a matrix transformation. We leverage linear identifiability to approximate an analytical solution of the IB problem. This approximation demonstrates that when class means exh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;OPTWIN&#27010;&#24565;&#28418;&#31227;&#26816;&#27979;&#22120;&#65292;&#20351;&#29992;&#28369;&#21160;&#23376;&#31383;&#21475;&#26041;&#27861;&#26816;&#27979;&#27010;&#24565;&#28418;&#31227;&#65292;&#33719;&#24471;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#20302;&#30340;&#20551;&#38451;&#24615;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.11942</link><description>&lt;p&gt;
OPTWIN: &#20351;&#29992;&#26368;&#20248;&#23376;&#31383;&#21475;&#36827;&#34892;&#28418;&#31227;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
OPTWIN: Drift identification with optimal sub-windows. (arXiv:2305.11942v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;OPTWIN&#27010;&#24565;&#28418;&#31227;&#26816;&#27979;&#22120;&#65292;&#20351;&#29992;&#28369;&#21160;&#23376;&#31383;&#21475;&#26041;&#27861;&#26816;&#27979;&#27010;&#24565;&#28418;&#31227;&#65292;&#33719;&#24471;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#20302;&#30340;&#20551;&#38451;&#24615;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#23398;&#20064;&#65288;OL&#65289;&#26159;&#19968;&#20010;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;OL&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#27010;&#24565;&#28418;&#31227;&#30340;&#20869;&#22312;&#23384;&#22312;&#65292;&#27010;&#24565;&#28418;&#31227;&#36890;&#24120;&#34987;&#23450;&#20041;&#20026;&#38543;&#26102;&#38388;&#32780;&#26469;&#30340;&#20837;&#31449;&#25968;&#25454;&#27969;&#30340;&#32479;&#35745;&#23646;&#24615;&#30340;&#19981;&#21487;&#39044;&#35265;&#24615;&#21464;&#21270;&#12290;&#30446;&#21069;&#30340;&#27010;&#24565;&#28418;&#31227;&#26816;&#27979;&#22120;&#34920;&#29616;&#24456;&#22909;&#65292;&#21363;&#23384;&#22312;&#36739;&#20302;&#30340;&#20551;&#38452;&#24615;&#29575;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#20542;&#21521;&#20110;&#22312;&#27010;&#24565;&#28418;&#31227;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20551;&#38451;&#24615;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;OPTWIN&#65292;&#21363;&#8220;OPTimal WINdow&#8221;&#27010;&#24565;&#28418;&#31227;&#26816;&#27979;&#22120;&#12290;OPTWIN&#20351;&#29992;&#28369;&#21160;&#31383;&#21475;&#26041;&#27861;&#20998;&#26512;&#25968;&#25454;&#27969;&#30340;&#23376;&#31383;&#21475;&#65292;&#20197;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#20302;&#30340;&#20551;&#38451;&#24615;&#29575;&#26816;&#27979;&#27010;&#24565;&#28418;&#31227;&#12290;OPTWIN&#30340;&#23454;&#39564;&#35780;&#20272;&#26174;&#31034;&#23427;&#22312;&#20551;&#38451;&#24615;&#21644;&#30495;&#38451;&#24615;&#29575;&#20043;&#38388;&#36798;&#21040;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online Learning (OL) is a field of research that is increasingly gaining attention both in academia and industry. One of the main challenges of OL is the inherent presence of concept drifts, which are commonly defined as unforeseeable changes in the statistical properties of an incoming data stream over time. The detection of concept drifts typically involves analyzing the error rates produced by an underlying OL algorithm in order to identify if a concept drift occurred or not, such that the OL algorithm can adapt accordingly. Current concept-drift detectors perform very well, i.e., with low false negative rates, but they still tend to exhibit high false positive rates in the concept-drift detection. This may impact the performance of the learner and result in an undue amount of computational resources spent on retraining a model that actually still performs within its expected range. In this paper, we propose OPTWIN, our "OPTimal WINdow" concept drift detector. OPTWIN uses a sliding 
&lt;/p&gt;</description></item><item><title>iCaloFlow&#26159;&#19968;&#20010;&#22522;&#20110;&#24402;&#32435;&#27969;&#30340;&#24555;&#36895;&#25506;&#27979;&#22120;&#27169;&#25311;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#39640;&#36798;&#20197;&#24448;10-100&#20493;&#30340;&#20998;&#36776;&#29575;&#36827;&#34892;&#24555;&#36895;&#12289;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#12290;</title><link>http://arxiv.org/abs/2305.11934</link><description>&lt;p&gt;
&#22522;&#20110;&#24402;&#32435;&#27969;&#30340;&#24555;&#36895;&#31890;&#23376;&#25506;&#27979;&#22120;&#27169;&#25311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Inductive CaloFlow. (arXiv:2305.11934v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11934
&lt;/p&gt;
&lt;p&gt;
iCaloFlow&#26159;&#19968;&#20010;&#22522;&#20110;&#24402;&#32435;&#27969;&#30340;&#24555;&#36895;&#25506;&#27979;&#22120;&#27169;&#25311;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#39640;&#36798;&#20197;&#24448;10-100&#20493;&#30340;&#20998;&#36776;&#29575;&#36827;&#34892;&#24555;&#36895;&#12289;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#31890;&#23376;&#25506;&#27979;&#22120;&#21709;&#24212;&#26159;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#35745;&#31639;&#27969;&#31243;&#20013;&#26368;&#26114;&#36149;&#30340;&#27493;&#39588;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24402;&#19968;&#21270;&#27969;&#21487;&#20197;&#21152;&#24555;&#27492;&#36807;&#31243;&#65292;&#24182;&#23454;&#29616;&#21069;&#25152;&#26410;&#26377;&#30340;&#31934;&#24230;&#27700;&#24179;&#65292;&#20294;&#23558;&#27492;&#26041;&#27861;&#25193;&#23637;&#21040;&#19982;&#26410;&#26469;&#25506;&#27979;&#22120;&#21319;&#32423;&#30456;&#20851;&#30340;&#26356;&#39640;&#20998;&#36776;&#29575;&#26102;&#20250;&#23548;&#33268;&#38480;&#21046;&#24615;&#30340;&#20869;&#23384;&#32422;&#26463;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#24402;&#32435;&#31995;&#21015;&#24402;&#19968;&#21270;&#27969;&#30340;&#24555;&#36895;&#25506;&#27979;&#22120;&#27169;&#25311;&#26694;&#26550;iCaloFlow&#65292;&#23427;&#26159;&#22312;&#25104;&#23545;&#30340;&#36830;&#32493;&#33021;&#37327;&#27785;&#31215;&#23618;&#20013;&#35757;&#32451;&#30340;&#12290;&#20026;&#20102;&#22686;&#21152;&#37319;&#26679;&#36895;&#24230;&#32780;&#19981;&#22833;&#34920;&#29616;&#21147;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#24072;&#29983;&#33976;&#39311;&#12290;&#27491;&#22914;&#25105;&#20204;&#22312;CaloChallenge2022&#30340;&#25968;&#25454;&#38598;2&#21644;&#25968;&#25454;&#38598;3&#20013;&#23637;&#31034;&#30340;&#37027;&#26679;&#65292;iCaloFlow&#21487;&#20197;&#23454;&#29616;&#24402;&#19968;&#21270;&#27969;&#22312;&#36827;&#34892;&#24555;&#36895;&#12289;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#26102;&#30340;&#28508;&#21147;&#65292;&#36825;&#20123;&#27169;&#25311;&#23545;&#24212;&#30340;&#25506;&#27979;&#22120;&#20960;&#20309;&#32422;&#27604;&#20197;&#21069;&#32771;&#34385;&#30340;&#39640;10-100&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulating particle detector response is the single most expensive step in the Large Hadron Collider computational pipeline. Recently it was shown that normalizing flows can accelerate this process while achieving unprecedented levels of accuracy, but scaling this approach up to higher resolutions relevant for future detector upgrades leads to prohibitive memory constraints. To overcome this problem, we introduce Inductive CaloFlow (iCaloFlow), a framework for fast detector simulation based on an inductive series of normalizing flows trained on the pattern of energy depositions in pairs of consecutive calorimeter layers. We further use a teacher-student distillation to increase sampling speed without loss of expressivity. As we demonstrate with Datasets 2 and 3 of the CaloChallenge2022, iCaloFlow can realize the potential of normalizing flows in performing fast, high-fidelity simulation on detector geometries that are ~ 10 - 100 times higher granularity than previously considered.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#23558;spotPython&#36229;&#21442;&#25968;&#35843;&#35856;&#22120;&#38598;&#25104;&#21040;PyTorch&#35757;&#32451;&#24037;&#20316;&#27969;&#20013;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#25110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20197;CIFAR10&#22270;&#20687;&#20998;&#31867;&#22120;&#20026;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.11930</link><description>&lt;p&gt;
PyTorch&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#8212;&#8212;&#38754;&#21521;spotPython&#30340;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
PyTorch Hyperparameter Tuning -- A Tutorial for spotPython. (arXiv:2305.11930v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#23558;spotPython&#36229;&#21442;&#25968;&#35843;&#35856;&#22120;&#38598;&#25104;&#21040;PyTorch&#35757;&#32451;&#24037;&#20316;&#27969;&#20013;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#25110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20197;CIFAR10&#22270;&#20687;&#20998;&#31867;&#22120;&#20026;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#35843;&#25972;&#65288;&#25110;&#36229;&#21442;&#25968;&#20248;&#21270;&#65289;&#30340;&#30446;&#26631;&#26159;&#20248;&#21270;&#36229;&#21442;&#25968;&#20197;&#25552;&#39640;&#26426;&#22120;&#25110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;spotPython&#26159;&#30693;&#21517;&#36229;&#21442;&#25968;&#35843;&#35856;&#22120;SPOT&#30340;Python&#29256;&#26412;&#65292;SPOT&#24050;&#32463;&#22312;R&#32534;&#31243;&#29615;&#22659;&#20013;&#20026;&#32479;&#35745;&#20998;&#26512;&#24320;&#21457;&#20102;&#21313;&#24180;&#20197;&#19978;&#12290;PyTorch&#26159;&#19968;&#31181;&#22522;&#20110;GPU&#21644;CPU&#30340;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#24352;&#37327;&#24211;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;spotPython&#36229;&#21442;&#25968;&#35843;&#35856;&#22120;&#38598;&#25104;&#21040;PyTorch&#35757;&#32451;&#24037;&#20316;&#27969;&#20013;&#12290;&#20197;CIFAR10&#22270;&#20687;&#20998;&#31867;&#22120;&#20026;&#20363;&#65292;&#20171;&#32461;&#20102;spotPython&#20197;&#21450;&#19982;Ray Tune&#30340;&#31616;&#30701;&#27604;&#36739;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;spotPython&#30340;&#20351;&#29992;&#32463;&#39564;&#65292;&#20197;&#21450;&#22914;&#20309;&#20351;&#29992;hook&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#21160;&#35843;&#25972;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of hyperparameter tuning (or hyperparameter optimization) is to optimize the hyperparameters to improve the performance of the machine or deep learning model. spotPython (``Sequential Parameter Optimization Toolbox in Python'') is the Python version of the well-known hyperparameter tuner SPOT, which has been developed in the R programming environment for statistical analysis for over a decade. PyTorch is an optimized tensor library for deep learning using GPUs and CPUs. This document shows how to integrate the spotPython hyperparameter tuner into the PyTorch training workflow. As an example, the results of the CIFAR10 image classifier are used. In addition to an introduction to spotPython, this tutorial also includes a brief comparison with Ray Tune, a Python library for running experiments and tuning hyperparameters. This comparison is based on the PyTorch hyperparameter tuning tutorial. The advantages and disadvantages of both approaches are discussed. We show that spotPytho
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#23398;&#20064;&#33258;&#21160;&#26426;&#23454;&#29616;&#20102;&#33410;&#33021;&#30340;AI&#30828;&#20214;&#35774;&#35745;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11928</link><description>&lt;p&gt;
&#20351;&#29992;&#23398;&#20064;&#33258;&#21160;&#26426;&#30340;&#33410;&#33021;&#19988;&#21487;&#35299;&#37322;AI&#30828;&#20214;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Energy-frugal and Interpretable AI Hardware Design using Learning Automata. (arXiv:2305.11928v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#23398;&#20064;&#33258;&#21160;&#26426;&#23454;&#29616;&#20102;&#33410;&#33021;&#30340;AI&#30828;&#20214;&#35774;&#35745;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24494;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#19979;&#65292;&#33021;&#25928;&#26159;&#23454;&#29616;&#24378;&#22823;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#37325;&#35201;&#38656;&#27714;&#12290;&#36890;&#36807;&#33410;&#33021;&#30340;&#35745;&#31639;&#36164;&#28304;&#37197;&#32622;&#23454;&#29616;&#30828;&#20214;&#21152;&#36895;&#26159;&#38477;&#20302;&#33021;&#32791;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26032;&#20852;&#24212;&#29992;&#36824;&#38656;&#35201;&#37319;&#29992;&#21487;&#35299;&#37322;&#20915;&#31574;&#27169;&#22411;&#65292;&#20197;&#30830;&#31435;&#36131;&#20219;&#21644;&#36879;&#26126;&#24230;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#22330;&#26223;&#20013;&#25552;&#20379;&#21487;&#36798;&#29366;&#24577;&#38656;&#35201;&#39069;&#22806;&#30340;&#36164;&#28304;&#65292;&#36825;&#32473;&#33021;&#25928;&#35774;&#35745;&#24102;&#26469;&#20102;&#20914;&#31361;&#24615;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;Tsetlin&#26426;&#22120;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#21407;&#29702;&#65292;&#19982;&#31639;&#26415;&#19981;&#21516;&#65292;&#21463;&#30410;&#20110;&#33258;&#28982;&#36923;&#36753;&#25903;&#25745;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#36866;&#24403;&#35843;&#25972;&#36229;&#21442;&#25968;&#26469;&#23454;&#29616;&#33410;&#33021;&#30340;&#20154;&#24037;&#26234;&#33021;&#30828;&#20214;&#35774;&#35745;&#65292;&#24182;&#20445;&#25345;&#39640;&#25928;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#20026;&#20102;&#23637;&#31034;&#20854;&#28508;&#21147;&#65292;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#20248;&#21270;&#25216;&#26415;&#19979;&#22312;&#21487;&#32534;&#31243;&#36923;&#36753;&#38376;&#38453;&#21015;&#65288;FPGAs&#65289;&#19978;&#23454;&#29616;&#20102;Tsetlin&#26426;&#22120;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#21033;&#29992;&#23398;&#20064;&#33258;&#21160;&#26426;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#26174;&#33879;&#30340;&#33021;&#28304;&#33410;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy efficiency is a crucial requirement for enabling powerful artificial intelligence applications at the microedge. Hardware acceleration with frugal architectural allocation is an effective method for reducing energy. Many emerging applications also require the systems design to incorporate interpretable decision models to establish responsibility and transparency. The design needs to provision for additional resources to provide reachable states in real-world data scenarios, defining conflicting design tradeoffs between energy efficiency. is challenging.  Recently a new machine learning algorithm, called the Tsetlin machine, has been proposed. The algorithm is fundamentally based on the principles of finite-state automata and benefits from natural logic underpinning rather than arithmetic. In this paper, we investigate methods of energy-frugal artificial intelligence hardware design by suitably tuning the hyperparameters, while maintaining high learning efficacy. To demonstrate i
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20351;&#29992;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#22312;&#21019;&#24314;&#35745;&#31639;&#26426;&#35270;&#35273;&#20998;&#31867;&#21644;&#26816;&#27979;&#27169;&#22411;&#26102;&#24110;&#21161;&#29992;&#25143;&#35782;&#21035;&#21644;&#25913;&#36827;&#27169;&#22411;&#19978;&#30340;&#38382;&#39064;&#65292;&#26377;&#25928;&#20943;&#23569;&#35774;&#35745;&#24072;&#30340;&#24037;&#20316;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.11927</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#21738;&#20123;&#22320;&#26041;&#20250;&#20986;&#38169;&#65311;&#20351;&#29992;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#25214;&#20986;&#24182;&#25913;&#36827;CV&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Where does a computer vision model make mistakes? Using interactive visualizations to find where and how CV models can improve. (arXiv:2305.11927v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11927
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20351;&#29992;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#22312;&#21019;&#24314;&#35745;&#31639;&#26426;&#35270;&#35273;&#20998;&#31867;&#21644;&#26816;&#27979;&#27169;&#22411;&#26102;&#24110;&#21161;&#29992;&#25143;&#35782;&#21035;&#21644;&#25913;&#36827;&#27169;&#22411;&#19978;&#30340;&#38382;&#39064;&#65292;&#26377;&#25928;&#20943;&#23569;&#35774;&#35745;&#24072;&#30340;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#22797;&#26434;&#21644;&#32321;&#29712;&#30340;&#36807;&#31243;&#65292;&#32780;&#20351;&#32456;&#31471;&#29992;&#25143;&#21487;&#20197;&#26500;&#24314;&#12289;&#26816;&#26597;&#21644;&#25913;&#36827;&#36825;&#20123;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35270;&#35282;&#24050;&#32463;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#12290;&#20026;&#20102;&#25552;&#39640;&#20855;&#26377;&#19981;&#21516;&#32423;&#21035;&#26426;&#22120;&#23398;&#20064;&#19987;&#19994;&#25216;&#33021;&#30340;&#32456;&#31471;&#29992;&#25143;&#30340;&#20307;&#39564;&#65292;&#25105;&#20204;&#22312;Sprite&#30340;&#19978;&#19979;&#25991;&#20013;&#35774;&#35745;&#21644;&#35780;&#20272;&#20102;&#20004;&#20010;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20026;&#20174;&#35270;&#39057;&#20013;&#25277;&#21462;&#30340;&#22270;&#20687;&#21019;&#24314;CV&#20998;&#31867;&#21644;&#26816;&#27979;&#27169;&#22411;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#21487;&#35270;&#21270;&#24037;&#20855;&#22914;&#20309;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#24490;&#29615;&#30340;&#19968;&#37096;&#20998;&#65292;&#24110;&#21161;&#29992;&#25143;&#35782;&#21035;&#65288;&#35780;&#20272;&#65289;&#21644;&#36873;&#25321;&#65288;&#35268;&#21010;&#65289;&#27169;&#22411;&#23384;&#22312;&#38382;&#39064;&#30340;&#22270;&#20687;&#65292;&#24182;&#25913;&#21892;&#27491;&#22312;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#36825;&#20123;&#21487;&#35270;&#21270;&#24037;&#20855;&#30340;&#29992;&#25143;&#22312;&#26356;&#24191;&#27867;&#30340;&#27169;&#22411;&#38169;&#35823;&#31867;&#22411;&#21644;&#19968;&#20010;&#25110;&#22810;&#20010;&#27169;&#22411;&#30340;&#39044;&#27979;&#34892;&#20026;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#26041;&#38754;&#21457;&#29616;&#20102;&#26356;&#22810;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35774;&#35745;&#24072;&#21019;&#24314;&#21644;&#25913;&#36827;CV&#27169;&#22411;&#25152;&#38656;&#30340;&#28508;&#22312;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating Computer Vision (CV) models remains a complex and taxing practice for end-users to build, inspect, and improve these models. Interactive ML perspectives have helped address some of these issues by considering a teacher-in-the-loop where planning, teaching, and evaluating tasks take place. To improve the experience of end-users with various levels of ML expertise, we designed and evaluated two interactive visualizations in the context of Sprite, a system for creating CV classification and detection models for images originating from videos. We study how these visualizations, as part of the machine teaching loop, help users identify (evaluate) and select (plan) images where a model is struggling and improve the model being trained. We found that users who had used the visualizations found more images across a wider set of potential types of model errors, as well as in assessing and contrasting the prediction behavior of one or more models, thus reducing the potential effort requ
&lt;/p&gt;</description></item><item><title>MParrotTTS&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#35828;&#35805;&#20154;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#65292;&#20197;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#20026;&#22522;&#30784;&#65307;&#23427;&#21487;&#20197;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#20165;&#20351;&#29992;&#23569;&#37327;&#26377;&#30417;&#30563;&#25968;&#25454;&#23601;&#36866;&#24212;&#20110;&#26032;&#35821;&#35328;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#24179;&#34892;&#25110;&#21452;&#35821;&#35821;&#26009;&#30340;&#24773;&#20917;&#19979;&#20256;&#36882;&#35828;&#35805;&#20154;&#29305;&#23450;&#30340;&#35821;&#38899;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.11926</link><description>&lt;p&gt;
MParrotTTS&#65306;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#30340;&#22810;&#35821;&#35328;&#22810;&#35828;&#35805;&#20154;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
MParrotTTS: Multilingual Multi-speaker Text to Speech Synthesis in Low Resource Setting. (arXiv:2305.11926v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11926
&lt;/p&gt;
&lt;p&gt;
MParrotTTS&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#35828;&#35805;&#20154;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#65292;&#20197;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#20026;&#22522;&#30784;&#65307;&#23427;&#21487;&#20197;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#20165;&#20351;&#29992;&#23569;&#37327;&#26377;&#30417;&#30563;&#25968;&#25454;&#23601;&#36866;&#24212;&#20110;&#26032;&#35821;&#35328;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#24179;&#34892;&#25110;&#21452;&#35821;&#35821;&#26009;&#30340;&#24773;&#20917;&#19979;&#20256;&#36882;&#35828;&#35805;&#20154;&#29305;&#23450;&#30340;&#35821;&#38899;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;MParrotTTS&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#35828;&#35805;&#20154;&#25991;&#26412;&#36716;&#35821;&#38899;(TTS)&#21512;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#12290;MParrotTTS&#21463;&#30410;&#20110;&#27169;&#22359;&#21270;&#22521;&#35757;&#33539;&#24335;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#65292;&#20197;&#26368;&#23567;&#30340;&#30417;&#30563;&#25968;&#25454;&#36866;&#24212;&#20110;&#26032;&#35821;&#35328;&#65292;&#24182;&#22312;&#35757;&#32451;&#33258;&#30417;&#30563;&#21518;&#39592;&#24178;&#20013;&#23545;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#36827;&#34892;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;MParrotTTS&#19981;&#38656;&#35201;&#20219;&#20309;&#21452;&#35821;&#25110;&#24179;&#34892;&#31034;&#20363;&#30340;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#35821;&#38899;&#20013;&#20256;&#36882;&#35821;&#38899;&#65292;&#21516;&#26102;&#20445;&#30041;&#35828;&#35805;&#20154;&#30340;&#29305;&#23450;&#29305;&#24449;&#65292;&#20363;&#22914;&#20351;&#29992;&#27861;&#35821;&#28436;&#35762;&#32773;&#30340;&#22768;&#38899;&#21644;&#21475;&#38899;&#21512;&#25104;&#27969;&#21033;&#30340;&#21360;&#22320;&#35821;&#35821;&#38899;&#12290;&#25105;&#20204;&#22312;&#20845;&#31181;&#35821;&#35328;&#19978;&#25552;&#20986;&#20102;&#24191;&#27867;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#24182;&#34892;&#21644;&#36328;&#35821;&#35328;&#32508;&#21512;&#30340;&#35821;&#38899;&#33258;&#28982;&#24230;&#21644;&#35828;&#35805;&#20154;&#30456;&#20284;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#21482;&#20351;&#29992;&#23569;&#37327;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;TTS&#27169;&#22411;&#21644;&#22522;&#32447;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#22312;https://paper2438.github.io/tts&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MParrotTTS, a unified multilingual, multi-speaker text-to-speech (TTS) synthesis model that can produce high-quality speech. Benefiting from a modularized training paradigm exploiting self-supervised speech representations, MParrotTTS adapts to a new language with minimal supervised data and generalizes to languages not seen while training the self-supervised backbone. Moreover, without training on any bilingual or parallel examples, MParrotTTS can transfer voices across languages while preserving the speaker-specific characteristics, e.g., synthesizing fluent Hindi speech using a French speaker's voice and accent. We present extensive results on six languages in terms of speech naturalness and speaker similarity in parallel and cross-lingual synthesis. The proposed model outperforms the state-of-the-art multilingual TTS models and baselines, using only a small fraction of supervised training data. Speech samples from our model can be found at https://paper2438.github.io/tts
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27604;&#36739;&#32467;&#26524;&#23637;&#31034;&#26041;&#27861;&#8212;&#8212;&#22810;&#20803;&#27604;&#36739;&#30697;&#38453;&#65288;MCM&#65289;&#65292;&#20351;&#24471;&#27604;&#36739;&#38598;&#21512;&#31283;&#23450;&#26080;&#35823;&#24046;&#65292;&#21487;&#36991;&#20813;&#24120;&#29992;&#26041;&#27861;&#23384;&#22312;&#30340;&#26080;&#24847;&#21644;&#26377;&#24847;&#30340;&#25805;&#32437;&#31354;&#38388;&#65292;&#24182;&#19988;&#20854;&#37319;&#29992;Python&#23454;&#29616;&#65292;&#24050;&#22312;&#20844;&#24320;&#25552;&#20379;&#12290;</title><link>http://arxiv.org/abs/2305.11921</link><description>&lt;p&gt;
&#19968;&#31181;&#23545;&#27604;&#38598;&#21512;&#31283;&#23450;&#26080;&#35823;&#24046;&#30340;&#22810;&#37325;&#27604;&#36739;&#22522;&#20934;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Approach to Multiple Comparison Benchmark Evaluations that is Stable Under Manipulation of the Comparate Set. (arXiv:2305.11921v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27604;&#36739;&#32467;&#26524;&#23637;&#31034;&#26041;&#27861;&#8212;&#8212;&#22810;&#20803;&#27604;&#36739;&#30697;&#38453;&#65288;MCM&#65289;&#65292;&#20351;&#24471;&#27604;&#36739;&#38598;&#21512;&#31283;&#23450;&#26080;&#35823;&#24046;&#65292;&#21487;&#36991;&#20813;&#24120;&#29992;&#26041;&#27861;&#23384;&#22312;&#30340;&#26080;&#24847;&#21644;&#26377;&#24847;&#30340;&#25805;&#32437;&#31354;&#38388;&#65292;&#24182;&#19988;&#20854;&#37319;&#29992;Python&#23454;&#29616;&#65292;&#24050;&#22312;&#20844;&#24320;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20934;&#35780;&#20272;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#34913;&#37327;&#36827;&#27493;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24120;&#29992;&#30340;&#26041;&#27861;&#23545;&#20110;&#22810;&#20010;&#31639;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#22522;&#20934;&#27604;&#36739;&#32467;&#26524;&#20998;&#26512;&#21644;&#23637;&#31034;&#65292;&#22914;Dem\v{s}ar&#65288;2006&#65289;&#24341;&#20837;&#30340;&#20851;&#38190;&#24046;&#24322;&#22270;&#23384;&#22312;&#37325;&#22823;&#32570;&#38519;&#65292;&#24182;&#19988;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#26080;&#24847;&#21644;&#26377;&#24847;&#30340;&#25805;&#32437;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27604;&#36739;&#32467;&#26524;&#23637;&#31034;&#26041;&#27861;&#8212;&#8212;&#22810;&#20803;&#27604;&#36739;&#30697;&#38453;&#65288;MCM&#65289;&#65292;&#35813;&#26041;&#27861;&#20248;&#20808;&#32771;&#34385;&#25104;&#23545;&#27604;&#36739;&#65292;&#25490;&#38500;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#25805;&#32437;&#23454;&#39564;&#32467;&#26524;&#30340;&#26041;&#24335;&#12290;MCM&#21487;&#29992;&#20110;&#26174;&#31034;&#20840;&#23545;&#27604;&#32467;&#26524;&#65292;&#25110;&#26174;&#31034;&#19968;&#20010;&#25110;&#22810;&#20010;&#36873;&#25321;&#30340;&#31639;&#27861;&#19982;&#25216;&#26415;&#30340;&#23545;&#27604;&#32467;&#26524;&#12290;MCM&#37319;&#29992;Python&#23454;&#29616;&#65292;&#24182;&#20844;&#24320;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
The measurement of progress using benchmarks evaluations is ubiquitous in computer science and machine learning. However, common approaches to analyzing and presenting the results of benchmark comparisons of multiple algorithms over multiple datasets, such as the critical difference diagram introduced by Dem\v{s}ar (2006), have important shortcomings and, we show, are open to both inadvertent and intentional manipulation. To address these issues, we propose a new approach to presenting the results of benchmark comparisons, the Multiple Comparison Matrix (MCM), that prioritizes pairwise comparisons and precludes the means of manipulating experimental results in existing approaches. MCM can be used to show the results of an all-pairs comparison, or to show the results of a comparison between one or more selected algorithms and the state of the art. MCM is implemented in Python and is publicly available.
&lt;/p&gt;</description></item><item><title>Elektrum&#26159;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#37238;&#21453;&#24212;&#65292;&#21033;&#29992;&#26377;&#38480;&#20294;&#27905;&#20928;&#30340;&#20307;&#22806;&#25968;&#25454;&#21644;&#22122;&#22768;&#20294;&#20016;&#23500;&#30340;&#20307;&#20869;&#25968;&#25454;&#12290;Elektrum&#21487;&#20197;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#25581;&#31034;&#37238;&#27963;&#24615;&#30340;&#20851;&#38190;&#24207;&#21015;&#30456;&#20851;&#20915;&#23450;&#22240;&#32032;&#65292;&#24182;&#21457;&#29616;&#28508;&#22312;&#30340;&#27835;&#30103;&#24178;&#39044;&#38774;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.11917</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#19982;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;&#29702;&#35299;&#24207;&#21015;&#20381;&#36182;&#30340;&#37238;&#21453;&#24212;
&lt;/p&gt;
&lt;p&gt;
Interpretable neural architecture search and transfer learning for understanding sequence dependent enzymatic reactions. (arXiv:2305.11917v1 [q-bio.MN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11917
&lt;/p&gt;
&lt;p&gt;
Elektrum&#26159;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#37238;&#21453;&#24212;&#65292;&#21033;&#29992;&#26377;&#38480;&#20294;&#27905;&#20928;&#30340;&#20307;&#22806;&#25968;&#25454;&#21644;&#22122;&#22768;&#20294;&#20016;&#23500;&#30340;&#20307;&#20869;&#25968;&#25454;&#12290;Elektrum&#21487;&#20197;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#25581;&#31034;&#37238;&#27963;&#24615;&#30340;&#20851;&#38190;&#24207;&#21015;&#30456;&#20851;&#20915;&#23450;&#22240;&#32032;&#65292;&#24182;&#21457;&#29616;&#28508;&#22312;&#30340;&#27835;&#30103;&#24178;&#39044;&#38774;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#32454;&#35843;&#33410;&#30340;&#37238;&#36884;&#24452;&#25511;&#21046;&#30528;&#32454;&#32990;&#36807;&#31243;&#65292;&#23427;&#20204;&#30340;&#22833;&#35843;&#21487;&#33021;&#23548;&#33268;&#30142;&#30149;&#12290;&#20026;&#36825;&#20123;&#36884;&#24452;&#21019;&#24314;&#39044;&#27979;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#36825;&#20123;&#36884;&#24452;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#32454;&#32990;&#21644;&#22522;&#22240;&#32452;&#32972;&#26223;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Elektrum&#65292;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#21644;&#29983;&#29289;&#29289;&#29702;&#35299;&#37322;&#27169;&#22411;&#65292;&#30830;&#23450;&#29983;&#21270;&#31995;&#32479;&#21160;&#21147;&#23398;&#65292;&#20174;&#32780;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#23427;&#20351;&#29992;&#20307;&#22806;&#21160;&#21147;&#23398;&#27979;&#23450;&#24555;&#36895;&#20551;&#35774;&#39640;&#36136;&#37327;&#30340;&#21487;&#35299;&#37322;&#21160;&#21147;&#23398;&#31070;&#32463;&#32593;&#32476;&#65288;KINN&#65289;&#65292;&#29992;&#20110;&#39044;&#27979;&#21453;&#24212;&#36895;&#29575;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#26032;&#39062;&#30340;&#36801;&#31227;&#23398;&#20064;&#27493;&#39588;&#65292;&#23558;KINN&#20316;&#20026;&#20013;&#20171;&#23618;&#25554;&#20837;&#26356;&#28145;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24494;&#35843;&#21453;&#24212;&#30456;&#20851;&#30340;&#20307;&#20869;&#32467;&#26524;&#30340;&#39044;&#27979;&#12290;Elektrum&#26377;&#25928;&#21033;&#29992;&#20102;&#26377;&#38480;&#20294;&#27905;&#20928;&#30340;&#20307;&#22806;&#25968;&#25454;&#21644;&#25429;&#33719;&#32454;&#32990;&#32972;&#26223;&#30340;&#22122;&#22768;&#20294;&#20016;&#23500;&#30340;&#20307;&#20869;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;Elektrum&#24212;&#29992;&#20110;&#29702;&#35299;&#19982;&#38750;&#37202;&#31934;&#24615;&#33026;&#32938;&#32925;&#30149;&#30456;&#20851;&#30340;&#30899;&#27700;&#21270;&#21512;&#29289;&#21644;&#33026;&#36136;&#20195;&#35874;&#20013;&#28041;&#21450;&#30340;&#37238;&#21453;&#24212;&#12290;&#25105;&#20204;&#35777;&#26126;Elektrum&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#65288;1&#65289;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#39044;&#27979;&#20307;&#20869;&#21453;&#24212;&#36895;&#29575;; (2) &#25581;&#31034;&#37238;&#27963;&#24615;&#30340;&#20851;&#38190;&#24207;&#21015;&#30456;&#20851;&#20915;&#23450;&#22240;&#32032;; &#20197;&#21450;&#65288;3&#65289;&#21457;&#29616;&#27835;&#30103;&#24178;&#39044;&#30340;&#28508;&#22312;&#38774;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finely-tuned enzymatic pathways control cellular processes, and their dysregulation can lead to disease. Creating predictive and interpretable models for these pathways is challenging because of the complexity of the pathways and of the cellular and genomic contexts. Here we introduce Elektrum, a deep learning framework which addresses these challenges with data-driven and biophysically interpretable models for determining the kinetics of biochemical systems. First, it uses in vitro kinetic assays to rapidly hypothesize an ensemble of high-quality Kinetically Interpretable Neural Networks (KINNs) that predict reaction rates. It then employs a novel transfer learning step, where the KINNs are inserted as intermediary layers into deeper convolutional neural networks, fine-tuning the predictions for reaction-dependent in vivo outcomes. Elektrum makes effective use of the limited, but clean in vitro data and the noisy, yet plentiful in vivo data that captures cellular context. We apply Ele
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;$\mathbb{R}$-&#20809;&#28369;Banach&#31354;&#38388;&#20013;&#25903;&#25345;PINNs&#35823;&#24046;&#20272;&#35745;&#30340;&#38750;&#32447;&#24615;&#26041;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#29992;&#20110;&#38480;&#21046;&#27531;&#24046;&#30340;Bramble-Hilbert&#24341;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.11915</link><description>&lt;p&gt;
&#22312;$\mathbb{R}$-&#20809;&#28369;Banach&#31354;&#38388;&#20013;&#65292;PINNs&#35823;&#24046;&#20272;&#35745;&#38750;&#32447;&#24615;&#26041;&#31243;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
PINNs error estimates for nonlinear equations in $\mathbb{R}$-smooth Banach spaces. (arXiv:2305.11915v1 [math.FA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;$\mathbb{R}$-&#20809;&#28369;Banach&#31354;&#38388;&#20013;&#25903;&#25345;PINNs&#35823;&#24046;&#20272;&#35745;&#30340;&#38750;&#32447;&#24615;&#26041;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#29992;&#20110;&#38480;&#21046;&#27531;&#24046;&#30340;Bramble-Hilbert&#24341;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#31639;&#23376;&#24418;&#24335;&#25551;&#36848;&#20102;&#19968;&#31867;&#25903;&#25345;PINN&#35823;&#24046;&#20272;&#35745;&#30340;PDE&#65292;&#24182;&#19988;&#23545;&#20110;$L^p$&#31354;&#38388;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;Bramble-Hilbert&#24341;&#29702;&#65292;&#20316;&#20026;&#19982;PINN&#27531;&#24046;&#36793;&#30028;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the paper, we describe in operator form classes of PDEs that admit PINN's error estimation. Also, for $L^p$ spaces, we obtain a Bramble-Hilbert type lemma that is a tool for PINN's residuals bounding.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39640;&#24230;&#29616;&#23454;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#23545;&#31232;&#30095;&#38647;&#36798;&#25968;&#25454;&#36827;&#34892;&#30456;&#20301;&#30456;&#20851;&#30340;&#27874;&#28010;&#34920;&#38754;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2305.11913</link><description>&lt;p&gt;
&#29992;&#20110;&#20174;&#31232;&#30095;&#36828;&#31243;&#20256;&#24863;&#22120;&#25968;&#25454;&#37325;&#24314;&#38750;&#32447;&#24615;&#28023;&#27915;&#27874;&#28010;&#34920;&#38754;&#30456;&#20301;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Machine learning for phase-resolved reconstruction of nonlinear ocean wave surface elevations from sparse remote sensing data. (arXiv:2305.11913v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39640;&#24230;&#29616;&#23454;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#23545;&#31232;&#30095;&#38647;&#36798;&#25968;&#25454;&#36827;&#34892;&#30456;&#20301;&#30456;&#20851;&#30340;&#27874;&#28010;&#34920;&#38754;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#30456;&#20301;&#30456;&#20851;&#30340;&#27700;&#27874;&#26465;&#20214;&#23545;&#20110;&#28023;&#27915;&#24037;&#31243;&#30340;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36828;&#31243;&#30417;&#27979;&#27874;&#28010;&#39044;&#27979;&#27169;&#22411;&#30340;&#21021;&#22987;&#21270;&#39318;&#20808;&#38656;&#35201;&#20174;&#31867;&#20284;&#38647;&#36798;&#30340;&#31232;&#30095;&#27979;&#37327;&#20013;&#37325;&#24314;&#27874;&#28010;&#34920;&#38754;&#12290;&#29616;&#26377;&#30340;&#37325;&#24314;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#31616;&#21270;&#30340;&#27169;&#22411;&#20551;&#35774;&#65292;&#36825;&#20250;&#24433;&#21709;&#25972;&#20010;&#39044;&#27979;&#36807;&#31243;&#30340;&#23454;&#26102;&#24615;&#25110;&#20934;&#30830;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;U-Net&#21644;Fourier&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#32467;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#30456;&#20301;&#30456;&#20851;&#30340;&#27874;&#28010;&#34920;&#38754;&#37325;&#24314;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20855;&#26377;&#39640;&#24230;&#29616;&#23454;&#24615;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#22312;&#22343;&#21248;&#30340;&#19968;&#32500;&#32593;&#26684;&#19978;&#30001;&#27874;&#28010;&#27169;&#25311;&#30340;&#39640;&#38454;&#35889;&#26041;&#27861;&#21644;&#20960;&#20309;&#38647;&#36798;&#24314;&#27169;&#26041;&#27861;&#29983;&#25104;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#31181;&#27169;&#22411;&#37117;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#27874;&#28010;&#37325;&#24314;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate short-term prediction of phase-resolved water wave conditions is crucial for decision-making in ocean engineering. However, the initialization of remote-sensing-based wave prediction models first requires a reconstruction of wave surfaces from sparse measurements like radar. Existing reconstruction methods either rely on computationally intensive optimization procedures or simplistic modeling assumptions that compromise real-time capability or accuracy of the entire prediction process. We therefore address these issues by proposing a novel approach for phase-resolved wave surface reconstruction using neural networks based on the U-Net and Fourier neural operator (FNO) architectures. Our approach utilizes synthetic yet highly realistic training data on uniform one-dimensional grids, that is generated by the high-order spectral method for wave simulation and a geometric radar modeling approach. The investigation reveals that both models deliver accurate wave reconstruction resul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#22269;&#23478;&#27700;&#36164;&#28304;&#27169;&#22411;&#21644;&#25968;&#20540;&#22825;&#27668;&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#21450;&#21355;&#26143;&#26816;&#32034;&#26469;&#39044;&#27979;&#32654;&#22269;&#36830;&#32493;&#26412;&#22303;&#19978;&#30340;&#27515;&#20129;&#29123;&#26009;&#28287;&#24230;&#26816;&#32034;&#65292;&#36229;&#36807;&#20102;&#26082;&#26377;&#30340;&#27599;&#26085;&#21644;&#27599;&#23567;&#26102;&#27668;&#20505;&#32479;&#35745;&#26041;&#27861;&#12290;VIIRS&#26816;&#32034;&#23545;&#39044;&#27979;FMC&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2305.11910</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#19982;VIIRS&#21355;&#26143;&#26816;&#32034;&#22312;&#37326;&#28779;&#31649;&#29702;&#30340;&#29123;&#26009;&#28287;&#24230;&#30417;&#27979;&#20013;&#30340;&#25216;&#26415;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine Learning and VIIRS Satellite Retrievals for Skillful Fuel Moisture Content Monitoring in Wildfire Management. (arXiv:2305.11910v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#22269;&#23478;&#27700;&#36164;&#28304;&#27169;&#22411;&#21644;&#25968;&#20540;&#22825;&#27668;&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#21450;&#21355;&#26143;&#26816;&#32034;&#26469;&#39044;&#27979;&#32654;&#22269;&#36830;&#32493;&#26412;&#22303;&#19978;&#30340;&#27515;&#20129;&#29123;&#26009;&#28287;&#24230;&#26816;&#32034;&#65292;&#36229;&#36807;&#20102;&#26082;&#26377;&#30340;&#27599;&#26085;&#21644;&#27599;&#23567;&#26102;&#27668;&#20505;&#32479;&#35745;&#26041;&#27861;&#12290;VIIRS&#26816;&#32034;&#23545;&#39044;&#27979;FMC&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#27979;&#26893;&#34987;&#29123;&#26009;&#28287;&#24230;&#23545;&#20110;&#37326;&#28779;&#30340;&#31649;&#29702;&#21644;&#20943;&#36731;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#21033;&#29992;&#37326;&#22806;&#29123;&#26009;&#28287;&#24230;&#35266;&#27979;&#12289;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#21644;&#21355;&#26143;&#26816;&#32034;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#20272;&#35745;&#32654;&#22269;&#36830;&#32493;&#26412;&#22303;&#19978;&#30340;&#27515;&#20129;&#29123;&#26009;&#28287;&#24230;&#26816;&#32034;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;National Water Model&#21644;High-Resolution Rapid Refresh&#65288;HRRR&#65289;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#21464;&#37327;&#20197;&#21450;&#34920;&#38754;&#23646;&#24615;&#65292;&#20197;&#21450;Suomi-NPP&#21355;&#26143;&#31995;&#32479;&#19978;&#30340;VIIRS&#20202;&#22120;&#30340;&#34920;&#38754;&#21453;&#23556;&#29575;&#21644;&#38470;&#22320;&#34920;&#38754;&#28201;&#24230;&#26816;&#32034;&#65292;&#35757;&#32451;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#36229;&#21442;&#35843;&#20248;&#65292;&#19982;&#27599;&#26085;&#27668;&#20505;&#32479;&#35745;&#35823;&#24046;&#65288;+44&#65285;&#65289;&#21644;&#27599;&#23567;&#26102;&#27668;&#20505;&#32479;&#35745;&#35823;&#24046;&#65288;+24&#65285;&#65289;&#30456;&#27604;&#65292;&#24471;&#21040;&#20102;&#25216;&#26415;&#23092;&#29087;&#30340;FMC&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;VIIRS&#26816;&#32034;&#20316;&#20026;&#19968;&#20010;&#32676;&#20307;&#23545;&#20272;&#35745;FMC&#26159;&#37325;&#35201;&#30340;&#39044;&#27979;&#22240;&#23376;&#65292;&#26377;&#26174;&#33879;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monitoring the fuel moisture content (FMC) of vegetation is crucial for managing and mitigating the impact of wildland fires. The combination of in situ FMC observations with numerical weather prediction (NWP) models and satellite retrievals has enabled the development of machine learning (ML) models to estimate dead FMC retrievals over the contiguous US (CONUS). In this study, ML models were trained using variables from the National Water Model and the High-Resolution Rapid Refresh (HRRR) NWP models, and static variables characterizing the surface properties, as well as surface reflectances and land surface temperature (LST) retrievals from the VIIRS instrument on board the Suomi-NPP satellite system. Extensive hyper-parameter optimization yielded skillful FMC models compared to a daily climatography RMSE (+44\%) and to an hourly climatography RMSE (+24\%). Furthermore, VIIRS retrievals were important predictors for estimating FMC, contributing significantly as a group due to their hi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24207;&#21015;&#26368;&#20248;&#33218;&#35782;&#21035;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#33041;-&#26426;&#25509;&#21475;&#20013;&#30340;&#25340;&#20889;&#31995;&#32479;&#12290;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#24555;&#22320;&#36827;&#34892;&#23398;&#20064;&#24182;&#25552;&#39640;&#20449;&#24687;&#20256;&#36755;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.11908</link><description>&lt;p&gt;
&#24207;&#21015;&#26368;&#20248;&#33218;&#35782;&#21035;&#21450;&#20854;&#22312;&#33041;-&#26426;&#25509;&#21475;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Sequential Best-Arm Identification with Application to Brain-Computer Interface. (arXiv:2305.11908v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24207;&#21015;&#26368;&#20248;&#33218;&#35782;&#21035;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#33041;-&#26426;&#25509;&#21475;&#20013;&#30340;&#25340;&#20889;&#31995;&#32479;&#12290;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#24555;&#22320;&#36827;&#34892;&#23398;&#20064;&#24182;&#25552;&#39640;&#20449;&#24687;&#20256;&#36755;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;-&#26426;&#25509;&#21475;&#26159;&#19968;&#31181;&#20351;&#22823;&#33041;&#19982;&#22806;&#37096;&#35774;&#22791;&#25110;&#35745;&#31639;&#26426;&#31995;&#32479;&#30452;&#25509;&#36890;&#20449;&#30340;&#25216;&#26415;&#65292;&#23427;&#20801;&#35768;&#20010;&#20307;&#21482;&#20351;&#29992;&#24605;&#32500;&#19982;&#35774;&#22791;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#20855;&#26377;&#22312;&#21307;&#23398;&#12289;&#24247;&#22797;&#21644;&#20154;&#20307;&#22686;&#24378;&#31561;&#39046;&#22495;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290; &#22522;&#20110;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#21644;&#20107;&#20214;&#30456;&#20851;&#30005;&#20301;&#65288;ERP&#65289;&#30340;&#25340;&#20889;&#22120;&#31995;&#32479;&#26159;&#19968;&#31181;&#31867;&#22411;&#30340;&#33041;-&#26426;&#25509;&#21475;&#65292;&#23427;&#20801;&#35768;&#29992;&#25143;&#22312;&#19981;&#20351;&#29992;&#29289;&#29702;&#38190;&#30424;&#30340;&#24773;&#20917;&#19979;&#25340;&#20889;&#21333;&#35789;&#65292;&#32780;&#26159;&#36890;&#36807;&#35760;&#24405;&#21644;&#35299;&#37322;&#22312;&#19981;&#21516;&#30340;&#21050;&#28608;&#21576;&#29616;&#33539;&#20363;&#19979;&#30340;&#33041;&#20449;&#21495;&#12290;&#20256;&#32479;&#30340;&#38750;&#33258;&#36866;&#24212;&#33539;&#20363;&#23558;&#27599;&#20010;&#21333;&#35789;&#36873;&#25321;&#35270;&#20026;&#29420;&#31435;&#30340;&#65292;&#23548;&#33268;&#20102;&#28459;&#38271;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#20026;&#20102;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#22810;&#33218;&#32769;&#34382;&#26426;&#20013;&#19968;&#31995;&#21015;&#26368;&#20248;&#33218;&#35782;&#21035;&#20219;&#21153;&#12290;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#25105;&#20204;&#21033;&#29992;&#20174;&#20808;&#21069;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#30340;&#20808;&#39564;&#30693;&#35782;&#26469;&#36890;&#30693;&#21644;&#20419;&#36827;&#21518;&#32493;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#24555;&#30340;&#23398;&#20064;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#20449;&#24687;&#20256;&#36755;&#36895;&#29575;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;ERP&#25340;&#20889;&#23454;&#39564;&#21644;&#30495;&#23454;&#30340;EEG&#25171;&#23383;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A brain-computer interface (BCI) is a technology that enables direct communication between the brain and an external device or computer system. It allows individuals to interact with the device using only their thoughts, and holds immense potential for a wide range of applications in medicine, rehabilitation, and human augmentation. An electroencephalogram (EEG) and event-related potential (ERP)-based speller system is a type of BCI that allows users to spell words without using a physical keyboard, but instead by recording and interpreting brain signals under different stimulus presentation paradigms. Conventional non-adaptive paradigms treat each word selection independently, leading to a lengthy learning process. To improve the sampling efficiency, we cast the problem as a sequence of best-arm identification tasks in multi-armed bandits. Leveraging pre-trained large language models (LLMs), we utilize the prior knowledge learned from previous tasks to inform and facilitate subsequent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;ENEC&#21644;&#22522;&#20110;z&#20998;&#25968;&#65288;ZVE&#65289;&#26041;&#24046;&#30340;&#26657;&#20934;&#35823;&#24046;&#65307;&#25351;&#20986;&#22312;&#26657;&#20934;&#33391;&#22909;&#25110;&#20960;&#20046;&#26657;&#20934;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#35823;&#24046;&#19982;&#20998;&#32452;&#25968;&#37327;&#30340;&#24179;&#26041;&#26681;&#25104;&#27604;&#20363;&#20851;&#31995;&#65292;&#25552;&#20986;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#20197;&#25512;&#26029;ENCE&#21644;ZVE&#30340;&#20540;&#65292;&#24182;&#25552;&#20379;&#32479;&#35745;&#26657;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2305.11905</link><description>&lt;p&gt;
&#20851;&#20110;ENCE&#21644;&#20854;&#20182;&#22522;&#20110;MAD&#30340;&#26657;&#20934;&#24230;&#37327;&#30340;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Properties of the ENCE and other MAD-based calibration metrics. (arXiv:2305.11905v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;ENEC&#21644;&#22522;&#20110;z&#20998;&#25968;&#65288;ZVE&#65289;&#26041;&#24046;&#30340;&#26657;&#20934;&#35823;&#24046;&#65307;&#25351;&#20986;&#22312;&#26657;&#20934;&#33391;&#22909;&#25110;&#20960;&#20046;&#26657;&#20934;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#35823;&#24046;&#19982;&#20998;&#32452;&#25968;&#37327;&#30340;&#24179;&#26041;&#26681;&#25104;&#27604;&#20363;&#20851;&#31995;&#65292;&#25552;&#20986;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#20197;&#25512;&#26029;ENCE&#21644;ZVE&#30340;&#20540;&#65292;&#24182;&#25552;&#20379;&#32479;&#35745;&#26657;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12300;&#26399;&#26395;&#24402;&#19968;&#21270;&#26657;&#20934;&#35823;&#24046;&#65288;ENCE&#65289;&#12301;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#29992;&#20110;&#35780;&#20272;&#22238;&#24402;&#38382;&#39064;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#36136;&#37327;&#30340;&#24120;&#35265;&#26657;&#20934;&#32479;&#35745;&#37327;&#65292;&#20854;&#20272;&#35745;&#22522;&#20110;&#26657;&#20934;&#25968;&#25454;&#30340;&#20998;&#32452;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;ENCE&#30340;&#19968;&#20010;&#20196;&#20154;&#24700;&#28779;&#30340;&#29305;&#24615;&#65292;&#21363;&#22312;&#26657;&#20934;&#33391;&#22909;&#25110;&#20960;&#20046;&#26657;&#20934;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#23427;&#19982;&#20998;&#32452;&#25968;&#37327;&#30340;&#24179;&#26041;&#26681;&#25104;&#27604;&#20363;&#20851;&#31995;&#12290;&#31867;&#20284;&#30340;&#34892;&#20026;&#36824;&#24433;&#21709;&#20102;&#22522;&#20110;z&#20998;&#25968;&#65288;ZVE&#65289;&#26041;&#24046;&#30340;&#26657;&#20934;&#35823;&#24046;&#65292;&#24182;&#19988;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#27492;&#29305;&#24615;&#26159;&#20351;&#29992;&#24179;&#22343;&#32477;&#23545;&#20559;&#24046;&#65288;MAD&#65289;&#32479;&#35745;&#37327;&#20272;&#35745;&#26657;&#20934;&#35823;&#24046;&#30340;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#36873;&#25321;&#20998;&#32452;&#25968;&#20197;&#21487;&#38752;&#22320;&#20272;&#35745;&#26657;&#20934;&#35823;&#24046;&#32479;&#35745;&#37327;&#25104;&#20026;&#19968;&#20010;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#25512;&#26029;ENCE&#21644;ZVE&#30340;&#20540;&#65292;&#20551;&#35774;&#25968;&#25454;&#38598;&#24050;&#32463;&#26657;&#20934;&#65292;&#24182;&#21516;&#26102;&#25552;&#20379;&#32479;&#35745;&#26657;&#20934;&#27979;&#35797;&#12290;&#21516;&#26102;&#36824;&#34920;&#26126;&#65292;&#23545;&#20110;&#19981;&#26029;&#22686;&#21152;&#30340;&#20998;&#32452;&#23494;&#24230;&#65292;ZVE&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#31561;&#20215;&#20110;ENCE&#12290;
&lt;/p&gt;
&lt;p&gt;
The Expected Normalized Calibration Error (ENCE) is a popular calibration statistic used in Machine Learning to assess the quality of prediction uncertainties for regression problems. Estimation of the ENCE is based on the binning of calibration data. In this short note, I illustrate an annoying property of the ENCE, i.e. its proportionality to the square root of the number of bins for well calibrated or nearly calibrated datasets. A similar behavior affects the calibration error based on the variance of z-scores (ZVE), and in both cases this property is a consequence of the use of a Mean Absolute Deviation (MAD) statistic to estimate calibration errors. Hence, the question arises of which number of bins to choose for a reliable estimation of calibration error statistics. A solution is proposed to infer ENCE and ZVE values that do not depend on the number of bins for datasets assumed to be calibrated, providing simultaneously a statistical calibration test. It is also shown that the ZV
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22522;&#20110;&#28023;&#27915;&#35760;&#24518;&#25928;&#24212;&#24320;&#21457;&#20102;&#19968;&#20010;LSTM&#27169;&#22411;&#65292;&#32467;&#21512;&#36807;&#21435;&#30340;ASI&#21644;&#23612;&#23068;&#25351;&#25968;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;ASI&#39044;&#27979;&#65292;&#20026;&#25552;&#21069;&#21046;&#23450;&#31354;&#27668;&#36136;&#37327;&#31649;&#29702;&#35745;&#21010;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2305.11901</link><description>&lt;p&gt;
&#21033;&#29992;&#28023;&#27915;&#35760;&#24518;&#25928;&#24212;&#39044;&#27979;&#20013;&#22269;&#21335;&#26041;&#20908;&#23395;&#31354;&#27668;&#31283;&#23450;&#25351;&#25968;&#30340;&#38271;&#26399;&#39044;&#25253;
&lt;/p&gt;
&lt;p&gt;
Long-lead forecasts of wintertime air stagnation index in southern China using oceanic memory effects. (arXiv:2305.11901v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11901
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22522;&#20110;&#28023;&#27915;&#35760;&#24518;&#25928;&#24212;&#24320;&#21457;&#20102;&#19968;&#20010;LSTM&#27169;&#22411;&#65292;&#32467;&#21512;&#36807;&#21435;&#30340;ASI&#21644;&#23612;&#23068;&#25351;&#25968;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;ASI&#39044;&#27979;&#65292;&#20026;&#25552;&#21069;&#21046;&#23450;&#31354;&#27668;&#36136;&#37327;&#31649;&#29702;&#35745;&#21010;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21033;&#20110;&#31354;&#27668;&#27745;&#26579;&#29289;&#30340;&#31232;&#37322;&#21644;&#28165;&#38500;&#65292;&#26159;&#23548;&#33268;&#31354;&#27668;&#27745;&#26579;&#30340;&#20027;&#35201;&#22240;&#32032;&#20043;&#19968;&#12290;&#31354;&#27668;&#31283;&#23450;&#25351;&#25968;&#65288;ASI&#65289;&#26159;&#27979;&#37327;&#22823;&#27668;&#28165;&#38500;&#31354;&#27668;&#27745;&#26579;&#29289;&#33021;&#21147;&#30340;&#19968;&#39033;&#37325;&#35201;&#27668;&#35937;&#25351;&#26631;&#12290;&#22240;&#27492;&#65292;&#36827;&#34892;&#38271;&#26399;ASI&#39044;&#25253;&#23545;&#20110;&#25552;&#21069;&#21046;&#23450;&#31354;&#27668;&#36136;&#37327;&#31649;&#29702;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#30001;&#28023;&#34920;&#28201;&#24230;&#24322;&#24120;&#25512;&#23548;&#20986;&#30340;&#31179;&#23395;&#23612;&#23068;&#25351;&#25968;&#19982;&#20013;&#22269;&#21335;&#26041;&#20908;&#23395;ASI&#21576;&#36127;&#30456;&#20851;&#65292;&#20026;&#39044;&#27979;&#20908;&#23395;ASI&#25552;&#20379;&#20102;&#21069;&#26223;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;LSTM&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;ASI&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#20803;&#36755;&#20837;&#65288;&#36807;&#21435;&#30340;ASI&#21644;&#23612;&#23068;&#25351;&#25968;&#65289;&#27604;&#21333;&#20803;&#36755;&#20837;&#65288;&#20165;&#36807;&#21435;&#30340;ASI&#65289;&#20855;&#26377;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#22312;&#23454;&#38469;&#21644;&#39044;&#27979;ASI&#20043;&#38388;&#23454;&#29616;&#20102;0.778&#30340;&#30456;&#20851;&#31995;&#25968;&#65292;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stagnant weather condition is one of the major contributors to air pollution as it is favorable for the formation and accumulation of pollutants. To measure the atmosphere's ability to dilute air pollutants, Air Stagnation Index (ASI) has been introduced as an important meteorological index. Therefore, making long-lead ASI forecasts is vital to make plans in advance for air quality management. In this study, we found that autumn Ni\~no indices derived from sea surface temperature (SST) anomalies show a negative correlation with wintertime ASI in southern China, offering prospects for a prewinter forecast. We developed an LSTM-based model to predict the future wintertime ASI. Results demonstrated that multivariate inputs (past ASI and Ni\~no indices) achieve better forecast performance than univariate input (only past ASI). The model achieves a correlation coefficient of 0.778 between the actual and predicted ASI, exhibiting a high degree of consistency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Particle Photon&#21644;&#26234;&#33021;&#25163;&#26426;&#30340;&#33258;&#21160;&#21270;&#33410;&#33021;&#31995;&#32479;&#65288;APCS&#65289;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;IR&#20256;&#24863;&#22120;&#26816;&#27979;&#25945;&#23460;&#20869;&#20154;&#21592;&#30340;&#23384;&#22312;&#65292;&#24182;&#33258;&#21160;&#25511;&#21046;&#28783;&#21644;&#39118;&#25159;&#30340;&#24320;&#20851;&#65292;&#33410;&#30465;&#30005;&#21147;&#28040;&#32791;&#21644;&#33410;&#32422;&#23453;&#36149;&#33258;&#28982;&#36164;&#28304;&#12290;&#36890;&#36807;&#26234;&#33021;&#25163;&#26426;&#24212;&#29992;&#31243;&#24207;&#23545;&#31995;&#32479;&#36827;&#34892;&#25511;&#21046;&#21644;&#30417;&#25511;&#65292;&#26131;&#20110;&#23454;&#26045;&#21644;&#32500;&#25252;&#12290;</title><link>http://arxiv.org/abs/2305.11889</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;Particle Photon&#21644;&#26234;&#33021;&#25163;&#26426;&#30340;&#33258;&#21160;&#21270;&#33410;&#33021;&#31995;&#32479;&#65288;APCS&#65289;
&lt;/p&gt;
&lt;p&gt;
An Automated Power Conservation System (APCS) using Particle Photon and Smartphone. (arXiv:2305.11889v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Particle Photon&#21644;&#26234;&#33021;&#25163;&#26426;&#30340;&#33258;&#21160;&#21270;&#33410;&#33021;&#31995;&#32479;&#65288;APCS&#65289;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;IR&#20256;&#24863;&#22120;&#26816;&#27979;&#25945;&#23460;&#20869;&#20154;&#21592;&#30340;&#23384;&#22312;&#65292;&#24182;&#33258;&#21160;&#25511;&#21046;&#28783;&#21644;&#39118;&#25159;&#30340;&#24320;&#20851;&#65292;&#33410;&#30465;&#30005;&#21147;&#28040;&#32791;&#21644;&#33410;&#32422;&#23453;&#36149;&#33258;&#28982;&#36164;&#28304;&#12290;&#36890;&#36807;&#26234;&#33021;&#25163;&#26426;&#24212;&#29992;&#31243;&#24207;&#23545;&#31995;&#32479;&#36827;&#34892;&#25511;&#21046;&#21644;&#30417;&#25511;&#65292;&#26131;&#20110;&#23454;&#26045;&#21644;&#32500;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20154;&#20204;&#22312;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#37117;&#20351;&#29992;&#30005;&#21147;&#65292;&#22240;&#27492;&#30005;&#21147;&#28040;&#32791;&#36880;&#28176;&#22686;&#21152;&#12290;&#30001;&#20110;&#20154;&#20026;&#30095;&#24573;&#12289;&#26085;&#20809;&#31561;&#21508;&#31181;&#21407;&#22240;&#65292;&#30005;&#21147;&#21487;&#33021;&#20250;&#28010;&#36153;&#12290;&#22240;&#27492;&#65292;&#33410;&#32422;&#33021;&#28304;&#26159;&#24403;&#21153;&#20043;&#24613;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#8220;&#33258;&#21160;&#21270;&#33410;&#33021;&#31995;&#32479;&#65288;APCS&#65289;&#8221;&#30340;&#21046;&#20316;&#65292;&#20855;&#26377;&#22810;&#31181;&#22909;&#22788;&#65292;&#22914;&#33410;&#30465;&#30005;&#21147;&#28040;&#32791;&#65292;&#20174;&#32780;&#33410;&#30465;&#32452;&#32455;&#30340;&#30005;&#36153;&#65292;&#28040;&#38500;&#20154;&#20026;&#24178;&#39044;&#21644;&#25163;&#21160;&#24320;&#20851;&#28783;&#21644;&#30005;&#27668;&#35013;&#32622;&#25152;&#38656;&#30340;&#20154;&#21147;&#65292;&#26368;&#37325;&#35201;&#30340;&#26159;&#36890;&#36807;&#20943;&#23569;&#30005;&#27668;&#33021;&#32791;&#26469;&#33410;&#32422;&#23453;&#36149;&#30340;&#33258;&#28982;&#36164;&#28304;&#12290;&#35813;&#39033;&#30446;&#20351;&#29992;&#20102;&#20004;&#20010;IR&#20256;&#24863;&#22120;&#65292;&#29992;&#20110;&#26816;&#27979;&#25945;&#23460;&#20013;&#20154;&#21592;&#30340;&#23384;&#22312;&#12290;&#24403;APCS&#26816;&#27979;&#21040;&#26377;&#20154;&#23384;&#22312;&#26102;&#65292;&#23427;&#20250;&#33258;&#21160;&#25171;&#24320;&#25945;&#23460;&#20013;&#30340;&#39118;&#25159;&#21644;&#28783;&#65292;&#22312;&#20154;&#31163;&#24320;&#26102;&#33258;&#21160;&#20851;&#38381;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#26234;&#33021;&#25163;&#26426;&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#25511;&#21046;&#21644;&#30417;&#25511;&#65292;&#26041;&#20415;&#29992;&#25143;&#38543;&#26102;&#35775;&#38382;&#31995;&#32479;&#12290;&#30001;&#20110;&#26131;&#20110;&#23454;&#26045;&#21644;&#32500;&#25252;&#65292;APCS&#24050;&#34987;&#35777;&#26126;&#26159;&#39640;&#25928;&#21644;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, people use electricity in all aspects of their lives so that electricity consumption increases gradually. There can be wastage of electricity due to various reasons, such as human negligence, daylighting, etc. Hence, conservation of energy is the need of the day. This paper deals with the fabrication of an "Automated Power Conservation System (APCS)" that has multiple benefits like saving on power consumption there by saving on electricity bills of the organization, eliminating human involvement and manpower which is often required to manually toggle the lights and electrical devices on/off, and last but most importantly conserve the precious natural resources by reducing electrical energy consumption. Two IR sensors are used in this project and these two sensors are used for detecting the presence of a person in the classroom. When the existence of the person is detected by the APCS it automatically turns on the fans and lights in that classroom and during the absence they w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#21644;&#39057;&#29575;&#22686;&#24378;&#26426;&#21046;&#30340;&#39118;&#36895;&#39044;&#27979;&#27169;&#22411;GFST-WSF&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#30701;&#26399;&#39118;&#36895;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11526</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#21644;&#39057;&#29575;&#22686;&#24378;&#26426;&#21046;&#30340;&#30701;&#26399;&#39118;&#36895;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Short-Term Wind Speed Forecasting using Graph Attention and Frequency-Enhanced Mechanisms. (arXiv:2305.11526v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#21644;&#39057;&#29575;&#22686;&#24378;&#26426;&#21046;&#30340;&#39118;&#36895;&#39044;&#27979;&#27169;&#22411;GFST-WSF&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#30701;&#26399;&#39118;&#36895;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39118;&#30005;&#22823;&#35268;&#27169;&#38598;&#25104;&#30005;&#32593;&#20013;&#65292;&#39118;&#21147;&#30340;&#39640;&#21487;&#21464;&#24615;&#21644;&#38543;&#26426;&#24615;&#23545;&#30005;&#21147;&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#31283;&#23450;&#36816;&#34892;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#39118;&#21147;&#39044;&#27979;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#20854;&#20013;&#39118;&#36895;&#39044;&#27979;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#26041;&#38754;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#21644;&#39057;&#29575;&#22686;&#24378;&#26426;&#21046;&#30340;&#22270;&#27880;&#24847;&#21147;&#39057;&#29575;&#22686;&#24378;&#26102;&#31354;&#39118;&#36895;&#39044;&#27979;&#27169;&#22411;&#65288;GFST-WSF&#65289;&#65292;&#20197;&#25552;&#39640;&#30701;&#26399;&#39118;&#36895;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;GFST-WSF&#21253;&#25324;&#29992;&#20110;&#25552;&#21462;&#26102;&#38388;&#29305;&#24449;&#30340;Transformer&#26550;&#26500;&#21644;&#29992;&#20110;&#25552;&#21462;&#31354;&#38388;&#29305;&#24449;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GAT&#65289;&#12290;GAT&#34987;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25429;&#25417;&#39118;&#36895;&#31449;&#20043;&#38388;&#30340;&#22797;&#26434;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#32858;&#21512;&#22270;&#20013;&#30456;&#37051;&#33410;&#28857;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#22686;&#24378;&#25968;&#25454;&#30340;&#31354;&#38388;&#34920;&#31034;&#12290;&#20026;&#20102;&#27169;&#25311;&#37051;&#36817;&#39118;&#22330;&#20043;&#38388;&#30340;&#39118;&#36895;&#30456;&#20851;&#30340;&#26102;&#38388;&#28382;&#21518;
&lt;/p&gt;
&lt;p&gt;
The safe and stable operation of power systems is greatly challenged by the high variability and randomness of wind power in large-scale wind-power-integrated grids. Wind power forecasting is an effective solution to tackle this issue, with wind speed forecasting being an essential aspect. In this paper, a Graph-attentive Frequency-enhanced Spatial-Temporal Wind Speed Forecasting model based on graph attention and frequency-enhanced mechanisms, i.e., GFST-WSF, is proposed to improve the accuracy of short-term wind speed forecasting. The GFST-WSF comprises a Transformer architecture for temporal feature extraction and a Graph Attention Network (GAT) for spatial feature extraction. The GAT is specifically designed to capture the complex spatial dependencies among wind speed stations to effectively aggregate information from neighboring nodes in the graph, thus enhancing the spatial representation of the data. To model the time lag in wind speed correlation between adjacent wind farms cau
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36861;&#36214;&#33976;&#39311;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20256;&#32479;&#37319;&#26679;&#31639;&#27861;&#65292;&#35753;&#36895;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#24403;&#21069;&#26102;&#21051;&#36755;&#20986;&#19982;&#20854;&#20808;&#21069;&#26102;&#21051;&#36755;&#20986;&#21644;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#23545;&#40784;&#65292;&#20174;&#32780;&#23454;&#29616;&#21482;&#38656;&#19968;&#27425;&#35757;&#32451;&#20415;&#33021;&#21152;&#36895;&#37319;&#26679;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10769</link><description>&lt;p&gt;
&#36861;&#36214;&#33976;&#39311;&#65306;&#21152;&#36895;&#37319;&#26679;&#21482;&#38656;&#19968;&#27425;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Catch-Up Distillation: You Only Need to Train Once for Accelerating Sampling. (arXiv:2305.10769v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36861;&#36214;&#33976;&#39311;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20256;&#32479;&#37319;&#26679;&#31639;&#27861;&#65292;&#35753;&#36895;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#24403;&#21069;&#26102;&#21051;&#36755;&#20986;&#19982;&#20854;&#20808;&#21069;&#26102;&#21051;&#36755;&#20986;&#21644;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#23545;&#40784;&#65292;&#20174;&#32780;&#23454;&#29616;&#21482;&#38656;&#19968;&#27425;&#35757;&#32451;&#20415;&#33021;&#21152;&#36895;&#37319;&#26679;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#36890;&#24120;&#38656;&#35201;&#25191;&#34892;&#22823;&#37327;&#30340;&#37319;&#26679;&#27493;&#39588;&#65292;&#36825;&#38459;&#30861;&#20102;&#23454;&#26102;&#26679;&#26412;&#21512;&#25104;&#30340;&#21487;&#33021;&#24615;&#12290;&#20256;&#32479;&#30340;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21152;&#36895;&#37319;&#26679;&#30340;&#31639;&#27861;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#26435;&#37325;&#21644;&#31163;&#25955;&#26102;&#38388;&#27493;&#39588;&#22330;&#26223;&#65292;&#38656;&#35201;&#39069;&#22806;&#30340;&#22521;&#35757;&#35838;&#31243;&#25165;&#33021;&#23454;&#29616;&#20182;&#20204;&#30340;&#30446;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36861;&#36214;&#33976;&#39311;&#65288;CUD&#65289;&#65292;&#23427;&#40723;&#21169;&#36895;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#24403;&#21069;&#26102;&#21051;&#36755;&#20986;&#8220;&#36861;&#36214;&#8221;&#20854;&#20808;&#21069;&#26102;&#21051;&#36755;&#20986;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CUD&#35843;&#25972;&#20102;&#21407;&#22987;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#35757;&#32451;&#30446;&#26631;&#65292;&#20197;&#20351;&#24403;&#21069;&#26102;&#21051;&#36755;&#20986;&#19982;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#21644;&#20808;&#21069;&#26102;&#21051;&#36755;&#20986;&#23545;&#40784;&#65292;&#21033;&#29992;&#22522;&#20110;&#40857;&#26684;-&#24211;&#22612;&#30340;&#22810;&#27493;&#23545;&#40784;&#33976;&#39311;&#36827;&#34892;&#31934;&#30830;&#30340;ODE&#20272;&#35745;&#65292;&#21516;&#26102;&#38450;&#27490;&#24322;&#27493;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion Probability Models (DPMs) have made impressive advancements in various machine learning domains. However, achieving high-quality synthetic samples typically involves performing a large number of sampling steps, which impedes the possibility of real-time sample synthesis. Traditional accelerated sampling algorithms via knowledge distillation rely on pre-trained model weights and discrete time step scenarios, necessitating additional training sessions to achieve their goals. To address these issues, we propose the Catch-Up Distillation (CUD), which encourages the current moment output of the velocity estimation model ``catch up'' with its previous moment output. Specifically, CUD adjusts the original Ordinary Differential Equation (ODE) training objective to align the current moment output with both the ground truth label and the previous moment output, utilizing Runge-Kutta-based multi-step alignment distillation for precise ODE estimation while preventing asynchronous updates
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#31283;&#24577;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#25240;&#25187;&#27748;&#26222;&#26862;&#25277;&#26679;&#31639;&#27861;&#65288;DS-TS&#65289;&#65292;&#21487;&#20197;&#35299;&#20915;&#31361;&#28982;&#24615;&#21464;&#21270;&#21644;&#24179;&#28369;&#24615;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#30340;&#36951;&#25022;&#19978;&#38480;&#12290;</title><link>http://arxiv.org/abs/2305.10718</link><description>&lt;p&gt;
&#38024;&#23545;&#38750;&#31283;&#24577;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#25240;&#25187;&#27748;&#26222;&#26862;&#25277;&#26679;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Discounted Thompson Sampling for Non-Stationary Bandit Problems. (arXiv:2305.10718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10718
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#31283;&#24577;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#25240;&#25187;&#27748;&#26222;&#26862;&#25277;&#26679;&#31639;&#27861;&#65288;DS-TS&#65289;&#65292;&#21487;&#20197;&#35299;&#20915;&#31361;&#28982;&#24615;&#21464;&#21270;&#21644;&#24179;&#28369;&#24615;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#30340;&#36951;&#25022;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38750;&#31283;&#24577;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#21463;&#21040;&#20102;&#26174;&#33879;&#20851;&#27880;&#12290;NS-MAB&#36890;&#24120;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#36827;&#34892;&#24314;&#27169;&#65306;&#31361;&#28982;&#24615;&#21464;&#21270;&#21644;&#24179;&#28369;&#24615;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#26377;&#39640;&#26031;&#20808;&#39564;&#30340;&#25240;&#25187;&#27748;&#26222;&#26862;&#37319;&#26679;&#31639;&#27861;&#65288;DS-TS&#65289;&#20197;&#35299;&#20915;&#36825;&#20004;&#20010;&#38750;&#31283;&#24577;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#23558;&#25240;&#25187;&#22240;&#23376;&#32435;&#20837;&#27748;&#26222;&#26862;&#37319;&#26679;&#26469;&#34987;&#21160;&#36866;&#24212;&#21464;&#21270;&#12290;DS-TS&#26041;&#27861;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#20294;&#32570;&#20047;&#23545;&#36951;&#25022;&#19978;&#38480;&#30340;&#20998;&#26512;&#12290;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24102;&#26377;&#39640;&#26031;&#20808;&#39564;&#30340;DS-TS&#21487;&#20197;&#22312;&#31361;&#28982;&#24615;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#36817;&#20046;&#26368;&#20248;&#30340;&#36951;&#25022;&#19978;&#38480;&#65288;$\tilde{O} (\sqrt {TB_T})$&#65289;&#65292;&#22312;&#24179;&#28369;&#24615;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616; $\tilde{O}(T^{\beta})$ &#30340;&#36817;&#20046;&#26368;&#20248;&#36951;&#25022;&#19978;&#38480;&#65292;&#20854;&#20013; $T$ &#26159;&#26102;&#38388;&#27493;&#25968;&#65292;$B_T$ &#26159;&#26029;&#28857;&#25968;&#65292;$\beta$ &#19982;&#25910;&#30410;&#20998;&#24067;&#30340;&#24179;&#28369;&#24615;&#26377;&#20851;&#65292;$\tilde{O}$ &#26159;&#23545;&#25968;&#36951;&#25022;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-stationary multi-armed bandit (NS-MAB) problems have recently received significant attention. NS-MAB are typically modelled in two scenarios: abruptly changing, where reward distributions remain constant for a certain period and change at unknown time steps, and smoothly changing, where reward distributions evolve smoothly based on unknown dynamics. In this paper, we propose Discounted Thompson Sampling (DS-TS) with Gaussian priors to address both non-stationary settings. Our algorithm passively adapts to changes by incorporating a discounted factor into Thompson Sampling. DS-TS method has been experimentally validated, but analysis of the regret upper bound is currently lacking. Under mild assumptions, we show that DS-TS with Gaussian priors can achieve nearly optimal regret bound on the order of $\tilde{O}(\sqrt{TB_T})$ for abruptly changing and $\tilde{O}(T^{\beta})$ for smoothly changing, where $T$ is the number of time steps, $B_T$ is the number of breakpoints, $\beta$ is asso
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#24352;&#37327;&#31215;&#22312;&#36229;&#32500;&#35745;&#31639;&#20013;&#30340;&#25968;&#23398;&#20851;&#31995;&#65292;&#23558;&#20854;&#30830;&#23450;&#20026;&#20013;&#24515;&#34920;&#31034;&#65292;&#24182;&#21457;&#29616;&#23427;&#26159;&#26368;&#36890;&#29992;&#12289;&#26368;&#20855;&#34920;&#29616;&#21147;&#21644;&#26368;&#21387;&#32553;&#30340;&#34920;&#31034;&#65292;&#21516;&#26102;&#20855;&#26377;&#26080;&#35823;&#24046;&#35299;&#32465;&#21644;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.10572</link><description>&lt;p&gt;
&#24352;&#37327;&#31215;&#19982;&#36229;&#32500;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Tensor Products and Hyperdimensional Computing. (arXiv:2305.10572v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#24352;&#37327;&#31215;&#22312;&#36229;&#32500;&#35745;&#31639;&#20013;&#30340;&#25968;&#23398;&#20851;&#31995;&#65292;&#23558;&#20854;&#30830;&#23450;&#20026;&#20013;&#24515;&#34920;&#31034;&#65292;&#24182;&#21457;&#29616;&#23427;&#26159;&#26368;&#36890;&#29992;&#12289;&#26368;&#20855;&#34920;&#29616;&#21147;&#21644;&#26368;&#21387;&#32553;&#30340;&#34920;&#31034;&#65292;&#21516;&#26102;&#20855;&#26377;&#26080;&#35823;&#24046;&#35299;&#32465;&#21644;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20043;&#21069;&#23545;&#22270;&#23884;&#20837;&#30340;&#20998;&#26512;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23558;&#19968;&#20123;&#32467;&#26524;&#25512;&#24191;&#21644;&#25299;&#23637;&#21040;&#21521;&#37327;&#31526;&#21495;&#32467;&#26500; (VSA) &#21644;&#36229;&#32500;&#35745;&#31639; (HDC) &#30340;&#19968;&#33324;&#35774;&#32622;&#20013;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25506;&#32034;&#36229;&#21472;&#21152;&#12289;&#27491;&#20132;&#21644;&#24352;&#37327;&#31215;&#20043;&#38388;&#30340;&#25968;&#23398;&#20851;&#31995;&#12290;&#25105;&#20204;&#23558;&#24352;&#37327;&#31215;&#34920;&#31034;&#30830;&#23450;&#20026;&#20013;&#24515;&#34920;&#31034;&#65292;&#24182;&#20855;&#26377;&#19968;&#22871;&#29420;&#29305;&#30340;&#23646;&#24615;&#12290;&#36825;&#21253;&#25324;&#23427;&#26159;&#26368;&#36890;&#29992;&#21644;&#26368;&#20855;&#34920;&#29616;&#21147;&#30340;&#34920;&#31034;&#65292;&#20063;&#26159;&#26368;&#21387;&#32553;&#30340;&#34920;&#31034;&#65292;&#20855;&#26377;&#26080;&#35823;&#24046;&#35299;&#32465;&#21644;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following up on a previous analysis of graph embeddings, we generalize and expand some results to the general setting of vector symbolic architectures (VSA) and hyperdimensional computing (HDC). Importantly, we explore the mathematical relationship between superposition, orthogonality, and tensor product. We establish the tensor product representation as the central representation, with a suite of unique properties. These include it being the most general and expressive representation, as well as being the most compressed representation that has errorrless unbinding and detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20855;&#26377;&#36317;&#31163;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#22810;&#31034;&#20363;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#26681;&#25454;&#34917;&#19969;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#21160;&#24577;&#35843;&#25972;&#26435;&#37325;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10552</link><description>&lt;p&gt;
&#20855;&#26377;&#36317;&#31163;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#22810;&#31034;&#20363;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Multiple Instance Learning with Distance-Aware Self-Attention. (arXiv:2305.10552v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20855;&#26377;&#36317;&#31163;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#22810;&#31034;&#20363;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#26681;&#25454;&#34917;&#19969;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#21160;&#24577;&#35843;&#25972;&#26435;&#37325;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#35201;&#27714;&#23545;&#35757;&#32451;&#38598;&#20013;&#30340;&#27599;&#20010;&#23454;&#20363;&#36827;&#34892;&#26631;&#35760;&#65292;&#20294;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#26631;&#35760;&#20165;&#23545;&#23454;&#20363;&#30340;&#38598;&#21512;&#65288;&#21253;&#65289;&#21487;&#29992;&#12290;&#36825;&#31181;&#38382;&#39064;&#35774;&#32622;&#34987;&#31216;&#20026;&#22810;&#37325;&#31034;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#65292;&#22312;&#21307;&#30103;&#39046;&#22495;&#23588;&#20854;&#30456;&#20851;&#65292;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#34987;&#20998;&#25104;&#36739;&#23567;&#30340;&#34917;&#19969;&#65292;&#20294;&#26631;&#31614;&#36866;&#29992;&#20110;&#25972;&#20010;&#22270;&#20687;&#12290;&#26368;&#36817;&#30340;MIL&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#37319;&#29992;&#33258;&#25105;&#20851;&#27880;&#26469;&#25429;&#25417;&#34917;&#19969;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#26681;&#25454;&#21253;&#20013;&#25152;&#26377;&#20854;&#20182;&#34917;&#19969;&#23545;&#27599;&#20010;&#34917;&#19969;&#36827;&#34892;&#19981;&#21516;&#30340;&#21152;&#26435;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#27809;&#26377;&#32771;&#34385;&#36739;&#22823;&#22270;&#20687;&#20013;&#34917;&#19969;&#20043;&#38388;&#30340;&#30456;&#23545;&#31354;&#38388;&#20851;&#31995;&#65292;&#36825;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#20013;&#23588;&#20026;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;MIL&#27169;&#22411;&#65292;&#20855;&#26377;&#36317;&#31163;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#65288;DAS-MIL&#65289;&#65292;&#23427;&#22312;&#24314;&#27169;&#34917;&#19969;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#26102;&#26126;&#30830;&#32771;&#34385;&#30456;&#23545;&#31354;&#38388;&#20449;&#24687;&#12290;&#19982;&#29616;&#26377;&#30456;&#20851;&#27169;&#22411;&#19981;&#21516;&#65292;DAS-MIL&#20351;&#29992;&#36317;&#31163;&#24863;&#30693;&#27880;&#24847;&#26426;&#21046;&#26681;&#25454;&#34917;&#19969;&#20043;&#38388;&#30340;&#36317;&#31163;&#21160;&#24577;&#35843;&#25972;&#34917;&#19969;&#26435;&#37325;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional supervised learning tasks require a label for every instance in the training set, but in many real-world applications, labels are only available for collections (bags) of instances. This problem setting, known as multiple instance learning (MIL), is particularly relevant in the medical domain, where high-resolution images are split into smaller patches, but labels apply to the image as a whole. Recent MIL models are able to capture correspondences between patches by employing self-attention, allowing them to weigh each patch differently based on all other patches in the bag. However, these approaches still do not consider the relative spatial relationships between patches within the larger image, which is especially important in computational pathology. To this end, we introduce a novel MIL model with distance-aware self-attention (DAS-MIL), which explicitly takes into account relative spatial information when modelling the interactions between patches. Unlike existing rela
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#22312;&#26497;&#20302;&#39057;&#26234;&#33021;&#30005;&#34920;&#25968;&#25454;&#20013;&#26816;&#27979;&#19981;&#21516;&#23478;&#30005;&#30340;&#23384;&#22312;/&#32570;&#22833;&#36827;&#34892;&#20102;&#28145;&#20837;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;......</title><link>http://arxiv.org/abs/2305.10352</link><description>&lt;p&gt;
&#37319;&#29992;&#26497;&#20302;&#39057;&#26234;&#33021;&#30005;&#34920;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#23478;&#30005;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Appliance Detection Using Very Low-Frequency Smart Meter Time Series. (arXiv:2305.10352v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#22312;&#26497;&#20302;&#39057;&#26234;&#33021;&#30005;&#34920;&#25968;&#25454;&#20013;&#26816;&#27979;&#19981;&#21516;&#23478;&#30005;&#30340;&#23384;&#22312;/&#32570;&#22833;&#36827;&#34892;&#20102;&#28145;&#20837;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;......
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26234;&#33021;&#30005;&#34920;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20197;&#25913;&#21892;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#30340;&#31649;&#29702;&#65292;&#36825;&#20123;&#30005;&#34920;&#36890;&#24120;&#20197;&#26497;&#20302;&#30340;&#39057;&#29575;&#65288;&#27599;30&#20998;&#38047;&#65289;&#25910;&#38598;&#33021;&#28304;&#28040;&#32791;&#25968;&#25454;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#21521;&#23458;&#25143;&#35745;&#36153;&#12290;&#20026;&#20102;&#25552;&#20379;&#26356;&#20010;&#24615;&#21270;&#30340;&#24314;&#35758;&#65292;&#19979;&#19968;&#27493;&#26159;&#26816;&#27979;&#23458;&#25143;&#25317;&#26377;&#30340;&#23478;&#30005;&#65292;&#30001;&#20110;&#26497;&#20302;&#30340;&#35745;&#37327;&#35835;&#25968;&#39057;&#29575;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#23478;&#30005;&#26816;&#27979;&#38382;&#39064;&#21487;&#20197;&#34987;&#35270;&#20026;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#19988;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#36825;&#26679;&#30340;&#20998;&#31867;&#22120;&#65292;&#20294;&#27809;&#26377;&#30740;&#31350;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#36825;&#20010;&#20855;&#20307;&#30340;&#38382;&#39064;&#24182;&#36827;&#34892;&#27604;&#36739;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#26368;&#26032;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#22312;&#26497;&#20302;&#39057;&#26234;&#33021;&#30005;&#34920;&#25968;&#25454;&#20013;&#26816;&#27979;&#19981;&#21516;&#23478;&#30005;&#30340;&#23384;&#22312;/&#32570;&#22833;&#36827;&#34892;&#28145;&#20837;&#35780;&#20272;&#21644;&#27604;&#36739;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;5&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;13&#20010;&#26102;&#24207;&#20998;&#31867;&#22120;&#26816;&#27979;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, smart meters have been widely adopted by electricity suppliers to improve the management of the smart grid system. These meters usually collect energy consumption data at a very low frequency (every 30min), enabling utilities to bill customers more accurately. To provide more personalized recommendations, the next step is to detect the appliances owned by customers, which is a challenging problem, due to the very-low meter reading frequency. Even though the appliance detection problem can be cast as a time series classification problem, with many such classifiers having been proposed in the literature, no study has applied and compared them on this specific problem. This paper presents an in-depth evaluation and comparison of state-of-the-art time series classifiers applied to detecting the presence/absence of diverse appliances in very low-frequency smart meter data. We report results with five real datasets. We first study the impact of the detection quality of 13 di
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#26684;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#8220;&#38543;&#26426;&#36830;&#32493;&#23884;&#20837;&#8221;&#65288;Random Continuous Embedding&#65292;RCE&#65289;&#65292;&#33021;&#22815;&#25552;&#39640; Transformer-based &#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#65292;&#22823;&#24133;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#20351;&#24471;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#22312;&#30417;&#30563;&#34920;&#26684;&#23398;&#20064;&#20013;&#20248;&#20110;&#26641;&#24418;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10308</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#32771;&#34385;&#34920;&#26684;&#25968;&#25454;&#25968;&#25454;&#22686;&#24378;&#30340;&#26032;&#24605;&#36335;
&lt;/p&gt;
&lt;p&gt;
Rethinking Data Augmentation for Tabular Data in Deep Learning. (arXiv:2305.10308v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#26684;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#8220;&#38543;&#26426;&#36830;&#32493;&#23884;&#20837;&#8221;&#65288;Random Continuous Embedding&#65292;RCE&#65289;&#65292;&#33021;&#22815;&#25552;&#39640; Transformer-based &#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#65292;&#22823;&#24133;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#20351;&#24471;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#22312;&#30417;&#30563;&#34920;&#26684;&#23398;&#20064;&#20013;&#20248;&#20110;&#26641;&#24418;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#26684;&#24335;&#12290;&#34429;&#28982;&#22312;&#26377;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#26641;&#24418;&#26041;&#27861;&#20248;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65307;&#20294;&#26368;&#36817;&#30340;&#25991;&#29486;&#25253;&#21578;&#31216;&#65292;Transformer-based &#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#20248;&#20110;&#26641;&#24418;&#26041;&#27861;&#12290;&#22312;&#20851;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#29616;&#26377;&#25991;&#29486;&#20013;&#65292;&#23545;&#27604;&#23398;&#20064;&#26159;&#20027;&#23548;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#29420;&#29305;&#32467;&#26500;&#21644;&#39640;&#22797;&#26434;&#24615;&#65292;&#34920;&#26684;&#25968;&#25454;&#30340;&#25968;&#25454;&#22686;&#24378;&#19968;&#30452;&#26159;&#22256;&#38590;&#30340;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#23558;&#27169;&#22411;&#32467;&#26500;&#12289;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#21644;&#25968;&#25454;&#22686;&#24378;&#19977;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#19968;&#36215;&#25552;&#20986;&#12290;&#22240;&#27492;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#22312;&#32508;&#21512;&#32771;&#34385;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23545;&#27604;&#65292;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#23545;&#23454;&#38469;&#24615;&#33021;&#30340;&#24433;&#21709;&#36824;&#19981;&#28165;&#26970;&#12290;&#26412;&#30740;&#31350;&#20851;&#27880;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#8220;&#38543;&#26426;&#36830;&#32493;&#23884;&#20837;&#8221;&#65288;RCE&#65289;&#65292;&#36890;&#36807;&#21521;&#36830;&#32493;&#21464;&#37327;&#27880;&#20837;&#22122;&#22768;&#26469;&#29983;&#25104;&#22686;&#24378;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126; RCE &#22312;&#20351;&#29992; Transformer-based &#27169;&#22411;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#26102;&#19968;&#33268;&#20248;&#20110;&#29616;&#26377;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#31579;&#36873;&#30740;&#31350;&#20197;&#26174;&#31034; RCE &#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126; RCE &#20351; Transformer-based &#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#21487;&#22312;&#30417;&#30563;&#34920;&#26684;&#23398;&#20064;&#20013;&#20248;&#20110;&#26641;&#24418;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular data is the most widely used data format in machine learning (ML). While tree-based methods outperform DL-based methods in supervised learning, recent literature reports that self-supervised learning with Transformer-based models outperforms tree-based methods. In the existing literature on self-supervised learning for tabular data, contrastive learning is the predominant method. In contrastive learning, data augmentation is important to generate different views. However, data augmentation for tabular data has been difficult due to the unique structure and high complexity of tabular data. In addition, three main components are proposed together in existing methods: model structure, self-supervised learning methods, and data augmentation. Therefore, previous works have compared the performance without comprehensively considering these components, and it is not clear how each component affects the actual performance.  In this study, we focus on data augmentation to address these 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#34892;&#25919;&#27807;&#36890;&#20013;&#30340;&#21457;&#22768;&#31505;&#22768;&#23545;&#20110;&#31038;&#20250;&#35748;&#21487;&#30340;&#31215;&#26497;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#24403;&#21457;&#29983;&#21452;&#21521;&#31505;&#22768;&#26102;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#24433;&#21709;&#38543;&#30528;&#32452;&#32455;&#19994;&#32489;&#30340;&#19979;&#38477;&#32780;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2305.09485</link><description>&lt;p&gt;
&#34892;&#25919;&#20154;&#21592;&#30340;&#21457;&#22768;&#31505;&#22768;&#19982;&#31038;&#20250;&#35748;&#21487;&#65306;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#25506;&#31350;&#12290; (arXiv:2305.09485v1 [&#32463;&#27982;&#23398;.GN])
&lt;/p&gt;
&lt;p&gt;
Executive Voiced Laughter and Social Approval: An Explorative Machine Learning Study. (arXiv:2305.09485v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#34892;&#25919;&#27807;&#36890;&#20013;&#30340;&#21457;&#22768;&#31505;&#22768;&#23545;&#20110;&#31038;&#20250;&#35748;&#21487;&#30340;&#31215;&#26497;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#24403;&#21457;&#29983;&#21452;&#21521;&#31505;&#22768;&#26102;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#24433;&#21709;&#38543;&#30528;&#32452;&#32455;&#19994;&#32489;&#30340;&#19979;&#38477;&#32780;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#34892;&#25919;&#20154;&#21592;&#27807;&#36890;&#20013;&#30340;&#21457;&#22768;&#31505;&#22768;&#20197;&#21450;&#23427;&#23545;&#31038;&#20250;&#35748;&#21487;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#31505;&#22768;&#65292;&#24773;&#24863;&#20316;&#20026;&#20449;&#24687;&#21644;&#20449;&#24687;&#23186;&#20171;&#23545;&#20844;&#21496;&#30340;&#31038;&#20250;&#35780;&#20215;&#30340;&#30740;&#31350;&#30456;&#32467;&#21512;&#65292;&#20551;&#35774;&#34892;&#25919;&#20154;&#21592;&#27807;&#36890;&#20013;&#30340;&#21457;&#22768;&#31505;&#22768;&#23545;&#31038;&#20250;&#35748;&#21487;&#26377;&#31215;&#26497;&#24433;&#21709;&#65292;&#31038;&#20250;&#35748;&#21487;&#26159;&#25351;&#21463;&#20247;&#23545;&#19968;&#20010;&#32452;&#32455;&#30340;&#20146;&#21644;&#21147;&#30340;&#24863;&#30693;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19982;&#20247;&#31505;&#30340;&#25928;&#26524;&#23588;&#20854;&#24378;&#65292;&#21363;&#22312;&#32473;&#23450;&#30340;&#27807;&#36890;&#22330;&#21512;&#20013;&#65292;&#32858;&#28966;&#30340;&#34892;&#25919;&#20154;&#21592;&#21644;&#35266;&#20247;&#21516;&#26102;&#21457;&#31505;&#30340;&#27425;&#25968;&#12290;&#26368;&#21518;&#65292;&#32467;&#21512;&#24773;&#24863;&#20316;&#20026;&#20449;&#24687;&#21644;&#20154;&#31867;&#35748;&#30693;&#30340;&#36127;&#38754;&#20559;&#35265;&#65292;&#25105;&#20204;&#20551;&#35774;&#31505;&#22768;&#23545;&#31038;&#20250;&#35748;&#21487;&#30340;&#31215;&#26497;&#24433;&#21709;&#38543;&#30528;&#32452;&#32455;&#19994;&#32489;&#30340;&#19979;&#38477;&#32780;&#22686;&#21152;&#12290;&#25105;&#20204;&#22312;902&#20010;&#24503;&#22269;&#24052;&#26519;&#24503;&#26031;&#21033;&#21152;&#36275;&#29699;&#26032;&#38395;&#21457;&#24067;&#20250;&#21644;&#23186;&#20307;&#21313;&#22823;&#25968;&#25454;&#20013;&#36827;&#34892;&#27979;&#35797;&#65292;&#24212;&#29992;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#31505;&#22768;&#26816;&#27979;&#65292;&#24182;&#25214;&#21040;&#20102;&#37096;&#20998;&#25903;&#25345;&#25105;&#20204;&#24819;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study voiced laughter in executive communication and its effect on social approval. Integrating research on laughter, affect-as-information, and infomediaries' social evaluations of firms, we hypothesize that voiced laughter in executive communication positively affects social approval, defined as audience perceptions of affinity towards an organization. We surmise that the effect of laughter is especially strong for joint laughter, i.e., the number of instances in a given communication venue for which the focal executive and the audience laugh simultaneously. Finally, combining the notions of affect-as-information and negativity bias in human cognition, we hypothesize that the positive effect of laughter on social approval increases with bad organizational performance. We find partial support for our ideas when testing them on panel data comprising 902 German Bundesliga soccer press conferences and media tenor, applying state-of-the-art machine learning approaches for laughter dete
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;&#25239;&#20307;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#65292;&#35299;&#20915;&#20960;&#20309;&#24314;&#27169;&#21644;&#20302;&#25928;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09480</link><description>&lt;p&gt;
&#20132;&#21449;&#38376;&#25511;&#22810;&#23618;&#24863;&#30693;&#26426;&#19979;&#30340;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#19981;&#21464;&#23884;&#20837;&#26159;&#19968;&#31181;&#19968;&#27425;&#24615;&#25239;&#20307;&#35774;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Protein Complex Invariant Embedding with Cross-Gate MLP is A One-Shot Antibody Designer. (arXiv:2305.09480v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;&#25239;&#20307;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#65292;&#35299;&#20915;&#20960;&#20309;&#24314;&#27169;&#21644;&#20302;&#25928;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#20307;&#26159;&#30001;&#20813;&#30123;&#31995;&#32479;&#20135;&#29983;&#30340;&#38024;&#23545;&#22806;&#26469;&#29289;&#36136;&#25110;&#25239;&#21407;&#30340;&#37325;&#35201;&#34507;&#30333;&#36136;&#12290;&#25239;&#20307;&#30340;&#29305;&#24322;&#24615;&#30001;&#20854;&#20114;&#34917;&#20915;&#23450;&#21306;&#65288;CDR&#65289;&#20915;&#23450;&#65292;CDR&#20301;&#20110;&#25239;&#20307;&#38142;&#30340;&#21487;&#21464;&#21306;&#22495;&#20013;&#65292;&#24418;&#25104;&#19982;&#25239;&#21407;&#32467;&#21512;&#30340;&#20301;&#28857;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#21033;&#29992;&#22797;&#26434;&#30340;&#25216;&#26415;&#29983;&#25104;CDR&#65292;&#20294;&#23427;&#20204;&#36973;&#21463;&#20102;&#20960;&#20309;&#24314;&#27169;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#24120;&#35265;&#30340;&#36845;&#20195;&#31934;&#21270;&#31574;&#30053;&#23548;&#33268;&#20102;&#20302;&#25928;&#30340;&#25512;&#26029;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#25239;&#20307;CDR&#35774;&#35745;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;i&#65289;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20960;&#20309;&#24314;&#27169;&#21644;&#65288;ii&#65289;&#24207;&#21015;&#32467;&#26500;&#20849;&#23398;&#20064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#19981;&#21464;&#23884;&#20837;&#65292;&#21487;&#25429;&#25417;&#34507;&#30333;&#36136;&#39592;&#26550;&#21407;&#23376;&#65288;&#21253;&#25324;C&#945;&#12289;N&#12289;C&#21644;O&#21407;&#23376;&#65289;&#20043;&#38388;&#30340;&#20869;&#37096;&#21644;&#22806;&#37096;&#32452;&#20998;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#23454;&#29616;&#20840;&#38754;&#30340;&#20960;&#20309;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Antibodies are crucial proteins produced by the immune system in response to foreign substances or antigens. The specificity of an antibody is determined by its complementarity-determining regions (CDRs), which are located in the variable domains of the antibody chains and form the antigen-binding site. Previous studies have utilized complex techniques to generate CDRs, but they suffer from inadequate geometric modeling. Moreover, the common iterative refinement strategies lead to an inefficient inference. In this paper, we propose a deep generative model that can co-design 1D sequences and 3D structures of CDRs in a one-shot manner. To achieve this, we decouple the antibody CDR design into two stages: (i) geometric modeling of protein structures and (ii) sequence-structure co-learning. We develop a protein complex invariant embedding that captures both intra- and inter-component interactions among the backbone atoms including C$\alpha$, N, C, and O atoms to achieve comprehensive geome
&lt;/p&gt;</description></item><item><title>BIMT&#26041;&#27861;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#26356;&#21152;&#27169;&#22359;&#21270;&#21644;&#21487;&#35808;&#37322;&#65292;&#24182;&#19988;&#33021;&#22815;&#30452;&#25509;&#23637;&#31034;&#27169;&#22359;&#21270;&#32467;&#26500;&#65292;&#20026;&#35768;&#22810;&#31616;&#21333;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#24182;&#21487;&#20197;&#34917;&#20805;&#24403;&#21069;&#30340;&#26426;&#29702;&#35299;&#37322;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.08746</link><description>&lt;p&gt;
&#35265;&#35777;&#23601;&#26159;&#20449;&#20208;&#65306;&#33041;&#21551;&#21457;&#27169;&#22359;&#21270;&#35757;&#32451;&#20419;&#36827;&#26426;&#29702;&#35808;&#37322;
&lt;/p&gt;
&lt;p&gt;
Seeing is Believing: Brain-Inspired Modular Training for Mechanistic Interpretability. (arXiv:2305.08746v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08746
&lt;/p&gt;
&lt;p&gt;
BIMT&#26041;&#27861;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#26356;&#21152;&#27169;&#22359;&#21270;&#21644;&#21487;&#35808;&#37322;&#65292;&#24182;&#19988;&#33021;&#22815;&#30452;&#25509;&#23637;&#31034;&#27169;&#22359;&#21270;&#32467;&#26500;&#65292;&#20026;&#35768;&#22810;&#31616;&#21333;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#24182;&#21487;&#20197;&#34917;&#20805;&#24403;&#21069;&#30340;&#26426;&#29702;&#35299;&#37322;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33041;&#21551;&#21457;&#27169;&#22359;&#21270;&#35757;&#32451;&#65288;Brain-Inspired Modular Training, BIMT&#65289;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20351;&#31070;&#32463;&#32593;&#32476;&#26356;&#21152;&#27169;&#22359;&#21270;&#21644;&#21487;&#35808;&#37322;&#12290;BIMT&#20174;&#22823;&#33041;&#21463;&#21551;&#21457;&#65292;&#23558;&#31070;&#32463;&#20803;&#23884;&#20837;&#21040;&#20960;&#20309;&#31354;&#38388;&#20013;&#65292;&#24182;&#36890;&#36807;&#25104;&#26412;&#19982;&#31070;&#32463;&#20803;&#36830;&#25509;&#38271;&#24230;&#25104;&#27491;&#27604;&#30340;&#26041;&#24335;&#22686;&#24378;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;BIMT&#21487;&#20197;&#20026;&#35768;&#22810;&#31616;&#21333;&#20219;&#21153;&#21457;&#29616;&#26377;&#29992;&#30340;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25581;&#31034;&#20102;&#31526;&#21495;&#20844;&#24335;&#20013;&#30340;&#32452;&#21512;&#32467;&#26500;&#12289;&#21487;&#35299;&#37322;&#30340;&#20915;&#31574;&#36793;&#30028;&#21644;&#20998;&#31867;&#29305;&#24449;&#65292;&#20197;&#21450;&#31639;&#27861;&#25968;&#25454;&#38598;&#20013;&#30340;&#25968;&#23398;&#32467;&#26500;&#12290;&#30452;&#25509;&#30524;&#30555;&#30475;&#21040;&#27169;&#22359;&#30340;&#33021;&#21147;&#21487;&#20197;&#34917;&#20805;&#24403;&#21069;&#30340;&#26426;&#29702;&#35299;&#37322;&#31574;&#30053;&#65292;&#20363;&#22914;&#25506;&#38024;&#65292;&#24178;&#39044;&#25110;&#20957;&#35270;&#25152;&#26377;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Brain-Inspired Modular Training (BIMT), a method for making neural networks more modular and interpretable. Inspired by brains, BIMT embeds neurons in a geometric space and augments the loss function with a cost proportional to the length of each neuron connection. We demonstrate that BIMT discovers useful modular neural networks for many simple tasks, revealing compositional structures in symbolic formulas, interpretable decision boundaries and features for classification, and mathematical structure in algorithmic datasets. The ability to directly see modules with the naked eye can complement current mechanistic interpretability strategies such as probes, interventions or staring at all weights.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#32852;&#21512;&#23398;&#20064;&#36827;&#34892;&#20986;&#31199;&#36710;&#38656;&#27714;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#22810;&#20010;&#21442;&#19982;&#26041;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24182;&#20445;&#25345;&#25968;&#25454;&#31169;&#23494;&#21644;&#23433;&#20840;&#12290;&#25991;&#31456;&#23545;&#20110;&#31867;&#21035;&#19981;&#24179;&#34913;&#12289;&#25968;&#25454;&#31232;&#32570;&#21644;&#27169;&#22411;&#27867;&#21270;&#31561;&#25216;&#26415;&#25361;&#25112;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#26368;&#32456;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.08107</link><description>&lt;p&gt;
&#24179;&#34913;&#20986;&#31199;&#36710;&#26102;&#31354;&#25968;&#25454;&#30340;&#38544;&#31169;&#21644;&#25928;&#29992;&#29992;&#20110;&#38656;&#27714;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Balancing Privacy and Utility of Spatio-Temporal Data for Taxi-Demand Prediction. (arXiv:2305.08107v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#32852;&#21512;&#23398;&#20064;&#36827;&#34892;&#20986;&#31199;&#36710;&#38656;&#27714;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#22810;&#20010;&#21442;&#19982;&#26041;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24182;&#20445;&#25345;&#25968;&#25454;&#31169;&#23494;&#21644;&#23433;&#20840;&#12290;&#25991;&#31456;&#23545;&#20110;&#31867;&#21035;&#19981;&#24179;&#34913;&#12289;&#25968;&#25454;&#31232;&#32570;&#21644;&#27169;&#22411;&#27867;&#21270;&#31561;&#25216;&#26415;&#25361;&#25112;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#26368;&#32456;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20986;&#31199;&#36710;&#38656;&#27714;&#39044;&#27979;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#35201;&#24212;&#29992;&#65292;&#23427;&#21487;&#20197;&#20351;&#20986;&#31199;&#36710;&#25552;&#20379;&#35774;&#26045;&#20248;&#21270;&#20854;&#36816;&#33829;&#65292;&#22478;&#24066;&#35268;&#21010;&#32773;&#25913;&#21892;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#21644;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#20013;&#20351;&#29992;&#25935;&#24863;&#25968;&#25454;&#24341;&#21457;&#20102;&#38544;&#31169;&#21644;&#23433;&#20840;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#32852;&#21512;&#23398;&#20064;&#36827;&#34892;&#20986;&#31199;&#36710;&#38656;&#27714;&#39044;&#27979;&#65292;&#20801;&#35768;&#22810;&#20010;&#21442;&#19982;&#26041;&#22312;&#20445;&#25345;&#25968;&#25454;&#31169;&#23494;&#21644;&#23433;&#20840;&#30340;&#21516;&#26102;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#21487;&#20197;&#20351;&#32452;&#32455;&#22312;&#19981;&#24471;&#20197;&#33719;&#21462;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#27169;&#22411;&#12290;&#23613;&#31649;&#32852;&#21512;&#23398;&#20064;&#20026;&#20986;&#31199;&#36710;&#38656;&#27714;&#39044;&#27979;&#24102;&#26469;&#20102;&#28508;&#22312;&#30340;&#30410;&#22788;&#65292;&#20294;&#23427;&#20063;&#38754;&#20020;&#30528;&#19968;&#20123;&#25216;&#26415;&#25361;&#25112;&#65292;&#20363;&#22914;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#26576;&#20123;&#21442;&#19982;&#26041;&#30340;&#25968;&#25454;&#31232;&#32570;&#20197;&#21450;&#38656;&#35201;&#30830;&#20445;&#27169;&#22411;&#27867;&#21270;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#35774;&#26045;&#21644;&#22320;&#29702;&#21306;&#22495;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21033;&#29992;&#21306;&#22495;&#26080;&#20851;&#30340;&#32534;&#30721;&#36827;&#34892;&#22320;&#29702;&#29305;&#24449;&#22788;&#29702;&#65292;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#26469;&#24179;&#34913;&#25968;&#25454;&#31232;&#32570;&#65292;&#20197;&#21450;&#25968;&#25454;&#22686;&#24378;&#26469;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#30340;&#26102;&#31354;&#20986;&#31199;&#36710;&#38656;&#27714;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#20445;&#25252;&#38544;&#31169;&#21644;&#20445;&#25345;&#25968;&#25454;&#25928;&#29992;&#26041;&#38754;&#36798;&#21040;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Taxi-demand prediction is an important application of machine learning that enables taxi-providing facilities to optimize their operations and city planners to improve transportation infrastructure and services. However, the use of sensitive data in these systems raises concerns about privacy and security. In this paper, we propose the use of federated learning for taxi-demand prediction that allows multiple parties to train a machine learning model on their own data while keeping the data private and secure. This can enable organizations to build models on data they otherwise would not be able to access. Despite its potential benefits, federated learning for taxi-demand prediction poses several technical challenges, such as class imbalance, data scarcity among some parties, and the need to ensure model generalization to accommodate diverse facilities and geographic regions. To effectively address these challenges, we propose a system that utilizes region-independent encoding for geogr
&lt;/p&gt;</description></item><item><title>CodeT5+&#26159;&#19968;&#32452;&#28789;&#27963;&#32452;&#21512;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;LLM&#26063;&#65292;&#29992;&#20110;&#20195;&#30721;&#65292;&#28151;&#21512;&#20102;&#22810;&#31181;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#21253;&#25324;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#31243;&#24207;&#21512;&#25104;&#65292;&#21487;&#20197;&#36866;&#24212;&#22810;&#31181;&#19981;&#21516;&#30340;&#19979;&#28216;&#20195;&#30721;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#27604;&#29616;&#26377;&#20195;&#30721;-specific LLMs&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.07922</link><description>&lt;p&gt;
CodeT5+: &#29992;&#20110;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#24320;&#25918;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeT5+: Open Code Large Language Models for Code Understanding and Generation. (arXiv:2305.07922v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07922
&lt;/p&gt;
&lt;p&gt;
CodeT5+&#26159;&#19968;&#32452;&#28789;&#27963;&#32452;&#21512;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;LLM&#26063;&#65292;&#29992;&#20110;&#20195;&#30721;&#65292;&#28151;&#21512;&#20102;&#22810;&#31181;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#21253;&#25324;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#31243;&#24207;&#21512;&#25104;&#65292;&#21487;&#20197;&#36866;&#24212;&#22810;&#31181;&#19981;&#21516;&#30340;&#19979;&#28216;&#20195;&#30721;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#27604;&#29616;&#26377;&#20195;&#30721;-specific LLMs&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22312;&#22823;&#37327;&#28304;&#20195;&#30721;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20195;&#30721;&#26234;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20195;&#30721;LLM&#22312;&#26550;&#26500;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#26041;&#38754;&#26377;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#36890;&#24120;&#37319;&#29992;&#29305;&#23450;&#30340;&#26550;&#26500;(&#20165;&#32534;&#30721;&#22120;&#25110;&#20165;&#35299;&#30721;&#22120;)&#25110;&#20381;&#36182;&#20110;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#30340;&#32479;&#19968;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#12290;&#21069;&#19968;&#31181;&#33539;&#24335;&#21463;&#21040;&#24212;&#29992;&#28789;&#27963;&#24615;&#30340;&#38480;&#21046;&#65292;&#32780;&#22312;&#21518;&#19968;&#31181;&#33539;&#24335;&#20013;&#65292;&#27169;&#22411;&#34987;&#35270;&#20026;&#25152;&#26377;&#20219;&#21153;&#30340;&#21333;&#19968;&#31995;&#32479;&#65292;&#23548;&#33268;&#22312;&#26576;&#20123;&#20219;&#21153;&#30340;&#23376;&#38598;&#19978;&#24615;&#33021;&#19981;&#20248;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#36890;&#24120;&#37319;&#29992;&#26377;&#38480;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#36825;&#20123;&#30446;&#26631;&#21487;&#33021;&#19982;&#26576;&#20123;&#19979;&#28216;&#20219;&#21153;&#19981;&#30456;&#20851;&#65292;&#22240;&#27492;&#20250;&#23548;&#33268;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;CodeT5+&#8221;&#65292;&#36825;&#26159;&#19968;&#32452;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;LLM&#26063;&#65292;&#29992;&#20110;&#20195;&#30721;&#65292;&#20854;&#20013;&#32452;&#20214;&#27169;&#22359;&#21487;&#20197;&#28789;&#27963;&#32452;&#21512;&#20197;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#20195;&#30721;&#20219;&#21153;&#12290;&#36825;&#31181;&#28789;&#27963;&#24615;&#26159;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#28151;&#21512;&#39044;&#35757;&#32451;&#30446;&#26631;&#23454;&#29616;&#30340;&#65292;&#21253;&#25324;&#20195;&#30721;&#29983;&#25104;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#31243;&#24207;&#21512;&#25104;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#35777;&#26126;CodeT5+&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#20195;&#30721;&#29305;&#23450;LLM&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading to suboptimal performance on a subset of tasks. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some downstream tasks and hence result in substantial performance degrade. To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks. Such flexibility is enabled by our proposed mixture of pretrai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#24179;&#22343;&#25216;&#26415;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#25439;&#22833;/&#38169;&#35823;&#26223;&#35266;&#25581;&#31034;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#29615;&#32469;&#20840;&#23616;&#27169;&#22411;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#30406;&#22320;&#20869;&#65292;&#24182;&#19988;&#21457;&#29616;&#20840;&#23616;&#27169;&#22411;&#22312;&#26089;&#26399;&#35757;&#32451;&#21518;&#30340;&#35823;&#24046;&#20027;&#35201;&#26469;&#33258;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#20043;&#38388;&#38750;&#37325;&#21472;&#30340;&#25968;&#25454;&#21450;&#20840;&#23616;&#27169;&#22411;&#19982;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#20004;&#20010;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.07845</link><description>&lt;p&gt;
&#29702;&#35299;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#24179;&#22343;
&lt;/p&gt;
&lt;p&gt;
Understanding Model Averaging in Federated Learning on Heterogeneous Data. (arXiv:2305.07845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#24179;&#22343;&#25216;&#26415;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#25439;&#22833;/&#38169;&#35823;&#26223;&#35266;&#25581;&#31034;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#29615;&#32469;&#20840;&#23616;&#27169;&#22411;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#30406;&#22320;&#20869;&#65292;&#24182;&#19988;&#21457;&#29616;&#20840;&#23616;&#27169;&#22411;&#22312;&#26089;&#26399;&#35757;&#32451;&#21518;&#30340;&#35823;&#24046;&#20027;&#35201;&#26469;&#33258;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#20043;&#38388;&#38750;&#37325;&#21472;&#30340;&#25968;&#25454;&#21450;&#20840;&#23616;&#27169;&#22411;&#19982;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#20004;&#20010;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#24179;&#22343;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#24191;&#27867;&#37319;&#29992;&#30340;&#19968;&#31181;&#25216;&#26415;&#65292;&#23427;&#20250;&#32858;&#38598;&#35757;&#32451;&#20110;&#24322;&#26500;&#25968;&#25454;&#19978;&#30340;&#22810;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#20197;&#33719;&#24471;&#34920;&#29616;&#33391;&#22909;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20854;&#25104;&#21151;&#32972;&#21518;&#30340;&#21407;&#29702;&#23578;&#19981;&#26159;&#24456;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#21487;&#35270;&#21270;&#25439;&#22833;/&#38169;&#35823;&#26223;&#35266;&#26469;&#30740;&#31350;&#27169;&#22411;&#24179;&#22343;&#30340;&#20960;&#20309;&#29305;&#24615;&#65292;&#25581;&#31034;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#29615;&#32469;&#20840;&#23616;&#27169;&#22411;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#30406;&#22320;&#20869;&#65292;&#24182;&#19988;&#21363;&#20351;&#20840;&#23616;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;&#20063;&#21487;&#33021;&#20559;&#31163;&#30406;&#22320;&#24213;&#37096;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20840;&#23616;&#27169;&#22411;&#22312;&#26089;&#26399;&#35757;&#32451;&#21518;&#30340;&#35823;&#24046;&#20027;&#35201;&#26469;&#33258;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#20043;&#38388;&#38750;&#37325;&#21472;&#30340;&#25968;&#25454;&#21450;&#20840;&#23616;&#27169;&#22411;&#19982;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#20004;&#20010;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model averaging, a widely adopted technique in federated learning (FL), aggregates multiple client models trained on heterogeneous data to obtain a well-performed global model. However, the rationale behind its success is not well understood. To shed light on this issue, we investigate the geometric properties of model averaging by visualizing the loss/error landscape. The geometrical visualization shows that the client models surround the global model within a common basin, and the global model may deviate from the bottom of the basin even though it performs better than the client models. To further understand this phenomenon, we decompose the expected prediction error of the global model into five factors related to client models. Specifically, we find that the global-model error after early training mainly comes from i) the client-model error on non-overlapping data between client datasets and the global dataset and ii) the maximal distance between the global and client models. Insp
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20998;&#31163;&#38543;&#26426;&#36924;&#36817;&#26694;&#26550;&#65292;&#20351;&#29992;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20998;&#21035;&#26356;&#26032;&#27169;&#22411;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#21442;&#25968;&#12290;&#27492;&#31639;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#39640;&#25928;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07484</link><description>&lt;p&gt;
&#20998;&#31163;&#38543;&#26426;&#36924;&#36817;&#26694;&#26550;&#19979;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Learning Under A Separable Stochastic Approximation Framework. (arXiv:2305.07484v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20998;&#31163;&#38543;&#26426;&#36924;&#36817;&#26694;&#26550;&#65292;&#20351;&#29992;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20998;&#21035;&#26356;&#26032;&#27169;&#22411;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#21442;&#25968;&#12290;&#27492;&#31639;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#39640;&#25928;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#31163;&#38543;&#26426;&#36924;&#36817;&#26694;&#26550;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#19968;&#31867;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#30340;&#37325;&#28857;&#22312;&#20110;&#35266;&#23519;&#27169;&#22411;&#20013;&#26576;&#20123;&#21442;&#25968;&#27604;&#20854;&#20182;&#21442;&#25968;&#26356;&#23481;&#26131;&#20248;&#21270;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#19968;&#31867;&#32447;&#24615;&#21442;&#25968;&#36739;&#22810;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#29992;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#65288;RLS&#65289;&#31639;&#27861;&#26469;&#26356;&#26032;&#32447;&#24615;&#21442;&#25968;&#65292;&#28982;&#21518;&#22522;&#20110;&#26356;&#26032;&#21518;&#30340;&#32447;&#24615;&#21442;&#25968;&#65292;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31639;&#27861;&#26469;&#26356;&#26032;&#38750;&#32447;&#24615;&#21442;&#25968;&#12290;&#36825;&#20010;&#31639;&#27861;&#21487;&#20197;&#29702;&#35299;&#20026;&#22359;&#22352;&#26631;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30340;&#38543;&#26426;&#36924;&#36817;&#29256;&#26412;&#65292;&#22312;&#36825;&#20010;&#29256;&#26412;&#20013;&#65292;&#20854;&#20013;&#19968;&#37096;&#20998;&#21442;&#25968;&#20351;&#29992;&#20108;&#38454;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26356;&#26032;&#65292;&#32780;&#21478;&#19968;&#37096;&#20998;&#21442;&#25968;&#20351;&#29992;&#19968;&#38454;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#12290;&#34429;&#28982;&#35813;&#31639;&#27861;&#23545;&#20110;&#38750;&#20984;&#38382;&#39064;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#27809;&#26377;&#35752;&#35770;&#65292;&#20294;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an online learning algorithm for a class of machine learning models under a separable stochastic approximation framework. The essence of our idea lies in the observation that certain parameters in the models are easier to optimize than others. In this paper, we focus on models where some parameters have a linear nature, which is common in machine learning. In one routine of the proposed algorithm, the linear parameters are updated by the recursive least squares (RLS) algorithm, which is equivalent to a stochastic Newton method; then, based on the updated linear parameters, the nonlinear parameters are updated by the stochastic gradient method (SGD). The proposed algorithm can be understood as a stochastic approximation version of block coordinate gradient descent approach in which one part of the parameters is updated by a second-order SGD method while the other part is updated by a first-order SGD. Global convergence of the proposed online algorithm for non-convex cases is 
&lt;/p&gt;</description></item><item><title>MEGABYTE&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#23610;&#24230;Transformer&#30340;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#33021;&#22815;&#23545;&#36229;&#36807;&#19968;&#30334;&#19975;&#23383;&#33410;&#30340;&#24207;&#21015;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#21487;&#24494;&#24314;&#27169;&#65292;&#22312;&#35757;&#32451;&#21644;&#29983;&#25104;&#36807;&#31243;&#20013;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#38477;&#20302;&#20102;&#25104;&#26412;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#19978;&#19979;&#25991;&#26080;&#38656;&#26631;&#35760;&#30340;&#33258;&#22238;&#24402;&#24207;&#21015;&#24314;&#27169;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07185</link><description>&lt;p&gt;
MEGABYTE: &#22522;&#20110;&#22810;&#23610;&#24230;Transformer&#30340;&#30334;&#19975;&#23383;&#33410;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers. (arXiv:2305.07185v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07185
&lt;/p&gt;
&lt;p&gt;
MEGABYTE&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#23610;&#24230;Transformer&#30340;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#33021;&#22815;&#23545;&#36229;&#36807;&#19968;&#30334;&#19975;&#23383;&#33410;&#30340;&#24207;&#21015;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#21487;&#24494;&#24314;&#27169;&#65292;&#22312;&#35757;&#32451;&#21644;&#29983;&#25104;&#36807;&#31243;&#20013;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#38477;&#20302;&#20102;&#25104;&#26412;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#19978;&#19979;&#25991;&#26080;&#38656;&#26631;&#35760;&#30340;&#33258;&#22238;&#24402;&#24207;&#21015;&#24314;&#27169;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;transformer&#27169;&#22411;&#22312;&#30701;&#24207;&#21015;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23545;&#20110;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#12289;&#25773;&#23458;&#12289;&#20195;&#30721;&#25110;&#22270;&#20070;&#31561;&#38271;&#24207;&#21015;&#30340;&#22788;&#29702;&#33021;&#21147;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Megabyte&#65292;&#19968;&#31181;&#22810;&#23610;&#24230;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#33021;&#22815;&#23545;&#36229;&#36807;&#19968;&#30334;&#19975;&#23383;&#33410;&#30340;&#24207;&#21015;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#21487;&#24494;&#24314;&#27169;&#12290;Megabyte&#23558;&#24207;&#21015;&#20998;&#20026;&#22270;&#22359;&#65292;&#24182;&#22312;&#22270;&#22359;&#20869;&#20351;&#29992;&#23616;&#37096;&#23376;&#27169;&#22411;&#65292;&#22312;&#22270;&#22359;&#20043;&#38388;&#20351;&#29992;&#20840;&#23616;&#27169;&#22411;&#12290;&#36825;&#20351;&#24471;&#23376;&#20108;&#27425;&#33258;&#27880;&#24847;&#12289;&#26356;&#22823;&#30340;&#21069;&#39304;&#23618;&#21644;&#26356;&#22909;&#30340;&#35299;&#30721;&#24182;&#34892;&#24615;&#24471;&#20197;&#23454;&#29616;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#21644;&#29983;&#25104;&#36807;&#31243;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#25104;&#26412;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Megabyte&#21487;&#20197;&#20351;&#22522;&#20110;&#23383;&#33410;&#30340;&#27169;&#22411;&#22312;&#38271;&#19978;&#19979;&#25991;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#19982;&#22522;&#20110;&#23376;&#35789;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#65292;&#22312;ImageNet&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#23494;&#24230;&#20272;&#35745;&#65292;&#21487;&#20197;&#27169;&#25311;&#26469;&#33258;&#21407;&#22987;&#25991;&#20214;&#30340;&#38899;&#39057;&#12290;&#36825;&#20123;&#32467;&#26524;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#19978;&#19979;&#25991;&#26080;&#38656;&#26631;&#35760;&#30340;&#33258;&#22238;&#24402;&#24207;&#21015;&#24314;&#27169;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive transformers are spectacular models for short sequences but scale poorly to long sequences such as high-resolution images, podcasts, code, or books. We proposed Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes. Megabyte segments sequences into patches and uses a local submodel within patches and a global model between patches. This enables sub-quadratic self-attention, much larger feedforward layers for the same compute, and improved parallelism during decoding -- unlocking better performance at reduced cost for both training and generation. Extensive experiments show that Megabyte allows byte-level models to perform competitively with subword models on long context language modeling, achieve state-of-the-art density estimation on ImageNet, and model audio from raw files. Together, these results establish the viability of tokenization-free autoregressive sequence modeling at scale.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#40784;&#30340;&#35270;&#35273;-&#36951;&#20256;&#25512;&#29702;&#31354;&#38388;&#26469;&#25552;&#39640;&#23569;&#37327;&#22270;&#20687;&#25968;&#25454;&#29645;&#31232;&#29289;&#31181;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#23884;&#20837;&#27169;&#22411;&#23454;&#29616;&#23545;&#40784;&#65292;&#36866;&#29992;&#20110;&#25552;&#39640;&#31232;&#26377;&#29289;&#31181;&#30340;&#38271;&#23614;&#35782;&#21035;&#65292;&#24182;&#19988;&#21487;&#20197;&#26174;&#33879;&#26377;&#30410;&#20110;&#20165;&#22522;&#20110;&#35270;&#35273;&#30340;&#31232;&#26377;&#29289;&#31181;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2305.06695</link><description>&lt;p&gt;
&#28145;&#24230;&#35270;&#35273;&#21644;&#36951;&#20256;&#29983;&#29289;&#27979;&#23450;&#29992;&#20110;&#23569;&#37327;&#22270;&#20687;&#25968;&#25454;&#29645;&#31232;&#29289;&#31181;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Visual-Genetic Biometrics for Taxonomic Classification of Rare Species. (arXiv:2305.06695v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#40784;&#30340;&#35270;&#35273;-&#36951;&#20256;&#25512;&#29702;&#31354;&#38388;&#26469;&#25552;&#39640;&#23569;&#37327;&#22270;&#20687;&#25968;&#25454;&#29645;&#31232;&#29289;&#31181;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#23884;&#20837;&#27169;&#22411;&#23454;&#29616;&#23545;&#40784;&#65292;&#36866;&#29992;&#20110;&#25552;&#39640;&#31232;&#26377;&#29289;&#31181;&#30340;&#38271;&#23614;&#35782;&#21035;&#65292;&#24182;&#19988;&#21487;&#20197;&#26174;&#33879;&#26377;&#30410;&#20110;&#20165;&#22522;&#20110;&#35270;&#35273;&#30340;&#31232;&#26377;&#29289;&#31181;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#24212;&#29992;&#20013;&#65292;&#35270;&#35273;&#21644;&#36951;&#20256;&#29983;&#29289;&#27979;&#23450;&#36890;&#24120;&#29992;&#20110;&#35782;&#21035;&#29289;&#31181;&#21644;&#20010;&#20307;&#12290;&#28982;&#32780;&#65292;&#22312;&#35745;&#31639;&#19978;&#22686;&#24378;&#23569;&#37327;&#22270;&#20687;&#25968;&#25454;&#31232;&#26377;&#31867;&#21035;&#30340;&#35270;&#35273;&#20998;&#31867;&#26041;&#38754;&#65292;&#35813;&#39046;&#22495;&#23578;&#26410;&#36827;&#34892;&#23581;&#35797;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#40784;&#30340;&#35270;&#35273;-&#36951;&#20256;&#25512;&#29702;&#31354;&#38388;&#65292;&#26088;&#22312;&#38544;&#24335;&#32534;&#30721;&#36328;&#22495;&#20851;&#32852;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#36825;&#31181;&#23545;&#40784;&#21487;&#20197;&#36890;&#36807;&#28145;&#24230;&#23884;&#20837;&#27169;&#22411;&#23454;&#29616;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#30452;&#25509;&#36866;&#29992;&#20110;&#25552;&#39640;&#31232;&#26377;&#29289;&#31181;&#30340;&#38271;&#23614;&#35782;&#21035;&#65288;LTR&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#20110;32&#20010;&#29289;&#31181;&#12289;&#36229;&#36807;30,000&#20010;&#28014;&#28216;&#26377;&#23380;&#34411;&#22771;&#30340;&#26174;&#24494;&#22270;&#20687;&#24182;&#19982;&#29420;&#31435;&#30340;&#36951;&#20256;&#25968;&#25454;&#26679;&#26412;&#19968;&#36215;&#20351;&#29992;&#26469;&#23454;&#39564;&#23460;&#23637;&#29616;&#20102;&#35813;&#27010;&#24565;&#30340;&#25928;&#21147;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#23545;&#20174;&#19994;&#32773;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35270;&#35273;-&#36951;&#20256;&#23545;&#40784;&#21487;&#20197;&#26174;&#33879;&#26377;&#30410;&#20110;&#20165;&#22522;&#20110;&#35270;&#35273;&#30340;&#31232;&#26377;&#29289;&#31181;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual as well as genetic biometrics are routinely employed to identify species and individuals in biological applications. However, no attempts have been made in this domain to computationally enhance visual classification of rare classes with little image data via genetics. In this paper, we thus propose aligned visual-genetic inference spaces with the aim to implicitly encode cross-domain associations for improved performance. We demonstrate for the first time that such alignment can be achieved via deep embedding models and that the approach is directly applicable to boosting long-tailed recognition (LTR) particularly for rare species. We experimentally demonstrate the efficacy of the concept via application to microscopic imagery of 30k+ planktic foraminifer shells across 32 species when used together with independent genetic data samples. Most importantly for practitioners, we show that visual-genetic alignment can significantly benefit visual-only recognition of the rarest speci
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20132;&#36890;&#25968;&#25454;&#25554;&#20540;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#21644;&#21452;&#21521;&#31070;&#32463;&#32593;&#32476;&#25429;&#25417;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#22788;&#29702;&#32570;&#22833;&#20540;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.06480</link><description>&lt;p&gt;
ST-GIN:&#19968;&#31181;&#20855;&#26377;&#26102;&#31354;&#22270;&#27880;&#24847;&#21147;&#21644;&#21452;&#21521;&#24490;&#29615;&#32852;&#21512;&#31070;&#32463;&#32593;&#32476;&#30340;&#20132;&#36890;&#25968;&#25454;&#25554;&#20540;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ST-GIN: An Uncertainty Quantification Approach in Traffic Data Imputation with Spatio-temporal Graph Attention and Bidirectional Recurrent United Neural Networks. (arXiv:2305.06480v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20132;&#36890;&#25968;&#25454;&#25554;&#20540;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#21644;&#21452;&#21521;&#31070;&#32463;&#32593;&#32476;&#25429;&#25417;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#22788;&#29702;&#32570;&#22833;&#20540;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#25968;&#25454;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20174;&#29615;&#36335;&#26816;&#27979;&#22120;&#25110;&#31867;&#20284;&#26469;&#28304;&#25910;&#38598;&#30340;&#29616;&#23454;&#19990;&#30028;&#20132;&#36890;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#32570;&#22833;&#20540;(MVs)&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#30456;&#20851;&#24212;&#29992;&#21644;&#30740;&#31350;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#20026;&#20102;&#24674;&#22797;&#36825;&#20123;&#32570;&#22833;&#20540;&#65292;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#25968;&#23383;&#32479;&#35745;&#12289;&#24352;&#37327;&#20998;&#35299;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#31561;&#26041;&#27861;&#23454;&#29616;&#20102;&#25968;&#25454;&#25554;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#25554;&#20540;&#32570;&#22833;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#22270;&#27880;&#24847;&#26550;&#26500;&#26469;&#25429;&#25417;&#20132;&#36890;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#21033;&#29992;&#21452;&#21521;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26102;&#38388;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#22522;&#20934;&#25216;&#26415;&#65292;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic data serves as a fundamental component in both research and applications within intelligent transportation systems. However, real-world transportation data, collected from loop detectors or similar sources, often contain missing values (MVs), which can adversely impact associated applications and research. Instead of discarding this incomplete data, researchers have sought to recover these missing values through numerical statistics, tensor decomposition, and deep learning techniques. In this paper, we propose an innovative deep-learning approach for imputing missing data. A graph attention architecture is employed to capture the spatial correlations present in traffic data, while a bidirectional neural network is utilized to learn temporal information. Experimental results indicate that our proposed method outperforms all other benchmark techniques, thus demonstrating its effectiveness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#24322;&#27493;&#32858;&#21512;FEEL&#26426;&#21046;PAOTA&#65292;&#20197;&#25913;&#21892;&#25968;&#25454;&#21644;&#35774;&#22791;&#23384;&#22312;&#26174;&#33879;&#24322;&#36136;&#24615;&#30340;&#24773;&#20917;&#19979;FEEL&#31995;&#32479;&#30340;&#35757;&#32451;&#25928;&#29575;&#65292;&#36890;&#36807;&#35843;&#25972;&#36793;&#32536;&#35774;&#22791;&#30340;&#19978;&#34892;&#20256;&#36755;&#21151;&#29575;&#26469;&#26368;&#23567;&#21270;FEEL&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#19978;&#30028;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26426;&#21046;&#22312;&#36798;&#21040;&#30456;&#21516;&#30340;&#30446;&#26631;&#31934;&#24230;&#19979;&#65292;&#35757;&#32451;&#36895;&#24230;&#26174;&#33879;&#24555;&#20110;&#20855;&#26377;&#31354;&#20013;&#35745;&#31639;&#26041;&#26696;&#30340;&#20256;&#32479;&#21516;&#27493;FEEL&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.04066</link><description>&lt;p&gt;
&#22522;&#20110;&#31354;&#20013;&#35745;&#31639;&#30340;&#21322;&#24322;&#27493;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Semi-Asynchronous Federated Edge Learning Mechanism via Over-the-air Computation. (arXiv:2305.04066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#24322;&#27493;&#32858;&#21512;FEEL&#26426;&#21046;PAOTA&#65292;&#20197;&#25913;&#21892;&#25968;&#25454;&#21644;&#35774;&#22791;&#23384;&#22312;&#26174;&#33879;&#24322;&#36136;&#24615;&#30340;&#24773;&#20917;&#19979;FEEL&#31995;&#32479;&#30340;&#35757;&#32451;&#25928;&#29575;&#65292;&#36890;&#36807;&#35843;&#25972;&#36793;&#32536;&#35774;&#22791;&#30340;&#19978;&#34892;&#20256;&#36755;&#21151;&#29575;&#26469;&#26368;&#23567;&#21270;FEEL&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#19978;&#30028;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26426;&#21046;&#22312;&#36798;&#21040;&#30456;&#21516;&#30340;&#30446;&#26631;&#31934;&#24230;&#19979;&#65292;&#35757;&#32451;&#36895;&#24230;&#26174;&#33879;&#24555;&#20110;&#20855;&#26377;&#31354;&#20013;&#35745;&#31639;&#26041;&#26696;&#30340;&#20256;&#32479;&#21516;&#27493;FEEL&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#20013;&#35745;&#31639;&#26159;&#25552;&#39640;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#65288;FEEL&#65289;&#25928;&#29575;&#30340;&#26377;&#25928;&#20256;&#36755;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20855;&#26377;&#31354;&#20013;&#35745;&#31639;&#26041;&#26696;&#30340;FEEL&#31995;&#32479;&#36890;&#24120;&#22312;&#27599;&#20010;&#20840;&#23616;&#36718;&#27425;&#20013;&#37319;&#29992;&#20256;&#32479;&#30340;&#21516;&#27493;&#32858;&#21512;&#26426;&#21046;&#65292;&#32780;&#36825;&#20123;&#26426;&#21046;&#23481;&#26131;&#21463;&#21040;&#28382;&#21518;&#32773;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31354;&#20013;&#35745;&#31639;&#26041;&#26696;&#30340;&#21322;&#24322;&#27493;&#32858;&#21512;FEEL&#26426;&#21046;&#65288;PAOTA&#65289;&#65292;&#20197;&#25913;&#21892;&#25968;&#25454;&#21644;&#35774;&#22791;&#23384;&#22312;&#26174;&#33879;&#24322;&#36136;&#24615;&#30340;&#24773;&#20917;&#19979;FEEL&#31995;&#32479;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#32771;&#34385;&#21040;&#26469;&#33258;&#36793;&#32536;&#35774;&#22791;&#27169;&#22411;&#26356;&#26032;&#30340;&#38472;&#26087;&#24615;&#21644;&#21457;&#25955;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#27599;&#20010;&#32858;&#21512;&#26399;&#35843;&#25972;&#36793;&#32536;&#35774;&#22791;&#30340;&#19978;&#34892;&#20256;&#36755;&#21151;&#29575;&#26469;&#26368;&#23567;&#21270;FEEL&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#19978;&#30028;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#25509;&#36817;&#29702;&#24819;&#30340;&#23616;&#37096;SGD&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;&#30456;&#21516;&#30340;&#30446;&#26631;&#20934;&#30830;&#24230;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#26426;&#21046;&#30340;&#35757;&#32451;&#36895;&#24230;&#26174;&#30528;&#24555;&#20110;&#20855;&#26377;&#31354;&#20013;&#35745;&#31639;&#26041;&#26696;&#30340;&#20256;&#32479;&#21516;&#27493;FEEL&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over-the-air Computation (AirComp) has been demonstrated as an effective transmission scheme to boost the efficiency of federated edge learning (FEEL). However, existing FEEL systems with AirComp scheme often employ traditional synchronous aggregation mechanisms for local model aggregation in each global round, which suffer from the stragglers issues. In this paper, we propose a semi-asynchronous aggregation FEEL mechanism with AirComp scheme (PAOTA) to improve the training efficiency of the FEEL system in the case of significant heterogeneity in data and devices. Taking the staleness and divergence of model updates from edge devices into consideration, we minimize the convergence upper bound of the FEEL global model by adjusting the uplink transmit power of edge devices at each aggregation period. The simulation results demonstrate that our proposed algorithm achieves convergence performance close to that of the ideal Local SGD. Furthermore, with the same target accuracy, the training
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CAAFE&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#25968;&#25454;&#38598;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;ROC AUC&#34920;&#29616;&#25552;&#39640;&#33267;0.822&#12290;</title><link>http://arxiv.org/abs/2305.03403</link><description>&lt;p&gt;
GPT&#29992;&#20110;&#21322;&#33258;&#21160;&#21270;&#25968;&#25454;&#31185;&#23398;&#65306;&#24341;&#20837;CAAFE&#23454;&#29616;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
GPT for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering. (arXiv:2305.03403v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03403
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CAAFE&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#25968;&#25454;&#38598;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;ROC AUC&#34920;&#29616;&#25552;&#39640;&#33267;0.822&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#36825;&#20123;&#31995;&#32479;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24378;&#22823;&#21151;&#33021;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#21517;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#65288;CAAFE&#65289;&#65292;&#23427;&#21033;&#29992;LLM&#26681;&#25454;&#25968;&#25454;&#38598;&#30340;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#20135;&#29983;&#29992;&#20110;&#21019;&#24314;&#26032;&#29305;&#24449;&#30340;Python&#20195;&#30721;&#65292;&#24182;&#25552;&#20379;&#29983;&#25104;&#29305;&#24449;&#30340;&#25928;&#29992;&#35828;&#26126;&#12290;&#23613;&#31649;&#26041;&#27861;&#35770;&#19978;&#24456;&#31616;&#21333;&#65292;&#20294;CAAFE&#25552;&#39640;&#20102;14&#20010;&#25968;&#25454;&#38598;&#20013;11&#20010;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#19982;2&#20010;&#25968;&#25454;&#38598;&#24182;&#21015;&#65292;&#21482;&#26377;1&#20010;&#25968;&#25454;&#38598;&#24615;&#33021;&#19979;&#38477;&#65292;&#20174;&#32780;&#20351;&#25152;&#26377;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;ROC AUC&#34920;&#29616;&#20174;0.798&#25552;&#21319;&#33267;0.822&#12290;&#23545;&#20110;&#25152;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#19968;&#25913;&#36827;&#19982;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#65288;AUC 0.782&#65289;&#20195;&#26367;&#36923;&#36753;&#22238;&#24402;&#65288;AUC 0.754&#65289;&#25152;&#33719;&#24471;&#30340;&#24179;&#22343;&#25913;&#36827;&#30456;&#20284;&#12290;&#27492;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
As the field of automated machine learning (AutoML) advances, it becomes increasingly important to include domain knowledge within these systems. We present an approach for doing so by harnessing the power of large language models (LLMs). Specifically, we introduce Context-Aware Automated Feature Engineering (CAAFE), a feature engineering method for tabular datasets that utilizes an LLM to generate additional semantically meaningful features for tabular datasets based on their descriptions. The method produces both Python code for creating new features and explanations for the utility of the generated features.  Despite being methodologically simple, CAAFE enhances performance on 11 out of 14 datasets, ties on 2 and looses on 1 - boosting mean ROC AUC performance from 0.798 to 0.822 across all datasets. On the evaluated datasets, this improvement is similar to the average improvement achieved by using a random forest (AUC 0.782) instead of logistic regression (AUC 0.754).  Furthermore,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35782;&#21035;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#25935;&#24863;&#21464;&#37327;&#65292;&#20197;&#20415;&#20415;&#20110;&#21435;&#36523;&#20221;&#21270;&#36807;&#31243;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#35299;&#20915;&#19981;&#21516;&#25968;&#25454;&#38598;PHI&#23383;&#27573;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03169</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#25935;&#24863;&#25968;&#25454;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Sensitive Data Detection with High-Throughput Machine Learning Models in Electrical Health Records. (arXiv:2305.03169v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03169
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35782;&#21035;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#25935;&#24863;&#21464;&#37327;&#65292;&#20197;&#20415;&#20415;&#20110;&#21435;&#36523;&#20221;&#21270;&#36807;&#31243;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#35299;&#20915;&#19981;&#21516;&#25968;&#25454;&#38598;PHI&#23383;&#27573;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#65292;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#12289;&#31038;&#21306;&#21644;&#30740;&#31350;&#20154;&#21592;&#38656;&#35201;&#20998;&#20139;&#25968;&#25454;&#24182;&#21512;&#20316;&#25913;&#21892;&#20581;&#24247;&#32467;&#26524;&#12289;&#33719;&#21462;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#21644;&#25512;&#36827;&#30740;&#31350;&#12290;1996&#24180;&#12298;&#20581;&#24247;&#20445;&#38505;&#27969;&#36890;&#19982;&#36131;&#20219;&#27861;&#26696;&#12299;(HIPAA)&#26159;&#19968;&#39033;&#32852;&#37030;&#27861;&#24459;&#65292;&#26088;&#22312;&#36890;&#36807;&#21046;&#23450;&#20445;&#25252;&#20581;&#24247;&#20449;&#24687;&#30340;&#35268;&#23450;&#26469;&#20445;&#25252;&#25935;&#24863;&#20581;&#24247;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#25454;&#20849;&#20139;&#20043;&#21069;&#65292;HIPAA&#27809;&#26377;&#25552;&#20379;&#26377;&#25928;&#30340;&#26816;&#27979;&#25110;&#21024;&#38500;PHI&#30340;&#24037;&#20855;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35782;&#21035;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#25935;&#24863;&#21464;&#37327;&#65292;&#20174;&#32780;&#20415;&#20110;&#21435;&#36523;&#20221;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of big data, there is an increasing need for healthcare providers, communities, and researchers to share data and collaborate to improve health outcomes, generate valuable insights, and advance research. The Health Insurance Portability and Accountability Act of 1996 (HIPAA) is a federal law designed to protect sensitive health information by defining regulations for protected health information (PHI). However, it does not provide efficient tools for detecting or removing PHI before data sharing. One of the challenges in this area of research is the heterogeneous nature of PHI fields in data across different parties. This variability makes rule-based sensitive variable identification systems that work on one database fail on another. To address this issue, our paper explores the use of machine learning algorithms to identify sensitive variables in structured data, thus facilitating the de-identification process. We made a key observation that the distributions of metadata of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21322;&#30417;&#30563;&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#31867;&#21035;&#20998;&#24067;&#24863;&#30693;&#20266;&#26631;&#35760;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#25511;&#21046;&#20266;&#26631;&#31614;&#25968;&#30446;&#30340;&#24773;&#20917;&#19979;&#65292;&#26356;&#20934;&#30830;&#22320;&#36924;&#36817;&#30495;&#23454;&#20998;&#24067;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02795</link><description>&lt;p&gt;
&#38754;&#21521;&#21322;&#30417;&#30563;&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#31867;&#21035;&#20998;&#24067;&#24863;&#30693;&#20266;&#26631;&#35760;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Class-Distribution-Aware Pseudo Labeling for Semi-Supervised Multi-Label Learning. (arXiv:2305.02795v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21322;&#30417;&#30563;&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#31867;&#21035;&#20998;&#24067;&#24863;&#30693;&#20266;&#26631;&#35760;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#25511;&#21046;&#20266;&#26631;&#31614;&#25968;&#30446;&#30340;&#24773;&#20917;&#19979;&#65292;&#26356;&#20934;&#30830;&#22320;&#36924;&#36817;&#30495;&#23454;&#20998;&#24067;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20266;&#26631;&#35760;&#26159;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#20449;&#24687;&#30340;&#27969;&#34892;&#19988;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#23454;&#20363;&#30340;&#20266;&#26631;&#35760;&#26041;&#27861;&#36890;&#24120;&#26681;&#25454;&#20854;&#39044;&#27979;&#27010;&#29575;&#20026;&#27599;&#20010;&#26410;&#26631;&#35760;&#23454;&#20363;&#20998;&#37197;&#19968;&#20010;&#20266;&#26631;&#31614;&#12290;&#30001;&#20110;&#30495;&#23454;&#26631;&#31614;&#25968;&#30446;&#26410;&#30693;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#22810;&#26631;&#31614;&#23398;&#20064;&#65288;SSMLL&#65289;&#22330;&#26223;&#19979;&#38590;&#20197;&#24456;&#22909;&#22320;&#25512;&#24191;&#65292;&#22240;&#20026;&#23427;&#20204;&#20250;&#38754;&#20020;&#24341;&#20837;&#20551;&#27491;&#26631;&#31614;&#25110;&#24573;&#30053;&#30495;&#27491;&#26631;&#31614;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;SSMLL&#38382;&#39064;&#30340;&#31867;&#21035;&#20998;&#24067;&#24863;&#30693;&#20266;&#26631;&#35760;&#65288;CAP&#65289;&#26041;&#27861;&#65292;&#40723;&#21169;&#20266;&#26631;&#31614;&#30340;&#31867;&#21035;&#20998;&#24067;&#25509;&#36817;&#30495;&#23454;&#20998;&#24067;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21253;&#25324;&#31867;&#21035;&#24863;&#30693;&#38408;&#20540;&#30340;&#27491;&#21017;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#25511;&#21046;&#27599;&#20010;&#31867;&#21035;&#30340;&#20266;&#26631;&#31614;&#25968;&#30446;&#12290;&#37492;&#20110;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#31034;&#20363;&#26159;&#26681;&#25454;&#21516;&#19968;&#20998;&#24067;&#37319;&#26679;&#30340;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#26631;&#35760;&#31034;&#20363;&#30340;&#31867;&#21035;&#20998;&#24067;&#30830;&#23450;&#38408;&#20540;&#24182;&#38543;&#30528;&#27169;&#22411;&#21442;&#25968;&#19968;&#36215;&#26356;&#26032;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#24615;&#33021;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pseudo labeling is a popular and effective method to leverage the information of unlabeled data. Conventional instance-aware pseudo labeling methods often assign each unlabeled instance with a pseudo label based on its predicted probabilities. However, due to the unknown number of true labels, these methods cannot generalize well to semi-supervised multi-label learning (SSMLL) scenarios, since they would suffer from the risk of either introducing false positive labels or neglecting true positive ones. In this paper, we propose to solve the SSMLL problems by performing Class-distribution-Aware Pseudo labeling (CAP), which encourages the class distribution of pseudo labels to approximate the true one. Specifically, we design a regularized learning framework consisting of the class-aware thresholds to control the number of pseudo labels for each class. Given that the labeled and unlabeled examples are sampled according to the same distribution, we determine the thresholds by exploiting th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26448;&#26009;&#21457;&#29616;&#26694;&#26550;&#65292;&#21033;&#29992;&#26448;&#26009;&#31185;&#23398;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#23884;&#20837;&#20316;&#20026;&#26448;&#26009;&#30340;&#32452;&#25104;&#21644;&#32467;&#26500;&#29305;&#24449;&#36827;&#34892;&#34920;&#31034;&#65292;&#24182;&#19988;&#32852;&#21512;&#37319;&#29992;&#20102;&#34920;&#31034;&#30456;&#20284;&#24615;&#21484;&#22238;&#20505;&#36873;&#26448;&#26009;&#21644;&#22522;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#23545;&#20505;&#36873;&#26448;&#26009;&#36827;&#34892;&#30446;&#26631;&#23646;&#24615;&#25490;&#21517;&#30340;&#26041;&#26696;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25506;&#32034;&#24191;&#38420;&#30340;&#26448;&#26009;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#30830;&#23450;&#39640;&#24615;&#33021;&#20505;&#36873;&#26448;&#26009;&#12290;</title><link>http://arxiv.org/abs/2305.01101</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#34920;&#31034;&#36827;&#34892;&#26448;&#26009;&#25512;&#33616;&#12289;&#25490;&#21517;&#21644;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Leveraging Language Representation for Material Recommendation, Ranking, and Exploration. (arXiv:2305.01101v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26448;&#26009;&#21457;&#29616;&#26694;&#26550;&#65292;&#21033;&#29992;&#26448;&#26009;&#31185;&#23398;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#23884;&#20837;&#20316;&#20026;&#26448;&#26009;&#30340;&#32452;&#25104;&#21644;&#32467;&#26500;&#29305;&#24449;&#36827;&#34892;&#34920;&#31034;&#65292;&#24182;&#19988;&#32852;&#21512;&#37319;&#29992;&#20102;&#34920;&#31034;&#30456;&#20284;&#24615;&#21484;&#22238;&#20505;&#36873;&#26448;&#26009;&#21644;&#22522;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#23545;&#20505;&#36873;&#26448;&#26009;&#36827;&#34892;&#30446;&#26631;&#23646;&#24615;&#25490;&#21517;&#30340;&#26041;&#26696;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25506;&#32034;&#24191;&#38420;&#30340;&#26448;&#26009;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#30830;&#23450;&#39640;&#24615;&#33021;&#20505;&#36873;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#20852;&#25216;&#26415;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#26448;&#26009;&#21457;&#29616;&#21644;&#35774;&#35745;&#24050;&#32463;&#24471;&#21040;&#20102;&#21152;&#36895;&#12290;&#34429;&#28982;&#22312;&#23398;&#20064;&#26448;&#26009;&#32467;&#26500;&#19982;&#24615;&#36136;&#20851;&#31995;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#20294;&#33021;&#22815;&#26377;&#25928;&#25506;&#32034;&#24191;&#38420;&#30340;&#26448;&#26009;&#25628;&#32034;&#31354;&#38388;&#24182;&#30830;&#23450;&#39640;&#24615;&#33021;&#20505;&#36873;&#26448;&#26009;&#30340;&#26041;&#27861;&#20173;&#28982;&#21313;&#20998;&#26377;&#38480;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26448;&#26009;&#21457;&#29616;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#20174;&#26448;&#26009;&#31185;&#23398;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#20013;&#24471;&#20986;&#30340;&#33258;&#28982;&#35821;&#35328;&#23884;&#20837;&#20316;&#20026;&#32452;&#25104;&#21644;&#32467;&#26500;&#29305;&#24449;&#30340;&#34920;&#31034;&#12290;&#35813;&#21457;&#29616;&#26694;&#26550;&#30001;&#19968;&#20010;&#32852;&#21512;&#26041;&#26696;&#32452;&#25104;&#65292;&#32473;&#23450;&#19968;&#20010;&#26597;&#35810;&#26448;&#26009;&#65292;&#39318;&#20808;&#22522;&#20110;&#34920;&#31034;&#30456;&#20284;&#24615;&#21484;&#22238;&#20505;&#36873;&#26448;&#26009;&#65292;&#20877;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#23545;&#20505;&#36873;&#26448;&#26009;&#36827;&#34892;&#30446;&#26631;&#23646;&#24615;&#25490;&#21517;&#12290;&#35821;&#35328;&#34920;&#31034;&#20013;&#32534;&#30721;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#34987;&#21457;&#29616;&#21487;&#20197;&#20256;&#36798;&#26377;&#20851;&#26448;&#26009;&#24615;&#36136;&#21644;&#32467;&#26500;&#30340;&#20449;&#24687;&#65292;&#20351;&#24471;&#30456;&#20284;&#24615;&#20998;&#26512;&#21644;&#39640;&#24615;&#33021;&#26448;&#26009;&#21457;&#29616;&#21464;&#24471;&#26356;&#21152;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven approaches for material discovery and design have been accelerated by emerging efforts in machine learning. While there is enormous progress towards learning the structure to property relationship of materials, methods that allow for general representations of crystals to effectively explore the vast material search space and identify high-performance candidates remain limited. In this work, we introduce a material discovery framework that uses natural language embeddings derived from material science-specific language models as representations of compositional and structural features. The discovery framework consists of a joint scheme that, given a query material, first recalls candidates based on representational similarity, and ranks the candidates based on target properties through multi-task learning. The contextual knowledge encoded in language representations is found to convey information about material properties and structures, enabling both similarity analysis fo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65306;&#22312;&#21160;&#24577;&#22270;&#24418;&#29615;&#22659;&#19979;&#22914;&#20309;&#26377;&#25928;&#22320;&#36827;&#34892;&#36328;&#22270;&#36801;&#31227;&#23398;&#20064;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#39046;&#22495;&#28436;&#21270;&#23545;&#27867;&#21270;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.00664</link><description>&lt;p&gt;
&#36328;&#22270;&#21160;&#24577;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dynamic Transfer Learning across Graphs. (arXiv:2305.00664v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00664
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65306;&#22312;&#21160;&#24577;&#22270;&#24418;&#29615;&#22659;&#19979;&#22914;&#20309;&#26377;&#25928;&#22320;&#36827;&#34892;&#36328;&#22270;&#36801;&#31227;&#23398;&#20064;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#39046;&#22495;&#28436;&#21270;&#23545;&#27867;&#21270;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#65292;&#36328;&#22270;&#20256;&#36755;&#30693;&#35782;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21253;&#25324;&#36816;&#36755;&#32593;&#32476;&#12289;&#30005;&#23376;&#21830;&#21153;&#32593;&#32476;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#37329;&#34701;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#38382;&#39064;&#65306;&#22312;&#21160;&#24577;&#35774;&#32622;&#19979;&#65292;&#32771;&#34385;&#24050;&#35266;&#23519;&#21040;&#30340;&#20855;&#26377;&#26631;&#31614;&#30340;&#28304;&#22270;&#21644;&#26631;&#31614;&#31232;&#30095;&#30340;&#30446;&#26631;&#22270;&#65292;&#22914;&#20309;&#26377;&#25928;&#22320;&#34920;&#24449;&#19981;&#26029;&#21464;&#21270;&#30340;&#39046;&#22495;&#20559;&#24046;&#65292;&#24182;&#20248;&#21270;&#30446;&#26631;&#22495;&#22312;&#19979;&#19968;&#20010;&#26102;&#38388;&#25139;&#30340;&#27867;&#21270;&#24615;&#33021;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#36328;&#22270;&#21160;&#24577;&#36801;&#31227;&#23398;&#20064;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#21270;&#30028;&#38480;&#65292;&#36825;&#24847;&#21619;&#30528;&#27867;&#21270;&#24615;&#33021;&#30001;&#39046;&#22495;&#28436;&#21270;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transferring knowledge across graphs plays a pivotal role in many high-stake domains, ranging from transportation networks to e-commerce networks, from neuroscience to finance. To date, the vast majority of existing works assume both source and target domains are sampled from a universal and stationary distribution. However, many real-world systems are intrinsically dynamic, where the underlying domains are evolving over time. To bridge the gap, we propose to shift the problem to the dynamic setting and ask: given the label-rich source graphs and the label-scarce target graphs observed in previous T timestamps, how can we effectively characterize the evolving domain discrepancy and optimize the generalization performance of the target domain at the incoming T+1 timestamp? To answer the question, for the first time, we propose a generalization bound under the setting of dynamic transfer learning across graphs, which implies the generalization performance is dominated by domain evolution
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25351;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#35859;&#30340;&#26032;&#20852;&#25216;&#33021;&#26159;&#30740;&#31350;&#32773;&#20998;&#26512;&#30340;&#20135;&#29289;&#65292;&#19981;&#26159;&#27169;&#22411;&#34892;&#20026;&#30340;&#22522;&#26412;&#21464;&#21270;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#24230;&#37327;&#26631;&#20934;&#36873;&#25321;&#21644;&#21487;&#33021;&#30740;&#31350;&#20154;&#21592;&#30340;&#20559;&#35265;&#65292;&#21487;&#33021;&#23548;&#33268;&#36825;&#31181;&#26032;&#20852;&#25216;&#33021;&#30340;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.15004</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#34920;&#29616;&#30340;&#26032;&#20852;&#25216;&#33021;&#26159;&#21542;&#20026;&#24187;&#35273;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Emergent Abilities of Large Language Models a Mirage?. (arXiv:2304.15004v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.15004
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25351;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#35859;&#30340;&#26032;&#20852;&#25216;&#33021;&#26159;&#30740;&#31350;&#32773;&#20998;&#26512;&#30340;&#20135;&#29289;&#65292;&#19981;&#26159;&#27169;&#22411;&#34892;&#20026;&#30340;&#22522;&#26412;&#21464;&#21270;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#24230;&#37327;&#26631;&#20934;&#36873;&#25321;&#21644;&#21487;&#33021;&#30740;&#31350;&#20154;&#21592;&#30340;&#20559;&#35265;&#65292;&#21487;&#33021;&#23548;&#33268;&#36825;&#31181;&#26032;&#20852;&#25216;&#33021;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#22768;&#31216;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#26032;&#20852;&#25216;&#33021;&#65292;&#36825;&#20123;&#25216;&#33021;&#22312;&#26356;&#23567;&#35268;&#27169;&#30340;&#27169;&#22411;&#20013;&#19981;&#23384;&#22312;&#65292;&#20294;&#22312;&#26356;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#20013;&#23384;&#22312;&#12290;&#26032;&#20852;&#25216;&#33021;&#35753;&#20154;&#24863;&#21040;&#22256;&#24785;&#30340;&#26159;&#20004;&#26041;&#38754;&#65306;&#23427;&#20204;&#30340;&#28165;&#26224;&#24230;&#65292;&#20284;&#20046;&#30636;&#38388;&#20174;&#19981;&#23384;&#22312;&#21040;&#23384;&#22312;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#65292;&#20284;&#20046;&#22312;&#19981;&#21487;&#39044;&#35265;&#30340;&#27169;&#22411;&#35268;&#27169;&#19979;&#20986;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#20852;&#25216;&#33021;&#30340;&#21478;&#19968;&#31181;&#35299;&#37322;&#65292;&#21363;&#23545;&#20110;&#29305;&#23450;&#20219;&#21153;&#21644;&#27169;&#22411;&#26063;&#65292;&#24403;&#20998;&#26512;&#22266;&#23450;&#30340;&#27169;&#22411;&#36755;&#20986;&#26102;&#65292;&#21487;&#20197;&#36873;&#25321;&#23548;&#33268;&#25512;&#26029;&#20986;&#26032;&#20852;&#25216;&#33021;&#25110;&#19981;&#23548;&#33268;&#25512;&#26029;&#20986;&#26032;&#20852;&#25216;&#33021;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#35299;&#37322;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#26032;&#20852;&#25216;&#33021;&#22768;&#26126;&#26159;&#30740;&#31350;&#20154;&#21592;&#20998;&#26512;&#30340;&#20135;&#29289;&#65292;&#32780;&#19981;&#26159;&#29305;&#23450;&#20219;&#21153;&#20013;&#27169;&#22411;&#34892;&#20026;&#30340;&#22522;&#26412;&#21464;&#21270;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#23398;&#27169;&#22411;&#20013;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#35299;&#37322;&#65292;&#28982;&#21518;&#36890;&#36807;&#19977;&#31181;&#20114;&#34917;&#30340;&#26041;&#24335;&#36827;&#34892;&#20102;&#27979;&#35797;&#65306;&#25105;&#20204;(1)&#21046;&#20316;&#12289;&#27979;&#35797;&#24182;&#39564;&#35777;&#20102;&#20851;&#20110;&#25253;&#21578;&#30340;&#26032;&#20852;&#25216;&#33021;&#30340;&#24230;&#37327;&#36873;&#25321;&#30340;&#19977;&#20010;&#39044;&#27979;&#25928;&#24212;&#65307;(2)&#23637;&#31034;&#20102;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#31243;&#24207;&#30340;&#31616;&#21333;&#21464;&#21270;&#20250;&#22312;&#19968;&#20010;&#24050;&#32463;&#30830;&#23450;&#30340;&#20219;&#21153;&#20013;&#20135;&#29983;&#22823;&#30340;&#26032;&#20852;&#33021;&#21147;&#24046;&#24322;&#65307;(3)&#23637;&#31034;&#25152;&#35859;&#30340;&#26032;&#20852;&#25216;&#33021;&#21487;&#20197;&#36890;&#36807;&#26377;&#24847;&#20248;&#21270;&#25152;&#36873;&#25321;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#23454;&#29616;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#35748;&#20026;&#30446;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26032;&#20852;&#33021;&#21147;&#30340;&#22768;&#26126;&#24456;&#21487;&#33021;&#24182;&#19981;&#26159;&#30495;&#23454;&#23384;&#22312;&#30340;&#65292;&#32780;&#26159;&#24230;&#37327;&#26631;&#20934;&#20219;&#24847;&#36873;&#25321;&#21644;&#21487;&#33021;&#30340;&#30740;&#31350;&#20154;&#21592;&#20559;&#35265;&#30340;&#20135;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, one can choose a metric which leads to the inference of an emergent ability or another metric which does not. Thus, our alternative suggests that existing claims of emergent abilities are creations of the researcher's analyses, not fundamental changes in model behavior on specific tasks with scale. We present our explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metri
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#19968;&#31181;&#38750;&#32447;&#24615;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#65306;&#25193;&#25955;&#26144;&#23556;&#12290;&#26412;&#25991;&#38416;&#36848;&#22914;&#20309;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#21151;&#33021;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#19982;&#21151;&#33021;&#20027;&#25104;&#20998;&#20998;&#26512;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2304.14378</link><description>&lt;p&gt;
&#21151;&#33021;&#25193;&#25955;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Functional Diffusion Maps. (arXiv:2304.14378v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#19968;&#31181;&#38750;&#32447;&#24615;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#65306;&#25193;&#25955;&#26144;&#23556;&#12290;&#26412;&#25991;&#38416;&#36848;&#22914;&#20309;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#21151;&#33021;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#19982;&#21151;&#33021;&#20027;&#25104;&#20998;&#20998;&#26512;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#34987;&#35270;&#20026;&#26159;&#21151;&#33021;&#24615;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#29983;&#25104;&#23427;&#20204;&#30340;&#36807;&#31243;&#26159;&#36830;&#32493;&#30340;&#12290;&#36825;&#31181;&#31867;&#22411;&#25968;&#25454;&#30340;&#19968;&#20010;&#22522;&#26412;&#29305;&#24615;&#26159;&#65292;&#29702;&#35770;&#19978;&#23427;&#20204;&#23646;&#20110;&#26080;&#38480;&#32500;&#31354;&#38388;&#12290;&#23613;&#31649;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#36890;&#24120;&#21482;&#33021;&#24471;&#21040;&#26377;&#38480;&#25968;&#37327;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#23427;&#20204;&#20173;&#28982;&#26159;&#39640;&#32500;&#30340;&#65292;&#22240;&#27492;&#38477;&#32500;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#21151;&#33021;&#25968;&#25454;&#20998;&#26512;&#30340;&#20027;&#35201;&#29616;&#26377;&#26041;&#27861;&#26159;&#21151;&#33021;&#20027;&#25104;&#20998;&#20998;&#26512;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#36825;&#31181;&#32463;&#20856;&#25216;&#26415;&#20551;&#35774;&#25968;&#25454;&#20301;&#20110;&#19968;&#20010;&#32447;&#24615;&#27969;&#24418;&#20013;&#65292;&#22240;&#27492;&#24403;&#36825;&#20010;&#20551;&#35774;&#19981;&#25104;&#31435;&#26102;&#21487;&#33021;&#20250;&#20986;&#29616;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#19968;&#31181;&#38750;&#32447;&#24615;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#65306;&#25193;&#25955;&#26144;&#23556;&#12290;&#26412;&#25991;&#35299;&#37322;&#20102;&#22914;&#20309;&#23558;&#36825;&#31181;&#22810;&#21464;&#37327;&#26041;&#27861;&#25193;&#23637;&#21040;&#21151;&#33021;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#34892;&#20026;&#19982;&#21151;&#33021;&#20027;&#25104;&#20998;&#20998;&#26512;&#22312;&#19981;&#21516;&#30340;&#27169;&#25311;&#21644;&#23454;&#38469;&#20363;&#23376;&#20013;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays many real-world datasets can be considered as functional, in the sense that the processes which generate them are continuous. A fundamental property of this type of data is that in theory they belong to an infinite-dimensional space. Although in practice we usually receive finite observations, they are still high-dimensional and hence dimensionality reduction methods are crucial. In this vein, the main state-of-the-art method for functional data analysis is Functional PCA. Nevertheless, this classic technique assumes that the data lie in a linear manifold, and hence it could have problems when this hypothesis is not fulfilled. In this research, attention has been placed on a non-linear manifold learning method: Diffusion Maps. The article explains how to extend this multivariate method to functional data and compares its behavior against Functional PCA over different simulated and real examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#27010;&#24565;&#23884;&#20837;&#30340;&#21487;&#35299;&#37322;&#27010;&#24565;&#27169;&#22411;DCR&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#27010;&#24565;&#27169;&#22411;&#25552;&#39640;&#20102;&#39640;&#36798;+25&#65285;&#65292;&#24182;&#20135;&#29983;&#33021;&#22815;&#35299;&#37322;&#20854;&#39044;&#27979;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#21644;&#30495;&#20540;&#24230;&#65292;&#36866;&#24212;&#24615;&#24378;&#12290;</title><link>http://arxiv.org/abs/2304.14068</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#31526;&#21495;&#27010;&#24565;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Interpretable Neural-Symbolic Concept Reasoning. (arXiv:2304.14068v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#27010;&#24565;&#23884;&#20837;&#30340;&#21487;&#35299;&#37322;&#27010;&#24565;&#27169;&#22411;DCR&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#27010;&#24565;&#27169;&#22411;&#25552;&#39640;&#20102;&#39640;&#36798;+25&#65285;&#65292;&#24182;&#20135;&#29983;&#33021;&#22815;&#35299;&#37322;&#20854;&#39044;&#27979;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#21644;&#30495;&#20540;&#24230;&#65292;&#36866;&#24212;&#24615;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#39640;&#24230;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#23427;&#20204;&#19981;&#36879;&#26126;&#30340;&#20915;&#31574;&#36807;&#31243;&#38459;&#27490;&#20102;&#23427;&#20204;&#33719;&#24471;&#23436;&#20840;&#30340;&#20154;&#31867;&#20449;&#20219;&#12290;&#27010;&#24565;&#27169;&#22411;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#27010;&#24565;&#27169;&#22411;&#20381;&#36182;&#20110;&#39640;&#32500;&#27010;&#24565;&#23884;&#20837;&#34920;&#31034;&#65292;&#32570;&#20047;&#26126;&#30830;&#30340;&#35821;&#20041;&#21547;&#20041;&#65292;&#22240;&#27492;&#36136;&#30097;&#20854;&#20915;&#31574;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Deep Concept Reasoner(DCR)&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#27010;&#24565;&#23884;&#20837;&#30340;&#21487;&#35299;&#37322;&#27010;&#24565;&#27169;&#22411;&#12290;&#22312;DCR&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#19981;&#30452;&#25509;&#36827;&#34892;&#20219;&#21153;&#39044;&#27979;&#65292;&#32780;&#26159;&#20351;&#29992;&#27010;&#24565;&#23884;&#20837;&#24314;&#31435;&#35821;&#27861;&#35268;&#21017;&#32467;&#26500;&#12290;&#28982;&#21518;DCR&#22312;&#26377;&#24847;&#20041;&#30340;&#27010;&#24565;&#30495;&#20540;&#24230;&#19978;&#25191;&#34892;&#36825;&#20123;&#35268;&#21017;&#65292;&#20197;&#19981;&#21487;&#24494;&#20998;&#30340;&#26041;&#24335;&#25552;&#20379;&#26368;&#32456;&#30340;&#21487;&#35299;&#37322;&#21644;&#35821;&#20041;&#19968;&#33268;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;DCR&#65306;(i)&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#27010;&#24565;&#27169;&#22411;&#25552;&#39640;&#20102;&#39640;&#36798;+25&#65285;;(ii)&#20135;&#29983;&#33021;&#22815;&#35299;&#37322;&#20854;&#39044;&#27979;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#21644;&#30495;&#20540;&#24230;;(iii)&#24456;&#23481;&#26131;&#36866;&#24212;&#26032;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning methods are highly accurate, yet their opaque decision process prevents them from earning full human trust. Concept-based models aim to address this issue by learning tasks based on a set of human-understandable concepts. However, state-of-the-art concept-based models rely on high-dimensional concept embedding representations which lack a clear semantic meaning, thus questioning the interpretability of their decision process. To overcome this limitation, we propose the Deep Concept Reasoner (DCR), the first interpretable concept-based model that builds upon concept embeddings. In DCR, neural networks do not make task predictions directly, but they build syntactic rule structures using concept embeddings. DCR then executes these rules on meaningful concept truth degrees to provide a final interpretable and semantically-consistent prediction in a differentiable manner. Our experiments show that DCR: (i) improves up to +25% w.r.t. state-of-the-art interpretable concept-based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#37327;&#23376;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;(QNPG)&#31639;&#27861;&#65292;&#21033;&#29992;&#20102;&#37327;&#23376;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#30340;&#39640;&#25928;&#36817;&#20284;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#22522;&#20110;&#19968;&#38454;&#26799;&#24230;&#30340;&#35757;&#32451;&#65292;QNPG&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#21487;&#20197;&#20943;&#23569;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.13571</link><description>&lt;p&gt;
&#37327;&#23376;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#65306;&#21521;&#26679;&#26412;&#39640;&#25928;&#22686;&#24378;&#23398;&#20064;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Quantum Natural Policy Gradients: Towards Sample-Efficient Reinforcement Learning. (arXiv:2304.13571v1 [quant-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#37327;&#23376;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;(QNPG)&#31639;&#27861;&#65292;&#21033;&#29992;&#20102;&#37327;&#23376;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#30340;&#39640;&#25928;&#36817;&#20284;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#22522;&#20110;&#19968;&#38454;&#26799;&#24230;&#30340;&#35757;&#32451;&#65292;QNPG&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#21487;&#20197;&#20943;&#23569;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#26041;&#21521;&#65292;&#20294;&#23398;&#20064;&#36807;&#31243;&#36890;&#24120;&#24456;&#32791;&#36153;&#36164;&#28304;&#12290;&#20351;&#29992;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#20316;&#20026;&#20989;&#25968;&#36924;&#36817;&#22120;&#21487;&#20197;&#20943;&#23569;&#25104;&#26412;&#65292;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#25928;&#29575;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37327;&#23376;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;(QNPG)&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#37327;&#23376;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#30340;&#39640;&#25928;&#36817;&#20284;&#26041;&#27861;&#65292;&#26159;&#19968;&#31181;&#20108;&#38454;&#26799;&#24230;&#30340;&#22522;&#20110;&#31574;&#30053;&#30340;&#31639;&#27861;&#12290;&#22312;Contextual Bandits&#29615;&#22659;&#19979;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;QNPG &#27604;&#22522;&#20110;&#19968;&#38454;&#26799;&#24230;&#30340;&#35757;&#32451;&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#38469;&#21487;&#34892;&#24615;&#65292;&#24182;&#22312;12&#37327;&#23376;&#27604;&#29305;&#30340;&#30828;&#20214;&#35774;&#22791;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning is a growing field in AI with a lot of potential. Intelligent behavior is learned automatically through trial and error in interaction with the environment. However, this learning process is often costly. Using variational quantum circuits as function approximators can reduce this cost. In order to implement this, we propose the quantum natural policy gradient (QNPG) algorithm -- a second-order gradient-based routine that takes advantage of an efficient approximation of the quantum Fisher information matrix. We experimentally demonstrate that QNPG outperforms first-order based training on Contextual Bandits environments regarding convergence speed and stability and thereby reduces the sample complexity. Furthermore, we provide evidence for the practical feasibility of our approach by training on a 12-qubit hardware device.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38416;&#36848;&#20102;BN&#21644;ReLU&#20043;&#38388;&#30340;&#19981;&#21644;&#35856;&#26159;&#23548;&#33268;&#26799;&#24230;&#29190;&#28856;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#21516;&#26102;&#21457;&#29616;&#36755;&#20837;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#31639;&#27861;&#65292;&#22312;&#22823;&#25209;&#37327;&#35757;&#32451;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#21487;&#26367;&#20195;WarmUp&#65292;&#22312;&#23567;&#25209;&#37327;&#35757;&#32451;&#20013;&#20063;&#34920;&#29616;&#19981;&#38169;&#12290;</title><link>http://arxiv.org/abs/2304.11692</link><description>&lt;p&gt;
BN&#19982;ReLU&#20043;&#38388;&#30340;&#19981;&#21644;&#35856;&#24341;&#36215;&#26799;&#24230;&#29190;&#28856;&#65292;&#20294;&#34987;&#28608;&#27963;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#25152;&#25269;&#28040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Disharmony Between BN and ReLU Causes Gradient Explosion, but is Offset by the Correlation Between Activations. (arXiv:2304.11692v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38416;&#36848;&#20102;BN&#21644;ReLU&#20043;&#38388;&#30340;&#19981;&#21644;&#35856;&#26159;&#23548;&#33268;&#26799;&#24230;&#29190;&#28856;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#21516;&#26102;&#21457;&#29616;&#36755;&#20837;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#31639;&#27861;&#65292;&#22312;&#22823;&#25209;&#37327;&#35757;&#32451;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#21487;&#26367;&#20195;WarmUp&#65292;&#22312;&#23567;&#25209;&#37327;&#35757;&#32451;&#20013;&#20063;&#34920;&#29616;&#19981;&#38169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25209;&#26631;&#20934;&#21270;&#21644;ReLU&#31561;&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#20250;&#22312;&#35757;&#32451;&#21021;&#26399;&#30001;&#20110;&#26102;&#38388;&#26799;&#24230;&#29190;&#28856;&#32780;&#20986;&#29616;&#19981;&#31283;&#23450;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;ReLU&#22914;&#20309;&#27604;&#39044;&#26399;&#26356;&#22810;&#22320;&#20943;&#23569;&#26041;&#24046;&#65292;&#20197;&#21450;&#25209;&#26631;&#20934;&#21270;&#22914;&#20309;&#22312;&#24674;&#22797;&#26399;&#38388;&#25918;&#22823;&#26799;&#24230;&#65292;&#23548;&#33268;&#21069;&#21521;&#20256;&#25773;&#20445;&#25345;&#31283;&#23450;&#32780;&#26799;&#24230;&#29190;&#28856;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#21160;&#21147;&#23398;&#21464;&#21270;&#20197;&#21450;&#36755;&#20837;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#22914;&#20309;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#24863;&#26469;&#33258;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#30340;&#26356;&#22909;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#31639;&#27861;&#65292;&#22312;&#22823;&#25209;&#37327;&#35757;&#32451;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#23398;&#20064;&#29575;&#32553;&#25918;&#26041;&#27861;&#65292;&#24182;&#21487;&#26367;&#25442;&#23567;&#25209;&#37327;&#35757;&#32451;&#20013;&#30340;WarmUp&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks based on batch normalization and ReLU-like activation functions can experience instability during the early stages of training due to the high gradient induced by temporal gradient explosion. We explain how ReLU reduces variance more than expected, and how batch normalization amplifies the gradient during recovery, which causes gradient explosion while forward propagation remains stable. Additionally, we discuss how the dynamics of a deep neural network change during training and how the correlation between inputs can alleviate this problem. Lastly, we propose a better adaptive learning rate algorithm inspired by second-order optimization algorithms, which outperforms existing learning rate scaling methods in large batch training and can also replace WarmUp in small batch training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#35299;&#37322;&#38544;&#24335;Q&#23398;&#20064;(IQL)&#20316;&#20026;Actor-Critic&#26041;&#27861;&#65292;&#25552;&#20986;&#20351;&#29992;&#25193;&#25955;&#34892;&#20026;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#26435;&#37325;&#26469;&#24179;&#34913;&#22870;&#21169;&#26368;&#22823;&#21270;&#21644;&#19982;&#34892;&#20026;&#31574;&#30053;&#30340;&#20998;&#27495;&#12290;&#36825;&#20010;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#21644;&#22810;&#23792;&#29305;&#24449;&#30340;Actor&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.10573</link><description>&lt;p&gt;
IDQL: &#20316;&#20026;&#19968;&#31181;&#25193;&#25955;&#31574;&#30053;&#30340;Actor-Critic&#26041;&#27861;&#30340;&#38544;&#24335;Q&#23398;&#20064;&#12290; (arXiv:2304.10573v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
IDQL: Implicit Q-Learning as an Actor-Critic Method with Diffusion Policies. (arXiv:2304.10573v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#35299;&#37322;&#38544;&#24335;Q&#23398;&#20064;(IQL)&#20316;&#20026;Actor-Critic&#26041;&#27861;&#65292;&#25552;&#20986;&#20351;&#29992;&#25193;&#25955;&#34892;&#20026;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#26435;&#37325;&#26469;&#24179;&#34913;&#22870;&#21169;&#26368;&#22823;&#21270;&#21644;&#19982;&#34892;&#20026;&#31574;&#30053;&#30340;&#20998;&#27495;&#12290;&#36825;&#20010;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#21644;&#22810;&#23792;&#29305;&#24449;&#30340;Actor&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#31163;&#32447;RL&#26041;&#27861;&#38656;&#35201;&#27491;&#30830;&#22788;&#29702;&#36229;&#20986;&#20998;&#24067;&#30340;&#34892;&#20026;&#12290;&#38544;&#24335;Q&#23398;&#20064;&#65288;IQL&#65289;&#36890;&#36807;&#20165;&#20351;&#29992;&#25968;&#25454;&#38598;&#34892;&#21160;&#36890;&#36807;&#20462;&#25913;&#21518;&#30340;Bellman Backup&#26469;&#35757;&#32451;Q&#20989;&#25968;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#19981;&#28165;&#26970;&#21738;&#20010;&#31574;&#30053;&#23454;&#38469;&#19978;&#23454;&#29616;&#20102;&#27492;&#38544;&#21547;&#35757;&#32451;&#30340;Q&#20989;&#25968;&#25152;&#20195;&#34920;&#30340;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;IQL&#37325;&#26032;&#35299;&#37322;&#20026;Actor-Critic&#26041;&#27861;&#65292;&#36890;&#36807;&#24191;&#20041;&#21270;&#35780;&#21028;&#30446;&#26631;&#24182;&#23558;&#20854;&#36830;&#25509;&#21040;&#34892;&#20026;&#35268;&#33539;&#21270;&#30340;&#38544;&#24335;Actor&#26469;&#23454;&#29616;&#12290;&#36825;&#31181;&#27867;&#21270;&#26174;&#31034;&#20102;&#24341;&#20837;&#30340;Actor&#22914;&#20309;&#24179;&#34913;&#22870;&#21169;&#26368;&#22823;&#21270;&#21644;&#19982;&#34892;&#20026;&#31574;&#30053;&#30340;&#20998;&#27495;&#65292;&#20855;&#20307;&#30340;&#25439;&#22833;&#36873;&#25321;&#20915;&#23450;&#20102;&#36825;&#31181;&#26435;&#34913;&#30340;&#24615;&#36136;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20010;Actor&#21487;&#20197;&#34920;&#29616;&#20986;&#22797;&#26434;&#21644;&#22810;&#23792;&#30340;&#29305;&#24449;&#65292;&#36825;&#34920;&#26126;&#20102;&#21033;&#29992;&#20248;&#21183;&#21152;&#26435;&#22238;&#24402;&#65288;AWR&#65289;&#20013;&#20351;&#29992;&#30340;&#26465;&#20214;&#39640;&#26031;Actor&#30340;&#25311;&#21512;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#26469;&#33258;&#21442;&#25968;&#21270;&#25193;&#25955;&#34892;&#20026;&#31574;&#30053;&#30340;&#26679;&#26412;&#21644;&#30001;&#35780;&#21028;&#22120;&#35745;&#31639;&#30340;&#26435;&#37325;&#65292;&#28982;&#21518;&#23558;&#20854;&#23548;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective offline RL methods require properly handling out-of-distribution actions. Implicit Q-learning (IQL) addresses this by training a Q-function using only dataset actions through a modified Bellman backup. However, it is unclear which policy actually attains the values represented by this implicitly trained Q-function. In this paper, we reinterpret IQL as an actor-critic method by generalizing the critic objective and connecting it to a behavior-regularized implicit actor. This generalization shows how the induced actor balances reward maximization and divergence from the behavior policy, with the specific loss choice determining the nature of this tradeoff. Notably, this actor can exhibit complex and multimodal characteristics, suggesting issues with the conditional Gaussian actor fit with advantage weighted regression (AWR) used in prior methods. Instead, we propose using samples from a diffusion parameterized behavior policy and weights computed from the critic to then importa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; Zip-NeRF &#25216;&#26415;&#65292;&#23558; mip-NeRF 360 &#21644;&#22522;&#20110;&#32593;&#26684;&#30340;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#25239;&#38191;&#40831;&#12289;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#24182;&#38477;&#20302;&#35823;&#24046;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.06706</link><description>&lt;p&gt;
Zip-NeRF&#65306;&#25239;&#38191;&#40831;&#32593;&#26684;&#21270;&#31070;&#32463;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields. (arXiv:2304.06706v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; Zip-NeRF &#25216;&#26415;&#65292;&#23558; mip-NeRF 360 &#21644;&#22522;&#20110;&#32593;&#26684;&#30340;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#25239;&#38191;&#40831;&#12289;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#24182;&#38477;&#20302;&#35823;&#24046;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#30340;&#32593;&#26684;&#21270;&#34920;&#31034;&#21487;&#20197;&#21152;&#36895;&#35757;&#32451;&#65292;&#20294;&#32570;&#20047;&#23545;&#27604;&#20363;&#30340;&#26126;&#30830;&#29702;&#35299;&#65292;&#23481;&#26131;&#24341;&#20837;&#38191;&#40831;&#25110;&#20002;&#22833;&#22330;&#26223;&#20869;&#23481;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#28210;&#26579;&#21644;&#20449;&#21495;&#22788;&#29702;&#24605;&#24819;&#29992;&#20110;&#23558; mip-NeRF 360 &#21644;&#22522;&#20110;&#32593;&#26684;&#30340;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#35823;&#24046;&#29575;&#27604;&#20808;&#21069;&#30340;&#25216;&#26415;&#20302;8%&#21040;76%&#65292;&#24182;&#27604; mip-NeRF 360 &#24555;22&#20493;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Radiance Field training can be accelerated through the use of grid-based representations in NeRF's learned mapping from spatial coordinates to colors and volumetric density. However, these grid-based approaches lack an explicit understanding of scale and therefore often introduce aliasing, usually in the form of jaggies or missing scene content. Anti-aliasing has previously been addressed by mip-NeRF 360, which reasons about sub-volumes along a cone rather than points along a ray, but this approach is not natively compatible with current grid-based techniques. We show how ideas from rendering and signal processing can be used to construct a technique that combines mip-NeRF 360 and grid-based models such as Instant NGP to yield error rates that are 8% - 76% lower than either prior technique, and that trains 22x faster than mip-NeRF 360.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;Segment Any Medical Model (SAMM)&#65292;&#23427;&#26159;&#29992;&#20110;3D Slicer&#30340;SAM&#30340;&#25193;&#23637;&#12290;SAMM&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#22312;&#23454;&#26102;&#24615;&#21644;&#36890;&#29992;&#24615;&#26041;&#38754;&#37117;&#26377;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#25512;&#26029;&#20986;&#25513;&#27169;&#12290;</title><link>http://arxiv.org/abs/2304.05622</link><description>&lt;p&gt;
SAMM&#65288;Segment Any Medical Model&#65289;&#65306;&#29992;&#20110;SAM&#30340;3D Slicer&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
SAMM (Segment Any Medical Model): A 3D Slicer Integration to SAM. (arXiv:2304.05622v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05622
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;Segment Any Medical Model (SAMM)&#65292;&#23427;&#26159;&#29992;&#20110;3D Slicer&#30340;SAM&#30340;&#25193;&#23637;&#12290;SAMM&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#22312;&#23454;&#26102;&#24615;&#21644;&#36890;&#29992;&#24615;&#26041;&#38754;&#37117;&#26377;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#25512;&#26029;&#20986;&#25513;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Segment Anything Model&#65288;SAM&#65289;&#26159;&#19968;&#20010;&#26032;&#30340;&#22270;&#20687;&#20998;&#21106;&#24037;&#20855;&#65292;&#20351;&#29992;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#20998;&#21106;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#35813;&#27169;&#22411;&#34920;&#26126;&#23427;&#21487;&#20197;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#20998;&#21106;&#25513;&#27169;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#23454;&#26102;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;&#24615;&#33021;&#38656;&#35201;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;&#20026;&#20102;&#21327;&#21161;&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#24320;&#21457;&#65292;&#35780;&#20272;&#21644;&#21033;&#29992;SAM&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Segment Any Medical Model&#65288;SAMM&#65289;&#65292;&#23427;&#26159;SAM&#22312;3D Slicer&#19978;&#30340;&#25193;&#23637;&#12290;3D Slicer&#26159;&#19968;&#20010;&#24191;&#27867;&#20351;&#29992;&#20110;&#21307;&#23398;&#24433;&#20687;&#22788;&#29702;&#21644;&#21487;&#35270;&#21270;&#36719;&#20214;&#30340;&#24320;&#28304;&#36719;&#20214;&#12290;&#36825;&#20010;&#24320;&#28304;&#25193;&#23637;&#31243;&#24207;&#21450;&#20854;&#28436;&#31034;&#24050;&#21457;&#24067;&#22312;GitHub&#19978;&#65288;https://github.com/bingogome/samm&#65289;&#12290;SAMM&#22312;&#23436;&#25972;&#21608;&#26399;&#20013;&#23454;&#29616;&#20102;0.6&#31186;&#30340;&#24310;&#36831;&#65292;&#24182;&#21487;&#20197;&#23454;&#26102;&#25512;&#26029;&#20986;&#22270;&#20687;&#25513;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) is a new image segmentation tool trained with the largest segmentation dataset at this time. The model has demonstrated that it can create high-quality masks for image segmentation with good promptability and generalizability. However, the performance of the model on medical images requires further validation. To assist with the development, assessment, and utilization of SAM on medical images, we introduce Segment Any Medical Model (SAMM), an extension of SAM on 3D Slicer, a widely-used open-source image processing and visualization software that has been extensively used in the medical imaging community. This open-source extension to 3D Slicer and its demonstrations are posted on GitHub (https://github.com/bingogome/samm). SAMM achieves 0.6-second latency of a complete cycle and can infer image masks in nearly real-time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#25512;&#29702;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#27979;&#35797;&#25512;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#30001;&#30456;&#20114;&#24433;&#21709;&#24378;&#28872;&#30340;&#23616;&#37096;&#21464;&#37327;&#38598;&#32676;&#32452;&#25104;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#36890;&#36807;&#19968;&#27493;&#27493;&#30340;&#25512;&#29702;&#65292;&#33021;&#22815;&#23558;&#20934;&#30830;&#30340;&#23616;&#37096;&#25512;&#29702;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#20272;&#31639;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2304.03843</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#35201;&#36880;&#27493;&#24605;&#32771;&#65311;&#25512;&#29702;&#28304;&#20110;&#32463;&#39564;&#30340;&#23616;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why think step-by-step? Reasoning emerges from the locality of experience. (arXiv:2304.03843v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#25512;&#29702;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#27979;&#35797;&#25512;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#30001;&#30456;&#20114;&#24433;&#21709;&#24378;&#28872;&#30340;&#23616;&#37096;&#21464;&#37327;&#38598;&#32676;&#32452;&#25104;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#36890;&#36807;&#19968;&#27493;&#27493;&#30340;&#25512;&#29702;&#65292;&#33021;&#22815;&#23558;&#20934;&#30830;&#30340;&#23616;&#37096;&#25512;&#29702;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#20272;&#31639;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#30528;&#24378;&#22823;&#32780;&#31070;&#31192;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#32431;&#31929;&#30340;&#24605;&#32500;&#27493;&#39588;&#65292;&#25105;&#20204;&#21487;&#20197;&#25512;&#29702;&#20986;&#25105;&#20204;&#26080;&#27861;&#30452;&#25509;&#24471;&#20986;&#30340;&#25512;&#35770; - &#23613;&#31649;&#25105;&#20204;&#20174;&#19990;&#30028;&#19978;&#27809;&#26377;&#24471;&#21040;&#20219;&#20309;&#39069;&#22806;&#25968;&#25454;&#12290;&#21516;&#26679;&#22320;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#19968;&#27493;&#27493;&#30340;&#25512;&#29702;&#65292;&#22312;&#22238;&#31572;&#38382;&#39064;&#20043;&#21069;&#29983;&#25104;&#20013;&#38388;&#27493;&#39588;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#23436;&#25104;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#25512;&#29702;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#27979;&#35797;&#25512;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#30001;&#30456;&#20114;&#24433;&#21709;&#24378;&#28872;&#30340;&#23616;&#37096;&#21464;&#37327;&#38598;&#32676;&#32452;&#25104;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#36825;&#20123;&#35757;&#32451;&#26465;&#20214;&#33021;&#22815;&#23558;&#20934;&#30830;&#30340;&#23616;&#37096;&#25512;&#29702;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#20272;&#31639;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#23450;&#20041;&#30340;&#32852;&#21512;&#20998;&#24067;&#30340;&#26679;&#21697;&#23545;&#33258;&#22238;&#24402;&#21464;&#21387;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#27599;&#20010;&#26679;&#21697;&#21482;&#21253;&#25324;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#21464;&#37327;&#12290;&#25105;&#20204;&#27604;&#36739;&#20351;&#29992;&#25512;&#29702;&#29983;&#25104;&#30340;&#21464;&#37327;&#23376;&#38598;&#19982;&#20351;&#29992;&#23436;&#25972;&#38598;&#21512;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans have a powerful and mysterious capacity to reason. By working through a series of purely mental steps, we can make inferences we would not be capable of making directly -- despite that fact that we get no additional data from the world. Similarly, large language models can perform better at complex tasks through chain-of-thought reasoning, where they generate intermediate steps before answering a question. We use language models to investigate the questions of when and why reasoning is helpful, testing the hypothesis that reasoning is effective when training data consisting of local clusters of variables that influence each other strongly. These training conditions enable the chaining of accurate local inferences in order to estimate relationships between variables that were not seen together in training. We train an autoregressive transformer on samples from joint distributions defined by Bayes nets, but only include a subset of all the variables in each sample. We compare lang
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#27454;&#21517;&#20026; Torch-Choice &#30340; PyTorch &#36719;&#20214;&#21253;&#65292;&#29992;&#20110;&#31649;&#29702;&#25968;&#25454;&#24211;&#12289;&#26500;&#24314;&#22810;&#39033;&#24335;Logit&#21644;&#23884;&#22871;Logit&#27169;&#22411;&#65292;&#24182;&#25903;&#25345;GPU&#21152;&#36895;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01906</link><description>&lt;p&gt;
Torch-Choice: &#29992;Python&#23454;&#29616;&#22823;&#35268;&#27169;&#36873;&#25321;&#24314;&#27169;&#30340;PyTorch&#21253;
&lt;/p&gt;
&lt;p&gt;
Torch-Choice: A PyTorch Package for Large-Scale Choice Modelling with Python. (arXiv:2304.01906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#27454;&#21517;&#20026; Torch-Choice &#30340; PyTorch &#36719;&#20214;&#21253;&#65292;&#29992;&#20110;&#31649;&#29702;&#25968;&#25454;&#24211;&#12289;&#26500;&#24314;&#22810;&#39033;&#24335;Logit&#21644;&#23884;&#22871;Logit&#27169;&#22411;&#65292;&#24182;&#25903;&#25345;GPU&#21152;&#36895;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
$\texttt{torch-choice}$ &#26159;&#19968;&#27454;&#24320;&#28304;&#36719;&#20214;&#21253;&#65292;&#20351;&#29992;Python&#21644;PyTorch&#23454;&#29616;&#28789;&#27963;&#12289;&#24555;&#36895;&#30340;&#36873;&#25321;&#24314;&#27169;&#12290;&#23427;&#25552;&#20379;&#20102; $\texttt{ChoiceDataset}$ &#25968;&#25454;&#32467;&#26500;&#65292;&#20197;&#20415;&#28789;&#27963;&#32780;&#39640;&#25928;&#22320;&#31649;&#29702;&#25968;&#25454;&#24211;&#12290;&#26412;&#25991;&#28436;&#31034;&#20102;&#22914;&#20309;&#20174;&#21508;&#31181;&#26684;&#24335;&#30340;&#25968;&#25454;&#24211;&#20013;&#26500;&#24314; $\texttt{ChoiceDataset}$&#65292;&#24182;&#23637;&#31034;&#20102; $\texttt{ChoiceDataset}$ &#30340;&#21508;&#31181;&#21151;&#33021;&#12290;&#35813;&#36719;&#20214;&#21253;&#23454;&#29616;&#20102;&#20004;&#31181;&#24120;&#29992;&#30340;&#27169;&#22411;: &#22810;&#39033;&#24335;Logit&#21644;&#23884;&#22871;Logit&#27169;&#22411;&#65292;&#24182;&#25903;&#25345;&#27169;&#22411;&#20272;&#35745;&#26399;&#38388;&#30340;&#27491;&#21017;&#21270;&#12290;&#35813;&#36719;&#20214;&#21253;&#36824;&#25903;&#25345;&#20351;&#29992;GPU&#36827;&#34892;&#20272;&#35745;&#65292;&#20351;&#20854;&#21487;&#20197;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#32780;&#19988;&#22312;&#35745;&#31639;&#19978;&#26356;&#39640;&#25928;&#12290;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;R&#39118;&#26684;&#30340;&#20844;&#24335;&#23383;&#31526;&#20018;&#25110;Python&#23383;&#20856;&#36827;&#34892;&#21021;&#22987;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102; $\texttt{torch-choice}$ &#21644; R&#20013;&#30340; $\texttt{mlogit}$ &#22312;&#20197;&#19979;&#20960;&#20010;&#26041;&#38754;&#30340;&#35745;&#31639;&#25928;&#29575;: (1) &#35266;&#27979;&#25968;&#22686;&#21152;&#26102;&#65292;(2) &#21327;&#21464;&#37327;&#20010;&#25968;&#22686;&#21152;&#26102;&#65292; (3) &#27979;&#35797;&#25968;&#21319;&#39640;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
The $\texttt{torch-choice}$ is an open-source library for flexible, fast choice modeling with Python and PyTorch. $\texttt{torch-choice}$ provides a $\texttt{ChoiceDataset}$ data structure to manage databases flexibly and memory-efficiently. The paper demonstrates constructing a $\texttt{ChoiceDataset}$ from databases of various formats and functionalities of $\texttt{ChoiceDataset}$. The package implements two widely used models, namely the multinomial logit and nested logit models, and supports regularization during model estimation. The package incorporates the option to take advantage of GPUs for estimation, allowing it to scale to massive datasets while being computationally efficient. Models can be initialized using either R-style formula strings or Python dictionaries. We conclude with a comparison of the computational efficiencies of $\texttt{torch-choice}$ and $\texttt{mlogit}$ in R as (1) the number of observations increases, (2) the number of covariates increases, and (3) th
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26041;&#27861;&#65292;&#20803;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#26469;&#20248;&#21270;&#36229;&#21442;&#25968;&#37197;&#32622;&#30340;&#24615;&#33021;&#25490;&#21517;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#26469;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#26032;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.15212</link><description>&lt;p&gt;
&#28145;&#24230;&#25490;&#21517;&#38598;&#25104;&#29992;&#20110;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Deep Ranking Ensembles for Hyperparameter Optimization. (arXiv:2303.15212v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15212
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26041;&#27861;&#65292;&#20803;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#26469;&#20248;&#21270;&#36229;&#21442;&#25968;&#37197;&#32622;&#30340;&#24615;&#33021;&#25490;&#21517;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#26469;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#26032;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#36229;&#21442;&#25968;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20027;&#35201;&#38382;&#39064;&#20043;&#19968;&#12290;&#29616;&#26377;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#24037;&#20316;&#35757;&#32451;&#20102;&#20195;&#29702;&#27169;&#22411;&#26469;&#36817;&#20284;&#36229;&#21442;&#25968;&#21709;&#24212;&#38754;&#20316;&#20026;&#22238;&#24402;&#20219;&#21153;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20551;&#35774;&#35757;&#32451;&#20195;&#29702;&#30340;&#26368;&#20339;&#31574;&#30053;&#26159;&#20445;&#25345;&#36229;&#21442;&#25968;&#37197;&#32622;&#30340;&#24615;&#33021;&#25490;&#21517;&#20316;&#20026;&#19968;&#20010;&#23398;&#20064;&#25490;&#21517;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20803;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#26469;&#20248;&#21270;&#37197;&#32622;&#30340;&#24615;&#33021;&#25490;&#21517;&#65292;&#21516;&#26102;&#36890;&#36807;&#38598;&#25104;&#30340;&#26041;&#24335;&#23545;&#20854;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23454;&#39564;&#21327;&#35758;&#20013;&#65292;&#21253;&#21547;12&#20010;&#22522;&#32447;&#65292;16&#20010;HPO&#25628;&#32034;&#31354;&#38388;&#21644;86&#20010;&#25968;&#25454;&#38598;/&#20219;&#21153;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;HPO&#26041;&#38754;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically optimizing the hyperparameters of Machine Learning algorithms is one of the primary open questions in AI. Existing work in Hyperparameter Optimization (HPO) trains surrogate models for approximating the response surface of hyperparameters as a regression task. In contrast, we hypothesize that the optimal strategy for training surrogates is to preserve the ranks of the performances of hyperparameter configurations as a Learning to Rank problem. As a result, we present a novel method that meta-learns neural network surrogates optimized for ranking the configurations' performances while modeling their uncertainty via ensembling. In a large-scale experimental protocol comprising 12 baselines, 16 HPO search spaces and 86 datasets/tasks, we demonstrate that our method achieves new state-of-the-art results in HPO.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#35268;&#33539;&#24314;&#27169;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#26816;&#27979;&#20986;&#22810;&#31181;&#25104;&#20687;&#21644;&#29983;&#29289;&#21464;&#37327;&#20013;&#30340;&#24322;&#24120;&#24615;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#30740;&#31350;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#30142;&#30149;&#12290;</title><link>http://arxiv.org/abs/2303.12706</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#36328;&#22810;&#31181;&#25104;&#20687;&#27169;&#24577;&#36827;&#34892;&#35268;&#33539;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Variational Autoencoders for normative modelling across multiple imaging modalities. (arXiv:2303.12706v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#35268;&#33539;&#24314;&#27169;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#26816;&#27979;&#20986;&#22810;&#31181;&#25104;&#20687;&#21644;&#29983;&#29289;&#21464;&#37327;&#20013;&#30340;&#24322;&#24120;&#24615;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#30740;&#31350;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#30142;&#30149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24120;&#35265;&#31070;&#32463;&#30142;&#30149;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#30142;&#30149;&#24322;&#36136;&#24615;&#65292;&#21253;&#25324;&#30149;&#22240;&#12289;&#31070;&#32463;&#25104;&#20687;&#29305;&#24449;&#12289;&#21512;&#24182;&#30151;&#25110;&#22522;&#22240;&#21464;&#24322;&#30340;&#24046;&#24322;&#12290;&#35268;&#33539;&#24314;&#27169;&#24050;&#25104;&#20026;&#30740;&#31350;&#36825;&#31181;&#20154;&#32676;&#30340;&#27969;&#34892;&#26041;&#27861;&#65292;&#20854;&#20013;&#23545;&#29983;&#29702;&#31995;&#32479;&#30340;&#8220;&#27491;&#24120;&#8221;&#34892;&#20026;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#20010;&#20307;&#23618;&#38754;&#19978;&#26816;&#27979;&#19982;&#30142;&#30149;&#30149;&#29702;&#30456;&#20851;&#30340;&#20559;&#24046;&#12290;&#23545;&#20110;&#35768;&#22810;&#24322;&#36136;&#24615;&#30142;&#30149;&#65292;&#25105;&#20204;&#39044;&#35745;&#20250;&#35266;&#23519;&#21040;&#22810;&#31181;&#31070;&#32463;&#25104;&#20687;&#21644;&#29983;&#29289;&#21464;&#37327;&#30340;&#24322;&#24120;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#35268;&#33539;&#27169;&#22411;&#20027;&#35201;&#26159;&#20026;&#20102;&#30740;&#31350;&#21333;&#19968;&#25104;&#20687;&#27169;&#24577;&#32780;&#24320;&#21457;&#30340;&#12290;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#22810;&#27169;&#24577;&#35268;&#33539;&#24314;&#27169;&#26694;&#26550;&#65292;&#22312;&#22810;&#20010;&#27169;&#24577;&#30340;&#21464;&#37327;&#20013;&#32858;&#21512;&#24322;&#24120;&#24615;&#65292;&#24182;&#19988;&#27604;&#21333;&#27169;&#24335;&#22522;&#32447;&#26356;&#33021;&#26816;&#27979;&#21040;&#20559;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#26816;&#27979;T1&#21644;DTI&#25968;&#25454;&#20013;&#30340;&#20010;&#20307;&#23618;&#38754;&#20559;&#24046;&#30340;&#22810;&#27169;&#24577;VAE&#35268;&#33539;&#27169;&#22411;&#12290;&#19982;&#21333;&#27169;&#24335;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#26816;&#27979;&#21040;&#36731;&#24230;&#35748;&#30693;&#21463;&#25439;&#30340;&#21463;&#35797;&#32773;&#20013;&#30340;&#20559;&#24046;&#65292;&#35777;&#26126;&#20102;&#22810;&#27169;&#24577;&#35268;&#33539;&#24314;&#27169;&#29992;&#20110;&#30142;&#30149;&#24322;&#36136;&#24615;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the challenges of studying common neurological disorders is disease heterogeneity including differences in causes, neuroimaging characteristics, comorbidities, or genetic variation. Normative modelling has become a popular method for studying such cohorts where the 'normal' behaviour of a physiological system is modelled and can be used at subject level to detect deviations relating to disease pathology. For many heterogeneous diseases, we expect to observe abnormalities across a range of neuroimaging and biological variables. However, thus far, normative models have largely been developed for studying a single imaging modality. We aim to develop a multi-modal normative modelling framework where abnormality is aggregated across variables of multiple modalities and is better able to detect deviations than uni-modal baselines. We propose two multi-modal VAE normative models to detect subject level deviations across T1 and DTI data. Our proposed models were better able to detect di
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;SUPMER&#65292;&#21253;&#25324;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;&#65292;&#36890;&#36807;&#38170;&#23450;&#30340;&#20803;&#35757;&#32451;&#20219;&#21153;&#21644;&#22522;&#20110;&#35838;&#31243;&#30340;&#20219;&#21153;&#22686;&#24378;&#20016;&#23500;&#20102;&#20219;&#21153;&#20998;&#24067;&#65292;&#35299;&#20915;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#33391;&#22909;&#21021;&#22987;&#21270;&#36719;&#25552;&#31034;&#21644;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12314</link><description>&lt;p&gt;
&#20855;&#26377;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#30340;&#33258;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization. (arXiv:2303.12314v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12314
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;SUPMER&#65292;&#21253;&#25324;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;&#65292;&#36890;&#36807;&#38170;&#23450;&#30340;&#20803;&#35757;&#32451;&#20219;&#21153;&#21644;&#22522;&#20110;&#35838;&#31243;&#30340;&#20219;&#21153;&#22686;&#24378;&#20016;&#23500;&#20102;&#20219;&#21153;&#20998;&#24067;&#65292;&#35299;&#20915;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#33391;&#22909;&#21021;&#22987;&#21270;&#36719;&#25552;&#31034;&#21644;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#26159;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#36719;&#25552;&#31034;&#24182;&#20351;&#20923;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#23613;&#31649;&#26377;&#25928;&#65292;&#20294;&#25552;&#31034;&#35843;&#25972;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#19968;&#26041;&#38754;&#20005;&#37325;&#20381;&#36182;&#20110;&#33391;&#22909;&#30340;&#36719;&#25552;&#31034;&#21021;&#22987;&#21270;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#24456;&#23481;&#26131;&#23548;&#33268;&#36807;&#24230;&#25311;&#21512;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#25110;&#30417;&#30563;&#20803;&#23398;&#20064;&#26469;&#21021;&#22987;&#21270;&#36719;&#25552;&#31034;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#23545;&#26410;&#35265;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#25968;&#25454;&#26377;&#25928;&#30340;&#27867;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;&#65288;SUPMER&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#32452;&#33258;&#30417;&#30563;&#38170;&#23450;&#30340;&#20803;&#35757;&#32451;&#20219;&#21153;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#20219;&#21153;&#26684;&#24335;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#35838;&#31243;&#30340;&#20219;&#21153;&#22686;&#24378;&#36827;&#19968;&#27493;&#20016;&#23500;&#20102;&#20219;&#21153;&#20998;&#24067;&#12290;&#28982;&#21518;&#23558;&#19968;&#31181;&#26032;&#30340;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;&#38598;&#25104;&#21040;&#20803;&#25552;&#31034;&#23398;&#20064;&#20013;&#12290;&#23427;&#20803;&#23398;&#20064;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#22914;&#20309;&#36716;&#25442;&#21407;&#22987;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning is a parameter-efficient method, which learns soft prompts and conditions frozen language models to perform specific downstream tasks. Though effective, prompt tuning under few-shot settings on the one hand heavily relies on a good initialization of soft prompts. On the other hand, it can easily result in overfitting. Existing works leverage pre-training or supervised meta-learning to initialize soft prompts but they cannot data-efficiently generalize to unseen downstream tasks. To address the above problems, this paper proposes a novel Self-sUpervised meta-Prompt learning framework with meta-gradient Regularization for few-shot generalization (SUPMER). We first design a set of self-supervised anchor meta-training tasks with different task formats and further enrich the task distribution with curriculum-based task augmentation. Then a novel meta-gradient regularization method is integrated into meta-prompt learning. It meta-learns to transform the raw gradients during few
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#20855;&#26377;&#37325;&#23614;&#22122;&#22768;&#30340;&#38543;&#26426;&#38750;&#20809;&#28369;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22635;&#34917;&#20102;&#22312;&#20989;&#25968;&#38750;&#20809;&#28369;&#22330;&#26223;&#19979;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2303.12277</link><description>&lt;p&gt;
&#24102;&#26377;&#37325;&#23614;&#22122;&#22768;&#30340;&#38543;&#26426;&#38750;&#20809;&#28369;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stochastic Nonsmooth Convex Optimization with Heavy-Tailed Noises. (arXiv:2303.12277v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#20855;&#26377;&#37325;&#23614;&#22122;&#22768;&#30340;&#38543;&#26426;&#38750;&#20809;&#28369;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22635;&#34917;&#20102;&#22312;&#20989;&#25968;&#38750;&#20809;&#28369;&#22330;&#26223;&#19979;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#23558;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#32771;&#34385;&#22312;&#37325;&#23614;&#22122;&#22768;&#33539;&#24335;&#19979;&#65292;&#21363;&#20551;&#35774;&#38543;&#26426;&#26799;&#24230;&#21644;&#30495;&#23454;&#26799;&#24230;&#20043;&#38388;&#30340;&#24046;&#24322;&#20855;&#26377;&#26377;&#38480;&#30340; $p$ &#38454;&#30697;&#65288;&#20363;&#22914;&#34987;&#26576;&#20010; $\sigma \geq0$ &#19978;&#30028;&#38480;&#21046;&#20026; $\sigma^{p}$&#65289;&#65292;&#20854;&#20013; $p\in (1,2]$&#65292;&#36825;&#19981;&#20165;&#27867;&#21270;&#20102;&#20256;&#32479;&#30340;&#26377;&#38480;&#26041;&#24046;&#20551;&#35774;&#65288;$p=2$&#65289;&#65292;&#32780;&#19988;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#37117;&#34987;&#35266;&#23519;&#21040;&#12290;&#22312;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20551;&#35774;&#19979;&#65292;&#38024;&#23545;&#20984;&#25110;&#38750;&#20984;&#38382;&#39064;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22810;&#26032;&#36827;&#23637;&#65292;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#21482;&#32771;&#34385;&#20809;&#28369;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#30456;&#21453;&#65292;&#22312;&#20989;&#25968;&#38750;&#20809;&#28369;&#26102;&#65292;&#20154;&#20204;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#24182;&#23436;&#20840;&#29702;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23545;&#24102;&#26377;&#37325;&#23614;&#22122;&#22768;&#30340;&#38543;&#26426;&#38750;&#20809;&#28369;&#20984;&#20248;&#21270;&#25552;&#20379;&#20840;&#38754;&#20998;&#26512;&#26469;&#22635;&#34917;&#36825;&#19968;&#20851;&#38190;&#31354;&#30333;&#12290;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20110;&#35009;&#21098;&#30340;&#31639;&#27861;&#65292;&#28982;&#32780;&#65292;&#36825;&#20010;&#31639;&#27861;&#21482;&#34987;&#35777;&#26126;&#33021;&#20197;&#26399;&#26395;&#26041;&#24335;&#25910;&#25947;&#65292;&#20294;&#22312;&#38468;&#21152;
&lt;/p&gt;
&lt;p&gt;
Recently, several studies consider the stochastic optimization problem but in a heavy-tailed noise regime, i.e., the difference between the stochastic gradient and the true gradient is assumed to have a finite $p$-th moment (say being upper bounded by $\sigma^{p}$ for some $\sigma\geq0$) where $p\in(1,2]$, which not only generalizes the traditional finite variance assumption ($p=2$) but also has been observed in practice for several different tasks. Under this challenging assumption, lots of new progress has been made for either convex or nonconvex problems, however, most of which only consider smooth objectives. In contrast, people have not fully explored and well understood this problem when functions are nonsmooth. This paper aims to fill this crucial gap by providing a comprehensive analysis of stochastic nonsmooth convex optimization with heavy-tailed noises. We revisit a simple clipping-based algorithm, whereas, which is only proved to converge in expectation but under the additi
&lt;/p&gt;</description></item><item><title>STDLens &#26159;&#19968;&#31181;&#21487;&#20197;&#38450;&#27490;FL&#21463;&#21040;&#27169;&#22411;&#25375;&#25345;&#30340;&#25915;&#20987;&#30340;&#23433;&#20840;&#26041;&#27861;&#12290;&#23427;&#22522;&#20110;&#19977;&#23618;&#30340;&#21462;&#35777;&#26694;&#26550;&#26469;&#35782;&#21035;&#21644;&#25490;&#38500;&#29305;&#27530;&#30340;&#26799;&#24230;&#65292;&#24182;&#24674;&#22797;FL&#30340;&#24615;&#33021;&#12290;STDLens&#22312;&#29289;&#20307;&#26816;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#24182;&#19988;&#20855;&#26377;&#38450;&#27490;&#27169;&#22411;&#25375;&#25345;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11511</link><description>&lt;p&gt;
STDLens&#65306;&#22522;&#20110;&#27169;&#22411;&#25375;&#25345;&#30340;&#29289;&#20307;&#26816;&#27979;&#32852;&#37030;&#23398;&#20064;&#30340;&#23433;&#20840;&#38450;&#25252;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
STDLens: Model Hijacking-resilient Federated Learning for Object Detection. (arXiv:2303.11511v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11511
&lt;/p&gt;
&lt;p&gt;
STDLens &#26159;&#19968;&#31181;&#21487;&#20197;&#38450;&#27490;FL&#21463;&#21040;&#27169;&#22411;&#25375;&#25345;&#30340;&#25915;&#20987;&#30340;&#23433;&#20840;&#26041;&#27861;&#12290;&#23427;&#22522;&#20110;&#19977;&#23618;&#30340;&#21462;&#35777;&#26694;&#26550;&#26469;&#35782;&#21035;&#21644;&#25490;&#38500;&#29305;&#27530;&#30340;&#26799;&#24230;&#65292;&#24182;&#24674;&#22797;FL&#30340;&#24615;&#33021;&#12290;STDLens&#22312;&#29289;&#20307;&#26816;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#24182;&#19988;&#20855;&#26377;&#38450;&#27490;&#27169;&#22411;&#25375;&#25345;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#21327;&#21516;&#23398;&#20064;&#26694;&#26550;&#22312;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#20013;&#35757;&#32451;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#24050;&#32463;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#23613;&#31649;&#23427;&#20855;&#26377;&#35832;&#22810;&#20248;&#28857;&#65292;FL&#23481;&#26131;&#21463;&#21040;&#27169;&#22411;&#25375;&#25345;&#30340;&#25915;&#20987;&#12290;&#25915;&#20987;&#32773;&#21487;&#20197;&#20165;&#20165;&#21033;&#29992;&#19968;&#23567;&#37096;&#20998;&#21487;&#20197;&#34987;&#25915;&#20987;&#30340;&#23458;&#25143;&#31471;&#25511;&#21046;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#30340;&#27491;&#30830;&#24615;&#65292;&#36890;&#36807;&#26893;&#20837;&#29305;&#27530;&#26799;&#24230;&#23454;&#29616;&#25915;&#20987;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STDLens&#30340;&#23433;&#20840;&#26041;&#27861;&#20197;&#20445;&#25252;FL&#20813;&#21463;&#27492;&#31867;&#25915;&#20987;&#12290;&#25105;&#20204;&#39318;&#20808;&#35843;&#26597;&#29616;&#26377;&#30340;&#32531;&#35299;&#26426;&#21046;&#24182;&#20998;&#26512;&#23427;&#20204;&#22312;&#31354;&#38388;&#32858;&#31867;&#20998;&#26512;&#26799;&#24230;&#26102;&#30001;&#20110;&#22266;&#26377;&#35823;&#24046;&#32780;&#20135;&#29983;&#30340;&#22833;&#36133;&#24773;&#20917;&#12290;&#22522;&#20110;&#36825;&#20123;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#30340;&#21462;&#35777;&#26694;&#26550;&#26469;&#35782;&#21035;&#21644;&#25490;&#38500;&#36825;&#31181;&#29305;&#27530;&#30340;&#26799;&#24230;&#65292;&#24182;&#22312;FL&#36807;&#31243;&#20013;&#24674;&#22797;&#24615;&#33021;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#33258;&#36866;&#24212;&#25915;&#20987;&#65292;&#24182;&#23637;&#31034;&#20102;STDLens&#23545;&#39640;&#32423;&#23545;&#25163;&#20855;&#26377;&#30340;&#31283;&#20581;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;STDLens&#22312;&#29289;&#20307;&#26816;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#38450;&#27490;&#27169;&#22411;&#25375;&#25345;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has been gaining popularity as a collaborative learning framework to train deep learning-based object detection models over a distributed population of clients. Despite its advantages, FL is vulnerable to model hijacking. The attacker can control how the object detection system should misbehave by implanting Trojaned gradients using only a small number of compromised clients in the collaborative learning process. This paper introduces STDLens, a principled approach to safeguarding FL against such attacks. We first investigate existing mitigation mechanisms and analyze their failures caused by the inherent errors in spatial clustering analysis on gradients. Based on the insights, we introduce a three-tier forensic framework to identify and expel Trojaned gradients and reclaim the performance over the course of FL. We consider three types of adaptive attacks and demonstrate the robustness of STDLens against advanced adversaries. Extensive experiments show that STD
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986; Reflexion &#26041;&#27861;&#65292;&#32473;&#26234;&#33021;&#20307;&#36171;&#20104;&#20102;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#20854;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.11366</link><description>&lt;p&gt;
Reflexion&#65306;&#20855;&#26377;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Reflexion: an autonomous agent with dynamic memory and self-reflection. (arXiv:2303.11366v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986; Reflexion &#26041;&#27861;&#65292;&#32473;&#26234;&#33021;&#20307;&#36171;&#20104;&#20102;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#20854;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20915;&#31574;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#30340;&#21457;&#23637;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20869;&#37096;&#27169;&#22411;&#24494;&#35843;&#12289;&#22806;&#37096;&#27169;&#22411;&#24494;&#35843;&#25110;&#22312;&#23450;&#20041;&#30340;&#29366;&#24577;&#31354;&#38388;&#19978;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#12290;&#30001;&#20110;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#25110;&#32570;&#20047;&#33391;&#22909;&#23450;&#20041;&#30340;&#29366;&#24577;&#31354;&#38388;&#65292;&#23454;&#29616;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#20195;&#29702;&#27809;&#26377;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#22266;&#26377;&#30340;&#26576;&#20123;&#21697;&#36136;&#65292;&#29305;&#21035;&#26159;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#21453;&#24605;&#65292;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#35797;&#38169;&#36807;&#31243;&#39640;&#25928;&#22320;&#35299;&#20915;&#26032;&#30340;&#38382;&#39064;&#12290;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986; Reflexion&#65292;&#19968;&#31181;&#23558;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#36171;&#20104;&#26234;&#33021;&#20307;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20854;&#29616;&#26377;&#30340;&#25512;&#29702;&#36712;&#36857;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, specifically the ability to learn from mistakes. Self-reflection allows humans to efficiently solve novel problems through a process of trial and error. Building on recent research, we propose Reflexion, an approach that endows an agent with dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities. To achieve full automation, we introduce a straightforward yet effective 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21322;&#30417;&#30563;Few-Shot&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#65292;&#21457;&#29616;&#26410;&#26631;&#35760;&#25968;&#25454;&#23545;&#25552;&#39640;&#21322;&#30417;&#30563;FSOD&#26377;&#22909;&#22788;&#12290;&#21463;&#27492;&#21457;&#29616;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SoftER Teacher&#65292;&#19968;&#31181;&#24378;&#22823;&#30340;&#26816;&#27979;&#22120;&#65292;&#29992;&#20110;&#25913;&#36827;FSOD&#65292;&#19981;&#38656;&#35201;&#20016;&#23500;&#30340;&#26631;&#31614;&#65292;&#24182;&#33021;&#22312;&#24615;&#33021;&#26041;&#38754;&#36229;&#36234;&#24378;&#26377;&#21147;&#30340;&#30417;&#30563;&#26816;&#27979;&#22120;&#65292;&#32780;&#19988;&#19981;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2303.05739</link><description>&lt;p&gt;
&#20351;&#29992;SoftER Teacher&#22686;&#24378;&#21322;&#30417;&#30563;Few-Shot&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Boosting Semi-Supervised Few-Shot Object Detection with SoftER Teacher. (arXiv:2303.05739v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21322;&#30417;&#30563;Few-Shot&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#65292;&#21457;&#29616;&#26410;&#26631;&#35760;&#25968;&#25454;&#23545;&#25552;&#39640;&#21322;&#30417;&#30563;FSOD&#26377;&#22909;&#22788;&#12290;&#21463;&#27492;&#21457;&#29616;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SoftER Teacher&#65292;&#19968;&#31181;&#24378;&#22823;&#30340;&#26816;&#27979;&#22120;&#65292;&#29992;&#20110;&#25913;&#36827;FSOD&#65292;&#19981;&#38656;&#35201;&#20016;&#23500;&#30340;&#26631;&#31614;&#65292;&#24182;&#33021;&#22312;&#24615;&#33021;&#26041;&#38754;&#36229;&#36234;&#24378;&#26377;&#21147;&#30340;&#30417;&#30563;&#26816;&#27979;&#22120;&#65292;&#32780;&#19988;&#19981;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Few-shot&#30446;&#26631;&#26816;&#27979;&#65288;FSOD&#65289;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#20174;&#23569;&#37327;&#26679;&#26412;&#20013;&#26816;&#27979;&#26032;&#27010;&#24565;&#12290;&#29616;&#26377;&#30340;FSOD&#26041;&#27861;&#20551;&#23450;&#26377;&#20016;&#23500;&#30340;&#22522;&#30784;&#26631;&#31614;&#26469;&#36866;&#24212;&#26032;&#23545;&#35937;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21322;&#30417;&#30563;FSOD&#20219;&#21153;&#65292;&#32771;&#34385;&#21040;&#22522;&#30784;&#21644;&#26032;&#26631;&#31614;&#21516;&#26102;&#24456;&#23569;&#30340;&#29616;&#23454;&#24773;&#20917;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#21457;&#29616;&#20854;&#36890;&#36807;&#21306;&#22495;&#25552;&#35758;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#21322;&#30417;&#30563;FSOD&#30340;&#26174;&#30528;&#33021;&#21147;&#12290;&#21463;&#27492;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SoftER Teacher&#65292;&#19968;&#31181;&#32467;&#21512;&#21306;&#22495;&#25552;&#35758;&#19978;&#30340;&#20266;&#26631;&#35760;&#21644;&#34920;&#31034;&#23398;&#20064;&#30340;&#24378;&#22823;&#26816;&#27979;&#22120;&#65292;&#20197;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#25913;&#36827;FSOD&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20016;&#23500;&#30340;&#26631;&#31614;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SoftER Teacher&#36229;&#36234;&#20102;&#24378;&#26377;&#21147;&#30340;&#30417;&#30563;&#26816;&#27979;&#22120;&#30340;&#26032;&#24615;&#33021;&#65292;&#20165;&#20351;&#29992;&#25152;&#38656;&#22522;&#30784;&#26631;&#31614;&#30340;10&#65285;&#65292;&#32780;&#19981;&#20250;&#20986;&#29616;&#20043;&#21069;&#26041;&#27861;&#20013;&#35266;&#23519;&#21040;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#25581;&#31034;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;Few-Shot&#30446;&#26631;&#26816;&#27979;&#20043;&#38388;&#28508;&#22312;&#20851;&#31995;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot object detection (FSOD) is an emerging problem aimed at detecting novel concepts from few exemplars. Existing approaches to FSOD assume abundant base labels to adapt to novel objects. This paper studies the task of semi-supervised FSOD by considering a realistic scenario in which both base and novel labels are simultaneously scarce. We explore the utility of unlabeled data and discover its remarkable ability to boost semi-supervised FSOD by way of region proposals. Motivated by this finding, we introduce SoftER Teacher, a robust detector combining pseudo-labeling with representation learning on region proposals, to harness unlabeled data for improved FSOD without relying on abundant labels. Extensive experiments show that SoftER Teacher surpasses the novel performance of a strong supervised detector using only 10% of required base labels, without experiencing catastrophic forgetting observed in prior approaches. Our work also sheds light on a potential relationship between sem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#25552;&#21319;&#30340;&#24046;&#20998;&#38544;&#31169;&#32447;&#24615;&#22238;&#24402;&#26032;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#21098;&#20999;&#38408;&#20540;&#22266;&#23450;&#30340;&#24773;&#20917;&#19979;&#22987;&#32456;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.03451</link><description>&lt;p&gt;
&#21033;&#29992;&#26799;&#24230;&#25552;&#21319;&#25913;&#36827;&#24046;&#20998;&#38544;&#31169;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Improved Differentially Private Regression via Gradient Boosting. (arXiv:2303.03451v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#25552;&#21319;&#30340;&#24046;&#20998;&#38544;&#31169;&#32447;&#24615;&#22238;&#24402;&#26032;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#21098;&#20999;&#38408;&#20540;&#22266;&#23450;&#30340;&#24773;&#20917;&#19979;&#22987;&#32456;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37325;&#26032;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#24179;&#26041;&#35823;&#24046;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#23545;&#20110;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#26159;&#25935;&#24863;&#30340;&#65292;&#21253;&#25324;&#8220;&#21098;&#20999;&#38408;&#20540;&#8221;&#65292;&#19981;&#33021;&#20197;&#25968;&#25454;&#29420;&#31435;&#30340;&#26041;&#24335;&#36827;&#34892;&#26368;&#20339;&#35774;&#32622;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#25552;&#21319;&#30340;&#24046;&#20998;&#38544;&#31169;&#32447;&#24615;&#22238;&#24402;&#26032;&#31639;&#27861;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#23558;&#21098;&#20999;&#38408;&#20540;&#22266;&#23450;&#20026;&#19981;&#30693;&#36947;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#32780;&#19988;&#65292;&#21363;&#20351;&#25105;&#20204;&#19981;&#20197;&#38544;&#31169;&#26041;&#24335;&#20248;&#21270;&#31454;&#20105;&#31639;&#27861;&#30340;&#36229;&#21442;&#25968;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20063;&#19981;&#20250;&#26356;&#24046;&#65292;&#32780;&#19988;&#36890;&#24120;&#26356;&#22909;&#12290;&#38500;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#23519;&#20197;&#35299;&#37322;&#36825;&#31181;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit the problem of differentially private squared error linear regression. We observe that existing state-of-the-art methods are sensitive to the choice of hyperparameters -- including the ``clipping threshold'' that cannot be set optimally in a data-independent way. We give a new algorithm for private linear regression based on gradient boosting. We show that our method consistently improves over the previous state of the art when the clipping threshold is taken to be fixed without knowledge of the data, rather than optimized in a non-private way -- and that even when we optimize the hyperparameters of competitor algorithms non-privately, our algorithm is no worse and often better. In addition to a comprehensive set of experiments, we give theoretical insights to explain this behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25913;&#36827;&#20102;&#20998;&#24067;&#24335;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#25552;&#20986;&#20102;&#31283;&#20581;&#20998;&#38454;&#27573;&#20215;&#20540;&#23398;&#20064;&#65288;RPVL&#65289;&#31639;&#27861;&#26469;&#35299;&#20915;&#34920;&#26684;&#21095;&#24773;&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.02783</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#30028;&#38480;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improved Sample Complexity Bounds for Distributionally Robust Reinforcement Learning. (arXiv:2303.02783v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25913;&#36827;&#20102;&#20998;&#24067;&#24335;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#25552;&#20986;&#20102;&#31283;&#20581;&#20998;&#38454;&#27573;&#20215;&#20540;&#23398;&#20064;&#65288;RPVL&#65289;&#31639;&#27861;&#26469;&#35299;&#20915;&#34920;&#26684;&#21095;&#24773;&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#35757;&#32451;&#29615;&#22659;&#19982;&#27979;&#35797;&#29615;&#22659;&#20043;&#38388;&#21442;&#25968;&#19981;&#21305;&#37197;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#20854;&#21046;&#23450;&#20026;&#19968;&#20010;&#20998;&#24067;&#24335;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;(DR-RL)&#38382;&#39064;&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#23398;&#20064;&#22312;&#19981;&#30830;&#23450;&#38598;&#20013;&#38024;&#23545;&#29615;&#22659;&#26368;&#22351;&#30340;&#38543;&#26426;&#27169;&#22411;&#19979;&#26368;&#22823;&#21270;&#20215;&#20540;&#20989;&#25968;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#34920;&#26684;&#21095;&#24773;&#23398;&#20064;&#29615;&#22659;&#65292;&#22312;&#19981;&#30830;&#23450;&#38598;&#34987;&#23450;&#20041;&#22312;&#21517;&#20041;&#65288;&#35757;&#32451;&#65289;&#29615;&#22659;&#30340;&#29983;&#25104;&#27169;&#22411;&#21608;&#22260;&#30340;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#21487;&#20197;&#35775;&#38382;&#35813;&#29615;&#22659;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31283;&#20581;&#20998;&#38454;&#27573;&#20215;&#20540;&#23398;&#20064;(RPVL)&#31639;&#27861;&#26469;&#35299;&#20915;&#29992;&#22235;&#31181;&#19981;&#21516;&#21457;&#25955;&#24230;&#25351;&#23450;&#30340;&#19981;&#30830;&#23450;&#38598;&#30340;&#38382;&#39064;: &#20840;&#21464;&#20998;&#12289;&#21345;&#26041;&#12289;Kullback-Leibler&#21644;Wasserstein&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#36798;&#21040;&#20102; $\tilde{\mathcal{O}}(|\mathcal{S}||\mathcal{A}| H^{5})$ &#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#36825;&#27604;&#29616;&#26377;&#32467;&#26524;&#24179;&#22343;&#22909;&#20102;&#19968;&#20493;&#30340; $|\mathcal{S}|$
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning a control policy that is robust against the parameter mismatches between the training environment and testing environment. We formulate this as a distributionally robust reinforcement learning (DR-RL) problem where the objective is to learn the policy which maximizes the value function against the worst possible stochastic model of the environment in an uncertainty set. We focus on the tabular episodic learning setting where the algorithm has access to a generative model of the nominal (training) environment around which the uncertainty set is defined. We propose the Robust Phased Value Learning (RPVL) algorithm to solve this problem for the uncertainty sets specified by four different divergences: total variation, chi-square, Kullback-Leibler, and Wasserstein. We show that our algorithm achieves $\tilde{\mathcal{O}}(|\mathcal{S}||\mathcal{A}| H^{5})$ sample complexity, which is uniformly better than the existing results by a factor of $|\mathcal{S}|
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#39532;&#23572;&#31185;&#22827;&#25277;&#26679;&#26041;&#26696;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;MC-SAG&#31639;&#27861;&#23454;&#29616;&#20102;&#29992;&#20110;&#20998;&#24067;&#24335;&#31639;&#27861;&#30340;&#36890;&#20449;&#25928;&#29575;&#39640;&#30340;token &#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.14428</link><description>&lt;p&gt;
&#22522;&#20110;&#39532;&#23572;&#31185;&#22827;&#25277;&#26679;&#26041;&#26696;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Descent under Markovian Sampling Schemes. (arXiv:2302.14428v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#39532;&#23572;&#31185;&#22827;&#25277;&#26679;&#26041;&#26696;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;MC-SAG&#31639;&#27861;&#23454;&#29616;&#20102;&#29992;&#20110;&#20998;&#24067;&#24335;&#31639;&#27861;&#30340;&#36890;&#20449;&#25928;&#29575;&#39640;&#30340;token &#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21464;&#24418;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#20854;&#20013;&#20248;&#21270;&#22120;&#21482;&#33021;&#35775;&#38382;&#39532;&#23572;&#31185;&#22827;&#25277;&#26679;&#26041;&#26696;&#12290;&#36825;&#20123;&#26041;&#26696;&#28085;&#30422;&#20174;&#20855;&#26377;&#38543;&#26426;&#34892;&#36208;&#32773;&#65288;token&#31639;&#27861;&#65289;&#30340;&#20998;&#25955;&#20248;&#21270;&#21040;RL&#21644;&#22312;&#32447;&#31995;&#32479;&#35782;&#21035;&#38382;&#39064;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#22312;&#23545;&#22522;&#30784;&#39532;&#23572;&#31185;&#22827;&#38142;&#21644;&#20248;&#21270;&#20989;&#25968;&#26045;&#21152;&#26368;&#19981;&#38480;&#21046;&#24615;&#30340;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#39318;&#20808;&#25581;&#31034;&#20102;&#26679;&#26412;&#38543;&#26426;&#26799;&#24230;&#27839;&#30528;&#39532;&#23572;&#21487;&#22827;&#38142;&#36335;&#24452;&#25277;&#26679;&#30340;&#26041;&#27861;&#30340;&#29702;&#35770;&#19979;&#30028;&#65292;&#20351;&#20986;&#29616;&#20102;&#23545;&#22522;&#30784;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#21629;&#20013;&#26102;&#38388;&#30340;&#20381;&#36182;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27604;&#20043;&#21069;&#20316;&#21697;&#26356;&#28201;&#21644;&#30340;&#35268;&#24459;&#24615;&#20551;&#35774;&#19979;&#30340;Markov&#38142;SGD&#65288;MC-SGD&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MC-SAG&#65292;&#36825;&#26159;MC-SGD&#30340;&#19968;&#31181;&#24102;&#26377;&#26041;&#24046;&#32553;&#20943;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20165;&#21462;&#20915;&#20110;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#30896;&#25758;&#26102;&#38388;&#65292;&#22240;&#27492;&#33719;&#24471;&#20102;&#36890;&#20449;&#25928;&#29575;&#39640;&#30340;token &#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a variation of vanilla stochastic gradient descent where the optimizer only has access to a Markovian sampling scheme. These schemes encompass applications that range from decentralized optimization with a random walker (token algorithms), to RL and online system identification problems. We focus on obtaining rates of convergence under the least restrictive assumptions possible on the underlying Markov chain and on the functions optimized. We first unveil the theoretical lower bound for methods that sample stochastic gradients along the path of a Markov chain, making appear a dependency in the hitting time of the underlying Markov chain. We then study Markov chain SGD (MC-SGD) under much milder regularity assumptions than prior works. We finally introduce MC-SAG, an alternative to MC-SGD with variance reduction, that only depends on the hitting time of the Markov chain, therefore obtaining a communication-efficient token algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#23376;&#32676;&#20307;&#36716;&#21464;&#30340;&#21508;&#31181;&#26426;&#21046;&#65292;&#23545;20&#20010;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#22312;12&#20010;&#39046;&#22495;&#20869;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#29616;&#26377;&#31639;&#27861;&#21482;&#33021;&#24212;&#23545;&#26576;&#20123;&#36716;&#21464;&#65292;&#36827;&#19968;&#27493;&#22320;&#65292;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#26131;&#34892;&#30340;&#36873;&#25321;&#26631;&#20934;&#26469;&#25913;&#21892;&#29616;&#26377;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.12254</link><description>&lt;p&gt;
&#25913;&#21464;&#24456;&#38590;&#65306;&#23376;&#32676;&#20307;&#36716;&#21464;&#30340;&#28145;&#20837;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Change is Hard: A Closer Look at Subpopulation Shift. (arXiv:2302.12254v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#23376;&#32676;&#20307;&#36716;&#21464;&#30340;&#21508;&#31181;&#26426;&#21046;&#65292;&#23545;20&#20010;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#22312;12&#20010;&#39046;&#22495;&#20869;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#29616;&#26377;&#31639;&#27861;&#21482;&#33021;&#24212;&#23545;&#26576;&#20123;&#36716;&#21464;&#65292;&#36827;&#19968;&#27493;&#22320;&#65292;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#26131;&#34892;&#30340;&#36873;&#25321;&#26631;&#20934;&#26469;&#25913;&#21892;&#29616;&#26377;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#23376;&#32676;&#20307;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23548;&#33268;&#23376;&#32676;&#20307;&#36716;&#21464;&#30340;&#26426;&#21046;&#20197;&#21450;&#31639;&#27861;&#22312;&#22914;&#27492;&#19981;&#21516;&#30340;&#36716;&#21464;&#20013;&#22914;&#20309;&#36827;&#34892;&#26222;&#36941;&#21270;&#65292;&#25105;&#20204;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#23376;&#32676;&#20307;&#36716;&#21464;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#21078;&#26512;&#21644;&#35299;&#37322;&#23376;&#32676;&#20307;&#20013;&#30340;&#24120;&#35265;&#36716;&#21464;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#21307;&#30103;&#39046;&#22495;&#30340;12&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23545;20&#20010;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#35757;&#32451;10,000&#22810;&#20010;&#27169;&#22411;&#24471;&#21040;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#26410;&#26469;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#36827;&#23637;&#30340;&#26377;&#36259;&#35266;&#23519;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#29616;&#26377;&#31639;&#27861;&#20165;&#33021;&#22312;&#26576;&#20123;&#31867;&#22411;&#30340;&#36716;&#21464;&#19978;&#25552;&#39640;&#23376;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#22312;&#20854;&#20182;&#31867;&#22411;&#30340;&#36716;&#21464;&#19978;&#21017;&#19981;&#33021;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#24403;&#21069;&#31639;&#27861;&#20381;&#36182;&#20110;&#32676;&#20307;&#26631;&#27880;&#30340;&#39564;&#35777;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#26368;&#24046;&#31867;&#21035;&#20934;&#30830;&#24230;&#30340;&#31616;&#21333;&#36873;&#25321;&#26631;&#20934;&#20854;&#23454;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models often perform poorly on subgroups that are underrepresented in the training data. Yet, little is understood on the variation in mechanisms that cause subpopulation shifts, and how algorithms generalize across such diverse shifts at scale. In this work, we provide a fine-grained analysis of subpopulation shift. We first propose a unified framework that dissects and explains common shifts in subgroups. We then establish a comprehensive benchmark of 20 state-of-the-art algorithms evaluated on 12 real-world datasets in vision, language, and healthcare domains. With results obtained from training over 10,000 models, we reveal intriguing observations for future progress in this space. First, existing algorithms only improve subgroup robustness over certain types of shifts but not others. Moreover, while current algorithms rely on group-annotated validation data for model selection, we find that a simple selection criterion based on worst-class accuracy is surprisingly
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Frozen Pretrained Transformer (FPT) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36827;&#32780;&#20351;&#20854;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#22791;&#30528;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.11939</link><description>&lt;p&gt;
&#19968;&#31449;&#24335;&#35299;&#20915;&#26041;&#26696;&#65306;&#21033;&#29992;&#39044;&#35757;&#32451; LM &#36827;&#34892;&#24378;&#22823;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
One Fits All:Power General Time Series Analysis by Pretrained LM. (arXiv:2302.11939v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Frozen Pretrained Transformer (FPT) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36827;&#32780;&#20351;&#20854;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#22791;&#30528;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#21644;&#35745;&#31639;&#26426;&#35270;&#35273; (CV) &#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22312;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#26377;&#38480;&#12290;&#19982; NLP &#21644; CV &#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#20123;&#39046;&#22495;&#37319;&#29992;&#32479;&#19968;&#27169;&#22411;&#21363;&#21487;&#25191;&#34892;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#32780;&#22312;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#19987;&#38376;&#35774;&#35745;&#30340;&#26041;&#27861;&#20173;&#28982;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#22914;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#38459;&#30861;&#39044;&#35757;&#32451;&#27169;&#22411;&#21457;&#23637;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#22823;&#37327;&#29992;&#20110;&#35757;&#32451;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36991;&#20813;&#25913;&#21464;&#39044;&#35757;&#32451;&#35821;&#35328;&#25110;&#22270;&#20687;&#27169;&#22411;&#20013;&#27531;&#24046;&#22359;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#21521;&#20256;&#36882;&#23618;&#12290;&#36825;&#31181;&#27169;&#22411;&#34987;&#31216;&#20026;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120; (FPT)&#65292;&#36890;&#36807;&#23545;&#28041;&#21450;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#30340;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#31561;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;FPT &#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although we have witnessed great success of pre-trained models in natural language processing (NLP) and computer vision (CV), limited progress has been made for general time series analysis. Unlike NLP and CV where a unified model can be used to perform different tasks, specially designed approach still dominates in each time series analysis task such as classification, anomaly detection, forecasting, and few-shot learning. The main challenge that blocks the development of pre-trained model for time series analysis is the lack of a large amount of data for training. In this work, we address this challenge by leveraging language or CV models, pre-trained from billions of tokens, for time series analysis. Specifically, we refrain from altering the self-attention and feedforward layers of the residual blocks in the pre-trained language or image model. This model, known as the Frozen Pretrained Transformer (FPT), is evaluated through fine-tuning on all major types of tasks involving time s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36816;&#21160;&#31995;&#32479;&#30340;&#31163;&#25955;&#24418;&#24577;&#23545;&#31216;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#21644;&#23454;&#36341;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#31995;&#32479;&#30340;&#24418;&#24577;&#23545;&#31216;&#32676;&#65292;&#24182;&#20998;&#26512;&#23545;&#31216;&#24615;&#22312;&#25968;&#25454;&#22686;&#24378;&#21644;&#25511;&#21046;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.10433</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#31163;&#25955;&#23545;&#31216;&#24615;: &#22522;&#20110;&#32676;&#35770;&#21644;&#25968;&#25454;&#39537;&#21160;&#20998;&#26512;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On discrete symmetries of robotics systems: A group-theoretic and data-driven analysis. (arXiv:2302.10433v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36816;&#21160;&#31995;&#32479;&#30340;&#31163;&#25955;&#24418;&#24577;&#23545;&#31216;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#21644;&#23454;&#36341;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#31995;&#32479;&#30340;&#24418;&#24577;&#23545;&#31216;&#32676;&#65292;&#24182;&#20998;&#26512;&#23545;&#31216;&#24615;&#22312;&#25968;&#25454;&#22686;&#24378;&#21644;&#25511;&#21046;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#36816;&#21160;&#31995;&#32479;&#30340;&#31163;&#25955;&#24418;&#24577;&#23545;&#31216;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#36825;&#22312;&#29983;&#29289;&#21644;&#20154;&#24037;&#36816;&#21160;&#31995;&#32479;&#20013;&#32463;&#24120;&#35266;&#23519;&#21040;&#65292;&#20363;&#22914;&#22810;&#33151;&#12289;&#28216;&#27891;&#21644;&#39134;&#34892;&#30340;&#21160;&#29289;/&#26426;&#22120;&#20154;/&#34394;&#25311;&#35282;&#33394;&#12290;&#36825;&#20123;&#23545;&#31216;&#24615;&#28304;&#33258;&#31995;&#32479;&#24418;&#24577;&#20013;&#19968;&#20010;&#25110;&#22810;&#20010;&#24179;&#38754;/&#36724;&#30340;&#23545;&#31216;&#24615;&#23384;&#22312;&#65292;&#23548;&#33268;&#36523;&#20307;&#37096;&#20214;&#30340;&#35856;&#27874;&#22797;&#21046;&#21644;&#20998;&#24067;&#12290;&#25105;&#20204;&#38416;&#36848;&#20102;&#24418;&#24577;&#23545;&#31216;&#24615;&#22914;&#20309;&#24310;&#20280;&#21040;&#31995;&#32479;&#21160;&#21147;&#23398;&#12289;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#20197;&#21450;&#19982;&#31995;&#32479;&#21160;&#21147;&#23398;&#28436;&#21270;&#30456;&#20851;&#30340;&#25152;&#26377;&#26412;&#20307;&#24863;&#21644;&#22806;&#24863;&#27979;&#37327;&#20013;&#30340;&#23545;&#31216;&#24615;&#12290;&#22312;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;&#31216;&#24615;&#20195;&#34920;&#19968;&#31181;&#24402;&#32435;&#20559;&#32622;&#65292;&#35777;&#26126;&#20102;&#25968;&#25454;&#22686;&#24378;&#25110;&#23545;&#31216;&#20989;&#25968;&#36924;&#36817;&#22120;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#21644;&#23454;&#36341;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#31995;&#32479;&#30340;&#24418;&#24577;&#23545;&#31216;&#32676;G&#24182;&#25551;&#36848;&#20854;&#22312;&#26412;&#20307;&#24863;&#21644;&#22806;&#24863;&#27979;&#37327;&#12289;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#20197;&#21450;&#31995;&#32479;&#21160;&#21147;&#23398;&#26041;&#38754;&#30340;&#23545;&#31216;&#24615;&#12290;&#35813;&#26694;&#26550;&#28041;&#21450;&#21040;&#25968;&#25454;&#39537;&#21160;&#21644;&#32676;&#35770;&#24037;&#20855;&#65292;&#20363;&#22914;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#32622;&#25442;&#27979;&#35797;&#21644;&#34920;&#31034;&#29702;&#35770;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#23398;&#36890;&#36807;&#30830;&#23450;&#19968;&#20010;&#20223;&#29983;&#33034;&#26894;&#26426;&#22120;&#20154;&#30340;&#23545;&#31216;&#32676;&#24182;&#20998;&#26512;&#20854;&#23545;&#25968;&#25454;&#22686;&#24378;&#21644;&#25511;&#21046;&#35774;&#35745;&#30340;&#24433;&#21709;&#26469;&#36827;&#34892;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive study on discrete morphological symmetries of dynamical systems, which are commonly observed in biological and artificial locomoting systems, such as legged, swimming, and flying animals/robots/virtual characters. These symmetries arise from the presence of one or more planes/axis of symmetry in the system's morphology, resulting in harmonious duplication and distribution of body parts. Significantly, we characterize how morphological symmetries extend to symmetries in the system's dynamics, optimal control policies, and in all proprioceptive and exteroceptive measurements related to the system's dynamics evolution. In the context of data-driven methods, symmetry represents an inductive bias that justifies the use of data augmentation or symmetric function approximators. To tackle this, we present a theoretical and practical framework for identifying the system's morphological symmetry group $\G$ and characterizing the symmetries in proprioceptive and exteroc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#40657;&#30418;&#27169;&#22411;&#20013;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#40657;&#30418;&#27169;&#22411;&#20998;&#25104;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#28151;&#21512;&#29289;&#21644;&#27531;&#24046;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#19968;&#38454;&#36923;&#36753;&#23545;&#21487;&#35299;&#37322;&#27169;&#22411;&#36827;&#34892;&#22522;&#26412;&#25512;&#29702;&#12290;&#27492;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#19988;&#20135;&#29983;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.10289</link><description>&lt;p&gt;
&#23558;&#40657;&#21283;&#23376;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#28151;&#21512;&#29289;&#65306;&#36335;&#32447;&#35268;&#21010;&#65292;&#35299;&#37322;&#65292;&#37325;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat. (arXiv:2302.10289v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#40657;&#30418;&#27169;&#22411;&#20013;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#40657;&#30418;&#27169;&#22411;&#20998;&#25104;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#28151;&#21512;&#29289;&#21644;&#27531;&#24046;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#19968;&#38454;&#36923;&#36753;&#23545;&#21487;&#35299;&#37322;&#27169;&#22411;&#36827;&#34892;&#22522;&#26412;&#25512;&#29702;&#12290;&#27492;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#19988;&#20135;&#29983;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#35201;&#20040;&#20174;&#35299;&#37322;&#24615;&#27169;&#22411;&#24320;&#22987;&#65292;&#35201;&#20040;&#20174;&#40657;&#30418;&#24320;&#22987;&#24182;&#20107;&#21518;&#35299;&#37322;&#12290;&#40657;&#30418;&#27169;&#22411;&#28789;&#27963;&#20294;&#38590;&#20197;&#35299;&#37322;&#65292;&#32780;&#35299;&#37322;&#24615;&#27169;&#22411;&#26412;&#36136;&#19978;&#26159;&#21487;&#35299;&#37322;&#30340;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#24615;&#27169;&#22411;&#38656;&#35201;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#30693;&#35782;&#65292;&#24182;&#19988;&#24448;&#24448;&#27604;&#23427;&#20204;&#30340;&#40657;&#30418;&#21464;&#20307;&#19981;&#22815;&#28789;&#27963;&#21644;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#26088;&#22312;&#27169;&#31946;&#40657;&#30418;&#30340;&#20107;&#21518;&#35299;&#37322;&#21644;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#20043;&#38388;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#20174;&#40657;&#30418;&#24320;&#22987;&#65292;&#36845;&#20195;&#22320;Carve&#20986;&#19968;&#31181;&#28151;&#21512;&#35299;&#37322;&#27169;&#22411;&#65288;MoIE&#65289;&#21644;&#19968;&#20010;&#27531;&#20313;&#32593;&#32476;&#12290;&#27599;&#20010;&#21487;&#35299;&#37322;&#27169;&#22411;&#19987;&#38376;&#22788;&#29702;&#19968;&#20010;&#26679;&#26412;&#23376;&#38598;&#65292;&#24182;&#20351;&#29992;&#19968;&#38454;&#36923;&#36753;(FOL)&#23545;&#20854;&#36827;&#34892;&#35299;&#37322;&#65292;&#20174;&#40657;&#30418;&#20013;&#25552;&#20379;&#22522;&#26412;&#25512;&#29702;&#27010;&#24565;&#12290;&#25105;&#20204;&#36890;&#36807;&#28789;&#27963;&#30340;&#27531;&#24046;&#36335;&#30001;&#20854;&#20313;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#27531;&#36716;&#32593;&#32476;&#19978;&#37325;&#22797;&#35813;&#26041;&#27861;&#65292;&#30452;&#21040;&#25152;&#26377;&#21487;&#35299;&#37322;&#27169;&#22411;&#35299;&#37322;&#25152;&#38656;&#27604;&#20363;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#36335;&#32447;&#35268;&#21010;&#65292;&#35299;&#37322;&#21644;&#37325;&#22797;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#30446;&#21069;&#20960;&#31181;&#40657;&#21283;&#23376;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#20135;&#29983;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
ML model design either starts with an interpretable model or a Blackbox and explains it post hoc. Blackbox models are flexible but difficult to explain, while interpretable models are inherently explainable. Yet, interpretable models require extensive ML knowledge and tend to be less flexible and underperforming than their Blackbox variants. This paper aims to blur the distinction between a post hoc explanation of a Blackbox and constructing interpretable models. Beginning with a Blackbox, we iteratively carve out a mixture of interpretable experts (MoIE) and a residual network. Each interpretable model specializes in a subset of samples and explains them using First Order Logic (FOL), providing basic reasoning on concepts from the Blackbox. We route the remaining samples through a flexible residual. We repeat the method on the residual network until all the interpretable models explain the desired proportion of data. Our extensive experiments show that our route, interpret, and repeat
&lt;/p&gt;</description></item><item><title>Navya3DSeg&#26159;&#19968;&#20010;&#26032;&#30340;&#12289;&#20855;&#26377;&#22810;&#26679;&#26631;&#31614;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#65292;&#23545;&#24212;&#20110;&#22823;&#35268;&#27169;&#29983;&#20135;&#32423;&#25805;&#20316;&#39046;&#22495;&#65292;&#21487;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#24863;&#30693;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#39034;&#24207;&#25968;&#25454;&#38598;&#25286;&#20998;&#12290;</title><link>http://arxiv.org/abs/2302.08292</link><description>&lt;p&gt;
Navya3DSeg -- &#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;Navya&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#25968;&#25454;&#38598;&#21644;&#25286;&#20998;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Navya3DSeg -- Navya 3D Semantic Segmentation Dataset &amp; split generation for autonomous vehicles. (arXiv:2302.08292v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08292
&lt;/p&gt;
&lt;p&gt;
Navya3DSeg&#26159;&#19968;&#20010;&#26032;&#30340;&#12289;&#20855;&#26377;&#22810;&#26679;&#26631;&#31614;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#65292;&#23545;&#24212;&#20110;&#22823;&#35268;&#27169;&#29983;&#20135;&#32423;&#25805;&#20316;&#39046;&#22495;&#65292;&#21487;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#24863;&#30693;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#39034;&#24207;&#25968;&#25454;&#38598;&#25286;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#24863;&#30693;&#30446;&#21069;&#20005;&#37325;&#20381;&#36182;&#20110;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#38656;&#35201;&#22823;&#35268;&#27169;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#30456;&#24212;&#30340;&#31574;&#21010;&#19982;&#26631;&#27880;&#25104;&#26412;&#12290;&#19977;&#32500;&#35821;&#20041;&#25968;&#25454;&#23545;&#20110;&#26680;&#24515;&#24863;&#30693;&#20219;&#21153;&#22914;&#38556;&#30861;&#26816;&#27979;&#21644;&#33258;&#25105;&#23450;&#20301;&#38750;&#24120;&#26377;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;Navya3DSeg&#65292;&#20855;&#26377;&#22810;&#26679;&#30340;&#26631;&#31614;&#31354;&#38388;&#65292;&#23545;&#24212;&#20110;&#22823;&#35268;&#27169;&#29983;&#20135;&#32423;&#25805;&#20316;&#39046;&#22495;&#65292;&#21253;&#25324;&#26469;&#33258;13&#20010;&#22269;&#23478;&#30340;&#22478;&#24066;&#65292;&#20065;&#26449;&#65292;&#24037;&#19994;&#21306;&#21644;&#22823;&#23398;&#12290;&#23427;&#21253;&#21547;23&#20010;&#24102;&#26631;&#31614;&#24207;&#21015;&#21644;25&#20010;&#27809;&#26377;&#26631;&#31614;&#30340;&#34917;&#20805;&#24207;&#21015;&#65292;&#26088;&#22312;&#25506;&#35752;&#22522;&#20110;&#28857;&#20113;&#30340;&#33258;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#22522;&#20934;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#22810;&#26631;&#31614;&#20998;&#23618;&#30340;&#39034;&#24207;&#25968;&#25454;&#38598;&#25286;&#20998;&#29983;&#25104;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#28436;&#31034;&#20102;&#27604;SemanticKITTI&#25968;&#25454;&#38598;&#25552;&#20986;&#30340;&#21407;&#22987;&#25286;&#20998;+1.2&#65285; mIoU&#30340;&#25913;&#36827;&#12290;&#36825;&#26159;&#19968;&#20010;&#23436;&#25972;&#30340;&#35821;&#20041;&#20998;&#21106;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous driving (AD) perception today relies heavily on deep learning based architectures requiring large scale annotated datasets with their associated costs for curation and annotation. The 3D semantic data are useful for core perception tasks such as obstacle detection and ego-vehicle localization. We propose a new dataset, Navya 3D Segmentation (Navya3DSeg), with a diverse label space corresponding to a large scale production grade operational domain, including rural, urban, industrial sites and universities from 13 countries. It contains 23 labeled sequences and 25 supplementary sequences without labels, designed to explore self-supervised and semi-supervised semantic segmentation benchmarks on point clouds. We also propose a novel method for sequential dataset split generation based on iterative multi-label stratification, and demonstrated to achieve a +1.2% mIoU improvement over the original split proposed by SemanticKITTI dataset. A complete benchmark for semantic segmentati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36816;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#20250;&#35745;&#21496;&#27861;&#19987;&#23478;&#30340;&#30693;&#35782;&#21644;&#32463;&#39564;&#65292;&#39044;&#27979;&#24847;&#22823;&#21033;&#22478;&#24066;&#30340;&#36130;&#25919;&#22256;&#22659;&#12290;</title><link>http://arxiv.org/abs/2302.05780</link><description>&lt;p&gt;
&#39044;&#27979;&#36130;&#25919;&#22256;&#22659;&#19979;&#30340;&#22478;&#24066;:&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Predicting municipalities in financial distress: a machine learning approach enhanced by domain expertise. (arXiv:2302.05780v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36816;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#20250;&#35745;&#21496;&#27861;&#19987;&#23478;&#30340;&#30693;&#35782;&#21644;&#32463;&#39564;&#65292;&#39044;&#27979;&#24847;&#22823;&#21033;&#22478;&#24066;&#30340;&#36130;&#25919;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#19982;&#31169;&#33829;&#20844;&#21496;&#30340;&#30772;&#20135;&#30456;&#27604;&#21487;&#20197;&#31867;&#27604;&#65292;&#22478;&#24066;&#30340;&#36130;&#25919;&#22256;&#22659;&#23545;&#31038;&#21306;&#30340;&#31119;&#31049;&#26377;&#30528;&#26356;&#20005;&#37325;&#30340;&#24433;&#21709;&#12290;&#39044;&#27979;&#22478;&#24066;&#30340;&#36130;&#25919;&#22256;&#22659;&#21487;&#33021;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#20102;&#29702;&#35299;&#35768;&#22810;&#22240;&#32032;&#65292;&#36825;&#20123;&#22240;&#32032;&#21487;&#33021;&#20250;&#24433;&#21709;&#21040;&#22478;&#24066;&#30340;&#36130;&#25919;&#20581;&#24247;&#29366;&#20917;&#12290;&#26412;&#25991;&#36890;&#36807;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#24847;&#22823;&#21033;&#22478;&#24066;&#30340;&#36130;&#25919;&#22256;&#22659;&#12290;&#36890;&#36807;&#23558;&#20250;&#35745;&#21496;&#27861;&#19987;&#23478;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#32463;&#39564;&#32435;&#20837;&#29305;&#24449;&#25552;&#21462;&#36807;&#31243;&#65292;&#21487;&#20197;&#30830;&#20445;&#27169;&#22411;&#32771;&#34385;&#21040;&#20102;&#19982;&#22478;&#24066;&#36130;&#25919;&#20581;&#24247;&#29366;&#20917;&#30456;&#20851;&#30340;&#24191;&#27867;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#32467;&#21512;&#39046;&#22495;&#19987;&#23478;&#30340;&#30693;&#35782;&#21644;&#32463;&#39564;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#39044;&#27979;&#22478;&#24066;&#30340;&#36130;&#25919;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial distress of municipalities, although comparable to bankruptcy of private companies, has a far more serious impact on the well-being of communities. For this reason, it is essential to detect deficits as soon as possible. Predicting financial distress in municipalities can be a complex task, as it involves understanding a wide range of factors that can affect a municipality's financial health. In this paper, we evaluate machine learning models to predict financial distress in Italian municipalities. Accounting judiciary experts have specialized knowledge and experience in evaluating the financial performance, and they use a range of indicators to make their assessments. By incorporating these indicators in the feature extraction process, we can ensure that the model is taking into account a wide range of information that is relevant to the financial health of municipalities. The results of this study indicate that using machine learning models in combination with the knowledge
&lt;/p&gt;</description></item><item><title>DeepVATS&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#28145;&#24230;&#35270;&#35273;&#20998;&#26512;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#35757;&#32451;&#25513;&#30721;&#26102;&#38388;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#65292;&#37325;&#24314;&#26102;&#38388;&#24207;&#21015;&#30340;&#34917;&#19969;&#65292;&#24182;&#23558;&#27169;&#22411;&#23884;&#20837;&#20013;&#25152;&#21253;&#21547;&#30340;&#30693;&#35782;&#25237;&#24433;&#21040;&#20132;&#20114;&#24335;&#22270;&#20013;&#65292;&#20174;&#32780;&#21487;&#20197;&#36731;&#26494;&#35782;&#21035;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#21644;&#24322;&#24120;&#12290;</title><link>http://arxiv.org/abs/2302.03858</link><description>&lt;p&gt;
DeepVATS&#65306;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#30340;&#28145;&#24230;&#35270;&#35273;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
DeepVATS: Deep Visual Analytics for Time Series. (arXiv:2302.03858v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03858
&lt;/p&gt;
&lt;p&gt;
DeepVATS&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#28145;&#24230;&#35270;&#35273;&#20998;&#26512;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#35757;&#32451;&#25513;&#30721;&#26102;&#38388;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#65292;&#37325;&#24314;&#26102;&#38388;&#24207;&#21015;&#30340;&#34917;&#19969;&#65292;&#24182;&#23558;&#27169;&#22411;&#23884;&#20837;&#20013;&#25152;&#21253;&#21547;&#30340;&#30693;&#35782;&#25237;&#24433;&#21040;&#20132;&#20114;&#24335;&#22270;&#20013;&#65292;&#20174;&#32780;&#21487;&#20197;&#36731;&#26494;&#35782;&#21035;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#21644;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#35270;&#35273;&#20998;&#26512;&#65288;DVA&#65289;&#36817;&#26469;&#20852;&#36215;&#65292;&#30446;&#30340;&#26159;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25903;&#25345;&#21487;&#35270;&#20132;&#20114;&#31995;&#32479;&#65292;&#20197;&#25552;&#20379;&#22823;&#35268;&#27169;&#25968;&#25454;&#22788;&#29702;&#33021;&#21147;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#21644;&#39046;&#22495;&#20013;&#32479;&#19968;&#20854;&#23454;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DeepVATS&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#24037;&#20855;&#65292;&#23558;DVA&#39046;&#22495;&#24341;&#20837;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;DeepVATS&#20197;&#33258;&#30417;&#30563;&#26041;&#24335;&#35757;&#32451;&#25513;&#30721;&#26102;&#38388;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#65292;&#37325;&#24314;&#26102;&#38388;&#24207;&#21015;&#30340;&#34917;&#19969;&#65292;&#24182;&#23558;&#27169;&#22411;&#23884;&#20837;&#20013;&#25152;&#21253;&#21547;&#30340;&#30693;&#35782;&#25237;&#24433;&#21040;&#20132;&#20114;&#24335;&#22270;&#20013;&#65292;&#20174;&#32780;&#21487;&#20197;&#36731;&#26494;&#35782;&#21035;&#26102;&#38388;&#24207;&#21015;&#30340;&#27169;&#24335;&#21644;&#24322;&#24120;&#12290;&#24037;&#20855;&#21253;&#25324;&#29992;&#20110;&#25968;&#25454;&#22788;&#29702;&#27969;&#27700;&#32447;&#21644;&#27169;&#22411;&#35757;&#32451;&#30340;&#21518;&#31471;&#20197;&#21450;&#24102;&#26377;&#20132;&#20114;&#24335;&#29992;&#25143;&#30028;&#38754;&#30340;&#21069;&#31471;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#36816;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;DeepVATS&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;&#20195;&#30721;&#20844;&#24320;&#22312;https://github.com/vrodriguez&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Deep Visual Analytics (DVA) has recently arisen from the idea of developing Visual Interactive Systems supported by deep learning, in order to provide them with large-scale data processing capabilities and to unify their implementation across different data and domains. In this paper we present DeepVATS, an open-source tool that brings the field of DVA into time series data. DeepVATS trains, in a self-supervised way, a masked time series autoencoder that reconstructs patches of a time series, and projects the knowledge contained in the embeddings of that model in an interactive plot, from which time series patterns and anomalies emerge and can be easily spotted. The tool includes a back-end for data processing pipeline and model training, as well as a front-end with a interactive user interface. We report on results that validate the utility of DeepVATS, running experiments on both synthetic and real datasets. The code is publicly available on https://github.com/vrodriguez
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31687;&#28145;&#24230;&#23398;&#20064;&#22312;&#24037;&#31243;&#35774;&#35745;&#20013;&#24230;&#37327;&#26041;&#27861;&#30340;&#32508;&#36848;&#21644;&#25351;&#21335;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#20284;&#28982;&#24615;&#30340;&#32479;&#35745;&#24230;&#37327;&#26041;&#27861;&#22312;&#23545;&#24037;&#31243;&#24212;&#29992;&#30340;&#35201;&#27714;&#19978;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#65292;&#22240;&#27492;&#26412;&#25991;&#32534;&#36753;&#20102;&#19968;&#32452;&#20840;&#38754;&#30340;&#26032;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#24230;&#37327;&#26631;&#20934;&#30340;&#32570;&#28857;&#65292;&#24182;&#26356;&#22909;&#22320;&#19982;&#24037;&#31243;&#35774;&#35745;&#30340;&#38656;&#27714;&#30456;&#19968;&#33268;&#12290;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#22914;&#20309;&#24212;&#29992;&#20110;&#35780;&#20272;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#24037;&#31243;&#35774;&#35745;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#22312;&#25429;&#25417;&#35774;&#35745;&#30340;&#37325;&#35201;&#32454;&#24494;&#24046;&#21035;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#32479;&#35745;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2302.02913</link><description>&lt;p&gt;
&#36229;&#36234;&#32479;&#35745;&#30456;&#20284;&#24615;&#65306;&#37325;&#26032;&#24605;&#32771;&#26426;&#22120;&#23398;&#20064;&#22312;&#24037;&#31243;&#35774;&#35745;&#20013;&#30340;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Beyond Statistical Similarity: Rethinking Metrics for Deep Generative Models in Engineering Design. (arXiv:2302.02913v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31687;&#28145;&#24230;&#23398;&#20064;&#22312;&#24037;&#31243;&#35774;&#35745;&#20013;&#24230;&#37327;&#26041;&#27861;&#30340;&#32508;&#36848;&#21644;&#25351;&#21335;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#20284;&#28982;&#24615;&#30340;&#32479;&#35745;&#24230;&#37327;&#26041;&#27861;&#22312;&#23545;&#24037;&#31243;&#24212;&#29992;&#30340;&#35201;&#27714;&#19978;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#65292;&#22240;&#27492;&#26412;&#25991;&#32534;&#36753;&#20102;&#19968;&#32452;&#20840;&#38754;&#30340;&#26032;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#24230;&#37327;&#26631;&#20934;&#30340;&#32570;&#28857;&#65292;&#24182;&#26356;&#22909;&#22320;&#19982;&#24037;&#31243;&#35774;&#35745;&#30340;&#38656;&#27714;&#30456;&#19968;&#33268;&#12290;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#22914;&#20309;&#24212;&#29992;&#20110;&#35780;&#20272;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#24037;&#31243;&#35774;&#35745;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#22312;&#25429;&#25417;&#35774;&#35745;&#30340;&#37325;&#35201;&#32454;&#24494;&#24046;&#21035;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#32479;&#35745;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#65292;&#25193;&#25955;&#27169;&#22411;&#21644;Transformer&#31561;&#65292;&#22312;&#22270;&#20687;&#21644;&#35821;&#38899;&#21512;&#25104;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#33647;&#29289;&#24320;&#21457;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#24037;&#31243;&#35774;&#35745;&#38382;&#39064;&#20013;&#24212;&#29992;&#36825;&#20123;&#27169;&#22411;&#26102;&#65292;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#22522;&#20110;&#20284;&#28982;&#24615;&#30340;&#32479;&#35745;&#24230;&#37327;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#24037;&#31243;&#24212;&#29992;&#30340;&#35201;&#27714;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#31687;&#28145;&#24230;&#23398;&#20064;&#22312;&#24037;&#31243;&#35774;&#35745;&#20013;&#30340;&#24230;&#37327;&#25351;&#21335;&#21644;&#32508;&#36848;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#8220;&#32463;&#20856;&#8221;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#36825;&#20123;&#26631;&#20934;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#29702;&#35770;&#21644;&#20856;&#22411;&#30340;&#35745;&#31639;&#26426;&#24212;&#29992;&#65292;&#28982;&#21518;&#20351;&#29992;&#26696;&#20363;&#30740;&#31350;&#65292;&#24378;&#35843;&#20102;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#20026;&#20309;&#24456;&#23569;&#33021;&#22815;&#36716;&#21270;&#20026;&#35774;&#35745;&#38382;&#39064;&#20294;&#21448;&#22240;&#32570;&#20047;&#30830;&#31435;&#30340;&#26367;&#20195;&#36873;&#25321;&#32780;&#32463;&#24120;&#20351;&#29992;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#32534;&#36753;&#20102;&#19968;&#32452;&#20840;&#38754;&#30340;&#26032;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#24230;&#37327;&#26631;&#20934;&#30340;&#32570;&#28857;&#65292;&#24182;&#26356;&#22909;&#22320;&#19982;&#24037;&#31243;&#35774;&#35745;&#30340;&#38656;&#27714;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#24212;&#29992;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#24037;&#31243;&#35774;&#35745;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#20986;&#30340;&#24230;&#37327;&#26041;&#27861;&#22312;&#25429;&#25417;&#35774;&#35745;&#30340;&#37325;&#35201;&#32454;&#24494;&#24046;&#21035;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#32479;&#35745;&#24230;&#37327;&#26631;&#20934;&#65292;&#22240;&#27492;&#22312;&#24037;&#31243;&#35774;&#35745;&#24773;&#22659;&#20013;&#20026;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models, such as Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), Diffusion Models, and Transformers, have shown great promise in a variety of applications, including image and speech synthesis, natural language processing, and drug discovery. However, when applied to engineering design problems, evaluating the performance of these models can be challenging, as traditional statistical metrics based on likelihood may not fully capture the requirements of engineering applications. This paper doubles as a review and a practical guide to evaluation metrics for deep generative models (DGMs) in engineering design. We first summarize well-accepted `classic' evaluation metrics for deep generative models grounded in machine learning theory and typical computer science applications. Using case studies, we then highlight why these metrics seldom translate well to design problems but see frequent use due to the lack of established alternatives. Next, we curat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#30340;&#21512;&#25104;&#37197;&#26041;&#30340;&#30693;&#35782;&#24211;&#65292;&#33258;&#21160;&#23398;&#20064;&#25512;&#33616;&#26032;&#30446;&#26631;&#26448;&#26009;&#30340;&#21069;&#20307;&#26448;&#26009;&#65292;&#36890;&#36807;&#23398;&#20064;&#26448;&#26009;&#21270;&#23398;&#30456;&#20284;&#24615;&#24182;&#23558;&#26032;&#30446;&#26631;&#26448;&#26009;&#30340;&#21512;&#25104;&#21442;&#29031;&#31867;&#20284;&#26448;&#26009;&#30340;&#20808;&#21069;&#21512;&#25104;&#31243;&#24207;&#65292;&#25104;&#21151;&#29575;&#33267;&#23569;&#20026;82%&#12290;</title><link>http://arxiv.org/abs/2302.02303</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#26448;&#26009;&#30456;&#20284;&#24615;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25512;&#33616;&#37197;&#26041;&#26469;&#20419;&#36827;&#26080;&#26426;&#21512;&#25104;&#21069;&#20307;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Precursor recommendation for inorganic synthesis by machine learning materials similarity from scientific literature. (arXiv:2302.02303v2 [cond-mat.mtrl-sci] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#30340;&#21512;&#25104;&#37197;&#26041;&#30340;&#30693;&#35782;&#24211;&#65292;&#33258;&#21160;&#23398;&#20064;&#25512;&#33616;&#26032;&#30446;&#26631;&#26448;&#26009;&#30340;&#21069;&#20307;&#26448;&#26009;&#65292;&#36890;&#36807;&#23398;&#20064;&#26448;&#26009;&#21270;&#23398;&#30456;&#20284;&#24615;&#24182;&#23558;&#26032;&#30446;&#26631;&#26448;&#26009;&#30340;&#21512;&#25104;&#21442;&#29031;&#31867;&#20284;&#26448;&#26009;&#30340;&#20808;&#21069;&#21512;&#25104;&#31243;&#24207;&#65292;&#25104;&#21151;&#29575;&#33267;&#23569;&#20026;82%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#39044;&#27979;&#26159;&#25512;&#21160;&#20808;&#36827;&#26448;&#26009;&#24555;&#36895;&#35774;&#35745;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26080;&#26426;&#26448;&#26009;&#26469;&#35828;&#65292;&#30830;&#23450;&#21512;&#25104;&#21464;&#37327;&#65292;&#22914;&#21069;&#20307;&#26448;&#26009;&#30340;&#36873;&#25321;&#65292;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#22312;&#21152;&#28909;&#36807;&#31243;&#20013;&#21453;&#24212;&#39034;&#24207;&#19981;&#26159;&#24456;&#28165;&#26970;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#30340;29,900&#20010;&#22266;&#24577;&#21512;&#25104;&#37197;&#26041;&#30340;&#30693;&#35782;&#24211;&#65292;&#33258;&#21160;&#23398;&#20064;&#25512;&#33616;&#26032;&#30446;&#26631;&#26448;&#26009;&#30340;&#21069;&#20307;&#26448;&#26009;&#12290;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#26448;&#26009;&#21270;&#23398;&#30456;&#20284;&#24615;&#65292;&#24182;&#23558;&#26032;&#30446;&#26631;&#26448;&#26009;&#30340;&#21512;&#25104;&#21442;&#29031;&#31867;&#20284;&#26448;&#26009;&#30340;&#20808;&#21069;&#21512;&#25104;&#31243;&#24207;&#65292;&#27169;&#25311;&#20154;&#31867;&#21512;&#25104;&#35774;&#35745;&#12290;&#24403;&#38024;&#23545;2,654&#20010;&#26410;&#35265;&#36807;&#30340;&#27979;&#35797;&#30446;&#26631;&#26448;&#26009;&#25552;&#20986;&#20116;&#31181;&#21069;&#20307;&#32452;&#21512;&#26102;&#65292;&#25512;&#33616;&#31574;&#30053;&#30340;&#25104;&#21151;&#29575;&#33267;&#23569;&#20026;82&#65285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#25968;&#23398;&#24418;&#24335;&#25429;&#25417;&#20102;&#20960;&#21313;&#24180;&#30340;&#32463;&#39564;&#21512;&#25104;&#25968;&#25454;&#65292;&#20351;&#24471;&#23427;&#21487;&#20197;&#29992;&#20110;&#25512;&#33616;&#24341;&#25806;&#21644;&#33258;&#27835;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesis prediction is a key accelerator for the rapid design of advanced materials. However, determining synthesis variables such as the choice of precursor materials is challenging for inorganic materials because the sequence of reactions during heating is not well understood. In this work, we use a knowledge base of 29,900 solid-state synthesis recipes, text-mined from the scientific literature, to automatically learn which precursors to recommend for the synthesis of a novel target material. The data-driven approach learns chemical similarity of materials and refers the synthesis of a new target to precedent synthesis procedures of similar materials, mimicking human synthesis design. When proposing five precursor sets for each of 2,654 unseen test target materials, the recommendation strategy achieves a success rate of at least 82%. Our approach captures decades of heuristic synthesis data in a mathematical form, making it accessible for use in recommendation engines and autonomou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25143;&#20013;&#24515;&#24322;&#26500;&#34892;&#21160;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;UCHA-DRL&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#22312;&#26080;&#32447;&#32593;&#32476;&#30340;&#20803;&#23431;&#23449;&#34394;&#25311;&#29616;&#23454;&#20013;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#26381;&#21153;&#22120;&#21521;&#29992;&#25143;&#19979;&#34892;&#36890;&#20449;&#30340;&#20449;&#36947;&#35775;&#38382;&#23433;&#25490;&#21644;&#20256;&#36755;&#21151;&#29575;&#26469;&#25552;&#39640;&#29992;&#25143;&#30340;&#25928;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.01471</link><description>&lt;p&gt;
&#29992;&#25143;&#20013;&#24515;&#24322;&#26500;&#34892;&#21160;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#26080;&#32447;&#32593;&#32476;&#30340;&#20803;&#23431;&#23449;&#34394;&#25311;&#29616;&#23454;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
User-centric Heterogeneous-action Deep Reinforcement Learning for Virtual Reality in the Metaverse over Wireless Networks. (arXiv:2302.01471v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25143;&#20013;&#24515;&#24322;&#26500;&#34892;&#21160;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;UCHA-DRL&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#22312;&#26080;&#32447;&#32593;&#32476;&#30340;&#20803;&#23431;&#23449;&#34394;&#25311;&#29616;&#23454;&#20013;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#26381;&#21153;&#22120;&#21521;&#29992;&#25143;&#19979;&#34892;&#36890;&#20449;&#30340;&#20449;&#36947;&#35775;&#38382;&#23433;&#25490;&#21644;&#20256;&#36755;&#21151;&#29575;&#26469;&#25552;&#39640;&#29992;&#25143;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21508;&#31181;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#20803;&#23431;&#23449;&#27491;&#22312;&#23835;&#36215;&#12290;&#34394;&#25311;&#29616;&#23454;&#25216;&#26415;&#26159;&#20803;&#23431;&#23449;&#20013;&#34394;&#25311;&#23431;&#23449;&#30340;&#25903;&#25745;&#65292;&#33021;&#22815;&#20026;&#29992;&#25143;&#24102;&#26469;&#39640;&#24230;&#27785;&#28024;&#24335;&#30340;&#20307;&#39564;&#12290;&#22312;&#20803;&#23431;&#23449;&#20013;&#65292;&#31227;&#21160;&#24615;&#22791;&#21463;&#24378;&#35843;&#65292;&#34394;&#25311;&#29616;&#23454;&#35774;&#22791;&#36890;&#36807;&#20943;&#23569;&#26412;&#22320;&#30340;&#35745;&#31639;&#33021;&#21147;&#26469;&#20943;&#36731;&#37325;&#37327;&#12290;&#38024;&#23545;&#20803;&#23431;&#23449;&#26381;&#21153;&#22120;&#21644;&#22810;&#20010;&#34394;&#25311;&#29616;&#23454;&#29992;&#25143;&#30340;&#31995;&#32479;&#65292;&#26412;&#25991;&#32771;&#34385;&#20102;&#20004;&#31181;&#24773;&#20917;&#65306;&#65288;i&#65289;&#26381;&#21153;&#22120;&#29983;&#25104;&#24103;&#24182;&#23558;&#23427;&#20204;&#20256;&#36755;&#32473;&#29992;&#25143;&#65307;&#65288;ii&#65289;&#29992;&#25143;&#22312;&#26412;&#22320;&#29983;&#25104;&#24103;&#65292;&#22240;&#27492;&#32791;&#36153;&#35774;&#22791;&#33021;&#37327;&#12290;&#27492;&#22806;&#65292;&#22312;&#20803;&#23431;&#23449;&#20013;&#30340;&#22810;&#29992;&#25143;&#34394;&#25311;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#29992;&#25143;&#23545;&#20110;&#24103;&#29575;&#26377;&#19981;&#21516;&#30340;&#29305;&#28857;&#21644;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#32852;&#21512;&#20248;&#21270;&#26381;&#21153;&#22120;&#21521;&#29992;&#25143;&#19979;&#34892;&#36890;&#20449;&#30340;&#20449;&#36947;&#35775;&#38382;&#23433;&#25490;&#65288;&#21253;&#25324;&#24103;&#29983;&#25104;&#20301;&#32622;&#30340;&#20915;&#31574;&#65289;&#21644;&#20256;&#36755;&#21151;&#29575;&#26469;&#25552;&#39640;&#29992;&#25143;&#30340;&#25928;&#29992;&#12290;&#36825;&#20010;&#32852;&#21512;&#20248;&#21270;&#38382;&#39064;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#29992;&#25143;&#20013;&#24515;&#24322;&#26500;&#34892;&#21160;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;UCHA-DRL&#65289;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#26032;&#30340;Q&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#65292;&#31216;&#20026;&#24322;&#26500;&#34892;&#21160;Q&#32593;&#32476;&#65288;HAQN&#65289;&#26469;&#27714;&#35299;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;UCHA-DRL&#31639;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#12289;&#29992;&#25143;&#20999;&#25442;&#27425;&#25968;&#21644;&#29992;&#25143;&#24179;&#22343;&#24103;&#29575;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#21644;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Metaverse is emerging as maturing technologies are empowering the different facets. Virtual Reality (VR) technologies serve as the backbone of the virtual universe within the Metaverse to offer a highly immersive user experience. As mobility is emphasized in the Metaverse context, VR devices reduce their weights at the sacrifice of local computation abilities. In this paper, for a system consisting of a Metaverse server and multiple VR users, we consider two cases of (i) the server generating frames and transmitting them to users, and (ii) users generating frames locally and thus consuming device energy. Moreover, in our multi-user VR scenario for the Metaverse, users have different characteristics and demands for Frames Per Second (FPS). Then the channel access arrangement (including the decisions on frame generation location), and transmission powers for the downlink communications from the server to the users are jointly optimized to improve the utilities of users. This joint op
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#20004;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#32047;&#35745;&#30446;&#26631;&#20540;&#26368;&#23567;&#21270;&#65292;&#21516;&#26102;&#20445;&#35777;&#38271;&#26399;&#24179;&#22343;&#31532;&#20108;&#38454;&#27573;&#20915;&#31574;&#32467;&#26524;&#23646;&#20110;&#19968;&#20010;&#38598;&#21512;&#12290;&#37319;&#29992;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#20174;&#22312;&#32447;&#20004;&#38454;&#27573;&#38382;&#39064;&#20013;&#24320;&#21457;&#22312;&#32447;&#31639;&#27861;&#65292;&#20854;&#36951;&#25022;&#30028;&#21487;&#20197;&#38477;&#33267;&#23884;&#20837;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#65292;&#24182;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#33719;&#24471;&#20102;&#26032;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.00997</link><description>&lt;p&gt;
&#21463;&#38480;&#22312;&#32447;&#20004;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#65306;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33719;&#24471;&#36817;&#20284;&#26368;&#20248;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Constrained Online Two-stage Stochastic Optimization: Near Optimal Algorithms via Adversarial Learning. (arXiv:2302.00997v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00997
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20004;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#32047;&#35745;&#30446;&#26631;&#20540;&#26368;&#23567;&#21270;&#65292;&#21516;&#26102;&#20445;&#35777;&#38271;&#26399;&#24179;&#22343;&#31532;&#20108;&#38454;&#27573;&#20915;&#31574;&#32467;&#26524;&#23646;&#20110;&#19968;&#20010;&#38598;&#21512;&#12290;&#37319;&#29992;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#20174;&#22312;&#32447;&#20004;&#38454;&#27573;&#38382;&#39064;&#20013;&#24320;&#21457;&#22312;&#32447;&#31639;&#27861;&#65292;&#20854;&#36951;&#25022;&#30028;&#21487;&#20197;&#38477;&#33267;&#23884;&#20837;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#65292;&#24182;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#33719;&#24471;&#20102;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#22312;&#32447;&#20004;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20855;&#26377;&#26377;&#38480;&#30340;$T$&#26399;&#32039;&#32422;&#26463;&#26465;&#20214;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;&#27573;&#65292;&#25105;&#20204;&#20808;&#20316;&#20986;&#31532;&#19968;&#38454;&#27573;&#20915;&#31574;&#65292;&#28982;&#21518;&#35266;&#23519;&#27169;&#22411;&#21442;&#25968;&#30340;&#23454;&#29616;&#65292;&#26368;&#21518;&#20174;&#21462;&#20915;&#20110;&#31532;&#19968;&#38454;&#27573;&#20915;&#31574;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#21487;&#34892;&#38598;&#20013;&#20570;&#20986;&#31532;&#20108;&#38454;&#27573;&#20915;&#31574;&#12290;&#25105;&#20204;&#26088;&#22312;&#26368;&#23567;&#21270;&#32047;&#35745;&#30446;&#26631;&#20540;&#65292;&#21516;&#26102;&#20445;&#35777;&#38271;&#26399;&#24179;&#22343;&#30340;&#31532;&#20108;&#38454;&#27573;&#20915;&#31574;&#23646;&#20110;&#19968;&#20010;&#38598;&#21512;&#12290;&#25105;&#20204;&#21033;&#29992;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#20174;&#22312;&#32447;&#20004;&#38454;&#27573;&#38382;&#39064;&#20013;&#24320;&#21457;&#22312;&#32447;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#21487;&#20197;&#38477;&#33267;&#23884;&#20837;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#25105;&#20204;&#37117;&#33719;&#24471;&#20102;&#26032;&#30340;&#32467;&#26524;&#12290;&#24403;&#27599;&#20010;&#26102;&#38388;&#27573;&#30340;&#27169;&#22411;&#21442;&#25968;&#37117;&#26159;&#20174;&#30456;&#21516;&#30340;&#20998;&#24067;&#20013;&#25277;&#21462;&#30340;&#26102;&#20505;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;$O&#65288;\sqrt{T}&#65289;$&#36951;&#25022;&#30028;&#65292;&#36825;&#27604;&#20043;&#21069;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#30028;&#26377;&#25152;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36824;&#21487;&#20197;&#25269;&#25239;&#27169;&#22411;&#30340;&#25932;&#23545;&#24615;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider an online two-stage stochastic optimization with long-term constraints over a finite horizon of $T$ periods. At each period, we take the first-stage action, observe a model parameter realization and then take the second-stage action from a feasible set that depends both on the first-stage decision and the model parameter. We aim to minimize the cumulative objective value while guaranteeing that the long-term average second-stage decision belongs to a set. We develop online algorithms for the online two-stage problem from adversarial learning algorithms. Also, the regret bound of our algorithm cam be reduced to the regret bound of embedded adversarial learning algorithms. Based on our framework, we obtain new results under various settings. When the model parameter at each period is drawn from identical distributions, we derive state-of-art $O(\sqrt{T})$ regret that improves previous bounds under special cases. Our algorithm is also robust to adversarial corruptions of model
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26041;&#24046;&#30456;&#20851;&#36951;&#25022;&#30028;&#38480;&#24212;&#29992;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#29615;&#22659;&#35268;&#33539;&#26469;&#34920;&#24449;&#29615;&#22659;&#30340;&#26041;&#24046;&#23646;&#24615;&#65292;&#24182;&#35774;&#35745;&#20986;&#22522;&#20110;&#27169;&#22411;&#21644;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#23545;&#20110;&#38543;&#26426;&#21644;&#30830;&#23450;&#24615;&#29615;&#22659;&#21516;&#26102;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#30028;&#38480;&#26159;&#31532;&#19968;&#27425;&#34987;&#35777;&#26126;&#20986;&#26469;&#30340;&#12290;</title><link>http://arxiv.org/abs/2301.13446</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23574;&#38160;&#26041;&#24046;&#30456;&#20851;&#30028;&#38480;&#65306;&#38543;&#26426;&#21644;&#30830;&#23450;&#24615;&#29615;&#22659;&#30340;&#26368;&#20339;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Sharp Variance-Dependent Bounds in Reinforcement Learning: Best of Both Worlds in Stochastic and Deterministic Environments. (arXiv:2301.13446v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26041;&#24046;&#30456;&#20851;&#36951;&#25022;&#30028;&#38480;&#24212;&#29992;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#29615;&#22659;&#35268;&#33539;&#26469;&#34920;&#24449;&#29615;&#22659;&#30340;&#26041;&#24046;&#23646;&#24615;&#65292;&#24182;&#35774;&#35745;&#20986;&#22522;&#20110;&#27169;&#22411;&#21644;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#23545;&#20110;&#38543;&#26426;&#21644;&#30830;&#23450;&#24615;&#29615;&#22659;&#21516;&#26102;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#30028;&#38480;&#26159;&#31532;&#19968;&#27425;&#34987;&#35777;&#26126;&#20986;&#26469;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#30340;&#26041;&#24046;&#30456;&#20851;&#36951;&#25022;&#30028;&#38480;&#12290;&#20855;&#26377;&#26041;&#24046;&#30456;&#20851;&#36951;&#25022;&#20445;&#35777;&#30340;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#21033;&#29992;&#20855;&#26377;&#20302;&#26041;&#24046;&#65288;&#20363;&#22914;&#65292;&#22312;&#30830;&#23450;&#24615;MDP&#19978;&#20139;&#26377;&#24120;&#37327;&#36951;&#25022;&#65289;&#30340;&#29615;&#22659;&#12290;&#29616;&#26377;&#31639;&#27861;&#35201;&#20040;&#29420;&#31435;&#20110;&#26041;&#24046;&#35201;&#20040;&#27425;&#20248;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20004;&#20010;&#26032;&#30340;&#29615;&#22659;&#35268;&#33539;&#26469;&#34920;&#24449;&#29615;&#22659;&#30340;&#32454;&#31890;&#24230;&#26041;&#24046;&#23646;&#24615;&#12290;&#23545;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;MVP&#31639;&#27861;(Zhang&#31561;&#65292;2021a)&#30340;&#21464;&#31181;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#20998;&#26512;&#25216;&#26415;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#30456;&#23545;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#35268;&#33539;&#20139;&#26377;&#26041;&#24046;&#30456;&#20851;&#30340;&#30028;&#38480;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#19968;&#30028;&#38480;&#23545;&#20110;&#38543;&#26426;&#21644;&#30830;&#23450;&#24615;MDP&#21516;&#26102;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#65292;&#36825;&#26159;&#20854;&#31181;&#31867;&#20013;&#30340;&#31532;&#19968;&#20010;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#21442;&#32771;&#20989;&#25968;&#30340;&#31639;&#27861;&#20197;&#21450;&#19968;&#20010;&#26032;&#30340;&#24102;&#26377;&#19978;&#38480;&#21152;&#20493;&#21442;&#32771;&#26356;&#26032;&#36827;&#24230;&#34920;&#30340;&#31574;&#30053;&#21551;&#21160;&#20102;&#20851;&#20110;&#20855;&#26377;&#26041;&#24046;&#30456;&#20851;&#36951;&#25022;&#30028;&#38480;&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#30340;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20123;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study variance-dependent regret bounds for Markov decision processes (MDPs). Algorithms with variance-dependent regret guarantees can automatically exploit environments with low variance (e.g., enjoying constant regret on deterministic MDPs). The existing algorithms are either variance-independent or suboptimal. We first propose two new environment norms to characterize the fine-grained variance properties of the environment. For model-based methods, we design a variant of the MVP algorithm (Zhang et al., 2021a) and use new analysis techniques show to this algorithm enjoys variance-dependent bounds with respect to our proposed norms. In particular, this bound is simultaneously minimax optimal for both stochastic and deterministic MDPs, the first result of its kind. We further initiate the study on model-free algorithms with variance-dependent regret bounds by designing a reference-function-based algorithm with a novel capped-doubling reference update schedule. Lastly, we also provid
&lt;/p&gt;</description></item><item><title>FedEBA+&#26159;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#37319;&#29992;&#20844;&#24179;&#32858;&#21512;&#26041;&#26696;&#21644;&#23545;&#40784;&#26356;&#26032;&#26041;&#27861;&#65292;&#22312;&#21516;&#26102;&#25552;&#39640;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;FedEBA+&#20248;&#20110;&#20854;&#20182;&#20844;&#24179;&#24615;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.12407</link><description>&lt;p&gt;
FedEBA+&#65306;&#22522;&#20110;&#29109;&#30340;&#27169;&#22411;&#23454;&#29616;&#20844;&#24179;&#21644;&#26377;&#25928;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedEBA+: Towards Fair and Effective Federated Learning via Entropy-Based Model. (arXiv:2301.12407v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12407
&lt;/p&gt;
&lt;p&gt;
FedEBA+&#26159;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#37319;&#29992;&#20844;&#24179;&#32858;&#21512;&#26041;&#26696;&#21644;&#23545;&#40784;&#26356;&#26032;&#26041;&#27861;&#65292;&#22312;&#21516;&#26102;&#25552;&#39640;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;FedEBA+&#20248;&#20110;&#20854;&#20182;&#20844;&#24179;&#24615;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#20844;&#24179;&#24615;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#26041;&#38754;&#65292;&#23427;&#20351;&#27169;&#22411;&#22312;&#25152;&#26377;&#23458;&#25143;&#31471;&#19978;&#20445;&#25345;&#19968;&#33268;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#19968;&#31181;&#21487;&#20197;&#21516;&#26102;&#25552;&#39640;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#21644;&#20419;&#36827;&#20844;&#24179;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#33392;&#24040;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23454;&#29616;&#21518;&#32773;&#36890;&#24120;&#38656;&#35201;&#19982;&#21069;&#32773;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;FedEBA+&#65292;&#23427;&#22312;&#21516;&#26102;&#25552;&#39640;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#39640;&#20844;&#24179;&#24615;&#65292;&#35813;&#31639;&#27861;&#37319;&#29992;&#20844;&#24179;&#32858;&#21512;&#26041;&#26696;&#21644;&#23545;&#40784;&#26356;&#26032;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#25910;&#25947;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;FedEBA+&#30340;&#20844;&#24179;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;FedEBA+&#22312;&#20844;&#24179;&#24615;&#21644;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#22343;&#20248;&#20110;&#20854;&#20182;SOTA&#30340;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring fairness is a crucial aspect of Federated Learning (FL), which enables the model to perform consistently across all clients. However, designing an FL algorithm that simultaneously improves global model performance and promotes fairness remains a formidable challenge, as achieving the latter often necessitates a trade-off with the former.To address this challenge, we propose a new FL algorithm, FedEBA+, which enhances fairness while simultaneously improving global model performance. FedEBA+ incorporates a fair aggregation scheme that assigns higher weights to underperforming clients and an alignment update method. In addition, we provide theoretical convergence analysis and show the fairness of FedEBA+. Extensive experiments demonstrate that FedEBA+ outperforms other SOTA fairness FL methods in terms of both fairness and global model performance.
&lt;/p&gt;</description></item><item><title>BiBench&#25552;&#20379;&#20102;&#19968;&#20010;&#20005;&#35880;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#32593;&#32476;&#20108;&#20540;&#21270;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#29305;&#24615;&#65292;&#20026;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2301.11233</link><description>&lt;p&gt;
BiBench&#65306;&#32593;&#32476;&#20108;&#20540;&#21270;&#22522;&#20934;&#27979;&#35797;&#21644;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
BiBench: Benchmarking and Analyzing Network Binarization. (arXiv:2301.11233v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11233
&lt;/p&gt;
&lt;p&gt;
BiBench&#25552;&#20379;&#20102;&#19968;&#20010;&#20005;&#35880;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#32593;&#32476;&#20108;&#20540;&#21270;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#29305;&#24615;&#65292;&#20026;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20108;&#20540;&#21270;&#25104;&#20026;&#26368;&#26377;&#21069;&#36884;&#30340;&#21387;&#32553;&#26041;&#27861;&#20043;&#19968;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#27604;&#29305;&#23485;&#24230;&#65292;&#25552;&#20379;&#20102;&#38750;&#20961;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#33410;&#30465;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#19979;&#65292;&#23558;&#29616;&#26377;&#30340;&#20108;&#20540;&#21270;&#31639;&#27861;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#12289;&#26550;&#26500;&#21644;&#30828;&#20214;&#20173;&#28982;&#19981;&#26159;&#19968;&#20214;&#23481;&#26131;&#30340;&#20107;&#24773;&#12290;&#20108;&#20540;&#21270;&#30340;&#24120;&#35265;&#25361;&#25112;&#65292;&#22914;&#31934;&#24230;&#38477;&#20302;&#21644;&#25928;&#29575;&#38480;&#21046;&#65292;&#34920;&#26126;&#20854;&#23646;&#24615;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BiBench&#65292;&#19968;&#20010;&#20005;&#35880;&#35774;&#35745;&#30340;&#32593;&#32476;&#20108;&#20540;&#21270;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#39318;&#20808;&#20180;&#32454;&#23457;&#26597;&#20102;&#23454;&#38469;&#29983;&#20135;&#20013;&#23545;&#20108;&#20540;&#21270;&#30340;&#35201;&#27714;&#65292;&#20026;&#20840;&#38754;&#20844;&#27491;&#22320;&#36827;&#34892;&#30740;&#31350;&#23450;&#20041;&#20102;&#35780;&#20272;&#36712;&#36947;&#21644;&#24230;&#37327;&#26631;&#20934;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#21644;&#20998;&#26512;&#20102;&#19968;&#31995;&#21015;&#22312;&#25805;&#20316;&#21592;&#32423;&#21035;&#21644;&#24433;&#21709;&#24191;&#27867;&#30340;&#37324;&#31243;&#30865;&#20108;&#20540;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#20102; 1&#65289;&#20108;&#20540;&#21270;&#25805;&#20316;&#21592;&#23545;&#26368;&#32456;&#31934;&#24230;&#26377;&#20851;&#38190;&#24433;&#21709;&#65307;2&#65289;&#19981;&#24212;&#20302;&#20272;&#32593;&#32476;&#25299;&#25169;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#37325;&#35201;&#24615;&#65307;&#20197;&#21450;3&#65289;&#19981;&#21516;&#31639;&#27861;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#26550;&#26500;&#19979;&#30340;&#24615;&#33021;&#24046;&#24322;&#26174;&#30528;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#32593;&#32476;&#20108;&#20540;&#21270;&#30340;&#29305;&#24615;&#21644;&#25361;&#25112;&#25552;&#20379;&#20102;&#28145;&#20837;&#35265;&#35299;&#65292;&#20026;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network binarization emerges as one of the most promising compression approaches offering extraordinary computation and memory savings by minimizing the bit-width. However, recent research has shown that applying existing binarization algorithms to diverse tasks, architectures, and hardware in realistic scenarios is still not straightforward. Common challenges of binarization, such as accuracy degradation and efficiency limitation, suggest that its attributes are not fully understood. To close this gap, we present BiBench, a rigorously designed benchmark with in-depth analysis for network binarization. We first carefully scrutinize the requirements of binarization in the actual production and define evaluation tracks and metrics for a comprehensive and fair investigation. Then, we evaluate and analyze a series of milestone binarization algorithms that function at the operator level and with extensive influence. Our benchmark reveals that 1) the binarized operator has a crucial impact o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PI+ToD&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#28120;&#27760;&#27491;&#21017;&#21270;&#26469;&#39640;&#25928;&#20272;&#31639;&#32463;&#39564;&#23545;RL&#20195;&#29702;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2301.11168</link><description>&lt;p&gt;
&#21738;&#20123;&#32463;&#39564;&#21487;&#20197;&#24433;&#21709;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#65311;&#20855;&#26377;&#28120;&#27760;&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#36845;&#20195;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Which Experiences Are Influential for Your Agent? Policy Iteration with Turn-over Dropout. (arXiv:2301.11168v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PI+ToD&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#28120;&#27760;&#27491;&#21017;&#21270;&#26469;&#39640;&#25928;&#20272;&#31639;&#32463;&#39564;&#23545;RL&#20195;&#29702;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32463;&#39564;&#22238;&#25918;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#65292;&#23384;&#20648;&#22312;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#30340;&#32463;&#39564;&#20250;&#24433;&#21709;RL&#20195;&#29702;&#30340;&#24615;&#33021;&#12290;&#26377;&#20851;&#32463;&#39564;&#24433;&#21709;&#30340;&#20449;&#24687;&#23545;&#20110;&#32463;&#39564;&#28165;&#29702;&#21644;&#20998;&#26512;&#31561;&#21508;&#31181;&#30446;&#30340;&#37117;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#19968;&#20010;&#20272;&#35745;&#21333;&#20010;&#32463;&#39564;&#24433;&#21709;&#30340;&#26041;&#27861;&#26159;&#20195;&#29702;&#27604;&#36739;&#65292;&#20294;&#24403;&#32463;&#39564;&#25968;&#37327;&#24456;&#22810;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#25104;&#26412;&#26159;&#38590;&#20197;&#25215;&#21463;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PI+ToD&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#20272;&#31639;&#32463;&#39564;&#24433;&#21709;&#30340;&#26041;&#27861;&#12290; PI+ToD&#26159;&#19968;&#31181;&#31574;&#30053;&#36845;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28120;&#27760;&#27491;&#21017;&#21270;&#26469;&#39640;&#25928;&#20272;&#31639;&#32463;&#39564;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;MuJoCo&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;PI + ToD&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning (RL) with experience replay, experiences stored in a replay buffer influence the RL agent's performance. Information about the influence is valuable for various purposes, including experience cleansing and analysis. One method for estimating the influence of individual experiences is agent comparison, but it is prohibitively expensive when there is a large number of experiences. In this paper, we present PI+ToD as a method for efficiently estimating the influence of experiences. PI+ToD is a policy iteration that efficiently estimates the influence of experiences by utilizing turn-over dropout. We demonstrate the efficiency of PI+ToD with experiments in MuJoCo environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#35328;&#35299;&#37322;&#30340;&#21435;&#20559;&#35265;(B2T)&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#22270;&#20687;&#26631;&#39064;&#20013;&#30340;&#20851;&#38190;&#35789;&#65292;&#27604;&#36739;&#20851;&#38190;&#35789;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#35782;&#21035;&#21644;&#20943;&#32531;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#20559;&#35265;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2301.11104</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#35299;&#37322;&#30340;&#21435;&#20559;&#35265;: &#36890;&#36807;&#35821;&#35328;&#35299;&#37322;&#28040;&#38500;&#26410;&#30693;&#30340;&#35270;&#35273;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Bias-to-Text: Debiasing Unknown Visual Biases through Language Interpretation. (arXiv:2301.11104v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#35328;&#35299;&#37322;&#30340;&#21435;&#20559;&#35265;(B2T)&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#22270;&#20687;&#26631;&#39064;&#20013;&#30340;&#20851;&#38190;&#35789;&#65292;&#27604;&#36739;&#20851;&#38190;&#35789;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#35782;&#21035;&#21644;&#20943;&#32531;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#20559;&#35265;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#22312;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26102;&#26500;&#25104;&#37325;&#35201;&#38382;&#39064;&#65292;&#20294;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#35786;&#26029;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21435;&#20559;&#35265;(B2T)&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#35821;&#35328;&#35299;&#37322;&#26469;&#35782;&#21035;&#21644;&#32531;&#35299;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#20363;&#22914;&#22270;&#35937;&#20998;&#31867;&#22120;&#21644;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#23545;&#35270;&#35273;&#20559;&#24046;&#30340;&#35821;&#35328;&#25551;&#36848;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#24418;&#24335;&#65292;&#20351;&#24471;&#33021;&#22815;&#21457;&#29616;&#26032;&#30340;&#20559;&#35265;&#24182;&#26377;&#25928;&#22320;&#23545;&#27169;&#22411;&#36827;&#34892;&#21435;&#20559;&#35265;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#34987;&#35823;&#39044;&#27979;&#25110;&#29983;&#25104;&#30340;&#22270;&#20687;&#26631;&#39064;&#20013;&#30340;&#24120;&#35265;&#20851;&#38190;&#35789;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#36890;&#36807;&#27604;&#36739;&#20559;&#35265;&#20851;&#38190;&#35789;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#36991;&#20813;&#26631;&#39064;&#20013;&#30340;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;B2T&#26694;&#26550;&#20013;&#30340;&#20559;&#35265;&#20851;&#38190;&#35789;&#23545;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#21435;&#20559;&#35265;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#21508;&#31181;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biases in models pose a critical issue when deploying machine learning systems, but diagnosing them in an explainable manner can be challenging. To address this, we introduce the bias-to-text (B2T) framework, which uses language interpretation to identify and mitigate biases in vision models, such as image classifiers and text-to-image generative models. Our language descriptions of visual biases provide explainable forms that enable the discovery of novel biases and effective model debiasing. To achieve this, we analyze common keywords in the captions of mispredicted or generated images. Here, we propose novel score functions to avoid biases in captions by comparing the similarities between bias keywords and those images. Additionally, we present strategies to debias zero-shot classifiers and text-to-image diffusion models using the bias keywords from the B2T framework. We demonstrate the effectiveness of our framework on various image classification and generation tasks. For classifi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23454;&#35777;&#21457;&#29616;&#65292;&#22122;&#22768;&#35843;&#24230;&#23545;&#20110;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#65292;&#26368;&#20248;&#30340;&#22122;&#22768;&#35843;&#24230;&#31574;&#30053;&#20250;&#38543;&#30528;&#20219;&#21153;&#30340;&#19981;&#21516;&#32780;&#26377;&#25152;&#19981;&#21516;&#65292;&#24182;&#19988;&#65292;&#22312;&#22270;&#20687;&#22823;&#23567;&#22686;&#21152;&#26102;&#26368;&#20248;&#22122;&#22768;&#35843;&#24230;&#31574;&#30053;&#20250;&#26397;&#26356;&#22024;&#26434;&#30340;&#26041;&#21521;&#36716;&#31227;&#12290;&#23558;&#36755;&#20837;&#25968;&#25454;&#25353;&#27604;&#20363;&#32553;&#25918;&#24182;&#20445;&#25345;&#22122;&#22768;&#35843;&#24230;&#20989;&#25968;&#19981;&#21464;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#22270;&#20687;&#22823;&#23567;&#19978;&#33719;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.10972</link><description>&lt;p&gt;
&#22122;&#22768;&#35843;&#24230;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Importance of Noise Scheduling for Diffusion Models. (arXiv:2301.10972v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23454;&#35777;&#21457;&#29616;&#65292;&#22122;&#22768;&#35843;&#24230;&#23545;&#20110;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#65292;&#26368;&#20248;&#30340;&#22122;&#22768;&#35843;&#24230;&#31574;&#30053;&#20250;&#38543;&#30528;&#20219;&#21153;&#30340;&#19981;&#21516;&#32780;&#26377;&#25152;&#19981;&#21516;&#65292;&#24182;&#19988;&#65292;&#22312;&#22270;&#20687;&#22823;&#23567;&#22686;&#21152;&#26102;&#26368;&#20248;&#22122;&#22768;&#35843;&#24230;&#31574;&#30053;&#20250;&#26397;&#26356;&#22024;&#26434;&#30340;&#26041;&#21521;&#36716;&#31227;&#12290;&#23558;&#36755;&#20837;&#25968;&#25454;&#25353;&#27604;&#20363;&#32553;&#25918;&#24182;&#20445;&#25345;&#22122;&#22768;&#35843;&#24230;&#20989;&#25968;&#19981;&#21464;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#22270;&#20687;&#22823;&#23567;&#19978;&#33719;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#22122;&#22768;&#35843;&#24230;&#31574;&#30053;&#23545;&#21435;&#22122;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#24471;&#20986;&#20102;&#19977;&#20010;&#32467;&#35770;&#65306;&#65288;1&#65289;&#22122;&#22768;&#35843;&#24230;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#26368;&#20248;&#30340;&#22122;&#22768;&#35843;&#24230;&#31574;&#30053;&#21462;&#20915;&#20110;&#20219;&#21153;&#65288;&#20363;&#22914;&#22270;&#20687;&#22823;&#23567;&#65289;&#65307;&#65288;2&#65289;&#22312;&#22270;&#20687;&#22823;&#23567;&#22686;&#21152;&#26102;&#65292;&#26368;&#20248;&#22122;&#22768;&#35843;&#24230;&#20250;&#26397;&#26356;&#22024;&#26434;&#30340;&#26041;&#21521;&#36716;&#31227;&#65288;&#30001;&#20110;&#20687;&#32032;&#20887;&#20313;&#22686;&#21152;&#65289;&#65307;&#65288;3&#65289;&#20165;&#36890;&#36807;&#23558;&#36755;&#20837;&#25968;&#25454;&#25353;&#27604;&#20363;$b$&#32553;&#25918;&#24182;&#20445;&#25345;&#22122;&#22768;&#35843;&#24230;&#20989;&#25968;&#19981;&#21464;&#65288;&#30456;&#24403;&#20110;&#23558;logSNR&#21521;&#19978;&#31227;&#21160;$\log b$&#65289;&#65292;&#20415;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21508;&#31181;&#22270;&#20687;&#22823;&#23567;&#30340;&#33391;&#22909;&#31574;&#30053;&#12290;&#23558;&#36825;&#19968;&#31616;&#21333;&#26041;&#27861;&#19982;&#26368;&#36817;&#25552;&#20986;&#30340;&#36882;&#24402;&#25509;&#21475;&#32593;&#32476;&#65288;RIN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#22312;ImageNet&#19978;&#29983;&#25104;1024$\times$1024&#20998;&#36776;&#29575;&#30340;&#39640;&#20445;&#30495;&#24230;&#22270;&#20687;&#12290;&#36825;&#19968;&#26041;&#27861;&#33021;&#22815;&#20197;&#21333;&#27493;&#12289;&#31471;&#21040;&#31471;&#26041;&#24335;&#29983;&#25104;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
We empirically study the effect of noise scheduling strategies for denoising diffusion generative models. There are three findings: (1) the noise scheduling is crucial for the performance, and the optimal one depends on the task (e.g., image sizes), (2) when increasing the image size, the optimal noise scheduling shifts towards a noisier one (due to increased redundancy in pixels), and (3) simply scaling the input data by a factor of $b$ while keeping the noise schedule function fixed (equivalent to shifting the logSNR by $\log b$) is a good strategy across image sizes. This simple recipe, when combined with recently proposed Recurrent Interface Network (RIN), yields state-of-the-art pixel-based diffusion models for high-resolution images on ImageNet, enabling single-stage, end-to-end generation of diverse and high-fidelity images at 1024$\times$1024 resolution (without upsampling/cascades).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25277;&#26679;&#30340;Nystr&#246;m&#36924;&#36817;&#26041;&#27861;&#29992;&#20110;&#26680;&#31215;&#20998;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#38750;i.i.d.&#22320;&#26631;&#28857;&#30340;&#29702;&#35770;&#20445;&#35777;&#26041;&#27861;&#65292;&#20351;&#24471;&#25552;&#39640;&#20102;&#36924;&#36817;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2301.09517</link><description>&lt;p&gt;
&#22522;&#20110;&#25277;&#26679;&#30340;Nystr&#246;m&#36924;&#36817;&#21644;&#26680;&#31215;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sampling-based Nystr\"om Approximation and Kernel Quadrature. (arXiv:2301.09517v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25277;&#26679;&#30340;Nystr&#246;m&#36924;&#36817;&#26041;&#27861;&#29992;&#20110;&#26680;&#31215;&#20998;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#38750;i.i.d.&#22320;&#26631;&#28857;&#30340;&#29702;&#35770;&#20445;&#35777;&#26041;&#27861;&#65292;&#20351;&#24471;&#25552;&#39640;&#20102;&#36924;&#36817;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#19982;&#27010;&#29575;&#27979;&#37327;&#30456;&#20851;&#30340;&#27491;&#23450;&#26680;&#30340;Nystr&#246;m&#36924;&#36817;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#20256;&#32479;Nystr&#246;m&#36924;&#36817;&#22312;&#36830;&#32493;&#21306;&#38388;&#20013;&#20351;&#29992;i.i.d.&#25277;&#26679;&#21644;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#25913;&#36827;&#35823;&#24046;&#30028;&#65292;&#35777;&#26126;&#25216;&#24039;&#20511;&#37492;&#20102;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;Nystr&#246;m&#36924;&#36817;&#20013;&#30340;&#23376;&#31354;&#38388;&#31934;&#32454;&#36873;&#25321;&#65292;&#36825;&#26159;&#36866;&#29992;&#20110;&#38750;i.i.d.&#22320;&#26631;&#28857;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#20984;&#26680;&#31215;&#20998;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#32473;&#20986;&#20102;&#26032;&#30340;&#29702;&#35770;&#20445;&#35777;&#20197;&#21450;&#25968;&#20540;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the Nystr\"om approximation of a positive definite kernel associated with a probability measure. We first prove an improved error bound for the conventional Nystr\"om approximation with i.i.d. sampling and singular-value decomposition in the continuous regime; the proof techniques are borrowed from statistical learning theory. We further introduce a refined selection of subspaces in Nystr\"om approximation with theoretical guarantees that is applicable to non-i.i.d. landmark points. Finally, we discuss their application to convex kernel quadrature and give novel theoretical guarantees as well as numerical observations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;#DNN-Verification&#38382;&#39064;&#65292;&#21363;&#35745;&#31639;&#36829;&#21453;&#29305;&#23450;&#23433;&#20840;&#24615;&#36136;&#30340;DNN&#36755;&#20837;&#37197;&#32622;&#25968;&#37327;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#19968;&#31181;&#38543;&#26426;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#20998;&#21035;&#32473;&#20986;&#20102;&#30830;&#20999;&#30340;&#36829;&#35268;&#35745;&#25968;&#21644;&#21487;&#35777;&#26126;&#27010;&#29575;&#30028;&#65292;&#24182;&#22312;&#23433;&#20840;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2301.07068</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19981;&#23433;&#20840;&#36755;&#20837;&#35745;&#25968;&#30340;#DNN-Verification&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The #DNN-Verification problem: Counting Unsafe Inputs for Deep Neural Networks. (arXiv:2301.07068v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;#DNN-Verification&#38382;&#39064;&#65292;&#21363;&#35745;&#31639;&#36829;&#21453;&#29305;&#23450;&#23433;&#20840;&#24615;&#36136;&#30340;DNN&#36755;&#20837;&#37197;&#32622;&#25968;&#37327;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#19968;&#31181;&#38543;&#26426;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#20998;&#21035;&#32473;&#20986;&#20102;&#30830;&#20999;&#30340;&#36829;&#35268;&#35745;&#25968;&#21644;&#21487;&#35777;&#26126;&#27010;&#29575;&#30028;&#65292;&#24182;&#22312;&#23433;&#20840;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#38656;&#35201;&#39640;&#24230;&#23433;&#20840;&#24615;&#30340;&#20851;&#38190;&#20219;&#21153;&#20013;&#65292;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#20013;&#36234;&#26469;&#36234;&#34987;&#37319;&#29992;&#12290;&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#39564;&#35777;&#22120;&#21487;&#20197;&#29992;&#26469;&#26816;&#26597;DNN&#26159;&#21542;&#19981;&#23433;&#20840;&#65292;&#21363;&#26159;&#21542;&#23384;&#22312;&#33267;&#23569;&#19968;&#31181;&#19981;&#23433;&#20840;&#30340;&#36755;&#20837;&#37197;&#32622;&#65292;&#20294;&#23427;&#20204;&#30340;&#26159;/&#21542;&#36755;&#20986;&#23545;&#20110;&#20854;&#20182;&#30446;&#30340;&#65288;&#22914;&#23631;&#34109;&#12289;&#27169;&#22411;&#36873;&#25321;&#25110;&#22521;&#35757;&#25913;&#36827;&#65289;&#30340;&#20449;&#24687;&#19981;&#36275;&#22815;&#35814;&#32454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;#DNN-Verification&#38382;&#39064;&#65292;&#23427;&#28041;&#21450;&#35745;&#31639;&#23548;&#33268;DNN&#36829;&#21453;&#29305;&#23450;&#23433;&#20840;&#24615;&#36136;&#30340;&#36755;&#20837;&#37197;&#32622;&#25968;&#37327;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#36820;&#22238;&#30830;&#20999;&#30340;&#36829;&#35268;&#35745;&#25968;&#12290;&#30001;&#20110;&#35813;&#38382;&#39064;&#30340;#P&#23436;&#22791;&#24615;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#27491;&#30830;&#35745;&#25968;&#30340;&#21487;&#35777;&#26126;&#27010;&#29575;&#30028;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#35201;&#27714;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#23433;&#20840;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#19978;&#21576;&#29616;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#39564;&#35777;&#22120;&#21644;&#22522;&#20110;&#35745;&#25968;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks are increasingly adopted in critical tasks that require a high level of safety, e.g., autonomous driving. While state-of-the-art verifiers can be employed to check whether a DNN is unsafe w.r.t. some given property (i.e., whether there is at least one unsafe input configuration), their yes/no output is not informative enough for other purposes, such as shielding, model selection, or training improvements. In this paper, we introduce the #DNN-Verification problem, which involves counting the number of input configurations of a DNN that result in a violation of a particular safety property. We analyze the complexity of this problem and propose a novel approach that returns the exact count of violations. Due to the #P-completeness of the problem, we also propose a randomized, approximate method that provides a provable probabilistic bound of the correct count while significantly reducing computational requirements. We present experimental results on a set of safety-cr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FedTOP&#30340;&#32852;&#37030;&#36801;&#31227;&#26377;&#24207;&#20010;&#24615;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#21644;&#31995;&#32479;&#24322;&#26500;&#24615;&#12289;&#22823;&#35268;&#27169;&#24182;&#34892;&#36890;&#20449;&#36164;&#28304;&#12289;&#24694;&#24847;&#25915;&#20987;&#21644;&#25968;&#25454;&#27745;&#26579;&#31561;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26694;&#26550;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#23458;&#25143;&#31471;&#20998;&#21035;&#36798;&#21040;92.32&#65285;&#21644;95.96&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#22312;&#20934;&#30830;&#29575;&#26041;&#38754;&#25552;&#39640;&#20102;462&#65285;&#65292;&#24182;&#38477;&#20302;&#20102;37.46&#65285;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2301.04829</link><description>&lt;p&gt;
&#38754;&#21521;&#39550;&#39542;&#21592;&#30417;&#27979;&#24212;&#29992;&#30340;&#32852;&#37030;&#36801;&#31227;&#26377;&#24207;&#20010;&#24615;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Transfer-Ordered-Personalized Learning for Driver Monitoring Application. (arXiv:2301.04829v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FedTOP&#30340;&#32852;&#37030;&#36801;&#31227;&#26377;&#24207;&#20010;&#24615;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#21644;&#31995;&#32479;&#24322;&#26500;&#24615;&#12289;&#22823;&#35268;&#27169;&#24182;&#34892;&#36890;&#20449;&#36164;&#28304;&#12289;&#24694;&#24847;&#25915;&#20987;&#21644;&#25968;&#25454;&#27745;&#26579;&#31561;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26694;&#26550;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#23458;&#25143;&#31471;&#20998;&#21035;&#36798;&#21040;92.32&#65285;&#21644;95.96&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#22312;&#20934;&#30830;&#29575;&#26041;&#38754;&#25552;&#39640;&#20102;462&#65285;&#65292;&#24182;&#38477;&#20302;&#20102;37.46&#65285;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#20849;&#20139;&#22312;&#26412;&#22320;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#21442;&#25968;&#26469;&#23454;&#29616;&#21327;&#20316;&#23398;&#20064;&#65292;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#65292;&#22312;&#29289;&#32852;&#32593;&#20013;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#21069;&#26223;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FedTOP&#30340;&#32852;&#37030;&#36801;&#31227;&#26377;&#24207;&#20010;&#24615;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#21644;&#31995;&#32479;&#24322;&#26500;&#24615;&#12289;&#22823;&#35268;&#27169;&#24182;&#34892;&#36890;&#20449;&#36164;&#28304;&#12289;&#24694;&#24847;&#25915;&#20987;&#21644;&#25968;&#25454;&#27745;&#26579;&#31561;&#38382;&#39064;&#12290;&#35770;&#25991;&#22312;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#27604;&#36739;&#20102;&#19977;&#31181;&#25193;&#23637;&#30340;&#24615;&#33021; - &#36801;&#31227;&#12289;&#26377;&#24207;&#21644;&#20010;&#24615;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#23458;&#25143;&#31471;&#20998;&#21035;&#36798;&#21040;92.32&#65285;&#21644;95.96&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#22312;&#20934;&#30830;&#29575;&#26041;&#38754;&#25552;&#39640;&#20102;462&#65285;&#65292;&#24182;&#38477;&#20302;&#20102;37.46&#65285;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) shines through in the internet of things (IoT) with its ability to realize collaborative learning and improve learning efficiency by sharing client model parameters trained on local data. Although FL has been successfully applied to various domains, including driver monitoring applications (DMAs) on the internet of vehicles (IoV), its usages still face some open issues, such as data and system heterogeneity, large-scale parallelism communication resources, malicious attacks, and data poisoning. This paper proposes a federated transfer-ordered-personalized learning (FedTOP) framework to address the above problems and test on two real-world datasets with and without system heterogeneity. The performance of the three extensions, transfer, ordered, and personalized, is compared by an ablation study and achieves 92.32% and 95.96% accuracy on the test clients of two datasets, respectively. Compared to the baseline, there is a 462% improvement in accuracy and a 37.46% 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39034;&#24207;&#25512;&#33616;&#27169;&#22411;&#24120;&#29992;&#25439;&#22833;&#20989;&#25968;&#30340;&#20248;&#21155;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#25913;&#36827;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#22810;&#31181;&#39034;&#24207;&#25512;&#33616;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.00979</link><description>&lt;p&gt;
&#19968;&#31181;&#25913;&#36827;&#30340;&#25439;&#22833;&#20989;&#25968;&#25552;&#21319;&#39034;&#24207;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Sequential Recommendation Models with an Enhanced Loss Function. (arXiv:2301.00979v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39034;&#24207;&#25512;&#33616;&#27169;&#22411;&#24120;&#29992;&#25439;&#22833;&#20989;&#25968;&#30340;&#20248;&#21155;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#25913;&#36827;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#22810;&#31181;&#39034;&#24207;&#25512;&#33616;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#20110;&#39034;&#24207;&#25512;&#33616;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#22797;&#29616;/&#25913;&#36827;&#29616;&#26377;&#27169;&#22411;&#30340;&#24037;&#20316;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#24120;&#29992;&#30340;&#39034;&#24207;&#25512;&#33616;&#25439;&#22833;&#20989;&#25968;&#30340;&#20248;&#21155;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#30340;&#20248;&#28857;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#25913;&#36827;&#30340;&#25439;&#22833;&#20989;&#25968;&#26174;&#33879;&#25552;&#21319;&#20102; GRU4Rec&#65292;SASRec&#65292;SR-GNN&#21644; S3Rec&#31561;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a growing interest in benchmarking sequential recommendation models and reproducing/improving existing models. For example, Rendle et al. improved matrix factorization models by tuning their parameters and hyperparameters. Petrov and Macdonald developed a more efficient and effective implementation of BERT4Rec, which resolved inconsistencies in performance comparison between BERT4Rec and SASRec in previous works. In particular, BERT4Rec and SASRec share a similar network structure, with the main difference lying in their training objective/loss function. Therefore, we analyzed the advantages and disadvantages of commonly used loss functions in sequential recommendation and proposed an improved loss function that leverages their strengths. We conduct extensive experiments on two influential open-source libraries, and the results demonstrate that our improved loss function significantly enhances the performance of GRU4Rec, SASRec, SR-GNN, and S3Rec models, improving their 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26032;&#39062;&#35299;&#37322;&#25216;&#26415;&#65306;&#19978;&#19979;&#25991;&#38271;&#24230;&#25506;&#27979;&#65292;&#36890;&#36807;&#36319;&#36394;&#27169;&#22411;&#39044;&#27979;&#19982;&#21487;&#29992;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#20851;&#31995;&#26469;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20998;&#37197;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#24471;&#20998;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#26377;&#21033;&#20110;&#30740;&#31350;&#36828;&#36317;&#31163;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.14815</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#38271;&#24230;&#25506;&#31350;&#40657;&#21283;&#23376;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Black-box language model explanation by context length probing. (arXiv:2212.14815v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14815
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26032;&#39062;&#35299;&#37322;&#25216;&#26415;&#65306;&#19978;&#19979;&#25991;&#38271;&#24230;&#25506;&#27979;&#65292;&#36890;&#36807;&#36319;&#36394;&#27169;&#22411;&#39044;&#27979;&#19982;&#21487;&#29992;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#20851;&#31995;&#26469;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20998;&#37197;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#24471;&#20998;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#26377;&#21033;&#20110;&#30740;&#31350;&#36828;&#36317;&#31163;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#37319;&#29992;&#24378;&#35843;&#20102;&#25913;&#21892;&#20854;&#21487;&#35299;&#37322;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#25216;&#26415;&#65306;&#19978;&#19979;&#25991;&#38271;&#24230;&#25506;&#27979;&#65292;&#23427;&#22522;&#20110;&#36319;&#36394;&#27169;&#22411;&#39044;&#27979;&#20316;&#20026;&#21487;&#29992;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#20989;&#25968;&#65292;&#24182;&#20801;&#35768;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20998;&#37197;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#24471;&#20998;&#12290;&#35813;&#25216;&#26415;&#26159;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#65292;&#19981;&#20381;&#36182;&#20110;&#38500;&#35745;&#31639;token&#32423;&#27010;&#29575;&#20043;&#22806;&#30340;&#27169;&#22411;&#20869;&#37096;&#35775;&#38382;&#12290;&#25105;&#20204;&#23558;&#19978;&#19979;&#25991;&#38271;&#24230;&#25506;&#27979;&#24212;&#29992;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#21021;&#22987;&#30340;&#20998;&#26512;&#21644;&#35265;&#35299;&#65292;&#21253;&#25324;&#30740;&#31350;&#36828;&#36317;&#31163;&#20381;&#36182;&#24615;&#30340;&#28508;&#21147;&#12290;&#26041;&#27861;&#30340;&#28304;&#20195;&#30721;&#21644;&#20132;&#20114;&#24335;&#28436;&#31034;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasingly widespread adoption of large language models has highlighted the need for improving their explainability. We present context length probing, a novel explanation technique for causal language models, based on tracking the predictions of a model as a function of the length of available context, and allowing to assign differential importance scores to different contexts. The technique is model-agnostic and does not rely on access to model internals beyond computing token-level probabilities. We apply context length probing to large pre-trained language models and offer some initial analyses and insights, including the potential for studying long-range dependencies. The source code and an interactive demo of the method are available.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;KNIFE&#65292;&#21487;&#20197;&#20174;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#20013;&#25552;&#21462;&#25512;&#29702;&#30693;&#35782;&#65292;&#36827;&#32780;&#22312;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.09721</link><description>&lt;p&gt;
KNIFE: &#20174;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#20013;&#25552;&#21462;&#25512;&#29702;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
KNIFE: Distilling Reasoning Knowledge From Free-Text Rationales. (arXiv:2212.09721v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09721
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;KNIFE&#65292;&#21487;&#20197;&#20174;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#20013;&#25552;&#21462;&#25512;&#29702;&#30693;&#35782;&#65292;&#36827;&#32780;&#22312;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#30340;&#24847;&#22806;&#38169;&#35823;&#24341;&#36215;&#20102;&#23545;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#24576;&#30097;&#12290;&#22240;&#27492;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#24494;&#35843;/&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#24863;&#20852;&#36259;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#21253;&#21547;&#20219;&#21153;&#23454;&#20363;&#21644;&#20854;&#20851;&#32852;&#30340;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#65288;FTR&#65289;&#65292;&#36825;&#20123;&#29702;&#30001;&#35299;&#37322;&#20102;&#39044;&#27979;&#27491;&#30830;&#20219;&#21153;&#36755;&#20986;&#30340;&#27491;&#30830;&#25512;&#29702;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24494;&#35843;&#26041;&#27861;&#26080;&#27861;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32780;&#25552;&#31034;&#38656;&#35201;&#36807;&#22823;&#65288;&#21363;&gt;50B&#65289;&#30340;&#35821;&#35328;&#27169;&#22411;&#25165;&#33021;&#33391;&#22909;&#24037;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;KNIFE&#65292;&#35777;&#26126;&#20174;FTR&#20013;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#21462;&#25512;&#29702;&#30693;&#35782;&#65292;&#23558;&#20854;&#28748;&#36755;&#21040;&#23567;&#22411;&#65288;&#21363;&lt;1B&#65289;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;KNIFE&#23545;&#19968;&#20010;&#24072;&#29983;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65288;&#32473;&#23450;&#20219;&#21153;&#36755;&#20837;&#21644;FTR&#65289;&#65292;&#20197;&#39044;&#27979;&#20219;&#21153;&#36755;&#20986;&#65292;&#23558;&#25512;&#29702;&#30693;&#35782;&#20174;FTR&#36716;&#31227;&#33267;&#24072;&#29983;&#38544;&#34255;&#29366;&#24577;&#12290;&#20854;&#27425;&#65292;KNIFE&#23545;&#19968;&#20010;&#23398;&#29983;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65288;&#20165;&#32473;&#23450;&#20219;&#21153;&#36755;&#20837;&#65289;&#65292;&#20197;&#20351;&#20854;&#38544;&#34255;&#29366;&#24577;&#31867;&#20284;&#20110;&#24072;&#29983;&#27169;&#22411;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have yielded impressive results on many language reasoning tasks, but their unexpected errors raise doubts about their reasoning abilities. In light of this, there is growing interest in finetuning/prompting LMs with both task instances and their associated free-text rationales (FTRs), which explain the correct reasoning process for predicting the correct task output (i.e., how to be "right for the right reasons"). However, existing finetuning methods fail to improve LM performance, while prompting needs prohibitively large (i.e., &gt;50B) LMs to work well. We propose KNIFE, which shows that reasoning knowledge can be effectively distilled from FTRs into a small (i.e., &lt;1B) LM and improve the LM's performance. First, KNIFE finetunes a teacher LM (given task input and FTR) to predict the task output, transferring reasoning knowledge from the FTRs to the teacher's hidden states. Second, KNIFE finetunes a student LM (given task input only) such that its hidden states ar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#19979;&#30340;&#28145;&#24230;&#23398;&#20064;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#27668;&#35937;&#19987;&#19994;&#30693;&#35782;&#20197;&#35299;&#26512;&#26041;&#31243;&#24335;&#30340;&#24418;&#24335;&#65292;&#23454;&#29616;&#29289;&#29702;&#19968;&#33268;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#29790;&#22763;&#22320;&#38754;&#22825;&#27668;&#30340;&#21518;&#22788;&#29702;&#20013;&#65292;&#36890;&#36807;&#24378;&#21046;&#25191;&#34892;&#28909;&#21147;&#23398;&#29366;&#24577;&#26041;&#31243;&#26469;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20135;&#29983;&#29289;&#29702;&#19978;&#19968;&#33268;&#30340;&#28201;&#28287;&#24230;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.04487</link><description>&lt;p&gt;
&#29289;&#29702;&#32422;&#26463;&#19979;&#30340;&#28201;&#28287;&#24230;&#28145;&#24230;&#23398;&#20064;&#21518;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Physics-constrained deep learning postprocessing of temperature and humidity. (arXiv:2212.04487v2 [physics.ao-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#19979;&#30340;&#28145;&#24230;&#23398;&#20064;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#27668;&#35937;&#19987;&#19994;&#30693;&#35782;&#20197;&#35299;&#26512;&#26041;&#31243;&#24335;&#30340;&#24418;&#24335;&#65292;&#23454;&#29616;&#29289;&#29702;&#19968;&#33268;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#29790;&#22763;&#22320;&#38754;&#22825;&#27668;&#30340;&#21518;&#22788;&#29702;&#20013;&#65292;&#36890;&#36807;&#24378;&#21046;&#25191;&#34892;&#28909;&#21147;&#23398;&#29366;&#24577;&#26041;&#31243;&#26469;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20135;&#29983;&#29289;&#29702;&#19978;&#19968;&#33268;&#30340;&#28201;&#28287;&#24230;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#27668;&#39044;&#25253;&#20013;&#24515;&#30446;&#21069;&#20381;&#36182;&#20110;&#32479;&#35745;&#21518;&#22788;&#29702;&#26041;&#27861;&#26469;&#26368;&#23567;&#21270;&#39044;&#25253;&#35823;&#24046;&#12290;&#36825;&#25552;&#39640;&#20102;&#39044;&#25253;&#25216;&#33021;&#65292;&#20294;&#21487;&#33021;&#23548;&#33268;&#36829;&#21453;&#29289;&#29702;&#21407;&#29702;&#25110;&#24573;&#30053;&#21464;&#37327;&#20043;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#39044;&#27979;&#65292;&#36825;&#21487;&#33021;&#23545;&#19979;&#28216;&#24212;&#29992;&#21644;&#21518;&#22788;&#29702;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#26377;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#20204;&#22522;&#20110;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26102;&#12290;&#20511;&#37492;&#29289;&#29702;&#30693;&#35782;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#25972;&#21512;&#27668;&#35937;&#19987;&#19994;&#30693;&#35782;&#20197;&#35299;&#26512;&#26041;&#31243;&#24335;&#30340;&#24418;&#24335;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#21518;&#22788;&#29702;&#27169;&#22411;&#20013;&#23454;&#29616;&#29289;&#29702;&#19968;&#33268;&#24615;&#12290;&#24212;&#29992;&#20110;&#29790;&#22763;&#22320;&#38754;&#22825;&#27668;&#30340;&#21518;&#22788;&#29702;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#24378;&#21046;&#25191;&#34892;&#28909;&#21147;&#23398;&#29366;&#24577;&#26041;&#31243;&#26469;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#20250;&#20135;&#29983;&#29289;&#29702;&#19978;&#19968;&#33268;&#30340;&#28201;&#28287;&#24230;&#39044;&#27979;&#32467;&#26524;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#32570;&#26102;&#23588;&#20854;&#26377;&#20248;&#21183;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#32435;&#20837;&#28145;&#24230;&#23398;&#20064;&#21518;&#22788;&#29702;&#27169;&#22411;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weather forecasting centers currently rely on statistical postprocessing methods to minimize forecast error. This improves skill but can lead to predictions that violate physical principles or disregard dependencies between variables, which can be problematic for downstream applications and for the trustworthiness of postprocessing models, especially when they are based on new machine learning approaches. Building on recent advances in physics-informed machine learning, we propose to achieve physical consistency in deep learning-based postprocessing models by integrating meteorological expertise in the form of analytic equations. Applied to the post-processing of surface weather in Switzerland, we find that constraining a neural network to enforce thermodynamic state equations yields physically-consistent predictions of temperature and humidity without compromising performance. Our approach is especially advantageous when data is scarce, and our findings suggest that incorporating doma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#36741;&#21161;&#20195;&#25968;&#36712;&#36857;&#20449;&#24687;&#26126;&#30830;&#28155;&#21152;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#31283;&#23450;&#21270;&#21644;&#25237;&#24433;&#26041;&#27861;&#21512;&#24182;&#20449;&#24687;&#65292;&#23545;&#22810;&#20307;&#25670;&#21644;&#20998;&#23376;&#21160;&#21147;&#23398;&#24773;&#26223;&#36827;&#34892;&#20102;&#27169;&#25311;&#23454;&#39564;&#12290;&#35813;&#26041;&#27861;&#26131;&#20110;&#23454;&#29616;&#65292;&#23545;&#35757;&#32451;&#24615;&#33021;&#24433;&#21709;&#26377;&#38480;&#65292;&#22312;&#25512;&#29702;&#26041;&#38754;&#32473;&#20986;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2211.14302</link><description>&lt;p&gt;
&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#65306;&#31070;&#32463;DAEs
&lt;/p&gt;
&lt;p&gt;
Neural DAEs: Constrained neural networks. (arXiv:2211.14302v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#36741;&#21161;&#20195;&#25968;&#36712;&#36857;&#20449;&#24687;&#26126;&#30830;&#28155;&#21152;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#31283;&#23450;&#21270;&#21644;&#25237;&#24433;&#26041;&#27861;&#21512;&#24182;&#20449;&#24687;&#65292;&#23545;&#22810;&#20307;&#25670;&#21644;&#20998;&#23376;&#21160;&#21147;&#23398;&#24773;&#26223;&#36827;&#34892;&#20102;&#27169;&#25311;&#23454;&#39564;&#12290;&#35813;&#26041;&#27861;&#26131;&#20110;&#23454;&#29616;&#65292;&#23545;&#35757;&#32451;&#24615;&#33021;&#24433;&#21709;&#26377;&#38480;&#65292;&#22312;&#25512;&#29702;&#26041;&#38754;&#32473;&#20986;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#36741;&#21161;&#20195;&#25968;&#36712;&#36857;&#20449;&#24687;&#26126;&#30830;&#28155;&#21152;&#21040;&#21160;&#24577;&#31995;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20174;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#21644;&#27969;&#24418;&#19978;&#30340;&#24494;&#20998;&#26041;&#31243;&#39046;&#22495;&#27762;&#21462;&#28789;&#24863;&#65292;&#24182;&#22312;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#30456;&#20851;&#26041;&#27861;&#65292;&#23613;&#31649;&#23384;&#22312;&#19968;&#20123;&#22522;&#26412;&#24773;&#22659;&#19978;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#31283;&#23450;&#21270;&#21644;&#25237;&#24433;&#26041;&#27861;&#65292;&#23558;&#32422;&#26463;&#25110;&#36741;&#21161;&#20449;&#24687;&#25928;&#26524;&#21512;&#24182;&#65292;&#24182;&#36890;&#36807;&#23545;&#22810;&#20307;&#25670;&#21644;&#20998;&#23376;&#21160;&#21147;&#23398;&#24773;&#26223;&#30340;&#27169;&#25311;&#23454;&#39564;&#23637;&#31034;&#20102;&#20309;&#26102;&#20351;&#29992;&#21738;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#19968;&#20123;&#26041;&#27861;&#26131;&#20110;&#22312;&#29616;&#26377;&#20195;&#30721;&#20013;&#23454;&#29616;&#65292;&#24182;&#23545;&#35757;&#32451;&#24615;&#33021;&#24433;&#21709;&#26377;&#38480;&#65292;&#21516;&#26102;&#22312;&#25512;&#29702;&#26041;&#38754;&#32473;&#20986;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article investigates the effect of explicitly adding auxiliary algebraic trajectory information to neural networks for dynamical systems. We draw inspiration from the field of differential-algebraic equations and differential equations on manifolds and implement related methods in residual neural networks, despite some fundamental scenario differences. Constraint or auxiliary information effects are incorporated through stabilization as well as projection methods, and we show when to use which method based on experiments involving simulations of multi-body pendulums and molecular dynamics scenarios. Several of our methods are easy to implement in existing code and have limited impact on training performance while giving significant boosts in terms of inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#33258;&#21160;&#29305;&#24449;&#29983;&#25104;&#24037;&#20855;OpenFE&#21487;&#20197;&#19982;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#25552;&#20379;&#30340;&#32467;&#26524;&#30456;&#23218;&#32654;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#29305;&#28857;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#29305;&#24449;&#25552;&#21319;&#26041;&#27861;&#21644;&#20004;&#38454;&#27573;&#20462;&#21098;&#31639;&#27861;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2211.12507</link><description>&lt;p&gt;
OpenFE: &#20855;&#26377;&#19987;&#23478;&#32423;&#24615;&#33021;&#30340;&#33258;&#21160;&#29305;&#24449;&#29983;&#25104;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
OpenFE: Automated Feature Generation with Expert-level Performance. (arXiv:2211.12507v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#33258;&#21160;&#29305;&#24449;&#29983;&#25104;&#24037;&#20855;OpenFE&#21487;&#20197;&#19982;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#25552;&#20379;&#30340;&#32467;&#26524;&#30456;&#23218;&#32654;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#29305;&#28857;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#29305;&#24449;&#25552;&#21319;&#26041;&#27861;&#21644;&#20004;&#38454;&#27573;&#20462;&#21098;&#31639;&#27861;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#29305;&#24449;&#29983;&#25104;&#30340;&#30446;&#26631;&#26159;&#20351;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#25670;&#33073;&#25163;&#21160;&#29305;&#24449;&#29983;&#25104;&#30340;&#32321;&#29712;&#20219;&#21153;&#65292;&#36825;&#23545;&#20110;&#25552;&#39640;&#34920;&#26684;&#25968;&#25454;&#30340;&#23398;&#20064;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#33258;&#21160;&#29305;&#24449;&#29983;&#25104;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#20174;&#22823;&#37327;&#20505;&#36873;&#29305;&#24449;&#20013;&#39640;&#25928;&#20934;&#30830;&#22320;&#35782;&#21035;&#26377;&#25928;&#29305;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;OpenFE&#65292;&#19968;&#31181;&#33258;&#21160;&#29305;&#24449;&#29983;&#25104;&#24037;&#20855;&#65292;&#21487;&#20197;&#19982;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#25552;&#20379;&#30340;&#32467;&#26524;&#30456;&#23218;&#32654;&#12290;OpenFE&#36890;&#36807;&#20004;&#20010;&#32452;&#20214;&#23454;&#29616;&#39640;&#25928;&#21644;&#20934;&#30830;&#65306;1&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#25552;&#21319;&#26041;&#27861;&#65292;&#29992;&#20110;&#20934;&#30830;&#22320;&#35780;&#20272;&#20505;&#36873;&#29305;&#24449;&#30340;&#22686;&#37327;&#24615;&#33021;&#65307;2&#65289;&#19968;&#31181;&#20004;&#38454;&#27573;&#20462;&#21098;&#31639;&#27861;&#65292;&#20197;&#31895;&#21040;&#32454;&#30340;&#26041;&#24335;&#36827;&#34892;&#29305;&#24449;&#20462;&#21098;&#12290;&#22312;&#21313;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;OpenFE&#27604;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#20004;&#20010;Kaggle&#27604;&#36187;&#20013;&#23545;OpenFE&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#36825;&#20123;&#27604;&#36187;&#26377;&#25968;&#21315;&#20010;&#25968;&#25454;&#31185;&#23398;&#22242;&#38431;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of automated feature generation is to liberate machine learning experts from the laborious task of manual feature generation, which is crucial for improving the learning performance of tabular data. The major challenge in automated feature generation is to efficiently and accurately identify effective features from a vast pool of candidate features. In this paper, we present OpenFE, an automated feature generation tool that provides competitive results against machine learning experts. OpenFE achieves high efficiency and accuracy with two components: 1) a novel feature boosting method for accurately evaluating the incremental performance of candidate features and 2) a two-stage pruning algorithm that performs feature pruning in a coarse-to-fine manner. Extensive experiments on ten benchmark datasets show that OpenFE outperforms existing baseline methods by a large margin. We further evaluate OpenFE in two Kaggle competitions with thousands of data science teams participating. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;MetaC&#24694;&#24847;&#25915;&#20987;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27450;&#35784;&#32773;&#26816;&#27979;&#27169;&#22359;PDR&#65292;&#26126;&#30830;&#32771;&#34385;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.11534</link><description>&lt;p&gt;
&#20174;&#33258;&#36866;&#24212;&#27450;&#35784;&#32773;&#26816;&#27979;&#25506;&#31350;&#23545;&#25239;&#40065;&#26834;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards Adversarially Robust Recommendation from Adaptive Fraudster Detection. (arXiv:2211.11534v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;MetaC&#24694;&#24847;&#25915;&#20987;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27450;&#35784;&#32773;&#26816;&#27979;&#27169;&#22359;PDR&#65292;&#26126;&#30830;&#32771;&#34385;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#33410;&#28857;&#27880;&#20837;&#25915;&#20987;&#19979;&#30340;&#40065;&#26834;&#24615;&#22791;&#21463;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#31995;&#32479;GraphRfi&#65292;&#23427;&#26377;&#25928;&#20943;&#36731;&#20102;&#27880;&#20837;&#30340;&#34394;&#20551;&#29992;&#25143;&#30340;&#24433;&#21709;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;GraphRfi&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#22240;&#20026;&#20854;&#27450;&#35784;&#32773;&#26816;&#27979;&#32452;&#20214;&#30340;&#30417;&#30563;&#24615;&#36136;&#65292;&#22312;&#23454;&#36341;&#20013;&#24456;&#38590;&#33719;&#24471;&#24178;&#20928;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;MetaC&#24694;&#24847;&#25915;&#20987;&#65292;&#38024;&#23545;GNN-based&#21644;MF-based&#25512;&#33616;&#31995;&#32479;&#12290;&#26681;&#25454;&#25105;&#20204;&#20174;&#26131;&#21463;&#25915;&#20987;&#24615;&#20998;&#26512;&#20013;&#24471;&#21040;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27450;&#35784;&#32773;&#26816;&#27979;&#27169;&#22359;&#65292;&#26126;&#30830;&#32771;&#34385;&#20102;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#27169;&#22359;&#21487;&#20197;&#20316;&#20026;&#19981;&#21516;&#25512;&#33616;&#31995;&#32479;&#30340;&#25554;&#20214;&#65292;&#24418;&#25104;&#19968;&#20010;&#31283;&#20581;&#30340;&#26694;&#26550;&#65288;PDR&#65289;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#38450;&#24481;&#26041;&#27861;&#22312;&#25915;&#20987;&#19979;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#22312;&#26500;&#24314;&#27450;&#35784;&#32773;&#26816;&#27979;&#27169;&#22359;&#26102;&#32771;&#34385;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#25913;&#21892;&#25512;&#33616;&#31995;&#32479;&#23545;&#33410;&#28857;&#27880;&#20837;&#25915;&#20987;&#40065;&#26834;&#24615;&#30340;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robustness of recommender systems under node injection attacks has garnered significant attention. Recently, GraphRfi, a GNN-based recommender system, was proposed and shown to effectively mitigate the impact of injected fake users. However, we demonstrate that GraphRfi remains vulnerable to attacks due to the supervised nature of its fraudster detection component, where obtaining clean labels is challenging in practice. In particular, we propose a powerful poisoning attack, MetaC, against both GNN-based and MF-based recommender systems. Furthermore, we analyze why GraphRfi fails under such an attack. Then, based on our insights obtained from vulnerability analysis, we design an adaptive fraudster detection module that explicitly considers label uncertainty. This module can serve as a plug-in for different recommender systems, resulting in a robust framework named PDR. Comprehensive experiments show that our defense approach outperforms other benchmark methods under attacks. Overal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21019;&#24314;&#21517;&#20026;&#26041;&#27861;&#30340;&#32479;&#19968;&#22522;&#20934;&#30340;&#23581;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;NLP&#27169;&#22411;&#20013;&#30340;OOD&#40065;&#26834;&#24615;&#65292;&#35813;&#22522;&#20934;&#21253;&#25324;13&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;OOD&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;21&#20010;&#24120;&#29992;&#30340;PLMs&#19978;&#23545;8&#20010;&#32463;&#20856;NLP&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2211.08073</link><description>&lt;p&gt;
GLUE-X: &#20174;ODD&#26222;&#36866;&#24615;&#35282;&#24230;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective. (arXiv:2211.08073v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21019;&#24314;&#21517;&#20026;&#26041;&#27861;&#30340;&#32479;&#19968;&#22522;&#20934;&#30340;&#23581;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;NLP&#27169;&#22411;&#20013;&#30340;OOD&#40065;&#26834;&#24615;&#65292;&#35813;&#22522;&#20934;&#21253;&#25324;13&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;OOD&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;21&#20010;&#24120;&#29992;&#30340;PLMs&#19978;&#23545;8&#20010;&#32463;&#20856;NLP&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#36890;&#36807;&#21033;&#29992;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24050;&#30693;&#21487;&#20197;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;NLP&#20219;&#21153;&#20013;&#30340;ODD&#26222;&#36866;&#24615;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#38480;&#21046;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37096;&#32626;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21019;&#24314;&#21517;&#20026;&#26041;&#27861;&#30340;&#32479;&#19968;&#22522;&#20934;&#30340;&#23581;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;NLP&#27169;&#22411;&#20013;&#30340;OOD&#40065;&#26834;&#24615;&#65292;&#24378;&#35843;OOD&#40065;&#26834;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#22914;&#20309;&#34913;&#37327;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#20197;&#21450;&#22914;&#20309;&#25913;&#21892;&#27169;&#22411;&#30340;&#35265;&#35299;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;13&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;OOD&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;21&#20010;&#24120;&#29992;&#30340;PLMs&#65288;&#21253;&#25324;GPT-3&#21644;GPT-3.5&#65289;&#19978;&#23545;8&#20010;&#32463;&#20856;NLP&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#30830;&#35748;&#20102;&#22312;&#25152;&#26377;&#35774;&#32622;&#19979;&#65292;&#19982;ID&#20934;&#30830;&#24230;&#30456;&#27604;&#65292;&#23384;&#22312;&#26174;&#30528;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#38656;&#35201;&#25913;&#21892;NLP&#20219;&#21153;&#20013;&#30340;OOD&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) are known to improve the generalization performance of natural language understanding models by leveraging large amounts of data during the pre-training phase. However, the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods. This paper presents the first attempt at creating a unified benchmark named \method for evaluating OOD robustness in NLP models, highlighting the importance of OOD robustness and providing insights on how to measure the robustness of a model and how to improve it. The benchmark includes 13 publicly available datasets for OOD testing, and evaluations are conducted on 8 classic NLP tasks over 21 popularly used PLMs, including GPT-3 and GPT-3.5. Our findings confirm the need for improved OOD accuracy in NLP tasks, as significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#19979;&#26368;&#20248;&#38544;&#31169;&#20445;&#25252;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#37327;&#21270;&#21644;&#20108;&#39033;&#26426;&#21046;&#21442;&#25968;&#20197;&#21450;&#36890;&#20449;&#36164;&#28304;&#65292;&#26368;&#22823;&#21270;&#25910;&#25947;&#36895;&#24230;&#24182;&#20445;&#35777;DP&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2211.07166</link><description>&lt;p&gt;
&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#19979;&#32852;&#37030;&#23398;&#20064;&#30340;&#26368;&#20248;&#38544;&#31169;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Optimal Privacy Preserving for Federated Learning in Mobile Edge Computing. (arXiv:2211.07166v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#19979;&#26368;&#20248;&#38544;&#31169;&#20445;&#25252;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#37327;&#21270;&#21644;&#20108;&#39033;&#26426;&#21046;&#21442;&#25968;&#20197;&#21450;&#36890;&#20449;&#36164;&#28304;&#65292;&#26368;&#22823;&#21270;&#25910;&#25947;&#36895;&#24230;&#24182;&#20445;&#35777;DP&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#65292;&#37319;&#29992;&#37327;&#21270;&#21644;&#26377;&#24847;&#28155;&#21152;&#30340;&#22122;&#22768;&#30340;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20445;&#25252;&#29992;&#25143;&#30340;&#24046;&#20998;&#38544;&#31169;&#65292;&#21516;&#26102;&#20943;&#23569;&#26080;&#32447;&#36164;&#28304;&#30340;&#20351;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#21487;&#20197;&#19982;&#26469;&#33258;&#22810;&#20010;&#29992;&#25143;&#30340;&#22522;&#20110;&#20108;&#39033;&#26426;&#21046;&#30340;&#37327;&#21270;&#26356;&#26032;&#36827;&#34892;&#34701;&#21512;&#12290;&#20294;&#26159;&#65292;&#20248;&#21270;&#37327;&#21270;&#21442;&#25968;&#65292;&#36890;&#20449;&#36164;&#28304;&#65288;&#20363;&#22914;&#20256;&#36755;&#21151;&#29575;&#12289;&#24102;&#23485;&#21644;&#37327;&#21270;&#27604;&#29305;&#65289;&#20197;&#21450;&#28155;&#21152;&#30340;&#22122;&#22768;&#65292;&#20197;&#20445;&#35777;DP&#35201;&#27714;&#21644;&#23398;&#20064;&#30340;FL&#27169;&#22411;&#30340;&#24615;&#33021;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#26088;&#22312;&#32852;&#21512;&#20248;&#21270;&#37327;&#21270;&#21644;&#20108;&#39033;&#26426;&#21046;&#21442;&#25968;&#20197;&#21450;&#36890;&#20449;&#36164;&#28304;&#65292;&#20197;&#22312;&#26080;&#32447;&#32593;&#32476;&#21644;DP&#35201;&#27714;&#30340;&#32422;&#26463;&#19979;&#26368;&#22823;&#21270;&#25910;&#25947;&#36895;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20102;FL&#19982;&#37327;&#21270;/&#22122;&#22768;&#30340;DP&#39044;&#31639;&#20272;&#35745;&#65292;&#35813;&#20272;&#35745;&#27604;&#29616;&#26377;&#30340;&#19978;&#30028;&#26356;&#32039;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25910;&#25947;&#36895;&#24230;&#30340;&#29702;&#35770;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) with quantization and deliberately added noise over wireless networks is a promising approach to preserve user differential privacy (DP) while reducing wireless resources. Specifically, an FL process can be fused with quantized Binomial mechanism-based updates contributed by multiple users. However, optimizing quantization parameters, communication resources (e.g., transmit power, bandwidth, and quantization bits), and the added noise to guarantee the DP requirement and performance of the learned FL model remains an open and challenging problem. This article aims to jointly optimize the quantization and Binomial mechanism parameters and communication resources to maximize the convergence rate under the constraints of the wireless network and DP requirement. To that end, we first derive a novel DP budget estimation of the FL with quantization/noise that is tighter than the state-of-the-art bound. We then provide a theoretical bound on the convergence rate. This t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#32447;&#21512;&#21516;&#35774;&#35745;&#20013;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#25351;&#25968;&#32423;&#30340;$m$&#20010;&#26679;&#26412;&#23601;&#36275;&#20197;&#23398;&#20064;&#19968;&#20010;&#36817;&#20046;&#26368;&#20248;&#30340;&#21512;&#21516;&#12290;</title><link>http://arxiv.org/abs/2211.05732</link><description>&lt;p&gt;
&#22312;&#32447;&#21512;&#21516;&#35774;&#35745;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
The Sample Complexity of Online Contract Design. (arXiv:2211.05732v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#32447;&#21512;&#21516;&#35774;&#35745;&#20013;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#25351;&#25968;&#32423;&#30340;$m$&#20010;&#26679;&#26412;&#23601;&#36275;&#20197;&#23398;&#20064;&#19968;&#20010;&#36817;&#20046;&#26368;&#20248;&#30340;&#21512;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#32447;&#24773;&#22659;&#19979;&#30340;&#38544;&#34255;-&#34892;&#21160;&#22996;&#25176;&#38382;&#39064;&#12290;&#22312;&#27599;&#36718;&#20013;&#65292;&#22996;&#25176;&#20154;&#21457;&#24067;&#19968;&#20221;&#21512;&#21516;&#65292;&#26681;&#25454;&#27599;&#20010;&#32467;&#26524;&#35268;&#23450;&#20195;&#29702;&#20154;&#30340;&#25903;&#20184;&#12290;&#20195;&#29702;&#20154;&#28982;&#21518;&#20570;&#20986;&#19968;&#20010;&#26368;&#22823;&#21270;&#22905;&#33258;&#24049;&#25928;&#29992;&#30340;&#25112;&#30053;&#34892;&#21160;&#36873;&#25321;&#65292;&#20294;&#30452;&#25509;&#35266;&#23519;&#19981;&#21040;&#34892;&#21160;&#12290;&#22996;&#25176;&#20154;&#35266;&#23519;&#32467;&#26524;&#24182;&#20174;&#20195;&#29702;&#20154;&#30340;&#34892;&#21160;&#36873;&#25321;&#20013;&#33719;&#24471;&#25928;&#29992;&#12290;&#26681;&#25454;&#36807;&#21435;&#30340;&#35266;&#23519;&#65292;&#22996;&#25176;&#20154;&#21160;&#24577;&#22320;&#35843;&#25972;&#21512;&#21516;&#65292;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#20854;&#25928;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#20854;Stackelberg&#36951;&#25022;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#21512;&#21516;&#31354;&#38388;&#20026;$[0,1]^m$&#26102;&#65292;Stackelberg&#36951;&#25022;&#30340;&#19978;&#30028;&#20026;$\widetilde O(\sqrt{m} \cdot T^{1-1/(2m+1)})$&#65292;&#19979;&#30028;&#20026;$\Omega(T^{1-1/(m+2)})$&#65292;&#20854;&#20013;$\widetilde O$&#25490;&#38500;&#23545;&#25968;&#22240;&#23376;&#12290; &#36825;&#20010;&#32467;&#26524;&#34920;&#26126;&#65292;&#25351;&#25968;&#32423;&#30340;$m$&#20010;&#26679;&#26412;&#23601;&#36275;&#20197;&#23398;&#20064;&#19968;&#20010;&#36817;&#20046;&#26368;&#20248;&#30340;&#21512;&#21516;&#65292;&#35299;&#20915;&#20102;&#22312;&#32447;&#21512;&#21516;&#35774;&#35745;&#20013;&#30340;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the hidden-action principal-agent problem in an online setting. In each round, the principal posts a contract that specifies the payment to the agent based on each outcome. The agent then makes a strategic choice of action that maximizes her own utility, but the action is not directly observable by the principal. The principal observes the outcome and receives utility from the agent's choice of action. Based on past observations, the principal dynamically adjusts the contracts with the goal of maximizing her utility.  We introduce an online learning algorithm and provide an upper bound on its Stackelberg regret. We show that when the contract space is $[0,1]^m$, the Stackelberg regret is upper bounded by $\widetilde O(\sqrt{m} \cdot T^{1-1/(2m+1)})$, and lower bounded by $\Omega(T^{1-1/(m+2)})$, where $\widetilde O$ omits logarithmic factors. This result shows that exponential-in-$m$ samples are sufficient and necessary to learn a near-optimal contract, resolving an open probl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#30456;&#20851;&#25991;&#29486;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22522;&#20110;&#22122;&#22768;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#35774;&#22791;&#30340;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#22312;&#32463;&#20856;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22522;&#20110;&#26410;&#26469;&#23481;&#38169;&#30828;&#20214;&#30340;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#20013;&#19968;&#20123;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#37327;&#23376;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2211.03464</link><description>&lt;p&gt;
&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Quantum Reinforcement Learning. (arXiv:2211.03464v1 [quant-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#30456;&#20851;&#25991;&#29486;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22522;&#20110;&#22122;&#22768;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#35774;&#22791;&#30340;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#22312;&#32463;&#20856;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22522;&#20110;&#26410;&#26469;&#23481;&#38169;&#30828;&#20214;&#30340;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#20013;&#19968;&#20123;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#37327;&#23376;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#26159;&#37327;&#23376;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#20132;&#21449;&#39046;&#22495;&#20013;&#30340;&#26032;&#20852;&#39046;&#22495;&#12290;&#26412;&#25991;&#23558;&#25552;&#20379;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#25991;&#29486;&#30340;&#24191;&#27867;&#27010;&#36848;&#65292;&#20294;&#25105;&#20204;&#29305;&#21035;&#24378;&#35843;&#26368;&#36817;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#24050;&#32463;&#21487;&#29992;&#30340;&#22122;&#22768;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#35774;&#22791;&#65292;&#36825;&#20123;&#35774;&#22791;&#21253;&#25324;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#65292;&#23427;&#22312;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#19979;&#20805;&#24403;&#20989;&#25968;&#36924;&#36817;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#22522;&#20110;&#26410;&#26469;&#23481;&#38169;&#30828;&#20214;&#30340;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#20013;&#19968;&#20123;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#37327;&#23376;&#20248;&#21183;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#35813;&#39046;&#22495;&#30340;&#20463;&#30640;&#20197;&#21450;&#23545;&#25991;&#29486;&#20013;&#37096;&#20998;&#20869;&#23481;&#30340;&#24635;&#32467;&#21644;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum reinforcement learning is an emerging field at the intersection of quantum computing and machine learning. While we intend to provide a broad overview of the literature on quantum reinforcement learning (our interpretation of this term will be clarified below), we put particular emphasis on recent developments. With a focus on already available noisy intermediate-scale quantum devices, these include variational quantum circuits acting as function approximators in an otherwise classical reinforcement learning setting. In addition, we survey quantum reinforcement learning algorithms based on future fault-tolerant hardware, some of which come with a provable quantum advantage. We provide both a birds-eye-view of the field, as well as summaries and reviews for selected parts of the literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#23545;&#35937;&#36712;&#36857;&#34920;&#31034;&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#21644;&#25311;&#21512;&#35823;&#24046;&#20043;&#38388;&#30340;&#26435;&#34913;&#36827;&#34892;&#20102;&#32463;&#39564;&#20998;&#26512;&#65292;&#21457;&#29616;&#31616;&#21333;&#30340;&#32447;&#24615;&#27169;&#22411;&#23601;&#33021;&#22815;&#39640;&#24230;&#37325;&#29616;&#30495;&#23454;&#19990;&#30028;&#30340;&#36712;&#36857;&#65292;&#36890;&#36807;&#20351;&#29992;&#32463;&#39564;&#36125;&#21494;&#26031;&#26041;&#27861;&#21487;&#20197;&#20026;&#36712;&#36857;&#36319;&#36394;&#38382;&#39064;&#20013;&#24517;&#35201;&#30340;&#36816;&#21160;&#27169;&#22411;&#25552;&#20379;&#20449;&#24687;&#65292;&#24182;&#21487;&#20197;&#24110;&#21161;&#35268;&#33539;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2211.01696</link><description>&lt;p&gt;
&#23545;&#35937;&#36712;&#36857;&#34920;&#31034;&#27169;&#22411;&#30340;&#32463;&#39564;&#36125;&#21494;&#26031;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Empirical Bayes Analysis of Object Trajectory Representation Models. (arXiv:2211.01696v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#23545;&#35937;&#36712;&#36857;&#34920;&#31034;&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#21644;&#25311;&#21512;&#35823;&#24046;&#20043;&#38388;&#30340;&#26435;&#34913;&#36827;&#34892;&#20102;&#32463;&#39564;&#20998;&#26512;&#65292;&#21457;&#29616;&#31616;&#21333;&#30340;&#32447;&#24615;&#27169;&#22411;&#23601;&#33021;&#22815;&#39640;&#24230;&#37325;&#29616;&#30495;&#23454;&#19990;&#30028;&#30340;&#36712;&#36857;&#65292;&#36890;&#36807;&#20351;&#29992;&#32463;&#39564;&#36125;&#21494;&#26031;&#26041;&#27861;&#21487;&#20197;&#20026;&#36712;&#36857;&#36319;&#36394;&#38382;&#39064;&#20013;&#24517;&#35201;&#30340;&#36816;&#21160;&#27169;&#22411;&#25552;&#20379;&#20449;&#24687;&#65292;&#24182;&#21487;&#20197;&#24110;&#21161;&#35268;&#33539;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#23545;&#35937;&#36712;&#36857;&#24314;&#27169;&#20013;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#25311;&#21512;&#35823;&#24046;&#20043;&#38388;&#30340;&#26435;&#34913;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#32463;&#39564;&#20998;&#26512;&#12290;&#36890;&#36807;&#20998;&#26512;&#22810;&#20010;&#22823;&#22411;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#30340;&#32447;&#24615;&#27169;&#22411;&#22312;&#30456;&#20851;&#26102;&#38388;&#33539;&#22260;&#20869;&#20351;&#29992;&#36739;&#23569;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;&#23601;&#33021;&#22815;&#39640;&#24230;&#37325;&#29616;&#30495;&#23454;&#19990;&#30028;&#30340;&#36712;&#36857;&#12290;&#36825;&#19968;&#21457;&#29616;&#20801;&#35768;&#23558;&#36712;&#36857;&#36319;&#36394;&#21644;&#39044;&#27979;&#20316;&#20026;&#36125;&#21494;&#26031;&#36807;&#28388;&#38382;&#39064;&#36827;&#34892;&#20844;&#24335;&#21270;&#12290;&#25105;&#20204;&#37319;&#29992;&#32463;&#39564;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#30340;&#20808;&#39564;&#20998;&#24067;&#65292;&#36825;&#20123;&#20808;&#39564;&#20998;&#24067;&#21487;&#20197;&#20026;&#36712;&#36857;&#36319;&#36394;&#38382;&#39064;&#20013;&#24517;&#35201;&#30340;&#36816;&#21160;&#27169;&#22411;&#25552;&#20379;&#20449;&#24687;&#65292;&#24182;&#21487;&#20197;&#24110;&#21161;&#35268;&#33539;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#20027;&#24352;&#22312;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#20013;&#20351;&#29992;&#32447;&#24615;&#36712;&#36857;&#34920;&#31034;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20204;&#30446;&#21069;&#24182;&#19981;&#20250;&#38480;&#21046;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an in-depth empirical analysis of the trade-off between model complexity and fit error in modelling object trajectories. Analyzing several large public datasets, we show that simple linear models do represent real-world trajectories with high fidelity over relevant time scales at very moderate model complexity. This finding allows the formulation of trajectory tracking and prediction as a Bayesian filtering problem. Using an Empirical Bayes approach, we estimate prior distributions over model parameters from the data. These prior distributions inform the motion models necessary in the trajectory tracking problem and can help regularize prediction models. We argue for the use of linear trajectory representation models in trajectory prediction tasks as they do not limit prediction performance currently.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#65288;TAMP&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#24207;&#21015;&#39044;&#27979;&#36741;&#21161;&#25628;&#32034;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35268;&#21010;&#25928;&#29575;&#65292;&#36816;&#34892;&#26102;&#38388;&#32553;&#30701;&#20102;80%&#12290;</title><link>http://arxiv.org/abs/2211.01576</link><description>&lt;p&gt;
&#22522;&#20110;&#24207;&#21015;&#30340;&#35745;&#21010;&#21487;&#34892;&#24615;&#39044;&#27979;&#29992;&#20110;&#39640;&#25928;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Sequence-Based Plan Feasibility Prediction for Efficient Task and Motion Planning. (arXiv:2211.01576v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#65288;TAMP&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#24207;&#21015;&#39044;&#27979;&#36741;&#21161;&#25628;&#32034;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35268;&#21010;&#25928;&#29575;&#65292;&#36816;&#34892;&#26102;&#38388;&#32553;&#30701;&#20102;80%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#21487;&#29992;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#65288;TAMP&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#23384;&#22312;&#22810;&#20010;&#20851;&#33410;&#21644;&#21487;&#31227;&#21160;&#38556;&#30861;&#29289;&#30340;&#29615;&#22659;&#20013;&#35299;&#20915;&#31227;&#21160;&#25805;&#20316;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24605;&#36335;&#26159;&#36890;&#36807;&#23398;&#20064;&#30340;&#35745;&#21010;&#21487;&#34892;&#24615;&#39044;&#27979;&#22120;&#23545;&#20256;&#32479;TAMP&#35268;&#21010;&#22120;&#30340;&#25628;&#32034;&#36807;&#31243;&#36827;&#34892;&#20559;&#32622;&#12290;&#31639;&#27861;&#30340;&#26680;&#24515;&#26159;PIGINet&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#25509;&#25910;&#20219;&#21153;&#35745;&#21010;&#12289;&#30446;&#26631;&#21644;&#21021;&#22987;&#29366;&#24577;&#65292;&#24182;&#39044;&#27979;&#19982;&#20219;&#21153;&#35745;&#21010;&#30456;&#20851;&#32852;&#30340;&#36816;&#21160;&#36712;&#36857;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#23558;PIGINet&#38598;&#25104;&#21040;&#19968;&#20010;TAMP&#35268;&#21010;&#22120;&#20013;&#65292;&#35813;&#35268;&#21010;&#22120;&#29983;&#25104;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#39640;&#23618;&#20219;&#21153;&#35745;&#21010;&#65292;&#25353;&#29031;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;&#25490;&#24207;&#65292;&#24182;&#20381;&#27425;&#36827;&#34892;&#32454;&#21270;&#12290;&#25105;&#20204;&#23545;&#19971;&#31181;&#21416;&#25151;&#37325;&#25490;&#38382;&#39064;&#30340;TAMP&#31639;&#27861;&#36816;&#34892;&#26102;&#38388;&#36827;&#34892;&#35780;&#20272;&#65292;&#23558;&#20854;&#24615;&#33021;&#19982;&#38750;&#23398;&#20064;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PIGINet&#26174;&#30528;&#25552;&#39640;&#20102;&#35268;&#21010;&#25928;&#29575;&#65292;&#22312;&#29366;&#24577;&#36739;&#23567;&#30340;&#38382;&#39064;&#19978;&#32553;&#30701;&#20102;&#36816;&#34892;&#26102;&#38388;80%&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a learning-enabled Task and Motion Planning (TAMP) algorithm for solving mobile manipulation problems in environments with many articulated and movable obstacles. Our idea is to bias the search procedure of a traditional TAMP planner with a learned plan feasibility predictor. The core of our algorithm is PIGINet, a novel Transformer-based learning method that takes in a task plan, the goal, and the initial state, and predicts the probability of finding motion trajectories associated with the task plan. We integrate PIGINet within a TAMP planner that generates a diverse set of high-level task plans, sorts them by their predicted likelihood of feasibility, and refines them in that order. We evaluate the runtime of our TAMP algorithm on seven families of kitchen rearrangement problems, comparing its performance to that of non-learning baselines. Our experiments show that PIGINet substantially improves planning efficiency, cutting down runtime by 80% on problems with small state
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;COVID-19&#26816;&#27979;&#30340;&#26032;&#39062;&#21306;&#22495;&#24341;&#23548;&#30340;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#32954;&#25513;&#27169;&#20449;&#24687;&#26469;&#35782;&#21035;&#26377;&#25928;&#21306;&#22495;&#65292;&#20197;&#23398;&#20064;&#26356;&#26377;&#29992;&#30340;COVID-19&#26816;&#27979;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2211.00313</link><description>&lt;p&gt;
RGMIM: &#21306;&#22495;&#24341;&#23548;&#30340;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#29992;&#20110;COVID-19&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
RGMIM: Region-Guided Masked Image Modeling for COVID-19 Detection. (arXiv:2211.00313v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;COVID-19&#26816;&#27979;&#30340;&#26032;&#39062;&#21306;&#22495;&#24341;&#23548;&#30340;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#32954;&#25513;&#27169;&#20449;&#24687;&#26469;&#35782;&#21035;&#26377;&#25928;&#21306;&#22495;&#65292;&#20197;&#23398;&#20064;&#26356;&#26377;&#29992;&#30340;COVID-19&#26816;&#27979;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#33258;&#30417;&#30563;&#23398;&#20064;&#27491;&#22312;&#24555;&#36895;&#25512;&#36827;&#21307;&#23398;&#39046;&#22495;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#12290;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#25513;&#30422;&#20102;&#19968;&#32452;&#36755;&#20837;&#20687;&#32032;&#24182;&#35797;&#22270;&#39044;&#27979;&#36974;&#30422;&#30340;&#20687;&#32032;&#12290;&#20256;&#32479;&#30340;MIM&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#38543;&#26426;&#25513;&#33180;&#31574;&#30053;&#12290;&#19982;&#26222;&#36890;&#22270;&#20687;&#30456;&#27604;&#65292;&#21307;&#23398;&#22270;&#20687;&#24448;&#24448;&#20855;&#26377;&#29992;&#20110;&#30142;&#30149;&#26816;&#27979;&#30340;&#23567;&#21306;&#22495;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#19987;&#27880;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#33258;&#21160;COVID-19&#35782;&#21035;&#26041;&#38754;&#36827;&#34892;&#35780;&#20272;&#12290;&#26041;&#27861;&#65306;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21306;&#22495;&#24341;&#23548;&#30340;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65288;RGMIM&#65289;&#29992;&#20110;COVID-19&#26816;&#27979;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#25513;&#33180;&#31574;&#30053;&#65292;&#21033;&#29992;&#32954;&#25513;&#27169;&#20449;&#24687;&#26469;&#35782;&#21035;&#26377;&#25928;&#21306;&#22495;&#65292;&#20197;&#23398;&#20064;&#26356;&#26377;&#29992;&#30340;COVID-19&#26816;&#27979;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#20116;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65288;MAE&#65292;SKD&#65292;Cross&#65292;BYOL&#21644;SimSiam&#65289;&#36827;&#34892;&#23545;&#27604;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23450;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: Self-supervised learning is rapidly advancing computer-aided diagnosis in the medical field. Masked image modeling (MIM) is one of the self-supervised learning methods that masks a subset of input pixels and attempts to predict the masked pixels. Traditional MIM methods often employ a random masking strategy. In comparison to ordinary images, medical images often have a small region of interest for disease detection. Consequently, we focus on fixing the problem in this work, which is evaluated by automatic COVID-19 identification. Methods: In this study, we propose a novel region-guided masked image modeling method (RGMIM) for COVID-19 detection in this paper. In our method, we devise a new masking strategy that employed lung mask information to identify valid regions to learn more useful information for COVID-19 detection. The proposed method was contrasted with five self-supervised learning techniques (MAE, SKD, Cross, BYOL, and, SimSiam). We present a quantitative evaluatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VRDS&#30340;&#20998;&#23618;&#25277;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#20215;&#20540;&#65292;&#20197;&#32553;&#23567;&#25490;&#21015;&#25277;&#26679;&#26041;&#27861;&#30340;&#20272;&#35745;&#26041;&#24046;&#65292;&#24182;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#21644;&#25968;&#25454;&#21024;&#38500;&#24212;&#29992;&#31243;&#24207;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2210.16835</link><description>&lt;p&gt;
&#21487;&#20449;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#30340;&#26041;&#24046;&#32553;&#23567;Shapley&#20540;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Variance reduced Shapley value estimation for trustworthy data valuation. (arXiv:2210.16835v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VRDS&#30340;&#20998;&#23618;&#25277;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#20215;&#20540;&#65292;&#20197;&#32553;&#23567;&#25490;&#21015;&#25277;&#26679;&#26041;&#27861;&#30340;&#20272;&#35745;&#26041;&#24046;&#65292;&#24182;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#21644;&#25968;&#25454;&#21024;&#38500;&#24212;&#29992;&#31243;&#24207;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#65292;&#29305;&#21035;&#26159;&#22312;&#31639;&#27861;&#39044;&#27979;&#21644;&#20915;&#31574;&#20013;&#37327;&#21270;&#25968;&#25454;&#20215;&#20540;&#65292;&#26159;&#25968;&#25454;&#20132;&#26131;&#22330;&#26223;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#30446;&#21069;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#26159;&#23450;&#20041;&#25968;&#25454;Shapley&#20540;&#65292;&#28982;&#21518;&#36890;&#36807;&#25490;&#24207;&#25277;&#26679;&#31639;&#27861;&#36827;&#34892;&#36817;&#20284;&#35745;&#31639;&#12290;&#20026;&#20102;&#24357;&#34917;&#25490;&#21015;&#25277;&#26679;&#31639;&#27861;&#30340;&#22823;&#20272;&#35745;&#26041;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31283;&#20581;&#30340;&#25968;&#25454;&#20272;&#20215;&#26041;&#27861;&#65292;&#20351;&#29992;&#20998;&#23618;&#25277;&#26679;&#65292;&#24182;&#21629;&#21517;&#20026;&#26041;&#24046;&#32553;&#23567;&#25968;&#25454;Shapley&#20540;&#65288;VRDS&#65289;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#23637;&#31034;&#20102;&#22914;&#20309;&#36827;&#34892;&#20998;&#23618;&#25277;&#26679;&#65292;&#27599;&#20010;&#23618;&#25277;&#22810;&#23569;&#26679;&#26412;&#65292;&#20197;&#21450;VRDS&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#21644;&#25968;&#25454;&#21024;&#38500;&#24212;&#29992;&#31243;&#24207;&#20013;&#35828;&#26126;&#20102;VRDS&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data valuation, especially quantifying data value in algorithmic prediction and decision-making, is a fundamental problem in data trading scenarios. The most widely used method is to define the data Shapley and approximate it by means of the permutation sampling algorithm. To make up for the large estimation variance of the permutation sampling that hinders the development of the data marketplace, we propose a more robust data valuation method using stratified sampling, named variance reduced data Shapley (VRDS for short). We theoretically show how to stratify, how many samples are taken at each stratum, and the sample complexity analysis of VRDS. Finally, the effectiveness of VRDS is illustrated in different types of datasets and data removal applications.
&lt;/p&gt;</description></item><item><title>&#20026;&#35299;&#20915;&#35821;&#38899;&#22686;&#24378;&#39046;&#22495;&#20013;&#8220;&#26080;&#28165;&#26224;&#35821;&#38899;&#8221;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#22686;&#24378;&#35821;&#38899;&#20316;&#20026;&#30446;&#26631;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#21363;&#20351;&#22312;&#22495;&#20869;&#21644;&#22495;&#22806;&#22122;&#22768;&#24046;&#24322;&#36739;&#22823;&#30340;&#24773;&#20917;&#19979;&#20173;&#33021;&#26377;&#25928;&#65292;&#23454;&#39564;&#32467;&#26524;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.15368</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#22122;&#22768;&#22686;&#24378;&#35821;&#38899;&#20316;&#20026;&#30446;&#26631;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#29992;&#20110;&#26080;&#28165;&#26224;&#35821;&#38899;&#30340;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
A Training and Inference Strategy Using Noisy and Enhanced Speech as Target for Speech Enhancement without Clean Speech. (arXiv:2210.15368v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15368
&lt;/p&gt;
&lt;p&gt;
&#20026;&#35299;&#20915;&#35821;&#38899;&#22686;&#24378;&#39046;&#22495;&#20013;&#8220;&#26080;&#28165;&#26224;&#35821;&#38899;&#8221;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#22686;&#24378;&#35821;&#38899;&#20316;&#20026;&#30446;&#26631;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#21363;&#20351;&#22312;&#22495;&#20869;&#21644;&#22495;&#22806;&#22122;&#22768;&#24046;&#24322;&#36739;&#22823;&#30340;&#24773;&#20917;&#19979;&#20173;&#33021;&#26377;&#25928;&#65292;&#23454;&#39564;&#32467;&#26524;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#28165;&#26224;&#35821;&#38899;&#26159;&#21457;&#23637;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#30340;&#23454;&#38469;&#25361;&#25112;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#30340;&#35757;&#32451;&#20934;&#21017;&#21644;&#35780;&#20272;&#25351;&#26631;&#20043;&#38388;&#23384;&#22312;&#19981;&#21487;&#36991;&#20813;&#30340;&#19981;&#21305;&#37197;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#19981;&#21033;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#20854;&#20013;&#20351;&#29992;&#22686;&#24378;&#35821;&#38899;&#20316;&#20026;&#30446;&#26631;&#26469;&#25913;&#36827;&#20808;&#21069;&#25552;&#20986;&#30340;&#22122;&#22768;&#30446;&#26631;&#35757;&#32451;&#65288;NyTT&#65289;&#12290;&#30001;&#20110;&#22495;&#20869;&#22122;&#22768;&#19982;&#22806;&#37096;&#22122;&#22768;&#30340;&#21516;&#36136;&#24615;&#26159;NyTT&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#65292;&#25105;&#20204;&#36890;&#36807;&#28151;&#38899;&#35757;&#32451;&#22810;&#20010;&#23398;&#29983;&#27169;&#22411;&#65292;&#21253;&#25324;&#65306;1&#65289;&#20351;&#29992;&#25945;&#24072;&#27169;&#22411;&#20272;&#35745;&#30340;&#35821;&#38899;&#21644;&#22122;&#22768;&#36827;&#34892;&#22686;&#24378;&#30446;&#26631;&#35757;&#32451;&#65292;&#25110;&#32773;2&#65289;&#20351;&#29992;&#21407;&#22987;&#30340;&#22122;&#22768;&#35821;&#38899;&#21644;&#25945;&#24072;&#27169;&#22411;&#20272;&#35745;&#30340;&#22122;&#22768;&#36827;&#34892;&#22122;&#22768;&#30446;&#26631;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20960;&#31181;&#22522;&#32447;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#25945;&#24072;/&#23398;&#29983;&#25512;&#29702;&#26041;&#38754;&#65292;&#20854;&#20013;&#39044;&#27979;&#30340;&#28165;&#26224;&#35821;&#38899;&#26159;&#36890;&#36807;&#25945;&#24072;&#21644;&#26368;&#32456;&#23398;&#29983;&#27169;&#22411;&#25104;&#21151;&#22320;&#25512;&#23548;&#20986;&#26469;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The lack of clean speech is a practical challenge to the development of speech enhancement systems, which means that there is an inevitable mismatch between their training criterion and evaluation metric. In response to this unfavorable situation, we propose a training and inference strategy that additionally uses enhanced speech as a target by improving the previously proposed noisy-target training (NyTT). Because homogeneity between in-domain noise and extraneous noise is the key to the effectiveness of NyTT, we train various student models by remixing 1) the teacher model's estimated speech and noise for enhanced-target training or 2) raw noisy speech and the teacher model's estimated noise for noisy-target training. Experimental results show that our proposed method outperforms several baselines, especially with the teacher/student inference, where predicted clean speech is derived successively through the teacher and final student models.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;DiffusionDB&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#35268;&#27169;&#24222;&#22823;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25552;&#31034;&#25968;&#25454;&#38598;&#65292;&#24635;&#35745;&#21253;&#21547;1400&#19975;&#24352;&#22270;&#20687;&#21644;180&#19975;&#20010;&#21807;&#19968;&#25552;&#31034;&#12290;&#35813;&#25968;&#25454;&#38598;&#34987;&#29992;&#26469;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#35299;&#20915;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#26102;&#25152;&#38656;&#30340;&#36866;&#24403;&#25552;&#31034;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#20102;&#19968;&#20123;&#29305;&#23450;&#30340;&#25552;&#31034;&#26679;&#24335;&#21644;&#36229;&#21442;&#25968;&#20540;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#38169;&#35823;&#65292;&#29978;&#33267;&#29983;&#25104;&#35823;&#23548;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2210.14896</link><description>&lt;p&gt;
DiffusionDB: &#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#25552;&#31034;&#30011;&#24266;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models. (arXiv:2210.14896v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14896
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;DiffusionDB&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#35268;&#27169;&#24222;&#22823;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25552;&#31034;&#25968;&#25454;&#38598;&#65292;&#24635;&#35745;&#21253;&#21547;1400&#19975;&#24352;&#22270;&#20687;&#21644;180&#19975;&#20010;&#21807;&#19968;&#25552;&#31034;&#12290;&#35813;&#25968;&#25454;&#38598;&#34987;&#29992;&#26469;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#35299;&#20915;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#26102;&#25152;&#38656;&#30340;&#36866;&#24403;&#25552;&#31034;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#20102;&#19968;&#20123;&#29305;&#23450;&#30340;&#25552;&#31034;&#26679;&#24335;&#21644;&#36229;&#21442;&#25968;&#20540;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#38169;&#35823;&#65292;&#29978;&#33267;&#29983;&#25104;&#35823;&#23548;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#32534;&#20889;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#32454;&#33410;&#30340;&#22270;&#20687;&#38656;&#35201;&#36866;&#24403;&#30340;&#25552;&#31034;&#65292;&#32780;&#19988;&#24448;&#24448;&#19981;&#28165;&#26970;&#27169;&#22411;&#23545;&#19981;&#21516;&#25552;&#31034;&#30340;&#21453;&#24212;&#25110;&#26368;&#20339;&#25552;&#31034;&#26159;&#20160;&#20040;&#12290;&#20026;&#20102;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#35299;&#20915;&#36825;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DiffusionDB&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25552;&#31034;&#25968;&#25454;&#38598;&#65292;&#24635;&#35745;6.5TB&#65292;&#21253;&#21547;&#20351;&#29992;Stable Diffusion&#29983;&#25104;&#30340;1400&#19975;&#24352;&#22270;&#20687;&#65292;180&#19975;&#20010;&#21807;&#19968;&#25552;&#31034;&#21644;&#30001;&#30495;&#23454;&#29992;&#25143;&#25351;&#23450;&#30340;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25552;&#31034;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#29305;&#24449;&#65292;&#24182;&#25351;&#20986;&#20102;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#38169;&#35823;&#30340;&#29305;&#23450;&#36229;&#21442;&#25968;&#20540;&#21644;&#25552;&#31034;&#26679;&#24335;&#65292;&#24182;&#25552;&#20379;&#20102;&#28508;&#22312;&#26377;&#23475;&#27169;&#22411;&#20351;&#29992;&#30340;&#35777;&#25454;&#65292;&#22914;&#29983;&#25104;&#35823;&#23548;&#20449;&#24687;&#12290;&#36825;&#20010;&#20154;&#20026;&#39537;&#21160;&#30340;&#25968;&#25454;&#38598;&#30340;&#31354;&#21069;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#20026;&#20102;&#35299;&#25552;&#31034;&#21644;&#29983;&#25104;&#22270;&#20687;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#25552;&#20379;&#20102;&#28608;&#21160;&#20154;&#24515;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
With recent advancements in diffusion models, users can generate high-quality images by writing text prompts in natural language. However, generating images with desired details requires proper prompts, and it is often unclear how a model reacts to different prompts or what the best prompts are. To help researchers tackle these critical challenges, we introduce DiffusionDB, the first large-scale text-to-image prompt dataset totaling 6.5TB, containing 14 million images generated by Stable Diffusion, 1.8 million unique prompts, and hyperparameters specified by real users. We analyze the syntactic and semantic characteristics of prompts. We pinpoint specific hyperparameter values and prompt styles that can lead to model errors and present evidence of potentially harmful model usage, such as the generation of misinformation. The unprecedented scale and diversity of this human-actuated dataset provide exciting research opportunities in understanding the interplay between prompts and generat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2210.14891</link><description>&lt;p&gt;
&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65288;BNSL&#65289;&#65289;&#65292;&#23427;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65288;&#21363;&#24863;&#20852;&#36259;&#30340;&#35780;&#20272;&#25351;&#26631;&#38543;&#29992;&#20110;&#35757;&#32451;&#30340;&#35745;&#31639;&#37327;&#12289;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#19978;&#28216;&#24615;&#33021;&#21464;&#21270;&#32780;&#21464;&#21270;&#65289;&#23545;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#27599;&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#25193;&#25955;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#22810;&#27169;&#24577;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;AI&#23545;&#40784;&#12289;&#26426;&#22120;&#20154;&#12289;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#33976;&#39311;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#21517;&#20026;LABOR&#65292;&#26088;&#22312;&#26367;&#20195;&#29616;&#26377;&#30340;Neighbor Sampling&#31639;&#27861;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23427;&#33021;&#22815;&#37319;&#26679;&#26356;&#23569;&#30340;&#39030;&#28857;&#20294;&#19981;&#20250;&#29306;&#29298;&#36136;&#37327;&#65292;&#24182;&#19988;&#22312;&#30456;&#21516;&#30340;&#39030;&#28857;&#37319;&#26679;&#39044;&#31639;&#32422;&#26463;&#19979;&#65292;&#25910;&#25947;&#26356;&#24555;&#65292;&#21487;&#20197;&#20351;&#29992;&#26356;&#22823;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2210.13339</link><description>&lt;p&gt;
Layer-Neighbor Sampling -- GNN&#20013;&#32531;&#35299;&#37051;&#23621;&#29190;&#28856;&#38382;&#39064;&#30340;&#37319;&#26679;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Layer-Neighbor Sampling -- Defusing Neighborhood Explosion in GNNs. (arXiv:2210.13339v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#21517;&#20026;LABOR&#65292;&#26088;&#22312;&#26367;&#20195;&#29616;&#26377;&#30340;Neighbor Sampling&#31639;&#27861;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23427;&#33021;&#22815;&#37319;&#26679;&#26356;&#23569;&#30340;&#39030;&#28857;&#20294;&#19981;&#20250;&#29306;&#29298;&#36136;&#37327;&#65292;&#24182;&#19988;&#22312;&#30456;&#21516;&#30340;&#39030;&#28857;&#37319;&#26679;&#39044;&#31639;&#32422;&#26463;&#19979;&#65292;&#25910;&#25947;&#26356;&#24555;&#65292;&#21487;&#20197;&#20351;&#29992;&#26356;&#22823;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#22312;&#22823;&#35268;&#27169;&#35757;&#32451;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#37319;&#29992;&#23567;&#25209;&#37327;&#35757;&#32451;&#21644;&#37319;&#26679;&#21487;&#29992;&#20110;&#32531;&#35299;&#27492;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#21463;&#21040;&#37051;&#22495;&#29190;&#28856;&#29616;&#35937;&#30340;&#24433;&#21709;&#65292;&#35201;&#20040;&#24615;&#33021;&#36739;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#31639;&#27861;LAyer-neighBOR sampling&#65288;LABOR&#65289;&#12290;&#23427;&#34987;&#35774;&#35745;&#25104;Neighbor Sampling&#65288;NS&#65289;&#30340;&#30452;&#25509;&#26367;&#20195;&#21697;&#65292;&#20855;&#26377;&#30456;&#21516;&#30340;&#25193;&#23637;&#22240;&#23376;&#36229;&#21442;&#25968;&#65292;&#21516;&#26102;&#37319;&#26679;&#30340;&#39030;&#28857;&#25968;&#26368;&#22810;&#23569;7&#20493;&#65292;&#19981;&#20250;&#29306;&#29298;&#36136;&#37327;&#12290;&#36890;&#36807;&#35774;&#35745;&#65292;&#27599;&#20010;&#39030;&#28857;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#19982;&#21333;&#20010;&#39030;&#28857;&#19978;&#30340;NS&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#22312;&#30456;&#21516;&#30340;&#39030;&#28857;&#37319;&#26679;&#39044;&#31639;&#32422;&#26463;&#19979;&#65292;LABOR&#27604;&#29616;&#26377;&#30340;&#23618;&#37319;&#26679;&#26041;&#27861;&#25910;&#25947;&#26356;&#24555;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#27604;NS&#22823;112&#20493;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have received significant attention recently, but training them at a large scale remains a challenge. Mini-batch training coupled with sampling is used to alleviate this challenge. However, existing approaches either suffer from the neighborhood explosion phenomenon or have poor performance. To address these issues, we propose a new sampling algorithm called LAyer-neighBOR sampling (LABOR). It is designed to be a direct replacement for Neighbor Sampling (NS) with the same fanout hyperparameter while sampling up to 7 times fewer vertices, without sacrificing quality. By design, the variance of the estimator of each vertex matches NS from the point of view of a single vertex. Moreover, under the same vertex sampling budget constraints, LABOR converges faster than existing layer sampling approaches and can use up to 112 times larger batch sizes compared to NS.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19978;&#19979;&#25991;&#21518;&#35265;&#24615;&#30340; LMDP &#24378;&#21270;&#23398;&#20064;&#36951;&#25022;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22411;&#22522;&#30784;&#31639;&#27861;&#26694;&#26550;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#19982;&#35745;&#21010;&#35270;&#37326;&#23545;&#25968;&#30456;&#20851;&#30340; $\widetilde{O}\left(\sqrt{M \Gamma S A K}\right)$ &#36951;&#25022;&#24230;&#19978;&#38480;&#65292;&#24182;&#23545; alpha &#21521;&#37327;&#30340;&#24635;&#26041;&#24046;&#36827;&#34892;&#20998;&#26512;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010; $\Omega\left(\sqrt{M S A K}\right)$ &#30340;&#36951;&#25022;&#24230;&#19979;&#38480;&#65292;&#23427;&#22312; $\Gamma=2$ &#26102;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#19978;&#30028;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2210.11604</link><description>&lt;p&gt;
&#26080;&#35270;&#35268;&#21010;&#22320;&#25512;&#24191;&#27178;&#36328;&#21464;&#37327;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Horizon-Free and Variance-Dependent Reinforcement Learning for Latent Markov Decision Processes. (arXiv:2210.11604v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19978;&#19979;&#25991;&#21518;&#35265;&#24615;&#30340; LMDP &#24378;&#21270;&#23398;&#20064;&#36951;&#25022;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22411;&#22522;&#30784;&#31639;&#27861;&#26694;&#26550;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#19982;&#35745;&#21010;&#35270;&#37326;&#23545;&#25968;&#30456;&#20851;&#30340; $\widetilde{O}\left(\sqrt{M \Gamma S A K}\right)$ &#36951;&#25022;&#24230;&#19978;&#38480;&#65292;&#24182;&#23545; alpha &#21521;&#37327;&#30340;&#24635;&#26041;&#24046;&#36827;&#34892;&#20998;&#26512;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010; $\Omega\left(\sqrt{M S A K}\right)$ &#30340;&#36951;&#25022;&#24230;&#19979;&#38480;&#65292;&#23427;&#22312; $\Gamma=2$ &#26102;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#19978;&#30028;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#21518;&#35265;&#24615;&#19978;&#19979;&#25991;&#30340;&#28508;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243; (LMDPs) &#24378;&#21270;&#23398;&#20064; (RL) &#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#27169;&#22411;&#20048;&#35266;&#25110;&#20540;&#20048;&#35266;&#27714;&#35299;&#22120;&#23454;&#20363;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#20851;&#20110;&#36951;&#25022;&#24230;&#30340;&#36739;&#23567;&#37327;&#32423;&#20026; $\widetilde{O}\left(\sqrt{M \Gamma S A K}\right)$ &#30340;&#30028;&#38480;&#65292;&#20854;&#20013; $M$ &#26159;&#19978;&#19979;&#25991;&#25968;&#37327;&#65292;$S$ &#26159;&#29366;&#24577;&#25968;&#37327;&#65292;$A$ &#26159;&#21160;&#20316;&#25968;&#37327;&#65292;$K$ &#26159;&#22238;&#21512;&#25968;&#37327;&#65292;&#32780; $\Gamma \le S$ &#26159;&#20219;&#20309;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#26368;&#22823;&#36716;&#31227;&#27425;&#25968;&#12290;&#36951;&#25022;&#24230;&#21482;&#22312;&#35268;&#21010;&#35270;&#37326;&#20013;&#20197;&#23545;&#25968;&#24418;&#24335;&#32553;&#25918;&#65292;&#25152;&#20197; LMDP &#30340;&#35268;&#21010;&#35270;&#37326;&#30340;&#31532;&#19968;&#20010;(&#20960;&#20046;)&#26080;&#35270;&#30028;&#38480;&#23601;&#34987;&#20135;&#29983;&#20102;&#12290;&#25105;&#20204;&#30340;&#35770;&#35777;&#30340;&#20851;&#38190;&#26159;&#23545; alpha &#21521;&#37327;&#30340;&#24635;&#26041;&#24046;&#36827;&#34892;&#20998;&#26512;&#65292;&#35813;&#26041;&#24046;&#36890;&#36807;&#36882;&#24402;&#25216;&#26415;&#36827;&#34892;&#20102;&#20180;&#32454;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26032;&#30340; $\Omega\left(\sqrt{M S A K}\right)$ &#36951;&#25022;&#24615;&#19979;&#38480;&#34917;&#20805;&#20102;&#25105;&#20204;&#30340;&#27491;&#34917;&#32467;&#26524;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403; $\Gamma=2$ &#26102;&#65292;&#25105;&#20204;&#30340;&#19978;&#30028;&#26159;&#26497;&#23567;&#21270;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study regret minimization for reinforcement learning (RL) in Latent Markov Decision Processes (LMDPs) with context in hindsight. We design a novel model-based algorithmic framework which can be instantiated with both a model-optimistic and a value-optimistic solver. We prove an $\widetilde{O}\left(\sqrt{M \Gamma S A K}\right)$ regret bound where $M$ is the number of contexts, $S$ is the number of states, $A$ is the number of actions, $K$ is the number of episodes, and $\Gamma \le S$ is the maximum transition degree of any state-action pair. The regret bound only scales logarithmically with the planning horizon, thus yielding the first (nearly) horizon-free regret bound for LMDP. Key in our proof is an analysis of the total variance of alpha vectors, which is carefully bounded by a recursion-based technique. We complement our positive result with a novel $\Omega\left(\sqrt{M S A K}\right)$ regret lower bound with $\Gamma = 2$, which shows our upper bound minimax optimal when $\Gamma$
&lt;/p&gt;</description></item><item><title>DICTDIS&#26159;&#19968;&#31181;&#26032;&#39062;&#26377;&#35789;&#20856;&#32422;&#26463;&#30340;NMT&#31995;&#32479;&#65292;&#20854;&#21033;&#29992;&#22810;&#20010;&#23383;&#20856;&#20505;&#36873;&#39033;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#20174;&#22810;&#20041;&#35789;&#20013;&#28040;&#38500;&#32763;&#35793;&#27495;&#20041;&#30340;&#30446;&#30340;&#65292;&#25552;&#39640;&#20102;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2210.06996</link><description>&lt;p&gt;
DICTDIS&#65306;&#22522;&#20110;&#35789;&#20856;&#32422;&#26463;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#28040;&#27495;&#26041;&#27861;&#23545; NMT &#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
DICTDIS: Dictionary Constrained Disambiguation for Improved NMT. (arXiv:2210.06996v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06996
&lt;/p&gt;
&lt;p&gt;
DICTDIS&#26159;&#19968;&#31181;&#26032;&#39062;&#26377;&#35789;&#20856;&#32422;&#26463;&#30340;NMT&#31995;&#32479;&#65292;&#20854;&#21033;&#29992;&#22810;&#20010;&#23383;&#20856;&#20505;&#36873;&#39033;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#20174;&#22810;&#20041;&#35789;&#20013;&#28040;&#38500;&#32763;&#35793;&#27495;&#20041;&#30340;&#30446;&#30340;&#65292;&#25552;&#39640;&#20102;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#29305;&#23450;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65288;&#20363;&#22914;&#25945;&#32946;&#24212;&#29992;&#31243;&#24207;&#65289;&#22312;&#22810;&#35821;&#35328;&#31038;&#20250;&#20013;&#24110;&#21161;&#20351;&#20449;&#24687;&#23545;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#29992;&#25143;&#21487;&#35775;&#38382;&#26159;&#20855;&#26377;&#31038;&#20250;&#24847;&#20041;&#30340;&#12290;&#36825;&#31181; NMT &#31995;&#32479;&#24212;&#35813;&#20855;&#26377;&#35789;&#27719;&#32422;&#26463;&#24182;&#20174;&#39046;&#22495;&#29305;&#23450;&#30340;&#35789;&#20856;&#20013;&#27762;&#21462;&#12290;&#30001;&#20110;&#21333;&#35789;&#30340;&#22810;&#20041;&#24615;&#65292;&#35789;&#20856;&#20013;&#21487;&#33021;&#20250;&#20026;&#28304;&#21333;&#35789;&#25110;&#30701;&#35821;&#21576;&#29616;&#22810;&#20010;&#20505;&#36873;&#32763;&#35793;&#12290;&#36825;&#26102;&#65292;NMT &#27169;&#22411;&#38656;&#35201;&#36873;&#25321;&#19982;&#35821;&#22659;&#26368;&#30456;&#20851;&#30340;&#20505;&#36873;&#32763;&#35793;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#24573;&#30053;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#32780;&#20391;&#37325;&#20110;&#21333;&#20010;&#20505;&#36873;&#32422;&#26463;&#35774;&#32622;&#65292;&#20854;&#20013;&#30446;&#26631;&#35789;&#25110;&#30701;&#35821;&#34987;&#21333;&#20010;&#32422;&#26463;&#26367;&#25442;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DICTDIS&#30340;&#35789;&#20856;&#32422;&#26463; NMT &#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#28040;&#38500;&#20102;&#20174;&#23383;&#20856;&#20013;&#24471;&#20986;&#30340;&#22810;&#20010;&#20505;&#36873;&#32763;&#35793;&#30340;&#27495;&#20041;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#35757;&#32451;&#25968;&#25454;&#19982;&#22810;&#20010;&#23383;&#20856;&#20505;&#36873;&#39033;&#36827;&#34892;&#22686;&#37327;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#26399;&#38388;&#31215;&#26497;&#40723;&#21169;&#28040;&#38500;&#27495;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain-specific neural machine translation (NMT) systems (\eg, in educational applications) are socially significant with the potential to help make information accessible to a diverse set of users in multilingual societies. It is desirable that such NMT systems be lexically constrained and draw from domain-specific dictionaries. Dictionaries could present multiple candidate translations for a source word/phrase due to the polysemous nature of words. The onus is then on the NMT model to choose the contextually most appropriate candidate. Prior work has largely ignored this problem and focused on the single candidate constraint setting wherein the target word or phrase is replaced by a single constraint. In this work we present \dictdis, a lexically constrained NMT system that disambiguates between multiple candidate translations derived from dictionaries. We achieve this by augmenting training data with multiple dictionary candidates to actively encourage disambiguation during training
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#29992;&#25143;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#29992;&#25143;&#22870;&#21169;&#30697;&#38453;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2210.05355</link><description>&lt;p&gt;
&#20302;&#31209;&#22870;&#21169;&#19979;&#30340;&#22810;&#29992;&#25143;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-User Reinforcement Learning with Low Rank Rewards. (arXiv:2210.05355v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#29992;&#25143;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#29992;&#25143;&#22870;&#21169;&#30697;&#38453;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#21327;&#20316;&#22810;&#29992;&#25143;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#27492;&#35774;&#32622;&#20013;&#65292;&#26377;&#22810;&#20010;&#29992;&#25143;&#20855;&#26377;&#30456;&#21516;&#30340;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#21644;&#36716;&#31227;&#27010;&#29575;&#65292;&#20294;&#20855;&#26377;&#19981;&#21516;&#30340;&#22870;&#21169;&#12290;&#22312;&#20551;&#35774;N&#20010;&#29992;&#25143;&#30340;&#22870;&#21169;&#30697;&#38453;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#33021;&#21542;&#35774;&#35745;&#20855;&#26377;&#26174;&#30528;&#36739;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#65292;&#19982;&#20026;&#27599;&#20010;&#29992;&#25143;&#20998;&#21035;&#23398;&#20064;MDP&#30340;&#31639;&#27861;&#30456;&#27604;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#23427;&#19982;N&#20010;&#29992;&#25143;&#29305;&#23450;&#30340;MDP&#19968;&#36215;&#25506;&#32034;&#22870;&#21169;&#65292;&#24182;&#21487;&#20197;&#22312;&#20004;&#20010;&#20851;&#38190;&#35774;&#32622;&#20013;&#39640;&#25928;&#22320;&#23398;&#20064;&#22870;&#21169;&#65306;&#34920;&#26684;MDP&#21644;&#32447;&#24615;MDP&#12290;&#24403;N&#24456;&#22823;&#19988;&#31209;&#26159;&#24120;&#25968;&#26102;&#65292;&#27599;&#20010;MDP&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#23545;&#29366;&#24577;&#31354;&#38388;&#22823;&#23567;&#21462;&#23545;&#25968;&#65292;&#36825;&#20195;&#34920;&#20102;&#22312;&#29366;&#24577;&#31354;&#38388;&#22823;&#23567;&#19978;&#30340;&#25351;&#25968;&#38477;&#20302;&#65288;&#19982;&#26631;&#20934;&#30340;&#8220;&#38750;&#21327;&#20316;&#8221;&#30456;&#27604;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we consider the problem of collaborative multi-user reinforcement learning. In this setting there are multiple users with the same state-action space and transition probabilities but with different rewards. Under the assumption that the reward matrix of the $N$ users has a low-rank structure -- a standard and practically successful assumption in the offline collaborative filtering setting -- the question is can we design algorithms with significantly lower sample complexity compared to the ones that learn the MDP individually for each user. Our main contribution is an algorithm which explores rewards collaboratively with $N$ user-specific MDPs and can learn rewards efficiently in two key settings: tabular MDPs and linear MDPs. When $N$ is large and the rank is constant, the sample complexity per MDP depends logarithmically over the size of the state-space, which represents an exponential reduction (in the state-space size) when compared to the standard ``non-collaborative
&lt;/p&gt;</description></item><item><title>Multi-CLS BERT&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;BERT&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;CLS&#26631;&#35760;&#24182;&#40723;&#21169;&#23427;&#20204;&#22810;&#26679;&#24615;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#25972;&#20307;&#20934;&#30830;&#24615;&#21644;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2210.05043</link><description>&lt;p&gt;
Multi-CLS BERT&#65306;&#19968;&#31181;&#39640;&#25928;&#30340;&#20256;&#32479;&#32452;&#21512;&#26041;&#27861;&#26367;&#20195;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Multi-CLS BERT: An Efficient Alternative to Traditional Ensembling. (arXiv:2210.05043v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05043
&lt;/p&gt;
&lt;p&gt;
Multi-CLS BERT&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;BERT&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;CLS&#26631;&#35760;&#24182;&#40723;&#21169;&#23427;&#20204;&#22810;&#26679;&#24615;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#25972;&#20307;&#20934;&#30830;&#24615;&#21644;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;CLS-based&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#32452;&#21512;BERT&#27169;&#22411;&#36890;&#24120;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#38656;&#35201;&#26356;&#22810;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;Multi-CLS BERT&#65292;&#23427;&#20351;&#29992;&#22810;&#20010;CLS&#26631;&#35760;&#21644;&#19968;&#20010;&#40723;&#21169;&#22810;&#26679;&#24615;&#30340;&#21442;&#25968;&#21270;&#21644;&#30446;&#26631;&#65292;&#20960;&#20046;&#19982;&#21333;&#20010;BERT&#27169;&#22411;&#19968;&#26679;&#39640;&#25928;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Multi-CLS BERT&#21487;&#38752;&#22320;&#25552;&#39640;&#20102;&#25972;&#20307;&#20934;&#30830;&#24615;&#21644;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensembling BERT models often significantly improves accuracy, but at the cost of significantly more computation and memory footprint. In this work, we propose Multi-CLS BERT, a novel ensembling method for CLS-based prediction tasks that is almost as efficient as a single BERT model. Multi-CLS BERT uses multiple CLS tokens with a parameterization and objective that encourages their diversity. Thus instead of fine-tuning each BERT model in an ensemble (and running them all at test time), we need only fine-tune our single Multi-CLS BERT model (and run the one model at test time, ensembling just the multiple final CLS embeddings). To test its effectiveness, we build Multi-CLS BERT on top of a state-of-the-art pretraining method for BERT (Aroca-Ouellette and Rudzicz, 2020). In experiments on GLUE and SuperGLUE we show that our Multi-CLS BERT reliably improves both overall accuracy and confidence estimation. When only 100 training samples are available in GLUE, the Multi-CLS BERT_Base model 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#26377;&#38480;&#25903;&#25345;&#30340;&#19968;&#33324;&#21442;&#25968;&#26680;&#36827;&#34892;TPP&#25512;&#29702;&#30340;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#31163;&#25955;&#21270;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#22810;&#39033;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.04635</link><description>&lt;p&gt;
FaDIn: &#38024;&#23545;&#20855;&#26377;&#19968;&#33324;&#21442;&#25968;&#26680;&#30340;Hawkes&#36807;&#31243;&#30340;&#24555;&#36895;&#31163;&#25955;&#21270;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
FaDIn: Fast Discretized Inference for Hawkes Processes with General Parametric Kernels. (arXiv:2210.04635v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#26377;&#38480;&#25903;&#25345;&#30340;&#19968;&#33324;&#21442;&#25968;&#26680;&#36827;&#34892;TPP&#25512;&#29702;&#30340;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#31163;&#25955;&#21270;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#22810;&#39033;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#28857;&#36807;&#31243;&#26159;&#24314;&#27169;&#20107;&#20214;&#25968;&#25454;&#30340;&#33258;&#28982;&#24037;&#20855;&#12290;&#22312;&#25152;&#26377;&#30340;&#26102;&#38388;&#28857;&#36807;&#31243;&#27169;&#22411;&#20013;&#65292;Hawkes&#36807;&#31243;&#34987;&#35777;&#26126;&#26159;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23427;&#20204;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#30340;&#36866;&#24403;&#24314;&#27169;&#65292;&#29305;&#21035;&#26159;&#22312;&#32771;&#34385;&#25351;&#25968;&#25110;&#38750;&#21442;&#25968;&#26680;&#26102;&#12290;&#23613;&#31649;&#38750;&#21442;&#25968;&#26680;&#26159;&#19968;&#31181;&#36873;&#25321;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#32780;&#25351;&#25968;&#26680;&#26356;&#20855;&#25968;&#25454;&#25928;&#29575;&#65292;&#23545;&#20110;&#31435;&#21363;&#35302;&#21457;&#26356;&#22810;&#20107;&#20214;&#30340;&#29305;&#23450;&#24212;&#29992;&#26356;&#26377;&#25928;&#65292;&#20294;&#23545;&#20110;&#38656;&#35201;&#20272;&#35745;&#24310;&#36831;&#30340;&#24212;&#29992;&#65288;&#22914;&#31070;&#32463;&#31185;&#23398;&#65289;&#65292;&#23427;&#20204;&#19981;&#22826;&#36866;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#26377;&#38480;&#25903;&#25345;&#30340;&#19968;&#33324;&#21442;&#25968;&#26680;&#36827;&#34892;TPP&#25512;&#29702;&#30340;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#25152;&#24320;&#21457;&#30340;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#21033;&#29992;&#20107;&#20214;&#30340;&#31163;&#25955;&#21270;&#30340;&#24555;&#36895;$\ell_2$&#26799;&#24230;&#27714;&#35299;&#22120;&#12290;&#22312;&#29702;&#35770;&#19978;&#25903;&#25345;&#31163;&#25955;&#21270;&#30340;&#20351;&#29992;&#21518;&#65292;&#36890;&#36807;&#22810;&#31181;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#26032;&#26041;&#27861;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal point processes (TPP) are a natural tool for modeling event-based data. Among all TPP models, Hawkes processes have proven to be the most widely used, mainly due to their adequate modeling for various applications, particularly when considering exponential or non-parametric kernels. Although non-parametric kernels are an option, such models require large datasets. While exponential kernels are more data efficient and relevant for specific applications where events immediately trigger more events, they are ill-suited for applications where latencies need to be estimated, such as in neuroscience. This work aims to offer an efficient solution to TPP inference using general parametric kernels with finite support. The developed solution consists of a fast $\ell_2$ gradient-based solver leveraging a discretized version of the events. After theoretically supporting the use of discretization, the statistical and computational efficiency of the novel approach is demonstrated through va
&lt;/p&gt;</description></item><item><title>&#36890;&#29992;&#30446;&#26631;&#26465;&#20214;&#27169;&#22411;GNM&#21487;&#20197;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#26426;&#22120;&#20154;&#25968;&#25454;&#23454;&#29616;&#26356;&#24378;&#22823;&#12289;&#26356;&#20581;&#22766;&#30340;&#23548;&#33322;&#24615;&#33021;&#65292;&#24182;&#33021;&#39537;&#21160;&#20219;&#20309;&#20855;&#26377;&#36866;&#24403;&#35270;&#35273;&#24863;&#30693;&#36755;&#20837;&#30340;&#26426;&#22120;&#20154;&#12290;</title><link>http://arxiv.org/abs/2210.03370</link><description>&lt;p&gt;
GNM: &#36890;&#29992;&#23548;&#33322;&#27169;&#22411;&#39537;&#21160;&#20219;&#20309;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
GNM: A General Navigation Model to Drive Any Robot. (arXiv:2210.03370v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03370
&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#30446;&#26631;&#26465;&#20214;&#27169;&#22411;GNM&#21487;&#20197;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#26426;&#22120;&#20154;&#25968;&#25454;&#23454;&#29616;&#26356;&#24378;&#22823;&#12289;&#26356;&#20581;&#22766;&#30340;&#23548;&#33322;&#24615;&#33021;&#65292;&#24182;&#33021;&#39537;&#21160;&#20219;&#20309;&#20855;&#26377;&#36866;&#24403;&#35270;&#35273;&#24863;&#30693;&#36755;&#20837;&#30340;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26159;&#35270;&#35273;&#23548;&#33322;&#30340;&#24378;&#26377;&#21147;&#24037;&#20855;&#65292;&#20294;&#22522;&#20110;&#23398;&#20064;&#30340;&#31574;&#30053;&#30340;&#33021;&#21147;&#21463;&#21040;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#26469;&#33258;&#22810;&#20010;&#19981;&#21516;&#20294;&#32467;&#26500;&#30456;&#20284;&#30340;&#26426;&#22120;&#20154;&#30340;&#25968;&#25454;&#22522;&#30784;&#19978;&#35757;&#32451;&#38754;&#21521;&#35270;&#35273;&#23548;&#33322;&#30340;&#36890;&#29992;&#30446;&#26631;&#26465;&#20214;&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#22312;&#21508;&#31181;&#29615;&#22659;&#21644;&#26426;&#36523;&#19978;&#30340;&#24191;&#27867;&#27867;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#38388;&#25968;&#25454;&#20849;&#20139;&#30340;&#24517;&#35201;&#35774;&#35745;&#20915;&#31574;&#65292;&#21253;&#25324;&#20351;&#29992;&#26102;&#38388;&#19978;&#19979;&#25991;&#21644;&#26631;&#20934;&#21270;&#30340;&#21160;&#20316;&#31354;&#38388;&#65292;&#24182;&#35777;&#26126;&#20102;&#20174;&#24322;&#26500;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#20840;&#23616;&#31574;&#30053;&#20248;&#20110;&#20219;&#20309;&#21333;&#19968;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;6&#20010;&#19981;&#21516;&#26426;&#22120;&#20154;&#30340;60&#23567;&#26102;&#23548;&#33322;&#36712;&#36857;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#26032;&#26426;&#22120;&#20154;&#19978;&#37096;&#32626;&#35757;&#32451;&#21518;&#30340;GNM&#65292;&#21253;&#25324;&#19968;&#20010;&#27424;&#39537;&#21160;&#30340;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35757;&#32451;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#21644;&#26356;&#21152;&#31283;&#20581;&#30340;&#23548;&#33322;&#24615;&#33021;&#65292;&#32780;GNM&#21487;&#20197;&#39537;&#21160;&#20219;&#20309;&#20855;&#26377;&#36866;&#24403;&#35270;&#35273;&#24863;&#30693;&#36755;&#20837;&#30340;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning provides a powerful tool for vision-based navigation, but the capabilities of learning-based policies are constrained by limited training data. If we could combine data from all available sources, including multiple kinds of robots, we could train more powerful navigation models. In this paper, we study how a general goal-conditioned model for vision-based navigation can be trained on data obtained from many distinct but structurally similar robots, and enable broad generalization across environments and embodiments. We analyze the necessary design decisions for effective data sharing across robots, including the use of temporal context and standardized action spaces, and demonstrate that an omnipolicy trained from heterogeneous datasets outperforms policies trained on any single dataset. We curate 60 hours of navigation trajectories from 6 distinct robots, and deploy the trained GNM on a range of new robots, including an underactuated quadrotor. We find that training on diver
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2210.01969</link><description>&lt;p&gt;
&#20998;&#23618;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Adversarial Inverse Reinforcement Learning. (arXiv:2210.01969v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#19968;&#33324;&#29992;&#20110;&#20174;&#28436;&#31034;&#20013;&#24674;&#22797;&#19987;&#23478;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#39640;&#24230;&#22797;&#26434;&#30340;&#12289;&#38271;&#26102;&#31243;&#20219;&#21153;&#65292;&#24674;&#22797;&#21333;&#19968;&#25972;&#20307;&#31574;&#30053;&#26159;&#22256;&#38590;&#30340;&#65292;&#32780;&#19987;&#23478;&#31574;&#30053;&#36890;&#24120;&#21253;&#21547;&#23376;&#20219;&#21153;&#23618;&#27425;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#32773;&#24320;&#21457;&#20102;&#20998;&#23618;&#27169;&#20223;&#23398;&#20064;&#65288;HIL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36873;&#39033;&#26694;&#26550;&#20013;&#26174;&#24335;&#22320;&#24314;&#27169;&#20219;&#21153;&#20013;&#30340;&#27963;&#21160;&#32467;&#26500;&#26469;&#23398;&#20064;&#20998;&#23618;&#31574;&#30053;&#12290;&#29616;&#26377;&#30340;HIL&#26041;&#27861;&#35201;&#20040;&#24573;&#35270;&#20102;&#23376;&#20219;&#21153;&#32467;&#26500;&#19982;&#23398;&#20064;&#31574;&#30053;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#35201;&#20040;&#26080;&#27861;&#21516;&#26102;&#22312;&#20998;&#23618;&#26694;&#26550;&#20013;&#23398;&#20064;&#39640;&#32423;&#21035;&#21644;&#20302;&#32423;&#21035;&#31574;&#30053;&#65292;&#23548;&#33268;&#20122;&#26368;&#20248;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;HIL&#31639;&#27861;&#8212;&#8212;&#20998;&#23618;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;H-AIRL&#65289;&#65292;&#23427;&#22312;&#26368;&#26032;&#30340;IL&#31639;&#27861;AIRL&#19978;&#25193;&#23637;&#20102;&#19968;&#27493;&#36873;&#39033;&#26694;&#26550;&#65292;&#37325;&#26032;&#23450;&#20041;&#20102;AIRL&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning (IL) has been proposed to recover the expert policy from demonstrations. However, it would be difficult to learn a single monolithic policy for highly-complex long-horizon tasks of which the expert policy usually contains subtask hierarchies. Therefore, Hierarchical Imitation Learning (HIL) has been developed to learn a hierarchical policy from expert demonstrations through explicitly modelling the activity structure in a task with the option framework. Existing HIL methods either overlook the causal relationship between the subtask structure and the learned policy, or fail to learn the high-level and low-level policy in the hierarchical framework in conjuncture, which leads to suboptimality. In this work, we propose a novel HIL algorithm -Hierarchical Adversarial Inverse Reinforcement Learning (H-AIRL), which extends a state-of-the-art (SOTA) IL algorithm -- AIRL, with the one-step option framework. Specifically, we redefine the AIRL objectives on the extended sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26080;&#38656;&#25104;&#21592;&#26597;&#35810;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#23398;&#20064;&#22823;&#23567;&#20026;$t$&#30340;&#20915;&#31574;&#26641;&#30340;&#31639;&#27861;&#65292;&#24182;&#25104;&#21151;&#37327;&#21270;&#20102;Kalai&#21644;Kanade&#30340;&#19981;&#30693;&#24615;&#22686;&#24378;&#31639;&#27861;&#65292;&#24471;&#21040;&#20102;&#31532;&#19968;&#20010;&#39640;&#25928;&#30340;&#37327;&#23376;&#19981;&#30693;&#24615;&#22686;&#24378;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.00212</link><description>&lt;p&gt;
&#39640;&#25928;&#37327;&#23376;&#19981;&#21487;&#30693;&#19981;&#24403;&#20915;&#31574;&#26641;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Quantum Agnostic Improper Learning of Decision Trees. (arXiv:2210.00212v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26080;&#38656;&#25104;&#21592;&#26597;&#35810;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#23398;&#20064;&#22823;&#23567;&#20026;$t$&#30340;&#20915;&#31574;&#26641;&#30340;&#31639;&#27861;&#65292;&#24182;&#25104;&#21151;&#37327;&#21270;&#20102;Kalai&#21644;Kanade&#30340;&#19981;&#30693;&#24615;&#22686;&#24378;&#31639;&#27861;&#65292;&#24471;&#21040;&#20102;&#31532;&#19968;&#20010;&#39640;&#25928;&#30340;&#37327;&#23376;&#19981;&#30693;&#24615;&#22686;&#24378;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30693;&#24615;&#35774;&#32622;&#26159;&#26368;&#31867;&#20284;&#20110;&#23398;&#20064;&#23545;&#25239;&#22122;&#22768;&#30340;PAC&#27169;&#22411;&#30340;&#26368;&#22256;&#38590;&#30340;&#27867;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;Poly$(n,t,{\frac{1}{\varepsilon}})$&#37327;&#23376;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#30693;&#24615;&#35774;&#32622;&#20013;&#26080;&#38656;&#25104;&#21592;&#26597;&#35810;&#21363;&#21487;&#23398;&#20064;&#22823;&#23567;&#20026;$t$&#30340;&#20915;&#31574;&#26641;&#65292;&#24182;&#19988;&#23454;&#20363;&#38388;&#20855;&#26377;&#22343;&#21248;&#36793;&#38469;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#23398;&#20064;&#20915;&#31574;&#26641;&#30340;&#31639;&#27861;&#65288;&#32463;&#20856;&#25110;&#37327;&#23376;&#65289;&#65292;&#19988;&#26080;&#38656;&#25104;&#21592;&#26597;&#35810;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35774;&#35745;&#37327;&#23376;&#29256;&#26412;&#30340;Goldreich-Levin&#31639;&#27861;&#65292;&#20351;&#29992;&#39640;&#24230;&#20559;&#32622;&#30340;&#20989;&#25968;&#39044;&#35328;&#26426;&#26469;&#26500;&#24314;&#37327;&#23376;&#19981;&#30693;&#24615;&#24369;&#23398;&#20064;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#37327;&#21270;Kalai&#21644;Kanade&#65288;NIPS 2009&#65289;&#30340;&#19981;&#30693;&#24615;&#22686;&#24378;&#31639;&#27861;&#65292;&#20197;&#33719;&#24471;&#31532;&#19968;&#20010;&#39640;&#25928;&#30340;&#37327;&#23376;&#19981;&#30693;&#24615;&#22686;&#24378;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#37327;&#23376;&#22686;&#24378;&#31639;&#27861;&#22312;&#36866;&#24212;&#24615;&#37327;&#23376;&#22686;&#24378;&#31639;&#27861;&#20013;&#65292;&#25152;&#26377;&#24369;&#23398;&#20064;&#22120;&#20559;&#24046;&#20381;&#36182;&#24615;&#26041;&#38754;&#37117;&#20855;&#26377;&#22810;&#39033;&#24335;&#25913;&#36827;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#22312;$V$&#20013;&#30340;&#26631;&#20934;&#21152;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The agnostic setting is the hardest generalization of the PAC model since it is akin to learning with adversarial noise. In this paper, we give a poly$(n,t,{\frac{1}{\varepsilon}})$ quantum algorithm for learning size $t$ decision trees with uniform marginal over instances, in the agnostic setting, without membership queries. Our algorithm is the first algorithm (classical or quantum) for learning decision trees in polynomial time without membership queries. We show how to construct a quantum agnostic weak learner by designing a quantum version of the classical Goldreich-Levin algorithm that works with strongly biased function oracles. We show how to quantize the agnostic boosting algorithm by Kalai and Kanade (NIPS 2009) to obtain the first efficient quantum agnostic boosting algorithm. Our quantum boosting algorithm has a polynomial improvement in the dependence of the bias of the weak learner over all adaptive quantum boosting algorithms while retaining the standard speedup in the V
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#20351;&#29992;&#37327;&#23376;&#38567;&#31359;&#34892;&#36208;&#65288;QTW&#65289;&#22312;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#20135;&#29983;&#30340;&#37327;&#23376;&#21152;&#36895;&#25928;&#24212;&#12290;&#24403;&#19981;&#21516;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#30001;&#39640;&#20294;&#34180;&#30340;&#21183;&#22418;&#20998;&#38548;&#32780;&#26497;&#23567;&#20540;&#26159;&#24179;&#22374;&#30340;&#26102;&#20505;&#65292;QTW&#23545;&#27604;&#32463;&#20856;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#23454;&#29616;&#20102;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2209.14501</link><description>&lt;p&gt;
&#20851;&#20110;&#37327;&#23376;&#38567;&#31359;&#34892;&#36208;&#22312;&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;&#21152;&#36895;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
On Quantum Speedups for Nonconvex Optimization via Quantum Tunneling Walks. (arXiv:2209.14501v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#20351;&#29992;&#37327;&#23376;&#38567;&#31359;&#34892;&#36208;&#65288;QTW&#65289;&#22312;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#20135;&#29983;&#30340;&#37327;&#23376;&#21152;&#36895;&#25928;&#24212;&#12290;&#24403;&#19981;&#21516;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#30001;&#39640;&#20294;&#34180;&#30340;&#21183;&#22418;&#20998;&#38548;&#32780;&#26497;&#23567;&#20540;&#26159;&#24179;&#22374;&#30340;&#26102;&#20505;&#65292;QTW&#23545;&#27604;&#32463;&#20856;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#23454;&#29616;&#20102;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#24120;&#24773;&#20917;&#19979;&#65292;&#32463;&#20856;&#31639;&#27861;&#19981;&#33021;&#26377;&#25928;&#35299;&#20915;&#23616;&#37096;&#26497;&#23567;&#20540;&#30001;&#39640;&#33021;&#22721;&#38548;&#24320;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#21033;&#29992;&#37327;&#23376;&#38567;&#31359;&#30340;&#20840;&#23616;&#24433;&#21709;&#21487;&#33021;&#20135;&#29983;&#30340;&#38750;&#20984;&#20248;&#21270;&#37327;&#23376;&#21152;&#36895;&#25928;&#24212;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#37327;&#23376;&#38567;&#31359;&#34892;&#36208;&#65288;QTW&#65289;&#30340;&#37327;&#23376;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#23616;&#37096;&#26497;&#23567;&#20540;&#22823;&#32422;&#26159;&#20840;&#23616;&#26497;&#23567;&#20540;&#30340;&#38750;&#20984;&#38382;&#39064;&#19978;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19981;&#21516;&#23616;&#37096;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#38556;&#30861;&#24456;&#39640;&#20294;&#24456;&#34180;&#65292;&#21516;&#26102;&#26497;&#23567;&#20540;&#26159;&#24179;&#22374;&#30340;&#24773;&#20917;&#19979;&#65292;QTW&#30456;&#23545;&#20110;&#32463;&#20856;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#23454;&#29616;&#20102;&#37327;&#23376;&#21152;&#36895;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#21452;&#38449;&#26223;&#35266;&#65292;&#22312;&#24050;&#30693;&#20117;&#38519;&#30340;&#24773;&#20917;&#19979;&#65292;&#32463;&#20856;&#31639;&#27861;&#26080;&#27861;&#26377;&#25928;&#22320;&#25915;&#20987;&#19968;&#20010;&#30446;&#26631;&#20117;&#38519;&#65292;&#20294;&#26159;&#22312;&#32473;&#23450;&#21512;&#36866;&#30340;&#21021;&#22987;&#29366;&#24577;&#38468;&#36817;&#65292;QTW&#21487;&#20197;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#29992;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical algorithms are often not effective for solving nonconvex optimization problems where local minima are separated by high barriers. In this paper, we explore possible quantum speedups for nonconvex optimization by leveraging the global effect of quantum tunneling. Specifically, we introduce a quantum algorithm termed the quantum tunneling walk (QTW) and apply it to nonconvex problems where local minima are approximately global minima. We show that QTW achieves quantum speedup over classical stochastic gradient descents (SGD) when the barriers between different local minima are high but thin and the minima are flat. Based on this observation, we construct a specific double-well landscape, where classical algorithms cannot efficiently hit one target well knowing the other well but QTW can when given proper initial states near the known well. Finally, we corroborate our findings with numerical experiments.
&lt;/p&gt;</description></item><item><title>L2XGNN&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#36873;&#25321;&#35299;&#37322;&#23376;&#22270;&#65288;&#27169;&#20307;&#65289;&#23454;&#29616;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#35782;&#21035;&#36127;&#36131;&#39044;&#27979;&#22270;&#23646;&#24615;&#30340;&#27169;&#20307;&#65292;&#24182;&#23454;&#29616;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#21516;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2209.14402</link><description>&lt;p&gt;
L2XGNN&#65306;&#23398;&#20064;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
L2XGNN: Learning to Explain Graph Neural Networks. (arXiv:2209.14402v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14402
&lt;/p&gt;
&lt;p&gt;
L2XGNN&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#36873;&#25321;&#35299;&#37322;&#23376;&#22270;&#65288;&#27169;&#20307;&#65289;&#23454;&#29616;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#35782;&#21035;&#36127;&#36131;&#39044;&#27979;&#22270;&#23646;&#24615;&#30340;&#27169;&#20307;&#65292;&#24182;&#23454;&#29616;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#21516;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#19968;&#31867;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#23398;&#20064;&#35299;&#37322;&#65288;L2X&#65289;&#33539;&#24335;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;L2XGNN&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#20379;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;L2XGNN&#23398;&#20064;&#19968;&#31181;&#26426;&#21046;&#65292;&#29992;&#20110;&#36873;&#25321;&#35299;&#37322;&#23376;&#22270;&#65288;&#27169;&#20307;&#65289;&#65292;&#36825;&#20123;&#23376;&#22270;&#20165;&#29992;&#20110;GNN&#30340;&#20449;&#24687;&#20256;&#36882;&#25805;&#20316;&#20013;&#12290;&#23545;&#27169;&#20307;&#26045;&#21152;&#36825;&#26679;&#30340;&#38480;&#21046;&#36890;&#24120;&#20250;&#23548;&#33268;&#26356;&#26131;&#35299;&#37322;&#21644;&#26356;&#26377;&#25928;&#30340;&#35299;&#37322;&#12290;&#22312;&#20960;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;L2XGNN&#23454;&#29616;&#20102;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#21516;&#30340;&#20998;&#31867;&#31934;&#24230;&#65292;&#21516;&#26102;&#30830;&#20445;&#20165;&#20351;&#29992;&#25552;&#20379;&#30340;&#35299;&#37322;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;L2XGNN&#33021;&#22815;&#35782;&#21035;&#36127;&#36131;&#39044;&#27979;&#22270;&#23646;&#24615;&#30340;&#27169;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are a popular class of machine learning models. Inspired by the learning to explain (L2X) paradigm, we propose L2XGNN, a framework for explainable GNNs which provides faithful explanations by design. L2XGNN learns a mechanism for selecting explanatory subgraphs (motifs) which are exclusively used in the GNNs message-passing operations. L2XGNN is able to select, for each input graph, a subgraph with specific properties such as being sparse and connected. Imposing such constraints on the motifs often leads to more interpretable and effective explanations. Experiments on several datasets suggest that L2XGNN achieves the same classification accuracy as baseline methods using the entire input graph while ensuring that only the provided explanations are used to make predictions. Moreover, we show that L2XGNN is able to identify motifs responsible for the graph's properties it is intended to predict.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20027;&#21160;&#21160;&#24577;&#20559;&#22909;(ADP)&#26041;&#27861;&#65292;&#22312;&#31283;&#20581;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#29109;&#37327;&#21270;&#21160;&#24577;&#20559;&#22909;&#26469;&#27963;&#36291;&#22320;&#24179;&#34913;&#31574;&#30053;&#30340;&#25506;&#32034;&#21644;&#21033;&#29992;&#65292;&#26377;&#25928;&#36991;&#20813;&#31574;&#30053;&#30340;&#36807;&#20110;&#20445;&#23432;&#25110;&#36807;&#20110;&#20048;&#35266;&#65292;&#25552;&#39640;&#20102;&#31574;&#30053;&#22312;&#21508;&#31181;&#30446;&#26631;&#22495;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#36229;&#36807;&#29616;&#26377;&#30340;state-of-the-art&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.11596</link><description>&lt;p&gt;
&#37327;&#21270;&#20248;&#20110;&#36873;&#25321;&#65306;&#20351;&#29992;&#20027;&#21160;&#21160;&#24577;&#20559;&#22909;&#36827;&#34892;&#31283;&#20581;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Quantification before Selection: Active Dynamics Preference for Robust Reinforcement Learning. (arXiv:2209.11596v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20027;&#21160;&#21160;&#24577;&#20559;&#22909;(ADP)&#26041;&#27861;&#65292;&#22312;&#31283;&#20581;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#29109;&#37327;&#21270;&#21160;&#24577;&#20559;&#22909;&#26469;&#27963;&#36291;&#22320;&#24179;&#34913;&#31574;&#30053;&#30340;&#25506;&#32034;&#21644;&#21033;&#29992;&#65292;&#26377;&#25928;&#36991;&#20813;&#31574;&#30053;&#30340;&#36807;&#20110;&#20445;&#23432;&#25110;&#36807;&#20110;&#20048;&#35266;&#65292;&#25552;&#39640;&#20102;&#31574;&#30053;&#22312;&#21508;&#31181;&#30446;&#26631;&#22495;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#36229;&#36807;&#29616;&#26377;&#30340;state-of-the-art&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#31574;&#30053;&#25110;&#22788;&#29702;&#19981;&#21516;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#26410;&#30693;&#21160;&#24577;&#22833;&#37197;&#65292;&#23545;&#20110;&#35757;&#32451;&#40065;&#26834;&#24615;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#39046;&#22495;&#38543;&#26426;&#21270;(DR)&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#20248;&#38597;&#30340;&#26041;&#27861;&#65292;&#23427;&#35757;&#32451;&#19968;&#20010;&#20445;&#23432;&#30340;&#31574;&#30053;&#26469;&#25269;&#28040;&#19981;&#21516;&#30340;&#21160;&#24577;&#31995;&#32479;&#65292;&#32780;&#19981;&#38656;&#35201;&#20851;&#20110;&#30446;&#26631;&#31995;&#32479;&#21442;&#25968;&#30340;&#19987;&#23478;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;DR&#35757;&#32451;&#30340;&#31574;&#30053;&#24448;&#24448;&#36807;&#20110;&#20445;&#23432;&#65292;&#22312;&#30446;&#26631;&#22495;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#65292;&#20855;&#26377;&#19981;&#21516;&#21442;&#25968;&#30340;&#21160;&#24577;&#31995;&#32479;&#20026;&#31574;&#30053;&#25552;&#20379;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#38590;&#24230;&#65292;&#32780;&#22312;&#31995;&#32479;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#38590;&#24230;&#30001;&#20110;&#31574;&#30053;&#30340;&#28436;&#21464;&#32780;&#19981;&#26029;&#21464;&#21270;&#12290;&#22914;&#26524;&#25105;&#20204;&#33021;&#22815;&#22312;&#36816;&#34892;&#36807;&#31243;&#20013;&#31215;&#26497;&#22320;&#37319;&#26679;&#36866;&#21512;&#31574;&#30053;&#38590;&#24230;&#30340;&#31995;&#32479;&#65292;&#23601;&#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#65292;&#38450;&#27490;&#31574;&#30053;&#36807;&#20110;&#20445;&#23432;&#25110;&#36807;&#20110;&#20048;&#35266;&#12290;&#20026;&#20102;&#33853;&#23454;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20027;&#21160;&#21160;&#24577;&#20559;&#22909;~(ADP)&#65292;&#36890;&#36807;&#29109;&#37327;&#21270;&#21160;&#24577;&#20559;&#22909;&#24182;&#20027;&#21160;&#36873;&#25321;&#31574;&#30053;&#19982;&#20043;&#20132;&#20114;&#30340;&#21160;&#24577;&#31995;&#32479;&#65292;&#22312;&#25506;&#32034;&#21644;&#21033;&#29992;&#20043;&#38388;&#24179;&#34913;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;ADP&#26174;&#33879;&#25552;&#39640;&#20102;&#31574;&#30053;&#22312;&#21508;&#31181;&#30446;&#26631;&#22495;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#29366;&#24577;-of-the-art&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training a robust policy is critical for policy deployment in real-world systems or dealing with unknown dynamics mismatch in different dynamic systems. Domain Randomization~(DR) is a simple and elegant approach that trains a conservative policy to counter different dynamic systems without expert knowledge about the target system parameters. However, existing works reveal that the policy trained through DR tends to be over-conservative and performs poorly in target domains. Our key insight is that dynamic systems with different parameters provide different levels of difficulty for the policy, and the difficulty of behaving well in a system is constantly changing due to the evolution of the policy. If we can actively sample the systems with proper difficulty for the policy on the fly, it will stabilize the training process and prevent the policy from becoming over-conservative or over-optimistic. To operationalize this idea, we introduce Active Dynamics Preference~(ADP), which quantifie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#24515;&#29702;&#23398;&#24341;&#23548;&#24605;&#32500;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#38544;&#21947;&#29702;&#35299;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#20102;&#38544;&#21547;&#21464;&#37327;&#21644;&#20851;&#31995;&#26469;&#36873;&#25321;&#27491;&#30830;&#30340;&#37322;&#20041;&#12290;</title><link>http://arxiv.org/abs/2209.08141</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#29992;&#24515;&#29702;&#23398;&#21551;&#21457;&#30340;&#24605;&#32500;&#38142;&#35302;&#21457;&#35782;&#21035;&#38544;&#21547;&#21464;&#37327;&#21644;&#25512;&#29702;&#20851;&#31995;&#36827;&#34892;&#38544;&#21947;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Psychologically-informed chain-of-thought prompts for metaphor understanding in large language models. (arXiv:2209.08141v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#24515;&#29702;&#23398;&#24341;&#23548;&#24605;&#32500;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#38544;&#21947;&#29702;&#35299;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#20102;&#38544;&#21547;&#21464;&#37327;&#21644;&#20851;&#31995;&#26469;&#36873;&#25321;&#27491;&#30830;&#30340;&#37322;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#29702;&#35299;&#30340;&#27010;&#29575;&#27169;&#22411;&#26159;&#30740;&#31350;&#20154;&#20204;&#35821;&#35328;&#20351;&#29992;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#25163;&#21160;&#35774;&#35745;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#29992;&#36328;&#39046;&#22495;&#30340;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#27010;&#29575;&#27169;&#22411;&#30340;&#32467;&#26500;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#37319;&#29992;&#24605;&#32500;&#38142;&#35302;&#21457;&#26041;&#24335;&#26469;&#23558;&#27010;&#29575;&#27169;&#22411;&#20013;&#30340;&#32467;&#26500;&#24341;&#20837;LLMs&#20013;&#65292;&#20197;&#38544;&#21947;&#29702;&#35299;&#20026;&#20363;&#26469;&#25506;&#31350;&#36825;&#19968;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24605;&#32500;&#38142;&#35302;&#21457;&#26041;&#24335;&#23548;&#33268;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#38544;&#21547;&#21464;&#37327;&#65292;&#24182;&#24605;&#32771;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#36873;&#25321;&#36866;&#24403;&#30340;&#38544;&#21947;&#37322;&#20041;&#12290;&#25152;&#36873;&#25321;&#30340;&#38544;&#21547;&#21464;&#37327;&#21644;&#20851;&#31995;&#37117;&#22522;&#20110;&#35748;&#30693;&#24515;&#29702;&#23398;&#20013;&#30340;&#38544;&#21947;&#29702;&#35299;&#29702;&#35770;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#25552;&#31034;&#24212;&#29992;&#20110;GPT-3&#30340;&#20004;&#20010;&#26368;&#22823;&#29256;&#26412;&#65292;&#24182;&#26174;&#31034;&#23427;&#20204;&#21487;&#20197;&#25552;&#39640;&#37322;&#20041;&#36873;&#25321;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic models of language understanding are valuable tools for investigating human language use. However, they need to be hand-designed for a particular domain. In contrast, large language models (LLMs) are trained on text that spans a wide array of domains, but they lack the structure and interpretability of probabilistic models. In this paper, we use chain-of-thought prompts to introduce structures from probabilistic models into LLMs. We explore this approach in the case of metaphor understanding. Our chain-of-thought prompts lead language models to infer latent variables and reason about their relationships in order to choose appropriate paraphrases for metaphors. The latent variables and relationships chosen are informed by theories of metaphor understanding from cognitive psychology. We apply these prompts to the two largest versions of GPT-3 and show that they can improve performance in a paraphrase selection task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#27169;&#22411;&#29305;&#23450;&#20132;&#20114;&#26816;&#27979;&#26041;&#27861;&#26469;&#33258;&#21160;&#21270;&#23547;&#25214;GLMs&#20013;&#24212;&#28155;&#21152;&#30340;&#20132;&#20114;&#20316;&#29992;&#20197;&#25552;&#39640;&#20854;&#39044;&#27979;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.08030</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#20132;&#20114;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Detection of Interacting Variables for Generalized Linear Models via Neural Networks. (arXiv:2209.08030v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#27169;&#22411;&#29305;&#23450;&#20132;&#20114;&#26816;&#27979;&#26041;&#27861;&#26469;&#33258;&#21160;&#21270;&#23547;&#25214;GLMs&#20013;&#24212;&#28155;&#21152;&#30340;&#20132;&#20114;&#20316;&#29992;&#20197;&#25552;&#39640;&#20854;&#39044;&#27979;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GLMs&#65289;&#26159;&#20445;&#38505;&#20844;&#21496;&#32463;&#24120;&#20351;&#29992;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#20854;&#36136;&#37327;&#21462;&#20915;&#20110;&#20132;&#20114;&#21464;&#37327;&#30340;&#36873;&#25321;&#12290;&#23547;&#25214;&#20132;&#20114;&#20316;&#29992;&#23545;&#20110;&#20855;&#26377;&#35768;&#22810;&#21464;&#37327;&#30340;&#25968;&#25454;&#38598;&#26469;&#35828;&#38750;&#24120;&#32791;&#26102;&#65292;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#31934;&#31639;&#24072;&#30340;&#19987;&#19994;&#21028;&#26029;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#35270;&#35273;&#24615;&#33021;&#25351;&#26631;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#23547;&#25214;GLMs&#20013;&#24212;&#28155;&#21152;&#30340;&#20132;&#20114;&#20316;&#29992;&#20197;&#25552;&#39640;&#20854;&#39044;&#27979;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;&#27169;&#22411;&#29305;&#23450;&#30340;&#20132;&#20114;&#26816;&#27979;&#26041;&#27861;&#65292;&#36825;&#27604;&#20256;&#32479;&#26041;&#27861;&#65288;&#22914;Friedman H&#32479;&#35745;&#37327;&#25110;SHAP&#20540;&#65289;&#35201;&#24555;&#12290;&#22312;&#25968;&#23383;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20154;&#24037;&#29983;&#25104;&#25968;&#25454;&#20197;&#21450;&#24320;&#28304;&#25968;&#25454;&#19978;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quality of generalized linear models (GLMs), frequently used by insurance companies, depends on the choice of interacting variables. The search for interactions is time-consuming, especially for data sets with a large number of variables, depends much on expert judgement of actuaries, and often relies on visual performance indicators. Therefore, we present an approach to automating the process of finding interactions that should be added to GLMs to improve their predictive power. Our approach relies on neural networks and a model-specific interaction detection method, which is computationally faster than the traditionally used methods like Friedman H-Statistic or SHAP values. In numerical studies, we provide the results of our approach on artificially generated data as well as open-source data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#23398;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#24320;&#38144;&#38459;&#22622;&#26816;&#27979;&#21644;&#39044;&#32534;&#30721;&#26041;&#27861;&#65292;&#20351;&#29992;&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#36890;&#36807;&#20809;&#32447;&#36861;&#36394;&#24471;&#21040;&#20449;&#36947;&#20272;&#35745;&#65292;&#32463;&#36807;&#36880;&#27493;&#35757;&#32451;&#21644;&#35774;&#35745;&#39044;&#32534;&#30721;&#22120;&#65292;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.07350</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#23398;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#24320;&#38144;&#38459;&#22622;&#26816;&#27979;&#21644;&#39044;&#32534;&#30721;&#65306;LIDAR &#25968;&#25454;&#36935;&#19978;&#20809;&#32447;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
Overhead-Free Blockage Detection and Precoding Through Physics-Based Graph Neural Networks: LIDAR Data Meets Ray Tracing. (arXiv:2209.07350v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#23398;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#24320;&#38144;&#38459;&#22622;&#26816;&#27979;&#21644;&#39044;&#32534;&#30721;&#26041;&#27861;&#65292;&#20351;&#29992;&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#36890;&#36807;&#20809;&#32447;&#36861;&#36394;&#24471;&#21040;&#20449;&#36947;&#20272;&#35745;&#65292;&#32463;&#36807;&#36880;&#27493;&#35757;&#32451;&#21644;&#35774;&#35745;&#39044;&#32534;&#30721;&#22120;&#65292;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#38142;&#36335;&#65292;&#35299;&#20915;&#20102;&#26080;&#38656;&#36890;&#20449;&#24320;&#38144;&#30340;&#38459;&#22622;&#26816;&#27979;&#21644;&#39044;&#32534;&#30721;&#35774;&#35745;&#38382;&#39064;&#12290;&#38459;&#22622;&#26816;&#27979;&#36890;&#36807;&#22522;&#20110;&#29289;&#29702;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23545;&#28608;&#20809;&#38647;&#36798;&#65288;LIDAR&#65289;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#23454;&#29616;&#12290;&#23545;&#20110;&#39044;&#32534;&#30721;&#35774;&#35745;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545; LIDAR &#25968;&#25454;&#20135;&#29983;&#30340; 3D &#34920;&#38754;&#36827;&#34892;&#20809;&#32447;&#36861;&#36394;&#65292;&#24471;&#21040;&#19968;&#20010;&#21021;&#27493;&#30340;&#20449;&#36947;&#20272;&#35745;&#65292;&#20877;&#36827;&#34892;&#36880;&#27493;&#35757;&#32451;&#21644;&#35774;&#35745;&#39044;&#32534;&#30721;&#22120;&#12290;&#25968;&#20540;&#27169;&#25311;&#34920;&#26126;&#65292;&#38459;&#22622;&#26816;&#27979;&#25104;&#21151;&#29575;&#36798;&#21040; 95%&#12290;&#25105;&#20204;&#30340;&#25968;&#23383;&#39044;&#32534;&#30721;&#36798;&#21040;&#20102; 90% &#30340;&#23481;&#37327;&#65292;&#32780;&#27169;&#25311;&#39044;&#32534;&#30721;&#24615;&#33021;&#20248;&#20110;&#20197;&#24448;&#21033;&#29992; LIDAR &#36827;&#34892;&#39044;&#32534;&#30721;&#35774;&#35745;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this letter, we address blockage detection and precoder design for multiple-input multiple-output (MIMO) links, without communication overhead required. Blockage detection is achieved by classifying light detection and ranging (LIDAR) data through a physics-based graph neural network (GNN). For precoder design, a preliminary channel estimate is obtained by running ray tracing on a 3D surface obtained from LIDAR data. This estimate is successively refined and the precoder is designed accordingly. Numerical simulations show that blockage detection is successful with 95% accuracy. Our digital precoding achieves 90% of the capacity and analog precoding outperforms previous works exploiting LIDAR for precoder design.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20010;&#26032;&#30340;&#20559;&#35265;&#38382;&#39064;&#65306;&#37325;&#22797;&#20351;&#29992;&#20559;&#35265;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#8212;&#8212;&#37325;&#22797;&#20351;&#29992;&#24863;&#30693;&#37325;&#35201;&#24615;&#21152;&#26435;&#65288;RAW&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;RAW&#26174;&#33879;&#25552;&#39640;&#20102;&#31163;&#32447;&#26041;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.07074</link><description>&lt;p&gt;
&#20851;&#20110;&#31163;&#32447;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#20013;&#37325;&#22797;&#20351;&#29992;&#20559;&#35265;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Reuse Bias in Off-Policy Reinforcement Learning. (arXiv:2209.07074v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20010;&#26032;&#30340;&#20559;&#35265;&#38382;&#39064;&#65306;&#37325;&#22797;&#20351;&#29992;&#20559;&#35265;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#8212;&#8212;&#37325;&#22797;&#20351;&#29992;&#24863;&#30693;&#37325;&#35201;&#24615;&#21152;&#26435;&#65288;RAW&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;RAW&#26174;&#33879;&#25552;&#39640;&#20102;&#31163;&#32447;&#26041;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#35201;&#24615;&#37319;&#26679;&#26159;&#31163;&#32447;&#35780;&#20272;&#20013;&#24120;&#29992;&#30340;&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#30340;&#36712;&#36857;&#25910;&#30410;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#35757;&#32451;&#21487;&#33021;&#19981;&#31283;&#23450;&#65292;&#24182;&#19988;&#20197;&#21069;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#23581;&#35797;&#20027;&#35201;&#38598;&#20013;&#22312;&#20998;&#26512;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#26041;&#24046;&#19978;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#36825;&#31181;&#19981;&#31283;&#23450;&#24615;&#20063;&#19982;&#19968;&#20010;&#26032;&#30340;&#37325;&#22797;&#20351;&#29992;&#20559;&#35265;&#26377;&#20851;&#8212;&#8212;&#30001;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#35780;&#20272;&#21644;&#20248;&#21270;&#37325;&#22797;&#20351;&#29992;&#36896;&#25104;&#30340;&#31163;&#32447;&#35780;&#20272;&#20013;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#23637;&#31034;&#20102;&#24403;&#21069;&#31574;&#30053;&#30340;&#31163;&#32447;&#35780;&#20272;&#21644;&#20248;&#21270;&#19982;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#25968;&#25454;&#23548;&#33268;&#30446;&#26631;&#30340;&#36807;&#39640;&#20272;&#35745;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#30340;&#26799;&#24230;&#26356;&#26032;&#24182;&#36864;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#19968;&#20010;&#37325;&#22797;&#20351;&#29992;&#20559;&#35265;&#30340;&#39640;&#27010;&#29575;&#19978;&#38480;&#65292;&#24182;&#23637;&#31034;&#36890;&#36807;&#24341;&#20837;&#31163;&#32447;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#27010;&#24565;&#65292;&#21487;&#20197;&#36890;&#36807;&#25511;&#21046;&#19978;&#38480;&#30340;&#26576;&#19968;&#39033;&#26469;&#25511;&#21046;&#37325;&#22797;&#20351;&#29992;&#20559;&#24046;&#12290;&#22522;&#20110;&#36825;&#20123;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#37325;&#22797;&#20351;&#29992;&#24863;&#30693;&#37325;&#35201;&#24615;&#21152;&#26435;&#65288;RAW&#65289;&#65292;&#26469;&#32416;&#27491;&#37325;&#22797;&#20351;&#29992;&#20559;&#35265;&#24182;&#25552;&#39640;&#31163;&#32447;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#26469;&#35777;&#26126;&#65292;RAW&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#31163;&#32447;&#26041;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#65292;&#21253;&#25324;DDPG&#12289;SAC&#21644;TD3&#12290;
&lt;/p&gt;
&lt;p&gt;
Importance sampling (IS) is a popular technique in off-policy evaluation, which re-weights the return of trajectories in the replay buffer to boost sample efficiency. However, training with IS can be unstable and previous attempts to address this issue mainly focus on analyzing the variance of IS. In this paper, we reveal that the instability is also related to a new notion of Reuse Bias of IS -- the bias in off-policy evaluation caused by the reuse of the replay buffer for evaluation and optimization. We theoretically show that the off-policy evaluation and optimization of the current policy with the data from the replay buffer result in an overestimation of the objective, which may cause an erroneous gradient update and degenerate the performance. We further provide a high-probability upper bound of the Reuse Bias, and show that controlling one term of the upper bound can control the Reuse Bias by introducing the concept of stability for off-policy algorithms. Based on these analyses
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#33707;&#37324;-&#33576;&#26106;&#40784;&#26684;&#24418;&#24335;&#20027;&#20041;&#30340;&#28145;&#24230;&#23398;&#20064;&#26032;&#34920;&#36848;&#65292;&#24341;&#20837;&#20102;&#31070;&#32463;&#32593;&#32476;&#35760;&#24518;&#30340;&#26032;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#32447;&#24615;&#31639;&#23376;&#26041;&#31243;&#30452;&#25509;&#21521;&#21069;&#21644;&#21521;&#21518;&#20256;&#25773;&#24863;&#20852;&#36259;&#30340;&#37327;&#12290;&#25910;&#32553;&#26144;&#23556;&#29702;&#35770;&#34987;&#29992;&#26469;&#24320;&#21457;&#35760;&#24518;&#34928;&#20943;&#38543;&#32593;&#32476;&#23618;&#25968;&#22686;&#21152;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2209.05544</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#33707;&#37324;-&#33576;&#26106;&#40784;&#26684;&#34920;&#36848;
&lt;/p&gt;
&lt;p&gt;
The Mori-Zwanzig formulation of deep learning. (arXiv:2209.05544v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#33707;&#37324;-&#33576;&#26106;&#40784;&#26684;&#24418;&#24335;&#20027;&#20041;&#30340;&#28145;&#24230;&#23398;&#20064;&#26032;&#34920;&#36848;&#65292;&#24341;&#20837;&#20102;&#31070;&#32463;&#32593;&#32476;&#35760;&#24518;&#30340;&#26032;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#32447;&#24615;&#31639;&#23376;&#26041;&#31243;&#30452;&#25509;&#21521;&#21069;&#21644;&#21521;&#21518;&#20256;&#25773;&#24863;&#20852;&#36259;&#30340;&#37327;&#12290;&#25910;&#32553;&#26144;&#23556;&#29702;&#35770;&#34987;&#29992;&#26469;&#24320;&#21457;&#35760;&#24518;&#34928;&#20943;&#38543;&#32593;&#32476;&#23618;&#25968;&#22686;&#21152;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#19981;&#21487;&#36870;&#32479;&#35745;&#21147;&#23398;&#30340;&#33707;&#37324;-&#33576;&#26106;&#40784;&#26684;&#65288;MZ&#65289;&#24418;&#24335;&#20027;&#20041;&#65292;&#25552;&#20986;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#34920;&#36848;&#12290;&#36825;&#31181;&#26032;&#30340;&#34920;&#36848;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#20598;&#20851;&#31995;&#65292;&#36890;&#36807;&#32447;&#24615;&#31639;&#23376;&#26041;&#31243;&#30452;&#25509;&#21521;&#21069;&#21644;&#21521;&#21518;&#20256;&#25773;&#24863;&#20852;&#36259;&#30340;&#37327;&#65288;&#26465;&#20214;&#26399;&#26395;&#21644;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65289;&#12290;&#36825;&#20123;&#26032;&#26041;&#31243;&#21487;&#20197;&#20316;&#20026;&#24320;&#21457;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26032;&#30340;&#26377;&#25928;&#21442;&#25968;&#21270;&#30340;&#36215;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#36807;&#31639;&#23376;&#29702;&#35770;&#26041;&#27861;&#30740;&#31350;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;&#25152;&#25552;&#20986;&#30340;MZ&#24418;&#24335;&#20027;&#20041;&#33258;&#28982;&#24341;&#20837;&#20102;&#31070;&#32463;&#32593;&#32476;&#35760;&#24518;&#30340;&#26032;&#27010;&#24565;&#65292;&#22312;&#20302;&#32500;&#24314;&#27169;&#21644;&#21442;&#25968;&#21270;&#20013;&#36215;&#30528; fundamental &#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#25910;&#32553;&#26144;&#23556;&#29702;&#35770;&#65292;&#25105;&#20204;&#24320;&#21457;&#20986;&#20102;&#35760;&#24518;&#34928;&#20943;&#38543;&#32593;&#32476;&#23618;&#25968;&#22686;&#21152;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a new formulation of deep learning based on the Mori-Zwanzig (MZ) formalism of irreversible statistical mechanics. The new formulation is built upon the well-known duality between deep neural networks and discrete dynamical systems, and it allows us to directly propagate quantities of interest (conditional expectations and probability density functions) forward and backward through the network by means of exact linear operator equations. Such new equations can be used as a starting point to develop new effective parameterizations of deep neural networks, and provide a new framework to study deep-learning via operator theoretic methods. The proposed MZ formulation of deep learning naturally introduces a new concept, i.e., the memory of the neural network, which plays a fundamental role in low-dimensional modeling and parameterization. By using the theory of contraction mappings, we develop sufficient conditions for the memory of the neural network to decay with the number of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21452;&#35889;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#25968;&#25454;&#30340;&#38544;&#21547;&#23545;&#31216;&#24615;&#20013;&#23398;&#20064;&#32676;&#12289;&#19981;&#21487;&#32422;&#34920;&#31034;&#21644;&#23545;&#24212;&#30340;&#23436;&#20840;&#19981;&#21464;&#26144;&#23556;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#22522;&#20110;&#19981;&#21464;&#24615;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.03416</link><description>&lt;p&gt;
&#21452;&#35889;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Bispectral Neural Networks. (arXiv:2209.03416v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21452;&#35889;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#25968;&#25454;&#30340;&#38544;&#21547;&#23545;&#31216;&#24615;&#20013;&#23398;&#20064;&#32676;&#12289;&#19981;&#21487;&#32422;&#34920;&#31034;&#21644;&#23545;&#24212;&#30340;&#23436;&#20840;&#19981;&#21464;&#26144;&#23556;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#22522;&#20110;&#19981;&#21464;&#24615;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21452;&#35889;&#31070;&#32463;&#32593;&#32476;(BNNs)&#65292;&#29992;&#20110;&#23398;&#20064;&#34920;&#31034;&#22312;&#32039;&#33268;&#21487;&#20132;&#25442;&#32676;&#22312;&#23450;&#20041;&#20449;&#21495;&#30340;&#31354;&#38388;&#19978;&#30340;&#20316;&#29992;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#21452;&#35889;&#30340;&#24605;&#24819;&#65292;&#21363;&#26159;&#19968;&#20010;&#35299;&#26512;&#23450;&#20041;&#30340;&#32676;&#19981;&#21464;&#37327;&#65292;&#23427;&#26159;&#23436;&#25972;&#30340;&#8212;&#8212;&#20063;&#23601;&#26159;&#35828;&#65292;&#23427;&#20445;&#30041;&#20102;&#25152;&#26377;&#20449;&#21495;&#32467;&#26500;&#65292;&#21516;&#26102;&#21482;&#21435;&#38500;&#20102;&#30001;&#20110;&#32676;&#20316;&#29992;&#24341;&#36215;&#30340;&#21464;&#21270;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;BNNs&#33021;&#22815;&#36890;&#36807;&#25968;&#25454;&#20013;&#30340;&#38544;&#21547;&#23545;&#31216;&#24615;&#21516;&#26102;&#23398;&#20064;&#32676;&#12289;&#23427;&#20204;&#30340;&#19981;&#21487;&#32422;&#34920;&#31034;&#21644;&#23545;&#24212;&#30340;&#31561;&#21464;&#21644;&#23436;&#20840;&#19981;&#21464;&#26144;&#23556;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23436;&#25972;&#24615;&#23646;&#24615;&#36171;&#20104;&#20102;&#36825;&#20123;&#32593;&#32476;&#24378;&#22823;&#30340;&#22522;&#20110;&#19981;&#21464;&#24615;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#36825;&#39033;&#24037;&#20316;&#23558;Bispectral Neural Networks&#30830;&#31435;&#20026;&#31283;&#20581;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#30340;&#24378;&#22823;&#35745;&#31639;&#21407;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a neural network architecture, Bispectral Neural Networks (BNNs) for learning representations that are invariant to the actions of compact commutative groups on the space over which a signal is defined. The model incorporates the ansatz of the bispectrum, an analytically defined group invariant that is complete -- that is, it preserves all signal structure while removing only the variation due to group actions. Here, we demonstrate that BNNs are able to simultaneously learn groups, their irreducible representations, and corresponding equivariant and complete-invariant maps purely from the symmetries implicit in data. Further, we demonstrate that the completeness property endows these networks with strong invariance-based adversarial robustness. This work establishes Bispectral Neural Networks as a powerful computational primitive for robust invariant representation learning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21452;&#23556;&#21464;&#25442;&#26041;&#27861;&#65288;&#8220;&#25605;&#25292;&#8221;&#21644;&#8220;&#25671;&#26179;&#8221;&#65289;&#65292;&#29992;&#20110;&#25913;&#21892;&#28145;&#24230;&#33258;&#22238;&#24402;&#27169;&#22411;PixelCNN++&#20284;&#28982;&#24615;&#20013;&#30340;&#20302;&#32423;&#20559;&#24046;&#65292;&#24182;&#38548;&#31163;&#38271;&#31243;&#20381;&#36182;&#30340;&#36129;&#29486;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#22312;&#35780;&#20272;&#26102;&#24456;&#23481;&#26131;&#35745;&#31639;&#65292;&#24182;&#19988;&#22312;&#31163;&#32676;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.13579</link><description>&lt;p&gt;
&#25671;&#26179;&#30528;&#21069;&#34892;&#65306;&#22522;&#20110;PixelCNN++&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#23454;&#29616;&#20581;&#22766;&#31163;&#32676;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Shaken, and Stirred: Long-Range Dependencies Enable Robust Outlier Detection with PixelCNN++. (arXiv:2208.13579v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.13579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21452;&#23556;&#21464;&#25442;&#26041;&#27861;&#65288;&#8220;&#25605;&#25292;&#8221;&#21644;&#8220;&#25671;&#26179;&#8221;&#65289;&#65292;&#29992;&#20110;&#25913;&#21892;&#28145;&#24230;&#33258;&#22238;&#24402;&#27169;&#22411;PixelCNN++&#20284;&#28982;&#24615;&#20013;&#30340;&#20302;&#32423;&#20559;&#24046;&#65292;&#24182;&#38548;&#31163;&#38271;&#31243;&#20381;&#36182;&#30340;&#36129;&#29486;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#22312;&#35780;&#20272;&#26102;&#24456;&#23481;&#26131;&#35745;&#31639;&#65292;&#24182;&#19988;&#22312;&#31163;&#32676;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#31163;&#32676;&#26816;&#27979;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#24050;&#32463;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#30001;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#30340;&#20284;&#28982;&#24615;&#36890;&#24120;&#34987;&#35748;&#20026;&#22312;&#31163;&#32676;&#26816;&#27979;&#26041;&#38754;&#19981;&#23454;&#29992;&#12290;&#39318;&#20808;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20284;&#28982;&#24615;&#26131;&#21463;&#20302;&#32423;&#36755;&#20837;&#32479;&#35745;&#30340;&#20559;&#35265;&#24433;&#21709;&#12290;&#20854;&#27425;&#65292;&#35768;&#22810;&#26368;&#36817;&#30340;&#35299;&#20915;&#26041;&#26696;&#23545;&#32416;&#27491;&#36825;&#20123;&#20559;&#35265;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#25110;&#32773;&#22312;&#22797;&#26434;&#30340;&#33258;&#28982;&#25968;&#25454;&#38598;&#19978;&#27867;&#21270;&#33021;&#21147;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#33258;&#22238;&#24402;&#27169;&#22411;PixelCNN++&#30340;&#31163;&#32676;&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;PixelCNN++&#20284;&#28982;&#24615;&#20013;&#30340;&#20559;&#35265;&#20027;&#35201;&#26469;&#33258;&#20110;&#22522;&#20110;&#23616;&#37096;&#20381;&#36182;&#20851;&#31995;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21452;&#23556;&#21464;&#25442;&#26063;--&#8220;&#25605;&#25292;&#8221;&#21644;&#8220;&#25671;&#26179;&#8221;&#65292;&#36825;&#21487;&#20197;&#25913;&#21892;&#20302;&#32423;&#20559;&#24046;&#65292;&#24182;&#23558;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#30340;&#36129;&#29486;&#38548;&#31163;&#22312;PixelCNN++&#30340;&#20284;&#28982;&#24615;&#20013;&#12290;&#36825;&#20123;&#21464;&#25442;&#25104;&#26412;&#20302;&#24265;&#65292;&#24182;&#19988;&#22312;&#35780;&#20272;&#26102;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#35745;&#31639;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliable outlier detection is critical for real-world deployment of deep learning models. Although extensively studied, likelihoods produced by deep generative models have been largely dismissed as being impractical for outlier detection. First, deep generative model likelihoods are readily biased by low-level input statistics. Second, many recent solutions for correcting these biases are computationally expensive, or do not generalize well to complex, natural datasets. Here, we explore outlier detection with a state-of-the-art deep autoregressive model: PixelCNN++. We show that biases in PixelCNN++ likelihoods arise primarily from predictions based on local dependencies. We propose two families of bijective transformations -- ``stirring'' and ``shaking'' -- which ameliorate low-level biases and isolate the contribution of long-range dependencies to PixelCNN++ likelihoods. These transformations are inexpensive and readily computed at evaluation time. We test our approaches extensively 
&lt;/p&gt;</description></item><item><title>LAMDA-SSL&#26159;&#19968;&#27454;Python&#21322;&#30417;&#30563;&#23398;&#20064;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#29992;&#27861;&#25991;&#26723;&#21644;&#23454;&#29616;&#30340;&#25152;&#26377;&#31639;&#27861;&#65292;&#26497;&#22823;&#22320;&#26041;&#20415;&#20102;&#29992;&#25143;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2208.04610</link><description>&lt;p&gt;
LAMDA-SSL&#65306;Python&#20013;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
LAMDA-SSL: Semi-Supervised Learning in Python. (arXiv:2208.04610v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04610
&lt;/p&gt;
&lt;p&gt;
LAMDA-SSL&#26159;&#19968;&#27454;Python&#21322;&#30417;&#30563;&#23398;&#20064;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#29992;&#27861;&#25991;&#26723;&#21644;&#23454;&#29616;&#30340;&#25152;&#26377;&#31639;&#27861;&#65292;&#26497;&#22823;&#22320;&#26041;&#20415;&#20102;&#29992;&#25143;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LAMDA-SSL&#22312;GitHub&#19978;&#24320;&#28304;&#65292;&#20854;&#35814;&#32454;&#29992;&#27861;&#25991;&#26723;&#21487;&#20197;&#22312;https://ygzwqzd.github.io/LAMDA-SSL/&#19978;&#33719;&#24471;&#12290;&#35813;&#25991;&#26723;&#20174;&#19981;&#21516;&#35282;&#24230;&#35814;&#32454;&#20171;&#32461;&#20102;LAMDA-SSL&#65292;&#24182;&#21487;&#20197;&#20998;&#20026;&#22235;&#20010;&#37096;&#20998;&#12290;&#31532;&#19968;&#37096;&#20998;&#20171;&#32461;&#20102;LAMDA-SSL&#30340;&#35774;&#35745;&#29702;&#24565;&#12289;&#29305;&#28857;&#21644;&#21151;&#33021;&#12290;&#31532;&#20108;&#37096;&#20998;&#36890;&#36807;&#20016;&#23500;&#30340;&#20363;&#23376;&#35814;&#32454;&#35828;&#26126;&#20102;LAMDA-SSL&#30340;&#29992;&#27861;&#12290;&#31532;&#19977;&#37096;&#20998;&#20171;&#32461;&#20102;&#30001;LAMDA-SSL&#23454;&#29616;&#30340;&#25152;&#26377;&#31639;&#27861;&#65292;&#24110;&#21161;&#29992;&#25143;&#24555;&#36895;&#20102;&#35299;&#21644;&#36873;&#25321;SSL&#31639;&#27861;&#12290;&#31532;&#22235;&#37096;&#20998;&#23637;&#31034;&#20102;LAMDA-SSL&#30340;API&#12290;&#36825;&#20221;&#35814;&#32454;&#30340;&#25991;&#26723;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#29992;&#25143;&#29087;&#24713;LAMDA-SSL&#24037;&#20855;&#21253;&#21644;SSL&#31639;&#27861;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
LAMDA-SSL is open-sourced on GitHub and its detailed usage documentation is available at https://ygzwqzd.github.io/LAMDA-SSL/. This documentation introduces LAMDA-SSL in detail from various aspects and can be divided into four parts. The first part introduces the design idea, features and functions of LAMDA-SSL. The second part shows the usage of LAMDA-SSL by abundant examples in detail. The third part introduces all algorithms implemented by LAMDA-SSL to help users quickly understand and choose SSL algorithms. The fourth part shows the APIs of LAMDA-SSL. This detailed documentation greatly reduces the cost of familiarizing users with LAMDA-SSL toolkit and SSL algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;Meta&#30740;&#31350;&#21644;&#37096;&#32626;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22238;&#24402;&#39044;&#27979;&#27969;&#31243;&#65292;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#24615;&#33021;&#39044;&#27979;&#38382;&#39064;&#30340;&#22266;&#26377;&#38590;&#24230;&#65292;SuperPerforator&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2208.04351</link><description>&lt;p&gt;
&#23398;&#20064;&#23398;&#20064;&#22312;Meta&#30340;&#29983;&#20135;&#20013;&#39044;&#27979;&#24615;&#33021;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Learning to Learn to Predict Performance Regressions in Production at Meta. (arXiv:2208.04351v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;Meta&#30740;&#31350;&#21644;&#37096;&#32626;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22238;&#24402;&#39044;&#27979;&#27969;&#31243;&#65292;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#24615;&#33021;&#39044;&#27979;&#38382;&#39064;&#30340;&#22266;&#26377;&#38590;&#24230;&#65292;SuperPerforator&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#25429;&#25417;&#21644;&#24402;&#22240;&#20110;&#20195;&#30721;&#21464;&#26356;&#24341;&#36215;&#30340;&#24615;&#33021;&#22238;&#24402;&#26159;&#22256;&#38590;&#30340;&#65307;&#39044;&#27979;&#23427;&#20204;&#22312;&#21069;&#26399;&#26356;&#21152;&#22256;&#38590;&#12290;&#26412;&#25991;&#26159;&#20851;&#20110;&#33258;&#21160;&#23398;&#20064;&#39044;&#27979;&#36719;&#20214;&#24615;&#33021;&#22238;&#24402;&#30340;&#20837;&#38376;&#20171;&#32461;&#65292;&#25105;&#20204;&#22312;Meta&#30740;&#31350;&#21644;&#37096;&#32626;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22238;&#24402;&#39044;&#27979;&#27969;&#27700;&#32447;&#21518;&#33719;&#24471;&#20102;&#19968;&#20123;&#32463;&#39564;&#12290;&#26412;&#25991;&#25253;&#21578;&#20102;&#19968;&#20010;&#27604;&#36739;&#30740;&#31350;&#32467;&#26524;&#65292;&#21253;&#25324;&#22235;&#20010;&#36880;&#28176;&#22686;&#21152;&#22797;&#26434;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65306;(1) &#27169;&#31946;&#20195;&#30721;&#65292;(2) &#35789;&#34955;&#27169;&#22411;&#65292;(3) &#39044;&#20808;&#35757;&#32451;&#22909;&#30340;Transformer&#65292;&#21644;(4) &#33258;&#23450;&#20041;Transformer&#27169;&#22411;&#65292;&#21517;&#20026;SuperPerforator&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26174;&#31034;&#20102;&#24615;&#33021;&#39044;&#27979;&#38382;&#39064;&#30340;&#22266;&#26377;&#38590;&#24230;&#65292;&#36825;&#19968;&#38382;&#39064;&#30340;&#29305;&#28857;&#26159;&#33391;&#24615;&#21464;&#26356;&#23545;&#24694;&#24615;&#22238;&#24402;&#21464;&#26356;&#25968;&#37327;&#30340;&#24040;&#22823;&#19981;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#36136;&#30097;&#20102;Transformer&#26550;&#26500;&#22312;&#24615;&#33021;&#39044;&#27979;&#19978;&#30340;&#26222;&#36866;&#24615;&#65306;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;CodeBERT&#26041;&#27861;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340; poor &#34920;&#29616;&#65307;&#25105;&#20204;&#39640;&#24230;&#33258;&#23450;&#20041;&#30340;SuperPerforator&#27169;&#22411;--&#37319;&#29992;&#39640;&#32423;&#25216;&#26415;&#65292;&#22914;&#26080;&#25928;&#20195;&#30721;&#31227;&#38500;&#12289;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#21644;&#38543;&#26426;&#26435;&#37325;&#24179;&#22343;--&#34920;&#29616;&#26368;&#20339;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#25152;&#23398;&#32463;&#39564;&#65292;&#24182;&#21246;&#21202;&#20986;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#36259;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Catching and attributing code change-induced performance regressions in production is hard; predicting them beforehand, even harder. A primer on automatically learning to predict performance regressions in software, this article gives an account of the experiences we gained when researching and deploying an ML-based regression prediction pipeline at Meta. In this paper, we report on a comparative study with four ML models of increasing complexity, from (1) code-opaque, over (2) Bag of Words, (3) off-the-shelve Transformer-based, to (4) a bespoke Transformer-based model, coined SuperPerforator. Our investigation shows the inherent difficulty of the performance prediction problem, which is characterized by a large imbalance of benign onto regressing changes. Our results also call into question the general applicability of Transformer-based architectures for performance prediction: an off-the-shelve CodeBERT-based approach had surprisingly poor performance; our highly customized SuperPerf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;DNN&#35757;&#32451;&#26041;&#27861;DIVISION&#65292;&#36890;&#36807;&#20445;&#30041;LFC&#30340;&#39640;&#31934;&#24230;&#65292;&#23558;HFC&#21387;&#32553;&#25104;&#20302;&#31934;&#24230;&#30340;&#36731;&#37327;&#21103;&#26412;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20869;&#23384;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2208.04187</link><description>&lt;p&gt;
DIVISION: &#36890;&#36807;&#21452;&#28608;&#27963;&#31934;&#24230;&#23454;&#29616;&#39640;&#25928;&#20869;&#23384;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
DIVISION: Memory Efficient Training via Dual Activation Precision. (arXiv:2208.04187v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;DNN&#35757;&#32451;&#26041;&#27861;DIVISION&#65292;&#36890;&#36807;&#20445;&#30041;LFC&#30340;&#39640;&#31934;&#24230;&#65292;&#23558;HFC&#21387;&#32553;&#25104;&#20302;&#31934;&#24230;&#30340;&#36731;&#37327;&#21103;&#26412;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20869;&#23384;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28608;&#27963;&#21387;&#32553;&#35757;&#32451;&#25552;&#20379;&#20102;&#19968;&#20010;&#20943;&#23569;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20869;&#23384;&#25104;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#24037;&#20316;&#23558;&#37327;&#21270;&#20301;&#23485;&#25628;&#32034;&#19982;&#35757;&#32451;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20351;&#24471;&#36807;&#31243;&#22797;&#26434;&#19988;&#19981;&#22815;&#36879;&#26126;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#21387;&#32553;DNN&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24471;&#21040;&#20102;&#22914;&#19979;&#32467;&#35770;&#65306;&#31070;&#32463;&#32593;&#32476;&#21453;&#21521;&#20256;&#25773;&#20027;&#35201;&#21033;&#29992;&#28608;&#27963;&#22270;&#30340;&#20302;&#39057;&#32452;&#25104;&#37096;&#20998;&#65288;LFC)&#65292;&#32780;&#22823;&#37096;&#20998;&#20869;&#23384;&#26159;&#29992;&#26469;&#32531;&#23384;&#35757;&#32451;&#26399;&#38388;&#30340;&#39640;&#39057;&#32452;&#25104;&#37096;&#20998;&#65288;HFC&#65289;&#12290;&#36825;&#34920;&#26126;&#28608;&#27963;&#22270;&#30340;HFC&#22312;DNN&#35757;&#32451;&#26399;&#38388;&#39640;&#24230;&#20887;&#20313;&#19988;&#26131;&#20110;&#21387;&#32553;&#65292;&#36825;&#20063;&#21551;&#21457;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;Dual Activation Precision (DIVISION)&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;DIVISION&#20445;&#30041;LFC&#30340;&#39640;&#31934;&#24230;&#21103;&#26412;&#65292;&#24182;&#23558;HFC&#21387;&#32553;&#25104;&#20302;&#25968;&#20540;&#31934;&#24230;&#30340;&#36731;&#37327;&#21103;&#26412;&#12290;&#36825;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#20869;&#23384;&#25104;&#26412;&#32780;&#19981;&#20250;&#23545;DNN&#35757;&#32451;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Activation compressed training provides a solution towards reducing the memory cost of training deep neural networks~(DNNs). However, state-of-the-art work combines a search of quantization bit-width with the training, which makes the procedure complicated and less transparent. To this end, we propose a simple and effective method to compress DNN training. Our method is motivated by an instructive observation: DNN backward propagation mainly utilizes the low-frequency component (LFC) of the activation maps, while the majority of memory is for caching the high-frequency component (HFC) during the training. This indicates the HFC of activation maps is highly redundant and compressible during DNN training, which inspires our proposed Dual Activation Precision (DIVISION). During the training, DIVISION preserves the high-precision copy of LFC and compresses the HFC into a light-weight copy with low numerical precision. This can significantly reduce the memory cost without negatively affecti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#27169;&#22411;&#21644;Slater&#35889;&#27010;&#24565;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#21028;&#26029;&#21644;&#35299;&#20915;&#32447;&#24615;&#25490;&#24207;&#38382;&#39064;&#20013;&#25968;&#25454;&#26159;&#21542;&#21487;&#25490;&#24207;&#24182;&#32473;&#20986;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2208.03860</link><description>&lt;p&gt;
&#25490;&#24207;&#21487;&#34892;&#24615;&#21450;&#32447;&#24615;&#25490;&#24207;&#38382;&#39064;&#65306;&#26032;&#30340;&#27010;&#29575;&#27934;&#35265;&#19982;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rankability and Linear Ordering Problem: New Probabilistic Insight and Algorithms. (arXiv:2208.03860v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03860
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#27169;&#22411;&#21644;Slater&#35889;&#27010;&#24565;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#21028;&#26029;&#21644;&#35299;&#20915;&#32447;&#24615;&#25490;&#24207;&#38382;&#39064;&#20013;&#25968;&#25454;&#26159;&#21542;&#21487;&#25490;&#24207;&#24182;&#32473;&#20986;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#25490;&#24207;&#38382;&#39064;&#65288;LOP&#65289;&#24120;&#29992;&#20110;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#20854;&#30446;&#30340;&#22312;&#20110;&#23545;M&#20010;&#23545;&#35937;&#36827;&#34892;&#25490;&#24207;&#12290;&#34429;&#28982;&#24050;&#32463;&#21162;&#21147;&#21046;&#23450;&#39640;&#25928;&#30340;LOP&#31639;&#27861;&#65292;&#20294;&#39564;&#35777;&#25968;&#25454;&#26159;&#21542;&#26159;&#21487;&#25490;&#24207;&#30340;&#65292;&#20063;&#23601;&#26159;LOP&#35299;&#26159;&#21542;&#20855;&#26377;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#65292;&#21364;&#24471;&#21040;&#20102;&#24456;&#23569;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#27010;&#29575;&#35282;&#24230;&#65292;&#23558;&#25104;&#23545;&#27604;&#36739;&#30340;&#32467;&#26524;&#24314;&#27169;&#20026;&#20855;&#26377;&#20844;&#20849;&#21442;&#25968;&#30340;&#20271;&#21162;&#21033;&#21464;&#37327;&#65292;&#24182;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#21442;&#25968;&#12290;&#30001;&#20110;&#25152;&#38656;&#26522;&#20030;&#30340;&#34542;&#21147;&#26041;&#27861;&#20855;&#26377;O&#65288;M&#65281;&#65289;&#30340;&#31105;&#27490;&#22797;&#26434;&#24615;&#65292;&#22240;&#27492;&#25105;&#20204;&#37325;&#26032;&#26500;&#24605;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#25512;&#24191;Slater&#25351;&#25968;&#30340;Slater&#35889;&#27010;&#24565;&#65292;&#28982;&#21518;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#25214;&#21040;&#35889;&#65292;&#24182;&#20855;&#26377;O&#65288;M^3 2^M&#65289;&#30340;&#22797;&#26434;&#24230;&#65292;&#23545;&#20110;&#20013;&#31561;&#22823;&#23567;&#30340;M&#32780;&#35328;&#26159;&#21487;&#31649;&#29702;&#30340;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#31639;&#27861;&#36827;&#34892;&#20102;&#24494;&#23567;&#30340;&#20462;&#25913;&#65292;&#25105;&#20204;&#33021;&#22815;&#25214;&#21040;&#25152;&#26377;&#21487;&#25490;&#24207;&#24615;&#21644;&#25490;&#21517;&#30340;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The linear ordering problem (LOP), which consists in ordering M objects from their pairwise comparisons, is commonly applied in many areas of research. While efforts have been made to devise efficient LOP algorithms, verification of whether the data are rankable, that is, if the linear ordering problem (LOP) solutions have a meaningful interpretation, received much less attention. To address this problem, we adopt a probabilistic perspective where the results of pairwise comparisons are modeled as Bernoulli variables with a common parameter and we estimate the latter from the observed data. The brute-force approach to the required enumeration has a prohibitive complexity of O(M !), so we reformulate the problem and introduce a concept of the Slater spectrum that generalizes the Slater index, and then devise an algorithm to find the spectrum with complexity O(M^3 2^M) that is manageable for moderate values of M. Furthermore, with a minor modification of the algorithm, we are able to fin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#21487;&#24494;&#20998;&#30340;&#22522;&#20110;Agent&#30340;&#27969;&#34892;&#30149;&#27169;&#25311;&#35774;&#35745;&#8212;&#8212;GradABM&#65292;&#33021;&#22815;&#22312;&#36739;&#30701;&#26102;&#38388;&#20869;&#24555;&#36895;&#27169;&#25311;&#30334;&#19975;&#32423;&#21035;&#30340;&#20154;&#21475;&#65292;&#24182;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#21644;&#25509;&#21463;&#24322;&#26500;&#25968;&#25454;&#28304;&#65292;&#20026;&#26657;&#20934;&#12289;&#39044;&#27979;&#21644;&#35780;&#20272;&#25919;&#31574;&#24178;&#39044;&#25552;&#20379;&#20102;&#20415;&#21033;&#12290;</title><link>http://arxiv.org/abs/2207.09714</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#30340;&#22522;&#20110;Agent&#30340;&#27969;&#34892;&#30149;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Differentiable Agent-based Epidemiology. (arXiv:2207.09714v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#21487;&#24494;&#20998;&#30340;&#22522;&#20110;Agent&#30340;&#27969;&#34892;&#30149;&#27169;&#25311;&#35774;&#35745;&#8212;&#8212;GradABM&#65292;&#33021;&#22815;&#22312;&#36739;&#30701;&#26102;&#38388;&#20869;&#24555;&#36895;&#27169;&#25311;&#30334;&#19975;&#32423;&#21035;&#30340;&#20154;&#21475;&#65292;&#24182;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#21644;&#25509;&#21463;&#24322;&#26500;&#25968;&#25454;&#28304;&#65292;&#20026;&#26657;&#20934;&#12289;&#39044;&#27979;&#21644;&#35780;&#20272;&#25919;&#31574;&#24178;&#39044;&#25552;&#20379;&#20102;&#20415;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#26800;&#27169;&#25311;&#22120;&#26159;&#27969;&#34892;&#30149;&#23398;&#25506;&#32034;&#22797;&#26434;&#65292;&#21160;&#24577;&#24863;&#26579;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#24182;&#22312;&#19981;&#30830;&#23450;&#30340;&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#19981;&#21487;&#25110;&#32570;&#24037;&#20855;&#12290; &#22522;&#20110;&#20195;&#29702;&#30340;&#27169;&#22411;&#65288;ABM&#65289;&#26159;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#20223;&#30495;&#33539;&#20363;&#65292;&#21487;&#20197;&#29992;&#32454;&#33410;&#34920;&#31034;&#25509;&#35302;&#20132;&#20114;&#30340;&#24322;&#36136;&#24615;&#21644;&#20010;&#20307;&#34892;&#20026;&#30340;&#20195;&#29702;&#12290; &#28982;&#32780;&#65292;&#20256;&#32479;ABM&#26694;&#26550;&#19981;&#21487;&#24494;&#20998;&#24182;&#19988;&#22312;&#21487;&#25193;&#23637;&#24615;&#19978;&#23384;&#22312;&#25361;&#25112;&#65307;&#22240;&#27492;&#65292;&#23558;&#23427;&#20204;&#36830;&#25509;&#21040;&#36741;&#21161;&#25968;&#25454;&#28304;&#26159;&#19981;&#24179;&#20961;&#30340;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GradABM&#65306;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#65292;&#21487;&#24494;&#20998;&#30340;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#35774;&#35745;&#65292;&#36866;&#21512;&#20110;&#20351;&#29992;&#33258;&#21160;&#24494;&#20998;&#36827;&#34892;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#12290; GradABM&#21487;&#20197;&#22312;&#21830;&#21697;&#30828;&#20214;&#19978;&#24555;&#36895;&#27169;&#25311;&#25968;&#30334;&#19975;&#35268;&#27169;&#30340;&#20154;&#21475;&#65292;&#24182;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#21644;&#25509;&#21463;&#24322;&#26500;&#25968;&#25454;&#28304;&#12290; &#36825;&#20026;&#26657;&#20934;&#65292;&#39044;&#27979;&#21644;&#35780;&#20272;&#25919;&#31574;&#24178;&#39044;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#23454;&#38469;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mechanistic simulators are an indispensable tool for epidemiology to explore the behavior of complex, dynamic infections under varying conditions and navigate uncertain environments. Agent-based models (ABMs) are an increasingly popular simulation paradigm that can represent the heterogeneity of contact interactions with granular detail and agency of individual behavior. However, conventional ABM frameworks are not differentiable and present challenges in scalability; due to which it is non-trivial to connect them to auxiliary data sources. In this paper, we introduce GradABM: a scalable, differentiable design for agent-based modeling that is amenable to gradient-based learning with automatic differentiation. GradABM can quickly simulate million-size populations in few seconds on commodity hardware, integrate with deep neural networks and ingest heterogeneous data sources. This provides an array of practical benefits for calibration, forecasting, and evaluating policy interventions. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#35760;&#24518;&#39537;&#21160;&#21464;&#21387;&#22120;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#26469;&#39537;&#21160;&#39044;&#27979;&#36807;&#31243;&#65292;&#36880;&#27493;&#24341;&#20837;&#22122;&#22768;&#20197;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#31168;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.07827</link><description>&lt;p&gt;
&#22810;&#20803;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36890;&#29992;&#35760;&#24518;&#39537;&#21160;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Generalizable Memory-driven Transformer for Multivariate Long Sequence Time-series Forecasting. (arXiv:2207.07827v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#35760;&#24518;&#39537;&#21160;&#21464;&#21387;&#22120;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#26469;&#39537;&#21160;&#39044;&#27979;&#36807;&#31243;&#65292;&#36880;&#27493;&#24341;&#20837;&#22122;&#22768;&#20197;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#31168;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;(M-LSTF)&#26159;&#19968;&#20010;&#23454;&#38469;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#19982;&#20256;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#19981;&#21516;&#65292;M-LSTF&#20219;&#21153;&#20174;&#20004;&#20010;&#26041;&#38754;&#26356;&#20855;&#25361;&#25112;&#24615;&#65306;1) M-LSTF&#27169;&#22411;&#38656;&#35201;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#26102;&#38388;&#29305;&#24449;&#20043;&#38388;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#65307;2)&#22312;&#28378;&#21160;&#39044;&#27979;&#35774;&#32622;&#20013;&#65292;&#20004;&#20010;&#36830;&#32493;&#35757;&#32451;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#38543;&#30528;&#39044;&#27979;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#36825;&#20351;&#24471;&#27169;&#22411;&#26356;&#26131;&#20110;&#36807;&#25311;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#35760;&#24518;&#39537;&#21160;&#21464;&#21387;&#22120;&#26469;&#35299;&#20915;M-LSTF&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#23616;&#23618;&#38754;&#30340;&#35760;&#24518;&#32452;&#20214;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#26469;&#39537;&#21160;&#39044;&#27979;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#28176;&#36827;&#24335;&#30340;&#26041;&#24335;&#26469;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#20197;&#22686;&#24378;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#36880;&#27493;&#22312;&#35757;&#32451;&#26679;&#26412;&#20013;&#24341;&#20837;&#20271;&#21162;&#21033;&#22122;&#22768;&#12290;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#20116;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#24322;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate long sequence time-series forecasting (M-LSTF) is a practical but challenging problem. Unlike traditional timer-series forecasting tasks, M-LSTF tasks are more challenging from two aspects: 1) M-LSTF models need to learn time-series patterns both within and between multiple time features; 2) Under the rolling forecasting setting, the similarity between two consecutive training samples increases with the increasing prediction length, which makes models more prone to overfitting. In this paper, we propose a generalizable memory-driven Transformer to target M-LSTF problems. Specifically, we first propose a global-level memory component to drive the forecasting procedure by integrating multiple time-series features. In addition, we adopt a progressive fashion to train our model to increase its generalizability, in which we gradually introduce Bernoulli noises to training samples. Extensive experiments have been performed on five different datasets across multiple fields. Exper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DeepTime&#26694;&#26550;&#65292;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#28145;&#24230;&#26102;&#38388;&#32034;&#24341;&#27169;&#22411;&#30340;&#20803;&#20248;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#19982;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.06046</link><description>&lt;p&gt;
&#23398;&#20064;&#28145;&#24230;&#26102;&#38388;&#32034;&#24341;&#27169;&#22411;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Deep Time-index Models for Time Series Forecasting. (arXiv:2207.06046v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DeepTime&#26694;&#26550;&#65292;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#28145;&#24230;&#26102;&#38388;&#32034;&#24341;&#27169;&#22411;&#30340;&#20803;&#20248;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#19982;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#65292;&#23548;&#33268;&#21382;&#21490;&#20215;&#20540;&#27169;&#22411;&#31867;&#21035;&#20013;&#28044;&#29616;&#20986;&#22823;&#37327;&#26032;&#26041;&#27861;&#12290;&#34429;&#28982;&#26102;&#38388;&#32034;&#24341;&#27169;&#22411;&#20855;&#26377;&#21560;&#24341;&#20154;&#30340;&#29305;&#24615;&#65292;&#27604;&#22914;&#33021;&#22815;&#23545;&#24213;&#23618;&#26102;&#38388;&#24207;&#21015;&#21160;&#24577;&#24615;&#24314;&#27169;&#65292;&#20294;&#20173;&#26410;&#21463;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DeepTime&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#20803;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#30340;&#28145;&#24230;&#26102;&#38388;&#32034;&#24341;&#27169;&#22411;&#65292;&#22312;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#35774;&#32622;&#19979;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23637;&#24320;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has been actively applied to time series forecasting, leading to a deluge of new methods, belonging to the class of historical-value models. Yet, despite the attractive properties of time-index models, such as being able to model the continuous nature of underlying time series dynamics, little attention has been given to them. Indeed, while naive deep time-index models are far more expressive than the manually predefined function representations of classical time-index models, they are inadequate for forecasting, being unable to generalize to unseen time steps due to the lack of inductive bias. In this paper, we propose DeepTime, a meta-optimization framework to learn deep time-index models which overcome these limitations, yielding an efficient and accurate forecasting model. Extensive experiments on real world datasets in the long sequence time-series forecasting setting demonstrate that our approach achieves competitive results with state-of-the-art methods, and is hig
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#23398;&#20064;&#26159;&#19968;&#31181;&#25226;&#22823;&#35268;&#27169;&#23436;&#25972;/&#19981;&#23436;&#25972;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#22266;&#26377;&#20449;&#24687;&#21516;&#27493;&#24314;&#27169;&#24182;&#20351;&#29992;&#19968;&#20010;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#25152;&#26377;&#32852;&#21512;/&#26465;&#20214;/&#36793;&#38469;&#25968;&#25454;&#33021;&#21147;&#24182;&#32479;&#19968;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#30340;&#36890;&#29992;&#23398;&#20064;&#33539;&#24335;&#12290;</title><link>http://arxiv.org/abs/2207.03899</link><description>&lt;p&gt;
&#22823;&#22411;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Big Learning. (arXiv:2207.03899v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03899
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#23398;&#20064;&#26159;&#19968;&#31181;&#25226;&#22823;&#35268;&#27169;&#23436;&#25972;/&#19981;&#23436;&#25972;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#22266;&#26377;&#20449;&#24687;&#21516;&#27493;&#24314;&#27169;&#24182;&#20351;&#29992;&#19968;&#20010;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#25152;&#26377;&#32852;&#21512;/&#26465;&#20214;/&#36793;&#38469;&#25968;&#25454;&#33021;&#21147;&#24182;&#32479;&#19968;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#30340;&#36890;&#29992;&#23398;&#20064;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;/&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#20026;&#28145;&#24230;&#23398;&#20064;&#24320;&#36767;&#20102;&#19968;&#26465;&#20805;&#28385;&#24076;&#26395;&#30340;&#36947;&#36335;&#65292;&#20854;&#20013;&#36335;&#32447;&#22270;&#31283;&#27493;&#20174;&#22823;&#25968;&#25454;&#21040;&#22823;&#27169;&#22411;&#21040;&#65288;&#26032;&#24341;&#20837;&#30340;&#65289;&#22823;&#22411;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22823;&#22411;&#23398;&#20064;&#36890;&#36807;&#21516;&#27493;&#24314;&#27169;&#21487;&#33021;&#23384;&#22312;&#30340;&#22810;&#20010;/&#20840;&#37096;&#32852;&#21512;/&#26465;&#20214;/&#36793;&#38469;&#25968;&#25454;&#20998;&#24067;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#23436;&#25972;/&#19981;&#23436;&#25972;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#20449;&#24687;&#65292;&#20351;&#29992;&#19968;&#20010;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#25581;&#31034;&#22823;&#22411;&#23398;&#20064;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;&#65288;i&#65289;&#26159;&#22823;&#22810;&#25968;&#29616;&#26377;&#22522;&#30784;&#27169;&#22411;&#30340;&#22522;&#30784;&#65307;&#65288;ii&#65289;&#20855;&#26377;&#23545;&#23436;&#25972;/&#19981;&#23436;&#25972;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#21487;&#20449;&#25968;&#25454;&#20219;&#21153;&#30340;&#38750;&#20961;&#28789;&#27963;&#24615;&#65307;&#65288;iii&#65289;&#33021;&#22815;&#20351;&#29992;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#25552;&#20379;&#25152;&#26377;&#32852;&#21512;/&#26465;&#20214;/&#36793;&#38469;&#25968;&#25454;&#33021;&#21147;&#65307;&#65288;iv&#65289;&#32479;&#19968;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#24182;&#21551;&#29992;&#23427;&#20204;&#30340;&#28789;&#27963;&#21327;&#20316;&#65292;&#20307;&#29616;&#20026;&#36890;&#29992;&#23398;&#20064;&#33539;&#24335;&#12290;&#36827;&#34892;&#20102;&#19981;&#21516;&#30340;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in big/foundation models reveal a promising path for deep learning, where the roadmap steadily moves from big data to big models to (the newly-introduced) big learning. Specifically, the big learning exhaustively exploits the information inherent in its large-scale complete/incomplete training data, by simultaneously modeling many/all joint/conditional/marginal data distributions across potentially diverse domains, with one universal foundation model. We reveal that big learning ($i$) underlies most existing foundation models, ($ii$) is equipped with extraordinary flexibilities for complete/incomplete training data and trustworthy data tasks, ($iii$) is capable of delivering all joint/conditional/marginal data capabilities with one universal model, and ($iv$) unifies conventional machine learning paradigms and enables their flexible cooperations, manifested as a universal learning paradigm. Diverse experiments are carried out to validate the effectiveness of the present
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25345;&#32493;&#21516;&#35843;&#30340;&#20108;&#32500;&#25551;&#36848;&#31526;PD&#65292;&#29992;&#20110;&#26500;&#24314;&#34920;&#31034;&#21407;&#23376;&#37197;&#32622;&#30340;&#19981;&#21464;&#25551;&#36848;&#31526;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#21183;&#30340;&#26500;&#24314;&#20013;&#65292;PD&#33021;&#22815;&#25429;&#33719;&#19981;&#21516;&#38271;&#24230;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.13727</link><description>&lt;p&gt;
&#22522;&#20110;&#25345;&#32493;&#21516;&#35843;&#30340;&#25551;&#36848;&#31526;&#29992;&#20110;&#26080;&#23450;&#24418;&#26448;&#26009;&#26426;&#22120;&#23398;&#20064;&#21183;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Persistent homology-based descriptor for machine-learning potential of amorphous structures. (arXiv:2206.13727v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25345;&#32493;&#21516;&#35843;&#30340;&#20108;&#32500;&#25551;&#36848;&#31526;PD&#65292;&#29992;&#20110;&#26500;&#24314;&#34920;&#31034;&#21407;&#23376;&#37197;&#32622;&#30340;&#19981;&#21464;&#25551;&#36848;&#31526;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#21183;&#30340;&#26500;&#24314;&#20013;&#65292;PD&#33021;&#22815;&#25429;&#33719;&#19981;&#21516;&#38271;&#24230;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20957;&#32858;&#24577;&#29289;&#29702;&#23398;&#20013;&#65292;&#39640;&#31934;&#24230;&#39044;&#27979;&#26080;&#23450;&#24418;&#26448;&#26009;&#30340;&#29289;&#29702;&#24615;&#36136;&#26159;&#19968;&#39033;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26426;&#22120;&#23398;&#20064;&#21183;&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#29992;&#20110;&#35745;&#31639;&#26426;&#36153;&#26102;&#30340;&#20174;&#22836;&#24320;&#22987;&#35745;&#31639;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21183;&#26102;&#65292;&#26500;&#24314;&#34920;&#31034;&#21407;&#23376;&#37197;&#32622;&#30340;&#25551;&#36848;&#31526;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20123;&#25551;&#36848;&#31526;&#24212;&#35813;&#23545;&#23545;&#31216;&#25805;&#20316;&#20855;&#26377;&#19981;&#21464;&#24615;&#12290;&#20351;&#29992;&#21407;&#23376;&#20301;&#32622;&#30340;&#24179;&#28369;&#37325;&#21472;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#25163;&#24037;&#21046;&#20316;&#34920;&#31034;&#27861;&#26159;&#26500;&#24314;&#23545;&#31216;&#19981;&#21464;&#25551;&#36848;&#31526;&#30340;&#26041;&#27861;&#30340;&#31034;&#20363;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25345;&#32493;&#21516;&#35843;&#65288;PH&#65289;&#30340;&#20108;&#32500;&#25551;&#36848;&#31526;&#65292;&#22522;&#20110;&#25345;&#32493;&#21516;&#35843;&#20135;&#29983;&#30340;&#20108;&#32500;&#34920;&#24449;&#65292;&#31216;&#20026;&#25345;&#32493;&#21516;&#35843;&#22270;&#65288;PD&#65289;&#65292;&#26469;&#26500;&#24314;&#25551;&#36848;&#31526;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20174;PD&#33719;&#24471;&#30340;&#24402;&#19968;&#21270;&#30340;&#20108;&#32500;&#30452;&#26041;&#22270;&#21487;&#20197;&#39044;&#27979;&#19981;&#21516;&#23494;&#24230;&#19979;&#26080;&#23450;&#24418;&#30899;&#65288;aC&#65289;&#30340;&#27599;&#20010;&#21407;&#23376;&#30340;&#24179;&#22343;&#33021;&#37327;&#65292;&#21363;&#20351;&#20351;&#29992;&#31616;&#21333;&#27169;&#22411;&#20063;&#21487;&#20197;&#23454;&#29616;&#12290;&#20854;&#27425;&#65292;PD&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#25551;&#36848;&#31526;&#25429;&#33719;&#20102;aC&#20013;&#19981;&#21516;&#30340;&#30456;&#20851;&#38271;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;PD&#30340;&#25551;&#36848;&#31526;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#21183;&#65292;&#24182;&#35777;&#26126;&#20854;&#20248;&#20110;&#22522;&#20110;GNN&#21644;&#24179;&#28369;&#37325;&#21472;&#30340;&#25551;&#36848;&#31526;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25345;&#32493;&#21516;&#35843;&#22522;&#30784;&#25551;&#36848;&#31526;&#26159;&#26500;&#24314;&#36866;&#29992;&#20110;&#26080;&#23450;&#24418;&#26448;&#26009;&#30340;&#26426;&#22120;&#23398;&#20064;&#21183;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-accuracy prediction of the physical properties of amorphous materials is challenging in condensed-matter physics. A promising method to achieve this is machine-learning potentials, which is an alternative to computationally demanding ab initio calculations. When applying machine-learning potentials, the construction of descriptors to represent atomic configurations is crucial. These descriptors should be invariant to symmetry operations. Handcrafted representations using a smooth overlap of atomic positions and graph neural networks (GNN) are examples of methods used for constructing symmetry-invariant descriptors. In this study, we propose a novel descriptor based on a persistence diagram (PD), a two-dimensional representation of persistent homology (PH). First, we demonstrated that the normalized two-dimensional histogram obtained from PD could predict the average energy per atom of amorphous carbon (aC) at various densities, even when using a simple model. Second, an analysis o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#21487;&#20197;&#20027;&#21160;&#23547;&#27714;&#24110;&#21161;&#20197;&#35299;&#20915;&#34920;&#36798;&#38656;&#27714;&#24335;&#35270;&#35273;&#23548;&#33322;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#38590;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#32570;&#20047;&#21453;&#39304;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20381;&#28982;&#31283;&#20581;&#30340;&#25239;&#24178;&#25200;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2206.10606</link><description>&lt;p&gt;
&#22909;&#26102;&#26426;: &#34920;&#36798;&#38656;&#27714;&#24335;&#35270;&#35273;&#23548;&#33322;&#20013;&#21551;&#31034;&#27714;&#21161;&#30340;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Good Time to Ask: A Learning Framework for Asking for Help in Embodied Visual Navigation. (arXiv:2206.10606v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10606
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#21487;&#20197;&#20027;&#21160;&#23547;&#27714;&#24110;&#21161;&#20197;&#35299;&#20915;&#34920;&#36798;&#38656;&#27714;&#24335;&#35270;&#35273;&#23548;&#33322;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#38590;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#32570;&#20047;&#21453;&#39304;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20381;&#28982;&#31283;&#20581;&#30340;&#25239;&#24178;&#25200;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#19978;&#65292;&#35810;&#38382;&#27714;&#21161;&#27604;&#22312;&#26410;&#30693;&#20301;&#32622;&#30340;&#31354;&#38388;&#20869;&#25628;&#32034;&#35201;&#26356;&#21152;&#39640;&#25928;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#22312;&#36825;&#31181;&#24863;&#30693;&#35270;&#35273;&#20219;&#21153;&#20013;&#20027;&#21160;&#22320;&#23547;&#27714;&#24110;&#21161;&#65292;&#20854;&#20013;&#21453;&#39304;&#20449;&#24687;&#21578;&#30693;&#20195;&#29702;&#30446;&#26631;&#22312;&#20854;&#35270;&#37326;&#20013;&#30340;&#20301;&#32622;&#12290;&#20026;&#20102;&#27169;&#25311;&#29616;&#23454;&#24773;&#20917;&#19979;&#32769;&#24072;&#19981;&#24635;&#26159;&#22312;&#22330;&#30340;&#22330;&#26223;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35757;&#32451;&#35838;&#31243;&#65292;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#21453;&#39304;&#20449;&#24687;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#30340;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#30446;&#26631;&#20301;&#32622;&#65292;&#22312;&#32463;&#39564;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20351;&#22312;&#32570;&#20047;&#21453;&#39304;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20195;&#29702;&#20381;&#28982;&#20445;&#25345;&#20102;&#25239;&#24178;&#25200;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reality, it is often more efficient to ask for help than to search the entire space to find an object with an unknown location. We present a learning framework that enables an agent to actively ask for help in such embodied visual navigation tasks, where the feedback informs the agent of where the goal is in its view. To emulate the real-world scenario that a teacher may not always be present, we propose a training curriculum where feedback is not always available. We formulate an uncertainty measure of where the goal is and use empirical results to show that through this approach, the agent learns to ask for help effectively while remaining robust when feedback is not available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#65292;&#21363;&#20010;&#24615;&#21270;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#65292;&#19987;&#27880;&#20110;&#32852;&#21512;&#25913;&#36827;&#30456;&#20851;&#30340;&#26412;&#22320;GNN&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;FED-PUB&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.10206</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Personalized Subgraph Federated Learning. (arXiv:2206.10206v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#65292;&#21363;&#20010;&#24615;&#21270;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#65292;&#19987;&#27880;&#20110;&#32852;&#21512;&#25913;&#36827;&#30456;&#20851;&#30340;&#26412;&#22320;GNN&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;FED-PUB&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#23616;&#22270;&#30340;&#23376;&#22270;&#21487;&#33021;&#20998;&#24067;&#22312;&#22810;&#20010;&#35774;&#22791;&#19978;&#65292;&#30001;&#20110;&#38544;&#31169;&#38480;&#21046;&#32780;&#21482;&#33021;&#26412;&#22320;&#35775;&#38382;&#65292;&#23613;&#31649;&#23376;&#22270;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#38142;&#25509;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22788;&#29702;&#36328;&#26412;&#22320;&#23376;&#22270;&#30340;&#32570;&#22833;&#38142;&#25509;&#65292;&#21516;&#26102;&#22312;&#20854;&#19978;&#20998;&#24067;&#24335;&#22320;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24573;&#35270;&#20102;&#20840;&#23616;&#22270;&#19981;&#21516;&#31038;&#21306;&#32452;&#25104;&#30340;&#23376;&#22270;&#20043;&#38388;&#30340;&#19981;&#21487;&#36991;&#20813;&#30340;&#24322;&#36136;&#24615;&#65292;&#22240;&#27492;&#23548;&#33268;&#26412;&#22320;GNN&#27169;&#22411;&#30340;&#19981;&#20860;&#23481;&#30693;&#35782;&#30340;&#23849;&#28291;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#65292;&#21363;&#20010;&#24615;&#21270;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#65292;&#23427;&#19987;&#27880;&#20110;&#32852;&#21512;&#25913;&#36827;&#30456;&#20851;&#30340;&#26412;&#22320;GNN&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#21333;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;FEDerated Personalized sUBgraph learning (FED-PUB)&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#30001;&#20110;&#26381;&#21153;&#22120;&#26080;&#27861;&#35775;&#38382;&#27599;&#20010;&#23458;&#25143;&#31471;&#20013;&#30340;&#23376;&#22270;&#65292;FED-PUB&#21033;&#29992;&#38543;&#26426;&#22270;&#20316;&#20026;&#36755;&#20837;&#65292;&#21033;&#29992;&#26412;&#22320;GNN&#30340;&#21151;&#33021;&#23884;&#20837;&#26469;&#21033;&#29992;&#23376;&#22270;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12290;&#25105;&#20204;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;FED-PUB&#22312;&#31934;&#24230;&#12289;&#36890;&#20449;&#25928;&#29575;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#37117;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subgraphs of a larger global graph may be distributed across multiple devices, and only locally accessible due to privacy restrictions, although there may be links between subgraphs. Recently proposed subgraph Federated Learning (FL) methods deal with those missing links across local subgraphs while distributively training Graph Neural Networks (GNNs) on them. However, they have overlooked the inevitable heterogeneity between subgraphs comprising different communities of a global graph, consequently collapsing the incompatible knowledge from local GNN models. To this end, we introduce a new subgraph FL problem, personalized subgraph FL, which focuses on the joint improvement of the interrelated local GNNs rather than learning a single global model, and propose a novel framework, FEDerated Personalized sUBgraph learning (FED-PUB), to tackle it. Since the server cannot access the subgraph in each client, FED-PUB utilizes functional embeddings of the local GNNs using random graphs as inpu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#20844;&#24179;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;(RFIB)&#65292;&#20860;&#39038;&#20102;&#34920;&#31034;&#25928;&#29992;&#12289;&#20844;&#24179;&#24615;&#21644;&#32039;&#20945;&#24615;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22270;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#20998;&#31867;&#20013;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#21040;&#20102;&#20154;&#21475;&#24179;&#31561;&#21644;&#31561;&#21270;&#36180;&#29575;&#31561;&#19981;&#21516;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#36890;&#36807;&#28041;&#21450;&#32463;&#20856;&#20449;&#24687;&#29942;&#39048;(IB)&#24230;&#37327;&#30340;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#23454;&#29616;&#20844;&#24179;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.10043</link><description>&lt;p&gt;
&#21487;&#35843;&#20449;&#24687;&#29942;&#39048;&#21644;R&#233;nyi&#24230;&#37327;&#36890;&#36807;&#20998;&#31867;&#25928;&#29992;&#12289;&#20844;&#24179;&#24615;&#21644;&#32039;&#20945;&#24615;
&lt;/p&gt;
&lt;p&gt;
Classification Utility, Fairness, and Compactness via Tunable Information Bottleneck and R\'enyi Measures. (arXiv:2206.10043v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#20844;&#24179;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;(RFIB)&#65292;&#20860;&#39038;&#20102;&#34920;&#31034;&#25928;&#29992;&#12289;&#20844;&#24179;&#24615;&#21644;&#32039;&#20945;&#24615;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22270;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#20998;&#31867;&#20013;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#21040;&#20102;&#20154;&#21475;&#24179;&#31561;&#21644;&#31561;&#21270;&#36180;&#29575;&#31561;&#19981;&#21516;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#36890;&#36807;&#28041;&#21450;&#32463;&#20856;&#20449;&#24687;&#29942;&#39048;(IB)&#24230;&#37327;&#30340;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#23454;&#29616;&#20844;&#24179;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#35774;&#35745;&#21487;&#20197;&#20934;&#30830;&#33719;&#21462;&#20449;&#24687;&#32780;&#19981;&#27495;&#35270;&#20219;&#20309;&#25935;&#24863;&#23646;&#24615;&#30340;&#31639;&#27861;&#23545;&#20110;&#31038;&#20250;&#25509;&#21463;AI&#29992;&#20110;&#20851;&#38190;&#24212;&#29992;&#22330;&#26223;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#20844;&#24179;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;R&#233;nyi&#20844;&#24179;&#20449;&#24687;&#29942;&#39048;&#26041;&#27861;(RFIB)&#65292;&#23427;&#20860;&#39038;&#20102;&#34920;&#31034;&#30340;&#25928;&#29992;&#12289;&#20844;&#24179;&#24615;&#21644;&#32039;&#20945;&#24615;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22270;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#20998;&#31867;&#20013;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32771;&#34385;&#21040;&#20102;&#20154;&#21475;&#24179;&#31561;&#21644;&#31561;&#21270;&#36180;&#29575;&#31561;&#19981;&#21516;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#20351;&#24471;&#28385;&#36275;&#36825;&#20004;&#20010;&#20934;&#21017;&#26356;&#21152;&#31934;&#32454;&#12290;&#21033;&#29992;&#21464;&#20998;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30446;&#26631;&#21487;&#20197;&#20135;&#29983;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#65292;&#20854;&#20013;&#28041;&#21450;&#20102;&#32463;&#20856;&#20449;&#24687;&#29942;&#39048;(IB)&#24230;&#37327;&#65292;&#24182;&#22312;&#20004;&#20010;R&#233;nyi$(\alpha)$&#24230;&#37327;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#19978;&#30028;&#65292;&#29992;&#20110;&#34913;&#37327;&#36755;&#20837;&#21644;&#34920;&#31034;&#20043;&#38388;&#30340;&#32039;&#20945;&#24230;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#29616;&#20844;&#24179;&#24615;&#30340;&#21516;&#26102;&#65292;&#20063;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing machine learning algorithms that are accurate yet fair, not discriminating based on any sensitive attribute, is of paramount importance for society to accept AI for critical applications. In this article, we propose a novel fair representation learning method termed the R\'enyi Fair Information Bottleneck Method (RFIB) which incorporates constraints for utility, fairness, and compactness (compression) of representation, and apply it to image and tabular data classification. A key attribute of our approach is that we consider - in contrast to most prior work - both demographic parity and equalized odds as fairness constraints, allowing for a more nuanced satisfaction of both criteria. Leveraging a variational approach, we show that our objectives yield a loss function involving classical Information Bottleneck (IB) measures and establish an upper bound in terms of two R\'enyi measures of order $\alpha$ on the mutual information IB term measuring compactness between the input a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#25668;&#20687;&#22836;&#38236;&#22836;&#65292;&#23545;&#35270;&#39057;&#36827;&#34892;&#22788;&#29702;&#20197;&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#65292;&#24182;&#22312;&#32500;&#25252;&#27963;&#21160;&#35782;&#21035;&#29305;&#24449;&#30340;&#21516;&#26102;&#36827;&#34892;&#20102;&#27169;&#25311;&#21644;&#30828;&#20214;&#23454;&#39564;&#30340;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2206.03891</link><description>&lt;p&gt;
PrivHAR&#65306;&#20174;&#38544;&#31169;&#20445;&#25252;&#35282;&#24230;&#35782;&#21035;&#20154;&#31867;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
PrivHAR: Recognizing Human Actions From Privacy-preserving Lens. (arXiv:2206.03891v2 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03891
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#25668;&#20687;&#22836;&#38236;&#22836;&#65292;&#23545;&#35270;&#39057;&#36827;&#34892;&#22788;&#29702;&#20197;&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#65292;&#24182;&#22312;&#32500;&#25252;&#27963;&#21160;&#35782;&#21035;&#29305;&#24449;&#30340;&#21516;&#26102;&#36827;&#34892;&#20102;&#27169;&#25311;&#21644;&#30828;&#20214;&#23454;&#39564;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#25668;&#20687;&#26426;&#30340;&#24191;&#27867;&#20351;&#29992;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#38544;&#31169;&#21644;&#23433;&#20840;&#30340;&#26085;&#30410;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#22312;&#35782;&#21035;&#20154;&#31867;&#27963;&#21160;&#30340;&#24212;&#29992;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#25668;&#20687;&#22836;&#38236;&#22836;&#65292;&#25104;&#21151;&#22320;&#38477;&#20302;&#20102;&#35270;&#39057;&#36136;&#37327;&#65292;&#20197;&#25233;&#21046;&#38544;&#31169;&#23646;&#24615;&#24182;&#38450;&#27490;&#25932;&#23545;&#25915;&#20987;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#27963;&#21160;&#35782;&#21035;&#30456;&#20851;&#29305;&#24449;&#65292;&#25552;&#20379;&#24378;&#22823;&#30340;&#35270;&#35273;&#38544;&#31169;&#20445;&#25252;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#27169;&#25311;&#21644;&#30828;&#20214;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accelerated use of digital cameras prompts an increasing concern about privacy and security, particularly in applications such as action recognition. In this paper, we propose an optimizing framework to provide robust visual privacy protection along the human action recognition pipeline. Our framework parameterizes the camera lens to successfully degrade the quality of the videos to inhibit privacy attributes and protect against adversarial attacks while maintaining relevant features for activity recognition. We validate our approach with extensive simulations and hardware experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#32452;&#21512;&#22797;&#21512;&#20307;&#36825;&#19968;&#26032;&#22411;&#25299;&#25169;&#22495;&#12290;&#32452;&#21512;&#22797;&#21512;&#20307;&#32467;&#21512;&#20102;&#36229;&#22270;&#21644;&#32990;&#33108;&#22797;&#21512;&#20307;&#30340;&#20248;&#28857;&#65292;&#20801;&#35768;&#26500;&#24314;&#20998;&#23618;&#39640;&#38454;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2206.00606</link><description>&lt;p&gt;
&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#65306;&#36229;&#36234;&#22270;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Topological Deep Learning: Going Beyond Graph Data. (arXiv:2206.00606v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#32452;&#21512;&#22797;&#21512;&#20307;&#36825;&#19968;&#26032;&#22411;&#25299;&#25169;&#22495;&#12290;&#32452;&#21512;&#22797;&#21512;&#20307;&#32467;&#21512;&#20102;&#36229;&#22270;&#21644;&#32990;&#33108;&#22797;&#21512;&#20307;&#30340;&#20248;&#28857;&#65292;&#20801;&#35768;&#26500;&#24314;&#20998;&#23618;&#39640;&#38454;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#19982;&#24320;&#21457;&#25903;&#25345;&#20110;&#25299;&#25169;&#22495;&#19978;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26377;&#20851;&#65292;&#20363;&#22914;&#21333;&#32431;&#22797;&#21512;&#20307;&#12289;&#32990;&#33108;&#22797;&#21512;&#20307;&#21644;&#36229;&#22270;&#12290;&#36825;&#20123;&#25299;&#25169;&#22495;&#22312;&#31185;&#23398;&#35745;&#31639;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#31435;&#22312;&#26356;&#20016;&#23500;&#25968;&#25454;&#32467;&#26500;&#20043;&#19978;&#30340;&#32479;&#19968;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21253;&#25324;&#25299;&#25169;&#22495;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#32452;&#21512;&#22797;&#21512;&#20307;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#25299;&#25169;&#22495;&#12290;&#32452;&#21512;&#22797;&#21512;&#20307;&#21487;&#20197;&#30475;&#20316;&#26159;&#20445;&#25345;&#26576;&#20123;&#29702;&#24819;&#24615;&#36136;&#30340;&#22270;&#30340;&#25512;&#24191;&#12290;&#31867;&#20284;&#20110;&#36229;&#22270;&#65292;&#32452;&#21512;&#22797;&#21512;&#20307;&#23545;&#20851;&#31995;&#38598;&#21512;&#19981;&#26045;&#21152;&#20219;&#20309;&#32422;&#26463;&#12290;&#27492;&#22806;&#65292;&#32452;&#21512;&#22797;&#21512;&#20307;&#20801;&#35768;&#26500;&#24314;&#20998;&#23618;&#39640;&#38454;&#20851;&#31995;&#65292;&#31867;&#20284;&#20110;&#21333;&#32431;&#21644;&#32990;&#33108;&#22797;&#21512;&#20307;&#20013;&#30340;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#32452;&#21512;&#22797;&#21512;&#20307;&#25512;&#24191;&#24182;&#32467;&#21512;&#20102;&#36229;&#22270;&#21644;&#32990;&#33108;&#22797;&#21512;&#20307;&#30340;&#26377;&#29992;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topological deep learning is a rapidly growing field that pertains to the development of deep learning models for data supported on topological domains such as simplicial complexes, cell complexes, and hypergraphs, which generalize many domains encountered in scientific computations. In this paper, we present a unifying deep learning framework built upon a richer data structure that includes widely adopted topological domains.  Specifically, we first introduce combinatorial complexes, a novel type of topological domain. Combinatorial complexes can be seen as generalizations of graphs that maintain certain desirable properties. Similar to hypergraphs, combinatorial complexes impose no constraints on the set of relations. In addition, combinatorial complexes permit the construction of hierarchical higher-order relations, analogous to those found in simplicial and cell complexes. Thus, combinatorial complexes generalize and combine useful traits of both hypergraphs and cell complexes, whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#30340;&#26377;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#26631;&#31614;&#20998;&#24067;&#26469;&#22686;&#24378;&#26377;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#36866;&#24212;&#21508;&#31181;&#29616;&#26377;&#25216;&#26415;&#65292;&#21253;&#25324;CutMix&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#22312;CIFAR-10&#65292;CIFAR-100&#21644;ImageNet&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.00384</link><description>&lt;p&gt;
&#24191;&#20041;&#30340;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generalized Supervised Contrastive Learning. (arXiv:2206.00384v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#30340;&#26377;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#26631;&#31614;&#20998;&#24067;&#26469;&#22686;&#24378;&#26377;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#36866;&#24212;&#21508;&#31181;&#29616;&#26377;&#25216;&#26415;&#65292;&#21253;&#25324;CutMix&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#22312;CIFAR-10&#65292;CIFAR-100&#21644;ImageNet&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#23545;&#27604;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#24050;&#25104;&#21151;&#23558;&#36825;&#20123;&#23545;&#27604;&#26041;&#27861;&#25193;&#23637;&#21040;&#26377;&#30417;&#30563;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20132;&#21449;&#29109;&#12290;&#28982;&#32780;&#65292;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#22266;&#26377;&#22320;&#20351;&#29992;&#20197;&#19968;&#20301;&#28909;&#30446;&#26631;&#21521;&#37327;&#30340;&#24418;&#24335;&#34920;&#31034;&#30340;&#26631;&#31614;&#20449;&#24687;&#65292;&#36825;&#31181;&#32467;&#26500;&#26080;&#27861;&#36866;&#24212;&#21033;&#29992;&#26631;&#31614;&#20449;&#24687;&#20316;&#20026;&#27010;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#22914;CutMix&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#24191;&#20041;&#30340;&#26377;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#65292;&#35813;&#25439;&#22833;&#24230;&#37327;&#20102;&#26631;&#31614;&#30456;&#20284;&#24615;&#21644;&#28508;&#22312;&#30456;&#20284;&#24615;&#20043;&#38388;&#30340;&#20132;&#21449;&#29109;&#12290;&#36825;&#20010;&#27010;&#24565;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#26631;&#31614;&#20998;&#24067;&#65292;&#24182;&#20351;&#21508;&#31181;&#29616;&#26377;&#25216;&#26415;&#36866;&#24212;&#20110;&#35757;&#32451;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#32780;&#22686;&#24378;&#20102;&#26377;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#30340;&#33021;&#21147;&#12290;&#21033;&#29992;&#36825;&#31181;&#24191;&#20041;&#30340;&#26377;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#65292;&#25105;&#20204;&#20026;&#22270;&#20687;&#20998;&#31867;&#26500;&#24314;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21253;&#25324;CIFAR-10&#65292;CIFAR-100&#21644;ImageNet&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#27604;&#29616;&#26377;&#30340;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;CIFAR-10&#65292;CIFAR-100&#21644;ImageNet&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the recent promising results of contrastive learning in the self-supervised learning paradigm, supervised contrastive learning has successfully extended these contrastive approaches to supervised contexts, outperforming cross-entropy on various datasets. However, supervised contrastive learning inherently employs label information in a binary form--either positive or negative--using a one-hot target vector. This structure struggles to adapt to methods that exploit label information as a probability distribution, such as CutMix and knowledge distillation. In this paper, we introduce a generalized supervised contrastive loss, which measures cross-entropy between label similarity and latent similarity. This concept enhances the capabilities of supervised contrastive loss by fully utilizing the label distribution and enabling the adaptation of various existing techniques for training modern neural networks. Leveraging this generalized supervised contrastive loss, we construct a tailor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25509;&#36817;&#21333;&#36890;&#36947;&#35821;&#38899;&#20998;&#31163;&#30028;&#38480;&#30340;&#26041;&#27861;SepIt&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22312;5&#20010;&#21644;10&#20010;&#35828;&#35805;&#20154;&#30340;&#24773;&#20917;&#19979;&#20173;&#26377;&#25552;&#39640;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2205.11801</link><description>&lt;p&gt;
SepIt: &#25509;&#36817;&#21333;&#36890;&#36947;&#35821;&#38899;&#20998;&#31163;&#30028;&#38480;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SepIt: Approaching a Single Channel Speech Separation Bound. (arXiv:2205.11801v4 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11801
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25509;&#36817;&#21333;&#36890;&#36947;&#35821;&#38899;&#20998;&#31163;&#30028;&#38480;&#30340;&#26041;&#27861;SepIt&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22312;5&#20010;&#21644;10&#20010;&#35828;&#35805;&#20154;&#30340;&#24773;&#20917;&#19979;&#20173;&#26377;&#25552;&#39640;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#36890;&#36947;&#35821;&#38899;&#20998;&#31163;&#20219;&#21153;&#30340;&#19978;&#30028;&#65292;&#35813;&#19978;&#30028;&#22522;&#20110;&#23545;&#35821;&#38899;&#30701;&#26102;&#27573;&#24615;&#36136;&#30340;&#20551;&#35774;&#12290;&#20351;&#29992;&#35813;&#19978;&#30028;&#65292;&#25105;&#20204;&#33021;&#22815;&#23637;&#31034;&#65292;&#34429;&#28982;&#26368;&#36817;&#30340;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#22312;&#23569;&#25968;&#20960;&#20010;&#35828;&#35805;&#20154;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;5&#20010;&#21644;10&#20010;&#35828;&#35805;&#20154;&#30340;&#24773;&#20917;&#19979;&#36824;&#26377;&#25552;&#39640;&#30340;&#31354;&#38388;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;SepIt&#26469;&#36845;&#20195;&#22320;&#25913;&#36827;&#19981;&#21516;&#35828;&#35805;&#20154;&#30340;&#20272;&#35745;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;SepIt&#23545;&#20110;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#30340;&#36845;&#20195;&#27425;&#25968;&#26159;&#21487;&#21464;&#30340;&#65292;&#22522;&#20110;&#25105;&#20204;&#20998;&#26512;&#20013;&#20986;&#29616;&#30340;&#20114;&#20449;&#24687;&#26631;&#20934;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;SepIt&#22312;2&#12289;3&#12289;5&#21644;10&#20010;&#35828;&#35805;&#20154;&#30340;&#24773;&#20917;&#19979;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an upper bound for the Single Channel Speech Separation task, which is based on an assumption regarding the nature of short segments of speech. Using the bound, we are able to show that while the recent methods have made significant progress for a few speakers, there is room for improvement for five and ten speakers. We then introduce a Deep neural network, SepIt, that iteratively improves the different speakers' estimation. At test time, SpeIt has a varying number of iterations per test sample, based on a mutual information criterion that arises from our analysis. In an extensive set of experiments, SepIt outperforms the state-of-the-art neural networks for 2, 3, 5, and 10 speakers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#23567;&#36229;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26368;&#23567;&#30340;NTK&#29305;&#24449;&#20540;&#30340;&#19979;&#30028;&#65292;&#20855;&#26377;&#27425;&#32447;&#24615;&#23618;&#23485;&#30340;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#26159;&#24378;&#22823;&#30340;&#35760;&#24518;&#22120;&#21644;&#20248;&#21270;&#22120;&#65292;&#21482;&#35201;&#21442;&#25968;&#25968;&#37327;&#36229;&#36807;&#26679;&#26412;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2205.10217</link><description>&lt;p&gt;
&#24102;&#26377;&#26368;&#23567;&#36229;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35760;&#24518;&#21270;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Memorization and Optimization in Deep Neural Networks with Minimum Over-parameterization. (arXiv:2205.10217v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#23567;&#36229;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26368;&#23567;&#30340;NTK&#29305;&#24449;&#20540;&#30340;&#19979;&#30028;&#65292;&#20855;&#26377;&#27425;&#32447;&#24615;&#23618;&#23485;&#30340;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#26159;&#24378;&#22823;&#30340;&#35760;&#24518;&#22120;&#21644;&#20248;&#21270;&#22120;&#65292;&#21482;&#35201;&#21442;&#25968;&#25968;&#37327;&#36229;&#36807;&#26679;&#26412;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#24050;&#25104;&#20026;&#25552;&#20379;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35760;&#24518;&#21270;&#12289;&#20248;&#21270;&#21644;&#27867;&#21270;&#20445;&#35777;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#37096;&#20998;&#23398;&#32773;&#24050;&#30740;&#31350;&#20102;&#33267;&#23569;&#19968;&#23618;&#20855;&#26377;$\Omega(N)$&#20010;&#31070;&#32463;&#20803;&#30340;&#20004;&#23618;&#21644;&#28145;&#23618;&#32593;&#32476;&#30340;NTK&#35889;&#65292;&#20854;&#20013;$N$&#26159;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#20855;&#26377;&#27425;&#32447;&#24615;&#23618;&#23485;&#30340;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#26159;&#24378;&#22823;&#30340;&#35760;&#24518;&#22120;&#21644;&#20248;&#21270;&#22120;&#65292;&#21482;&#35201;&#21442;&#25968;&#25968;&#37327;&#36229;&#36807;&#26679;&#26412;&#25968;&#37327;&#21363;&#21487;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#26159;&#22312;&#36825;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27425;&#32447;&#24615;&#35774;&#32622;&#19979;&#65292;NTK&#26159;&#21542;&#23384;&#22312;&#33391;&#22909;&#30340;&#26465;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32943;&#23450;&#22320;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#25216;&#26415;&#36129;&#29486;&#26159;&#25552;&#20379;&#20102;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26368;&#23567;&#30340;NTK&#29305;&#24449;&#20540;&#30340;&#19979;&#30028;&#65292;&#21363;&#21442;&#25968;&#25968;&#37327;&#22823;&#32422;&#20026;$\Omega(N)$&#65292;&#22240;&#27492;&#31070;&#32463;&#20803;&#25968;&#37327;&#33267;&#23569;&#20026;$\Omega(\sqrt{N})$&#12290;&#20026;&#23637;&#31034;&#25105;&#20204;&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#22312;&#22810;&#39033;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Neural Tangent Kernel (NTK) has emerged as a powerful tool to provide memorization, optimization and generalization guarantees in deep neural networks. A line of work has studied the NTK spectrum for two-layer and deep networks with at least a layer with $\Omega(N)$ neurons, $N$ being the number of training samples. Furthermore, there is increasing evidence suggesting that deep networks with sub-linear layer widths are powerful memorizers and optimizers, as long as the number of parameters exceeds the number of samples. Thus, a natural open question is whether the NTK is well conditioned in such a challenging sub-linear setup. In this paper, we answer this question in the affirmative. Our key technical contribution is a lower bound on the smallest NTK eigenvalue for deep networks with the minimum possible over-parameterization: the number of parameters is roughly $\Omega(N)$ and, hence, the number of neurons is as little as $\Omega(\sqrt{N})$. To showcase the applicability of our N
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#24212;&#23545;&#20219;&#24847;&#24310;&#36831;&#30340;&#26080;&#25237;&#24433;&#22312;&#32447;&#31639;&#27861;&#65292;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#27604;&#65292;&#20854;&#20855;&#26377;&#26356;&#22909;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2204.04964</link><description>&lt;p&gt;
&#20219;&#24847;&#24310;&#36831;&#30340;&#26080;&#25237;&#24433;&#22312;&#32447;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Projection-free Online Learning with Arbitrary Delays. (arXiv:2204.04964v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.04964
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#24212;&#23545;&#20219;&#24847;&#24310;&#36831;&#30340;&#26080;&#25237;&#24433;&#22312;&#32447;&#31639;&#27861;&#65292;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#27604;&#65292;&#20854;&#20855;&#26377;&#26356;&#22909;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#30001;&#20110;&#22312;&#22788;&#29702;&#20855;&#26377;&#22797;&#26434;&#32422;&#26463;&#30340;&#39640;&#32500;&#38382;&#39064;&#26102;&#20855;&#26377;&#39640;&#25928;&#24615;&#65292;&#36991;&#20813;&#20102;&#25237;&#24433;&#25805;&#20316;&#24182;&#36890;&#36807;&#36739;&#23569;&#30340;&#35745;&#31639;&#22914;&#32447;&#24615;&#20248;&#21270;&#65288;LO&#65289;&#35745;&#31639;&#30340;&#26080;&#25237;&#24433;&#22312;&#32447;&#23398;&#20064;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20294;&#26159;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#20551;&#23450;&#20219;&#20309;&#26597;&#35810;&#26799;&#24230;&#37117;&#20250;&#31435;&#21363;&#26174;&#31034;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#24182;&#19981;&#25104;&#31435;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#23558;&#22312;&#32447;Frank-Wolfe&#65288;OFW&#65289;&#31639;&#27861;&#21644;&#22312;&#32447;&#24179;&#28369;&#26080;&#25237;&#24433;&#65288;OSPF&#65289;&#31639;&#27861;&#25512;&#24191;&#21040;&#24310;&#36831;&#35774;&#32622;&#20013;&#65292;&#20854;&#20013;&#26597;&#35810;&#30340;&#26799;&#24230;&#21487;&#20197;&#34987;&#20219;&#24847;&#22238;&#21512;&#24310;&#36831;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25512;&#24191;&#30340;OFW&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#22312;&#25910;&#21040;&#20219;&#20309;&#24310;&#36831;&#30340;&#26799;&#24230;&#21518;&#25191;&#34892;&#31867;&#20284;&#20110;&#21407;&#22987;OFW&#30340;&#26356;&#26032;&#65292;&#24182;&#20026;&#27599;&#19968;&#36718;&#25773;&#25918;&#26368;&#26032;&#30340;&#20915;&#31574;&#12290;&#27492;&#22806;&#65292;OSPF&#30340;&#22522;&#26412;&#25913;&#21464;&#26159;&#29992;&#32771;&#34385;&#20219;&#24847;&#24310;&#36831;&#30340;&#21152;&#26435;&#21644;&#26367;&#25442;&#20102;&#26597;&#35810;&#26799;&#24230;&#30340;&#24635;&#21644;&#12290;&#22312;&#21508;&#31181;&#24310;&#36831;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#24191;&#20041;OFW&#21644;OSPF&#30340;&#20122;&#32447;&#24615;&#36951;&#25022;&#20445;&#35777;&#65292;&#24182;&#26174;&#31034;&#23427;&#20204;&#30340;&#36951;&#25022;&#30028;&#19982;&#35774;&#35745;&#29992;&#20110;&#26080;&#24310;&#36831;&#35774;&#32622;&#30340;&#29616;&#26377;&#31639;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Projection-free online learning, which eschews the projection operation via less expensive computations such as linear optimization (LO), has received much interest recently due to its efficiency in handling high-dimensional problems with complex constraints. However, previous studies assume that any queried gradient is revealed immediately, which may not hold in practice and limits their applications. To address this limitation, we generalize the online Frank-Wolfe (OFW) algorithm and the online smooth projection-free (OSPF) algorithm, which are state-of-the-art LO-based projection-free online algorithms for non-smooth and smooth functions respectively, into a delayed setting where queried gradients can be delayed by arbitrary rounds. Specifically, the main idea of our generalized OFW is to perform an update similar to the original OFW after receiving any delayed gradient, and play the latest decision for each round. Moreover, the essential change on OSPF is to replace the sum of quer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#31216;&#20114;&#21160;&#30340;&#38750;&#21333;&#35843;&#24191;&#20041;&#32435;&#20160;&#22343;&#34913;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#20998;&#25955;&#32435;&#20160;&#22343;&#34913;&#23547;&#25214;&#31639;&#27861;&#65292;&#20854;&#20013;&#65292;&#21327;&#35843;&#21592;&#36890;&#36807;&#25972;&#21512;&#20195;&#29702;&#30340;&#21453;&#39304;&#26469;&#23398;&#20064;&#20195;&#29702;&#30340;&#20266;&#26799;&#24230;&#24182;&#20026;&#20854;&#35774;&#35745;&#20010;&#24615;&#21270;&#30340;&#28608;&#21169;&#65292;&#20195;&#29702;&#35745;&#31639;&#25193;&#23637;&#21338;&#24328;&#30340;&#35299;&#24182;&#21453;&#39304;&#27979;&#37327;&#32473;&#21327;&#35843;&#21592;&#65292;&#35813;&#31639;&#27861;&#21487;&#36820;&#22238;&#38745;&#24577;&#35774;&#32622;&#19979;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2203.12948</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#28608;&#21169;&#20316;&#20026;&#24191;&#20041;&#32435;&#20160;&#22343;&#34913;&#38382;&#39064;&#20013;&#30340;&#21453;&#39304;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Personalized incentives as feedback design in generalized Nash equilibrium problems. (arXiv:2203.12948v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.12948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#31216;&#20114;&#21160;&#30340;&#38750;&#21333;&#35843;&#24191;&#20041;&#32435;&#20160;&#22343;&#34913;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#20998;&#25955;&#32435;&#20160;&#22343;&#34913;&#23547;&#25214;&#31639;&#27861;&#65292;&#20854;&#20013;&#65292;&#21327;&#35843;&#21592;&#36890;&#36807;&#25972;&#21512;&#20195;&#29702;&#30340;&#21453;&#39304;&#26469;&#23398;&#20064;&#20195;&#29702;&#30340;&#20266;&#26799;&#24230;&#24182;&#20026;&#20854;&#35774;&#35745;&#20010;&#24615;&#21270;&#30340;&#28608;&#21169;&#65292;&#20195;&#29702;&#35745;&#31639;&#25193;&#23637;&#21338;&#24328;&#30340;&#35299;&#24182;&#21453;&#39304;&#27979;&#37327;&#32473;&#21327;&#35843;&#21592;&#65292;&#35813;&#31639;&#27861;&#21487;&#36820;&#22238;&#38745;&#24577;&#35774;&#32622;&#19979;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#31216;&#20114;&#21160;&#30340;&#38745;&#24577;&#21644;&#21160;&#24577;&#38750;&#21333;&#35843;&#24191;&#20041;&#32435;&#20160;&#22343;&#34913;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#24050;&#30693;&#20855;&#26377;&#28508;&#21183;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#21487;&#33021;&#23384;&#22312;&#36825;&#26679;&#19968;&#31181;&#24773;&#20917;&#65292;&#21363;&#24213;&#23618;&#28508;&#22312;&#20989;&#25968;&#30340;&#27491;&#24335;&#34920;&#36798;&#24335;&#19981;&#21487;&#29992;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21322;&#20998;&#25955;&#32435;&#20160;&#22343;&#34913;&#23547;&#25214;&#31639;&#27861;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#20004;&#23618;&#26041;&#26696;&#20013;&#65292;&#21327;&#35843;&#21592;&#36845;&#20195;&#22320;&#25972;&#21512;&#20195;&#29702;&#30340;&#65288;&#21487;&#33021;&#26159;&#22122;&#22768;&#21644;&#38646;&#25955;&#30340;&#65289;&#21453;&#39304;&#20197;&#23398;&#20064;&#20195;&#29702;&#30340;&#20266;&#26799;&#24230;&#65292;&#28982;&#21518;&#20026;&#20182;&#20204;&#35774;&#35745;&#20010;&#24615;&#21270;&#30340;&#28608;&#21169;&#12290;&#22312;&#20195;&#29702;&#26041;&#38754;&#65292;&#20195;&#29702;&#25509;&#25910;&#21040;&#20010;&#24615;&#21270;&#30340;&#28608;&#21169;&#65292;&#35745;&#31639;&#25193;&#23637;&#21338;&#24328;&#30340;&#35299;&#65292;&#28982;&#21518;&#21521;&#21327;&#35843;&#21592;&#36820;&#22238;&#21453;&#39304;&#27979;&#37327;&#12290;&#22312;&#38745;&#24577;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#21327;&#35843;&#21592;&#25317;&#26377;&#26631;&#20934;&#23398;&#20064;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#36820;&#22238;&#32435;&#20160;&#22343;&#34913;&#65292;&#32780;&#22312;&#21487;&#35843;&#25972;&#30340;&#24120;&#25968;&#35823;&#24046;&#20869;&#36820;&#22238;&#32435;&#20160;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate both stationary and time-varying, nonmonotone generalized Nash equilibrium problems that exhibit symmetric interactions among the agents, which are known to be potential. As may happen in practical cases, however, we envision a scenario in which the formal expression of the underlying potential function is not available, and we design a semi-decentralized Nash equilibrium seeking algorithm. In the proposed two-layer scheme, a coordinator iteratively integrates the (possibly noisy and sporadic) agents' feedback to learn the pseudo-gradients of the agents, and then design personalized incentives for them. On their side, the agents receive those personalized incentives, compute a solution to an extended game, and then return feedback measurements to the coordinator. In the stationary setting, our algorithm returns a Nash equilibrium in case the coordinator is endowed with standard learning policies, while it returns a Nash equilibrium up to a constant, yet adjustable, error
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#30446;&#26631;&#25506;&#27979;&#21644;&#20301;&#32622;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#20248;&#21270;&#22810;&#26234;&#33021;&#20307;&#20027;&#21160;&#25628;&#32034;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2203.04524</link><description>&lt;p&gt;
&#20351;&#29992;&#25506;&#27979;&#21644;&#20301;&#32622;&#19981;&#30830;&#23450;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#20027;&#21160;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Active Search using Detection and Location Uncertainty. (arXiv:2203.04524v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.04524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#30446;&#26631;&#25506;&#27979;&#21644;&#20301;&#32622;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#20248;&#21270;&#22810;&#26234;&#33021;&#20307;&#20027;&#21160;&#25628;&#32034;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29615;&#22659;&#30417;&#27979;&#25110;&#28798;&#23475;&#21709;&#24212;&#20219;&#21153;&#31561;&#24212;&#29992;&#20013;&#65292;&#20027;&#21160;&#25628;&#32034;&#28041;&#21450;&#21040;&#33258;&#20027;&#20195;&#29702;&#22312;&#25628;&#32034;&#31354;&#38388;&#20013;&#20351;&#29992;&#20915;&#31574;&#31639;&#27861;&#26816;&#27979;&#30446;&#26631;&#65292;&#36825;&#20123;&#31639;&#27861;&#20250;&#26681;&#25454;&#21382;&#21490;&#35266;&#23519;&#32467;&#26524;&#36827;&#34892;&#35843;&#25972;&#12290;&#20027;&#21160;&#25628;&#32034;&#31639;&#27861;&#24517;&#39035;&#22788;&#29702;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#26816;&#27979;&#19981;&#30830;&#23450;&#24615;&#21644;&#20301;&#32622;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#65292;&#26356;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#19987;&#27880;&#20110;&#20301;&#32622;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#23558;&#26816;&#27979;&#27010;&#29575;&#38408;&#20540;&#35774;&#23450;&#20026;&#38646;&#25110;&#19968;&#26469;&#28040;&#38500;&#26816;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#30456;&#21453;&#65292;&#22312;&#31232;&#30095;&#20449;&#21495;&#22788;&#29702;&#25991;&#29486;&#20013;&#65292;&#36890;&#24120;&#20551;&#23450;&#30446;&#26631;&#20301;&#32622;&#20934;&#30830;&#65292;&#24182;&#19987;&#27880;&#20110;&#20854;&#26816;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#19968;&#31181;&#25512;&#29702;&#26041;&#27861;&#26469;&#21516;&#26102;&#22788;&#29702;&#30446;&#26631;&#26816;&#27979;&#21644;&#20301;&#32622;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#36825;&#20010;&#25512;&#29702;&#26041;&#27861;&#26500;&#24314;&#19968;&#20010;&#20351;&#29992;&#27748;&#26222;&#26862;&#25277;&#26679;&#30340;&#20915;&#31574;&#31639;&#27861;&#65292;&#20351;&#24471;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#20027;&#21160;&#25628;&#32034;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#27169;&#25311;&#23454;&#39564;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#26816;&#27979;&#29575;&#21644;&#36164;&#28304;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active search, in applications like environment monitoring or disaster response missions, involves autonomous agents detecting targets in a search space using decision making algorithms that adapt to the history of their observations. Active search algorithms must contend with two types of uncertainty: detection uncertainty and location uncertainty. The more common approach in robotics is to focus on location uncertainty and remove detection uncertainty by thresholding the detection probability to zero or one. In contrast, it is common in the sparse signal processing literature to assume the target location is accurate and instead focus on the uncertainty of its detection. In this work, we first propose an inference method to jointly handle both target detection and location uncertainty. We then build a decision making algorithm on this inference method that uses Thompson sampling to enable decentralized multi-agent active search. We perform simulation experiments to show that our algo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#26354;&#32447;&#36710;&#36947;&#26816;&#27979;&#26041;&#27861;&#65292;&#22312;&#35745;&#31639;&#19978;&#26356;&#21152;&#31616;&#21333;&#19988;&#23481;&#26131;&#22788;&#29702;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#21487;&#20197;&#20316;&#20026;&#26410;&#26469;&#36710;&#36947;&#26816;&#27979;&#39046;&#22495;&#30340;&#26032;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2203.02431</link><description>&lt;p&gt;
&#26354;&#32447;&#24314;&#27169;&#19979;&#30340;&#36710;&#36947;&#26816;&#27979;&#26041;&#27861;&#37325;&#26032;&#24605;&#32771;
&lt;/p&gt;
&lt;p&gt;
Rethinking Efficient Lane Detection via Curve Modeling. (arXiv:2203.02431v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.02431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#26354;&#32447;&#36710;&#36947;&#26816;&#27979;&#26041;&#27861;&#65292;&#22312;&#35745;&#31639;&#19978;&#26356;&#21152;&#31616;&#21333;&#19988;&#23481;&#26131;&#22788;&#29702;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#21487;&#20197;&#20316;&#20026;&#26410;&#26469;&#36710;&#36947;&#26816;&#27979;&#39046;&#22495;&#30340;&#26032;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21442;&#25968;&#26354;&#32447;&#30340;RGB&#22270;&#20687;&#36710;&#36947;&#26816;&#27979;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#20998;&#21106;&#21644;&#22522;&#20110;&#28857;&#26816;&#27979;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#26354;&#32447;&#26041;&#27861;&#21487;&#20197;&#33258;&#28982;&#22320;&#23398;&#20064;&#25972;&#20307;&#36710;&#36947;&#34920;&#31034;&#65292;&#32780;&#19981;&#38656;&#35201;&#35299;&#30721;&#39044;&#27979;&#25110;&#21046;&#23450;&#22823;&#37327;&#38170;&#28857;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#20026;&#20102;&#22788;&#29702;&#29616;&#26377;&#22810;&#39033;&#24335;&#26354;&#32447;&#26041;&#27861;&#30340;&#20248;&#21270;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#21442;&#25968;B&#233;zier&#26354;&#32447;&#65292;&#22240;&#20026;&#23427;&#26131;&#20110;&#35745;&#31639;&#12289;&#31283;&#23450;&#19988;&#20855;&#26377;&#36739;&#39640;&#30340;&#33258;&#30001;&#21464;&#25442;&#24230;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#21464;&#24418;&#21367;&#31215;&#30340;&#29305;&#24449;&#32763;&#36716;&#34701;&#21512;&#26041;&#27861;&#65292;&#20197;&#21033;&#29992;&#39550;&#39542;&#22330;&#26223;&#20013;&#36710;&#36947;&#30340;&#23545;&#31216;&#24615;&#36136;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#27969;&#34892;&#30340;LLAMAS&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#12290;&#23427;&#20063;&#22312;TuSimple&#21644;CULane&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#36739;&#20302;&#30340;&#24310;&#36831;&#65288;&gt; 150 FPS&#65289;&#21644;&#23567;&#27169;&#22411;&#22823;&#23567;&#65288;&lt;10M&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#26032;&#30340;&#22522;&#20934;&#65292;&#20197;&#22312;&#36335;&#26631;&#26816;&#27979;&#39046;&#22495;&#25512;&#21160;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel parametric curve-based method for lane detection in RGB images. Unlike state-of-the-art segmentation-based and point detection-based methods that typically require heuristics to either decode predictions or formulate a large sum of anchors, the curve-based methods can learn holistic lane representations naturally. To handle the optimization difficulties of existing polynomial curve methods, we propose to exploit the parametric B\'ezier curve due to its ease of computation, stability, and high freedom degrees of transformations. In addition, we propose the deformable convolution-based feature flip fusion, for exploiting the symmetry properties of lanes in driving scenes. The proposed method achieves a new state-of-the-art performance on the popular LLAMAS benchmark. It also achieves favorable accuracy on the TuSimple and CULane datasets, while retaining both low latency (&gt; 150 FPS) and small model size (&lt; 10M). Our method can serve as a new baseline, to shed 
&lt;/p&gt;</description></item><item><title>&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#26159;&#19968;&#31181;&#33021;&#22815;&#20174;&#37051;&#23621;&#33410;&#28857;&#30340;&#29305;&#24449;&#20013;&#32858;&#21512;&#20449;&#24687;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#19978;&#19979;&#25991;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#22312;&#8220;&#26131;&#8221;&#21306;&#38388;&#20869;&#65292;&#23427;&#33021;&#22815;&#21306;&#20998;&#36328;&#31867;&#21644;&#20869;&#31867;&#36793;&#32536;&#24182;&#32500;&#25252;&#37325;&#35201;&#36793;&#32536;&#30340;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2202.13060</link><description>&lt;p&gt;
&#22270;&#27880;&#24847;&#21147;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
Graph Attention Retrospective. (arXiv:2202.13060v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.13060
&lt;/p&gt;
&lt;p&gt;
&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#26159;&#19968;&#31181;&#33021;&#22815;&#20174;&#37051;&#23621;&#33410;&#28857;&#30340;&#29305;&#24449;&#20013;&#32858;&#21512;&#20449;&#24687;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#19978;&#19979;&#25991;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#22312;&#8220;&#26131;&#8221;&#21306;&#38388;&#20869;&#65292;&#23427;&#33021;&#22815;&#21306;&#20998;&#36328;&#31867;&#21644;&#20869;&#31867;&#36793;&#32536;&#24182;&#32500;&#25252;&#37325;&#35201;&#36793;&#32536;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#24555;&#36895;&#21457;&#23637;&#30340;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#24212;&#29992;&#20110;&#31038;&#20132;&#32593;&#32476;&#12289;&#24341;&#25991;&#32593;&#32476;&#21644;&#29983;&#29289;&#20449;&#24687;&#23398;&#12290;&#20854;&#20013;&#26368;&#27969;&#34892;&#30340;&#27169;&#22411;&#20043;&#19968;&#26159;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#12290;&#23427;&#20204;&#34987;&#24341;&#20837;&#20351;&#33410;&#28857;&#33021;&#22815;&#20197;&#38750;&#32479;&#19968;&#30340;&#26041;&#24335;&#20174;&#37051;&#23621;&#33410;&#28857;&#30340;&#29305;&#24449;&#20013;&#32858;&#21512;&#20449;&#24687;&#65292;&#19982;&#31616;&#21333;&#30340;&#22270;&#21367;&#31215;&#19981;&#21516;&#65292;&#21518;&#32773;&#19981;&#33021;&#21306;&#20998;&#33410;&#28857;&#30340;&#37051;&#23621;&#12290;&#26412;&#25991;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#22270;&#27880;&#24847;&#32593;&#32476;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#23545;&#19978;&#19979;&#25991;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#35777;&#26126;&#20102;&#22270;&#27880;&#24847;&#26426;&#21046;&#30340;&#22810;&#20010;&#24615;&#33021;&#32467;&#26524;&#12290;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#33410;&#28857;&#29305;&#24449;&#26469;&#33258;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#36793;&#32536;&#26469;&#33258;&#20110;&#38543;&#26426;&#22359;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#8220;&#26131;&#8221;&#21306;&#38388;&#20869;&#65292;&#39640;&#26031;&#20998;&#24067;&#22343;&#20540;&#20043;&#38388;&#30340;&#36317;&#31163;&#36275;&#22815;&#22823;&#26102;&#65292;&#22270;&#27880;&#24847;&#21147;&#21487;&#20197;&#21306;&#20998;&#36328;&#31867;&#21644;&#20869;&#31867;&#36793;&#32536;&#12290;&#22240;&#27492;&#65292;&#23427;&#32500;&#25252;&#20102;&#37325;&#35201;&#36793;&#32536;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-based learning is a rapidly growing sub-field of machine learning with applications in social networks, citation networks, and bioinformatics. One of the most popular models is graph attention networks. They were introduced to allow a node to aggregate information from features of neighbor nodes in a non-uniform way, in contrast to simple graph convolution which does not distinguish the neighbors of a node. In this paper, we theoretically study the behaviour of graph attention networks. We prove multiple results on the performance of the graph attention mechanism for the problem of node classification for a contextual stochastic block model. Here, the node features are obtained from a mixture of Gaussians and the edges from a stochastic block model. We show that in an "easy" regime, where the distance between the means of the Gaussians is large enough, graph attention is able to distinguish inter-class from intra-class edges. Thus it maintains the weights of important edges and s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31995;&#21015;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#65292;&#24182;&#20855;&#26377;&#32479;&#35745;&#20445;&#35777;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.05250</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#40065;&#26834;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive and Robust Multi-Task Learning. (arXiv:2202.05250v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31995;&#21015;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#65292;&#24182;&#20855;&#26377;&#32479;&#35745;&#20445;&#35777;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#20915;&#20174;&#19981;&#21516;&#26469;&#28304;&#25910;&#38598;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#24182;&#23545;&#27599;&#20010;&#25968;&#25454;&#38598;&#23398;&#20064;&#19968;&#20010;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#33258;&#21160;&#21033;&#29992;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#21516;&#26102;&#22788;&#29702;&#23427;&#20204;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#23545;&#24322;&#24120;&#20219;&#21153;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the multi-task learning problem that aims to simultaneously analyze multiple datasets collected from different sources and learn one model for each of them. We propose a family of adaptive methods that automatically utilize possible similarities among those tasks while carefully handling their differences. We derive sharp statistical guarantees for the methods and prove their robustness against outlier tasks. Numerical experiments on synthetic and real datasets demonstrate the efficacy of our new methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19977;&#20010;&#30495;&#23454;&#29992;&#20363;&#21644;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#19981;&#29616;&#23454;&#23545;&#25239;&#26679;&#26412;&#23545;&#29616;&#23454;&#23545;&#25239;&#25915;&#20987;&#30340;&#20445;&#25252;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#38450;&#24481;&#19981;&#29616;&#23454;&#25915;&#20987;&#30340;&#31574;&#30053;&#26410;&#24517;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2202.03277</link><description>&lt;p&gt;
&#20851;&#20110;&#19981;&#29616;&#23454;&#23545;&#25239;&#30828;&#21270;&#31574;&#30053;&#23545;&#29616;&#23454;&#23545;&#25239;&#25915;&#20987;&#30340;&#23454;&#35777;&#26377;&#25928;&#24615;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
On The Empirical Effectiveness of Unrealistic Adversarial Hardening Against Realistic Adversarial Attacks. (arXiv:2202.03277v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19977;&#20010;&#30495;&#23454;&#29992;&#20363;&#21644;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#19981;&#29616;&#23454;&#23545;&#25239;&#26679;&#26412;&#23545;&#29616;&#23454;&#23545;&#25239;&#25915;&#20987;&#30340;&#20445;&#25252;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#38450;&#24481;&#19981;&#29616;&#23454;&#25915;&#20987;&#30340;&#31574;&#30053;&#26410;&#24517;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#23433;&#20840;&#25915;&#20987;&#21644;&#38450;&#24481;&#25991;&#29486;&#22823;&#22810;&#20851;&#27880;&#20110;&#19981;&#29616;&#23454;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#29616;&#23454;&#23545;&#25239;&#25915;&#20987;&#39046;&#22495;&#21644;&#20854;&#23545;&#29616;&#23454;&#31995;&#32479;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#36890;&#36807;&#35780;&#20272;&#19977;&#20010;&#30495;&#23454;&#29992;&#20363;&#65288;&#25991;&#26412;&#20998;&#31867;&#12289;&#20725;&#23608;&#32593;&#32476;&#26816;&#27979;&#12289;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#65289;&#21644;&#20116;&#20010;&#25968;&#25454;&#38598;&#65292;&#25506;&#31350;&#20102;&#19981;&#29616;&#23454;&#23545;&#25239;&#26679;&#26412;&#33021;&#21542;&#29992;&#26469;&#20445;&#25252;&#27169;&#22411;&#20813;&#21463;&#29616;&#23454;&#23545;&#25239;&#25915;&#20987;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#19981;&#21516;&#30340;&#29992;&#20363;&#20013;&#23384;&#22312;&#24046;&#24322;&#65292;&#19981;&#29616;&#23454;&#30340;&#20363;&#23376;&#21487;&#20197;&#20687;&#29616;&#23454;&#30340;&#20363;&#23376;&#19968;&#26679;&#26377;&#25928;&#65292;&#20063;&#21487;&#33021;&#21482;&#25552;&#20379;&#26377;&#38480;&#30340;&#25913;&#21892;&#12290;&#20026;&#20102;&#35299;&#37322;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20351;&#29992;&#29616;&#23454;&#21644;&#19981;&#29616;&#23454;&#25915;&#20987;&#29983;&#25104;&#30340;&#23545;&#25239;&#20363;&#23376;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#26174;&#31034;&#23427;&#20204;&#23384;&#22312;&#26174;&#30528;&#30340;&#24046;&#24322;&#12290;&#36825;&#34920;&#26126;&#65292;&#30446;&#21069;&#30340;&#26681;&#25454;&#19981;&#29616;&#23454;&#25915;&#20987;&#35774;&#35745;&#30340;&#38450;&#24481;&#26426;&#21046;&#21487;&#33021;&#26080;&#27861;&#26377;&#25928;&#22320;&#20445;&#25252;&#20813;&#21463;&#29616;&#23454;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the literature on security attacks and defense of Machine Learning (ML) systems mostly focuses on unrealistic adversarial examples, recent research has raised concern about the under-explored field of realistic adversarial attacks and their implications on the robustness of real-world systems. Our paper paves the way for a better understanding of adversarial robustness against realistic attacks and makes two major contributions. First, we conduct a study on three real-world use cases (text classification, botnet detection, malware detection)) and five datasets in order to evaluate whether unrealistic adversarial examples can be used to protect models against realistic examples. Our results reveal discrepancies across the use cases, where unrealistic examples can either be as effective as the realistic ones or may offer only limited improvement. Second, to explain these results, we analyze the latent representation of the adversarial examples generated with realistic and unrealist
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23558;&#21028;&#21035;&#32593;&#32476;&#36716;&#25442;&#20026;&#29983;&#25104;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#39640;&#26031;&#26680;&#26367;&#25442;&#22810;&#38754;&#20307;&#20013;&#30340;&#20223;&#23556;&#20989;&#25968;&#26469;&#29983;&#25104;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#25968;&#25454;&#26657;&#20934;&#38382;&#39064;&#65292;&#24182;&#22312; CIFAR-10&#65292;CIFAR-100 &#21644; SVHN &#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.13001</link><description>&lt;p&gt;
&#28145;&#24230;&#21028;&#21035;&#21040;&#26680;&#29983;&#25104;&#32593;&#32476;&#30340;&#23450;&#26631;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Discriminative to Kernel Generative Networks for Calibrated Inference. (arXiv:2201.13001v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.13001
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23558;&#21028;&#21035;&#32593;&#32476;&#36716;&#25442;&#20026;&#29983;&#25104;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#39640;&#26031;&#26680;&#26367;&#25442;&#22810;&#38754;&#20307;&#20013;&#30340;&#20223;&#23556;&#20989;&#25968;&#26469;&#29983;&#25104;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#25968;&#25454;&#26657;&#20934;&#38382;&#39064;&#65292;&#24182;&#22312; CIFAR-10&#65292;CIFAR-100 &#21644; SVHN &#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21028;&#21035;&#19982;&#29983;&#25104;&#32593;&#32476;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#26234;&#33021;&#30340;&#30740;&#31350;&#20013;&#37117;&#26377;&#20854;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20108;&#32773;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#21028;&#21035;&#32593;&#32476;&#36716;&#25442;&#20026;&#26680;&#29983;&#25104;&#32593;&#32476;&#12290;&#25105;&#20204;&#23558;&#28145;&#24230;&#27169;&#22411;&#35270;&#20026;&#24191;&#20041;&#30340;&#21010;&#20998;&#35268;&#21017;&#65292;&#24182;&#20351;&#29992;&#39640;&#26031;&#26680;&#26367;&#25442;&#30001;&#35757;&#32451;&#25968;&#25454;&#26500;&#25104;&#30340;&#22810;&#38754;&#20307;&#20013;&#30340;&#20223;&#23556;&#20989;&#25968;&#65292;&#26469;&#33719;&#24471;&#29983;&#25104;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fight between discriminative versus generative goes deep, in both the study of artificial and natural intelligence. In our view, both camps have complementary values. So, we sought to synergistically combine them. Here, we propose a methodology to convert deep discriminative networks to kernel generative networks. We leveraged the fact that deep models, including both random forests and deep networks, learn internal representations which are unions of polytopes with affine activation functions to conceptualize them both as generalized partitioning rules. We replace the affine function in each polytope populated by the training data with Gaussian kernel that results in a generative model. Theoretically, we derive the conditions under which our generative models are a consistent estimator of the corresponding class conditional density. Moreover, our proposed models obtain well calibrated posteriors for in-distribution, and extrapolate beyond the training data to handle out-of-distrib
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#19988;&#28789;&#27963;&#30340;EM&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#30340;&#28151;&#21512;&#26925;&#22278;&#20998;&#24067;&#65292;&#35299;&#20915;&#20102;&#38750;&#39640;&#26031;&#25968;&#25454;&#25554;&#34917;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2201.12020</link><description>&lt;p&gt;
&#19968;&#31181;&#40065;&#26834;&#32780;&#28789;&#27963;&#30340;&#26925;&#22278;&#20998;&#24067;&#28151;&#21512;&#32570;&#22833;&#25968;&#25454;EM&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Robust and Flexible EM Algorithm for Mixtures of Elliptical Distributions with Missing Data. (arXiv:2201.12020v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#19988;&#28789;&#27963;&#30340;EM&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#30340;&#28151;&#21512;&#26925;&#22278;&#20998;&#24067;&#65292;&#35299;&#20915;&#20102;&#38750;&#39640;&#26031;&#25968;&#25454;&#25554;&#34917;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22122;&#22768;&#21644;&#38750;&#39640;&#26031;&#25968;&#25454;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#30340;&#38382;&#39064;&#12290;Gaussian&#28151;&#21512;&#27169;&#22411;&#30340;&#26399;&#26395;&#26497;&#22823;&#31639;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#27604;&#22522;&#20110;k&#36817;&#37051;&#25110;&#22522;&#20110;&#22810;&#37325;&#26041;&#31243;&#38142;&#30340;&#25554;&#34917;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#20294;&#26159;&#65292;Gaussian&#28151;&#21512;&#27169;&#22411;&#23545;&#20110;&#24322;&#26500;&#25968;&#25454;&#26159;&#38750;&#40065;&#26834;&#30340;&#65292;&#24403;&#25968;&#25454;&#21463;&#21040;&#31163;&#32676;&#28857;&#25110;&#36981;&#24490;&#38750;&#39640;&#26031;&#20998;&#24067;&#30340;&#24433;&#21709;&#26102;&#20250;&#23548;&#33268;&#24615;&#33021;&#20272;&#35745;&#36739;&#24046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;EM&#31639;&#27861;&#65292;&#29992;&#20110;&#26925;&#22278;&#20998;&#24067;&#30340;&#28151;&#21512;&#29289;&#65292;&#20855;&#26377;&#22788;&#29702;&#28508;&#22312;&#32570;&#22833;&#25968;&#25454;&#30340;&#29305;&#24615;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#24402;&#32467;&#20026;&#22312;&#36890;&#29992;&#26465;&#20214;&#19979;&#65288;&#21363;&#27599;&#20010;&#26679;&#26412;&#37117;&#26159;&#20174;&#19968;&#20010;&#21487;&#33021;&#19981;&#21516;&#30340;&#26925;&#22278;&#20998;&#24067;&#28151;&#21512;&#29289;&#20013;&#25277;&#21462;&#30340;&#65289;&#65292;&#20272;&#35745;angular Gaussian&#20998;&#24067;&#30340;&#28151;&#21512;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper tackles the problem of missing data imputation for noisy and non-Gaussian data. A classical imputation method, the Expectation Maximization (EM) algorithm for Gaussian mixture models, has shown interesting properties when compared to other popular approaches such as those based on k-nearest neighbors or on multiple imputations by chained equations. However, Gaussian mixture models are known to be non-robust to heterogeneous data, which can lead to poor estimation performance when the data is contaminated by outliers or follows non-Gaussian distributions. To overcome this issue, a new EM algorithm is investigated for mixtures of elliptical distributions with the property of handling potential missing data. This paper shows that this problem reduces to the estimation of a mixture of Angular Gaussian distributions under generic assumptions (i.e., each sample is drawn from a mixture of elliptical distributions, which is possibly different for one sample to another). In that case
&lt;/p&gt;</description></item><item><title>PoNet&#26159;&#19968;&#31181;&#27744;&#21270;&#32593;&#32476;&#65292;&#21487;&#29992;&#20110;&#38271;&#24207;&#21015;&#20013;&#30340;token&#28151;&#21512;&#65292;&#20854;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#24182;&#21487;&#20197;&#27604;Transformer&#26356;&#22909;&#22320;&#22788;&#29702;&#38271;&#24207;&#21015;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#33258;&#27880;&#24847;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2110.02442</link><description>&lt;p&gt;
PoNet&#65306;&#38271;&#24207;&#21015;&#20013;&#39640;&#25928;Token&#28151;&#21512;&#30340;&#27744;&#21270;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
PoNet: Pooling Network for Efficient Token Mixing in Long Sequences. (arXiv:2110.02442v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.02442
&lt;/p&gt;
&lt;p&gt;
PoNet&#26159;&#19968;&#31181;&#27744;&#21270;&#32593;&#32476;&#65292;&#21487;&#29992;&#20110;&#38271;&#24207;&#21015;&#20013;&#30340;token&#28151;&#21512;&#65292;&#20854;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#24182;&#21487;&#20197;&#27604;Transformer&#26356;&#22909;&#22320;&#22788;&#29702;&#38271;&#24207;&#21015;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#33258;&#27880;&#24847;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;NLP&#12289;&#35270;&#35273;&#21644;&#35821;&#38899;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;Transformer&#30340;&#26680;&#24515;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#24207;&#21015;&#38271;&#24230;&#30340;&#24179;&#26041;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#65292;&#38459;&#30861;&#20102;Transformer-based&#27169;&#22411;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#26377;&#35768;&#22810;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#20363;&#22914;&#31232;&#30095;&#27880;&#24847;&#26426;&#21046;&#12289;&#20302;&#31209;&#30697;&#38453;&#36817;&#20284;&#21644;&#21487;&#25193;&#23637;&#30340;&#26680;&#20989;&#25968;&#65292;&#20197;&#21450;&#26367;&#20195;&#33258;&#27880;&#24847;&#30340;token&#28151;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38271;&#24207;&#21015;&#20013;token&#28151;&#21512;&#30340;&#26032;&#22411;&#27744;&#21270;&#32593;&#32476;(PoNet)&#65292;&#20854;&#22797;&#26434;&#24230;&#20026;&#32447;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22810;&#31890;&#24230;&#27744;&#21270;&#21644;&#27744;&#21270;&#34701;&#21512;&#26469;&#25429;&#25417;&#19981;&#21516;&#32423;&#21035;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#19982;token&#30340;&#20132;&#20114;&#32467;&#21512;&#36215;&#26469;&#12290;&#22312;&#38271;&#24207;&#21015;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;PoNet&#26174;&#33879;&#20248;&#20110;Transformer&#65292;&#24182;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#19988;&#22312;&#25152;&#26377;&#22312;GPU&#19978;&#27979;&#37327;&#30340;&#24207;&#21015;&#38271;&#24230;&#19978;&#65292;&#23427;&#20165;&#27604;&#26368;&#24555;&#30340;&#27169;&#22411;FNet&#31245;&#24930;&#12290;&#25105;&#20204;&#36824;&#33021;&#21487;&#35270;&#21270;PoNet&#30340;&#27880;&#24847;&#21147;&#22270;&#65292;&#20197;&#23637;&#31034;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#28151;&#21512;&#20855;&#26377;&#19981;&#21516;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;token&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#22788;&#29702;&#38271;&#24207;&#21015;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#33258;&#27880;&#24847;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have achieved great success in various NLP, vision, and speech tasks. However, the core of Transformer, the self-attention mechanism, has a quadratic time and memory complexity with respect to the sequence length, which hinders applications of Transformer-based models to long sequences. Many approaches have been proposed to mitigate this problem, such as sparse attention mechanisms, low-rank matrix approximations and scalable kernels, and token mixing alternatives to self-attention. We propose a novel Pooling Network (PoNet) for token mixing in long sequences with linear complexity. We design multi-granularity pooling and pooling fusion to capture different levels of contextual information and combine their interactions with tokens. On the Long Range Arena benchmark, PoNet significantly outperforms Transformer and achieves competitive accuracy, while being only slightly slower than the fastest model, FNet, across all sequence lengths measured on GPUs. We also c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#19978;&#30028;&#65292;&#23427;&#19981;&#21253;&#21547;&#21452;&#37325;&#20381;&#36182;&#24615;&#30340;&#26415;&#35821;&#65292;&#22312;&#39046;&#22495;&#24402;&#32435;&#20013;&#20248;&#21270;&#20102;&#26410;&#35265;&#22495;&#30340;&#39118;&#38505;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2109.01902</link><description>&lt;p&gt;
&#22810;&#20010;&#39046;&#22495;&#19979;&#36890;&#29992;&#30340;&#36136;&#24515;&#23545;&#40784;&#21644;&#37325;&#26500;&#25439;&#22833;&#26368;&#23567;&#21270;&#39046;&#22495;&#24402;&#32435;&#35770;&#25991;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Barycentric-alignment and reconstruction loss minimization for domain generalization. (arXiv:2109.01902v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.01902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#19978;&#30028;&#65292;&#23427;&#19981;&#21253;&#21547;&#21452;&#37325;&#20381;&#36182;&#24615;&#30340;&#26415;&#35821;&#65292;&#22312;&#39046;&#22495;&#24402;&#32435;&#20013;&#20248;&#21270;&#20102;&#26410;&#35265;&#22495;&#30340;&#39118;&#38505;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25512;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#39046;&#22495;&#24402;&#32435;&#65288;DG&#65289;&#29702;&#35770;&#21644;&#23454;&#36341;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20856;&#22411;&#30340;DG&#35774;&#32622;&#65292;&#20854;&#20013;&#20551;&#35774;&#30001;&#34920;&#31034;&#26144;&#23556;&#21644;&#26631;&#35760;&#20989;&#25968;&#32452;&#25104;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#22823;&#22810;&#25968;&#27969;&#34892;&#30340;DG&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#26368;&#23567;&#21270;&#26410;&#35265;&#22495;&#20013;&#30340;&#20998;&#31867;&#39118;&#38505;&#30340;&#24050;&#30693;&#19978;&#30028;&#20849;&#21516;&#23398;&#20064;&#34920;&#31034;&#21644;&#26631;&#35760;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#22522;&#20110;&#36825;&#20010;&#29702;&#35770;&#19978;&#30028;&#30340;&#26041;&#27861;&#24573;&#30053;&#20102;&#19968;&#20010;&#30001;&#20110;&#20854;&#23545;&#34920;&#31034;&#26144;&#23556;&#21644;&#26410;&#30693;&#26368;&#20248;&#26631;&#35760;&#20989;&#25968;&#30340;&#21452;&#37325;&#20381;&#36182;&#20851;&#31995;&#32780;&#26080;&#27861;&#30452;&#25509;&#20248;&#21270;&#30340;&#26415;&#35821;&#12290;&#20026;&#20102;&#24357;&#21512;&#29702;&#35770;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#19978;&#30028;&#65292;&#23427;&#19981;&#21253;&#21547;&#36825;&#31181;&#21452;&#37325;&#20381;&#36182;&#24615;&#30340;&#26415;&#35821;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#20010;&#21487;&#20197;&#23436;&#20840;&#20248;&#21270;&#30340;&#26410;&#35265;&#22495;&#39118;&#38505;&#19978;&#30028;&#12290;&#25105;&#20204;&#30340;&#25512;&#23548;&#21033;&#29992;&#20102;&#23558;&#26368;&#20248;&#20256;&#36755;&#24230;&#37327;&#19982;&#20449;&#24687;&#30456;&#36830;&#30340;&#32463;&#20856;&#21644;&#26368;&#36817;&#30340;&#20256;&#36755;&#19981;&#31561;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper advances the theory and practice of Domain Generalization (DG) in machine learning. We consider the typical DG setting where the hypothesis is composed of a representation mapping followed by a labeling function. Within this setting, the majority of popular DG methods aim to jointly learn the representation and the labeling functions by minimizing a well-known upper bound for the classification risk in the unseen domain. In practice, however, methods based on this theoretical upper bound ignore a term that cannot be directly optimized due to its dual dependence on both the representation mapping and the unknown optimal labeling function in the unseen domain. To bridge this gap between theory and practice, we introduce a new upper bound that is free of terms having such dual dependence, resulting in a fully optimizable risk upper bound for the unseen domain. Our derivation leverages classical and recent transport inequalities that link optimal transport metrics with informati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#31354;&#20154;&#32676;&#27969;&#37327;&#39044;&#27979;&#20013;&#30340;&#19978;&#19979;&#25991;&#27867;&#21270;&#24615;&#65292;&#24314;&#31435;&#20102;&#22522;&#20934;&#65292;&#25552;&#20986;&#20102;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#20026;&#19978;&#19979;&#25991;&#36873;&#25321;&#21644;&#24314;&#27169;&#25552;&#20379;&#20102;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2106.16046</link><description>&lt;p&gt;
&#25506;&#32034;&#26102;&#31354;&#20154;&#32676;&#27969;&#37327;&#39044;&#27979;&#20013;&#30340;&#19978;&#19979;&#25991;&#27867;&#21270;&#24615;&#65306;&#22522;&#20934;&#21644;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Exploring the Context Generalizability in Spatiotemporal Crowd Flow Prediction: Benchmark and Guideline. (arXiv:2106.16046v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.16046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#31354;&#20154;&#32676;&#27969;&#37327;&#39044;&#27979;&#20013;&#30340;&#19978;&#19979;&#25991;&#27867;&#21270;&#24615;&#65292;&#24314;&#31435;&#20102;&#22522;&#20934;&#65292;&#25552;&#20986;&#20102;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#20026;&#19978;&#19979;&#25991;&#36873;&#25321;&#21644;&#24314;&#27169;&#25552;&#20379;&#20102;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#29305;&#24449;&#26159;&#26500;&#24314;&#26102;&#31354;&#20154;&#32676;&#27969;&#37327;&#39044;&#27979;&#65288;STCFP&#65289;&#27169;&#22411;&#30340;&#37325;&#35201;&#25968;&#25454;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#24212;&#29992;&#19978;&#19979;&#25991;&#30340;&#22256;&#38590;&#22312;&#20110;&#19981;&#21516;&#22330;&#26223;&#20013;&#19978;&#19979;&#25991;&#29305;&#24449;&#65288;&#20363;&#22914;&#22825;&#27668;&#12289;&#20551;&#26085;&#21644;&#20852;&#36259;&#28857;&#65289;&#21644;&#19978;&#19979;&#25991;&#24314;&#27169;&#25216;&#26415;&#30340;&#26410;&#30693;&#27867;&#21270;&#24615;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#30001;&#22823;&#35268;&#27169;&#26102;&#31354;&#20154;&#32676;&#27969;&#37327;&#25968;&#25454;&#12289;&#19978;&#19979;&#25991;&#25968;&#25454;&#21644;&#26368;&#20808;&#36827;&#30340;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#32452;&#25104;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22478;&#24066;&#20154;&#32676;&#27969;&#37327;&#39044;&#27979;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#20197;&#23450;&#37327;&#30740;&#31350;&#19981;&#21516;&#19978;&#19979;&#25991;&#29305;&#24449;&#21644;&#24314;&#27169;&#25216;&#26415;&#30340;&#27867;&#21270;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22522;&#20110;&#23545;&#27969;&#34892;&#30740;&#31350;&#30340;&#24191;&#27867;&#35843;&#26597;&#65292;&#24320;&#21457;&#20102;&#19978;&#19979;&#25991;&#24314;&#27169;&#25216;&#26415;&#30340;&#36890;&#29992;&#20998;&#31867;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#25968;&#30334;&#19975;&#26465;&#35760;&#24405;&#21644;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#25968;&#25454;&#65292;&#35757;&#32451;&#21644;&#27979;&#35797;&#20102;&#25968;&#30334;&#31181;&#27169;&#22411;&#20197;&#25429;&#25417;&#19978;&#19979;&#25991;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;STCFP&#20013;&#30340;&#19978;&#19979;&#25991;&#36873;&#25321;&#21644;&#24314;&#27169;&#25552;&#20379;&#20102;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contextual features are important data sources for building spatiotemporal crowd flow prediction (STCFP) models. However, the difficulty of applying context lies in the unknown generalizability of both contextual features (e.g., weather, holiday, and points of interests) and context modeling techniques across different scenarios. In this paper, we build a benchmark composed of large-scale spatiotemporal crowd flow data, contextual data, and state-of-the-art spatiotemporal prediction models. We conduct a comprehensive experimental study to quantitatively investigate the generalizability of different contextual features and modeling techniques in several urban crowd flow prediction scenarios (including bike flow, metro passenger flow, electric vehicle charging demand and so on). In particular, we develop a general taxonomy of context modeling techniques based on extensive investigations in prevailing research. With millions of records and rich context data, we have trained and tested hun
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#30456;&#20114;&#20316;&#29992;&#24494;&#31890;&#24179;&#22343;&#22330;&#26041;&#31243;&#20013;&#30456;&#20114;&#20316;&#29992;&#26680;&#30340;&#21487;&#36776;&#35782;&#24615;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#20108;&#27425;&#25439;&#22833;&#20989;&#25968;&#20165;&#22312;&#29305;&#23450;&#30340;&#20989;&#25968;&#31354;&#38388;&#20013;&#25165;&#20855;&#26377;&#21807;&#19968;&#30340;&#26368;&#23567;&#21270;&#22120;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#35745;&#31639;&#23454;&#36341;&#65292;&#30740;&#31350;&#35777;&#26126;&#20102;&#21453;&#38382;&#39064;&#30340;&#30149;&#24577;&#24615;&#36136;&#65292;&#38656;&#35201;&#36827;&#34892;&#27491;&#21017;&#21270;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2106.05565</link><description>&lt;p&gt;
&#30456;&#20114;&#20316;&#29992;&#24494;&#31890;&#24179;&#22343;&#22330;&#26041;&#31243;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#26680;&#21487;&#36776;&#35782;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Identifiability of interaction kernels in mean-field equations of interacting particles. (arXiv:2106.05565v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.05565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#30456;&#20114;&#20316;&#29992;&#24494;&#31890;&#24179;&#22343;&#22330;&#26041;&#31243;&#20013;&#30456;&#20114;&#20316;&#29992;&#26680;&#30340;&#21487;&#36776;&#35782;&#24615;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#20108;&#27425;&#25439;&#22833;&#20989;&#25968;&#20165;&#22312;&#29305;&#23450;&#30340;&#20989;&#25968;&#31354;&#38388;&#20013;&#25165;&#20855;&#26377;&#21807;&#19968;&#30340;&#26368;&#23567;&#21270;&#22120;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#35745;&#31639;&#23454;&#36341;&#65292;&#30740;&#31350;&#35777;&#26126;&#20102;&#21453;&#38382;&#39064;&#30340;&#30149;&#24577;&#24615;&#36136;&#65292;&#38656;&#35201;&#36827;&#34892;&#27491;&#21017;&#21270;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#30456;&#20114;&#20316;&#29992;&#24494;&#31890;&#25110;&#20195;&#29702;&#20154;&#30340;&#24179;&#22343;&#22330;&#26041;&#31243;&#20013;&#30456;&#20114;&#20316;&#29992;&#26680;&#30340;&#21487;&#36776;&#35782;&#24615;&#38382;&#39064;&#65292;&#36825;&#26159;&#21508;&#31181;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#26085;&#30410;&#20851;&#27880;&#30340;&#39046;&#22495;&#12290;&#20027;&#35201;&#20851;&#27880;&#28857;&#22312;&#20110;&#30830;&#23450;&#25968;&#25454;&#30456;&#20851;&#20989;&#25968;&#31354;&#38388;&#65292;&#20854;&#20013;&#20108;&#27425;&#25439;&#22833;&#20989;&#25968;&#25317;&#26377;&#21807;&#19968;&#30340;&#26368;&#23567;&#21270;&#22120;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#20010;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;$L^2$&#31354;&#38388;&#65306;&#19968;&#20010;&#26159;&#30001;&#25968;&#25454;&#33258;&#36866;&#24212;&#26435;&#37325;&#34913;&#37327;&#30340;&#65292;&#21478;&#19968;&#20010;&#20351;&#29992;&#21202;&#36125;&#26684;&#27979;&#24230;&#12290;&#22312;&#27599;&#20010;$L^2$&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#21487;&#36776;&#35782;&#24615;&#30340;&#20989;&#25968;&#31354;&#38388;&#26159;&#19982;&#27714;&#31215;&#20998;&#31639;&#23376;&#30456;&#20851;&#30340;RKHS&#38381;&#21253;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#30456;&#36741;&#30456;&#25104;&#65292;&#26412;&#30740;&#31350;&#23436;&#25104;&#20102;&#20855;&#26377;&#26377;&#38480;&#25110;&#26080;&#38480;&#24494;&#31890;&#30340;&#30456;&#20114;&#20316;&#29992;&#24494;&#31890;&#31995;&#32479;&#30340;&#21487;&#36776;&#35782;&#24615;&#30340;&#20840;&#38754;&#25551;&#36848;&#65292;&#31361;&#26174;&#20102;&#36825;&#20004;&#31181;&#24773;&#20917;&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#21487;&#36776;&#35782;&#24615;&#20998;&#26512;&#23545;&#20110;&#35745;&#31639;&#23454;&#36341;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#23427;&#34920;&#26126;&#21453;&#38382;&#39064;&#26159;&#30149;&#24577;&#30340;&#65292;&#38656;&#35201;&#27491;&#21017;&#21270;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study examines the identifiability of interaction kernels in mean-field equations of interacting particles or agents, an area of growing interest across various scientific and engineering fields. The main focus is identifying data-dependent function spaces where a quadratic loss functional possesses a unique minimizer. We consider two data-adaptive $L^2$ spaces: one weighted by a data-adaptive measure and the other using the Lebesgue measure. In each $L^2$ space, we show that the function space of identifiability is the closure of the RKHS associated with the integral operator of inversion.  Alongside prior research, our study completes a full characterization of identifiability in interacting particle systems with either finite or infinite particles, highlighting critical differences between these two settings. Moreover, the identifiability analysis has important implications for computational practice. It shows that the inverse problem is ill-posed, necessitating regularization.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#27979;&#37327;&#25968;&#25454;&#21644;&#29289;&#29702;&#30693;&#35782;&#30340;&#26377;&#38480;&#20808;&#39564;&#32479;&#19968;&#26469;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#38750;&#32447;&#24615;&#21453;&#24212;-&#25193;&#25955;&#36807;&#31243;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2106.04781</link><description>&lt;p&gt;
&#23558;&#29289;&#29702;&#24314;&#27169;&#29992;&#20110;&#23398;&#20064;&#21453;&#24212;-&#25193;&#25955;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Encoding physics to learn reaction-diffusion processes. (arXiv:2106.04781v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.04781
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#27979;&#37327;&#25968;&#25454;&#21644;&#29289;&#29702;&#30693;&#35782;&#30340;&#26377;&#38480;&#20808;&#39564;&#32479;&#19968;&#26469;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#38750;&#32447;&#24615;&#21453;&#24212;-&#25193;&#25955;&#36807;&#31243;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#22797;&#26434;&#30340;&#26102;&#31354;&#21160;&#24577;&#31995;&#32479;&#65292;&#20363;&#22914;&#21453;&#24212;-&#25193;&#25955;&#36807;&#31243;&#65292;&#22823;&#22810;&#20381;&#36182;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19968;&#20123;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#21160;&#24577;&#31995;&#32479;&#65288;&#22914;&#21270;&#23398;&#12289;&#29983;&#29289;&#12289;&#22320;&#36136;&#12289;&#29289;&#29702;&#21644;&#29983;&#24577;&#31995;&#32479;&#65289;&#32570;&#20047;&#20808;&#39564;&#30693;&#35782;&#65292;&#19988;&#29992;&#20110;&#25551;&#36848;&#31995;&#32479;&#21464;&#37327;&#38750;&#32447;&#24615;&#36807;&#31243;&#30340;PDE&#20844;&#24335;&#19981;&#26126;&#30830;&#65292;&#22240;&#27492;&#39044;&#27979;&#36825;&#31181;&#31995;&#32479;&#30340;&#28436;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#23558;&#27979;&#37327;&#25968;&#25454;&#21644;&#26377;&#38480;&#30340;&#20808;&#39564;&#29289;&#29702;&#30693;&#35782;&#32479;&#19968;&#36215;&#26469;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26032;&#36884;&#24452;&#12290;&#29616;&#26377;&#30340;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#33539;&#24335;&#36890;&#36807;&#36719;&#32422;&#26463;&#24809;&#32602;&#23454;&#26045;&#29289;&#29702;&#23450;&#24459;&#65292;&#20854;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#20027;&#35201;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#30340;&#35797;&#38169;&#36866;&#24403;&#35774;&#32622;&#12290;&#30001;&#20110;&#36825;&#20123;&#26041;&#27861;&#30340;&#26680;&#24515;&#20173;&#28982;&#26681;&#26893;&#20110;&#40657;&#30418;&#31070;&#32463;&#32593;&#32476;&#65292;&#25152;&#24471;&#27169;&#22411;&#36890;&#24120;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#24182;&#19988;&#23384;&#22312;&#22806;&#25512;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling complex spatiotemporal dynamical systems, such as the reaction-diffusion processes, have largely relied on partial differential equations (PDEs). However, due to insufficient prior knowledge on some under-explored dynamical systems, such as those in chemistry, biology, geology, physics and ecology, and the lack of explicit PDE formulation used for describing the nonlinear process of the system variables, to predict the evolution of such a system remains a challenging task. Unifying measurement data and our limited prior physics knowledge via machine learning provides us with a new path to solving this problem. Existing physics-informed learning paradigms impose physics laws through soft penalty constraints, whose solution quality largely depends on a trial-and-error proper setting of hyperparameters. Since the core of such methods is still rooted in black-box neural networks, the resulting model generally lacks interpretability and suffers from critical issues of extrapolation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#23545;&#32852;&#37030;&#23398;&#20064;&#30340;&#30899;&#36275;&#36857;&#36827;&#34892;&#30340;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#65292;&#21457;&#29616;FL&#30340;&#30899;&#25490;&#25918;&#37327;&#26368;&#22810;&#21487;&#36798;&#20256;&#32479;&#23398;&#20064;&#30340;&#20004;&#20493;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2102.07627</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#30340;&#30899;&#36275;&#36857;&#21021;&#25506;
&lt;/p&gt;
&lt;p&gt;
A first look into the carbon footprint of federated learning. (arXiv:2102.07627v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.07627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#23545;&#32852;&#37030;&#23398;&#20064;&#30340;&#30899;&#36275;&#36857;&#36827;&#34892;&#30340;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#65292;&#21457;&#29616;FL&#30340;&#30899;&#25490;&#25918;&#37327;&#26368;&#22810;&#21487;&#36798;&#20256;&#32479;&#23398;&#20064;&#30340;&#20004;&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#65292;&#20294;&#30001;&#25968;&#25454;&#20013;&#24515;&#36827;&#34892;&#30340;&#35757;&#32451;&#36807;&#31243;&#20063;&#24341;&#36215;&#20102;&#20005;&#37325;&#30340;&#38544;&#31169;&#21644;&#29615;&#22659;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#22914;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#12290;&#28982;&#32780;&#65292;&#19982;FL&#30456;&#20851;&#30340;&#28508;&#22312;&#29615;&#22659;&#24433;&#21709;&#23578;&#19981;&#26126;&#30830;&#25110;&#26410;&#34987;&#25506;&#31350;&#12290;&#36825;&#31687;&#35770;&#25991;&#38024;&#23545;FL&#30340;&#30899;&#36275;&#36857;&#36827;&#34892;&#20102;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#27169;&#22411;&#26469;&#37327;&#21270;&#30899;&#36275;&#36857;&#65292;&#20174;&#32780;&#26041;&#20415;&#30740;&#31350;FL&#35774;&#35745;&#19982;&#30899;&#25490;&#25918;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#23558;FL&#30340;&#30899;&#36275;&#36857;&#19982;&#20256;&#32479;&#30340;&#38598;&#20013;&#24335;&#23398;&#20064;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#26681;&#25454;&#19981;&#21516;&#30340;&#37197;&#32622;&#65292;FL&#30340;&#30899;&#25490;&#25918;&#37327;&#26368;&#22810;&#21487;&#36798;&#20256;&#32479;&#23398;&#20064;&#30340;&#20004;&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite impressive results, deep learning-based technologies also raise severe privacy and environmental concerns induced by the training procedure often conducted in data centers. In response, alternatives to centralized training such as Federated Learning (FL) have emerged. Perhaps unexpectedly, FL is starting to be deployed at a global scale by companies that must adhere to new legal demands and policies originating from governments and social groups advocating for privacy protection. \textit{However, the potential environmental impact related to FL remains unclear and unexplored. This paper offers the first-ever systematic study of the carbon footprint of FL.} First, we propose a rigorous model to quantify the carbon footprint, hence facilitating the investigation of the relationship between FL design and carbon emissions. Then, we compare the carbon footprint of FL to traditional centralized learning. Our findings show that, depending on the configuration, FL can emit up to two or
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#31070;&#32463;&#20803;&#20013;&#19982;/&#25110;&#20989;&#25968;&#30340;&#36830;&#32493;&#24615;&#23545;&#20998;&#31867;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#22686;&#21152;&#31867;AND&#31070;&#32463;&#20803;&#22312;&#32593;&#32476;&#20013;&#27604;&#20363;&#30340;&#25514;&#26045;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2102.07389</link><description>&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#20803;&#20013;&#30340;&#19982;/&#25110;&#26435;&#34913;&#65306;&#23545;&#25239;&#40065;&#26834;&#24615;&#24433;&#21709;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
And/or trade-off in artificial neurons: impact on adversarial robustness. (arXiv:2102.07389v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.07389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#31070;&#32463;&#20803;&#20013;&#19982;/&#25110;&#20989;&#25968;&#30340;&#36830;&#32493;&#24615;&#23545;&#20998;&#31867;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#22686;&#21152;&#31867;AND&#31070;&#32463;&#20803;&#22312;&#32593;&#32476;&#20013;&#27604;&#20363;&#30340;&#25514;&#26045;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#20998;&#31867;&#40065;&#26834;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#23545;&#25239;&#24615;&#26679;&#26412;&#19978;&#34920;&#29616;&#23588;&#20026;&#31361;&#20986;&#12290;&#26412;&#25991;&#36890;&#36807;&#20851;&#27880;&#20154;&#24037;&#31070;&#32463;&#20803;&#20013;&#23454;&#29616;&#30340;&#20989;&#25968;&#36830;&#32493;&#24615;&#65292;&#20174;&#32431;AND&#38376;&#21040;&#32431;OR&#38376;&#30340;&#33539;&#22260;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#22312;&#32593;&#32476;&#20013;&#23384;&#22312;&#36275;&#22815;&#25968;&#37327;&#30340;&#31867;OR&#30340;&#31070;&#32463;&#20803;&#20250;&#23548;&#33268;&#20998;&#31867;&#30340;&#33030;&#24369;&#24615;&#21644;&#22686;&#21152;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#31867;AND&#30340;&#31070;&#32463;&#20803;&#65292;&#24182;&#25552;&#20986;&#20102;&#22686;&#21152;&#23427;&#20204;&#22312;&#32593;&#32476;&#20013;&#27604;&#20363;&#30340;&#25514;&#26045;&#12290;&#36825;&#20123;&#25514;&#26045;&#28041;&#21450;&#23558;&#36755;&#20837;&#25918;&#32553;&#21040;[-1,1]&#21306;&#38388;&#24182;&#20943;&#23569;S&#22411;&#28608;&#27963;&#20989;&#25968;&#38497;&#23789;&#37096;&#20998;&#30340;&#28857;&#25968;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#37096;&#20998;&#26159;&#27604;&#36739;&#24403;&#31070;&#32463;&#20803;&#29992;&#23454;&#38469;&#25968;&#25454;&#38598;&#21644;&#31216;&#20026;&#8220;&#20081;&#24207;&#25968;&#25454;&#38598;&#8221;&#30340;&#38543;&#26426;&#29256;&#26412;&#36755;&#20837;&#26102;&#30340;&#36755;&#20986;&#20998;&#24067;&#12290;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24456;&#26377;&#21069;&#36884;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of neural networks, the issue of classification robustness remains, particularly highlighted by adversarial examples. In this paper, we address this challenge by focusing on the continuum of functions implemented in artificial neurons, ranging from pure AND gates to pure OR gates. Our hypothesis is that the presence of a sufficient number of OR-like neurons in a network can lead to classification brittleness and increased vulnerability to adversarial attacks. We define AND-like neurons and propose measures to increase their proportion in the network. These measures involve rescaling inputs to the [-1,1] interval and reducing the number of points in the steepest section of the sigmoidal activation function. A crucial component of our method is the comparison between a neuron's output distribution when fed with the actual dataset and a randomised version called the "scrambled dataset." Experimental results on the MNIST dataset suggest that our approach holds promise a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#26694;&#26550;&#65292;&#20801;&#35768;&#20154;&#20204;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#23450;&#21021;&#22987;&#34892;&#20026;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#35789;&#27719;&#20915;&#31574;&#26641;&#65292;&#20026;&#26426;&#22120;&#20154;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2101.07140</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;RL&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Natural Language Specification of Reinforcement Learning Policies through Differentiable Decision Trees. (arXiv:2101.07140v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.07140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#26694;&#26550;&#65292;&#20801;&#35768;&#20154;&#20204;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#23450;&#21021;&#22987;&#34892;&#20026;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#35789;&#27719;&#20915;&#31574;&#26641;&#65292;&#20026;&#26426;&#22120;&#20154;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#25919;&#31574;&#35268;&#33539;&#36807;&#31243;&#65292;&#21487;&#20197;&#20351;&#20154;&#31867;&#19982;&#26426;&#22120;&#20154;&#20849;&#21516;&#21551;&#21160;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#12290;&#35813;&#36807;&#31243;&#21253;&#21547;&#20004;&#20010;&#27493;&#39588;&#65306;&#25919;&#31574;&#35268;&#33539;&#19982;&#25919;&#31574;&#20248;&#21270;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#21327;&#20316;&#26694;&#26550;&#65292;&#20801;&#35768;&#20154;&#36890;&#36807;&#38750;&#32467;&#26500;&#21270;&#33258;&#28982;&#35821;&#35328;&#25351;&#23450;&#21021;&#22987;&#34892;&#20026;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#35789;&#27719;&#20915;&#31574;&#26641;&#26469;&#21551;&#21160;&#21644;&#35299;&#37322;&#19968;&#20010;&#33258;&#20027;&#20195;&#29702;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-AI policy specification is a novel procedure we define in which humans can collaboratively warm-start a robot's reinforcement learning policy. This procedure is comprised of two steps; (1) Policy Specification, i.e. humans specifying the behavior they would like their companion robot to accomplish, and (2) Policy Optimization, i.e. the robot applying reinforcement learning to improve the initial policy. Existing approaches to enabling collaborative policy specification are often unintelligible black-box methods, and are not catered towards making the autonomous system accessible to a novice end-user. In this paper, we develop a novel collaborative framework to allow humans to initialize and interpret an autonomous agent's behavior. Through our framework, we enable humans to specify an initial behavior model via unstructured, natural language (NL), which we convert to lexical decision trees. Next, we leverage these translated specifications, to warm-start reinforcement learning an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31163;&#31574;&#30053;TDC&#31639;&#27861;&#30340;&#26041;&#24046;&#32553;&#20943;&#26041;&#26696;&#65292;&#24182;&#22312;i.i.d.&#21644;&#39532;&#23572;&#21487;&#22827;&#25277;&#26679;&#20013;&#20998;&#26512;&#20102;&#20854;&#25910;&#25947;&#29575;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#23454;&#29616;&#20102;&#19982;&#24050;&#30693;&#26368;&#20339;&#19979;&#38480;&#30456;&#21305;&#37197;&#30340;i.i.d.&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#25509;&#36817;&#26368;&#20248;&#30340;&#39532;&#23572;&#21487;&#22827;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2010.13272</link><description>&lt;p&gt;
&#26041;&#24046;&#32553;&#20943;&#30340;&#31163;&#31574;&#30053;TDC&#23398;&#20064;: &#38750;&#28176;&#36827;&#25910;&#25947;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Variance-Reduced Off-Policy TDC Learning: Non-Asymptotic Convergence Analysis. (arXiv:2010.13272v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.13272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31163;&#31574;&#30053;TDC&#31639;&#27861;&#30340;&#26041;&#24046;&#32553;&#20943;&#26041;&#26696;&#65292;&#24182;&#22312;i.i.d.&#21644;&#39532;&#23572;&#21487;&#22827;&#25277;&#26679;&#20013;&#20998;&#26512;&#20102;&#20854;&#25910;&#25947;&#29575;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#23454;&#29616;&#20102;&#19982;&#24050;&#30693;&#26368;&#20339;&#19979;&#38480;&#30456;&#21305;&#37197;&#30340;i.i.d.&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#25509;&#36817;&#26368;&#20248;&#30340;&#39532;&#23572;&#21487;&#22827;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#26102;&#38388;&#24046;&#20998;&#65288;TD&#65289;&#23398;&#20064;&#65292;&#24182;&#26377;&#21161;&#20110;&#25552;&#39640;&#31574;&#30053;&#35780;&#20272;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#35201;&#20040;&#23558;&#26041;&#24046;&#32553;&#20943;&#24212;&#29992;&#20110;&#36739;&#19981;&#27969;&#34892;&#30340;&#21333;&#26102;&#38388;&#23610;&#24230;TD&#31639;&#27861;&#65292;&#35201;&#20040;&#23558;&#20854;&#24212;&#29992;&#20110;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#30340;GTD&#31639;&#27861;&#65292;&#20294;&#20165;&#20351;&#29992;&#26377;&#38480;&#25968;&#30446;&#30340;i.i.d.&#26679;&#26412;&#65292;&#24182;&#19988;&#20004;&#31181;&#31639;&#27861;&#20165;&#36866;&#29992;&#20110;&#22312;&#32447;&#31574;&#30053;&#35774;&#32622;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20026;&#31163;&#31574;&#30053;&#35774;&#32622;&#20013;&#30340;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;TDC&#31639;&#27861;&#24320;&#21457;&#20102;&#26041;&#24046;&#32553;&#20943;&#26041;&#26696;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#22312;i.i.d.&#21644;&#39532;&#23572;&#21487;&#22827;&#25277;&#26679;&#20013;&#30340;&#38750;&#28176;&#36827;&#25910;&#25947;&#29575;&#12290;&#22312;i.i.d.&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;&#24050;&#30693;&#26368;&#20339;&#19979;&#38480;&#30456;&#21305;&#37197;$\tilde{O}(\epsilon^{-1}$)&#12290;&#22312;&#39532;&#23572;&#21487;&#22827;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;$O(\epsilon^{-1} \log {\epsilon}^{-1})$&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#24046;&#32553;&#20943;TDC&#31639;&#27861;&#30340;&#28176;&#36817;&#25910;&#25947;&#35823;&#24046;&#27604;&#20256;&#32479;&#30340;TDC&#21644;&#26041;&#24046;&#32553;&#20943;GTD&#31639;&#27861;&#37117;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variance reduction techniques have been successfully applied to temporal-difference (TD) learning and help to improve the sample complexity in policy evaluation. However, the existing work applied variance reduction to either the less popular one time-scale TD algorithm or the two time-scale GTD algorithm but with a finite number of i.i.d.\ samples, and both algorithms apply to only the on-policy setting. In this work, we develop a variance reduction scheme for the two time-scale TDC algorithm in the off-policy setting and analyze its non-asymptotic convergence rate over both i.i.d.\ and Markovian samples. In the i.i.d.\ setting, our algorithm {matches the best-known lower bound $\tilde{O}(\epsilon^{-1}$).} In the Markovian setting, our algorithm achieves the state-of-the-art sample complexity $O(\epsilon^{-1} \log {\epsilon}^{-1})$ that is near-optimal. Experiments demonstrate that the proposed variance-reduced TDC achieves a smaller asymptotic convergence error than both the conventi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;Deep Dynamic Factor Model (D$^2$FM)&#65292;&#33021;&#22815;&#23558;&#25968;&#30334;&#20010;&#23439;&#35266;&#32463;&#27982;&#21644;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#21487;&#29992;&#20449;&#24687;&#32534;&#30721;&#20026;&#23569;&#37327;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#28508;&#22312;&#29366;&#24577;&#65292;&#24182;&#33021;&#25552;&#39640;&#29616;&#22312;&#39044;&#27979;&#21644;&#39044;&#27979;&#23454;&#39564;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2007.11887</link><description>&lt;p&gt;
&#28145;&#24230;&#21160;&#24577;&#22240;&#23376;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deep Dynamic Factor Models. (arXiv:2007.11887v2 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.11887
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;Deep Dynamic Factor Model (D$^2$FM)&#65292;&#33021;&#22815;&#23558;&#25968;&#30334;&#20010;&#23439;&#35266;&#32463;&#27982;&#21644;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#21487;&#29992;&#20449;&#24687;&#32534;&#30721;&#20026;&#23569;&#37327;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#28508;&#22312;&#29366;&#24577;&#65292;&#24182;&#33021;&#25552;&#39640;&#29616;&#22312;&#39044;&#27979;&#21644;&#39044;&#27979;&#23454;&#39564;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#8212;&#8212;Deep Dynamic Factor Model (D$^2$FM)&#65292;&#33021;&#22815;&#23558;&#25968;&#30334;&#20010;&#23439;&#35266;&#32463;&#27982;&#21644;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#21487;&#29992;&#20449;&#24687;&#32534;&#30721;&#20026;&#23569;&#37327;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#28508;&#22312;&#29366;&#24577;&#12290;&#19982;&#20256;&#32479;&#21160;&#24577;&#22240;&#23376;&#27169;&#22411;&#65288;DFMs&#65289;&#31867;&#20284;&#65292;&#20294;&#30001;&#20110;&#33258;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#19982;&#35266;&#27979;&#20540;&#20043;&#38388;&#20801;&#35768;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#19978;&#65292;&#27169;&#22411;&#30340;&#28508;&#22312;&#29366;&#24577;&#20173;&#21487;&#20687;&#26631;&#20934;&#22240;&#23376;&#27169;&#22411;&#37027;&#26679;&#36827;&#34892;&#35299;&#37322;&#12290;&#22312;&#20351;&#29992;&#32654;&#22269;&#25968;&#25454;&#36827;&#34892;&#20840;&#23454;&#26102;&#22806;&#26679;&#26412;&#29616;&#22312;&#39044;&#27979;&#21644;&#39044;&#27979;&#23454;&#39564;&#20197;&#21450;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#20013;&#65292;D$^2$FM&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;DFM&#12290;
&lt;/p&gt;
&lt;p&gt;
A novel deep neural network framework -- that we refer to as Deep Dynamic Factor Model (D$^2$FM) --, is able to encode the information available, from hundreds of macroeconomic and financial time-series into a handful of unobserved latent states. While similar in spirit to traditional dynamic factor models (DFMs), differently from those, this new class of models allows for nonlinearities between factors and observables due to the autoencoder neural network structure. However, by design, the latent states of the model can still be interpreted as in a standard factor model. Both in a fully real-time out-of-sample nowcasting and forecasting exercise with US data and in a Monte Carlo experiment, the D$^2$FM improves over the performances of a state-of-the-art DFM.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21487;&#20197;&#22312;&#38750;&#24120;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#28145;&#24230;&#35821;&#38899;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#32467;&#26524;&#30456;&#24403;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/1912.05946</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25552;&#21319;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Leveraging End-to-End Speech Recognition with Neural Architecture Search. (arXiv:1912.05946v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1912.05946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21487;&#20197;&#22312;&#38750;&#24120;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#28145;&#24230;&#35821;&#38899;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#32467;&#26524;&#30456;&#24403;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#34987;&#35777;&#23454;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#26041;&#38754;&#20248;&#20110;&#35768;&#22810;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#26377;&#25928;&#23454;&#26045;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21487;&#20197;&#22312;&#38750;&#24120;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#28145;&#24230;&#35821;&#38899;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#20351;&#29992;&#27969;&#34892;&#30340;LibriSpeech&#21644;TIMIT&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#30340;&#38899;&#32032;&#35782;&#21035;&#27979;&#35797;&#35777;&#26126;&#20102;&#36825;&#19968;&#20107;&#23454;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20960;&#20010;&#23567;&#26102;&#20043;&#20869;&#65288;&#19981;&#21040;&#19968;&#22825;&#65289;&#65292;&#27604;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;seq2seq&#27169;&#22411;&#24555;&#22810;&#27425;&#65292;&#25506;&#27979;&#21644;&#35757;&#32451;&#26032;&#30340;&#20505;&#36873;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;LibriSpeech&#35821;&#26009;&#24211;&#19978;&#30340;&#27979;&#35797;&#35823;&#24046;&#29575;&#65288;WER&#65289;&#20026;7&#65285;&#65292;&#22312;TIMIT&#35821;&#26009;&#24211;&#19978;&#30340;&#38899;&#32032;&#35823;&#24046;&#29575;&#65288;PER&#65289;&#20026;13&#65285;&#65292;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#32467;&#26524;&#30456;&#24403;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have been demonstrated to outperform many traditional machine learning algorithms in Automatic Speech Recognition (ASR). In this paper, we show that a large improvement in the accuracy of deep speech models can be achieved with effective Neural Architecture Optimization at a very low computational cost. Phone recognition tests with the popular LibriSpeech and TIMIT benchmarks proved this fact by displaying the ability to discover and train novel candidate models within a few hours (less than a day) many times faster than the attention-based seq2seq models. Our method achieves test error of 7% Word Error Rate (WER) on the LibriSpeech corpus and 13% Phone Error Rate (PER) on the TIMIT corpus, on par with state-of-the-art results.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#21644;&#33410;&#33021;&#22411;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#65292;&#36890;&#36807;&#26234;&#33021;&#30340;&#29926;&#29255;&#20998;&#27966;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22788;&#29702;&#19981;&#21516;RNN&#32500;&#24230;&#30340;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/1911.01258</link><description>&lt;p&gt;
SHARP&#65306;&#19968;&#31181;&#36866;&#24212;&#24615;&#21644;&#33410;&#33021;&#22411;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
SHARP: An Adaptable, Energy-Efficient Accelerator for Recurrent Neural Network. (arXiv:1911.01258v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1911.01258
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#21644;&#33410;&#33021;&#22411;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#65292;&#36890;&#36807;&#26234;&#33021;&#30340;&#29926;&#29255;&#20998;&#27966;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22788;&#29702;&#19981;&#21516;RNN&#32500;&#24230;&#30340;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#24341;&#21457;&#20102;&#23545;RNN&#25512;&#29702;&#21152;&#36895;&#30340;&#20852;&#36259;&#12290;&#30001;&#20110;RNN&#35745;&#31639;&#30340;&#36882;&#24402;&#24615;&#21644;&#25968;&#25454;&#20381;&#36182;&#24615;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#35774;&#35745;&#20102;&#19987;&#38376;&#38024;&#23545;RNN&#35745;&#31639;&#27169;&#24335;&#30340;&#23450;&#21046;&#21270;&#26550;&#26500;&#65292;&#20026;&#26576;&#20123;&#36873;&#25321;&#30340;&#27169;&#22411;&#22823;&#23567;&#33719;&#24471;&#20102;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;RNN&#30340;&#32500;&#24230;&#23545;&#20110;&#19981;&#21516;&#20219;&#21153;&#21464;&#21270;&#24456;&#22823;&#65292;&#23558;&#36825;&#31181;&#25928;&#29575;&#25512;&#24191;&#21040;&#21508;&#31181;&#19981;&#21516;&#37197;&#32622;&#38750;&#24120;&#20851;&#38190;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36866;&#24212;&#24615;&#26159;&#24403;&#20170;RNN&#21152;&#36895;&#22120;&#25152;&#32570;&#23569;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#26174;&#31034;&#20102;&#29616;&#26377;&#30340;GPU&#12289;FPGA&#21644;ASIC&#26550;&#26500;&#30340;&#26368;&#26032;RNN&#23454;&#29616;&#22312;&#36164;&#28304;&#21033;&#29992;&#29575;&#21644;&#36866;&#24212;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#30340;&#22522;&#20110;&#29926;&#29255;&#30340;&#20998;&#27966;&#26426;&#21046;&#65292;&#20197;&#22686;&#21152;RNN&#35745;&#31639;&#30340;&#36866;&#24212;&#24615;&#65292;&#20197;&#20415;&#26377;&#25928;&#22320;&#22788;&#29702;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of Recurrent Neural Networks (RNNs) for tasks such as Automatic Speech Recognition has fostered interest in RNN inference acceleration. Due to the recurrent nature and data dependencies of RNN computations, prior work has designed customized architectures specifically tailored to the computation pattern of RNN, getting high computation efficiency for certain chosen model sizes. However, given that the dimensionality of RNNs varies a lot for different tasks, it is crucial to generalize this efficiency to diverse configurations. In this work, we identify adaptiveness as a key feature that is missing from today's RNN accelerators. In particular, we first show the problem of low resource-utilization and low adaptiveness for the state-of-the-art RNN implementations on GPU, FPGA and ASIC architectures. To solve these issues, we propose an intelligent tiled-based dispatching mechanism for increasing the adaptiveness of RNN computation, in order to efficiently handle the data
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#36807;&#25311;&#21512;&#12289;&#20132;&#21449;&#39564;&#35777;&#12289;&#27491;&#21017;&#21270;&#12289;&#35013;&#34955;&#27861;&#21644;&#25552;&#21319;&#27861;&#30340;&#30456;&#20851;&#29702;&#35770;&#65292;&#21253;&#25324;&#23450;&#20041;&#21644;&#20855;&#20307;&#23454;&#29616;&#65292;&#24182;&#32473;&#20986;&#20102;AdaBoost&#30340;&#27867;&#21270;&#35823;&#24046;&#19978;&#38480;&#30340;&#20855;&#20307;&#35745;&#31639;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/1905.12787</link><description>&lt;p&gt;
&#36807;&#25311;&#21512;&#12289;&#20132;&#21449;&#39564;&#35777;&#12289;&#27491;&#21017;&#21270;&#12289;&#35013;&#34955;&#27861;&#21644;&#25552;&#21319;&#27861;&#32972;&#21518;&#30340;&#29702;&#35770;&#65306;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
The Theory Behind Overfitting, Cross Validation, Regularization, Bagging, and Boosting: Tutorial. (arXiv:1905.12787v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1905.12787
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#36807;&#25311;&#21512;&#12289;&#20132;&#21449;&#39564;&#35777;&#12289;&#27491;&#21017;&#21270;&#12289;&#35013;&#34955;&#27861;&#21644;&#25552;&#21319;&#27861;&#30340;&#30456;&#20851;&#29702;&#35770;&#65292;&#21253;&#25324;&#23450;&#20041;&#21644;&#20855;&#20307;&#23454;&#29616;&#65292;&#24182;&#32473;&#20986;&#20102;AdaBoost&#30340;&#27867;&#21270;&#35823;&#24046;&#19978;&#38480;&#30340;&#20855;&#20307;&#35745;&#31639;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#25945;&#31243;&#24615;&#30340;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;&#38543;&#26426;&#21464;&#37327;&#21644;&#20998;&#31867;/&#39044;&#27979;&#27169;&#22411;&#30340;&#22343;&#26041;&#35823;&#24046;&#12289;&#26041;&#24046;&#12289;&#21327;&#26041;&#24046;&#21644;&#20559;&#24046;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;Stein&#30340;&#26080;&#20559;&#39118;&#38505;&#20272;&#35745;&#22120;&#65288;SURE&#65289;&#21046;&#23450;&#20102;&#27169;&#22411;&#30340;&#30495;&#23454;&#21644;&#27867;&#21270;&#35823;&#24046;&#65292;&#21253;&#25324;&#35757;&#32451;&#21644;&#39564;&#35777;/&#27979;&#35797;&#23454;&#20363;&#12290;&#21033;&#29992;&#24471;&#21040;&#30340;&#30495;&#23454;&#21644;&#27867;&#21270;&#35823;&#24046;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#36807;&#25311;&#21512;&#12289;&#27424;&#25311;&#21512;&#21644;&#27867;&#21270;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20132;&#21449;&#39564;&#35777;&#21644;&#20004;&#20010;&#33879;&#21517;&#30340;&#20363;&#23376;&#65292;&#21363;K&#20493;&#20132;&#21449;&#39564;&#35777;&#21644;&#30041;&#19968;&#27861;&#20132;&#21449;&#39564;&#35777;&#12290;&#25105;&#20204;&#31616;&#35201;&#20171;&#32461;&#20102;&#24191;&#20041;&#20132;&#21449;&#39564;&#35777;&#65292;&#28982;&#21518;&#36716;&#21521;&#27491;&#21017;&#21270;&#65292;&#22312;&#36825;&#37324;&#25105;&#20204;&#20877;&#27425;&#20351;&#29992;SURE&#12290;&#25105;&#20204;&#23545;$\ell_2$&#21644;$\ell_1$&#33539;&#25968;&#27491;&#21017;&#21270;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#20030;&#32858;&#21512;&#65288;bagging&#65289;&#22914;&#20309;&#38477;&#20302;&#20272;&#35745;&#26041;&#24046;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#25552;&#21319;&#27861;&#65292;&#29305;&#21035;&#26159;AdaBoost&#65292;&#24182;&#35299;&#37322;&#20102;&#23427;&#20316;&#20026;&#19968;&#20010;&#21152;&#24615;&#27169;&#22411;&#21644;&#26368;&#22823;&#38388;&#38548;&#27169;&#22411;&#65288;&#21363;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#65289;&#30340;&#21407;&#29702;&#12290;&#32473;&#20986;&#20102;AdaBoost&#30340;&#27867;&#21270;&#35823;&#24046;&#19978;&#38480;&#65292;&#21253;&#25324;&#25351;&#25968;&#25439;&#22833;&#21644;0-1&#25439;&#22833;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#25945;&#31243;&#30340;&#20027;&#35201;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this tutorial paper, we first define mean squared error, variance, covariance, and bias of both random variables and classification/predictor models. Then, we formulate the true and generalization errors of the model for both training and validation/test instances where we make use of the Stein's Unbiased Risk Estimator (SURE). We define overfitting, underfitting, and generalization using the obtained true and generalization errors. We introduce cross validation and two well-known examples which are $K$-fold and leave-one-out cross validations. We briefly introduce generalized cross validation and then move on to regularization where we use the SURE again. We work on both $\ell_2$ and $\ell_1$ norm regularizations. Then, we show that bootstrap aggregating (bagging) reduces the variance of estimation. Boosting, specifically AdaBoost, is introduced and it is explained as both an additive model and a maximum margin model, i.e., Support Vector Machine (SVM). The upper bound on the gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#19968;&#31687;&#38416;&#36848;&#29305;&#24449;&#20540;&#21644;&#24191;&#20041;&#29305;&#24449;&#20540;&#38382;&#39064;&#30340;&#25945;&#31243;&#12290;&#29305;&#24449;&#20540;&#21644;&#24191;&#20041;&#29305;&#24449;&#20540;&#38382;&#39064;&#21487;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#22914;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;Fisher&#21028;&#21035;&#20998;&#26512;&#31561;&#12290;</title><link>http://arxiv.org/abs/1903.11240</link><description>&lt;p&gt;
&#29305;&#24449;&#20540;&#21644;&#24191;&#20041;&#29305;&#24449;&#20540;&#38382;&#39064;&#65306;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Eigenvalue and Generalized Eigenvalue Problems: Tutorial. (arXiv:1903.11240v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1903.11240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#19968;&#31687;&#38416;&#36848;&#29305;&#24449;&#20540;&#21644;&#24191;&#20041;&#29305;&#24449;&#20540;&#38382;&#39064;&#30340;&#25945;&#31243;&#12290;&#29305;&#24449;&#20540;&#21644;&#24191;&#20041;&#29305;&#24449;&#20540;&#38382;&#39064;&#21487;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#22914;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;Fisher&#21028;&#21035;&#20998;&#26512;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#29305;&#24449;&#20540;&#21644;&#24191;&#20041;&#29305;&#24449;&#20540;&#38382;&#39064;&#30340;&#25945;&#31243;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#29305;&#24449;&#20540;&#38382;&#39064;&#12289;&#29305;&#24449;&#20540;&#20998;&#35299;&#65288;&#35889;&#20998;&#35299;&#65289;&#21644;&#24191;&#20041;&#29305;&#24449;&#20540;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#21040;&#20102;&#23548;&#33268;&#29305;&#24449;&#20540;&#21644;&#24191;&#20041;&#29305;&#24449;&#20540;&#38382;&#39064;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20363;&#23376;&#65292;&#21253;&#25324;&#20027;&#25104;&#20998;&#20998;&#26512;&#12289;&#26680;&#30417;&#30563;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;Fisher&#21028;&#21035;&#20998;&#26512;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#20250;&#23548;&#33268;&#29305;&#24449;&#20540;&#21644;&#24191;&#20041;&#29305;&#24449;&#20540;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#35299;&#20915;&#29305;&#24449;&#20540;&#21644;&#24191;&#20041;&#29305;&#24449;&#20540;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is a tutorial for eigenvalue and generalized eigenvalue problems. We first introduce eigenvalue problem, eigen-decomposition (spectral decomposition), and generalized eigenvalue problem. Then, we mention the optimization problems which yield to the eigenvalue and generalized eigenvalue problems. We also provide examples from machine learning, including principal component analysis, kernel supervised principal component analysis, and Fisher discriminant analysis, which result in eigenvalue and generalized eigenvalue problems. Finally, we introduce the solutions to both eigenvalue and generalized eigenvalue problems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33539;&#25968;&#27714;&#21644;&#27491;&#21017;&#21270;&#39033;&#30340;&#20984;&#24615;&#26368;&#20248;&#20256;&#36755;&#31243;&#24207;&#65292;&#22312;&#20960;&#20309;&#20551;&#35774;&#26465;&#20214;&#19979;&#21487;&#35777;&#26126;&#24674;&#22797;&#22522;&#30784;&#31867;&#32467;&#26500;&#12290;&#35813;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#21152;&#36895;&#30340;&#36817;&#31471;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21807;&#19968;&#24615;&#20248;&#21270;&#26041;&#24335;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#26032;&#30340;&#27491;&#21017;&#21270;&#31243;&#24207;&#19981;&#20165;&#21487;&#20197;&#26356;&#22909;&#22320;&#20445;&#30041;&#25968;&#25454;&#20013;&#30340;&#31867;&#32467;&#26500;&#65292;&#36824;&#21487;&#20197;&#22312;&#25968;&#25454;&#20960;&#20309;&#24418;&#29366;&#26041;&#38754;&#25552;&#20379;&#39069;&#22806;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/1903.03850</link><description>&lt;p&gt;
&#22522;&#20110;&#31867;&#32467;&#26500;&#30340;&#26368;&#20248;&#20256;&#36755;&#24674;&#22797;&#30028;&#38480;: &#19968;&#31181;&#33539;&#25968;&#27714;&#21644;&#27491;&#21017;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Recovery Bounds on Class-Based Optimal Transport: A Sum-of-Norms Regularization Framework. (arXiv:1903.03850v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1903.03850
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33539;&#25968;&#27714;&#21644;&#27491;&#21017;&#21270;&#39033;&#30340;&#20984;&#24615;&#26368;&#20248;&#20256;&#36755;&#31243;&#24207;&#65292;&#22312;&#20960;&#20309;&#20551;&#35774;&#26465;&#20214;&#19979;&#21487;&#35777;&#26126;&#24674;&#22797;&#22522;&#30784;&#31867;&#32467;&#26500;&#12290;&#35813;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#21152;&#36895;&#30340;&#36817;&#31471;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21807;&#19968;&#24615;&#20248;&#21270;&#26041;&#24335;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#26032;&#30340;&#27491;&#21017;&#21270;&#31243;&#24207;&#19981;&#20165;&#21487;&#20197;&#26356;&#22909;&#22320;&#20445;&#30041;&#25968;&#25454;&#20013;&#30340;&#31867;&#32467;&#26500;&#65292;&#36824;&#21487;&#20197;&#22312;&#25968;&#25454;&#20960;&#20309;&#24418;&#29366;&#26041;&#38754;&#25552;&#20379;&#39069;&#22806;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#23637;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#23562;&#37325;&#31867;&#32467;&#26500;&#30340;&#26368;&#20248;&#20256;&#36755;&#26041;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#33539;&#25968;&#27714;&#21644;&#27491;&#21017;&#21270;&#39033;&#30340;&#20984;&#24615;&#26368;&#20248;&#20256;&#36755;&#31243;&#24207;&#65292;&#35813;&#31243;&#24207;&#22312;&#20960;&#20309;&#20551;&#35774;&#26465;&#20214;&#19979;&#21487;&#35777;&#26126;&#24674;&#22797;&#22522;&#30784;&#31867;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#31181;&#21152;&#36895;&#30340;&#36817;&#31471;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#38381;&#24335;&#25237;&#24433;&#21644;&#36817;&#31471;&#25805;&#20316;&#31526;&#26041;&#26696;&#65292;&#20174;&#32780;&#20026;&#35745;&#31639;&#26368;&#20248;&#20256;&#36755;&#35745;&#21010;&#25552;&#20379;&#20102;&#26356;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#21807;&#19968;&#24615;&#20248;&#21270;&#26041;&#24335;&#65292;&#21363;&#20351;&#22312;&#32570;&#20047;&#24378;&#20984;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#21487;&#24471;&#21040;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20197;&#21069;&#30340;&#27491;&#21017;&#21270;&#31243;&#24207;&#30456;&#27604;&#65292;&#26032;&#30340;&#27491;&#21017;&#21270;&#31243;&#24207;&#19981;&#20165;&#21487;&#20197;&#26356;&#22909;&#22320;&#20445;&#30041;&#25968;&#25454;&#20013;&#30340;&#31867;&#32467;&#26500;&#65292;&#36824;&#21487;&#20197;&#22312;&#25968;&#25454;&#20960;&#20309;&#24418;&#29366;&#26041;&#38754;&#25552;&#20379;&#39069;&#22806;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a novel theoretical framework for understating OT schemes respecting a class structure. For this purpose, we propose a convex OT program with a sum-of-norms regularization term, which provably recovers the underlying class structure under geometric assumptions. Furthermore, we derive an accelerated proximal algorithm with a closed-form projection and proximal operator scheme, thereby affording a more scalable algorithm for computing optimal transport plans. We provide a novel argument for the uniqueness of the optimum even in the absence of strong convexity. Our experiments show that the new regularizer not only results in a better preservation of the class structure in the data but also yields additional robustness to the data geometry, compared to previous regularizers.
&lt;/p&gt;</description></item></channel></rss>