<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIRECT&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#30830;&#23450;&#31867;&#21035;&#20998;&#21106;&#38408;&#20540;&#24182;&#26631;&#35760;&#26368;&#19981;&#30830;&#23450;&#19988;&#31163;&#20854;&#26368;&#36817;&#30340;&#31034;&#20363;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#32597;&#35265;&#31867;&#21644;&#23569;&#25968;&#31867;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#25209;&#27425;&#26631;&#35760;&#21644;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#23481;&#24525;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2312.09196</link><description>&lt;p&gt;
DIRECT: &#22788;&#29702;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DIRECT: Deep Active Learning under Imbalance and Label Noise
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.09196
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIRECT&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#30830;&#23450;&#31867;&#21035;&#20998;&#21106;&#38408;&#20540;&#24182;&#26631;&#35760;&#26368;&#19981;&#30830;&#23450;&#19988;&#31163;&#20854;&#26368;&#36817;&#30340;&#31034;&#20363;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#32597;&#35265;&#31867;&#21644;&#23569;&#25968;&#31867;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#25209;&#27425;&#26631;&#35760;&#21644;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#23481;&#24525;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#31867;&#21035;&#19981;&#24179;&#34913;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#32597;&#35265;&#21644;&#23569;&#25968;&#31867;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20027;&#21160;&#23398;&#20064;&#21487;&#33021;&#26159;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26368;&#26377;&#25928;&#25216;&#26415;&#65292;&#23427;&#20174;&#26681;&#26412;&#19978;&#37319;&#38598;&#26356;&#24179;&#34913;&#21644;&#20855;&#26377;&#20449;&#24687;&#37327;&#30340;&#26631;&#35760;&#31034;&#20363;&#36827;&#34892;&#27880;&#37322;&#12290;&#26631;&#31614;&#22122;&#22768;&#26159;&#25968;&#25454;&#27880;&#37322;&#20219;&#21153;&#20013;&#21478;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#65292;&#23545;&#20110;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#35828;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#20027;&#21160;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#31283;&#20581;&#22320;&#30830;&#23450;&#31867;&#21035;&#20998;&#21106;&#38408;&#20540;&#24182;&#26631;&#35760;&#26368;&#19981;&#30830;&#23450;&#19988;&#31163;&#20854;&#26368;&#36817;&#30340;&#31034;&#20363;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#19968;&#32500;&#20027;&#21160;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;DIRECT&#33021;&#22815;&#21033;&#29992;&#32463;&#20856;&#30340;&#20027;&#21160;&#23398;&#20064;&#25991;&#29486;&#26469;&#35299;&#20915;&#25209;&#27425;&#26631;&#35760;&#21644;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#23481;&#24525;&#31561;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class imbalance is a prevalent issue in real world machine learning applications, often leading to poor performance in rare and minority classes. With an abundance of wild unlabeled data, active learning is perhaps the most effective technique in solving the problem at its root -- collecting a more balanced and informative set of labeled examples during annotation. Label noise is another common issue in data annotation jobs, which is especially challenging for active learning methods. In this work, we conduct the first study of active learning under both class imbalance and label noise. We propose a novel algorithm that robustly identifies the class separation threshold and annotates the most uncertain examples that are closest from it. Through a novel reduction to one-dimensional active learning, our algorithm DIRECT is able to leverage the classic active learning literature to address issues such as batch labeling and tolerance towards label noise. We present extensive experiments on
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#31070;&#32463;&#32593;&#32476;&#35268;&#33539;&#21270;&#26041;&#27861;CB-Norm&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26799;&#24230;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#21152;&#36895;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.16798</link><description>&lt;p&gt;
&#22522;&#20110;&#32858;&#31867;&#30340;&#31070;&#32463;&#32593;&#32476;&#35268;&#33539;&#21270;&#23618;
&lt;/p&gt;
&lt;p&gt;
Cluster-Based Normalization Layer for Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16798
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#31070;&#32463;&#32593;&#32476;&#35268;&#33539;&#21270;&#26041;&#27861;CB-Norm&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26799;&#24230;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#21152;&#36895;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#38754;&#20020;&#37325;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#20869;&#37096;&#21327;&#21464;&#37327;&#28418;&#31227;&#12289;&#26631;&#31614;&#28418;&#31227;&#12289;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#12289;&#36807;&#25311;&#21512;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#20256;&#32479;&#30340;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#22914;&#25209;&#26631;&#20934;&#21270;&#65292;&#26088;&#22312;&#35299;&#20915;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#20294;&#36890;&#24120;&#20381;&#36182;&#20110;&#38480;&#21046;&#20854;&#36866;&#24212;&#24615;&#30340;&#20551;&#35774;&#12290;&#28151;&#21512;&#35268;&#33539;&#21270;&#22312;&#22788;&#29702;&#22810;&#20010;&#39640;&#26031;&#20998;&#24067;&#26102;&#38754;&#20020;&#35745;&#31639;&#38556;&#30861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#32858;&#31867;&#30340;&#35268;&#33539;&#21270;&#65288;CB-Norm&#65289;&#30340;&#20004;&#20010;&#21464;&#20307;&#8212;&#8212;&#30417;&#30563;&#24335;&#22522;&#20110;&#32858;&#31867;&#30340;&#35268;&#33539;&#21270;&#65288;SCB-Norm&#65289;&#21644;&#26080;&#30417;&#30563;&#24335;&#22522;&#20110;&#32858;&#31867;&#30340;&#35268;&#33539;&#21270;&#65288;UCB-Norm&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#19968;&#27493;&#35268;&#33539;&#21270;&#26041;&#27861;&#12290;CB-Norm&#21033;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#26469;&#19987;&#38376;&#35299;&#20915;&#19982;&#26799;&#24230;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#21152;&#36895;&#26377;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16798v1 Announce Type: cross  Abstract: Deep learning faces significant challenges during the training of neural networks, including internal covariate shift, label shift, vanishing/exploding gradients, overfitting, and computational complexity. While conventional normalization methods, such as Batch Normalization, aim to tackle some of these issues, they often depend on assumptions that constrain their adaptability. Mixture Normalization faces computational hurdles in its pursuit of handling multiple Gaussian distributions.   This paper introduces Cluster-Based Normalization (CB-Norm) in two variants - Supervised Cluster-Based Normalization (SCB-Norm) and Unsupervised Cluster-Based Normalization (UCB-Norm) - proposing a groundbreaking one-step normalization approach. CB-Norm leverages a Gaussian mixture model to specifically address challenges related to gradient stability and learning acceleration.   For SCB-Norm, a supervised variant, the novel mechanism involves introduc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.14472</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#32534;&#36753;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#27602;&#21270;
&lt;/p&gt;
&lt;p&gt;
Detoxifying Large Language Models via Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21435;&#27602;&#21270;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;SafeEdit&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20061;&#31181;&#19981;&#23433;&#20840;&#31867;&#21035;&#65292;&#20855;&#26377;&#21508;&#31181;&#24378;&#22823;&#30340;&#25915;&#20987;&#25552;&#31034;&#65292;&#24182;&#37197;&#22791;&#20102;&#20840;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#32447;&#65292;&#32467;&#26524;&#34920;&#26126;&#30693;&#35782;&#32534;&#36753;&#26377;&#28508;&#21147;&#22312;&#23545;LLMs&#36827;&#34892;&#21435;&#27602;&#21270;&#26102;&#65292;&#22312;&#23545;&#19968;&#33324;&#24615;&#33021;&#30340;&#24433;&#21709;&#30456;&#23545;&#26377;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#20934;&#32447;&#65292;&#31216;&#20026;&#36890;&#36807;&#26415;&#20013;&#31070;&#32463;&#30417;&#27979;&#21435;&#27602;&#21270;&#65288;DINM&#65289;&#65292;&#36890;&#36807;&#20165;&#19968;&#27425;&#23454;&#20363;&#30340;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;LLMs&#30340;&#27602;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#34920;&#26126;&#20808;&#21069;&#30340;&#26041;&#27861;&#22914;SFT&#21644;DPO&#21487;&#33021;&#20165;&#25233;&#21046;&#26377;&#27602;&#21442;&#25968;&#30340;&#28608;&#27963;&#65292;&#32780;DINM&#21017;&#20943;&#36731;&#26377;&#27602;&#21442;&#25968;&#30340;&#27602;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14472v1 Announce Type: cross  Abstract: This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments to compare knowledge editing approaches with previous baselines, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to
&lt;/p&gt;</description></item><item><title>STG-Mamba &#26159;&#39318;&#20010;&#21033;&#29992;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#26102;&#31354;&#22270;&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#22270;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22359;&#65288;GS3B&#65289;&#31934;&#30830;&#34920;&#24449;STG&#30340;&#21160;&#24577;&#28436;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.12418</link><description>&lt;p&gt;
STG-Mamba: &#36890;&#36807;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#26102;&#31354;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12418
&lt;/p&gt;
&lt;p&gt;
STG-Mamba &#26159;&#39318;&#20010;&#21033;&#29992;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#26102;&#31354;&#22270;&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#22270;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22359;&#65288;GS3B&#65289;&#31934;&#30830;&#34920;&#24449;STG&#30340;&#21160;&#24577;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Spatial-Temporal Graph&#65288;STG&#65289;&#25968;&#25454;&#20855;&#26377;&#21160;&#24577;&#24615;&#12289;&#24322;&#36136;&#24615;&#21644;&#38750;&#24179;&#31283;&#24615;&#29305;&#28857;&#65292;&#23548;&#33268;&#26102;&#31354;&#22270;&#23398;&#20064;&#25345;&#32493;&#38754;&#20020;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#65292;&#20027;&#35201;&#38598;&#20013;&#20110;&#27169;&#25311;STG&#32593;&#32476;&#20013;&#33410;&#28857;&#20010;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;&#38543;&#26102;&#38388;&#23384;&#22312;&#30340;STG&#31995;&#32479;&#26412;&#36136;&#29305;&#24449;&#30340;&#24314;&#27169;&#37325;&#35201;&#24615;&#12290;&#30456;&#21453;&#65292;&#29616;&#20195;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSSMs&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#31934;&#24515;&#25506;&#32034;&#20102;STG&#31995;&#32479;&#22312;&#26102;&#38388;&#32500;&#24230;&#19978;&#30340;&#21160;&#24577;&#29366;&#24577;&#28436;&#21464;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Spatial-Temporal Graph Mamba&#65288;STG-Mamba&#65289;&#65292;&#20316;&#20026;&#39318;&#20010;&#21033;&#29992;&#24378;&#22823;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;STG&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#22270;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22359;&#65288;GS3B&#65289;&#31934;&#30830;&#34920;&#24449;STG&#30340;&#21160;&#24577;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12418v1 Announce Type: cross  Abstract: Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous, and non-stationary, leading to the continuous challenge of spatial-temporal graph learning. In the past few years, various GNN-based methods have been proposed to solely focus on mimicking the relationships among node individuals of the STG network, ignoring the significance of modeling the intrinsic features that exist in STG system over time. In contrast, modern Selective State Space Models (SSSMs) present a new approach which treat STG Network as a system, and meticulously explore the STG system's dynamic state evolution across temporal dimension. In this work, we introduce Spatial-Temporal Graph Mamba (STG-Mamba) as the first exploration of leveraging the powerful selective state space models for STG learning by treating STG Network as a system, and employing the Graph Selective State Space Block (GS3B) to precisely characterize the dynamic evolution of ST
&lt;/p&gt;</description></item><item><title>&#36816;&#29992;&#23548;&#25968;&#20449;&#24687;&#30340;&#31070;&#32463;&#31639;&#23376;&#21152;&#36895;&#20102;&#20960;&#20309;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#26174;&#33879;&#21152;&#24555;&#20102;&#35299;&#20915;&#38750;&#32447;&#24615;&#36125;&#21494;&#26031;&#21453;&#38382;&#39064;&#30340;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.08220</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#36125;&#21494;&#26031;&#21453;&#38382;&#39064;&#30340;&#39640;&#25928;&#20960;&#20309;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65306;&#21033;&#29992;&#23548;&#25968;&#20449;&#24687;&#30340;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Efficient geometric Markov chain Monte Carlo for nonlinear Bayesian inversion enabled by derivative-informed neural operators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08220
&lt;/p&gt;
&lt;p&gt;
&#36816;&#29992;&#23548;&#25968;&#20449;&#24687;&#30340;&#31070;&#32463;&#31639;&#23376;&#21152;&#36895;&#20102;&#20960;&#20309;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#26174;&#33879;&#21152;&#24555;&#20102;&#35299;&#20915;&#38750;&#32447;&#24615;&#36125;&#21494;&#26031;&#21453;&#38382;&#39064;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36816;&#31639;&#23398;&#20064;&#26041;&#27861;&#26469;&#21152;&#36895;&#20960;&#20309;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#20197;&#35299;&#20915;&#26080;&#38480;&#32500;&#38750;&#32447;&#24615;&#36125;&#21494;&#26031;&#21453;&#38382;&#39064;&#12290;&#34429;&#28982;&#20960;&#20309;MCMC&#37319;&#29992;&#36866;&#24212;&#21518;&#39564;&#23616;&#37096;&#20960;&#20309;&#30340;&#39640;&#36136;&#37327;&#25552;&#35758;&#65292;&#20294;&#22312;&#21442;&#25968;&#21040;&#21487;&#35266;&#27979;&#65288;PtO&#65289;&#26144;&#23556;&#36890;&#36807;&#26114;&#36149;&#30340;&#27169;&#22411;&#27169;&#25311;&#23450;&#20041;&#26102;&#65292;&#38656;&#35201;&#35745;&#31639;&#23545;&#25968;&#20284;&#28982;&#30340;&#23616;&#37096;&#26799;&#24230;&#21644;Hessian&#20449;&#24687;&#65292;&#36896;&#25104;&#39640;&#25104;&#26412;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#30001;PtO&#26144;&#23556;&#30340;&#31070;&#32463;&#31639;&#23376;&#26367;&#20195;&#39537;&#21160;&#30340;&#24310;&#36831;&#25509;&#21463;&#20960;&#20309;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#20854;&#20013;&#25552;&#35758;&#34987;&#35774;&#35745;&#20026;&#21033;&#29992;&#23545;&#25968;&#20284;&#28982;&#21644;&#20854;&#26799;&#24230;&#21644;Hessian&#30340;&#24555;&#36895;&#26367;&#20195;&#20272;&#35745;&#12290;&#20026;&#20102;&#23454;&#29616;&#26174;&#33879;&#21152;&#36895;&#65292;&#26367;&#20195;&#21697;&#38656;&#35201;&#20934;&#30830;&#39044;&#27979;&#21487;&#35266;&#27979;&#21450;&#20854;&#21442;&#25968;&#23548;&#25968;&#65288;&#21487;&#35266;&#27979;&#19982;&#21442;&#25968;&#20043;&#38388;&#30340;&#23548;&#25968;&#65289;&#12290;&#36890;&#36807;&#20256;&#32479;&#30340;&#26041;&#27861;&#23545;&#36825;&#26679;&#30340;&#26367;&#20195;&#21697;&#36827;&#34892;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08220v1 Announce Type: cross  Abstract: We propose an operator learning approach to accelerate geometric Markov chain Monte Carlo (MCMC) for solving infinite-dimensional nonlinear Bayesian inverse problems. While geometric MCMC employs high-quality proposals that adapt to posterior local geometry, it requires computing local gradient and Hessian information of the log-likelihood, incurring a high cost when the parameter-to-observable (PtO) map is defined through expensive model simulations. We consider a delayed-acceptance geometric MCMC method driven by a neural operator surrogate of the PtO map, where the proposal is designed to exploit fast surrogate approximations of the log-likelihood and, simultaneously, its gradient and Hessian. To achieve a substantial speedup, the surrogate needs to be accurate in predicting both the observable and its parametric derivative (the derivative of the observable with respect to the parameter). Training such a surrogate via conventional o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#21512;&#35268;&#21644;&#30417;&#30563;&#30340;AI&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;&#65292;&#24378;&#35843;&#20102;DSA&#21644;AIA&#30417;&#31649;&#26694;&#26550;&#20013;&#23384;&#22312;&#30340;&#30417;&#31649;&#31354;&#30333;&#65292;&#24182;&#35201;&#27714;AIA&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#31038;&#20250;&#20844;&#27665;&#25552;&#20379;&#25968;&#25454;&#21644;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;</title><link>https://arxiv.org/abs/2403.07904</link><description>&lt;p&gt;
&#27491;&#35270;&#30417;&#31649;&#31354;&#30333;&#65306;&#36890;&#36807;&#32435;&#20837;&#31038;&#20250;&#20844;&#27665;&#25171;&#36896;&#36229;&#36234;AIA&#30340;&#27431;&#30431;AI&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Addressing the Regulatory Gap: Moving Towards an EU AI Audit Ecosystem Beyond the AIA by Including Civil Society
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07904
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#21512;&#35268;&#21644;&#30417;&#30563;&#30340;AI&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;&#65292;&#24378;&#35843;&#20102;DSA&#21644;AIA&#30417;&#31649;&#26694;&#26550;&#20013;&#23384;&#22312;&#30340;&#30417;&#31649;&#31354;&#30333;&#65292;&#24182;&#35201;&#27714;AIA&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#31038;&#20250;&#20844;&#27665;&#25552;&#20379;&#25968;&#25454;&#21644;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27431;&#27954;&#31435;&#27861;&#26426;&#26500;&#25552;&#20986;&#20102;&#25968;&#23383;&#26381;&#21153;&#27861;&#26696;&#65288;DSA&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#65288;AIA&#65289;&#26469;&#35268;&#33539;&#24179;&#21488;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20135;&#21697;&#12290;&#26412;&#25991;&#23457;&#26597;&#20102;&#31532;&#19977;&#26041;&#23457;&#35745;&#22312;&#36825;&#20004;&#39033;&#27861;&#24459;&#20013;&#30340;&#22320;&#20301;&#20197;&#21450;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#25552;&#20379;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#35775;&#38382;&#26435;&#38480;&#12290;&#36890;&#36807;&#32771;&#34385;&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;&#20013;&#31532;&#19977;&#26041;&#23457;&#35745;&#21644;&#31532;&#19977;&#26041;&#25968;&#25454;&#35775;&#38382;&#30340;&#20215;&#20540;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#30417;&#31649;&#31354;&#30333;&#65292;&#21363;&#12298;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#12299;&#27809;&#26377;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#31038;&#20250;&#20844;&#27665;&#25552;&#20379;&#25968;&#25454;&#35775;&#38382;&#26435;&#38480;&#12290;&#25105;&#20204;&#23545;&#25991;&#29486;&#30340;&#36129;&#29486;&#21253;&#25324;&#65306;&#65288;1&#65289;&#23450;&#20041;&#20102;&#19968;&#20010;&#34701;&#21512;&#21512;&#35268;&#21644;&#30417;&#30563;&#30340;AI&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;&#12290;&#65288;2&#65289;&#24378;&#35843;&#20102;DSA&#21644;AIA&#30417;&#31649;&#26694;&#26550;&#20013;&#23384;&#22312;&#30340;&#30417;&#31649;&#31354;&#30333;&#65292;&#38459;&#30861;&#20102;AI&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;&#30340;&#24314;&#31435;&#12290;&#65288;3&#65289;&#24378;&#35843;&#30740;&#31350;&#21644;&#31038;&#20250;&#20844;&#27665;&#30340;&#31532;&#19977;&#26041;&#23457;&#35745;&#24517;&#39035;&#25104;&#20026;&#35813;&#29983;&#24577;&#31995;&#32479;&#30340;&#19968;&#37096;&#20998;&#65292;&#24182;&#35201;&#27714;AIA&#21253;&#25324;&#25968;&#25454;&#21644;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07904v1 Announce Type: cross  Abstract: The European legislature has proposed the Digital Services Act (DSA) and Artificial Intelligence Act (AIA) to regulate platforms and Artificial Intelligence (AI) products. We review to what extent third-party audits are part of both laws and to what extent access to models and data is provided. By considering the value of third-party audits and third-party data access in an audit ecosystem, we identify a regulatory gap in that the Artificial Intelligence Act does not provide access to data for researchers and civil society. Our contributions to the literature include: (1) Defining an AI audit ecosystem that incorporates compliance and oversight. (2) Highlighting a regulatory gap within the DSA and AIA regulatory framework, preventing the establishment of an AI audit ecosystem. (3) Emphasizing that third-party audits by research and civil society must be part of that ecosystem and demand that the AIA include data and model access for ce
&lt;/p&gt;</description></item><item><title>GNN&#30340;&#26680;&#24515;&#26550;&#26500;&#26377;&#20004;&#20010;&#29256;&#26412;&#65292;&#31532;&#19968;&#20010;&#29256;&#26412;&#28040;&#24687;&#20165;&#21462;&#20915;&#20110;&#28304;&#39030;&#28857;&#30340;&#29366;&#24577;&#65292;&#32780;&#31532;&#20108;&#20010;&#29256;&#26412;&#28040;&#24687;&#21462;&#20915;&#20110;&#28304;&#39030;&#28857;&#21644;&#30446;&#26631;&#39030;&#28857;&#30340;&#29366;&#24577;&#12290;</title><link>https://arxiv.org/abs/2403.06817</link><description>&lt;p&gt;
&#30446;&#26631;&#20449;&#24687;&#26356;&#26377;&#25928;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Targeted Messages More Effective?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06817
&lt;/p&gt;
&lt;p&gt;
GNN&#30340;&#26680;&#24515;&#26550;&#26500;&#26377;&#20004;&#20010;&#29256;&#26412;&#65292;&#31532;&#19968;&#20010;&#29256;&#26412;&#28040;&#24687;&#20165;&#21462;&#20915;&#20110;&#28304;&#39030;&#28857;&#30340;&#29366;&#24577;&#65292;&#32780;&#31532;&#20108;&#20010;&#29256;&#26412;&#28040;&#24687;&#21462;&#20915;&#20110;&#28304;&#39030;&#28857;&#21644;&#30446;&#26631;&#39030;&#28857;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#29992;&#20110;&#22270;&#24418;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#26412;&#36136;&#19978;&#65292;GNN&#26159;&#19968;&#20010;&#20998;&#24067;&#24335;&#30340;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#65292;&#20854;&#21463;&#21040;&#20174;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#21442;&#25968;&#30340;&#25511;&#21046;&#12290;&#23427;&#22312;&#22270;&#30340;&#39030;&#28857;&#19978;&#25805;&#20316;&#65306;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#39030;&#28857;&#22312;&#27599;&#20010;&#20256;&#20837;&#36793;&#19978;&#25509;&#25910;&#19968;&#26465;&#28040;&#24687;&#65292;&#32858;&#21512;&#36825;&#20123;&#28040;&#24687;&#65292;&#28982;&#21518;&#26681;&#25454;&#23427;&#20204;&#24403;&#21069;&#30340;&#29366;&#24577;&#21644;&#32858;&#21512;&#30340;&#28040;&#24687;&#26356;&#26032;&#23427;&#20204;&#30340;&#29366;&#24577;&#12290;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#21487;&#20197;&#29992;&#24102;&#35745;&#25968;&#30340;&#19968;&#38454;&#36923;&#36753;&#30340;&#26576;&#20123;&#29255;&#27573;&#21644;Weisfeiler-Lehman&#31639;&#27861;&#26469;&#25551;&#36848;&#12290;GNN&#30340;&#26680;&#24515;&#26550;&#26500;&#26377;&#20004;&#20010;&#19981;&#21516;&#30340;&#29256;&#26412;&#12290;&#22312;&#31532;&#19968;&#20010;&#29256;&#26412;&#20013;&#65292;&#28040;&#24687;&#20165;&#21462;&#20915;&#20110;&#28304;&#39030;&#28857;&#30340;&#29366;&#24577;&#65292;&#32780;&#22312;&#31532;&#20108;&#20010;&#29256;&#26412;&#20013;&#65292;&#28040;&#24687;&#21462;&#20915;&#20110;&#28304;&#39030;&#28857;&#21644;&#30446;&#26631;&#39030;&#28857;&#30340;&#29366;&#24577;&#12290;&#23454;&#38469;&#19978;&#65292;&#36825;&#20004;&#20010;&#29256;&#26412;&#37117;&#34987;&#20351;&#29992;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;GNN&#30340;&#29702;&#35770;&#22823;&#22810;&#38598;&#20013;&#22312;&#31532;&#19968;&#20010;&#29256;&#26412;&#19978;&#12290;&#22312;&#36923;&#36753;&#26041;&#38754;&#65292;&#36825;&#20004;&#20010;&#29256;&#26412;&#23545;&#24212;&#30528;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06817v1 Announce Type: cross  Abstract: Graph neural networks (GNN) are deep learning architectures for graphs. Essentially, a GNN is a distributed message passing algorithm, which is controlled by parameters learned from data. It operates on the vertices of a graph: in each iteration, vertices receive a message on each incoming edge, aggregate these messages, and then update their state based on their current state and the aggregated messages. The expressivity of GNNs can be characterised in terms of certain fragments of first-order logic with counting and the Weisfeiler-Lehman algorithm.   The core GNN architecture comes in two different versions. In the first version, a message only depends on the state of the source vertex, whereas in the second version it depends on the states of the source and target vertices. In practice, both of these versions are used, but the theory of GNNs so far mostly focused on the first one. On the logical side, the two versions correspond to 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34394;&#20551;&#38451;&#24615;&#37319;&#26679;&#30340;&#26032;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#20351;&#29992;&#34987;&#35782;&#21035;&#20026;&#34394;&#20551;&#38451;&#24615;&#30340;&#28857;&#20113;&#65292;&#20197;&#25552;&#39640;3D&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.02639</link><description>&lt;p&gt;
&#22522;&#20110;&#34394;&#20551;&#38451;&#24615;&#37319;&#26679;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#25552;&#39640;3D&#29289;&#20307;&#26816;&#27979;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
False Positive Sampling-based Data Augmentation for Enhanced 3D Object Detection Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34394;&#20551;&#38451;&#24615;&#37319;&#26679;&#30340;&#26032;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#20351;&#29992;&#34987;&#35782;&#21035;&#20026;&#34394;&#20551;&#38451;&#24615;&#30340;&#28857;&#20113;&#65292;&#20197;&#25552;&#39640;3D&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#25552;&#21319;3D&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#19978;&#12290;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#37319;&#26679;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#22686;&#24378;&#25216;&#26415;&#26469;&#35299;&#20915;&#26377;&#38480;&#30340;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#37319;&#26679;&#30340;&#19968;&#20010;&#22266;&#26377;&#38382;&#39064;&#26159;&#20854;&#20542;&#21521;&#22686;&#21152;&#34394;&#20551;&#38451;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20811;&#26381;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#37319;&#26679;&#30340;&#23616;&#38480;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#21517;&#20026;&#34394;&#20551;&#38451;&#24615;&#37319;&#26679;&#30340;&#26032;&#22686;&#24378;&#25216;&#26415;&#26469;&#25552;&#39640;3D&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#34394;&#20551;&#38451;&#24615;&#37319;&#26679;&#28041;&#21450;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#29992;&#22312;&#27169;&#22411;&#39044;&#27979;&#20013;&#34987;&#35782;&#21035;&#20026;&#34394;&#20551;&#38451;&#24615;&#30340;&#28857;&#20113;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21516;&#26102;&#21033;&#29992;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#21644;&#34394;&#20551;&#38451;&#24615;&#37319;&#26679;&#30340;&#31639;&#27861;&#65292;&#20197;&#21450;&#19968;&#20010;&#24314;&#31435;&#34394;&#20551;&#38451;&#24615;&#26679;&#26412;&#25968;&#25454;&#24211;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#30001;&#20110;&#34394;&#20551;&#38451;&#24615;&#37319;&#26679;&#23548;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#32972;&#21518;&#30340;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02639v1 Announce Type: cross  Abstract: Recent studies have focused on enhancing the performance of 3D object detection models. Among various approaches, ground-truth sampling has been proposed as an augmentation technique to address the challenges posed by limited ground-truth data. However, an inherent issue with ground-truth sampling is its tendency to increase false positives. Therefore, this study aims to overcome the limitations of ground-truth sampling and improve the performance of 3D object detection models by developing a new augmentation technique called false-positive sampling. False-positive sampling involves retraining the model using point clouds that are identified as false positives in the model's predictions. We propose an algorithm that utilizes both ground-truth and false-positive sampling and an algorithm for building the false-positive sample database. Additionally, we analyze the principles behind the performance enhancement due to false-positive sampl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#27599;&#38598;&#20351;&#29992;&#20984;&#20248;&#21270;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#26368;&#20248;$Q$-&#20989;&#25968;&#65292;&#30830;&#20445;&#25910;&#25947;&#21442;&#25968;&#21487;&#20197;&#26080;&#38480;&#25509;&#36817;&#26368;&#20248;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.19212</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#20010;&#20984;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning: A Convex Optimization Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#27599;&#38598;&#20351;&#29992;&#20984;&#20248;&#21270;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#26368;&#20248;$Q$-&#20989;&#25968;&#65292;&#30830;&#20445;&#25910;&#25947;&#21442;&#25968;&#21487;&#20197;&#26080;&#38480;&#25509;&#36817;&#26368;&#20248;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#38598;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#22312;&#27599;&#20010;&#38598;&#20013;&#20351;&#29992;&#20984;&#20248;&#21270;&#26469;&#25214;&#21040;&#26368;&#20248;$Q$-&#20989;&#25968;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#12290;&#20984;&#20248;&#21270;&#26041;&#27861;&#30830;&#20445;&#27599;&#20010;&#38598;&#21512;&#20013;&#35745;&#31639;&#30340;&#26435;&#37325;&#26159;&#26368;&#20248;&#30340;&#65292;&#20851;&#20110;&#24403;&#21069;&#38598;&#21512;&#30340;&#37319;&#26679;&#29366;&#24577;&#21644;&#21160;&#20316;&#12290;&#23545;&#20110;&#31283;&#23450;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#25910;&#25947;&#65292;&#24182;&#19988;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#25947;&#21442;&#25968;&#21487;&#20197;&#19982;&#26368;&#20248;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#26080;&#38480;&#25509;&#36817;&#12290;&#29305;&#21035;&#26159;&#65292;&#22914;&#26524;&#27491;&#21017;&#21270;&#21442;&#25968;&#20026;$\rho$&#65292;&#26102;&#38388;&#38271;&#24230;&#20026;$T$&#65292;&#37027;&#20040;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#25910;&#25947;&#21040;$w$&#65292;&#20854;&#20013;$w$&#19982;&#26368;&#20248;&#21442;&#25968;$w^\star$&#20043;&#38388;&#30340;&#36317;&#31163;&#21463;&#21040;$\mathcal{O}(\rho T^{-1})$&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19212v1 Announce Type: cross  Abstract: In this paper, we consider reinforcement learning of nonlinear systems with continuous state and action spaces. We present an episodic learning algorithm, where we for each episode use convex optimization to find a two-layer neural network approximation of the optimal $Q$-function. The convex optimization approach guarantees that the weights calculated at each episode are optimal, with respect to the given sampled states and actions of the current episode. For stable nonlinear systems, we show that the algorithm converges and that the converging parameters of the trained neural network can be made arbitrarily close to the optimal neural network parameters. In particular, if the regularization parameter is $\rho$ and the time horizon is $T$, then the parameters of the trained neural network converge to $w$, where the distance between $w$ from the optimal parameters $w^\star$ is bounded by $\mathcal{O}(\rho T^{-1})$. That is, when the nu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RAGFormer&#30340;&#26032;&#26694;&#26550;&#65292;&#21516;&#26102;&#23558;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#23884;&#20837;&#30446;&#26631;&#33410;&#28857;&#65292;&#20197;&#25913;&#36827;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17472</link><description>&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#20840;&#23616;&#21644;&#23616;&#37096;&#20851;&#31995;&#20132;&#20114;&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fraud Detection with Binding Global and Local Relational Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17472
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RAGFormer&#30340;&#26032;&#26694;&#26550;&#65292;&#21516;&#26102;&#23558;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#23884;&#20837;&#30446;&#26631;&#33410;&#28857;&#65292;&#20197;&#25913;&#36827;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#27450;&#35784;&#26816;&#27979;&#20855;&#26377;&#26377;&#25928;&#24615;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#22312;&#25972;&#20307;&#35270;&#35282;&#20013;&#32534;&#30721;&#33410;&#28857;&#20132;&#20114;&#21644;&#32858;&#21512;&#29305;&#24449;&#12290;&#26368;&#36817;&#65292;&#20855;&#26377;&#20986;&#33394;&#24207;&#21015;&#32534;&#30721;&#33021;&#21147;&#30340;Transformer&#32593;&#32476;&#22312;&#25991;&#29486;&#20013;&#20063;&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;GNN&#21644;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#21482;&#32534;&#30721;&#25972;&#20010;&#22270;&#30340;&#19968;&#20010;&#35270;&#35282;&#65292;&#32780;GNN&#32534;&#30721;&#20840;&#23616;&#29305;&#24449;&#65292;Transformer&#32593;&#32476;&#32534;&#30721;&#23616;&#37096;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#24573;&#35270;&#20102;&#20351;&#29992;&#21333;&#29420;&#32593;&#32476;&#32534;&#30721;&#24322;&#26500;&#22270;&#30340;&#20840;&#23616;&#20132;&#20114;&#29305;&#24449;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Relation-Aware GNN with transFormer&#65288;RAGFormer&#65289;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#23558;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#21516;&#26102;&#23884;&#20837;&#30446;&#26631;&#33410;&#28857;&#20013;&#12290;&#36825;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#32593;&#32476;&#24212;&#29992;&#20102;&#19968;&#20010;&#20462;&#25913;&#21518;&#30340;GAGA&#27169;&#22359;&#65292;&#20854;&#20013;&#27599;&#20010;Transformer&#23618;&#21518;&#38754;&#37117;&#36319;&#30528;&#19968;&#20010;&#36328;&#20851;&#31995;&#32858;&#21512;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17472v1 Announce Type: cross  Abstract: Graph Neural Network has been proved to be effective for fraud detection for its capability to encode node interaction and aggregate features in a holistic view. Recently, Transformer network with great sequence encoding ability, has also outperformed other GNN-based methods in literatures. However, both GNN-based and Transformer-based networks only encode one perspective of the whole graph, while GNN encodes global features and Transformer network encodes local ones. Furthermore, previous works ignored encoding global interaction features of the heterogeneous graph with separate networks, thus leading to suboptimal performance. In this work, we present a novel framework called Relation-Aware GNN with transFormer (RAGFormer) which simultaneously embeds local and global features into a target node. The simple yet effective network applies a modified GAGA module where each transformer layer is followed by a cross-relation aggregation lay
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;HiGPT&#27169;&#22411;&#65292;&#33268;&#21147;&#20110;&#35299;&#20915;&#24322;&#36136;&#22270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#27867;&#21270;&#38480;&#21046;&#21644;&#20998;&#24067;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16024</link><description>&lt;p&gt;
HiGPT&#65306;&#24322;&#36136;&#22270;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
HiGPT: Heterogeneous Graph Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16024
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;HiGPT&#27169;&#22411;&#65292;&#33268;&#21147;&#20110;&#35299;&#20915;&#24322;&#36136;&#22270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#27867;&#21270;&#38480;&#21046;&#21644;&#20998;&#24067;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#23398;&#20064;&#26088;&#22312;&#25429;&#25417;&#24322;&#26500;&#22270;&#20013;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#21644;&#22810;&#26679;&#21270;&#20851;&#31995;&#35821;&#20041;&#65292;&#20197;&#33719;&#24471;&#33410;&#28857;&#21644;&#36793;&#30340;&#26377;&#24847;&#20041;&#34920;&#31034;&#12290;&#26368;&#36817;&#22312;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNNs&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#32771;&#34385;&#20851;&#31995;&#30340;&#24322;&#36136;&#24615;&#24182;&#20351;&#29992;&#19987;&#38376;&#30340;&#28040;&#24687;&#20989;&#25968;&#21644;&#32858;&#21512;&#35268;&#21017;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26694;&#26550;&#22312;&#27867;&#21270;&#21040;&#19981;&#21516;&#30340;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#12290;&#22823;&#22810;&#25968;&#36825;&#20123;&#26694;&#26550;&#37117;&#36981;&#24490;&#21516;&#19968;&#25968;&#25454;&#38598;&#19978;&#30340;&#8220;&#39044;&#35757;&#32451;&#8221;&#21644;&#8220;&#24494;&#35843;&#8221;&#33539;&#24335;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#36866;&#24212;&#26032;&#30340;&#21644;&#30475;&#19981;&#35265;&#30340;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#8220;&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#23558;&#24322;&#36136;&#22270;&#27169;&#22411;&#27867;&#21270;&#20026;&#36866;&#24212;&#20855;&#26377;&#33410;&#28857;&#20196;&#29260;&#38598;&#21644;&#20851;&#31995;&#31867;&#22411;&#24322;&#36136;&#24615;&#20998;&#24067;&#21464;&#21270;&#30340;&#19981;&#21516;&#19979;&#28216;&#23398;&#20064;&#20219;&#21153;&#65311;&#8221;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;p
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16024v1 Announce Type: new  Abstract: Heterogeneous graph learning aims to capture complex relationships and diverse relational semantics among entities in a heterogeneous graph to obtain meaningful representations for nodes and edges. Recent advancements in heterogeneous graph neural networks (HGNNs) have achieved state-of-the-art performance by considering relation heterogeneity and using specialized message functions and aggregation rules. However, existing frameworks for heterogeneous graph learning have limitations in generalizing across diverse heterogeneous graph datasets. Most of these frameworks follow the "pre-train" and "fine-tune" paradigm on the same dataset, which restricts their capacity to adapt to new and unseen data. This raises the question: "Can we generalize heterogeneous graph models to be well-adapted to diverse downstream learning tasks with distribution shifts in both node token sets and relation type heterogeneity?'' To tackle those challenges, we p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#19982;&#20844;&#24179;&#24615;&#32422;&#26463;&#19979;&#30340;&#20108;&#20803;&#20998;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#25216;&#26415;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#24341;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#20445;&#35777;&#20844;&#24179;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#20102;&#38544;&#31169;&#24615;&#33021;&#21644;&#25928;&#29992;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.15603</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#20844;&#24179;&#20108;&#20803;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Fair Binary Classifications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15603
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#19982;&#20844;&#24179;&#24615;&#32422;&#26463;&#19979;&#30340;&#20108;&#20803;&#20998;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#25216;&#26415;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#24341;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#20445;&#35777;&#20844;&#24179;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#20102;&#38544;&#31169;&#24615;&#33021;&#21644;&#25928;&#29992;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#32422;&#26463;&#19979;&#30340;&#20108;&#20803;&#20998;&#31867;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#32806;&#25216;&#26415;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#19968;&#20010;&#20165;&#20855;&#26377;&#20844;&#24179;&#24615;&#20445;&#35777;&#30340;&#20998;&#31867;&#22120;&#12290;&#35813;&#31639;&#27861;&#25509;&#21463;&#38024;&#23545;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#28385;&#36275;&#32479;&#35745;&#24179;&#34913;&#30340;&#21333;&#19968;&#20998;&#31867;&#22120;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#35813;&#31639;&#27861;&#20197;&#32435;&#20837;&#24046;&#20998;&#38544;&#31169;&#12290;&#26368;&#32456;&#31639;&#27861;&#30340;&#24615;&#33021;&#22312;&#38544;&#31169;&#12289;&#20844;&#24179;&#24615;&#21644;&#25928;&#29992;&#20445;&#35777;&#26041;&#38754;&#24471;&#21040;&#20102;&#20005;&#26684;&#26816;&#39564;&#12290;&#23545;Adult&#21644;&#20449;&#29992;&#21345;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#20844;&#24179;&#24615;&#20445;&#35777;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#30456;&#21516;&#27700;&#24179;&#30340;&#38544;&#31169;&#21644;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15603v1 Announce Type: new  Abstract: In this work, we investigate binary classification under the constraints of both differential privacy and fairness. We first propose an algorithm based on the decoupling technique for learning a classifier with only fairness guarantee. This algorithm takes in classifiers trained on different demographic groups and generates a single classifier satisfying statistical parity. We then refine this algorithm to incorporate differential privacy. The performance of the final algorithm is rigorously examined in terms of privacy, fairness, and utility guarantees. Empirical evaluations conducted on the Adult and Credit Card datasets illustrate that our algorithm outperforms the state-of-the-art in terms of fairness guarantees, while maintaining the same level of privacy and utility.
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#20154;&#24212;&#35813;&#36890;&#36807;&#20272;&#35745;&#25216;&#33021;&#30340;&#33021;&#21147;&#65292;&#25512;&#26029;&#36890;&#36807;&#32451;&#20064;&#33021;&#21147;&#30340;&#25552;&#21319;&#65292;&#24182;&#23558;&#20854;&#25918;&#32622;&#22312;&#20219;&#21153;&#20998;&#37197;&#20013;&#65292;&#20197;&#36873;&#25321;&#35201;&#32451;&#20064;&#30340;&#25216;&#33021;&#26469;&#26368;&#22823;&#21270;&#26410;&#26469;&#20219;&#21153;&#25104;&#21151;&#30340;&#39044;&#26399;&#12290;</title><link>https://arxiv.org/abs/2402.15025</link><description>&lt;p&gt;
&#32451;&#20064;&#26041;&#33021;&#33268;&#23436;&#32654;&#65306;&#35268;&#21010;&#23398;&#20064;&#25216;&#33021;&#21442;&#25968;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Practice Makes Perfect: Planning to Learn Skill Parameter Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15025
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#24212;&#35813;&#36890;&#36807;&#20272;&#35745;&#25216;&#33021;&#30340;&#33021;&#21147;&#65292;&#25512;&#26029;&#36890;&#36807;&#32451;&#20064;&#33021;&#21147;&#30340;&#25552;&#21319;&#65292;&#24182;&#23558;&#20854;&#25918;&#32622;&#22312;&#20219;&#21153;&#20998;&#37197;&#20013;&#65292;&#20197;&#36873;&#25321;&#35201;&#32451;&#20064;&#30340;&#25216;&#33021;&#26469;&#26368;&#22823;&#21270;&#26410;&#26469;&#20219;&#21153;&#25104;&#21151;&#30340;&#39044;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#20915;&#31574;&#26041;&#27861;&#26159;&#23558;&#21442;&#25968;&#21270;&#25216;&#33021;&#24207;&#21015;&#36215;&#26469;&#65292;&#32771;&#34385;&#21040;&#26426;&#22120;&#20154;&#26368;&#21021;&#20250;&#37197;&#22791;&#19968;&#31995;&#21015;&#21442;&#25968;&#21270;&#25216;&#33021;&#24211;&#12289;&#19968;&#20010;AI&#35268;&#21010;&#22120;&#29992;&#20110;&#26681;&#25454;&#30446;&#26631;&#23545;&#25216;&#33021;&#36827;&#34892;&#25490;&#24207;&#65292;&#24182;&#19988;&#20855;&#26377;&#29992;&#20110;&#36873;&#25321;&#25216;&#33021;&#21442;&#25968;&#30340;&#38750;&#24120;&#26222;&#36941;&#30340;&#20808;&#39564;&#20998;&#24067;&#12290;&#19968;&#26086;&#37096;&#32626;&#65292;&#26426;&#22120;&#20154;&#24212;&#35813;&#36890;&#36807;&#23558;&#20854;&#25216;&#33021;&#21442;&#25968;&#36873;&#25321;&#31574;&#30053;&#19987;&#38376;&#21270;&#21040;&#20854;&#29615;&#22659;&#20013;&#30340;&#29305;&#23450;&#23545;&#35937;&#12289;&#30446;&#26631;&#21644;&#32422;&#26463;&#65292;&#26469;&#36805;&#36895;&#33258;&#20027;&#22320;&#23398;&#20064;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#65292;&#21363;&#36873;&#25321;&#35201;&#32451;&#20064;&#30340;&#25216;&#33021;&#20197;&#26368;&#22823;&#21270;&#26410;&#26469;&#20219;&#21153;&#25104;&#21151;&#30340;&#39044;&#26399;&#12290;&#25105;&#20204;&#25552;&#20986;&#26426;&#22120;&#20154;&#24212;&#35813;&#20272;&#35745;&#27599;&#20010;&#25216;&#33021;&#30340;&#33021;&#21147;&#65292;&#25512;&#26029;&#33021;&#21147;&#65288;&#21363;&#8220;&#36890;&#36807;&#32451;&#20064;&#33021;&#21147;&#20250;&#25552;&#21319;&#22810;&#23569;&#65311;&#8221;&#65289;&#65292;&#24182;&#23558;&#35813;&#25216;&#33021;&#32622;&#20110;&#20219;&#21153;&#20998;&#37197;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15025v1 Announce Type: cross  Abstract: One promising approach towards effective robot decision making in complex, long-horizon tasks is to sequence together parameterized skills. We consider a setting where a robot is initially equipped with (1) a library of parameterized skills, (2) an AI planner for sequencing together the skills given a goal, and (3) a very general prior distribution for selecting skill parameters. Once deployed, the robot should rapidly and autonomously learn to improve its performance by specializing its skill parameter selection policy to the particular objects, goals, and constraints in its environment. In this work, we focus on the active learning problem of choosing which skills to practice to maximize expected future task success. We propose that the robot should estimate the competence of each skill, extrapolate the competence (asking: "how much would the competence improve through practice?"), and situate the skill in the task distribution throu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#24046;&#26657;&#27491;&#26041;&#27861;&#65292;&#23558;&#26657;&#20934;&#35270;&#20026;&#27010;&#29575;&#27169;&#22411;&#32780;&#19981;&#26159;&#31639;&#27861;&#27969;&#31243;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27010;&#29575;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#36866;&#37197;&#20559;&#24046;&#26657;&#27491;&#20219;&#21153;&#65292;&#21487;&#20934;&#30830;&#26657;&#27491;&#20855;&#26377;&#38271;&#26399;&#26102;&#38388;&#23646;&#24615;&#30340;&#27668;&#20505;&#32479;&#35745;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#36827;&#34892;&#21487;&#38752;&#24433;&#21709;&#30740;&#31350;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14169</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27880;&#24847;&#21147;&#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#20559;&#24046;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
A Temporal Bias Correction using a Machine Learning Attention model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#24046;&#26657;&#27491;&#26041;&#27861;&#65292;&#23558;&#26657;&#20934;&#35270;&#20026;&#27010;&#29575;&#27169;&#22411;&#32780;&#19981;&#26159;&#31639;&#27861;&#27969;&#31243;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27010;&#29575;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#36866;&#37197;&#20559;&#24046;&#26657;&#27491;&#20219;&#21153;&#65292;&#21487;&#20934;&#30830;&#26657;&#27491;&#20855;&#26377;&#38271;&#26399;&#26102;&#38388;&#23646;&#24615;&#30340;&#27668;&#20505;&#32479;&#35745;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#36827;&#34892;&#21487;&#38752;&#24433;&#21709;&#30740;&#31350;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#27169;&#22411;&#22312;&#19982;&#30495;&#23454;&#19990;&#30028;&#35266;&#27979;&#25968;&#25454;&#30456;&#27604;&#23384;&#22312;&#20559;&#24046;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#24433;&#21709;&#30740;&#31350;&#20043;&#21069;&#36827;&#34892;&#26657;&#20934;&#12290;&#20351;&#26657;&#20934;&#25104;&#20026;&#21487;&#33021;&#30340;&#32479;&#35745;&#26041;&#27861;&#38598;&#21512;&#34987;&#31216;&#20026;&#20559;&#24046;&#26657;&#27491;&#65288;BC&#65289;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;BC&#26041;&#27861;&#22312;&#35843;&#25972;&#26102;&#38388;&#20559;&#24046;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#24573;&#30053;&#20102;&#36830;&#32493;&#26102;&#38388;&#28857;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#20855;&#26377;&#38271;&#26399;&#26102;&#38388;&#23646;&#24615;&#30340;&#27668;&#20505;&#32479;&#35745;&#25968;&#25454;&#65288;&#22914;&#28909;&#28010;&#25345;&#32493;&#26102;&#38388;&#21644;&#39057;&#29575;&#65289;&#26080;&#27861;&#20934;&#30830;&#26657;&#27491;&#65292;&#36825;&#20351;&#24471;&#22312;&#36825;&#20123;&#27668;&#20505;&#32479;&#35745;&#25968;&#25454;&#19978;&#36827;&#34892;&#21487;&#38752;&#24433;&#21709;&#30740;&#31350;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;BC&#26041;&#27861;&#26469;&#26657;&#27491;&#26102;&#38388;&#20559;&#24046;&#12290;&#36825;&#24471;&#30410;&#20110;&#23558;BC&#37325;&#26032;&#26500;&#24819;&#20026;&#27010;&#29575;&#27169;&#22411;&#32780;&#19981;&#26159;&#31639;&#27861;&#27969;&#31243;&#65292;&#24182;&#23558;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27010;&#29575;&#20851;&#27880;&#27169;&#22411;&#35843;&#25972;&#21040;BC&#20219;&#21153;&#20013;&#12290;&#36890;&#36807;&#23612;&#26085;&#21033;&#20122;&#38463;&#24067;&#36158;&#30340;&#28909;&#28010;&#25345;&#32493;&#26102;&#38388;&#32479;&#35745;&#26696;&#20363;&#30740;&#31350;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14169v1 Announce Type: new  Abstract: Climate models are biased with respect to real world observations and usually need to be calibrated prior to impact studies. The suite of statistical methods that enable such calibrations is called bias correction (BC). However, current BC methods struggle to adjust for temporal biases, because they disregard the dependence between consecutive time-points. As a result, climate statistics with long-range temporal properties, such as heatwave duration and frequency, cannot be corrected accurately, making it more difficult to produce reliable impact studies on such climate statistics. In this paper, we offer a novel BC methodology to correct for temporal biases. This is made possible by i) re-thinking BC as a probability model rather than an algorithmic procedure, and ii) adapting state-of-the-art machine-learning (ML) probabilistic attention models to fit the BC task. With a case study of heatwave duration statistics in Abuja, Nigeria, and
&lt;/p&gt;</description></item><item><title>E2USD&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;&#36827;&#34892;&#32534;&#30721;&#65292;&#20197;&#21450;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#28040;&#38500;&#20551;&#38452;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;SOTA&#20934;&#30830;&#24615;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2402.14041</link><description>&lt;p&gt;
E2USD&#65306;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#29366;&#24577;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
E2USD: Efficient-yet-effective Unsupervised State Detection for Multivariate Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14041
&lt;/p&gt;
&lt;p&gt;
E2USD&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;&#36827;&#34892;&#32534;&#30721;&#65292;&#20197;&#21450;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#28040;&#38500;&#20551;&#38452;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;SOTA&#20934;&#30830;&#24615;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;E2USD&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#32780;&#20934;&#30830;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#12290;E2USD&#21033;&#29992;&#22522;&#20110;&#24555;&#36895;&#20613;&#31435;&#21494;&#21464;&#25442;&#30340;&#26102;&#38388;&#24207;&#21015;&#21387;&#32553;&#22120;(FFTCompress)&#21644;&#20998;&#35299;&#30340;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;(DDEM)&#65292;&#19968;&#36215;&#20197;&#20302;&#35745;&#31639;&#24320;&#38144;&#23545;&#36755;&#20837;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#38452;&#24615;&#21462;&#28040;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;(FNCCLearning)&#65292;&#20197;&#25269;&#28040;&#20551;&#38452;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#23454;&#29616;&#26356;&#21451;&#22909;&#30340;&#31751;&#23884;&#20837;&#31354;&#38388;&#12290;&#20026;&#20102;&#22312;&#27969;&#24335;&#35774;&#32622;&#20013;&#36827;&#19968;&#27493;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#38408;&#20540;&#26816;&#27979;(ADATD)&#12290;&#36890;&#36807;&#20351;&#29992;&#20845;&#20010;&#22522;&#32447;&#27169;&#22411;&#21644;&#20845;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;E2USD&#33021;&#22815;&#22312;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;SOTA&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/AI4CTS/E2Usd &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14041v1 Announce Type: cross  Abstract: We propose E2USD that enables efficient-yet-accurate unsupervised MTS state detection. E2USD exploits a Fast Fourier Transform-based Time Series Compressor (FFTCompress) and a Decomposed Dual-view Embedding Module (DDEM) that together encode input MTSs at low computational overhead. Additionally, we propose a False Negative Cancellation Contrastive Learning method (FNCCLearning) to counteract the effects of false negatives and to achieve more cluster-friendly embedding spaces. To reduce computational overhead further in streaming settings, we introduce Adaptive Threshold Detection (ADATD). Comprehensive experiments with six baselines and six datasets offer evidence that E2USD is capable of SOTA accuracy at significantly reduced computational overhead. Our code is available at https://github.com/AI4CTS/E2Usd.
&lt;/p&gt;</description></item><item><title>UniST&#26159;&#19968;&#31181;&#20026;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#35774;&#35745;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#28789;&#27963;&#24615;&#12289;&#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20197;&#21450;&#20016;&#23500;&#30340;&#25513;&#30721;&#31574;&#30053;&#25104;&#21151;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.11838</link><description>&lt;p&gt;
UniST&#65306;&#19968;&#31181;&#20026;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#35774;&#35745;&#30340;&#25552;&#31034;&#22686;&#24378;&#22411;&#36890;&#29992;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11838
&lt;/p&gt;
&lt;p&gt;
UniST&#26159;&#19968;&#31181;&#20026;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#35774;&#35745;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#28789;&#27963;&#24615;&#12289;&#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20197;&#21450;&#20016;&#23500;&#30340;&#25513;&#30721;&#31574;&#30053;&#25104;&#21151;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11838v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#23545;&#20110;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#20132;&#36890;&#31649;&#29702;&#12289;&#36164;&#28304;&#20248;&#21270;&#21644;&#22478;&#24066;&#35268;&#21010;&#12290;&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#31361;&#30772;&#65292;&#20854;&#20013;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#36328;&#22810;&#20010;&#39046;&#22495;&#30340;&#22810;&#20010;&#20219;&#21153;&#65292;&#20294;&#22478;&#24066;&#26102;&#31354;&#24314;&#27169;&#33853;&#21518;&#12290;&#29616;&#26377;&#30340;&#22478;&#24066;&#39044;&#27979;&#26041;&#27861;&#36890;&#24120;&#38024;&#23545;&#29305;&#23450;&#30340;&#26102;&#31354;&#22330;&#26223;&#36827;&#34892;&#23450;&#21046;&#65292;&#38656;&#35201;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#22823;&#37327;&#22495;&#20869;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#30340;&#36890;&#29992;&#27169;&#22411;UniST&#12290;&#20511;&#37492;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;UniST&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#21462;&#24471;&#25104;&#21151;&#65306;(i) &#23545;&#19981;&#21516;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#29305;&#24449;&#30340;&#28789;&#27963;&#24615;&#65292;(ii) &#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#65292;&#37319;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#25513;&#30721;&#31574;&#30053;&#26469;&#25429;&#25417;&#22797;&#26434;&#30340;&#31354;&#38388;&#26102;&#38388;&#20851;&#31995;&#65292;(iii) &#26102;&#31354;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11838v1 Announce Type: new  Abstract: Urban spatio-temporal prediction is crucial for informed decision-making, such as transportation management, resource optimization, and urban planning. Although pretrained foundation models for natural languages have experienced remarkable breakthroughs, wherein one general-purpose model can tackle multiple tasks across various domains, urban spatio-temporal modeling lags behind. Existing approaches for urban prediction are usually tailored for specific spatio-temporal scenarios, requiring task-specific model designs and extensive in-domain training data. In this work, we propose a universal model, UniST, for urban spatio-temporal prediction. Drawing inspiration from large language models, UniST achieves success through: (i) flexibility towards diverse spatio-temporal data characteristics, (ii) effective generative pre-training with elaborated masking strategies to capture complex spatio-temporal relationships, (iii) spatio-temporal know
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Smart Parallel Auto-Correct Decoding (SPACE)&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21322;&#33258;&#22238;&#24402;&#25512;&#29702;&#21644;&#29468;&#27979;&#35299;&#30721;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#21152;&#36895;&#21644;&#24182;&#34892;&#29983;&#25104;&#39564;&#35777;&#20196;&#29260;&#30340;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11809</link><description>&lt;p&gt;
&#26234;&#33021;&#24182;&#34892;&#33258;&#21160;&#32416;&#38169;&#35299;&#30721;&#65306;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11809
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Smart Parallel Auto-Correct Decoding (SPACE)&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21322;&#33258;&#22238;&#24402;&#25512;&#29702;&#21644;&#29468;&#27979;&#35299;&#30721;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#21152;&#36895;&#21644;&#24182;&#34892;&#29983;&#25104;&#39564;&#35777;&#20196;&#29260;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#21152;&#36895;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#26234;&#33021;&#24182;&#34892;&#33258;&#21160;&#32416;&#38169;&#35299;&#30721;&#8221;&#65288;SPACE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;LLMs&#30340;&#26080;&#25439;&#21152;&#36895;&#12290;&#36890;&#36807;&#38598;&#25104;&#21322;&#33258;&#22238;&#24402;&#25512;&#29702;&#21644;&#29468;&#27979;&#35299;&#30721;&#33021;&#21147;&#65292;SPACE&#29420;&#29305;&#22320;&#20351;&#33258;&#22238;&#24402;LLMs&#33021;&#22815;&#24182;&#34892;&#29983;&#25104;&#21644;&#39564;&#35777;&#20196;&#29260;&#12290;&#36825;&#26159;&#36890;&#36807;&#19987;&#38376;&#30340;&#21322;&#33258;&#22238;&#24402;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#30340;&#65292;&#35813;&#36807;&#31243;&#20351;&#29616;&#26377;LLMs&#20855;&#26377;&#21516;&#26102;&#39044;&#27979;&#22810;&#20010;&#20196;&#29260;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#19968;&#31181;&#33258;&#21160;&#32416;&#38169;&#35299;&#30721;&#31639;&#27861;&#20419;&#36827;&#20102;&#21333;&#20010;&#27169;&#22411;&#35843;&#29992;&#20869;&#20196;&#29260;&#24207;&#21015;&#30340;&#21516;&#26102;&#29983;&#25104;&#21644;&#39564;&#35777;&#12290;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;LLMs&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;SPACE&#22312;HumanEval-X&#19978;&#34920;&#29616;&#20986;2.7&#20493;&#33267;4.0&#20493;&#30340;&#25512;&#29702;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11809v1 Announce Type: cross  Abstract: This research aims to accelerate the inference speed of large language models (LLMs) with billions of parameters. We propose \textbf{S}mart \textbf{P}arallel \textbf{A}uto-\textbf{C}orrect d\textbf{E}coding (SPACE), an innovative approach designed for achieving lossless acceleration of LLMs. By integrating semi-autoregressive inference and speculative decoding capabilities, SPACE uniquely enables autoregressive LLMs to parallelize token generation and verification. This is realized through a specialized semi-autoregressive supervised fine-tuning process that equips existing LLMs with the ability to simultaneously predict multiple tokens. Additionally, an auto-correct decoding algorithm facilitates the simultaneous generation and verification of token sequences within a single model invocation. Through extensive experiments on a range of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x on HumanEval-X while maintaini
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#26032;&#26694;&#26550;ResQuNNs&#65292;&#22312;quanvolutional&#23618;&#20013;&#24341;&#20837;&#21487;&#35757;&#32451;&#24615;&#65292;&#36890;&#36807;&#27531;&#24046;&#23398;&#20064;&#30340;&#27010;&#24565;&#35299;&#20915;&#20102;&#36328;&#23618;&#26799;&#24230;&#35775;&#38382;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09146</link><description>&lt;p&gt;
ResQuNNs: &#23454;&#29616;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#26032;&#26694;&#26550;ResQuNNs&#65292;&#22312;quanvolutional&#23618;&#20013;&#24341;&#20837;&#21487;&#35757;&#32451;&#24615;&#65292;&#36890;&#36807;&#27531;&#24046;&#23398;&#20064;&#30340;&#27010;&#24565;&#35299;&#20915;&#20102;&#36328;&#23618;&#26799;&#24230;&#35775;&#38382;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;QuNNs&#65289;&#24615;&#33021;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;quanvolutional&#23618;&#24182;&#35299;&#20915;&#19982;&#20854;&#30456;&#20851;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;quanvolutional&#23618;&#34429;&#28982;&#26377;&#21161;&#20110;&#29305;&#24449;&#25552;&#21462;&#65292;&#20294;&#24448;&#24448;&#26159;&#38745;&#24577;&#30340;&#65292;&#36866;&#24212;&#24615;&#26377;&#38480;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#22312;&#36825;&#20123;&#23618;&#20869;&#37096;&#36827;&#34892;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;QuNNs&#30340;&#28789;&#27963;&#24615;&#21644;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22810;&#20010;&#21487;&#35757;&#32451;&#30340;quanvolutional&#23618;&#30340;&#24341;&#20837;&#32473;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#24102;&#26469;&#20102;&#22797;&#26434;&#24615;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#38590;&#20197;&#22312;&#36825;&#20123;&#23618;&#20043;&#38388;&#35775;&#38382;&#26799;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;Residual Quanvolutional Neural Networks (ResQuNNs)&#65292;&#21033;&#29992;&#27531;&#24046;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#22312;&#36825;&#20123;&#23618;&#20043;&#38388;&#28155;&#21152;&#36339;&#36807;&#36830;&#25509;&#20197;&#20419;&#36827;&#26799;&#24230;&#30340;&#27969;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09146v1 Announce Type: new Abstract: In this paper, we present a novel framework for enhancing the performance of Quanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional layers and addressing the critical challenges associated with them. Traditional quanvolutional layers, although beneficial for feature extraction, have largely been static, offering limited adaptability. Unlike state-of-the-art, our research overcomes this limitation by enabling training within these layers, significantly increasing the flexibility and potential of QuNNs. However, the introduction of multiple trainable quanvolutional layers induces complexities in gradient-based optimization, primarily due to the difficulty in accessing gradients across these layers. To resolve this, we propose a novel architecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging the concept of residual learning, which facilitates the flow of gradients by adding skip connections between 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#35299;&#30721;&#22120;&#22312;&#34920;&#24449;&#23481;&#37327;&#26041;&#38754;&#30340;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#20197;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#31616;&#21333;&#20462;&#27491;&#26041;&#26696;&#65292;&#32780;&#19981;&#25913;&#21464;&#20869;&#31215;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.06662</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#35299;&#30721;&#22120;&#30340;&#31526;&#21495;&#31209;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Sign Rank Limitations for Attention-Based Graph Decoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06662
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#35299;&#30721;&#22120;&#22312;&#34920;&#24449;&#23481;&#37327;&#26041;&#38754;&#30340;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#20197;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#31616;&#21333;&#20462;&#27491;&#26041;&#26696;&#65292;&#32780;&#19981;&#25913;&#21464;&#20869;&#31215;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#31215;&#22411;&#35299;&#30721;&#22120;&#26159;&#25552;&#21462;&#28508;&#22312;&#23884;&#20837;&#25968;&#25454;&#20013;&#26377;&#24847;&#20041;&#20449;&#24687;&#30340;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#26694;&#26550;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35299;&#30721;&#22120;&#22312;&#34920;&#24449;&#23481;&#37327;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#22312;&#22270;&#37325;&#24314;&#38382;&#39064;&#20013;&#23588;&#20026;&#26126;&#26174;&#12290;&#26412;&#25991;&#39318;&#27425;&#22312;&#22270;&#25968;&#25454;&#20013;&#25552;&#20379;&#20102;&#23545;&#36825;&#19968;&#26222;&#36941;&#29616;&#35937;&#30340;&#29702;&#35770;&#38416;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#30452;&#25509;&#20462;&#25913;&#26041;&#26696;&#20197;&#35299;&#20915;&#35813;&#38382;&#39064;&#65292;&#19988;&#19981;&#33073;&#31163;&#20869;&#31215;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inner product-based decoders are among the most influential frameworks used to extract meaningful data from latent embeddings. However, such decoders have shown limitations in representation capacity in numerous works within the literature, which have been particularly notable in graph reconstruction problems. In this paper, we provide the first theoretical elucidation of this pervasive phenomenon in graph data, and suggest straightforward modifications to circumvent this issue without deviating from the inner product framework.
&lt;/p&gt;</description></item><item><title>REMEDI&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#36827;&#31070;&#32463;&#29109;&#20272;&#35745;&#30340;&#26657;&#27491;&#36716;&#25442;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#21449;&#29109;&#26368;&#23567;&#21270;&#21644;&#30456;&#23545;&#29109;&#20272;&#35745;&#22522;&#27169;&#22411;&#30340;&#20559;&#24046;&#65292;&#25552;&#39640;&#20102;&#20272;&#35745;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05718</link><description>&lt;p&gt;
REMEDI: &#25913;&#36827;&#31070;&#32463;&#29109;&#20272;&#35745;&#30340;&#26657;&#27491;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
REMEDI: Corrective Transformations for Improved Neural Entropy Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05718
&lt;/p&gt;
&lt;p&gt;
REMEDI&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#36827;&#31070;&#32463;&#29109;&#20272;&#35745;&#30340;&#26657;&#27491;&#36716;&#25442;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#21449;&#29109;&#26368;&#23567;&#21270;&#21644;&#30456;&#23545;&#29109;&#20272;&#35745;&#22522;&#27169;&#22411;&#30340;&#20559;&#24046;&#65292;&#25552;&#39640;&#20102;&#20272;&#35745;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#35770;&#37327;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#25968;&#25454;&#21644;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#20351;&#24471;&#20934;&#30830;&#20272;&#35745;&#36825;&#20123;&#37327;&#30340;&#38656;&#27714;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#32500;&#24230;&#30340;&#22686;&#21152;&#65292;&#20272;&#35745;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#30456;&#23545;&#36739;&#20302;&#30340;&#32500;&#24230;&#20013;&#24050;&#32463;&#22256;&#38590;&#37325;&#37325;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;REMEDI&#65292;&#29992;&#20110;&#39640;&#25928;&#20934;&#30830;&#22320;&#20272;&#35745;&#24494;&#20998;&#29109;&#65292;&#19968;&#31181;&#22522;&#26412;&#30340;&#20449;&#24687;&#35770;&#37327;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#31616;&#21333;&#33258;&#36866;&#24212;&#22522;&#27169;&#22411;&#30340;&#20132;&#21449;&#29109;&#26368;&#23567;&#21270;&#21644;&#20854;&#30456;&#23545;&#29109;&#20174;&#25968;&#25454;&#23494;&#24230;&#20013;&#20272;&#35745;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#20272;&#35745;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#25913;&#36827;&#65292;&#21253;&#25324;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#33258;&#28982;&#25968;&#25454;&#30340;&#29109;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#37325;&#35201;&#30340;&#29702;&#35770;&#19968;&#33268;&#24615;&#32467;&#26524;&#25193;&#23637;&#21040;&#25105;&#20204;&#26041;&#27861;&#25152;&#38656;&#30340;&#26356;&#24191;&#20041;&#30340;&#35774;&#32622;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#25552;&#39640;&#29109;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information theoretic quantities play a central role in machine learning. The recent surge in the complexity of data and models has increased the demand for accurate estimation of these quantities. However, as the dimension grows the estimation presents significant challenges, with existing methods struggling already in relatively low dimensions. To address this issue, in this work, we introduce $\texttt{REMEDI}$ for efficient and accurate estimation of differential entropy, a fundamental information theoretic quantity. The approach combines the minimization of the cross-entropy for simple, adaptive base models and the estimation of their deviation, in terms of the relative entropy, from the data density. Our approach demonstrates improvement across a broad spectrum of estimation tasks, encompassing entropy estimation on both synthetic and natural data. Further, we extend important theoretical consistency results to a more generalized setting required by our approach. We illustrate how
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;ReLU-like&#28608;&#27963;&#20989;&#25968;&#20197;&#32463;&#39564;&#24179;&#26041;&#25439;&#22833;&#35757;&#32451;&#30340;&#21333;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#25552;&#20986;&#20102;&#31283;&#23450;&#28857;&#26465;&#20214;&#21644;&#36867;&#36920;&#31070;&#32463;&#20803;&#30340;&#23450;&#20041;&#65292;&#24182;&#23558;&#38797;&#28857;&#36867;&#36920;&#19982;&#36867;&#36920;&#31070;&#32463;&#20803;&#30340;&#21442;&#25968;&#21464;&#21270;&#32852;&#31995;&#36215;&#26469;&#12290;</title><link>https://arxiv.org/abs/2402.05626</link><description>&lt;p&gt;
&#27973;&#23618;ReLU-like&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#65306;&#31283;&#23450;&#28857;&#12289;&#38797;&#28857;&#36867;&#36920;&#21644;&#32593;&#32476;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
The Loss Landscape of Shallow ReLU-like Neural Networks: Stationary Points, Saddle Escaping, and Network Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;ReLU-like&#28608;&#27963;&#20989;&#25968;&#20197;&#32463;&#39564;&#24179;&#26041;&#25439;&#22833;&#35757;&#32451;&#30340;&#21333;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#25552;&#20986;&#20102;&#31283;&#23450;&#28857;&#26465;&#20214;&#21644;&#36867;&#36920;&#31070;&#32463;&#20803;&#30340;&#23450;&#20041;&#65292;&#24182;&#23558;&#38797;&#28857;&#36867;&#36920;&#19982;&#36867;&#36920;&#31070;&#32463;&#20803;&#30340;&#21442;&#25968;&#21464;&#21270;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;ReLU-like&#28608;&#27963;&#20989;&#25968;&#20197;&#32463;&#39564;&#24179;&#26041;&#25439;&#22833;&#35757;&#32451;&#30340;&#21333;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#12290;&#30001;&#20110;&#28608;&#27963;&#20989;&#25968;&#26159;&#19981;&#21487;&#24494;&#30340;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#22914;&#20309;&#23436;&#20840;&#25551;&#36848;&#31283;&#23450;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#38750;&#21487;&#24494;&#21644;&#21487;&#24494;&#24773;&#20917;&#30340;&#31283;&#23450;&#28857;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#26524;&#19968;&#20010;&#31283;&#23450;&#28857;&#19981;&#21253;&#21547;&#8220;&#36867;&#36920;&#31070;&#32463;&#20803;&#8221;&#65288;&#36890;&#36807;&#19968;&#38454;&#26465;&#20214;&#23450;&#20041;&#65289;&#65292;&#37027;&#20040;&#23427;&#24517;&#23450;&#26159;&#19968;&#20010;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#27492;&#22806;&#65292;&#22312;&#26631;&#37327;&#36755;&#20986;&#24773;&#20917;&#19979;&#65292;&#36867;&#36920;&#31070;&#32463;&#20803;&#30340;&#23384;&#22312;&#20445;&#35777;&#20102;&#31283;&#23450;&#28857;&#19981;&#26159;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#25551;&#36848;&#20102;&#20174;&#26080;&#31351;&#23567;&#65288;&#28040;&#22833;&#65289;&#21021;&#22987;&#21270;&#24320;&#22987;&#30340;&#27973;&#23618;ReLU-like&#32593;&#32476;&#30340;&#38797;&#28857;&#21040;&#38797;&#28857;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#30452;&#25509;&#23558;&#38797;&#28857;&#36867;&#36920;&#19982;&#36867;&#36920;&#31070;&#32463;&#20803;&#30340;&#21442;&#25968;&#21464;&#21270;&#32852;&#31995;&#36215;&#26469;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23436;&#20840;&#35752;&#35770;&#20102;&#32593;&#32476;&#23884;&#20837;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the loss landscape of one-hidden-layer neural networks with ReLU-like activation functions trained with the empirical squared loss. As the activation function is non-differentiable, it is so far unclear how to completely characterize the stationary points. We propose the conditions for stationarity that apply to both non-differentiable and differentiable cases. Additionally, we show that, if a stationary point does not contain "escape neurons", which are defined with first-order conditions, then it must be a local minimum. Moreover, for the scalar-output case, the presence of an escape neuron guarantees that the stationary point is not a local minimum. Our results refine the description of the saddle-to-saddle training process starting from infinitesimally small (vanishing) initialization for shallow ReLU-like networks, linking saddle escaping directly with the parameter changes of escape neurons. Moreover, we are also able to fully discuss how network emb
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(WCE-GNN)&#23454;&#29616;&#20102;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;WCE-GNN&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.05569</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hypergraph Node Classification With Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(WCE-GNN)&#23454;&#29616;&#20102;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;WCE-GNN&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#26159;&#29992;&#26469;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#20851;&#38190;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#25104;&#21151;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#20855;&#26377;&#25104;&#23545;&#20132;&#20114;&#30340;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#36825;&#28608;&#21457;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#20855;&#26377;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#25968;&#25454;&#30340;&#24819;&#27861;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HyperGNNs&#65289;&#30340;&#21457;&#23637;&#12290;GNNs&#21644;HyperGNNs&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#19981;&#21516;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#34987;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#19981;&#21516;&#20960;&#20309;&#25299;&#25169;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22312;&#33410;&#28857;&#20998;&#31867;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#22823;&#22810;&#25968;HyperGNNs&#21487;&#20197;&#20351;&#29992;&#24102;&#26377;&#36229;&#22270;&#30340;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;GNN&#26469;&#36817;&#20284;&#12290;&#36825;&#23548;&#33268;&#20102;WCE-GNN&#65292;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;GNN&#21644;&#19968;&#20010;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#65288;WCE&#65289;&#65292;&#29992;&#20110;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23545;&#20110;&#20061;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;WCE-GNN&#19981;&#20165;&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#65292;&#32780;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs, with hyperedges connecting more than two nodes, are key for modelling higher-order interactions in real-world data. The success of graph neural networks (GNNs) reveals the capability of neural networks to process data with pairwise interactions. This inspires the usage of neural networks for data with higher-order interactions, thereby leading to the development of hypergraph neural networks (HyperGNNs). GNNs and HyperGNNs are typically considered distinct since they are designed for data on different geometric topologies. However, in this paper, we theoretically demonstrate that, in the context of node classification, most HyperGNNs can be approximated using a GNN with a weighted clique expansion of the hypergraph. This leads to WCE-GNN, a simple and efficient framework comprising a GNN and a weighted clique expansion (WCE), for hypergraph node classification. Experiments on nine real-world hypergraph node classification benchmarks showcase that WCE-GNN demonstrates not o
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RSCNet&#30340;&#21160;&#24577;CSI&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#21387;&#32553;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#26469;&#20943;&#23569;&#29289;&#32852;&#32593;&#35774;&#22791;&#21521;&#20113;&#26381;&#21153;&#22120;&#20256;&#36755;CSI&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;RSCNet&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21333;&#20803;&#21644;&#20248;&#21270;&#30340;CSI&#31383;&#21475;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#24863;&#30693;&#21644;CSI&#37325;&#24314;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23454;&#26102;&#30340;&#20113;&#22522;WiFi&#24863;&#30693;&#12290;</title><link>https://arxiv.org/abs/2402.04888</link><description>&lt;p&gt;
RSCNet&#65306;&#20113;&#22522;WiFi&#24863;&#30693;&#30340;&#21160;&#24577;CSI&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
RSCNet: Dynamic CSI Compression for Cloud-based WiFi Sensing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04888
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RSCNet&#30340;&#21160;&#24577;CSI&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#21387;&#32553;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#26469;&#20943;&#23569;&#29289;&#32852;&#32593;&#35774;&#22791;&#21521;&#20113;&#26381;&#21153;&#22120;&#20256;&#36755;CSI&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;RSCNet&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21333;&#20803;&#21644;&#20248;&#21270;&#30340;CSI&#31383;&#21475;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#24863;&#30693;&#21644;CSI&#37325;&#24314;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23454;&#26102;&#30340;&#20113;&#22522;WiFi&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
WiFi&#36830;&#25509;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#27491;&#22312;&#20174;&#32431;&#31929;&#30340;&#36890;&#20449;&#35774;&#22791;&#21457;&#23637;&#20026;&#21033;&#29992;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#25552;&#21462;&#33021;&#21147;&#30340;&#24863;&#30693;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36164;&#28304;&#26377;&#38480;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#35201;&#27714;&#23558;CSI&#20256;&#36755;&#21040;&#20113;&#26381;&#21153;&#22120;&#36827;&#34892;&#24863;&#30693;&#12290;&#23613;&#31649;&#21487;&#34892;&#65292;&#20294;&#36825;&#20250;&#23548;&#33268;&#22823;&#37327;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#26102;&#24863;&#30693;&#21644;&#21387;&#32553;&#32593;&#32476;&#65288;RSCNet&#65289;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#21387;&#32553;CSI&#26469;&#23454;&#29616;&#24863;&#30693;&#65292;&#20174;&#32780;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;RSCNet&#22312;&#30001;&#23569;&#37327;CSI&#24103;&#32452;&#25104;&#30340;CSI&#31383;&#21475;&#20043;&#38388;&#36827;&#34892;&#20248;&#21270;&#12290;&#19968;&#26086;&#20256;&#36755;&#21040;&#20113;&#26381;&#21153;&#22120;&#65292;&#23427;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21333;&#20803;&#20174;&#20808;&#21069;&#30340;&#31383;&#21475;&#20013;&#25552;&#21462;&#25968;&#25454;&#65292;&#20174;&#32780;&#22686;&#24378;&#24863;&#30693;&#20934;&#30830;&#24615;&#21644;CSI&#37325;&#24314;&#12290;RSCNet&#24039;&#22937;&#22320;&#24179;&#34913;&#20102;CSI&#21387;&#32553;&#21644;&#24863;&#30693;&#31934;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#23454;&#26102;&#20113;&#22522;WiFi&#24863;&#30693;&#65292;&#24182;&#20943;&#23569;&#20102;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
WiFi-enabled Internet-of-Things (IoT) devices are evolving from mere communication devices to sensing instruments, leveraging Channel State Information (CSI) extraction capabilities. Nevertheless, resource-constrained IoT devices and the intricacies of deep neural networks necessitate transmitting CSI to cloud servers for sensing. Although feasible, this leads to considerable communication overhead. In this context, this paper develops a novel Real-time Sensing and Compression Network (RSCNet) which enables sensing with compressed CSI; thereby reducing the communication overheads. RSCNet facilitates optimization across CSI windows composed of a few CSI frames. Once transmitted to cloud servers, it employs Long Short-Term Memory (LSTM) units to harness data from prior windows, thus bolstering both the sensing accuracy and CSI reconstruction. RSCNet adeptly balances the trade-off between CSI compression and sensing precision, thus streamlining real-time cloud-based WiFi sensing with redu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#28145;&#24230;&#22343;&#34913;&#27169;&#22411;&#21644;&#26174;&#24335;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39640;&#32500;&#39640;&#26031;&#28151;&#21512;&#25968;&#25454;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#27973;&#26174;&#24335;&#32593;&#32476;&#26469;&#23454;&#29616;&#19982;&#32473;&#23450;&#28145;&#24230;&#22343;&#34913;&#27169;&#22411;&#30456;&#21516;&#30340;&#29305;&#24449;&#20809;&#35889;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.02697</link><description>&lt;p&gt;
&#28145;&#24230;&#22343;&#34913;&#27169;&#22411;&#19982;&#39640;&#32500;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20013;&#19981;&#22826;&#28145;&#30340;&#26174;&#24335;&#27169;&#22411;&#20960;&#20046;&#31561;&#20215;
&lt;/p&gt;
&lt;p&gt;
Deep Equilibrium Models are Almost Equivalent to Not-so-deep Explicit Models for High-dimensional Gaussian Mixtures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#28145;&#24230;&#22343;&#34913;&#27169;&#22411;&#21644;&#26174;&#24335;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39640;&#32500;&#39640;&#26031;&#28151;&#21512;&#25968;&#25454;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#27973;&#26174;&#24335;&#32593;&#32476;&#26469;&#23454;&#29616;&#19982;&#32473;&#23450;&#28145;&#24230;&#22343;&#34913;&#27169;&#22411;&#30456;&#21516;&#30340;&#29305;&#24449;&#20809;&#35889;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22343;&#34913;&#27169;&#22411;&#65288;DEQs&#65289;&#20316;&#20026;&#20856;&#22411;&#30340;&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#38544;&#24335;DEQ&#21644;&#26174;&#24335;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20043;&#38388;&#30340;&#36830;&#25509;&#21644;&#24046;&#24322;&#32570;&#20047;&#29702;&#35770;&#19978;&#30340;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#26368;&#36817;&#22312;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#23545;&#39640;&#32500;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36755;&#20837;&#25968;&#25454;&#19979;&#65292;&#38544;&#24335;DEQ&#30340;&#20849;&#36717;&#26680;&#65288;CK&#65289;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#30697;&#38453;&#30340;&#29305;&#24449;&#20809;&#35889;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#35777;&#26126;&#20102;&#36825;&#20123;&#38544;&#24335;-CKs&#21644;NTKs&#30340;&#20809;&#35889;&#34892;&#20026;&#21462;&#20915;&#20110;DEQ&#28608;&#27963;&#20989;&#25968;&#21644;&#21021;&#22987;&#26435;&#37325;&#26041;&#24046;&#65292;&#20294;&#20165;&#36890;&#36807;&#19968;&#32452;&#22235;&#20010;&#38750;&#32447;&#24615;&#26041;&#31243;&#12290;&#20316;&#20026;&#36825;&#19968;&#29702;&#35770;&#32467;&#26524;&#30340;&#30452;&#25509;&#24433;&#21709;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#31934;&#24515;&#35774;&#35745;&#19968;&#20010;&#27973;&#26174;&#24335;&#32593;&#32476;&#26469;&#20135;&#29983;&#19982;&#32473;&#23450;DEQ&#30456;&#21516;&#30340;CK&#25110;NTK&#12290;&#23613;&#31649;&#36825;&#37324;&#26159;&#38024;&#23545;&#39640;&#26031;&#28151;&#21512;&#25968;&#25454;&#25512;&#23548;&#30340;&#65292;&#32463;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
Deep equilibrium models (DEQs), as a typical implicit neural network, have demonstrated remarkable success on various tasks. There is, however, a lack of theoretical understanding of the connections and differences between implicit DEQs and explicit neural network models. In this paper, leveraging recent advances in random matrix theory (RMT), we perform an in-depth analysis on the eigenspectra of the conjugate kernel (CK) and neural tangent kernel (NTK) matrices for implicit DEQs, when the input data are drawn from a high-dimensional Gaussian mixture. We prove, in this setting, that the spectral behavior of these Implicit-CKs and NTKs depend on the DEQ activation function and initial weight variances, but only via a system of four nonlinear equations. As a direct consequence of this theoretical result, we demonstrate that a shallow explicit network can be carefully designed to produce the same CK or NTK as a given DEQ. Despite derived here for Gaussian mixture data, empirical results 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#40065;&#26834;Q&#23398;&#20064;&#30340;&#26368;&#20248;&#21270;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#19988;&#31283;&#24577;&#30340;&#26368;&#20248;&#40065;&#26834;&#31574;&#30053;&#23384;&#22312;&#65292;&#35813;&#31574;&#30053;&#19982;&#36125;&#23572;&#26364;&#26368;&#20248;&#31574;&#30053;&#19968;&#33268;&#12290;&#21516;&#26102;&#65292;&#38416;&#26126;&#20102;&#22312;&#26368;&#23567;&#21270;&#36125;&#23572;&#26364;&#35823;&#24046;&#20197;&#33719;&#24471;&#26368;&#20248;&#40065;&#26834;&#31574;&#30053;&#26102;&#20351;&#29992;$L^{\infty}$-&#33539;&#25968;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02165</link><description>&lt;p&gt;
&#23545;&#25239;&#40065;&#26834;Q&#23398;&#20064;&#30340;&#26368;&#20248;&#21270;&#30740;&#31350;&#19982;&#36125;&#23572;&#26364;&#26080;&#31351;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Towards Optimal Adversarial Robust Q-learning with Bellman Infinity-error
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#40065;&#26834;Q&#23398;&#20064;&#30340;&#26368;&#20248;&#21270;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#19988;&#31283;&#24577;&#30340;&#26368;&#20248;&#40065;&#26834;&#31574;&#30053;&#23384;&#22312;&#65292;&#35813;&#31574;&#30053;&#19982;&#36125;&#23572;&#26364;&#26368;&#20248;&#31574;&#30053;&#19968;&#33268;&#12290;&#21516;&#26102;&#65292;&#38416;&#26126;&#20102;&#22312;&#26368;&#23567;&#21270;&#36125;&#23572;&#26364;&#35823;&#24046;&#20197;&#33719;&#24471;&#26368;&#20248;&#40065;&#26834;&#31574;&#30053;&#26102;&#20351;&#29992;$L^{\infty}$-&#33539;&#25968;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#40065;&#26834;&#31574;&#30053;&#23545;&#20110;&#25269;&#24481;&#24433;&#21709;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26234;&#33021;&#20307;&#30340;&#25915;&#20987;&#25110;&#24178;&#25200;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#29366;&#24577;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#24182;&#26263;&#31034;&#20102;&#32570;&#20047;&#26368;&#20248;&#40065;&#26834;&#31574;&#30053;&#65288;ORP&#65289;&#30340;&#28508;&#22312;&#38382;&#39064;&#65292;&#36825;&#32473;&#35774;&#23450;&#20005;&#26684;&#30340;&#40065;&#26834;&#24615;&#32422;&#26463;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;ORP&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31574;&#30053;&#19968;&#33268;&#24615;&#20551;&#35774;&#65288;CAP&#65289;&#65292;&#35813;&#20551;&#35774;&#25351;&#20986;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#26368;&#20248;&#21160;&#20316;&#22312;&#24494;&#23567;&#25200;&#21160;&#19979;&#20445;&#25345;&#19968;&#33268;&#65292;&#24471;&#21040;&#20102;&#23454;&#35777;&#21644;&#29702;&#35770;&#35777;&#25454;&#30340;&#25903;&#25345;&#12290;&#22312;CAP&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20851;&#38190;&#22320;&#35777;&#26126;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#19988;&#31283;&#24577;&#30340;ORP&#30340;&#23384;&#22312;&#65292;&#35813;ORP&#19982;&#36125;&#23572;&#26364;&#26368;&#20248;&#31574;&#30053;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#38416;&#26126;&#20102;&#22312;&#26368;&#23567;&#21270;&#36125;&#23572;&#26364;&#35823;&#24046;&#20197;&#33719;&#24471;ORP&#26102;&#65292;$L^{\infty}$-&#33539;&#25968;&#30340;&#24517;&#35201;&#24615;&#12290;&#36825;&#19968;&#21457;&#29616;&#28548;&#28165;&#20102;&#20808;&#21069;&#38024;&#23545;&#36125;&#23572;&#26364;&#26368;&#20248;&#31574;&#30053;&#20351;&#29992;$L^{1}$-&#33539;&#25968;&#30340;DRL&#31639;&#27861;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#28608;&#21169;&#25105;&#20204;&#35757;&#32451;&#19968;&#31181;&#19968;&#33268;&#24615;&#23545;&#25239;&#40065;&#26834;&#28145;&#24230;Q&#32593;&#32476;&#65288;CA&#65289;
&lt;/p&gt;
&lt;p&gt;
Establishing robust policies is essential to counter attacks or disturbances affecting deep reinforcement learning (DRL) agents. Recent studies explore state-adversarial robustness and suggest the potential lack of an optimal robust policy (ORP), posing challenges in setting strict robustness constraints. This work further investigates ORP: At first, we introduce a consistency assumption of policy (CAP) stating that optimal actions in the Markov decision process remain consistent with minor perturbations, supported by empirical and theoretical evidence. Building upon CAP, we crucially prove the existence of a deterministic and stationary ORP that aligns with the Bellman optimal policy. Furthermore, we illustrate the necessity of $L^{\infty}$-norm when minimizing Bellman error to attain ORP. This finding clarifies the vulnerability of prior DRL algorithms that target the Bellman optimal policy with $L^{1}$-norm and motivates us to train a Consistent Adversarial Robust Deep Q-Network (CA
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21361;&#23475;&#25918;&#22823;&#29616;&#35937;&#24182;&#21457;&#23637;&#20102;&#37327;&#21270;&#21361;&#23475;&#25918;&#22823;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#27169;&#22411;&#36755;&#20986;&#30340;&#21361;&#23475;&#19982;&#29992;&#25143;&#36755;&#20837;&#30340;&#24773;&#22659;&#12290;&#25105;&#20204;&#36824;&#23454;&#35777;&#22320;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#37327;&#21270;&#20102;&#30001;&#21361;&#23475;&#25918;&#22823;&#24341;&#36215;&#30340;&#24615;&#21035;&#20043;&#38388;&#30340;&#24433;&#21709;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.01787</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#21361;&#23475;&#25918;&#22823;
&lt;/p&gt;
&lt;p&gt;
Harm Amplification in Text-to-Image Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01787
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21361;&#23475;&#25918;&#22823;&#29616;&#35937;&#24182;&#21457;&#23637;&#20102;&#37327;&#21270;&#21361;&#23475;&#25918;&#22823;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#27169;&#22411;&#36755;&#20986;&#30340;&#21361;&#23475;&#19982;&#29992;&#25143;&#36755;&#20837;&#30340;&#24773;&#22659;&#12290;&#25105;&#20204;&#36824;&#23454;&#35777;&#22320;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#37327;&#21270;&#20102;&#30001;&#21361;&#23475;&#25918;&#22823;&#24341;&#36215;&#30340;&#24615;&#21035;&#20043;&#38388;&#30340;&#24433;&#21709;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687; (T2I) &#27169;&#22411;&#24050;&#25104;&#20026;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#28982;&#32780;&#65292;&#23384;&#22312;&#23433;&#20840;&#38382;&#39064;&#65292;&#21363;&#20351;&#29992;&#25143;&#36755;&#20837;&#30475;&#20284;&#23433;&#20840;&#30340;&#25552;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#20063;&#21487;&#33021;&#29983;&#25104;&#26377;&#23475;&#22270;&#20687;&#12290;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#21361;&#23475;&#25918;&#22823;&#65292;&#23427;&#27604;&#23545;&#25239;&#25552;&#31034;&#26356;&#20855;&#28508;&#22312;&#39118;&#38505;&#65292;&#20351;&#29992;&#25143;&#26080;&#24847;&#38388;&#36973;&#21463;&#20260;&#23475;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#21361;&#23475;&#25918;&#22823;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#24182;&#36827;&#19968;&#27493;&#36129;&#29486;&#20110;&#24320;&#21457;&#29992;&#20110;&#37327;&#21270;&#21361;&#23475;&#25918;&#22823;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#27169;&#22411;&#36755;&#20986;&#30340;&#21361;&#23475;&#19982;&#29992;&#25143;&#36755;&#20837;&#30340;&#24773;&#22659;&#12290;&#25105;&#20204;&#36824;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;&#20102;&#22914;&#20309;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#32626;&#22330;&#26223;&#65292;&#21253;&#25324;&#37327;&#21270;&#30001;&#21361;&#23475;&#25918;&#22823;&#24341;&#36215;&#30340;&#19981;&#21516;&#24615;&#21035;&#20043;&#38388;&#30340;&#24433;&#21709;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#20026;&#30740;&#31350;&#32773;&#25552;&#20379;&#24037;&#20855;&#21435;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image (T2I) models have emerged as a significant advancement in generative AI; however, there exist safety concerns regarding their potential to produce harmful image outputs even when users input seemingly safe prompts. This phenomenon, where T2I models generate harmful representations that were not explicit in the input, poses a potentially greater risk than adversarial prompts, leaving users unintentionally exposed to harms. Our paper addresses this issue by first introducing a formal definition for this phenomenon, termed harm amplification. We further contribute to the field by developing methodologies to quantify harm amplification in which we consider the harm of the model output in the context of user input. We then empirically examine how to apply these different methodologies to simulate real-world deployment scenarios including a quantification of disparate impacts across genders resulting from harm amplification. Together, our work aims to offer researchers tools to
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Scheduled Curiosity-Deep Dyna-Q (SC-DDQ)&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#39044;&#23450;&#23398;&#20064;&#21644;&#22909;&#22855;&#24515;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20195;&#29702;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.00085</link><description>&lt;p&gt;
&#39044;&#23450;&#22909;&#22855;&#24515;-&#28145;&#24230;&#21160;&#24577;-Q&#65306;&#23545;&#35805;&#31574;&#30053;&#23398;&#20064;&#30340;&#39640;&#25928;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Scheduled Curiosity-Deep Dyna-Q: Efficient Exploration for Dialog Policy Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Scheduled Curiosity-Deep Dyna-Q (SC-DDQ)&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#39044;&#23450;&#23398;&#20064;&#21644;&#22909;&#22855;&#24515;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20195;&#29702;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20195;&#29702;&#30340;&#22521;&#35757;&#26159;&#32791;&#26102;&#30340;&#65292;&#24182;&#38656;&#35201;&#19982;&#30495;&#23454;&#29992;&#25143;&#36827;&#34892;&#22823;&#37327;&#30340;&#20132;&#20114;&#12290;&#22914;&#20309;&#22312;&#26377;&#38480;&#30340;&#23545;&#35805;&#32463;&#39564;&#20013;&#25484;&#25569;&#23545;&#35805;&#31574;&#30053;&#20173;&#28982;&#26159;&#20351;&#20195;&#29702;&#22521;&#35757;&#36807;&#31243;&#26356;&#21152;&#39640;&#25928;&#30340;&#38556;&#30861;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#26694;&#26550;&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#22521;&#35757;&#26679;&#26412;&#24320;&#22987;&#22521;&#35757;&#65292;&#36825;&#19982;&#20154;&#31867;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;&#25439;&#23475;&#20102;&#22521;&#35757;&#30340;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#23545;&#35805;&#27169;&#22411;Deep Dyna-Q(DDQ)&#30340;&#39044;&#23450;&#22909;&#22855;&#24515;-&#28145;&#24230;&#21160;&#24577;-Q (SC-DDQ)&#30340;&#22909;&#22855;&#24515;&#39537;&#21160;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#21035;&#20026;SC-DDQ&#21644;DDQ&#35774;&#35745;&#20102;&#23398;&#20064;&#35745;&#21010;&#65292;&#36981;&#24490;&#20004;&#31181;&#30456;&#21453;&#30340;&#22521;&#35757;&#31574;&#30053;&#65306;&#32463;&#20856;&#35838;&#31243;&#23398;&#20064;&#21450;&#20854;&#36870;&#21521;&#29256;&#26412;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#24341;&#20837;&#39044;&#23450;&#23398;&#20064;&#21644;&#22909;&#22855;&#24515;&#65292;&#26032;&#26694;&#26550;&#22312;DDQ&#21644;Dee&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training task-oriented dialog agents based on reinforcement learning is time-consuming and requires a large number of interactions with real users. How to grasp dialog policy within limited dialog experiences remains an obstacle that makes the agent training process less efficient. In addition, most previous frameworks start training by randomly choosing training samples, which differs from the human learning method and hurts the efficiency and stability of training. Therefore, we propose Scheduled Curiosity-Deep Dyna-Q (SC-DDQ), a curiosity-driven curriculum learning framework based on a state-of-the-art model-based reinforcement learning dialog model, Deep Dyna-Q (DDQ). Furthermore, we designed learning schedules for SC-DDQ and DDQ, respectively, following two opposite training strategies: classic curriculum learning and its reverse version. Our results show that by introducing scheduled learning and curiosity, the new framework leads to a significant improvement over the DDQ and Dee
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21435;&#20013;&#24515;&#21270;&#21452;&#23618;&#20248;&#21270;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#22312;&#24322;&#26500;&#35774;&#32622;&#19979;&#20855;&#26377;&#36739;&#23567;&#30340;&#36890;&#20449;&#25104;&#26412;&#21644;&#36718;&#27425;&#65292;&#24182;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#22909;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2311.11342</link><description>&lt;p&gt;
&#20851;&#20110;&#21435;&#20013;&#24515;&#21270;&#21452;&#23618;&#20248;&#21270;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
On the Communication Complexity of Decentralized Bilevel Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21435;&#20013;&#24515;&#21270;&#21452;&#23618;&#20248;&#21270;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#22312;&#24322;&#26500;&#35774;&#32622;&#19979;&#20855;&#26377;&#36739;&#23567;&#30340;&#36890;&#20449;&#25104;&#26412;&#21644;&#36718;&#27425;&#65292;&#24182;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#22909;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#21435;&#20013;&#24515;&#21270;&#21452;&#23618;&#20248;&#21270;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#22240;&#20026;&#23427;&#22312;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31639;&#27861;&#30001;&#20110;&#20272;&#35745;&#38543;&#26426;&#36229;&#26799;&#24230;&#32780;&#23548;&#33268;&#36890;&#20449;&#22797;&#26434;&#24230;&#36739;&#22823;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#24322;&#26500;&#35774;&#32622;&#19979;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#27599;&#36718;&#20013;&#20855;&#26377;&#36739;&#23567;&#30340;&#36890;&#20449;&#25104;&#26412;&#21644;&#36739;&#23569;&#30340;&#36890;&#20449;&#36718;&#27425;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#24322;&#26500;&#24615;&#30340;&#24378;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#22909;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#24322;&#26500;&#35774;&#32622;&#19979;&#23454;&#29616;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#30340;&#38543;&#26426;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11342v2 Announce Type: replace Abstract: Decentralized bilevel optimization has been actively studied in the past few years since it has widespread applications in machine learning. However, existing algorithms suffer from large communication complexity caused by the estimation of stochastic hypergradient, limiting their application to real-world tasks. To address this issue, we develop a novel decentralized stochastic bilevel gradient descent algorithm under the heterogeneous setting, which enjoys a small communication cost in each round and a small number of communication rounds. As such, it can achieve a much better communication complexity than existing algorithms without any strong assumptions regarding heterogeneity. To the best of our knowledge, this is the first stochastic algorithm achieving these theoretical results under the heterogeneous setting. At last, the experimental results confirm the efficacy of our algorithm.
&lt;/p&gt;</description></item><item><title>Tactics2D&#26159;&#19968;&#20010;&#20855;&#26377;&#33258;&#21160;&#29983;&#25104;&#20132;&#36890;&#22330;&#26223;&#21151;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#24211;&#65292;&#26088;&#22312;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#25506;&#32034;&#23398;&#20064;&#39537;&#21160;&#30340;&#39550;&#39542;&#20915;&#31574;&#27169;&#22411;&#30340;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2311.11058</link><description>&lt;p&gt;
Tactics2D&#65306;&#19968;&#31181;&#20855;&#26377;&#29983;&#25104;&#22330;&#26223;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#24211;&#65292;&#29992;&#20110;&#39550;&#39542;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Tactics2D: A Reinforcement Learning Environment Library with Generative Scenarios for Driving Decision-making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11058
&lt;/p&gt;
&lt;p&gt;
Tactics2D&#26159;&#19968;&#20010;&#20855;&#26377;&#33258;&#21160;&#29983;&#25104;&#20132;&#36890;&#22330;&#26223;&#21151;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#24211;&#65292;&#26088;&#22312;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#25506;&#32034;&#23398;&#20064;&#39537;&#21160;&#30340;&#39550;&#39542;&#20915;&#31574;&#27169;&#22411;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tactics2D&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#24211;&#65292;&#20855;&#26377;&#33258;&#21160;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20132;&#36890;&#22330;&#26223;&#30340;&#21151;&#33021;&#12290;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#19968;&#20010;&#24320;&#31665;&#21363;&#29992;&#30340;&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#25506;&#32034;&#22522;&#20110;&#23398;&#20064;&#30340;&#39550;&#39542;&#20915;&#31574;&#27169;&#22411;&#12290;&#35813;&#24211;&#23454;&#29616;&#20102;&#22522;&#20110;&#35268;&#21017;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26469;&#29983;&#25104;&#20114;&#21160;&#20132;&#36890;&#22330;&#26223;&#12290;Tactics2D&#30340;&#26174;&#33879;&#29305;&#28857;&#21253;&#25324;&#19982;&#29616;&#23454;&#19990;&#30028;&#26085;&#24535;&#21644;&#25968;&#25454;&#26684;&#24335;&#30340;&#24191;&#27867;&#20860;&#23481;&#24615;&#65292;&#21487;&#23450;&#21046;&#30340;&#20132;&#36890;&#22330;&#26223;&#32452;&#20214;&#20197;&#21450;&#20016;&#23500;&#30340;&#20869;&#32622;&#21151;&#33021;&#27169;&#26495;&#12290;Tactics2D&#32771;&#34385;&#21040;&#29992;&#25143;&#21451;&#22909;&#24615;&#32780;&#24320;&#21457;&#65292;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#25991;&#26723;&#21644;&#20132;&#20114;&#24335;&#22312;&#32447;&#25945;&#31243;&#12290;&#35813;&#36719;&#20214;&#20445;&#25345;&#20102;&#31283;&#22266;&#30340;&#21487;&#38752;&#24615;&#65292;&#36229;&#36807;90%&#30340;&#20195;&#30721;&#36890;&#36807;&#20102;&#21333;&#20803;&#27979;&#35797;&#12290;&#35201;&#35775;&#38382;&#28304;&#20195;&#30721;&#24182;&#21442;&#19982;&#35752;&#35770;&#65292;&#35831;&#35775;&#38382;Tactics2D&#30340;&#23448;&#26041;GitHub&#39029;&#38754;https://github.com/WoodOxen/Tactics2D&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11058v2 Announce Type: replace  Abstract: Tactics2D is an open-source Reinforcement Learning environment library featured with auto-generation of diverse and challenging traffic scenarios. Its primary goal is to provide an out-of-the-box toolkit for researchers to explore learning-based driving decision-making models. This library implements both rule-based and data-driven approaches to generate interactive traffic scenarios. Noteworthy features of Tactics2D include expansive compatibility with real-world log and data formats, customizable traffic scenario components, and rich built-in functional templates. Developed with user-friendliness in mind, Tactics2D offers detailed documentation and an interactive online tutorial. The software maintains robust reliability, with over 90% code passing unit testing. For access to the source code and participation in discussions, visit the official GitHub page for Tactcis2D at https://github.com/WoodOxen/Tactics2D.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#23558;&#21512;&#36866;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#24212;&#29992;&#20110;&#35299;&#20915;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#20013;&#30340;&#25968;&#25454;&#38598;&#36716;&#31227;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#22312;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;EEG&#20449;&#21495;&#20998;&#31867;&#31995;&#32479;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2210.06554</link><description>&lt;p&gt;
&#36816;&#29992;XAI&#26041;&#27861;&#20110;&#22522;&#20110;EEG&#30340;&#31995;&#32479;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Toward the application of XAI methods in EEG-based systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.06554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23558;&#21512;&#36866;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#24212;&#29992;&#20110;&#35299;&#20915;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#20013;&#30340;&#25968;&#25454;&#38598;&#36716;&#31227;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#22312;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;EEG&#20449;&#21495;&#20998;&#31867;&#31995;&#32479;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#25968;&#25454;&#38598;&#36716;&#31227;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#36259;&#26696;&#20363;&#26159;&#22312;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#32972;&#26223;&#19979;&#23545;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#12290; EEG&#20449;&#21495;&#30340;&#38750;&#38745;&#27490;&#24615;&#21487;&#33021;&#23548;&#33268;BCI&#20998;&#31867;&#31995;&#32479;&#22312;&#19981;&#21516;&#20250;&#35805;&#20013;&#20351;&#29992;&#26102;&#24615;&#33021;&#27867;&#21270;&#24046;&#65292;&#29978;&#33267;&#26159;&#21516;&#19968;&#34987;&#35797;&#39564;&#12290; &#26412;&#25991;&#30340;&#20986;&#21457;&#28857;&#26159;&#65292;&#36890;&#36807;&#21033;&#29992;&#21512;&#36866;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#26469;&#23450;&#20301;&#21644;&#36716;&#25442;&#36755;&#20837;&#30340;&#30456;&#20851;&#29305;&#24449;&#65292;&#20174;&#32780;&#32531;&#35299;&#25968;&#25454;&#38598;&#36716;&#31227;&#38382;&#39064;&#12290; &#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#23545;&#20960;&#31181;XAI&#26041;&#27861;&#22312;&#22312;&#20856;&#22411;&#30340;&#29992;&#20110;&#24773;&#32490;&#35782;&#21035;&#30340;EEG&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;ML&#31995;&#32479;&#19978;&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#23454;&#39564;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;XAI&#26041;&#27861;&#25214;&#21040;&#30340;&#35768;&#22810;&#30456;&#20851;&#32452;&#20214;&#22312;&#20250;&#35805;&#20043;&#38388;&#26159;&#20849;&#20139;&#30340;&#65292;&#21487;&#20197;&#29992;&#26469;&#26500;&#24314;&#19968;&#20010;&#20855;&#26377;&#26356;&#22909;&#27867;&#21270;&#33021;&#21147;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.06554v3 Announce Type: replace-cross  Abstract: An interesting case of the well-known Dataset Shift Problem is the classification of Electroencephalogram (EEG) signals in the context of Brain-Computer Interface (BCI). The non-stationarity of EEG signals can lead to poor generalisation performance in BCI classification systems used in different sessions, also from the same subject. In this paper, we start from the hypothesis that the Dataset Shift problem can be alleviated by exploiting suitable eXplainable Artificial Intelligence (XAI) methods to locate and transform the relevant characteristics of the input for the goal of classification. In particular, we focus on an experimental analysis of explanations produced by several XAI methods on an ML system trained on a typical EEG dataset for emotion recognition. Results show that many relevant components found by XAI methods are shared across the sessions and can be used to build a system able to generalise better. However, re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26694;&#26550;&#65292;&#36890;&#36807;&#20943;&#23569;&#26041;&#24046;&#26469;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#27969;&#27700;&#32447;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#26469;&#26816;&#27979;&#26377;&#25928;&#24314;&#27169;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.16692</link><description>&lt;p&gt;
&#39044;&#26657;&#20934;&#21644;&#35745;&#31639;&#65306;&#28145;&#24230;&#28857;&#20987;&#29575;&#39044;&#27979;&#27169;&#22411;&#20013;&#19968;&#31181;&#26041;&#24046;&#20943;&#23569;&#30340;&#24230;&#37327;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Calibration-then-Calculation: A Variance Reduced Metric Framework in Deep Click-Through Rate Prediction Models. (arXiv:2401.16692v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26694;&#26550;&#65292;&#36890;&#36807;&#20943;&#23569;&#26041;&#24046;&#26469;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#27969;&#27700;&#32447;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#26469;&#26816;&#27979;&#26377;&#25928;&#24314;&#27169;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#22312;&#21508;&#20010;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27969;&#27700;&#32447;&#30340;&#24615;&#33021;&#35780;&#20272;&#20851;&#27880;&#36739;&#23569;&#12290;&#38543;&#30528;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#22797;&#26434;&#27169;&#22411;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#36890;&#24120;&#21482;&#36816;&#34892;&#19968;&#27425;&#35757;&#32451;&#36807;&#31243;&#24182;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#35780;&#20272;&#25351;&#26631;&#30340;&#26041;&#24046;&#65292;&#36825;&#31181;&#36807;&#31243;&#21487;&#33021;&#23548;&#33268;&#19981;&#31934;&#30830;&#30340;&#27604;&#36739;&#12290;&#25351;&#26631;&#26041;&#24046;&#26469;&#33258;&#28145;&#24230;&#23398;&#20064;&#27969;&#27700;&#32447;&#35757;&#32451;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38543;&#26426;&#24615;&#12290;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#22914;&#22810;&#27425;&#36816;&#34892;&#35757;&#32451;&#36807;&#31243;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#24448;&#24448;&#19981;&#21487;&#34892;&#65292;&#22240;&#20026;&#35745;&#31639;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26694;&#26550;&#65292;&#31216;&#20026;&#26657;&#20934;&#25439;&#22833;&#24230;&#37327;&#65292;&#36890;&#36807;&#20943;&#23569;&#20854;&#22522;&#20934;&#27169;&#22411;&#20013;&#30340;&#26041;&#24046;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#32467;&#26524;&#65292;&#36825;&#20010;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#26469;&#26816;&#27979;&#26377;&#25928;&#24314;&#27169;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24471;&#21040;&#20102;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#21644;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has been widely adopted across various fields, but there has been little focus on evaluating the performance of deep learning pipelines. With the increased use of large datasets and complex models, it has become common to run the training process only once and compare the result to previous benchmarks. However, this procedure can lead to imprecise comparisons due to the variance in neural network evaluation metrics. The metric variance comes from the randomness inherent in the training process of deep learning pipelines. Traditional solutions such as running the training process multiple times are usually not feasible in deep learning due to computational limitations. In this paper, we propose a new metric framework, Calibrated Loss Metric, that addresses this issue by reducing the variance in its vanilla counterpart. As a result, the new metric has a higher accuracy to detect effective modeling improvement. Our approach is supported by theoretical justifications and exte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;Chen-Fliess&#24207;&#21015;&#23637;&#24320;&#23558;&#36830;&#32493;&#28145;&#24230;&#31070;&#32463;ODE&#27169;&#22411;&#36716;&#21270;&#20026;&#21333;&#23618;&#12289;&#26080;&#38480;&#23485;&#24230;&#30340;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;&#27492;&#26694;&#26550;&#25512;&#23548;&#20986;&#20102;&#23558;&#21021;&#22987;&#26465;&#20214;&#26144;&#23556;&#21040;&#26576;&#20010;&#32456;&#31471;&#26102;&#38388;&#30340;ODE&#27169;&#22411;&#30340;Rademacher&#22797;&#26434;&#24230;&#30340;&#32039;&#20945;&#34920;&#36798;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.16655</link><description>&lt;p&gt;
&#36890;&#36807;Chen-Fliess&#24207;&#21015;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#36830;&#32493;&#28145;&#24230;&#31070;&#32463;ODE&#27169;&#22411;&#26500;&#24314;&#20026;&#21333;&#23618;&#12289;&#26080;&#38480;&#23485;&#24230;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rademacher Complexity of Neural ODEs via Chen-Fliess Series. (arXiv:2401.16655v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;Chen-Fliess&#24207;&#21015;&#23637;&#24320;&#23558;&#36830;&#32493;&#28145;&#24230;&#31070;&#32463;ODE&#27169;&#22411;&#36716;&#21270;&#20026;&#21333;&#23618;&#12289;&#26080;&#38480;&#23485;&#24230;&#30340;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;&#27492;&#26694;&#26550;&#25512;&#23548;&#20986;&#20102;&#23558;&#21021;&#22987;&#26465;&#20214;&#26144;&#23556;&#21040;&#26576;&#20010;&#32456;&#31471;&#26102;&#38388;&#30340;ODE&#27169;&#22411;&#30340;Rademacher&#22797;&#26434;&#24230;&#30340;&#32039;&#20945;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#36830;&#32493;&#28145;&#24230;&#31070;&#32463;ODE&#27169;&#22411;&#20351;&#29992;Chen-Fliess&#24207;&#21015;&#23637;&#24320;&#20026;&#21333;&#23618;&#12289;&#26080;&#38480;&#23485;&#24230;&#30340;&#32593;&#32476;&#12290;&#22312;&#36825;&#20010;&#32593;&#32476;&#20013;&#65292;&#36755;&#20986;&#30340;&#8220;&#26435;&#37325;&#8221;&#26469;&#33258;&#25511;&#21046;&#36755;&#20837;&#30340;&#29305;&#24449;&#24207;&#21015;&#65292;&#23427;&#30001;&#25511;&#21046;&#36755;&#20837;&#22312;&#21333;&#32431;&#24418;&#19978;&#30340;&#36845;&#20195;&#31215;&#20998;&#26500;&#25104;&#12290;&#32780;&#8220;&#29305;&#24449;&#8221;&#21017;&#22522;&#20110;&#21463;&#25511;ODE&#27169;&#22411;&#20013;&#36755;&#20986;&#20989;&#25968;&#30456;&#23545;&#20110;&#21521;&#37327;&#22330;&#30340;&#36845;&#20195;&#26446;&#23548;&#25968;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#65292;&#24212;&#29992;&#36825;&#20010;&#26694;&#26550;&#25512;&#23548;&#20986;&#20102;&#23558;&#21021;&#22987;&#26465;&#20214;&#26144;&#23556;&#21040;&#26576;&#20010;&#32456;&#31471;&#26102;&#38388;&#30340;ODE&#27169;&#22411;&#30340;Rademacher&#22797;&#26434;&#24230;&#30340;&#32039;&#20945;&#34920;&#36798;&#24335;&#12290;&#36825;&#19968;&#32467;&#26524;&#21033;&#29992;&#20102;&#21333;&#23618;&#32467;&#26500;&#25152;&#24102;&#26469;&#30340;&#30452;&#25509;&#20998;&#26512;&#24615;&#36136;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20123;&#20855;&#20307;&#31995;&#32479;&#30340;&#20363;&#23376;&#23454;&#20363;&#21270;&#35813;&#30028;&#65292;&#24182;&#35752;&#35770;&#20102;&#21487;&#33021;&#30340;&#21518;&#32493;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show how continuous-depth neural ODE models can be framed as single-layer, infinite-width nets using the Chen--Fliess series expansion for nonlinear ODEs. In this net, the output ''weights'' are taken from the signature of the control input -- a tool used to represent infinite-dimensional paths as a sequence of tensors -- which comprises iterated integrals of the control input over a simplex. The ''features'' are taken to be iterated Lie derivatives of the output function with respect to the vector fields in the controlled ODE model. The main result of this work applies this framework to derive compact expressions for the Rademacher complexity of ODE models that map an initial condition to a scalar output at some terminal time. The result leverages the straightforward analysis afforded by single-layer architectures. We conclude with some examples instantiating the bound for some specific systems and discuss potential follow-up work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#19982;&#26368;&#26032;&#30340;&#20915;&#31574;&#29702;&#35770;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#40065;&#26834;&#20248;&#21270;&#20934;&#21017;&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#20013;&#33719;&#24471;&#26377;&#31283;&#23450;&#24615;&#21644;&#20248;&#36234;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.15771</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#19982;&#25968;&#25454;&#39537;&#21160;&#40065;&#26834;&#20248;&#21270;&#30340;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Bayesian Nonparametrics meets Data-Driven Robust Optimization. (arXiv:2401.15771v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#19982;&#26368;&#26032;&#30340;&#20915;&#31574;&#29702;&#35770;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#40065;&#26834;&#20248;&#21270;&#20934;&#21017;&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#20013;&#33719;&#24471;&#26377;&#31283;&#23450;&#24615;&#21644;&#20248;&#36234;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#27169;&#22411;&#36890;&#24120;&#28041;&#21450;&#20248;&#21270;&#25968;&#25454;&#39537;&#21160;&#30340;&#39118;&#38505;&#20934;&#21017;&#12290;&#39118;&#38505;&#36890;&#24120;&#26159;&#26681;&#25454;&#32463;&#39564;&#25968;&#25454;&#20998;&#24067;&#35745;&#31639;&#30340;&#65292;&#20294;&#30001;&#20110;&#20998;&#24067;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19981;&#31283;&#23450;&#21644;&#19981;&#22909;&#30340;&#26679;&#26412;&#22806;&#34920;&#29616;&#12290;&#22312;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30340;&#31934;&#31070;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#40065;&#26834;&#20934;&#21017;&#65292;&#23558;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#65288;&#21363;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#65289;&#29702;&#35770;&#21644;&#26368;&#36817;&#30340;&#24179;&#28369;&#27169;&#31946;&#35268;&#36991;&#20559;&#22909;&#30340;&#20915;&#31574;&#29702;&#35770;&#27169;&#22411;&#30340;&#35265;&#35299;&#30456;&#32467;&#21512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#19982;&#26631;&#20934;&#27491;&#21017;&#21270;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#25216;&#26415;&#30340;&#26032;&#36830;&#25509;&#65292;&#20854;&#20013;&#21253;&#25324;&#23725;&#22238;&#24402;&#21644;&#22871;&#32034;&#22238;&#24402;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#40065;&#26834;&#20248;&#21270;&#36807;&#31243;&#22312;&#26377;&#38480;&#26679;&#26412;&#21644;&#28176;&#36817;&#32479;&#35745;&#20445;&#35777;&#26041;&#38754;&#30340;&#26377;&#21033;&#24615;&#23384;&#22312;&#12290;&#23545;&#20110;&#23454;&#38469;&#23454;&#26045;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#22522;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#34920;&#31034;&#30340;&#21487;&#34892;&#36817;&#20284;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training machine learning and statistical models often involves optimizing a data-driven risk criterion. The risk is usually computed with respect to the empirical data distribution, but this may result in poor and unstable out-of-sample performance due to distributional uncertainty. In the spirit of distributionally robust optimization, we propose a novel robust criterion by combining insights from Bayesian nonparametric (i.e., Dirichlet Process) theory and recent decision-theoretic models of smooth ambiguity-averse preferences. First, we highlight novel connections with standard regularized empirical risk minimization techniques, among which Ridge and LASSO regressions. Then, we theoretically demonstrate the existence of favorable finite-sample and asymptotic statistical guarantees on the performance of the robust optimization procedure. For practical implementation, we propose and study tractable approximations of the criterion based on well-known Dirichlet Process representations. 
&lt;/p&gt;</description></item><item><title>Marabou 2.0&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#20998;&#26512;&#22120;&#65292;&#20855;&#26377;&#21019;&#26032;&#30340;&#26550;&#26500;&#35774;&#35745;&#21644;&#24341;&#20837;&#30340;&#20027;&#35201;&#21151;&#33021;&#21644;&#32452;&#20214;&#12290;</title><link>http://arxiv.org/abs/2401.14461</link><description>&lt;p&gt;
Marabou 2.0: &#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#20998;&#26512;&#22120;
&lt;/p&gt;
&lt;p&gt;
Marabou 2.0: A Versatile Formal Analyzer of Neural Networks. (arXiv:2401.14461v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14461
&lt;/p&gt;
&lt;p&gt;
Marabou 2.0&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#20998;&#26512;&#22120;&#65292;&#20855;&#26377;&#21019;&#26032;&#30340;&#26550;&#26500;&#35774;&#35745;&#21644;&#24341;&#20837;&#30340;&#20027;&#35201;&#21151;&#33021;&#21644;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#20851;&#20110;Marabou&#26694;&#26550;2.0&#29256;&#26412;&#30340;&#32508;&#21512;&#31995;&#32479;&#25551;&#36848;&#65292;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24418;&#24335;&#20998;&#26512;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#24037;&#20855;&#30340;&#26550;&#26500;&#35774;&#35745;&#65292;&#24182;&#20171;&#32461;&#20102;&#33258;&#21021;&#22987;&#21457;&#24067;&#20197;&#26469;&#24341;&#20837;&#30340;&#20027;&#35201;&#21151;&#33021;&#21644;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper serves as a comprehensive system description of version 2.0 of the Marabou framework for formal analysis of neural networks. We discuss the tool's architectural design and highlight the major features and components introduced since its initial release.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;NLBAC&#26694;&#26550;&#65292;&#29992;&#20110;&#31283;&#23450;&#21644;&#23433;&#20840;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#26469;&#36817;&#20284;&#31995;&#32479;&#21160;&#21147;&#23398;&#65292;&#24182;&#23558;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;CBF&#65289;&#21644;&#25511;&#21046;Lyapunov&#20989;&#25968;&#65288;CLF&#65289;&#26694;&#26550;&#19982;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#38598;&#25104;&#65292;&#20197;&#32500;&#25345;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13148</link><description>&lt;p&gt;
NLBAC: &#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#31283;&#23450;&#21644;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
NLBAC: A Neural Ordinary Differential Equations-based Framework for Stable and Safe Reinforcement Learning. (arXiv:2401.13148v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;NLBAC&#26694;&#26550;&#65292;&#29992;&#20110;&#31283;&#23450;&#21644;&#23433;&#20840;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#26469;&#36817;&#20284;&#31995;&#32479;&#21160;&#21147;&#23398;&#65292;&#24182;&#23558;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;CBF&#65289;&#21644;&#25511;&#21046;Lyapunov&#20989;&#25968;&#65288;CLF&#65289;&#26694;&#26550;&#19982;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#38598;&#25104;&#65292;&#20197;&#32500;&#25345;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#35270;&#39057;&#28216;&#25103;&#21644;&#26426;&#22120;&#20154;&#31561;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#30495;&#23454;&#19990;&#30028;&#31995;&#32479;&#26102;&#65292;&#30830;&#20445;&#23433;&#20840;&#21644;&#31283;&#23450;&#24615;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#39318;&#20808;&#32473;&#20986;&#20102;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#31283;&#23450;&#24615;&#23450;&#20041;&#65292;&#28982;&#21518;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;Lyapunov&#23631;&#38556;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;NLBAC&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#26469;&#36817;&#20284;&#31995;&#32479;&#21160;&#21147;&#23398;&#65292;&#24182;&#23558;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;CBF&#65289;&#21644;&#25511;&#21046;Lyapunov&#20989;&#25968;&#65288;CLF&#65289;&#26694;&#26550;&#19982;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#38598;&#25104;&#65292;&#20197;&#24110;&#21161;&#32500;&#25345;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#22686;&#24191;Lagrangian&#26041;&#27861;&#26469;&#26356;&#26032;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#22312;CBF&#32422;&#26463;&#29992;&#20110;&#23433;&#20840;&#24615;&#21644;CLF&#32422;&#26463;&#29992;&#20110;&#31283;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#22791;&#20221;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) excels in applications such as video games and robotics, but ensuring safety and stability remains challenging when using RL to control real-world systems where using model-free algorithms suffering from low sample efficiency might be prohibitive. This paper first provides safety and stability definitions for the RL system, and then introduces a Neural ordinary differential equations-based Lyapunov-Barrier Actor-Critic (NLBAC) framework that leverages Neural Ordinary Differential Equations (NODEs) to approximate system dynamics and integrates the Control Barrier Function (CBF) and Control Lyapunov Function (CLF) frameworks with the actor-critic method to assist in maintaining the safety and stability for the system. Within this framework, we employ the augmented Lagrangian method to update the RL-based controller parameters. Additionally, we introduce an extra backup controller in situations where CBF constraints for safety and the CLF constraint for stabili
&lt;/p&gt;</description></item><item><title>PatchAD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22359;&#30340;MLP-Mixer&#20307;&#31995;&#32467;&#26500;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;&#23427;&#20855;&#26377;&#39640;&#25928;&#21644;&#36731;&#37327;&#32423;&#30340;&#26550;&#26500;&#65292;&#24182;&#37319;&#29992;&#21019;&#26032;&#30340;&#21452;&#39033;&#30446;&#32422;&#26463;&#27169;&#22359;&#26469;&#25552;&#39640;&#34920;&#31034;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.09793</link><description>&lt;p&gt;
PatchAD: &#22522;&#20110;&#22359;&#30340;MLP-Mixer&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
PatchAD: Patch-based MLP-Mixer for Time Series Anomaly Detection. (arXiv:2401.09793v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09793
&lt;/p&gt;
&lt;p&gt;
PatchAD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22359;&#30340;MLP-Mixer&#20307;&#31995;&#32467;&#26500;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;&#23427;&#20855;&#26377;&#39640;&#25928;&#21644;&#36731;&#37327;&#32423;&#30340;&#26550;&#26500;&#65292;&#24182;&#37319;&#29992;&#21019;&#26032;&#30340;&#21452;&#39033;&#30446;&#32422;&#26463;&#27169;&#22359;&#26469;&#25552;&#39640;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26159;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#26088;&#22312;&#35782;&#21035;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#20013;&#30340;&#24322;&#24120;&#20107;&#20214;&#12290;&#36825;&#19968;&#20219;&#21153;&#30340;&#26680;&#24515;&#25361;&#25112;&#22312;&#20110;&#22312;&#32570;&#20047;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23398;&#20064;&#27491;&#24120;&#21644;&#24322;&#24120;&#27169;&#24335;&#30340;&#34920;&#31034;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#22823;&#22810;&#20381;&#36182;&#20110;&#22522;&#20110;&#37325;&#26500;&#30340;&#26041;&#27861;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#34920;&#24449;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19981;&#22815;&#36731;&#37327;&#32423;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#35774;&#35745;&#19968;&#20010;&#26356;&#39640;&#25928;&#30340;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PatchAD&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#23610;&#24230;&#22522;&#20110;&#22359;&#30340;MLP-Mixer&#20307;&#31995;&#32467;&#26500;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#34920;&#24449;&#25552;&#21462;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PatchAD&#30001;&#22235;&#20010;&#29420;&#29305;&#30340;MLP Mixer&#32452;&#25104;&#65292;&#19987;&#38376;&#21033;&#29992;MLP&#26550;&#26500;&#23454;&#29616;&#39640;&#25928;&#21644;&#36731;&#37327;&#32423;&#30340;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21019;&#26032;&#22320;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#39033;&#30446;&#32422;&#26463;&#27169;&#22359;&#26469;&#32531;&#35299;&#28508;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection stands as a crucial aspect of time series analysis, aiming to identify abnormal events in time series samples. The central challenge of this task lies in effectively learning the representations of normal and abnormal patterns in a label-lacking scenario. Previous research mostly relied on reconstruction-based approaches, restricting the representational abilities of the models. In addition, most of the current deep learning-based methods are not lightweight enough, which prompts us to design a more efficient framework for anomaly detection. In this study, we introduce PatchAD, a novel multi-scale patch-based MLP-Mixer architecture that leverages contrastive learning for representational extraction and anomaly detection. Specifically, PatchAD is composed of four distinct MLP Mixers, exclusively utilizing the MLP architecture for high efficiency and lightweight architecture. Additionally, we also innovatively crafted a dual project constraint module to mitigate potenti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#31639;&#27861;&#65292;&#31216;&#20026;&#20998;&#24067;&#24335;&#38543;&#26426;&#32593;&#32476;&#33976;&#39311;&#65288;DRND&#65289;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#33976;&#39311;&#38543;&#26426;&#32593;&#32476;&#30340;&#20998;&#24067;&#21644;&#38544;&#24335;&#34701;&#20837;&#20266;&#35745;&#25968;&#26469;&#25913;&#36827;&#22870;&#21169;&#20998;&#37197;&#30340;&#31934;&#24230;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#25506;&#32034;&#36807;&#31243;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#22343;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09750</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24067;&#24335;&#38543;&#26426;&#32593;&#32476;&#33976;&#39311;&#30340;&#25506;&#32034;&#21644;&#21453;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploration and Anti-Exploration with Distributional Random Network Distillation. (arXiv:2401.09750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09750
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#31639;&#27861;&#65292;&#31216;&#20026;&#20998;&#24067;&#24335;&#38543;&#26426;&#32593;&#32476;&#33976;&#39311;&#65288;DRND&#65289;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#33976;&#39311;&#38543;&#26426;&#32593;&#32476;&#30340;&#20998;&#24067;&#21644;&#38544;&#24335;&#34701;&#20837;&#20266;&#35745;&#25968;&#26469;&#25913;&#36827;&#22870;&#21169;&#20998;&#37197;&#30340;&#31934;&#24230;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#25506;&#32034;&#36807;&#31243;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#22343;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25506;&#32034;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#23545;&#20110;&#19968;&#20010;&#26234;&#33021;&#20307;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#21462;&#24471;&#39640;&#22238;&#25253;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#30446;&#21069;&#30340;&#25506;&#32034;&#38543;&#26426;&#32593;&#32476;&#33976;&#39311;&#65288;Random Network Distillation&#65292;RND&#65289;&#31639;&#27861;&#24050;&#22312;&#35768;&#22810;&#29615;&#22659;&#20013;&#35777;&#26126;&#26377;&#25928;&#65292;&#20294;&#23427;&#22312;&#22870;&#21169;&#20998;&#37197;&#19978;&#24448;&#24448;&#38656;&#35201;&#26356;&#39640;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;&#26412;&#25991;&#31361;&#20986;&#20102;RND&#20013;&#30340;&#8220;&#22870;&#21169;&#19981;&#19968;&#33268;&#8221;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#20102;&#20854;&#20027;&#35201;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#24067;&#24335;RND&#65288;DRND&#65289;&#65292;&#23427;&#26159;RND&#30340;&#19968;&#20010;&#21464;&#20307;&#12290;DRND&#36890;&#36807;&#33976;&#39311;&#38543;&#26426;&#32593;&#32476;&#30340;&#20998;&#24067;&#24182;&#38544;&#24335;&#22320;&#34701;&#20837;&#20266;&#35745;&#25968;&#26469;&#25913;&#36827;&#22870;&#21169;&#20998;&#37197;&#30340;&#31934;&#24230;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#25506;&#32034;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#32780;&#19981;&#20250;&#24341;&#20837;&#26174;&#33879;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#22343;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration remains a critical issue in deep reinforcement learning for an agent to attain high returns in unknown environments. Although the prevailing exploration Random Network Distillation (RND) algorithm has been demonstrated to be effective in numerous environments, it often needs more discriminative power in bonus allocation. This paper highlights the ``bonus inconsistency'' issue within RND, pinpointing its primary limitation. To address this issue, we introduce the Distributional RND (DRND), a derivative of the RND. DRND enhances the exploration process by distilling a distribution of random networks and implicitly incorporating pseudo counts to improve the precision of bonus allocation. This refinement encourages agents to engage in more extensive exploration. Our method effectively mitigates the inconsistency issue without introducing significant computational overhead. Both theoretical analysis and experimental results demonstrate the superiority of our approach over the or
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#23454;&#29616;&#25968;&#23383;&#23402;&#29983;(DT)&#30340;&#28508;&#21147;&#12290;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26080;&#32593;&#26684;&#26694;&#26550;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;PINNs&#22312;&#21442;&#25968;&#21270;&#30340;Navier-Stokes&#26041;&#31243;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#22810;&#20445;&#30495;&#24230;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.08667</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65306;&#25968;&#23383;&#23402;&#29983;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Physics-Informed Neural Networks: A Digital Twin Perspective. (arXiv:2401.08667v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08667
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#23454;&#29616;&#25968;&#23383;&#23402;&#29983;(DT)&#30340;&#28508;&#21147;&#12290;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26080;&#32593;&#26684;&#26694;&#26550;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;PINNs&#22312;&#21442;&#25968;&#21270;&#30340;Navier-Stokes&#26041;&#31243;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#22810;&#20445;&#30495;&#24230;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#19981;&#21516;&#35282;&#24230;&#25506;&#32034;&#20102;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#23454;&#29616;&#25968;&#23383;&#23402;&#29983;(DT)&#30340;&#28508;&#21147;&#12290;&#39318;&#20808;&#65292;&#30740;&#31350;&#20102;&#29992;&#20110;&#37197;&#28857;&#30340;&#21508;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#39564;&#35777;&#23427;&#20204;&#22312;&#26080;&#32593;&#26684;&#26694;&#26550;&#30340;PINNs&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#33258;&#21160;&#26500;&#24314;&#34394;&#25311;&#34920;&#31034;&#65292;&#26080;&#38656;&#25163;&#21160;&#29983;&#25104;&#32593;&#26684;&#12290;&#28982;&#21518;&#65292;&#26816;&#39564;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;PINNs(DD-PINNs)&#26694;&#26550;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#21033;&#29992;&#22312;DT&#22330;&#26223;&#20013;&#33719;&#24471;&#30340;&#25968;&#25454;&#38598;&#12290;&#20854;&#23545;&#21442;&#25968;&#21270;&#30340;Navier-Stokes&#26041;&#31243;&#30340;&#26356;&#19968;&#33324;&#29289;&#29702;&#24615;&#30340;&#21487;&#25193;&#23637;&#24615;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#20854;&#20013;PINNs&#22312;&#38647;&#35834;&#25968;&#21464;&#21270;&#26102;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23454;&#38469;&#19978;&#25968;&#25454;&#38598;&#32463;&#24120;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#20445;&#30495;&#24230;/&#31232;&#30095;&#24230;&#19979;&#25910;&#38598;&#65292;&#36824;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#22810;&#20445;&#30495;&#24230;&#30340;DD-PINNs&#12290;&#23427;&#20204;&#22312;&#22806;&#25512;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#30456;&#23545;&#20110;&#21333;&#20445;&#30495;&#24230;&#26041;&#27861;&#25552;&#39640;&#20102;42&#65285;&#21040;62&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the potential of physics-informed neural networks (PINNs) for the realization of digital twins (DT) from various perspectives. First, various adaptive sampling approaches for collocation points are investigated to verify their effectiveness in the mesh-free framework of PINNs, which allows automated construction of virtual representation without manual mesh generation. Then, the overall performance of the data-driven PINNs (DD-PINNs) framework is examined, which can utilize the acquired datasets in DT scenarios. Its scalability to more general physics is validated within parametric Navier-Stokes equations, where PINNs do not need to be retrained as the Reynolds number varies. In addition, since datasets can be often collected from different fidelity/sparsity in practice, multi-fidelity DD-PINNs are also proposed and evaluated. They show remarkable prediction performance even in the extrapolation tasks, with $42\sim62\%$ improvement over the single-fidelity approach.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#25945;&#23398;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36845;&#20195;&#26426;&#22120;&#25945;&#23398;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05467</link><description>&lt;p&gt;
&#22522;&#20110;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#26426;&#22120;&#25945;&#23398;
&lt;/p&gt;
&lt;p&gt;
Machine Teaching for Building Modular AI Agents based on Zero-shot Learners. (arXiv:2401.05467v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05467
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#25945;&#23398;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36845;&#20195;&#26426;&#22120;&#25945;&#23398;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#23548;&#33268;&#20102;&#35768;&#22810;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#21019;&#24314;&#12290;&#36825;&#20123;&#20195;&#29702;&#20351;&#29992;LLMs&#20316;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#65292;&#22312;&#20154;&#31867;&#29992;&#25143;&#35774;&#23450;&#30340;&#22797;&#26434;&#20219;&#21153;&#20013;&#25191;&#34892;&#23376;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22686;&#24378;&#21033;&#29992;LLMs&#20316;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#36845;&#20195;&#26426;&#22120;&#25945;&#23398;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#30340;&#20154;&#31867;&#21453;&#39304;&#19979;&#36880;&#28176;&#25945;&#23548;AI&#20195;&#29702;&#30340;&#39640;&#25928;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#38646;&#26679;&#26412;&#23398;&#20064;&#36136;&#37327;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20027;&#24352;&#21033;&#29992;&#21021;&#22987;&#37096;&#32626;&#30340;&#25968;&#25454;&#36861;&#36394;&#20197;&#21450;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#36755;&#20986;&#25110;&#27880;&#37322;&#26469;&#35757;&#32451;&#26356;&#23567;&#19988;&#20219;&#21153;&#29305;&#23450;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#21487;&#20197;&#20943;&#23569;&#32463;&#27982;&#25104;&#26412;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26426;&#22120;&#25945;&#23398;&#36807;&#31243;&#21033;&#29992;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#26469;&#32416;&#27491;&#39640;&#27010;&#29575;&#35823;&#26631;&#27880;&#30340;&#31034;&#20363;&#12290;&#22312;&#19977;&#20010;&#24120;&#35265;&#23545;&#35805;AI&#20195;&#29702;&#20219;&#21153;&#19978;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25509;&#36817;&#29702;&#24819;&#24615;&#33021;&#21487;&#20197;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in large language models (LLMs) have led to the creation of many modular AI agents. These agents employ LLMs as zero-shot learners to perform sub-tasks in order to solve complex tasks set forth by human users. We propose an approach to enhance the robustness and performance of modular AI agents that utilize LLMs as zero-shot learners. Our iterative machine teaching method offers an efficient way to teach AI agents over time with limited human feedback, addressing the limit posed by the quality of zero-shot learning. We advocate leveraging the data traces from initial deployments and outputs or annotations from the zero-shot learners to train smaller and task-specific substitute models which can reduce both the monetary costs and environmental impact. Our machine teaching process avails human expertise to correct examples with a high likelihood of misannotations. Results on three tasks, common to conversational AI agents, show that close-to-oracle performance can be 
&lt;/p&gt;</description></item><item><title>&#26657;&#20934;&#25915;&#20987;&#26159;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#21644;&#32452;&#32455;&#25915;&#20987;&#26469;&#20351;&#21463;&#23475;&#27169;&#22411;&#22833;&#21435;&#20934;&#30830;&#26657;&#20934;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#21407;&#22987;&#20934;&#30830;&#24615;&#12290;&#36825;&#23545;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#21644;&#22522;&#20110;&#32622;&#20449;&#20998;&#25968;&#30340;&#20915;&#31574;&#26500;&#25104;&#20005;&#37325;&#23041;&#32961;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#31181;&#26657;&#20934;&#25915;&#20987;&#24418;&#24335;&#65292;&#24182;&#23545;&#24120;&#29992;&#30340;&#23545;&#25239;&#38450;&#24481;&#21644;&#26657;&#20934;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2401.02718</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#26657;&#20934;&#25915;&#20987;&#65306;&#38024;&#23545;&#26657;&#20934;&#24615;&#30340;&#23545;&#25239;&#25915;&#20987;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Calibration Attack: A Framework For Adversarial Attacks Targeting Calibration. (arXiv:2401.02718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02718
&lt;/p&gt;
&lt;p&gt;
&#26657;&#20934;&#25915;&#20987;&#26159;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#21644;&#32452;&#32455;&#25915;&#20987;&#26469;&#20351;&#21463;&#23475;&#27169;&#22411;&#22833;&#21435;&#20934;&#30830;&#26657;&#20934;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#21407;&#22987;&#20934;&#30830;&#24615;&#12290;&#36825;&#23545;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#21644;&#22522;&#20110;&#32622;&#20449;&#20998;&#25968;&#30340;&#20915;&#31574;&#26500;&#25104;&#20005;&#37325;&#23041;&#32961;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#31181;&#26657;&#20934;&#25915;&#20987;&#24418;&#24335;&#65292;&#24182;&#23545;&#24120;&#29992;&#30340;&#23545;&#25239;&#38450;&#24481;&#21644;&#26657;&#20934;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#26657;&#20934;&#25915;&#20987;&#30340;&#26032;&#23545;&#25239;&#25915;&#20987;&#26694;&#26550;&#65292;&#20854;&#20013;&#25915;&#20987;&#34987;&#29983;&#25104;&#21644;&#32452;&#32455;&#20197;&#20351;&#21463;&#23475;&#27169;&#22411;&#22833;&#21435;&#20934;&#30830;&#26657;&#20934;&#65292;&#21516;&#26102;&#19981;&#25913;&#21464;&#20854;&#21407;&#22987;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#20005;&#37325;&#21361;&#21450;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#21644;&#22522;&#20110;&#20854;&#32622;&#20449;&#20998;&#25968;&#30340;&#20219;&#20309;&#20915;&#31574;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22235;&#31181;&#26032;&#22411;&#26657;&#20934;&#25915;&#20987;&#24418;&#24335;&#65306;&#20302;&#32622;&#20449;&#25915;&#20987;&#12289;&#39640;&#32622;&#20449;&#25915;&#20987;&#12289;&#26368;&#22823;&#22833;&#30495;&#25915;&#20987;&#21644;&#38543;&#26426;&#32622;&#20449;&#25915;&#20987;&#65292;&#36866;&#29992;&#20110;&#30333;&#30418;&#21644;&#40657;&#30418;&#35774;&#32622;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#23545;&#20856;&#22411;&#30340;&#21463;&#23475;&#27169;&#22411;&#36827;&#34892;&#20102;&#36825;&#20123;&#26032;&#22411;&#25915;&#20987;&#30340;&#27979;&#35797;&#65292;&#35777;&#26126;&#21363;&#20351;&#21482;&#36827;&#34892;&#30456;&#23545;&#36739;&#23569;&#30340;&#26597;&#35810;&#65292;&#25915;&#20987;&#20063;&#33021;&#36896;&#25104;&#37325;&#22823;&#30340;&#26657;&#20934;&#38169;&#35823;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#20998;&#26512;&#20197;&#20102;&#35299;&#26657;&#20934;&#25915;&#20987;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#23545;&#25239;&#38450;&#24481;&#21644;&#26657;&#20934;&#26041;&#27861;&#23545;&#36825;&#20123;&#25915;&#20987;&#31867;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new framework of adversarial attacks, named calibration attacks, in which the attacks are generated and organized to trap victim models to be miscalibrated without altering their original accuracy, hence seriously endangering the trustworthiness of the models and any decision-making based on their confidence scores. Specifically, we identify four novel forms of calibration attacks: underconfidence attacks, overconfidence attacks, maximum miscalibration attacks, and random confidence attacks, in both the black-box and white-box setups. We then test these new attacks on typical victim models with comprehensive datasets, demonstrating that even with a relatively low number of queries, the attacks can create significant calibration mistakes. We further provide detailed analyses to understand different aspects of calibration attacks. Building on that, we investigate the effectiveness of widely used adversarial defences and calibration methods against these types of attacks, w
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#39640;&#25928;&#36801;&#31227;&#27979;&#35797;&#38598;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#29992;&#25143;&#24863;&#20852;&#36259;&#30340;&#23646;&#24615;&#30340;&#33391;&#22909;&#27979;&#35797;&#38598;&#65292;&#20197;&#36798;&#21040;&#25913;&#21892;&#21487;&#39564;&#35777;&#24615;&#21644;&#27979;&#35797;&#24615;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2311.00801</link><description>&lt;p&gt;
GIST: &#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#29983;&#25104;&#36755;&#20837;&#38598;&#21512;&#30340;&#21487;&#36801;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
GIST: Generated Inputs Sets Transferability in Deep Learning. (arXiv:2311.00801v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00801
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#39640;&#25928;&#36801;&#31227;&#27979;&#35797;&#38598;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#29992;&#25143;&#24863;&#20852;&#36259;&#30340;&#23646;&#24615;&#30340;&#33391;&#22909;&#27979;&#35797;&#38598;&#65292;&#20197;&#36798;&#21040;&#25913;&#21892;&#21487;&#39564;&#35777;&#24615;&#21644;&#27979;&#35797;&#24615;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#31070;&#32463;&#32593;&#32476;&#21487;&#39564;&#35777;&#24615;&#21644;&#21487;&#27979;&#35797;&#24615;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#29983;&#25104;&#27979;&#35797;&#38598;&#30340;&#26041;&#27861;&#34987;&#24320;&#21457;&#20986;&#26469;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#20013;&#30340;&#27599;&#19968;&#31181;&#37117;&#20542;&#21521;&#20110;&#24378;&#35843;&#29305;&#23450;&#30340;&#27979;&#35797;&#26041;&#38754;&#65292;&#24182;&#19988;&#21487;&#33021;&#38750;&#24120;&#32791;&#26102;&#12290;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#31616;&#21333;&#35299;&#20915;&#26041;&#26696;&#26159;&#26681;&#25454;&#24076;&#26395;&#36801;&#31227;&#30340;&#26399;&#26395;&#23646;&#24615;&#65292;&#22312;&#19968;&#20123;&#32463;&#36807;&#22522;&#20934;&#27979;&#35797;&#30340;&#27169;&#22411;&#21644;&#26032;&#27979;&#35797;&#27169;&#22411;&#20043;&#38388;&#36716;&#31227;&#27979;&#35797;&#38598;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;GIST&#65288;&#29983;&#25104;&#36755;&#20837;&#38598;&#21512;&#30340;&#21487;&#36801;&#31227;&#24615;&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#39640;&#25928;&#36801;&#31227;&#27979;&#35797;&#38598;&#30340;&#26032;&#26041;&#27861;&#12290;&#32473;&#23450;&#29992;&#25143;&#24076;&#26395;&#36801;&#31227;&#30340;&#19968;&#20010;&#24863;&#20852;&#36259;&#30340;&#23646;&#24615;&#65288;&#20363;&#22914;&#65292;&#35206;&#30422;&#20934;&#21017;&#65289;&#65292;GIST&#33021;&#22815;&#20174;&#22522;&#20934;&#25552;&#20379;&#30340;&#21487;&#29992;&#27979;&#35797;&#38598;&#20013;&#65292;&#20174;&#35813;&#23646;&#24615;&#30340;&#35282;&#24230;&#36873;&#25321;&#33391;&#22909;&#30340;&#27979;&#35797;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#27169;&#24577;&#21644;&#19981;&#21516;&#30340;&#27979;&#35797;&#38598;&#29983;&#25104;&#36807;&#31243;&#65292;&#22312;&#25925;&#38556;&#31867;&#22411;&#35206;&#30422;&#23646;&#24615;&#19978;&#23545;GIST&#36827;&#34892;&#32463;&#39564;&#35780;&#20272;&#65292;&#20197;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the demand for verifiability and testability of neural networks continues to rise, an increasing number of methods for generating test sets are being developed. However, each of these techniques tends to emphasize specific testing aspects and can be quite time-consuming. A straightforward solution to mitigate this issue is to transfer test sets between some benchmarked models and a new model under test, based on a desirable property one wishes to transfer. This paper introduces GIST (Generated Inputs Sets Transferability), a novel approach for the efficient transfer of test sets among Deep Learning models. Given a property of interest that a user wishes to transfer (e.g., coverage criterion), GIST enables the selection of good test sets from the point of view of this property among available ones from a benchmark. We empirically evaluate GIST on fault types coverage property with two modalities and different test set generation procedures to demonstrate the approach's feasibility. E
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#20154;&#31867;&#35760;&#24518;&#26426;&#21046;&#21551;&#21457;&#30340;&#31070;&#32463;&#27169;&#22359;&#65292;&#27169;&#25311;&#20154;&#31867;&#21644;&#26426;&#22120;&#22914;&#20309;&#23545;&#24403;&#21069;&#36755;&#20837;&#36827;&#34892;&#20851;&#32852;&#25512;&#29702;&#21644;&#38382;&#31572;&#65292;&#24182;&#23558;&#20854;&#19982;&#36807;&#21435;&#30340;&#35760;&#24518;&#32467;&#21512;&#36215;&#26469;&#12290;&#36890;&#36807;&#24863;&#30693;&#12289;&#35760;&#24518;&#21644;&#25512;&#29702;&#32452;&#20214;&#65292;&#35813;&#27169;&#22359;&#23454;&#29616;&#20102;&#24863;&#30693;&#26356;&#26032;&#12289;&#35760;&#24518;&#34701;&#21512;&#21644;&#20449;&#24687;&#26816;&#32034;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09297</link><description>&lt;p&gt;
&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#35748;&#30693;&#65306;&#21463;&#20154;&#31867;&#35760;&#24518;&#26426;&#21046;&#21551;&#21457;&#30340;&#25512;&#29702;&#31070;&#32463;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Understanding AI Cognition: A Neural Module for Inference Inspired by Human Memory Mechanisms. (arXiv:2310.09297v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09297
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#20154;&#31867;&#35760;&#24518;&#26426;&#21046;&#21551;&#21457;&#30340;&#31070;&#32463;&#27169;&#22359;&#65292;&#27169;&#25311;&#20154;&#31867;&#21644;&#26426;&#22120;&#22914;&#20309;&#23545;&#24403;&#21069;&#36755;&#20837;&#36827;&#34892;&#20851;&#32852;&#25512;&#29702;&#21644;&#38382;&#31572;&#65292;&#24182;&#23558;&#20854;&#19982;&#36807;&#21435;&#30340;&#35760;&#24518;&#32467;&#21512;&#36215;&#26469;&#12290;&#36890;&#36807;&#24863;&#30693;&#12289;&#35760;&#24518;&#21644;&#25512;&#29702;&#32452;&#20214;&#65292;&#35813;&#27169;&#22359;&#23454;&#29616;&#20102;&#24863;&#30693;&#26356;&#26032;&#12289;&#35760;&#24518;&#34701;&#21512;&#21644;&#20449;&#24687;&#26816;&#32034;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21644;&#26426;&#22120;&#22914;&#20309;&#23558;&#24403;&#21069;&#30340;&#36755;&#20837;&#19982;&#36807;&#21435;&#30340;&#35760;&#24518;&#32467;&#21512;&#36215;&#26469;&#65292;&#36827;&#34892;&#20851;&#32852;&#25512;&#29702;&#21644;&#38382;&#31572;&#65292;&#24182;&#23558;&#24863;&#30693;&#21040;&#30340;&#20449;&#24687;&#32622;&#20110;&#19978;&#19979;&#25991;&#20013;&#65292;&#36825;&#26159;&#35748;&#30693;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35868;&#39064;&#12290;&#21463;&#21040;&#20154;&#33041;&#35760;&#24518;&#31995;&#32479;&#21644;&#35748;&#30693;&#32467;&#26500;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#24863;&#30693;&#12289;&#35760;&#24518;&#21644;&#25512;&#29702;&#32452;&#20214;&#30340;PMI&#26694;&#26550;&#12290;&#29305;&#21035;&#22320;&#65292;&#35760;&#24518;&#27169;&#22359;&#21253;&#25324;&#24037;&#20316;&#35760;&#24518;&#21644;&#38271;&#26399;&#35760;&#24518;&#65292;&#20854;&#20013;&#21518;&#32773;&#20855;&#26377;&#26356;&#39640;&#38454;&#30340;&#32467;&#26500;&#26469;&#20445;&#30041;&#26356;&#22810;&#30340;&#32047;&#31215;&#30693;&#35782;&#21644;&#32463;&#39564;&#12290;&#36890;&#36807;&#21487;&#21306;&#20998;&#30340;&#31454;&#20105;&#20889;&#20837;&#35775;&#38382;&#65292;&#24403;&#21069;&#30340;&#24863;&#30693;&#26356;&#26032;&#24037;&#20316;&#35760;&#24518;&#65292;&#20043;&#21518;&#36890;&#36807;&#22806;&#31215;&#20851;&#32852;&#19982;&#38271;&#26399;&#35760;&#24518;&#34701;&#21512;&#65292;&#36991;&#20813;&#20869;&#23384;&#28322;&#20986;&#24182;&#26368;&#23567;&#21270;&#20449;&#24687;&#20914;&#31361;&#12290;&#22312;&#25512;&#29702;&#27169;&#22359;&#20013;&#65292;&#30456;&#20851;&#20449;&#24687;&#20174;&#20004;&#20010;&#21333;&#29420;&#30340;&#35760;&#24518;&#28304;&#26816;&#32034;&#24182;&#32467;&#21512;&#65292;&#20197;&#33719;&#24471;&#26356;&#20840;&#38754;&#21644;&#31934;&#30830;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
How humans and machines make sense of current inputs for relation reasoning and question-answering while putting the perceived information into context of our past memories, has been a challenging conundrum in cognitive science and artificial intelligence. Inspired by human brain's memory system and cognitive architectures, we propose a PMI framework that consists of perception, memory and inference components. Notably, the memory module comprises working and long-term memory, with the latter endowed with a higher-order structure to retain more accumulated knowledge and experiences. Through a differentiable competitive write access, current perceptions update working memory, which is later merged with long-term memory via outer product associations, averting memory overflow and minimizing information conflicts. In the inference module, relevant information is retrieved from two separate memory origins and associatively integrated to attain a more comprehensive and precise interpretatio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#19968;&#20010;&#39044;&#27979;&#20154;&#21592;&#25110;&#23478;&#24237;&#26159;&#21542;&#20250;&#22312;&#25509;&#19979;&#26469;&#30340;&#20004;&#24180;&#20869;&#25644;&#36801;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21033;&#29992;&#27169;&#22411;&#30340;&#39044;&#27979;&#20197;&#21450;&#20844;&#24320;&#30340;&#35757;&#32451;&#25968;&#25454;&#36793;&#38469;&#20998;&#24067;&#26469;&#25512;&#26029;&#30446;&#26631;&#20010;&#20307;&#30340;&#25935;&#24863;&#23646;&#24615;&#20540;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#29992;&#21512;&#25104;&#25968;&#25454;&#26367;&#20195;&#21407;&#22987;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#23545;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.08775</link><description>&lt;p&gt;
&#24403;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27844;&#28431;&#65306;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
When Machine Learning Models Leak: An Exploration of Synthetic Training Data. (arXiv:2310.08775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#19968;&#20010;&#39044;&#27979;&#20154;&#21592;&#25110;&#23478;&#24237;&#26159;&#21542;&#20250;&#22312;&#25509;&#19979;&#26469;&#30340;&#20004;&#24180;&#20869;&#25644;&#36801;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21033;&#29992;&#27169;&#22411;&#30340;&#39044;&#27979;&#20197;&#21450;&#20844;&#24320;&#30340;&#35757;&#32451;&#25968;&#25454;&#36793;&#38469;&#20998;&#24067;&#26469;&#25512;&#26029;&#30446;&#26631;&#20010;&#20307;&#30340;&#25935;&#24863;&#23646;&#24615;&#20540;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#29992;&#21512;&#25104;&#25968;&#25454;&#26367;&#20195;&#21407;&#22987;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#23545;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25915;&#20987;&#65292;&#35813;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#19968;&#20010;&#20154;&#25110;&#23478;&#24237;&#22312;&#25509;&#19979;&#26469;&#30340;&#20004;&#24180;&#20869;&#26159;&#21542;&#20250;&#25644;&#36801;&#65292;&#21363;&#36801;&#31227;&#20542;&#21521;&#20998;&#31867;&#22120;&#12290;&#25915;&#20987;&#20551;&#35774;&#25915;&#20987;&#32773;&#21487;&#20197;&#26597;&#35810;&#27169;&#22411;&#20197;&#33719;&#21462;&#39044;&#27979;&#65292;&#24182;&#19988;&#27169;&#22411;&#35757;&#32451;&#26102;&#20351;&#29992;&#30340;&#25968;&#25454;&#30340;&#36793;&#38469;&#20998;&#24067;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;&#25915;&#20987;&#36824;&#20551;&#35774;&#25915;&#20987;&#32773;&#24050;&#32463;&#33719;&#21462;&#20102;&#19968;&#23450;&#25968;&#37327;&#30446;&#26631;&#20010;&#20307;&#30340;&#38750;&#25935;&#24863;&#23646;&#24615;&#20540;&#12290;&#25915;&#20987;&#30340;&#30446;&#26631;&#26159;&#25512;&#26029;&#36825;&#20123;&#30446;&#26631;&#20010;&#20307;&#30340;&#25935;&#24863;&#23646;&#24615;&#20540;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#35757;&#32451;&#27169;&#22411;&#26102;&#29992;&#21512;&#25104;&#25968;&#25454;&#26367;&#20195;&#21407;&#22987;&#25968;&#25454;&#23545;&#25915;&#20987;&#32773;&#25104;&#21151;&#25512;&#26029;&#25935;&#24863;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate an attack on a machine learning model that predicts whether a person or household will relocate in the next two years, i.e., a propensity-to-move classifier. The attack assumes that the attacker can query the model to obtain predictions and that the marginal distribution of the data on which the model was trained is publicly available. The attack also assumes that the attacker has obtained the values of non-sensitive attributes for a certain number of target individuals. The objective of the attack is to infer the values of sensitive attributes for these target individuals. We explore how replacing the original data with synthetic data when training the model impacts how successfully the attacker can infer sensitive attributes.\footnote{Original paper published at PSD 2022. The paper was subsequently updated.}
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35266;&#23519;&#25193;&#25955;&#21644;&#20449;&#24687;&#20998;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#32454;&#31890;&#24230;&#20851;&#31995;&#65292;&#36827;&#19968;&#27493;&#35299;&#20915;&#20102;&#39640;&#32500;&#31354;&#38388;&#20013;&#20449;&#24687;&#25658;&#24102;&#21464;&#37327;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07972</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#20449;&#24687;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Interpretable Diffusion via Information Decomposition. (arXiv:2310.07972v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35266;&#23519;&#25193;&#25955;&#21644;&#20449;&#24687;&#20998;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#32454;&#31890;&#24230;&#20851;&#31995;&#65292;&#36827;&#19968;&#27493;&#35299;&#20915;&#20102;&#39640;&#32500;&#31354;&#38388;&#20013;&#20449;&#24687;&#25658;&#24102;&#21464;&#37327;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29992;&#20110;&#22797;&#26434;&#20851;&#31995;&#30340;&#26465;&#20214;&#29983;&#25104;&#21644;&#23494;&#24230;&#24314;&#27169;&#65292;&#22914;&#22270;&#20687;&#21644;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#21040;&#30340;&#20851;&#31995;&#30340;&#26412;&#36136;&#26159;&#19981;&#36879;&#26126;&#30340;&#65292;&#22240;&#27492;&#24456;&#38590;&#20934;&#30830;&#29702;&#35299;&#21333;&#35789;&#21644;&#22270;&#20687;&#37096;&#20998;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25110;&#32773;&#39044;&#27979;&#24178;&#39044;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#35266;&#23519;&#25193;&#25955;&#21644;&#20449;&#24687;&#20998;&#35299;&#20043;&#38388;&#30340;&#31934;&#30830;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#32454;&#31890;&#24230;&#20851;&#31995;&#12290;&#20114;&#20449;&#24687;&#21644;&#26465;&#20214;&#20114;&#20449;&#24687;&#30340;&#31934;&#30830;&#34920;&#36798;&#21487;&#20197;&#36890;&#36807;&#21435;&#22122;&#27169;&#22411;&#26469;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#20063;&#21487;&#20197;&#36731;&#26494;&#20272;&#35745;&#22312;&#29305;&#23450;&#22270;&#20687;&#21644;&#26631;&#39064;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36827;&#19968;&#27493;&#23545;&#20449;&#24687;&#36827;&#34892;&#20998;&#35299;&#65292;&#20197;&#29702;&#35299;&#39640;&#32500;&#31354;&#38388;&#20013;&#21738;&#20123;&#21464;&#37327;&#25658;&#24102;&#20449;&#24687;&#65292;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#38750;&#36127;&#20449;&#24687;&#20998;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models enable conditional generation and density modeling of complex relationships like images and text. However, the nature of the learned relationships is opaque making it difficult to understand precisely what relationships between words and parts of an image are captured, or to predict the effect of an intervention. We illuminate the fine-grained relationships learned by diffusion models by noticing a precise relationship between diffusion and information decomposition. Exact expressions for mutual information and conditional mutual information can be written in terms of the denoising model. Furthermore, pointwise estimates can be easily estimated as well, allowing us to ask questions about the relationships between specific images and captions. Decomposing information even further to understand which variables in a high-dimensional space carry information is a long-standing problem. For diffusion models, we show that a natural non-negative decomposition of mutu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22609;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#23545;&#20445;&#25345;&#21487;&#22609;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#25439;&#22833;&#26159;&#39640;&#25928;&#35757;&#32451;&#30340;&#20027;&#35201;&#38480;&#21046;&#22240;&#32032;&#65292;&#24182;&#19988;&#26410;&#21450;&#26102;&#24674;&#22797;&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#23558;&#23548;&#33268;&#28798;&#38590;&#24615;&#32467;&#26524;&#12290;&#36825;&#20026;&#35299;&#20915;&#39640;&#37325;&#25918;&#27604;&#22256;&#22659;&#25552;&#20379;&#20102;&#26032;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.07418</link><description>&lt;p&gt;
&#37325;&#23457;&#35270;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22609;&#24615;&#65306;&#25968;&#25454;&#12289;&#27169;&#22359;&#21644;&#35757;&#32451;&#38454;&#27573;
&lt;/p&gt;
&lt;p&gt;
Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages. (arXiv:2310.07418v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22609;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#23545;&#20445;&#25345;&#21487;&#22609;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#25439;&#22833;&#26159;&#39640;&#25928;&#35757;&#32451;&#30340;&#20027;&#35201;&#38480;&#21046;&#22240;&#32032;&#65292;&#24182;&#19988;&#26410;&#21450;&#26102;&#24674;&#22797;&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#23558;&#23548;&#33268;&#28798;&#38590;&#24615;&#32467;&#26524;&#12290;&#36825;&#20026;&#35299;&#20915;&#39640;&#37325;&#25918;&#27604;&#22256;&#22659;&#25552;&#20379;&#20102;&#26032;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#22609;&#24615;&#65292;&#31070;&#32463;&#32593;&#32476;&#38543;&#26032;&#25968;&#25454;&#28436;&#36827;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#39640;&#24615;&#33021;&#21644;&#26679;&#26412;&#39640;&#25928;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;(VRL)&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#37325;&#32622;&#21644;&#27491;&#21017;&#21270;&#31561;&#26041;&#27861;&#21487;&#33021;&#33021;&#22815;&#32531;&#35299;&#21487;&#22609;&#24615;&#25439;&#22833;&#65292;&#20294;VRL&#26694;&#26550;&#20869;&#21508;&#31181;&#32452;&#20214;&#23545;&#20195;&#29702;&#30340;&#21487;&#22609;&#24615;&#30340;&#24433;&#21709;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#32463;&#39564;&#24615;&#25506;&#32034;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#19977;&#20010;&#20027;&#35201;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#26041;&#38754;&#65292;&#24182;&#24471;&#20986;&#20197;&#19979;&#26377;&#28145;&#20837;&#35265;&#35299;&#30340;&#32467;&#35770;&#65306;(1)&#25968;&#25454;&#22686;&#24378;&#23545;&#20110;&#20445;&#25345;&#21487;&#22609;&#24615;&#33267;&#20851;&#37325;&#35201;&#65307;(2)&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#25439;&#22833;&#26159;&#38459;&#30861;&#39640;&#25928;&#35757;&#32451;&#30340;&#20027;&#35201;&#29942;&#39048;&#65307;(3)&#22312;&#26089;&#26399;&#38454;&#27573;&#27809;&#26377;&#21450;&#26102;&#24178;&#39044;&#20197;&#24674;&#22797;&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#65292;&#20854;&#25439;&#22833;&#23558;&#21464;&#24471;&#28798;&#38590;&#24615;&#12290;&#36825;&#20123;&#35265;&#35299;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#23545;&#39640;&#37325;&#25918;&#27604;&#65288;RR&#65289;&#22256;&#22659;&#30340;&#26032;&#31574;&#30053;&#65292;&#20854;&#20013;&#21152;&#21095;&#30340;&#21487;&#22609;&#24615;&#25439;&#22833;&#22952;&#30861;&#20102;&#36890;&#36807;&#22686;&#21152;&#37325;&#25918;&#25968;&#37327;&#24102;&#26469;&#30340;&#26679;&#26412;&#25928;&#29575;&#30340;&#28508;&#22312;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plasticity, the ability of a neural network to evolve with new data, is crucial for high-performance and sample-efficient visual reinforcement learning (VRL). Although methods like resetting and regularization can potentially mitigate plasticity loss, the influences of various components within the VRL framework on the agent's plasticity are still poorly understood. In this work, we conduct a systematic empirical exploration focusing on three primary underexplored facets and derive the following insightful conclusions: (1) data augmentation is essential in maintaining plasticity; (2) the critic's plasticity loss serves as the principal bottleneck impeding efficient training; and (3) without timely intervention to recover critic's plasticity in the early stages, its loss becomes catastrophic. These insights suggest a novel strategy to address the high replay ratio (RR) dilemma, where exacerbated plasticity loss hinders the potential improvements of sample efficiency brought by increased
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;Shapley Value&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#20102;Integrated Gradients&#30340;&#22522;&#32447;&#36873;&#25321;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#32447;&#26500;&#24314;&#26041;&#27861;&#21483;&#20570;Shapley Integrated Gradients (SIG)&#12290;&#22312;GridWorl&#19978;&#30340;&#27169;&#25311;&#23454;&#39564;&#34920;&#26126;&#65292;SIG&#33021;&#22815;&#29983;&#25104;&#26377;&#24847;&#20041;&#21644;&#26080;&#20559;&#30340;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.04821</link><description>&lt;p&gt;
&#20174;Shapley Value&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;Integrated Gradients&#30340;&#22522;&#32447;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Rethink Baseline of Integrated Gradients from the Perspective of Shapley Value. (arXiv:2310.04821v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04821
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;Shapley Value&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#20102;Integrated Gradients&#30340;&#22522;&#32447;&#36873;&#25321;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#32447;&#26500;&#24314;&#26041;&#27861;&#21483;&#20570;Shapley Integrated Gradients (SIG)&#12290;&#22312;GridWorl&#19978;&#30340;&#27169;&#25311;&#23454;&#39564;&#34920;&#26126;&#65292;SIG&#33021;&#22815;&#29983;&#25104;&#26377;&#24847;&#20041;&#21644;&#26080;&#20559;&#30340;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26041;&#27861;&#24050;&#32463;&#23581;&#35797;&#36890;&#36807;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#39044;&#27979;&#24402;&#22240;&#20110;&#20854;&#36755;&#20837;&#29305;&#24449;&#26469;&#35299;&#37322;DNN&#12290;&#20854;&#20013;&#19968;&#20010;&#30740;&#31350;&#20805;&#20998;&#30340;&#24402;&#22240;&#26041;&#27861;&#26159;Integrated Gradients&#65288;IG&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36873;&#25321;IG&#30340;&#22522;&#32447;&#26159;&#22312;&#19981;&#21516;&#24773;&#26223;&#19979;&#29983;&#25104;&#26377;&#24847;&#20041;&#21644;&#26080;&#20559;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#30340;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21033;&#29992;&#21333;&#19968;&#22522;&#32447;&#30340;&#20570;&#27861;&#26410;&#33021;&#23454;&#29616;&#36825;&#20010;&#24895;&#26395;&#65292;&#22240;&#27492;&#38656;&#35201;&#22810;&#20010;&#22522;&#32447;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;IG&#19982;&#22885;&#26364;&#8212;&#22799;&#26222;&#21033;&#65288;Aumann-Shapley&#65289;&#20215;&#20540;&#20043;&#38388;&#30340;&#20869;&#22312;&#32852;&#31995;&#24418;&#25104;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35270;&#35282;&#65292;&#37325;&#26032;&#24605;&#32771;&#20102;&#22522;&#32447;&#30340;&#35774;&#35745;&#12290;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20986;&#19968;&#32452;&#22522;&#32447;&#19982;Shapley Value&#20013;&#30340;&#32852;&#30431;&#30456;&#23545;&#24212;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#32447;&#26500;&#24314;&#26041;&#27861;&#65292;&#31216;&#20026;Shapley Integrated Gradients&#65288;SIG&#65289;&#65292;&#36890;&#36807;&#27604;&#20363;&#25277;&#26679;&#26469;&#25628;&#32034;&#19968;&#32452;&#22522;&#32447;&#65292;&#20197;&#37096;&#20998;&#27169;&#25311;Shapley Value&#30340;&#35745;&#31639;&#36335;&#24452;&#12290;&#22312;GridWorl&#19978;&#36827;&#34892;&#20102;&#27169;&#25311;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous approaches have attempted to interpret deep neural networks (DNNs) by attributing the prediction of DNN to its input features. One of the well-studied attribution methods is Integrated Gradients (IG). Specifically, the choice of baselines for IG is a critical consideration for generating meaningful and unbiased explanations for model predictions in different scenarios. However, current practice of exploiting a single baseline fails to fulfill this ambition, thus demanding multiple baselines. Fortunately, the inherent connection between IG and Aumann-Shapley Value forms a unique perspective to rethink the design of baselines. Under certain hypothesis, we theoretically analyse that a set of baseline aligns with the coalitions in Shapley Value. Thus, we propose a novel baseline construction method called Shapley Integrated Gradients (SIG) that searches for a set of baselines by proportional sampling to partly simulate the computation path of Shapley Value. Simulations on GridWorl
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25972;&#21512;&#22522;&#20110;&#32467;&#26500;&#30340;&#23545;&#25509;&#21644;&#22522;&#20110;&#24207;&#21015;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#20803;&#27169;&#22411;&#26694;&#26550;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#37197;&#20307;-&#34507;&#30333;&#36136;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03946</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#27169;&#22411;&#25913;&#36827;&#20102;&#37197;&#20307;-&#34507;&#30333;&#36136;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improved prediction of ligand-protein binding affinities by meta-modeling. (arXiv:2310.03946v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03946
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#22522;&#20110;&#32467;&#26500;&#30340;&#23545;&#25509;&#21644;&#22522;&#20110;&#24207;&#21015;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#20803;&#27169;&#22411;&#26694;&#26550;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#37197;&#20307;-&#34507;&#30333;&#36136;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#20934;&#30830;&#31579;&#36873;&#20505;&#36873;&#33647;&#29289;&#37197;&#20307;&#19982;&#38774;&#34507;&#30333;&#30340;&#32467;&#21512;&#26159;&#33647;&#29289;&#24320;&#21457;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#65292;&#22240;&#20026;&#31579;&#36873;&#28508;&#22312;&#20505;&#36873;&#29289;&#33021;&#22815;&#33410;&#30465;&#25214;&#33647;&#29289;&#30340;&#26102;&#38388;&#21644;&#36153;&#29992;&#12290;&#36825;&#31181;&#34394;&#25311;&#31579;&#36873;&#37096;&#20998;&#20381;&#36182;&#20110;&#39044;&#27979;&#37197;&#20307;&#21644;&#34507;&#30333;&#36136;&#20043;&#38388;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#26041;&#27861;&#12290;&#37492;&#20110;&#23384;&#22312;&#35768;&#22810;&#35745;&#31639;&#27169;&#22411;&#23545;&#19981;&#21516;&#30446;&#26631;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#32467;&#26524;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#24320;&#21457;&#20102;&#19968;&#20010;&#20803;&#27169;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#24050;&#21457;&#34920;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#23545;&#25509;&#21644;&#22522;&#20110;&#24207;&#21015;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#26500;&#24314;&#12290;&#22312;&#26500;&#24314;&#36825;&#20010;&#26694;&#26550;&#26102;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#35768;&#22810;&#32452;&#21512;&#30340;&#20010;&#21035;&#27169;&#22411;&#12289;&#35757;&#32451;&#25968;&#25454;&#24211;&#20197;&#21450;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#30340;&#20803;&#27169;&#22411;&#26041;&#27861;&#12290;&#25105;&#20204;&#26174;&#31034;&#20986;&#35768;&#22810;&#20803;&#27169;&#22411;&#22312;&#20146;&#21644;&#21147;&#39044;&#27979;&#19978;&#26174;&#33879;&#25913;&#21892;&#20102;&#20010;&#21035;&#22522;&#30784;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#20803;&#27169;&#22411;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#32431;&#32467;&#26500;&#20026;&#22522;&#30784;&#30340;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#20803;&#27169;&#22411;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#37197;&#20307;-&#34507;&#30333;&#36136;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate screening of candidate drug ligands against target proteins through computational approaches is of prime interest to drug development efforts, as filtering potential candidates would save time and expenses for finding drugs. Such virtual screening depends in part on methods to predict the binding affinity between ligands and proteins. Given many computational models for binding affinity prediction with varying results across targets, we herein develop a meta-modeling framework by integrating published empirical structure-based docking and sequence-based deep learning models. In building this framework, we evaluate many combinations of individual models, training databases, and linear and nonlinear meta-modeling approaches. We show that many of our meta-models significantly improve affinity predictions over individual base models. Our best meta-models achieve comparable performance to state-of-the-art exclusively structure-based deep learning tools. Overall, we demonstrate 
&lt;/p&gt;</description></item><item><title>ComSD&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#21512;&#29702;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#21644;&#21160;&#24577;&#21152;&#26435;&#30340;&#20869;&#22312;&#22870;&#21169;&#26469;&#24179;&#34913;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#20013;&#30340;&#34892;&#20026;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17203</link><description>&lt;p&gt;
ComSD: &#22312;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#20013;&#24179;&#34913;&#34892;&#20026;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
ComSD: Balancing Behavioral Quality and Diversity in Unsupervised Skill Discovery. (arXiv:2309.17203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17203
&lt;/p&gt;
&lt;p&gt;
ComSD&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#21512;&#29702;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#21644;&#21160;&#24577;&#21152;&#26435;&#30340;&#20869;&#22312;&#22870;&#21169;&#26469;&#24179;&#34913;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#20013;&#30340;&#34892;&#20026;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#30340;&#29702;&#24819;&#26041;&#27861;&#33021;&#22815;&#22312;&#27809;&#26377;&#22806;&#37096;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#22810;&#26679;&#19988;&#21512;&#26684;&#30340;&#25216;&#33021;&#65292;&#21516;&#26102;&#21457;&#29616;&#30340;&#25216;&#33021;&#38598;&#33021;&#22815;&#20197;&#21508;&#31181;&#26041;&#24335;&#39640;&#25928;&#22320;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Contrastive multi-objectives Skill Discovery (ComSD)&#65292;&#36890;&#36807;&#26356;&#21512;&#29702;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#21644;&#21160;&#24577;&#21152;&#26435;&#30340;&#20869;&#22312;&#22870;&#21169;&#26469;&#20943;&#36731;&#21457;&#29616;&#30340;&#34892;&#20026;&#22312;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning diverse and qualified behaviors for utilization and adaptation without supervision is a key ability of intelligent creatures. Ideal unsupervised skill discovery methods are able to produce diverse and qualified skills in the absence of extrinsic reward, while the discovered skill set can efficiently adapt to downstream tasks in various ways. Maximizing the Mutual Information (MI) between skills and visited states can achieve ideal skill-conditioned behavior distillation in theory. However, it's difficult for recent advanced methods to well balance behavioral quality (exploration) and diversity (exploitation) in practice, which may be attributed to the unreasonable MI estimation by their rigid intrinsic reward design. In this paper, we propose Contrastive multi-objectives Skill Discovery (ComSD) which tries to mitigate the quality-versus-diversity conflict of discovered behaviors through a more reasonable MI estimation and a dynamically weighted intrinsic reward. ComSD proposes
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#39033;&#26032;&#39062;&#30340;&#23454;&#39564;&#24182;&#25910;&#38598;&#20102;52&#21517;&#21442;&#19982;&#32773;&#30340;&#20840;&#38754;&#33041;&#30005;&#22270;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#35273;&#37266;&#21644;&#30561;&#30496;&#29366;&#24577;&#19979;&#31070;&#32463;&#34920;&#31034;&#30340;&#24046;&#24322;&#38382;&#39064;&#12290;&#30740;&#31350;&#22242;&#38431;&#24320;&#21457;&#20102;&#36890;&#29992;&#30561;&#30496;&#35299;&#30721;&#22120;&#65288;USD&#65289;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#20010;&#20307;&#38388;&#23545;&#40784;&#35273;&#37266;&#21644;&#30561;&#30496;&#30340;&#31070;&#32463;&#27169;&#24335;&#65292;&#24182;&#21462;&#24471;&#20102;&#19982;&#20351;&#29992;&#20010;&#21035;&#30561;&#30496;&#25968;&#25454;&#36827;&#34892;&#35299;&#30721;&#30456;&#24403;&#30340;&#20934;&#30830;&#29575;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#22312;&#27979;&#35797;&#20010;&#20307;&#19978;&#23545;USD&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#35299;&#30721;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16457</link><description>&lt;p&gt;
&#36890;&#29992;&#30561;&#30496;&#35299;&#30721;&#22120;&#65306;&#23558;&#35273;&#37266;&#21644;&#30561;&#30496;&#31070;&#32463;&#34920;&#31034;&#23545;&#40784;&#20110;&#19981;&#21516;&#20010;&#20307;&#38388;
&lt;/p&gt;
&lt;p&gt;
Universal Sleep Decoder: Aligning awake and sleep neural representation across subjects. (arXiv:2309.16457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16457
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#39033;&#26032;&#39062;&#30340;&#23454;&#39564;&#24182;&#25910;&#38598;&#20102;52&#21517;&#21442;&#19982;&#32773;&#30340;&#20840;&#38754;&#33041;&#30005;&#22270;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#35273;&#37266;&#21644;&#30561;&#30496;&#29366;&#24577;&#19979;&#31070;&#32463;&#34920;&#31034;&#30340;&#24046;&#24322;&#38382;&#39064;&#12290;&#30740;&#31350;&#22242;&#38431;&#24320;&#21457;&#20102;&#36890;&#29992;&#30561;&#30496;&#35299;&#30721;&#22120;&#65288;USD&#65289;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#20010;&#20307;&#38388;&#23545;&#40784;&#35273;&#37266;&#21644;&#30561;&#30496;&#30340;&#31070;&#32463;&#27169;&#24335;&#65292;&#24182;&#21462;&#24471;&#20102;&#19982;&#20351;&#29992;&#20010;&#21035;&#30561;&#30496;&#25968;&#25454;&#36827;&#34892;&#35299;&#30721;&#30456;&#24403;&#30340;&#20934;&#30830;&#29575;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#22312;&#27979;&#35797;&#20010;&#20307;&#19978;&#23545;USD&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#35299;&#30721;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33041;&#27963;&#21160;&#35299;&#30721;&#30561;&#30496;&#20013;&#30340;&#35760;&#24518;&#20869;&#23481;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#31070;&#32463;&#31185;&#23398;&#30340;&#30446;&#26631;&#12290;&#34429;&#28982;&#24050;&#30693;&#21870;&#40831;&#31867;&#21160;&#29289;&#22312;&#30561;&#30496;&#20013;&#33258;&#21457;&#22320;&#37325;&#26032;&#28608;&#27963;&#35760;&#24518;&#20197;&#25903;&#25345;&#35760;&#24518;&#24041;&#22266;&#21644;&#31163;&#32447;&#23398;&#20064;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#32463;&#36807;&#23436;&#25972;&#27880;&#37322;&#30340;&#30561;&#30496;&#25968;&#25454;&#38598;&#20197;&#21450;&#28165;&#37266;&#29366;&#24577;&#21644;&#30561;&#30496;&#29366;&#24577;&#20043;&#38388;&#31070;&#32463;&#27169;&#24335;&#30340;&#24040;&#22823;&#24046;&#24322;&#65292;&#25429;&#25417;&#20154;&#31867;&#30340;&#35760;&#24518;&#20877;&#29616;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#39033;&#26032;&#39062;&#30340;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#23454;&#39564;&#65292;&#24182;&#20174;52&#21517;&#21442;&#19982;&#32773;&#25910;&#38598;&#20102;&#19968;&#20221;&#20840;&#38754;&#12289;&#23436;&#25972;&#27880;&#37322;&#30340;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#35273;&#37266;&#21644;&#30561;&#30496;&#20004;&#31181;&#29366;&#24577;&#12290;&#21033;&#29992;&#36825;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#36890;&#29992;&#30561;&#30496;&#35299;&#30721;&#22120;&#65288;USD&#65289;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#20010;&#20307;&#38388;&#23545;&#40784;&#35273;&#37266;&#19982;&#30561;&#30496;&#30340;&#31070;&#32463;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#20010;&#20307;&#19978;&#23454;&#29616;&#20102;&#39640;&#36798;16.6%&#30340;top-1&#38646;&#26679;&#26412;&#20934;&#30830;&#29575;&#65292;&#19982;&#20351;&#29992;&#20010;&#21035;&#30561;&#30496;&#25968;&#25454;&#36827;&#34892;&#35299;&#30721;&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#23545;&#27979;&#35797;&#20010;&#20307;&#30340;USD&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#35299;&#30721;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding memory content from brain activity during sleep has long been a goal in neuroscience. While spontaneous reactivation of memories during sleep in rodents is known to support memory consolidation and offline learning, capturing memory replay in humans is challenging due to the absence of well-annotated sleep datasets and the substantial differences in neural patterns between wakefulness and sleep. To address these challenges, we designed a novel cognitive neuroscience experiment and collected a comprehensive, well-annotated electroencephalography (EEG) dataset from 52 subjects during both wakefulness and sleep. Leveraging this benchmark dataset, we developed the Universal Sleep Decoder (USD) to align neural representations between wakefulness and sleep across subjects. Our model achieves up to 16.6% top-1 zero-shot accuracy on unseen subjects, comparable to decoding performances using individual sleep data. Furthermore, fine-tuning USD on test subjects enhances decoding accuracy
&lt;/p&gt;</description></item><item><title>MaGNet&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#39034;&#24207;&#22320;&#25972;&#21512;&#19981;&#21516;&#39034;&#24207;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#32039;&#20945;&#22270;&#32467;&#26500;&#25552;&#20379;&#26377;&#24847;&#20041;&#19988;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.13459</link><description>&lt;p&gt;
&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#25972;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Model-Agnostic Graph Neural Network for Integrating Local and Global Information. (arXiv:2309.13459v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13459
&lt;/p&gt;
&lt;p&gt;
MaGNet&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#39034;&#24207;&#22320;&#25972;&#21512;&#19981;&#21516;&#39034;&#24207;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#32039;&#20945;&#22270;&#32467;&#26500;&#25552;&#20379;&#26377;&#24847;&#20041;&#19988;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#31181;&#20197;&#22270;&#20026;&#37325;&#28857;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;GNN&#23384;&#22312;&#20004;&#20010;&#37325;&#35201;&#38480;&#21046;&#65306;&#30001;&#20110;&#40657;&#30418;&#29305;&#24615;&#65292;&#32467;&#26524;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65307;&#26080;&#27861;&#23398;&#20064;&#19981;&#21516;&#39034;&#24207;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MaGNet&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#39034;&#24207;&#22320;&#25972;&#21512;&#19981;&#21516;&#39034;&#24207;&#30340;&#20449;&#24687;&#65292;&#20174;&#39640;&#38454;&#37051;&#23621;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#32039;&#20945;&#22270;&#32467;&#26500;&#25552;&#20379;&#26377;&#24847;&#20041;&#19988;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;&#29305;&#21035;&#22320;&#65292;MaGNet&#30001;&#20004;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;&#22270;&#25299;&#25169;&#19979;&#22797;&#26434;&#20851;&#31995;&#30340;&#28508;&#22312;&#34920;&#31034;&#30340;&#20272;&#35745;&#27169;&#22411;&#21644;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#33410;&#28857;&#12289;&#36793;&#21644;&#37325;&#35201;&#33410;&#28857;&#29305;&#24449;&#30340;&#35299;&#37322;&#27169;&#22411;&#12290;&#20174;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;Rademacher&#22797;&#26434;&#24230;&#24314;&#31435;&#20102;MaGNet&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have achieved promising performance in a variety of graph-focused tasks. Despite their success, existing GNNs suffer from two significant limitations: a lack of interpretability in results due to their black-box nature, and an inability to learn representations of varying orders. To tackle these issues, we propose a novel Model-agnostic Graph Neural Network (MaGNet) framework, which is able to sequentially integrate information of various orders, extract knowledge from high-order neighbors, and provide meaningful and interpretable results by identifying influential compact graph structures. In particular, MaGNet consists of two components: an estimation model for the latent representation of complex relationships under graph topology, and an interpretation model that identifies influential nodes, edges, and important node features. Theoretically, we establish the generalization error bound for MaGNet via empirical Rademacher complexity, and showcase its pow
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;K-pop&#27468;&#35789;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25581;&#31034;&#20102;K-pop&#27468;&#35789;&#32763;&#35793;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#31070;&#32463;&#27468;&#35789;&#32763;&#35793;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#19987;&#29992;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11093</link><description>&lt;p&gt;
K-pop&#27468;&#35789;&#32763;&#35793;&#65306;&#25968;&#25454;&#38598;&#12289;&#20998;&#26512;&#19982;&#31070;&#32463;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling. (arXiv:2309.11093v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11093
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;K-pop&#27468;&#35789;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25581;&#31034;&#20102;K-pop&#27468;&#35789;&#32763;&#35793;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#31070;&#32463;&#27468;&#35789;&#32763;&#35793;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#19987;&#29992;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27468;&#35789;&#32763;&#35793;&#20316;&#20026;&#19968;&#20010;&#30740;&#31350;&#20102;&#19968;&#20010;&#19990;&#32426;&#30340;&#39046;&#22495;&#65292;&#22914;&#20170;&#21560;&#24341;&#30528;&#35745;&#31639;&#35821;&#35328;&#23398;&#30740;&#31350;&#32773;&#30340;&#27880;&#24847;&#12290;&#25105;&#20204;&#22312;&#20197;&#24448;&#30740;&#31350;&#20013;&#21457;&#29616;&#20102;&#20004;&#20010;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#22312;&#27468;&#35789;&#32763;&#35793;&#30740;&#31350;&#20013;&#65292;&#23613;&#31649;K-pop&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#20294;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#35199;&#26041;&#27969;&#27966;&#21644;&#35821;&#35328;&#65292;&#27809;&#26377;&#30740;&#31350;&#38598;&#20013;&#22312;K-pop&#19978;&#12290;&#20854;&#27425;&#65292;&#27468;&#35789;&#32763;&#35793;&#39046;&#22495;&#32570;&#20047;&#21487;&#20844;&#24320;&#33719;&#24471;&#30340;&#25968;&#25454;&#38598;&#65307;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#23578;&#26080;&#27492;&#31867;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#25299;&#23485;&#27468;&#35789;&#32763;&#35793;&#30740;&#31350;&#30340;&#27969;&#27966;&#21644;&#35821;&#35328;&#33539;&#22260;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#21809;&#27468;&#35789;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#32422;89%&#20026;K-pop&#27468;&#35789;&#12290;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#36880;&#34892;&#21644;&#36880;&#33410;&#23545;&#40784;&#20102;&#38889;&#35821;&#21644;&#33521;&#35821;&#27468;&#35789;&#12290;&#25105;&#20204;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#25581;&#31034;&#20102;K-pop&#27468;&#35789;&#32763;&#35793;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#19982;&#20854;&#20182;&#24191;&#27867;&#30740;&#31350;&#30340;&#27969;&#27966;&#21306;&#20998;&#24320;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#31070;&#32463;&#27468;&#35789;&#32763;&#35793;&#27169;&#22411;&#65292;&#20174;&#32780;&#24378;&#35843;&#20102;&#19987;&#29992;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lyric translation, a field studied for over a century, is now attracting computational linguistics researchers. We identified two limitations in previous studies. Firstly, lyric translation studies have predominantly focused on Western genres and languages, with no previous study centering on K-pop despite its popularity. Second, the field of lyric translation suffers from a lack of publicly available datasets; to the best of our knowledge, no such dataset exists. To broaden the scope of genres and languages in lyric translation studies, we introduce a novel singable lyric translation dataset, approximately 89\% of which consists of K-pop song lyrics. This dataset aligns Korean and English lyrics line-by-line and section-by-section. We leveraged this dataset to unveil unique characteristics of K-pop lyric translation, distinguishing it from other extensively studied genres, and to construct a neural lyric translation model, thereby underscoring the importance of a dedicated dataset for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#20808;&#32771;&#34385;&#38752;&#36817;&#20915;&#31574;&#36793;&#30028;&#30340;&#26679;&#26412;&#24182;&#20998;&#37197;&#36739;&#39640;&#30340;&#26435;&#37325;&#65292;&#25552;&#39640;&#20102;&#20844;&#24179;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.08375</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#20248;&#20808;&#32423;&#37325;&#26032;&#21152;&#26435;&#20197;&#25552;&#39640;&#20844;&#24179;&#24615;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Adaptive Priority Reweighing for Generalizing Fairness Improvement. (arXiv:2309.08375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#20808;&#32771;&#34385;&#38752;&#36817;&#20915;&#31574;&#36793;&#30028;&#30340;&#26679;&#26412;&#24182;&#20998;&#37197;&#36739;&#39640;&#30340;&#26435;&#37325;&#65292;&#25552;&#39640;&#20102;&#20844;&#24179;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#22312;&#20851;&#38190;&#20915;&#31574;&#39046;&#22495;&#30340;&#26222;&#21450;&#65292;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#21628;&#22768;&#36234;&#26469;&#36234;&#22823;&#12290;&#23613;&#31649;&#24050;&#32463;&#36890;&#36807;&#23398;&#20064;&#20844;&#24179;&#32422;&#26463;&#26469;&#25913;&#21892;&#31639;&#27861;&#30340;&#20844;&#24179;&#24615;&#30340;&#21508;&#31181;&#26041;&#24335;&#65292;&#20294;&#23427;&#20204;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#24615;&#33021;&#24182;&#19981;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#12290;&#38656;&#35201;&#19968;&#31181;&#24615;&#33021;&#26377;&#21069;&#26223;&#19988;&#20855;&#26377;&#26356;&#22909;&#27867;&#21270;&#33021;&#21147;&#30340;&#20844;&#24179;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#65292;&#20197;&#28040;&#38500;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#20998;&#24067;&#20559;&#31227;&#23545;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#25552;&#35758;&#20026;&#27599;&#20010;&#65288;&#23376;&#65289;&#32452;&#20998;&#37197;&#19968;&#20010;&#32479;&#19968;&#30340;&#26435;&#37325;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32454;&#31890;&#24230;&#22320;&#24314;&#27169;&#20102;&#26679;&#26412;&#39044;&#27979;&#19982;&#20915;&#31574;&#36793;&#30028;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#20248;&#20808;&#32771;&#34385;&#38752;&#36817;&#20915;&#31574;&#36793;&#30028;&#30340;&#26679;&#26412;&#65292;&#24182;&#20998;&#37197;&#36739;&#39640;&#30340;&#26435;&#37325;&#26469;&#25552;&#39640;&#20844;&#24179;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing penetration of machine learning applications in critical decision-making areas, calls for algorithmic fairness are more prominent. Although there have been various modalities to improve algorithmic fairness through learning with fairness constraints, their performance does not generalize well in the test set. A performance-promising fair algorithm with better generalizability is needed. This paper proposes a novel adaptive reweighing method to eliminate the impact of the distribution shifts between training and test data on model generalizability. Most previous reweighing methods propose to assign a unified weight for each (sub)group. Rather, our method granularly models the distance from the sample predictions to the decision boundary. Our adaptive reweighing method prioritizes samples closer to the decision boundary and assigns a higher weight to improve the generalizability of fair classifiers. Extensive experiments are performed to validate the generalizability 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20248;&#21270;&#22823;&#32928;&#30340;&#20998;&#21106;&#32467;&#26524;&#65292;&#24182;&#32467;&#21512;&#20808;&#36827;&#30340;&#34920;&#38754;&#37325;&#26500;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#22823;&#32928;3D&#24418;&#29366;&#30340;&#31934;&#21270;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2309.08289</link><description>&lt;p&gt;
&#21033;&#29992;&#28857;&#25193;&#25955;&#27169;&#22411;&#23545;&#22823;&#32928;&#30340;3D&#24418;&#29366;&#36827;&#34892;&#31934;&#21270;&#20197;&#29983;&#25104;&#25968;&#23383;&#24187;&#24433;
&lt;/p&gt;
&lt;p&gt;
Large Intestine 3D Shape Refinement Using Point Diffusion Models for Digital Phantom Generation. (arXiv:2309.08289v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20248;&#21270;&#22823;&#32928;&#30340;&#20998;&#21106;&#32467;&#26524;&#65292;&#24182;&#32467;&#21512;&#20808;&#36827;&#30340;&#34920;&#38754;&#37325;&#26500;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#22823;&#32928;3D&#24418;&#29366;&#30340;&#31934;&#21270;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#24314;&#27169;&#20154;&#20307;&#22120;&#23448;&#22312;&#26500;&#24314;&#34394;&#25311;&#25104;&#20687;&#35797;&#39564;&#30340;&#35745;&#31639;&#20223;&#30495;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20174;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#20013;&#29983;&#25104;&#35299;&#21078;&#23398;&#19978;&#21487;&#20449;&#30340;&#22120;&#23448;&#34920;&#38754;&#37325;&#24314;&#20173;&#28982;&#23545;&#20154;&#20307;&#32467;&#26500;&#20013;&#30340;&#35768;&#22810;&#22120;&#23448;&#26469;&#35828;&#26159;&#20010;&#25361;&#25112;&#12290;&#22312;&#22788;&#29702;&#22823;&#32928;&#26102;&#65292;&#36825;&#20010;&#25361;&#25112;&#23588;&#20026;&#26126;&#26174;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#26469;&#20248;&#21270;&#22823;&#32928;&#20998;&#21106;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#22120;&#23448;&#34920;&#31034;&#20026;&#20174;3D&#20998;&#21106;&#25513;&#27169;&#34920;&#38754;&#37319;&#26679;&#24471;&#21040;&#30340;&#28857;&#20113;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#33719;&#24471;&#22120;&#23448;&#24418;&#29366;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#20998;&#23618;&#28508;&#22312;&#31354;&#38388;&#20013;&#35757;&#32451;&#20004;&#20010;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#26469;&#36827;&#34892;&#24418;&#29366;&#31934;&#21270;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#34920;&#38754;&#37325;&#26500;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#24418;&#29366;&#30340;&#26356;&#22909;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate 3D modeling of human organs plays a crucial role in building computational phantoms for virtual imaging trials. However, generating anatomically plausible reconstructions of organ surfaces from computed tomography scans remains challenging for many structures in the human body. This challenge is particularly evident when dealing with the large intestine. In this study, we leverage recent advancements in geometric deep learning and denoising diffusion probabilistic models to refine the segmentation results of the large intestine. We begin by representing the organ as point clouds sampled from the surface of the 3D segmentation mask. Subsequently, we employ a hierarchical variational autoencoder to obtain global and local latent representations of the organ's shape. We train two conditional denoising diffusion models in the hierarchical latent space to perform shape refinement. To further enhance our method, we incorporate a state-of-the-art surface reconstruction model, allowin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#23558;&#20256;&#32479;&#30340;&#23398;&#20064;&#32467;&#26500;&#36816;&#21160;&#38382;&#39064;&#20013;&#30340;&#23376;&#38382;&#39064;&#26367;&#25442;&#20026;&#30452;&#25509;&#23398;&#20064;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#24207;&#21015;&#30340;&#24555;&#36895;&#25512;&#26029;&#65292;&#25552;&#39640;&#20102;&#32467;&#26500;&#36816;&#21160;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.15984</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#23398;&#20064;&#32467;&#26500;&#36816;&#21160;
&lt;/p&gt;
&lt;p&gt;
Learning Structure-from-Motion with Graph Attention Networks. (arXiv:2308.15984v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#23558;&#20256;&#32479;&#30340;&#23398;&#20064;&#32467;&#26500;&#36816;&#21160;&#38382;&#39064;&#20013;&#30340;&#23376;&#38382;&#39064;&#26367;&#25442;&#20026;&#30452;&#25509;&#23398;&#20064;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#24207;&#21015;&#30340;&#24555;&#36895;&#25512;&#26029;&#65292;&#25552;&#39640;&#20102;&#32467;&#26500;&#36816;&#21160;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#35299;&#20915;&#23398;&#20064;&#32467;&#26500;&#36816;&#21160;&#65288;SfM&#65289;&#30340;&#38382;&#39064;&#12290;SfM&#26159;&#19968;&#20010;&#32463;&#20856;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#65292;&#36890;&#36807;&#36845;&#20195;&#26368;&#23567;&#21270;&#37325;&#25237;&#24433;&#35823;&#24046;&#65288;&#31216;&#20026;&#26463;&#35843;&#25972;&#65289;&#35299;&#20915;&#65292;&#20174;&#33391;&#22909;&#30340;&#21021;&#22987;&#21270;&#24320;&#22987;&#12290;&#20026;&#20102;&#33719;&#24471;&#36275;&#22815;&#22909;&#30340;&#21021;&#22987;&#21270;&#32467;&#26524;&#65292;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#31995;&#21015;&#23376;&#38382;&#39064;&#65288;&#22914;&#25104;&#23545;&#23039;&#24577;&#20272;&#35745;&#12289;&#23039;&#24577;&#24179;&#22343;&#21270;&#25110;&#19977;&#35282;&#27979;&#37327;&#65289;&#65292;&#36825;&#20123;&#23376;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#21021;&#22987;&#35299;&#65292;&#28982;&#21518;&#20351;&#29992;&#26463;&#35843;&#25972;&#23545;&#20854;&#36827;&#34892;&#25913;&#36827;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#27169;&#22411;&#65292;&#23558;&#36825;&#20123;&#23376;&#38382;&#39064;&#26367;&#25442;&#20026;&#20197;&#22810;&#20010;&#35270;&#22270;&#19978;&#26816;&#27979;&#21040;&#30340;2D&#20851;&#38190;&#28857;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#30456;&#24212;&#30340;&#30456;&#26426;&#23039;&#24577;&#21644;3D&#20851;&#38190;&#28857;&#22352;&#26631;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;SfM&#29305;&#23450;&#30340;&#21407;&#35821;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#23427;&#21487;&#20197;&#29992;&#20110;&#24555;&#36895;&#25512;&#26029;&#26032;&#30340;&#21644;&#26410;&#35265;&#36807;&#30340;&#24207;&#21015;&#30340;&#37325;&#24314;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32467;&#26500;&#36816;&#21160;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we tackle the problem of learning Structure-from-Motion (SfM) through the use of graph attention networks. SfM is a classic computer vision problem that is solved though iterative minimization of reprojection errors, referred to as Bundle Adjustment (BA), starting from a good initialization. In order to obtain a good enough initialization to BA, conventional methods rely on a sequence of sub-problems (such as pairwise pose estimation, pose averaging or triangulation) which provides an initial solution that can then be refined using BA. In this work we replace these sub-problems by learning a model that takes as input the 2D keypoints detected across multiple views, and outputs the corresponding camera poses and 3D keypoint coordinates. Our model takes advantage of graph neural networks to learn SfM-specific primitives, and we show that it can be used for fast inference of the reconstruction for new and unseen sequences. The experimental results show that the proposed mode
&lt;/p&gt;</description></item><item><title>TransGNN&#26159;&#19968;&#31181;&#23558;Transformer&#21644;GNN&#23618;&#20132;&#26367;&#32467;&#21512;&#20197;&#30456;&#20114;&#22686;&#24378;&#20854;&#33021;&#21147;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#24403;&#21069;&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#30340;&#24863;&#21463;&#22495;&#26377;&#38480;&#21644;&#23384;&#22312;&#22122;&#38899;&#36830;&#25509;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.14355</link><description>&lt;p&gt;
TransGNN: &#21033;&#29992;Transformer&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21327;&#21516;&#33021;&#21147;&#26469;&#20570;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
TransGNN: Harnessing the Collaborative Power of Transformers and Graph Neural Networks for Recommender Systems. (arXiv:2308.14355v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14355
&lt;/p&gt;
&lt;p&gt;
TransGNN&#26159;&#19968;&#31181;&#23558;Transformer&#21644;GNN&#23618;&#20132;&#26367;&#32467;&#21512;&#20197;&#30456;&#20114;&#22686;&#24378;&#20854;&#33021;&#21147;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#24403;&#21069;&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#30340;&#24863;&#21463;&#22495;&#26377;&#38480;&#21644;&#23384;&#22312;&#22122;&#38899;&#36830;&#25509;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23545;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#22270;&#36827;&#34892;&#24314;&#27169;&#26469;&#36827;&#34892;&#21327;&#21516;&#36807;&#28388;(CF)&#12290;&#29616;&#26377;&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#26680;&#24515;&#26159;&#36890;&#36807;&#22312;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#36793;&#19978;&#36827;&#34892;&#36882;&#24402;&#28040;&#24687;&#20256;&#36882;&#26469;&#25913;&#36827;&#32534;&#30721;&#23884;&#20837;&#12290;&#23613;&#31649;&#23427;&#20204;&#24050;&#32463;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#26159;&#24403;&#21069;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#38754;&#20020;&#30528;&#26377;&#38480;&#30340;&#24863;&#21463;&#22495;&#21644;&#23384;&#22312;&#22122;&#38899; "&#20852;&#36259;&#26080;&#20851;" &#36830;&#25509;&#30340;&#25361;&#25112;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#22312;&#33258;&#36866;&#24212;&#21644;&#20840;&#23616;&#20449;&#24687;&#32858;&#21512;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25429;&#25417;&#22797;&#26434;&#12289;&#32416;&#32544;&#30340;&#32467;&#26500;&#20449;&#24687;&#26041;&#38754;&#22312;&#22823;&#35268;&#27169;&#20132;&#20114;&#22270;&#20013;&#30340;&#24212;&#29992;&#21463;&#21040;&#22256;&#25200;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TransGNN&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20132;&#26367;&#22320;&#32467;&#21512;Transformer&#21644;GNN&#23618;&#26469;&#30456;&#20114;&#22686;&#24378;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged as promising solutions for collaborative filtering (CF) through the modeling of user-item interaction graphs. The nucleus of existing GNN-based recommender systems involves recursive message passing along user-item interaction edges to refine encoded embeddings. Despite their demonstrated effectiveness, current GNN-based methods encounter challenges of limited receptive fields and the presence of noisy ``interest-irrelevant'' connections. In contrast, Transformer-based methods excel in aggregating information adaptively and globally. Nevertheless, their application to large-scale interaction graphs is hindered by inherent complexities and challenges in capturing intricate, entangled structural information. In this paper, we propose TransGNN, a novel model that integrates Transformer and GNN layers in an alternating fashion to mutually enhance their capabilities. Specifically, TransGNN leverages Transformer layers to broaden the receptive field 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37325;&#26032;&#35782;&#21035;&#21311;&#21517;&#20010;&#20154;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#12289;&#36755;&#20837;&#38271;&#24230;&#21644;&#25351;&#20196;&#35843;&#25972;&#26159;&#26368;&#37325;&#35201;&#30340;&#20915;&#23450;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2308.11103</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20877;&#35782;&#21035;&#33021;&#21147;&#65306;&#21311;&#21517;&#38754;&#20020;&#39118;&#38505;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models. (arXiv:2308.11103v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37325;&#26032;&#35782;&#21035;&#21311;&#21517;&#20010;&#20154;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#12289;&#36755;&#20837;&#38271;&#24230;&#21644;&#25351;&#20196;&#35843;&#25972;&#26159;&#26368;&#37325;&#35201;&#30340;&#20915;&#23450;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27431;&#30431;&#21644;&#29790;&#22763;&#65292;&#27861;&#38498;&#35009;&#20915;&#20013;&#33258;&#28982;&#20154;&#21644;&#27861;&#20154;&#30340;&#21311;&#21517;&#24615;&#26159;&#38544;&#31169;&#20445;&#25252;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#23545;&#20110;&#21311;&#21517;&#20154;&#21592;&#30340;&#22823;&#35268;&#27169;&#20877;&#35782;&#21035;&#30340;&#25285;&#24551;&#26085;&#30410;&#22686;&#38271;&#12290;&#26681;&#25454;&#29790;&#22763;&#32852;&#37030;&#26368;&#39640;&#27861;&#38498;&#30340;&#35201;&#27714;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#29790;&#22763;&#32852;&#37030;&#26368;&#39640;&#27861;&#38498;&#30340;&#23454;&#38469;&#27861;&#24459;&#25968;&#25454;&#26500;&#24314;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#26469;&#25506;&#35752;LLMs&#37325;&#26032;&#35782;&#21035;&#27861;&#38498;&#35009;&#20915;&#20013;&#20010;&#20154;&#30340;&#28508;&#21147;&#12290;&#22312;&#26368;&#21021;&#30340;&#23454;&#39564;&#20043;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#32463;&#36807;&#21311;&#21517;&#21270;&#22788;&#29702;&#30340;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#19968;&#20010;&#26356;&#20005;&#26684;&#30340;&#27979;&#35797;&#22330;&#22320;&#26469;&#36827;&#19968;&#27493;&#30740;&#31350;&#30740;&#31350;&#32467;&#26524;&#12290;&#36890;&#36807;&#24341;&#20837;&#24182;&#24212;&#29992;&#25991;&#26412;&#20013;&#20877;&#35782;&#21035;&#20154;&#21592;&#30340;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26032;&#30340;&#24615;&#33021;&#34913;&#37327;&#25351;&#26631;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#24433;&#21709;&#25104;&#21151;&#20877;&#35782;&#21035;&#30340;&#22240;&#32032;&#65292;&#30830;&#23450;&#27169;&#22411;&#22823;&#23567;&#12289;&#36755;&#20837;&#38271;&#24230;&#21644;&#25351;&#20196;&#35843;&#25972;&#26159;&#26368;&#37325;&#35201;&#30340;&#20915;&#23450;&#22240;&#32032;&#20043;&#19968;&#12290;&#23613;&#31649;&#22312;&#21311;&#21517;&#21270;&#22788;&#29702;&#21518;&#65292;LLMs&#22312;&#37325;&#26032;&#35782;&#21035;&#19978;&#30340;&#25104;&#21151;&#29575;&#24456;&#39640;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20173;&#28982;&#23384;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anonymity of both natural and legal persons in court rulings is a critical aspect of privacy protection in the European Union and Switzerland. With the advent of LLMs, concerns about large-scale re-identification of anonymized persons are growing. In accordance with the Federal Supreme Court of Switzerland, we explore the potential of LLMs to re-identify individuals in court rulings by constructing a proof-of-concept using actual legal data from the Swiss federal supreme court. Following the initial experiment, we constructed an anonymized Wikipedia dataset as a more rigorous testing ground to further investigate the findings. With the introduction and application of the new task of re-identifying people in texts, we also introduce new metrics to measure performance. We systematically analyze the factors that influence successful re-identifications, identifying model size, input length, and instruction tuning among the most critical determinants. Despite high re-identification rates on
&lt;/p&gt;</description></item><item><title>Flamingo&#26159;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#36328;&#22823;&#37327;&#23458;&#25143;&#31471;&#23433;&#20840;&#32858;&#21512;&#30340;&#31995;&#32479;&#65292;&#22312;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#28040;&#38500;&#27599;&#36718;&#35774;&#32622;&#21644;&#24341;&#20837;&#36731;&#37327;&#32423;&#30340;&#20002;&#22833;&#23481;&#24525;&#21327;&#35758;&#65292;Flamingo&#35299;&#20915;&#20102;&#20197;&#24448;&#21327;&#35758;&#22312;&#22810;&#36718;&#35774;&#32622;&#19979;&#30340;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#26412;&#22320;&#36873;&#25321;&#23458;&#25143;&#31471;&#37051;&#22495;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.09883</link><description>&lt;p&gt;
Flamingo: &#22810;&#36718;&#21333;&#26381;&#21153;&#22120;&#23433;&#20840;&#32858;&#21512;&#21450;&#20854;&#22312;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Flamingo: Multi-Round Single-Server Secure Aggregation with Applications to Private Federated Learning. (arXiv:2308.09883v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09883
&lt;/p&gt;
&lt;p&gt;
Flamingo&#26159;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#36328;&#22823;&#37327;&#23458;&#25143;&#31471;&#23433;&#20840;&#32858;&#21512;&#30340;&#31995;&#32479;&#65292;&#22312;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#28040;&#38500;&#27599;&#36718;&#35774;&#32622;&#21644;&#24341;&#20837;&#36731;&#37327;&#32423;&#30340;&#20002;&#22833;&#23481;&#24525;&#21327;&#35758;&#65292;Flamingo&#35299;&#20915;&#20102;&#20197;&#24448;&#21327;&#35758;&#22312;&#22810;&#36718;&#35774;&#32622;&#19979;&#30340;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#26412;&#22320;&#36873;&#25321;&#23458;&#25143;&#31471;&#37051;&#22495;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Flamingo&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#36328;&#22823;&#37327;&#23458;&#25143;&#31471;&#23433;&#20840;&#32858;&#21512;&#25968;&#25454;&#30340;&#31995;&#32479;&#12290;&#22312;&#23433;&#20840;&#32858;&#21512;&#20013;&#65292;&#26381;&#21153;&#22120;&#23545;&#23458;&#25143;&#31471;&#30340;&#31169;&#26377;&#36755;&#20837;&#36827;&#34892;&#27714;&#21644;&#65292;&#24182;&#22312;&#19981;&#20102;&#35299;&#20010;&#20307;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#32467;&#26524;&#65292;&#20165;&#33021;&#25512;&#26029;&#20986;&#26368;&#32456;&#24635;&#21644;&#12290;Flamingo&#19987;&#27880;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#22810;&#36718;&#35774;&#32622;&#65292;&#20854;&#20013;&#25191;&#34892;&#22810;&#20010;&#36830;&#32493;&#30340;&#27169;&#22411;&#26435;&#37325;&#27714;&#21644;&#65288;&#24179;&#22343;&#65289;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;&#20043;&#21069;&#30340;&#21327;&#35758;&#65288;&#20363;&#22914;Bell&#31561;&#20154;&#30340;CCS '20&#65289;&#20165;&#36866;&#29992;&#20110;&#21333;&#36718;&#65292;&#24182;&#36890;&#36807;&#22810;&#27425;&#37325;&#22797;&#35813;&#21327;&#35758;&#26469;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;&#30340;&#35774;&#32622;&#12290;Flamingo&#28040;&#38500;&#20102;&#20043;&#21069;&#21327;&#35758;&#27599;&#36718;&#35774;&#32622;&#30340;&#38656;&#27714;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36731;&#37327;&#32423;&#30340;&#20002;&#22833;&#23481;&#24525;&#21327;&#35758;&#65292;&#20197;&#30830;&#20445;&#22914;&#26524;&#23458;&#25143;&#31471;&#22312;&#27714;&#21644;&#36807;&#31243;&#20013;&#31163;&#24320;&#65292;&#26381;&#21153;&#22120;&#20173;&#28982;&#21487;&#20197;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;Flamingo&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26412;&#22320;&#36873;&#25321;&#25152;&#35859;&#30340;&#23458;&#25143;&#31471;&#37051;&#22495;&#30340;&#26041;&#24335;&#65292;&#27492;&#27010;&#24565;&#30001;Bell&#31561;&#20154;&#25552;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Flamingo, a system for secure aggregation of data across a large set of clients. In secure aggregation, a server sums up the private inputs of clients and obtains the result without learning anything about the individual inputs beyond what is implied by the final sum. Flamingo focuses on the multi-round setting found in federated learning in which many consecutive summations (averages) of model weights are performed to derive a good model. Previous protocols, such as Bell et al. (CCS '20), have been designed for a single round and are adapted to the federated learning setting by repeating the protocol multiple times. Flamingo eliminates the need for the per-round setup of previous protocols, and has a new lightweight dropout resilience protocol to ensure that if clients leave in the middle of a sum the server can still obtain a meaningful result. Furthermore, Flamingo introduces a new way to locally choose the so-called client neighborhood introduced by Bell et al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#36830;&#32493;&#30340;&#23545;&#31216;&#24615;&#20462;&#25913;&#26435;&#37325;&#65292;&#20855;&#26377;&#19982;&#20256;&#32479;&#27169;&#22411;&#19981;&#21516;&#30340;&#29305;&#28857;&#65292;&#24076;&#26395;&#23558;&#23545;&#31216;&#24615;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26399;&#26395;&#29305;&#24615;&#65292;&#24182;&#25512;&#24191;&#23558;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35270;&#35282;&#24212;&#29992;&#20110;ConvNet&#30340;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2308.01621</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#36830;&#32493;&#23545;&#31216;&#24615;&#30340;&#26032;&#22411;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Novel Convolutional Neural Network Architecture with a Continuous Symmetry. (arXiv:2308.01621v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#36830;&#32493;&#30340;&#23545;&#31216;&#24615;&#20462;&#25913;&#26435;&#37325;&#65292;&#20855;&#26377;&#19982;&#20256;&#32479;&#27169;&#22411;&#19981;&#21516;&#30340;&#29305;&#28857;&#65292;&#24076;&#26395;&#23558;&#23545;&#31216;&#24615;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26399;&#26395;&#29305;&#24615;&#65292;&#24182;&#25512;&#24191;&#23558;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35270;&#35282;&#24212;&#29992;&#20110;ConvNet&#30340;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(ConvNet)&#26550;&#26500;&#65292;&#20854;&#28789;&#24863;&#26469;&#33258;&#20110;&#19968;&#31867;&#31216;&#20026;&#25311;&#32447;&#24615;&#21452;&#26354;&#31995;&#32479;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#23427;&#20801;&#35768;&#36890;&#36807;&#36830;&#32493;&#30340;&#23545;&#31216;&#24615;&#20462;&#25913;&#26435;&#37325;&#12290;&#36825;&#26159;&#19982;&#20256;&#32479;&#27169;&#22411;&#20013;&#22522;&#26412;&#22266;&#23450;&#30340;&#26550;&#26500;&#21644;&#26435;&#37325;&#30456;&#27604;&#30340;&#37325;&#22823;&#36716;&#21464;&#12290;&#25105;&#20204;&#24076;&#26395;&#23558;(&#20869;&#37096;)&#23545;&#31216;&#24615;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26399;&#26395;&#29305;&#24615;&#65292;&#24182;&#22312;&#26356;&#24191;&#27867;&#30340;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#20013;&#21560;&#24341;&#23545;PDE&#35270;&#35282;&#20998;&#26512;&#21644;&#35299;&#37322;ConvNet&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new Convolutional Neural Network (ConvNet) architecture inspired by a class of partial differential equations (PDEs) called quasi-linear hyperbolic systems. With comparable performance on image classification task, it allows for the modification of the weights via a continuous group of symmetry. This is a significant shift from traditional models where the architecture and weights are essentially fixed. We wish to promote the (internal) symmetry as a new desirable property for a neural network, and to draw attention to the PDE perspective in analyzing and interpreting ConvNets in the broader Deep Learning community.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#22240;&#31361;&#21464;&#22312;&#30284;&#30151;&#20013;&#30340;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#21644;&#26368;&#26032;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.14361</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;LSTM&#12289;BiLSTM&#12289;CNN&#12289;GRU&#21644;GloVe&#30340;&#28151;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#22522;&#22240;&#31361;&#21464;&#22312;&#30284;&#30151;&#20013;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe. (arXiv:2307.14361v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#22240;&#31361;&#21464;&#22312;&#30284;&#30151;&#20013;&#30340;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#21644;&#26368;&#26032;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#65292;&#23558;LSTM&#12289;BiLSTM&#12289;CNN&#12289;GRU&#21644;GloVe&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#22312;Kaggle&#30340;&#8220;&#20010;&#24615;&#21270;&#21307;&#23398;&#65306;&#37325;&#26032;&#23450;&#20041;&#30284;&#30151;&#27835;&#30103;&#8221;&#25968;&#25454;&#38598;&#20013;&#23545;&#22522;&#22240;&#31361;&#21464;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#19982;BERT&#12289;Electra&#12289;Roberta&#12289;XLNet&#12289;Distilbert&#20197;&#21450;&#23427;&#20204;&#30340;LSTM&#38598;&#25104;&#31561;&#30693;&#21517;&#36716;&#25442;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#12289;F1&#20998;&#25968;&#21644;&#22343;&#26041;&#35823;&#24046;&#26041;&#38754;&#37117;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#36824;&#38656;&#35201;&#36739;&#23569;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#21644;&#25928;&#29575;&#30340;&#23436;&#32654;&#32467;&#21512;&#12290;&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#38598;&#25104;&#27169;&#22411;&#22312;&#22522;&#22240;&#31361;&#21464;&#20998;&#31867;&#31561;&#22256;&#38590;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents an ensemble model combining LSTM, BiLSTM, CNN, GRU, and GloVe to classify gene mutations using Kaggle's Personalized Medicine: Redefining Cancer Treatment dataset. The results were compared against well-known transformers like as BERT, Electra, Roberta, XLNet, Distilbert, and their LSTM ensembles. Our model outperformed all other models in terms of accuracy, precision, recall, F1 score, and Mean Squared Error. Surprisingly, it also needed less training time, resulting in a perfect combination of performance and efficiency. This study demonstrates the utility of ensemble models for difficult tasks such as gene mutation classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#22312;&#26631;&#20934;&#27491;&#24577;&#21327;&#21464;&#37327;&#19979;&#30340;&#21442;&#25968;&#20272;&#35745;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#21457;&#29616;&#26679;&#26412;&#22797;&#26434;&#24230;&#26354;&#32447;&#22312;&#36870;&#28201;&#24230;&#26041;&#38754;&#26377;&#20004;&#20010;&#36716;&#25240;&#28857;&#65292;&#26126;&#30830;&#21010;&#20998;&#20102;&#20302;&#12289;&#20013;&#21644;&#39640;&#28201;&#24230;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2307.04191</link><description>&lt;p&gt;
&#20851;&#20110;&#36923;&#36753;&#22238;&#24402;&#20013;&#21442;&#25968;&#20272;&#35745;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the sample complexity of estimation in logistic regression. (arXiv:2307.04191v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#22312;&#26631;&#20934;&#27491;&#24577;&#21327;&#21464;&#37327;&#19979;&#30340;&#21442;&#25968;&#20272;&#35745;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#21457;&#29616;&#26679;&#26412;&#22797;&#26434;&#24230;&#26354;&#32447;&#22312;&#36870;&#28201;&#24230;&#26041;&#38754;&#26377;&#20004;&#20010;&#36716;&#25240;&#28857;&#65292;&#26126;&#30830;&#21010;&#20998;&#20102;&#20302;&#12289;&#20013;&#21644;&#39640;&#28201;&#24230;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#26159;&#22122;&#22768;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#20043;&#19968;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26631;&#20934;&#27491;&#24577;&#21327;&#21464;&#37327;&#19979;&#65292;&#20197;$\ell_2$&#35823;&#24046;&#20026;&#38480;&#65292;&#20272;&#35745;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#21442;&#25968;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#32771;&#34385;&#20102;&#32500;&#24230;&#21644;&#36870;&#28201;&#24230;&#30340;&#24433;&#21709;&#12290;&#36870;&#28201;&#24230;&#25511;&#21046;&#20102;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#20449;&#22122;&#27604;&#12290;&#34429;&#28982;&#36923;&#36753;&#22238;&#24402;&#30340;&#24191;&#20041;&#30028;&#38480;&#21644;&#28176;&#36817;&#24615;&#33021;&#24050;&#32463;&#26377;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#20294;&#20851;&#20110;&#21442;&#25968;&#20272;&#35745;&#30340;&#38750;&#28176;&#36817;&#26679;&#26412;&#22797;&#26434;&#24230;&#22312;&#20043;&#21069;&#30340;&#20998;&#26512;&#20013;&#27809;&#26377;&#35752;&#35770;&#20854;&#19982;&#35823;&#24046;&#21644;&#36870;&#28201;&#24230;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#26354;&#32447;&#22312;&#36870;&#28201;&#24230;&#26041;&#38754;&#20855;&#26377;&#20004;&#20010;&#36716;&#25240;&#28857;&#65288;&#25110;&#20020;&#30028;&#28857;&#65289;&#65292;&#26126;&#30830;&#21010;&#20998;&#20102;&#20302;&#12289;&#20013;&#21644;&#39640;&#28201;&#24230;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
The logistic regression model is one of the most popular data generation model in noisy binary classification problems. In this work, we study the sample complexity of estimating the parameters of the logistic regression model up to a given $\ell_2$ error, in terms of the dimension and the inverse temperature, with standard normal covariates. The inverse temperature controls the signal-to-noise ratio of the data generation process. While both generalization bounds and asymptotic performance of the maximum-likelihood estimator for logistic regression are well-studied, the non-asymptotic sample complexity that shows the dependence on error and the inverse temperature for parameter estimation is absent from previous analyses. We show that the sample complexity curve has two change-points (or critical points) in terms of the inverse temperature, clearly separating the low, moderate, and high temperature regimes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#26131;&#20986;&#38169;&#27979;&#35797;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27979;&#35797;&#20195;&#30721;&#26469;&#39044;&#27979;&#27979;&#35797;&#30340;&#20462;&#22797;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;UniXcoder&#20248;&#20110;CodeBERT&#12290;</title><link>http://arxiv.org/abs/2307.00012</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#39044;&#27979;&#26131;&#20986;&#38169;&#27979;&#35797;&#20462;&#22797;&#31867;&#21035;
&lt;/p&gt;
&lt;p&gt;
Black-Box Prediction of Flaky Test Fix Categories Using Language Models. (arXiv:2307.00012v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#26131;&#20986;&#38169;&#27979;&#35797;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27979;&#35797;&#20195;&#30721;&#26469;&#39044;&#27979;&#27979;&#35797;&#30340;&#20462;&#22797;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;UniXcoder&#20248;&#20110;CodeBERT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26131;&#20986;&#38169;&#27979;&#35797;&#20250;&#22312;&#30456;&#21516;&#36719;&#20214;&#29256;&#26412;&#30340;&#27979;&#35797;&#19979;&#38750;&#30830;&#23450;&#24615;&#22320;&#36890;&#36807;&#25110;&#22833;&#36133;&#65292;&#24341;&#36215;&#28151;&#20081;&#24182;&#28010;&#36153;&#24320;&#21457;&#32773;&#26102;&#38388;&#12290;&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#34987;&#29992;&#20110;&#39044;&#27979;&#26131;&#20986;&#38169;&#24615;&#21450;&#20854;&#26681;&#26412;&#21407;&#22240;&#65292;&#20294;&#22312;&#25552;&#20379;&#20462;&#22797;&#25903;&#25345;&#26041;&#38754;&#20173;&#26377;&#36739;&#23569;&#24037;&#20316;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#20998;&#26512;&#27979;&#35797;&#20195;&#30721;&#33258;&#21160;&#29983;&#25104;13&#20010;&#20462;&#22797;&#31867;&#21035;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#35757;&#32451;&#27169;&#22411;&#26469;&#39044;&#27979;&#26131;&#20986;&#38169;&#27979;&#35797;&#30340;&#20462;&#22797;&#31867;&#21035;&#12290;&#34429;&#28982;&#22312;&#24403;&#21069;&#38454;&#27573;&#20934;&#30830;&#39044;&#27979;&#20462;&#22797;&#26412;&#36523;&#26159;&#19981;&#29616;&#23454;&#30340;&#65292;&#20294;&#36825;&#20123;&#31867;&#21035;&#25552;&#20379;&#20102;&#20851;&#20110;&#38656;&#35201;&#26816;&#26597;&#30340;&#27979;&#35797;&#20195;&#30721;&#37096;&#20998;&#30340;&#31934;&#30830;&#25351;&#23548;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;CodeBERT&#21644;UniXcoder&#65292;&#20854;&#36755;&#20986;&#32463;&#36807;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#25110;&#22522;&#20110;&#23402;&#29983;&#32593;&#32476;&#30340;Few Shot Learning&#65288;FSL&#65289;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UniXcoder&#22312;&#27491;&#30830;&#39044;&#27979;&#22823;&#22810;&#25968;&#20462;&#22797;&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;CodeBERT&#12290;
&lt;/p&gt;
&lt;p&gt;
Flaky tests are problematic because they non-deterministically pass or fail for the same software version under test, causing confusion and wasting developer time. While machine learning models have been used to predict flakiness and its root causes, there is less work on providing support to fix the problem. To address this gap, we propose a framework that automatically generates labeled datasets for 13 fix categories and train models to predict the fix category of a flaky test by analyzing the test code only. Though it is unrealistic at this stage to accurately predict the fix itself, the categories provide precise guidance about what part of the test code to look at. Our approach is based on language models, namely CodeBERT and UniXcoder, whose output is fine-tuned with a Feed Forward Neural Network (FNN) or a Siamese Network-based Few Shot Learning (FSL). Our experimental results show that UniXcoder outperforms CodeBERT, in correctly predicting most of the categories of fixes a dev
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#32622;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#20844;&#24179;LTR-RC&#65292;&#23427;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#35757;&#32451;&#65292;&#22312;&#20445;&#35777;&#20844;&#24179;&#24615;&#30340;&#21516;&#26102;&#65292;&#36824;&#33021;&#22312;&#25928;&#29992;&#21644;&#20844;&#24179;&#20043;&#38388;&#23454;&#29616;&#26377;&#25928;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.07188</link><description>&lt;p&gt;
&#26080;&#20998;&#24067;&#39118;&#38505;&#25511;&#21046;&#30340;&#20844;&#24179;&#23398;&#20064;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Fair Learning to Rank with Distribution-free Risk Control. (arXiv:2306.07188v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#32622;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#20844;&#24179;LTR-RC&#65292;&#23427;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#35757;&#32451;&#65292;&#22312;&#20445;&#35777;&#20844;&#24179;&#24615;&#30340;&#21516;&#26102;&#65292;&#36824;&#33021;&#22312;&#25928;&#29992;&#21644;&#20844;&#24179;&#20043;&#38388;&#23454;&#29616;&#26377;&#25928;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#32463;&#27982;&#20013;&#65292;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#23545;&#29992;&#25143;&#21644;&#29289;&#21697;&#25552;&#20379;&#32773;&#33267;&#20851;&#37325;&#35201;&#12290;LTR&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#23545;&#20110;&#25353;&#27604;&#20363;&#20998;&#37197;&#26333;&#20809;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#20855;&#26377;&#30456;&#21516;&#30456;&#20851;&#24615;&#30340;&#39033;&#25509;&#25910;&#30053;&#26377;&#19981;&#21516;&#30340;&#20998;&#25968;&#26102;&#65292;&#30830;&#23450;&#24615;&#25490;&#21517;&#27169;&#22411;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#26333;&#20809;&#20998;&#37197;&#12290;&#38543;&#26426;LTR&#27169;&#22411;&#65292;&#21253;&#25324;Plackett-Luce&#65288;PL&#65289;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#20294;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#24615;&#33021;&#20445;&#35777;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20844;&#24179;LTR-RC&#65292;&#19968;&#31181;&#26032;&#30340;&#21518;&#32622;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#12290;&#20844;&#24179;LTR-RC&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#35780;&#20998;&#20989;&#25968;&#21019;&#24314;&#38543;&#26426;LTR&#27169;&#22411;&#65292;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;&#35757;&#32451;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#20844;&#24179;LTR-RC&#20351;&#29992;&#26080;&#20998;&#24067;&#24335;&#39118;&#38505;&#25511;&#21046;&#26694;&#26550;&#23545;&#29992;&#25143;&#25351;&#23450;&#30340;&#25928;&#29992;&#25552;&#20379;&#26377;&#38480;&#30340;&#26679;&#26412;&#20445;&#35777;&#12290;&#36890;&#36807;&#21478;&#22806;&#32467;&#21512;Thresholded PL&#65288;TPL&#65289;&#27169;&#22411;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#25928;&#29992;&#21644;&#20844;&#24179;&#20043;&#38388;&#23454;&#29616;&#26377;&#25928;&#30340;&#26435;&#34913;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;FairLTR-RC&#22312;&#20844;&#24179;&#24615;&#21644;&#25928;&#29992;&#24615;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to Rank (LTR) methods are vital in online economies, affecting users and item providers. Fairness in LTR models is crucial to allocate exposure proportionally to item relevance. The deterministic ranking model can lead to unfair exposure distribution when items with the same relevance receive slightly different scores. Stochastic LTR models, incorporating the Plackett-Luce (PL) model, address fairness issues but have limitations in computational cost and performance guarantees. To overcome these limitations, we propose FairLTR-RC, a novel post-hoc model-agnostic method. FairLTR-RC leverages a pretrained scoring function to create a stochastic LTR model, eliminating the need for expensive training. Furthermore, FairLTR-RC provides finite-sample guarantees on a user-specified utility using distribution-free risk control framework. By additionally incorporating the Thresholded PL (TPL) model, we are able to achieve an effective trade-off between utility and fairness. Experimental
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#30417;&#30563;&#32479;&#35745;&#29305;&#24449;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#20256;&#24863;&#22120;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#20195;&#34920;&#24615;&#30340;&#21512;&#25104;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#30495;&#23454;&#19990;&#30028;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#27880;&#37322;&#22256;&#38590;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.05285</link><description>&lt;p&gt;
&#22522;&#20110;&#38750;&#30417;&#30563;&#32479;&#35745;&#29305;&#24449;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#20256;&#24863;&#22120;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Statistical Feature-Guided Diffusion Model for Sensor-based Human Activity Recognition. (arXiv:2306.05285v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#30417;&#30563;&#32479;&#35745;&#29305;&#24449;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#20256;&#24863;&#22120;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#20195;&#34920;&#24615;&#30340;&#21512;&#25104;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#30495;&#23454;&#19990;&#30028;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#27880;&#37322;&#22256;&#38590;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#35782;&#21035;&#20154;&#31867;&#27963;&#21160;&#26159;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#20294;&#33719;&#24471;&#22810;&#26679;&#21270;&#21644;&#26631;&#35760;&#20256;&#24863;&#22120;&#25968;&#25454;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#25104;&#26412;&#39640;&#26114;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#30417;&#30563;&#32479;&#35745;&#29305;&#24449;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#20256;&#24863;&#22120;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26088;&#22312;&#29983;&#25104;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#26631;&#35760;&#25968;&#25454;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#19982;&#30495;&#23454;&#19990;&#30028;&#20256;&#24863;&#22120;&#25968;&#25454;&#30456;&#20851;&#30340;&#31232;&#32570;&#24615;&#21644;&#27880;&#37322;&#22256;&#38590;&#24615;&#12290;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#26465;&#20214;&#35774;&#32622;&#20026;&#32479;&#35745;&#20449;&#24687;&#65288;&#20363;&#22914;&#24179;&#22343;&#20540;&#12289;&#26631;&#20934;&#20559;&#24046;&#12289;Z&#24471;&#20998;&#21644;&#20559;&#24230;&#65289;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#22810;&#26679;&#21270;&#21644;&#20195;&#34920;&#24615;&#30340;&#21512;&#25104;&#20256;&#24863;&#22120;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#20844;&#20849;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#20256;&#32479;&#36807;&#37319;&#26679;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognizing human activities from sensor data is a vital task in various domains, but obtaining diverse and labeled sensor data remains challenging and costly. In this paper, we propose an unsupervised statistical feature-guided diffusion model for sensor-based human activity recognition. The proposed method aims to generate synthetic time-series sensor data without relying on labeled data, addressing the scarcity and annotation difficulties associated with real-world sensor data. By conditioning the diffusion model on statistical information such as mean, standard deviation, Z-score, and skewness, we generate diverse and representative synthetic sensor data. We conducted experiments on public human activity recognition datasets and compared the proposed method to conventional oversampling methods and state-of-the-art generative adversarial network methods. The experimental results demonstrate that the proposed method can improve the performance of human activity recognition and outper
&lt;/p&gt;</description></item><item><title>MultiLegalPile&#26159;&#19968;&#20010;689GB&#30340;&#22810;&#35821;&#35328;&#27861;&#24459;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#26469;&#33258;17&#20010;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;24&#31181;&#35821;&#35328;&#30340;&#19981;&#21516;&#27861;&#24459;&#25968;&#25454;&#28304;&#65292;&#20801;&#35768;&#22312;&#20844;&#24179;&#20351;&#29992;&#19979;&#38024;&#23545;&#39044;&#35757;&#32451;NLP&#27169;&#22411;&#12290;&#35813;&#35821;&#26009;&#24211;&#20026;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25552;&#20379;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#65292;&#24182;&#22312;LexGLUE&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2306.02069</link><description>&lt;p&gt;
MultiLegalPile&#65306;689GB&#30340;&#22810;&#35821;&#35328;&#27861;&#24459;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
MultiLegalPile: A 689GB Multilingual Legal Corpus. (arXiv:2306.02069v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02069
&lt;/p&gt;
&lt;p&gt;
MultiLegalPile&#26159;&#19968;&#20010;689GB&#30340;&#22810;&#35821;&#35328;&#27861;&#24459;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#26469;&#33258;17&#20010;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;24&#31181;&#35821;&#35328;&#30340;&#19981;&#21516;&#27861;&#24459;&#25968;&#25454;&#28304;&#65292;&#20801;&#35768;&#22312;&#20844;&#24179;&#20351;&#29992;&#19979;&#38024;&#23545;&#39044;&#35757;&#32451;NLP&#27169;&#22411;&#12290;&#35813;&#35821;&#26009;&#24211;&#20026;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25552;&#20379;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#65292;&#24182;&#22312;LexGLUE&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#23545;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20026;&#27490;&#65292;&#19987;&#19994;&#39046;&#22495;&#65288;&#22914;&#27861;&#24459;&#65289;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#24456;&#23569;&#65292;&#32780;&#19988;&#32463;&#24120;&#20165;&#38480;&#20110;&#33521;&#35821;&#12290;&#25105;&#20204;&#25972;&#29702;&#24182;&#21457;&#24067;&#20102;MultiLegalPile&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;17&#20010;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;24&#31181;&#35821;&#35328;&#30340;689GB&#35821;&#26009;&#24211;&#12290;MultiLegalPile&#35821;&#26009;&#24211;&#21253;&#25324;&#21508;&#31181;&#35768;&#21487;&#35777;&#30340;&#19981;&#21516;&#27861;&#24459;&#25968;&#25454;&#28304;&#65292;&#20801;&#35768;&#22312;&#20844;&#24179;&#20351;&#29992;&#19979;&#38024;&#23545;&#39044;&#35757;&#32451;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#27169;&#22411;&#65292;&#23545;&#20110;Eurlex Resources&#21644;Legal mC4&#23376;&#38598;&#25317;&#26377;&#26356;&#23485;&#26494;&#30340;&#35768;&#21487;&#35777;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;RoBERTa&#27169;&#22411;&#21644;&#19968;&#20010;&#22810;&#35821;&#35328;Longformer&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#20998;&#21035;&#22312;&#27599;&#31181;&#29305;&#23450;&#35821;&#35328;&#23376;&#38598;&#19978;&#36827;&#34892;&#20102;24&#20010;&#21333;&#35821;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;LEXTREME&#19978;&#23545;&#23427;&#20204;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;LexGLUE&#19978;&#23545;&#33521;&#35821;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;LEXTREME&#19978;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;(SotA)&#65292;&#33521;&#35821;&#27169;&#22411;&#21017;&#22312;LexGLUE&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;&#25105;&#20204;&#23558;&#25968;&#25454;&#38598;&#12289;&#35757;&#32451;&#27169;&#22411;&#21644;&#20195;&#30721;&#20840;&#37096;&#37322;&#25918;&#22312;&#26368;&#24320;&#25918;&#30340;&#35768;&#21487;&#35777;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large, high-quality datasets are crucial for training Large Language Models (LLMs). However, so far, there are few datasets available for specialized critical domains such as law and the available ones are often only for the English language. We curate and release MultiLegalPile, a 689GB corpus in 24 languages from 17 jurisdictions. The MultiLegalPile corpus, which includes diverse legal data sources with varying licenses, allows for pretraining NLP models under fair use, with more permissive licenses for the Eurlex Resources and Legal mC4 subsets. We pretrain two RoBERTa models and one Longformer multilingually, and 24 monolingual models on each of the language-specific subsets and evaluate them on LEXTREME. Additionally, we evaluate the English and multilingual models on LexGLUE. Our multilingual models set a new SotA on LEXTREME and our English models on LexGLUE. We release the dataset, the trained models, and all of the code under the most open possible licenses.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20811;&#26381;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#31283;&#23450;&#24615;&#24046;&#36317;&#65292;&#24182;&#36890;&#36807;&#21457;&#29616;&#19968;&#31181;&#26174;&#33879;&#20943;&#23569;&#36825;&#31181;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#23454;&#39564;&#20013;&#22823;&#24133;&#20943;&#23569;&#20102;&#32593;&#32476;&#26356;&#26032;&#30340;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.01904</link><description>&lt;p&gt;
&#20811;&#26381;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#31283;&#23450;&#24615;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Overcoming the Stability Gap in Continual Learning. (arXiv:2306.01904v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20811;&#26381;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#31283;&#23450;&#24615;&#24046;&#36317;&#65292;&#24182;&#36890;&#36807;&#21457;&#29616;&#19968;&#31181;&#26174;&#33879;&#20943;&#23569;&#36825;&#31181;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#23454;&#39564;&#20013;&#22823;&#24133;&#20943;&#23569;&#20102;&#32593;&#32476;&#26356;&#26032;&#30340;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#38543;&#30528;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#22686;&#38271;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24448;&#24448;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#12290;&#32771;&#34385;&#21040;&#37325;&#26032;&#35757;&#32451;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#20154;&#20204;&#35748;&#20026;&#36830;&#32493;&#23398;&#20064;&#21487;&#20197;&#20351;&#32593;&#32476;&#26356;&#26032;&#26356;&#21152;&#39640;&#25928;&#12290;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#38556;&#30861;&#26159;&#31283;&#23450;&#24615;&#24046;&#36317;&#65292;&#21363;&#22312;&#26356;&#26032;&#26032;&#25968;&#25454;&#26102;&#65292;&#20808;&#21069;&#23398;&#20064;&#30340;&#25968;&#25454;&#24615;&#33021;&#20250;&#19979;&#38477;&#65292;&#28982;&#21518;&#25165;&#24471;&#20197;&#24674;&#22797;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#20943;&#23569;&#32593;&#32476;&#26356;&#26032;&#30340;&#27425;&#25968;&#65292;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#32531;&#35299;&#31283;&#23450;&#24615;&#24046;&#36317;&#65292;&#24182;&#27979;&#35797;&#20102;&#22810;&#31181;&#20551;&#35774;&#20197;&#20102;&#35299;&#20854;&#20135;&#29983;&#21407;&#22240;&#12290;&#36825;&#20351;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#26174;&#33879;&#20943;&#23569;&#31283;&#23450;&#24615;&#24046;&#36317;&#30340;&#26041;&#27861;&#12290;&#22312;&#22823;&#35268;&#27169;&#30340;&#22686;&#37327;&#31867;&#21035;&#23398;&#20064;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#36830;&#32493;&#23398;&#20064;&#25152;&#38656;&#30340;&#32593;&#32476;&#26356;&#26032;&#27425;&#25968;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26377;&#21487;&#33021;&#25512;&#21160;&#36830;&#32493;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world applications, deep neural networks are retrained from scratch as a dataset grows in size. Given the computational expense for retraining networks, it has been argued that continual learning could make updating networks more efficient. An obstacle to achieving this goal is the stability gap, which refers to an observation that when updating on new data, performance on previously learned data degrades before recovering. Addressing this problem would enable learning new data with fewer network updates, resulting in increased computational efficiency. We study how to mitigate the stability gap. We test a variety of hypotheses to understand why the stability gap occurs. This leads us to discover a method that vastly reduces this gap. In large-scale class incremental learning experiments, we are able to significantly reduce the number of network updates needed for continual learning. Our work has the potential to advance the state-of-the-art in continual learning for real-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;Vandermonde&#31070;&#32463;&#31639;&#23376;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#38750;&#22343;&#21248;&#20998;&#24067;&#28857;&#19978;&#30340;&#36755;&#20837;&#25968;&#25454;&#65292;&#21516;&#26102;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#19978;&#30456;&#36739;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#26377;&#25152;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.19663</link><description>&lt;p&gt;
Vandermonde&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Vandermonde Neural Operators. (arXiv:2305.19663v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;Vandermonde&#31070;&#32463;&#31639;&#23376;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#38750;&#22343;&#21248;&#20998;&#24067;&#28857;&#19978;&#30340;&#36755;&#20837;&#25968;&#25454;&#65292;&#21516;&#26102;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#19978;&#30456;&#36739;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#26377;&#25152;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Fourier&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#24050;&#25104;&#20026;&#38750;&#24120;&#21463;&#27426;&#36814;&#30340;&#26426;&#22120;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#25805;&#20316;&#31526;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#22312;PDE&#20013;&#20986;&#29616;&#30340;&#25805;&#20316;&#31526;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;FNO&#20381;&#36182;&#20110;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#20197;&#23454;&#29616;&#35745;&#31639;&#25928;&#29575;&#65292;&#25152;&#20197;&#35813;&#20307;&#31995;&#32467;&#26500;&#21487;&#33021;&#20165;&#38480;&#20110;&#31515;&#21345;&#23572;&#32593;&#26684;&#19978;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;FNO&#25512;&#24191;&#21040;&#22788;&#29702;&#20998;&#24067;&#22312;&#38750;&#22343;&#21248;&#28857;&#20998;&#24067;&#19978;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#31216;&#20026;Vandermonde&#31070;&#32463;&#36816;&#31639;&#31526;&#65288;VNO&#65289;&#65292;&#21033;&#29992;Vandermonde&#32467;&#26500;&#30697;&#38453;&#26469;&#39640;&#25928;&#22320;&#35745;&#31639;&#27491;&#21521;&#21644;&#21453;&#21521;&#30340;&#20613;&#37324;&#21494;&#21464;&#25442;&#65292;&#21363;&#20351;&#22312;&#20219;&#24847;&#20998;&#24067;&#30340;&#28857;&#19978;&#20063;&#21487;&#20197;&#22914;&#27492;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#35777;&#26126;VNO&#21487;&#20197;&#27604;FNO&#24555;&#24471;&#22810;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#25913;&#36827;&#20102;&#21487;&#27604;&#30340;&#38750;&#22343;&#21248;&#26041;&#27861;&#65288;&#22914;Geo-FNO&#65289;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fourier Neural Operators (FNOs) have emerged as very popular machine learning architectures for learning operators, particularly those arising in PDEs. However, as FNOs rely on the fast Fourier transform for computational efficiency, the architecture can be limited to input data on equispaced Cartesian grids. Here, we generalize FNOs to handle input data on non-equispaced point distributions. Our proposed model, termed as Vandermonde Neural Operator (VNO), utilizes Vandermonde-structured matrices to efficiently compute forward and inverse Fourier transforms, even on arbitrarily distributed points. We present numerical experiments to demonstrate that VNOs can be significantly faster than FNOs, while retaining comparable accuracy, and improve upon accuracy of comparable non-equispaced methods such as the Geo-FNO.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20449;&#24230;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#40657;&#30418;&#27169;&#22411;&#20013;&#32622;&#20449;&#24230;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#36873;&#25321;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.19187</link><description>&lt;p&gt;
&#29983;&#25104;&#21487;&#20449;&#30340;&#25991;&#26412;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models. (arXiv:2305.19187v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20449;&#24230;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#40657;&#30418;&#27169;&#22411;&#20013;&#32622;&#20449;&#24230;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#36873;&#25321;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#19987;&#38376;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#35780;&#20272;LLMs&#29983;&#25104;&#30340;&#32467;&#26524;&#30340;&#21487;&#20449;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#30740;&#31350;&#20063;&#36739;&#23569;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#25991;&#29486;&#36890;&#24120;&#20551;&#23450;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#30333;&#30418;&#35775;&#38382;&#65292;&#36825;&#35201;&#20040;&#26159;&#30001;&#20110;&#26368;&#26032;&#30340;LLMs&#30340;&#23553;&#38381;&#28304;&#20195;&#30721;&#30340;&#24615;&#36136;&#65292;&#35201;&#20040;&#26159;&#30001;&#20110;&#35745;&#31639;&#38480;&#21046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#40657;&#30418;LLMs&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#21306;&#20998;&#20102;&#20004;&#31181;&#23494;&#20999;&#30456;&#20851;&#30340;&#27010;&#24565;: &#21482;&#19982;&#36755;&#20837;&#26377;&#20851;&#30340;&#8220;&#19981;&#30830;&#23450;&#24615;&#8221;&#21644;&#36824;&#19982;&#29983;&#25104;&#30340;&#22238;&#22797;&#26377;&#20851;&#30340;&#8220;&#32622;&#20449;&#24230;&#8221;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#20960;&#20010;&#32622;&#20449;&#24230;/&#19981;&#30830;&#23450;&#24230;&#25351;&#26631;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#8220;&#36873;&#25321;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#8221;&#65292;&#20854;&#20013;&#19981;&#21487;&#38752;&#30340;&#32467;&#26524;&#21487;&#20197;&#34987;&#24573;&#30053;&#25110;&#32773;&#31227;&#20132;&#32473;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) specializing in natural language generation (NLG) have recently started exhibiting promising capabilities across a variety of domains. However, gauging the trustworthiness of responses generated by LLMs remains an open challenge, with limited research on uncertainty quantification for NLG. Furthermore, existing literature typically assumes white-box access to language models, which is becoming unrealistic either due to the closed-source nature of the latest LLMs or due to computational constraints. In this work, we investigate uncertainty quantification in NLG for $\textit{black-box}$ LLMs. We first differentiate two closely-related notions: $\textit{uncertainty}$, which depends only on the input, and $\textit{confidence}$, which additionally depends on the generated response. We then propose and compare several confidence/uncertainty metrics, applying them to $\textit{selective NLG}$, where unreliable results could either be ignored or yielded for further 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#33410;&#28857;&#25490;&#24207;&#26041;&#27861;&#65288;MGL2Rank&#65289;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#36947;&#36335;&#32593;&#32476;&#30340;&#20016;&#23500;&#29305;&#24449;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.14375</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#22270;&#34701;&#21512;&#30340;&#36947;&#36335;&#32593;&#32476;&#33410;&#28857;&#37325;&#35201;&#24615;&#25490;&#24207;&#26041;&#27861;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to Rank the Importance of Nodes in Road Networks Based on Multi-Graph Fusion. (arXiv:2305.14375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#33410;&#28857;&#25490;&#24207;&#26041;&#27861;&#65288;MGL2Rank&#65289;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#36947;&#36335;&#32593;&#32476;&#30340;&#20016;&#23500;&#29305;&#24449;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22478;&#24066;&#35268;&#21010;&#39046;&#22495;&#20013;&#65292;&#35782;&#21035;&#20855;&#26377;&#24378;&#20256;&#25773;&#33021;&#21147;&#30340;&#37325;&#35201;&#33410;&#28857;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35838;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#33410;&#28857;&#37325;&#35201;&#24615;&#30340;&#26041;&#27861;&#20165;&#32771;&#34385;&#25299;&#25169;&#20449;&#24687;&#21644;&#20132;&#36890;&#27969;&#37327;&#65292;&#24573;&#30053;&#20102;&#36947;&#36335;&#32593;&#32476;&#30340;&#22810;&#26679;&#24615;&#29305;&#24449;&#65292;&#22914;&#36710;&#36947;&#25968;&#37327;&#21644;&#36947;&#36335;&#27573;&#30340;&#24179;&#22343;&#36895;&#24230;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#33410;&#28857;&#25490;&#24207;&#26041;&#27861;&#65288;MGL2Rank&#65289;&#65292;&#23427;&#38598;&#25104;&#20102;&#36947;&#36335;&#32593;&#32476;&#30340;&#20016;&#23500;&#29305;&#24449;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#20010;&#37319;&#26679;&#31639;&#27861;&#65288;MGWalk&#65289;&#65292;&#21033;&#29992;&#22810;&#22270;&#34701;&#21512;&#26469;&#24314;&#31435;&#22522;&#20110;&#23646;&#24615;&#30340;&#36947;&#36335;&#27573;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23884;&#20837;&#27169;&#22359;&#65292;&#29992;&#20110;&#23398;&#20064;&#27599;&#20010;&#36947;&#36335;&#27573;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#26368;&#21518;&#65292;&#24471;&#21040;&#30340;&#33410;&#28857;&#34920;&#31034;&#29992;&#20110;&#23398;&#20064;&#36947;&#36335;&#27573;&#30340;&#37325;&#35201;&#24615;&#25490;&#24207;&#12290;&#25105;&#20204;&#22312;&#20013;&#22269;&#27784;&#38451;&#24066;&#21306;&#22495;&#36947;&#36335;&#32593;&#32476;&#19978;&#36827;&#34892;&#20102;&#20223;&#30495;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;MGL2Rank&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MGL2Rank&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#33410;&#28857;&#25490;&#24207;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying important nodes with strong propagation capabilities in road networks is a significant topic in the field of urban planning. However, existing methods for evaluating nodes importance consider only topological information and traffic volumes, ignoring the diversity of characteristics in road networks, such as the number of lanes and average speed of road segments, limiting their performance. To address this issue, this paper proposes a graph learning-based node ranking method (MGL2Rank) that integrates the rich characteristics of the road network. In this method, we first develop a sampling algorithm (MGWalk) that utilizes multi-graph fusion to establish association between road segments based on their attributes. Then, an embedding module is proposed to learn latent representation for each road segment. Finally, the obtained node representation is used to learn importance ranking of road segments. We conduct simulation experiments on the regional road network of Shenyang ci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;&#27969;&#39640;&#25928;&#23398;&#20064;&#8221;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#26088;&#22312;&#35299;&#20915;&#20174;&#25968;&#25454;&#27969;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#29575;&#38382;&#39064;&#65292;&#20854;&#27867;&#21270;&#24615;&#33021;&#19981;&#20165;&#21462;&#20915;&#20110;&#25509;&#25910;&#21040;&#20102;&#22810;&#23569;&#25968;&#25454;&#65292;&#32780;&#19988;&#36824;&#21462;&#20915;&#20110;&#26377;&#22810;&#23569;&#25968;&#25454;&#33021;&#22815;&#21450;&#26102;&#26377;&#25928;&#22320;&#34987;&#21033;&#29992;&#65292;&#21152;&#19978;&#36164;&#28304;&#21644;&#36895;&#24230;&#30340;&#32771;&#34385;&#12290;</title><link>http://arxiv.org/abs/2305.02217</link><description>&lt;p&gt;
&#27969;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Stream Efficient Learning. (arXiv:2305.02217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;&#27969;&#39640;&#25928;&#23398;&#20064;&#8221;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#26088;&#22312;&#35299;&#20915;&#20174;&#25968;&#25454;&#27969;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#29575;&#38382;&#39064;&#65292;&#20854;&#27867;&#21270;&#24615;&#33021;&#19981;&#20165;&#21462;&#20915;&#20110;&#25509;&#25910;&#21040;&#20102;&#22810;&#23569;&#25968;&#25454;&#65292;&#32780;&#19988;&#36824;&#21462;&#20915;&#20110;&#26377;&#22810;&#23569;&#25968;&#25454;&#33021;&#22815;&#21450;&#26102;&#26377;&#25928;&#22320;&#34987;&#21033;&#29992;&#65292;&#21152;&#19978;&#36164;&#28304;&#21644;&#36895;&#24230;&#30340;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#25968;&#25454;&#24448;&#24448;&#38543;&#30528;&#26102;&#38388;&#30340;&#31215;&#32047;&#20197;&#27969;&#30340;&#24418;&#24335;&#36827;&#34892;&#12290;&#19982;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20851;&#27880;&#20110;&#20174;&#32473;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#19981;&#21516;&#65292;&#20174;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#19981;&#33021;&#24573;&#35270;&#27969;&#20837;&#30340;&#25968;&#25454;&#27969;&#21487;&#33021;&#26159;&#26080;&#20241;&#27490;&#30340;&#12289;&#35268;&#27169;&#24040;&#22823;&#12289;&#21464;&#21270;&#26410;&#30693;&#65292;&#24182;&#19988;&#20551;&#35774;&#26377;&#36275;&#22815;&#30340;&#35745;&#31639;/&#23384;&#20648;&#36164;&#28304;&#21487;&#20197;&#21450;&#26102;&#22788;&#29702;&#25152;&#26377;&#25509;&#25910;&#21040;&#30340;&#25968;&#25454;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22240;&#27492;&#65292;&#20174;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#19981;&#20165;&#21462;&#20915;&#20110;&#25509;&#25910;&#21040;&#20102;&#22810;&#23569;&#25968;&#25454;&#65292;&#32780;&#19988;&#21462;&#20915;&#20110;&#26377;&#22810;&#23569;&#25968;&#25454;&#33021;&#22815;&#34987;&#21450;&#26102;&#22320;&#26377;&#25928;&#21033;&#29992;&#65292;&#21152;&#19978;&#36164;&#28304;&#21644;&#36895;&#24230;&#30340;&#32771;&#34385;&#65292;&#20877;&#21152;&#19978;&#23398;&#20064;&#31639;&#27861;&#30340;&#33021;&#21147;&#21644;&#38382;&#39064;&#30340;&#22797;&#26434;&#24230;&#12290;&#20026;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#25105;&#20204;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#21534;&#21520;&#37327;&#30340;&#27010;&#24565;&#65292;&#23450;&#20041;&#20102;&#27969;&#39640;&#25928;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21021;&#27493;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data in many real-world applications are often accumulated over time, like a stream. In contrast to conventional machine learning studies that focus on learning from a given training data set, learning from data streams cannot ignore the fact that the incoming data stream can be potentially endless with overwhelming size and unknown changes, and it is impractical to assume to have sufficient computational/storage resource such that all received data can be handled in time. Thus, the generalization performance of learning from data streams depends not only on how many data have been received, but also on how many data can be well exploited timely, with resource and rapidity concerns, in addition to the ability of learning algorithm and complexity of the problem. For this purpose, in this article we introduce the notion of machine learning throughput, define Stream Efficient Learning and present a preliminary theoretical framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#37096;&#20998;&#21442;&#19982;&#29615;&#22659;&#19979;&#30340;&#28151;&#21512;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#21253;&#25324;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#21644;&#37096;&#20998;&#32858;&#21512;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#21152;&#24555;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#19988;&#19981;&#24433;&#21709;&#27169;&#22411;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.05397</link><description>&lt;p&gt;
&#37096;&#20998;&#21442;&#19982;&#19979;&#21152;&#36895;&#28151;&#21512;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Accelerating Hybrid Federated Learning Convergence under Partial Participation. (arXiv:2304.05397v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#37096;&#20998;&#21442;&#19982;&#29615;&#22659;&#19979;&#30340;&#28151;&#21512;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#21253;&#25324;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#21644;&#37096;&#20998;&#32858;&#21512;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#21152;&#24555;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#19988;&#19981;&#24433;&#21709;&#27169;&#22411;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32852;&#37030;&#23398;&#20064;(Federated Learning&#65292;FL)&#24050;&#25104;&#20026;&#19968;&#31181;&#24120;&#35265;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#12290;FL&#28041;&#21450;&#19968;&#32452;&#26377;&#30528;&#20998;&#25955;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#65292;&#36890;&#36807;&#38598;&#20013;&#26381;&#21153;&#22120;&#30340;&#21327;&#35843;&#21512;&#20316;&#23398;&#20064;&#19968;&#20010;&#20844;&#20849;&#27169;&#22411;&#65292;&#20854;&#30446;&#30340;&#26159;&#36890;&#36807;&#30830;&#20445;&#26412;&#22320;&#25968;&#25454;&#38598;&#27704;&#36828;&#19981;&#20250;&#31163;&#24320;&#23458;&#25143;&#31471;&#65292;&#21482;&#26377;&#26381;&#21153;&#22120;&#36827;&#34892;&#27169;&#22411;&#32858;&#21512;&#26469;&#20445;&#25252;&#23458;&#25143;&#31471;&#30340;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#26381;&#21153;&#22120;&#21487;&#33021;&#33021;&#22815;&#25910;&#38598;&#23569;&#37327;&#25968;&#25454;&#20197;&#36817;&#20284;&#24635;&#20307;&#20998;&#24067;&#65292;&#24182;&#20855;&#26377;&#26356;&#24378;&#30340;&#35745;&#31639;&#33021;&#21147;&#26469;&#25191;&#34892;&#23398;&#20064;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#32858;&#28966;&#20110;&#28151;&#21512;FL&#26694;&#26550;&#12290;&#22312;&#20808;&#21069;&#28151;&#21512;FL&#24037;&#20316;&#20013;&#65292;&#24050;&#32463;&#35777;&#26126;&#20102;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#30340;&#20132;&#26367;&#35757;&#32451;&#21487;&#20197;&#22686;&#21152;&#25910;&#25947;&#36895;&#24230;&#65292;&#20294;&#26159;&#23427;&#20165;&#20851;&#27880;&#23458;&#25143;&#31471;&#23436;&#20840;&#21442;&#19982;&#30340;&#24773;&#20917;&#65292;&#32780;&#24573;&#30053;&#20102;&#37096;&#20998;&#21442;&#19982;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#21644;&#37096;&#20998;&#32858;&#21512;&#26041;&#27861;&#26469;&#25913;&#36827;&#28151;&#21512;FL&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#30340;&#21516;&#26102;&#19981;&#20250;&#36896;&#25104;&#22826;&#22823;&#31934;&#24230;&#25439;&#22833;&#65292;&#19982;&#20256;&#32479;FL&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, Federated Learning (FL) has become a popular distributed machine learning paradigm. FL involves a group of clients with decentralized data who collaborate to learn a common model under the coordination of a centralized server, with the goal of protecting clients' privacy by ensuring that local datasets never leave the clients and that the server only performs model aggregation. However, in realistic scenarios, the server may be able to collect a small amount of data that approximately mimics the population distribution and has stronger computational ability to perform the learning process. To address this, we focus on the hybrid FL framework in this paper. While previous hybrid FL work has shown that the alternative training of clients and server can increase convergence speed, it has focused on the scenario where clients fully participate and ignores the negative effect of partial participation. In this paper, we provide theoretical analysis of hybrid FL under
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;X&#23556;&#32447;CT&#22270;&#20687;&#25581;&#31034;&#36203;&#24211;&#20848;&#23612;&#22982;&#32440;&#33609;&#21367;&#38544;&#34255;&#25991;&#26412;&#30340;&#36719;&#20214;&#31649;&#36947;&#21644;&#25968;&#25454;&#38598;&#12290;&#20182;&#20204;&#36816;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#20960;&#20309;&#26694;&#26550;&#30340;&#26041;&#27861;&#26816;&#27979;&#8220;&#19981;&#21487;&#35265;&#8221;&#30340;&#30899;&#22696;&#65292;&#36798;&#21040;&#20102;&#20154;&#31867;&#19987;&#23478;&#26631;&#35760;&#32773;&#38590;&#20197;&#36798;&#21040;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.02084</link><description>&lt;p&gt;
&#12298;EduceLab-Scrolls&#65306;&#21033;&#29992;X&#23556;&#32447;CT&#20174;&#36203;&#24211;&#20848;&#23612;&#22982;&#32440;&#33609;&#21367;&#20013;&#21487;&#39564;&#35777;&#22320;&#24674;&#22797;&#25991;&#26412;&#12299;
&lt;/p&gt;
&lt;p&gt;
EduceLab-Scrolls: Verifiable Recovery of Text from Herculaneum Papyri using X-ray CT. (arXiv:2304.02084v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02084
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;X&#23556;&#32447;CT&#22270;&#20687;&#25581;&#31034;&#36203;&#24211;&#20848;&#23612;&#22982;&#32440;&#33609;&#21367;&#38544;&#34255;&#25991;&#26412;&#30340;&#36719;&#20214;&#31649;&#36947;&#21644;&#25968;&#25454;&#38598;&#12290;&#20182;&#20204;&#36816;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#20960;&#20309;&#26694;&#26550;&#30340;&#26041;&#27861;&#26816;&#27979;&#8220;&#19981;&#21487;&#35265;&#8221;&#30340;&#30899;&#22696;&#65292;&#36798;&#21040;&#20102;&#20154;&#31867;&#19987;&#23478;&#26631;&#35760;&#32773;&#38590;&#20197;&#36798;&#21040;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25581;&#31034;&#36203;&#24211;&#20848;&#23612;&#22982;&#32440;&#33609;&#21367;&#38544;&#34255;&#25991;&#26412;&#30340;&#23436;&#25972;&#36719;&#20214;&#31649;&#36947;&#12290;&#36825;&#20010;&#22686;&#24378;&#30340;&#34394;&#25311;&#23637;&#24320;&#27969;&#27700;&#32447;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#20309;&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#23558;&#19977;&#32500;&#21644;&#20108;&#32500;&#22270;&#20687;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;EduceLab-Scrolls&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#20195;&#34920;&#20102;&#20108;&#21313;&#24180;&#26469;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#30740;&#31350;&#21162;&#21147;&#12290;EduceLab-Scrolls&#21253;&#21547;&#20102;&#19968;&#32452;&#23567;&#30862;&#29255;&#21644;&#23436;&#25972;&#21367;&#36724;&#30340;&#20307;&#31215;X&#23556;&#32447;CT&#22270;&#20687;&#12290;&#35813;&#25968;&#25454;&#38598;&#36824;&#21253;&#21547;&#29992;&#20110;&#30417;&#30563;&#35757;&#32451;&#27833;&#22696;&#26816;&#27979;&#27169;&#22411;&#30340;&#20108;&#32500;&#22270;&#20687;&#26631;&#31614;&#12290;&#36890;&#36807;&#23558;&#21367;&#36724;&#30862;&#29255;&#30340;&#39057;&#35889;&#29031;&#29255;&#19982;&#30456;&#21516;&#30862;&#29255;&#30340;X&#23556;&#32447;CT&#22270;&#20687;&#23545;&#20934;&#65292;&#20174;&#32780;&#21019;&#24314;&#20102;&#19968;&#20010;&#21487;&#26426;&#22120;&#23398;&#20064;&#30340;&#22270;&#20687;&#31354;&#38388;&#21644;&#27169;&#24577;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#36825;&#31181;&#23545;&#20934;&#20801;&#35768;&#26377;&#30417;&#30563;&#22320;&#23398;&#20064;&#26816;&#27979;X&#23556;&#32447;CT&#20013;&#8220;&#38544;&#24418;&#8221;&#30899;&#22696;&#30340;&#20219;&#21153;&#65292;&#21363;&#20351;&#23545;&#20110;&#20154;&#31867;&#19987;&#23478;&#26631;&#35760;&#32773;&#26469;&#35828;&#20063;&#26159;&#8220;&#19981;&#21487;&#33021;&#8221;&#30340;&#20219;&#21153;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23545;&#40784;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a complete software pipeline for revealing the hidden texts of the Herculaneum papyri using X-ray CT images. This enhanced virtual unwrapping pipeline combines machine learning with a novel geometric framework linking 3D and 2D images. We also present EduceLab-Scrolls, a comprehensive open dataset representing two decades of research effort on this problem. EduceLab-Scrolls contains a set of volumetric X-ray CT images of both small fragments and intact, rolled scrolls. The dataset also contains 2D image labels that are used in the supervised training of an ink detection model. Labeling is enabled by aligning spectral photography of scroll fragments with X-ray CT images of the same fragments, thus creating a machine-learnable mapping between image spaces and modalities. This alignment permits supervised learning for the detection of "invisible" carbon ink in X-ray CT, a task that is "impossible" even for human expert labelers. To our knowledge, this is the first aligned datas
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#20004;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#32047;&#35745;&#30446;&#26631;&#20540;&#26368;&#23567;&#21270;&#65292;&#21516;&#26102;&#20445;&#35777;&#38271;&#26399;&#24179;&#22343;&#31532;&#20108;&#38454;&#27573;&#20915;&#31574;&#32467;&#26524;&#23646;&#20110;&#19968;&#20010;&#38598;&#21512;&#12290;&#37319;&#29992;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#20174;&#22312;&#32447;&#20004;&#38454;&#27573;&#38382;&#39064;&#20013;&#24320;&#21457;&#22312;&#32447;&#31639;&#27861;&#65292;&#20854;&#36951;&#25022;&#30028;&#21487;&#20197;&#38477;&#33267;&#23884;&#20837;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#65292;&#24182;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#33719;&#24471;&#20102;&#26032;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.00997</link><description>&lt;p&gt;
&#21463;&#38480;&#22312;&#32447;&#20004;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#65306;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33719;&#24471;&#36817;&#20284;&#26368;&#20248;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Constrained Online Two-stage Stochastic Optimization: Near Optimal Algorithms via Adversarial Learning. (arXiv:2302.00997v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00997
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20004;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#32047;&#35745;&#30446;&#26631;&#20540;&#26368;&#23567;&#21270;&#65292;&#21516;&#26102;&#20445;&#35777;&#38271;&#26399;&#24179;&#22343;&#31532;&#20108;&#38454;&#27573;&#20915;&#31574;&#32467;&#26524;&#23646;&#20110;&#19968;&#20010;&#38598;&#21512;&#12290;&#37319;&#29992;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#20174;&#22312;&#32447;&#20004;&#38454;&#27573;&#38382;&#39064;&#20013;&#24320;&#21457;&#22312;&#32447;&#31639;&#27861;&#65292;&#20854;&#36951;&#25022;&#30028;&#21487;&#20197;&#38477;&#33267;&#23884;&#20837;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#65292;&#24182;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#33719;&#24471;&#20102;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#22312;&#32447;&#20004;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20855;&#26377;&#26377;&#38480;&#30340;$T$&#26399;&#32039;&#32422;&#26463;&#26465;&#20214;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;&#27573;&#65292;&#25105;&#20204;&#20808;&#20316;&#20986;&#31532;&#19968;&#38454;&#27573;&#20915;&#31574;&#65292;&#28982;&#21518;&#35266;&#23519;&#27169;&#22411;&#21442;&#25968;&#30340;&#23454;&#29616;&#65292;&#26368;&#21518;&#20174;&#21462;&#20915;&#20110;&#31532;&#19968;&#38454;&#27573;&#20915;&#31574;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#21487;&#34892;&#38598;&#20013;&#20570;&#20986;&#31532;&#20108;&#38454;&#27573;&#20915;&#31574;&#12290;&#25105;&#20204;&#26088;&#22312;&#26368;&#23567;&#21270;&#32047;&#35745;&#30446;&#26631;&#20540;&#65292;&#21516;&#26102;&#20445;&#35777;&#38271;&#26399;&#24179;&#22343;&#30340;&#31532;&#20108;&#38454;&#27573;&#20915;&#31574;&#23646;&#20110;&#19968;&#20010;&#38598;&#21512;&#12290;&#25105;&#20204;&#21033;&#29992;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#20174;&#22312;&#32447;&#20004;&#38454;&#27573;&#38382;&#39064;&#20013;&#24320;&#21457;&#22312;&#32447;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#21487;&#20197;&#38477;&#33267;&#23884;&#20837;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#25105;&#20204;&#37117;&#33719;&#24471;&#20102;&#26032;&#30340;&#32467;&#26524;&#12290;&#24403;&#27599;&#20010;&#26102;&#38388;&#27573;&#30340;&#27169;&#22411;&#21442;&#25968;&#37117;&#26159;&#20174;&#30456;&#21516;&#30340;&#20998;&#24067;&#20013;&#25277;&#21462;&#30340;&#26102;&#20505;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;$O&#65288;\sqrt{T}&#65289;$&#36951;&#25022;&#30028;&#65292;&#36825;&#27604;&#20043;&#21069;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#30028;&#26377;&#25152;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36824;&#21487;&#20197;&#25269;&#25239;&#27169;&#22411;&#30340;&#25932;&#23545;&#24615;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider an online two-stage stochastic optimization with long-term constraints over a finite horizon of $T$ periods. At each period, we take the first-stage action, observe a model parameter realization and then take the second-stage action from a feasible set that depends both on the first-stage decision and the model parameter. We aim to minimize the cumulative objective value while guaranteeing that the long-term average second-stage decision belongs to a set. We develop online algorithms for the online two-stage problem from adversarial learning algorithms. Also, the regret bound of our algorithm cam be reduced to the regret bound of embedded adversarial learning algorithms. Based on our framework, we obtain new results under various settings. When the model parameter at each period is drawn from identical distributions, we derive state-of-art $O(\sqrt{T})$ regret that improves previous bounds under special cases. Our algorithm is also robust to adversarial corruptions of model
&lt;/p&gt;</description></item><item><title>PECAN&#26159;&#19968;&#31181;&#26377;&#25928;&#19988;&#32463;&#36807;&#35748;&#35777;&#30340;&#21518;&#38376;&#25915;&#20987;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#30456;&#20132;&#20998;&#21306;&#19978;&#35757;&#32451;&#19968;&#32452;&#31070;&#32463;&#32593;&#32476;&#24182;&#24212;&#29992;&#27979;&#35797;&#26102;&#38388;&#36867;&#36991;&#35748;&#35777;&#25216;&#26415;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#38450;&#24481;&#24378;&#24230;&#21644;&#25928;&#29575;&#65292;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.11824</link><description>&lt;p&gt;
PECAN: &#19968;&#31181;&#38024;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#30830;&#23450;&#24615;&#35748;&#35777;&#38450;&#24481;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PECAN: A Deterministic Certified Defense Against Backdoor Attacks. (arXiv:2301.11824v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11824
&lt;/p&gt;
&lt;p&gt;
PECAN&#26159;&#19968;&#31181;&#26377;&#25928;&#19988;&#32463;&#36807;&#35748;&#35777;&#30340;&#21518;&#38376;&#25915;&#20987;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#30456;&#20132;&#20998;&#21306;&#19978;&#35757;&#32451;&#19968;&#32452;&#31070;&#32463;&#32593;&#32476;&#24182;&#24212;&#29992;&#27979;&#35797;&#26102;&#38388;&#36867;&#36991;&#35748;&#35777;&#25216;&#26415;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#38450;&#24481;&#24378;&#24230;&#21644;&#25928;&#29575;&#65292;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#24694;&#24847;&#27745;&#26579;&#35757;&#32451;&#38598;&#24182;&#22312;&#27979;&#35797;&#36755;&#20837;&#20013;&#25554;&#20837;&#35302;&#21457;&#22120;&#20197;&#25913;&#21464;&#21463;&#23475;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#38450;&#24481;&#26041;&#27861;&#35201;&#20040;&#27809;&#26377;&#25552;&#20379;&#27491;&#24335;&#30340;&#20445;&#35777;&#65292;&#35201;&#20040;&#20855;&#26377;&#35745;&#31639;&#26114;&#36149;&#19988;&#26080;&#25928;&#30340;&#27010;&#29575;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#32463;&#36807;&#35748;&#35777;&#30340;&#21518;&#38376;&#25915;&#20987;&#38450;&#24481;&#26041;&#27861;PECAN&#12290;PECAN&#30340;&#26680;&#24515;&#27934;&#35265;&#26159;&#22312;&#25968;&#25454;&#30340;&#19981;&#30456;&#20132;&#20998;&#21306;&#19978;&#35757;&#32451;&#19968;&#32452;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#24212;&#29992;&#29616;&#25104;&#30340;&#27979;&#35797;&#26102;&#38388;&#36867;&#36991;&#35748;&#35777;&#25216;&#26415;&#12290;&#25105;&#20204;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;PECAN&#12290;&#32467;&#26524;&#34920;&#26126;PECAN&#22312;&#38450;&#24481;&#24378;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#32463;&#36807;&#35748;&#35777;&#30340;&#21518;&#38376;&#38450;&#24481;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#30495;&#23454;&#21518;&#38376;&#25915;&#20987;&#20013;&#65292;&#19982;&#25991;&#29486;&#20013;&#30340;&#19968;&#31995;&#21015;&#22522;&#32447;&#30456;&#27604;&#65292;PECAN&#21487;&#20197;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are vulnerable to backdoor poisoning attacks, where the attackers maliciously poison the training set and insert triggers into the test input to change the prediction of the victim model. Existing defenses for backdoor attacks either provide no formal guarantees or come with expensive-to-compute and ineffective probabilistic guarantees. We present PECAN, an efficient and certified approach for defending against backdoor attacks. The key insight powering PECAN is to apply off-the-shelf test-time evasion certification techniques on a set of neural networks trained on disjoint partitions of the data. We evaluate PECAN on image classification and malware detection datasets. Our results demonstrate that PECAN can (1) significantly outperform the state-of-the-art certified backdoor defense, both in defense strength and efficiency, and (2) on real back-door attacks, PECAN can reduce attack success rate by order of magnitude when compared to a range of baselines from the litera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#37327;&#21270;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#65288;QTD&#65289;&#22312;&#19968;&#23450;&#29366;&#24577;&#19979;&#30340;&#25910;&#25947;&#27010;&#29575;&#20026;1&#65292;&#24314;&#31435;&#20102;QTD&#19982;&#38750;&#32447;&#24615;&#24494;&#20998;&#21253;&#21547;&#24335;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2301.04462</link><description>&lt;p&gt;
&#37327;&#21270;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis of Quantile Temporal-Difference Learning. (arXiv:2301.04462v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#37327;&#21270;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#65288;QTD&#65289;&#22312;&#19968;&#23450;&#29366;&#24577;&#19979;&#30340;&#25910;&#25947;&#27010;&#29575;&#20026;1&#65292;&#24314;&#31435;&#20102;QTD&#19982;&#38750;&#32447;&#24615;&#24494;&#20998;&#21253;&#21547;&#24335;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65306;&#37327;&#21270;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#65288;QTD&#65289;&#65292;&#35813;&#31639;&#27861;&#24050;&#25104;&#20026;&#22810;&#20010;&#25104;&#21151;&#30340;&#24378;&#21270;&#23398;&#20064;&#22823;&#35268;&#27169;&#24212;&#29992;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#23613;&#31649;&#22312;&#23454;&#35777;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;QTD&#30340;&#29702;&#35770;&#35748;&#35782;&#19968;&#30452;&#38590;&#20197;&#25417;&#25720;&#12290;&#19982;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#38543;&#26426;&#36924;&#36817;&#24037;&#20855;&#26469;&#36827;&#34892;&#20998;&#26512;&#30340;&#32463;&#20856;TD&#23398;&#20064;&#19981;&#21516;&#65292;QTD&#30340;&#26356;&#26032;&#24182;&#19981;&#36817;&#20284;&#20110;&#25910;&#32553;&#31639;&#23376;&#65292;&#39640;&#24230;&#38750;&#32447;&#24615;&#24182;&#19988;&#21487;&#33021;&#20855;&#26377;&#22810;&#20010;&#19981;&#21160;&#28857;&#12290;&#26412;&#25991;&#30340;&#26680;&#24515;&#32467;&#26524;&#26159;&#35777;&#26126;&#22312;&#19982;&#19968;&#31867;&#21160;&#24577;&#35268;&#21010;&#31243;&#24207;&#30340;&#19981;&#21160;&#28857;&#30456;&#24212;&#30340;&#29366;&#24577;&#19979;&#65292;QTD&#30340;&#25910;&#25947;&#27010;&#29575;&#20026;1&#65292;&#20174;&#32780;&#35753;QTD&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20102;&#30830;&#23450;&#24615;&#30340;&#22522;&#30784;&#12290;&#35777;&#26126;&#36890;&#36807;&#38543;&#26426;&#36924;&#36817;&#29702;&#35770;&#21644;&#38750;&#20809;&#28369;&#20998;&#26512;&#23558;QTD&#19982;&#38750;&#32447;&#24615;&#24494;&#20998;&#21253;&#21547;&#24335;&#24314;&#31435;&#20102;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyse quantile temporal-difference learning (QTD), a distributional reinforcement learning algorithm that has proven to be a key component in several successful large-scale applications of reinforcement learning. Despite these empirical successes, a theoretical understanding of QTD has proven elusive until now. Unlike classical TD learning, which can be analysed with standard stochastic approximation tools, QTD updates do not approximate contraction mappings, are highly non-linear, and may have multiple fixed points. The core result of this paper is a proof of convergence to the fixed points of a related family of dynamic programming procedures with probability 1, putting QTD on firm theoretical footing. The proof establishes connections between QTD and non-linear differential inclusions through stochastic approximation theory and non-smooth analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#35774;&#35745;&#21644;&#35780;&#20272;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#25968;&#25454;&#20013;&#30340;&#39044;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#27979;&#24615;&#33021;&#20272;&#35745;&#37327;&#30340;&#36793;&#30028;&#36827;&#34892;&#21435;&#20559;&#20506;&#30340;&#26426;&#22120;&#23398;&#20064;&#20272;&#35745;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#39044;&#27979;&#31639;&#27861;&#22312;&#36873;&#25321;&#24615;&#35266;&#23519;&#24773;&#22659;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.09844</link><description>&lt;p&gt;
&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#19979;&#65292;&#39044;&#27979;&#31639;&#27861;&#30340;&#40065;&#26834;&#35774;&#35745;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Robust Design and Evaluation of Predictive Algorithms under Unobserved Confounding. (arXiv:2212.09844v4 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#35774;&#35745;&#21644;&#35780;&#20272;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#25968;&#25454;&#20013;&#30340;&#39044;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#27979;&#24615;&#33021;&#20272;&#35745;&#37327;&#30340;&#36793;&#30028;&#36827;&#34892;&#21435;&#20559;&#20506;&#30340;&#26426;&#22120;&#23398;&#20064;&#20272;&#35745;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#39044;&#27979;&#31639;&#27861;&#22312;&#36873;&#25321;&#24615;&#35266;&#23519;&#24773;&#22659;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#31639;&#27861;&#22312;&#20154;&#31867;&#20915;&#31574;&#32773;&#20316;&#20986;&#36873;&#25321;&#21518;&#36873;&#25321;&#24615;&#22320;&#35266;&#23519;&#21040;&#32467;&#26524;&#30340;&#24773;&#22659;&#20013;&#36827;&#34892;&#37325;&#35201;&#20915;&#31574;&#12290;&#36890;&#24120;&#23384;&#22312;&#30528;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#24433;&#21709;&#20915;&#31574;&#32773;&#30340;&#36873;&#25321;&#21644;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#25968;&#25454;&#19979;&#23545;&#39044;&#27979;&#31639;&#27861;&#36827;&#34892;&#40065;&#26834;&#35774;&#35745;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#39044;&#27979;&#31639;&#27861;&#30340;&#26465;&#20214;&#24179;&#22343;&#32467;&#26524;&#22312;&#24050;&#35266;&#23519;&#21040;&#30340;&#21327;&#21464;&#37327;&#21644;&#24050;&#35782;&#21035;&#30340;&#24178;&#25200;&#21442;&#25968;&#19978;&#21487;&#33021;&#30340;&#21464;&#21270;&#31243;&#24230;&#25552;&#20986;&#20102;&#19968;&#33324;&#24615;&#30340;&#20551;&#35774;&#65292;&#20174;&#32780;&#24418;&#24335;&#21270;&#20102;&#29992;&#20110;&#22635;&#20805;&#32570;&#22833;&#25968;&#25454;&#30340;&#24120;&#29992;&#23454;&#35777;&#31574;&#30053;&#65292;&#22914;&#20195;&#29702;&#32467;&#26524;&#21644;&#24037;&#20855;&#21464;&#37327;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#21435;&#20559;&#20506;&#30340;&#26426;&#22120;&#23398;&#20064;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#23545;&#22823;&#31867;&#39044;&#27979;&#24615;&#33021;&#20272;&#35745;&#37327;&#30340;&#36793;&#30028;&#65292;&#20363;&#22914;&#32467;&#26524;&#30340;&#26465;&#20214;&#20284;&#28982;&#12289;&#39044;&#27979;&#31639;&#27861;&#30340;&#22343;&#26041;&#35823;&#24046;&#12289;&#30495;/&#20551;&#38451;&#24615;&#29575;&#31561;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive algorithms inform consequential decisions in settings where the outcome is selectively observed given some choices made by human decision makers. There often exists unobserved confounders that affected the decision maker's choice and the outcome. We propose a unified methodology for the robust design and evaluation of predictive algorithms in selectively observed data under such unobserved confounding. Our approach imposes general assumptions on how much the outcome may vary on average between unselected and selected units conditional on observed covariates and identified nuisance parameters, formalizing popular empirical strategies for imputing missing data such as proxy outcomes and instrumental variables. We develop debiased machine learning estimators for the bounds on a large class of predictive performance estimands, such as the conditional likelihood of the outcome, a predictive algorithm's mean square error, true/false positive rate, and many others, under these assu
&lt;/p&gt;</description></item><item><title>&#21327;&#21464;&#37327;&#36716;&#31227;&#21644;&#23545;&#25239;&#25200;&#21160;&#23545;&#32479;&#35745;&#23398;&#20064;&#30340;&#31283;&#20581;&#24615;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#22312;&#26080;&#38480;&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#30740;&#31350;&#20102;&#23545;&#25239;&#21327;&#21464;&#37327;&#36716;&#31227;&#23545;&#22806;&#25512;&#21306;&#22495;&#30340;&#24433;&#21709;&#20197;&#21450;&#20854;&#23545;&#21518;&#32493;&#23398;&#20064;&#30340;&#24179;&#34913;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2212.02457</link><description>&lt;p&gt;
&#21327;&#21464;&#37327;&#36716;&#31227;&#30340;&#31069;&#31119;&#21644;&#35781;&#21650;&#65306;&#23545;&#25239;&#23398;&#20064;&#21160;&#24577;&#12289;&#26041;&#21521;&#25910;&#25947;&#21644;&#24179;&#34913;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Blessings and Curses of Covariate Shifts: Adversarial Learning Dynamics, Directional Convergence, and Equilibria. (arXiv:2212.02457v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02457
&lt;/p&gt;
&lt;p&gt;
&#21327;&#21464;&#37327;&#36716;&#31227;&#21644;&#23545;&#25239;&#25200;&#21160;&#23545;&#32479;&#35745;&#23398;&#20064;&#30340;&#31283;&#20581;&#24615;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#22312;&#26080;&#38480;&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#30740;&#31350;&#20102;&#23545;&#25239;&#21327;&#21464;&#37327;&#36716;&#31227;&#23545;&#22806;&#25512;&#21306;&#22495;&#30340;&#24433;&#21709;&#20197;&#21450;&#20854;&#23545;&#21518;&#32493;&#23398;&#20064;&#30340;&#24179;&#34913;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21464;&#37327;&#20998;&#24067;&#36716;&#31227;&#21644;&#23545;&#25239;&#25200;&#21160;&#23545;&#20256;&#32479;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#30340;&#31283;&#20581;&#24615;&#25552;&#20986;&#20102;&#25361;&#25112;&#65306;&#27979;&#35797;&#21327;&#21464;&#37327;&#20998;&#24067;&#20013;&#30340;&#36731;&#24494;&#36716;&#31227;&#33021;&#26174;&#33879;&#24433;&#21709;&#22522;&#20110;&#35757;&#32451;&#20998;&#24067;&#23398;&#20064;&#30340;&#32479;&#35745;&#27169;&#22411;&#24615;&#33021;&#12290;&#24403;&#22806;&#25512;&#21457;&#29983;&#26102;&#65292;&#21363;&#21327;&#21464;&#37327;&#36716;&#31227;&#21040;&#35757;&#32451;&#20998;&#24067;&#31232;&#32570;&#30340;&#21306;&#22495;&#26102;&#65292;&#27169;&#22411;&#24615;&#33021;&#36890;&#24120;&#20250;&#38477;&#20302;&#65292;&#22240;&#27492;&#65292;&#23398;&#20064;&#27169;&#22411;&#20449;&#24687;&#24456;&#23569;&#12290;&#20026;&#20102;&#31283;&#20581;&#24615;&#21644;&#27491;&#21017;&#21270;&#32771;&#34385;&#65292;&#24314;&#35758;&#37319;&#29992;&#23545;&#25239;&#25200;&#21160;&#25216;&#26415;&#65292;&#28982;&#32780;&#65292;&#38656;&#35201;&#23545;&#32473;&#23450;&#23398;&#20064;&#27169;&#22411;&#26102;&#23545;&#25239;&#21327;&#21464;&#37327;&#36716;&#31227;&#30340;&#22806;&#25512;&#21306;&#22495;&#36827;&#34892;&#20180;&#32454;&#30740;&#31350;&#12290;&#26412;&#25991;&#22312;&#26080;&#38480;&#32500;&#24230;&#30340;&#35774;&#32622;&#20013;&#31934;&#30830;&#21051;&#30011;&#20102;&#22806;&#25512;&#21306;&#22495;&#65292;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#26041;&#38754;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#30740;&#31350;&#20102;&#23545;&#25239;&#21327;&#21464;&#37327;&#36716;&#31227;&#23545;&#38543;&#21518;&#30340;&#24179;&#34913;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Covariate distribution shifts and adversarial perturbations present robustness challenges to the conventional statistical learning framework: mild shifts in the test covariate distribution can significantly affect the performance of the statistical model learned based on the training distribution. The model performance typically deteriorates when extrapolation happens: namely, covariates shift to a region where the training distribution is scarce, and naturally, the learned model has little information. For robustness and regularization considerations, adversarial perturbation techniques are proposed as a remedy; however, careful study needs to be carried out about what extrapolation region adversarial covariate shift will focus on, given a learned model. This paper precisely characterizes the extrapolation region, examining both regression and classification in an infinite-dimensional setting. We study the implications of adversarial covariate shifts to subsequent learning of the equi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#26041;&#27861;&#26469;&#26377;&#25928;&#26816;&#27979;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#20998;&#24067;&#28418;&#31227;&#65292;&#20855;&#26377;&#20302;&#35823;&#25253;&#29575;&#21644;&#39640;&#25928;&#29575;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2211.09916</link><description>&lt;p&gt;
&#22312;&#32447;&#20998;&#24067;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#22522;&#20110;&#26368;&#36817;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Online Distribution Shift Detection via Recency Prediction. (arXiv:2211.09916v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#26041;&#27861;&#26469;&#26377;&#25928;&#26816;&#27979;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#20998;&#24067;&#28418;&#31227;&#65292;&#20855;&#26377;&#20302;&#35823;&#25253;&#29575;&#21644;&#39640;&#25928;&#29575;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#37096;&#32626;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#26102;&#65292;&#26816;&#27979;&#20998;&#24067;&#28418;&#31227;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20998;&#24067;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#26426;&#22120;&#20154;&#29615;&#22659;&#65292;&#22240;&#20026;&#25968;&#25454;&#36890;&#24120;&#20197;&#27969;&#24335;&#26041;&#24335;&#21040;&#36798;&#24182;&#19988;&#21487;&#33021;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#32500;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#26041;&#27861;&#26469;&#26816;&#27979;&#20998;&#24067;&#28418;&#31227;&#65292;&#24182;&#23545;&#35823;&#25253;&#29575;&#25552;&#20379;&#20102;&#20445;&#35777; - &#21363;&#24403;&#27809;&#26377;&#20998;&#24067;&#28418;&#31227;&#26102;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#38750;&#24120;&#19981;&#21487;&#33021;&#65288;&#27010;&#29575;&#23567;&#20110; epsilon&#65289;&#21457;&#20986;&#38169;&#35823;&#30340;&#35686;&#25253;&#65307;&#22240;&#27492;&#65292;&#20219;&#20309;&#21457;&#20986;&#30340;&#35686;&#25253;&#24212;&#35813;&#34987;&#37325;&#35270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19987;&#20026;&#39640;&#32500;&#25968;&#25454;&#30340;&#39640;&#25928;&#26816;&#27979;&#32780;&#35774;&#35745;&#65292;&#24182;&#19988;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;11&#20493;&#30340;&#24555;&#36895;&#26816;&#27979;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#30340;&#35823;&#25253;&#29575;&#65288;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#24403;&#23384;&#22312;&#20998;&#24067;&#28418;&#31227;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30830;&#23454;&#21457;&#20986;&#20102;&#35686;&#25253;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
When deploying modern machine learning-enabled robotic systems in high-stakes applications, detecting distribution shift is critical. However, most existing methods for detecting distribution shift are not well-suited to robotics settings, where data often arrives in a streaming fashion and may be very high-dimensional. In this work, we present an online method for detecting distribution shift with guarantees on the false positive rate - i.e., when there is no distribution shift, our system is very unlikely (with probability $&lt; \epsilon$) to falsely issue an alert; any alerts that are issued should therefore be heeded. Our method is specifically designed for efficient detection even with high dimensional data, and it empirically achieves up to 11x faster detection on realistic robotics settings compared to prior work while maintaining a low false negative rate in practice (whenever there is a distribution shift in our experiments, our method indeed emits an alert). We demonstrate our a
&lt;/p&gt;</description></item><item><title>RulE&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#23454;&#20307;&#12289;&#20851;&#31995;&#21644;&#36923;&#36753;&#35268;&#21017;&#32479;&#19968;&#34920;&#31034;&#22312;&#19968;&#20010;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#26377;&#25928;&#21033;&#29992;&#36923;&#36753;&#35268;&#21017;&#25552;&#21319;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;&#21516;&#26102;&#65292;RulE&#27880;&#20837;&#20808;&#21069;&#30340;&#36923;&#36753;&#35268;&#21017;&#20449;&#24687;&#65292;&#25913;&#36827;&#20102;&#23454;&#20307;/&#20851;&#31995;&#23884;&#20837;&#65292;&#20351;&#24471;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#20063;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2210.14905</link><description>&lt;p&gt;
RulE: &#20351;&#29992;&#35268;&#21017;&#23884;&#20837;&#30340;&#31070;&#32463;-&#31526;&#21495;&#30693;&#35782;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
RulE: Neural-Symbolic Knowledge Graph Reasoning with Rule Embedding. (arXiv:2210.14905v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14905
&lt;/p&gt;
&lt;p&gt;
RulE&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#23454;&#20307;&#12289;&#20851;&#31995;&#21644;&#36923;&#36753;&#35268;&#21017;&#32479;&#19968;&#34920;&#31034;&#22312;&#19968;&#20010;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#26377;&#25928;&#21033;&#29992;&#36923;&#36753;&#35268;&#21017;&#25552;&#21319;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;&#21516;&#26102;&#65292;RulE&#27880;&#20837;&#20808;&#21069;&#30340;&#36923;&#36753;&#35268;&#21017;&#20449;&#24687;&#65292;&#25913;&#36827;&#20102;&#23454;&#20307;/&#20851;&#31995;&#23884;&#20837;&#65292;&#20351;&#24471;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#20063;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#65288;KG&#65289;&#25512;&#29702;&#23545;&#20110;&#30693;&#35782;&#22270;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#26377;&#21407;&#21017;&#23450;&#20301;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;RulE&#65288;&#20195;&#34920;&#35268;&#21017;&#23884;&#20837;&#65289;&#65292;&#20197;&#26377;&#25928;&#21033;&#29992;&#36923;&#36753;&#35268;&#21017;&#26469;&#22686;&#24378;KG&#25512;&#29702;&#12290;&#19982;&#30693;&#35782;&#22270;&#23884;&#20837;&#65288;KGE&#65289;&#26041;&#27861;&#19981;&#21516;&#65292;RulE&#36890;&#36807;&#22312;&#32479;&#19968;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#32852;&#21512;&#34920;&#31034;&#23454;&#20307;&#12289;&#20851;&#31995;&#21644;&#36923;&#36753;&#35268;&#21017;&#65292;&#20174;&#29616;&#26377;&#19977;&#20803;&#32452;&#21644;&#19968;&#38454;&#35268;&#21017;&#20013;&#23398;&#20064;&#35268;&#21017;&#23884;&#20837;&#12290;&#22522;&#20110;&#23398;&#20064;&#21040;&#30340;&#35268;&#21017;&#23884;&#20837;&#65292;&#21487;&#20197;&#35745;&#31639;&#27599;&#20010;&#35268;&#21017;&#30340;&#32622;&#20449;&#24230;&#24471;&#20998;&#65292;&#21453;&#26144;&#20854;&#19982;&#35266;&#23519;&#21040;&#30340;&#19977;&#20803;&#32452;&#30340;&#19968;&#33268;&#24615;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#20197;&#36719;&#26041;&#24335;&#36827;&#34892;&#36923;&#36753;&#35268;&#21017;&#25512;&#29702;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#36923;&#36753;&#30340;&#33030;&#24369;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;RulE&#23558;&#20808;&#21069;&#30340;&#36923;&#36753;&#35268;&#21017;&#20449;&#24687;&#27880;&#20837;&#21040;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#20016;&#23500;&#21644;&#35268;&#33539;&#21270;&#23454;&#20307;/&#20851;&#31995;&#23884;&#20837;&#12290;&#36825;&#20063;&#20351;&#24471;&#20165;&#20351;&#29992;KGE&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;RulE&#22312;&#27010;&#24565;&#19978;&#31616;&#21333;&#19988;&#22312;&#23454;&#39564;&#19978;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph (KG) reasoning is an important problem for knowledge graphs. In this paper, we propose a novel and principled framework called \textbf{RulE} (stands for {Rul}e {E}mbedding) to effectively leverage logical rules to enhance KG reasoning. Unlike knowledge graph embedding (KGE) methods, RulE learns rule embeddings from existing triplets and first-order {rules} by jointly representing \textbf{entities}, \textbf{relations} and \textbf{logical rules} in a unified embedding space. Based on the learned rule embeddings, a confidence score can be calculated for each rule, reflecting its consistency with the observed triplets. This allows us to perform logical rule inference in a soft way, thus alleviating the brittleness of logic. On the other hand, RulE injects prior logical rule information into the embedding space, enriching and regularizing the entity/relation embeddings. This makes KGE alone perform better too. RulE is conceptually simple and empirically effective. We conduct
&lt;/p&gt;</description></item><item><title>Teal&#26159;&#19968;&#31181;&#23398;&#20064;&#21152;&#36895;&#30340;&#24191;&#22495;&#32593;&#27969;&#37327;&#24037;&#31243;&#20248;&#21270;&#31639;&#27861;&#65292;&#21033;&#29992;GPU&#30340;&#24182;&#34892;&#22788;&#29702;&#33021;&#21147;&#21152;&#36895;TE&#25511;&#21046;&#12290;&#23427;&#20351;&#29992;&#27969;&#20026;&#20013;&#24515;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#25429;&#25417;WAN&#36830;&#25509;&#21644;&#32593;&#32476;&#27969;&#37327;&#65292;&#24182;&#37319;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#29420;&#31435;&#20998;&#37197;&#21644;&#20013;&#24515;&#21270;&#20248;&#21270;&#12290;&#26368;&#21518;&#65292;&#23427;&#20351;&#29992;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861;&#23545;&#20998;&#37197;&#36827;&#34892;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2210.13763</link><description>&lt;p&gt;
Teal: &#23398;&#20064;&#21152;&#36895;&#30340;&#24191;&#22495;&#32593;&#27969;&#37327;&#24037;&#31243;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Teal: Learning-Accelerated Optimization of WAN Traffic Engineering. (arXiv:2210.13763v3 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13763
&lt;/p&gt;
&lt;p&gt;
Teal&#26159;&#19968;&#31181;&#23398;&#20064;&#21152;&#36895;&#30340;&#24191;&#22495;&#32593;&#27969;&#37327;&#24037;&#31243;&#20248;&#21270;&#31639;&#27861;&#65292;&#21033;&#29992;GPU&#30340;&#24182;&#34892;&#22788;&#29702;&#33021;&#21147;&#21152;&#36895;TE&#25511;&#21046;&#12290;&#23427;&#20351;&#29992;&#27969;&#20026;&#20013;&#24515;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#25429;&#25417;WAN&#36830;&#25509;&#21644;&#32593;&#32476;&#27969;&#37327;&#65292;&#24182;&#37319;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#29420;&#31435;&#20998;&#37197;&#21644;&#20013;&#24515;&#21270;&#20248;&#21270;&#12290;&#26368;&#21518;&#65292;&#23427;&#20351;&#29992;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861;&#23545;&#20998;&#37197;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#20113;&#24191;&#22495;&#32593;&#65288;WAN&#65289;&#30340;&#24555;&#36895;&#25193;&#23637;&#32473;&#21830;&#19994;&#20248;&#21270;&#24341;&#25806;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#22914;&#20309;&#39640;&#25928;&#35299;&#20915;&#35268;&#27169;&#21270;&#30340;&#32593;&#32476;&#27969;&#37327;&#24037;&#31243;&#65288;TE&#65289;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#21152;&#36895;&#31574;&#30053;&#23558;TE&#20248;&#21270;&#20998;&#35299;&#20026;&#24182;&#21457;&#30340;&#23376;&#38382;&#39064;&#65292;&#20294;&#30001;&#20110;&#36816;&#34892;&#26102;&#38388;&#21644;&#20998;&#37197;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#23454;&#29616;&#30340;&#24182;&#34892;&#21270;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Teal&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;TE&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;GPU&#30340;&#24182;&#34892;&#22788;&#29702;&#33021;&#21147;&#21152;&#36895;TE&#25511;&#21046;&#12290;&#39318;&#20808;&#65292;Teal&#35774;&#35745;&#20102;&#19968;&#20010;&#20197;&#27969;&#20026;&#20013;&#24515;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#25429;&#25417;WAN&#36830;&#25509;&#21644;&#32593;&#32476;&#27969;&#37327;&#65292;&#23398;&#20064;&#27969;&#29305;&#24449;&#20316;&#20026;&#19979;&#28216;&#20998;&#37197;&#30340;&#36755;&#20837;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#20943;&#23569;&#38382;&#39064;&#35268;&#27169;&#24182;&#20351;&#23398;&#20064;&#21487;&#34892;&#65292;Teal&#37319;&#29992;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65292;&#29420;&#31435;&#20998;&#37197;&#27599;&#20010;&#27969;&#37327;&#38656;&#27714;&#65292;&#21516;&#26102;&#20248;&#21270;&#19968;&#20010;&#20013;&#24515;&#30340;TE&#30446;&#26631;&#12290;&#26368;&#21518;&#65292;Teal&#20351;&#29992;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861;&#65288;ADMM&#65289;&#23545;&#20998;&#37197;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid expansion of global cloud wide-area networks (WANs) has posed a challenge for commercial optimization engines to efficiently solve network traffic engineering (TE) problems at scale. Existing acceleration strategies decompose TE optimization into concurrent subproblems but realize limited parallelism due to an inherent tradeoff between run time and allocation performance.  We present Teal, a learning-based TE algorithm that leverages the parallel processing power of GPUs to accelerate TE control. First, Teal designs a flow-centric graph neural network (GNN) to capture WAN connectivity and network flows, learning flow features as inputs to downstream allocation. Second, to reduce the problem scale and make learning tractable, Teal employs a multi-agent reinforcement learning (RL) algorithm to independently allocate each traffic demand while optimizing a central TE objective. Finally, Teal fine-tunes allocations with ADMM (Alternating Direction Method of Multipliers), a highly 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#25968;&#25454;&#30340;&#20215;&#26684;&#27495;&#35270;&#65292;&#25581;&#31034;&#20102;&#20219;&#20309;&#22522;&#20110;&#25968;&#25454;&#30340;&#23450;&#20215;&#31574;&#30053;&#22312;&#25910;&#20837;&#29983;&#25104;&#26041;&#38754;&#30340;&#20449;&#24687;&#35770;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#32463;&#39564;&#25910;&#30410;&#26368;&#22823;&#21270;&#65288;ERM&#65289;&#31574;&#30053;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2204.12723</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#30340;&#20215;&#26684;&#27495;&#35270;&#30340;&#20449;&#24687;&#35770;&#38480;&#21046;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Information-theoretic limitations of data-based price discrimination. (arXiv:2204.12723v3 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.12723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#25968;&#25454;&#30340;&#20215;&#26684;&#27495;&#35270;&#65292;&#25581;&#31034;&#20102;&#20219;&#20309;&#22522;&#20110;&#25968;&#25454;&#30340;&#23450;&#20215;&#31574;&#30053;&#22312;&#25910;&#20837;&#29983;&#25104;&#26041;&#38754;&#30340;&#20449;&#24687;&#35770;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#32463;&#39564;&#25910;&#30410;&#26368;&#22823;&#21270;&#65288;ERM&#65289;&#31574;&#30053;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20272;&#20540;&#21644;&#22806;&#29983;&#21464;&#37327;&#25968;&#25454;&#38543;&#26426;&#26679;&#26412;&#30340;&#31532;&#19977;&#24230;&#20215;&#26684;&#27495;&#35270;&#65288;3PD&#65289;&#65292;&#20854;&#20013;&#22806;&#29983;&#21464;&#37327;&#26159;&#36830;&#32493;&#30340;&#65292;&#25968;&#25454;&#20998;&#24067;&#23545;&#21334;&#26041;&#26469;&#35828;&#26159;&#26410;&#30693;&#30340;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#32467;&#26524;&#26377;&#20004;&#20010;&#26041;&#38754;&#12290;&#31532;&#19968;&#32452;&#32467;&#26524;&#26159;&#23450;&#20215;&#31574;&#30053;&#26080;&#20851;&#30340;&#65292;&#25581;&#31034;&#20102;&#20219;&#20309;&#22522;&#20110;&#25968;&#25454;&#30340;&#23450;&#20215;&#31574;&#30053;&#22312;&#25910;&#20837;&#29983;&#25104;&#26041;&#38754;&#30340;&#20449;&#24687;&#35770;&#38480;&#21046;&#65292;&#20998;&#20026;3PD&#21644;&#22343;&#21248;&#23450;&#20215;&#20004;&#31181;&#24773;&#20917;&#12290;&#31532;&#20108;&#32452;&#32467;&#26524;&#25552;&#20986;&#20102;$K$-markets&#32463;&#39564;&#25910;&#30410;&#26368;&#22823;&#21270;&#65288;ERM&#65289;&#31574;&#30053;&#65292;&#24182;&#26174;&#31034;$K$-markets ERM&#21644;&#22343;&#21248;ERM&#31574;&#30053;&#23454;&#29616;&#20102;&#25910;&#20837;&#25910;&#25947;&#21040;&#21508;&#33258;&#30495;&#23454;&#20998;&#24067;3PD&#21644;&#22343;&#21248;&#23450;&#20215;&#26368;&#20248;&#35299;&#30340;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#26679;&#26412;&#37327;&#36275;&#22815;&#23567;&#30340;&#26102;&#20505;&#65292;&#22343;&#21248;&#65288;&#21363;$1$-market&#65289;ERM&#31574;&#30053;&#20135;&#29983;&#30340;&#25910;&#20837;&#27604;$K$-markets ERM&#31574;&#30053;&#26356;&#39640;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies third-degree price discrimination (3PD) based on a random sample of valuation and covariate data, where the covariate is continuous, and the distribution of the data is unknown to the seller. The main results of this paper are twofold. The first set of results is pricing strategy independent and reveals the fundamental information-theoretic limitation of any data-based pricing strategy in revenue generation for two cases: 3PD and uniform pricing. The second set of results proposes the $K$-markets empirical revenue maximization (ERM) strategy and shows that the $K$-markets ERM and the uniform ERM strategies achieve the optimal rate of convergence in revenue to that generated by their respective true-distribution 3PD and uniform pricing optima. Our theoretical and numerical results suggest that the uniform (i.e., $1$-market) ERM strategy generates a larger revenue than the $K$-markets ERM strategy when the sample size is small enough, and vice versa.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#20445;&#38505;&#34892;&#19994;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#20559;&#35265;&#21644;&#27495;&#35270;&#65292;&#25552;&#20986;&#20102;&#20844;&#24179;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23454;&#29616;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#30340;&#21516;&#26102;&#20445;&#35777;&#25968;&#25454;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2112.09466</link><description>&lt;p&gt;
&#20844;&#24179;&#20027;&#21160;&#23398;&#20064;&#65306;&#35299;&#20915;&#20445;&#38505;&#34892;&#19994;&#20013;&#30340;&#26631;&#27880;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Fair Active Learning: Solving the Labeling Problem in Insurance. (arXiv:2112.09466v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.09466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#20445;&#38505;&#34892;&#19994;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#20559;&#35265;&#21644;&#27495;&#35270;&#65292;&#25552;&#20986;&#20102;&#20844;&#24179;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23454;&#29616;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#30340;&#21516;&#26102;&#20445;&#35777;&#25968;&#25454;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22312;&#20445;&#38505;&#34892;&#19994;&#24191;&#27867;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#37325;&#22823;&#38556;&#30861;&#65292;&#29305;&#21035;&#20851;&#27880;&#20419;&#36827;&#20844;&#24179;&#24615;&#12290;&#26368;&#21021;&#30340;&#25361;&#25112;&#22312;&#20110;&#26377;&#25928;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#20445;&#38505;&#25968;&#25454;&#65292;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#38477;&#20302;&#26631;&#27880;&#30340;&#24037;&#20316;&#37327;&#65292;&#24182;&#24378;&#35843;&#25968;&#25454;&#30456;&#20851;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21508;&#31181;&#20027;&#21160;&#23398;&#20064;&#25277;&#26679;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#23545;&#21512;&#25104;&#21644;&#23454;&#38469;&#20445;&#38505;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#12290;&#35813;&#20998;&#26512;&#24378;&#35843;&#20102;&#23454;&#29616;&#20844;&#27491;&#27169;&#22411;&#25512;&#26029;&#30340;&#22256;&#38590;&#65292;&#22240;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#20250;&#22797;&#21046;&#24213;&#23618;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#21644;&#27495;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#30456;&#20114;&#20851;&#32852;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20844;&#24179;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#37319;&#26679;&#20449;&#24687;&#37327;&#20805;&#36275;&#19988;&#20844;&#24179;&#30340;&#23454;&#20363;&#65292;&#22312;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#65292;&#36825;&#19968;&#28857;&#22312;&#20445;&#38505;&#25968;&#25454;&#38598;&#19978;&#30340;&#25968;&#20540;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses significant obstacles that arise from the widespread use of machine learning models in the insurance industry, with a specific focus on promoting fairness. The initial challenge lies in effectively leveraging unlabeled data in insurance while reducing the labeling effort and emphasizing data relevance through active learning techniques. The paper explores various active learning sampling methodologies and evaluates their impact on both synthetic and real insurance datasets. This analysis highlights the difficulty of achieving fair model inferences, as machine learning models may replicate biases and discrimination found in the underlying data. To tackle these interconnected challenges, the paper introduces an innovative fair active learning method. The proposed approach samples informative and fair instances, achieving a good balance between model predictive performance and fairness, as confirmed by numerical experiments on insurance datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#23454;&#39564;&#65292;&#21457;&#29616;&#32467;&#26500;&#27169;&#22359;&#21270;&#24182;&#19981;&#19968;&#23450;&#33021;&#22815;&#30830;&#20445;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#22312;&#29305;&#23450;&#29615;&#22659;&#21644;&#36164;&#28304;&#38480;&#21046;&#19979;&#65292;&#25165;&#33021;&#22815;&#20986;&#29616;&#19987;&#19994;&#21270;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2106.02626</link><description>&lt;p&gt;
&#32422;&#26463;&#36164;&#28304;&#19979;&#31070;&#32463;&#27169;&#22359;&#19987;&#19994;&#21270;&#30340;&#21160;&#21147;&#23398;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Dynamics of specialization in neural modules under resource constraints. (arXiv:2106.02626v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.02626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#23454;&#39564;&#65292;&#21457;&#29616;&#32467;&#26500;&#27169;&#22359;&#21270;&#24182;&#19981;&#19968;&#23450;&#33021;&#22815;&#30830;&#20445;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#22312;&#29305;&#23450;&#29615;&#22659;&#21644;&#36164;&#28304;&#38480;&#21046;&#19979;&#65292;&#25165;&#33021;&#22815;&#20986;&#29616;&#19987;&#19994;&#21270;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#20204;&#19968;&#30452;&#35748;&#20026;&#22823;&#33041;&#22312;&#32467;&#26500;&#21644;&#21151;&#33021;&#19978;&#39640;&#24230;&#27169;&#22359;&#21270;&#65292;&#20294;&#26368;&#36817;&#30340;&#35777;&#25454;&#20351;&#19968;&#20123;&#20154;&#23545;&#20004;&#31181;&#27169;&#22359;&#21270;&#30340;&#31243;&#24230;&#20135;&#29983;&#20102;&#24576;&#30097;&#12290;&#25105;&#20204;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#27979;&#35797;&#32467;&#26500;&#27169;&#22359;&#21270;&#26159;&#21542;&#36275;&#20197;&#20445;&#35777;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#24182;&#21457;&#29616;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#24182;&#19981;&#19968;&#23450;&#25104;&#31435;&#65292;&#38500;&#38750;&#22312;&#26497;&#31471;&#27700;&#24179;&#19978;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#29615;&#22659;&#21644;&#32593;&#32476;&#30340;&#21738;&#20123;&#29305;&#24449;&#20250;&#23548;&#33268;&#19987;&#19994;&#21270;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#29609;&#20855;&#29615;&#22659;&#12289;&#20219;&#21153;&#21644;&#32593;&#32476;&#65292;&#20197;&#31934;&#30830;&#25511;&#21046;&#26465;&#20214;&#65292;&#24182;&#34920;&#26126;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#20960;&#20010;&#19981;&#21516;&#30340;&#19987;&#19994;&#21270;&#24230;&#37327;&#25351;&#26631;&#32473;&#20986;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;&#65288;1&#65289;&#19987;&#19994;&#21270;&#21482;&#33021;&#22312;&#29615;&#22659;&#20013;&#37027;&#20123;&#21487;&#20197;&#26126;&#30830;&#20998;&#31163;&#30340;&#29305;&#24449;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20986;&#29616;&#65292;&#65288;2&#65289;&#19987;&#19994;&#21270;&#26356;&#23481;&#26131;&#22312;&#32593;&#32476;&#36164;&#28304;&#21463;&#21040;&#24378;&#28872;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#20986;&#29616;&#65292;&#65288;3&#65289;&#36825;&#20123;&#21457;&#29616;&#22312; qualitatively &#19978;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has long been believed that the brain is highly modular both in terms of structure and function, although recent evidence has led some to question the extent of both types of modularity. We used artificial neural networks to test the hypothesis that structural modularity is sufficient to guarantee functional specialization, and find that in general, this doesn't necessarily hold except at extreme levels. We then systematically tested which features of the environment and network do lead to the emergence of specialization. We used a simple toy environment, task and network, allowing us precise control, and show that in this setup, several distinct measures of specialization give qualitatively similar results. We further find that (1) specialization can only emerge in environments where features of that environment are meaningfully separable, (2) specialization preferentially emerges when the network is strongly resource-constrained, and (3) these findings are qualitatively similar ac
&lt;/p&gt;</description></item></channel></rss>