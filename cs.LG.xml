<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#22686;&#21152;&#39044;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;&#21487;&#20197;&#25552;&#39640;&#33258;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#65292;&#20294;&#20165;&#22312;&#19982;&#19979;&#28216;&#25968;&#25454;&#30340;&#20998;&#24067;&#36317;&#31163;&#36739;&#23567;&#26102;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2403.13808</link><description>&lt;p&gt;
&#20851;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Pretraining Data Diversity for Self-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13808
&lt;/p&gt;
&lt;p&gt;
&#22686;&#21152;&#39044;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;&#21487;&#20197;&#25552;&#39640;&#33258;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#65292;&#20294;&#20165;&#22312;&#19982;&#19979;&#28216;&#25968;&#25454;&#30340;&#20998;&#24067;&#36317;&#31163;&#36739;&#23567;&#26102;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#26356;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#23545;&#33258;&#30417;&#30563;&#23398;&#20064;(SSL)&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#26159;&#21807;&#19968;&#26679;&#26412;&#25968;&#37327;&#65292;&#22312;&#22266;&#23450;&#30340;&#35745;&#31639;&#39044;&#31639;&#19979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#19968;&#33268;&#34920;&#26126;&#65292;&#22686;&#21152;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21487;&#20197;&#25552;&#39640;SSL&#24615;&#33021;&#65292;&#23613;&#31649;&#21482;&#26377;&#24403;&#19982;&#19979;&#28216;&#25968;&#25454;&#30340;&#20998;&#24067;&#36317;&#31163;&#24456;&#23567;&#30340;&#26102;&#20505;&#25165;&#26159;&#22914;&#27492;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#36890;&#36807;&#32593;&#32476;&#29228;&#34411;&#25110;&#25193;&#25955;&#29983;&#25104;&#30340;&#25968;&#25454;&#31561;&#26041;&#24335;&#23454;&#29616;&#20102;&#24322;&#24120;&#22823;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#20998;&#24067;&#36716;&#31227;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28085;&#30422;&#20102;&#19971;&#31181;SSL&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#35832;&#22914;ImageNet&#21644;YFCC100M&#31561;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24635;&#35745;&#36229;&#36807;200&#20010;GPU&#22825;&#12290;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#23558;&#22312;https://github.com/hammoudhasan/DiversitySSL &#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13808v1 Announce Type: cross  Abstract: We explore the impact of training with more diverse datasets, characterized by the number of unique samples, on the performance of self-supervised learning (SSL) under a fixed computational budget. Our findings consistently demonstrate that increasing pretraining data diversity enhances SSL performance, albeit only when the distribution distance to the downstream data is minimal. Notably, even with an exceptionally large pretraining data diversity achieved through methods like web crawling or diffusion-generated data, among other ways, the distribution shift remains a challenge. Our experiments are comprehensive with seven SSL methods using large-scale datasets such as ImageNet and YFCC100M amounting to over 200 GPU days. Code and trained models will be available at https://github.com/hammoudhasan/DiversitySSL .
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;EMCID&#65292;&#29992;&#20110;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#36827;&#34892;&#28023;&#37327;&#27010;&#24565;&#32534;&#36753;&#65292;&#36890;&#36807;&#20869;&#23384;&#20248;&#21270;&#21644;&#22810;&#23618;&#27169;&#22411;&#32534;&#36753;&#26469;&#21516;&#26102;&#22788;&#29702;&#29983;&#25104;&#20869;&#23481;&#20013;&#30340;&#36807;&#26102;&#12289;&#21463;&#29256;&#26435;&#20445;&#25252;&#12289;&#19981;&#27491;&#30830;&#21644;&#26377;&#20559;&#35265;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.13807</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#32534;&#36753;&#28023;&#37327;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Editing Massive Concepts in Text-to-Image Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13807
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;EMCID&#65292;&#29992;&#20110;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#36827;&#34892;&#28023;&#37327;&#27010;&#24565;&#32534;&#36753;&#65292;&#36890;&#36807;&#20869;&#23384;&#20248;&#21270;&#21644;&#22810;&#23618;&#27169;&#22411;&#32534;&#36753;&#26469;&#21516;&#26102;&#22788;&#29702;&#29983;&#25104;&#20869;&#23481;&#20013;&#30340;&#36807;&#26102;&#12289;&#21463;&#29256;&#26435;&#20445;&#25252;&#12289;&#19981;&#27491;&#30830;&#21644;&#26377;&#20559;&#35265;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#23384;&#22312;&#29983;&#25104;&#36807;&#26102;&#30340;&#12289;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#12289;&#19981;&#27491;&#30830;&#30340;&#21644;&#26377;&#20559;&#35265;&#30340;&#20869;&#23481;&#30340;&#39118;&#38505;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#26041;&#27861;&#24050;&#22312;&#23567;&#35268;&#27169;&#19978;&#32531;&#35299;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#22312;&#26356;&#22823;&#35268;&#27169;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#21516;&#26102;&#22788;&#29702;&#23427;&#20204;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#21363;&#32534;&#36753;&#28023;&#37327;&#27010;&#24565;&#25193;&#25955;&#27169;&#22411;&#65288;EMCID&#65289;&#12290;&#31532;&#19968;&#38454;&#27573;&#36890;&#36807;&#25991;&#26412;&#23545;&#40784;&#25439;&#22833;&#21644;&#25193;&#25955;&#22122;&#22768;&#39044;&#27979;&#25439;&#22833;&#23454;&#29616;&#23545;&#27599;&#20010;&#21333;&#29420;&#27010;&#24565;&#30340;&#20869;&#23384;&#20248;&#21270;&#30340;&#21452;&#33258;&#33976;&#39311;&#12290;&#31532;&#20108;&#38454;&#27573;&#36827;&#34892;&#28023;&#37327;&#27010;&#24565;&#32534;&#36753;&#65292;&#37319;&#29992;&#22810;&#23618;&#12289;&#23553;&#38381;&#24418;&#24335;&#30340;&#27169;&#22411;&#32534;&#36753;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ImageNet&#27010;&#24565;&#32534;&#36753;&#22522;&#20934;&#65288;ICEB&#65289;&#30340;&#32508;&#21512;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#38024;&#23545;T2I&#27169;&#22411;&#30340;&#28023;&#37327;&#27010;&#24565;&#32534;&#36753;&#65292;&#21253;&#25324;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;&#33258;&#30001;&#24418;&#24335;&#25552;&#31034;&#21644;&#28023;&#37327;&#27010;&#24565;&#31867;&#21035;&#65292;&#20197;&#21450;&#24191;&#27867;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20934;&#21644;&#20808;&#21069;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13807v1 Announce Type: cross  Abstract: Text-to-image diffusion models suffer from the risk of generating outdated, copyrighted, incorrect, and biased content. While previous methods have mitigated the issues on a small scale, it is essential to handle them simultaneously in larger-scale real-world scenarios. We propose a two-stage method, Editing Massive Concepts In Diffusion Models (EMCID). The first stage performs memory optimization for each individual concept with dual self-distillation from text alignment loss and diffusion noise prediction loss. The second stage conducts massive concept editing with multi-layer, closed form model editing. We further propose a comprehensive benchmark, named ImageNet Concept Editing Benchmark (ICEB), for evaluating massive concept editing for T2I models with two subtasks, free-form prompts, massive concept categories, and extensive evaluation metrics. Extensive experiments conducted on our proposed benchmark and previous benchmarks demo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;RAR&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;CLIP&#21644;MLLMs&#30340;&#20248;&#21183;&#65292;&#22686;&#24378;&#20102;&#23569;&#26679;&#26412;/&#38646;&#26679;&#26412;&#35782;&#21035;&#33021;&#21147;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20855;&#26377;&#24191;&#27867;&#21644;&#32454;&#31890;&#24230;&#35789;&#27719;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.13805</link><description>&lt;p&gt;
RAR&#65306;&#29992;&#20110;&#35270;&#35273;&#35782;&#21035;&#30340;&#26816;&#32034;&#21644;&#25490;&#21517;&#22686;&#24378;MLLMs
&lt;/p&gt;
&lt;p&gt;
RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13805
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;RAR&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;CLIP&#21644;MLLMs&#30340;&#20248;&#21183;&#65292;&#22686;&#24378;&#20102;&#23569;&#26679;&#26412;/&#38646;&#26679;&#26412;&#35782;&#21035;&#33021;&#21147;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20855;&#26377;&#24191;&#27867;&#21644;&#32454;&#31890;&#24230;&#35789;&#27719;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13805v1 &#36890;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;CLIP&#65288;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65289;&#21033;&#29992;&#20174;&#22122;&#22768;&#22270;&#20687;&#25991;&#26412;&#23545;&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#22312;&#35782;&#21035;&#21508;&#31181;&#20505;&#36873;&#39033;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#23545;&#24191;&#27867;&#20851;&#32852;&#30340;&#20851;&#27880;&#38477;&#20302;&#20102;&#22312;&#21306;&#20998;&#32454;&#31890;&#24230;&#39033;&#30446;&#20013;&#24494;&#22937;&#24046;&#24322;&#30340;&#31934;&#24230;&#12290;&#30456;&#21453;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#20998;&#31867;&#32454;&#31890;&#24230;&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#24402;&#21151;&#20110;&#23427;&#20204;&#22312;&#22522;&#20110;&#32593;&#32476;&#30340;&#35821;&#26009;&#24211;&#19978;&#30340;&#39044;&#35757;&#32451;&#25152;&#20855;&#26377;&#30340;&#22823;&#37327;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31867;&#21035;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;MLLMs&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#19981;&#26029;&#22686;&#21152;&#30340;&#22797;&#26434;&#24615;&#21644;&#26377;&#38480;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#21327;&#21516;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#24182;&#22686;&#24378;&#38024;&#23545;&#20855;&#26377;&#24191;&#27867;&#21644;&#32454;&#31890;&#24230;&#35789;&#27719;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#30340;&#23569;&#26679;&#26412;/&#38646;&#26679;&#26412;&#35782;&#21035;&#33021;&#21147;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;RAR&#65292;&#19968;&#31181;&#29992;&#20110;MLLMs&#30340;&#26816;&#32034;&#21644;&#25490;&#21517;&#22686;&#24378;&#26041;&#27861;&#12290;&#25105;&#20204;&#26368;&#21021;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20110;CLIP&#30340;&#22810;&#27169;&#24577;&#26816;&#32034;&#22120;&#65292;&#29992;&#20110;&#21019;&#24314;&#21644;&#23384;&#20648;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13805v1 Announce Type: cross  Abstract: CLIP (Contrastive Language-Image Pre-training) uses contrastive learning from noise image-text pairs to excel at recognizing a wide array of candidates, yet its focus on broad associations hinders the precision in distinguishing subtle differences among fine-grained items. Conversely, Multimodal Large Language Models (MLLMs) excel at classifying fine-grained categories, thanks to their substantial knowledge from pre-training on web-level corpora. However, the performance of MLLMs declines with an increase in category numbers, primarily due to growing complexity and constraints of limited context window size. To synergize the strengths of both approaches and enhance the few-shot/zero-shot recognition abilities for datasets characterized by extensive and fine-grained vocabularies, this paper introduces RAR, a Retrieving And Ranking augmented method for MLLMs. We initially establish a multi-modal retriever based on CLIP to create and stor
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#21644;&#27169;&#22411;&#30693;&#35782;&#20256;&#36882;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#33268;&#24615;&#30446;&#26631;&#26469;&#22686;&#24378;&#39044;&#35757;&#32451;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#23450;&#20301;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.13804</link><description>&lt;p&gt;
&#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#20013;&#23398;&#20064;&#36827;&#34892;&#35270;&#35273;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Learning from Models and Data for Visual Grounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13804
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#21644;&#27169;&#22411;&#30693;&#35782;&#20256;&#36882;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#33268;&#24615;&#30446;&#26631;&#26469;&#22686;&#24378;&#39044;&#35757;&#32451;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#23450;&#20301;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;SynGround&#65292;&#36825;&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#21644;&#20174;&#21508;&#31181;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#23450;&#20301;&#33021;&#21147;&#12290;&#20174;&#27169;&#22411;&#20013;&#36827;&#34892;&#30340;&#30693;&#35782;&#20256;&#36882;&#24341;&#21457;&#20102;&#36890;&#36807;&#22270;&#20687;&#25551;&#36848;&#29983;&#25104;&#22120;&#29983;&#25104;&#22270;&#20687;&#25551;&#36848;&#12290;&#36825;&#20123;&#25551;&#36848;&#20855;&#26377;&#21452;&#37325;&#20316;&#29992;&#65306;&#23427;&#20204;&#20316;&#20026;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;&#21512;&#25104;&#22270;&#20687;&#30340;&#25552;&#31034;&#65292;&#20197;&#21450;&#20316;&#20026;&#26597;&#35810;&#26469;&#21512;&#25104;&#25991;&#26412;&#65292;&#20174;&#20854;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#30701;&#35821;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#24320;&#25918;&#35789;&#27719;&#30340;&#23545;&#35937;&#26816;&#27979;&#22120;&#20026;&#21512;&#25104;&#22270;&#20687;&#21644;&#25991;&#26412;&#29983;&#25104;&#21512;&#25104;&#36793;&#30028;&#26694;&#12290;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#36974;&#32617;-&#27880;&#24847;&#21147;&#19968;&#33268;&#24615;&#30446;&#26631;&#65292;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#30446;&#26631;&#23558;&#21306;&#22495;&#27880;&#37322;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#27169;&#22411;&#35299;&#37322;&#36827;&#34892;&#23545;&#40784;&#12290;&#26368;&#32456;&#30340;&#27169;&#22411;&#25552;&#21319;&#20102;&#23450;&#20301;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13804v1 Announce Type: cross  Abstract: We introduce SynGround, a novel framework that combines data-driven learning and knowledge transfer from various large-scale pretrained models to enhance the visual grounding capabilities of a pretrained vision-and-language model. The knowledge transfer from the models initiates the generation of image descriptions through an image description generator. These descriptions serve dual purposes: they act as prompts for synthesizing images through a text-to-image generator, and as queries for synthesizing text, from which phrases are extracted using a large language model. Finally, we leverage an open-vocabulary object detector to generate synthetic bounding boxes for the synthetic images and texts. We finetune a pretrained vision-and-language model on this dataset by optimizing a mask-attention consistency objective that aligns region annotations with gradient-based model explanations. The resulting model improves the grounding capabilit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Zigzag Mamba&#30340;&#38646;&#21442;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#32416;&#27491;&#24403;&#21069;Mamba-based&#35270;&#35273;&#26041;&#27861;&#20013;&#23545;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#24573;&#35270;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36895;&#24230;&#21644;&#20869;&#23384;&#21033;&#29992;&#65292;&#21516;&#26102;&#22312;&#22823;&#20998;&#36776;&#29575;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13802</link><description>&lt;p&gt;
ZigMa&#65306;&#34623;&#34578;&#26364;&#24052;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ZigMa: Zigzag Mamba Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Zigzag Mamba&#30340;&#38646;&#21442;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#32416;&#27491;&#24403;&#21069;Mamba-based&#35270;&#35273;&#26041;&#27861;&#20013;&#23545;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#24573;&#35270;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36895;&#24230;&#21644;&#20869;&#23384;&#21033;&#29992;&#65292;&#21516;&#26102;&#22312;&#22823;&#20998;&#36776;&#29575;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#21463;&#21040;&#21487;&#20280;&#32553;&#24615;&#21644;&#20108;&#27425;&#22797;&#26434;&#24615;&#38382;&#39064;&#30340;&#22256;&#25200;&#65292;&#29305;&#21035;&#26159;&#22312;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#32467;&#26500;&#20869;&#37096;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#21033;&#29992;&#19968;&#31181;&#31216;&#20026;&#26364;&#24052;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#33021;&#21147;&#65292;&#20197;&#25193;&#23637;&#20854;&#22312;&#35270;&#35273;&#25968;&#25454;&#29983;&#25104;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22823;&#22810;&#25968;&#24403;&#21069;&#22522;&#20110;&#26364;&#24052;&#30340;&#35270;&#35273;&#26041;&#27861;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#30095;&#24573;&#65292;&#21363;&#26364;&#24052;&#30340;&#25195;&#25551;&#26041;&#26696;&#20013;&#32570;&#20047;&#23545;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#32771;&#34385;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;&#36825;&#19968;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Zigzag Mamba&#30340;&#31616;&#21333;&#12289;&#21363;&#25554;&#21363;&#29992;&#12289;&#38646;&#21442;&#25968;&#26041;&#27861;&#65292;&#23427;&#20248;&#20110;&#22522;&#20110;&#26364;&#24052;&#30340;&#22522;&#32447;&#65292;&#24182;&#34920;&#29616;&#20986;&#27604;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22522;&#32447;&#26356;&#24555;&#36895;&#21644;&#26356;&#22909;&#30340;&#20869;&#23384;&#21033;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;Zigzag Mamba&#38598;&#25104;&#21040;&#38543;&#26426;&#25554;&#20540;&#26694;&#26550;&#20013;&#65292;&#20197;&#30740;&#31350;&#27169;&#22411;&#22312;&#22823;&#20998;&#36776;&#29575;&#35270;&#35273;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;FacesHQ $1024\times 1024$&#21644;UCF101&#65292;MultiModal-CelebA-HQ&#65289;&#19978;&#30340;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13802v1 Announce Type: cross  Abstract: The diffusion model has long been plagued by scalability and quadratic complexity issues, especially within transformer-based structures. In this study, we aim to leverage the long sequence modeling capability of a State-Space Model called Mamba to extend its applicability to visual data generation. Firstly, we identify a critical oversight in most current Mamba-based vision methods, namely the lack of consideration for spatial continuity in the scan scheme of Mamba. Secondly, building upon this insight, we introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba, which outperforms Mamba-based baselines and demonstrates improved speed and memory utilization compared to transformer-based baselines. Lastly, we integrate Zigzag Mamba with the Stochastic Interpolant framework to investigate the scalability of the model on large-resolution visual datasets, such as FacesHQ $1024\times 1024$ and UCF101, MultiModal-CelebA-HQ
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20998;&#23618;&#31070;&#32463;&#31526;&#21495;&#33539;&#24335;&#30340;&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#22312;&#36339;&#27700;&#20013;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#34920;&#29616;&#65292;&#36890;&#36807;&#20174;&#35270;&#39057;&#25968;&#25454;&#20013;&#25552;&#21462;&#21487;&#35299;&#37322;&#31526;&#21495;&#24182;&#24212;&#29992;&#35268;&#21017;&#36827;&#34892;&#36136;&#37327;&#35780;&#20272;&#65292;&#20248;&#20110;&#31471;&#21040;&#31471;&#31070;&#32463;&#27169;&#22411;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#21160;&#20316;&#35782;&#21035;&#21644;&#26102;&#38388;&#20998;&#21106;&#65292;&#24182;&#29983;&#25104;&#35814;&#32454;&#25253;&#21578;&#12290;</title><link>https://arxiv.org/abs/2403.13798</link><description>&lt;p&gt;
&#20998;&#23618;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#29992;&#20110;&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Hierarchical NeuroSymbolic Approach for Action Quality Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13798
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20998;&#23618;&#31070;&#32463;&#31526;&#21495;&#33539;&#24335;&#30340;&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#22312;&#36339;&#27700;&#20013;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#34920;&#29616;&#65292;&#36890;&#36807;&#20174;&#35270;&#39057;&#25968;&#25454;&#20013;&#25552;&#21462;&#21487;&#35299;&#37322;&#31526;&#21495;&#24182;&#24212;&#29992;&#35268;&#21017;&#36827;&#34892;&#36136;&#37327;&#35780;&#20272;&#65292;&#20248;&#20110;&#31471;&#21040;&#31471;&#31070;&#32463;&#27169;&#22411;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#21160;&#20316;&#35782;&#21035;&#21644;&#26102;&#38388;&#20998;&#21106;&#65292;&#24182;&#29983;&#25104;&#35814;&#32454;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;&#65288;AQA&#65289;&#24212;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#23450;&#37327;&#35780;&#20272;&#20154;&#31867;&#21160;&#20316;&#30340;&#34920;&#29616;&#25110;&#25191;&#34892;&#12290;&#24403;&#21069;&#30340;AQA&#26041;&#27861;&#26159;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#32570;&#20047;&#36879;&#26126;&#24230;&#24182;&#19988;&#26131;&#21463;&#20559;&#35265;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#22522;&#20110;&#20027;&#35266;&#20154;&#31867;&#21028;&#26029;&#20316;&#20026;&#22320;&#38754;&#30495;&#30456;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;AQA&#30340;&#31070;&#32463;&#31526;&#21495;&#33539;&#24335;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20174;&#35270;&#39057;&#25968;&#25454;&#20013;&#25277;&#35937;&#20986;&#21487;&#35299;&#37322;&#30340;&#31526;&#21495;&#65292;&#24182;&#36890;&#36807;&#23558;&#35268;&#21017;&#24212;&#29992;&#20110;&#36825;&#20123;&#31526;&#21495;&#36827;&#34892;&#36136;&#37327;&#35780;&#20272;&#12290;&#25105;&#20204;&#20197;&#36339;&#27700;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#39046;&#22495;&#19987;&#23478;&#26356;&#21916;&#27426;&#25105;&#20204;&#30340;&#31995;&#32479;&#65292;&#24182;&#21457;&#29616;&#20854;&#27604;&#32431;&#31070;&#32463;&#26041;&#27861;&#26356;&#20855;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#36824;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21160;&#20316;&#35782;&#21035;&#21644;&#26102;&#38388;&#20998;&#21106;&#65292;&#24182;&#33258;&#21160;&#29983;&#25104;&#20102;&#19968;&#20221;&#35814;&#32454;&#25253;&#21578;&#65292;&#23558;&#36339;&#27700;&#20998;&#35299;&#20026;&#20854;&#20803;&#32032;&#65292;&#24182;&#25552;&#20379;&#24102;&#26377;&#35270;&#35273;&#35777;&#25454;&#30340;&#23458;&#35266;&#35780;&#20998;&#12290;&#32463;&#19968;&#32452;&#39046;&#22495;&#19987;&#23478;&#39564;&#35777;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13798v1 Announce Type: cross  Abstract: Action quality assessment (AQA) applies computer vision to quantitatively assess the performance or execution of a human action. Current AQA approaches are end-to-end neural models, which lack transparency and tend to be biased because they are trained on subjective human judgements as ground-truth. To address these issues, we introduce a neuro-symbolic paradigm for AQA, which uses neural networks to abstract interpretable symbols from video data and makes quality assessments by applying rules to those symbols. We take diving as the case study. We found that domain experts prefer our system and find it more informative than purely neural approaches to AQA in diving. Our system also achieves state-of-the-art action recognition and temporal segmentation, and automatically generates a detailed report that breaks the dive down into its elements and provides objective scoring with visual evidence. As verified by a group of domain experts, t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#35821;&#35328;-Only VLM&#36873;&#25321;&#20013;&#30340;&#20004;&#20010;&#22266;&#26377;&#25361;&#25112;&#65306;&#12300;&#27169;&#24577;&#24046;&#36317;&#12301;&#21644;&#12300;&#33021;&#21147;&#24046;&#36317;&#12301;&#65292;&#24182;&#25552;&#20986;&#20102;VLM&#36873;&#25321;&#20013;&#24357;&#21512;&#36825;&#20004;&#20010;&#24046;&#36317;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.13797</link><description>&lt;p&gt;
&#24357;&#21512;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#20013;&#30340;&#27169;&#24577;&#24046;&#36317;&#21644;&#33021;&#21147;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Bridge the Modality and Capacity Gaps in Vision-Language Model Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#35821;&#35328;-Only VLM&#36873;&#25321;&#20013;&#30340;&#20004;&#20010;&#22266;&#26377;&#25361;&#25112;&#65306;&#12300;&#27169;&#24577;&#24046;&#36317;&#12301;&#21644;&#12300;&#33021;&#21147;&#24046;&#36317;&#12301;&#65292;&#24182;&#25552;&#20986;&#20102;VLM&#36873;&#25321;&#20013;&#24357;&#21512;&#36825;&#20004;&#20010;&#24046;&#36317;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#36890;&#36807;&#23558;&#22270;&#20687;&#19982;&#25991;&#26412;&#31867;&#21035;&#21517;&#31216;&#37197;&#23545;&#65292;&#22312;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#39044;&#35757;&#32451;&#30340;VLMs&#30340;&#19981;&#26029;&#22686;&#21152;&#20351;&#24471;&#29305;&#23450;&#20219;&#21153;&#30340;VLM&#36873;&#25321;&#26356;&#26377;&#21487;&#33021;&#26631;&#35782;&#20986;&#36866;&#21512;&#30340;VLM&#12290;&#22240;&#27492;&#65292;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#31574;&#30053;&#26159;&#20174;VLM&#21160;&#29289;&#22253;&#20013;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;VLM&#65292;&#20165;&#20381;&#36182;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#25991;&#26412;&#25968;&#25454;&#32780;&#26080;&#38656;&#35775;&#38382;&#25968;&#25454;&#38598;&#30340;&#22270;&#20687;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#36825;&#31181;&#20165;&#35821;&#35328;VLM&#36873;&#25321;&#20013;&#20004;&#20010;&#22266;&#26377;&#25361;&#25112;&#65306;&#12300;&#27169;&#24577;&#24046;&#36317;&#12301;&#8212;&#8212;VLM&#22312;&#20004;&#20010;&#19981;&#21516;&#27169;&#24577;&#19979;&#30340;&#23884;&#20837;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20351;&#24471;&#25991;&#26412;&#25104;&#20026;&#22270;&#20687;&#30340;&#19968;&#20010;&#19981;&#22826;&#21487;&#38752;&#30340;&#26367;&#20195;&#21697;&#65307;&#12300;&#33021;&#21147;&#24046;&#36317;&#12301;&#8212;&#8212;VLM&#30340;&#25972;&#20307;&#25490;&#21517;&#19982;&#20854;&#22312;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#25490;&#21517;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#38459;&#30861;&#20102;&#30452;&#25509;&#20174;&#27169;&#22411;&#30340;&#25972;&#20307;&#34920;&#29616;&#26469;&#39044;&#27979;&#20854;&#25968;&#25454;&#38598;&#29305;&#23450;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;VLM&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13797v1 Announce Type: new  Abstract: Vision Language Models (VLMs) excel in zero-shot image classification by pairing images with textual category names. The expanding variety of Pre-Trained VLMs enhances the likelihood of identifying a suitable VLM for specific tasks. Thus, a promising zero-shot image classification strategy is selecting the most appropriate Pre-Trained VLM from the VLM Zoo, relying solely on the text data of the target dataset without access to the dataset's images. In this paper, we analyze two inherent challenges in assessing the ability of a VLM in this Language-Only VLM selection: the "Modality Gap" -- the disparity in VLM's embeddings across two different modalities, making text a less reliable substitute for images; and the "Capability Gap" -- the discrepancy between the VLM's overall ranking and its ranking for target dataset, hindering direct prediction of a model's dataset-specific performance from its general performance. We propose VLM Selectio
&lt;/p&gt;</description></item><item><title>PyVRP&#26159;&#19968;&#20010;&#23454;&#29616;&#28151;&#21512;&#36951;&#20256;&#25628;&#32034;&#20316;&#20026;&#26368;&#20808;&#36827;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRP&#65289;&#27714;&#35299;&#22120;&#30340;Python&#21253;&#65292;&#22312;&#22810;&#20010;&#31454;&#36187;&#20013;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#65292;&#32467;&#21512;&#20102;Python&#30340;&#28789;&#27963;&#24615;&#21644;C++&#30340;&#24615;&#33021;&#65292;&#20195;&#30721;&#36136;&#37327;&#39640;&#19988;&#22312;VRPTW&#21644;&#26377;&#23481;&#37327;&#38480;&#21046;&#30340;VRP&#38382;&#39064;&#19978;&#36798;&#21040;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.13795</link><description>&lt;p&gt;
PyVRP: &#19968;&#20010;&#39640;&#24615;&#33021;&#30340;VRP&#27714;&#35299;&#22120;&#21253;
&lt;/p&gt;
&lt;p&gt;
PyVRP: a high-performance VRP solver package
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13795
&lt;/p&gt;
&lt;p&gt;
PyVRP&#26159;&#19968;&#20010;&#23454;&#29616;&#28151;&#21512;&#36951;&#20256;&#25628;&#32034;&#20316;&#20026;&#26368;&#20808;&#36827;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRP&#65289;&#27714;&#35299;&#22120;&#30340;Python&#21253;&#65292;&#22312;&#22810;&#20010;&#31454;&#36187;&#20013;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#65292;&#32467;&#21512;&#20102;Python&#30340;&#28789;&#27963;&#24615;&#21644;C++&#30340;&#24615;&#33021;&#65292;&#20195;&#30721;&#36136;&#37327;&#39640;&#19988;&#22312;VRPTW&#21644;&#26377;&#23481;&#37327;&#38480;&#21046;&#30340;VRP&#38382;&#39064;&#19978;&#36798;&#21040;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;PyVRP&#65292;&#19968;&#20010;&#23454;&#29616;&#28151;&#21512;&#36951;&#20256;&#25628;&#32034;&#20316;&#20026;&#26368;&#20808;&#36827;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRP&#65289;&#27714;&#35299;&#22120;&#30340;Python&#21253;&#12290;&#35813;&#21253;&#26088;&#22312;&#29992;&#20110;&#24102;&#26377;&#26102;&#38388;&#31383;&#21475;&#65288;VRPTW&#65289;&#30340;VRP&#65292;&#20294;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#20197;&#25903;&#25345;&#20854;&#20182;VRP&#21464;&#20307;&#12290;PyVRP&#32467;&#21512;&#20102;Python&#30340;&#28789;&#27963;&#24615;&#21644;C++&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22312;C++&#20013;&#23454;&#29616;&#31639;&#27861;&#30340;&#24615;&#33021;&#20851;&#38190;&#37096;&#20998;&#65292;&#21516;&#26102;&#22312;Python&#32423;&#21035;&#20840;&#38754;&#23450;&#21046;&#21270;&#12290;PyVRP&#26159;&#35813;&#31639;&#27861;&#30340;&#19968;&#20010;&#23436;&#21892;&#23454;&#29616;&#65292;&#22312;2021&#24180;DIMACS VRPTW&#25361;&#25112;&#36187;&#20013;&#21517;&#21015;&#31532;&#19968;&#65292;&#22312;&#25913;&#36827;&#21518;&#65292;&#22312;2022&#24180;EURO Meets NeurIPS&#36710;&#36742;&#36335;&#24452;&#31454;&#36187;&#30340;&#38745;&#24577;&#21464;&#20307;&#19978;&#21517;&#21015;&#31532;&#19968;&#12290;&#35813;&#20195;&#30721;&#36981;&#24490;&#33391;&#22909;&#30340;&#36719;&#20214;&#24037;&#31243;&#23454;&#36341;&#65292;&#24182;&#19988;&#26377;&#30528;&#33391;&#22909;&#30340;&#25991;&#26723;&#21644;&#21333;&#20803;&#27979;&#35797;&#12290;PyVRP&#22312;&#33258;&#30001;&#30340;MIT&#35768;&#21487;&#35777;&#19979;&#20813;&#36153;&#25552;&#20379;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;PyVRP&#22312;VRPTW&#21644;&#26377;&#23481;&#37327;&#38480;&#21046;&#30340;VRP&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13795v1 Announce Type: cross  Abstract: We introduce PyVRP, a Python package that implements Hybrid Genetic Search as a state-of-the-art Vehicle Routing Problem (VRP) solver. The package is designed for the VRP with Time Windows (VRPTW), but can be easily extended to support other VRP variants. PyVRP combines the flexibility of Python with the performance of C++, by implementing (only) performance critical parts of the algorithm in C++, while being fully customisable at the Python level. PyVRP is a polished implementation of the algorithm that ranked 1st in the 2021 DIMACS VRPTW Challenge and, after improvements, ranked 1st on the static variant of the EURO Meets NeurIPS 2022 Vehicle Routing Competition. The code follows good software engineering practices, and is well-documented and unit tested. PyVRP is freely available under the liberal MIT license. Through numerical experiments we show that PyVRP achieves state-of-the-art results on the VRPTW and Capacitated VRP. We hope
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#26032;AI&#31995;&#32479;&#21361;&#38505;&#33021;&#21147;&#30340;&#21069;&#27839;&#27169;&#22411;&#65292;&#24182;&#22312;Gemini 1.0&#27169;&#22411;&#19978;&#24320;&#23637;&#20102;&#26032;&#30340;&#35780;&#20272;&#39033;&#30446;&#65292;&#34429;&#26410;&#21457;&#29616;&#24378;&#22823;&#21361;&#38505;&#33021;&#21147;&#35777;&#25454;&#20294;&#25552;&#20986;&#26089;&#26399;&#39044;&#35686;&#20449;&#21495;&#65292;&#26088;&#22312;&#25512;&#36827;&#21361;&#38505;&#33021;&#21147;&#35780;&#20272;&#31185;&#23398;&#65292;&#20026;&#26410;&#26469;&#27169;&#22411;&#20570;&#20934;&#22791;&#12290;</title><link>https://arxiv.org/abs/2403.13793</link><description>&lt;p&gt;
&#35780;&#20272;&#21361;&#38505;&#33021;&#21147;&#30340;&#21069;&#27839;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Frontier Models for Dangerous Capabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13793
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#26032;AI&#31995;&#32479;&#21361;&#38505;&#33021;&#21147;&#30340;&#21069;&#27839;&#27169;&#22411;&#65292;&#24182;&#22312;Gemini 1.0&#27169;&#22411;&#19978;&#24320;&#23637;&#20102;&#26032;&#30340;&#35780;&#20272;&#39033;&#30446;&#65292;&#34429;&#26410;&#21457;&#29616;&#24378;&#22823;&#21361;&#38505;&#33021;&#21147;&#35777;&#25454;&#20294;&#25552;&#20986;&#26089;&#26399;&#39044;&#35686;&#20449;&#21495;&#65292;&#26088;&#22312;&#25512;&#36827;&#21361;&#38505;&#33021;&#21147;&#35780;&#20272;&#31185;&#23398;&#65292;&#20026;&#26410;&#26469;&#27169;&#22411;&#20570;&#20934;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20102;&#35299;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#33021;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#25105;&#20204;&#24517;&#39035;&#20102;&#35299;&#23427;&#33021;&#20570;&#20160;&#20040;&#65292;&#20197;&#21450;&#19981;&#33021;&#20570;&#20160;&#20040;&#12290;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;&#8220;&#21361;&#38505;&#33021;&#21147;&#8221;&#35780;&#20272;&#35745;&#21010;&#65292;&#24182;&#22312;Gemini 1.0&#27169;&#22411;&#19978;&#36827;&#34892;&#35797;&#28857;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#28085;&#30422;&#22235;&#20010;&#39046;&#22495;&#65306;&#65288;1&#65289;&#35828;&#26381;&#21644;&#27450;&#39575;&#65307;&#65288;2&#65289;&#32593;&#32476;&#23433;&#20840;&#65307;&#65288;3&#65289;&#33258;&#25105;&#25193;&#25955;&#65307;&#21644;&#65288;4&#65289;&#33258;&#25105;&#25512;&#29702;&#12290;&#25105;&#20204;&#27809;&#26377;&#21457;&#29616;&#25105;&#20204;&#35780;&#20272;&#30340;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#21361;&#38505;&#33021;&#21147;&#30340;&#35777;&#25454;&#65292;&#20294;&#25105;&#20204;&#26631;&#35760;&#20102;&#26089;&#26399;&#39044;&#35686;&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25512;&#21160;&#19968;&#20010;&#20005;&#26684;&#30340;&#21361;&#38505;&#33021;&#21147;&#35780;&#20272;&#31185;&#23398;&#65292;&#20026;&#26410;&#26469;&#27169;&#22411;&#20570;&#22909;&#20934;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13793v1 Announce Type: new  Abstract: To understand the risks posed by a new AI system, we must understand what it can and cannot do. Building on prior work, we introduce a programme of new "dangerous capability" evaluations and pilot them on Gemini 1.0 models. Our evaluations cover four areas: (1) persuasion and deception; (2) cyber-security; (3) self-proliferation; and (4) self-reasoning. We do not find evidence of strong dangerous capabilities in the models we evaluated, but we flag early warning signs. Our goal is to help advance a rigorous science of dangerous capability evaluation, in preparation for future models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;RewardBench, &#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22870;&#21169;&#27169;&#22411;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#24211;&#65292;&#26088;&#22312;&#22686;&#24378;&#23545;&#22870;&#21169;&#27169;&#22411;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.13787</link><description>&lt;p&gt;
RewardBench&#65306;&#35780;&#20272;&#29992;&#20110;&#35821;&#35328;&#24314;&#27169;&#30340;&#22870;&#21169;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RewardBench: Evaluating Reward Models for Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;RewardBench, &#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22870;&#21169;&#27169;&#22411;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#24211;&#65292;&#26088;&#22312;&#22686;&#24378;&#23545;&#22870;&#21169;&#27169;&#22411;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#27169;&#22411;&#65288;RMs&#65289;&#26159;&#25104;&#21151;RLHF&#30340;&#20851;&#38190;&#65292;&#29992;&#20110;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#65292;&#28982;&#32780;&#30456;&#23545;&#36739;&#23569;&#30340;&#30740;&#31350;&#20851;&#27880;&#23545;&#36825;&#20123;&#22870;&#21169;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;&#35780;&#20272;&#22870;&#21169;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#20102;&#35299;&#29992;&#20110;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#36879;&#26126;&#25216;&#26415;&#21450;&#20854;&#23884;&#20837;&#20160;&#20040;&#20215;&#20540;&#30340;&#26426;&#20250;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20960;&#20046;&#27809;&#26377;&#20851;&#20110;&#33021;&#21147;&#25551;&#36848;&#12289;&#35757;&#32451;&#26041;&#27861;&#25110;&#24320;&#28304;&#22870;&#21169;&#27169;&#22411;&#30340;&#25551;&#36848;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;RewardBench&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#24211;&#65292;&#20197;&#22686;&#24378;&#23545;&#22870;&#21169;&#27169;&#22411;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;RewardBench&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#36328;&#23545;&#35805;&#12289;&#25512;&#29702;&#21644;&#23433;&#20840;&#24615;&#30340;&#25552;&#31034;-&#36194;-&#36755;&#19977;&#20803;&#32452;&#38598;&#21512;&#65292;&#29992;&#20110;&#35780;&#20272;&#22870;&#21169;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#12289;&#32467;&#26500;&#21270;&#21644;&#36229;&#20998;&#24067;&#26597;&#35810;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20026;RMs&#21019;&#24314;&#20102;&#29305;&#23450;&#30340;&#27604;&#36739;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#26377;&#24494;&#22937;&#20294;&#21487;&#39564;&#35777;&#30340;&#21407;&#22240;&#65288;&#20363;&#22914;&#38169;&#35823;&#12289;&#19981;&#27491;&#30830;&#30340;&#20107;&#23454;&#65289;&#65292;&#35299;&#37322;&#20026;&#20160;&#20040;&#19968;&#20010;&#31572;&#26696;&#24212;&#35813;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13787v1 Announce Type: new  Abstract: Reward models (RMs) are at the crux of successful RLHF to align pretrained models to human preferences, yet there has been relatively little study that focuses on evaluation of those reward models. Evaluating reward models presents an opportunity to understand the opaque technologies used for alignment of language models and which values are embedded in them. To date, very few descriptors of capabilities, training methods, or open-source reward models exist. In this paper, we present RewardBench, a benchmark dataset and code-base for evaluation, to enhance scientific understanding of reward models. The RewardBench dataset is a collection of prompt-win-lose trios spanning chat, reasoning, and safety, to benchmark how reward models perform on challenging, structured and out-of-distribution queries. We created specific comparison datasets for RMs that have subtle, but verifiable reasons (e.g. bugs, incorrect facts) why one answer should be 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#39044;&#27979;&#32500;&#25252;&#22330;&#26223;&#19979;&#25193;&#23637;&#25925;&#38556;&#26641;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#29616;&#20195;&#21487;&#38752;&#24615;&#30740;&#31350;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.13785</link><description>&lt;p&gt;
&#22312;&#39044;&#27979;&#32500;&#25252;&#22330;&#26223;&#19979;&#25193;&#23637;&#25925;&#38556;&#26641;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards an extension of Fault Trees in the Predictive Maintenance Scenario
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13785
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#39044;&#27979;&#32500;&#25252;&#22330;&#26223;&#19979;&#25193;&#23637;&#25925;&#38556;&#26641;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#29616;&#20195;&#21487;&#38752;&#24615;&#30740;&#31350;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#38556;&#26641;&#65288;FTs&#65289;&#26368;&#21463;&#27426;&#36814;&#30340;&#29305;&#28857;&#20043;&#19968;&#26159;&#20854;&#31616;&#21333;&#24615;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#24037;&#19994;&#27969;&#31243;&#12290;&#38543;&#30528;&#36825;&#20123;&#27969;&#31243;&#38543;&#26102;&#38388;&#21457;&#23637;&#65292;&#22312;&#32771;&#34385;&#26032;&#30340;&#22823;&#22411;&#29616;&#20195;&#31995;&#32479;&#26041;&#38754;&#65292;&#22522;&#20110;FTs&#30340;&#24314;&#27169;&#25216;&#26415;&#24050;&#32463;&#36866;&#24212;&#20102;&#36825;&#20123;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25193;&#23637;FTs&#20197;&#32771;&#34385;&#39044;&#27979;&#32500;&#25252;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#29616;&#20195;&#21487;&#38752;&#24615;&#30740;&#31350;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290;&#25991;&#31456;&#21246;&#21202;&#20102;&#39044;&#27979;&#25925;&#38556;&#26641;&#35821;&#35328;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#20351;&#29992;&#26696;&#20363;&#26469;&#25903;&#25345;&#23545;&#20854;&#22312;&#20855;&#20307;&#24037;&#19994;&#29615;&#22659;&#20013;&#30340;&#24314;&#27169;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13785v1 Announce Type: new  Abstract: One of the most appreciated features of Fault Trees (FTs) is their simplicity, making them fit into industrial processes. As such processes evolve in time, considering new aspects of large modern systems, modelling techniques based on FTs have adapted to these needs. This paper proposes an extension of FTs to take into account the problem of Predictive Maintenance, one of the challenges of the modern dependability field of study. The paper sketches the Predictive Fault Tree language and proposes some use cases to support their modelling and analysis in concrete industrial settings.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;&#65288;MOF&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#25490;&#21517;&#20998;&#31867;&#31995;&#32479;&#65292;&#26681;&#25454;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#20419;&#36827;&#23436;&#25972;&#24615;&#12289;&#24320;&#25918;&#24615;&#20197;&#21450;&#36981;&#24490;&#24320;&#25918;&#31185;&#23398;&#21407;&#21017;&#65292;&#21487;&#20197;&#24110;&#21161;&#20934;&#30830;&#35782;&#21035;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13784</link><description>&lt;p&gt;
&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;: &#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#21487;&#37325;&#29616;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#29992;&#24615;&#30340;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency and Usability in AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13784
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;&#65288;MOF&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#25490;&#21517;&#20998;&#31867;&#31995;&#32479;&#65292;&#26681;&#25454;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#20419;&#36827;&#23436;&#25972;&#24615;&#12289;&#24320;&#25918;&#24615;&#20197;&#21450;&#36981;&#24490;&#24320;&#25918;&#31185;&#23398;&#21407;&#21017;&#65292;&#21487;&#20197;&#24110;&#21161;&#20934;&#30830;&#35782;&#21035;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#21487;&#33021;&#24615;&#65292;&#20294;&#20854;&#21830;&#19994;&#21270;&#24341;&#21457;&#20102;&#20851;&#20110;&#36879;&#26126;&#24230;&#12289;&#21487;&#37325;&#29616;&#24615;&#12289;&#20559;&#35265;&#21644;&#23433;&#20840;&#24615;&#30340;&#25285;&#24551;&#12290;&#35768;&#22810;"&#24320;&#28304;"&#30340;GAI&#27169;&#22411;&#32570;&#20047;&#23436;&#25972;&#29702;&#35299;&#21644;&#20877;&#29616;&#25152;&#24517;&#38656;&#30340;&#32452;&#20214;&#65292;&#19968;&#20123;&#37319;&#29992;&#38480;&#21046;&#24615;&#35768;&#21487;&#35777;&#65292;&#36825;&#31181;&#34892;&#20026;&#34987;&#31216;&#20026;"&#24320;&#28304;&#27927;&#30333;"&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;&#65288;MOF&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26681;&#25454;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25490;&#21517;&#20998;&#31867;&#30340;&#31995;&#32479;&#65292;&#36981;&#24490;&#24320;&#25918;&#31185;&#23398;&#12289;&#24320;&#28304;&#12289;&#24320;&#25918;&#25968;&#25454;&#21644;&#24320;&#25918;&#33719;&#21462;&#30340;&#21407;&#21017;&#12290;MOF&#35201;&#27714;&#27169;&#22411;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#30340;&#29305;&#23450;&#32452;&#20214;&#34987;&#21253;&#21547;&#24182;&#26681;&#25454;&#36866;&#24403;&#30340;&#24320;&#25918;&#35768;&#21487;&#35777;&#21457;&#24067;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#38450;&#27490;&#23459;&#31216;&#33258;&#24049;&#26159;&#24320;&#25918;&#30340;&#27169;&#22411;&#34987;&#35823;&#35299;&#65292;&#25351;&#23548;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#20197;&#23485;&#26494;&#30340;&#35768;&#21487;&#35777;&#21457;&#24067;&#25152;&#26377;&#27169;&#22411;&#32452;&#20214;&#65292;&#24182;&#24110;&#21161;&#20844;&#21496;&#12289;&#23398;&#26415;&#30028;&#21644;&#29233;&#22909;&#32773;&#35782;&#21035;&#21487;&#20197;&#23433;&#20840;&#37319;&#29992;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13784v1 Announce Type: new  Abstract: Generative AI (GAI) offers unprecedented possibilities but its commercialization has raised concerns about transparency, reproducibility, bias, and safety. Many "open-source" GAI models lack the necessary components for full understanding and reproduction, and some use restrictive licenses, a practice known as "openwashing." We propose the Model Openness Framework (MOF), a ranked classification system that rates machine learning models based on their completeness and openness, following principles of open science, open source, open data, and open access. The MOF requires specific components of the model development lifecycle to be included and released under appropriate open licenses. This framework aims to prevent misrepresentation of models claiming to be open, guide researchers and developers in providing all model components under permissive licenses, and help companies, academia, and hobbyists identify models that can be safely adop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31232;&#30095;&#23454;&#29616;&#30340;&#22810;&#21151;&#33021;&#22270;&#20449;&#24687;&#23618;&#65292;&#36890;&#36807;&#21033;&#29992;&#37051;&#25509;&#30697;&#38453;&#30340;&#31232;&#30095;&#24615;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#65292;&#25913;&#36827;&#20102;&#22270;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(GINNs)&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13781</link><description>&lt;p&gt;
&#31232;&#30095;&#23454;&#29616;&#22810;&#21151;&#33021;&#22270;&#20449;&#24687;&#23618;
&lt;/p&gt;
&lt;p&gt;
Sparse Implementation of Versatile Graph-Informed Layers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31232;&#30095;&#23454;&#29616;&#30340;&#22810;&#21151;&#33021;&#22270;&#20449;&#24687;&#23618;&#65292;&#36890;&#36807;&#21033;&#29992;&#37051;&#25509;&#30697;&#38453;&#30340;&#31232;&#30095;&#24615;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#65292;&#25913;&#36827;&#20102;&#22270;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(GINNs)&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24050;&#32463;&#25104;&#20026;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#23398;&#20064;&#20219;&#21153;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#26368;&#36817;&#65292;&#24341;&#20837;&#20102;&#22270;&#20449;&#24687;(GI)&#23618;&#65292;&#20197;&#35299;&#20915;&#22270;&#33410;&#28857;&#19978;&#30340;&#22238;&#24402;&#20219;&#21153;&#65292;&#25193;&#23637;&#20102;&#23427;&#20204; beyond &#32463;&#20856; GNNs &#30340;&#36866;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23494;&#38598;&#20869;&#23384;&#20998;&#37197;&#65292;&#29616;&#26377;&#30340; GI &#23618;&#23454;&#29616;&#32570;&#20047;&#25928;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; GI &#23618;&#30340;&#31232;&#30095;&#23454;&#29616;&#65292;&#21033;&#29992;&#37051;&#25509;&#30697;&#38453;&#30340;&#31232;&#30095;&#24615;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102; GI &#23618;&#30340;&#36890;&#29992;&#24418;&#24335;&#65292;&#20351;&#20854;&#33021;&#22815;&#24212;&#29992;&#20110;&#22270;&#33410;&#28857;&#30340;&#23376;&#38598;&#12290;&#25152;&#25552;&#20986;&#30340;&#31232;&#30095;&#23454;&#29616;&#25913;&#36827;&#20102; GI &#23618;&#30340;&#20855;&#20307;&#35745;&#31639;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#20801;&#35768;&#26500;&#24314;&#26356;&#28145;&#30340;&#22270;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(GINNs)&#24182;&#20419;&#36827;&#20854;&#21521;&#26356;&#22823;&#22270;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13781v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have emerged as effective tools for learning tasks on graph-structured data. Recently, Graph-Informed (GI) layers were introduced to address regression tasks on graph nodes, extending their applicability beyond classic GNNs. However, existing implementations of GI layers lack efficiency due to dense memory allocation. This paper presents a sparse implementation of GI layers, leveraging the sparsity of adjacency matrices to reduce memory usage significantly. Additionally, a versatile general form of GI layers is introduced, enabling their application to subsets of graph nodes. The proposed sparse implementation improves the concrete computational efficiency and scalability of the GI layers, permitting to build deeper Graph-Informed Neural Networks (GINNs) and facilitating their scalability to larger graphs.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861; Describe-and-Dissect&#65288;DnD&#65289;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#26080;&#38656;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#39044;&#23450;&#20041;&#30340;&#27010;&#24565;&#36873;&#25321;&#65292;&#24182;&#19988;&#36890;&#36807;&#24191;&#27867;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#26174;&#31034;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.13771</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#35270;&#35273;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#20803;&#65306;&#25551;&#36848;&#19982;&#35299;&#21078;
&lt;/p&gt;
&lt;p&gt;
Describe-and-Dissect: Interpreting Neurons in Vision Networks with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13771
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861; Describe-and-Dissect&#65288;DnD&#65289;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#26080;&#38656;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#39044;&#23450;&#20041;&#30340;&#27010;&#24565;&#36873;&#25321;&#65292;&#24182;&#19988;&#36890;&#36807;&#24191;&#27867;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#26174;&#31034;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Describe-and-Dissect&#65288;DnD&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25551;&#36848;&#35270;&#35273;&#32593;&#32476;&#20013;&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#20316;&#29992;&#12290;DnD&#21033;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#29983;&#25104;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#26080;&#38656;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#39044;&#23450;&#20041;&#30340;&#27010;&#24565;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;DnD&#26159;&#26080;&#38656;&#35757;&#32451;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#19981;&#35757;&#32451;&#20219;&#20309;&#26032;&#27169;&#22411;&#65292;&#26410;&#26469;&#21487;&#20197;&#36731;&#26494;&#21033;&#29992;&#26356;&#24378;&#22823;&#30340;&#36890;&#29992;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#34920;&#26126;DnD&#36890;&#36807;&#25552;&#20379;&#26356;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#20803;&#25551;&#36848;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24179;&#22343;&#25552;&#20379;&#26368;&#39640;&#36136;&#37327;&#30340;&#26631;&#31614;&#65292;&#24182;&#19988;&#34987;&#36873;&#20026;&#31070;&#32463;&#20803;&#30340;&#26368;&#20339;&#35299;&#37322;&#30340;&#27010;&#29575;&#26159;&#26368;&#20339;&#22522;&#32447;&#30340;&#20004;&#20493;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13771v1 Announce Type: cross  Abstract: In this paper, we propose Describe-and-Dissect (DnD), a novel method to describe the roles of hidden neurons in vision networks. DnD utilizes recent advancements in multimodal deep learning to produce complex natural language descriptions, without the need for labeled training data or a predefined set of concepts to choose from. Additionally, DnD is training-free, meaning we don't train any new models and can easily leverage more capable general purpose models in the future. We have conducted extensive qualitative and quantitative analysis to show that DnD outperforms prior work by providing higher quality neuron descriptions. Specifically, our method on average provides the highest quality labels and is more than 2 times as likely to be selected as the best explanation for a neuron than the best baseline.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#35270;&#39057;&#25968;&#25454;&#36827;&#34892;&#20915;&#31574;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35266;&#23519;&#20013;&#23384;&#22312;&#22806;&#29983;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#35777;&#26126;&#20102;&#26102;&#38388;&#23545;&#27604;&#23398;&#20064;&#21644;&#21069;&#21521;&#24314;&#27169;&#30340;&#29702;&#35770;&#19978;&#38480;&#12290;</title><link>https://arxiv.org/abs/2403.13765</link><description>&lt;p&gt;
&#38754;&#21521;&#24378;&#21270;&#23398;&#20064;&#35270;&#39057;&#30340;&#22522;&#20110;&#21407;&#21017;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Principled Representation Learning from Videos for Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13765
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#35270;&#39057;&#25968;&#25454;&#36827;&#34892;&#20915;&#31574;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35266;&#23519;&#20013;&#23384;&#22312;&#22806;&#29983;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#35777;&#26126;&#20102;&#26102;&#38388;&#23545;&#27604;&#23398;&#20064;&#21644;&#21069;&#21521;&#24314;&#27169;&#30340;&#29702;&#35770;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20351;&#29992;&#35270;&#39057;&#25968;&#25454;&#36827;&#34892;&#20915;&#31574;&#30340;&#34920;&#31034;&#39044;&#35757;&#32451;&#65292;&#36825;&#20123;&#25968;&#25454;&#22312;&#35832;&#22914;&#28216;&#25103;&#20195;&#29702;&#21644;&#36719;&#20214;&#27979;&#35797;&#31561;&#20219;&#21153;&#20013;&#20016;&#23500;&#21487;&#29992;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32463;&#39564;&#36827;&#23637;&#65292;&#20294;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#32570;&#20047;&#12290;&#25105;&#20204;&#24320;&#22987;&#23545;&#34920;&#31034;&#23398;&#20064;&#30340;&#22522;&#20110;&#21407;&#21017;&#30340;&#26041;&#27861;&#36827;&#34892;&#29702;&#35770;&#30740;&#31350;&#65292;&#24182;&#19987;&#27880;&#20110;&#20351;&#29992;&#35270;&#39057;&#25968;&#25454;&#23398;&#20064;&#28508;&#22312;MDP&#30340;&#29366;&#24577;&#34920;&#31034;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#35774;&#32622;&#65306;&#19968;&#31181;&#26159;&#35266;&#23519;&#20013;&#23384;&#22312;iid&#22122;&#22768;&#30340;&#24773;&#20917;&#65292;&#21478;&#19968;&#31181;&#26159;&#20855;&#26377;&#22806;&#29983;&#22122;&#22768;&#23384;&#22312;&#30340;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#35774;&#32622;&#65292;&#22806;&#29983;&#22122;&#22768;&#26159;&#25351;&#22312;&#26102;&#38388;&#19978;&#30456;&#20851;&#30340;&#38750;iid&#22122;&#22768;&#65292;&#22914;&#32972;&#26223;&#20013;&#20154;&#21592;&#25110;&#27773;&#36710;&#30340;&#36816;&#21160;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#24120;&#29992;&#26041;&#27861;&#65306;&#33258;&#21160;&#32534;&#30721;&#12289;&#26102;&#38388;&#23545;&#27604;&#23398;&#20064;&#21644;&#21069;&#21521;&#24314;&#27169;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#26102;&#38388;&#23545;&#27604;&#23398;&#20064;&#21644;&#21069;&#21521;&#24314;&#27169;&#30340;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13765v1 Announce Type: new  Abstract: We study pre-training representations for decision-making using video data, which is abundantly available for tasks such as game agents and software testing. Even though significant empirical advances have been made on this problem, a theoretical understanding remains absent. We initiate the theoretical investigation into principled approaches for representation learning and focus on learning the latent state representations of the underlying MDP using video data. We study two types of settings: one where there is iid noise in the observation, and a more challenging setting where there is also the presence of exogenous noise, which is non-iid noise that is temporally correlated, such as the motion of people or cars in the background. We study three commonly used approaches: autoencoding, temporal contrastive learning, and forward modeling. We prove upper bounds for temporal contrastive learning and forward modeling in the presence of onl
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#26032;&#30340;&#22270;&#21516;&#26500;&#27979;&#35797;&#23618;&#27425;&#32467;&#26500;$r$-$\ell{}$WL&#20197;&#21450;&#30456;&#24212;&#30340;GNN&#26694;&#26550;$r$-$\ell{}$MPNN&#65292;&#33021;&#22815;&#35745;&#25968;&#38271;&#24230;&#26368;&#22810;&#20026;$r + 2$&#30340;&#24490;&#29615;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#35745;&#25968;&#33021;&#21147;&#65292;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13749</link><description>&lt;p&gt;
Weisfeiler&#21644;Leman&#21464;&#24471;&#30127;&#29378;&#65306;&#19968;&#31181;&#26032;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#23618;&#27425;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Weisfeiler and Leman Go Loopy: A New Hierarchy for Graph Representational Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13749
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#26032;&#30340;&#22270;&#21516;&#26500;&#27979;&#35797;&#23618;&#27425;&#32467;&#26500;$r$-$\ell{}$WL&#20197;&#21450;&#30456;&#24212;&#30340;GNN&#26694;&#26550;$r$-$\ell{}$MPNN&#65292;&#33021;&#22815;&#35745;&#25968;&#38271;&#24230;&#26368;&#22810;&#20026;$r + 2$&#30340;&#24490;&#29615;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#35745;&#25968;&#33021;&#21147;&#65292;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;$r$-loopy Weisfeiler-Leman ($r$-$\ell{}$WL)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#21516;&#26500;&#27979;&#35797;&#23618;&#27425;&#32467;&#26500;&#20197;&#21450;&#30456;&#24212;&#30340;GNN&#26694;&#26550;$r$-$\ell{}$MPNN&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#35745;&#25968;&#38271;&#24230;&#26368;&#22810;&#20026;$r + 2$&#30340;&#24490;&#29615;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;$r$-$\ell{}$WL&#21487;&#20197;&#35745;&#25968;&#20185;&#20154;&#25484;&#22270;&#30340;&#21516;&#24577;&#12290;&#36825;&#20005;&#26684;&#22320;&#25193;&#23637;&#20102;&#32463;&#20856;&#30340;1-WL&#65292;&#21518;&#32773;&#21482;&#33021;&#35745;&#25968;&#26641;&#30340;&#21516;&#24577;&#65292;&#23454;&#38469;&#19978;&#19982;&#20219;&#24847;&#22266;&#23450;&#30340;$k$-WL&#26159;&#19981;&#21487;&#27604;&#30340;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#32463;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;$r$-$\ell{}$MPNN&#30340;&#34920;&#36798;&#21644;&#35745;&#25968;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/RPaolino/loopy&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13749v1 Announce Type: new  Abstract: We introduce $r$-loopy Weisfeiler-Leman ($r$-$\ell{}$WL), a novel hierarchy of graph isomorphism tests and a corresponding GNN framework, $r$-$\ell{}$MPNN, that can count cycles up to length $r + 2$. Most notably, we show that $r$-$\ell{}$WL can count homomorphisms of cactus graphs. This strictly extends classical 1-WL, which can only count homomorphisms of trees and, in fact, is incomparable to $k$-WL for any fixed $k$. We empirically validate the expressive and counting power of the proposed $r$-$\ell{}$MPNN on several synthetic datasets and present state-of-the-art predictive performance on various real-world datasets. The code is available at https://github.com/RPaolino/loopy
&lt;/p&gt;</description></item><item><title>&#19981;&#21516;&#30340;&#25955;&#24230;&#25490;&#24207;&#21487;&#20197;&#36890;&#36807;&#23427;&#20204;&#30340;&#21464;&#20998;&#36817;&#20284;&#35823;&#20272;&#19981;&#30830;&#23450;&#24615;&#30340;&#21508;&#31181;&#24230;&#37327;&#65292;&#24182;&#19988;&#22240;&#23376;&#21270;&#36817;&#20284;&#26080;&#27861;&#21516;&#26102;&#21305;&#37197;&#36825;&#20123;&#24230;&#37327;&#20013;&#30340;&#20219;&#24847;&#20004;&#20010;</title><link>https://arxiv.org/abs/2403.13748</link><description>&lt;p&gt;
&#21464;&#20998;&#25512;&#26029;&#20013;&#22240;&#23376;&#21270;&#39640;&#26031;&#36817;&#20284;&#30340;&#24046;&#24322;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
An Ordering of Divergences for Variational Inference with Factorized Gaussian Approximations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13748
&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#30340;&#25955;&#24230;&#25490;&#24207;&#21487;&#20197;&#36890;&#36807;&#23427;&#20204;&#30340;&#21464;&#20998;&#36817;&#20284;&#35823;&#20272;&#19981;&#30830;&#23450;&#24615;&#30340;&#21508;&#31181;&#24230;&#37327;&#65292;&#24182;&#19988;&#22240;&#23376;&#21270;&#36817;&#20284;&#26080;&#27861;&#21516;&#26102;&#21305;&#37197;&#36825;&#20123;&#24230;&#37327;&#20013;&#30340;&#20219;&#24847;&#20004;&#20010;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21464;&#20998;&#25512;&#26029;&#65288;VI&#65289;&#20013;&#65292;&#32473;&#23450;&#19968;&#20010;&#38590;&#20197;&#22788;&#29702;&#30340;&#20998;&#24067;$p$&#65292;&#38382;&#39064;&#26159;&#20174;&#19968;&#20123;&#26356;&#26131;&#22788;&#29702;&#30340;&#26063;$\mathcal{Q}$&#20013;&#35745;&#31639;&#26368;&#20339;&#36817;&#20284;$q$&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#36817;&#20284;&#26159;&#36890;&#36807;&#26368;&#23567;&#21270;Kullback-Leibler (KL)&#25955;&#24230;&#26469;&#25214;&#21040;&#30340;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#20854;&#20182;&#26377;&#25928;&#30340;&#25955;&#24230;&#36873;&#25321;&#65292;&#24403;$\mathcal{Q}$&#19981;&#21253;&#21547;$p$&#26102;&#65292;&#27599;&#20010;&#25955;&#24230;&#37117;&#25903;&#25345;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#39640;&#26031;&#30340;&#23494;&#38598;&#21327;&#26041;&#24046;&#30697;&#38453;&#34987;&#23545;&#35282;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#39640;&#26031;&#36817;&#20284;&#25152;&#24433;&#21709;&#30340;VI&#32467;&#26524;&#20013;&#65292;&#25955;&#24230;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;VI&#32467;&#26524;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#21516;&#30340;&#25955;&#24230;&#21487;&#20197;&#36890;&#36807;&#23427;&#20204;&#30340;&#21464;&#20998;&#36817;&#20284;&#35823;&#20272;&#19981;&#30830;&#23450;&#24615;&#30340;&#21508;&#31181;&#24230;&#37327;&#65292;&#22914;&#26041;&#24046;&#12289;&#31934;&#24230;&#21644;&#29109;&#65292;&#36827;&#34892;\textit{&#25490;&#24207;}&#12290;&#25105;&#20204;&#36824;&#24471;&#20986;&#19968;&#20010;&#19981;&#21487;&#33021;&#23450;&#29702;&#65292;&#34920;&#26126;&#26080;&#27861;&#36890;&#36807;&#22240;&#23376;&#21270;&#36817;&#20284;&#21516;&#26102;&#21305;&#37197;&#36825;&#20123;&#24230;&#37327;&#20013;&#30340;&#20219;&#24847;&#20004;&#20010;&#65307;&#22240;&#27492;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13748v1 Announce Type: cross  Abstract: Given an intractable distribution $p$, the problem of variational inference (VI) is to compute the best approximation $q$ from some more tractable family $\mathcal{Q}$. Most commonly the approximation is found by minimizing a Kullback-Leibler (KL) divergence. However, there exist other valid choices of divergences, and when $\mathcal{Q}$ does not contain~$p$, each divergence champions a different solution. We analyze how the choice of divergence affects the outcome of VI when a Gaussian with a dense covariance matrix is approximated by a Gaussian with a diagonal covariance matrix. In this setting we show that different divergences can be \textit{ordered} by the amount that their variational approximations misestimate various measures of uncertainty, such as the variance, precision, and entropy. We also derive an impossibility theorem showing that no two of these measures can be simultaneously matched by a factorized approximation; henc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#27010;&#29575;&#33258;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#65288;Prob-PSENN&#65289;&#65292;&#36890;&#36807;&#27010;&#29575;&#20998;&#24067;&#21462;&#20195;&#28857;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#26356;&#28789;&#27963;&#30340;&#21407;&#22411;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.13740</link><description>&lt;p&gt;
&#36890;&#36807;&#27010;&#29575;&#33258;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#35748;&#30693;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Explanations Through Probabilistic Self-Explainable Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#27010;&#29575;&#33258;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#65288;Prob-PSENN&#65289;&#65292;&#36890;&#36807;&#27010;&#29575;&#20998;&#24067;&#21462;&#20195;&#28857;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#26356;&#28789;&#27963;&#30340;&#21407;&#22411;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#36879;&#26126;&#24615;&#25345;&#32493;&#38480;&#21046;&#20854;&#21487;&#38752;&#24615;&#21644;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#27010;&#29575;&#33258;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#65288;Prob-PSENN&#65289;&#65292;&#37319;&#29992;&#27010;&#29575;&#20998;&#24067;&#20195;&#26367;&#21407;&#22411;&#30340;&#28857;&#20272;&#35745;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#28789;&#27963;&#30340;&#21407;&#22411;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13740v1 Announce Type: new  Abstract: The lack of transparency of Deep Neural Networks continues to be a limitation that severely undermines their reliability and usage in high-stakes applications. Promising approaches to overcome such limitations are Prototype-Based Self-Explainable Neural Networks (PSENNs), whose predictions rely on the similarity between the input at hand and a set of prototypical representations of the output classes, offering therefore a deep, yet transparent-by-design, architecture. So far, such models have been designed by considering pointwise estimates for the prototypes, which remain fixed after the learning phase of the model. In this paper, we introduce a probabilistic reformulation of PSENNs, called Prob-PSENN, which replaces point estimates for the prototypes with probability distributions over their values. This provides not only a more flexible framework for an end-to-end learning of prototypes, but can also capture the explanatory uncertaint
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22797;&#21046;&#21644;&#25299;&#23637;&#20102;&#26368;&#36817;&#19968;&#39033;&#30740;&#31350;&#65292;&#21457;&#29616;&#24378;&#21270;&#23398;&#20064;&#22312;&#22312;&#32447;&#27979;&#35797;&#20013;&#24182;&#19981;&#27604;&#32431;&#38543;&#26426;&#27979;&#35797;&#29983;&#25104;&#20248;&#31168;&#65292;&#25299;&#23637;&#24037;&#20316;&#26088;&#22312;&#28040;&#38500;&#36896;&#25104;&#24615;&#33021;&#19981;&#20339;&#30340;&#21487;&#33021;&#21407;&#22240;&#12290;</title><link>https://arxiv.org/abs/2403.13729</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#22312;&#32447;&#27979;&#35797;&#65306;&#19968;&#39033;&#22797;&#21046;&#21644;&#25299;&#23637;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Online Testing of Autonomous Driving Systems: a Replication and Extension Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13729
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22797;&#21046;&#21644;&#25299;&#23637;&#20102;&#26368;&#36817;&#19968;&#39033;&#30740;&#31350;&#65292;&#21457;&#29616;&#24378;&#21270;&#23398;&#20064;&#22312;&#22312;&#32447;&#27979;&#35797;&#20013;&#24182;&#19981;&#27604;&#32431;&#38543;&#26426;&#27979;&#35797;&#29983;&#25104;&#20248;&#31168;&#65292;&#25299;&#23637;&#24037;&#20316;&#26088;&#22312;&#28040;&#38500;&#36896;&#25104;&#24615;&#33021;&#19981;&#20339;&#30340;&#21487;&#33021;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#19968;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#32467;&#21512;&#22810;&#30446;&#26631;&#25628;&#32034;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;Deep Neural Network-enabled&#31995;&#32479;&#30340;&#22312;&#32447;&#27979;&#35797;&#20013;&#20248;&#20110;&#38543;&#26426;&#25628;&#32034;&#21644;&#22810;&#30446;&#26631;&#25628;&#32034;&#31561;&#26367;&#20195;&#25216;&#26415;&#12290;&#36825;&#20123;&#25216;&#26415;&#30340;&#32463;&#39564;&#35780;&#20272;&#26159;&#22312;&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#65288;ADS&#65289;&#19978;&#36827;&#34892;&#30340;&#12290;&#35813;&#24037;&#20316;&#26159;&#23545;&#35813;&#32463;&#39564;&#30740;&#31350;&#30340;&#22797;&#21046;&#21644;&#25299;&#23637;&#12290;&#25105;&#20204;&#30340;&#22797;&#21046;&#34920;&#26126;&#65292;&#22312;&#19982;&#21407;&#30740;&#31350;&#30456;&#21516;&#35774;&#32622;&#19979;&#36827;&#34892;&#30340;&#23545;&#27604;&#20013;&#65292;RL&#24182;&#19981;&#27604;&#32431;&#38543;&#26426;&#27979;&#35797;&#29983;&#25104;&#20248;&#31168;&#65292;&#20294;&#27809;&#26377;&#30896;&#25758;&#27979;&#37327;&#26041;&#24335;&#24102;&#26469;&#30340;&#28151;&#26434;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#25299;&#23637;&#26088;&#22312;&#28040;&#38500;&#36896;&#25104;RL&#24615;&#33021;&#19981;&#20339;&#30340;&#19968;&#20123;&#21487;&#33021;&#21407;&#22240;&#65306;&#65288;1&#65289;&#22870;&#21169;&#32452;&#20214;&#25552;&#20379;&#20102;&#19982;RL&#20195;&#29702;&#30456;&#30683;&#30462;&#25110;&#26080;&#29992;&#30340;&#21453;&#39304;&#65307;&#65288;2&#65289;&#20351;&#29992;&#20102;&#38656;&#35201;&#31163;&#25955;&#21270;&#30340;RL&#31639;&#27861;&#65288;Q-learning&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13729v1 Announce Type: cross  Abstract: In a recent study, Reinforcement Learning (RL) used in combination with many-objective search, has been shown to outperform alternative techniques (random search and many-objective search) for online testing of Deep Neural Network-enabled systems. The empirical evaluation of these techniques was conducted on a state-of-the-art Autonomous Driving System (ADS). This work is a replication and extension of that empirical study. Our replication shows that RL does not outperform pure random test generation in a comparison conducted under the same settings of the original study, but with no confounding factor coming from the way collisions are measured. Our extension aims at eliminating some of the possible reasons for the poor performance of RL observed in our replication: (1) the presence of reward components providing contrasting or useless feedback to the RL agent; (2) the usage of an RL algorithm (Q-learning) which requires discretizatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#20998;&#23618;&#36755;&#20986;&#21453;&#39304;&#20248;&#21270;&#30340;&#26041;&#24335;&#65292;&#21033;&#29992;&#20056;&#23376;&#35825;&#23548;&#30340;&#25439;&#22833;&#26223;&#35266;&#35843;&#24230;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#22797;&#26434;&#25439;&#22833;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13728</link><description>&lt;p&gt;
M-HOF-Opt: &#22810;&#30446;&#26631;&#20998;&#23618;&#36755;&#20986;&#21453;&#39304;&#20248;&#21270;&#65306;&#22522;&#20110;&#20056;&#23376;&#35825;&#23548;&#25439;&#22833;&#26223;&#35266;&#35843;&#24230;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
M-HOF-Opt: Multi-Objective Hierarchical Output Feedback Optimization via Multiplier Induced Loss Landscape Scheduling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13728
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#20998;&#23618;&#36755;&#20986;&#21453;&#39304;&#20248;&#21270;&#30340;&#26041;&#24335;&#65292;&#21033;&#29992;&#20056;&#23376;&#35825;&#23548;&#30340;&#25439;&#22833;&#26223;&#35266;&#35843;&#24230;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#22797;&#26434;&#25439;&#22833;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#25439;&#22833;&#20989;&#25968;&#30001;&#35768;&#22810;&#39033;&#32452;&#25104;&#26102;&#65292;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#23545;&#26435;&#37325;&#20056;&#23376;&#30340;&#32452;&#21512;&#36873;&#25321;&#24418;&#25104;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#22270;&#27169;&#22411;&#65288;PGM&#65289;&#65292;&#29992;&#20110;&#32852;&#21512;&#27169;&#22411;&#21442;&#25968;&#21644;&#20056;&#23376;&#28436;&#21270;&#36807;&#31243;&#65292;&#20855;&#26377;&#22522;&#20110;&#36229;&#20307;&#31215;&#30340;&#20284;&#28982;&#65292;&#20419;&#36827;&#27599;&#20010;&#25439;&#22833;&#39033;&#30340;&#22810;&#30446;&#26631;&#19979;&#38477;&#12290;&#30456;&#24212;&#30340;&#21442;&#25968;&#21644;&#20056;&#23376;&#20272;&#35745;&#20316;&#20026;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#34987;&#36716;&#21270;&#20026;&#19968;&#20010;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#20013;&#22810;&#30446;&#26631;&#19979;&#38477;&#30446;&#26631;&#34987;&#20998;&#23618;&#22320;&#20998;&#27966;&#21040;&#19968;&#31995;&#21015;&#32422;&#26463;&#20248;&#21270;&#23376;&#38382;&#39064;&#20013;&#12290;&#23376;&#38382;&#39064;&#32422;&#26463;&#26681;&#25454;&#24085;&#32047;&#25176;&#25903;&#37197;&#33258;&#21160;&#36866;&#24212;&#24182;&#20316;&#20026;&#20302;&#23618;&#20056;&#23376;&#25511;&#21046;&#22120;&#35843;&#24230;&#25439;&#22833;&#26223;&#35266;&#30340;&#35774;&#23450;&#28857;&#65292;&#36890;&#36807;&#27599;&#20010;&#25439;&#22833;&#39033;&#30340;&#36755;&#20986;&#21453;&#39304;&#26469;&#36816;&#34892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26080;&#20056;&#23376;&#30340;&#65292;&#24182;&#19988;&#22312;&#26102;&#20195;&#23610;&#24230;&#19978;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13728v1 Announce Type: new  Abstract: When a neural network parameterized loss function consists of many terms, the combinatorial choice of weight multipliers during the optimization process forms a challenging problem. To address this, we proposed a probabilistic graphical model (PGM) for the joint model parameter and multiplier evolution process, with a hypervolume based likelihood that promotes multi-objective descent of each loss term. The corresponding parameter and multiplier estimation as a sequential decision process is then cast into an optimal control problem, where the multi-objective descent goal is dispatched hierarchically into a series of constraint optimization sub-problems. The sub-problem constraint automatically adapts itself according to Pareto dominance and serves as the setpoint for the low level multiplier controller to schedule loss landscapes via output feedback of each loss term. Our method is multiplier-free and operates at the timescale of epochs,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#24314;&#27169;&#30340;&#21160;&#24577;&#31995;&#32479;&#27010;&#29575;&#39044;&#27979;&#26694;&#26550;&#65292;&#21033;&#29992;&#38543;&#26426;&#25554;&#20540;&#22120;&#26500;&#24314;&#34394;&#26500;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#65292;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#26080;&#20559;&#35265;&#22320;&#29983;&#25104;&#26410;&#26469;&#31995;&#32479;&#29366;&#24577;&#30340;&#27010;&#29575;&#24615;&#39044;&#27979;&#38598;&#21512;</title><link>https://arxiv.org/abs/2403.13724</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#25554;&#20540;&#22120;&#21644;F\"ollmer&#36807;&#31243;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Forecasting with Stochastic Interpolants and F\"ollmer Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13724
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#24314;&#27169;&#30340;&#21160;&#24577;&#31995;&#32479;&#27010;&#29575;&#39044;&#27979;&#26694;&#26550;&#65292;&#21033;&#29992;&#38543;&#26426;&#25554;&#20540;&#22120;&#26500;&#24314;&#34394;&#26500;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#65292;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#26080;&#20559;&#35265;&#22320;&#29983;&#25104;&#26410;&#26469;&#31995;&#32479;&#29366;&#24577;&#30340;&#27010;&#29575;&#24615;&#39044;&#27979;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#24314;&#27169;&#30340;&#21160;&#24577;&#31995;&#32479;&#27010;&#29575;&#39044;&#27979;&#26694;&#26550;&#12290;&#22312;&#32473;&#23450;&#31995;&#32479;&#38543;&#26102;&#38388;&#30340;&#29366;&#24577;&#35266;&#27979;&#20043;&#21518;&#65292;&#25105;&#20204;&#23558;&#39044;&#27979;&#38382;&#39064;&#26500;&#24314;&#20026;&#20174;&#32473;&#23450;&#24403;&#21069;&#29366;&#24577;&#30340;&#26465;&#20214;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#24471;&#21040;&#26410;&#26469;&#31995;&#32479;&#29366;&#24577;&#30340;&#20998;&#24067;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#38543;&#26426;&#25554;&#20540;&#22120;&#30340;&#26694;&#26550;&#65292;&#36825;&#26377;&#21161;&#20110;&#26500;&#24314;&#22312;&#20219;&#24847;&#22522;&#30784;&#20998;&#24067;&#21644;&#30446;&#26631;&#20043;&#38388;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#34394;&#26500;&#30340;&#12289;&#38750;&#29289;&#29702;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#65292;&#20854;&#20197;&#24403;&#21069;&#31995;&#32479;&#29366;&#24577;&#20316;&#20026;&#21021;&#22987;&#26465;&#20214;&#65292;&#24182;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#26080;&#20559;&#35265;&#22320;&#29983;&#25104;&#19968;&#20010;&#26469;&#33258;&#30446;&#26631;&#26465;&#20214;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;&#36825;&#20010;&#36807;&#31243;&#23558;&#20197;&#24403;&#21069;&#29366;&#24577;&#20026;&#20013;&#24515;&#30340;&#28857;&#29366;&#36136;&#37327;&#26144;&#23556;&#21040;&#19968;&#20010;&#27010;&#29575;&#24615;&#30340;&#39044;&#27979;&#38598;&#21512;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23454;&#29616;&#36825;&#19968;&#20219;&#21153;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;(SDE)&#20013;&#30340;&#28418;&#31227;&#31995;&#25968;&#26159;&#38750;&#22855;&#24322;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13724v1 Announce Type: new  Abstract: We propose a framework for probabilistic forecasting of dynamical systems based on generative modeling. Given observations of the system state over time, we formulate the forecasting problem as sampling from the conditional distribution of the future system state given its current state. To this end, we leverage the framework of stochastic interpolants, which facilitates the construction of a generative model between an arbitrary base distribution and the target. We design a fictitious, non-physical stochastic dynamics that takes as initial condition the current system state and produces as output a sample from the target conditional distribution in finite time and without bias. This process therefore maps a point mass centered at the current state onto a probabilistic ensemble of forecasts. We prove that the drift coefficient entering the stochastic differential equation (SDE) achieving this task is non-singular, and that it can be lear
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38544;&#24335;&#26174;&#24335;(IMEX)&#26102;&#38388;&#27493;&#36827;&#26041;&#27861;&#25913;&#36827;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;&#65288;ADAM&#65289;&#38543;&#26426;&#20248;&#21270;&#22120;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20248;&#21270;&#31639;&#27861;&#65292;&#27604;&#32463;&#20856;Adam&#22312;&#20960;&#20010;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.13704</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#24335;&#26174;&#24335;(IMEX)&#26102;&#38388;&#27493;&#36827;&#26041;&#27861;&#25913;&#36827;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;&#65288;ADAM&#65289;&#38543;&#26426;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Improving the Adaptive Moment Estimation (ADAM) stochastic optimizer through an Implicit-Explicit (IMEX) time-stepping approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13704
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38544;&#24335;&#26174;&#24335;(IMEX)&#26102;&#38388;&#27493;&#36827;&#26041;&#27861;&#25913;&#36827;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;&#65288;ADAM&#65289;&#38543;&#26426;&#20248;&#21270;&#22120;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20248;&#21270;&#31639;&#27861;&#65292;&#27604;&#32463;&#20856;Adam&#22312;&#20960;&#20010;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Adam&#20248;&#21270;&#22120;&#36890;&#24120;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#65292;&#23545;&#24212;&#20110;&#22312;&#38750;&#24120;&#23567;&#30340;&#23398;&#20064;&#36895;&#29575;&#38480;&#21046;&#19979;&#30340;&#22522;&#26412;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#32463;&#20856;Adam&#31639;&#27861;&#26159;&#24213;&#23618;ODE&#30340;&#19968;&#38454;&#38544;&#24335;&#26174;&#24335;(IMEX) Euler&#31163;&#25955;&#21270;&#12290;&#20174;&#26102;&#38388;&#31163;&#25955;&#21270;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#20351;&#29992;&#26356;&#39640;&#38454;IMEX&#26041;&#27861;&#26469;&#35299;&#20915;ODE&#30340;Adam&#26041;&#26696;&#30340;&#26032;&#25193;&#23637;&#12290;&#22522;&#20110;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20248;&#21270;&#31639;&#27861;&#65292;&#22312;&#20960;&#20010;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#19978;&#27604;&#32463;&#20856;Adam&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13704v1 Announce Type: cross  Abstract: The Adam optimizer, often used in Machine Learning for neural network training, corresponds to an underlying ordinary differential equation (ODE) in the limit of very small learning rates. This work shows that the classical Adam algorithm is a first order implicit-explicit (IMEX) Euler discretization of the underlying ODE. Employing the time discretization point of view, we propose new extensions of the Adam scheme obtained by using higher order IMEX methods to solve the ODE. Based on this approach, we derive a new optimization algorithm for neural network training that performs better than classical Adam on several regression and classification problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#22522;&#20110;&#35270;&#35273;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;&#36827;&#34892;&#27963;&#21160;&#32441;&#29702;&#35782;&#21035;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#36890;&#36807;&#23454;&#29616;&#20449;&#24687;&#35770;&#25506;&#32034;&#31574;&#30053;&#24182;&#36827;&#34892;&#28040;&#34701;&#30740;&#31350;&#21644;&#20154;&#31867;&#23454;&#39564;&#65292;&#21457;&#29616;&#20027;&#21160;&#37319;&#26679;&#31574;&#30053;&#23545;&#35782;&#21035;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>https://arxiv.org/abs/2403.13701</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#35270;&#35273;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;&#36827;&#34892;&#27963;&#21160;&#32441;&#29702;&#35782;&#21035;&#30340;&#20851;&#38190;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
What Matters for Active Texture Recognition With Vision-Based Tactile Sensors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#22522;&#20110;&#35270;&#35273;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;&#36827;&#34892;&#27963;&#21160;&#32441;&#29702;&#35782;&#21035;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#36890;&#36807;&#23454;&#29616;&#20449;&#24687;&#35770;&#25506;&#32034;&#31574;&#30053;&#24182;&#36827;&#34892;&#28040;&#34701;&#30740;&#31350;&#21644;&#20154;&#31867;&#23454;&#39564;&#65292;&#21457;&#29616;&#20027;&#21160;&#37319;&#26679;&#31574;&#30053;&#23545;&#35782;&#21035;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#22522;&#20110;&#35270;&#35273;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;&#36827;&#34892;&#26426;&#22120;&#20154;&#24863;&#30693;&#21644;&#20998;&#31867;&#32455;&#29289;&#32441;&#29702;&#30340;&#27963;&#21160;&#20256;&#24863;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#35302;&#35273;&#32455;&#29289;&#35782;&#21035;&#30340;&#32972;&#26223;&#19979;&#24418;&#24335;&#21270;&#20102;&#20027;&#21160;&#37319;&#26679;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#26368;&#23567;&#21270;&#39044;&#27979;&#29109;&#21644;&#27010;&#29575;&#27169;&#22411;&#26041;&#24046;&#30340;&#20449;&#24687;&#35770;&#25506;&#32034;&#31574;&#30053;&#30340;&#23454;&#29616;&#12290;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#21644;&#20154;&#31867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#21738;&#20123;&#32452;&#20214;&#23545;&#20110;&#24555;&#36895;&#21644;&#21487;&#38752;&#30340;&#32441;&#29702;&#35782;&#21035;&#33267;&#20851;&#37325;&#35201;&#12290;&#38500;&#20102;&#27963;&#21160;&#37319;&#26679;&#31574;&#30053;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12289;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12289;&#25968;&#25454;&#22686;&#24378;&#30340;&#24433;&#21709;&#20197;&#21450;&#25968;&#25454;&#38598;&#21464;&#24322;&#24615;&#12290;&#36890;&#36807;&#22312;&#20808;&#21069;&#21457;&#24067;&#30340;&#20027;&#21160;&#26381;&#35013;&#24863;&#30693;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#31995;&#32479;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30830;&#23450;&#20027;&#21160;&#25506;&#32034;&#31574;&#30053;&#30340;&#36873;&#25321;&#23545;&#35782;&#21035;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#20165;&#24456;&#23567;&#65292;&#32780;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13701v1 Announce Type: cross  Abstract: This paper explores active sensing strategies that employ vision-based tactile sensors for robotic perception and classification of fabric textures. We formalize the active sampling problem in the context of tactile fabric recognition and provide an implementation of information-theoretic exploration strategies based on minimizing predictive entropy and variance of probabilistic models. Through ablation studies and human experiments, we investigate which components are crucial for quick and reliable texture recognition. Along with the active sampling strategies, we evaluate neural network architectures, representations of uncertainty, influence of data augmentation, and dataset variability. By evaluating our method on a previously published Active Clothing Perception Dataset and on a real robotic system, we establish that the choice of the active exploration strategy has only a minor influence on the recognition accuracy, whereas data 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#22522;&#20110;&#20174;&#27493;&#34892;&#26426;&#22120;&#20154;&#25910;&#38598;&#30340;&#23454;&#26102;&#21487;&#21464;&#38271;&#24230;&#24863;&#30693;&#25968;&#25454;&#36827;&#34892;&#22320;&#24418;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2403.13695</link><description>&lt;p&gt;
&#20002;&#22833;&#27491;&#21017;&#21270;&#30340;&#26426;&#22120;&#20154;&#22320;&#24418;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Loss Regularizing Robotic Terrain Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#22522;&#20110;&#20174;&#27493;&#34892;&#26426;&#22120;&#20154;&#25910;&#38598;&#30340;&#23454;&#26102;&#21487;&#21464;&#38271;&#24230;&#24863;&#30693;&#25968;&#25454;&#36827;&#34892;&#22320;&#24418;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#36234;&#38590;&#30340;&#22320;&#24418;&#26102;&#27493;&#34892;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#21147;&#23398;&#26159;&#21512;&#36866;&#30340;&#12290;&#35782;&#21035;&#36825;&#20123;&#22320;&#24418;&#23545;&#20110;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#30340;&#21160;&#20316;&#30340;&#22810;&#21151;&#33021;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#26426;&#22120;&#20154;&#22320;&#24418;&#20998;&#31867;&#21464;&#24471;&#37325;&#35201;&#36215;&#26469;&#65292;&#20197;&#23454;&#26102;&#39640;&#20934;&#30830;&#24615;&#22320;&#23545;&#22320;&#24418;&#36827;&#34892;&#20998;&#31867;&#12290; Existing recurrent architectures are still evolving to improve accuracy of terrain classification based on live variable-length sensory data collected from legged robots. &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#36991;&#20813;&#39044;&#22788;&#29702;&#38271;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13695v1 Announce Type: cross  Abstract: Locomotion mechanics of legged robots are suitable when pacing through difficult terrains. Recognising terrains for such robots are important to fully yoke the versatility of their movements. Consequently, robotic terrain classification becomes significant to classify terrains in real time with high accuracy. The conventional classifiers suffer from overfitting problem, low accuracy problem, high variance problem, and not suitable for live dataset. On the other hand, classifying a growing dataset is difficult for convolution based terrain classification. Supervised recurrent models are also not practical for this classification. Further, the existing recurrent architectures are still evolving to improve accuracy of terrain classification based on live variable-length sensory data collected from legged robots. This paper proposes a new semi-supervised method for terrain classification of legged robots, avoiding preprocessing of long var
&lt;/p&gt;</description></item><item><title>PARAMANU-AYN&#26159;&#19968;&#31181;&#22522;&#20110;&#21360;&#24230;&#27861;&#24459;&#26696;&#20363;&#25991;&#20214;&#30340;&#39640;&#25928;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#32463;&#36807;&#38754;&#21521;&#25351;&#20196;&#30340;&#24494;&#35843;&#65292;&#22312;&#21508;&#31181;&#27861;&#24459;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.13681</link><description>&lt;p&gt;
PARAMANU-AYN&#65306;&#19968;&#31181;&#26377;&#25928;&#30340;&#26032;&#22411;&#29983;&#25104;&#24335;&#12289;&#38754;&#21521;&#21360;&#24230;&#27861;&#24459;&#26696;&#20363;&#25991;&#20214;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PARAMANU-AYN: An Efficient Novel Generative and Instruction-tuned Language Model for Indian Legal Case Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13681
&lt;/p&gt;
&lt;p&gt;
PARAMANU-AYN&#26159;&#19968;&#31181;&#22522;&#20110;&#21360;&#24230;&#27861;&#24459;&#26696;&#20363;&#25991;&#20214;&#30340;&#39640;&#25928;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#32463;&#36807;&#38754;&#21521;&#25351;&#20196;&#30340;&#24494;&#35843;&#65292;&#22312;&#21508;&#31181;&#27861;&#24459;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PARAMANU-AYN&#65292;&#36825;&#26159;&#19968;&#20010;&#20165;&#22522;&#20110;&#21360;&#24230;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#25991;&#20214;&#12289;&#21360;&#24230;&#23466;&#27861;&#21644;&#21360;&#24230;&#21009;&#27861;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#26159;&#20174;&#22836;&#24320;&#22987;&#22312;&#19978;&#19979;&#25991;&#22823;&#23567;&#20026;8192&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#22312;&#22256;&#24785;&#24230;&#25351;&#26631;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#27861;&#24459;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#23545;&#19968;&#32452;&#21253;&#25324;&#21508;&#31181;&#27861;&#24459;&#20219;&#21153;&#65288;&#22914;&#27861;&#24459;&#25512;&#29702;&#12289;&#21028;&#20915;&#35299;&#37322;&#12289;&#27861;&#24459;&#26465;&#27454;&#29983;&#25104;&#12289;&#27861;&#24459;&#33609;&#25311;&#12289;&#27861;&#24459;&#21512;&#21516;&#33609;&#25311;&#12289;&#26696;&#20214;&#25688;&#35201;&#12289;&#23466;&#27861;&#38382;&#39064;&#22238;&#31572;&#31561;&#65289;&#30340;10,763&#26465;&#25351;&#20196;&#36827;&#34892;&#20102;&#38024;&#23545;&#24615;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;GPT-3.5-Turbo&#23545;&#38754;&#21521;&#25351;&#20196;&#30340;&#27169;&#22411;&#30340;&#25552;&#31034;&#21709;&#24212;&#36827;&#34892;&#20102;&#22312;10&#20998;&#21046;&#24230;&#19978;&#30340;&#28165;&#26224;&#24230;&#12289;&#30456;&#20851;&#24615;&#12289;&#23436;&#25972;&#24615;&#21644;&#27861;&#24459;&#25512;&#29702;&#25351;&#26631;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;CPU&#19978;&#36816;&#34892;&#65292;&#24182;&#23454;&#29616;&#27599;&#31186;42.46&#20010;&#20196;&#29260;&#30340;CPU&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13681v1 Announce Type: new  Abstract: In this paper, we present PARAMANU-AYN, a language model based exclusively on case documents of the Supreme Court of India, the Constitution of India, and the Indian Penal Code. The novel Auto Regressive (AR) decoder based model is pretrained from scratch at a context size of 8192. We evaluated our pretrained legal model on perplexity metrics. We also instruction-tuned our pretrained model on a set of 10,763 instructions covering various legal tasks such as legal reasoning, judgement explanation, legal clause generation, legal drafting, legal contract drafting, case summarization, constitutional question-answering, etc. We also evaluated the responses of prompts for instruction-tuned models by GPT-3.5-Turbo on clarity, relevance, completeness, and legal reasoning metrics in a scale of 10. Our model can be run on CPU and achieved 42.46 tokens/sec CPU inference speed. We found that our models, despite not being pretrained on legal books, v
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;MESHFREE&#36719;&#20214;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;ML&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#12289;&#22238;&#24402;&#26641;&#21644;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#22312;&#22797;&#26434;&#27969;&#21160;&#39046;&#22495;&#20013;&#23454;&#29616;&#21442;&#25968;&#30340;&#33258;&#21160;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2403.13672</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#30340;&#26041;&#27861;&#29992;&#20110;MESHFREE&#27169;&#25311;&#20013;&#30340;&#21442;&#25968;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Optimized Approach for Parameter Selection in MESHFREE Simulations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13672
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;MESHFREE&#36719;&#20214;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;ML&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#12289;&#22238;&#24402;&#26641;&#21644;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#22312;&#22797;&#26434;&#27969;&#21160;&#39046;&#22495;&#20013;&#23454;&#29616;&#21442;&#25968;&#30340;&#33258;&#21160;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Meshfree&#27169;&#25311;&#26041;&#27861;&#27491;&#25104;&#20026;&#20256;&#32479;&#22522;&#20110;&#32593;&#26684;&#26041;&#27861;&#30340;&#24341;&#20154;&#27880;&#30446;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#29305;&#21035;&#22312;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#65288;CFD&#65289;&#21644;&#36830;&#32493;&#20171;&#36136;&#21147;&#23398;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#30740;&#31350;&#27010;&#36848;&#65292;&#32467;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;Fraunhofer&#30340;MESHFREE&#36719;&#20214;&#65288;www.meshfree.eu&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#25968;&#20540;&#28857;&#20113;&#36827;&#34892;&#24191;&#20041;&#26377;&#38480;&#24046;&#20998;&#27861;&#65288;GFDM&#65289;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#36825;&#20010;&#24037;&#20855;&#33021;&#26377;&#25928;&#22788;&#29702;&#22797;&#26434;&#30340;&#27969;&#21160;&#39046;&#22495;&#12289;&#31227;&#21160;&#20960;&#20309;&#24418;&#29366;&#21644;&#33258;&#30001;&#34920;&#38754;&#65292;&#21516;&#26102;&#20801;&#35768;&#29992;&#25143;&#31934;&#32454;&#35843;&#25972;&#26412;&#22320;&#32454;&#21270;&#21644;&#36136;&#37327;&#21442;&#25968;&#65292;&#20197;&#23454;&#29616;&#35745;&#31639;&#26102;&#38388;&#21644;&#32467;&#26524;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26368;&#20339;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#30830;&#23450;&#26368;&#20339;&#21442;&#25968;&#32452;&#21512;&#23545;&#20110;&#32463;&#39564;&#36739;&#23569;&#30340;&#29992;&#25143;&#32780;&#35328;&#23384;&#22312;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;ML&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#20027;&#21160;&#23398;&#20064;&#12289;&#22238;&#24402;&#26641;&#21644;&#21487;&#35270;&#21270;&#22312;MESHFREE&#27169;&#25311;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13672v1 Announce Type: new  Abstract: Meshfree simulation methods are emerging as compelling alternatives to conventional mesh-based approaches, particularly in the fields of Computational Fluid Dynamics (CFD) and continuum mechanics. In this publication, we provide a comprehensive overview of our research combining Machine Learning (ML) and Fraunhofer's MESHFREE software (www.meshfree.eu), a powerful tool utilizing a numerical point cloud in a Generalized Finite Difference Method (GFDM). This tool enables the effective handling of complex flow domains, moving geometries, and free surfaces, while allowing users to finely tune local refinement and quality parameters for an optimal balance between computation time and results accuracy. However, manually determining the optimal parameter combination poses challenges, especially for less experienced users. We introduce a novel ML-optimized approach, using active learning, regression trees, and visualization on MESHFREE simulatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;$\text{CardioVAE}_\text{X,G}$&#65289;&#65292;&#23558;&#20302;&#25104;&#26412;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#25968;&#25454;&#24418;&#24335;&#25972;&#21512;&#36215;&#26469;&#65292;&#24182;&#23454;&#29616;&#20102;&#20849;&#20139;&#29305;&#24449;&#21644;&#29420;&#29305;&#29305;&#24449;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.13658</link><description>&lt;p&gt;
&#29992;&#20110;&#20302;&#25104;&#26412;&#24515;&#33039;&#34880;&#28082;&#21160;&#21147;&#23398;&#19981;&#31283;&#23450;&#24615;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Multimodal Variational Autoencoder for Low-cost Cardiac Hemodynamics Instability Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13658
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;$\text{CardioVAE}_\text{X,G}$&#65289;&#65292;&#23558;&#20302;&#25104;&#26412;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#25968;&#25454;&#24418;&#24335;&#25972;&#21512;&#36215;&#26469;&#65292;&#24182;&#23454;&#29616;&#20102;&#20849;&#20139;&#29305;&#24449;&#21644;&#29420;&#29305;&#29305;&#24449;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#38750;&#20405;&#20837;&#24615;&#26816;&#27979;&#24515;&#33039;&#34880;&#28082;&#21160;&#21147;&#23398;&#19981;&#31283;&#23450;&#24615;&#65288;CHDI&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#21333;&#19968;&#25968;&#25454;&#24418;&#24335;&#65292;&#22914;&#24515;&#33039;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#22312;&#26631;&#35760;&#30340;&#24739;&#32773;&#25968;&#25454;&#37327;&#26377;&#38480;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#25928;&#26524;&#19981;&#20339;&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#30340;&#24120;&#35265;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#26041;&#27861;&#26469;&#30740;&#31350;CHDI&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#26114;&#36149;&#30340;&#25968;&#25454;&#24418;&#24335;&#65292;&#22914;&#24515;&#33039;MRI&#21644;&#24515;&#33039;&#36229;&#22768;&#22270;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;$\text{CardioVAE}_\text{X,G}$&#65289;&#26469;&#25972;&#21512;&#20302;&#25104;&#26412;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#25968;&#25454;&#24418;&#24335;&#65292;&#24182;&#22312;&#22823;&#22411;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;$\text{CardioVAE}_\text{X,G}$&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#27969;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#23398;&#20064;&#20849;&#20139;&#29305;&#24449;&#21644;&#21508;&#25968;&#25454;&#24418;&#24335;&#29420;&#26377;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;fi
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13658v1 Announce Type: new  Abstract: Recent advancements in non-invasive detection of cardiac hemodynamic instability (CHDI) primarily focus on applying machine learning techniques to a single data modality, e.g. cardiac magnetic resonance imaging (MRI). Despite their potential, these approaches often fall short especially when the size of labeled patient data is limited, a common challenge in the medical domain. Furthermore, only a few studies have explored multimodal methods to study CHDI, which mostly rely on costly modalities such as cardiac MRI and echocardiogram. In response to these limitations, we propose a novel multimodal variational autoencoder ($\text{CardioVAE}_\text{X,G}$) to integrate low-cost chest X-ray (CXR) and electrocardiogram (ECG) modalities with pre-training on a large unlabeled dataset. Specifically, $\text{CardioVAE}_\text{X,G}$ introduces a novel tri-stream pre-training strategy to learn both shared and modality-specific features, thus enabling fi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#25104;&#20998;&#30340;&#26799;&#24230;&#20248;&#21270;&#65292;&#20351;&#27169;&#22411;&#19982;&#30446;&#26631;&#24615;&#36136;&#23494;&#20999;&#23545;&#40784;&#65292;&#23454;&#29616;&#39640;&#25928;&#12289;&#24191;&#27867;&#30340;&#25628;&#32034;&#65292;&#24182;&#33021;&#36866;&#24212;&#26032;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#26174;&#33879;&#36827;&#23637;&#20102;&#26448;&#26009;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.13627</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#25104;&#20998;&#35774;&#35745;&#26041;&#27861;&#22312;&#39640;&#20020;&#30028;&#28201;&#24230;&#36229;&#23548;&#20307;&#30340;&#39640;&#25928;&#25506;&#32034;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient exploration of high-Tc superconductors by a gradient-based composition design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13627
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25104;&#20998;&#30340;&#26799;&#24230;&#20248;&#21270;&#65292;&#20351;&#27169;&#22411;&#19982;&#30446;&#26631;&#24615;&#36136;&#23494;&#20999;&#23545;&#40784;&#65292;&#23454;&#29616;&#39640;&#25928;&#12289;&#24191;&#27867;&#30340;&#25628;&#32034;&#65292;&#24182;&#33021;&#36866;&#24212;&#26032;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#26174;&#33879;&#36827;&#23637;&#20102;&#26448;&#26009;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26448;&#26009;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#25104;&#20998;&#30340;&#26799;&#24230;&#20248;&#21270;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65306;&#31351;&#20030;&#25968;&#25454;&#24211;&#25628;&#32034;&#21644;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#20248;&#21270;&#36755;&#20837;&#65292;&#20351;&#27169;&#22411;&#30340;&#36755;&#20986;&#19982;&#30446;&#26631;&#24615;&#36136;&#23494;&#20999;&#23545;&#40784;&#65292;&#26377;&#21161;&#20110;&#21457;&#29616;&#26410;&#21015;&#20986;&#30340;&#26448;&#26009;&#21644;&#31934;&#30830;&#24615;&#36136;&#30830;&#23450;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#33021;&#22815;&#22312;&#26032;&#26465;&#20214;&#19979;&#36827;&#34892;&#33258;&#36866;&#24212;&#20248;&#21270;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#24212;&#29992;&#20110;&#25506;&#32034;&#39640;&#20020;&#30028;&#28201;&#24230;&#36229;&#23548;&#20307;&#26102;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#36229;&#36807;&#29616;&#26377;&#25968;&#25454;&#24211;&#30340;&#28508;&#22312;&#25104;&#20998;&#65292;&#24182;&#36890;&#36807;&#26465;&#20214;&#20248;&#21270;&#21457;&#29616;&#20102;&#26032;&#30340;&#27682;&#36229;&#23548;&#20307;&#12290;&#36825;&#31181;&#26041;&#27861;&#26159;&#22810;&#21151;&#33021;&#30340;&#65292;&#36890;&#36807;&#23454;&#29616;&#39640;&#25928;&#12289;&#24191;&#27867;&#30340;&#25628;&#32034;&#21644;&#23545;&#26032;&#32422;&#26463;&#26465;&#20214;&#30340;&#36866;&#24212;&#24615;&#65292;&#26174;&#30528;&#25512;&#36827;&#20102;&#26448;&#26009;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13627v1 Announce Type: cross  Abstract: We propose a material design method via gradient-based optimization on compositions, overcoming the limitations of traditional methods: exhaustive database searches and conditional generation models. It optimizes inputs via backpropagation, aligning the model's output closely with the target property and facilitating the discovery of unlisted materials and precise property determination. Our method is also capable of adaptive optimization under new conditions without retraining. Applying to exploring high-Tc superconductors, we identified potential compositions beyond existing databases and discovered new hydrogen superconductors via conditional optimization. This method is versatile and significantly advances material design by enabling efficient, extensive searches and adaptability to new constraints.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#23398;&#20064;&#21644;&#22521;&#35757;&#26041;&#27861;&#20197;&#21450;&#21033;&#29992;&#28216;&#25103;&#21270;&#25552;&#39640;&#25216;&#26415;&#36716;&#31227;&#21644;&#25104;&#24180;&#20154;&#23398;&#20064;&#65292;&#26469;&#21152;&#24378;&#26816;&#27979;&#24656;&#24598;&#34701;&#36164;&#30340;&#25191;&#27861;&#22521;&#35757;&#12290;</title><link>https://arxiv.org/abs/2403.13625</link><description>&lt;p&gt;
&#21152;&#24378;&#25191;&#27861;&#22521;&#35757;&#65306;&#19968;&#31181;&#28216;&#25103;&#21270;&#26041;&#27861;&#26469;&#26816;&#27979;&#24656;&#24598;&#34701;&#36164;
&lt;/p&gt;
&lt;p&gt;
Enhancing Law Enforcement Training: A Gamified Approach to Detecting Terrorism Financing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13625
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#23398;&#20064;&#21644;&#22521;&#35757;&#26041;&#27861;&#20197;&#21450;&#21033;&#29992;&#28216;&#25103;&#21270;&#25552;&#39640;&#25216;&#26415;&#36716;&#31227;&#21644;&#25104;&#24180;&#20154;&#23398;&#20064;&#65292;&#26469;&#21152;&#24378;&#26816;&#27979;&#24656;&#24598;&#34701;&#36164;&#30340;&#25191;&#27861;&#22521;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#22825;&#25512;&#24191;&#21644;&#37096;&#32626;&#29992;&#20110;&#25171;&#20987;&#32593;&#32476;&#29359;&#32618;&#27963;&#21160;&#30340;&#26032;&#25216;&#26415;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20855;&#24448;&#24448;&#36807;&#20110;&#22797;&#26434;&#65292;&#38590;&#20197;&#20351;&#29992;&#65292;&#38656;&#35201;&#28145;&#20837;&#30340;&#39046;&#22495;&#30693;&#35782;&#21644;&#25216;&#26415;&#30693;&#35782;&#12290;&#36825;&#20123;&#29305;&#28857;&#24448;&#24448;&#38480;&#21046;&#20102;&#25191;&#27861;&#20154;&#21592;&#21644;&#26368;&#32456;&#29992;&#25143;&#21442;&#19982;&#36825;&#20123;&#25216;&#26415;&#65292;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#20173;&#28982;&#34987;&#35823;&#35299;&#12290;&#20986;&#20110;&#36825;&#20010;&#21407;&#22240;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#32467;&#21512;&#23398;&#20064;&#21644;&#22521;&#35757;&#26041;&#27861;&#20197;&#21450;&#21033;&#29992;&#28216;&#25103;&#21270;&#25552;&#21319;&#25216;&#26415;&#36716;&#31227;&#21644;&#22686;&#21152;&#25104;&#24180;&#20154;&#23398;&#20064;&#30340;&#28508;&#22312;&#22909;&#22788;&#30340;&#32463;&#39564;&#12290;&#20107;&#23454;&#19978;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21442;&#19982;&#32773;&#26159;&#26292;&#38706;&#20110;&#24656;&#24598;&#34701;&#36164;&#65288;&#22914;&#25191;&#27861;&#20154;&#21592;&#12289;&#37329;&#34701;&#35843;&#26597;&#21592;&#12289;&#31169;&#20154;&#20390;&#25506;&#31561;&#65289;&#30340;&#34892;&#19994;&#30340;&#32463;&#39564;&#20016;&#23500;&#30340;&#20174;&#19994;&#20154;&#21592;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19981;&#21516;&#32423;&#21035;&#30340;&#22521;&#35757;&#27963;&#21160;&#65292;&#20197;&#22686;&#21152;&#20851;&#20110;&#26032;&#36235;&#21183;&#21644;&#29359;&#32618;&#20316;&#26696;&#26041;&#24335;&#30340;&#20449;&#24687;&#20132;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13625v1 Announce Type: new  Abstract: Tools for fighting cyber-criminal activities using new technologies are promoted and deployed every day. However, too often, they are unnecessarily complex and hard to use, requiring deep domain and technical knowledge. These characteristics often limit the engagement of law enforcement and end-users in these technologies that, despite their potential, remain misunderstood. For this reason, in this study, we describe our experience in combining learning and training methods and the potential benefits of gamification to enhance technology transfer and increase adult learning. In fact, in this case, participants are experienced practitioners in professions/industries that are exposed to terrorism financing (such as Law Enforcement Officers, Financial Investigation Officers, private investigators, etc.) We define training activities on different levels for increasing the exchange of information about new trends and criminal modus operandi a
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#19978;&#30340;Mann-Whitney U&#26816;&#39564;&#65292;&#20197;&#30830;&#23450;&#22312;&#38544;&#31169;&#20445;&#25252;&#21512;&#25104;&#25968;&#25454;&#19978;&#25191;&#34892;&#30340;&#32479;&#35745;&#20551;&#35774;&#26816;&#39564;&#26159;&#21542;&#21487;&#33021;&#23548;&#33268;&#27979;&#35797;&#26377;&#25928;&#24615;&#30340;&#20007;&#22833;&#25110;&#21151;&#29575;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.13612</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#33021;&#23548;&#33268;&#21512;&#25104;&#21457;&#29616;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Differentially Private Synthetic Data Lead to Synthetic Discoveries?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13612
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#19978;&#30340;Mann-Whitney U&#26816;&#39564;&#65292;&#20197;&#30830;&#23450;&#22312;&#38544;&#31169;&#20445;&#25252;&#21512;&#25104;&#25968;&#25454;&#19978;&#25191;&#34892;&#30340;&#32479;&#35745;&#20551;&#35774;&#26816;&#39564;&#26159;&#21542;&#21487;&#33021;&#23548;&#33268;&#27979;&#35797;&#26377;&#25928;&#24615;&#30340;&#20007;&#22833;&#25110;&#21151;&#29575;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#24050;&#34987;&#25552;&#20986;&#20316;&#20026;&#20849;&#20139;&#25935;&#24863;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#21311;&#21517;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#21512;&#25104;&#25968;&#25454;&#24212;&#20445;&#30041;&#21407;&#22987;&#25968;&#25454;&#30340;&#32467;&#26500;&#21644;&#32479;&#35745;&#29305;&#24615;&#65292;&#21516;&#26102;&#20445;&#25252;&#20010;&#20307;&#20027;&#20307;&#30340;&#38544;&#31169;&#12290;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#30446;&#21069;&#34987;&#35748;&#20026;&#26159;&#24179;&#34913;&#36825;&#31181;&#26435;&#34913;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#35780;&#20272;&#22312;&#24046;&#20998;&#38544;&#31169;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;Mann-Whitney U&#26816;&#39564;&#22312;I&#22411;&#21644;II&#22411;&#38169;&#35823;&#26041;&#38754;&#65292;&#20197;&#30830;&#23450;&#22312;&#38544;&#31169;&#20445;&#25252;&#21512;&#25104;&#25968;&#25454;&#19978;&#25191;&#34892;&#30340;&#32479;&#35745;&#20551;&#35774;&#26816;&#39564;&#26159;&#21542;&#21487;&#33021;&#23548;&#33268;&#27979;&#35797;&#26377;&#25928;&#24615;&#30340;&#20007;&#22833;&#25110;&#21151;&#29575;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13612v1 Announce Type: new  Abstract: Background: Synthetic data has been proposed as a solution for sharing anonymized versions of sensitive biomedical datasets. Ideally, synthetic data should preserve the structure and statistical properties of the original data, while protecting the privacy of the individual subjects. Differential privacy (DP) is currently considered the gold standard approach for balancing this trade-off.   Objectives: The aim of this study is to evaluate the Mann-Whitney U test on DP-synthetic biomedical data in terms of Type I and Type II errors, in order to establish whether statistical hypothesis testing performed on privacy preserving synthetic data is likely to lead to loss of test's validity or decreased power.   Methods: We evaluate the Mann-Whitney U test on DP-synthetic data generated from real-world data, including a prostate cancer dataset (n=500) and a cardiovascular dataset (n=70 000), as well as on data drawn from two Gaussian distribution
&lt;/p&gt;</description></item><item><title>CONLINE&#26694;&#26550;&#25552;&#20986;&#20102;&#36890;&#36807;&#22312;&#32447;&#25628;&#32034;&#21644;&#27491;&#30830;&#24615;&#27979;&#35797;&#26469;&#22686;&#24378;&#22797;&#26434;&#20195;&#30721;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26174;&#33879;&#25552;&#39640;&#20102;&#20195;&#30721;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.13583</link><description>&lt;p&gt;
CONLINE: &#22797;&#26434;&#20195;&#30721;&#29983;&#25104;&#19982;&#22312;&#32447;&#25628;&#32034;&#21644;&#27491;&#30830;&#24615;&#27979;&#35797;&#30340;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
CONLINE: Complex Code Generation and Refinement with Online Searching and Correctness Testing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13583
&lt;/p&gt;
&lt;p&gt;
CONLINE&#26694;&#26550;&#25552;&#20986;&#20102;&#36890;&#36807;&#22312;&#32447;&#25628;&#32034;&#21644;&#27491;&#30830;&#24615;&#27979;&#35797;&#26469;&#22686;&#24378;&#22797;&#26434;&#20195;&#30721;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26174;&#33879;&#25552;&#39640;&#20102;&#20195;&#30721;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#36716;&#25442;&#20026;&#21487;&#25191;&#34892;&#20195;&#30721;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#22330;&#26223;&#19979;&#29983;&#25104;&#22797;&#26434;&#20195;&#30721;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#22312;&#20110;&#22797;&#26434;&#30340;&#32467;&#26500;&#12289;&#24494;&#22937;&#30340;&#38169;&#35823;&#12289;&#23545;&#39640;&#32423;&#25968;&#25454;&#31867;&#22411;&#30340;&#29702;&#35299;&#20197;&#21450;&#32570;&#23569;&#36741;&#21161;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CONLINE&#26694;&#26550;&#65292;&#36890;&#36807;&#35745;&#21010;&#30340;&#22312;&#32447;&#25628;&#32034;&#20449;&#24687;&#26816;&#32034;&#21644;&#33258;&#21160;&#27491;&#30830;&#24615;&#27979;&#35797;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#65292;&#36827;&#34892;&#36845;&#20195;&#31934;&#28860;&#12290;CONLINE&#36824;&#20018;&#34892;&#21270;&#20102;&#22797;&#26434;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#20197;&#25913;&#21892;&#29702;&#35299;&#65292;&#24182;&#29983;&#25104;&#27979;&#35797;&#29992;&#20363;&#65292;&#30830;&#20445;&#26694;&#26550;&#36866;&#29992;&#20110;&#29616;&#23454;&#24212;&#29992;&#12290;CONLINE&#36890;&#36807;&#23545;DS-1000&#21644;ClassEval&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#23454;&#39564;&#39564;&#35777;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;CONLINE&#26174;&#33879;&#25552;&#39640;&#20102;&#22797;&#26434;&#20195;&#30721;&#29983;&#25104;&#30340;&#36136;&#37327;&#65292;&#31361;&#26174;&#20102;&#20854;&#25552;&#21319;&#23454;&#36341;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13583v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have revolutionized code generation ability by converting natural language descriptions into executable code. However, generating complex code within real-world scenarios remains challenging due to intricate structures, subtle bugs, understanding of advanced data types, and lack of supplementary contents. To address these challenges, we introduce the CONLINE framework, which enhances code generation by incorporating planned online searches for information retrieval and automated correctness testing for iterative refinement. CONLINE also serializes the complex inputs and outputs to improve comprehension and generate test case to ensure the framework's adaptability for real-world applications. CONLINE is validated through rigorous experiments on the DS-1000 and ClassEval datasets. It shows that CONLINE substantially improves the quality of complex code generation, highlighting its potential to enhance the pra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#37325;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#22312;&#36741;&#23548;&#21592;&#21453;&#24605;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;&#65292;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#30340;&#36172;&#21338;&#26041;&#27861;DynOpt&#21644;C-DynaOpt&#65292;&#21160;&#24577;&#35843;&#25972;&#22810;&#20010;&#22870;&#21169;&#26435;&#37325;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.13578</link><description>&lt;p&gt;
&#22810;&#37325;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21160;&#24577;&#22870;&#21169;&#35843;&#25972;&#29992;&#20110;&#36741;&#23548;&#21592;&#21453;&#24605;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Dynamic Reward Adjustment in Multi-Reward Reinforcement Learning for Counselor Reflection Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#37325;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#22312;&#36741;&#23548;&#21592;&#21453;&#24605;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;&#65292;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#30340;&#36172;&#21338;&#26041;&#27861;DynOpt&#21644;C-DynaOpt&#65292;&#21160;&#24577;&#35843;&#25972;&#22810;&#20010;&#22870;&#21169;&#26435;&#37325;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#37325;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20197;&#20849;&#21516;&#20248;&#21270;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#22810;&#20010;&#25991;&#26412;&#36136;&#37327;&#12290;&#25105;&#20204;&#20851;&#27880;&#36741;&#23548;&#21592;&#21453;&#24605;&#29983;&#25104;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#20248;&#21270;&#29983;&#25104;&#22120;&#20197;&#21516;&#26102;&#25552;&#39640;&#29983;&#25104;&#30340;&#36741;&#23548;&#21592;&#22238;&#22797;&#30340;&#27969;&#30021;&#24615;&#12289;&#36830;&#36143;&#24615;&#21644;&#21453;&#24605;&#36136;&#37327;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#30340;&#36172;&#21338;&#26041;&#27861;&#65292;DynaOpt&#21644;C-DynaOpt&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#23558;&#22870;&#21169;&#32452;&#21512;&#25104;&#19968;&#20010;&#21333;&#19968;&#20540;&#24182;&#21516;&#26102;&#20248;&#21270;&#23427;&#20204;&#30340;&#24191;&#27867;&#31574;&#30053;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#38750;&#24773;&#22659;&#21644;&#24773;&#22659;&#22810;&#33218;&#36172;&#21338;&#26469;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#22810;&#20010;&#22870;&#21169;&#26435;&#37325;&#12290;&#36890;&#36807;&#33258;&#21160;&#21644;&#25163;&#21160;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;DynaOpt&#21644;C-DynaOpt&#20248;&#20110;&#29616;&#26377;&#30340;&#26420;&#32032;&#21644;&#36172;&#21338;&#22522;&#32447;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13578v1 Announce Type: new  Abstract: In this paper, we study the problem of multi-reward reinforcement learning to jointly optimize for multiple text qualities for natural language generation. We focus on the task of counselor reflection generation, where we optimize the generators to simultaneously improve the fluency, coherence, and reflection quality of generated counselor responses. We introduce two novel bandit methods, DynaOpt and C-DynaOpt, which rely on the broad strategy of combining rewards into a single value and optimizing them simultaneously. Specifically, we employ non-contextual and contextual multi-arm bandits to dynamically adjust multiple reward weights during training. Through automatic and manual evaluations, we show that our proposed techniques, DynaOpt and C-DynaOpt, outperform existing naive and bandit baselines, showcasing their potential for enhancing language models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#22238;&#24402;&#30340;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#21487;&#36801;&#31227;&#32467;&#26500;&#33258;&#36866;&#24212;&#26816;&#27979;&#21644;&#32858;&#21512;&#29305;&#24449;&#21644;&#26679;&#26412;&#30340;&#21487;&#36801;&#31227;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.13565</link><description>&lt;p&gt;
AdaTrans&#65306;&#38024;&#23545;&#39640;&#32500;&#22238;&#24402;&#30340;&#29305;&#24449;&#33258;&#36866;&#24212;&#19982;&#26679;&#26412;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AdaTrans: Feature-wise and Sample-wise Adaptive Transfer Learning for High-dimensional Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13565
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#22238;&#24402;&#30340;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#21487;&#36801;&#31227;&#32467;&#26500;&#33258;&#36866;&#24212;&#26816;&#27979;&#21644;&#32858;&#21512;&#29305;&#24449;&#21644;&#26679;&#26412;&#30340;&#21487;&#36801;&#31227;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#39640;&#32500;&#32972;&#26223;&#19979;&#30340;&#36801;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#29305;&#24449;&#32500;&#24230;&#22823;&#20110;&#26679;&#26412;&#22823;&#23567;&#12290;&#20026;&#20102;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20449;&#24687;&#65292;&#35813;&#20449;&#24687;&#21487;&#33021;&#22312;&#29305;&#24449;&#25110;&#28304;&#26679;&#26412;&#20043;&#38388;&#21464;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#21644;&#32858;&#21512;&#29305;&#24449;-wise (F-AdaTrans)&#25110;&#26679;&#26412;-wise (S-AdaTrans)&#21487;&#36801;&#31227;&#32467;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#34701;&#21512;&#24809;&#32602;&#26041;&#27861;&#65292;&#32467;&#21512;&#26435;&#37325;&#65292;&#21487;&#20197;&#26681;&#25454;&#21487;&#36801;&#31227;&#32467;&#26500;&#36827;&#34892;&#35843;&#25972;&#12290;&#20026;&#20102;&#36873;&#25321;&#26435;&#37325;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#29702;&#35770;&#19978;&#24314;&#31435;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#36807;&#31243;&#65292;&#20351;&#24471; F-AdaTrans &#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#23558;&#21487;&#36801;&#31227;&#30340;&#20449;&#21495;&#19982;&#30446;&#26631;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#21516;&#26102;&#28388;&#38500;&#38750;&#21487;&#36801;&#31227;&#30340;&#20449;&#21495;&#65292;S-AdaTrans&#21017;&#21487;&#20197;&#33719;&#24471;&#27599;&#20010;&#28304;&#26679;&#26412;&#20256;&#36882;&#30340;&#20449;&#24687;&#30340;&#26368;&#20339;&#32452;&#21512;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#38750;&#28176;&#36817;&#36895;&#29575;&#65292;&#21487;&#20197;&#22312;&#29305;&#27530;&#24773;&#20917;&#19979;&#24674;&#22797;&#29616;&#26377;&#30340;&#36817;&#26368;&#23567;&#20284;&#20046;&#26368;&#20248;&#36895;&#29575;&#12290;&#25928;&#26524;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13565v1 Announce Type: cross  Abstract: We consider the transfer learning problem in the high dimensional setting, where the feature dimension is larger than the sample size. To learn transferable information, which may vary across features or the source samples, we propose an adaptive transfer learning method that can detect and aggregate the feature-wise (F-AdaTrans) or sample-wise (S-AdaTrans) transferable structures. We achieve this by employing a novel fused-penalty, coupled with weights that can adapt according to the transferable structure. To choose the weight, we propose a theoretically informed, data-driven procedure, enabling F-AdaTrans to selectively fuse the transferable signals with the target while filtering out non-transferable signals, and S-AdaTrans to obtain the optimal combination of information transferred from each source sample. The non-asymptotic rates are established, which recover existing near-minimax optimal rates in special cases. The effectivene
&lt;/p&gt;</description></item><item><title>DL2Fence&#26159;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#24103;&#34701;&#21512;&#65292;&#29992;&#20110;&#22312;&#22823;&#35268;&#27169;NoCs&#20013;&#26816;&#27979;&#21644;&#23450;&#20301;&#32454;&#21270;&#30340;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#65292;&#20197;&#20986;&#33394;&#30340;&#26816;&#27979;&#24615;&#33021;&#21644;&#26497;&#20302;&#30340;&#30828;&#20214;&#24320;&#38144;&#33879;&#31216;&#12290;</title><link>https://arxiv.org/abs/2403.13563</link><description>&lt;p&gt;
DL2Fence: &#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#24103;&#34701;&#21512;&#38598;&#25104;&#65292;&#22686;&#24378;&#22823;&#35268;&#27169;NoCs&#20013;&#32454;&#21270;&#25298;&#32477;&#26381;&#21153;&#30340;&#26816;&#27979;&#21644;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
DL2Fence: Integrating Deep Learning and Frame Fusion for Enhanced Detection and Localization of Refined Denial-of-Service in Large-Scale NoCs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13563
&lt;/p&gt;
&lt;p&gt;
DL2Fence&#26159;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#24103;&#34701;&#21512;&#65292;&#29992;&#20110;&#22312;&#22823;&#35268;&#27169;NoCs&#20013;&#26816;&#27979;&#21644;&#23450;&#20301;&#32454;&#21270;&#30340;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#65292;&#20197;&#20986;&#33394;&#30340;&#26816;&#27979;&#24615;&#33021;&#21644;&#26497;&#20302;&#30340;&#30828;&#20214;&#24320;&#38144;&#33879;&#31216;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#31934;&#32454;&#30340;&#27867;&#27946;&#27880;&#20837;&#36895;&#29575;&#21487;&#35843;&#30340;&#32593;&#32476;&#33455;&#29255;&#65288;NoC&#65289;&#25298;&#32477;&#26381;&#21153;&#65288;DoS&#65289;&#27169;&#22411;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#25552;&#20986;&#20102;DL2Fence&#65292;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#21644;&#24103;&#34701;&#21512;&#65288;2F&#65289;&#36827;&#34892;DoS&#26816;&#27979;&#21644;&#23450;&#20301;&#30340;&#26032;&#39062;&#26694;&#26550;&#12290;&#24320;&#21457;&#20102;&#20004;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#29992;&#20110;DoS&#30340;&#20998;&#31867;&#21644;&#20998;&#21106;&#65292;&#20998;&#21035;&#23454;&#29616;&#20102;95.8&#65285;&#21644;91.7&#65285;&#30340;&#26816;&#27979;&#21644;&#23450;&#20301;&#20934;&#30830;&#29575;&#65292;&#22312;16x16&#32593;&#26684;NoC&#20013;&#30340;&#31934;&#24230;&#29575;&#20998;&#21035;&#20026;98.5&#65285;&#21644;99.3&#65285;&#12290;&#24403;&#20174;8x8&#25193;&#23637;&#21040;16x16 NoCs&#26102;&#65292;&#35813;&#26694;&#26550;&#30340;&#30828;&#20214;&#24320;&#38144;&#26174;&#30528;&#20943;&#23569;&#20102;76.3&#65285;&#65292;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#20854;&#30828;&#20214;&#35201;&#27714;&#23569;&#20102;42.4&#65285;&#12290;&#36825;&#19968;&#36827;&#23637;&#34920;&#26126;DL2Fence&#22312;&#24179;&#34913;&#22823;&#35268;&#27169;NoCs&#20013;&#20986;&#33394;&#30340;&#26816;&#27979;&#24615;&#33021;&#19982;&#26497;&#20302;&#30828;&#20214;&#24320;&#38144;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13563v1 Announce Type: cross  Abstract: This study introduces a refined Flooding Injection Rate-adjustable Denial-of-Service (DoS) model for Network-on-Chips (NoCs) and more importantly presents DL2Fence, a novel framework utilizing Deep Learning (DL) and Frame Fusion (2F) for DoS detection and localization. Two Convolutional Neural Networks models for classification and segmentation were developed to detect and localize DoS respectively. It achieves detection and localization accuracies of 95.8\% and 91.7\%, and precision rates of 98.5\% and 99.3\% in a 16x16 mesh NoC. The framework's hardware overhead notably decreases by 76.3\% when scaling from 8x8 to 16x16 NoCs, and it requires 42.4\% less hardware compared to state-of-the-arts. This advancement demonstrates DL2Fence's effectiveness in balancing outstanding detection performance in large-scale NoCs with extremely low hardware overhead.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Ground-A-Score&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35780;&#20998;&#33976;&#39311;&#36807;&#31243;&#20013;&#21152;&#20837;&#22522;&#20934;&#65292;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#25991;&#26412;&#25552;&#31034;&#35201;&#27714;&#30340;&#31934;&#30830;&#21453;&#26144;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#22270;&#20687;&#20869;&#23545;&#35937;&#20301;&#32622;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#33021;&#22815;&#31934;&#30830;&#32534;&#36753;&#21306;&#22495;&#24182;&#20445;&#25345;&#22270;&#20687;&#23545;&#35937;&#30340;&#23436;&#25972;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13551</link><description>&lt;p&gt;
Ground-A-Score&#65306;&#25193;&#23637;&#35780;&#20998;&#33976;&#39311;&#29992;&#20110;&#22810;&#23646;&#24615;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Ground-A-Score: Scaling Up the Score Distillation for Multi-Attribute Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13551
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Ground-A-Score&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35780;&#20998;&#33976;&#39311;&#36807;&#31243;&#20013;&#21152;&#20837;&#22522;&#20934;&#65292;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#25991;&#26412;&#25552;&#31034;&#35201;&#27714;&#30340;&#31934;&#30830;&#21453;&#26144;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#22270;&#20687;&#20869;&#23545;&#35937;&#20301;&#32622;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#33021;&#22815;&#31934;&#30830;&#32534;&#36753;&#21306;&#22495;&#24182;&#20445;&#25345;&#22270;&#20687;&#23545;&#35937;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20419;&#36827;&#20102;&#21508;&#31181;&#22270;&#20687;&#32534;&#36753;&#25216;&#26415;&#65292;&#20294;&#26159;&#22797;&#26434;&#30340;&#25991;&#26412;&#25552;&#31034;&#36890;&#24120;&#20250;&#30001;&#20110;&#22788;&#29702;&#25991;&#26412;&#20449;&#24687;&#30340;&#29942;&#39048;&#32780;&#23548;&#33268;&#23545;&#19968;&#20123;&#35831;&#27714;&#30340;&#24573;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Ground-A-Score&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35780;&#20998;&#33976;&#39311;&#36807;&#31243;&#20013;&#21152;&#20837;&#22522;&#20934;&#26469;&#30830;&#20445;&#23545;&#22797;&#26434;&#25552;&#31034;&#38656;&#27714;&#30340;&#20934;&#30830;&#21453;&#26144;&#22312;&#32534;&#36753;&#32467;&#26524;&#20013;&#65292;&#36824;&#32771;&#34385;&#20102;&#22270;&#20687;&#20869;&#23545;&#35937;&#20301;&#32622;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#20511;&#21161;&#26032;&#30340;&#24809;&#32602;&#31995;&#25968;&#21644;&#23545;&#27604;&#25439;&#22833;&#30340;&#36873;&#25321;&#24615;&#24212;&#29992;&#26377;&#21161;&#20110;&#31934;&#30830;&#23450;&#20301;&#32534;&#36753;&#21306;&#22495;&#65292;&#21516;&#26102;&#20445;&#25345;&#28304;&#22270;&#20687;&#20013;&#23545;&#35937;&#30340;&#23436;&#25972;&#24615;&#12290;&#23450;&#24615;&#35780;&#20272;&#21644;&#23450;&#37327;&#20998;&#26512;&#37117;&#35777;&#23454;&#20102;Ground-A-Score&#25104;&#21151;&#36981;&#24490;&#20102;&#25193;&#23637;&#21644;&#22810;&#26041;&#38754;&#25552;&#31034;&#30340;&#22797;&#26434;&#32454;&#33410;&#65292;&#30830;&#20445;&#20102;&#39640;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13551v1 Announce Type: cross  Abstract: Despite recent advancements in text-to-image diffusion models facilitating various image editing techniques, complex text prompts often lead to an oversight of some requests due to a bottleneck in processing text information. To tackle this challenge, we present Ground-A-Score, a simple yet powerful model-agnostic image editing method by incorporating grounding during score distillation. This approach ensures a precise reflection of intricate prompt requirements in the editing outcomes, taking into account the prior knowledge of the object locations within the image. Moreover, the selective application with a new penalty coefficient and contrastive loss helps to precisely target editing areas while preserving the integrity of the objects in the source image. Both qualitative assessments and quantitative analyses confirm that Ground-A-Score successfully adheres to the intricate details of extended and multifaceted prompts, ensuring high
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20132;&#36890;&#20107;&#20214;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#23558;&#35821;&#35328;&#27169;&#22411;&#29305;&#24449;&#19982;&#20256;&#32479;&#29305;&#24449;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#25110;&#21305;&#37197;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#20107;&#20214;&#20005;&#37325;&#24615;&#20998;&#31867;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.13547</link><description>&lt;p&gt;
&#22312;&#20132;&#36890;&#20107;&#20214;&#31649;&#29702;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20005;&#37325;&#24615;&#20998;&#31867;&#65306;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Integrating Large Language Models for Severity Classification in Traffic Incident Management: A Machine Learning Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20132;&#36890;&#20107;&#20214;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#23558;&#35821;&#35328;&#27169;&#22411;&#29305;&#24449;&#19982;&#20256;&#32479;&#29305;&#24449;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#25110;&#21305;&#37197;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#20107;&#20214;&#20005;&#37325;&#24615;&#20998;&#31867;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25552;&#21319;&#20132;&#36890;&#20107;&#20214;&#31649;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#23427;&#32771;&#23519;&#20102;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#29305;&#24449;&#22312;&#20351;&#29992;&#20107;&#25925;&#25253;&#21578;&#23545;&#20107;&#20214;&#20005;&#37325;&#24615;&#36827;&#34892;&#20998;&#31867;&#26102;&#22914;&#20309;&#25913;&#21892;&#25110;&#19982;&#39044;&#27979;&#20934;&#30830;&#24615;&#30456;&#21305;&#37197;&#12290;&#22312;&#35821;&#35328;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;&#21253;&#25324;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#65289;&#32452;&#21512;&#20043;&#38388;&#36827;&#34892;&#20102;&#22810;&#27425;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;&#25991;&#26412;&#21644;&#20107;&#20214;&#25253;&#21578;&#20013;&#20256;&#32479;&#29305;&#24449;&#21644;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#29305;&#24449;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#32452;&#21512;&#36827;&#34892;&#20005;&#37325;&#24615;&#20998;&#31867;&#12290;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#29305;&#24449;&#19982;&#30452;&#25509;&#20174;&#20107;&#20214;&#25253;&#21578;&#20013;&#33719;&#21462;&#30340;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#65292;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#23558;&#20107;&#20214;&#30340;&#20005;&#37325;&#32423;&#21035;&#25351;&#27966;&#32473;&#20107;&#20214;&#26102;&#65292;&#23588;&#20854;&#26159;&#22312;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#21644;Ex&#26102;&#65292;&#33021;&#22815;&#25552;&#39640;&#25110;&#33267;&#23569;&#21305;&#37197;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13547v1 Announce Type: new  Abstract: This study evaluates the impact of large language models on enhancing machine learning processes for managing traffic incidents. It examines the extent to which features generated by modern language models improve or match the accuracy of predictions when classifying the severity of incidents using accident reports. Multiple comparisons performed between combinations of language models and machine learning algorithms, including Gradient Boosted Decision Trees, Random Forests, and Extreme Gradient Boosting. Our research uses both conventional and language model-derived features from texts and incident reports, and their combinations to perform severity classification. Incorporating features from language models with those directly obtained from incident reports has shown to improve, or at least match, the performance of machine learning techniques in assigning severity levels to incidents, particularly when employing Random Forests and Ex
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#38382;&#39064;&#37325;&#26032;&#26500;&#36896;&#20026;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#23454;&#29616;&#20102;&#31532;&#20108;&#22825;&#28779;&#28798;&#39044;&#27979;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.13545</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#20998;&#21106;&#36827;&#34892;&#31532;&#20108;&#22825;&#28779;&#28798;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Next day fire prediction via semantic segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13545
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#38382;&#39064;&#37325;&#26032;&#26500;&#36896;&#20026;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#23454;&#29616;&#20102;&#31532;&#20108;&#22825;&#28779;&#28798;&#39044;&#27979;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#31532;&#20108;&#22825;&#28779;&#28798;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27969;&#27700;&#32447;&#12290;&#31532;&#20108;&#22825;&#28779;&#28798;&#39044;&#27979;&#20219;&#21153;&#21253;&#25324;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20197;&#30452;&#21040;&#26576;&#19968;&#22825;&#20026;&#27490;&#30340;&#29305;&#23450;&#22320;&#21306;&#20449;&#24687;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#39044;&#27979;&#31532;&#20108;&#22825;&#28779;&#28798;&#30340;&#21457;&#29983;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#37325;&#26032;&#26500;&#36896;&#20026;&#22270;&#20687;&#19978;&#30340;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#20854;&#20013;&#27599;&#20010;&#20687;&#32032;&#23545;&#24212;&#20110;&#19968;&#20010;&#22320;&#21306;&#30340;&#27599;&#26085;&#24555;&#29031;&#65292;&#32780;&#20854;&#36890;&#36947;&#34920;&#31034;&#20043;&#21069;&#30340;&#34920;&#26684;&#35757;&#32451;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#20010;&#38382;&#39064;&#30340;&#26032;&#26500;&#36896;&#22312;&#19968;&#20010;&#23436;&#25972;&#30340;&#27969;&#27700;&#32447;&#20869;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13545v1 Announce Type: cross  Abstract: In this paper we present a deep learning pipeline for next day fire prediction. The next day fire prediction task consists in learning models that receive as input the available information for an area up until a certain day, in order to predict the occurrence of fire for the next day. Starting from our previous problem formulation as a binary classification task on instances (daily snapshots of each area) represented by tabular feature vectors, we reformulate the problem as a semantic segmentation task on images; there, each pixel corresponds to a daily snapshot of an area, while its channels represent the formerly tabular training features. We demonstrate that this problem formulation, built within a thorough pipeline achieves state of the art results.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19968;&#31995;&#21015;&#28040;&#34701;&#23454;&#39564;&#65292;&#30830;&#23450;&#20102;ORCA&#20013;&#23884;&#20837;&#22120;&#35757;&#32451;&#23545;2D&#20219;&#21153;&#26080;&#24110;&#21161;&#12289;1D&#20219;&#21153;&#38656;&#35201;&#36866;&#37327;&#23884;&#20837;&#22120;&#35757;&#32451;&#12289;&#20197;&#21450;&#27169;&#22411;&#24494;&#35843;&#23545;&#24615;&#33021;&#24433;&#21709;&#26368;&#22823;&#30340;&#32467;&#35770;&#12290;</title><link>https://arxiv.org/abs/2403.13537</link><description>&lt;p&gt;
&#35299;&#37322;ORCA&#20132;&#21449;&#27169;&#24577;&#24494;&#35843;&#25104;&#21151;&#30340;&#22240;&#32032;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What explains the success of cross-modal fine-tuning with ORCA?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13537
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19968;&#31995;&#21015;&#28040;&#34701;&#23454;&#39564;&#65292;&#30830;&#23450;&#20102;ORCA&#20013;&#23884;&#20837;&#22120;&#35757;&#32451;&#23545;2D&#20219;&#21153;&#26080;&#24110;&#21161;&#12289;1D&#20219;&#21153;&#38656;&#35201;&#36866;&#37327;&#23884;&#20837;&#22120;&#35757;&#32451;&#12289;&#20197;&#21450;&#27169;&#22411;&#24494;&#35843;&#23545;&#24615;&#33021;&#24433;&#21709;&#26368;&#22823;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ORCA&#65288;Shen&#31561;&#20154;&#65292;2023&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#30340;&#20132;&#21449;&#27169;&#24577;&#24494;&#35843;&#25216;&#26415;&#65292;&#21363;&#23558;&#39044;&#35757;&#32451;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#24212;&#29992;&#20110;&#20854;&#35757;&#32451;&#25968;&#25454;&#20043;&#22806;&#30340;&#27169;&#24577;&#12290; &#35813;&#25216;&#26415;&#20027;&#35201;&#21253;&#25324;&#35757;&#32451;&#19968;&#20010;&#23884;&#20837;&#22120;&#24182;&#24494;&#35843;&#23884;&#20837;&#22120;&#21644;&#27169;&#22411;&#12290; &#23613;&#31649;&#23427;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#25105;&#20204;&#24182;&#19981;&#30830;&#20999;&#20102;&#35299;&#36825;&#20123;&#32452;&#20214;&#20013;&#30340;&#27599;&#20010;&#22914;&#20309;&#20419;&#25104;ORCA&#30340;&#25104;&#21151;&#12290; &#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#28040;&#34701;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#23884;&#20837;&#22120;&#35757;&#32451;&#23545;2D&#20219;&#21153;&#27627;&#26080;&#24110;&#21161;&#65292;&#19982;&#21407;&#22987;&#35770;&#25991;&#25152;&#35328;&#30456;&#21453;&#12290; &#22312;1D&#20219;&#21153;&#20013;&#65292;&#19968;&#23450;&#37327;&#30340;&#23884;&#20837;&#22120;&#35757;&#32451;&#26159;&#24517;&#35201;&#30340;&#65292;&#20294;&#26356;&#22810;&#24182;&#38750;&#24635;&#26159;&#26356;&#22909;&#12290; &#22312;&#25105;&#20204;&#23581;&#35797;&#30340;6&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;4&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;&#27169;&#22411;&#24494;&#35843;&#20135;&#29983;&#20102;&#26368;&#22823;&#30340;&#24046;&#24322;&#12290; &#36890;&#36807;&#25105;&#20204;&#30340;&#28040;&#34701;&#23454;&#39564;&#21644;&#22522;&#32447;&#65292;&#25105;&#20204;&#23545;ORCA&#30340;&#21508;&#20010;&#32452;&#20214;&#26377;&#20102;&#26356;&#22909;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13537v1 Announce Type: new  Abstract: ORCA (Shen et al., 2023) is a recent technique for cross-modal fine-tuning, i.e., applying pre-trained transformer models to modalities beyond their training data. The technique consists primarily of training an embedder and fine-tuning the embedder and model. Despite its high performance on a variety of downstream tasks, we do not understand precisely how each of these components contribute to ORCA's success. Therefore, we run a series of ablations and find that embedder training does not help 2D tasks at all, contrary to what the original paper posits. In 1D tasks, some amount of embedder training is necessary but more is not better. In 4 out of 6 datasets we experiment with, it is model fine-tuning that makes the biggest difference. Through our ablations and baselines, we contribute a better understanding of the individual components of ORCA.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#38450;&#24481;&#25968;&#25454;&#27602;&#33647;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#36807;&#28388;&#34987;&#27602;&#23475;&#30340;&#25968;&#25454;&#28857;&#65292;&#26377;&#25928;&#21306;&#20998;&#20986;&#29305;&#24449;&#21521;&#37327;&#31354;&#38388;&#20013;&#30340;&#24178;&#20928;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.13523</link><description>&lt;p&gt;
&#20320;&#20013;&#20102;&#25968;&#25454;&#27602;&#33647;&#21527;&#65311;&#38450;&#24481;&#31070;&#32463;&#32593;&#32476;&#20813;&#21463;&#25968;&#25454;&#27602;&#33647;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Have You Poisoned My Data? Defending Neural Networks against Data Poisoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#38450;&#24481;&#25968;&#25454;&#27602;&#33647;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#36807;&#28388;&#34987;&#27602;&#23475;&#30340;&#25968;&#25454;&#28857;&#65292;&#26377;&#25928;&#21306;&#20998;&#20986;&#29305;&#24449;&#21521;&#37327;&#31354;&#38388;&#20013;&#30340;&#24178;&#20928;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#31354;&#21069;&#21487;&#29992;&#24615;&#25512;&#21160;&#20102;&#24378;&#22823;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22914;&#27492;&#22823;&#37327;&#25968;&#25454;&#30340;&#38656;&#27714;&#23548;&#33268;&#20102;&#28508;&#22312;&#23041;&#32961;&#65292;&#22914;&#25968;&#25454;&#27602;&#33647;&#25915;&#20987;&#65306;&#38024;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#23545;&#25239;&#24615;&#31713;&#25913;&#65292;&#26088;&#22312;&#25439;&#23475;&#23398;&#20064;&#27169;&#22411;&#20197;&#23454;&#29616;&#32473;&#23450;&#30340;&#23545;&#25239;&#24615;&#30446;&#26631;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#38450;&#24481;&#23545;&#24178;&#20928;&#26631;&#31614;&#25968;&#25454;&#30340;&#27602;&#33647;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#36807;&#28388;&#22312;&#36801;&#31227;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#34987;&#27602;&#23475;&#25968;&#25454;&#28857;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#25968;&#25454;&#28857;&#30340;&#26032;&#29305;&#24449;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#23637;&#31034;&#20854;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#25968;&#25454;&#20998;&#24067;&#30340;&#22266;&#26377;&#23646;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#34920;&#26126;&#26377;&#25928;&#30340;&#27602;&#25104;&#20998;&#21487;&#20197;&#25104;&#21151;&#21306;&#20998;&#20986;&#29305;&#24449;&#21521;&#37327;&#31354;&#38388;&#20013;&#30340;&#24178;&#20928;&#28857;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#24182;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13523v1 Announce Type: new  Abstract: The unprecedented availability of training data fueled the rapid development of powerful neural networks in recent years. However, the need for such large amounts of data leads to potential threats such as poisoning attacks: adversarial manipulations of the training data aimed at compromising the learned model to achieve a given adversarial goal.   This paper investigates defenses against clean-label poisoning attacks and proposes a novel approach to detect and filter poisoned datapoints in the transfer learning setting. We define a new characteristic vector representation of datapoints and show that it effectively captures the intrinsic properties of the data distribution. Through experimental analysis, we demonstrate that effective poisons can be successfully differentiated from clean points in the characteristic vector space. We thoroughly evaluate our proposed approach and compare it to existing state-of-the-art defenses using multip
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;REAL&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21452;&#27969;&#22522;&#30784;&#39044;&#35757;&#32451;&#21644;&#34920;&#31034;&#22686;&#24378;&#33976;&#39311;&#36807;&#31243;&#26469;&#22686;&#24378;&#25552;&#21462;&#22120;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#26080;&#33539;&#20363;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13522</link><description>&lt;p&gt;
REAL&#65306;&#29992;&#20110;&#26080;&#33539;&#20363;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#34920;&#31034;&#22686;&#24378;&#20998;&#26512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
REAL: Representation Enhanced Analytic Learning for Exemplar-free Class-incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;REAL&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21452;&#27969;&#22522;&#30784;&#39044;&#35757;&#32451;&#21644;&#34920;&#31034;&#22686;&#24378;&#33976;&#39311;&#36807;&#31243;&#26469;&#22686;&#24378;&#25552;&#21462;&#22120;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#26080;&#33539;&#20363;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#33539;&#20363;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;(EFCIL)&#26088;&#22312;&#20943;&#36731;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#32780;&#27809;&#26377;&#21487;&#29992;&#30340;&#21382;&#21490;&#25968;&#25454;&#12290;&#19982;&#23384;&#20648;&#21382;&#21490;&#26679;&#26412;&#30340;&#22238;&#25918;&#24335;CIL&#30456;&#27604;&#65292;EFCIL&#22312;&#26080;&#33539;&#20363;&#32422;&#26463;&#19979;&#26356;&#23481;&#26131;&#36951;&#24536;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#21463;&#26368;&#36817;&#21457;&#23637;&#30340;&#22522;&#20110;&#20998;&#26512;&#23398;&#20064;(AL)&#30340;CIL&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;EFCIL&#30340;&#34920;&#31034;&#22686;&#24378;&#20998;&#26512;&#23398;&#20064;(REAL)&#12290;REAL&#26500;&#24314;&#20102;&#19968;&#20010;&#21452;&#27969;&#22522;&#30784;&#39044;&#35757;&#32451;(DS-BPT)&#21644;&#19968;&#20010;&#34920;&#31034;&#22686;&#24378;&#33976;&#39311;(RED)&#36807;&#31243;&#65292;&#20197;&#22686;&#24378;&#25552;&#21462;&#22120;&#30340;&#34920;&#31034;&#12290;DS-BPT&#22312;&#30417;&#30563;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;(SSCL)&#20004;&#20010;&#27969;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#30784;&#30693;&#35782;&#25552;&#21462;&#12290;RED&#36807;&#31243;&#23558;&#30417;&#30563;&#30693;&#35782;&#25552;&#28860;&#21040;SSCL&#39044;&#35757;&#32451;&#39592;&#24178;&#37096;&#20998;&#65292;&#20419;&#36827;&#21518;&#32493;&#30340;&#22522;&#20110;AL&#30340;CIL&#65292;&#23558;CIL&#36716;&#25442;&#20026;&#36882;&#24402;&#26368;&#23567;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13522v1 Announce Type: new  Abstract: Exemplar-free class-incremental learning (EFCIL) aims to mitigate catastrophic forgetting in class-incremental learning without available historical data. Compared with its counterpart (replay-based CIL) that stores historical samples, the EFCIL suffers more from forgetting issues under the exemplar-free constraint. In this paper, inspired by the recently developed analytic learning (AL) based CIL, we propose a representation enhanced analytic learning (REAL) for EFCIL. The REAL constructs a dual-stream base pretraining (DS-BPT) and a representation enhancing distillation (RED) process to enhance the representation of the extractor. The DS-BPT pretrains model in streams of both supervised learning and self-supervised contrastive learning (SSCL) for base knowledge extraction. The RED process distills the supervised knowledge to the SSCL pretrained backbone and facilitates a subsequent AL-basd CIL that converts the CIL to a recursive least
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#22312;&#33258;&#21160;&#25511;&#21046;&#31995;&#32479;&#20013;&#37096;&#32626;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#21644;&#19981;&#21516;&#38450;&#24481;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#31181;&#38450;&#24481;&#26041;&#27861;&#30340;&#26032;&#39062;&#20445;&#25252;&#26041;&#27861;&#65292;&#24182;&#20026;&#30830;&#20445;&#24037;&#19994;&#20013;&#30340;&#31283;&#20581;&#25925;&#38556;&#35786;&#26029;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.13502</link><description>&lt;p&gt;
&#22312;&#33258;&#21160;&#25511;&#21046;&#31995;&#32479;&#20013;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#19982;&#38450;&#24481;&#65306;&#19968;&#39033;&#32508;&#21512;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks and Defenses in Automated Control Systems: A Comprehensive Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13502
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#22312;&#33258;&#21160;&#25511;&#21046;&#31995;&#32479;&#20013;&#37096;&#32626;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#21644;&#19981;&#21516;&#38450;&#24481;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#31181;&#38450;&#24481;&#26041;&#27861;&#30340;&#26032;&#39062;&#20445;&#25252;&#26041;&#27861;&#65292;&#24182;&#20026;&#30830;&#20445;&#24037;&#19994;&#20013;&#30340;&#31283;&#20581;&#25925;&#38556;&#35786;&#26029;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26426;&#22120;&#23398;&#20064;&#25972;&#21512;&#21040;&#33258;&#21160;&#25511;&#21046;&#31995;&#32479;&#65288;ACS&#65289;&#20013;&#22686;&#24378;&#20102;&#24037;&#19994;&#36807;&#31243;&#31649;&#29702;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#20854;&#20013;&#19968;&#39033;&#38480;&#21046;&#24037;&#19994;&#26222;&#36941;&#37319;&#29992;&#36825;&#20123;&#25216;&#26415;&#30340;&#26159;&#31070;&#32463;&#32593;&#32476;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;Tennessee Eastman&#36807;&#31243;&#25968;&#25454;&#38598;&#22312;ACS&#20013;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25925;&#38556;&#35786;&#26029;&#26102;&#30340;&#23041;&#32961;&#12290;&#36890;&#36807;&#35780;&#20272;&#19977;&#31181;&#19981;&#21516;&#26550;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#23545;&#20854;&#36827;&#34892;&#20102;&#20845;&#31181;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24182;&#25506;&#35752;&#20102;&#20116;&#31181;&#19981;&#21516;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#24378;&#22823;&#33030;&#24369;&#24615;&#20197;&#21450;&#38450;&#24481;&#31574;&#30053;&#30340;&#19981;&#21516;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20445;&#25252;&#26041;&#27861;&#65292;&#23558;&#22810;&#31181;&#38450;&#24481;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#30830;&#20445;&#24037;&#19994;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#24615;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#65292;&#30830;&#20445;&#20102;&#24037;&#19994;&#20013;&#31283;&#20581;&#30340;&#25925;&#38556;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13502v1 Announce Type: new  Abstract: Integrating machine learning into Automated Control Systems (ACS) enhances decision-making in industrial process management. One of the limitations to the widespread adoption of these technologies in industry is the vulnerability of neural networks to adversarial attacks. This study explores the threats in deploying deep learning models for fault diagnosis in ACS using the Tennessee Eastman Process dataset. By evaluating three neural networks with different architectures, we subject them to six types of adversarial attacks and explore five different defense methods. Our results highlight the strong vulnerability of models to adversarial samples and the varying effectiveness of defense strategies. We also propose a novel protection approach by combining multiple defense methods and demonstrate it's efficacy. This research contributes several insights into securing machine learning within ACS, ensuring robust fault diagnosis in industrial 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VSTAR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#26102;&#24207;&#25252;&#29702;&#65288;GTN&#65289;&#30340;&#27010;&#24565;&#65292;&#33258;&#21160;&#29983;&#25104;&#35270;&#39057;&#26775;&#27010;&#24182;&#25913;&#21892;&#23545;&#26102;&#24207;&#21160;&#24577;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#29983;&#25104;&#26356;&#38271;&#12289;&#26356;&#21160;&#24577;&#30340;&#35270;&#39057;</title><link>https://arxiv.org/abs/2403.13501</link><description>&lt;p&gt;
VSTAR&#65306;&#29992;&#20110;&#29983;&#25104;&#38271;&#21160;&#24577;&#35270;&#39057;&#21512;&#25104;&#30340;&#26102;&#38388;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13501
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VSTAR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#26102;&#24207;&#25252;&#29702;&#65288;GTN&#65289;&#30340;&#27010;&#24565;&#65292;&#33258;&#21160;&#29983;&#25104;&#35270;&#39057;&#26775;&#27010;&#24182;&#25913;&#21892;&#23545;&#26102;&#24207;&#21160;&#24577;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#29983;&#25104;&#26356;&#38271;&#12289;&#26356;&#21160;&#24577;&#30340;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25991;&#26412;&#21040;&#35270;&#39057;&#65288;T2V&#65289;&#21512;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#20294;&#24320;&#28304;&#30340;T2V&#25193;&#25955;&#27169;&#22411;&#38590;&#20197;&#29983;&#25104;&#20855;&#26377;&#21160;&#24577;&#21464;&#21270;&#21644;&#19981;&#26029;&#36827;&#21270;&#20869;&#23481;&#30340;&#36739;&#38271;&#35270;&#39057;&#12290;&#23427;&#20204;&#24448;&#24448;&#21512;&#25104;&#20934;&#38745;&#24577;&#35270;&#39057;&#65292;&#24573;&#30053;&#20102;&#25991;&#26412;&#25552;&#31034;&#20013;&#28041;&#21450;&#30340;&#24517;&#35201;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#35270;&#35273;&#21464;&#21270;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#25193;&#23637;&#21040;&#23454;&#29616;&#26356;&#38271;&#12289;&#26356;&#21160;&#24577;&#30340;&#35270;&#39057;&#21512;&#25104;&#24448;&#24448;&#22312;&#35745;&#31639;&#19978;&#38590;&#20197;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29983;&#25104;&#26102;&#24207;&#25252;&#29702;&#65288;GTN&#65289;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21363;&#26102;&#25913;&#21464;&#29983;&#25104;&#36807;&#31243;&#65292;&#20197;&#25913;&#21892;&#23545;&#26102;&#24207;&#21160;&#24577;&#30340;&#25511;&#21046;&#65292;&#24182;&#23454;&#29616;&#29983;&#25104;&#26356;&#38271;&#30340;&#35270;&#39057;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;GTN&#26041;&#27861;&#65292;&#21517;&#20026;VSTAR&#65292;&#23427;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#35201;&#32032;&#65306;1&#65289;&#35270;&#39057;&#26775;&#27010;&#25552;&#31034;&#65288;VSP&#65289;-&#22522;&#20110;&#21407;&#22987;&#21333;&#20010;&#25552;&#31034;&#33258;&#21160;&#29983;&#25104;&#35270;&#39057;&#26775;&#27010;&#65292;&#21033;&#29992;LLMs&#25552;&#20379;&#20934;&#30830;&#30340;&#25991;&#26412;&#25351;&#23548;&#65292;&#20197;&#23454;&#29616;&#23545;&#26102;&#24207;&#21160;&#24577;&#30340;&#31934;&#30830;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13501v1 Announce Type: cross  Abstract: Despite tremendous progress in the field of text-to-video (T2V) synthesis, open-sourced T2V diffusion models struggle to generate longer videos with dynamically varying and evolving content. They tend to synthesize quasi-static videos, ignoring the necessary visual change-over-time implied in the text prompt. At the same time, scaling these models to enable longer, more dynamic video synthesis often remains computationally intractable. To address this challenge, we introduce the concept of Generative Temporal Nursing (GTN), where we aim to alter the generative process on the fly during inference to improve control over the temporal dynamics and enable generation of longer videos. We propose a method for GTN, dubbed VSTAR, which consists of two key ingredients: 1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis based on the original single prompt leveraging LLMs, which gives accurate textual guidance to differe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#40065;&#26834;&#24615;&#39564;&#35777;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#20915;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#24615;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13441</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#40065;&#26834;&#24615;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Robustness Verifcation in Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#40065;&#26834;&#24615;&#39564;&#35777;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#20915;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#24615;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#30340;&#24418;&#24335;&#21270;&#39564;&#35777;&#38382;&#39064;&#12290;&#20854;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#21253;&#25324;&#40065;&#26834;&#24615;&#21644;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#20363;&#22914;&#22312;&#32447;&#24615;&#35268;&#21010;&#23454;&#20363;&#30340;&#31526;&#21495;&#36755;&#20837;&#21644;&#36755;&#20986;&#35268;&#33539;&#24615;&#30340;&#22522;&#30784;&#19978;&#65292;&#26159;&#21542;&#23384;&#22312;&#26377;&#25928;&#30340;&#36755;&#20837;&#20351;&#24471;&#32593;&#32476;&#35745;&#31639;&#20986;&#26377;&#25928;&#30340;&#36755;&#20986;&#65311;&#36825;&#20010;&#24615;&#36136;&#26159;&#21542;&#23545;&#25152;&#26377;&#26377;&#25928;&#30340;&#36755;&#20837;&#37117;&#25104;&#31435;&#65311;&#20004;&#20010;&#32473;&#23450;&#30340;&#32593;&#32476;&#26159;&#21542;&#35745;&#31639;&#20986;&#30456;&#21516;&#30340;&#20989;&#25968;&#65311;&#26159;&#21542;&#23384;&#22312;&#19968;&#20010;&#26356;&#23567;&#30340;&#32593;&#32476;&#26469;&#35745;&#31639;&#30456;&#21516;&#30340;&#20989;&#25968;&#65311;&#36825;&#20123;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#26368;&#36817;&#20174;&#23454;&#29992;&#35282;&#24230;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#36890;&#36807;&#21551;&#21457;&#24335;&#31639;&#27861;&#36827;&#34892;&#20102;&#36817;&#20284;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#34917;&#20805;&#36825;&#20123;&#25104;&#26524;&#65292;&#35813;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20132;&#25442;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#24615;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#23427;&#20204;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#38382;&#39064;&#22312;&#21322;&#32447;&#24615;&#35774;&#32622;&#19979;&#26159;&#21487;&#20197;&#20811;&#26381;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13441v1 Announce Type: cross  Abstract: In this paper we investigate formal verification problems for Neural Network computations. Of central importance will be various robustness and minimization problems such as: Given symbolic specifications of allowed inputs and outputs in form of Linear Programming instances, one question is whether there do exist valid inputs such that the network computes a valid output? And does this property hold for all valid inputs? Do two given networks compute the same function? Is there a smaller network computing the same function?   The complexity of these questions have been investigated recently from a practical point of view and approximated by heuristic algorithms. We complement these achievements by giving a theoretical framework that enables us to interchange security and efficiency questions in neural networks and analyze their computational complexities. We show that the problems are conquerable in a semi-linear setting, meaning that 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#26816;&#27979;&#21644;&#20998;&#32423;&#27450;&#39575;&#34892;&#20026;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#32467;&#21512;&#26631;&#27880;&#31639;&#27861;&#21644;&#24369;&#30417;&#30563;&#27169;&#22411;&#26469;&#35782;&#21035;&#28508;&#22312;&#21487;&#30097;&#30340;&#35746;&#21333;&#31807;&#29366;&#24577;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2403.13429</link><description>&lt;p&gt;
&#20351;&#29992;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#26816;&#27979;&#21644;&#20998;&#32423;&#27450;&#39575;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Detecting and Triaging Spoofing using Temporal Convolutional Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13429
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#26816;&#27979;&#21644;&#20998;&#32423;&#27450;&#39575;&#34892;&#20026;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#32467;&#21512;&#26631;&#27880;&#31639;&#27861;&#21644;&#24369;&#30417;&#30563;&#27169;&#22411;&#26469;&#35782;&#21035;&#28508;&#22312;&#21487;&#30097;&#30340;&#35746;&#21333;&#31807;&#29366;&#24577;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31639;&#27861;&#20132;&#26131;&#21644;&#30005;&#23376;&#24066;&#22330;&#19981;&#26029;&#25913;&#21464;&#37329;&#34701;&#24066;&#22330;&#26684;&#23616;&#65292;&#26816;&#27979;&#21644;&#38450;&#33539;&#19981;&#31471;&#20195;&#29702;&#20197;&#32500;&#25345;&#20844;&#24179;&#21644;&#39640;&#25928;&#24066;&#22330;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#25968;&#25454;&#38598;&#30340;&#29190;&#28856;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#34892;&#19994;&#25216;&#24039;&#20351;&#24471;&#38590;&#20197;&#36866;&#24212;&#26032;&#30340;&#24066;&#22330;&#26465;&#20214;&#24182;&#26816;&#27979;&#19981;&#33391;&#34892;&#20026;&#32773;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#26816;&#27979;&#24066;&#22330;&#25805;&#32437;&#39046;&#22495;&#30340;&#21508;&#31181;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#37319;&#29992;&#19968;&#20010;&#26631;&#27880;&#31639;&#27861;&#65292;&#29992;&#20110;&#21019;&#24314;&#19968;&#20010;&#35757;&#32451;&#38598;&#65292;&#20197;&#23398;&#20064;&#19968;&#20010;&#24369;&#30417;&#30563;&#27169;&#22411;&#26469;&#35782;&#21035;&#28508;&#22312;&#21487;&#30097;&#30340;&#35746;&#21333;&#31807;&#29366;&#24577;&#24207;&#21015;&#12290;&#36825;&#37324;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23398;&#20064;&#35746;&#21333;&#31807;&#30340;&#34920;&#31034;&#65292;&#20197;&#20415;&#22312;&#23558;&#26469;&#21487;&#20197;&#36731;&#26494;&#27604;&#36739;&#20107;&#20214;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#19987;&#23478;&#35780;&#20272;&#32435;&#20837;&#65292;&#20197;&#23457;&#26597;&#29305;&#23450;&#26631;&#35760;&#30340;&#35746;&#21333;&#31807;&#29366;&#24577;&#12290;&#22312;&#19987;&#23478;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13429v1 Announce Type: cross  Abstract: As algorithmic trading and electronic markets continue to transform the landscape of financial markets, detecting and deterring rogue agents to maintain a fair and efficient marketplace is crucial. The explosion of large datasets and the continually changing tricks of the trade make it difficult to adapt to new market conditions and detect bad actors. To that end, we propose a framework that can be adapted easily to various problems in the space of detecting market manipulation. Our approach entails initially employing a labelling algorithm which we use to create a training set to learn a weakly supervised model to identify potentially suspicious sequences of order book states. The main goal here is to learn a representation of the order book that can be used to easily compare future events. Subsequently, we posit the incorporation of expert assessment to scrutinize specific flagged order book states. In the event of an expert's unavai
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;Robust Average Gradient Algorithm&#65288;RAGA&#65289;&#65292;&#26412;&#30740;&#31350;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#24694;&#24847;&#25308;&#21344;&#24237;&#25915;&#20987;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;RAGA&#30340;&#33391;&#22909;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13374</link><description>&lt;p&gt;
&#20855;&#26377;&#23545;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#33258;&#36866;&#24212;&#30340;&#25308;&#21344;&#24237;&#24377;&#24615;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Byzantine-resilient Federated Learning With Adaptivity to Data Heterogeneity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13374
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;Robust Average Gradient Algorithm&#65288;RAGA&#65289;&#65292;&#26412;&#30740;&#31350;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#24694;&#24847;&#25308;&#21344;&#24237;&#25915;&#20987;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;RAGA&#30340;&#33391;&#22909;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22788;&#29702;&#20102;&#22312;&#23384;&#22312;&#24694;&#24847;&#25308;&#21344;&#24237;&#25915;&#20987;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#24773;&#20917;&#19979;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40065;&#26834;&#24179;&#22343;&#26799;&#24230;&#31639;&#27861;&#65288;RAGA&#65289;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20960;&#20309;&#20013;&#20301;&#25968;&#36827;&#34892;&#32858;&#21512;&#65292;&#24182;&#21487;&#20197;&#33258;&#30001;&#36873;&#25321;&#26412;&#22320;&#26356;&#26032;&#30340;&#36718;&#25968;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24377;&#24615;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#25110;&#22343;&#21248;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25910;&#25947;&#20998;&#26512;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;&#24378;&#20984;&#21644;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#22312;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#20998;&#26512;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#21482;&#35201;&#24694;&#24847;&#29992;&#25143;&#25968;&#25454;&#38598;&#30340;&#27604;&#20363;&#23567;&#20110;&#19968;&#21322;&#65292;RAGA&#23601;&#21487;&#20197;&#20197;$\mathcal{O}({1}/{T^{2/3- \delta}})$&#30340;&#36895;&#24230;&#23454;&#29616;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#25910;&#25947;&#65292;&#20854;&#20013;$T$&#20026;&#36845;&#20195;&#27425;&#25968;&#65292;$\delta \in (0, 2/3)$&#65292;&#23545;&#20110;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#21017;&#21576;&#32447;&#24615;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#31283;&#23450;&#28857;&#25110;&#20840;&#23616;&#26368;&#20248;&#35299;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13374v1 Announce Type: new  Abstract: This paper deals with federated learning (FL) in the presence of malicious Byzantine attacks and data heterogeneity. A novel Robust Average Gradient Algorithm (RAGA) is proposed, which leverages the geometric median for aggregation and can freely select the round number for local updating. Different from most existing resilient approaches, which perform convergence analysis based on strongly-convex loss function or homogeneously distributed dataset, we conduct convergence analysis for not only strongly-convex but also non-convex loss function over heterogeneous dataset. According to our theoretical analysis, as long as the fraction of dataset from malicious users is less than half, RAGA can achieve convergence at rate $\mathcal{O}({1}/{T^{2/3- \delta}})$ where $T$ is the iteration number and $\delta \in (0, 2/3)$ for non-convex loss function, and at linear rate for strongly-convex loss function. Moreover, stationary point or global optim
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#38382;&#39064;&#23398;&#20064;&#22810;&#25968;&#26631;&#31614;&#65288;LML&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#25968;&#32593;&#32476;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27492;&#35745;&#25968;&#32593;&#32476;&#20248;&#20110;&#20256;&#32479;&#30340;MIL&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.13370</link><description>&lt;p&gt;
&#23398;&#20064;&#22810;&#25968;&#26631;&#31614;&#30340;&#35745;&#25968;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Counting Network for Learning from Majority Label
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13370
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#38382;&#39064;&#23398;&#20064;&#22810;&#25968;&#26631;&#31614;&#65288;LML&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#25968;&#32593;&#32476;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27492;&#35745;&#25968;&#32593;&#32476;&#20248;&#20110;&#20256;&#32479;&#30340;MIL&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22810;&#31867;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#20013;&#30340;&#19968;&#20010;&#26032;&#38382;&#39064;&#65292;&#31216;&#20026;&#23398;&#20064;&#22810;&#25968;&#26631;&#31614;&#65288;LML&#65289;&#12290;&#22312;LML&#20013;&#65292;&#23558;&#19968;&#20010;&#34955;&#23376;&#20013;&#30340;&#23454;&#20363;&#30340;&#22810;&#25968;&#31867;&#21035;&#20998;&#37197;&#20026;&#35813;&#34955;&#23376;&#30340;&#26631;&#31614;&#12290;LML&#26088;&#22312;&#20351;&#29992;&#34955;&#32423;&#22810;&#25968;&#31867;&#21035;&#26469;&#20998;&#31867;&#23454;&#20363;&#12290;&#29616;&#26377;&#30340;MIL&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;LML&#65292;&#22240;&#20026;&#23427;&#20204;&#32858;&#21512;&#32622;&#20449;&#24230;&#65292;&#21487;&#33021;&#23548;&#33268;&#34955;&#32423;&#26631;&#31614;&#19982;&#36890;&#36807;&#35745;&#31639;&#27599;&#20010;&#31867;&#21035;&#23454;&#20363;&#25968;&#37327;&#33719;&#24471;&#30340;&#26631;&#31614;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#30340;&#23454;&#20363;&#32423;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#25968;&#32593;&#32476;&#65292;&#36890;&#36807;&#35757;&#32451;&#26469;&#29983;&#25104;&#36890;&#36807;&#35745;&#31639;&#27599;&#20010;&#31867;&#21035;&#23454;&#20363;&#25968;&#37327;&#20272;&#35745;&#30340;&#34955;&#20043;&#32423;&#22810;&#25968;&#26631;&#31614;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#35745;&#25968;&#32593;&#32476;&#20248;&#20110;&#20256;&#32479;&#30340;MIL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13370v1 Announce Type: cross  Abstract: The paper proposes a novel problem in multi-class Multiple-Instance Learning (MIL) called Learning from the Majority Label (LML). In LML, the majority class of instances in a bag is assigned as the bag's label. LML aims to classify instances using bag-level majority classes. This problem is valuable in various applications. Existing MIL methods are unsuitable for LML due to aggregating confidences, which may lead to inconsistency between the bag-level label and the label obtained by counting the number of instances for each class. This may lead to incorrect instance-level classification. We propose a counting network trained to produce the bag-level majority labels estimated by counting the number of instances for each class. This led to the consistency of the majority class between the network outputs and one obtained by counting the number of instances. Experimental results show that our counting network outperforms conventional MIL 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#25216;&#26415;&#65292;&#22312;&#23569;&#36164;&#28304;&#35821;&#35328;&#24773;&#22659;&#19979;&#65292;&#20165;&#38656;&#23569;&#37327;&#26679;&#26412;&#35757;&#32451;&#21363;&#21487;&#25552;&#21462;&#20020;&#24202;&#20449;&#24687;&#65292;&#19988;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.13369</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#36827;&#34892;&#23569;&#36164;&#28304;&#35821;&#35328;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#20020;&#24202;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Clinical information extraction for Low-resource languages with Few-shot learning using Pre-trained language models and Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13369
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#25216;&#26415;&#65292;&#22312;&#23569;&#36164;&#28304;&#35821;&#35328;&#24773;&#22659;&#19979;&#65292;&#20165;&#38656;&#23569;&#37327;&#26679;&#26412;&#35757;&#32451;&#21363;&#21487;&#25552;&#21462;&#20020;&#24202;&#20449;&#24687;&#65292;&#19988;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20020;&#24202;&#25991;&#20214;&#20013;&#33258;&#21160;&#25552;&#21462;&#21307;&#30103;&#20449;&#24687;&#38754;&#20020;&#30528;&#20960;&#20010;&#25361;&#25112;&#65306;&#25152;&#38656;&#20020;&#24202;&#19987;&#19994;&#30693;&#35782;&#30340;&#39640;&#25104;&#26412;&#12289;&#27169;&#22411;&#39044;&#27979;&#30340;&#26377;&#38480;&#21487;&#35299;&#37322;&#24615;&#12289;&#21463;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#20197;&#21450;&#38544;&#31169;&#27861;&#35268;&#12290;&#26368;&#36817;&#22312;&#39046;&#22495;&#36866;&#24212;&#21644;&#25552;&#31034;&#26041;&#27861;&#19978;&#30340;&#36827;&#23637;&#26174;&#31034;&#65292;&#21033;&#29992;&#36731;&#37327;&#32423;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#22312;&#20351;&#29992;&#26497;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#65292;&#36825;&#20123;&#27169;&#22411;&#36866;&#29992;&#20110;&#25104;&#29087;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#27425;&#22312;&#23569;&#36164;&#28304;&#29615;&#22659;&#20013;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#36890;&#36807;&#22312;&#24503;&#22269;&#21307;&#29983;&#20449;&#20214;&#19978;&#36827;&#34892;&#22810;&#31867;&#21035;&#27573;&#20998;&#31867;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#31867;&#21035;&#32423;&#35780;&#20272;&#65292;&#25903;&#25345; Shapley &#20540;&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#30340;&#23567;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#24182;&#30830;&#20445;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#12289;&#39046;&#22495;&#36866;&#24212;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#20165;&#20165;&#25552;&#31034;&#20102; 20 &#27425;&#30340;&#24773;&#20917;&#19979;&#65292;&#32988;&#36807;&#20102;&#20256;&#32479;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13369v1 Announce Type: new  Abstract: Automatic extraction of medical information from clinical documents poses several challenges: high costs of required clinical expertise, limited interpretability of model predictions, restricted computational resources and privacy regulations. Recent advances in domain-adaptation and prompting methods showed promising results with minimal training data using lightweight masked language models, which are suited for well-established interpretability methods. We are first to present a systematic evaluation of these methods in a low-resource setting, by performing multi-class section classification on German doctor's letters. We conduct extensive class-wise evaluations supported by Shapley values, to validate the quality of our small training data set and to ensure the interpretability of model predictions. We demonstrate that a lightweight, domain-adapted pretrained model, prompted with just 20 shots, outperforms a traditional classificatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32479;&#19968;&#24322;&#24120;&#26816;&#27979;&#30340;Hierarchical Gaussian mixture normalizing flow (HGAD)&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#39640;&#26031;&#28151;&#21512;&#24314;&#27169;&#26469;&#25552;&#21319;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.13349</link><description>&lt;p&gt;
&#20998;&#23618;&#39640;&#26031;&#28151;&#21512;&#27491;&#35268;&#21270;&#27969;&#24314;&#27169;&#29992;&#20110;&#32479;&#19968;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Gaussian Mixture Normalizing Flow Modeling for Unified Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13349
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32479;&#19968;&#24322;&#24120;&#26816;&#27979;&#30340;Hierarchical Gaussian mixture normalizing flow (HGAD)&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#39640;&#26031;&#28151;&#21512;&#24314;&#27169;&#26469;&#25552;&#21319;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#19968;&#24322;&#24120;&#26816;&#27979;&#26159;&#24322;&#24120;&#26816;&#27979;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#20854;&#20013;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#20351;&#29992;&#26469;&#33258;&#22810;&#20010;&#31867;&#21035;&#30340;&#27491;&#24120;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#30446;&#26631;&#26159;&#26816;&#27979;&#36825;&#20123;&#31867;&#21035;&#20013;&#30340;&#24322;&#24120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#39640;&#26031;&#28151;&#21512;&#27491;&#35268;&#21270;&#27969;&#24314;&#27169;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;HGAD&#65292;&#29992;&#20110;&#23436;&#25104;&#32479;&#19968;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;HGAD&#21253;&#21547;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#36328;&#31867;&#21035;&#39640;&#26031;&#28151;&#21512;&#24314;&#27169;&#21644;&#31867;&#20869;&#28151;&#21512;&#31867;&#20013;&#24515;&#23398;&#20064;&#12290;&#19982;&#20808;&#21069;&#22522;&#20110;NF&#30340;AD&#26041;&#27861;&#30456;&#27604;&#65292;&#20998;&#23618;&#39640;&#26031;&#28151;&#21512;&#24314;&#27169;&#26041;&#27861;&#21487;&#20197;&#20026;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#24102;&#26469;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13349v1 Announce Type: new  Abstract: Unified anomaly detection (AD) is one of the most challenges for anomaly detection, where one unified model is trained with normal samples from multiple classes with the objective to detect anomalies in these classes. For such a challenging task, popular normalizing flow (NF) based AD methods may fall into a "homogeneous mapping" issue,where the NF-based AD models are biased to generate similar latent representations for both normal and abnormal features, and thereby lead to a high missing rate of anomalies. In this paper, we propose a novel Hierarchical Gaussian mixture normalizing flow modeling method for accomplishing unified Anomaly Detection, which we call HGAD. Our HGAD consists of two key components: inter-class Gaussian mixture modeling and intra-class mixed class centers learning. Compared to the previous NF-based AD methods, the hierarchical Gaussian mixture modeling approach can bring stronger representation capability to the 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;User Stateful Embedding&#65288;USE&#65289;&#26469;&#35299;&#20915;&#21160;&#24577;&#29992;&#25143;&#24314;&#27169;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#23384;&#20648;&#20808;&#21069;&#27169;&#22411;&#29366;&#24577;&#65292;&#29983;&#25104;&#29992;&#25143;&#23884;&#20837;&#24182;&#21453;&#26144;&#29992;&#25143;&#19981;&#26029;&#21457;&#23637;&#30340;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2403.13344</link><description>&lt;p&gt;
&#20351;&#29992;&#65306;&#24102;&#26377;&#26377;&#29366;&#24577;&#24207;&#21015;&#27169;&#22411;&#30340;&#21160;&#24577;&#29992;&#25143;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
USE: Dynamic User Modeling with Stateful Sequence Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13344
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;User Stateful Embedding&#65288;USE&#65289;&#26469;&#35299;&#20915;&#21160;&#24577;&#29992;&#25143;&#24314;&#27169;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#23384;&#20648;&#20808;&#21069;&#27169;&#22411;&#29366;&#24577;&#65292;&#29983;&#25104;&#29992;&#25143;&#23884;&#20837;&#24182;&#21453;&#26144;&#29992;&#25143;&#19981;&#26029;&#21457;&#23637;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#23884;&#20837;&#22312;&#29992;&#25143;&#21442;&#19982;&#24230;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#26381;&#21153;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#24207;&#21015;&#24314;&#27169;&#30340;&#26368;&#26032;&#36827;&#23637;&#24341;&#21457;&#20102;&#20174;&#34892;&#20026;&#25968;&#25454;&#20013;&#23398;&#20064;&#29992;&#25143;&#23884;&#20837;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#34892;&#20026;&#30340;&#29992;&#25143;&#23884;&#20837;&#23398;&#20064;&#38754;&#20020;&#21160;&#24577;&#29992;&#25143;&#24314;&#27169;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#38543;&#30528;&#29992;&#25143;&#19981;&#26029;&#19982;&#24212;&#29992;&#31243;&#24207;&#20132;&#20114;&#65292;&#29992;&#25143;&#23884;&#20837;&#24212;&#23450;&#26399;&#26356;&#26032;&#20197;&#32771;&#34385;&#29992;&#25143;&#30340;&#26368;&#36817;&#21644;&#38271;&#26399;&#34892;&#20026;&#27169;&#24335;&#12290;&#29616;&#26377;&#26041;&#27861;&#39640;&#24230;&#20381;&#36182;&#20110;&#32570;&#20047;&#21382;&#21490;&#34892;&#20026;&#35760;&#24518;&#30340;&#26080;&#29366;&#24577;&#24207;&#21015;&#27169;&#22411;&#12290;&#23427;&#20204;&#24517;&#39035;&#35201;&#20040;&#20002;&#24323;&#21382;&#21490;&#25968;&#25454;&#20165;&#20351;&#29992;&#26368;&#26032;&#25968;&#25454;&#65292;&#35201;&#20040;&#37325;&#26032;&#22788;&#29702;&#26087;&#25968;&#25454;&#21644;&#26032;&#25968;&#25454;&#12290;&#20004;&#31181;&#24773;&#20917;&#22343;&#20250;&#20135;&#29983;&#22823;&#37327;&#35745;&#31639;&#24320;&#38144;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#25143;&#26377;&#29366;&#24577;&#23884;&#20837;&#65288;USE&#65289;&#12290;USE&#29983;&#25104;&#29992;&#25143;&#23884;&#20837;&#24182;&#21453;&#26144;&#29992;&#25143;&#19981;&#26029;&#21457;&#23637;&#30340;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#36890;&#36807;&#23384;&#20648;&#20808;&#21069;&#30340;&#27169;&#22411;&#29366;&#24577;&#26469;&#36827;&#34892;&#35814;&#23613;&#30340;&#37325;&#26032;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13344v1 Announce Type: cross  Abstract: User embeddings play a crucial role in user engagement forecasting and personalized services. Recent advances in sequence modeling have sparked interest in learning user embeddings from behavioral data. Yet behavior-based user embedding learning faces the unique challenge of dynamic user modeling. As users continuously interact with the apps, user embeddings should be periodically updated to account for users' recent and long-term behavior patterns. Existing methods highly rely on stateless sequence models that lack memory of historical behavior. They have to either discard historical data and use only the most recent data or reprocess the old and new data jointly. Both cases incur substantial computational overhead. To address this limitation, we introduce User Stateful Embedding (USE). USE generates user embeddings and reflects users' evolving behaviors without the need for exhaustive reprocessing by storing previous model states and
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27979;&#35797;&#19987;&#38376;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#20998;&#24067;&#21644;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#21333;&#19968;&#20998;&#31867;&#22120;&#22312;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#36866;&#24212;&#38598;&#25104;&#31639;&#27861;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13335</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#32454;&#35843;Transformer&#38598;&#25104;&#29992;&#20110;LLM&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13335
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27979;&#35797;&#19987;&#38376;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#20998;&#24067;&#21644;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#21333;&#19968;&#20998;&#31867;&#22120;&#22312;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#36866;&#24212;&#38598;&#25104;&#31639;&#27861;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#36798;&#21040;&#20102;&#29983;&#25104;&#21508;&#31181;&#25991;&#26412;&#20869;&#23481;&#30340;&#20154;&#31867;&#27700;&#24179;&#65292;&#20984;&#26174;&#20102;&#26377;&#25928;&#26816;&#27979;&#34394;&#20551;&#25991;&#26412;&#30340;&#24517;&#35201;&#24615;&#65292;&#20197;&#36991;&#20813;&#28508;&#22312;&#39118;&#38505;&#65292;&#22914;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#20551;&#26032;&#38395;&#12290;&#22312;LLM&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#20219;&#21153;&#20013;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#22312;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#21333;&#19968;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;&#25105;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#31867;&#22411;&#25968;&#25454;&#19978;&#34920;&#29616;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#35797;&#20116;&#20010;&#19987;&#38376;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#20998;&#24067;&#21644;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#65292;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21333;&#19968;&#22522;&#20110;Transformer&#30340;&#20998;&#31867;&#22120;&#22312;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#36866;&#24212;&#38598;&#25104;&#31639;&#27861;&#32467;&#21512;&#20102;&#20010;&#20307;&#20998;&#31867;&#22120;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24179;&#22343;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13335v1 Announce Type: new  Abstract: Large language models (LLMs) have reached human-like proficiency in generating diverse textual content, underscoring the necessity for effective fake text detection to avoid potential risks such as fake news in social media. Previous research has mostly tested single models on in-distribution datasets, limiting our understanding of how these models perform on different types of data for LLM-generated text detection task. We researched this by testing five specialized transformer-based models on both in-distribution and out-of-distribution datasets to better assess their performance and generalizability. Our results revealed that single transformer-based classifiers achieved decent performance on in-distribution dataset but limited generalization ability on out-of-distribution dataset. To improve it, we combined the individual classifiers models using adaptive ensemble algorithms, which improved the average accuracy significantly from 91.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#22788;&#29702;&#26465;&#20214;&#35774;&#32622;&#22312;EHR&#30340;&#20540;&#21644;&#27979;&#37327;&#19978;&#65292;&#20197;&#25972;&#21512;&#20020;&#24202;&#25104;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#65292;&#26088;&#22312;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#20013;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.13319</link><description>&lt;p&gt;
HyperFusion&#65306;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#24314;&#27169;&#30340;&#22810;&#27169;&#24577;&#25972;&#21512;&#34920;&#26684;&#21644;&#21307;&#23398;&#25104;&#20687;&#25968;&#25454;&#30340;&#36229;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HyperFusion: A Hypernetwork Approach to Multimodal Integration of Tabular and Medical Imaging Data for Predictive Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13319
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#22788;&#29702;&#26465;&#20214;&#35774;&#32622;&#22312;EHR&#30340;&#20540;&#21644;&#27979;&#37327;&#19978;&#65292;&#20197;&#25972;&#21512;&#20020;&#24202;&#25104;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#65292;&#26088;&#22312;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#20013;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ARXIV: 2403.13319v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#25972;&#21512;&#21508;&#31181;&#20020;&#24202;&#27169;&#24335;&#65292;&#22914;&#21307;&#23398;&#25104;&#20687;&#21644;&#24739;&#32773;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#33719;&#24471;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#26159;&#29616;&#20195;&#21307;&#30103;&#20445;&#20581;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#22810;&#28304;&#25968;&#25454;&#30340;&#32508;&#21512;&#20998;&#26512;&#21487;&#20197;&#20840;&#38754;&#20102;&#35299;&#24739;&#32773;&#30340;&#29366;&#20917;&#65292;&#24182;&#21487;&#20197;&#22686;&#24378;&#35786;&#26029;&#21644;&#27835;&#30103;&#20915;&#31574;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#19968;&#30452;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#23558;&#21307;&#23398;&#25104;&#20687;&#19982;&#20197;&#25968;&#23383;&#34920;&#26684;&#25968;&#25454;&#34920;&#31034;&#30340;&#20020;&#24202;&#12289;&#20154;&#21475;&#32479;&#35745;&#21644;&#36951;&#20256;&#20449;&#24687;&#36827;&#34892;&#34701;&#21512;&#30340;&#22797;&#26434;&#21162;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#39640;&#24230;&#27963;&#36291;&#30340;&#25345;&#32493;&#30740;&#31350;&#36861;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13319v1 Announce Type: cross  Abstract: The integration of diverse clinical modalities such as medical imaging and the tabular data obtained by the patients' Electronic Health Records (EHRs) is a crucial aspect of modern healthcare. The integrative analysis of multiple sources can provide a comprehensive understanding of a patient's condition and can enhance diagnoses and treatment decisions. Deep Neural Networks (DNNs) consistently showcase outstanding performance in a wide range of multimodal tasks in the medical domain. However, the complex endeavor of effectively merging medical imaging with clinical, demographic and genetic information represented as numerical tabular data remains a highly active and ongoing research pursuit.   We present a novel framework based on hypernetworks to fuse clinical imaging and tabular data by conditioning the image processing on the EHR's values and measurements. This approach aims to leverage the complementary information present in these
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;Mathlib4&#30340;&#35821;&#20041;&#25628;&#32034;&#24341;&#25806;&#65292;&#33021;&#22815;&#25509;&#21463;&#38750;&#27491;&#24335;&#26597;&#35810;&#24182;&#25214;&#21040;&#30456;&#20851;&#23450;&#29702;&#65292;&#20026;&#35299;&#20915;&#22312;mathlib4&#20013;&#25628;&#32034;&#22256;&#38590;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.13310</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;Mathlib4&#30340;&#35821;&#20041;&#25628;&#32034;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
A Semantic Search Engine for Mathlib4
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13310
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;Mathlib4&#30340;&#35821;&#20041;&#25628;&#32034;&#24341;&#25806;&#65292;&#33021;&#22815;&#25509;&#21463;&#38750;&#27491;&#24335;&#26597;&#35810;&#24182;&#25214;&#21040;&#30456;&#20851;&#23450;&#29702;&#65292;&#20026;&#35299;&#20915;&#22312;mathlib4&#20013;&#25628;&#32034;&#22256;&#38590;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#23450;&#29702;&#35777;&#26126;&#22120;Lean&#20351;&#24471;&#21487;&#20197;&#39564;&#35777;&#27491;&#24335;&#25968;&#23398;&#35777;&#26126;&#65292;&#24182;&#19988;&#24471;&#21040;&#19968;&#20010;&#19981;&#26029;&#25193;&#22823;&#30340;&#31038;&#21306;&#30340;&#25903;&#25345;&#12290;&#35813;&#29983;&#24577;&#31995;&#32479;&#30340;&#26680;&#24515;&#26159;&#20854;&#25968;&#23398;&#24211;mathlib4&#65292;&#20026;&#25193;&#23637;&#33539;&#22260;&#30340;&#25968;&#23398;&#29702;&#35770;&#30340;&#24418;&#24335;&#21270;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#22312;mathlib4&#20013;&#25628;&#32034;&#23450;&#29702;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#25104;&#21151;&#22312;mathlib4&#20013;&#25628;&#32034;&#65292;&#29992;&#25143;&#36890;&#24120;&#38656;&#35201;&#29087;&#24713;&#20854;&#21629;&#21517;&#32422;&#23450;&#25110;&#25991;&#26723;&#23383;&#31526;&#20018;&#12290;&#22240;&#27492;&#65292;&#21019;&#24314;&#19968;&#20010;&#35821;&#20041;&#25628;&#32034;&#24341;&#25806;&#65292;&#21487;&#20197;&#26041;&#20415;&#22320;&#34987;&#20855;&#26377;&#19981;&#21516;&#29087;&#24713;&#31243;&#24230;&#30340;mathlib4&#30340;&#20010;&#20154;&#20351;&#29992;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;mathlib4&#30340;&#35821;&#20041;&#25628;&#32034;&#24341;&#25806;&#65292;&#21487;&#20197;&#25509;&#21463;&#38750;&#27491;&#24335;&#26597;&#35810;&#24182;&#25214;&#21040;&#30456;&#20851;&#23450;&#29702;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;mathlib4&#25628;&#32034;&#24341;&#25806;&#24615;&#33021;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13310v1 Announce Type: cross  Abstract: The interactive theorem prover, Lean, enables the verification of formal mathematical proofs and is backed by an expanding community. Central to this ecosystem is its mathematical library, mathlib4, which lays the groundwork for the formalization of an expanding range of mathematical theories. However, searching for theorems in mathlib4 can be challenging. To successfully search in mathlib4, users often need to be familiar with its naming conventions or documentation strings. Therefore, creating a semantic search engine that can be used easily by individuals with varying familiarity with mathlib4 is very important. In this paper, we present a semantic search engine for mathlib4 that accepts informal queries and finds the relevant theorems. We also establish a benchmark for assessing the performance of various search engines for mathlib4.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26680;&#21253;&#25216;&#26415;&#35777;&#26126;&#21453;&#21521;&#25311;&#21512;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#26680;&#22810;&#37325;&#32593;&#26684;&#31639;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#22686;&#24378;&#21453;&#21521;&#25311;&#21512;&#65292;&#36866;&#29992;&#20110;&#32467;&#26500;&#21270;&#21644;&#20998;&#25955;&#25968;&#25454;&#30340;&#21152;&#24615;GPs&#12290;</title><link>https://arxiv.org/abs/2403.13300</link><description>&lt;p&gt;
&#26680;&#22810;&#37325;&#32593;&#26684;&#65306;&#36890;&#36807;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#21152;&#36895;&#21453;&#21521;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Kernel Multigrid: Accelerate Back-fitting via Sparse Gaussian Process Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13300
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26680;&#21253;&#25216;&#26415;&#35777;&#26126;&#21453;&#21521;&#25311;&#21512;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#26680;&#22810;&#37325;&#32593;&#26684;&#31639;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#22686;&#24378;&#21453;&#21521;&#25311;&#21512;&#65292;&#36866;&#29992;&#20110;&#32467;&#26500;&#21270;&#21644;&#20998;&#25955;&#25968;&#25454;&#30340;&#21152;&#24615;GPs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28155;&#21152;&#39640;&#26031;&#36807;&#31243;(GPs)&#26159;&#38750;&#21442;&#25968;&#29305;&#24449;&#36873;&#25321;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#24120;&#35265;&#35757;&#32451;&#26041;&#27861;&#26159;&#36125;&#21494;&#26031;&#21453;&#21521;&#25311;&#21512;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#21152;&#24615;GPs&#26102;&#65292;&#21453;&#21521;&#25311;&#21512;&#30340;&#25910;&#25947;&#36895;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#31181;&#31216;&#20026;&#26680;&#21253;(KP)&#30340;&#25216;&#26415;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21453;&#21521;&#25311;&#21512;&#30340;&#25910;&#25947;&#36895;&#24230;&#19981;&#20250;&#27604;$(1-\mathcal{O}(\frac{1}{n}))^t$&#26356;&#24555;&#65292;&#20854;&#20013;$n$&#21644;$t$&#20998;&#21035;&#34920;&#31034;&#25968;&#25454;&#22823;&#23567;&#21644;&#36845;&#20195;&#27425;&#25968;&#12290;&#22240;&#27492;&#65292;&#21453;&#21521;&#25311;&#21512;&#38656;&#35201;&#26368;&#23569;$\mathcal{O}(n\log n)$&#27425;&#36845;&#20195;&#25165;&#33021;&#23454;&#29616;&#25910;&#25947;&#12290;&#22522;&#20110;KP&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26680;&#22810;&#37325;&#32593;&#26684;(KMG)&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#23558;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;(GPR)&#32435;&#20837;&#27599;&#20010;&#21453;&#21521;&#25311;&#21512;&#36845;&#20195;&#20043;&#21518;&#22788;&#29702;&#27531;&#24046;&#26469;&#22686;&#24378;&#21453;&#21521;&#25311;&#21512;&#12290;&#23427;&#36866;&#29992;&#20110;&#20855;&#26377;&#32467;&#26500;&#21270;&#21644;&#20998;&#25955;&#25968;&#25454;&#30340;&#21152;&#24615;GPs&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;K
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13300v1 Announce Type: cross  Abstract: Additive Gaussian Processes (GPs) are popular approaches for nonparametric feature selection. The common training method for these models is Bayesian Back-fitting. However, the convergence rate of Back-fitting in training additive GPs is still an open problem. By utilizing a technique called Kernel Packets (KP), we prove that the convergence rate of Back-fitting is no faster than $(1-\mathcal{O}(\frac{1}{n}))^t$, where $n$ and $t$ denote the data size and the iteration number, respectively. Consequently, Back-fitting requires a minimum of $\mathcal{O}(n\log n)$ iterations to achieve convergence. Based on KPs, we further propose an algorithm called Kernel Multigrid (KMG). This algorithm enhances Back-fitting by incorporating a sparse Gaussian Process Regression (GPR) to process the residuals subsequent to each Back-fitting iteration. It is applicable to additive GPs with both structured and scattered data. Theoretically, we prove that K
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31070;&#32463;&#31639;&#31526;&#23398;&#20064;, &#36890;&#36807;&#38598;&#25104;&#22810;&#20307;&#32791;&#25955;&#31890;&#23376;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#22522;&#20110;&#36830;&#32493;&#20307;&#30340;Rayleigh-Plesset&#27169;&#22411;, &#26725;&#25509;&#20102;&#22810;&#23610;&#24230;&#27668;&#27873;&#29983;&#38271;&#21160;&#21147;&#23398;&#20013;&#30340;&#24494;&#35266;&#21644;&#23439;&#35266;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2403.13299</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#31639;&#31526;&#23398;&#20064;&#22312;&#30456;&#20851;&#27874;&#21160;&#20013;&#22810;&#23610;&#24230;&#27668;&#27873;&#29983;&#38271;&#21160;&#21147;&#23398;&#20013;&#30340;&#26725;&#25509;
&lt;/p&gt;
&lt;p&gt;
Bridging scales in multiscale bubble growth dynamics with correlated fluctuations using neural operator learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13299
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#31639;&#31526;&#23398;&#20064;, &#36890;&#36807;&#38598;&#25104;&#22810;&#20307;&#32791;&#25955;&#31890;&#23376;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#22522;&#20110;&#36830;&#32493;&#20307;&#30340;Rayleigh-Plesset&#27169;&#22411;, &#26725;&#25509;&#20102;&#22810;&#23610;&#24230;&#27668;&#27873;&#29983;&#38271;&#21160;&#21147;&#23398;&#20013;&#30340;&#24494;&#35266;&#21644;&#23439;&#35266;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#27873;&#29983;&#38271;&#21160;&#21147;&#23398;&#36825;&#19968;&#22797;&#26434;&#36807;&#31243;&#28041;&#21450;&#20174;&#24494;&#35266;&#27668;&#27873;&#24418;&#25104;&#30340;&#24494;&#35266;&#21147;&#23398;&#21040;&#23439;&#35266;&#27668;&#27873;&#19982;&#21608;&#22260;&#28909;&#27969;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#29289;&#29702;&#29616;&#35937;&#30340;&#24191;&#27867;&#39046;&#22495;&#12290;&#20026;&#20102;&#24357;&#21512;&#24494;&#35266;&#38543;&#26426;&#27969;&#20307;&#27169;&#22411;&#21644;&#22522;&#20110;&#36830;&#32493;&#20307;&#30340;&#27969;&#20307;&#27169;&#22411;&#20043;&#38388;&#20851;&#20110;&#27668;&#27873;&#21160;&#21147;&#23398;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22797;&#21512;&#31070;&#32463;&#31639;&#31526;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#22522;&#20110;&#22810;&#20307;&#32791;&#25955;&#31890;&#23376;&#21160;&#21147;&#23398;&#65288;mDPD&#65289;&#27169;&#22411;&#21644;&#22522;&#20110;&#36830;&#32493;&#20307;&#30340;Rayleigh-Plesset&#65288;RP&#65289;&#27169;&#22411;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21253;&#25324;&#29992;&#20110;&#23398;&#20064;&#27668;&#27873;&#29983;&#38271;&#24179;&#22343;&#34892;&#20026;&#30340;&#28145;&#24230;&#31639;&#31526;&#32593;&#32476;&#65292;&#26469;&#32479;&#19968;&#38750;&#32447;&#24615;&#27668;&#27873;&#21160;&#21147;&#23398;&#30340;&#20998;&#26512;&#36328;&#24494;&#35266;&#23610;&#24230;&#21644;&#23439;&#35266;&#23610;&#24230;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13299v1 Announce Type: cross  Abstract: The intricate process of bubble growth dynamics involves a broad spectrum of physical phenomena from microscale mechanics of bubble formation to macroscale interplay between bubbles and surrounding thermo-hydrodynamics. Traditional bubble dynamics models including atomistic approaches and continuum-based methods segment the bubble dynamics into distinct scale-specific models. In order to bridge the gap between microscale stochastic fluid models and continuum-based fluid models for bubble dynamics, we develop a composite neural operator model to unify the analysis of nonlinear bubble dynamics across microscale and macroscale regimes by integrating a many-body dissipative particle dynamics (mDPD) model with a continuum-based Rayleigh-Plesset (RP) model through a novel neural network architecture, which consists of a deep operator network for learning the mean behavior of bubble growth subject to pressure variations and a long short-term 
&lt;/p&gt;</description></item><item><title>RoPE&#22312;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22806;&#25512;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;ImageNet-1k&#12289;COCO&#26816;&#27979;&#21644;ADE-20k&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13298</link><description>&lt;p&gt;
&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Rotary Position Embedding for Vision Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13298
&lt;/p&gt;
&lt;p&gt;
RoPE&#22312;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22806;&#25512;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;ImageNet-1k&#12289;COCO&#26816;&#27979;&#21644;ADE-20k&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#65288;RoPE&#65289;&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;Transformer&#30340;&#38271;&#24230;&#22806;&#25512;&#12290;&#28982;&#32780;&#65292;RoPE&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#65292;&#23613;&#31649;RoPE&#20284;&#20046;&#33021;&#22815;&#20687;&#35821;&#35328;&#39046;&#22495;&#19968;&#26679;&#22686;&#24378;&#35270;&#35273;&#21464;&#21387;&#22120;&#65288;ViT&#65289;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#23545;&#23558;RoPE&#24212;&#29992;&#20110;ViT&#26102;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21033;&#29992;RoPE&#22312;2D&#35270;&#35273;&#25968;&#25454;&#19978;&#30340;&#23454;&#38469;&#23454;&#29616;&#12290;&#20998;&#26512;&#26174;&#31034;&#65292;RoPE&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22806;&#25512;&#24615;&#33021;&#65292;&#21363;&#22312;&#25512;&#26029;&#26102;&#22312;&#22686;&#21152;&#22270;&#20687;&#20998;&#36776;&#29575;&#30340;&#21516;&#26102;&#20445;&#25345;&#31934;&#24230;&#12290;&#26368;&#32456;&#23548;&#33268;&#20102;ImageNet-1k&#12289;COCO&#26816;&#27979;&#21644;ADE-20k&#20998;&#21106;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#30456;&#20449;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23558;RoPE&#24212;&#29992;&#20110;ViT&#30340;&#35814;&#23613;&#25351;&#23548;&#65292;&#25215;&#35834;&#36890;&#36807;&#26368;&#23567;&#30340;&#39069;&#22806;&#35745;&#31639;&#24320;&#38144;&#25552;&#39640;&#39592;&#24178;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;&#32593;&#22336;https://&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13298v1 Announce Type: cross  Abstract: Rotary Position Embedding (RoPE) performs remarkably on language models, especially for length extrapolation of Transformers. However, the impacts of RoPE on computer vision domains have been underexplored, even though RoPE appears capable of enhancing Vision Transformer (ViT) performance in a way similar to the language domain. This study provides a comprehensive analysis of RoPE when applied to ViTs, utilizing practical implementations of RoPE for 2D vision data. The analysis reveals that RoPE demonstrates impressive extrapolation performance, i.e., maintaining precision while increasing image resolution at inference. It eventually leads to performance improvement for ImageNet-1k, COCO detection, and ADE-20k segmentation. We believe this study provides thorough guidelines to apply RoPE into ViT, promising improved backbone performance with minimal extra computational overhead. Our code and pre-trained models are available at https://
&lt;/p&gt;</description></item><item><title>AutoBuild&#36890;&#36807;&#23398;&#20064;&#23558;&#25805;&#20316;&#21644;&#26550;&#26500;&#27169;&#22359;&#30340;&#28508;&#22312;&#23884;&#20837;&#19982;&#20854;&#24615;&#33021;&#23545;&#40784;&#65292;&#36171;&#20104;&#26550;&#26500;&#27169;&#22359;&#21487;&#35299;&#37322;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#24615;&#33021;&#31070;&#32463;&#32593;&#32476;&#30340;&#26500;&#24314;&#12290;</title><link>https://arxiv.org/abs/2403.13293</link><description>&lt;p&gt;
&#21033;&#29992;&#21487;&#35299;&#37322;&#30693;&#35782;&#26500;&#24314;&#26368;&#20339;&#31070;&#32463;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Building Optimal Neural Architectures using Interpretable Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13293
&lt;/p&gt;
&lt;p&gt;
AutoBuild&#36890;&#36807;&#23398;&#20064;&#23558;&#25805;&#20316;&#21644;&#26550;&#26500;&#27169;&#22359;&#30340;&#28508;&#22312;&#23884;&#20837;&#19982;&#20854;&#24615;&#33021;&#23545;&#40784;&#65292;&#36171;&#20104;&#26550;&#26500;&#27169;&#22359;&#21487;&#35299;&#37322;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#24615;&#33021;&#31070;&#32463;&#32593;&#32476;&#30340;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26159;&#19968;&#31181;&#26114;&#36149;&#30340;&#20570;&#27861;&#12290;&#19968;&#20010;&#25628;&#32034;&#31354;&#38388;&#28085;&#30422;&#20102;&#22823;&#37327;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#27599;&#20010;&#26550;&#26500;&#35780;&#20272;&#37117;&#38656;&#35201;&#38750;&#24120;&#22823;&#30340;&#24320;&#38144;&#65292;&#36825;&#20351;&#24471;&#31639;&#27861;&#24456;&#38590;&#20805;&#20998;&#25506;&#32034;&#20505;&#36873;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;AutoBuild&#65292;&#19968;&#20010;&#26041;&#26696;&#65292;&#23427;&#23398;&#20064;&#23558;&#25805;&#20316;&#21644;&#26550;&#26500;&#27169;&#22359;&#30340;&#28508;&#22312;&#23884;&#20837;&#19982;&#23427;&#20204;&#25152;&#20986;&#29616;&#30340;&#26550;&#26500;&#30340;&#23454;&#38469;&#24615;&#33021;&#36827;&#34892;&#23545;&#40784;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;AutoBuild&#33021;&#22815;&#20026;&#26550;&#26500;&#27169;&#22359;&#20998;&#37197;&#21487;&#35299;&#37322;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#20363;&#22914;&#20010;&#21035;&#25805;&#20316;&#29305;&#24449;&#21644;&#26356;&#22823;&#30340;&#23439;&#25805;&#20316;&#24207;&#21015;&#65292;&#20174;&#32780;&#21487;&#20197;&#26500;&#24314;&#39640;&#24615;&#33021;&#31070;&#32463;&#32593;&#32476;&#32780;&#26080;&#38656;&#36827;&#34892;&#25628;&#32034;&#12290;&#36890;&#36807;&#22312;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20998;&#31867;&#12289;&#20998;&#21106;&#21644;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#25366;&#25496;&#19968;&#20010;&#30456;&#23545;&#36739;&#23567;&#30340;&#35780;&#20272;&#26550;&#26500;&#38598;&#65292;AutoBuild&#21487;&#20197;&#23398;&#20250;&#26500;&#24314;&#39640;&#25928;&#30340;&#31070;&#32463;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13293v1 Announce Type: cross  Abstract: Neural Architecture Search is a costly practice. The fact that a search space can span a vast number of design choices with each architecture evaluation taking nontrivial overhead makes it hard for an algorithm to sufficiently explore candidate networks. In this paper, we propose AutoBuild, a scheme which learns to align the latent embeddings of operations and architecture modules with the ground-truth performance of the architectures they appear in. By doing so, AutoBuild is capable of assigning interpretable importance scores to architecture modules, such as individual operation features and larger macro operation sequences such that high-performance neural networks can be constructed without any need for search. Through experiments performed on state-of-the-art image classification, segmentation, and Stable Diffusion models, we show that by mining a relatively small set of evaluated architectures, AutoBuild can learn to build high-q
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25277;&#26679;&#30340;&#20551;&#35774;&#26816;&#39564;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#22823;&#23646;&#24615;&#22270;&#20013;&#22788;&#29702;&#33410;&#28857;&#12289;&#36793;&#21644;&#36335;&#24452;&#20551;&#35774;&#65292;&#36890;&#36807;&#25552;&#20986;&#36335;&#24452;&#20551;&#35774;&#24863;&#30693;&#37319;&#26679;&#22120; PHASE &#20197;&#21450; PHASEopt&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#19988;&#39640;&#25928;&#30340;&#25277;&#26679;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#20551;&#35774;&#26816;&#39564;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.13286</link><description>&lt;p&gt;
&#22522;&#20110;&#25277;&#26679;&#30340;&#22823;&#23646;&#24615;&#22270;&#20551;&#35774;&#26816;&#39564;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Sampling-based Framework for Hypothesis Testing on Large Attributed Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25277;&#26679;&#30340;&#20551;&#35774;&#26816;&#39564;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#22823;&#23646;&#24615;&#22270;&#20013;&#22788;&#29702;&#33410;&#28857;&#12289;&#36793;&#21644;&#36335;&#24452;&#20551;&#35774;&#65292;&#36890;&#36807;&#25552;&#20986;&#36335;&#24452;&#20551;&#35774;&#24863;&#30693;&#37319;&#26679;&#22120; PHASE &#20197;&#21450; PHASEopt&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#19988;&#39640;&#25928;&#30340;&#25277;&#26679;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#20551;&#35774;&#26816;&#39564;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#26816;&#39564;&#26159;&#19968;&#31181;&#29992;&#20110;&#20174;&#26679;&#26412;&#25968;&#25454;&#20013;&#24471;&#20986;&#20851;&#20110;&#24635;&#20307;&#30340;&#32467;&#35770;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#36890;&#24120;&#29992;&#34920;&#26684;&#34920;&#31034;&#12290;&#38543;&#30528;&#29616;&#23454;&#24212;&#29992;&#20013;&#22270;&#34920;&#31034;&#30340;&#26222;&#21450;&#65292;&#22270;&#20013;&#30340;&#20551;&#35774;&#26816;&#39564;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#23545;&#23646;&#24615;&#22270;&#20013;&#30340;&#33410;&#28857;&#12289;&#36793;&#21644;&#36335;&#24452;&#20551;&#35774;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#25277;&#26679;&#30340;&#20551;&#35774;&#26816;&#39564;&#26694;&#26550;&#65292;&#21487;&#20197;&#23481;&#32435;&#29616;&#26377;&#30340;&#20551;&#35774;&#19981;&#21487;&#30693;&#30340;&#22270;&#25277;&#26679;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#25277;&#26679;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36335;&#24452;&#20551;&#35774;&#24863;&#30693;&#37319;&#26679;&#22120; PHASE&#65292;&#23427;&#26159;&#19968;&#31181;&#32771;&#34385;&#20551;&#35774;&#20013;&#25351;&#23450;&#36335;&#24452;&#30340; m-&#32500;&#38543;&#26426;&#28216;&#36208;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#20854;&#26102;&#38388;&#25928;&#29575;&#24182;&#25552;&#20986;&#20102; PHASEopt&#12290;&#23545;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#21033;&#29992;&#24120;&#35265;&#30340;&#22270;&#25277;&#26679;&#26041;&#27861;&#36827;&#34892;&#20551;&#35774;&#26816;&#39564;&#65292;&#24182;&#19988;&#22312;&#20934;&#30830;&#24615;&#21644;&#26102;&#38388;&#25928;&#29575;&#26041;&#38754;&#20551;&#35774;&#24863;&#30693;&#25277;&#26679;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13286v1 Announce Type: cross  Abstract: Hypothesis testing is a statistical method used to draw conclusions about populations from sample data, typically represented in tables. With the prevalence of graph representations in real-life applications, hypothesis testing in graphs is gaining importance. In this work, we formalize node, edge, and path hypotheses in attributed graphs. We develop a sampling-based hypothesis testing framework, which can accommodate existing hypothesis-agnostic graph sampling methods. To achieve accurate and efficient sampling, we then propose a Path-Hypothesis-Aware SamplEr, PHASE, an m- dimensional random walk that accounts for the paths specified in a hypothesis. We further optimize its time efficiency and propose PHASEopt. Experiments on real datasets demonstrate the ability of our framework to leverage common graph sampling methods for hypothesis testing, and the superiority of hypothesis-aware sampling in terms of accuracy and time efficiency.
&lt;/p&gt;</description></item><item><title>AFLoRA&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20923;&#32467;&#25237;&#24433;&#30697;&#38453;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#24182;&#25552;&#20379;&#23545;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.13269</link><description>&lt;p&gt;
AFLoRA: &#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#22312;&#22823;&#22411;&#27169;&#22411;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13269
&lt;/p&gt;
&lt;p&gt;
AFLoRA&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20923;&#32467;&#25237;&#24433;&#30697;&#38453;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#24182;&#25552;&#20379;&#23545;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#65288;AFLoRA&#65289;&#12290;&#20855;&#20307;&#22320;&#65292;&#23545;&#20110;&#27599;&#20010;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;&#26435;&#37325;&#24352;&#37327;&#65292;&#25105;&#20204;&#28155;&#21152;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#20302;&#31209;&#30697;&#38453;&#24182;&#34892;&#36335;&#24452;&#65292;&#21363;&#19979;&#25237;&#24433;&#21644;&#19978;&#25237;&#24433;&#30697;&#38453;&#65292;&#27599;&#20010;&#30697;&#38453;&#21518;&#38754;&#36319;&#30528;&#19968;&#20010;&#29305;&#24449;&#21464;&#25442;&#21521;&#37327;&#12290;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#20923;&#32467;&#20998;&#25968;&#65292;&#25105;&#20204;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#36880;&#27493;&#20923;&#32467;&#36825;&#20123;&#25237;&#24433;&#30697;&#38453;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#37327;&#24182;&#20943;&#36731;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#25913;&#21892;&#39640;&#36798;0.85&#65285;&#65292;&#21516;&#26102;&#21487;&#20943;&#23569;&#39640;&#36798;9.5&#20493;&#30340;&#24179;&#22343;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#22312;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#65292;&#19982;&#31867;&#20284;&#30340;PEFT&#22791;&#36873;&#26041;&#26696;&#30456;&#27604;&#65292;AFLoRA&#21487;&#20197;&#25552;&#20379;&#39640;&#36798;1.86&#20493;&#30340;&#25913;&#36827;&#12290;&#38500;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#38469;&#25928;&#29992;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13269v1 Announce Type: new  Abstract: We present a novel Parameter-Efficient Fine-Tuning (PEFT) method, dubbed as Adaptive Freezing of Low Rank Adaptation (AFLoRA). Specifically, for each pre-trained frozen weight tensor, we add a parallel path of trainable low-rank matrices, namely a down-projection and an up-projection matrix, each of which is followed by a feature transformation vector. Based on a novel freezing score, we the incrementally freeze these projection matrices during fine-tuning to reduce the computation and alleviate over-fitting. Our experimental results demonstrate that we can achieve state-of-the-art performance with an average improvement of up to $0.85\%$ as evaluated on GLUE benchmark while yeilding up to $9.5\times$ fewer average trainable parameters. While compared in terms of runtime, AFLoRA can yield up to $1.86\times$ improvement as opposed to similar PEFT alternatives. Besides the practical utility of our approach, we provide insights on the train
&lt;/p&gt;</description></item><item><title>Unifews&#36890;&#36807;&#32479;&#19968;&#36880;&#26465;&#31232;&#30095;&#21270;&#30340;&#26041;&#24335;&#65292;&#32852;&#21512;&#36793;&#26435;&#37325;&#31232;&#30095;&#21270;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#26550;&#26500;&#35774;&#35745;&#24182;&#20855;&#26377;&#36880;&#28176;&#22686;&#21152;&#31232;&#30095;&#24230;&#30340;&#33258;&#36866;&#24212;&#21387;&#32553;&#12290;</title><link>https://arxiv.org/abs/2403.13268</link><description>&lt;p&gt;
Unifews&#65306;&#29992;&#20110;&#39640;&#25928;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#19968;&#36880;&#26465;&#31232;&#30095;&#21270;
&lt;/p&gt;
&lt;p&gt;
Unifews: Unified Entry-Wise Sparsification for Efficient Graph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13268
&lt;/p&gt;
&lt;p&gt;
Unifews&#36890;&#36807;&#32479;&#19968;&#36880;&#26465;&#31232;&#30095;&#21270;&#30340;&#26041;&#24335;&#65292;&#32852;&#21512;&#36793;&#26435;&#37325;&#31232;&#30095;&#21270;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#26550;&#26500;&#35774;&#35745;&#24182;&#20855;&#26377;&#36880;&#28176;&#22686;&#21152;&#31232;&#30095;&#24230;&#30340;&#33258;&#36866;&#24212;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#20294;&#20195;&#20215;&#26159;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#35745;&#31639;&#12290;GNN&#26356;&#26032;&#30340;&#20027;&#35201;&#24320;&#38144;&#26469;&#33258;&#22270;&#20256;&#25773;&#21644;&#26435;&#37325;&#21464;&#25442;&#65292;&#20004;&#32773;&#37117;&#28041;&#21450;&#23545;&#22270;&#35268;&#27169;&#30697;&#38453;&#30340;&#25805;&#20316;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#21033;&#29992;&#22270;&#32423;&#21035;&#25110;&#32593;&#32476;&#32423;&#21035;&#30340;&#31232;&#30095;&#21270;&#25216;&#26415;&#26469;&#20943;&#23569;&#35745;&#31639;&#39044;&#31639;&#65292;&#20174;&#32780;&#20135;&#29983;&#32553;&#23567;&#30340;&#22270;&#25110;&#26435;&#37325;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Unifews&#65292;&#23427;&#20197;&#36880;&#20010;&#30697;&#38453;&#20803;&#32032;&#30340;&#26041;&#24335;&#32479;&#19968;&#20102;&#36825;&#20004;&#31181;&#25805;&#20316;&#65292;&#24182;&#36827;&#34892;&#32852;&#21512;&#36793;&#26435;&#37325;&#31232;&#30095;&#21270;&#20197;&#22686;&#24378;&#23398;&#20064;&#25928;&#29575;&#12290;Unifews&#30340;&#36880;&#26465;&#35774;&#35745;&#20351;&#20854;&#33021;&#22815;&#22312;GNN&#23618;&#20043;&#38388;&#36827;&#34892;&#33258;&#36866;&#24212;&#21387;&#32553;&#65292;&#31232;&#30095;&#24230;&#36880;&#28176;&#22686;&#21152;&#65292;&#24182;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#35774;&#35745;&#65292;&#20855;&#26377;&#21363;&#26102;&#25805;&#20316;&#31616;&#21270;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#26469;&#34920;&#24449;&#31232;&#30095;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13268v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have shown promising performance in various graph learning tasks, but at the cost of resource-intensive computations. The primary overhead of GNN update stems from graph propagation and weight transformation, both involving operations on graph-scale matrices. Previous studies attempt to reduce the computational budget by leveraging graph-level or network-level sparsification techniques, resulting in downsized graph or weights. In this work, we propose Unifews, which unifies the two operations in an entry-wise manner considering individual matrix elements, and conducts joint edge-weight sparsification to enhance learning efficiency. The entry-wise design of Unifews enables adaptive compression across GNN layers with progressively increased sparsity, and is applicable to a variety of architectural designs with on-the-fly operation simplification. Theoretically, we establish a novel framework to characterize spa
&lt;/p&gt;</description></item><item><title>&#21512;&#24182;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#21363;&#21487;&#21019;&#24314;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#35299;&#20915;AI&#20013;&#30340;&#22797;&#26434;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.13257</link><description>&lt;p&gt;
Arcee&#30340;MergeKit&#65306;&#29992;&#20110;&#21512;&#24182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
Arcee's MergeKit: A Toolkit for Merging Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13257
&lt;/p&gt;
&lt;p&gt;
&#21512;&#24182;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#21363;&#21487;&#21019;&#24314;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#35299;&#20915;AI&#20013;&#30340;&#22797;&#26434;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#30340;&#24555;&#36895;&#25193;&#24352;&#20026;&#36890;&#36807;&#21512;&#24182;&#20854;&#21442;&#25968;&#26469;&#32467;&#21512;&#36825;&#20123;&#27169;&#22411;&#26816;&#26597;&#28857;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#36801;&#31227;&#23398;&#20064;&#30340;&#36827;&#27493;&#23548;&#33268;&#20102;&#22823;&#37327;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#19987;&#38376;&#38024;&#23545;&#20010;&#21035;&#20219;&#21153;&#36827;&#34892;&#19987;&#38376;&#21270;&#65292;&#26080;&#27861;&#21033;&#29992;&#24444;&#27492;&#30340;&#20248;&#21183;&#12290;&#27169;&#22411;&#21512;&#24182;&#20419;&#36827;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#21019;&#24314;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#20026;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#21644;&#22810;&#21151;&#33021;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#36890;&#36807;&#20445;&#30041;&#21407;&#22987;&#27169;&#22411;&#30340;&#22266;&#26377;&#33021;&#21147;&#65292;&#27169;&#22411;&#21512;&#24182;&#35299;&#20915;&#20102;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#22797;&#26434;&#25361;&#25112;&#65292;&#21253;&#25324;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#19981;&#26029;&#25193;&#22823;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MergeKit&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#24320;&#28304;&#30340;&#24211;&#65292;&#26088;&#22312;&#20419;&#36827;&#27169;&#22411;&#21512;&#24182;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13257v1 Announce Type: new  Abstract: The rapid expansion of the open-source language model landscape presents an opportunity to merge the competencies of these model checkpoints by combining their parameters. Advances in transfer learning, the process of fine-tuning pre-trained models for specific tasks, has resulted in the development of vast amounts of task-specific models, typically specialized in individual tasks and unable to utilize each other's strengths. Model merging facilitates the creation of multitask models without the need for additional training, offering a promising avenue for enhancing model performance and versatility. By preserving the intrinsic capabilities of the original models, model merging addresses complex challenges in AI - including the difficulties of catastrophic forgetting and multi-task learning. To support this expanding area of research, we introduce MergeKit, a comprehensive, open-source library designed to facilitate the application of mo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#21644;&#36890;&#29992;&#30340;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#21508;&#31181;&#29616;&#26377;&#30340;&#26041;&#27861;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#23613;&#31649;&#26469;&#28304;&#22810;&#26679;&#65292;&#20294;&#23384;&#22312;&#20849;&#21516;&#30340;&#25968;&#23398;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.13249</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#21644;&#36890;&#29992;&#30340;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified and General Framework for Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13249
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#21644;&#36890;&#29992;&#30340;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#21508;&#31181;&#29616;&#26377;&#30340;&#26041;&#27861;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#23613;&#31649;&#26469;&#28304;&#22810;&#26679;&#65292;&#20294;&#23384;&#22312;&#20849;&#21516;&#30340;&#25968;&#23398;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#20391;&#37325;&#20110;&#20174;&#21160;&#24577;&#21644;&#21464;&#21270;&#30340;&#25968;&#25454;&#20998;&#24067;&#20013;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#30041;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#22522;&#20110;&#27491;&#21017;&#21270;&#12289;&#22522;&#20110;&#36125;&#21494;&#26031;&#21644;&#22522;&#20110;&#35760;&#24518;&#37325;&#25918;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#32479;&#19968;&#26694;&#26550;&#21644;&#20849;&#21516;&#26415;&#35821;&#26469;&#25551;&#36848;&#23427;&#20204;&#30340;&#26041;&#27861;&#12290;&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#20840;&#38754;&#19988;&#20840;&#38754;&#30340;&#26694;&#26550;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#35813;&#26694;&#26550;&#28085;&#30422;&#24182;&#35843;&#21644;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20010;&#26032;&#26694;&#26550;&#33021;&#22815;&#23558;&#24050;&#24314;&#31435;&#30340;CL&#26041;&#27861;&#20316;&#20026;&#32479;&#19968;&#21644;&#36890;&#29992;&#30340;&#20248;&#21270;&#30446;&#26631;&#20013;&#30340;&#29305;&#20363;&#12290;&#19968;&#20010;&#26377;&#36259;&#30340;&#21457;&#29616;&#26159;&#65292;&#23613;&#31649;&#23427;&#20204;&#36215;&#28304;&#21508;&#24322;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20849;&#20139;&#20849;&#21516;&#30340;&#25968;&#23398;&#32467;&#26500;&#12290;&#36825;&#19968;&#35266;&#23519;&#31361;&#26174;&#20102;&#36825;&#20123;&#30475;&#20284;&#19981;&#21516;&#25216;&#26415;&#30340;&#20860;&#23481;&#24615;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13249v1 Announce Type: new  Abstract: Continual Learning (CL) focuses on learning from dynamic and changing data distributions while retaining previously acquired knowledge. Various methods have been developed to address the challenge of catastrophic forgetting, including regularization-based, Bayesian-based, and memory-replay-based techniques. However, these methods lack a unified framework and common terminology for describing their approaches. This research aims to bridge this gap by introducing a comprehensive and overarching framework that encompasses and reconciles these existing methodologies. Notably, this new framework is capable of encompassing established CL approaches as special instances within a unified and general optimization objective. An intriguing finding is that despite their diverse origins, these methods share common mathematical structures. This observation highlights the compatibility of these seemingly distinct techniques, revealing their interconnec
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#23384;&#22312;&#22024;&#26434;&#36890;&#20449;&#36890;&#36947;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;FedNMUT&#65292;&#36890;&#36807;&#26799;&#24230;&#36319;&#36394;&#20943;&#23567;&#25968;&#25454;&#24322;&#36136;&#24615;&#24433;&#21709;&#24182;&#26368;&#23567;&#21270;&#36890;&#20449;&#24320;&#38144;&#65292;&#22312;&#22122;&#22768;&#21442;&#25968;&#20013;&#23454;&#29616;&#23458;&#25143;&#31471;&#20849;&#35782;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21644;&#20256;&#32479;&#21442;&#25968;&#28151;&#21512;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.13247</link><description>&lt;p&gt;
&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65306;&#22312;&#20449;&#24687;&#20998;&#20139;&#19981;&#23436;&#20840;&#24773;&#20917;&#19979;&#30340;&#27169;&#22411;&#26356;&#26032;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Decentralized Federated Learning: Model Update Tracking Under Imperfect Information Sharing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13247
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#23384;&#22312;&#22024;&#26434;&#36890;&#20449;&#36890;&#36947;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;FedNMUT&#65292;&#36890;&#36807;&#26799;&#24230;&#36319;&#36394;&#20943;&#23567;&#25968;&#25454;&#24322;&#36136;&#24615;&#24433;&#21709;&#24182;&#26368;&#23567;&#21270;&#36890;&#20449;&#24320;&#38144;&#65292;&#22312;&#22122;&#22768;&#21442;&#25968;&#20013;&#23454;&#29616;&#23458;&#25143;&#31471;&#20849;&#35782;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21644;&#20256;&#32479;&#21442;&#25968;&#28151;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#25955;&#24335;&#22024;&#26434;&#27169;&#22411;&#26356;&#26032;&#36319;&#36394;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65288;FedNMUT&#65289;&#65292;&#35813;&#31639;&#27861;&#26088;&#22312;&#22312;&#21453;&#26144;&#20449;&#24687;&#20132;&#25442;&#19981;&#23436;&#25972;&#30340;&#22024;&#26434;&#36890;&#20449;&#36890;&#36947;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#36816;&#34892;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;&#26799;&#24230;&#36319;&#36394;&#26469;&#26368;&#23567;&#21270;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#36890;&#20449;&#24320;&#38144;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#23558;&#22122;&#22768;&#32435;&#20837;&#20854;&#21442;&#25968;&#20013;&#65292;&#20197;&#27169;&#25311;&#22024;&#26434;&#36890;&#20449;&#28192;&#36947;&#30340;&#26465;&#20214;&#65292;&#20174;&#32780;&#36890;&#36807;&#36890;&#20449;&#22270;&#25299;&#25169;&#22312;&#36825;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20849;&#35782;&#12290;FedNMUT&#23558;&#21442;&#25968;&#20849;&#20139;&#21644;&#22122;&#22768;&#32435;&#20837;&#20316;&#20026;&#20248;&#20808;&#32771;&#34385;&#65292;&#20197;&#22686;&#24378;&#20998;&#25955;&#24335;&#23398;&#20064;&#31995;&#32479;&#23545;&#22024;&#26434;&#36890;&#20449;&#30340;&#25269;&#25239;&#21147;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#39564;&#35777;&#34920;&#26126;&#65292;&#22312;&#24615;&#33021;&#19978;&#65292;FedNMUT&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#21644;&#20256;&#32479;&#30340;&#21442;&#25968;&#28151;&#21512;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13247v1 Announce Type: new  Abstract: A novel Decentralized Noisy Model Update Tracking Federated Learning algorithm (FedNMUT) is proposed, which is tailored to function efficiently in the presence of noisy communication channels that reflect imperfect information exchange. This algorithm uses gradient tracking to minimize the impact of data heterogeneity while minimizing communication overhead. The proposed algorithm incorporates noise into its parameters to mimic the conditions of noisy communication channels, thereby enabling consensus among clients through a communication graph topology in such challenging environments. FedNMUT prioritizes parameter sharing and noise incorporation to increase the resilience of decentralized learning systems against noisy communications. Through theoretical and empirical validation, it is demonstrated that the performance of FedNMUT is superior compared to the existing state-of-the-art methods and conventional parameter-mixing approaches 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26234;&#33021;&#30005;&#34920;&#25968;&#25454;&#24320;&#21457;&#20102;&#19968;&#31181;&#23478;&#24237;&#20805;&#30005;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;Transformer&#27169;&#22411;&#25552;&#20379;&#26410;&#26469;EV&#20805;&#30005;&#20107;&#20214;&#30340;&#39044;&#27979;&#20449;&#24687;&#65292;&#22686;&#24378;&#20102;&#20805;&#30005;&#31649;&#29702;&#30340;&#23454;&#29992;&#24615;</title><link>https://arxiv.org/abs/2403.13246</link><description>&lt;p&gt;
&#20351;&#29992;&#26234;&#33021;&#30005;&#34920;&#25968;&#25454;&#39044;&#27979;&#30005;&#21160;&#36710;&#20805;&#30005;&#20107;&#20214;&#30340;&#20998;&#27835;Transformer&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Divide-Conquer Transformer Learning for Predicting Electric Vehicle Charging Events Using Smart Meter Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13246
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26234;&#33021;&#30005;&#34920;&#25968;&#25454;&#24320;&#21457;&#20102;&#19968;&#31181;&#23478;&#24237;&#20805;&#30005;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;Transformer&#27169;&#22411;&#25552;&#20379;&#26410;&#26469;EV&#20805;&#30005;&#20107;&#20214;&#30340;&#39044;&#27979;&#20449;&#24687;&#65292;&#22686;&#24378;&#20102;&#20805;&#30005;&#31649;&#29702;&#30340;&#23454;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#30005;&#21160;&#36710;&#65288;EV&#65289;&#20805;&#30005;&#20107;&#20214;&#23545;&#20110;&#36127;&#33655;&#35843;&#24230;&#21644;&#33021;&#28304;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#20419;&#36827;&#20102;&#20132;&#36890;&#30005;&#27668;&#21270;&#21644;&#20943;&#30899;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;EV&#20805;&#30005;&#38656;&#27714;&#39044;&#27979;&#19978;&#65292;&#20027;&#35201;&#38024;&#23545;&#20844;&#20849;&#20805;&#30005;&#31449;&#20351;&#29992;&#21382;&#21490;&#20805;&#30005;&#25968;&#25454;&#65292;&#32780;&#23478;&#24237;&#20805;&#30005;&#39044;&#27979;&#21516;&#26679;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23478;&#24237;&#20805;&#30005;&#25968;&#25454;&#30340;&#19981;&#21487;&#29992;&#24615;&#25110;&#26377;&#38480;&#35775;&#38382;&#24615;&#65292;&#29616;&#26377;&#30340;&#39044;&#27979;&#26041;&#27861;&#21487;&#33021;&#19981;&#22826;&#36866;&#29992;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#21463;&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#27979;&#65288;NILM&#65289;&#27010;&#24565;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#21382;&#21490;&#26234;&#33021;&#30005;&#34920;&#25968;&#25454;&#24320;&#21457;&#20102;&#19968;&#31181;&#23478;&#24237;&#20805;&#30005;&#39044;&#27979;&#26041;&#27861;&#12290;&#19982;NILM&#26816;&#27979;&#24050;&#21457;&#29983;&#30340;EV&#20805;&#30005;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#26410;&#26469;EV&#20805;&#30005;&#20107;&#20214;&#30340;&#39044;&#27979;&#20449;&#24687;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#20854;&#22312;&#20805;&#30005;&#31649;&#29702;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;Transformer&#27169;&#22411;&#65292;emp
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13246v1 Announce Type: new  Abstract: Predicting electric vehicle (EV) charging events is crucial for load scheduling and energy management, promoting seamless transportation electrification and decarbonization. While prior studies have focused on EV charging demand prediction, primarily for public charging stations using historical charging data, home charging prediction is equally essential. However, existing prediction methods may not be suitable due to the unavailability of or limited access to home charging data. To address this research gap, inspired by the concept of non-intrusive load monitoring (NILM), we develop a home charging prediction method using historical smart meter data. Different from NILM detecting EV charging that has already occurred, our method provides predictive information of future EV charging occurrences, thus enhancing its utility for charging management. Specifically, our method, leverages a self-attention mechanism-based transformer model, emp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#20013;&#30340;&#38646;&#27425;&#36890;&#29992;&#21270;&#65292;&#36890;&#36807;&#21327;&#20316;&#23398;&#20064;&#22810;&#20010;&#23398;&#20064;&#32773;&#21644;&#20013;&#22830;&#26381;&#21153;&#22120;&#65292;&#22312;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.13245</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#30340;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#19982;&#38646;&#27425;&#36890;&#29992;&#21270;
&lt;/p&gt;
&lt;p&gt;
Federated reinforcement learning for robot motion planning with zero-shot generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13245
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#20013;&#30340;&#38646;&#27425;&#36890;&#29992;&#21270;&#65292;&#36890;&#36807;&#21327;&#20316;&#23398;&#20064;&#22810;&#20010;&#23398;&#20064;&#32773;&#21644;&#20013;&#22830;&#26381;&#21153;&#22120;&#65292;&#22312;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20351;&#29992;&#38646;&#27425;&#36890;&#29992;&#21270;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#36827;&#34892;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#37096;&#32626;&#23398;&#20064;&#31574;&#30053;&#21040;&#26032;&#29615;&#22659;&#26102;&#19981;&#38656;&#35201;&#25968;&#25454;&#25910;&#38598;&#21644;&#31574;&#30053;&#35843;&#25972;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22810;&#20010;&#23398;&#20064;&#32773;&#21644;&#20013;&#22830;&#26381;&#21153;&#22120;&#65288;&#20113;&#31471;&#65289;&#30340;&#21327;&#20316;&#23398;&#20064;&#65292;&#32780;&#19981;&#20998;&#20139;&#21407;&#22987;&#25968;&#25454;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#27599;&#20010;&#23398;&#20064;&#32773;&#23558;&#20854;&#26412;&#22320;&#25511;&#21046;&#31574;&#30053;&#21644;&#30456;&#24212;&#30340;&#20272;&#35745;&#24402;&#19968;&#21270;&#21040;&#36798;&#26102;&#38388;&#19978;&#20256;&#33267;&#20113;&#31471;&#65292;&#28982;&#21518;&#20113;&#31471;&#22312;&#23398;&#20064;&#32773;&#38388;&#35745;&#31639;&#20840;&#23616;&#26368;&#20248;&#24182;&#23558;&#26368;&#20248;&#31574;&#30053;&#24191;&#25773;&#32473;&#23398;&#20064;&#32773;&#12290;&#27599;&#20010;&#23398;&#20064;&#32773;&#28982;&#21518;&#22312;&#19979;&#19968;&#27425;&#36845;&#20195;&#20013;&#20174;&#20854;&#26412;&#22320;&#25511;&#21046;&#31574;&#30053;&#21644;&#20113;&#31471;&#20013;&#36873;&#25321;&#12290;&#25552;&#20986;&#30340;&#26694;&#26550;&#21033;&#29992;&#20102;&#21040;&#36798;&#26102;&#38388;&#21644;&#23433;&#20840;&#24615;&#30340;&#38646;&#27425;&#36890;&#29992;&#21270;&#20445;&#35777;&#12290;&#23545;&#20110;&#20960;&#20046;&#24517;&#28982;&#25910;&#25947;&#65292;&#20960;&#20046;&#19968;&#33268;&#24615;&#65292;Pare&#30340;&#29702;&#35770;&#20445;&#35777;//}
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13245v1 Announce Type: cross  Abstract: This paper considers the problem of learning a control policy for robot motion planning with zero-shot generalization, i.e., no data collection and policy adaptation is needed when the learned policy is deployed in new environments. We develop a federated reinforcement learning framework that enables collaborative learning of multiple learners and a central server, i.e., the Cloud, without sharing their raw data. In each iteration, each learner uploads its local control policy and the corresponding estimated normalized arrival time to the Cloud, which then computes the global optimum among the learners and broadcasts the optimal policy to the learners. Each learner then selects between its local control policy and that from the Cloud for next iteration. The proposed framework leverages on the derived zero-shot generalization guarantees on arrival time and safety. Theoretical guarantees on almost-sure convergence, almost consensus, Pare
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#19977;&#31181;&#19981;&#21516;&#26041;&#27861;&#39044;&#27979;&#20855;&#26377;&#30456;&#20114;&#20316;&#29992;&#32570;&#38519;&#31995;&#32479;&#30340;&#33258;&#30001;&#33021;&#21464;&#21270;&#65292;&#21457;&#29616;&#22242;&#31751;&#23637;&#24320;&#27169;&#22411;&#21487;&#20197;&#22312;&#26377;&#38480;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#31934;&#30830;&#30340;&#33021;&#37327;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.13243</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30456;&#20114;&#20316;&#29992;&#32570;&#38519;&#33021;&#37327;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study of Machine Learning Models Predicting Energetics of Interacting Defects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13243
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#19977;&#31181;&#19981;&#21516;&#26041;&#27861;&#39044;&#27979;&#20855;&#26377;&#30456;&#20114;&#20316;&#29992;&#32570;&#38519;&#31995;&#32479;&#30340;&#33258;&#30001;&#33021;&#21464;&#21270;&#65292;&#21457;&#29616;&#22242;&#31751;&#23637;&#24320;&#27169;&#22411;&#21487;&#20197;&#22312;&#26377;&#38480;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#31934;&#30830;&#30340;&#33021;&#37327;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#32570;&#38519;&#31995;&#32479;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#30340;&#26448;&#26009;&#20013;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#28982;&#32780;&#20174;&#35745;&#31639;&#35282;&#24230;&#29702;&#35299;&#36825;&#20123;&#31995;&#32479;&#30340;&#21407;&#23376;&#32423;&#21035;&#26426;&#21046;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#20197;&#36827;&#34892;&#36229;&#26230;&#32990;&#35745;&#31639;&#12290;&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21152;&#36895;&#26448;&#26009;&#27169;&#25311;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#28041;&#21450;&#30456;&#20114;&#20316;&#29992;&#32570;&#38519;&#30340;&#31995;&#32479;&#19978;&#30340;&#24212;&#29992;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#20855;&#26377;&#30456;&#20114;&#20316;&#29992;&#32570;&#38519;&#30340;&#31995;&#32479;&#30340;&#33258;&#30001;&#33021;&#21464;&#21270;&#12290;&#25105;&#20204;&#21033;&#29992;&#20174;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;(DFT)&#35745;&#31639;&#20013;&#24471;&#20986;&#30340;&#26377;&#38480;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20351;&#29992;&#26448;&#26009;&#25551;&#36848;&#31526;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22242;&#31751;&#23637;&#24320;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#36825;&#20010;&#26377;&#38480;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#22242;&#31751;&#23637;&#24320;&#27169;&#22411;&#20063;&#21487;&#20197;&#23454;&#29616;&#31934;&#30830;&#30340;&#33021;&#37327;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13243v1 Announce Type: cross  Abstract: Interacting defect systems are ubiquitous in materials under realistic scenarios, yet gaining an atomic-level understanding of these systems from a computational perspective is challenging - it often demands substantial resources due to the necessity of employing supercell calculations. While machine learning techniques have shown potential in accelerating materials simulations, their application to systems involving interacting defects remains relatively rare. In this work, we present a comparative study of three different methods to predict the free energy change of systems with interacting defects. We leveraging a limited dataset from Density Functional Theory(DFT) calculations to assess the performance models using materials descriptors, graph neural networks and cluster expansion. Our findings indicate that the cluster expansion model can achieve precise energetics predictions even with this limited dataset. Furthermore, with synt
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32593;&#32476;&#21442;&#25968;&#38468;&#21152;&#20998;&#35299;&#26469;&#35299;&#32806;&#24178;&#20928;&#25968;&#25454;&#21644;&#38169;&#35823;&#26631;&#35760;&#25968;&#25454;&#30340;&#35760;&#24518;&#65292;&#20174;&#32780;&#20943;&#23569;&#22024;&#26434;&#26631;&#31614;&#23545;&#28145;&#24230;&#32593;&#32476;&#35757;&#32451;&#30340;&#21103;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.13241</link><description>&lt;p&gt;
&#20351;&#29992;&#32593;&#32476;&#21442;&#25968;&#38468;&#21152;&#20998;&#35299;&#35299;&#20915;&#26377;&#22024;&#26434;&#26631;&#31614;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tackling Noisy Labels with Network Parameter Additive Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32593;&#32476;&#21442;&#25968;&#38468;&#21152;&#20998;&#35299;&#26469;&#35299;&#32806;&#24178;&#20928;&#25968;&#25454;&#21644;&#38169;&#35823;&#26631;&#35760;&#25968;&#25454;&#30340;&#35760;&#24518;&#65292;&#20174;&#32780;&#20943;&#23569;&#22024;&#26434;&#26631;&#31614;&#23545;&#28145;&#24230;&#32593;&#32476;&#35757;&#32451;&#30340;&#21103;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#21040;&#20855;&#26377;&#22024;&#26434;&#26631;&#31614;&#30340;&#25968;&#25454;&#65292;&#36807;&#21442;&#24615;&#28145;&#24230;&#32593;&#32476;&#20250;&#22240;&#20026;&#36807;&#24230;&#25311;&#21512;&#38169;&#35823;&#26631;&#35760;&#30340;&#25968;&#25454;&#32780;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#19981;&#20339;&#12290;&#28145;&#24230;&#32593;&#32476;&#30340;&#35760;&#24518;&#25928;&#24212;&#34920;&#26126;&#65292;&#23613;&#31649;&#32593;&#32476;&#33021;&#22815;&#35760;&#24518;&#25152;&#26377;&#22024;&#26434;&#25968;&#25454;&#65292;&#20294;&#23427;&#20204;&#39318;&#20808;&#20250;&#35760;&#24518;&#24178;&#20928;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#28982;&#21518;&#36880;&#28176;&#35760;&#24518;&#38169;&#35823;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#19968;&#31181;&#21033;&#29992;&#35760;&#24518;&#25928;&#24212;&#26469;&#23545;&#25239;&#22024;&#26434;&#26631;&#31614;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#26159;&#26089;&#20572;&#27490;&#12290;&#28982;&#32780;&#65292;&#26089;&#20572;&#27490;&#26080;&#27861;&#21306;&#20998;&#23545;&#24178;&#20928;&#25968;&#25454;&#21644;&#38169;&#35823;&#26631;&#35760;&#25968;&#25454;&#30340;&#35760;&#24518;&#65292;&#23548;&#33268;&#32593;&#32476;&#20173;&#28982;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#19981;&#21487;&#36991;&#20813;&#22320;&#36807;&#24230;&#25311;&#21512;&#38169;&#35823;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#35299;&#32806;&#24178;&#20928;&#25968;&#25454;&#21644;&#38169;&#35823;&#26631;&#35760;&#25968;&#25454;&#30340;&#35760;&#24518;&#65292;&#24182;&#36827;&#19968;&#27493;&#20943;&#23569;&#38169;&#35823;&#26631;&#35760;&#25968;&#25454;&#30340;&#21103;&#20316;&#29992;&#65292;&#25105;&#20204;&#23545;&#32593;&#32476;&#21442;&#25968;&#36827;&#34892;&#20102;&#38468;&#21152;&#20998;&#35299;&#12290;&#21363;&#65292;&#23558;&#25152;&#26377;&#21442;&#25968;&#20998;&#35299;&#20026;&#20004;&#32452;&#65292;&#21363;&#21442;&#25968; $\mathbf{w}$ &#34987;&#20998;&#24320;&#22987;&#35299;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13241v1 Announce Type: new  Abstract: Given data with noisy labels, over-parameterized deep networks suffer overfitting mislabeled data, resulting in poor generalization. The memorization effect of deep networks shows that although the networks have the ability to memorize all noisy data, they would first memorize clean training data, and then gradually memorize mislabeled training data. A simple and effective method that exploits the memorization effect to combat noisy labels is early stopping. However, early stopping cannot distinguish the memorization of clean data and mislabeled data, resulting in the network still inevitably overfitting mislabeled data in the early training stage.In this paper, to decouple the memorization of clean data and mislabeled data, and further reduce the side effect of mislabeled data, we perform additive decomposition on network parameters. Namely, all parameters are additively decomposed into two groups, i.e., parameters $\mathbf{w}$ are deco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#40657;&#21283;&#23376;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#23454;&#29616;&#22522;&#20110;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#20248;&#21270;&#32467;&#26500;&#21270;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.13219</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#40657;&#21283;&#23376;&#20248;&#21270;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model for Data-Driven Black-Box Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#40657;&#21283;&#23376;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#23454;&#29616;&#22522;&#20110;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#20248;&#21270;&#32467;&#26500;&#21270;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Generative AI&#37325;&#26032;&#23450;&#20041;&#20102;&#20154;&#24037;&#26234;&#33021;&#65292;&#23454;&#29616;&#20102;&#21019;&#26032;&#20869;&#23481;&#21644;&#23450;&#21046;&#35299;&#20915;&#26041;&#26696;&#30340;&#21019;&#24314;&#65292;&#25512;&#21160;&#19994;&#21153;&#23454;&#36341;&#36827;&#20837;&#25928;&#29575;&#21644;&#21019;&#36896;&#21147;&#26032;&#26102;&#20195;&#12290;&#26412;&#25991;&#20851;&#27880;&#25193;&#25955;&#27169;&#22411;&#65292;&#19968;&#31181;&#24378;&#22823;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#24182;&#30740;&#31350;&#20854;&#22312;&#22797;&#26434;&#32467;&#26500;&#21464;&#37327;&#19978;&#36827;&#34892;&#40657;&#21283;&#23376;&#20248;&#21270;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#23454;&#38469;&#26631;&#31614;&#31867;&#22411;&#65306;1&#65289;&#30495;&#23454;&#25968;&#20540;&#22870;&#21169;&#20989;&#25968;&#30340;&#22024;&#26434;&#27979;&#37327;&#21644;2&#65289;&#22522;&#20110;&#25104;&#23545;&#27604;&#36739;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#25509;&#36817;&#26368;&#20248;&#24182;&#20445;&#30041;&#35774;&#35745;&#30340;&#28508;&#22312;&#32467;&#26500;&#30340;&#26032;&#35774;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#35774;&#35745;&#20248;&#21270;&#38382;&#39064;&#37325;&#26032;&#21046;&#23450;&#20026;&#26465;&#20214;&#25277;&#26679;&#38382;&#39064;&#65292;&#36825;&#20351;&#25105;&#20204;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13219v1 Announce Type: new  Abstract: Generative AI has redefined artificial intelligence, enabling the creation of innovative content and customized solutions that drive business practices into a new era of efficiency and creativity. In this paper, we focus on diffusion models, a powerful generative AI technology, and investigate their potential for black-box optimization over complex structured variables. Consider the practical scenario where one wants to optimize some structured design in a high-dimensional space, based on massive unlabeled data (representing design variables) and a small labeled dataset. We study two practical types of labels: 1) noisy measurements of a real-valued reward function and 2) human preference based on pairwise comparisons. The goal is to generate new designs that are near-optimal and preserve the designed latent structures. Our proposed method reformulates the design optimization problem into a conditional sampling problem, which allows us to
&lt;/p&gt;</description></item><item><title>Nellie&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#12289;&#26080;&#20559;&#35265;&#30340;&#31649;&#36947;&#65292;&#29992;&#20110;&#22312;2D/3D&#27963;&#32454;&#32990;&#26174;&#24494;&#38236;&#19979;&#20998;&#21106;&#12289;&#36319;&#36394;&#21644;&#25552;&#21462;&#22810;&#26679;&#32454;&#32990;&#20869;&#32467;&#26500;&#29305;&#24449;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#20998;&#23618;&#20998;&#21106;&#21644;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#20998;&#26512;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.13214</link><description>&lt;p&gt;
Nellie&#65306;&#33258;&#21160;&#30340;2D/3D&#27963;&#32454;&#32990;&#26174;&#24494;&#38236;&#19979;&#22120;&#23448;&#20998;&#21106;&#12289;&#36319;&#36394;&#21644;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Nellie: Automated organelle segmentation, tracking, and hierarchical feature extraction in 2D/3D live-cell microscopy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13214
&lt;/p&gt;
&lt;p&gt;
Nellie&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#12289;&#26080;&#20559;&#35265;&#30340;&#31649;&#36947;&#65292;&#29992;&#20110;&#22312;2D/3D&#27963;&#32454;&#32990;&#26174;&#24494;&#38236;&#19979;&#20998;&#21106;&#12289;&#36319;&#36394;&#21644;&#25552;&#21462;&#22810;&#26679;&#32454;&#32990;&#20869;&#32467;&#26500;&#29305;&#24449;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#20998;&#23618;&#20998;&#21106;&#21644;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#20998;&#26512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#32454;&#32990;&#22120;&#30340;&#20998;&#26512;&#20173;&#28982;&#26159;&#19968;&#20010;&#20005;&#23803;&#30340;&#25361;&#25112;&#65292;&#20294;&#23545;&#20110;&#29702;&#35299;&#29983;&#29289;&#23398;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Nellie&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#19988;&#26080;&#20559;&#35265;&#30340;&#31649;&#36947;&#65292;&#29992;&#20110;&#20998;&#21106;&#12289;&#36319;&#36394;&#21644;&#25552;&#21462;&#22810;&#26679;&#30340;&#32454;&#32990;&#20869;&#32467;&#26500;&#29305;&#24449;&#12290;Nellie&#33021;&#22815;&#36866;&#24212;&#22270;&#20687;&#30340;&#20803;&#25968;&#25454;&#65292;&#28040;&#38500;&#20102;&#29992;&#25143;&#30340;&#36755;&#20837;&#12290;Nellie&#30340;&#39044;&#22788;&#29702;&#31649;&#36947;&#22312;&#22810;&#20010;&#32454;&#32990;&#20869;&#23610;&#24230;&#19978;&#22686;&#24378;&#20102;&#32467;&#26500;&#23545;&#27604;&#24230;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20122;&#22120;&#23448;&#21306;&#22495;&#30340;&#24378;&#22823;&#20998;&#23618;&#20998;&#21106;&#12290;&#36890;&#36807;&#21322;&#24452;&#33258;&#36866;&#24212;&#30340;&#27169;&#24335;&#21305;&#37197;&#26041;&#26696;&#29983;&#25104;&#21644;&#36319;&#36394;&#20869;&#37096;&#36816;&#21160;&#25429;&#25417;&#26631;&#35760;&#65292;&#24182;&#29992;&#20316;&#20122;&#20307;&#31215;&#27969;&#25554;&#20540;&#30340;&#25351;&#21335;&#12290;Nellie&#22312;&#22810;&#20010;&#20998;&#23618;&#27700;&#24179;&#25552;&#21462;&#22823;&#37327;&#29305;&#24449;&#65292;&#29992;&#20110;&#28145;&#24230;&#21644;&#21487;&#23450;&#21046;&#30340;&#20998;&#26512;&#12290;Nellie&#20855;&#26377;&#22522;&#20110;Napari&#30340;GUI&#65292;&#23454;&#29616;&#26080;&#20195;&#30721;&#25805;&#20316;&#21644;&#21487;&#35270;&#21270;&#65292;&#21516;&#26102;&#20854;&#27169;&#22359;&#21270;&#30340;&#24320;&#28304;&#20195;&#30721;&#24211;&#25552;&#20379;&#20102;&#32463;&#39564;&#20016;&#23500;&#29992;&#25143;&#30340;&#33258;&#23450;&#20041;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13214v1 Announce Type: cross  Abstract: The analysis of dynamic organelles remains a formidable challenge, though key to understanding biological processes. We introduce Nellie, an automated and unbiased pipeline for segmentation, tracking, and feature extraction of diverse intracellular structures. Nellie adapts to image metadata, eliminating user input. Nellie's preprocessing pipeline enhances structural contrast on multiple intracellular scales allowing for robust hierarchical segmentation of sub-organellar regions. Internal motion capture markers are generated and tracked via a radius-adaptive pattern matching scheme, and used as guides for sub-voxel flow interpolation. Nellie extracts a plethora of features at multiple hierarchical levels for deep and customizable analysis. Nellie features a Napari-based GUI that allows for code-free operation and visualization, while its modular open-source codebase invites customization by experienced users. We demonstrate Nellie's wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#38024;&#23545;&#34920;&#29616;&#24615;&#20260;&#23475;&#21644;&#26381;&#21153;&#36136;&#37327;&#20260;&#23475;&#30340;&#32650;&#39548;2&#23433;&#20840;&#20445;&#38556;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.13213</link><description>&lt;p&gt;
&#20174;&#34920;&#29616;&#24615;&#20260;&#23475;&#21040;&#26381;&#21153;&#36136;&#37327;&#20260;&#23475;:&#32650;&#39548;2&#23433;&#20840;&#20445;&#38556;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#38024;&#23545;&#34920;&#29616;&#24615;&#20260;&#23475;&#21644;&#26381;&#21153;&#36136;&#37327;&#20260;&#23475;&#30340;&#32650;&#39548;2&#23433;&#20840;&#20445;&#38556;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#23548;&#33268;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36827;&#27493;&#20063;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#20854;&#23545;&#24050;&#32463;&#36793;&#32536;&#21270;&#20154;&#32676;&#30340;&#19981;&#21033;&#24433;&#21709;&#30340;&#25285;&#24551;&#12290;&#23613;&#31649;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#20943;&#36731;&#25514;&#26045;&#26469;&#24320;&#21457;&#23433;&#20840;&#20445;&#38556;&#25514;&#26045;&#65292;&#27604;&#22914;&#30417;&#30563;&#24335;&#30340;&#23433;&#20840;&#23450;&#21521;&#24494;&#35843;&#21644;&#21033;&#29992;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65292;&#20294;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#20869;&#22312;&#20559;&#35265;&#20173;&#23384;&#22312;&#22810;&#37325;&#20851;&#27880;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#65292;&#20026;&#20102;&#23433;&#20840;&#32780;&#20248;&#21270;&#30340;&#27169;&#22411;&#36890;&#24120;&#20250;&#23637;&#31034;&#22840;&#22823;&#30340;&#23433;&#20840;&#34892;&#20026;&#65292;&#27604;&#22914;&#20986;&#20110;&#39044;&#38450;&#25514;&#26045;&#32780;&#20542;&#21521;&#20110;&#19981;&#22238;&#24212;&#26576;&#20123;&#35831;&#27714;&#12290;&#22240;&#27492;&#65292;&#25991;&#29486;&#20013;&#24050;&#32463;&#35760;&#24405;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26126;&#26174;&#26435;&#34913;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#23433;&#20840;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13213v1 Announce Type: cross  Abstract: Recent progress in large language models (LLMs) has led to their widespread adoption in various domains. However, these advancements have also introduced additional safety risks and raised concerns regarding their detrimental impact on already marginalized populations. Despite growing mitigation efforts to develop safety safeguards, such as supervised safety-oriented fine-tuning and leveraging safe reinforcement learning from human feedback, multiple concerns regarding the safety and ingrained biases in these models remain. Furthermore, previous work has demonstrated that models optimized for safety often display exaggerated safety behaviors, such as a tendency to refrain from responding to certain requests as a precautionary measure. As such, a clear trade-off between the helpfulness and safety of these models has been documented in the literature. In this paper, we further investigate the effectiveness of safety measures by evaluatin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20419;&#36827;&#28145;&#24230;&#38598;&#25104;&#20869;&#22810;&#26679;&#24615;&#21644;&#24179;&#22374;&#24615;&#30340;&#23398;&#20064;&#31639;&#27861;DASH&#65292;&#36890;&#36807;&#40723;&#21169;&#22522;&#30784;&#23398;&#20064;&#22120;&#21521;&#26368;&#23567;&#38160;&#24230;&#21306;&#22495;&#30340;&#20302;&#25439;&#22833;&#21306;&#22495;&#21457;&#25955;&#31227;&#21160;&#26469;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.13204</link><description>&lt;p&gt;
&#22810;&#26679;&#24615;&#24863;&#30693;&#30340;&#26080;&#20559;&#23567;&#20154;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diversity-Aware Agnostic Ensemble of Sharpness Minimizers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13204
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20419;&#36827;&#28145;&#24230;&#38598;&#25104;&#20869;&#22810;&#26679;&#24615;&#21644;&#24179;&#22374;&#24615;&#30340;&#23398;&#20064;&#31639;&#27861;DASH&#65292;&#36890;&#36807;&#40723;&#21169;&#22522;&#30784;&#23398;&#20064;&#22120;&#21521;&#26368;&#23567;&#38160;&#24230;&#21306;&#22495;&#30340;&#20302;&#25439;&#22833;&#21306;&#22495;&#21457;&#25955;&#31227;&#21160;&#26469;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#26377;&#22823;&#37327;&#29702;&#35770;&#21644;&#32463;&#39564;&#35777;&#25454;&#25903;&#25345;&#38598;&#25104;&#23398;&#20064;&#30340;&#25104;&#21151;&#12290;&#29305;&#21035;&#26159;&#28145;&#24230;&#38598;&#25104;&#21033;&#29992;&#35757;&#32451;&#20013;&#30340;&#38543;&#26426;&#24615;&#21644;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#29616;&#21147;&#65292;&#33719;&#24471;&#39044;&#27979;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#26368;&#32456;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#12289;&#40065;&#26834;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#22312;&#27867;&#21270;&#26041;&#38754;&#65292;&#21457;&#29616;&#36861;&#27714;&#26356;&#24191;&#27867;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#20250;&#23548;&#33268;&#27169;&#22411;&#23545;&#35757;&#32451;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#30340;&#36716;&#21464;&#26356;&#21152;&#40065;&#26834;&#12290;&#22522;&#20110;&#36825;&#20004;&#31181;&#26041;&#27861;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#30740;&#31350;&#38382;&#39064;&#26159;&#65292;&#22914;&#26524;&#38598;&#25104;&#23398;&#20064;&#21644;&#25439;&#22833;&#38160;&#24230;&#26368;&#23567;&#21270;&#30456;&#32467;&#21512;&#65292;&#26159;&#21542;&#21487;&#20197;&#23454;&#29616;&#27867;&#21270;&#33021;&#21147;&#30340;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#36825;&#31181;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20419;&#36827;&#28145;&#24230;&#38598;&#25104;&#20869;&#22810;&#26679;&#24615;&#21644;&#24179;&#22374;&#24615;&#30340;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;DASH&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;DASH&#40723;&#21169;&#22522;&#30784;&#23398;&#20064;&#22120;&#21521;&#26368;&#23567;&#38160;&#24230;&#21306;&#22495;&#30340;&#20302;&#25439;&#22833;&#21306;&#22495;&#21457;&#25955;&#31227;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13204v1 Announce Type: new  Abstract: There has long been plenty of theoretical and empirical evidence supporting the success of ensemble learning. Deep ensembles in particular take advantage of training randomness and expressivity of individual neural networks to gain prediction diversity, ultimately leading to better generalization, robustness and uncertainty estimation. In respect of generalization, it is found that pursuing wider local minima result in models being more robust to shifts between training and testing sets. A natural research question arises out of these two approaches as to whether a boost in generalization ability can be achieved if ensemble learning and loss sharpness minimization are integrated. Our work investigates this connection and proposes DASH - a learning algorithm that promotes diversity and flatness within deep ensembles. More concretely, DASH encourages base learners to move divergently towards low-loss regions of minimal sharpness. We provid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ADAPT&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;prompt&#35843;&#20248;&#33539;&#24335;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#65292;&#22686;&#24378;&#35270;&#35273;Transformer&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13196</link><description>&lt;p&gt;
&#20351;Prompt&#35843;&#20248;&#35270;&#35273;Transformer&#26356;&#20026;&#20581;&#22766;&#30340;ADAPT
&lt;/p&gt;
&lt;p&gt;
ADAPT to Robustify Prompt Tuning Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ADAPT&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;prompt&#35843;&#20248;&#33539;&#24335;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#65292;&#22686;&#24378;&#35270;&#35273;Transformer&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35270;&#35273;Transformer&#65292;&#24050;&#30693;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#35768;&#22810;&#29616;&#26377;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#65292;&#22914;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#20381;&#36182;&#20110;&#23545;&#25972;&#20010;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#24494;&#35843;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#36825;&#20123;&#38450;&#24481;&#26041;&#27861;&#38656;&#35201;&#20026;&#27599;&#20010;&#20219;&#21153;&#23384;&#20648;&#25972;&#20010;&#27169;&#22411;&#30340;&#21103;&#26412;&#65292;&#32780;&#27169;&#22411;&#21487;&#33021;&#21253;&#21547;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21442;&#25968;&#39640;&#25928;&#30340;prompt&#35843;&#20248;&#34987;&#29992;&#26469;&#36866;&#24212;&#22823;&#22411;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#65292;&#26080;&#38656;&#20445;&#23384;&#22823;&#22411;&#21103;&#26412;&#12290;&#26412;&#25991;&#20174;&#31283;&#20581;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#23545;&#35270;&#35273;Transformer&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#21442;&#25968;&#39640;&#25928;prompt&#35843;&#20248;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20043;&#21069;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#22312;&#24212;&#29992;&#21040;prompt&#35843;&#20248;&#33539;&#24335;&#26102;&#65292;&#23384;&#22312;&#26799;&#24230;&#27169;&#31946;&#24182;&#23481;&#26131;&#21463;&#21040;&#33258;&#36866;&#24212;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;ADAPT&#65292;&#19968;&#31181;&#22312;prompt&#35843;&#20248;&#33539;&#24335;&#20013;&#25191;&#34892;&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13196v1 Announce Type: new  Abstract: The performance of deep models, including Vision Transformers, is known to be vulnerable to adversarial attacks. Many existing defenses against these attacks, such as adversarial training, rely on full-model fine-tuning to induce robustness in the models. These defenses require storing a copy of the entire model, that can have billions of parameters, for each task. At the same time, parameter-efficient prompt tuning is used to adapt large transformer-based models to downstream tasks without the need to save large copies. In this paper, we examine parameter-efficient prompt tuning of Vision Transformers for downstream tasks under the lens of robustness. We show that previous adversarial defense methods, when applied to the prompt tuning paradigm, suffer from gradient obfuscation and are vulnerable to adaptive attacks. We introduce ADAPT, a novel framework for performing adaptive adversarial training in the prompt tuning paradigm. Our meth
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39640;&#20934;&#30830;&#24615;&#20294;&#20302;&#35299;&#37322;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;PSI-KT&#30340;&#20998;&#23618;&#29983;&#25104;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#20010;&#20307;&#35748;&#30693;&#29305;&#24449;&#21644;&#30693;&#35782;&#32467;&#26500;&#30340;&#35299;&#37322;&#65292;&#21516;&#26102;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#23454;&#29616;&#20102;&#23545;&#19981;&#26029;&#22686;&#38271;&#30340;&#23398;&#20064;&#32773;&#32676;&#20307;&#30340;&#39640;&#25928;&#20010;&#24615;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.13179</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#39046;&#22495;&#19978;&#30340;&#39044;&#27979;&#24615;&#12289;&#21487;&#20280;&#32553;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30693;&#35782;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
Predictive, scalable and interpretable knowledge tracing on structured domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39640;&#20934;&#30830;&#24615;&#20294;&#20302;&#35299;&#37322;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;PSI-KT&#30340;&#20998;&#23618;&#29983;&#25104;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#20010;&#20307;&#35748;&#30693;&#29305;&#24449;&#21644;&#30693;&#35782;&#32467;&#26500;&#30340;&#35299;&#37322;&#65292;&#21516;&#26102;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#23454;&#29616;&#20102;&#23545;&#19981;&#26029;&#22686;&#38271;&#30340;&#23398;&#20064;&#32773;&#32676;&#20307;&#30340;&#39640;&#25928;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#36890;&#36807;&#20248;&#21270;&#23398;&#20064;&#26448;&#26009;&#30340;&#36873;&#25321;&#21644;&#26102;&#38388;&#23433;&#25490;&#26469;&#22686;&#24378;&#29702;&#35299;&#21644;&#38271;&#26399;&#35760;&#24518;&#12290;&#36825;&#38656;&#35201;&#23545;&#23398;&#20064;&#32773;&#30340;&#36827;&#24230;&#65288;''&#30693;&#35782;&#36861;&#36394;''; KT&#65289;&#21644;&#23398;&#20064;&#39046;&#22495;&#30340;&#20808;&#20915;&#26465;&#20214;&#32467;&#26500;&#65288;''&#30693;&#35782;&#26144;&#23556;''&#65289;&#36827;&#34892;&#20272;&#35745;&#12290;&#22312;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#39640;KT&#20934;&#30830;&#24615;&#26159;&#21487;&#20197;&#23454;&#29616;&#30340;&#65292;&#20294;&#36825;&#26159;&#20197;&#29306;&#29298;&#24515;&#29702;&#21551;&#21457;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#20026;&#20195;&#20215;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#36825;&#31181;&#26435;&#34913;&#30340;&#26041;&#26696;&#12290;PSI-KT&#26159;&#19968;&#31181;&#20998;&#23618;&#29983;&#25104;&#26041;&#27861;&#65292;&#26126;&#30830;&#24314;&#27169;&#20102;&#20010;&#20307;&#35748;&#30693;&#29305;&#24449;&#21644;&#30693;&#35782;&#30340;&#20808;&#20915;&#32467;&#26500;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#21160;&#24577;&#65292;&#20174;&#32780;&#36890;&#36807;&#35774;&#35745;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;PSI-KT&#38024;&#23545;&#29616;&#23454;&#19990;&#30028;&#20013;&#23545;&#39640;&#25928;&#20010;&#24615;&#21270;&#30340;&#38656;&#27714;&#65292;&#21363;&#20351;&#26377;&#30528;&#19981;&#26029;&#22686;&#38271;&#30340;&#23398;&#20064;&#32773;&#32676;&#20307;&#21644;&#23398;&#20064;&#21382;&#21490;&#12290;&#22312;&#22312;&#32447;&#23398;&#20064;&#24179;&#21488;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13179v1 Announce Type: new  Abstract: Intelligent tutoring systems optimize the selection and timing of learning materials to enhance understanding and long-term retention. This requires estimates of both the learner's progress (''knowledge tracing''; KT), and the prerequisite structure of the learning domain (''knowledge mapping''). While recent deep learning models achieve high KT accuracy, they do so at the expense of the interpretability of psychologically-inspired models. In this work, we present a solution to this trade-off. PSI-KT is a hierarchical generative approach that explicitly models how both individual cognitive traits and the prerequisite structure of knowledge influence learning dynamics, thus achieving interpretability by design. Moreover, by using scalable Bayesian inference, PSI-KT targets the real-world need for efficient personalization even with a growing body of learners and learning histories. Evaluated on three datasets from online learning platform
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;Kalman&#28388;&#27874;&#33539;&#24335;&#30340;&#26032;&#39062;&#21644;&#21487;&#25193;&#23637;&#30340;&#37319;&#26679;&#31639;&#27861;LKTD&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#25277;&#21462;&#26679;&#26412;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#21518;&#39564;&#26679;&#26412;&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2403.13178</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#24555;&#36895;&#20215;&#20540;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Fast Value Tracking for Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13178
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;Kalman&#28388;&#27874;&#33539;&#24335;&#30340;&#26032;&#39062;&#21644;&#21487;&#25193;&#23637;&#30340;&#37319;&#26679;&#31639;&#27861;LKTD&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#25277;&#21462;&#26679;&#26412;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#21518;&#39564;&#26679;&#26412;&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36890;&#36807;&#21019;&#24314;&#19982;&#29615;&#22659;&#20114;&#21160;&#30340;Agent&#26469;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31639;&#27861;&#36890;&#24120;&#23558;&#36825;&#20123;&#38382;&#39064;&#35270;&#20026;&#38745;&#24577;&#38382;&#39064;&#65292;&#19987;&#27880;&#20110;&#27169;&#22411;&#21442;&#25968;&#30340;&#28857;&#20272;&#35745;&#20197;&#26368;&#22823;&#21270;&#39044;&#26399;&#22870;&#21169;&#65292;&#24573;&#35270;&#20102;Agent-Environment&#20114;&#21160;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#21345;&#23572;&#26364;&#28388;&#27874;&#33539;&#24335;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#21487;&#25193;&#23637;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#31216;&#20026;Langevinized Kalman Temporal-Difference&#65288;LKTD&#65289;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;&#36825;&#31181;&#31639;&#27861;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;SGMCMC&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#25277;&#21462;&#26679;&#26412;&#12290;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;LKTD&#31639;&#27861;&#29983;&#25104;&#30340;&#21518;&#39564;&#26679;&#26412;&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#20998;&#24067;&#12290;&#36825;&#31181;&#25910;&#25947;&#19981;&#20165;&#20351;&#25105;&#20204;&#33021;&#22815;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13178v1 Announce Type: cross  Abstract: Reinforcement learning (RL) tackles sequential decision-making problems by creating agents that interacts with their environment. However, existing algorithms often view these problem as static, focusing on point estimates for model parameters to maximize expected rewards, neglecting the stochastic dynamics of agent-environment interactions and the critical role of uncertainty quantification. Our research leverages the Kalman filtering paradigm to introduce a novel and scalable sampling algorithm called Langevinized Kalman Temporal-Difference (LKTD) for deep reinforcement learning. This algorithm, grounded in Stochastic Gradient Markov Chain Monte Carlo (SGMCMC), efficiently draws samples from the posterior distribution of deep neural network parameters. Under mild conditions, we prove that the posterior samples generated by the LKTD algorithm converge to a stationary distribution. This convergence not only enables us to quantify uncer
&lt;/p&gt;</description></item><item><title>Castor&#26159;&#19968;&#20010;&#21033;&#29992;&#24418;&#29366;&#23376;&#24207;&#21015;&#36716;&#25442;&#26102;&#38388;&#24207;&#21015;&#30340;&#31616;&#21333;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#20998;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#24418;&#29366;&#23376;&#24207;&#21015;&#30340;&#31454;&#20105;&#32452;&#32455;&#26500;&#24314;&#22810;&#26679;&#21270;&#29305;&#24449;&#34920;&#31034;&#65292;&#20135;&#29983;&#27604;&#20854;&#20182;&#29616;&#26377;&#20998;&#31867;&#22120;&#26356;&#20934;&#30830;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.13176</link><description>&lt;p&gt;
Castor: &#29992;&#20110;&#24555;&#36895;&#20934;&#30830;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#31454;&#20105;&#24418;&#29366;&#23376;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Castor: Competing shapelets for fast and accurate time series classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13176
&lt;/p&gt;
&lt;p&gt;
Castor&#26159;&#19968;&#20010;&#21033;&#29992;&#24418;&#29366;&#23376;&#24207;&#21015;&#36716;&#25442;&#26102;&#38388;&#24207;&#21015;&#30340;&#31616;&#21333;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#20998;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#24418;&#29366;&#23376;&#24207;&#21015;&#30340;&#31454;&#20105;&#32452;&#32455;&#26500;&#24314;&#22810;&#26679;&#21270;&#29305;&#24449;&#34920;&#31034;&#65292;&#20135;&#29983;&#27604;&#20854;&#20182;&#29616;&#26377;&#20998;&#31867;&#22120;&#26356;&#20934;&#30830;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Shapelets&#26159;&#19968;&#31181;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#23376;&#24207;&#21015;&#65292;&#26368;&#21021;&#23884;&#20837;&#22312;&#22522;&#20110;&#24418;&#29366;&#23376;&#26641;&#30340;&#20915;&#31574;&#26641;&#20013;&#65292;&#20294;&#21518;&#26469;&#34987;&#25193;&#23637;&#20026;&#22522;&#20110;&#24418;&#29366;&#23376;&#24207;&#21015;&#30340;&#36716;&#25442;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#12289;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#31639;&#27861;Castor&#65292;&#21033;&#29992;&#24418;&#29366;&#23376;&#24207;&#21015;&#26469;&#36716;&#25442;&#26102;&#38388;&#24207;&#21015;&#12290;&#35813;&#36716;&#25442;&#23558;&#24418;&#29366;&#23376;&#24207;&#21015;&#32452;&#32455;&#25104;&#20855;&#26377;&#19981;&#21516;&#25193;&#24352;&#30340;&#32452;&#65292;&#24182;&#20801;&#35768;&#24418;&#29366;&#23376;&#24207;&#21015;&#22312;&#26102;&#38388;&#19978;&#31454;&#20105;&#65292;&#20197;&#26500;&#24314;&#22810;&#26679;&#21270;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;&#36890;&#36807;&#23558;&#24418;&#29366;&#23376;&#24207;&#21015;&#32452;&#32455;&#25104;&#32452;&#65292;&#25105;&#20204;&#20351;&#36716;&#25442;&#33021;&#22815;&#22312;&#31454;&#20105;&#27700;&#24179;&#20043;&#38388;&#36807;&#28193;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#25509;&#36817;&#22522;&#20110;&#36317;&#31163;&#30340;&#36716;&#25442;&#25110;&#22522;&#20110;&#35789;&#20856;&#30340;&#36716;&#25442;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#65292;Castor&#20135;&#29983;&#30340;&#36716;&#25442;&#20351;&#20998;&#31867;&#22120;&#27604;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#26174;&#30528;&#26356;&#20934;&#30830;&#12290;&#22312;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13176v1 Announce Type: new  Abstract: Shapelets are discriminative subsequences, originally embedded in shapelet-based decision trees but have since been extended to shapelet-based transformations. We propose Castor, a simple, efficient, and accurate time series classification algorithm that utilizes shapelets to transform time series. The transformation organizes shapelets into groups with varying dilation and allows the shapelets to compete over the time context to construct a diverse feature representation. By organizing the shapelets into groups, we enable the transformation to transition between levels of competition, resulting in methods that more closely resemble distance-based transformations or dictionary-based transformations. We demonstrate, through an extensive empirical investigation, that Castor yields transformations that result in classifiers that are significantly more accurate than several state-of-the-art classifiers. In an extensive ablation study, we exa
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#21464;&#31181;&#22312;&#35782;&#21035;&#12289;&#25512;&#29702;&#21644;&#22522;&#20934;&#30830;&#23450;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24191;&#27867;&#33021;&#21147;&#21644;&#38480;&#21046;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;</title><link>https://arxiv.org/abs/2403.13164</link><description>&lt;p&gt;
VL-ICL Bench: &#22522;&#20110;&#32454;&#33410;&#30340;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#32454;&#33410;&#20043;&#39764;
&lt;/p&gt;
&lt;p&gt;
VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13164
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#21464;&#31181;&#22312;&#35782;&#21035;&#12289;&#25512;&#29702;&#21644;&#22522;&#20934;&#30830;&#23450;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24191;&#27867;&#33021;&#21147;&#21644;&#38480;&#21046;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#20854;&#33879;&#21517;&#30340;&#20986;&#29616;&#24335;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#32780;&#38395;&#21517;&#8212;&#8212;&#21363;&#22312;&#20165;&#25552;&#20379;&#20960;&#20010;&#31034;&#20363;&#20316;&#20026;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#26356;&#26032;&#27169;&#22411;&#30340;&#26435;&#37325;&#12290;&#26500;&#24314;&#22312;LLMs&#20043;&#19978;&#30340;&#35270;&#35273;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;VLLMs&#65289;&#22312;&#35782;&#21035;&#12289;&#25512;&#29702;&#21644;&#22522;&#20934;&#30830;&#23450;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;\emph{&#22810;&#27169;&#24577;ICL}&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23569;&#26679;&#26412;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#65288;VQA&#65289;&#21644;&#22270;&#20687;&#23383;&#24149;&#19978;&#65292;&#25105;&#20204;&#23558;&#23637;&#31034;&#20108;&#32773;&#26082;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;ICL&#30340;&#20248;&#21183;&#65292;&#20063;&#27809;&#26377;&#27979;&#35797;&#20854;&#38480;&#21046;&#12290;&#23545;&#22810;&#27169;&#24577;ICL&#30340;&#26356;&#24191;&#27867;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797; VL-ICL Bench&#65292;&#28085;&#30422;&#20102;&#28041;&#21450;&#22270;&#20687;&#21644;&#25991;&#26412;&#20316;&#20026;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#24191;&#27867;&#20219;&#21153;&#33539;&#22260;&#65292;&#24182;&#28085;&#30422;&#20102;&#20174;{&#24863;&#30693;&#21040;&#25512;&#29702;&#21644;&#38271;&#26399;&#19978;&#19979;&#25991;&#38271;&#24230;}&#30340;&#19981;&#21516;&#31867;&#22411;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13164v1 Announce Type: new  Abstract: Large language models (LLMs) famously exhibit emergent in-context learning (ICL) -- the ability to rapidly adapt to new tasks using few-shot examples provided as a prompt, without updating the model's weights. Built on top of LLMs, vision large language models (VLLMs) have advanced significantly in areas such as recognition, reasoning, and grounding. However, investigations into \emph{multimodal ICL} have predominantly focused on few-shot visual question answering (VQA), and image captioning, which we will show neither exploit the strengths of ICL, nor test its limitations. The broader capabilities and limitations of multimodal ICL remain under-explored. In this study, we introduce a comprehensive benchmark VL-ICL Bench for multimodal in-context learning, encompassing a broad spectrum of tasks that involve both images and text as inputs and outputs, and different types of challenges, from {perception to reasoning and long context length}
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35780;&#20998;&#35268;&#21017;&#35757;&#32451;&#29983;&#23384;&#27169;&#22411;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#27169;&#22411;&#31867;&#21035;&#20013;&#24182;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#20248;&#21270;&#20363;&#31243;&#65292;&#24182;&#23637;&#31034;&#20102;&#20248;&#20110;&#22522;&#20110;&#20284;&#28982;&#24615;&#26041;&#27861;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13150</link><description>&lt;p&gt;
&#20351;&#29992;&#35780;&#20998;&#35268;&#21017;&#35757;&#32451;&#29983;&#23384;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Survival Models using Scoring Rules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13150
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35780;&#20998;&#35268;&#21017;&#35757;&#32451;&#29983;&#23384;&#27169;&#22411;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#27169;&#22411;&#31867;&#21035;&#20013;&#24182;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#20248;&#21270;&#20363;&#31243;&#65292;&#24182;&#23637;&#31034;&#20102;&#20248;&#20110;&#22522;&#20110;&#20284;&#28982;&#24615;&#26041;&#27861;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#20026;&#21508;&#20010;&#39046;&#22495;&#20013;&#37096;&#20998;&#19981;&#23436;&#25972;&#30340;&#20107;&#20214;&#21457;&#29983;&#26102;&#38388;&#25968;&#25454;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;&#23427;&#20063;&#26159;&#27010;&#29575;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#31034;&#20363;&#12290;&#25105;&#20204;&#30340;&#25552;&#26696;&#20197;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#24335;&#21033;&#29992;&#20102;&#39044;&#27979;&#30340;&#27010;&#29575;&#24615;&#36136;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#25311;&#21512;&#36807;&#31243;&#20013;&#20351;&#29992;&#65288;&#21512;&#36866;&#30340;&#65289;&#35780;&#20998;&#35268;&#21017;&#32780;&#38750;&#22522;&#20110;&#20284;&#28982;&#24615;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19981;&#21516;&#30340;&#21442;&#25968;&#21270;&#21644;&#38750;&#21442;&#25968;&#21270;&#23376;&#26694;&#26550;&#65292;&#20801;&#35768;&#19981;&#21516;&#31243;&#24230;&#30340;&#28789;&#27963;&#24615;&#12290;&#23558;&#20854;&#28151;&#20837;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#23548;&#33268;&#20102;&#19968;&#20010;&#35745;&#31639;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#20248;&#21270;&#20363;&#31243;&#65292;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#24674;&#22797;&#21508;&#31181;&#21442;&#25968;&#21270;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#22312;&#19982;&#22522;&#20110;&#20284;&#28982;&#24615;&#26041;&#27861;&#30340;&#27604;&#36739;&#20013;&#65292;&#20248;&#21270;&#25928;&#26524;&#21516;&#26679;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13150v1 Announce Type: new  Abstract: Survival Analysis provides critical insights for partially incomplete time-to-event data in various domains. It is also an important example of probabilistic machine learning. The probabilistic nature of the predictions can be exploited by using (proper) scoring rules in the model fitting process instead of likelihood-based optimization. Our proposal does so in a generic manner and can be used for a variety of model classes. We establish different parametric and non-parametric sub-frameworks that allow different degrees of flexibility. Incorporated into neural networks, it leads to a computationally efficient and scalable optimization routine, yielding state-of-the-art predictive performance. Finally, we show that using our framework, we can recover various parametric models and demonstrate that optimization works equally well when compared to likelihood-based methods.
&lt;/p&gt;</description></item><item><title>SIFT-DBT&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#30417;&#30563;&#21021;&#22987;&#21270;&#21644;&#24494;&#35843;&#35299;&#20915;&#19981;&#24179;&#34913;&#25968;&#23383;&#20083;&#33146;&#26029;&#23618;&#25668;&#24433;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#30041;&#31354;&#38388;&#20998;&#36776;&#29575;&#65292;&#23454;&#29616;&#20102;92.69%&#30340;&#20307;&#31215;&#32423;AUC&#12290;</title><link>https://arxiv.org/abs/2403.13148</link><description>&lt;p&gt;
SIFT-DBT&#65306;&#29992;&#20110;&#19981;&#24179;&#34913;&#25968;&#23383;&#20083;&#33146;&#26029;&#23618;&#25668;&#24433;&#22270;&#20687;&#20998;&#31867;&#30340;&#33258;&#30417;&#30563;&#21021;&#22987;&#21270;&#21644;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
SIFT-DBT: Self-supervised Initialization and Fine-Tuning for Imbalanced Digital Breast Tomosynthesis Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13148
&lt;/p&gt;
&lt;p&gt;
SIFT-DBT&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#30417;&#30563;&#21021;&#22987;&#21270;&#21644;&#24494;&#35843;&#35299;&#20915;&#19981;&#24179;&#34913;&#25968;&#23383;&#20083;&#33146;&#26029;&#23618;&#25668;&#24433;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#30041;&#31354;&#38388;&#20998;&#36776;&#29575;&#65292;&#23454;&#29616;&#20102;92.69%&#30340;&#20307;&#31215;&#32423;AUC&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#20083;&#33146;&#26029;&#23618;&#25668;&#24433;&#65288;DBT&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#20083;&#33146;&#30284;&#31579;&#26597;&#21644;&#35786;&#26029;&#30340;&#21307;&#23398;&#25104;&#20687;&#27169;&#24335;&#65292;&#36890;&#36807;&#20854;3D&#26679;&#30340;&#20083;&#33146;&#25104;&#20687;&#33021;&#21147;&#25552;&#20379;&#26356;&#39640;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#21644;&#26356;&#22810;&#30340;&#32454;&#33410;&#12290;&#28982;&#32780;&#65292;&#22686;&#21152;&#30340;&#25968;&#25454;&#37327;&#20063;&#24341;&#20837;&#20102;&#26126;&#26174;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#25361;&#25112;&#65292;&#20854;&#20013;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#20307;&#31215;&#21253;&#21547;&#21487;&#30097;&#32452;&#32455;&#12290;&#36825;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#30001;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#26696;&#20363;&#32423;&#20998;&#24067;&#32780;&#23548;&#33268;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#65292;&#24182;&#23548;&#33268;&#23398;&#20064;&#19968;&#20010;&#20165;&#39044;&#27979;&#22810;&#25968;&#31867;&#30340;&#29712;&#30862;&#20998;&#31867;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#35270;&#22270;&#32423;&#23545;&#27604;&#30340;&#33258;&#30417;&#30563;&#21021;&#22987;&#21270;&#21644;&#24494;&#35843;&#26469;&#35782;&#21035;&#24322;&#24120;&#30340;DBT&#22270;&#20687;&#65292;&#21363;SIFT-DBT&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#32423;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#26469;&#20445;&#30041;&#31354;&#38388;&#20998;&#36776;&#29575;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35780;&#20272;970&#20010;&#21807;&#19968;&#30740;&#31350;&#26102;&#23454;&#29616;&#20102;92.69%&#30340;&#20307;&#31215;&#32423;AUC&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13148v1 Announce Type: cross  Abstract: Digital Breast Tomosynthesis (DBT) is a widely used medical imaging modality for breast cancer screening and diagnosis, offering higher spatial resolution and greater detail through its 3D-like breast volume imaging capability. However, the increased data volume also introduces pronounced data imbalance challenges, where only a small fraction of the volume contains suspicious tissue. This further exacerbates the data imbalance due to the case-level distribution in real-world data and leads to learning a trivial classification model that only predicts the majority class. To address this, we propose a novel method using view-level contrastive Self-supervised Initialization and Fine-Tuning for identifying abnormal DBT images, namely SIFT-DBT. We further introduce a patch-level multi-instance learning method to preserve spatial resolution. The proposed method achieves 92.69% volume-wise AUC on an evaluation of 970 unique studies.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#19968;&#33324;&#22810;&#21464;&#37327;&#20989;&#25968;&#20026;&#31616;&#21333;&#20989;&#25968;&#26641;&#30340;&#26041;&#27861;&#65292;&#35813;&#26641;&#33021;&#22815;&#24555;&#36895;&#35782;&#21035;&#21644;&#35745;&#31639;&#20989;&#25968;&#30340;&#20027;&#35201;&#21644;&#20132;&#20114;&#25928;&#24212;&#30452;&#33267;&#39640;&#38454;&#65292;&#20197;&#22270;&#24418;&#21270;&#26041;&#24335;&#23637;&#31034;&#28041;&#21450;&#21040;&#22235;&#20010;&#21464;&#37327;&#30340;&#20132;&#20114;&#25928;&#24212;&#12290;</title><link>https://arxiv.org/abs/2403.13141</link><description>&lt;p&gt;
Function Trees: &#36879;&#26126;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Function Trees: Transparent Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13141
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#19968;&#33324;&#22810;&#21464;&#37327;&#20989;&#25968;&#20026;&#31616;&#21333;&#20989;&#25968;&#26641;&#30340;&#26041;&#27861;&#65292;&#35813;&#26641;&#33021;&#22815;&#24555;&#36895;&#35782;&#21035;&#21644;&#35745;&#31639;&#20989;&#25968;&#30340;&#20027;&#35201;&#21644;&#20132;&#20114;&#25928;&#24212;&#30452;&#33267;&#39640;&#38454;&#65292;&#20197;&#22270;&#24418;&#21270;&#26041;&#24335;&#23637;&#31034;&#28041;&#21450;&#21040;&#22235;&#20010;&#21464;&#37327;&#30340;&#20132;&#20114;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#36755;&#20986;&#36890;&#24120;&#21487;&#20197;&#29992;&#20854;&#36755;&#20837;&#21464;&#37327;&#30340;&#19968;&#20010;&#25110;&#22810;&#20010;&#22810;&#21464;&#37327;&#20989;&#25968;&#34920;&#31034;&#12290;&#20102;&#35299;&#36825;&#31867;&#20989;&#25968;&#30340;&#20840;&#23616;&#29305;&#24615;&#26377;&#21161;&#20110;&#29702;&#35299;&#29983;&#25104;&#25968;&#25454;&#30340;&#31995;&#32479;&#65292;&#20197;&#21450;&#35299;&#37322;&#21644;&#38416;&#37322;&#30456;&#24212;&#30340;&#27169;&#22411;&#39044;&#27979;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#19968;&#33324;&#22810;&#21464;&#37327;&#20989;&#25968;&#34920;&#31034;&#20026;&#31616;&#21333;&#20989;&#25968;&#26641;&#30340;&#26041;&#27861;&#12290;&#36825;&#26869;&#26641;&#36890;&#36807;&#25581;&#31034;&#21644;&#25551;&#36848;&#20854;&#36755;&#20837;&#21464;&#37327;&#23376;&#38598;&#30340;&#32852;&#21512;&#24433;&#21709;&#65292;&#26469;&#26292;&#38706;&#20989;&#25968;&#30340;&#20840;&#23616;&#20869;&#37096;&#32467;&#26500;&#12290;&#26681;&#25454;&#36755;&#20837;&#21644;&#23545;&#24212;&#30340;&#20989;&#25968;&#20540;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#20197;&#24555;&#36895;&#35782;&#21035;&#21644;&#35745;&#31639;&#20989;&#25968;&#30340;&#25152;&#26377;&#20027;&#35201;&#21644;&#20132;&#20114;&#25928;&#24212;&#30452;&#33267;&#39640;&#38454;&#30340;&#20989;&#25968;&#26641;&#12290;&#28041;&#21450;&#21040;&#22235;&#20010;&#21464;&#37327;&#30340;&#20132;&#20114;&#25928;&#24212;&#36827;&#34892;&#20102;&#22270;&#24418;&#21270;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13141v1 Announce Type: cross  Abstract: The output of a machine learning algorithm can usually be represented by one or more multivariate functions of its input variables. Knowing the global properties of such functions can help in understanding the system that produced the data as well as interpreting and explaining corresponding model predictions. A method is presented for representing a general multivariate function as a tree of simpler functions. This tree exposes the global internal structure of the function by uncovering and describing the combined joint influences of subsets of its input variables. Given the inputs and corresponding function values, a function tree is constructed that can be used to rapidly identify and compute all of the function's main and interaction effects up to high order. Interaction effects involving up to four variables are graphically visualized.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#19981;&#21516;&#22797;&#26434;&#24230;&#27169;&#22411;&#30340;&#22810;&#20445;&#30495;&#24230;&#20195;&#29702;&#65292;&#26469;&#35299;&#20915;&#28608;&#20809;&#23450;&#21521;&#33021;&#37327;&#27785;&#31215;&#20013;&#29076;&#27744;&#20960;&#20309;&#24418;&#29366;&#39044;&#27979;&#20013;&#36755;&#20837;&#31354;&#38388;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.13136</link><description>&lt;p&gt;
&#22810;&#20445;&#30495;&#24230;&#30340;&#24322;&#26500;&#36755;&#20837;&#31354;&#38388;&#20195;&#29702;&#27169;&#22411;&#29992;&#20110;&#28608;&#20809;&#23450;&#21521;&#33021;&#37327;&#27785;&#31215;&#20013;&#29076;&#27744;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Multi-fidelity surrogate with heterogeneous input spaces for modeling melt pools in laser-directed energy deposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#19981;&#21516;&#22797;&#26434;&#24230;&#27169;&#22411;&#30340;&#22810;&#20445;&#30495;&#24230;&#20195;&#29702;&#65292;&#26469;&#35299;&#20915;&#28608;&#20809;&#23450;&#21521;&#33021;&#37327;&#27785;&#31215;&#20013;&#29076;&#27744;&#20960;&#20309;&#24418;&#29366;&#39044;&#27979;&#20013;&#36755;&#20837;&#31354;&#38388;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20445;&#30495;&#24230;&#65288;MF&#65289;&#24314;&#27169;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#26234;&#33021;&#22320;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#20445;&#30495;&#24230;&#26469;&#28304;&#30340;&#25968;&#25454;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#39044;&#27979;&#28608;&#20809;&#23450;&#21521;&#33021;&#37327;&#27785;&#31215;&#65288;L-DED&#65289;&#20013;&#29076;&#27744;&#20960;&#20309;&#24418;&#29366;&#26041;&#38754;&#26377;&#30528;&#24341;&#20154;&#27880;&#30446;&#30340;&#24212;&#29992;&#12290;&#20351;&#29992;MF&#20195;&#29702;&#21512;&#24182;&#19968;&#31995;&#21015;&#29076;&#27744;&#27169;&#22411;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#36755;&#20837;&#31354;&#38388;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#19981;&#21516;&#22797;&#26434;&#24230;&#27169;&#22411;&#30340;MF&#20195;&#29702;&#26469;&#39044;&#27979;&#29076;&#27744;&#20960;&#20309;&#24418;&#29366;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#36755;&#20837;&#31354;&#38388;&#19978;&#36816;&#34892;&#12290;&#31532;&#19968;&#20010;&#28909;&#27169;&#22411;&#21253;&#21547;&#20116;&#20010;&#36755;&#20837;&#21442;&#25968;&#65292;&#21363;&#28608;&#20809;&#21151;&#29575;&#12289;&#25195;&#25551;&#36895;&#24230;&#12289;&#31881;&#26411;&#27969;&#37327;&#12289;&#36733;&#27668;&#27969;&#37327;&#21644;&#21943;&#22068;&#39640;&#24230;&#12290;&#30456;&#21453;&#65292;&#31532;&#20108;&#20010;&#28909;&#27169;&#22411;&#21482;&#33021;&#22788;&#29702;&#28608;&#20809;&#21151;&#29575;&#21644;&#25195;&#25551;&#36895;&#24230;&#12290;&#22312;&#24322;&#26500;&#36755;&#20837;&#31354;&#38388;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#31181;&#26144;&#23556;&#65292;&#20197;&#20415;&#23558;&#20116;&#32500;&#31354;&#38388;&#21464;&#24418;&#25104;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13136v1 Announce Type: new  Abstract: Multi-fidelity (MF) modeling is a powerful statistical approach that can intelligently blend data from varied fidelity sources. This approach finds a compelling application in predicting melt pool geometry for laser-directed energy deposition (L-DED). One major challenge in using MF surrogates to merge a hierarchy of melt pool models is the variability in input spaces. To address this challenge, this paper introduces a novel approach for constructing an MF surrogate for predicting melt pool geometry by integrating models of varying complexity, that operate on heterogeneous input spaces. The first thermal model incorporates five input parameters i.e., laser power, scan velocity, powder flow rate, carrier gas flow rate, and nozzle height. In contrast, the second thermal model can only handle laser power and scan velocity. A mapping is established between the heterogeneous input spaces so that the five-dimensional space can be morphed into 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#24037;&#20316;&#27969;&#31243;&#65292;&#21033;&#29992;&#33258;&#21160;&#26631;&#27880; Sentinel-2 &#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#39068;&#33394;&#38408;&#20540;&#23454;&#29616;&#20102;&#26497;&#22320;&#28023;&#20912;&#30340;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13135</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#21160;&#26631;&#27880; Sentinel-2 &#22270;&#20687;&#30340;&#24182;&#34892;&#24037;&#20316;&#27969;&#31243;&#36827;&#34892;&#26497;&#22320;&#28023;&#20912;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
A Parallel Workflow for Polar Sea-Ice Classification using Auto-labeling of Sentinel-2 Imagery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13135
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#24037;&#20316;&#27969;&#31243;&#65292;&#21033;&#29992;&#33258;&#21160;&#26631;&#27880; Sentinel-2 &#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#39068;&#33394;&#38408;&#20540;&#23454;&#29616;&#20102;&#26497;&#22320;&#28023;&#20912;&#30340;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26497;&#22320;&#28023;&#20912;&#35206;&#30422;&#30340;&#25512;&#31227;&#21644;&#36864;&#32553;&#27169;&#24335;&#30340;&#35266;&#27979;&#26159;&#20840;&#29699;&#21464;&#26262;&#30340;&#37325;&#35201;&#25351;&#26631;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#31283;&#20581;&#12289;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992; Sentinel-2&#65288;S2&#65289;&#22270;&#20687;&#23545;&#26497;&#22320;&#28023;&#20912;&#36827;&#34892;&#20998;&#31867;&#65292;&#22914;&#21402;&#38634;&#35206;&#30422;&#12289;&#24180;&#36731;&#34180;&#20912;&#25110;&#38706;&#27700;&#12290;&#30001;&#20110;S2&#21355;&#26143;&#22312;&#22320;&#29699;&#34920;&#38754;&#31215;&#26497;&#25429;&#25417;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#26377;&#35768;&#22810;&#38656;&#35201;&#20998;&#31867;&#30340;&#22270;&#20687;&#12290;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#32570;&#20047;&#26631;&#35760;&#30340;S2&#35757;&#32451;&#25968;&#25454;&#65288;&#22270;&#20687;&#65289;&#20316;&#20026;&#22522;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#20934;&#30830;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31934;&#24515;&#30830;&#23450;&#30340;&#39068;&#33394;&#38408;&#20540;&#23545;S2&#22270;&#20687;&#36827;&#34892;&#20998;&#21106;&#21644;&#33258;&#21160;&#26631;&#27880;&#12290;&#25105;&#20204;&#20351;&#29992; PySpark&#20013;&#30340;&#24182;&#34892;&#24037;&#20316;&#27969;&#31243;&#65292;&#22312;&#22522;&#20110;&#34180;&#20113;&#21644;&#38452;&#24433;&#36807;&#28388;&#30340;&#22522;&#20110;&#39068;&#33394;&#30340;&#20998;&#21106;&#30340;&#22522;&#30784;&#19978;&#65292;&#23454;&#29616;&#20102;9&#20493;&#25968;&#25454;&#21152;&#36733;&#21644;16&#20493; MapReduce &#21152;&#36895;&#65292;&#29992;&#20110;&#33258;&#21160;&#26631;&#27880; S2 &#22270;&#20687;&#20197;&#29983;&#25104;&#26631;&#31614;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13135v1 Announce Type: cross  Abstract: The observation of the advancing and retreating pattern of polar sea ice cover stands as a vital indicator of global warming. This research aims to develop a robust, effective, and scalable system for classifying polar sea ice as thick/snow-covered, young/thin, or open water using Sentinel-2 (S2) images. Since the S2 satellite is actively capturing high-resolution imagery over the earth's surface, there are lots of images that need to be classified. One major obstacle is the absence of labeled S2 training data (images) to act as the ground truth. We demonstrate a scalable and accurate method for segmenting and automatically labeling S2 images using carefully determined color thresholds. We employ a parallel workflow using PySpark to scale and achieve 9-fold data loading and 16-fold map-reduce speedup on auto-labeling S2 images based on thin cloud and shadow-filtered color-based segmentation to generate label data. The auto-labeled data
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#23545;&#25239;&#35757;&#32451;&#19979;&#30340;&#40065;&#26834;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#27867;&#21270;&#29702;&#35770;&#65292;&#26377;&#26395;&#26497;&#22823;&#22320;&#25512;&#21160;NAS&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.13134</link><description>&lt;p&gt;
&#38024;&#23545;&#23545;&#25239;&#35757;&#32451;&#30340;&#40065;&#26834;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#65306;&#22522;&#20934;&#12289;&#29702;&#35770;&#21450;&#20854;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Robust NAS under adversarial training: benchmark, theory, and beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13134
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#23545;&#25239;&#35757;&#32451;&#19979;&#30340;&#40065;&#26834;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#27867;&#21270;&#29702;&#35770;&#65292;&#26377;&#26395;&#26497;&#22823;&#22320;&#25512;&#21160;NAS&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#30340;&#21457;&#23637;&#24378;&#35843;&#32771;&#34385;&#38024;&#23545;&#24694;&#24847;&#25968;&#25454;&#30340;&#40065;&#26834;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#25628;&#32034;&#36825;&#20123;&#40065;&#26834;&#32467;&#26500;&#26102;&#65292;&#22312;&#32771;&#34385;&#23545;&#25239;&#35757;&#32451;&#26102;&#23384;&#22312;&#30528;&#26126;&#26174;&#30340;&#32570;&#20047;&#22522;&#20934;&#35780;&#20272;&#21644;&#29702;&#35770;&#20445;&#35777;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#20570;&#20986;&#21452;&#37325;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;NAS-Bench-201&#25628;&#32034;&#31354;&#38388;&#30340;&#24191;&#27867;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#32463;&#36807;&#23545;&#25239;&#35757;&#32451;&#30340;&#32593;&#32476;&#30340;&#24178;&#20928;&#31934;&#24230;&#21644;&#40065;&#26834;&#31934;&#24230;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#31070;&#32463;&#20999;&#21521;&#26680;(NTK)&#24037;&#20855;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22810;&#30446;&#26631;&#23545;&#25239;&#35757;&#32451;&#19979;&#25628;&#32034;&#32467;&#26500;&#30340;&#27867;&#21270;&#29702;&#35770;&#65292;&#20197;&#24178;&#20928;&#31934;&#24230;&#21644;&#40065;&#26834;&#31934;&#24230;&#20316;&#20026;&#32771;&#37327;&#12290;&#25105;&#20204;&#22362;&#20449;&#25105;&#20204;&#30340;&#22522;&#20934;&#21644;&#29702;&#35770;&#35265;&#35299;&#23558;&#26497;&#22823;&#22320;&#26377;&#30410;&#20110;NAS&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13134v1 Announce Type: new  Abstract: Recent developments in neural architecture search (NAS) emphasize the significance of considering robust architectures against malicious data. However, there is a notable absence of benchmark evaluations and theoretical guarantees for searching these robust architectures, especially when adversarial training is considered. In this work, we aim to address these two challenges, making twofold contributions. First, we release a comprehensive data set that encompasses both clean accuracy and robust accuracy for a vast array of adversarially trained networks from the NAS-Bench-201 search space on image datasets. Then, leveraging the neural tangent kernel (NTK) tool from deep learning theory, we establish a generalization theory for searching architecture in terms of clean accuracy and robust accuracy under multi-objective adversarial training. We firmly believe that our benchmark and theoretical insights will significantly benefit the NAS com
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#29983;&#25104;&#33021;&#21147;&#26469;&#26500;&#24314;&#33258;&#29983;&#25104;&#22238;&#25918;&#35760;&#24518;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13130</link><description>&lt;p&gt;
&#33258;&#29983;&#25104;&#30340;&#22238;&#25918;&#35760;&#24518;&#23545;&#25345;&#32493;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Self-generated Replay Memories for Continual Neural Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13130
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#29983;&#25104;&#33021;&#21147;&#26469;&#26500;&#24314;&#33258;&#29983;&#25104;&#22238;&#25918;&#35760;&#24518;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#22312;&#22810;&#31181;&#19981;&#21516;&#35821;&#35328;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#26029;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#25345;&#32493;&#23398;&#20064;&#30340;&#33021;&#21147;&#20173;&#28982;&#21463;&#21040;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#30340;&#20005;&#37325;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#21363;&#20854;&#29983;&#25104;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#25345;&#32493;&#23398;&#20064;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#33258;&#36523;&#20316;&#20026;&#24182;&#34892;&#21477;&#23376;&#29983;&#25104;&#22120;&#26469;&#26377;&#25928;&#22320;&#23398;&#20064;&#30001;&#19981;&#21516;&#35821;&#35328;&#32452;&#25104;&#30340;&#32463;&#39564;&#27969;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25269;&#28040;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#35760;&#24518;&#35757;&#32451;&#25968;&#25454;&#12290;&#20195;&#30721;&#23558;&#22312;&#21457;&#34920;&#21518;&#20844;&#24320;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13130v1 Announce Type: new  Abstract: Modern Neural Machine Translation systems exhibit strong performance in several different languages and are constantly improving. Their ability to learn continuously is, however, still severely limited by the catastrophic forgetting issue. In this work, we leverage a key property of encoder-decoder Transformers, i.e. their generative ability, to propose a novel approach to continually learning Neural Machine Translation systems. We show how this can effectively learn on a stream of experiences comprising different languages, by leveraging a replay memory populated by using the model itself as a generator of parallel sentences. We empirically demonstrate that our approach can counteract catastrophic forgetting without requiring explicit memorization of training data. Code will be publicly available upon publication. Code: https://github.com/m-resta/sg-rep
&lt;/p&gt;</description></item><item><title>AdaFish &#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#31532;&#20108;&#38454;&#31639;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20302;&#38454;&#20998;&#35299;&#26469;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#65292;&#25552;&#39640;&#20302;&#31209;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#25928;&#29575;</title><link>https://arxiv.org/abs/2403.13128</link><description>&lt;p&gt;
AdaFish: &#21033;&#29992;&#20108;&#38454;&#20449;&#24687;&#36827;&#34892;&#24555;&#36895;&#30340;&#20302;&#31209;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
AdaFish: Fast low-rank parameter-efficient fine-tuning by using second-order information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13128
&lt;/p&gt;
&lt;p&gt;
AdaFish &#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#31532;&#20108;&#38454;&#31639;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20302;&#38454;&#20998;&#35299;&#26469;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#65292;&#25552;&#39640;&#20302;&#31209;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13128v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#26368;&#36817;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#21508;&#31181;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20013;&#24222;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#38656;&#35201;&#22823;&#37327;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#23436;&#25972;&#35757;&#32451;&#12290;&#20026;&#20102;&#35843;&#25972;&#36825;&#20123;&#27169;&#22411;&#20197;&#36866;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#25110;&#29305;&#23450;&#24212;&#29992;&#23548;&#21521;&#30340;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#21442;&#25968;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21442;&#25968;&#21644;&#26102;&#20195;&#22826;&#22810;&#65292;&#36825;&#20173;&#28982;&#21487;&#33021;&#32791;&#26102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;AdaFish&#65292;&#19968;&#31181;&#25928;&#29575;&#39640;&#30340;&#31532;&#20108;&#38454;&#31867;&#22411;&#31639;&#27861;&#65292;&#26088;&#22312;&#22312;&#22522;&#20110;&#20302;&#31209;&#20998;&#35299;&#30340;&#24494;&#35843;&#26694;&#26550;&#20869;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13128v1 Announce Type: new  Abstract: Recent advancements in large-scale pretrained models have significantly improved performance across a variety of tasks in natural language processing and computer vision. However, the extensive number of parameters in these models necessitates substantial memory and computational resources for full training. To adapt these models for downstream tasks or specific application-oriented datasets, parameter-efficient fine-tuning methods leveraging pretrained parameters have gained considerable attention. However, it can still be time-consuming due to lots of parameters and epochs. In this work, we introduce AdaFish, an efficient algorithm of the second-order type designed to expedite the training process within low-rank decomposition-based fine-tuning frameworks. Our key observation is that the associated generalized Fisher information matrix is either low-rank or extremely small-scaled. Such a generalized Fisher information matrix is shown t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20984;&#20248;&#21270;&#26377;&#25928;&#22320;&#23558;&#27010;&#29575;&#21629;&#39064;&#36923;&#36753;&#32422;&#26463;&#38598;&#25104;&#21040;&#27010;&#29575;&#30005;&#36335;&#20013;&#30340;&#26041;&#27861;&#65292;&#36825;&#21487;&#20197;&#22312;&#19981;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#65292;&#36825;&#31181;&#32452;&#21512;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#22312;&#31232;&#30095;&#25110;&#19981;&#23436;&#25972;&#25968;&#25454;&#19979;&#30340;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#25514;&#26045;&#12290;</title><link>https://arxiv.org/abs/2403.13125</link><description>&lt;p&gt;
&#36890;&#36807;&#20984;&#20248;&#21270;&#23454;&#29616;&#24102;&#32422;&#26463;&#30340;&#27010;&#29575;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Circuits with Constraints via Convex Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13125
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20984;&#20248;&#21270;&#26377;&#25928;&#22320;&#23558;&#27010;&#29575;&#21629;&#39064;&#36923;&#36753;&#32422;&#26463;&#38598;&#25104;&#21040;&#27010;&#29575;&#30005;&#36335;&#20013;&#30340;&#26041;&#27861;&#65292;&#36825;&#21487;&#20197;&#22312;&#19981;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#65292;&#36825;&#31181;&#32452;&#21512;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#22312;&#31232;&#30095;&#25110;&#19981;&#23436;&#25972;&#25968;&#25454;&#19979;&#30340;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#35299;&#20915;&#20102;&#23558;&#27010;&#29575;&#21629;&#39064;&#36923;&#36753;&#32422;&#26463;&#38598;&#25104;&#21040;&#30001;&#27010;&#29575;&#30005;&#36335;&#65288;PC&#65289;&#32534;&#30721;&#30340;&#20998;&#24067;&#20013;&#30340;&#38382;&#39064;&#12290;PC&#26159;&#19968;&#31867;&#21487;&#35745;&#31639;&#30340;&#27169;&#22411;&#65292;&#20801;&#35768;&#36827;&#34892;&#39640;&#25928;&#35745;&#31639;&#65288;&#22914;&#26465;&#20214;&#27010;&#29575;&#21644;&#36793;&#32536;&#27010;&#29575;&#65289;&#65292;&#21516;&#26102;&#22312;&#26576;&#20123;&#39046;&#22495;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;PC&#21644;&#32422;&#26463;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#28385;&#36275;&#32422;&#26463;&#30340;&#26032;PC&#12290;&#36825;&#36890;&#36807;&#20984;&#20248;&#21270;&#26377;&#25928;&#22320;&#23436;&#25104;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#32422;&#26463;&#21644;PC&#30340;&#32452;&#21512;&#21487;&#20197;&#20855;&#26377;&#22810;&#31181;&#29992;&#36884;&#65292;&#21253;&#25324;&#22312;&#31232;&#30095;&#25110;&#19981;&#23436;&#25972;&#25968;&#25454;&#19979;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#65292;&#20197;&#21450;&#22312;&#19981;&#24433;&#21709;&#27169;&#22411;&#36866;&#24212;&#24230;&#30340;&#24773;&#20917;&#19979;&#23558;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#25514;&#26045;&#32435;&#20837;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#20123;&#24819;&#27861;&#23558;&#20026;&#28041;&#21450;&#23558;&#23545;&#25968;&#32467;&#21512;&#30340;&#22810;&#20010;&#20854;&#20182;&#24212;&#29992;&#24320;&#36767;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13125v1 Announce Type: new  Abstract: This work addresses integrating probabilistic propositional logic constraints into the distribution encoded by a probabilistic circuit (PC). PCs are a class of tractable models that allow efficient computations (such as conditional and marginal probabilities) while achieving state-of-the-art performance in some domains. The proposed approach takes both a PC and constraints as inputs, and outputs a new PC that satisfies the constraints. This is done efficiently via convex optimization without the need to retrain the entire model. Empirical evaluations indicate that the combination of constraints and PCs can have multiple use cases, including the improvement of model performance under scarce or incomplete data, as well as the enforcement of machine learning fairness measures into the model without compromising model fitness. We believe that these ideas will open possibilities for multiple other applications involving the combination of log
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20803;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#26032;&#22411;&#27169;&#24577;&#20998;&#26512;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#31232;&#30095;&#26102;&#38388;&#19981;&#35268;&#21017;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.13118</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20803;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#23545;&#26102;&#31354;&#25968;&#25454;&#36827;&#34892;&#27169;&#24577;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Modal Analysis of Spatiotemporal Data via Multivariate Gaussian Process Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13118
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20803;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#26032;&#22411;&#27169;&#24577;&#20998;&#26512;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#31232;&#30095;&#26102;&#38388;&#19981;&#35268;&#21017;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#24577;&#20998;&#26512;&#24050;&#25104;&#20026;&#29702;&#35299;&#22797;&#26434;&#27969;&#20307;&#30340;&#19968;&#31181;&#37325;&#35201;&#24037;&#20855;&#12290;&#20256;&#32479;&#30340;&#27169;&#24577;&#20998;&#26512;&#26041;&#27861;&#65292;&#22914;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#65288;DMD&#65289;&#21644;&#35889;Proper Orthogonal Decomposition&#65288;SPOD&#65289;&#65292;&#20381;&#36182;&#20110;&#22312;&#26102;&#38388;&#19978;&#23450;&#26399;&#21462;&#26679;&#30340;&#20805;&#20998;&#25968;&#25454;&#37327;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#38656;&#35201;&#22788;&#29702;&#31232;&#30095;&#30340;&#26102;&#38388;&#19981;&#35268;&#21017;&#25968;&#25454;&#65292;&#20363;&#22914;&#30001;&#20110;&#23454;&#39564;&#27979;&#37327;&#21644;&#20223;&#30495;&#31639;&#27861;&#12290;&#20026;&#20102;&#20811;&#26381;&#25968;&#25454;&#31232;&#32570;&#21644;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20803;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65288;MVGPR&#65289;&#30340;&#26032;&#22411;&#27169;&#24577;&#20998;&#26512;&#25216;&#26415;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#32447;&#24615;&#31995;&#32479;&#35782;&#21035;&#30340;&#35282;&#24230;&#24314;&#31435;&#20102;MVGPR&#19982;&#29616;&#26377;&#27169;&#24577;&#20998;&#26512;&#25216;&#26415;DMD&#21644;SPOD&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25509;&#19979;&#26469;&#65292;&#21033;&#29992;&#36825;&#31181;&#32852;&#31995;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;MVGPR&#30340;&#27169;&#24577;&#20998;&#26512;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#21069;&#36848;&#30340;&#38480;&#21046;&#12290;MVGPR&#30340;&#21151;&#33021;&#26159;&#36890;&#36807;&#20854;&#35880;&#24910;&#30340;&#21028;&#26029;&#33021;&#21147;&#36171;&#20104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13118v1 Announce Type: cross  Abstract: Modal analysis has become an essential tool to understand the coherent structure of complex flows. The classical modal analysis methods, such as dynamic mode decomposition (DMD) and spectral proper orthogonal decomposition (SPOD), rely on a sufficient amount of data that is regularly sampled in time. However, often one needs to deal with sparse temporally irregular data, e.g., due to experimental measurements and simulation algorithm. To overcome the limitations of data scarcity and irregular sampling, we propose a novel modal analysis technique using multi-variate Gaussian process regression (MVGPR). We first establish the connection between MVGPR and the existing modal analysis techniques, DMD and SPOD, from a linear system identification perspective. Next, leveraging this connection, we develop a MVGPR-based modal analysis technique that addresses the aforementioned limitations. The capability of MVGPR is endowed by its judiciously 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#20248;&#27969;&#21305;&#37197;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19968;&#27493;&#20013;&#23398;&#20064;&#23454;&#29616;&#20108;&#27425;&#25104;&#26412;&#19979;&#30340;&#30452;&#32447; OT &#20301;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.13117</link><description>&lt;p&gt;
&#26368;&#20248;&#27969;&#21305;&#37197;&#65306;&#22312;&#19968;&#27493;&#20013;&#23398;&#20064;&#30452;&#32447;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Optimal Flow Matching: Learning Straight Trajectories in Just One Step
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13117
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#20248;&#27969;&#21305;&#37197;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19968;&#27493;&#20013;&#23398;&#20064;&#23454;&#29616;&#20108;&#27425;&#25104;&#26412;&#19979;&#30340;&#30452;&#32447; OT &#20301;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#27969;&#21305;&#37197;&#26041;&#27861;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#24471;&#21040;&#20102;&#34028;&#21187;&#21457;&#23637;&#12290;&#31038;&#21306;&#36861;&#27714;&#30340;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#23646;&#24615;&#26159;&#33021;&#22815;&#23398;&#20064;&#20855;&#26377;&#30452;&#32447;&#36712;&#36857;&#30340;&#27969;&#65292;&#36825;&#20123;&#36712;&#36857;&#23454;&#29616;&#20102;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#32622;&#25442;&#12290;&#30452;&#32447;&#24615;&#23545;&#20110;&#24555;&#36895;&#38598;&#25104;&#23398;&#20064;&#27969;&#30340;&#36335;&#24452;&#33267;&#20851;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#27969;&#30452;&#32447;&#21270;&#26041;&#27861;&#37117;&#22522;&#20110;&#38750;&#24179;&#20961;&#30340;&#36845;&#20195;&#36807;&#31243;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#31215;&#32047;&#35823;&#24046;&#25110;&#21033;&#29992;&#21551;&#21457;&#24335;&#23567;&#25209;&#37327;OT&#36817;&#20284;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#20248;&#27969;&#21305;&#37197;&#26041;&#27861;&#65292;&#20165;&#36890;&#36807;&#19968;&#27425;&#27969;&#21305;&#37197;&#27493;&#39588;&#21363;&#21487;&#20026;&#20108;&#27425;&#25104;&#26412;&#24674;&#22797;&#30452;&#32447;OT&#32622;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13117v1 Announce Type: cross  Abstract: Over the several recent years, there has been a boom in development of flow matching methods for generative modeling. One intriguing property pursued by the community is the ability to learn flows with straight trajectories which realize the optimal transport (OT) displacements. Straightness is crucial for fast integration of the learned flow's paths. Unfortunately, most existing flow straightening methods are based on non-trivial iterative procedures which accumulate the error during training or exploit heuristic minibatch OT approximations. To address this issue, we develop a novel optimal flow matching approach which recovers the straight OT displacement for the quadratic cost in just one flow matching step.
&lt;/p&gt;</description></item><item><title>&#35813;&#33539;&#22260;&#23457;&#38405;&#20840;&#38754;&#23457;&#26597;&#20102;&#21307;&#30103;&#39044;&#27979;&#38382;&#39064;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#26631;&#31614;&#22122;&#22768;&#31649;&#29702;&#65292;&#21253;&#25324;&#26631;&#31614;&#22122;&#22768;&#26816;&#27979;&#12289;&#26631;&#31614;&#22122;&#22768;&#22788;&#29702;&#21644;&#35780;&#20272;&#65292;&#21516;&#26102;&#20063;&#21253;&#25324;&#30740;&#31350;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13111</link><description>&lt;p&gt;
&#21307;&#30103;&#39044;&#27979;&#38382;&#39064;&#20013;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#28145;&#24230;&#23398;&#20064;&#65306;&#19968;&#39033;&#33539;&#22260;&#23457;&#38405;
&lt;/p&gt;
&lt;p&gt;
Deep learning with noisy labels in medical prediction problems: a scoping review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13111
&lt;/p&gt;
&lt;p&gt;
&#35813;&#33539;&#22260;&#23457;&#38405;&#20840;&#38754;&#23457;&#26597;&#20102;&#21307;&#30103;&#39044;&#27979;&#38382;&#39064;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#26631;&#31614;&#22122;&#22768;&#31649;&#29702;&#65292;&#21253;&#25324;&#26631;&#31614;&#22122;&#22768;&#26816;&#27979;&#12289;&#26631;&#31614;&#22122;&#22768;&#22788;&#29702;&#21644;&#35780;&#20272;&#65292;&#21516;&#26102;&#20063;&#21253;&#25324;&#30740;&#31350;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#21307;&#30103;&#30740;&#31350;&#38754;&#20020;&#26469;&#33258;&#22240;&#32032;&#22914;&#19987;&#23478;&#38388;&#21464;&#24322;&#21644;&#26426;&#22120;&#25552;&#21462;&#26631;&#31614;&#31561;&#22122;&#22768;&#26631;&#31614;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22122;&#22768;&#26631;&#31614;&#31649;&#29702;&#30340;&#37319;&#29992;&#20173;&#28982;&#26377;&#38480;&#65292;&#22122;&#22768;&#26631;&#31614;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#35270;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#36827;&#34892;&#19968;&#39033;&#20391;&#37325;&#20110;&#38382;&#39064;&#31354;&#38388;&#30340;&#33539;&#22260;&#23457;&#38405;&#12290;&#26412;&#33539;&#22260;&#23457;&#38405;&#26088;&#22312;&#20840;&#38754;&#23457;&#26597;&#28145;&#24230;&#23398;&#20064;&#21307;&#30103;&#39044;&#27979;&#38382;&#39064;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#31649;&#29702;&#65292;&#20854;&#20013;&#21253;&#25324;&#26631;&#31614;&#22122;&#22768;&#26816;&#27979;&#12289;&#26631;&#31614;&#22122;&#22768;&#22788;&#29702;&#21644;&#35780;&#20272;&#12290;&#36824;&#21253;&#25324;&#28041;&#21450;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#30340;&#30740;&#31350;&#12290; &#26041;&#27861;&#65306;&#25105;&#20204;&#30340;&#33539;&#22260;&#23457;&#38405;&#36981;&#24490;&#39318;&#36873;&#25253;&#21578;&#39033;&#20026;&#31995;&#32479;&#24615;&#23457;&#26597;&#21644;Meta&#20998;&#26512;&#65288;PRISMA&#65289;&#25351;&#21335;&#12290;&#25105;&#20204;&#25628;&#32034;&#20102;4&#20010;&#25968;&#25454;&#24211;&#65292;&#21253;&#25324;PubMed&#12289;IEEE Xplore&#12289;Google Scholar&#21644;Semantic Scholar&#12290;&#25105;&#20204;&#30340;&#25628;&#32034;&#35789;&#21253;&#25324;&#8220;noisy label AND medical/healthcare/clinical&#8221;&#65292;&#8220;uncertainty AND medical/healthcare/clinical&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13111v1 Announce Type: new  Abstract: Objectives: Medical research faces substantial challenges from noisy labels attributed to factors like inter-expert variability and machine-extracted labels. Despite this, the adoption of label noise management remains limited, and label noise is largely ignored. To this end, there is a critical need to conduct a scoping review focusing on the problem space. This scoping review aims to comprehensively review label noise management in deep learning-based medical prediction problems, which includes label noise detection, label noise handling, and evaluation. Research involving label uncertainty is also included.   Methods: Our scoping review follows the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. We searched 4 databases, including PubMed, IEEE Xplore, Google Scholar, and Semantic Scholar. Our search terms include "noisy label AND medical / healthcare / clinical", "un-certainty AND medical / healt
&lt;/p&gt;</description></item><item><title>PSO-Fed&#31639;&#27861;&#30340;&#37096;&#20998;&#20849;&#20139;&#26426;&#21046;&#19981;&#20165;&#21487;&#20197;&#38477;&#20302;&#36890;&#20449;&#36127;&#36733;&#65292;&#36824;&#33021;&#22686;&#24378;&#31639;&#27861;&#23545;&#27169;&#22411;&#25237;&#27602;&#25915;&#20987;&#30340;&#25269;&#25239;&#21147;&#65292;&#24182;&#19988;&#22312;&#38754;&#23545;&#25308;&#21344;&#24237;&#23458;&#25143;&#31471;&#30340;&#24773;&#20917;&#19979;&#20381;&#28982;&#33021;&#20445;&#25345;&#25910;&#25947;&#12290;</title><link>https://arxiv.org/abs/2403.13108</link><description>&lt;p&gt;
&#20998;&#26512;&#37096;&#20998;&#20849;&#20139;&#23545;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#25269;&#25239;&#27169;&#22411;&#25237;&#27602;&#25915;&#20987;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Impact of Partial Sharing on the Resilience of Online Federated Learning Against Model Poisoning Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13108
&lt;/p&gt;
&lt;p&gt;
PSO-Fed&#31639;&#27861;&#30340;&#37096;&#20998;&#20849;&#20139;&#26426;&#21046;&#19981;&#20165;&#21487;&#20197;&#38477;&#20302;&#36890;&#20449;&#36127;&#36733;&#65292;&#36824;&#33021;&#22686;&#24378;&#31639;&#27861;&#23545;&#27169;&#22411;&#25237;&#27602;&#25915;&#20987;&#30340;&#25269;&#25239;&#21147;&#65292;&#24182;&#19988;&#22312;&#38754;&#23545;&#25308;&#21344;&#24237;&#23458;&#25143;&#31471;&#30340;&#24773;&#20917;&#19979;&#20381;&#28982;&#33021;&#20445;&#25345;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23457;&#26597;&#20102;&#37096;&#20998;&#20849;&#20139;&#30340;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#65288;PSO-Fed&#65289;&#31639;&#27861;&#23545;&#25269;&#25239;&#27169;&#22411;&#25237;&#27602;&#25915;&#20987;&#30340;&#38887;&#24615;&#12290; PSO-Fed&#36890;&#36807;&#20351;&#23458;&#25143;&#31471;&#22312;&#27599;&#20010;&#26356;&#26032;&#36718;&#27425;&#20165;&#19982;&#26381;&#21153;&#22120;&#20132;&#25442;&#37096;&#20998;&#27169;&#22411;&#20272;&#35745;&#26469;&#20943;&#23569;&#36890;&#20449;&#36127;&#36733;&#12290;&#27169;&#22411;&#20272;&#35745;&#30340;&#37096;&#20998;&#20849;&#20139;&#36824;&#22686;&#24378;&#20102;&#31639;&#27861;&#23545;&#27169;&#22411;&#25237;&#27602;&#25915;&#20987;&#30340;&#24378;&#24230;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;PSO-Fed&#31639;&#27861;&#22312;&#23384;&#22312;&#25308;&#21344;&#24237;&#23458;&#25143;&#31471;&#30340;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#36825;&#20123;&#23458;&#25143;&#31471;&#21487;&#33021;&#20250;&#22312;&#19982;&#26381;&#21153;&#22120;&#20849;&#20139;&#20043;&#21069;&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#36731;&#24494;&#31713;&#25913;&#20854;&#26412;&#22320;&#27169;&#22411;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PSO-Fed&#22312;&#22343;&#20540;&#21644;&#22343;&#26041;&#24847;&#20041;&#19978;&#37117;&#33021;&#20445;&#25345;&#25910;&#25947;&#65292;&#21363;&#20351;&#22312;&#27169;&#22411;&#25237;&#27602;&#25915;&#20987;&#30340;&#21387;&#21147;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25512;&#23548;&#20102;PSO-Fed&#30340;&#29702;&#35770;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#65292;&#23558;&#20854;&#19982;&#27493;&#38271;&#12289;&#25915;&#20987;&#27010;&#29575;&#12289;&#25968;&#23383;&#31561;&#21508;&#31181;&#21442;&#25968;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13108v1 Announce Type: new  Abstract: We scrutinize the resilience of the partial-sharing online federated learning (PSO-Fed) algorithm against model-poisoning attacks. PSO-Fed reduces the communication load by enabling clients to exchange only a fraction of their model estimates with the server at each update round. Partial sharing of model estimates also enhances the robustness of the algorithm against model-poisoning attacks. To gain better insights into this phenomenon, we analyze the performance of the PSO-Fed algorithm in the presence of Byzantine clients, malicious actors who may subtly tamper with their local models by adding noise before sharing them with the server. Through our analysis, we demonstrate that PSO-Fed maintains convergence in both mean and mean-square senses, even under the strain of model-poisoning attacks. We further derive the theoretical mean square error (MSE) of PSO-Fed, linking it to various parameters such as stepsize, attack probability, numb
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#32423;&#24635;&#32467;&#27861;&#23545;&#27861;&#24459;&#25991;&#26412;&#36827;&#34892;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;F1&#20998;&#25968;&#30340;&#26174;&#33879;&#25552;&#21319;</title><link>https://arxiv.org/abs/2403.13107</link><description>&lt;p&gt;
&#38754;&#21521;&#27861;&#24459;&#25991;&#26412;&#30340;&#22810;&#32423;&#24635;&#32467;&#26080;&#30417;&#30563;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards Unsupervised Question Answering System with Multi-level Summarization for Legal Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13107
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#32423;&#24635;&#32467;&#27861;&#23545;&#27861;&#24459;&#25991;&#26412;&#36827;&#34892;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;F1&#20998;&#25968;&#30340;&#26174;&#33879;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#22242;&#38431;SCaLAR&#22312;SemEval-2024&#20219;&#21153;5&#19978;&#30340;&#24037;&#20316;&#65306;&#27665;&#20107;&#31243;&#24207;&#20013;&#30340;&#27861;&#24459;&#35770;&#35777;&#25512;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#65292;&#30001;&#20110;&#28041;&#21450;&#21040;&#30340;&#27861;&#24459;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#32780;&#20196;&#20154;&#26395;&#32780;&#21364;&#27493;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#21448;&#26032;&#39062;&#30340;&#22522;&#20110;&#30456;&#20284;&#24230;&#21644;&#36317;&#31163;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#26469;&#29983;&#25104;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#38598;&#25104;&#29305;&#24449;&#65288;&#21253;&#25324;CNN&#12289;GRU&#21644;LSTM&#65289;&#30340;&#22810;&#32423;Legal-Bert&#23884;&#20837;&#30340;&#34701;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#38598;&#20013;&#27861;&#24459;&#35299;&#37322;&#30340;&#20887;&#38271;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;T5&#30340;&#20998;&#27573;&#25688;&#35201;&#65292;&#25104;&#21151;&#22320;&#20445;&#30041;&#20102;&#20851;&#38190;&#20449;&#24687;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26080;&#30417;&#30563;&#31995;&#32479;&#22312;&#24320;&#21457;&#38598;&#19978;&#35265;&#35777;&#20102;macro F1&#20998;&#25968;&#22686;&#21152;&#20102;20&#20010;&#30334;&#20998;&#28857;&#65292;&#22312;&#27979;&#35797;&#38598;&#19978;&#22686;&#21152;&#20102;10&#20010;&#30334;&#20998;&#28857;&#65292;&#32771;&#34385;&#21040;&#20854;&#31616;&#21333;&#30340;&#26550;&#26500;&#65292;&#36825;&#26159;&#20196;&#20154;&#40723;&#33310;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13107v1 Announce Type: new  Abstract: This paper summarizes Team SCaLAR's work on SemEval-2024 Task 5: Legal Argument Reasoning in Civil Procedure. To address this Binary Classification task, which was daunting due to the complexity of the Legal Texts involved, we propose a simple yet novel similarity and distance-based unsupervised approach to generate labels. Further, we explore the Multi-level fusion of Legal-Bert embeddings using ensemble features, including CNN, GRU, and LSTM. To address the lengthy nature of Legal explanation in the dataset, we introduce T5-based segment-wise summarization, which successfully retained crucial information, enhancing the model's performance. Our unsupervised system witnessed a 20-point increase in macro F1-score on the development set and a 10-point increase on the test set, which is promising given its uncomplicated architecture.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;Shapley Taylor&#20114;&#21160;&#25351;&#25968;&#65288;STII&#65289;&#20998;&#26512;&#20102;&#24213;&#23618;&#25968;&#25454;&#32467;&#26500;&#23545;&#21508;&#31181;&#27169;&#24577;&#12289;&#20219;&#21153;&#21644;&#26550;&#26500;&#20013;&#27169;&#22411;&#34920;&#24449;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20102;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#38899;&#27169;&#22411;&#20013;&#30340;&#26032;&#39062;&#29616;&#35937;&#65292;&#24182;&#23637;&#31034;&#20102;&#29305;&#24449;&#20132;&#20114;&#22914;&#20309;&#30452;&#35266;&#21453;&#26144;&#23545;&#35937;&#36793;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.13106</link><description>&lt;p&gt;
&#35748;&#35782;&#20320;&#30340;&#38750;&#32447;&#24615;&#65306;Shapley&#20114;&#21160;&#25581;&#31034;&#25968;&#25454;&#30340;&#28508;&#22312;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Knowing Your Nonlinearities: Shapley Interactions Reveal the Underlying Structure of Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;Shapley Taylor&#20114;&#21160;&#25351;&#25968;&#65288;STII&#65289;&#20998;&#26512;&#20102;&#24213;&#23618;&#25968;&#25454;&#32467;&#26500;&#23545;&#21508;&#31181;&#27169;&#24577;&#12289;&#20219;&#21153;&#21644;&#26550;&#26500;&#20013;&#27169;&#22411;&#34920;&#24449;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20102;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#38899;&#27169;&#22411;&#20013;&#30340;&#26032;&#39062;&#29616;&#35937;&#65292;&#24182;&#23637;&#31034;&#20102;&#29305;&#24449;&#20132;&#20114;&#22914;&#20309;&#30452;&#35266;&#21453;&#26144;&#23545;&#35937;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#37327;&#38750;&#32447;&#24615;&#29305;&#24449;&#20132;&#20114;&#26159;&#29702;&#35299;&#35768;&#22810;&#27169;&#22411;&#20013;&#22797;&#26434;&#24402;&#22240;&#27169;&#24335;&#30340;&#19968;&#31181;&#24050;&#24314;&#31435;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20351;&#29992;Shapley Taylor&#20114;&#21160;&#25351;&#25968;&#65288;STII&#65289;&#26469;&#20998;&#26512;&#24213;&#23618;&#25968;&#25454;&#32467;&#26500;&#23545;&#22810;&#31181;&#27169;&#24577;&#12289;&#20219;&#21153;&#21644;&#26550;&#26500;&#20013;&#27169;&#22411;&#34920;&#24449;&#30340;&#24433;&#21709;&#12290;&#22312;&#32771;&#34385;&#25513;&#30721;&#21644;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65288;MLMs&#21644;ALMs&#65289;&#20013;&#30340;&#35821;&#35328;&#32467;&#26500;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;STII&#22312;&#24815;&#29992;&#34920;&#36798;&#20013;&#22686;&#21152;&#65292;MLMs&#38543;&#21477;&#27861;&#36317;&#31163;&#25193;&#23637;STII&#65292;&#26356;&#22810;&#22320;&#20381;&#36182;&#35821;&#27861;&#22312;&#20854;&#38750;&#32447;&#24615;&#32467;&#26500;&#20013;&#30456;&#27604;ALMs&#12290;&#25105;&#20204;&#30340;&#35821;&#38899;&#27169;&#22411;&#30740;&#31350;&#21453;&#26144;&#20102;&#21475;&#33108;&#24352;&#24320;&#31243;&#24230;&#20915;&#23450;&#38899;&#32032;&#26681;&#25454;&#19978;&#19979;&#25991;&#21464;&#21270;&#30340;&#25968;&#37327;&#30340;&#21407;&#21017;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#22270;&#20687;&#20998;&#31867;&#22120;&#24182;&#35828;&#26126;&#29305;&#24449;&#20132;&#20114;&#30452;&#35266;&#21453;&#26144;&#23545;&#35937;&#36793;&#30028;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#36328;&#23398;&#31185;&#24037;&#20316;&#21644;&#39046;&#22495;&#20043;&#38388;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13106v1 Announce Type: cross  Abstract: Measuring nonlinear feature interaction is an established approach to understanding complex patterns of attribution in many models. In this paper, we use Shapley Taylor interaction indices (STII) to analyze the impact of underlying data structure on model representations in a variety of modalities, tasks, and architectures. Considering linguistic structure in masked and auto-regressive language models (MLMs and ALMs), we find that STII increases within idiomatic expressions and that MLMs scale STII with syntactic distance, relying more on syntax in their nonlinear structure than ALMs do. Our speech model findings reflect the phonetic principal that the openness of the oral cavity determines how much a phoneme varies based on its context. Finally, we study image classifiers and illustrate that feature interactions intuitively reflect object boundaries. Our wide range of results illustrates the benefits of interdisciplinary work and doma
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;AdaptSFL&#33258;&#36866;&#24212;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#21152;&#36895;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#31995;&#32479;&#20013;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13101</link><description>&lt;p&gt;
AdaptSFL&#65306;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#32593;&#32476;&#20013;&#30340;&#33258;&#36866;&#24212;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AdaptSFL: Adaptive Split Federated Learning in Resource-constrained Edge Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13101
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;AdaptSFL&#33258;&#36866;&#24212;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#21152;&#36895;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#31995;&#32479;&#20013;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26085;&#30410;&#22797;&#26434;&#20351;&#24471;&#23558;&#20854;&#27665;&#20027;&#21270;&#21040;&#36164;&#28304;&#26377;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#38754;&#20020;&#37325;&#35201;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#36890;&#36807;&#27169;&#22411;&#20998;&#21306;&#23558;&#20027;&#35201;&#35757;&#32451;&#24037;&#20316;&#36127;&#33655;&#36716;&#31227;&#21040;&#26381;&#21153;&#22120;&#19978;&#65292;&#24182;&#22312;&#36793;&#32536;&#35774;&#22791;&#20043;&#38388;&#23454;&#29616;&#24182;&#34892;&#35757;&#32451;&#30340;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#31995;&#32479;&#20248;&#21270;&#26497;&#22823;&#22320;&#24433;&#21709;&#20102;&#36164;&#28304;&#21463;&#38480;&#31995;&#32479;&#19979;SFL&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#20010;&#38382;&#39064;&#20173;&#28982;&#24456;&#22823;&#31243;&#24230;&#19978;&#27809;&#26377;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;SFL&#30340;&#25910;&#25947;&#20998;&#26512;&#65292;&#37327;&#21270;&#20102;&#27169;&#22411;&#20998;&#21106;&#65288;MS&#65289;&#21644;&#23458;&#25143;&#31471;&#27169;&#22411;&#32858;&#21512;&#65288;MA&#65289;&#23545;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20316;&#20026;&#29702;&#35770;&#22522;&#30784;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AdaptSFL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36164;&#28304;&#33258;&#36866;&#24212;SFL&#26694;&#26550;&#65292;&#20197;&#21152;&#36895;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#19979;&#30340;SFL&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;AdaptSFL&#33258;&#36866;&#24212;&#22320;&#25511;&#21046;&#23458;&#25143;&#31471;MA&#21644;MS&#65292;&#20197;&#24179;&#34913;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13101v1 Announce Type: new  Abstract: The increasing complexity of deep neural networks poses significant barriers to democratizing them to resource-limited edge devices. To address this challenge, split federated learning (SFL) has emerged as a promising solution by of floading the primary training workload to a server via model partitioning while enabling parallel training among edge devices. However, although system optimization substantially influences the performance of SFL under resource-constrained systems, the problem remains largely uncharted. In this paper, we provide a convergence analysis of SFL which quantifies the impact of model splitting (MS) and client-side model aggregation (MA) on the learning performance, serving as a theoretical foundation. Then, we propose AdaptSFL, a novel resource-adaptive SFL framework, to expedite SFL under resource-constrained edge computing systems. Specifically, AdaptSFL adaptively controls client-side MA and MS to balance commun
&lt;/p&gt;</description></item><item><title>&#35268;&#27169;&#26159;&#24433;&#21709;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#31616;&#21333;&#26041;&#27861;&#22914;AWAC&#21644;IQL&#36890;&#36807;&#22686;&#21152;&#32593;&#32476;&#22823;&#23567;&#20811;&#26381;&#20102;&#22312;&#22810;&#28304;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#22833;&#36133;&#27169;&#24335;&#65292;&#24182;&#22312;D4RL&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.13097</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#31616;&#21333;&#35201;&#32032;
&lt;/p&gt;
&lt;p&gt;
Simple Ingredients for Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13097
&lt;/p&gt;
&lt;p&gt;
&#35268;&#27169;&#26159;&#24433;&#21709;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#31616;&#21333;&#26041;&#27861;&#22914;AWAC&#21644;IQL&#36890;&#36807;&#22686;&#21152;&#32593;&#32476;&#22823;&#23567;&#20811;&#26381;&#20102;&#22312;&#22810;&#28304;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#22833;&#36133;&#27169;&#24335;&#65292;&#24182;&#22312;D4RL&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#19982;&#30446;&#26631;&#19979;&#28216;&#20219;&#21153;&#39640;&#24230;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#19978;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#20010;&#26032;&#30340;&#27979;&#35797;&#24179;&#21488;&#65288;MOOD&#65289;&#20013;&#65292;&#36712;&#36857;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#65292;&#25105;&#20204;&#23637;&#31034;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#22810;&#26679;&#21270;&#25968;&#25454;&#26102;&#23384;&#22312;&#22256;&#38590;&#65306;&#24403;&#20026;&#30456;&#20851;&#20294;&#19981;&#21516;&#30340;&#20219;&#21153;&#25910;&#38598;&#30340;&#25968;&#25454;&#34987;&#31616;&#21333;&#28155;&#21152;&#21040;&#31163;&#32447;&#32531;&#20914;&#21306;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20250;&#26126;&#26174;&#24694;&#21270;&#12290;&#37492;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#22823;&#22411;&#23454;&#35777;&#30740;&#31350;&#65292;&#21046;&#23450;&#24182;&#27979;&#35797;&#20102;&#20960;&#20010;&#20551;&#35774;&#20197;&#35299;&#37322;&#36825;&#31181;&#22833;&#36133;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#35268;&#27169;&#65292;&#32780;&#19981;&#26159;&#31639;&#27861;&#32771;&#34385;&#65292;&#26159;&#24433;&#21709;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20687;AWAC&#21644;IQL&#36825;&#26679;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#32593;&#32476;&#22823;&#23567;&#65292;&#20811;&#26381;&#20102;&#22312;MOOD&#20013;&#21253;&#21547;&#39069;&#22806;&#25968;&#25454;&#24341;&#36215;&#30340;&#30683;&#30462;&#25925;&#38556;&#27169;&#24335;&#65292;&#24182;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#22312;&#32463;&#20856;&#30340;D4RL&#22522;&#20934;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13097v1 Announce Type: new  Abstract: Offline reinforcement learning algorithms have proven effective on datasets highly connected to the target downstream task. Yet, leveraging a novel testbed (MOOD) in which trajectories come from heterogeneous sources, we show that existing methods struggle with diverse data: their performance considerably deteriorates as data collected for related but different tasks is simply added to the offline buffer. In light of this finding, we conduct a large empirical study where we formulate and test several hypotheses to explain this failure. Surprisingly, we find that scale, more than algorithmic considerations, is the key factor influencing performance. We show that simple methods like AWAC and IQL with increased network size overcome the paradoxical failure modes from the inclusion of additional data in MOOD, and notably outperform prior state-of-the-art algorithms on the canonical D4RL benchmark.
&lt;/p&gt;</description></item><item><title>JaxUED&#26159;&#19968;&#20010;&#22312;Jax&#20013;&#25552;&#20379;&#29616;&#20195;UED&#31639;&#27861;&#26368;&#23567;&#20381;&#36182;&#23454;&#29616;&#30340;&#24320;&#28304;&#24211;&#65292;&#21033;&#29992;&#30828;&#20214;&#21152;&#36895;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#26088;&#22312;&#21152;&#36895;&#23545;UED&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.13091</link><description>&lt;p&gt;
JaxUED: Jax&#20013;&#31616;&#21333;&#26131;&#29992;&#30340;UED&#24211;
&lt;/p&gt;
&lt;p&gt;
JaxUED: A simple and useable UED library in Jax
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13091
&lt;/p&gt;
&lt;p&gt;
JaxUED&#26159;&#19968;&#20010;&#22312;Jax&#20013;&#25552;&#20379;&#29616;&#20195;UED&#31639;&#27861;&#26368;&#23567;&#20381;&#36182;&#23454;&#29616;&#30340;&#24320;&#28304;&#24211;&#65292;&#21033;&#29992;&#30828;&#20214;&#21152;&#36895;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#26088;&#22312;&#21152;&#36895;&#23545;UED&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;JaxUED&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#22312;Jax&#20013;&#25552;&#20379;&#29616;&#20195;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65288;UED&#65289;&#31639;&#27861;&#30340;&#26368;&#23567;&#20381;&#36182;&#23454;&#29616;&#12290;JaxUED&#21033;&#29992;&#30828;&#20214;&#21152;&#36895;&#65292;&#30456;&#27604;&#20043;&#21069;&#22522;&#20110;CPU&#30340;&#23454;&#29616;&#65292;&#33719;&#24471;&#20102;100&#20493;&#24038;&#21491;&#30340;&#21152;&#36895;&#12290;&#21463;CleanRL&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20379;&#24555;&#36895;&#12289;&#28165;&#26224;&#12289;&#26131;&#20110;&#29702;&#35299;&#21644;&#26131;&#20110;&#20462;&#25913;&#30340;&#23454;&#29616;&#65292;&#26088;&#22312;&#21152;&#36895;&#23545;UED&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#30340;&#24211;&#24182;&#21253;&#21547;&#22522;&#20934;&#32467;&#26524;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/DramaCow/jaxued &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13091v1 Announce Type: new  Abstract: We present JaxUED, an open-source library providing minimal dependency implementations of modern Unsupervised Environment Design (UED) algorithms in Jax. JaxUED leverages hardware acceleration to obtain on the order of 100x speedups compared to prior, CPU-based implementations. Inspired by CleanRL, we provide fast, clear, understandable, and easily modifiable implementations, with the aim of accelerating research into UED. This paper describes our library and contains baseline results. Code can be found at https://github.com/DramaCow/jaxued.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Listenable Maps for Audio Classifiers (L-MAC)&#30340;&#21487;&#21548;&#22270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#24544;&#23454;&#19988;&#21487;&#21548;&#30340;&#38899;&#39057;&#20998;&#31867;&#22120;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.13086</link><description>&lt;p&gt;
&#21487;&#21548;&#22270;&#29992;&#20110;&#38899;&#39057;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Listenable Maps for Audio Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13086
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Listenable Maps for Audio Classifiers (L-MAC)&#30340;&#21487;&#21548;&#22270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#24544;&#23454;&#19988;&#21487;&#21548;&#30340;&#38899;&#39057;&#20998;&#31867;&#22120;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20854;&#22797;&#26434;&#24615;&#32473;&#35299;&#37322;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#36825;&#19968;&#25361;&#25112;&#22312;&#38899;&#39057;&#20449;&#21495;&#20013;&#23588;&#20026;&#26126;&#26174;&#65292;&#20256;&#36798;&#35299;&#37322;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#38899;&#39057;&#20998;&#31867;&#22120;&#30340;&#21487;&#21548;&#22270;&#65288;Listenable Maps for Audio Classifiers&#65292;L-MAC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29983;&#25104;&#24544;&#23454;&#19988;&#21487;&#21548;&#35299;&#37322;&#30340;&#21518;&#22788;&#29702;&#35299;&#37322;&#26041;&#27861;&#12290;L-MAC&#21033;&#29992;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#20043;&#19978;&#30340;&#35299;&#30721;&#22120;&#29983;&#25104;&#20108;&#20540;&#25513;&#30721;&#65292;&#31361;&#20986;&#26174;&#31034;&#36755;&#20837;&#38899;&#39057;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;&#25105;&#20204;&#29992;&#19968;&#31181;&#29305;&#27530;&#25439;&#22833;&#26469;&#35757;&#32451;&#35299;&#30721;&#22120;&#65292;&#35813;&#25439;&#22833;&#26368;&#22823;&#21270;&#20998;&#31867;&#22120;&#23545;&#36755;&#20837;&#38899;&#39057;&#30340;&#25513;&#30721;&#37096;&#20998;&#30340;&#32622;&#20449;&#24230;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#27169;&#22411;&#23545;&#25513;&#30721;&#37096;&#20998;&#36755;&#20986;&#30340;&#27010;&#29575;&#12290;&#23545;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#25968;&#25454;&#30340;&#23450;&#37327;&#35780;&#20272;&#34920;&#26126;&#65292;L-MAC&#22987;&#32456;&#20135;&#29983;&#27604;&#20960;&#31181;&#26799;&#24230;&#21644;&#25513;&#30721;&#26041;&#27861;&#26356;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13086v1 Announce Type: cross  Abstract: Despite the impressive performance of deep learning models across diverse tasks, their complexity poses challenges for interpretation. This challenge is particularly evident for audio signals, where conveying interpretations becomes inherently difficult. To address this issue, we introduce Listenable Maps for Audio Classifiers (L-MAC), a posthoc interpretation method that generates faithful and listenable interpretations. L-MAC utilizes a decoder on top of a pretrained classifier to generate binary masks that highlight relevant portions of the input audio. We train the decoder with a special loss that maximizes the confidence of the classifier decision on the masked-in portion of the audio while minimizing the probability of model output for the masked-out portion. Quantitative evaluations on both in-domain and out-of-domain data demonstrate that L-MAC consistently produces more faithful interpretations than several gradient and maskin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;&#20132;&#21449;&#26629;&#26639;&#30340;&#27169;&#25311;&#20869;&#23384;&#21152;&#36895;&#22120;&#20013;ADC&#29305;&#23450;&#25928;&#29575;&#20302;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#19977;&#20010;&#20851;&#38190;&#23646;&#24615;&#26469;&#38477;&#20302;ADC&#30340;&#33021;&#32791;</title><link>https://arxiv.org/abs/2403.13082</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;&#20132;&#21449;&#26629;&#26639;&#30340;&#27169;&#25311;&#20869;&#23384;&#21152;&#36895;&#22120;&#20013;&#21098;&#26525;&#20197;&#25552;&#39640;ADC&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Pruning for Improved ADC Efficiency in Crossbar-based Analog In-memory Accelerators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13082
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;&#20132;&#21449;&#26629;&#26639;&#30340;&#27169;&#25311;&#20869;&#23384;&#21152;&#36895;&#22120;&#20013;ADC&#29305;&#23450;&#25928;&#29575;&#20302;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#19977;&#20010;&#20851;&#38190;&#23646;&#24615;&#26469;&#38477;&#20302;ADC&#30340;&#33021;&#32791;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#38754;&#20020;&#30528;&#39640;&#35745;&#31639;&#38656;&#27714;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#23450;&#21046;&#21152;&#36895;&#22120;&#36827;&#34892;&#37096;&#32626;&#12290;&#22522;&#20110;&#20132;&#21449;&#26629;&#26639;&#30340;&#27169;&#25311;&#20869;&#23384;&#26550;&#26500;&#22240;&#22312;&#20869;&#23384;&#20013;&#32467;&#21512;&#23384;&#20648;&#21644;&#35745;&#31639;&#32780;&#20855;&#26377;&#39640;&#25968;&#25454;&#37325;&#29992;&#21644;&#39640;&#25928;&#29575;&#65292;&#32780;&#25104;&#20026;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#21560;&#24341;&#21147;&#25152;&#22312;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#27169;&#25311;-&#25968;&#23383;&#36716;&#25442;&#22120;&#65288;ADC&#65289;&#26469;&#36890;&#20449;&#20132;&#21449;&#26629;&#26639;&#36755;&#20986;&#12290;ADC&#28040;&#32791;&#20102;&#27599;&#20010;&#20132;&#21449;&#26629;&#26639;&#22788;&#29702;&#21333;&#20803;&#30340;&#22823;&#37096;&#20998;&#33021;&#37327;&#21644;&#38754;&#31215;&#65292;&#22240;&#27492;&#21066;&#24369;&#20102;&#28508;&#22312;&#30340;&#25928;&#29575;&#20248;&#21183;&#12290;&#21098;&#26525;&#26159;&#25552;&#39640;DNN&#25928;&#29575;&#30340;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#25216;&#26415;&#65292;&#20294;&#38656;&#35201;&#36827;&#34892;&#20462;&#25913;&#25165;&#33021;&#23545;&#20132;&#21449;&#26629;&#26377;&#25152;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;ADC&#29305;&#23450;&#25928;&#29575;&#20302;&#30340;&#20132;&#21449;&#26629;&#26639;&#35843;&#25972;&#21098;&#26525;&#30340;&#21160;&#26426;&#12290;&#36825;&#36890;&#36807;&#35782;&#21035;&#20250;&#24341;&#36215;&#31232;&#30095;&#24615;&#30340;&#19977;&#20010;&#20851;&#38190;&#23646;&#24615;&#65288;&#31216;&#20026;D.U.B.&#65289;&#26469;&#23454;&#29616;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#23646;&#24615;&#26469;&#38477;&#20302;ADC&#30340;&#33021;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13082v1 Announce Type: cross  Abstract: Deep learning has proved successful in many applications but suffers from high computational demands and requires custom accelerators for deployment. Crossbar-based analog in-memory architectures are attractive for acceleration of deep neural networks (DNN), due to their high data reuse and high efficiency enabled by combining storage and computation in memory. However, they require analog-to-digital converters (ADCs) to communicate crossbar outputs. ADCs consume a significant portion of energy and area of every crossbar processing unit, thus diminishing the potential efficiency benefits. Pruning is a well-studied technique to improve the efficiency of DNNs but requires modifications to be effective for crossbars. In this paper, we motivate crossbar-attuned pruning to target ADC-specific inefficiencies. This is achieved by identifying three key properties (dubbed D.U.B.) that induce sparsity that can be utilized to reduce ADC energy wi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#35780;&#20272;&#38750;&#31169;&#23494;&#25968;&#25454;&#30456;&#20851;&#39044;&#22788;&#29702;&#31639;&#27861;&#24341;&#36215;&#30340;&#39069;&#22806;&#38544;&#31169;&#25104;&#26412;&#65292;&#24182;&#21033;&#29992;&#24179;&#28369;DP&#21644;&#39044;&#22788;&#29702;&#31639;&#27861;&#30340;&#26377;&#30028;&#25935;&#24863;&#24615;&#24314;&#31435;&#25972;&#20307;&#38544;&#31169;&#20445;&#35777;&#30340;&#19978;&#38480;</title><link>https://arxiv.org/abs/2403.13041</link><description>&lt;p&gt;
&#20855;&#26377;&#38750;&#31169;&#23494;&#39044;&#22788;&#29702;&#30340;&#21487;&#35777;&#26126;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Provable Privacy with Non-Private Pre-Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13041
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#35780;&#20272;&#38750;&#31169;&#23494;&#25968;&#25454;&#30456;&#20851;&#39044;&#22788;&#29702;&#31639;&#27861;&#24341;&#36215;&#30340;&#39069;&#22806;&#38544;&#31169;&#25104;&#26412;&#65292;&#24182;&#21033;&#29992;&#24179;&#28369;DP&#21644;&#39044;&#22788;&#29702;&#31639;&#27861;&#30340;&#26377;&#30028;&#25935;&#24863;&#24615;&#24314;&#31435;&#25972;&#20307;&#38544;&#31169;&#20445;&#35777;&#30340;&#19978;&#38480;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20998;&#26512;&#24046;&#20998;&#31169;&#23494;&#65288;DP&#65289;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#26102;&#65292;&#36890;&#24120;&#20250;&#24573;&#30053;&#25968;&#25454;&#30456;&#20851;&#30340;&#39044;&#22788;&#29702;&#30340;&#28508;&#22312;&#38544;&#31169;&#25104;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#30001;&#38750;&#31169;&#23494;&#25968;&#25454;&#30456;&#20851;&#39044;&#22788;&#29702;&#31639;&#27861;&#24341;&#36215;&#30340;&#39069;&#22806;&#38544;&#31169;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#26032;&#30340;&#25216;&#26415;&#27010;&#24565;&#24314;&#31435;&#20102;&#25972;&#20307;&#38544;&#31169;&#20445;&#35777;&#30340;&#19978;&#38480;&#65306;&#19968;&#31181;&#31216;&#20026;&#24179;&#28369;DP&#30340;DP&#21464;&#20307;&#20197;&#21450;&#39044;&#22788;&#29702;&#31639;&#27861;&#30340;&#26377;&#30028;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13041v1 Announce Type: cross  Abstract: When analysing Differentially Private (DP) machine learning pipelines, the potential privacy cost of data-dependent pre-processing is frequently overlooked in privacy accounting. In this work, we propose a general framework to evaluate the additional privacy cost incurred by non-private data-dependent pre-processing algorithms. Our framework establishes upper bounds on the overall privacy guarantees by utilising two new technical notions: a variant of DP termed Smooth DP and the bounded sensitivity of the pre-processing algorithms. In addition to the generic framework, we provide explicit overall privacy guarantees for multiple data-dependent pre-processing algorithms, such as data imputation, quantization, deduplication and PCA, when used in combination with several DP algorithms. Notably, this framework is also simple to implement, allowing direct integration into existing DP pipelines.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#29289;&#29702;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;nnU-Net&#30340;&#30417;&#30563;&#26041;&#27861;&#26469;&#20248;&#21270;&#24515;&#23460;&#21521;&#37327;&#27969;&#21160;&#26144;&#23556;&#65292;&#25928;&#26524;&#19982;&#20256;&#32479;&#31639;&#27861;&#30456;&#24403;&#65292;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.13040</link><description>&lt;p&gt;
&#29289;&#29702;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#24515;&#23460;&#21521;&#37327;&#27969;&#21160;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Physics-Guided Neural Networks for Intraventricular Vector Flow Mapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13040
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#29289;&#29702;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;nnU-Net&#30340;&#30417;&#30563;&#26041;&#27861;&#26469;&#20248;&#21270;&#24515;&#23460;&#21521;&#37327;&#27969;&#21160;&#26144;&#23556;&#65292;&#25928;&#26524;&#19982;&#20256;&#32479;&#31639;&#27861;&#30456;&#24403;&#65292;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Intraventricular vector flow mapping (iVFM)&#26088;&#22312;&#22686;&#24378;&#21644;&#37327;&#21270;&#24515;&#33039;&#25104;&#20687;&#20013;&#30340;&#24425;&#33394;&#22810;&#26222;&#21202;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#21644;&#22522;&#20110;&#29289;&#29702;&#24341;&#23548;&#30340;nnU-Net&#30417;&#30563;&#26041;&#27861;&#26469;&#20248;&#21270;&#20256;&#32479;&#30340;iVFM&#20248;&#21270;&#26041;&#26696;&#12290;&#36890;&#36807;&#23545;&#22522;&#20110;&#24739;&#32773;&#29305;&#23450;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#22411;&#20135;&#29983;&#30340;&#27169;&#25311;&#24425;&#33394;&#22810;&#26222;&#21202;&#22270;&#20687;&#21644;&#20307;&#20869;&#22810;&#26222;&#21202;&#37319;&#38598;&#30340;&#20005;&#26684;&#35780;&#20272;&#65292;&#20004;&#31181;&#26041;&#27861;&#22343;&#23637;&#29616;&#20986;&#19982;&#21407;&#22987;iVFM&#31639;&#27861;&#30456;&#24403;&#30340;&#37325;&#24314;&#24615;&#33021;&#12290; PINNs&#30340;&#25928;&#29575;&#36890;&#36807;&#21452;&#38454;&#27573;&#20248;&#21270;&#21644;&#39044;&#20248;&#21270;&#26435;&#37325;&#24471;&#21040;&#25552;&#21319;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;nnU-Net&#26041;&#27861;&#22312;&#27867;&#21270;&#33021;&#21147;&#21644;&#23454;&#26102;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;nnU-Net&#22312;&#31232;&#30095;&#21644;&#25130;&#26029;&#22810;&#26222;&#21202;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#29420;&#31435;&#20110;&#26126;&#30830;&#30340;&#36793;&#30028;&#26465;&#20214;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13040v1 Announce Type: cross  Abstract: Intraventricular vector flow mapping (iVFM) seeks to enhance and quantify color Doppler in cardiac imaging. In this study, we propose novel alternatives to the traditional iVFM optimization scheme by utilizing physics-informed neural networks (PINNs) and a physics-guided nnU-Net-based supervised approach. Through rigorous evaluation on simulated color Doppler images derived from a patient-specific computational fluid dynamics model and in vivo Doppler acquisitions, both approaches demonstrate comparable reconstruction performance to the original iVFM algorithm. The efficiency of PINNs is boosted through dual-stage optimization and pre-optimized weights. On the other hand, the nnU-Net method excels in generalizability and real time capabilities. Notably, nnU-Net shows superior robustness on sparse and truncated Doppler data while maintaining independence from explicit boundary conditions. Overall, our results highlight the effectiveness
&lt;/p&gt;</description></item><item><title>BiLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#30340;&#20943;&#36731;&#36807;&#25311;&#21512;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20302;&#31209;&#22686;&#37327;&#30697;&#38453;&#36827;&#34892;&#21442;&#25968;&#21270;&#21644;&#23558;&#35757;&#32451;&#20998;&#20026;&#19981;&#21516;&#30340;&#23376;&#38598;&#65292;&#38477;&#20302;&#20102;&#23545;&#21333;&#19968;&#25968;&#25454;&#38598;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.13037</link><description>&lt;p&gt;
BiLoRA&#65306;&#19968;&#31181;&#38754;&#21521;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36807;&#24230;&#25311;&#21512;&#40065;&#26834;&#20302;&#31209;&#36866;&#24212;&#30340;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BiLoRA: A Bi-level Optimization Framework for Overfitting-Resilient Low-Rank Adaptation of Large Pre-trained Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13037
&lt;/p&gt;
&lt;p&gt;
BiLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#30340;&#20943;&#36731;&#36807;&#25311;&#21512;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20302;&#31209;&#22686;&#37327;&#30697;&#38453;&#36827;&#34892;&#21442;&#25968;&#21270;&#21644;&#23558;&#35757;&#32451;&#20998;&#20026;&#19981;&#21516;&#30340;&#23376;&#38598;&#65292;&#38477;&#20302;&#20102;&#23545;&#21333;&#19968;&#25968;&#25454;&#38598;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#24494;&#35843;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#30340;&#27969;&#34892;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20302;&#31209;&#22686;&#37327;&#30697;&#38453;&#12290;&#23613;&#31649;LoRA&#21450;&#20854;&#21464;&#20307;&#30456;&#23545;&#20110;&#23436;&#20840;&#24494;&#35843;&#26041;&#27861;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#20250;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#36807;&#25311;&#21512;&#65292;&#23548;&#33268;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#27425;&#20248;&#27867;&#21270;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BiLoRA&#65292;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#65288;BLO&#65289;&#30340;&#20943;&#36731;&#36807;&#25311;&#21512;&#24494;&#35843;&#26041;&#27861;&#12290;BiLoRA&#37319;&#29992;&#20266;&#22855;&#24322;&#20540;&#20998;&#35299;&#26469;&#21442;&#25968;&#21270;&#20302;&#31209;&#22686;&#37327;&#30697;&#38453;&#65292;&#24182;&#23558;&#20266;&#22855;&#24322;&#21521;&#37327;&#21644;&#20540;&#30340;&#35757;&#32451;&#20998;&#25104;&#20004;&#20010;&#19981;&#21516;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#12290;&#36825;&#31181;&#21010;&#20998;&#23884;&#20837;&#22312;BLO&#26694;&#26550;&#30340;&#19981;&#21516;&#23618;&#27425;&#20013;&#65292;&#26377;&#21161;&#20110;&#20943;&#36731;&#23545;&#21333;&#19968;&#25968;&#25454;&#38598;&#36807;&#24230;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#22312;&#28085;&#30422;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#30340;&#21313;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#24212;&#29992;&#20110;&#21508;&#31181;&#30693;&#21517;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13037v1 Announce Type: cross  Abstract: Low-rank adaptation (LoRA) is a popular method for fine-tuning large-scale pre-trained models in downstream tasks by learning low-rank incremental matrices. Though LoRA and its variants effectively reduce the number of trainable parameters compared to full fine-tuning methods, they often overfit training data, resulting in sub-optimal generalization on test data. To address this problem, we introduce BiLoRA, an overfitting-alleviating fine-tuning approach based on bi-level optimization (BLO). BiLoRA employs pseudo singular value decomposition to parameterize low-rank incremental matrices and splits the training of pseudo singular vectors and values across two different subsets of training data. This division, embedded within separate levels of the BLO framework, mitigates the risk of overfitting to a single dataset. Tested on ten datasets covering natural language understanding and generation tasks and applied to various well-known lar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26080;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65288;HULS&#65289;&#65292;&#29992;&#20110;&#30417;&#27979;&#22797;&#26434;&#24037;&#19994;&#27969;&#31243;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#22312;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#39640;&#24230;&#30456;&#20851;&#21464;&#37327;&#24773;&#20917;&#19979;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13032</link><description>&lt;p&gt;
&#29992;&#20110;&#30417;&#27979;&#24037;&#19994;&#25209;&#22788;&#29702;&#36807;&#31243;&#30340;&#28151;&#21512;&#26080;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Hybrid Unsupervised Learning Strategy for Monitoring Industrial Batch Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26080;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65288;HULS&#65289;&#65292;&#29992;&#20110;&#30417;&#27979;&#22797;&#26434;&#24037;&#19994;&#27969;&#31243;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#22312;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#39640;&#24230;&#30456;&#20851;&#21464;&#37327;&#24773;&#20917;&#19979;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#29983;&#20135;&#36807;&#31243;&#65292;&#23588;&#20854;&#26159;&#21046;&#33647;&#34892;&#19994;&#65292;&#26159;&#22797;&#26434;&#30340;&#31995;&#32479;&#65292;&#38656;&#35201;&#25345;&#32493;&#30417;&#27979;&#20197;&#30830;&#20445;&#25928;&#29575;&#12289;&#20135;&#21697;&#36136;&#37327;&#21644;&#23433;&#20840;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30417;&#27979;&#22797;&#26434;&#24037;&#19994;&#27969;&#31243;&#30340;&#28151;&#21512;&#26080;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65288;HULS&#65289;&#12290;HULS&#32467;&#21512;&#20102;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#33258;&#32452;&#32455;&#26144;&#23556;&#65288;SOM&#65289;&#22312;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#39640;&#24230;&#30456;&#20851;&#30340;&#36807;&#31243;&#21464;&#37327;&#24773;&#20917;&#19979;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;HULS&#27010;&#24565;&#30340;&#24615;&#33021;&#65292;&#36827;&#34892;&#20102;&#22522;&#20110;&#23454;&#39564;&#23460;&#25209;&#22788;&#29702;&#30340;&#27604;&#36739;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13032v1 Announce Type: new  Abstract: Industrial production processes, especially in the pharmaceutical industry, are complex systems that require continuous monitoring to ensure efficiency, product quality, and safety. This paper presents a hybrid unsupervised learning strategy (HULS) for monitoring complex industrial processes. Addressing the limitations of traditional Self-Organizing Maps (SOMs), especially in scenarios with unbalanced data sets and highly correlated process variables, HULS combines existing unsupervised learning techniques to address these challenges. To evaluate the performance of the HULS concept, comparative experiments are performed based on a laboratory batch
&lt;/p&gt;</description></item><item><title>RigorLLM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#39640;&#25928;&#26377;&#25928;&#22320;&#35843;&#33410;LLMs&#30340;&#26377;&#23475;&#21644;&#19981;&#23433;&#20840;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#21253;&#25324;&#33021;&#37327;&#25968;&#25454;&#22686;&#24378;&#12289;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#23433;&#20840;&#36755;&#20837;&#21518;&#32512;&#65292;&#20197;&#21450;&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#40065;&#26834;KNN&#19982;LLMs&#34701;&#21512;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.13031</link><description>&lt;p&gt;
RigorLLM&#65306;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25269;&#24481;&#19981;&#33391;&#20869;&#23481;&#30340;&#40065;&#26834;&#38450;&#25252;&#26639;
&lt;/p&gt;
&lt;p&gt;
RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13031
&lt;/p&gt;
&lt;p&gt;
RigorLLM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#39640;&#25928;&#26377;&#25928;&#22320;&#35843;&#33410;LLMs&#30340;&#26377;&#23475;&#21644;&#19981;&#23433;&#20840;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#21253;&#25324;&#33021;&#37327;&#25968;&#25454;&#22686;&#24378;&#12289;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#23433;&#20840;&#36755;&#20837;&#21518;&#32512;&#65292;&#20197;&#21450;&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#40065;&#26834;KNN&#19982;LLMs&#34701;&#21512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#20013;&#20986;&#29616;&#30340;&#20559;&#35265;&#20197;&#21450;&#22312;&#24694;&#24847;&#36755;&#20837;&#19979;&#20135;&#29983;&#26377;&#23475;&#20869;&#23481;&#30340;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#65292;&#37117;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#38450;&#25252;&#26639;&#65288;RigorLLM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#39640;&#25928;&#26377;&#25928;&#22320;&#35843;&#33410;LLMs&#30340;&#26377;&#23475;&#21644;&#19981;&#23433;&#20840;&#36755;&#20837;&#21644;&#36755;&#20986;&#12290;&#36890;&#36807;&#37319;&#29992;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#36890;&#36807;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#36827;&#34892;&#22522;&#20110;&#33021;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#12289;&#36890;&#36807;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38024;&#23545;&#36755;&#20837;&#20248;&#21270;&#23433;&#20840;&#21518;&#32512;&#65292;&#20197;&#21450;&#22522;&#20110;&#25105;&#20204;&#30340;&#25968;&#25454;&#22686;&#24378;&#23558;&#40065;&#26834;KNN&#19982;LLMs&#34701;&#21512;&#30340;&#22522;&#20110;&#34701;&#21512;&#30340;&#27169;&#22411;&#65292;RigorLLM&#20026;&#26377;&#23475;&#20869;&#23481;&#30340;&#35843;&#33410;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13031v1 Announce Type: cross  Abstract: Recent advancements in Large Language Models (LLMs) have showcased remarkable capabilities across various tasks in different domains. However, the emergence of biases and the potential for generating harmful content in LLMs, particularly under malicious inputs, pose significant challenges. Current mitigation strategies, while effective, are not resilient under adversarial attacks. This paper introduces Resilient Guardrails for Large Language Models (RigorLLM), a novel framework designed to efficiently and effectively moderate harmful and unsafe inputs and outputs for LLMs. By employing a multi-faceted approach that includes energy-based training data augmentation through Langevin dynamics, optimizing a safe suffix for inputs via minimax optimization, and integrating a fusion-based model combining robust KNN with LLMs based on our data augmentation, RigorLLM offers a robust solution to harmful content moderation. Our experimental evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27700;&#21360;LLMs&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#31639;&#27861;&#30340;&#27700;&#21360;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#22833;&#30495;&#21644;&#26816;&#27979;&#33021;&#21147;&#20043;&#38388;&#30340;&#26368;&#20248;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.13027</link><description>&lt;p&gt;
&#26356;&#22909;&#22320;&#32479;&#35745;&#29702;&#35299;&#27700;&#21360;LLMs
&lt;/p&gt;
&lt;p&gt;
Towards Better Statistical Understanding of Watermarking LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27700;&#21360;LLMs&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#31639;&#27861;&#30340;&#27700;&#21360;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#22833;&#30495;&#21644;&#26816;&#27979;&#33021;&#21147;&#20043;&#38388;&#30340;&#26368;&#20248;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27700;&#21360;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#27169;&#22411;&#22833;&#30495;&#21644;&#26816;&#27979;&#33021;&#21147;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#23558;&#20854;&#26500;&#24314;&#20026;&#22522;&#20110;Kirchenbauer&#31561;&#20154;&#65288;2023a&#65289;&#30340;&#32511;-&#32418;&#31639;&#27861;&#30340;&#21463;&#38480;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#20139;&#26377;&#33391;&#22909;&#30340;&#20998;&#26512;&#24615;&#36136;&#65292;&#36825;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#24182;&#21551;&#21457;&#27700;&#21360;&#36807;&#31243;&#30340;&#31639;&#27861;&#35774;&#35745;&#12290;&#25105;&#20204;&#26681;&#25454;&#36825;&#19968;&#20248;&#21270;&#20844;&#24335;&#24320;&#21457;&#20102;&#19968;&#20010;&#22312;&#32447;&#21452;&#26799;&#24230;&#19978;&#21319;&#27700;&#21360;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#27169;&#22411;&#22833;&#30495;&#21644;&#26816;&#27979;&#33021;&#21147;&#20043;&#38388;&#30340;&#28176;&#36817;&#24085;&#32047;&#25176;&#26368;&#20248;&#24615;&#12290;&#36825;&#26679;&#30340;&#32467;&#26524;&#20445;&#35777;&#20102;&#24179;&#22343;&#22686;&#21152;&#30340;&#32511;&#33394;&#21015;&#34920;&#27010;&#29575;&#65292;&#20174;&#32780;&#26126;&#30830;&#25552;&#39640;&#20102;&#26816;&#27979;&#33021;&#21147;&#65288;&#19982;&#20808;&#21069;&#32467;&#26524;&#30456;&#27604;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#27700;&#21360;&#38382;&#39064;&#30340;&#27169;&#22411;&#22833;&#30495;&#24230;&#37327;&#30340;&#36873;&#25321;&#36827;&#34892;&#20102;&#31995;&#32479;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13027v1 Announce Type: cross  Abstract: In this paper, we study the problem of watermarking large language models (LLMs). We consider the trade-off between model distortion and detection ability and formulate it as a constrained optimization problem based on the green-red algorithm of Kirchenbauer et al. (2023a). We show that the optimal solution to the optimization problem enjoys a nice analytical property which provides a better understanding and inspires the algorithm design for the watermarking process. We develop an online dual gradient ascent watermarking algorithm in light of this optimization formulation and prove its asymptotic Pareto optimality between model distortion and detection ability. Such a result guarantees an averaged increased green list probability and henceforth detection ability explicitly (in contrast to previous results). Moreover, we provide a systematic discussion on the choice of the model distortion metrics for the watermarking problem. We justi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#28418;&#31227;&#38450;&#33539;&#32593;&#32476;&#23433;&#20840;&#25915;&#20987;&#65292;&#29305;&#21035;&#20851;&#27880;&#26234;&#33021;&#24314;&#31569;&#20013;&#30340;&#26262;&#36890;&#31354;&#35843;&#31995;&#32479;&#65292;&#36890;&#36807;&#35782;&#21035;&#28418;&#31227;&#28304;&#26469;&#24212;&#29992;&#26377;&#38024;&#23545;&#24615;&#30340;&#32531;&#35299;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.13023</link><description>&lt;p&gt;
&#29992;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#28418;&#31227;&#38450;&#33539;&#32593;&#32476;&#23433;&#20840;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Thwarting Cybersecurity Attacks with Explainable Concept Drift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13023
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#28418;&#31227;&#38450;&#33539;&#32593;&#32476;&#23433;&#20840;&#25915;&#20987;&#65292;&#29305;&#21035;&#20851;&#27880;&#26234;&#33021;&#24314;&#31569;&#20013;&#30340;&#26262;&#36890;&#31354;&#35843;&#31995;&#32479;&#65292;&#36890;&#36807;&#35782;&#21035;&#28418;&#31227;&#28304;&#26469;&#24212;&#29992;&#26377;&#38024;&#23545;&#24615;&#30340;&#32531;&#35299;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23433;&#20840;&#25915;&#20987;&#23545;&#33258;&#20027;&#31995;&#32479;&#30340;&#36816;&#20316;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#12290;&#29305;&#21035;&#21463;&#24433;&#21709;&#30340;&#26159;&#26234;&#33021;&#24314;&#31569;&#20013;&#30340;&#26262;&#36890;&#31354;&#35843;&#31995;&#32479;&#65292;&#20854;&#20381;&#36182;&#20110;&#20256;&#24863;&#22120;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21033;&#29992;&#25429;&#25417;&#25968;&#25454;&#12290;&#25915;&#20987;&#21487;&#33021;&#25913;&#21464;&#36825;&#20123;&#20256;&#24863;&#22120;&#30340;&#35835;&#25968;&#65292;&#20005;&#37325;&#24433;&#21709;&#26262;&#36890;&#31354;&#35843;&#31995;&#32479;&#30340;&#36816;&#34892;&#65292;&#20174;&#32780;&#24433;&#21709;&#23621;&#27665;&#30340;&#33298;&#36866;&#21644;&#33410;&#33021;&#30446;&#26631;&#12290;&#36825;&#31181;&#25915;&#20987;&#21487;&#33021;&#23548;&#33268;&#22312;&#32447;&#25968;&#25454;&#20998;&#24067;&#30340;&#25913;&#21464;&#65292;&#36829;&#21453;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#30456;&#20284;&#24615;&#30340;&#22522;&#26412;&#20551;&#35774;&#12290;&#36825;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#19979;&#38477;&#65292;&#36825;&#26159;&#22240;&#20026;&#25152;&#35859;&#30340;&#27010;&#24565;&#28418;&#31227;&#65288;CD&#65289;&#8211;&#36755;&#20837;&#29305;&#24449;&#21644;&#30446;&#26631;&#21464;&#37327;&#20043;&#38388;&#20851;&#31995;&#30340;&#25913;&#21464;&#12290;&#35299;&#20915;CD&#38656;&#35201;&#35782;&#21035;&#28418;&#31227;&#28304;&#24182;&#24212;&#29992;&#26377;&#38024;&#23545;&#24615;&#30340;&#32531;&#35299;&#31574;&#30053;&#65292;&#36825;&#20010;&#36807;&#31243;&#34987;&#31216;&#20026;dr
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13023v1 Announce Type: cross  Abstract: Cyber-security attacks pose a significant threat to the operation of autonomous systems. Particularly impacted are the Heating, Ventilation, and Air Conditioning (HVAC) systems in smart buildings, which depend on data gathered by sensors and Machine Learning (ML) models using the captured data. As such, attacks that alter the readings of these sensors can severely affect the HVAC system operations impacting residents' comfort and energy reduction goals. Such attacks may induce changes in the online data distribution being fed to the ML models, violating the fundamental assumption of similarity in training and testing data distribution. This leads to a degradation in model prediction accuracy due to a phenomenon known as Concept Drift (CD) - the alteration in the relationship between input features and the target variable. Addressing CD requires identifying the source of drift to apply targeted mitigation strategies, a process termed dr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#23454;&#29616;&#20102;&#19968;&#31181;&#38544;&#24418;&#21518;&#38376;&#25915;&#20987;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#23545;&#20110;&#29983;&#25104;&#26377;&#27602;&#22270;&#20687;&#30340;&#26816;&#27979;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13018</link><description>&lt;p&gt;
&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#23454;&#29616;&#30340;&#38544;&#24418;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Invisible Backdoor Attack Through Singular Value Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13018
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#23454;&#29616;&#20102;&#19968;&#31181;&#38544;&#24418;&#21518;&#38376;&#25915;&#20987;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#23545;&#20110;&#29983;&#25104;&#26377;&#27602;&#22270;&#20687;&#30340;&#26816;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#23545;&#20854;&#23433;&#20840;&#24615;&#30340;&#20851;&#27880;&#20063;&#26174;&#33879;&#22686;&#21152;&#12290;&#22312;&#20854;&#20013;&#65292;&#21518;&#38376;&#25915;&#20987;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#26500;&#25104;&#20005;&#37325;&#23433;&#20840;&#23041;&#32961;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#38376;&#25915;&#20987;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#26088;&#22312;&#36890;&#36807;&#26893;&#20837;&#38544;&#34255;&#30340;&#26410;&#25480;&#26435;&#21151;&#33021;&#25110;&#35302;&#21457;&#22120;&#26469;&#30772;&#22351;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#20174;&#32780;&#23548;&#33268;&#35823;&#23548;&#24615;&#30340;&#39044;&#27979;&#25110;&#34892;&#20026;&#12290;&#20026;&#20102;&#20351;&#35302;&#21457;&#22120;&#19981;&#26131;&#23519;&#35273;&#29978;&#33267;&#26159;&#38544;&#24418;&#30340;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#38544;&#24418;&#21518;&#38376;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20165;&#32771;&#34385;&#20102;&#31354;&#38388;&#22495;&#20013;&#30340;&#19981;&#21487;&#35265;&#24615;&#65292;&#20351;&#24471;&#26368;&#36817;&#30340;&#38450;&#24481;&#26041;&#27861;&#24456;&#23481;&#26131;&#26816;&#27979;&#21040;&#29983;&#25104;&#30340;&#26377;&#27602;&#22270;&#20687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DEBA&#30340;&#38544;&#24418;&#21518;&#38376;&#25915;&#20987;&#12290;DEBA&#21033;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#30340;&#25968;&#23398;&#23646;&#24615;&#23884;&#20837;&#20102;&#19981;&#21487;&#23519;&#35273;&#30340;&#21518;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13018v1 Announce Type: cross  Abstract: With the widespread application of deep learning across various domains, concerns about its security have grown significantly. Among these, backdoor attacks pose a serious security threat to deep neural networks (DNNs). In recent years, backdoor attacks on neural networks have become increasingly sophisticated, aiming to compromise the security and trustworthiness of models by implanting hidden, unauthorized functionalities or triggers, leading to misleading predictions or behaviors. To make triggers less perceptible and imperceptible, various invisible backdoor attacks have been proposed. However, most of them only consider invisibility in the spatial domain, making it easy for recent defense methods to detect the generated toxic images.To address these challenges, this paper proposes an invisible backdoor attack called DEBA. DEBA leverages the mathematical properties of Singular Value Decomposition (SVD) to embed imperceptible backdo
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MLR&#30340;&#36229;&#21322;&#24179;&#38754;&#31354;&#38388;&#21521;&#37327;&#37327;&#21270;&#26041;&#27861;HyperVQ&#65292;&#32467;&#21512;&#20102;&#30690;&#37327;&#37327;&#21270;&#21644;&#36229;&#21322;&#24179;&#38754;&#31354;&#38388;&#30340;&#20248;&#21183;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#23398;&#20064;&#25968;&#25454;&#30340;&#32039;&#20945;&#28508;&#22312;&#34920;&#31034;&#24418;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.13015</link><description>&lt;p&gt;
HyperVQ&#65306;&#22522;&#20110;MLR&#30340;&#36229;&#21322;&#24179;&#38754;&#31354;&#38388;&#21521;&#37327;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
HyperVQ: MLR-based Vector Quantization in Hyperbolic Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13015
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MLR&#30340;&#36229;&#21322;&#24179;&#38754;&#31354;&#38388;&#21521;&#37327;&#37327;&#21270;&#26041;&#27861;HyperVQ&#65292;&#32467;&#21512;&#20102;&#30690;&#37327;&#37327;&#21270;&#21644;&#36229;&#21322;&#24179;&#38754;&#31354;&#38388;&#30340;&#20248;&#21183;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#23398;&#20064;&#25968;&#25454;&#30340;&#32039;&#20945;&#28508;&#22312;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#22312;&#25506;&#35752;&#22522;&#20110;tokenized&#25968;&#25454;&#30340;&#27169;&#22411;&#21462;&#24471;&#25104;&#21151;&#20043;&#21518;&#65292;&#23545;&#26377;&#25928;&#30340;tokenization&#26041;&#27861;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#23588;&#20854;&#26159;&#22312;&#28041;&#21450;&#38750;&#31163;&#25955;&#25968;&#25454;&#30340;&#35270;&#35273;&#25110;&#21548;&#35273;&#20219;&#21153;&#20013;&#12290;&#20854;&#20013;&#65292;&#26368;&#27969;&#34892;&#30340;tokenization&#26041;&#27861;&#20043;&#19968;&#26159;&#30690;&#37327;&#37327;&#21270;&#65288;VQ&#65289;&#65292;&#23427;&#26159;&#21508;&#20010;&#39046;&#22495;&#26368;&#26032;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20851;&#38190;&#32452;&#20214;&#20043;&#19968;&#12290;&#36890;&#24120;&#65292;VQ&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VQVAE&#65289;&#34987;&#35757;&#32451;&#29992;&#20110;&#23558;&#25968;&#25454;&#36716;&#25442;&#21040;&#20854;&#32463;&#36807;tokenization&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#28982;&#21518;&#20877;&#36716;&#25442;&#22238;&#21435;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;VQVAE&#26159;&#36890;&#36807;&#37325;&#26500;&#30446;&#26631;&#26469;&#35757;&#32451;&#30340;&#65292;&#23545;&#20110;&#23884;&#20837;&#26159;&#21542;&#34987;&#24456;&#22909;&#22320;&#20998;&#35299;&#20026;&#19981;&#21516;&#21442;&#25968;&#24182;&#27809;&#26377;&#32422;&#26463;&#65292;&#36825;&#23545;&#20110;&#23558;&#23427;&#20204;&#29992;&#20110;&#21306;&#20998;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#20316;&#21697;&#24050;&#32463;&#35777;&#26126;&#20102;&#21033;&#29992;&#36229;&#21322;&#24179;&#38754;&#31354;&#38388;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#22909;&#22788;&#12290;&#36229;&#21322;&#24179;&#38754;&#31354;&#38388;&#30001;&#20110;&#20854;&#25351;&#25968;&#32423;&#20307;&#31215;&#22686;&#38271;&#21644;&#22266;&#26377;&#30340;&#24314;&#27169;&#20998;&#23618;&#21644;&#32467;&#22270;&#30340;&#33021;&#21147;&#65292;&#23548;&#33268;&#20135;&#29983;&#20102;&#32039;&#20945;&#30340;&#28508;&#22312;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13015v1 Announce Type: cross  Abstract: The success of models operating on tokenized data has led to an increased demand for effective tokenization methods, particularly when applied to vision or auditory tasks, which inherently involve non-discrete data. One of the most popular tokenization methods is Vector Quantization (VQ), a key component of several recent state-of-the-art methods across various domains. Typically, a VQ Variational Autoencoder (VQVAE) is trained to transform data to and from its tokenized representation. However, since the VQVAE is trained with a reconstruction objective, there is no constraint for the embeddings to be well disentangled, a crucial aspect for using them in discriminative tasks. Recently, several works have demonstrated the benefits of utilizing hyperbolic spaces for representation learning. Hyperbolic spaces induce compact latent representations due to their exponential volume growth and inherent ability to model hierarchical and structu
&lt;/p&gt;</description></item><item><title>&#22312;3D&#20013;&#30340;&#19968;&#33324;&#32447;&#22352;&#26631;&#21487;&#35270;&#21270;&#31354;&#38388;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#20132;&#20114;&#24335;&#35270;&#35273;&#27169;&#24335;&#21457;&#29616;&#65292;&#20419;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#20351;&#24471;&#26368;&#32456;&#29992;&#25143;&#33021;&#22815;&#33258;&#20027;&#25511;&#21046;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#65292;&#21516;&#26102;&#20801;&#35768;&#25214;&#21040;&#26368;&#20339;&#30340;&#25968;&#25454;&#26597;&#30475;&#20301;&#32622;&#12290;</title><link>https://arxiv.org/abs/2403.13014</link><description>&lt;p&gt;
&#19977;&#32500;&#20013;&#30340;&#19968;&#33324;&#32447;&#22352;&#26631;
&lt;/p&gt;
&lt;p&gt;
General Line Coordinates in 3D
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13014
&lt;/p&gt;
&lt;p&gt;
&#22312;3D&#20013;&#30340;&#19968;&#33324;&#32447;&#22352;&#26631;&#21487;&#35270;&#21270;&#31354;&#38388;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#20132;&#20114;&#24335;&#35270;&#35273;&#27169;&#24335;&#21457;&#29616;&#65292;&#20419;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#20351;&#24471;&#26368;&#32456;&#29992;&#25143;&#33021;&#22815;&#33258;&#20027;&#25511;&#21046;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#65292;&#21516;&#26102;&#20801;&#35768;&#25214;&#21040;&#26368;&#20339;&#30340;&#25968;&#25454;&#26597;&#30475;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13014v1 &#20844;&#21578;&#31867;&#22411;:&#20132;&#21449; &#25688;&#35201;:&#22312;&#26080;&#25439;3D&#21487;&#35270;&#21270;&#20013;&#21487;&#35299;&#37322;&#30340;&#20132;&#20114;&#24335;&#35270;&#35273;&#27169;&#24335;&#21457;&#29616;&#26159;&#25512;&#21160;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#24335;&#12290;&#23427;&#20351;&#24471;&#38750;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#26368;&#32456;&#29992;&#25143;&#33021;&#22815;&#33258;&#20027;&#25511;&#21046;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#12290; &#23427;&#26159;&#22312;3D&#20013;&#30340;&#19968;&#33324;&#32447;&#22352;&#26631; (GLC) &#21487;&#35270;&#21270;&#31354;&#38388;&#20013;&#36827;&#34892;&#30340;&#65292;&#35813;&#31354;&#38388;&#20445;&#30041;&#20102;3D&#20013;&#30340;&#25152;&#26377; n-D &#20449;&#24687;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#32467;&#21512;&#20102;&#19977;&#31181; GLC &#31867;&#22411;:&#31227;&#20301;&#37197;&#23545;&#22352;&#26631; (SPC)&#12289;&#31227;&#20301;&#19977;&#27425;&#22352;&#26631; (STC) &#21644;&#19968;&#33324;&#32447;&#22352;&#26631;-&#32447;&#24615; (GLC-L) &#29992;&#20110;&#20132;&#20114;&#24335;&#35270;&#35273;&#27169;&#24335;&#21457;&#29616;&#12290;&#20174;&#20108;&#32500;&#21487;&#35270;&#21270;&#36807;&#28193;&#21040;&#19977;&#32500;&#21487;&#35270;&#21270;&#20801;&#35768;&#27604;&#22312;&#20108;&#32500;&#20013;&#26356;&#28165;&#26224;&#22320;&#21576;&#29616;&#35270;&#35273;&#27169;&#24335;&#65292;&#24182;&#19988;&#36824;&#21487;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#25968;&#25454;&#26597;&#30475;&#20301;&#32622;&#65292;&#36825;&#22312;&#20108;&#32500;&#20013;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#23427;&#20351;&#24471;&#26368;&#32456;&#29992;&#25143;&#33021;&#22815;&#28145;&#20837;&#20998;&#26512;&#21508;&#31181;&#31867;&#21035;&#29305;&#23450;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#22312;&#21407;&#22987;&#21487;&#35299;&#37322;&#23646;&#24615;&#20013;&#26131;&#20110;&#29702;&#35299;&#12290; &#25511;&#21046;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13014v1 Announce Type: cross  Abstract: Interpretable interactive visual pattern discovery in lossless 3D visualization is a promising way to advance machine learning. It enables end users who are not data scientists to take control of the model development process as a self-service. It is conducted in 3D General Line Coordinates (GLC) visualization space, which preserves all n-D information in 3D. This paper presents a system which combines three types of GLC: Shifted Paired Coordinates (SPC), Shifted Tripled Coordinates (STC), and General Line Coordinates-Linear (GLC-L) for interactive visual pattern discovery. A transition from 2-D visualization to 3-D visualization allows for a more distinct visual pattern than in 2-D and it also allows for finding the best data viewing positions, which are not available in 2-D. It enables in-depth visual analysis of various class-specific data subsets comprehensible for end users in the original interpretable attributes. Controlling mod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#20351;&#29992;&#20998;&#23618;&#20998;&#31867;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13013</link><description>&lt;p&gt;
&#20998;&#23618;&#20998;&#31867;&#29992;&#20110;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;: &#26377;&#25928;&#35774;&#35745;&#19982;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Classification for Intrusion Detection System: Effective Design and Empirical Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#20351;&#29992;&#20998;&#23618;&#20998;&#31867;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35832;&#22914;&#29289;&#32852;&#32593;(IoT)&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22686;&#21152;&#20351;&#29992;&#65292;&#26032;&#22411;&#32593;&#32476;&#25915;&#20987;&#19981;&#26029;&#28044;&#29616;&#12290;&#20026;&#20102;&#20445;&#25252;&#20851;&#38190;&#22522;&#30784;&#35774;&#26045;&#20813;&#21463;&#36825;&#20123;&#26032;&#20852;&#23041;&#32961;&#65292;&#37096;&#32626;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#19981;&#21516;&#31867;&#22411;&#25915;&#20987;&#21516;&#26102;&#26368;&#23567;&#21270;&#35823;&#25253;&#35686;&#25253;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(IDS)&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;IDS&#20013;&#20998;&#23618;&#20998;&#31867;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#19977;&#32423;&#20998;&#23618;&#20998;&#31867;&#27169;&#22411;&#23545;&#21508;&#31181;&#32593;&#32476;&#25915;&#20987;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13013v1 Announce Type: cross  Abstract: With the increased use of network technologies like Internet of Things (IoT) in many real-world applications, new types of cyberattacks have been emerging. To safeguard critical infrastructures from these emerging threats, it is crucial to deploy an Intrusion Detection System (IDS) that can detect different types of attacks accurately while minimizing false alarms. Machine learning approaches have been used extensively in IDS and they are mainly using flat multi-class classification to differentiate normal traffic and different types of attacks. Though cyberattack types exhibit a hierarchical structure where similar granular attack subtypes can be grouped into more high-level attack types, hierarchical classification approach has not been explored well. In this paper, we investigate the effectiveness of hierarchical classification approach in IDS. We use a three-level hierarchical classification model to classify various network attack
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#32467;&#26500;&#30340;&#33258;&#36866;&#24212;&#21333;&#31867;&#20998;&#31867;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#33021;&#22815;&#36866;&#24212;&#35782;&#21035;&#21644;&#23398;&#20064;&#26469;&#33258;&#26410;&#30693;&#25915;&#20987;&#30340;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13010</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#26032;&#20852;&#32593;&#32476;&#23041;&#32961;&#30340;&#21452;&#23618;&#33258;&#36866;&#24212;&#21333;&#31867;&#20998;&#31867;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Dual-Tier Adaptive One-Class Classification IDS for Emerging Cyberthreats
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13010
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#32467;&#26500;&#30340;&#33258;&#36866;&#24212;&#21333;&#31867;&#20998;&#31867;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#33021;&#22815;&#36866;&#24212;&#35782;&#21035;&#21644;&#23398;&#20064;&#26469;&#33258;&#26410;&#30693;&#25915;&#20987;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#25968;&#23383;&#26102;&#20195;&#65292;&#25105;&#20204;&#23545;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#21644;&#24037;&#19994;&#29289;&#32852;&#32593;&#65288;IIoT&#65289;&#31995;&#32479;&#30340;&#20381;&#36182;&#26085;&#30410;&#22686;&#38271;&#65292;&#36825;&#26377;&#21161;&#20110;&#36827;&#34892;&#38134;&#34892;&#20132;&#26131;&#12289;&#20010;&#20154;&#12289;&#20225;&#19994;&#25968;&#25454;&#21644;&#27861;&#24459;&#25991;&#20214;&#20132;&#25442;&#31561;&#25935;&#24863;&#27963;&#21160;&#12290;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;IDS&#65289;&#20316;&#20026;&#19968;&#31181;&#20027;&#35201;&#24037;&#20855;&#29992;&#20110;&#38450;&#33539;&#36825;&#31867;&#32593;&#32476;&#23041;&#32961;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;IDS&#65292;&#22312;&#38024;&#23545;&#29305;&#23450;&#25915;&#20987;&#27169;&#24335;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#24120;&#24120;&#20250;&#38169;&#35823;&#22320;&#23545;&#26032;&#20852;&#32593;&#32476;&#25915;&#20987;&#36827;&#34892;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#29992;&#20110;&#35757;&#32451;&#30417;&#30563;&#23398;&#20064;&#22120;&#30340;&#25915;&#20987;&#23454;&#20363;&#26377;&#38480;&#65292;&#32593;&#32476;&#23041;&#32961;&#19981;&#26029;&#28436;&#21464;&#30340;&#29305;&#24615;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#38382;&#39064;&#12290;&#36825;&#20984;&#26174;&#20102;&#38656;&#35201;&#19968;&#31181;&#33021;&#22815;&#38543;&#30528;&#26102;&#38388;&#35782;&#21035;&#24182;&#23398;&#20064;&#26469;&#33258;&#38476;&#29983;/&#26410;&#30693;&#25915;&#20987;&#30340;&#36866;&#24212;&#24615;IDS&#26694;&#26550;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20004;&#20010;&#23618;&#27425;&#32467;&#26500;&#30340;&#21333;&#31867;&#20998;&#31867;&#39537;&#21160;&#30340;IDS&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13010v1 Announce Type: cross  Abstract: In today's digital age, our dependence on IoT (Internet of Things) and IIoT (Industrial IoT) systems has grown immensely, which facilitates sensitive activities such as banking transactions and personal, enterprise data, and legal document exchanges. Cyberattackers consistently exploit weak security measures and tools. The Network Intrusion Detection System (IDS) acts as a primary tool against such cyber threats. However, machine learning-based IDSs, when trained on specific attack patterns, often misclassify new emerging cyberattacks. Further, the limited availability of attack instances for training a supervised learner and the ever-evolving nature of cyber threats further complicate the matter. This emphasizes the need for an adaptable IDS framework capable of recognizing and learning from unfamiliar/unseen attacks over time. In this research, we propose a one-class classification-driven IDS system structured on two tiers. The first
&lt;/p&gt;</description></item><item><title>&#36895;&#36890;&#20316;&#20026;&#32463;&#20856;&#27169;&#25311;&#20013;&#37327;&#23376;&#21147;&#23398;&#31616;&#21270;&#29256;&#26412;&#30340;&#34920;&#24449;&#65292;&#25506;&#35752;&#32463;&#20856;&#21147;&#23398;&#27169;&#25311;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#37327;&#23376;&#21147;&#23398;&#20197;&#21450;&#36335;&#24452;&#31215;&#20998;&#30340;&#25968;&#23398;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.13008</link><description>&lt;p&gt;
Speedrunning&#21644;&#36335;&#24452;&#31215;&#20998;
&lt;/p&gt;
&lt;p&gt;
Speedrunning and path integrals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13008
&lt;/p&gt;
&lt;p&gt;
&#36895;&#36890;&#20316;&#20026;&#32463;&#20856;&#27169;&#25311;&#20013;&#37327;&#23376;&#21147;&#23398;&#31616;&#21270;&#29256;&#26412;&#30340;&#34920;&#24449;&#65292;&#25506;&#35752;&#32463;&#20856;&#21147;&#23398;&#27169;&#25311;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#37327;&#23376;&#21147;&#23398;&#20197;&#21450;&#36335;&#24452;&#31215;&#20998;&#30340;&#25968;&#23398;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25506;&#35752;&#36895;&#36890;&#20316;&#20026;&#32463;&#20856;&#27169;&#25311;&#20013;&#37327;&#23376;&#21147;&#23398;&#31616;&#21270;&#29256;&#26412;&#30340;&#34920;&#24449;&#30340;&#27010;&#24565;&#12290;&#36825;&#31181;&#31867;&#27604;&#21487;&#20197;&#30475;&#20316;&#26159;&#29702;&#35299;&#37327;&#23376;&#21147;&#23398;&#21487;&#33021;&#30001;&#20110;&#27169;&#25311;&#30340;&#23616;&#38480;&#24615;&#32780;&#20174;&#32463;&#20856;&#21147;&#23398;&#27169;&#25311;&#20013;&#20986;&#29616;&#30340;&#19968;&#31181;&#31616;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#20174;&#27169;&#25311;&#20869;&#37096;&#30340;&#35270;&#35282;&#25506;&#35752;&#36895;&#36890;&#30340;&#27010;&#24565;&#65292;&#20854;&#20013;&#29609;&#23478;&#34987;&#35270;&#20026;&#21487;&#20197;&#36890;&#36807;&#29275;&#39039;&#31532;&#19968;&#23450;&#24459;&#35299;&#37322;&#30340;&#8220;&#33258;&#28982;&#21147;&#8221;&#12290;&#20174;&#36825;&#20010;&#19968;&#33324;&#20551;&#35774;&#20986;&#21457;&#65292;&#30446;&#26631;&#26159;&#36890;&#36807;&#20351;&#29992;&#36335;&#24452;&#31215;&#20998;&#30340;&#25968;&#23398;&#34920;&#31034;&#26469;&#24314;&#31435;&#36825;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#20998;&#26512;&#20102;&#23558;&#36825;&#31181;&#26041;&#27861;&#29992;&#20316;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#28216;&#25103;&#27169;&#25311;&#20043;&#38388;&#30340;&#20013;&#38388;&#23618;&#20197;&#23547;&#25214;&#26368;&#20339;&#31574;&#30053;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13008v1 Announce Type: cross  Abstract: In this article we will explore the concept of speedrunning as a representation of a simplified version of quantum mechanics within a classical simulation. This analogy can be seen as a simplified approach to understanding the broader idea that quantum mechanics may emerge from classical mechanics simulations due to the limitations of the simulation. The concept of speedrunning will be explored from the perspective inside the simulation, where the player is seen as a "force of nature" that can be interpreted through Newton's first law. Starting from this general assumption, the aim is to build a bridge between these two fields by using the mathematical representation of path integrals. The use of such an approach as an intermediate layer between machine learning techniques aimed at finding an optimal strategy and a game simulation is also analysed. This article will focus primarily on the relationship between classical and quantum phys
&lt;/p&gt;</description></item><item><title>Leap&#26159;&#19968;&#20010;&#20351;&#29992;GPT-2&#27169;&#22411;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#39044;&#27979;&#30340;&#21512;&#25104;&#36335;&#32447;&#28145;&#24230;&#65292;&#21160;&#24577;&#22320;&#21253;&#21547;&#20102;&#20851;&#38190;&#20013;&#38388;&#20307;&#30340;&#21487;&#29992;&#24615;&#20449;&#24687;&#65292;&#22312;&#21512;&#25104;&#21487;&#36798;&#24615;&#35780;&#20998;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.13005</link><description>&lt;p&gt;
Leap: &#20013;&#38388;&#20307;&#30340;&#20998;&#23376;&#21512;&#25104;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Leap: molecular synthesisability scoring with intermediates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13005
&lt;/p&gt;
&lt;p&gt;
Leap&#26159;&#19968;&#20010;&#20351;&#29992;GPT-2&#27169;&#22411;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#39044;&#27979;&#30340;&#21512;&#25104;&#36335;&#32447;&#28145;&#24230;&#65292;&#21160;&#24577;&#22320;&#21253;&#21547;&#20102;&#20851;&#38190;&#20013;&#38388;&#20307;&#30340;&#21487;&#29992;&#24615;&#20449;&#24687;&#65292;&#22312;&#21512;&#25104;&#21487;&#36798;&#24615;&#35780;&#20998;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20998;&#23376;&#26159;&#21542;&#21487;&#20197;&#21512;&#25104;&#26159;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#39318;&#35201;&#20219;&#21153;&#12290;&#23427;&#20351;&#35745;&#31639;&#21270;&#23398;&#23478;&#33021;&#22815;&#36807;&#28388;&#21487;&#34892;&#21270;&#21512;&#29289;&#25110;&#20559;&#21521;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#12290;&#21512;&#25104;&#24615;&#30340;&#27010;&#24565;&#26159;&#21160;&#24577;&#30340;&#65292;&#22240;&#20026;&#23427;&#20250;&#38543;&#30528;&#20851;&#38190;&#21270;&#21512;&#29289;&#30340;&#21487;&#29992;&#24615;&#32780;&#28436;&#21464;&#12290;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#28041;&#21450;&#25506;&#32034;&#21487;&#21512;&#25104;&#20013;&#38388;&#20307;&#21608;&#22260;&#30340;&#21270;&#23398;&#31354;&#38388;&#12290;&#36825;&#19968;&#31574;&#30053;&#25913;&#21892;&#20102;&#30001;&#20110;&#20851;&#38190;&#20013;&#38388;&#20307;&#30340;&#21487;&#29992;&#24615;&#32780;&#23548;&#33268;&#30340;&#34893;&#29983;&#20998;&#23376;&#30340;&#21512;&#25104;&#33021;&#21147;&#12290;&#29616;&#26377;&#30340;&#21512;&#25104;&#21487;&#36798;&#24615;&#35780;&#20998;&#26041;&#27861;&#65292;&#22914;SAScore&#12289;SCScore&#21644;RAScore&#65292;&#26080;&#27861;&#21160;&#24577;&#22320;&#26681;&#25454;&#20013;&#38388;&#20307;&#36827;&#34892;&#26465;&#20214;&#35780;&#20998;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;Leap&#26159;&#19968;&#20010;&#22312;&#39044;&#27979;&#30340;&#21512;&#25104;&#36335;&#32447;&#28145;&#24230;&#65288;&#25110;&#26368;&#38271;&#32447;&#24615;&#36335;&#24452;&#65289;&#19978;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#65292;&#20801;&#35768;&#22312;&#25512;&#26029;&#26102;&#21253;&#21547;&#20851;&#38190;&#20013;&#38388;&#20307;&#30340;&#21487;&#29992;&#24615;&#20449;&#24687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Leap&#22312;AUC sc&#19978;&#33267;&#23569;&#27604;&#25152;&#26377;&#20854;&#20182;&#35780;&#20998;&#26041;&#27861;&#39640;&#20986;5%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13005v1 Announce Type: cross  Abstract: Assessing whether a molecule can be synthesised is a primary task in drug discovery. It enables computational chemists to filter for viable compounds or bias molecular generative models. The notion of synthesisability is dynamic as it evolves depending on the availability of key compounds. A common approach in drug discovery involves exploring the chemical space surrounding synthetically-accessible intermediates. This strategy improves the synthesisability of the derived molecules due to the availability of key intermediates. Existing synthesisability scoring methods such as SAScore, SCScore and RAScore, cannot condition on intermediates dynamically. Our approach, Leap, is a GPT-2 model trained on the depth, or longest linear path, of predicted synthesis routes that allows information on the availability of key intermediates to be included at inference time. We show that Leap surpasses all other scoring methods by at least 5% on AUC sc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22522;&#20110;&#33539;&#30068;&#35770;&#35821;&#35328;&#20026;&#28145;&#24230;&#23398;&#20064;&#21457;&#23637;&#20102;&#26032;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#12289;&#32479;&#19968;&#19988;&#35268;&#33539;&#24615;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.13001</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65306;&#19968;&#20010;&#33539;&#30068;&#35770;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fundamental Components of Deep Learning: A category-theoretic approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22522;&#20110;&#33539;&#30068;&#35770;&#35821;&#35328;&#20026;&#28145;&#24230;&#23398;&#20064;&#21457;&#23637;&#20102;&#26032;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#12289;&#32479;&#19968;&#19988;&#35268;&#33539;&#24615;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#65292;&#20294;&#23427;&#20173;&#28982;&#26159;&#19968;&#20010;&#24180;&#36731;&#30340;&#39046;&#22495;&#12290;&#31867;&#20284;&#35768;&#22810;&#31185;&#23398;&#23398;&#31185;&#30340;&#26089;&#26399;&#38454;&#27573;&#65292;&#23427;&#20197;&#21457;&#29616;&#26032;&#29616;&#35937;&#12289;&#20020;&#26102;&#35774;&#35745;&#20915;&#31574;&#21644;&#32570;&#20047;&#32479;&#19968;&#21644;&#26500;&#25104;&#24615;&#30340;&#25968;&#23398;&#22522;&#30784;&#20026;&#29305;&#24449;&#12290;&#26412;&#35770;&#25991;&#22522;&#20110;&#33539;&#30068;&#29702;&#35770;&#30340;&#35821;&#35328;&#20026;&#28145;&#24230;&#23398;&#20064;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#23398;&#22522;&#30784;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#26159;a)&#31471;&#21040;&#31471;&#30340;&#65292;b)&#32479;&#19968;&#30340;&#65292;c)&#19981;&#20165;&#20165;&#26159;&#25551;&#36848;&#24615;&#30340;&#65292;&#32780;&#19988;&#26159;&#35268;&#33539;&#24615;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#36866;&#29992;&#20110;&#20855;&#26377;&#36275;&#22815;&#21151;&#33021;&#30340;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#30452;&#25509;&#23454;&#29616;&#12290;&#25105;&#20204;&#36824;&#23558;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#31995;&#32479;&#21270;&#65292;&#23558;&#35768;&#22810;&#29616;&#26377;&#26500;&#36896;&#25918;&#32622;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13001v1 Announce Type: new  Abstract: Deep learning, despite its remarkable achievements, is still a young field. Like the early stages of many scientific disciplines, it is marked by the discovery of new phenomena, ad-hoc design decisions, and the lack of a uniform and compositional mathematical foundation. From the intricacies of the implementation of backpropagation, through a growing zoo of neural network architectures, to the new and poorly understood phenomena such as double descent, scaling laws or in-context learning, there are few unifying principles in deep learning. This thesis develops a novel mathematical foundation for deep learning based on the language of category theory. We develop a new framework that is a) end-to-end, b) unform, and c) not merely descriptive, but prescriptive, meaning it is amenable to direct implementation in programming languages with sufficient features. We also systematise many existing approaches, placing many existing constructions a
&lt;/p&gt;</description></item><item><title>Duwak&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#21452;&#37325;&#31192;&#23494;&#27169;&#24335;&#30340;&#27700;&#21360;&#25216;&#26415;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27700;&#21360;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.13000</link><description>&lt;p&gt;
Duwak: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21452;&#37325;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Duwak: Dual Watermarks in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13000
&lt;/p&gt;
&lt;p&gt;
Duwak&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#21452;&#37325;&#31192;&#23494;&#27169;&#24335;&#30340;&#27700;&#21360;&#25216;&#26415;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27700;&#21360;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#26085;&#30410;&#20351;&#29992;&#65292;&#23457;&#35745;&#23427;&#20204;&#30340;&#29992;&#36884;&#12289;&#31649;&#29702;&#23427;&#20204;&#30340;&#24212;&#29992;&#24182;&#20943;&#36731;&#20854;&#28508;&#22312;&#21361;&#23475;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Duwak&#65292;&#36890;&#36807;&#22312;&#20196;&#29260;&#27010;&#29575;&#20998;&#24067;&#21644;&#25277;&#26679;&#26041;&#26696;&#20013;&#23884;&#20837;&#21452;&#37325;&#31192;&#23494;&#27169;&#24335;&#65292;&#20174;&#26681;&#26412;&#19978;&#25552;&#39640;&#20102;&#27700;&#21360;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13000v1 Announce Type: cross  Abstract: As large language models (LLM) are increasingly used for text generation tasks, it is critical to audit their usages, govern their applications, and mitigate their potential harms. Existing watermark techniques are shown effective in embedding single human-imperceptible and machine-detectable patterns without significantly affecting generated text quality and semantics. However, the efficiency in detecting watermarks, i.e., the minimum number of tokens required to assert detection with significance and robustness against post-editing, is still debatable. In this paper, we propose, Duwak, to fundamentally enhance the efficiency and quality of watermarking by embedding dual secret patterns in both token probability distribution and sampling schemes. To mitigate expression degradation caused by biasing toward certain tokens, we design a contrastive search to watermark the sampling scheme, which minimizes the token repetition and enhances 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25552;&#31034;&#36873;&#25321;&#21644;&#22686;&#24378;&#31639;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#31034;&#20363;&#36873;&#25321;&#21644;&#22686;&#24378;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#21644;&#26426;&#22120;&#20154;&#25511;&#21046;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12999</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23569;&#26679;&#26412;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#25552;&#31034;&#36873;&#25321;&#21644;&#22686;&#24378;&#20197;&#21450;&#20854;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Prompt Selection and Augmentation for Few Examples Code Generation in Large Language Model and its Application in Robotics Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25552;&#31034;&#36873;&#25321;&#21644;&#22686;&#24378;&#31639;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#31034;&#20363;&#36873;&#25321;&#21644;&#22686;&#24378;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#21644;&#26426;&#22120;&#20154;&#25511;&#21046;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#25552;&#31034;&#21644;&#36880;&#27493;&#25512;&#29702;&#24050;&#32463;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#21253;&#25324;&#20195;&#30721;&#29983;&#25104;&#22312;&#20869;&#30340;&#22797;&#26434;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#25913;&#21892;&#25968;&#23398;&#25512;&#29702;&#21644;&#26426;&#22120;&#20154;&#33218;&#25805;&#20316;&#30340;&#25552;&#31034;&#36873;&#25321;&#21644;&#22686;&#24378;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#22810;&#38454;&#27573;&#31034;&#20363;&#22686;&#24378;&#26041;&#26696;&#21644;&#31034;&#20363;&#36873;&#25321;&#26041;&#26696;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#36873;&#25321;&#19968;&#32452;&#22686;&#21152;&#22810;&#26679;&#24615;&#12289;&#26368;&#23567;&#21270;&#20887;&#20313;&#24182;&#22686;&#21152;&#19982;&#38382;&#39064;&#30456;&#20851;&#24615;&#30340;&#31034;&#20363;&#26469;&#25552;&#39640;LLM&#24615;&#33021;&#12290;&#24403;&#19982;&#8220;&#24605;&#32500;&#32534;&#31243;&#8221;&#25552;&#31034;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;GSM8K&#21644;SVAMP&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#24615;&#33021;&#25913;&#36827;&#65292;&#20998;&#21035;&#22686;&#21152;&#20102;0.3%&#21644;1.1%&#12290;&#27492;&#22806;&#65292;&#22312;&#27169;&#25311;&#26700;&#38754;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#23454;&#29616;&#25104;&#21151;&#20219;&#21153;&#23436;&#25104;&#29575;&#25552;&#39640;&#20102;3.4%&#65292;&#24182;&#19988;&#25104;&#21151;&#23436;&#25104;&#20219;&#21153;&#30340;&#26102;&#38388;&#20943;&#23569;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12999v1 Announce Type: cross  Abstract: Few-shot prompting and step-by-step reasoning have enhanced the capabilities of Large Language Models (LLMs) in tackling complex tasks including code generation. In this paper, we introduce a prompt selection and augmentation algorithm aimed at improving mathematical reasoning and robot arm operations. Our approach incorporates a multi-stage example augmentation scheme combined with an example selection scheme. This algorithm improves LLM performance by selecting a set of examples that increase diversity, minimize redundancy, and increase relevance to the question. When combined with the Program-of-Thought prompting, our algorithm demonstrates an improvement in performance on the GSM8K and SVAMP benchmarks, with increases of 0.3% and 1.1% respectively. Furthermore, in simulated tabletop environments, our algorithm surpasses the Code-as-Policies approach by achieving a 3.4% increase in successful task completions and a decrease of over 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38754;&#21521;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#22810;&#20219;&#21153;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#36890;&#36807;&#21367;&#31215;&#33258;&#21160;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#36947;&#36335;&#20132;&#36890;&#26631;&#24535;&#30340;&#35821;&#20041;&#32534;&#30721;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#22312;&#22797;&#26434;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#36890;&#20449;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12997</link><description>&lt;p&gt;
&#20026;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#35774;&#35745;&#30340;&#22810;&#20219;&#21153;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Multi-Task Oriented Semantic Communication Framework for Autonomous Vehicles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38754;&#21521;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#22810;&#20219;&#21153;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#36890;&#36807;&#21367;&#31215;&#33258;&#21160;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#36947;&#36335;&#20132;&#36890;&#26631;&#24535;&#30340;&#35821;&#20041;&#32534;&#30721;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#22312;&#22797;&#26434;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#36890;&#20449;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#30340;&#35821;&#20041;&#36890;&#20449;&#26159;&#19968;&#31181;&#26032;&#20852;&#25216;&#26415;&#65292;&#23427;&#20165;&#20256;&#36755;&#28040;&#24687;&#30340;&#30456;&#20851;&#35821;&#20041;&#32780;&#19981;&#26159;&#25972;&#20010;&#28040;&#24687;&#65292;&#20197;&#23454;&#29616;&#29305;&#23450;&#20219;&#21153;&#65292;&#38477;&#20302;&#24310;&#36831;&#65292;&#21387;&#32553;&#25968;&#25454;&#65292;&#22312;&#20302;&#20449;&#22122;&#27604;&#22330;&#26223;&#20013;&#26356;&#21152;&#40065;&#26834;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36830;&#25509;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;CAVs&#65289;&#30340;&#22810;&#20219;&#21153;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21367;&#31215;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;CAE&#65289;&#65292;&#29992;&#20110;&#23545;&#36947;&#36335;&#20132;&#36890;&#26631;&#24535;&#36827;&#34892;&#35821;&#20041;&#32534;&#30721;&#12290;&#28982;&#21518;&#65292;&#22312;&#25361;&#25112;&#24615;&#30340;&#22825;&#27668;&#26465;&#20214;&#19979;&#36890;&#36807;&#21355;&#26143;&#65292;&#23558;&#36825;&#20123;&#32534;&#30721;&#22270;&#20687;&#20174;&#19968;&#20010;CAV&#20256;&#36755;&#21040;&#21478;&#19968;&#20010;CAV&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#22270;&#20687;&#37325;&#24314;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#20219;&#21153;&#23548;&#21521;&#35821;&#20041;&#35299;&#30721;&#22120;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#37325;&#24314;&#22270;&#20687;&#30340;&#30456;&#20284;&#24615;&#21644;&#20998;&#31867;&#31934;&#24230;&#26041;&#38754;&#20248;&#20110;&#24120;&#35268;&#26041;&#26696;&#65292;&#22914;QAM-16&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12997v1 Announce Type: cross  Abstract: Task-oriented semantic communication is an emerging technology that transmits only the relevant semantics of a message instead of the whole message to achieve a specific task. It reduces latency, compresses the data, and is more robust in low SNR scenarios. This work presents a multi-task-oriented semantic communication framework for connected and autonomous vehicles (CAVs). We propose a convolutional autoencoder (CAE) that performs the semantic encoding of the road traffic signs. These encoded images are then transmitted from one CAV to another CAV through satellite in challenging weather conditions where visibility is impaired. In addition, we propose task-oriented semantic decoders for image reconstruction and classification tasks. Simulation results show that the proposed framework outperforms the conventional schemes, such as QAM-16, regarding the reconstructed image's similarity and the classification's accuracy. In addition, it 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#34507;&#30333;&#36136;&#21644;&#23567;&#20998;&#23376;&#30456;&#20851;&#20219;&#21153;&#20013;&#36229;&#36234;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#32479;&#19968;&#20998;&#23376;&#24314;&#27169;&#23454;&#29616;&#20102;&#20805;&#20998;&#21033;&#29992;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.12995</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#32479;&#19968;&#20998;&#23376;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Multi-Scale Protein Language Model for Unified Molecular Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12995
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#34507;&#30333;&#36136;&#21644;&#23567;&#20998;&#23376;&#30456;&#20851;&#20219;&#21153;&#20013;&#36229;&#36234;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#32479;&#19968;&#20998;&#23376;&#24314;&#27169;&#23454;&#29616;&#20102;&#20805;&#20998;&#21033;&#29992;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#22312;&#34507;&#30333;&#36136;&#24037;&#31243;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#20027;&#35201;&#22312;&#27531;&#22522;&#32423;&#21035;&#19978;&#36816;&#34892;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21407;&#23376;&#27700;&#24179;&#25552;&#20379;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#36825;&#19968;&#38480;&#21046;&#38459;&#30861;&#20102;&#25105;&#20204;&#20805;&#20998;&#21033;&#29992;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#22312;&#28041;&#21450;&#34507;&#30333;&#36136;&#21644;&#23567;&#20998;&#23376;&#30340;&#24212;&#29992;&#20013;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ms-ESM&#65288;&#22810;&#23610;&#24230;ESM&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#23610;&#24230;&#32479;&#19968;&#20998;&#23376;&#24314;&#27169;&#12290;ms-ESM&#36890;&#36807;&#39044;&#35757;&#32451;&#22810;&#23610;&#24230;&#20195;&#30721;&#20999;&#25442;&#34507;&#30333;&#36136;&#24207;&#21015;&#24182;&#21033;&#29992;&#22810;&#23610;&#24230;&#20301;&#32622;&#32534;&#30721;&#26469;&#25429;&#33719;&#27531;&#22522;&#21644;&#21407;&#23376;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ms-ESM&#22312;&#34507;&#30333;&#36136;-&#20998;&#23376;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#23545;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#25581;&#31034;&#36890;&#36807;&#32479;&#19968;&#20998;&#23376;&#24314;&#27169;&#65292;ms-ESM&#19981;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12995v1 Announce Type: cross  Abstract: Protein language models have demonstrated significant potential in the field of protein engineering. However, current protein language models primarily operate at the residue scale, which limits their ability to provide information at the atom level. This limitation prevents us from fully exploiting the capabilities of protein language models for applications involving both proteins and small molecules. In this paper, we propose ms-ESM (multi-scale ESM), a novel approach that enables multi-scale unified molecular modeling. ms-ESM achieves this by pre-training on multi-scale code-switch protein sequences and utilizing a multi-scale position encoding to capture relationships among residues and atoms. Experimental results indicate that ms-ESM surpasses previous methods in protein-molecule tasks, demonstrating the full utilization of protein language models. Further investigations reveal that through unified molecular modeling, ms-ESM not 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;FSCK&#26041;&#27861;&#30340;&#24615;&#36136;&#35774;&#35745;&#30340;&#31616;&#21333;MLP&#27169;&#22411;&#65288;SFM&#65289;&#23454;&#29616;&#20102;&#22312;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#23384;&#20648;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#33021;&#22815;&#39640;&#25928;&#22320;&#33719;&#21462;&#30456;&#20851;&#30340;k&#20540;&#21644;&#30456;&#24212;&#30340;ka&#20540;&#12290;</title><link>https://arxiv.org/abs/2403.12993</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#31616;&#21333;&#20840;&#20809;&#35889;&#30456;&#20851;k-&#20998;&#24067;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Simple Full-Spectrum Correlated k-Distribution Model based on Multilayer Perceptron
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12993
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;FSCK&#26041;&#27861;&#30340;&#24615;&#36136;&#35774;&#35745;&#30340;&#31616;&#21333;MLP&#27169;&#22411;&#65288;SFM&#65289;&#23454;&#29616;&#20102;&#22312;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#23384;&#20648;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#33021;&#22815;&#39640;&#25928;&#22320;&#33719;&#21462;&#30456;&#20851;&#30340;k&#20540;&#21644;&#30456;&#24212;&#30340;ka&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#20840;&#20809;&#35889;k-&#20998;&#24067;&#65288;FSCK&#65289;&#26041;&#27861;&#22312;&#22823;&#33539;&#22260;&#30340;&#28909;&#21147;&#23398;&#20013;&#65292;&#20294;&#30001;&#35757;&#32451;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#39044;&#27979;&#30340;k&#20540;&#65292;&#20173;&#28982;&#38656;&#35201;&#21363;&#26102;&#35745;&#31639;a&#20540;&#65292;&#36825;&#22312;&#29702;&#35770;&#19978;&#20250;&#38477;&#20302;FSCK&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#30446;&#21069;MLP&#27169;&#22411;&#30340;&#36807;&#20110;&#22797;&#26434;&#32467;&#26500;&#19981;&#21487;&#36991;&#20813;&#22320;&#38477;&#20302;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22312;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#23384;&#20648;&#20043;&#38388;&#36827;&#34892;&#34917;&#20607;&#65292;&#22522;&#20110;FSCK&#26041;&#27861;&#30340;&#24615;&#36136;&#32780;&#35774;&#35745;&#30340;&#31616;&#21333;MLP&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#21363;&#31616;&#21333;FSCK MLP&#65288;SFM&#65289;&#27169;&#22411;&#65292;&#20174;&#20013;&#21487;&#20197;&#39640;&#25928;&#22320;&#33719;&#21462;&#30456;&#20851;&#30340;k&#20540;&#21644;&#30456;&#24212;&#30340;ka&#20540;&#12290;&#36827;&#34892;&#20102;&#20960;&#20010;&#27979;&#35797;&#26696;&#20363;&#26469;&#27604;&#36739;&#24320;&#21457;&#30340;SFM&#27169;&#22411;&#21644;&#20854;&#20182;FSCK&#24037;&#20855;&#65292;&#21253;&#25324;&#26597;&#25214;&#34920;&#21644;&#20256;&#32479;FSCK MLP&#65288;TFM&#65289;&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;SFM&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#20248;&#31168;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12993v1 Announce Type: new  Abstract: While neural networks have been successfully applied to the full-spectrum k-distribution (FSCK) method at a large range of thermodynamics with k-values predicted by a trained multilayer perceptron (MLP) model, the required a-values still need to be calculated on-the-fly, which theoretically degrades the FSCK method and may lead to errors. On the other hand, too complicated structure of the current MLP model inevitably slows down the calculation efficiency. Therefore, to compensate among accuracy, efficiency and storage, the simple MLP designed based on the nature of FSCK method are developed, i.e., the simple FSCK MLP (SFM) model, from which those correlated k-values and corresponding ka-values can be efficiently obtained. Several test cases have been carried out to compare the developed SFM model and other FSCK tools including look-up tables and traditional FSCK MLP (TFM) model. Results show that the SFM model can achieve excellent accu
&lt;/p&gt;</description></item><item><title>Tel2Veh&#36890;&#36807;&#27719;&#21512;&#30005;&#20449;&#25968;&#25454;&#21644;&#36710;&#36742;&#27969;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65306;&#20351;&#29992;&#34562;&#31389;&#27969;&#37327;&#39044;&#27979;&#26080;&#25668;&#20687;&#22836;&#21306;&#22495;&#30340;&#36710;&#36742;&#27969;&#37327;&#65292;&#24182;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#29305;&#24449;&#34701;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#36710;&#36742;&#27969;&#37327;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.12991</link><description>&lt;p&gt;
Tel2Veh: &#27719;&#21512;&#30005;&#20449;&#25968;&#25454;&#21644;&#36710;&#36742;&#27969;&#37327;&#20197;&#36890;&#36807;&#26102;&#31354;&#26694;&#26550;&#39044;&#27979;&#26080;&#25668;&#20687;&#22836;&#20132;&#36890;
&lt;/p&gt;
&lt;p&gt;
Tel2Veh: Fusion of Telecom Data and Vehicle Flow to Predict Camera-Free Traffic via a Spatio-Temporal Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12991
&lt;/p&gt;
&lt;p&gt;
Tel2Veh&#36890;&#36807;&#27719;&#21512;&#30005;&#20449;&#25968;&#25454;&#21644;&#36710;&#36742;&#27969;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65306;&#20351;&#29992;&#34562;&#31389;&#27969;&#37327;&#39044;&#27979;&#26080;&#25668;&#20687;&#22836;&#21306;&#22495;&#30340;&#36710;&#36742;&#27969;&#37327;&#65292;&#24182;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#29305;&#24449;&#34701;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#36710;&#36742;&#27969;&#37327;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36742;&#27969;&#37327;&#26159;&#20132;&#36890;&#36816;&#36755;&#30340;&#37325;&#35201;&#25351;&#26631;&#65292;&#36890;&#24120;&#21463;&#26816;&#27979;&#22120;&#35206;&#30422;&#33539;&#22260;&#38480;&#21046;&#12290;&#38543;&#30528;&#24191;&#27867;&#30340;&#31227;&#21160;&#32593;&#32476;&#35206;&#30422;&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#36947;&#36335;&#19978;&#30340;&#31227;&#21160;&#29992;&#25143;&#27963;&#21160;&#25110;&#34562;&#31389;&#27969;&#37327;&#20316;&#20026;&#36710;&#36742;&#27969;&#37327;&#30340;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26469;&#33258;&#21508;&#31181;&#29992;&#25143;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#34562;&#31389;&#27969;&#37327;&#30340;&#35745;&#25968;&#21487;&#33021;&#19982;&#36710;&#36742;&#27969;&#37327;&#19981;&#23436;&#20840;&#19968;&#33268;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65306;&#20351;&#29992;&#34562;&#31389;&#27969;&#37327;&#39044;&#27979;&#26080;&#25668;&#20687;&#22836;&#21306;&#22495;&#30340;&#36710;&#36742;&#27969;&#37327;&#12290;&#20026;&#20102;&#25581;&#31034;&#22810;&#28304;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#22312;&#36873;&#23450;&#30340;&#36947;&#36335;&#19978;&#37096;&#32626;&#25668;&#20687;&#26426;&#24314;&#31435;&#20102;Tel2Veh&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#22823;&#37327;&#30340;&#34562;&#31389;&#27969;&#37327;&#21644;&#31232;&#30095;&#30340;&#36710;&#36742;&#27969;&#37327;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29420;&#31435;&#25552;&#21462;&#29305;&#24449;&#24182;&#23558;&#20854;&#19982;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#34701;&#21512;&#30456;&#32467;&#21512;&#65292;&#20197;&#35782;&#21035;&#24046;&#24322;&#65292;&#20174;&#32780;&#21033;&#29992;&#34562;&#31389;&#27969;&#37327;&#39044;&#27979;&#26410;&#35265;&#36710;&#36742;&#27969;&#37327;&#12290;&#36825;&#39033;&#24037;&#20316;&#25512;&#21160;&#20102;&#30005;&#20449;&#25968;&#25454;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12991v1 Announce Type: cross  Abstract: Vehicle flow, a crucial indicator for transportation, is often limited by detector coverage. With the advent of extensive mobile network coverage, we can leverage mobile user activities, or cellular traffic, on roadways as a proxy for vehicle flow. However, as counts of cellular traffic may not directly align with vehicle flow due to data from various user types, we present a new task: predicting vehicle flow in camera-free areas using cellular traffic. To uncover correlations within multi-source data, we deployed cameras on selected roadways to establish the Tel2Veh dataset, consisting of extensive cellular traffic and sparse vehicle flows. Addressing this challenge, we propose a framework that independently extracts features and integrates them with a graph neural network (GNN)-based fusion to discern disparities, thereby enabling the prediction of unseen vehicle flows using cellular traffic. This work advances the use of telecom dat
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;Delta&#20998;&#25968;&#65292;&#24320;&#21457;&#22522;&#20110;&#33021;&#37327;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#20998;&#23376;&#32467;&#21512;&#30340;&#29305;&#24322;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25913;&#21892;&#20102;&#20256;&#32479;&#30340;&#23545;&#25509;&#24471;&#20998;</title><link>https://arxiv.org/abs/2403.12987</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;SBDD&#20013;&#30340;&#29305;&#24322;&#24615;&#65306;&#21033;&#29992;Delta&#20998;&#25968;&#21644;&#33021;&#37327;&#24341;&#23548;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Rethinking Specificity in SBDD: Leveraging Delta Score and Energy-Guided Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12987
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;Delta&#20998;&#25968;&#65292;&#24320;&#21457;&#22522;&#20110;&#33021;&#37327;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#20998;&#23376;&#32467;&#21512;&#30340;&#29305;&#24322;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25913;&#21892;&#20102;&#20256;&#32479;&#30340;&#23545;&#25509;&#24471;&#20998;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32467;&#26500;&#22522;&#33647;&#29289;&#35774;&#35745;&#65288;SBDD&#65289;&#39046;&#22495;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#23545;&#25509;&#24471;&#20998;&#26041;&#38754;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#36827;&#19968;&#27493;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#21644;&#23545;&#25509;&#24471;&#20998;&#22312;&#29305;&#24322;&#24615;&#26041;&#38754;&#32570;&#20047;&#32771;&#34385;&#65292;&#36825;&#24847;&#21619;&#30528;&#29983;&#25104;&#30340;&#20998;&#23376;&#20960;&#20046;&#19982;&#27599;&#20010;&#34507;&#30333;&#36136;&#21475;&#34955;&#32467;&#21512;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#20146;&#21644;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Delta&#20998;&#25968;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#20998;&#23376;&#32467;&#21512;&#29305;&#24322;&#24615;&#30340;&#26032;&#24230;&#37327;&#26631;&#20934;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#23558;&#36825;&#19968;&#35265;&#35299;&#34701;&#20837;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#33021;&#37327;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#65292;&#36890;&#36807;&#23558;&#27963;&#24615;&#21270;&#21512;&#29289;&#20316;&#20026;&#35825;&#39285;&#65292;&#24341;&#23548;&#29983;&#25104;&#27169;&#22411;&#21019;&#36896;&#20855;&#26377;&#39640;&#29305;&#24322;&#24615;&#30340;&#20998;&#23376;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#22686;&#24378;&#20102;Delta&#20998;&#25968;&#65292;&#36824;&#20445;&#25345;&#25110;&#25913;&#21892;&#20102;&#20256;&#32479;&#30340;&#23545;&#25509;&#24471;&#20998;&#65292;&#25104;&#21151;&#22320;&#24357;&#21512;&#20102;SBDD&#21644;&#29616;&#23454;&#38656;&#27714;&#20043;&#38388;&#30340;&#40511;&#27807;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12987v1 Announce Type: cross  Abstract: In the field of Structure-based Drug Design (SBDD), deep learning-based generative models have achieved outstanding performance in terms of docking score. However, further study shows that the existing molecular generative methods and docking scores both have lacked consideration in terms of specificity, which means that generated molecules bind to almost every protein pocket with high affinity. To address this, we introduce the Delta Score, a new metric for evaluating the specificity of molecular binding. To further incorporate this insight for generation, we develop an innovative energy-guided approach using contrastive learning, with active compounds as decoys, to direct generative models toward creating molecules with high specificity. Our empirical results show that this method not only enhances the delta score but also maintains or improves traditional docking scores, successfully bridging the gap between SBDD and real-world need
&lt;/p&gt;</description></item><item><title>BaCon&#36890;&#36807;&#24179;&#34913;&#29305;&#24449;&#32423;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#35268;&#33539;&#20102;&#23454;&#20363;&#34920;&#31034;&#30340;&#20998;&#24067;&#65292;&#22312;&#35299;&#20915;&#19981;&#24179;&#34913;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2403.12986</link><description>&lt;p&gt;
BaCon&#65306;&#36890;&#36807;&#24179;&#34913;&#29305;&#24449;&#32423;&#23545;&#27604;&#23398;&#20064;&#22686;&#24378;&#19981;&#24179;&#34913;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
BaCon: Boosting Imbalanced Semi-supervised Learning via Balanced Feature-Level Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12986
&lt;/p&gt;
&lt;p&gt;
BaCon&#36890;&#36807;&#24179;&#34913;&#29305;&#24449;&#32423;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#35268;&#33539;&#20102;&#23454;&#20363;&#34920;&#31034;&#30340;&#20998;&#24067;&#65292;&#22312;&#35299;&#20915;&#19981;&#24179;&#34913;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#20943;&#23569;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#23545;&#22823;&#37327;&#26631;&#27880;&#30340;&#38656;&#27714;&#65292;&#20294;&#26159;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#26356;&#29616;&#23454;&#30340;&#25361;&#25112;&#8212;&#8212;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#20173;&#28982;&#36739;&#23569;&#34987;&#25506;&#35752;&#12290;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;(CISSL)&#20013;&#65292;&#19981;&#24179;&#34913;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#20250;&#21152;&#21095;&#30001;&#19981;&#21487;&#38752;&#20266;&#26631;&#31614;&#24341;&#20837;&#30340;&#20559;&#35265;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#25110;&#37325;&#37319;&#26679;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#20381;&#36182;&#20110;&#26377;&#20559;&#30340;&#39592;&#24178;&#34920;&#31034;&#65292;&#20854;&#24615;&#33021;&#21463;&#21040;&#20005;&#37325;&#38480;&#21046;&#12290;&#19968;&#20123;&#20854;&#20182;&#26041;&#27861;&#30830;&#23454;&#36827;&#34892;&#20102;&#29305;&#24449;&#32423;&#35843;&#25972;&#65292;&#27604;&#22914;&#29305;&#24449;&#34701;&#21512;&#65292;&#20294;&#21487;&#33021;&#24341;&#20837;&#19981;&#21033;&#30340;&#22122;&#22768;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#26356;&#24179;&#34913;&#30340;&#29305;&#24449;&#20998;&#24067;&#23545;CISSL&#38382;&#39064;&#30340;&#22909;&#22788;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;&#29305;&#24449;&#32423;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;(BaCon)&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#19968;&#31181;&#31934;&#24515;&#35774;&#35745;&#30340;&#23545;&#27604;&#26041;&#24335;&#30452;&#25509;&#35268;&#33539;&#20102;&#23454;&#20363;&#34920;&#31034;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12986v1 Announce Type: cross  Abstract: Semi-supervised Learning (SSL) reduces the need for extensive annotations in deep learning, but the more realistic challenge of imbalanced data distribution in SSL remains largely unexplored. In Class Imbalanced Semi-supervised Learning (CISSL), the bias introduced by unreliable pseudo-labels can be exacerbated by imbalanced data distributions. Most existing methods address this issue at instance-level through reweighting or resampling, but the performance is heavily limited by their reliance on biased backbone representation. Some other methods do perform feature-level adjustments like feature blending but might introduce unfavorable noise. In this paper, we discuss the bonus of a more balanced feature distribution for the CISSL problem, and further propose a Balanced Feature-Level Contrastive Learning method (BaCon). Our method directly regularizes the distribution of instances' representations in a well-designed contrastive manner. 
&lt;/p&gt;</description></item><item><title>&#23558;&#33647;&#29289;SMILES&#23383;&#31526;&#20018;&#35270;&#20026;&#21477;&#23376;&#24182;&#21033;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#33647;&#29289;&#20998;&#31867;&#65292;&#35777;&#23454;&#20102;&#36890;&#36807;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;</title><link>https://arxiv.org/abs/2403.12984</link><description>&lt;p&gt;
&#24403;SMILES&#25317;&#26377;&#35821;&#35328;&#65306;&#20351;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#23545;&#33647;&#29289;SMILES&#23383;&#31526;&#20018;&#36827;&#34892;&#33647;&#29289;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
When SMILES have Language: Drug Classification using Text Classification Methods on Drug SMILES Strings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12984
&lt;/p&gt;
&lt;p&gt;
&#23558;&#33647;&#29289;SMILES&#23383;&#31526;&#20018;&#35270;&#20026;&#21477;&#23376;&#24182;&#21033;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#33647;&#29289;&#20998;&#31867;&#65292;&#35777;&#23454;&#20102;&#36890;&#36807;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30340;&#21270;&#23398;&#32467;&#26500;&#65292;&#22914;&#33647;&#29289;&#65292;&#36890;&#24120;&#30001;SMILES&#23383;&#31526;&#20018;&#26469;&#23450;&#20041;&#65292;&#20316;&#20026;&#20998;&#23376;&#21644;&#38190;&#30340;&#24207;&#21015;&#12290;&#36825;&#20123;SMILES&#23383;&#31526;&#20018;&#22312;&#19981;&#21516;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33647;&#29289;&#30456;&#20851;&#30740;&#31350;&#21644;&#34920;&#31034;&#24037;&#20316;&#20013;&#20351;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25670;&#33073;&#22797;&#26434;&#30340;&#34920;&#31034;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22914;&#26524;&#25105;&#20204;&#23558;&#33647;&#29289;SMILES&#35270;&#20026;&#24120;&#35268;&#21477;&#23376;&#65292;&#24182;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#20197;&#36827;&#34892;&#33647;&#29289;&#20998;&#31867;&#20250;&#24590;&#26679;&#65311;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#31181;&#21487;&#33021;&#24615;&#65292;&#33719;&#24471;&#20102;&#38750;&#24120;&#26377;&#31454;&#20105;&#21147;&#30340;&#20998;&#25968;&#12290;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#27599;&#20010;&#21407;&#23376;&#21644;&#38190;&#35270;&#20026;&#21477;&#23376;&#32452;&#20214;&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;&#22522;&#26412;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#23545;&#33647;&#29289;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#34920;&#26126;&#22797;&#26434;&#30340;&#38382;&#39064;&#20063;&#21487;&#20197;&#29992;&#26356;&#31616;&#21333;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#12290;&#25968;&#25454;&#21644;&#20195;&#30721;&#21487;&#22312;&#27492;&#22788;&#25214;&#21040;&#65306;https://github.com/azminewasi/Drug-Classification-NLP&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12984v1 Announce Type: cross  Abstract: Complex chemical structures, like drugs, are usually defined by SMILES strings as a sequence of molecules and bonds. These SMILES strings are used in different complex machine learning-based drug-related research and representation works. Escaping from complex representation, in this work, we pose a single question: What if we treat drug SMILES as conventional sentences and engage in text classification for drug classification? Our experiments affirm the possibility with very competitive scores. The study explores the notion of viewing each atom and bond as sentence components, employing basic NLP methods to categorize drug types, proving that complex problems can also be solved with simpler perspectives. The data and code are available here: https://github.com/azminewasi/Drug-Classification-NLP.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#38024;&#23545;&#19968;&#27425;&#24615;&#32467;&#26500;&#20462;&#21098;&#38382;&#39064;&#65292;&#26080;&#38656;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#65292;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.12983</link><description>&lt;p&gt;
OSSCAR&#65306;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19968;&#27425;&#24615;&#32467;&#26500;&#20462;&#21098;&#19982;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#38024;&#23545;&#19968;&#27425;&#24615;&#32467;&#26500;&#20462;&#21098;&#38382;&#39064;&#65292;&#26080;&#38656;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#65292;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#20462;&#21098;&#26159;&#20943;&#23569;&#22823;&#22411;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#25104;&#26412;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31934;&#24515;&#36873;&#25321;&#32467;&#26500;&#65292;&#20363;&#22914;&#31070;&#32463;&#20803;&#25110;&#27880;&#24847;&#21147;&#22836;&#65292;&#21487;&#20197;&#22312;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#30828;&#20214;&#19978;&#23454;&#29616;&#35813;&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#27425;&#24615;&#65288;&#35757;&#32451;&#21518;&#65289;&#35774;&#32622;&#20013;&#30340;&#32467;&#26500;&#20462;&#21098;&#65292;&#36825;&#19981;&#38656;&#35201;&#20462;&#21098;&#21518;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#20026;&#36825;&#20010;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#37325;&#26500;&#30446;&#26631;&#21644;&#20180;&#32454;&#37325;&#26032;&#21046;&#23450;&#30340;&#26032;&#22411;&#32452;&#21512;&#20248;&#21270;&#26694;&#26550;&#65292;&#20351;&#24471;&#20248;&#21270;&#21487;&#25193;&#23637;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#26412;&#22320;&#32452;&#21512;&#20248;&#21270;&#31639;&#27861;&#65292;&#21033;&#29992;&#20302;&#31209;&#26356;&#26032;&#36827;&#34892;&#39640;&#25928;&#30340;&#23616;&#37096;&#25628;&#32034;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#26102;&#38388;&#21644;&#20869;&#23384;&#25928;&#29575;&#19978;&#37117;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#24182;&#19988;&#22312;&#35270;&#35273;&#27169;&#22411;&#65288;&#20363;&#22914;ResNet50&#12289;MobileNet&#65289;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;OPT-1.3B&#33267;OPT-30B&#65289;&#19978;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#19968;&#27425;&#24615;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12983v1 Announce Type: cross  Abstract: Structured pruning is a promising approach for reducing the inference costs of large vision and language models. By removing carefully chosen structures, e.g., neurons or attention heads, the improvements from this approach can be realized on standard deep learning hardware. In this work, we focus on structured pruning in the one-shot (post-training) setting, which does not require model retraining after pruning. We propose a novel combinatorial optimization framework for this problem, based on a layer-wise reconstruction objective and a careful reformulation that allows for scalable optimization. Moreover, we design a new local combinatorial optimization algorithm, which exploits low-rank updates for efficient local search. Our framework is time and memory-efficient and considerably improves upon state-of-the-art one-shot methods on vision models (e.g., ResNet50, MobileNet) and language models (e.g., OPT-1.3B -- OPT-30B). For language
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22522;&#20110;&#22823;&#25968;&#25454;&#21487;&#26395;&#25171;&#30772;&#20998;&#23376;&#26448;&#26009;&#30740;&#21457;&#22256;&#22659;&#65292;&#36801;&#31227;&#23398;&#20064;&#38477;&#20302;&#20102;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#35201;&#27714;&#65292;&#23545;&#35299;&#20915;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.12982</link><description>&lt;p&gt;
&#20998;&#23376;&#21644;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#30693;&#35782;&#37325;&#29992;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Reuse Transfer Learning Methods in Molecular and Material Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12982
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22522;&#20110;&#22823;&#25968;&#25454;&#21487;&#26395;&#25171;&#30772;&#20998;&#23376;&#26448;&#26009;&#30740;&#21457;&#22256;&#22659;&#65292;&#36801;&#31227;&#23398;&#20064;&#38477;&#20302;&#20102;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#35201;&#27714;&#65292;&#23545;&#35299;&#20915;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#21644;&#26448;&#26009;&#26159;&#29616;&#20195;&#20808;&#36827;&#20135;&#19994;&#21457;&#23637;&#30340;&#22522;&#30784;&#65292;&#22914;&#33021;&#28304;&#20648;&#23384;&#31995;&#32479;&#21644;&#21322;&#23548;&#20307;&#22120;&#20214;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#35797;&#38169;&#26041;&#27861;&#25110;&#29702;&#35770;&#35745;&#31639;&#20855;&#26377;&#26497;&#39640;&#30340;&#36164;&#28304;&#28040;&#32791;&#65292;&#26497;&#38271;&#30340;&#30740;&#21457;&#21608;&#26399;&#26080;&#27861;&#28385;&#36275;&#24037;&#19994;&#21457;&#23637;&#20013;&#23545;&#20998;&#23376;/&#26448;&#26009;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#22522;&#20110;&#22823;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26377;&#26395;&#25171;&#30772;&#36825;&#19968;&#22256;&#22659;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#33719;&#21462;&#21644;&#26631;&#27880;&#25104;&#26412;&#39640;&#26114;&#65292;&#38590;&#20197;&#26500;&#24314;&#22823;&#35268;&#27169;&#26032;&#20998;&#23376;/&#26032;&#26448;&#26009;&#25968;&#25454;&#38598;&#65292;&#38480;&#21046;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#21457;&#23637;&#12290;&#36801;&#31227;&#23398;&#20064;&#30340;&#24212;&#29992;&#38477;&#20302;&#20102;&#27169;&#22411;&#35757;&#32451;&#30340;&#25968;&#25454;&#35201;&#27714;&#65292;&#20351;&#24471;&#36801;&#31227;&#23398;&#20064;&#22312;&#35299;&#20915;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#30340;&#30740;&#31350;&#20013;&#33073;&#39062;&#32780;&#20986;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#19982;&#20998;&#23376;&#21644;&#26448;&#26009;&#31185;&#23398;&#30456;&#20851;&#30340;&#36801;&#31227;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12982v1 Announce Type: cross  Abstract: Molecules and materials are the foundation for the development of modern advanced industries such as energy storage systems and semiconductor devices. However, traditional trial-and-error methods or theoretical calculations are highly resource-intensive, and extremely long R&amp;D (Research and Development) periods cannot meet the urgent need for molecules/materials in industrial development. Machine learning (ML) methods based on big data are expected to break this dilemma. However, the difficulty in constructing large-scale datasets of new molecules/materials due to the high cost of data acquisition and annotation limits the development of machine learning. The application of transfer learning lowers the data requirements for model training, which makes transfer learning stand out in researches addressing data quality issues. In this review, we summarize recent advances in transfer learning related to molecular and materials science. We 
&lt;/p&gt;</description></item><item><title>&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#25512;&#26029;&#35831;&#27714;&#30340;&#24615;&#33021;&#24433;&#21709;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#37327;&#21270;&#20102;&#26381;&#21153;&#22120;&#24320;&#38144;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.12981</link><description>&lt;p&gt;
&#36229;&#36234;&#25512;&#26029;&#65306;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;DNN&#26381;&#21153;&#22120;&#24320;&#38144;&#30340;&#24615;&#33021;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Beyond Inference: Performance Analysis of DNN Server Overheads for Computer Vision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12981
&lt;/p&gt;
&lt;p&gt;
&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#25512;&#26029;&#35831;&#27714;&#30340;&#24615;&#33021;&#24433;&#21709;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#37327;&#21270;&#20102;&#26381;&#21153;&#22120;&#24320;&#38144;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#25512;&#26029;&#24050;&#25104;&#20026;&#35768;&#22810;&#25968;&#25454;&#20013;&#24515;&#24037;&#20316;&#36127;&#36733;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#23545;&#22312;&#21534;&#21520;&#37327;&#20248;&#21270;&#30340;&#26381;&#21153;&#22120;&#31995;&#32479;&#19978;&#25191;&#34892;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25512;&#26029;&#35831;&#27714;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#26381;&#21153;&#22120;&#24320;&#38144;&#65288;&#22914;&#25968;&#25454;&#31227;&#21160;&#12289;&#39044;&#22788;&#29702;&#21644;&#20004;&#20010;&#20197;&#19981;&#21516;&#36895;&#29575;&#20135;&#29983;&#36755;&#20986;&#30340;DNN&#20043;&#38388;&#30340;&#28040;&#24687;&#20195;&#29702;&#65289;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#28085;&#30422;&#20102;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#20998;&#21106;&#12289;&#26816;&#27979;&#12289;&#28145;&#24230;&#20272;&#35745;&#20197;&#21450;&#20855;&#26377;&#22810;&#20010;DNN&#30340;&#22797;&#26434;&#22788;&#29702;&#31649;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12981v1 Announce Type: cross  Abstract: Deep neural network (DNN) inference has become an important part of many data-center workloads. This has prompted focused efforts to design ever-faster deep learning accelerators such as GPUs and TPUs. However, an end-to-end DNN-based vision application contains more than just DNN inference, including input decompression, resizing, sampling, normalization, and data transfer. In this paper, we perform a thorough evaluation of computer vision inference requests performed on a throughput-optimized serving system. We quantify the performance impact of server overheads such as data movement, preprocessing, and message brokers between two DNNs producing outputs at different rates. Our empirical analysis encompasses many computer vision tasks including image classification, segmentation, detection, depth-estimation, and more complex processing pipelines with multiple DNNs. Our results consistently demonstrate that end-to-end application perfo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;AltGraph&#36825;&#19968;&#22522;&#20110;&#25628;&#32034;&#30340;&#30005;&#36335;&#36716;&#25442;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#22270;&#27169;&#22411;&#29983;&#25104;&#31561;&#25928;&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#20197;&#22312;&#20445;&#25345;&#31561;&#25928;&#24615;&#30340;&#21516;&#26102;&#20248;&#21270;&#30005;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.12979</link><description>&lt;p&gt;
AltGraph&#65306;&#21033;&#29992;&#29983;&#25104;&#22270;&#27169;&#22411;&#37325;&#26032;&#35774;&#35745;&#37327;&#23376;&#30005;&#36335;&#20197;&#36827;&#34892;&#39640;&#25928;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
AltGraph: Redesigning Quantum Circuits Using Generative Graph Models for Efficient Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12979
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;AltGraph&#36825;&#19968;&#22522;&#20110;&#25628;&#32034;&#30340;&#30005;&#36335;&#36716;&#25442;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#22270;&#27169;&#22411;&#29983;&#25104;&#31561;&#25928;&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#20197;&#22312;&#20445;&#25345;&#31561;&#25928;&#24615;&#30340;&#21516;&#26102;&#20248;&#21270;&#30005;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#30005;&#36335;&#36716;&#25442;&#26088;&#22312;&#29983;&#25104;&#31561;&#25928;&#30005;&#36335;&#30340;&#21516;&#26102;&#20248;&#21270;&#21508;&#26041;&#38754;&#65292;&#22914;&#30005;&#36335;&#28145;&#24230;&#12289;&#38376;&#25968;&#37327;&#20197;&#21450;&#19982;&#29616;&#20195;&#22024;&#26434;&#20013;&#38388;&#23610;&#24230;&#37327;&#23376;&#65288;NISQ&#65289;&#35774;&#22791;&#30340;&#20860;&#23481;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;AltGraph&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25628;&#32034;&#30340;&#30005;&#36335;&#36716;&#25442;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#29983;&#25104;&#22270;&#27169;&#22411;&#29983;&#25104;&#31561;&#25928;&#30340;&#37327;&#23376;&#30005;&#36335;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19977;&#20010;&#20027;&#35201;&#30340;&#22270;&#27169;&#22411;&#65306;DAG&#65288;&#26377;&#21521;&#26080;&#29615;&#22270;&#65289;&#12289;AND-OR&#22270;&#21644;&#34507;&#30333;&#36136;&#32467;&#26500;&#22270;&#65292;&#20197;&#22312;&#20445;&#25345;&#31561;&#25928;&#24615;&#30340;&#21516;&#26102;&#23545;&#30005;&#36335;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12979v1 Announce Type: cross  Abstract: Quantum circuit transformation aims to produce equivalent circuits while optimizing for various aspects such as circuit depth, gate count, and compatibility with modern Noisy Intermediate Scale Quantum (NISQ) devices. There are two techniques for circuit transformation. The first is a rule-based approach that greedily cancels out pairs of gates that equate to the identity unitary operation. Rule-based approaches are used in quantum compilers such as Qiskit, tket, and Quilc. The second is a search-based approach that tries to find an equivalent quantum circuit by exploring the quantum circuits search space. Search-based approaches typically rely on machine learning techniques such as generative models and Reinforcement Learning (RL). In this work, we propose AltGraph, a novel search-based circuit transformation approach that generates equivalent quantum circuits using existing generative graph models. We use three main graph models: DAG
&lt;/p&gt;</description></item><item><title>SportsNGEN&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#32463;&#36807;&#35757;&#32451;&#33021;&#25345;&#32493;&#29983;&#25104;&#36924;&#30495;&#30340;&#22810;&#20154;&#20307;&#32946;&#28216;&#25103;&#65292;&#21253;&#25324;&#27169;&#25311;&#25972;&#20010;&#32593;&#29699;&#27604;&#36187;&#21644;&#20026;&#25945;&#32451;&#21644;&#24191;&#25773;&#21592;&#25552;&#20379;&#27934;&#23519;&#21147;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.12977</link><description>&lt;p&gt;
SportsNGEN: &#25345;&#32493;&#29983;&#25104;&#22810;&#20154;&#20307;&#32946;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
SportsNGEN: Sustained Generation of Multi-player Sports Gameplay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12977
&lt;/p&gt;
&lt;p&gt;
SportsNGEN&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#32463;&#36807;&#35757;&#32451;&#33021;&#25345;&#32493;&#29983;&#25104;&#36924;&#30495;&#30340;&#22810;&#20154;&#20307;&#32946;&#28216;&#25103;&#65292;&#21253;&#25324;&#27169;&#25311;&#25972;&#20010;&#32593;&#29699;&#27604;&#36187;&#21644;&#20026;&#25945;&#32451;&#21644;&#24191;&#25773;&#21592;&#25552;&#20379;&#27934;&#23519;&#21147;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;SportsNGEN&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#20351;&#29992;&#36816;&#21160;&#21592;&#21644;&#29699;&#36861;&#36394;&#24207;&#21015;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#19988;&#25345;&#32493;&#30340;&#28216;&#25103;&#22330;&#26223;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#19987;&#19994;&#32593;&#29699;&#36861;&#36394;&#25968;&#25454;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;SportsNGEN&#65292;&#24182;&#23637;&#31034;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#27169;&#25311;&#19982;&#23556;&#20987;&#20998;&#31867;&#22120;&#21644;&#36923;&#36753;&#30456;&#32467;&#21512;&#26469;&#24320;&#22987;&#21644;&#32467;&#26463;&#29699;&#36187;&#65292;&#31995;&#32479;&#33021;&#22815;&#27169;&#25311;&#25972;&#20010;&#32593;&#29699;&#27604;&#36187;&#12290;&#27492;&#22806;&#65292;SportsNGEN&#30340;&#36890;&#29992;&#29256;&#26412;&#21487;&#20197;&#36890;&#36807;&#22312;&#21253;&#21547;&#35813;&#29699;&#21592;&#30340;&#27604;&#36187;&#25968;&#25454;&#19978;&#24494;&#35843;&#26469;&#23450;&#21046;&#29305;&#23450;&#29699;&#21592;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#32463;&#36807;&#33391;&#22909;&#26657;&#20934;&#65292;&#21487;&#20197;&#36890;&#36807;&#35780;&#20272;&#21453;&#20107;&#23454;&#25110;&#20551;&#35774;&#36873;&#39033;&#20026;&#25945;&#32451;&#21644;&#24191;&#25773;&#21592;&#25552;&#20379;&#27934;&#23519;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36136;&#37327;&#32467;&#26524;&#34920;&#26126;&#30456;&#21516;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#36275;&#29699;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12977v1 Announce Type: cross  Abstract: We present a transformer decoder based model, SportsNGEN, that is trained on sports player and ball tracking sequences that is capable of generating realistic and sustained gameplay. We train and evaluate SportsNGEN on a large database of professional tennis tracking data and demonstrate that by combining the generated simulations with a shot classifier and logic to start and end rallies, the system is capable of simulating an entire tennis match. In addition, a generic version of SportsNGEN can be customized to a specific player by fine-tuning on match data that includes that player. We show that our model is well calibrated and can be used to derive insights for coaches and broadcasters by evaluating counterfactual or what if options. Finally, we show qualitative results indicating the same approach works for football.
&lt;/p&gt;</description></item><item><title>&#24418;&#24577;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#23384;&#22312;&#25361;&#25112;&#65292;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25506;&#35752;&#20102;&#22522;&#20110;&#24494;&#20998;&#26041;&#27861;&#21644;&#21453;&#21521;&#20256;&#25773;&#23545;&#24418;&#24577;&#32593;&#32476;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#21021;&#22987;&#21270;&#21644;&#23398;&#20064;&#29575;&#30340;&#29702;&#35770;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2403.12975</link><description>&lt;p&gt;
&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#24418;&#24577;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#20123;&#29702;&#35770;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Training morphological neural networks with gradient descent: some theoretical insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12975
&lt;/p&gt;
&lt;p&gt;
&#24418;&#24577;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#23384;&#22312;&#25361;&#25112;&#65292;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25506;&#35752;&#20102;&#22522;&#20110;&#24494;&#20998;&#26041;&#27861;&#21644;&#21453;&#21521;&#20256;&#25773;&#23545;&#24418;&#24577;&#32593;&#32476;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#21021;&#22987;&#21270;&#21644;&#23398;&#20064;&#29575;&#30340;&#29702;&#35770;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#24577;&#31070;&#32463;&#32593;&#32476;&#25110;&#23618;&#21487;&#20197;&#25104;&#20026;&#25552;&#21319;&#25968;&#23398;&#24418;&#24577;&#23398;&#36827;&#23637;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#26080;&#35770;&#26159;&#22312;&#29702;&#35770;&#26041;&#38754;&#65292;&#22914;&#23436;&#25972;&#26684;&#31639;&#23376;&#30340;&#34920;&#31034;&#65292;&#36824;&#26159;&#22312;&#22270;&#20687;&#22788;&#29702;&#27969;&#31243;&#30340;&#24320;&#21457;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20123;&#26550;&#26500;&#21253;&#21547;&#22810;&#23618;&#24418;&#24577;&#23398;&#26102;&#65292;&#33267;&#23569;&#22312;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#20248;&#21270;&#31639;&#27861;&#30340;&#27969;&#34892;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20869;&#65292;&#36825;&#20123;&#32593;&#32476;&#24456;&#38590;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;&#24494;&#20998;&#26041;&#27861;&#21644;&#21453;&#21521;&#20256;&#25773;&#24212;&#29992;&#20110;&#24418;&#24577;&#32593;&#32476;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#32771;&#34385;&#21040;Bouligand&#23548;&#25968;&#30340;&#38750;&#20809;&#28369;&#20248;&#21270;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35265;&#35299;&#21644;&#39318;&#20010;&#29702;&#35770;&#25351;&#21335;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#21021;&#22987;&#21270;&#21644;&#23398;&#20064;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12975v1 Announce Type: cross  Abstract: Morphological neural networks, or layers, can be a powerful tool to boost the progress in mathematical morphology, either on theoretical aspects such as the representation of complete lattice operators, or in the development of image processing pipelines. However, these architectures turn out to be difficult to train when they count more than a few morphological layers, at least within popular machine learning frameworks which use gradient descent based optimization algorithms. In this paper we investigate the potential and limitations of differentiation based approaches and back-propagation applied to morphological networks, in light of the non-smooth optimization concept of Bouligand derivative. We provide insights and first theoretical guidelines, in particular regarding initialization and learning rates.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#24314;&#27169;&#20013;&#20351;&#29992;&#24352;&#37327;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38190;&#26680;MPS&#65292;&#25104;&#21151;&#24212;&#23545;&#20102;&#27169;&#22411;&#22797;&#26434;&#24615;&#19982;&#25968;&#25454;&#37327;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#23436;&#32654;&#30340;&#20998;&#31867;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.12969</link><description>&lt;p&gt;
&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#37327;&#23376;&#24352;&#37327;&#32593;&#32476;&#32416;&#32544;&#22312;&#19968;&#36215;
&lt;/p&gt;
&lt;p&gt;
Entangling Machine Learning with Quantum Tensor Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12969
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#24314;&#27169;&#20013;&#20351;&#29992;&#24352;&#37327;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38190;&#26680;MPS&#65292;&#25104;&#21151;&#24212;&#23545;&#20102;&#27169;&#22411;&#22797;&#26434;&#24615;&#19982;&#25968;&#25454;&#37327;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#23436;&#32654;&#30340;&#20998;&#31867;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#24352;&#37327;&#32593;&#32476;&#22312;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#65292;&#24352;&#37327;&#32593;&#32476;&#33021;&#22815;&#39640;&#25928;&#34920;&#31034;&#39640;&#32500;&#37327;&#23376;&#24577;&#12290;&#23427;&#26159;&#23545;(van der Poel, 2023)&#24037;&#20316;&#30340;&#25552;&#28860;&#21644;&#24310;&#32493;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#25277;&#35937;&#20026;&#24314;&#27169;Motzkin&#33258;&#26059;&#38142;&#65292;&#20854;&#23637;&#29616;&#20986;&#19982;&#35821;&#35328;&#20013;&#21457;&#29616;&#30340;&#38271;&#31243;&#30456;&#20851;&#24615;&#30456;&#20284;&#12290;&#30697;&#38453;&#20056;&#31215;&#29366;&#24577;&#65288;MPS&#65289;&#65292;&#20063;&#34987;&#31216;&#20026;&#24352;&#37327;&#21015;&#65292;&#20854;&#38190;&#21512;&#32500;&#24230;&#38543;&#20854;&#24314;&#27169;&#30340;&#24207;&#21015;&#38271;&#24230;&#25193;&#23637;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#24773;&#20917;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#38190;&#26680;MPS&#65292;&#20854;&#38190;&#21512;&#32500;&#24230;&#20197;&#27425;&#32447;&#24615;&#25193;&#23637;&#12290;&#25105;&#20204;&#21457;&#29616;&#24352;&#37327;&#27169;&#22411;&#36798;&#21040;&#20102;&#25509;&#36817;&#23436;&#32654;&#30340;&#20998;&#31867;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#20943;&#23569;&#21512;&#27861;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#26102;&#65292;&#20445;&#25345;&#20102;&#31283;&#23450;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12969v1 Announce Type: new  Abstract: This paper examines the use of tensor networks, which can efficiently represent high-dimensional quantum states, in language modeling. It is a distillation and continuation of the work done in (van der Poel, 2023). To do so, we will abstract the problem down to modeling Motzkin spin chains, which exhibit long-range correlations reminiscent of those found in language. The Matrix Product State (MPS), also known as the tensor train, has a bond dimension which scales as the length of the sequence it models. To combat this, we use the factored core MPS, whose bond dimension scales sub-linearly. We find that the tensor models reach near perfect classifying ability, and maintain a stable level of performance as the number of valid training examples is decreased.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#22312;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#21487;&#33021;&#23384;&#22312;&#31574;&#30053;&#20998;&#21449;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#25299;&#25169;&#20998;&#26512;&#20197;&#35777;&#26126;&#22312;&#19968;&#20123;&#24773;&#26223;&#19979;&#65292;&#31574;&#30053;&#38656;&#35201;&#20855;&#26377;&#19981;&#36830;&#32493;&#24615;&#25110;&#22810;&#20540;&#24615;&#65292;&#36825;&#23545;&#24212;&#20110;&#38556;&#30861;&#29289;&#33258;&#30001;&#29366;&#24577;&#31354;&#38388;&#20026;&#38750;&#21333;&#36830;&#36890;&#26102;&#38656;&#35201;&#31574;&#30053;&#20998;&#21449;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2403.12847</link><description>&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20998;&#21449;
&lt;/p&gt;
&lt;p&gt;
Policy Bifurcation in Safe Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12847
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#22312;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#21487;&#33021;&#23384;&#22312;&#31574;&#30053;&#20998;&#21449;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#25299;&#25169;&#20998;&#26512;&#20197;&#35777;&#26126;&#22312;&#19968;&#20123;&#24773;&#26223;&#19979;&#65292;&#31574;&#30053;&#38656;&#35201;&#20855;&#26377;&#19981;&#36830;&#32493;&#24615;&#25110;&#22810;&#20540;&#24615;&#65292;&#36825;&#23545;&#24212;&#20110;&#38556;&#30861;&#29289;&#33258;&#30001;&#29366;&#24577;&#31354;&#38388;&#20026;&#38750;&#21333;&#36830;&#36890;&#26102;&#38656;&#35201;&#31574;&#30053;&#20998;&#21449;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20026;&#21463;&#38480;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#25552;&#20379;&#20102;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#38544;&#21547;&#22320;&#20551;&#35774;&#31574;&#30053;&#20989;&#25968;&#20855;&#26377;&#36830;&#32493;&#24615;&#65292;&#21363;&#31574;&#30053;&#20197;&#24179;&#31283;&#12289;&#36830;&#32493;&#30340;&#26041;&#24335;&#23558;&#29366;&#24577;&#26144;&#23556;&#21040;&#21160;&#20316;&#65307;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#34892;&#31574;&#30053;&#24212;&#35813;&#26159;&#19981;&#36830;&#32493;&#25110;&#22810;&#20540;&#30340;&#65292;&#32780;&#22312;&#19981;&#36830;&#32493;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#20043;&#38388;&#25554;&#20540;&#21487;&#33021;&#20250;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#32422;&#26463;&#36829;&#35268;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#35782;&#21035;&#20986;&#36825;&#31181;&#29616;&#35937;&#29983;&#25104;&#26426;&#21046;&#30340;&#30740;&#31350;&#65292;&#24182;&#37319;&#29992;&#25299;&#25169;&#20998;&#26512;&#20005;&#35880;&#22320;&#35777;&#26126;&#20102;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#31574;&#30053;&#20998;&#21449;&#30340;&#23384;&#22312;&#65292;&#36825;&#23545;&#24212;&#20110;&#21487;&#36798;&#20803;&#32452;&#30340;&#21487;&#25910;&#32553;&#24615;&#12290;&#25105;&#20204;&#30340;&#23450;&#29702;&#25581;&#31034;&#20102;&#22312;&#38556;&#30861;&#29289;&#33258;&#30001;&#29366;&#24577;&#31354;&#38388;&#20026;&#38750;&#21333;&#36830;&#36890;&#30340;&#24773;&#26223;&#20013;&#65292;&#38656;&#35201;&#31574;&#30053;&#20998;&#21449;&#65292;&#24847;&#21619;&#30528;&#20854;&#36755;&#20986;&#21160;&#20316;&#38656;&#35201;&#36805;&#36895;&#21709;&#24212;&#29366;&#24577;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12847v1 Announce Type: new  Abstract: Safe reinforcement learning (RL) offers advanced solutions to constrained optimal control problems. Existing studies in safe RL implicitly assume continuity in policy functions, where policies map states to actions in a smooth, uninterrupted manner; however, our research finds that in some scenarios, the feasible policy should be discontinuous or multi-valued, interpolating between discontinuous local optima can inevitably lead to constraint violations. We are the first to identify the generating mechanism of such a phenomenon, and employ topological analysis to rigorously prove the existence of policy bifurcation in safe RL, which corresponds to the contractibility of the reachable tuple. Our theorem reveals that in scenarios where the obstacle-free state space is non-simply connected, a feasible policy is required to be bifurcated, meaning its output action needs to change abruptly in response to the varying state. To train such a bifu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;&#31227;&#21160;&#35774;&#22791;&#19978;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25191;&#34892;&#36827;&#34892;&#20102;&#39318;&#27425;&#31995;&#32479;&#24615;&#30740;&#31350;&#65292;&#24182;&#21019;&#24314;&#20102;&#33258;&#21160;&#21270;&#22522;&#30784;&#26550;&#26500;MELT&#26469;&#25903;&#25345;&#20854;&#35780;&#20272;&#21644;&#24615;&#33021;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2403.12844</link><description>&lt;p&gt;
MELTing point: &#31227;&#21160;&#35821;&#35328;&#36716;&#25442;&#22120;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
MELTing point: Mobile Evaluation of Language Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12844
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;&#31227;&#21160;&#35774;&#22791;&#19978;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25191;&#34892;&#36827;&#34892;&#20102;&#39318;&#27425;&#31995;&#32479;&#24615;&#30740;&#31350;&#65292;&#24182;&#21019;&#24314;&#20102;&#33258;&#21160;&#21270;&#22522;&#30784;&#26550;&#26500;MELT&#26469;&#25903;&#25345;&#20854;&#35780;&#20272;&#21644;&#24615;&#33021;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#36880;&#28176;&#24212;&#29992;&#20110;&#26085;&#24120;&#20219;&#21153;&#65292;&#36171;&#20104;&#25105;&#20204;&#30340;&#35745;&#31639;&#26426;&#8220;&#26234;&#33021;&#30340;&#28779;&#33457;&#8221;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#36816;&#34892;&#26102;&#38656;&#27714;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#24191;&#27867;&#37096;&#32626;&#12290;&#22312;&#20010;&#20154;&#35774;&#22791;&#21464;&#24471;&#36234;&#26469;&#36234;&#24378;&#22823;&#65292;&#20197;&#21450;&#36805;&#36895;&#38544;&#31169;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#32039;&#36843;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#25191;&#34892;&#30340;&#29616;&#29366;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#33258;&#24049;&#30340;&#33258;&#21160;&#21270;&#22522;&#30784;&#26550;&#26500;MELT&#65292;&#25903;&#25345;&#22312;&#35774;&#22791;&#19978;&#26080;&#30028;&#38754;&#25191;&#34892;&#21644;&#35780;&#20272;LLMs&#65292;&#24182;&#25903;&#25345;&#19981;&#21516;&#30340;&#27169;&#22411;&#12289;&#35774;&#22791;&#21644;&#26694;&#26550;&#65292;&#21253;&#25324;Android&#12289;iOS&#21644;Nvidia Jetson&#35774;&#22791;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#27969;&#34892;&#30340;&#25351;&#20196;&#24494;&#35843;&#30340;LLMs&#65292;&#24182;&#21033;&#29992;&#19981;&#21516;&#30340;&#26694;&#26550;&#26469;&#27979;&#37327;&#23427;&#20204;&#30340;&#31471;&#21040;&#31471;&#21644;&#32454;&#31890;&#24230;&#24615;&#33021;&#65292;&#36319;&#36394;&#23427;&#20204;&#30340;&#20869;&#23384;&#21644;&#33021;&#32791;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12844v1 Announce Type: new  Abstract: Transformers have revolutionized the machine learning landscape, gradually making their way into everyday tasks and equipping our computers with ``sparks of intelligence''. However, their runtime requirements have prevented them from being broadly deployed on mobile. As personal devices become increasingly powerful and prompt privacy becomes an ever more pressing issue, we explore the current state of mobile execution of Large Language Models (LLMs). To achieve this, we have created our own automation infrastructure, MELT, which supports the headless execution and benchmarking of LLMs on device, supporting different models, devices and frameworks, including Android, iOS and Nvidia Jetson devices. We evaluate popular instruction fine-tuned LLMs and leverage different frameworks to measure their end-to-end and granular performance, tracing their memory and energy requirements along the way.   Our analysis is the first systematic study of o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;LNPT&#65292;&#19968;&#31181;&#26080;&#26631;&#31614;&#32593;&#32476;&#20462;&#21098;&#21644;&#35757;&#32451;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#23398;&#20064;&#24046;&#36317;&#30340;&#27010;&#24565;&#65292;&#24378;&#35843;&#20854;&#20934;&#30830;&#30456;&#20851;&#24615;&#65292;&#20197;&#35299;&#20915;&#22312;&#26234;&#33021;&#35774;&#22791;&#19978;&#30830;&#23450;&#20462;&#21098;&#32467;&#26500;&#30340;&#38590;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12690</link><description>&lt;p&gt;
LNPT&#65306;&#26080;&#26631;&#31614;&#32593;&#32476;&#20462;&#21098;&#19982;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
LNPT: Label-free Network Pruning and Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LNPT&#65292;&#19968;&#31181;&#26080;&#26631;&#31614;&#32593;&#32476;&#20462;&#21098;&#21644;&#35757;&#32451;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#23398;&#20064;&#24046;&#36317;&#30340;&#27010;&#24565;&#65292;&#24378;&#35843;&#20854;&#20934;&#30830;&#30456;&#20851;&#24615;&#65292;&#20197;&#35299;&#20915;&#22312;&#26234;&#33021;&#35774;&#22791;&#19978;&#30830;&#23450;&#20462;&#21098;&#32467;&#26500;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#20043;&#21069;&#20462;&#21098;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#20854;&#33021;&#22815;&#37096;&#32626;&#22312;&#26234;&#33021;&#35774;&#22791;&#19978;&#12290;&#36890;&#36807;&#20445;&#30041;&#26377;&#21161;&#20110;&#27867;&#21270;&#30340;&#26435;&#37325;&#65292;&#20462;&#21098;&#21518;&#30340;&#32593;&#32476;&#21487;&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#26234;&#33021;&#35774;&#22791;&#19978;&#36816;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#24046;&#36317;&#30340;&#27010;&#24565;&#65292;&#24182;&#24378;&#35843;&#23427;&#19982;&#27867;&#21270;&#30340;&#20934;&#30830;&#30456;&#20851;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#23398;&#20064;&#24046;&#36317;&#36890;&#36807;&#32593;&#32476;&#20498;&#25968;&#31532;&#20108;&#23618;&#30340;&#29305;&#24449;&#22270;&#24418;&#24335;&#19982;&#27867;&#21270;&#24615;&#33021;&#30340;&#21464;&#21270;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550; LNPT&#65292;&#20351;&#24471;&#20113;&#31471;&#25104;&#29087;&#32593;&#32476;&#33021;&#22815;&#25552;&#20379;&#22312;&#32447;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12690v1 Announce Type: new  Abstract: Pruning before training enables the deployment of neural networks on smart devices. By retaining weights conducive to generalization, pruned networks can be accommodated on resource-constrained smart devices. It is commonly held that the distance on weight norms between the initialized and the fully-trained networks correlates with generalization performance. However, as we have uncovered, inconsistency between this metric and generalization during training processes, which poses an obstacle to determine the pruned structures on smart devices in advance. In this paper, we introduce the concept of the learning gap, emphasizing its accurate correlation with generalization. Experiments show that the learning gap, in the form of feature maps from the penultimate layer of networks, aligns with variations of generalization performance. We propose a novel learning framework, LNPT, which enables mature networks on the cloud to provide online gui
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignRec&#30340;&#23545;&#40784;&#21644;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#25512;&#33616;&#30446;&#26631;&#20998;&#35299;&#20026;&#19977;&#20010;&#23545;&#40784;&#37096;&#20998;&#65292;&#23454;&#29616;&#20869;&#23481;&#20869;&#37096;&#23545;&#40784;&#12289;&#20869;&#23481;&#19982;&#20998;&#31867;ID&#20043;&#38388;&#30340;&#23545;&#40784;&#20197;&#21450;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2403.12384</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#25512;&#33616;&#30340;&#23545;&#40784;&#21644;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Aligning and Training Framework for Multimodal Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12384
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignRec&#30340;&#23545;&#40784;&#21644;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#25512;&#33616;&#30446;&#26631;&#20998;&#35299;&#20026;&#19977;&#20010;&#23545;&#40784;&#37096;&#20998;&#65292;&#23454;&#29616;&#20869;&#23481;&#20869;&#37096;&#23545;&#40784;&#12289;&#20869;&#23481;&#19982;&#20998;&#31867;ID&#20043;&#38388;&#30340;&#23545;&#40784;&#20197;&#21450;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#23186;&#20307;&#24212;&#29992;&#30340;&#21457;&#23637;&#65292;&#22810;&#27169;&#24577;&#25512;&#33616;&#27491;&#22312;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#21033;&#29992;&#36229;&#36234;&#29992;&#25143;&#20132;&#20114;&#30340;&#20016;&#23500;&#19978;&#19979;&#25991;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#35270;&#20026;&#36741;&#21161;&#65292;&#29992;&#20110;&#24110;&#21161;&#23398;&#20064;ID&#29305;&#24449;&#65307;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#20869;&#23481;&#29305;&#24449;&#21644;ID&#29305;&#24449;&#20043;&#38388;&#23384;&#22312;&#35821;&#20041;&#24046;&#36317;&#65292;&#30452;&#25509;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#20316;&#20026;&#36741;&#21161;&#20351;&#29992;&#20250;&#23548;&#33268;&#29992;&#25143;&#21644;&#39033;&#30446;&#34920;&#31034;&#30340;&#19981;&#23545;&#40784;&#12290;&#26412;&#25991;&#39318;&#20808;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignRec&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;AlignRec&#20013;&#65292;&#25512;&#33616;&#30446;&#26631;&#34987;&#20998;&#35299;&#20026;&#19977;&#20010;&#23545;&#40784;&#37096;&#20998;&#65292;&#21363;&#20869;&#23481;&#20869;&#37096;&#23545;&#40784;&#65292;&#20869;&#23481;&#19982;&#20998;&#31867;ID&#20043;&#38388;&#30340;&#23545;&#40784;&#20197;&#21450;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#27599;&#20010;&#23545;&#40784;&#37096;&#20998;&#37117;&#30001;&#29305;&#23450;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#34920;&#24449;&#65292;&#24182;&#25972;&#21512;&#21040;&#25105;&#20204;&#30340;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12384v1 Announce Type: cross  Abstract: With the development of multimedia applications, multimodal recommendations are playing an essential role, as they can leverage rich contexts beyond user interactions. Existing methods mainly regard multimodal information as an auxiliary, using them to help learn ID features; however, there exist semantic gaps among multimodal content features and ID features, for which directly using multimodal information as an auxiliary would lead to misalignment in representations of users and items. In this paper, we first systematically investigate the misalignment issue in multimodal recommendations, and propose a solution named AlignRec. In AlignRec, the recommendation objective is decomposed into three alignments, namely alignment within contents, alignment between content and categorical ID, and alignment between users and items. Each alignment is characterized by a specific objective function and is integrated into our multimodal recommendat
&lt;/p&gt;</description></item><item><title>&#25913;&#36827;&#39044;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#23545;&#20110;&#25552;&#39640;CLIP&#24615;&#33021;&#27604;&#22686;&#21152;&#25968;&#25454;&#37327;&#26356;&#20026;&#26377;&#25928;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#38024;&#23545;CLIP&#30340;&#29702;&#35770;&#20005;&#35880;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#65292;&#22823;&#24133;&#25552;&#21319;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12267</link><description>&lt;p&gt;
&#25968;&#25454;&#39640;&#25928;&#30340;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65306;&#20248;&#20808;&#32771;&#34385;&#25968;&#25454;&#36136;&#37327;&#32780;&#38750;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Contrastive Language-Image Pretraining: Prioritizing Data Quality over Quantity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12267
&lt;/p&gt;
&lt;p&gt;
&#25913;&#36827;&#39044;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#23545;&#20110;&#25552;&#39640;CLIP&#24615;&#33021;&#27604;&#22686;&#21152;&#25968;&#25454;&#37327;&#26356;&#20026;&#26377;&#25928;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#38024;&#23545;CLIP&#30340;&#29702;&#35770;&#20005;&#35880;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#65292;&#22823;&#24133;&#25552;&#21319;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#26159;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#23383;&#24149;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#34920;&#31034;&#65292;&#33021;&#22815;&#23454;&#29616;&#26174;&#33879;&#30340;&#38646;&#27425;&#36890;&#29992;&#21270;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#25913;&#36827;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#24050;&#34987;&#35777;&#26126;&#27604;&#22686;&#21152;&#25968;&#37327;&#26356;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;CLIP&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25214;&#21040;&#33021;&#22815;&#35777;&#26126;&#36798;&#21040;&#26368;&#20339;&#27867;&#21270;&#25928;&#26524;&#30340;&#23567;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#19968;&#30452;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;CLIP&#30340;&#29702;&#35770;&#20005;&#35880;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33021;&#22815;&#35777;&#26126;&#23454;&#29616;&#21331;&#36234;&#27867;&#21270;&#24615;&#33021;&#30340;&#23376;&#38598;&#25509;&#36817;&#20445;&#30041;&#23436;&#25972;&#25968;&#25454;&#30340;&#22270;&#20687;&#21644;&#23383;&#24149;&#30340;&#20132;&#21449;&#21327;&#26041;&#24046;&#12290;&#25105;&#20204;&#22312;ConceptualCaptions3M&#21644;ConceptualCaptions12M&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;\method\&#25214;&#21040;&#30340;&#23376;&#38598;&#22312;ImageNet&#19978;&#30340;&#20934;&#30830;&#24615;&#27604;&#19979;&#19968;&#20010;&#26368;&#20339;&#22522;&#32447;&#25552;&#39640;&#20102;2.7&#20493;&#21644;1.4&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12267v1 Announce Type: cross  Abstract: Contrastive Language-Image Pre-training (CLIP) on large-scale image-caption datasets learns representations that can achieve remarkable zero-shot generalization. However, such models require a massive amount of pre-training data. Improving the quality of the pre-training data has been shown to be much more effective in improving CLIP's performance than increasing its volume. Nevertheless, finding small subsets of training data that provably generalize the best has remained an open question. In this work, we propose the first theoretically rigorous data selection method for CLIP. We show that subsets that closely preserve the cross-covariance of the images and captions of the full data provably achieve a superior generalization performance. Our extensive experiments on ConceptualCaptions3M and ConceptualCaptions12M demonstrate that subsets found by \method\ achieve over 2.7x and 1.4x the accuracy of the next best baseline on ImageNet an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#26680;&#24515;&#23376;&#38598;&#36873;&#25321;&#36827;&#34892;&#25968;&#25454;&#37325;&#26032;&#21152;&#26435;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#20248;&#21270;&#20102;&#35745;&#31639;&#26102;&#38388;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#31361;&#26174;&#20854;&#20316;&#20026;&#27169;&#22411;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#21644;&#31934;&#30830;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.12166</link><description>&lt;p&gt;
&#23569;&#25968;&#20010;&#20307;&#30340;&#21147;&#37327;&#65306;&#21033;&#29992;&#26680;&#24515;&#23376;&#38598;&#36873;&#25321;&#21152;&#36895;&#21644;&#20248;&#21270;&#25968;&#25454;&#37325;&#26032;&#21152;&#26435;
&lt;/p&gt;
&lt;p&gt;
The Power of Few: Accelerating and Enhancing Data Reweighting with Coreset Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12166
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#26680;&#24515;&#23376;&#38598;&#36873;&#25321;&#36827;&#34892;&#25968;&#25454;&#37325;&#26032;&#21152;&#26435;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#20248;&#21270;&#20102;&#35745;&#31639;&#26102;&#38388;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#31361;&#26174;&#20854;&#20316;&#20026;&#27169;&#22411;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#21644;&#31934;&#30830;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#19981;&#26029;&#21457;&#23637;&#65292;&#36235;&#21183;&#26159;&#25910;&#38598;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#24182;&#35757;&#32451;&#35268;&#27169;&#36234;&#26469;&#36234;&#22823;&#30340;&#27169;&#22411;&#12290;&#34429;&#28982;&#36825;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#20294;&#20063;&#23558;&#35745;&#31639;&#25104;&#26412;&#25552;&#39640;&#21040;&#19981;&#21487;&#25345;&#32493;&#30340;&#27700;&#24179;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#24494;&#22937;&#30340;&#24179;&#34913;&#65292;&#36825;&#26159;&#35813;&#39046;&#22495;&#20013;&#19968;&#30452;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#26680;&#24515;&#23376;&#38598;&#36873;&#25321;&#36827;&#34892;&#37325;&#26032;&#21152;&#26435;&#30340;&#26032;&#26041;&#27861;&#65292;&#26377;&#25928;&#20248;&#21270;&#20102;&#35745;&#31639;&#26102;&#38388;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;&#36890;&#36807;&#19987;&#27880;&#20110; strategically selected coreset&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#31283;&#20581;&#30340;&#34920;&#31034;&#65292;&#22240;&#20026;&#23427;&#26377;&#25928;&#22320;&#26368;&#23567;&#21270;&#20102;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#37325;&#26032;&#26657;&#20934;&#30340;&#26435;&#37325;&#34987;&#26144;&#23556;&#22238;&#24182;&#20256;&#25773;&#21040;&#25972;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#31361;&#26174;&#20102;&#23427;&#20316;&#20026;&#27169;&#22411;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#21644;&#31934;&#30830;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12166v1 Announce Type: new  Abstract: As machine learning tasks continue to evolve, the trend has been to gather larger datasets and train increasingly larger models. While this has led to advancements in accuracy, it has also escalated computational costs to unsustainable levels. Addressing this, our work aims to strike a delicate balance between computational efficiency and model accuracy, a persisting challenge in the field. We introduce a novel method that employs core subset selection for reweighting, effectively optimizing both computational time and model performance. By focusing on a strategically selected coreset, our approach offers a robust representation, as it efficiently minimizes the influence of outliers. The re-calibrated weights are then mapped back to and propagated across the entire dataset. Our experimental results substantiate the effectiveness of this approach, underscoring its potential as a scalable and precise solution for model training.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#21442;&#25968;&#30340;&#35745;&#31639;&#22270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#26469;&#23454;&#29616;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#20351;&#24471;&#21333;&#20010;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#22810;&#31181;&#26550;&#26500;&#30340;&#31070;&#32463;&#35745;&#31639;&#22270;&#12290;</title><link>https://arxiv.org/abs/2403.12143</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#31561;&#21464;&#34920;&#31034;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Learning Equivariant Representations of Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#21442;&#25968;&#30340;&#35745;&#31639;&#22270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#26469;&#23454;&#29616;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#20351;&#24471;&#21333;&#20010;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#22810;&#31181;&#26550;&#26500;&#30340;&#31070;&#32463;&#35745;&#31639;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#35832;&#22914;&#20998;&#31867;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12289;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#21644;&#39044;&#27979;&#27867;&#21270;&#38169;&#35823;&#31561;&#39046;&#22495;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#24573;&#35270;&#31070;&#32463;&#32593;&#32476;&#20013;&#22266;&#26377;&#30340;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#35201;&#20040;&#20381;&#36182;&#22797;&#26434;&#30340;&#26435;&#37325;&#20849;&#20139;&#27169;&#24335;&#26469;&#23454;&#29616;&#31561;&#21464;&#24615;&#65292;&#21516;&#26102;&#24573;&#30053;&#32593;&#32476;&#26550;&#26500;&#26412;&#36523;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#21442;&#25968;&#30340;&#35745;&#31639;&#22270;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#24378;&#22823;&#30340;&#20445;&#30041;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#21333;&#20010;&#27169;&#22411;&#33021;&#22815;&#23545;&#20855;&#26377;&#22810;&#26679;&#26550;&#26500;&#30340;&#31070;&#32463;&#35745;&#31639;&#22270;&#36827;&#34892;&#32534;&#30721;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21253;&#25324;&#20998;&#31867;&#21644;&#32534;&#36753;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12289;&#39044;&#27979;&#27867;&#21270;&#38169;&#35823;&#31561;&#22810;&#31181;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12143v1 Announce Type: cross  Abstract: Neural networks that process the parameters of other neural networks find applications in domains as diverse as classifying implicit neural representations, generating neural network weights, and predicting generalization errors. However, existing approaches either overlook the inherent permutation symmetry in the neural network or rely on intricate weight-sharing patterns to achieve equivariance, while ignoring the impact of the network architecture itself. In this work, we propose to represent neural networks as computational graphs of parameters, which allows us to harness powerful graph neural networks and transformers that preserve permutation symmetry. Consequently, our approach enables a single model to encode neural computational graphs with diverse architectures. We showcase the effectiveness of our method on a wide range of tasks, including classification and editing of implicit neural representations, predicting generalizati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#38899;&#39057;&#35270;&#35273;&#24773;&#24863;&#27169;&#20223;&#24378;&#24230;&#20272;&#35745;&#30340;&#39640;&#25928;&#29305;&#24449;&#25552;&#21462;&#21644;&#24310;&#36831;&#34701;&#21512;&#31574;&#30053;</title><link>https://arxiv.org/abs/2403.11757</link><description>&lt;p&gt;
&#38899;&#39057;&#35270;&#35273;&#24773;&#24863;&#27169;&#20223;&#24378;&#24230;&#20272;&#35745;&#30340;&#39640;&#25928;&#29305;&#24449;&#25552;&#21462;&#21644;&#24310;&#36831;&#34701;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Efficient Feature Extraction and Late Fusion Strategy for Audiovisual Emotional Mimicry Intensity Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#38899;&#39057;&#35270;&#35273;&#24773;&#24863;&#27169;&#20223;&#24378;&#24230;&#20272;&#35745;&#30340;&#39640;&#25928;&#29305;&#24449;&#25552;&#21462;&#21644;&#24310;&#36831;&#34701;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#24773;&#24863;&#27169;&#20223;&#24378;&#24230;&#65288;EMI&#65289;&#20272;&#35745;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#25361;&#25112;&#26159;&#31532;&#20845;&#23626;&#38754;&#21521;&#37326;&#22806;&#24773;&#24863;&#34892;&#20026;&#20998;&#26512;&#65288;ABAW&#65289;&#31454;&#36187;&#30340;&#19968;&#37096;&#20998;&#12290;EMI&#20272;&#35745;&#25361;&#25112;&#20219;&#21153;&#26088;&#22312;&#36890;&#36807;&#20174;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#24773;&#32490;&#31867;&#21035;&#65288;&#21363;&#65292;&#8220;&#23815;&#25308;&#8221;&#65292;&#8220;&#23089;&#20048;&#8221;&#65292;&#8220;&#20915;&#24515;&#8221;&#65292;&#8220;&#20849;&#24773;&#24615;&#30140;&#30171;&#8221;&#65292;&#8220;&#20852;&#22859;&#8221;&#21644;&#8220;&#21916;&#24742;&#8221;&#65289;&#20013;&#35780;&#20272;&#23427;&#20204;&#26469;&#35780;&#20272;&#31181;&#23376;&#35270;&#39057;&#30340;&#24773;&#24863;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11757v1 Announce Type: cross  Abstract: In this paper, we present the solution to the Emotional Mimicry Intensity (EMI) Estimation challenge, which is part of 6th Affective Behavior Analysis in-the-wild (ABAW) Competition.The EMI Estimation challenge task aims to evaluate the emotional intensity of seed videos by assessing them from a set of predefined emotion categories (i.e., "Admiration," "Amusement," "Determination," "Empathic Pain," "Excitement," and "Joy").
&lt;/p&gt;</description></item><item><title>&#22312;&#23616;&#37096;&#35775;&#38382;&#27169;&#22411;&#19979;&#65292;&#36890;&#36807;&#24341;&#20837;Lin-Confident-FTRL&#31639;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#20855;&#26377;&#26356;&#20248;&#31934;&#24230;&#30028;&#38480;&#21644;&#26356;&#22909;&#25193;&#23637;&#24615;&#30340;&#22343;&#20540;&#24179;&#34913;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.11544</link><description>&lt;p&gt;
&#29420;&#31435;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#19982;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65306;&#22312;&#23616;&#37096;&#35775;&#38382;&#27169;&#22411;&#19979;&#25913;&#36827;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
RL en Markov Games with Independent Function Approximation: Improved Sample Complexity Bound under the Local Access Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11544
&lt;/p&gt;
&lt;p&gt;
&#22312;&#23616;&#37096;&#35775;&#38382;&#27169;&#22411;&#19979;&#65292;&#36890;&#36807;&#24341;&#20837;Lin-Confident-FTRL&#31639;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#20855;&#26377;&#26356;&#20248;&#31934;&#24230;&#30028;&#38480;&#21644;&#26356;&#22909;&#25193;&#23637;&#24615;&#30340;&#22343;&#20540;&#24179;&#34913;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#33324;&#21644;Markov&#21338;&#24328;&#20013;&#26377;&#25928;&#22320;&#23398;&#20064;&#20855;&#26377;&#22823;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#22343;&#20540;&#24179;&#34913;&#65292;&#21516;&#26102;&#20811;&#26381;&#22810;&#26041;&#20195;&#29702;&#30340;&#22256;&#22659;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#20351;&#29992;&#29420;&#31435;&#32447;&#24615;&#20989;&#25968;&#31867;&#26469;&#36924;&#36817;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#36793;&#38469;$Q$&#20540;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#26679;&#19968;&#20010;&#26694;&#26550;&#19979;&#29616;&#26377;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#23545;&#25152;&#38656;&#31934;&#24230;$\varepsilon$&#25110;&#21160;&#20316;&#31354;&#38388;&#20855;&#26377;&#27425;&#20248;&#20381;&#36182;&#24615;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;Lin-Confident-FTRL&#65292;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#23616;&#37096;&#23545;&#27169;&#25311;&#22120;&#30340;&#35775;&#38382;&#26435;&#38480;&#30340;&#31895;&#31890;&#24230;&#30456;&#20851;&#22343;&#34913;&#65288;CCE&#65289;&#65292;&#21363;&#21487;&#20197;&#19982;&#35775;&#38382;&#29366;&#24577;&#30340;&#22522;&#30784;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#22312;&#23545;&#29366;&#24577;&#31354;&#38388;&#22823;&#23567;&#36827;&#34892;&#23545;&#25968;&#30456;&#20851;&#24615;&#30340;&#21516;&#26102;&#65292;Lin-Confident-FTRL&#23398;&#20064;$\epsilon$-CCE&#65292;&#24182;&#33719;&#24471;&#20855;&#26377;&#22791;&#20221;&#30340;&#26368;&#20339;&#31934;&#24230;&#30028;&#38480;$O(\epsilon^{-2})&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#23545;&#21160;&#20316;&#31354;&#38388;&#30340;&#32447;&#24615;&#20381;&#23384;&#20851;&#31995;&#65292;&#21516;&#26102;&#20197;&#22810;&#39033;&#24335;&#26041;&#24335;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11544v1 Announce Type: new  Abstract: Efficiently learning equilibria with large state and action spaces in general-sum Markov games while overcoming the curse of multi-agency is a challenging problem. Recent works have attempted to solve this problem by employing independent linear function classes to approximate the marginal $Q$-value for each agent. However, existing sample complexity bounds under such a framework have a suboptimal dependency on the desired accuracy $\varepsilon$ or the action space. In this work, we introduce a new algorithm, Lin-Confident-FTRL, for learning coarse correlated equilibria (CCE) with local access to the simulator, i.e., one can interact with the underlying environment on the visited states. Up to a logarithmic dependence on the size of the state space, Lin-Confident-FTRL learns $\epsilon$-CCE with a provable optimal accuracy bound $O(\epsilon^{-2})$ and gets rids of the linear dependency on the action space, while scaling polynomially with 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#28151;&#21512;&#31574;&#30053;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#39118;&#38505;&#20013;&#24615;&#31574;&#30053;&#21644;&#21487;&#35843;&#25972;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;CVaR&#20248;&#21270;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.11062</link><description>&lt;p&gt;
&#29992;&#20110;&#25552;&#39640;CVaR&#20248;&#21270;&#26679;&#26412;&#25928;&#29575;&#30340;&#31616;&#21333;&#28151;&#21512;&#31574;&#30053;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Simple Mixture Policy Parameterization for Improving Sample Efficiency of CVaR Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11062
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#28151;&#21512;&#31574;&#30053;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#39118;&#38505;&#20013;&#24615;&#31574;&#30053;&#21644;&#21487;&#35843;&#25972;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;CVaR&#20248;&#21270;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#31574;&#30053;&#26799;&#24230;(PG)&#20248;&#21270;&#26465;&#20214;&#20540;&#39118;&#38505;(CVaR)&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#28151;&#21512;&#31574;&#30053;&#21442;&#25968;&#21270;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#39118;&#38505;&#20013;&#24615;&#31574;&#30053;&#19982;&#21487;&#35843;&#25972;&#31574;&#30053;&#25972;&#21512;&#20026;&#19968;&#20010;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#12290;&#36890;&#36807;&#37319;&#29992;&#36825;&#31181;&#31574;&#30053;&#65292;&#25152;&#26377;&#25910;&#38598;&#21040;&#30340;&#36712;&#36857;&#37117;&#21487;&#20197;&#29992;&#20110;&#31574;&#30053;&#26356;&#26032;&#65292;&#24182;&#19988;&#36890;&#36807;&#39118;&#38505;&#20013;&#24615;&#32452;&#20214;&#21050;&#28608;&#26356;&#39640;&#30340;&#22238;&#25253;&#65292;&#20174;&#32780;&#25552;&#21319;&#23614;&#37096;&#24182;&#38450;&#27490;&#25153;&#24179;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#28151;&#21512;&#21442;&#25968;&#21270;&#26159;&#38750;&#24120;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11062v1 Announce Type: new  Abstract: Reinforcement learning algorithms utilizing policy gradients (PG) to optimize Conditional Value at Risk (CVaR) face significant challenges with sample inefficiency, hindering their practical applications. This inefficiency stems from two main facts: a focus on tail-end performance that overlooks many sampled trajectories, and the potential of gradient vanishing when the lower tail of the return distribution is overly flat. To address these challenges, we propose a simple mixture policy parameterization. This method integrates a risk-neutral policy with an adjustable policy to form a risk-averse policy. By employing this strategy, all collected trajectories can be utilized for policy updating, and the issue of vanishing gradients is counteracted by stimulating higher returns through the risk-neutral component, thus lifting the tail and preventing flatness. Our empirical study reveals that this mixture parameterization is uniquely effectiv
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#26434;&#24230;&#30340;&#25552;&#31034;&#36873;&#25321;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#31034;&#20363;&#19982;&#27979;&#35797;&#21477;&#23376;&#30340;&#21477;&#27861;-&#35821;&#20041;&#22797;&#26434;&#24230;&#23545;&#40784;&#65292;&#22312;&#23569;&#26679;&#26412;NER&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.03861</link><description>&lt;p&gt;
&#20026;&#23569;&#26679;&#26412;&#31034;&#20363;&#36873;&#25321;&#35774;&#35745;&#20449;&#24687;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Designing Informative Metrics for Few-Shot Example Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03861
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#26434;&#24230;&#30340;&#25552;&#31034;&#36873;&#25321;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#31034;&#20363;&#19982;&#27979;&#35797;&#21477;&#23376;&#30340;&#21477;&#27861;-&#35821;&#20041;&#22797;&#26434;&#24230;&#23545;&#40784;&#65292;&#22312;&#23569;&#26679;&#26412;NER&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#25552;&#20379;&#36866;&#24403;&#26684;&#24335;&#30340;&#31034;&#20363;&#26102;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#8220;&#26368;&#20339;&#8221;&#31034;&#20363;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#26434;&#24230;&#30340;&#25552;&#31034;&#36873;&#25321;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#35757;&#32451;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#36873;&#25321;&#31034;&#20363;&#30340;&#27169;&#22411;&#65292;&#32780;&#26159;&#20351;&#29992;&#29305;&#23450;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#23545;&#40784;&#27979;&#35797;&#21477;&#23376;&#21644;&#31034;&#20363;&#30340;&#21477;&#27861;-&#35821;&#20041;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#21477;&#23376;&#21644;&#21333;&#35789;&#32423;&#21035;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#23558;&#31034;&#20363;&#30340;&#22797;&#26434;&#24230;&#19982;&#32771;&#34385;&#20013;&#30340;&#65288;&#27979;&#35797;&#65289;&#21477;&#23376;&#36827;&#34892;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20174;PLMs&#20013;&#25552;&#21462;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65306;&#22312;&#23569;&#26679;&#26412;NER&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22312;CoNLL2003&#25968;&#25454;&#38598;&#19978;&#23545;GPT-4&#30340;F1&#20998;&#25968;&#23454;&#29616;&#20102;5%&#30340;&#32477;&#23545;&#25913;&#21892;&#12290;&#25105;&#20204;&#36824;&#22312;&#20687;GPT-j-6B&#36825;&#26679;&#30340;&#36739;&#23567;&#27169;&#22411;&#20013;&#30475;&#21040;&#20102;&#39640;&#36798;28.85&#20010;&#28857;&#65288;F1/Acc.&#65289;&#30340;&#26174;&#33879;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03861v1 Announce Type: new  Abstract: Pretrained language models (PLMs) have shown remarkable few-shot learning capabilities when provided with properly formatted examples. However, selecting the "best" examples remains an open challenge. We propose a complexity-based prompt selection approach for sequence tagging tasks. This approach avoids the training of a dedicated model for selection of examples, and instead uses certain metrics to align the syntactico-semantic complexity of test sentences and examples. We use both sentence- and word-level metrics to match the complexity of examples to the (test) sentence being considered. Our results demonstrate that our approach extracts greater performance from PLMs: it achieves state-of-the-art performance on few-shot NER, achieving a 5% absolute improvement in F1 score on the CoNLL2003 dataset for GPT-4. We also see large gains of upto 28.85 points (F1/Acc.) in smaller models like GPT-j-6B.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#28041;&#21450;&#23545;&#25239;&#25439;&#22833;&#21644;&#30828;&#32422;&#26463;&#30340;CMDP&#65292;&#22312;&#20004;&#31181;&#19981;&#21516;&#24773;&#24418;&#19979;&#35774;&#35745;&#20102;&#20855;&#26377;&#27425;&#32447;&#24615;&#36951;&#25022;&#30340;&#31639;&#27861;&#65292;&#22635;&#34917;&#20102;&#20808;&#21069;&#30740;&#31350;&#20013;&#23545;&#36825;&#19968;&#38382;&#39064;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.03672</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#38543;&#26426;&#30828;&#32422;&#26463;&#30340;&#23545;&#25239;MDP&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Adversarial MDPs with Stochastic Hard Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#28041;&#21450;&#23545;&#25239;&#25439;&#22833;&#21644;&#30828;&#32422;&#26463;&#30340;CMDP&#65292;&#22312;&#20004;&#31181;&#19981;&#21516;&#24773;&#24418;&#19979;&#35774;&#35745;&#20102;&#20855;&#26377;&#27425;&#32447;&#24615;&#36951;&#25022;&#30340;&#31639;&#27861;&#65292;&#22635;&#34917;&#20102;&#20808;&#21069;&#30740;&#31350;&#20013;&#23545;&#36825;&#19968;&#38382;&#39064;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#24102;&#26377;&#23545;&#25239;&#25439;&#22833;&#21644;&#38543;&#26426;&#30828;&#32422;&#26463;&#30340;&#21463;&#38480;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20004;&#31181;&#19981;&#21516;&#30340;&#24773;&#24418;&#12290;&#22312;&#31532;&#19968;&#31181;&#24773;&#24418;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#33324;CMDP&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#27425;&#32447;&#24615;&#36951;&#25022;&#21644;&#32047;&#31215;&#27491;&#32422;&#26463;&#36829;&#21453;&#12290;&#22312;&#31532;&#20108;&#31181;&#24773;&#24418;&#20013;&#65292;&#22312;&#19968;&#20010;&#25919;&#31574;&#20005;&#26684;&#28385;&#36275;&#32422;&#26463;&#23384;&#22312;&#19988;&#20026;&#23398;&#20064;&#32773;&#25152;&#20102;&#35299;&#30340;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#21516;&#26102;&#30830;&#20445;&#22312;&#27599;&#19968;&#36718;&#20013;&#32422;&#26463;&#20197;&#39640;&#27010;&#29575;&#24471;&#21040;&#28385;&#36275;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#30740;&#31350;&#26082;&#28041;&#21450;&#23545;&#25239;&#25439;&#22833;&#21448;&#28041;&#21450;&#30828;&#32422;&#26463;&#30340;CMDP&#30340;&#24037;&#20316;&#12290;&#23454;&#38469;&#19978;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#35201;&#20040;&#38598;&#20013;&#22312;&#26356;&#24369;&#30340;&#36719;&#32422;&#26463;&#19978;--&#20801;&#35768;&#27491;&#36829;&#21453;&#26469;&#25269;&#28040;&#36127;&#36829;&#21453;--&#35201;&#20040;&#23616;&#38480;&#20110;&#38543;&#26426;&#25439;&#22833;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#22788;&#29702;&#19968;&#33324;&#30340;&#38750;&#32479;&#35745;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03672v1 Announce Type: new  Abstract: We study online learning problems in constrained Markov decision processes (CMDPs) with adversarial losses and stochastic hard constraints. We consider two different scenarios. In the first one, we address general CMDPs, where we design an algorithm that attains sublinear regret and cumulative positive constraints violation. In the second scenario, under the mild assumption that a policy strictly satisfying the constraints exists and is known to the learner, we design an algorithm that achieves sublinear regret while ensuring that the constraints are satisfied at every episode with high probability. To the best of our knowledge, our work is the first to study CMDPs involving both adversarial losses and hard constraints. Indeed, previous works either focus on much weaker soft constraints--allowing for positive violation to cancel out negative ones--or are restricted to stochastic losses. Thus, our algorithms can deal with general non-stat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#24187;&#35273;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#29616;&#23454;&#22330;&#26223;&#20013;&#24212;&#29992;&#26102;&#36935;&#21040;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;LLMs&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02889</link><description>&lt;p&gt;
&#22312;&#23547;&#25214;&#30495;&#30456;&#65306;&#19968;&#31181;&#23457;&#38382;&#26041;&#27861;&#29992;&#20110;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
In Search of Truth: An Interrogation Approach to Hallucination Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02889
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#24187;&#35273;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#29616;&#23454;&#22330;&#26223;&#20013;&#24212;&#29992;&#26102;&#36935;&#21040;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;LLMs&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#35768;&#22810;&#36827;&#23637;&#24182;&#19988;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#36895;&#24230;&#24555;&#36895;&#21457;&#23637;&#65292;&#20294;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#65292;&#23427;&#20204;&#23545;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#30340;&#24433;&#21709;&#21644;&#25972;&#21512;&#20173;&#28982;&#26377;&#38480;&#12290;&#19968;&#20010;&#38459;&#30861;&#23427;&#20204;&#24191;&#27867;&#24212;&#29992;&#30340;&#20851;&#38190;&#22240;&#32032;&#26159;&#24187;&#35273;&#30340;&#21457;&#29983;&#65292;&#21363;LLMs&#21019;&#36896;&#20986;&#21548;&#36215;&#26469;&#30495;&#23454;&#20294;&#20559;&#31163;&#20107;&#23454;&#30495;&#30456;&#30340;&#31572;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#65292;&#36825;&#35299;&#20915;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#29616;&#23454;&#22330;&#26223;&#20013;&#24212;&#29992;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;LLMs&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#21253;&#25324;Llama-2&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#26368;&#26032;LLMs&#30340;&#24187;&#35273;&#27700;&#24179;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33258;&#21160;&#26816;&#27979;&#23427;&#20204;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#29305;&#23450;&#23454;&#39564;&#20013;&#35266;&#23519;&#21040;Llama-2&#36798;&#21040;62%&#30340;&#24187;&#35273;&#27700;&#24179;&#65292;&#32780;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27809;&#26377;&#20381;&#36182;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;87%&#30340;&#24179;&#34913;&#20934;&#30830;&#29575;&#65288;B-ACC&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02889v1 Announce Type: new  Abstract: Despite the many advances of Large Language Models (LLMs) and their unprecedented rapid evolution, their impact and integration into every facet of our daily lives is limited due to various reasons. One critical factor hindering their widespread adoption is the occurrence of hallucinations, where LLMs invent answers that sound realistic, yet drift away from factual truth. In this paper, we present a novel method for detecting hallucinations in large language models, which tackles a critical issue in the adoption of these models in various real-world scenarios. Through extensive evaluations across multiple datasets and LLMs, including Llama-2, we study the hallucination levels of various recent LLMs and demonstrate the effectiveness of our method to automatically detect them. Notably, we observe up to 62% hallucinations for Llama-2 in a specific experiment, where our method achieves a Balanced Accuracy (B-ACC) of 87%, all without relying 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#21462;&#20195;&#20154;&#24037;&#26631;&#27880;&#65292;&#23558;&#23450;&#20041;&#35270;&#35273;&#27010;&#24565;&#25152;&#38656;&#30340;&#20154;&#21147;&#25237;&#20837;&#20943;&#23569;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;</title><link>https://arxiv.org/abs/2403.02626</link><description>&lt;p&gt;
&#21033;&#29992;LLM&#24037;&#20855;&#23454;&#29616;&#20027;&#35266;&#35270;&#35273;&#20998;&#31867;&#30340;&#21327;&#20316;&#24314;&#27169;:&#20943;&#23569;&#20154;&#21147;&#25237;&#20837;
&lt;/p&gt;
&lt;p&gt;
Modeling Collaborator: Enabling Subjective Vision Classification With Minimal Human Effort via LLM Tool-Use
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02626
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#21462;&#20195;&#20154;&#24037;&#26631;&#27880;&#65292;&#23558;&#23450;&#20041;&#35270;&#35273;&#27010;&#24565;&#25152;&#38656;&#30340;&#20154;&#21147;&#25237;&#20837;&#20943;&#23569;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20869;&#23481;&#23457;&#26680;&#21040;&#37326;&#29983;&#21160;&#29289;&#20445;&#25252;&#65292;&#38656;&#35201;&#27169;&#22411;&#35782;&#21035;&#24494;&#22937;&#25110;&#20027;&#35266;&#35270;&#35273;&#27010;&#24565;&#30340;&#24212;&#29992;&#25968;&#37327;&#27491;&#22312;&#22686;&#21152;&#12290;&#20256;&#32479;&#19978;&#65292;&#20026;&#36825;&#31867;&#27010;&#24565;&#24320;&#21457;&#20998;&#31867;&#22120;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#24037;&#20316;&#65292;&#38656;&#35201;&#20197;&#26102;&#12289;&#22825;&#29978;&#33267;&#26376;&#26469;&#27979;&#37327;&#35782;&#21035;&#21644;&#27880;&#37322;&#35757;&#32451;&#25152;&#38656;&#25968;&#25454;&#12290;&#21363;&#20351;&#20351;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;&#25935;&#25463;&#24314;&#27169;&#25216;&#26415;&#65292;&#21487;&#20197;&#24555;&#36895;&#24341;&#23548;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#65292;&#29992;&#25143;&#20173;&#38656;&#35201;&#33457;&#36153;30&#20998;&#38047;&#29978;&#33267;&#26356;&#22810;&#30340;&#21333;&#35843;&#37325;&#22797;&#25968;&#25454;&#26631;&#27880;&#26102;&#38388;&#26469;&#35757;&#32451;&#21333;&#20010;&#20998;&#31867;&#22120;&#12290;&#20511;&#37492;&#33778;&#26031;&#20811;&#30340;&#35748;&#30693;&#25042;&#27721;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#29992;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#21462;&#20195;&#20154;&#24037;&#26631;&#27880;&#65292;&#20943;&#23569;&#23450;&#20041;&#27010;&#24565;&#25152;&#38656;&#30340;&#24635;&#20307;&#25237;&#20837;&#19968;&#20010;&#25968;&#37327;&#32423;&#65306;&#20174;&#26631;&#35760;2,000&#24352;&#22270;&#20687;&#21040;&#20165;&#38656;100&#24352;&#22270;&#20687;&#20877;&#21152;&#19978;&#19968;&#20123;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;f
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02626v1 Announce Type: cross  Abstract: From content moderation to wildlife conservation, the number of applications that require models to recognize nuanced or subjective visual concepts is growing. Traditionally, developing classifiers for such concepts requires substantial manual effort measured in hours, days, or even months to identify and annotate data needed for training. Even with recently proposed Agile Modeling techniques, which enable rapid bootstrapping of image classifiers, users are still required to spend 30 minutes or more of monotonous, repetitive data labeling just to train a single classifier. Drawing on Fiske's Cognitive Miser theory, we propose a new framework that alleviates manual effort by replacing human labeling with natural language interactions, reducing the total effort required to define a concept by an order of magnitude: from labeling 2,000 images to only 100 plus some natural language interactions. Our framework leverages recent advances in f
&lt;/p&gt;</description></item><item><title>&#36870;&#36716;&#35781;&#21650;&#38382;&#39064;&#26159;&#23548;&#33268;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#36827;&#34892;&#21452;&#21521;&#25512;&#29702;&#30340;&#26681;&#26412;&#21407;&#22240;&#20043;&#19968;&#65292;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#35821;&#20041;&#24863;&#30693;&#30340;&#32622;&#25442;&#35757;&#32451;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.00758</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#24863;&#30693;&#32622;&#25442;&#35757;&#32451;&#26469;&#32531;&#35299;&#36870;&#36716;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
Mitigating Reversal Curse via Semantic-aware Permutation Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00758
&lt;/p&gt;
&lt;p&gt;
&#36870;&#36716;&#35781;&#21650;&#38382;&#39064;&#26159;&#23548;&#33268;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#36827;&#34892;&#21452;&#21521;&#25512;&#29702;&#30340;&#26681;&#26412;&#21407;&#22240;&#20043;&#19968;&#65292;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#35821;&#20041;&#24863;&#30693;&#30340;&#32622;&#25442;&#35757;&#32451;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#65292;&#28982;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22240;&#26524;&#20851;&#31995;&#30340;LLM&#36973;&#36935;&#20102;&#8220;&#36870;&#36716;&#35781;&#21650;&#8221;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#20363;&#23376;&#26159;&#65292;&#27169;&#22411;&#30693;&#36947;&#8220;A&#30340;&#29238;&#20146;&#26159;B&#8221;&#65292;&#20294;&#26080;&#27861;&#25512;&#29702;&#20986;&#8220;B&#30340;&#23401;&#23376;&#26159;A&#8221;&#12290;&#36825;&#19968;&#23616;&#38480;&#24615;&#23545;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#36827;&#23637;&#26500;&#25104;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#26263;&#31034;&#20102;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#24212;&#29992;&#21452;&#21521;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#24046;&#36317;&#12290;&#26412;&#25991;&#39318;&#20808;&#36827;&#34892;&#20102;&#22823;&#37327;&#35780;&#20272;&#65292;&#24182;&#30830;&#23450;&#20102;&#36870;&#36716;&#35781;&#21650;&#30340;&#26681;&#26412;&#21407;&#22240;&#22312;&#20110;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#20043;&#38388;&#30340;&#35789;&#24207;&#19981;&#21516;&#65292;&#21363;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#39044;&#27979;&#20808;&#34892;&#35789;&#30340;&#33021;&#21147;&#19981;&#36275;&#12290;&#22240;&#27492;&#65292;&#32771;&#34385;&#21040;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#36827;&#34892;&#25490;&#21015;&#21487;&#20197;&#34987;&#35270;&#20026;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#36825;&#21487;&#20197;&#20351;&#27169;&#22411;&#39044;&#27979;&#20808;&#34892;&#35789;&#25110;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#25490;&#21015;&#26041;&#27861;&#21487;&#33021;&#21463;&#21040;&#25130;&#26029;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00758v1 Announce Type: cross  Abstract: While large language models (LLMs) have achieved impressive performance across diverse tasks, recent studies showcase that causal LLMs suffer from the "reversal curse". It is a typical example that the model knows "A's father is B", but is unable to reason "B's child is A". This limitation poses a challenge to the advancement of artificial general intelligence (AGI), as it suggests a gap in the models' ability to comprehend and apply bidirectional reasoning. In this paper, we first conduct substantial evaluation and identify that the root cause of the reversal curse lies in the different word order between the training and inference stage, namely, the poor ability of causal language models to predict antecedent words within the training data. Accordingly, permutation on the training data is considered as a potential solution, since this can make the model predict antecedent words or tokens. However, previous permutation methods may dis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#27599;&#38598;&#20351;&#29992;&#20984;&#20248;&#21270;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#26368;&#20248;$Q$-&#20989;&#25968;&#65292;&#30830;&#20445;&#25910;&#25947;&#21442;&#25968;&#21487;&#20197;&#26080;&#38480;&#25509;&#36817;&#26368;&#20248;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.19212</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#20010;&#20984;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning: A Convex Optimization Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#27599;&#38598;&#20351;&#29992;&#20984;&#20248;&#21270;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#26368;&#20248;$Q$-&#20989;&#25968;&#65292;&#30830;&#20445;&#25910;&#25947;&#21442;&#25968;&#21487;&#20197;&#26080;&#38480;&#25509;&#36817;&#26368;&#20248;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#38598;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#22312;&#27599;&#20010;&#38598;&#20013;&#20351;&#29992;&#20984;&#20248;&#21270;&#26469;&#25214;&#21040;&#26368;&#20248;$Q$-&#20989;&#25968;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#12290;&#20984;&#20248;&#21270;&#26041;&#27861;&#30830;&#20445;&#27599;&#20010;&#38598;&#21512;&#20013;&#35745;&#31639;&#30340;&#26435;&#37325;&#26159;&#26368;&#20248;&#30340;&#65292;&#20851;&#20110;&#24403;&#21069;&#38598;&#21512;&#30340;&#37319;&#26679;&#29366;&#24577;&#21644;&#21160;&#20316;&#12290;&#23545;&#20110;&#31283;&#23450;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#25910;&#25947;&#65292;&#24182;&#19988;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#25947;&#21442;&#25968;&#21487;&#20197;&#19982;&#26368;&#20248;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#26080;&#38480;&#25509;&#36817;&#12290;&#29305;&#21035;&#26159;&#65292;&#22914;&#26524;&#27491;&#21017;&#21270;&#21442;&#25968;&#20026;$\rho$&#65292;&#26102;&#38388;&#38271;&#24230;&#20026;$T$&#65292;&#37027;&#20040;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#25910;&#25947;&#21040;$w$&#65292;&#20854;&#20013;$w$&#19982;&#26368;&#20248;&#21442;&#25968;$w^\star$&#20043;&#38388;&#30340;&#36317;&#31163;&#21463;&#21040;$\mathcal{O}(\rho T^{-1})$&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19212v1 Announce Type: cross  Abstract: In this paper, we consider reinforcement learning of nonlinear systems with continuous state and action spaces. We present an episodic learning algorithm, where we for each episode use convex optimization to find a two-layer neural network approximation of the optimal $Q$-function. The convex optimization approach guarantees that the weights calculated at each episode are optimal, with respect to the given sampled states and actions of the current episode. For stable nonlinear systems, we show that the algorithm converges and that the converging parameters of the trained neural network can be made arbitrarily close to the optimal neural network parameters. In particular, if the regularization parameter is $\rho$ and the time horizon is $T$, then the parameters of the trained neural network converge to $w$, where the distance between $w$ from the optimal parameters $w^\star$ is bounded by $\mathcal{O}(\rho T^{-1})$. That is, when the nu
&lt;/p&gt;</description></item><item><title>TOTEM&#27169;&#22411;&#22312;&#24212;&#23545;&#20256;&#24863;&#22120;&#25925;&#38556;&#30340;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#20013;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18546</link><description>&lt;p&gt;
&#20256;&#24863;&#22120;&#25925;&#38556;&#19979;&#30340;&#27867;&#21270;&#24615;&#33021;&#65306;Tokenization + Transformers &#23454;&#29616;&#26356;&#20581;&#22766;&#30340;&#28508;&#22312;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Generalizability Under Sensor Failure: Tokenization + Transformers Enable More Robust Latent Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18546
&lt;/p&gt;
&lt;p&gt;
TOTEM&#27169;&#22411;&#22312;&#24212;&#23545;&#20256;&#24863;&#22120;&#25925;&#38556;&#30340;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#20013;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31185;&#23398;&#30340;&#19968;&#20010;&#20027;&#35201;&#30446;&#26631;&#26159;&#21457;&#29616;&#33021;&#22815;&#27867;&#21270;&#30340;&#31070;&#32463;&#25968;&#25454;&#34920;&#31034;&#12290;&#36825;&#19968;&#30446;&#26631;&#21463;&#21040;&#35760;&#24405;&#20250;&#35805;&#65288;&#20363;&#22914;&#29615;&#22659;&#65289;&#12289;&#21463;&#35797;&#32773;&#65288;&#20363;&#22914;&#21464;&#21270;&#30340;&#31070;&#32463;&#32467;&#26500;&#65289;&#21644;&#20256;&#24863;&#22120;&#65288;&#20363;&#22914;&#20256;&#24863;&#22120;&#22122;&#22768;&#65289;&#31561;&#22240;&#32032;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#24050;&#32463;&#24320;&#22987;&#35299;&#20915;&#36328;&#20250;&#35805;&#21644;&#21463;&#35797;&#32773;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#38024;&#23545;&#22312;&#31070;&#32463;&#31185;&#23398;&#23454;&#39564;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#20256;&#24863;&#22120;&#25925;&#38556;&#30340;&#31283;&#20581;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#27867;&#21270;&#24615;&#32500;&#24230;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;&#25105;&#20204;&#33258;&#24049;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#20010;&#20250;&#35805;&#12289;&#21463;&#35797;&#32773;&#21644;&#20256;&#24863;&#22120;&#65292;&#28982;&#21518;&#30740;&#31350;&#20102;&#20004;&#20010;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65306;EEGNet&#65288;Lawhern&#31561;&#20154;&#65292;2018&#65289;&#21644;TOTEM&#65288;Talukder&#31561;&#20154;&#65292;2024&#65289;&#12290;EEGNet &#26159;&#19968;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780; TOTEM &#26159;&#19968;&#20010;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#26631;&#35760;&#22120;&#21644; Transformer &#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25152;&#26377;&#27867;&#21270;&#26696;&#20363;&#20013;&#65292;TOTEM &#30340;&#34920;&#29616;&#20248;&#20110;&#25110;&#19982; EEGNet &#30456;&#21305;&#37197;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#20998;&#26512; TOTEM &#30340;&#28508;&#22312;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18546v1 Announce Type: new  Abstract: A major goal in neuroscience is to discover neural data representations that generalize. This goal is challenged by variability along recording sessions (e.g. environment), subjects (e.g. varying neural structures), and sensors (e.g. sensor noise), among others. Recent work has begun to address generalization across sessions and subjects, but few study robustness to sensor failure which is highly prevalent in neuroscience experiments. In order to address these generalizability dimensions we first collect our own electroencephalography dataset with numerous sessions, subjects, and sensors, then study two time series models: EEGNet (Lawhern et al., 2018) and TOTEM (Talukder et al., 2024). EEGNet is a widely used convolutional neural network, while TOTEM is a discrete time series tokenizer and transformer model. We find that TOTEM outperforms or matches EEGNet across all generalizability cases. Finally through analysis of TOTEM's latent cod
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24230;&#37327;&#23398;&#20064;&#21644;&#30417;&#30563;&#26041;&#27861;&#25972;&#21512;&#21040;fNIRS&#30740;&#31350;&#20013;&#65292;&#20197;&#25552;&#39640;&#32593;&#32476;&#22312;&#35782;&#21035;&#21644;&#25490;&#38500;&#20998;&#24067;&#20043;&#22806;&#30340;&#24322;&#24120;&#20540;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#21508;&#31181;fNIRS&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;transformer-based&#30340;&#32593;&#32476;&#65292;&#26174;&#31034;&#20986;&#26356;&#22823;&#30340;&#21487;&#38752;&#24615;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.18112</link><description>&lt;p&gt;
&#31616;&#21333;&#32780;&#26377;&#25928;&#65306;&#37325;&#26032;&#24605;&#32771;&#28145;&#24230;&#23398;&#20064;&#22312;fNIRS&#20013;&#25490;&#38500;&#24322;&#24120;&#36755;&#20837;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Simple But Effective: Rethinking the Ability of Deep Learning in fNIRS to Exclude Abnormal Input
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18112
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24230;&#37327;&#23398;&#20064;&#21644;&#30417;&#30563;&#26041;&#27861;&#25972;&#21512;&#21040;fNIRS&#30740;&#31350;&#20013;&#65292;&#20197;&#25552;&#39640;&#32593;&#32476;&#22312;&#35782;&#21035;&#21644;&#25490;&#38500;&#20998;&#24067;&#20043;&#22806;&#30340;&#24322;&#24120;&#20540;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#21508;&#31181;fNIRS&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;transformer-based&#30340;&#32593;&#32476;&#65292;&#26174;&#31034;&#20986;&#26356;&#22823;&#30340;&#21487;&#38752;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#24615;&#36817;&#32418;&#22806;&#20809;&#35889;&#65288;fNIRS&#65289;&#26159;&#19968;&#31181;&#38750;&#20405;&#20837;&#24335;&#30417;&#27979;&#22823;&#33041;&#27963;&#21160;&#30340;&#25216;&#26415;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#22823;&#33041;&#65292;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#24212;&#23545;fNIRS&#25968;&#25454;&#30340;&#20998;&#31867;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#24403;&#21069;fNIRS&#20013;&#30340;&#32593;&#32476;&#22312;&#20854;&#35757;&#32451;&#20998;&#24067;&#20869;&#36827;&#34892;&#39044;&#27979;&#26102;&#20934;&#30830;&#29575;&#24456;&#39640;&#65292;&#20294;&#22312;&#35782;&#21035;&#21644;&#25490;&#38500;&#20998;&#24067;&#20043;&#22806;&#30340;&#24322;&#24120;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#24433;&#21709;&#20102;&#20854;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#24230;&#37327;&#23398;&#20064;&#21644;&#30417;&#30563;&#26041;&#27861;&#25972;&#21512;&#21040;fNIRS&#30740;&#31350;&#20013;&#65292;&#20197;&#25552;&#39640;&#32593;&#32476;&#22312;&#35782;&#21035;&#21644;&#25490;&#38500;&#20998;&#24067;&#20043;&#22806;&#30340;&#24322;&#24120;&#20540;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#23427;&#26174;&#33879;&#25552;&#21319;&#20102;&#21508;&#31181;fNIRS&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;transformer&#30340;&#32593;&#32476;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#23558;&#22312;GitHub&#19978;&#20844;&#24320;&#25105;&#20204;&#30340;&#23454;&#39564;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18112v1 Announce Type: cross  Abstract: Functional near-infrared spectroscopy (fNIRS) is a non-invasive technique for monitoring brain activity. To better understand the brain, researchers often use deep learning to address the classification challenges of fNIRS data. Our study shows that while current networks in fNIRS are highly accurate for predictions within their training distribution, they falter at identifying and excluding abnormal data which is out-of-distribution, affecting their reliability. We propose integrating metric learning and supervised methods into fNIRS research to improve networks capability in identifying and excluding out-of-distribution outliers. This method is simple yet effective. In our experiments, it significantly enhances the performance of various networks in fNIRS, particularly transformer-based one, which shows the great improvement in reliability. We will make our experiment data available on GitHub.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#24518;&#38459;&#22120;&#24320;&#21457;&#38543;&#26426;&#36923;&#36753;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#33391;&#22909;&#35843;&#33410;&#27010;&#29575;&#21644;&#30456;&#20851;&#24615;&#30340;&#38543;&#26426;&#25968;&#23383;&#32534;&#30721;&#21644;&#22788;&#29702;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#38543;&#26426;Roberts&#20132;&#21449;&#31639;&#23376;&#29992;&#20110;&#36793;&#32536;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.16908</link><description>&lt;p&gt;
&#21033;&#29992;&#24518;&#38459;&#22120;&#21551;&#29992;&#30340;&#38543;&#26426;&#36923;&#36753;&#36827;&#34892;&#26412;&#22320;&#38543;&#26426;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Local stochastic computing using memristor-enabled stochastic logics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16908
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#24518;&#38459;&#22120;&#24320;&#21457;&#38543;&#26426;&#36923;&#36753;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#33391;&#22909;&#35843;&#33410;&#27010;&#29575;&#21644;&#30456;&#20851;&#24615;&#30340;&#38543;&#26426;&#25968;&#23383;&#32534;&#30721;&#21644;&#22788;&#29702;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#38543;&#26426;Roberts&#20132;&#21449;&#31639;&#23376;&#29992;&#20110;&#36793;&#32536;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#35745;&#31639;&#25552;&#20379;&#20102;&#19968;&#31181;&#27010;&#29575;&#26041;&#27861;&#26469;&#35299;&#20915;&#21508;&#20010;&#39046;&#22495;&#20013;&#30001;&#20110;&#19981;&#30830;&#23450;&#24615;&#21644;&#22122;&#22768;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#38543;&#26426;&#35745;&#31639;&#38754;&#20020;&#30528;&#21457;&#23637;&#21487;&#38752;&#30340;&#38543;&#26426;&#36923;&#36753;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#24518;&#38459;&#22120;&#24320;&#21457;&#38543;&#26426;&#36923;&#36753;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#24518;&#38459;&#22120;&#38598;&#25104;&#21040;&#36923;&#36753;&#30005;&#36335;&#20013;&#35774;&#35745;&#38543;&#26426;&#36923;&#36753;&#65292;&#20854;&#20013;&#24518;&#38459;&#22120;&#20999;&#25442;&#20013;&#22266;&#26377;&#30340;&#38543;&#26426;&#24615;&#34987;&#21033;&#29992;&#26469;&#23454;&#29616;&#20855;&#26377;&#33391;&#22909;&#35843;&#33410;&#27010;&#29575;&#21644;&#30456;&#20851;&#24615;&#30340;&#38543;&#26426;&#25968;&#23383;&#32534;&#30721;&#21644;&#22788;&#29702;&#12290;&#20316;&#20026;&#38543;&#26426;&#36923;&#36753;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#36793;&#32536;&#26816;&#27979;&#30340;&#32039;&#20945;&#22411;&#38543;&#26426;Roberts&#20132;&#21449;&#31639;&#23376;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35813;&#31639;&#23376;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#36718;&#24275;&#21644;&#32441;&#29702;&#25552;&#21462;&#33021;&#21147;&#65292;&#21363;&#20351;&#23384;&#22312;50%&#30340;&#22122;&#38899;&#65292;&#30001;&#20110;&#20854;&#27010;&#29575;&#24615;&#36136;&#21644;&#32039;&#20945;&#30340;&#35774;&#35745;&#65292;&#35813;&#31639;&#23376;&#33021;&#22815;&#33410;&#30465;95%&#30340;&#33021;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16908v1 Announce Type: cross  Abstract: Stochastic computing offers a probabilistic approach to address challenges posed by problems with uncertainty and noise in various fields, particularly machine learning. The realization of stochastic computing, however, faces the limitation of developing reliable stochastic logics. Here, we present stochastic logics development using memristors. Specifically, we integrate memristors into logic circuits to design the stochastic logics, wherein the inherent stochasticity in memristor switching is harnessed to enable stochastic number encoding and processing with well-regulated probabilities and correlations. As a practical application of the stochastic logics, we design a compact stochastic Roberts cross operator for edge detection. Remarkably, the operator demonstrates exceptional contour and texture extractions, even in the presence of 50% noise, and owning to the probabilistic nature and compact design, the operator can consume 95% le
&lt;/p&gt;</description></item><item><title>AgentOhana&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#20811;&#26381;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26234;&#33021;&#20307;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15506</link><description>&lt;p&gt;
AgentOhana&#65306;&#20026;&#26377;&#25928;&#26234;&#33021;&#20307;&#23398;&#20064;&#35774;&#35745;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15506
&lt;/p&gt;
&lt;p&gt;
AgentOhana&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#20811;&#26381;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26234;&#33021;&#20307;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#25903;&#25345;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#24341;&#36215;&#20102;&#37325;&#22823;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20805;&#20998;&#21033;&#29992;LLMs&#30340;&#28508;&#21147;&#36827;&#34892;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#38754;&#20020;&#22256;&#38590;&#65292;&#36825;&#26159;&#30001;&#20110;&#20855;&#26377;&#22810;&#36718;&#36712;&#36857;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#28304;&#30340;&#24322;&#26500;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;AgentOhana&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#12290;AgentOhana&#20174;&#19981;&#21516;&#29615;&#22659;&#20013;&#32858;&#21512;&#26234;&#33021;&#20307;&#36712;&#36857;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#24773;&#26223;&#12290;&#23427;&#31934;&#24515;&#22320;&#23558;&#36825;&#20123;&#36712;&#36857;&#26631;&#20934;&#21270;&#21644;&#32479;&#19968;&#21040;&#19968;&#33268;&#30340;&#26684;&#24335;&#20013;&#65292;&#31616;&#21270;&#20102;&#20026;&#26234;&#33021;&#20307;&#35757;&#32451;&#20248;&#21270;&#30340;&#36890;&#29992;&#25968;&#25454;&#21152;&#36733;&#22120;&#30340;&#21019;&#24314;&#12290;&#36890;&#36807;&#25968;&#25454;&#32479;&#19968;&#65292;&#25105;&#20204;&#30340;&#35757;&#32451;&#27969;&#27700;&#32447;&#22312;&#19981;&#21516;&#25968;&#25454;&#28304;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#21010;&#20998;&#21644;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#35774;&#22791;&#20043;&#38388;&#30340;&#29420;&#31435;&#38543;&#26426;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;xLAM-v0.1&#65292;&#19968;&#20010;&#22823;&#21160;&#20316;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15506v1 Announce Type: new  Abstract: Autonomous agents powered by large language models (LLMs) have garnered significant research attention. However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories. In this paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address these challenges. \textit{AgentOhana} aggregates agent trajectories from distinct environments, spanning a wide array of scenarios. It meticulously standardizes and unifies these trajectories into a consistent format, streamlining the creation of a generic data loader optimized for agent training. Leveraging the data unification, our training pipeline maintains equilibrium across different data sources and preserves independent randomness across devices during dataset partitioning and model training. Additionally, we present \textbf{xLAM-v0.1}, a large action mode
&lt;/p&gt;</description></item><item><title>&#22312;fNIRS&#39046;&#22495;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#26657;&#20934;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#20197;&#35780;&#20272;&#20854;&#21487;&#38752;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#35768;&#22810;&#29616;&#26377;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#33021;&#19981;&#20339;&#12290;</title><link>https://arxiv.org/abs/2402.15266</link><description>&lt;p&gt;
fNIRS&#20013;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#27169;&#22411;&#30340;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Calibration of Deep Learning Classification Models in fNIRS
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15266
&lt;/p&gt;
&lt;p&gt;
&#22312;fNIRS&#39046;&#22495;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#26657;&#20934;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#20197;&#35780;&#20272;&#20854;&#21487;&#38752;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#35768;&#22810;&#29616;&#26377;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#33021;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#24615;&#36817;&#32418;&#22806;&#20809;&#35889;&#65288;fNIRS&#65289;&#26159;&#29992;&#20110;&#30417;&#27979;&#33041;&#27963;&#21160;&#30340;&#23453;&#36149;&#26080;&#21019;&#24037;&#20855;&#12290;&#19982;&#24847;&#35782;&#27963;&#21160;&#30456;&#20851;&#30340;fNIRS&#25968;&#25454;&#20998;&#31867;&#23545;&#20110;&#25512;&#36827;&#25105;&#20204;&#23545;&#22823;&#33041;&#30340;&#29702;&#35299;&#21644;&#20419;&#36827;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#30340;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#36716;&#21521;&#28145;&#24230;&#23398;&#20064;&#26469;&#35299;&#20915;fNIRS&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#20998;&#31867;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#24456;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;&#22312;fNIRS&#30340;&#24212;&#29992;&#20013;&#65292;&#21487;&#38752;&#24615;&#38750;&#24120;&#37325;&#35201;&#65292;&#32622;&#20449;&#24230;&#21487;&#38752;&#24615;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#20043;&#19968;&#23601;&#26159;&#26657;&#20934;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#24573;&#35270;&#20102;&#26657;&#20934;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#26657;&#20934;&#34701;&#20837;fNIRS&#39046;&#22495;&#65292;&#24182;&#35780;&#20272;&#29616;&#26377;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35768;&#22810;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#26657;&#20934;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#25512;&#21160;fNIRS&#39046;&#22495;&#30340;&#26657;&#20934;&#21457;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986; ...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15266v1 Announce Type: new  Abstract: Functional near-infrared spectroscopy (fNIRS) is a valuable non-invasive tool for monitoring brain activity. The classification of fNIRS data in relation to conscious activity holds significance for advancing our understanding of the brain and facilitating the development of brain-computer interfaces (BCI). Many researchers have turned to deep learning to tackle the classification challenges inherent in fNIRS data due to its strong generalization and robustness. In the application of fNIRS, reliability is really important, and one mathematical formulation of the reliability of confidence is calibration. However, many researchers overlook the important issue of calibration. To address this gap, we propose integrating calibration into fNIRS field and assess the reliability of existing models. Surprisingly, our results indicate poor calibration performance in many proposed models. To advance calibration development in the fNIRS field, we su
&lt;/p&gt;</description></item><item><title>MC-DBN&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#20449;&#24565;&#32593;&#32476;&#30340;&#27169;&#24577;&#34917;&#20840;&#27169;&#22411;&#65292;&#21033;&#29992;&#23436;&#25972;&#25968;&#25454;&#30340;&#38544;&#24335;&#29305;&#24449;&#26469;&#24357;&#34917;&#38468;&#21152;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#24046;&#36317;&#65292;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09782</link><description>&lt;p&gt;
MC-DBN&#65306;&#22522;&#20110;&#28145;&#24230;&#20449;&#24565;&#32593;&#32476;&#30340;&#27169;&#24577;&#34917;&#20840;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MC-DBN: A Deep Belief Network-Based Model for Modality Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09782
&lt;/p&gt;
&lt;p&gt;
MC-DBN&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#20449;&#24565;&#32593;&#32476;&#30340;&#27169;&#24577;&#34917;&#20840;&#27169;&#22411;&#65292;&#21033;&#29992;&#23436;&#25972;&#25968;&#25454;&#30340;&#38544;&#24335;&#29305;&#24449;&#26469;&#24357;&#34917;&#38468;&#21152;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#24046;&#36317;&#65292;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#36827;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#32929;&#24066;&#39044;&#27979;&#21644;&#24515;&#29575;&#30417;&#27979;&#39046;&#22495;&#12290;&#21033;&#29992;&#22810;&#26679;&#30340;&#25968;&#25454;&#28304;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#39069;&#22806;&#30340;&#25968;&#25454;&#21487;&#33021;&#19981;&#24635;&#26159;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#30456;&#21563;&#21512;&#12290;&#25554;&#20540;&#26041;&#27861;&#36890;&#24120;&#29992;&#20110;&#22788;&#29702;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#20540;&#65292;&#20294;&#22312;&#31232;&#30095;&#20449;&#24687;&#24773;&#20917;&#19979;&#21487;&#33021;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#34917;&#20840;&#30340;&#28145;&#24230;&#20449;&#24565;&#32593;&#32476;&#27169;&#22411;&#65288;MC-DBN&#65289;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23436;&#25972;&#25968;&#25454;&#30340;&#38544;&#24335;&#29305;&#24449;&#26469;&#24357;&#34917;&#33258;&#36523;&#19982;&#38468;&#21152;&#19981;&#23436;&#25972;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23427;&#30830;&#20445;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#29305;&#24615;&#23494;&#20999;&#30456;&#31526;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26469;&#33258;&#32929;&#24066;&#39044;&#27979;&#21644;&#24515;&#29575;&#30417;&#27979;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;MC-DBN&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09782v1 Announce Type: cross  Abstract: Recent advancements in multi-modal artificial intelligence (AI) have revolutionized the fields of stock market forecasting and heart rate monitoring. Utilizing diverse data sources can substantially improve prediction accuracy. Nonetheless, additional data may not always align with the original dataset. Interpolation methods are commonly utilized for handling missing values in modal data, though they may exhibit limitations in the context of sparse information. Addressing this challenge, we propose a Modality Completion Deep Belief Network-Based Model (MC-DBN). This approach utilizes implicit features of complete data to compensate for gaps between itself and additional incomplete data. It ensures that the enhanced multi-modal data closely aligns with the dynamic nature of the real world to enhance the effectiveness of the model. We conduct evaluations of the MC-DBN model in two datasets from the stock market forecasting and heart rate
&lt;/p&gt;</description></item><item><title>NetInfoF&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#24555;&#36895;&#24230;&#37327;&#21644;&#21033;&#29992;&#33410;&#28857;&#23646;&#24615;&#22270;&#20013;&#30340;&#21487;&#21033;&#29992;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#33021;&#21516;&#26102;&#22788;&#29702;&#38142;&#36335;&#39044;&#27979;&#21644;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#20855;&#22791;&#29702;&#35770;&#20445;&#35777;&#21644;&#38381;&#24335;&#35299;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.07999</link><description>&lt;p&gt;
NetInfoF &#26694;&#26550;&#65306;&#24230;&#37327;&#21644;&#21033;&#29992;&#32593;&#32476;&#21487;&#21033;&#29992;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
NetInfoF Framework: Measuring and Exploiting Network Usable Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07999
&lt;/p&gt;
&lt;p&gt;
NetInfoF&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#24555;&#36895;&#24230;&#37327;&#21644;&#21033;&#29992;&#33410;&#28857;&#23646;&#24615;&#22270;&#20013;&#30340;&#21487;&#21033;&#29992;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#33021;&#21516;&#26102;&#22788;&#29702;&#38142;&#36335;&#39044;&#27979;&#21644;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#20855;&#22791;&#29702;&#35770;&#20445;&#35777;&#21644;&#38381;&#24335;&#35299;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19968;&#20010;&#33410;&#28857;&#23646;&#24615;&#22270;&#21644;&#19968;&#20010;&#22270;&#20219;&#21153;&#65288;&#38142;&#36335;&#39044;&#27979;&#25110;&#33410;&#28857;&#20998;&#31867;&#65289;&#65292;&#25105;&#20204;&#33021;&#21542;&#21028;&#26029;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#21542;&#33021;&#24456;&#22909;&#22320;&#23436;&#25104;&#20219;&#21153;&#65311;&#20855;&#20307;&#32780;&#35328;&#65292;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#26159;&#21542;&#21253;&#21547;&#36275;&#22815;&#21487;&#21033;&#29992;&#30340;&#20449;&#24687;&#26469;&#23436;&#25104;&#20219;&#21153;&#65311;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#65288;1&#65289;&#24320;&#21457;&#19968;&#20010;&#24555;&#36895;&#24037;&#20855;&#26469;&#24230;&#37327;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#37327;&#65292;&#20197;&#21450;&#65288;2&#65289;&#26681;&#25454;&#20449;&#24687;&#37327;&#26469;&#35299;&#20915;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;NetInfoF&#26694;&#26550;&#65292;&#21253;&#25324;NetInfoF_Probe&#21644;NetInfoF_Act&#20004;&#20010;&#37096;&#20998;&#65292;&#20998;&#21035;&#29992;&#20110;&#24230;&#37327;&#21644;&#21033;&#29992;&#32593;&#32476;&#21487;&#21033;&#29992;&#20449;&#24687;&#65288;NUI&#65289;&#12290;&#32473;&#23450;&#19968;&#20010;&#22270;&#25968;&#25454;&#65292;NetInfoF_Probe&#22312;&#19981;&#36827;&#34892;&#20219;&#20309;&#27169;&#22411;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#24230;&#37327;NUI&#65292;&#32780;NetInfoF_Act&#21017;&#29992;&#20110;&#35299;&#20915;&#38142;&#36335;&#39044;&#27979;&#21644;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#20004;&#20010;&#27169;&#22359;&#20849;&#20139;&#30456;&#21516;&#30340;&#39592;&#24178;&#32593;&#32476;&#12290;&#24635;&#20043;&#65292;NetInfoF&#20855;&#26377;&#20197;&#19979;&#26174;&#33879;&#20248;&#28857;&#65306;&#65288;a&#65289;&#36890;&#29992;&#24615;&#65292;&#33021;&#22788;&#29702;&#38142;&#36335;&#39044;&#27979;&#21644;&#33410;&#28857;&#20998;&#31867;&#20004;&#31181;&#20219;&#21153;&#65307;&#65288;b&#65289;&#21407;&#21017;&#24615;&#65292;&#20855;&#22791;&#29702;&#35770;&#20445;&#35777;&#21644;&#38381;&#24335;&#35299;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a node-attributed graph, and a graph task (link prediction or node classification), can we tell if a graph neural network (GNN) will perform well? More specifically, do the graph structure and the node features carry enough usable information for the task? Our goals are (1) to develop a fast tool to measure how much information is in the graph structure and in the node features, and (2) to exploit the information to solve the task, if there is enough. We propose NetInfoF, a framework including NetInfoF_Probe and NetInfoF_Act, for the measurement and the exploitation of network usable information (NUI), respectively. Given a graph data, NetInfoF_Probe measures NUI without any model training, and NetInfoF_Act solves link prediction and node classification, while two modules share the same backbone. In summary, NetInfoF has following notable advantages: (a) General, handling both link prediction and node classification; (b) Principled, with theoretical guarantee and closed-form solu
&lt;/p&gt;</description></item><item><title>HYPO&#26159;&#19968;&#20010;&#22312;&#36229;&#29699;&#38754;&#31354;&#38388;&#20013;&#23398;&#20064;&#22495;&#19981;&#21464;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#31867;&#21464;&#21270;&#21644;&#38388;&#31867;&#20998;&#31163;&#21407;&#21017;&#30340;&#24341;&#23548;&#65292;&#25552;&#39640;&#20102;&#31163;&#32676;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07785</link><description>&lt;p&gt;
HYPO&#65306;&#36229;&#29699;&#38754;&#31163;&#32676;&#20998;&#24067;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
HYPO: Hyperspherical Out-of-Distribution Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07785
&lt;/p&gt;
&lt;p&gt;
HYPO&#26159;&#19968;&#20010;&#22312;&#36229;&#29699;&#38754;&#31354;&#38388;&#20013;&#23398;&#20064;&#22495;&#19981;&#21464;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#31867;&#21464;&#21270;&#21644;&#38388;&#31867;&#20998;&#31163;&#21407;&#21017;&#30340;&#24341;&#23548;&#65292;&#25552;&#39640;&#20102;&#31163;&#32676;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32676;&#65288;OOD&#65289;&#27867;&#21270;&#23545;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#36825;&#19968;&#28857;&#21487;&#33021;&#20174;&#26681;&#26412;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#23398;&#20064;&#36328;&#19981;&#21516;&#39046;&#22495;&#25110;&#29615;&#22659;&#30340;&#19981;&#21464;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;HYPO&#65288;&#36229;&#29699;&#38754;OOD&#27867;&#21270;&#65289;&#65292;&#23427;&#33021;&#22815;&#35777;&#26126;&#22312;&#36229;&#29699;&#38754;&#31354;&#38388;&#20013;&#23398;&#20064;&#22495;&#19981;&#21464;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#36229;&#29699;&#38754;&#23398;&#20064;&#31639;&#27861;&#26159;&#26681;&#25454;&#20869;&#31867;&#21464;&#21270;&#21644;&#38388;&#31867;&#20998;&#31163;&#21407;&#21017;&#36827;&#34892;&#24341;&#23548;&#30340;&#65292;&#30830;&#20445;&#26469;&#33258;&#21516;&#19968;&#31867;&#21035;&#30340;&#29305;&#24449;&#65288;&#36328;&#19981;&#21516;&#35757;&#32451;&#39046;&#22495;&#65289;&#19982;&#20854;&#31867;&#21035;&#21407;&#22411;&#32039;&#23494;&#23545;&#40784;&#65292;&#32780;&#19981;&#21516;&#31867;&#21035;&#30340;&#21407;&#22411;&#20043;&#38388;&#21017;&#34987;&#26368;&#22823;&#21270;&#22320;&#20998;&#31163;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#20851;&#20110;&#25105;&#20204;&#30340;&#21407;&#22411;&#23398;&#20064;&#30446;&#26631;&#22914;&#20309;&#25913;&#21892;OOD&#27867;&#21270;&#30028;&#38480;&#30340;&#29702;&#35770;&#35777;&#26126;&#12290;&#36890;&#36807;&#23545;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;OOD&#22522;&#20934;&#30340;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#31454;&#20105;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) generalization is critical for machine learning models deployed in the real world. However, achieving this can be fundamentally challenging, as it requires the ability to learn invariant features across different domains or environments. In this paper, we propose a novel framework HYPO (HYPerspherical OOD generalization) that provably learns domain-invariant representations in a hyperspherical space. In particular, our hyperspherical learning algorithm is guided by intra-class variation and inter-class separation principles -- ensuring that features from the same class (across different training domains) are closely aligned with their class prototypes, while different class prototypes are maximally separated. We further provide theoretical justifications on how our prototypical learning objective improves the OOD generalization bound. Through extensive experiments on challenging OOD benchmarks, we demonstrate that our approach outperforms competitive baselines
&lt;/p&gt;</description></item><item><title>S$\Omega$I&#26159;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#37327;&#24230;&#65292;&#21487;&#20197;&#35745;&#31639;&#22810;&#21464;&#37327;&#31995;&#32479;&#20013;&#30340;&#21327;&#21516;-&#20887;&#20313;&#24179;&#34913;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#37327;&#24230;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.05667</link><description>&lt;p&gt;
S$\Omega$I: &#22522;&#20110;&#20998;&#25968;&#30340;O-INFORMATION&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
S$\Omega$I: Score-based O-INFORMATION Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05667
&lt;/p&gt;
&lt;p&gt;
S$\Omega$I&#26159;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#37327;&#24230;&#65292;&#21487;&#20197;&#35745;&#31639;&#22810;&#21464;&#37327;&#31995;&#32479;&#20013;&#30340;&#21327;&#21516;-&#20887;&#20313;&#24179;&#34913;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#37327;&#24230;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#25968;&#25454;&#21644;&#22797;&#26434;&#30340;&#22810;&#21464;&#37327;&#31995;&#32479;&#30340;&#20998;&#26512;&#38656;&#35201;&#25429;&#25417;&#22810;&#20010;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#20851;&#31995;&#30340;&#20449;&#24687;&#37327;&#12290;&#26368;&#36817;&#65292;&#26032;&#30340;&#20449;&#24687;&#35770;&#37327;&#24230;&#24050;&#34987;&#21457;&#23637;&#20986;&#26469;&#65292;&#20197;&#20811;&#26381;&#20256;&#32479;&#37327;&#24230;&#65288;&#22914;&#20114;&#20449;&#24687;&#65289;&#30340;&#23616;&#38480;&#24615;&#65292;&#21518;&#32773;&#21482;&#32771;&#34385;&#25104;&#23545;&#20132;&#20114;&#20316;&#29992;&#12290;&#20854;&#20013;&#65292;&#20449;&#24687;&#21327;&#21516;&#21644;&#20887;&#20313;&#30340;&#27010;&#24565;&#23545;&#20110;&#29702;&#35299;&#21464;&#37327;&#20043;&#38388;&#30340;&#39640;&#38454;&#20381;&#36182;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#36825;&#19968;&#27010;&#24565;&#30340;&#26368;&#33879;&#21517;&#21644;&#22810;&#29992;&#36884;&#30340;&#37327;&#24230;&#20043;&#19968;&#26159;O-information&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#28165;&#26224;&#19988;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#26469;&#37327;&#21270;&#22810;&#21464;&#37327;&#31995;&#32479;&#20013;&#30340;&#21327;&#21516;-&#20887;&#20313;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#38480;&#20110;&#31616;&#21270;&#24773;&#20917;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;S$\Omega$I&#65292;&#35813;&#26041;&#27861;&#39318;&#27425;&#20801;&#35768;&#35745;&#31639;O-information&#32780;&#19981;&#21463;&#23545;&#31995;&#32479;&#30340;&#38480;&#21046;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;&#20102;S$&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The analysis of scientific data and complex multivariate systems requires information quantities that capture relationships among multiple random variables. Recently, new information-theoretic measures have been developed to overcome the shortcomings of classical ones, such as mutual information, that are restricted to considering pairwise interactions. Among them, the concept of information synergy and redundancy is crucial for understanding the high-order dependencies between variables. One of the most prominent and versatile measures based on this concept is O-information, which provides a clear and scalable way to quantify the synergy-redundancy balance in multivariate systems. However, its practical application is limited to simplified cases. In this work, we introduce S$\Omega$I, which allows for the first time to compute O-information without restrictive assumptions about the system. Our experiments validate our approach on synthetic data, and demonstrate the effectiveness of S$
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21333;&#23618;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#22312;&#39640;&#32500;&#26497;&#38480;&#19979;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#39044;&#27979;&#65292;&#24182;&#25512;&#24191;&#20102;&#23545;&#22810;&#31181;&#25968;&#25454;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#23613;&#31649;GCN&#22312;&#25910;&#25947;&#36895;&#24230;&#19978;&#26159;&#19968;&#33268;&#30340;&#65292;&#20294;&#22312;&#20219;&#20309;&#24773;&#20917;&#19979;&#37117;&#19981;&#33021;&#36798;&#21040;&#36125;&#21494;&#26031;&#26368;&#20248;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.03818</link><description>&lt;p&gt;
&#21333;&#23618;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#28176;&#36827;&#27867;&#21270;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Asymptotic generalization error of a single-layer graph convolutional network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21333;&#23618;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#22312;&#39640;&#32500;&#26497;&#38480;&#19979;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#39044;&#27979;&#65292;&#24182;&#25512;&#24191;&#20102;&#23545;&#22810;&#31181;&#25968;&#25454;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#23613;&#31649;GCN&#22312;&#25910;&#25947;&#36895;&#24230;&#19978;&#26159;&#19968;&#33268;&#30340;&#65292;&#20294;&#22312;&#20219;&#20309;&#24773;&#20917;&#19979;&#37117;&#19981;&#33021;&#36798;&#21040;&#36125;&#21494;&#26031;&#26368;&#20248;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#23454;&#36341;&#20013;&#23637;&#29616;&#20986;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#26159;&#30456;&#23545;&#20110;&#24191;&#27867;&#30740;&#31350;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#20851;&#20110;&#20854;&#27867;&#21270;&#29305;&#24615;&#19982;&#26679;&#26412;&#25968;&#37327;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39044;&#27979;&#20102;&#22312;&#39640;&#32500;&#26497;&#38480;&#19979;&#65292;&#22522;&#20110;&#23646;&#24615;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;SBM&#65289;&#29983;&#25104;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#21333;&#23618;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#30340;&#24615;&#33021;&#12290;&#20043;&#21069;&#65292;&#20165;&#22312;Shi&#31561;&#20154;&#30340;&#25991;&#31456;&#20013;&#32771;&#34385;&#20102;&#19978;&#19979;&#25991;-SBM&#65288;CSBM&#65289;&#19978;&#30340;&#23725;&#22238;&#24402;&#20998;&#26512;&#65307;&#25105;&#20204;&#23558;&#20998;&#26512;&#25512;&#24191;&#21040;CSBM&#30340;&#20219;&#24847;&#20984;&#25439;&#22833;&#21644;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#24182;&#28155;&#21152;&#20102;&#23545;&#21478;&#19968;&#20010;&#25968;&#25454;&#27169;&#22411;&#8212;&#8212;&#31070;&#32463;&#20248;&#20808;SBM&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#39640;&#20449;&#22122;&#27604;&#26497;&#38480;&#65292;&#24182;&#35814;&#32454;&#20171;&#32461;&#20102;GCN&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#23613;&#31649;&#19968;&#33268;&#65292;&#20294;&#23545;&#20110;&#20219;&#20309;&#32771;&#34385;&#30340;&#24773;&#20917;&#37117;&#19981;&#33021;&#36798;&#21040;&#36125;&#21494;&#26031;&#26368;&#20248;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
While graph convolutional networks show great practical promises, the theoretical understanding of their generalization properties as a function of the number of samples is still in its infancy compared to the more broadly studied case of supervised fully connected neural networks. In this article, we predict the performances of a single-layer graph convolutional network (GCN) trained on data produced by attributed stochastic block models (SBMs) in the high-dimensional limit. Previously, only ridge regression on contextual-SBM (CSBM) has been considered in Shi et al. 2022; we generalize the analysis to arbitrary convex loss and regularization for the CSBM and add the analysis for another data model, the neural-prior SBM. We also study the high signal-to-noise ratio limit, detail the convergence rates of the GCN and show that, while consistent, it does not reach the Bayes-optimal rate for any of the considered cases.
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#21513;&#24067;&#26031;&#37319;&#26679;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#25193;&#25955;&#27169;&#22411;&#24182;&#24212;&#29992;&#21513;&#24067;&#26031;&#37319;&#26679;&#65292;&#26377;&#25928;&#22320;&#20174;&#20855;&#26377;&#36828;&#31243;&#21644;&#26029;&#24320;&#27169;&#24577;&#29305;&#24449;&#30340;&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#34920;&#29616;&#20986;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#28151;&#21512;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03008</link><description>&lt;p&gt;
&#25193;&#25955;&#21513;&#24067;&#26031;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Diffusive Gibbs Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03008
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#21513;&#24067;&#26031;&#37319;&#26679;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#25193;&#25955;&#27169;&#22411;&#24182;&#24212;&#29992;&#21513;&#24067;&#26031;&#37319;&#26679;&#65292;&#26377;&#25928;&#22320;&#20174;&#20855;&#26377;&#36828;&#31243;&#21644;&#26029;&#24320;&#27169;&#24577;&#29305;&#24449;&#30340;&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#34920;&#29616;&#20986;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#28151;&#21512;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#26041;&#27861;&#22312;&#22810;&#27169;&#24577;&#20998;&#24067;&#30340;&#28151;&#21512;&#19981;&#36275;&#26041;&#38754;&#23384;&#22312;&#30528;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#20998;&#23376;&#21160;&#21147;&#23398;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#8212;&#8212;&#25193;&#25955;&#21513;&#24067;&#26031;&#37319;&#26679;&#65288;DiGS&#65289;&#65292;&#29992;&#20110;&#26377;&#25928;&#37319;&#26679;&#20855;&#26377;&#36828;&#31243;&#21644;&#26029;&#24320;&#27169;&#24577;&#29305;&#24449;&#30340;&#20998;&#24067;&#12290;DiGS&#38598;&#25104;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#21033;&#29992;&#39640;&#26031;&#21367;&#31215;&#21019;&#24314;&#19968;&#20010;&#36741;&#21161;&#22122;&#22768;&#20998;&#24067;&#65292;&#20197;&#22312;&#21407;&#22987;&#31354;&#38388;&#20013;&#36830;&#25509;&#23396;&#31435;&#30340;&#27169;&#24577;&#65292;&#24182;&#24212;&#29992;&#21513;&#24067;&#26031;&#37319;&#26679;&#20174;&#20004;&#20010;&#31354;&#38388;&#20013;&#20132;&#26367;&#25277;&#21462;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#37319;&#26679;&#22810;&#27169;&#24577;&#20998;&#24067;&#26041;&#38754;&#34920;&#29616;&#20986;&#27604;&#24182;&#34892;&#28201;&#24230;&#27861;&#31561;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#28151;&#21512;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#37319;&#26679;&#22120;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#12289;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23376;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inadequate mixing of conventional Markov Chain Monte Carlo (MCMC) methods for multi-modal distributions presents a significant challenge in practical applications such as Bayesian inference and molecular dynamics. Addressing this, we propose Diffusive Gibbs Sampling (DiGS), an innovative family of sampling methods designed for effective sampling from distributions characterized by distant and disconnected modes. DiGS integrates recent developments in diffusion models, leveraging Gaussian convolution to create an auxiliary noisy distribution that bridges isolated modes in the original space and applying Gibbs sampling to alternately draw samples from both spaces. Our approach exhibits a better mixing property for sampling multi-modal distributions than state-of-the-art methods such as parallel tempering. We demonstrate that our sampler attains substantially improved results across various tasks, including mixtures of Gaussians, Bayesian neural networks and molecular dynamics.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#30340;&#28151;&#21512;&#35270;&#35273;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#20013;&#30340;&#20107;&#20214;&#20998;&#31867;&#20219;&#21153;&#12290;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#21644;&#25805;&#20316;&#26102;&#38388;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#19982;&#32463;&#20856;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00776</link><description>&lt;p&gt;
&#28151;&#21512;&#37327;&#23376;&#35270;&#35273;&#36716;&#25442;&#22120;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#20107;&#20214;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hybrid Quantum Vision Transformers for Event Classification in High Energy Physics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00776
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#30340;&#28151;&#21512;&#35270;&#35273;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#20013;&#30340;&#20107;&#20214;&#20998;&#31867;&#20219;&#21153;&#12290;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#21644;&#25805;&#20316;&#26102;&#38388;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#19982;&#32463;&#20856;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#20013;&#37117;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#38543;&#30528;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;&#22522;&#20110;&#37327;&#23376;&#30340;&#35270;&#35273;&#36716;&#25442;&#22120;&#27169;&#22411;&#21487;&#33021;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#21644;&#25805;&#20316;&#26102;&#38388;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#23578;&#19981;&#33021;&#25191;&#34892;&#39640;&#32500;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#25552;&#20379;&#20102;&#26410;&#26469;&#26368;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#37327;&#23376;&#28151;&#21512;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#20013;&#30340;&#20998;&#31867;&#38382;&#39064;&#65288;&#21306;&#20998;&#30005;&#23376;&#21644;&#20809;&#23376;&#22312;&#30005;&#30913;&#37327;&#33021;&#22120;&#20013;&#65289;&#12290;&#25105;&#20204;&#23558;&#23427;&#20204;&#19982;&#32463;&#20856;&#30340;&#35270;&#35273;&#36716;&#25442;&#22120;&#26550;&#26500;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#28151;&#21512;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#19982;&#32463;&#20856;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models based on vision transformer architectures are considered state-of-the-art when it comes to image classification tasks. However, they require extensive computational resources both for training and deployment. The problem is exacerbated as the amount and complexity of the data increases. Quantum-based vision transformer models could potentially alleviate this issue by reducing the training and operating time while maintaining the same predictive power. Although current quantum computers are not yet able to perform high-dimensional tasks yet, they do offer one of the most efficient solutions for the future. In this work, we construct several variations of a quantum hybrid vision transformer for a classification problem in high energy physics (distinguishing photons and electrons in the electromagnetic calorimeter). We test them against classical vision transformer architectures. Our findings indicate that the hybrid models can achieve comparable performance to their classical anal
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#22270;&#19978;&#23454;&#29616;&#36328;&#39046;&#22495;&#27867;&#21270;&#65292;&#35299;&#20915;&#20102;&#28304;HG&#19982;&#30446;&#26631;HG&#20998;&#24067;&#19981;&#21305;&#37197;&#23548;&#33268;&#30340;&#30693;&#35782;&#20256;&#36882;&#26080;&#25928;&#21644;&#23398;&#20064;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.03597</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#24322;&#26500;&#22270;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Causal Representation Learning for Out-of-Distribution Generalization on Heterogeneous Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03597
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#22270;&#19978;&#23454;&#29616;&#36328;&#39046;&#22495;&#27867;&#21270;&#65292;&#35299;&#20915;&#20102;&#28304;HG&#19982;&#30446;&#26631;HG&#20998;&#24067;&#19981;&#21305;&#37197;&#23548;&#33268;&#30340;&#30693;&#35782;&#20256;&#36882;&#26080;&#25928;&#21644;&#23398;&#20064;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;HGFL&#65289;&#24050;&#32463;&#34987;&#30740;&#21457;&#26469;&#35299;&#20915;&#24322;&#26500;&#22270;&#65288;HGs&#65289;&#20013;&#26631;&#31614;&#31232;&#30095;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#21508;&#31181;&#31867;&#22411;&#30340;&#33410;&#28857;&#21644;&#36793;&#12290;HGFL&#30340;&#26680;&#24515;&#27010;&#24565;&#26159;&#20174;&#28304;HG&#20013;&#23500;&#26631;&#35760;&#31867;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#23558;&#36825;&#20123;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;HG&#20197;&#20419;&#36827;&#23398;&#20064;&#26032;&#31867;&#21035;&#65292;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#26368;&#32456;&#22312;&#26410;&#26631;&#35760;&#30340;&#27979;&#35797;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#28304;HG&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#37117;&#20849;&#20139;&#30456;&#21516;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#19977;&#31181;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#24067;&#36716;&#21464;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#21407;&#22240;&#26377;&#20004;&#20010;&#65306;&#65288;1&#65289;&#28304;HG&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#19982;&#30446;&#26631;HG&#20998;&#24067;&#21305;&#37197;&#65292;&#20197;&#21450;&#65288;2&#65289;&#30446;&#26631;HG&#30340;&#19981;&#21487;&#39044;&#27979;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#12290;&#36825;&#31181;&#20998;&#24067;&#36716;&#21464;&#23548;&#33268;&#29616;&#26377;&#26041;&#27861;&#20013;&#30693;&#35782;&#20256;&#36882;&#26080;&#25928;&#21644;&#23398;&#20064;&#24615;&#33021;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03597v2 Announce Type: replace  Abstract: Heterogeneous graph few-shot learning (HGFL) has been developed to address the label sparsity issue in heterogeneous graphs (HGs), which consist of various types of nodes and edges. The core concept of HGFL is to extract knowledge from rich-labeled classes in a source HG, transfer this knowledge to a target HG to facilitate learning new classes with few-labeled training data, and finally make predictions on unlabeled testing data. Existing methods typically assume that the source HG, training data, and testing data all share the same distribution. However, in practice, distribution shifts among these three types of data are inevitable due to two reasons: (1) the limited availability of the source HG that matches the target HG distribution, and (2) the unpredictable data generation mechanism of the target HG. Such distribution shifts result in ineffective knowledge transfer and poor learning performance in existing methods, thereby le
&lt;/p&gt;</description></item><item><title>&#35813;&#26694;&#26550;&#26159;&#19968;&#20010;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32531;&#35299;&#36807;&#24230;&#24179;&#28369;&#12289;&#36807;&#24230;&#21387;&#32553;&#21644;&#26410;&#36798;&#21040;&#31561;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#22270;&#32593;&#32476;&#27169;&#22411;&#23545;&#38271;&#31243;&#20381;&#36182;&#24615;&#30340;&#24314;&#27169;&#33021;&#21147;</title><link>https://arxiv.org/abs/2312.16560</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#28040;&#24687;&#20256;&#36882;&#65306;&#32531;&#35299;&#36807;&#24230;&#24179;&#28369;&#12289;&#36807;&#24230;&#21387;&#32553;&#21644;&#26410;&#36798;&#21040;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Adaptive Message Passing: A General Framework to Mitigate Oversmoothing, Oversquashing, and Underreaching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16560
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26694;&#26550;&#26159;&#19968;&#20010;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32531;&#35299;&#36807;&#24230;&#24179;&#28369;&#12289;&#36807;&#24230;&#21387;&#32553;&#21644;&#26410;&#36798;&#21040;&#31561;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#22270;&#32593;&#32476;&#27169;&#22411;&#23545;&#38271;&#31243;&#20381;&#36182;&#24615;&#30340;&#24314;&#27169;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#23545;&#20110;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20013;&#27491;&#30830;&#25551;&#36848;&#22797;&#26434;&#31995;&#32479;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#35745;&#31639;&#20013;&#21253;&#21547;&#23427;&#20204;&#30340;&#20195;&#20215;&#26159;&#25972;&#20307;&#35745;&#31639;&#25104;&#26412;&#30340;&#24613;&#21095;&#22686;&#21152;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#22270;&#32593;&#32476;&#34987;&#29992;&#20316;&#39640;&#25928;&#30340;&#12289;&#25968;&#25454;&#39537;&#21160;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#34920;&#31034;&#20026;&#22270;&#30340;&#22797;&#26434;&#31995;&#32479;&#30340;&#24615;&#36136;&#12290;&#36825;&#20123;&#27169;&#22411;&#20381;&#36182;&#20110;&#19968;&#31181;&#23616;&#37096;&#30340;&#36845;&#20195;&#28040;&#24687;&#20256;&#36882;&#31574;&#30053;&#65292;&#29702;&#35770;&#19978;&#24212;&#35813;&#33021;&#22815;&#25429;&#33719;&#38271;&#31243;&#20449;&#24687;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#22320;&#23545;&#30456;&#24212;&#30340;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#22312;&#23454;&#36341;&#20013;&#65292;&#22823;&#22810;&#25968;&#28145;&#24230;&#22270;&#32593;&#32476;&#30001;&#20110;&#65288;&#21516;&#27493;&#65289;&#28040;&#24687;&#20256;&#36882;&#30340;&#22266;&#26377;&#38480;&#21046;&#65292;&#21363;&#36807;&#24230;&#24179;&#28369;&#12289;&#36807;&#24230;&#21387;&#32553;&#21644;&#26410;&#36798;&#21040;&#32780;&#19981;&#33021;&#30495;&#27491;&#23545;&#38271;&#31243;&#20381;&#36182;&#36827;&#34892;&#24314;&#27169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#30340;&#36890;&#29992;&#26694;&#26550;&#65306;&#22312;&#21464;&#20998;&#25512;&#29702;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#36171;&#20104;&#28040;&#24687;&#20256;&#36882;&#20307;&#31995;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16560v2 Announce Type: replace  Abstract: Long-range interactions are essential for the correct description of complex systems in many scientific fields. The price to pay for including them in the calculations, however, is a dramatic increase in the overall computational costs. Recently, deep graph networks have been employed as efficient, data-driven surrogate models for predicting properties of complex systems represented as graphs. These models rely on a local and iterative message passing strategy that should, in principle, capture long-range information without explicitly modeling the corresponding interactions. In practice, most deep graph networks cannot really model long-range dependencies due to the intrinsic limitations of (synchronous) message passing, namely oversmoothing, oversquashing, and underreaching. This work proposes a general framework that learns to mitigate these limitations: within a variational inference framework, we endow message passing architectu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#20195;&#30721;&#32534;&#36753;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#22312;&#25351;&#20196;&#24335;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#19978;&#21457;&#29616;&#20102;&#24320;&#25918;&#21644;&#23553;&#38381;&#27169;&#22411;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2312.12450</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#36827;&#34892;&#32534;&#36753;&#65311;&#35780;&#20272;&#20854;&#36981;&#24490;&#20195;&#30721;&#32534;&#36753;&#25351;&#20196;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Can It Edit? Evaluating the Ability of Large Language Models to Follow Code Editing Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12450
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#20195;&#30721;&#32534;&#36753;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#22312;&#25351;&#20196;&#24335;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#19978;&#21457;&#29616;&#20102;&#24320;&#25918;&#21644;&#23553;&#38381;&#27169;&#22411;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30740;&#31350;&#38598;&#20013;&#22312;&#24320;&#21457;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#21508;&#31181;&#20195;&#30721;&#21512;&#25104;&#20219;&#21153;&#12290;&#36825;&#20123;&#20219;&#21153;&#21253;&#25324;&#20174;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20013;&#21512;&#25104;&#20195;&#30721;&#65292;&#20174;&#20195;&#30721;&#20013;&#21512;&#25104;&#27979;&#35797;&#65292;&#20197;&#21450;&#20174;&#20195;&#30721;&#20013;&#21512;&#25104;&#35299;&#37322;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#20351;&#29992;LLMs&#36827;&#34892;&#25351;&#20196;&#24335;&#20195;&#30721;&#32534;&#36753;&#30340;&#34892;&#20026;&#30740;&#31350;&#19981;&#36275;&#12290;&#36825;&#20123;&#20219;&#21153;&#35201;&#27714;&#27169;&#22411;&#25353;&#29031;&#25552;&#20379;&#30340;&#25552;&#31034;&#26356;&#26032;&#19968;&#22359;&#20195;&#30721;&#12290;&#32534;&#36753;&#25351;&#20196;&#21487;&#33021;&#35201;&#27714;&#28155;&#21152;&#25110;&#21024;&#38500;&#21151;&#33021;&#65292;&#25551;&#36848;&#38169;&#35823;&#24182;&#35201;&#27714;&#20462;&#22797;&#65292;&#35201;&#27714;&#19981;&#21516;&#31867;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25110;&#32773;&#20854;&#20182;&#24120;&#35265;&#30340;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#22522;&#20934;&#65292;&#24182;&#29992;&#23427;&#35780;&#20272;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;LLMs&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#23637;&#31034;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;&#21644;&#23553;&#38381;&#27169;&#22411;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;&#20363;&#22914;&#65292;&#21363;&#20351;&#26159;GPT-3.5-Turbo&#20063;&#27604;&#26368;&#22909;&#30340;&#24320;&#25918;&#27169;&#22411;&#22312;&#32534;&#36753;&#20195;&#30721;&#26041;&#38754;&#22909;&#20102;8.8%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12450v4 Announce Type: replace-cross  Abstract: A significant amount of research is focused on developing and evaluating large language models for a variety of code synthesis tasks. These include synthesizing code from natural language instructions, synthesizing tests from code, and synthesizing explanations of code. In contrast, the behavior of instructional code editing with LLMs is understudied. These are tasks in which the model is instructed to update a block of code provided in a prompt. The editing instruction may ask for a feature to added or removed, describe a bug and ask for a fix, ask for a different kind of solution, or many other common code editing tasks.   We introduce a carefully crafted benchmark of code editing tasks and use it evaluate several cutting edge LLMs. Our evaluation exposes a significant gap between the capabilities of state-of-the-art open and closed models. For example, even GPT-3.5-Turbo is 8.8% better than the best open model at editing cod
&lt;/p&gt;</description></item><item><title>&#24120;&#29992;&#30340;&#20855;&#26377;&#26080;&#33618;&#21407;&#35777;&#26126;&#30340;&#27169;&#22411;&#20063;&#21487;&#20197;&#22312;&#36827;&#34892;&#21021;&#22987;&#25968;&#25454;&#37319;&#38598;&#38454;&#27573;&#20174;&#37327;&#23376;&#35774;&#22791;&#20013;&#25910;&#38598;&#19968;&#20123;&#32463;&#20856;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#32463;&#20856;&#27169;&#25311;</title><link>https://arxiv.org/abs/2312.09121</link><description>&lt;p&gt;
&#35777;&#23454;&#26080;&#33618;&#21407;&#23384;&#22312;&#26159;&#21542;&#24847;&#21619;&#30528;&#32463;&#20856;&#27169;&#25311;&#65311;&#25110;&#32773;&#65292;&#20026;&#20160;&#20040;&#25105;&#20204;&#38656;&#35201;&#37325;&#26032;&#24605;&#32771;&#21464;&#20998;&#37327;&#23376;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Does provable absence of barren plateaus imply classical simulability? Or, why we need to rethink variational quantum computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09121
&lt;/p&gt;
&lt;p&gt;
&#24120;&#29992;&#30340;&#20855;&#26377;&#26080;&#33618;&#21407;&#35777;&#26126;&#30340;&#27169;&#22411;&#20063;&#21487;&#20197;&#22312;&#36827;&#34892;&#21021;&#22987;&#25968;&#25454;&#37319;&#38598;&#38454;&#27573;&#20174;&#37327;&#23376;&#35774;&#22791;&#20013;&#25910;&#38598;&#19968;&#20123;&#32463;&#20856;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#32463;&#20856;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#33618;&#21407;&#29616;&#35937;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#12290; &#22312;&#36825;&#31687;&#35266;&#28857;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#38754;&#23545;&#20102;&#36234;&#26469;&#36234;&#26126;&#26174;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35768;&#22810;&#20154;&#26263;&#31034;&#20294;&#23578;&#26410;&#26126;&#30830;&#35299;&#20915;&#30340;&#38382;&#39064;&#65306;&#20801;&#35768;&#36991;&#20813;&#33618;&#21407;&#30340;&#32467;&#26500;&#26159;&#21542;&#20063;&#21487;&#20197;&#34987;&#21033;&#29992;&#26469;&#26377;&#25928;&#22320;&#32463;&#20856;&#27169;&#25311;&#25439;&#22833;&#65311; &#25105;&#20204;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#24120;&#29992;&#30340;&#20855;&#26377;&#26080;&#33618;&#21407;&#35777;&#26126;&#30340;&#27169;&#22411;&#20063;&#21487;&#20197;&#22312;&#36827;&#34892;&#21021;&#22987;&#25968;&#25454;&#37319;&#38598;&#38454;&#27573;&#20174;&#37327;&#23376;&#35774;&#22791;&#20013;&#25910;&#38598;&#19968;&#20123;&#32463;&#20856;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#32463;&#20856;&#27169;&#25311;&#12290; &#36825;&#26159;&#22240;&#20026;&#33618;&#21407;&#29616;&#35937;&#26159;&#30001;&#32500;&#24230;&#30340;&#35781;&#21650;&#23548;&#33268;&#30340;&#65292;&#32780;&#30446;&#21069;&#35299;&#20915;&#38382;&#39064;&#30340;&#26041;&#27861;&#26368;&#32456;&#23558;&#38382;&#39064;&#32534;&#30721;&#21040;&#19968;&#20123;&#23567;&#30340;&#12289;&#32463;&#20856;&#21487;&#27169;&#25311;&#30340;&#23376;&#31354;&#38388;&#20013;&#12290; &#22240;&#27492;&#65292;&#23613;&#31649;&#24378;&#35843;&#37327;&#23376;&#35745;&#31639;&#21487;&#20197;&#26159;&#25910;&#38598;&#25968;&#25454;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#24341;&#36215;&#20102;&#20005;&#37325;&#30340;&#24605;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09121v2 Announce Type: replace-cross  Abstract: A large amount of effort has recently been put into understanding the barren plateau phenomenon. In this perspective article, we face the increasingly loud elephant in the room and ask a question that has been hinted at by many but not explicitly addressed: Can the structure that allows one to avoid barren plateaus also be leveraged to efficiently simulate the loss classically? We present strong evidence that commonly used models with provable absence of barren plateaus are also classically simulable, provided that one can collect some classical data from quantum devices during an initial data acquisition phase. This follows from the observation that barren plateaus result from a curse of dimensionality, and that current approaches for solving them end up encoding the problem into some small, classically simulable, subspaces. Thus, while stressing quantum computers can be essential for collecting data, our analysis sheds seriou
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25193;&#23637;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#33267;&#38477;&#27700;&#36229;&#20998;&#36776;&#29575;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30830;&#23450;&#24615;&#38477;&#23610;&#24230;&#22120;&#21644;&#26242;&#26102;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#25429;&#25417;&#22122;&#22768;&#29305;&#24449;&#21644;&#39640;&#39057;&#29575;&#27169;&#24335;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.06071</link><description>&lt;p&gt;
&#20855;&#26377;&#26102;&#31354;&#35270;&#39057;&#25193;&#25955;&#30340;&#38477;&#27700;&#38477;&#23610;&#24230;
&lt;/p&gt;
&lt;p&gt;
Precipitation Downscaling with Spatiotemporal Video Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06071
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25193;&#23637;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#33267;&#38477;&#27700;&#36229;&#20998;&#36776;&#29575;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30830;&#23450;&#24615;&#38477;&#23610;&#24230;&#22120;&#21644;&#26242;&#26102;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#25429;&#25417;&#22122;&#22768;&#29305;&#24449;&#21644;&#39640;&#39057;&#29575;&#27169;&#24335;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27668;&#20505;&#31185;&#23398;&#21644;&#27668;&#35937;&#23398;&#39046;&#22495;&#65292;&#39640;&#20998;&#36776;&#29575;&#30340;&#23616;&#37096;&#38477;&#27700;&#65288;&#38632;&#38634;&#65289;&#39044;&#27979;&#21463;&#21040;&#22522;&#20110;&#27169;&#25311;&#26041;&#27861;&#30340;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#12290;&#32479;&#35745;&#38477;&#23610;&#24230;&#65292;&#25110;&#32773;&#31216;&#20026;&#36229;&#20998;&#36776;&#29575;&#65292;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#20854;&#20013;&#20302;&#20998;&#36776;&#29575;&#39044;&#27979;&#36890;&#36807;&#32479;&#35745;&#26041;&#27861;&#24471;&#21040;&#25913;&#36827;&#12290;&#19982;&#20256;&#32479;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19981;&#21516;&#65292;&#22825;&#27668;&#21644;&#27668;&#20505;&#24212;&#29992;&#38656;&#35201;&#25429;&#25417;&#32473;&#23450;&#20302;&#20998;&#36776;&#29575;&#27169;&#24335;&#30340;&#39640;&#20998;&#36776;&#29575;&#30340;&#20934;&#30830;&#26465;&#20214;&#20998;&#24067;&#65292;&#20197;&#30830;&#20445;&#21487;&#38752;&#30340;&#38598;&#21512;&#24179;&#22343;&#21644;&#26497;&#31471;&#20107;&#20214;&#65288;&#22914;&#26292;&#38632;&#65289;&#30340;&#26080;&#20559;&#20272;&#35745;&#12290;&#26412;&#30740;&#31350;&#23558;&#26368;&#26032;&#30340;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#25193;&#23637;&#21040;&#38477;&#27700;&#36229;&#20998;&#36776;&#29575;&#65292;&#20351;&#29992;&#30830;&#23450;&#24615;&#38477;&#23610;&#24230;&#22120;&#65292;&#28982;&#21518;&#26159;&#26242;&#26102;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#25429;&#25417;&#22122;&#22768;&#29305;&#24449;&#21644;&#39640;&#39057;&#29575;&#27169;&#24335;&#12290;&#25105;&#20204;&#22312;FV3GFS&#36755;&#20986;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#24050;&#24314;&#31435;&#30340;&#22823;&#35268;&#27169;&#20840;&#29699;&#22823;&#27668;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06071v2 Announce Type: replace-cross  Abstract: In climate science and meteorology, high-resolution local precipitation (rain and snowfall) predictions are limited by the computational costs of simulation-based methods. Statistical downscaling, or super-resolution, is a common workaround where a low-resolution prediction is improved using statistical approaches. Unlike traditional computer vision tasks, weather and climate applications require capturing the accurate conditional distribution of high-resolution given low-resolution patterns to assure reliable ensemble averages and unbiased estimates of extreme events, such as heavy rain. This work extends recent video diffusion models to precipitation super-resolution, employing a deterministic downscaler followed by a temporally-conditioned diffusion model to capture noise characteristics and high-frequency patterns. We test our approach on FV3GFS output, an established large-scale global atmosphere model, and compare it agai
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37325;&#26032;&#35774;&#35745;&#32593;&#32476;&#23618;&#26469;&#20445;&#25345;&#26399;&#26395;&#30340;&#28608;&#27963;&#12289;&#26435;&#37325;&#21644;&#26356;&#26032;&#24133;&#24230;&#65292;&#28040;&#38500;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#28418;&#31227;&#21644;&#19981;&#24179;&#34913;&#65292;&#20174;&#32780;&#22312;&#30456;&#21516;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#19979;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#32593;&#32476;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.02696</link><description>&lt;p&gt;
&#20998;&#26512;&#21644;&#25913;&#36827;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Analyzing and Improving the Training Dynamics of Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02696
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#35774;&#35745;&#32593;&#32476;&#23618;&#26469;&#20445;&#25345;&#26399;&#26395;&#30340;&#28608;&#27963;&#12289;&#26435;&#37325;&#21644;&#26356;&#26032;&#24133;&#24230;&#65292;&#28040;&#38500;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#28418;&#31227;&#21644;&#19981;&#24179;&#34913;&#65292;&#20174;&#32780;&#22312;&#30456;&#21516;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#19979;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#32593;&#32476;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30446;&#21069;&#22312;&#25968;&#25454;&#39537;&#21160;&#22270;&#20687;&#21512;&#25104;&#39046;&#22495;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#20854;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#26080;&#19982;&#20262;&#27604;&#30340;&#25193;&#23637;&#33021;&#21147;&#12290;&#26412;&#25991;&#22312;&#19981;&#25913;&#21464;&#20854;&#39640;&#32423;&#32467;&#26500;&#30340;&#21069;&#25552;&#19979;&#65292;&#35782;&#21035;&#24182;&#32416;&#27491;&#20102;&#27969;&#34892;&#30340;ADM&#25193;&#25955;&#27169;&#22411;&#26550;&#26500;&#20013;&#23548;&#33268;&#19981;&#22343;&#21248;&#21644;&#20302;&#25928;&#35757;&#32451;&#30340;&#20960;&#20010;&#21407;&#22240;&#12290;&#35266;&#23519;&#21040;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32593;&#32476;&#28608;&#27963;&#21644;&#26435;&#37325;&#30340;&#19981;&#21463;&#25511;&#21046;&#30340;&#24133;&#24230;&#21464;&#21270;&#21644;&#19981;&#24179;&#34913;&#65292;&#25105;&#20204;&#37325;&#26032;&#35774;&#35745;&#20102;&#32593;&#32476;&#23618;&#20197;&#20445;&#25345;&#26399;&#26395;&#19978;&#30340;&#28608;&#27963;&#12289;&#26435;&#37325;&#21644;&#26356;&#26032;&#24133;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31995;&#32479;&#24212;&#29992;&#36825;&#19968;&#29702;&#24565;&#28040;&#38500;&#20102;&#35266;&#23519;&#21040;&#30340;&#28418;&#31227;&#21644;&#19981;&#24179;&#34913;&#65292;&#23548;&#33268;&#30456;&#24403;&#26356;&#22909;&#30340;&#32593;&#32476;&#22312;&#31561;&#25928;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#19979;&#12290;&#25105;&#20204;&#30340;&#20462;&#25913;&#23558;&#20043;&#21069;&#22312;ImageNet-512&#21512;&#25104;&#20013;&#30340;&#35760;&#24405;FID&#20174;2.41&#25913;&#36827;&#21040;&#20102;1.81&#65292;&#37319;&#29992;&#20102;&#24555;&#36895;&#30830;&#23450;&#24615;&#37319;&#26679;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02696v2 Announce Type: replace-cross  Abstract: Diffusion models currently dominate the field of data-driven image synthesis with their unparalleled scaling to large datasets. In this paper, we identify and rectify several causes for uneven and ineffective training in the popular ADM diffusion model architecture, without altering its high-level structure. Observing uncontrolled magnitude changes and imbalances in both the network activations and weights over the course of training, we redesign the network layers to preserve activation, weight, and update magnitudes on expectation. We find that systematic application of this philosophy eliminates the observed drifts and imbalances, resulting in considerably better networks at equal computational complexity. Our modifications improve the previous record FID of 2.41 in ImageNet-512 synthesis to 1.81, achieved using fast deterministic sampling.   As an independent contribution, we present a method for setting the exponential mov
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20855;&#26377;&#20223;&#23556;&#32467;&#26500;&#30340;&#31616;&#21333;&#23398;&#20064;&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#22810;&#29615;&#22659;&#27867;&#21270;&#65292;&#33021;&#22815;&#35782;&#21035;&#29289;&#29702;&#31995;&#32479;&#30340;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#24615;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2312.00477</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#29289;&#29702;&#31995;&#32479;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Interpretable Meta-Learning of Physical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00477
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20855;&#26377;&#20223;&#23556;&#32467;&#26500;&#30340;&#31616;&#21333;&#23398;&#20064;&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#22810;&#29615;&#22659;&#27867;&#21270;&#65292;&#33021;&#22815;&#35782;&#21035;&#29289;&#29702;&#31995;&#32479;&#30340;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#24615;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#22312;&#31185;&#23398;&#36807;&#31243;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#38754;&#23545;&#25968;&#25454;&#26469;&#33258;&#19981;&#22343;&#21248;&#23454;&#39564;&#26465;&#20214;&#30340;&#25361;&#25112;&#24615;&#24773;&#22659;&#12290;&#36817;&#26399;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#40657;&#30418;&#31070;&#32463;&#32593;&#32476;&#65292;&#23548;&#33268;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#26377;&#38480;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#23398;&#20064;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#35748;&#20026;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#23398;&#20064;&#27169;&#22411;&#65292;&#21363;&#23545;&#20110;&#23398;&#20064;&#20219;&#21153;&#20855;&#26377;&#20223;&#23556;&#32467;&#26500;&#65292;&#26469;&#23454;&#29616;&#22810;&#29615;&#22659;&#27867;&#21270;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26550;&#26500;&#33021;&#22815;&#35782;&#21035;&#31995;&#32479;&#30340;&#29289;&#29702;&#21442;&#25968;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#23398;&#20064;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#19982;&#29289;&#29702;&#31995;&#32479;&#19978;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#20174;&#29609;&#20855;&#27169;&#22411;&#21040;&#22797;&#26434;&#30340;&#38750;&#35299;&#26512;&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#31454;&#20105;&#24615;&#27867;&#21270;&#24615;&#33021;&#21644;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00477v2 Announce Type: replace  Abstract: Machine learning methods can be a valuable aid in the scientific process, but they need to face challenging settings where data come from inhomogeneous experimental conditions. Recent meta-learning methods have made significant progress in multi-task learning, but they rely on black-box neural networks, resulting in high computational costs and limited interpretability. Leveraging the structure of the learning problem, we argue that multi-environment generalization can be achieved using a simpler learning model, with an affine structure with respect to the learning task. Crucially, we prove that this architecture can identify the physical parameters of the system, enabling interpreable learning. We demonstrate the competitive generalization performance and the low computational cost of our method by comparing it to state-of-the-art algorithms on physical systems, ranging from toy models to complex, non-analytical systems. The interpr
&lt;/p&gt;</description></item><item><title>&#21482;&#26377;&#24403;&#32771;&#34385;&#30340;&#25439;&#22833;&#20989;&#25968;&#20026;&#20984;&#20989;&#25968;&#26102;&#65292;&#38598;&#25104;&#27169;&#22411;&#19968;&#30452;&#22312;&#21464;&#24471;&#26356;&#22909;&#65292;&#24403;&#25439;&#22833;&#20989;&#25968;&#20026;&#38750;&#20984;&#20989;&#25968;&#26102;&#65292;&#22909;&#27169;&#22411;&#30340;&#38598;&#25104;&#21464;&#24471;&#26356;&#22909;&#65292;&#22351;&#27169;&#22411;&#30340;&#38598;&#25104;&#21464;&#24471;&#26356;&#31967;&#12290;</title><link>https://arxiv.org/abs/2311.17885</link><description>&lt;p&gt;
&#38598;&#25104;&#27169;&#22411;&#26159;&#21542;&#19968;&#30452;&#22312;&#19981;&#26029;&#36827;&#27493;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Ensembles Getting Better all the Time?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17885
&lt;/p&gt;
&lt;p&gt;
&#21482;&#26377;&#24403;&#32771;&#34385;&#30340;&#25439;&#22833;&#20989;&#25968;&#20026;&#20984;&#20989;&#25968;&#26102;&#65292;&#38598;&#25104;&#27169;&#22411;&#19968;&#30452;&#22312;&#21464;&#24471;&#26356;&#22909;&#65292;&#24403;&#25439;&#22833;&#20989;&#25968;&#20026;&#38750;&#20984;&#20989;&#25968;&#26102;&#65292;&#22909;&#27169;&#22411;&#30340;&#38598;&#25104;&#21464;&#24471;&#26356;&#22909;&#65292;&#22351;&#27169;&#22411;&#30340;&#38598;&#25104;&#21464;&#24471;&#26356;&#31967;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#26041;&#27861;&#32467;&#21512;&#20102;&#20960;&#20010;&#22522;&#30784;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26159;&#21542;&#22987;&#32456;&#23558;&#26356;&#22810;&#27169;&#22411;&#32435;&#20837;&#38598;&#25104;&#20250;&#25552;&#21319;&#20854;&#24179;&#22343;&#24615;&#33021;&#12290;&#36825;&#20010;&#38382;&#39064;&#21462;&#20915;&#20110;&#25152;&#32771;&#34385;&#30340;&#38598;&#25104;&#31867;&#22411;&#65292;&#20197;&#21450;&#36873;&#25321;&#30340;&#39044;&#27979;&#24230;&#37327;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#25152;&#26377;&#38598;&#25104;&#25104;&#21592;&#34987;&#39044;&#26399;&#34920;&#29616;&#30456;&#21516;&#30340;&#24773;&#20917;&#65292;&#36825;&#26159;&#20960;&#31181;&#27969;&#34892;&#26041;&#27861;&#65288;&#22914;&#38543;&#26426;&#26862;&#26519;&#25110;&#28145;&#24230;&#38598;&#25104;&#65289;&#30340;&#24773;&#20917;&#12290;&#22312;&#36825;&#31181;&#35774;&#23450;&#19979;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#21482;&#26377;&#24403;&#32771;&#34385;&#30340;&#25439;&#22833;&#20989;&#25968;&#20026;&#20984;&#20989;&#25968;&#26102;&#65292;&#38598;&#25104;&#25165;&#20250;&#19968;&#30452;&#21464;&#24471;&#26356;&#22909;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#38598;&#25104;&#30340;&#24179;&#22343;&#25439;&#22833;&#26159;&#27169;&#22411;&#25968;&#37327;&#30340;&#20943;&#20989;&#25968;&#12290;&#24403;&#25439;&#22833;&#20989;&#25968;&#20026;&#38750;&#20984;&#20989;&#25968;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#32467;&#26524;&#65292;&#21487;&#20197;&#24635;&#32467;&#20026;&#65306;&#22909;&#27169;&#22411;&#30340;&#38598;&#25104;&#20250;&#21464;&#24471;&#26356;&#22909;&#65292;&#22351;&#27169;&#22411;&#30340;&#38598;&#25104;&#20250;&#21464;&#24471;&#26356;&#31967;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20851;&#20110;&#23614;&#27010;&#29575;&#21333;&#35843;&#24615;&#30340;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17885v2 Announce Type: replace-cross  Abstract: Ensemble methods combine the predictions of several base models. We study whether or not including more models always improves their average performance. This question depends on the kind of ensemble considered, as well as the predictive metric chosen. We focus on situations where all members of the ensemble are a priori expected to perform as well, which is the case of several popular methods such as random forests or deep ensembles. In this setting, we show that ensembles are getting better all the time if, and only if, the considered loss function is convex. More precisely, in that case, the average loss of the ensemble is a decreasing function of the number of models. When the loss function is nonconvex, we show a series of results that can be summarised as: ensembles of good models keep getting better, and ensembles of bad models keep getting worse. To this end, we prove a new result on the monotonicity of tail probabiliti
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#36328;&#24230;&#30340;&#24179;&#22343;&#22238;&#25253;MDP&#20013;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#26159;&#39318;&#20010;&#22312;&#25152;&#26377;&#21442;&#25968;&#26041;&#38754;&#37117;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.13469</link><description>&lt;p&gt;
&#22522;&#20110;&#36328;&#24230;&#30340;&#24179;&#22343;&#22238;&#25253;MDP&#30340;&#26368;&#20248;&#37319;&#26679;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Span-Based Optimal Sample Complexity for Average Reward MDPs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13469
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#36328;&#24230;&#30340;&#24179;&#22343;&#22238;&#25253;MDP&#20013;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#26159;&#39318;&#20010;&#22312;&#25152;&#26377;&#21442;&#25968;&#26041;&#38754;&#37117;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#19979;&#23398;&#20064;&#24179;&#22343;&#22238;&#25253;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#20013;&#30340;$\varepsilon$-&#26368;&#20248;&#31574;&#30053;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#22797;&#26434;&#24230;&#30028;&#38480;$\widetilde{O}\left(SA\frac{H}{\varepsilon^2} \right)$&#65292;&#20854;&#20013;$H$&#26159;&#26368;&#20248;&#31574;&#30053;&#30340;&#20559;&#24046;&#20989;&#25968;&#30340;&#36328;&#24230;&#65292;$SA$&#26159;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#22522;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#31532;&#19968;&#20010;&#22312;&#25152;&#26377;&#21442;&#25968;$S,A,H$&#21644;$\varepsilon$&#20013;&#65288;&#26368;&#22810;&#23545;&#25968;&#22240;&#23376;&#65289;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#24037;&#20316;&#65292;&#29616;&#26377;&#24037;&#20316;&#35201;&#20040;&#20551;&#35774;&#25152;&#26377;&#31574;&#30053;&#30340;&#28151;&#21512;&#26102;&#38388;&#22343;&#21248;&#26377;&#30028;&#65292;&#35201;&#20040;&#23545;&#21442;&#25968;&#26377;&#27425;&#26368;&#20248;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22522;&#20110;&#23558;&#24179;&#22343;&#22238;&#25253;MDP&#38477;&#32423;&#20026;&#25240;&#25187;MDP&#12290;&#20026;&#20102;&#24314;&#31435;&#36825;&#31181;&#38477;&#32423;&#30340;&#26368;&#20248;&#24615;&#65292;&#25105;&#20204;&#20026;$\gamma$-&#25240;&#25187;MDP&#24320;&#21457;&#20102;&#25913;&#36827;&#30340;&#30028;&#38480;&#65292;&#34920;&#26126;$\widetilde{O}\left(SA\frac{H}{(1-\gamma)^2\varepsilon^2} \right)$&#20010;&#26679;&#26412;&#36275;&#20197;&#23398;&#20064;&#24369;ly c
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13469v2 Announce Type: replace  Abstract: We study the sample complexity of learning an $\varepsilon$-optimal policy in an average-reward Markov decision process (MDP) under a generative model. We establish the complexity bound $\widetilde{O}\left(SA\frac{H}{\varepsilon^2} \right)$, where $H$ is the span of the bias function of the optimal policy and $SA$ is the cardinality of the state-action space. Our result is the first that is minimax optimal (up to log factors) in all parameters $S,A,H$ and $\varepsilon$, improving on existing work that either assumes uniformly bounded mixing times for all policies or has suboptimal dependence on the parameters.   Our result is based on reducing the average-reward MDP to a discounted MDP. To establish the optimality of this reduction, we develop improved bounds for $\gamma$-discounted MDPs, showing that $\widetilde{O}\left(SA\frac{H}{(1-\gamma)^2\varepsilon^2} \right)$ samples suffice to learn a $\varepsilon$-optimal policy in weakly c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REDS&#30340;&#36164;&#28304;&#39640;&#25928;&#28145;&#24230;&#23376;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#20803;&#30340;&#25490;&#21015;&#19981;&#21464;&#24615;&#21644;&#26032;&#39062;&#30340;&#36845;&#20195;&#32972;&#21253;&#20248;&#21270;&#22120;&#26469;&#23454;&#29616;&#27169;&#22411;&#22312;&#19981;&#21516;&#36164;&#28304;&#32422;&#26463;&#19979;&#30340;&#33258;&#36866;&#24212;&#24615;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#35745;&#31639;&#22359;&#21644;&#37325;&#26032;&#23433;&#25490;&#25805;&#20316;&#39034;&#24207;&#31561;&#26041;&#27861;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2311.13349</link><description>&lt;p&gt;
REDS: &#36164;&#28304;&#39640;&#25928;&#30340;&#28145;&#24230;&#23376;&#32593;&#32476;&#29992;&#20110;&#21160;&#24577;&#36164;&#28304;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REDS&#30340;&#36164;&#28304;&#39640;&#25928;&#28145;&#24230;&#23376;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#20803;&#30340;&#25490;&#21015;&#19981;&#21464;&#24615;&#21644;&#26032;&#39062;&#30340;&#36845;&#20195;&#32972;&#21253;&#20248;&#21270;&#22120;&#26469;&#23454;&#29616;&#27169;&#22411;&#22312;&#19981;&#21516;&#36164;&#28304;&#32422;&#26463;&#19979;&#30340;&#33258;&#36866;&#24212;&#24615;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#35745;&#31639;&#22359;&#21644;&#37325;&#26032;&#23433;&#25490;&#25805;&#20316;&#39034;&#24207;&#31561;&#26041;&#27861;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#28145;&#24230;&#27169;&#22411;&#32463;&#24120;&#36935;&#21040;&#36164;&#28304;&#21464;&#21270;&#65292;&#36825;&#28304;&#20110;&#33021;&#37327;&#27700;&#24179;&#27874;&#21160;&#12289;&#26102;&#38388;&#32422;&#26463;&#25110;&#31995;&#32479;&#20013;&#20854;&#20182;&#20851;&#38190;&#20219;&#21153;&#30340;&#20248;&#20808;&#32423;&#12290;&#30446;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#29983;&#25104;&#30340;&#26159;&#36164;&#28304;&#19981;&#21487;&#30693;&#30340;&#27169;&#22411;&#65292;&#24182;&#19981;&#33021;&#22312;&#36816;&#34892;&#26102;&#36827;&#34892;&#35843;&#25972;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Resource-Efficient Deep Subnetworks (REDS)&#26469;&#24212;&#23545;&#21487;&#21464;&#36164;&#28304;&#19979;&#30340;&#27169;&#22411;&#36866;&#24212;&#24615;&#12290;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#27604;&#65292;REDS&#21033;&#29992;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#65292;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#20803;&#30340;&#25490;&#21015;&#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#20801;&#35768;&#30828;&#20214;&#29305;&#23450;&#30340;&#20248;&#21270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;REDS&#36890;&#36807;&#65288;1&#65289;&#36339;&#36807;&#30001;&#26032;&#39062;&#30340;&#36845;&#20195;&#32972;&#21253;&#20248;&#21270;&#22120;&#35782;&#21035;&#30340;&#39034;&#24207;&#35745;&#31639;&#22359;&#65292;&#20197;&#21450;&#65288;2&#65289;&#21033;&#29992;&#31616;&#21333;&#30340;&#25968;&#23398;&#37325;&#26032;&#23433;&#25490;REDS&#35745;&#31639;&#22270;&#20013;&#25805;&#20316;&#30340;&#39034;&#24207;&#65292;&#20197;&#21033;&#29992;&#25968;&#25454;&#32531;&#23384;&#32780;&#23454;&#29616;&#35745;&#31639;&#25928;&#29575;&#12290;REDS&#25903;&#25345;&#20256;&#32479;&#30340;&#28145;&#24230;&#32593;&#32476;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13349v2 Announce Type: replace  Abstract: Deep models deployed on edge devices frequently encounter resource variability, which arises from fluctuating energy levels, timing constraints, or prioritization of other critical tasks within the system. State-of-the-art machine learning pipelines generate resource-agnostic models, not capable to adapt at runtime. In this work we introduce Resource-Efficient Deep Subnetworks (REDS) to tackle model adaptation to variable resources. In contrast to the state-of-the-art, REDS use structured sparsity constructively by exploiting permutation invariance of neurons, which allows for hardware-specific optimizations. Specifically, REDS achieve computational efficiency by (1) skipping sequential computational blocks identified by a novel iterative knapsack optimizer, and (2) leveraging simple math to re-arrange the order of operations in REDS computational graph to take advantage of the data cache. REDS support conventional deep networks freq
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#25968;&#25454;&#31649;&#29702;&#30340;&#35282;&#24230;&#23457;&#35270;GNN&#35757;&#32451;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#21644;&#35780;&#20272;&#65292;&#20026;&#35774;&#35745;&#26410;&#26469;&#30340;GNN&#35757;&#32451;&#31995;&#32479;&#25552;&#20379;&#20102;&#19968;&#20123;&#23454;&#29992;&#30340;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2311.13279</link><description>&lt;p&gt;
GNN&#35757;&#32451;&#31995;&#32479;&#30340;&#32508;&#21512;&#35780;&#20272;&#65306;&#25968;&#25454;&#31649;&#29702;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Evaluation of GNN Training Systems: A Data Management Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#25968;&#25454;&#31649;&#29702;&#30340;&#35282;&#24230;&#23457;&#35270;GNN&#35757;&#32451;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#21644;&#35780;&#20272;&#65292;&#20026;&#35774;&#35745;&#26410;&#26469;&#30340;GNN&#35757;&#32451;&#31995;&#32479;&#25552;&#20379;&#20102;&#19968;&#20123;&#23454;&#29992;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#20102;&#35768;&#22810;&#25903;&#25345;&#39640;&#25928;GNN&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#35757;&#32451;&#31995;&#32479;&#12290;&#30001;&#20110;GNN&#20307;&#29616;&#20102;&#35757;&#32451;&#26679;&#26412;&#20043;&#38388;&#30340;&#22797;&#26434;&#25968;&#25454;&#20381;&#36182;&#20851;&#31995;&#65292;&#22312;&#25968;&#25454;&#31649;&#29702;&#26041;&#38754;&#65292;GNN&#30340;&#35757;&#32451;&#24212;&#35813;&#35299;&#20915;&#19981;&#21516;&#20110;DNN&#35757;&#32451;&#30340;&#25968;&#25454;&#21010;&#20998;&#12289;&#23567;&#25209;&#37327;&#35757;&#32451;&#25209;&#27425;&#20934;&#22791;&#21644;CPU&#19982;GPU&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#31561;&#29420;&#29305;&#25361;&#25112;&#12290;&#36825;&#20123;&#22240;&#32032;&#21344;&#25454;&#20102;&#35757;&#32451;&#26102;&#38388;&#30340;&#24456;&#22823;&#27604;&#20363;&#65292;&#20351;&#24471;GNN&#35757;&#32451;&#20013;&#30340;&#25968;&#25454;&#31649;&#29702;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#12290;&#26412;&#25991;&#20174;&#25968;&#25454;&#31649;&#29702;&#30340;&#35282;&#24230;&#23457;&#35270;GNN&#35757;&#32451;&#65292;&#24182;&#23545;&#20195;&#34920;&#24615;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#35768;&#22810;&#26377;&#36259;&#21644;&#26377;&#20215;&#20540;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20174;&#36825;&#20123;&#23454;&#39564;&#20013;&#23398;&#21040;&#30340;&#19968;&#20123;&#24314;&#35758;&#65292;&#36825;&#20123;&#23545;&#26410;&#26469;&#35774;&#35745;GNN&#35757;&#32451;&#31995;&#32479;&#24456;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13279v2 Announce Type: replace  Abstract: Many Graph Neural Network (GNN) training systems have emerged recently to support efficient GNN training. Since GNNs embody complex data dependencies between training samples, the training of GNNs should address distinct challenges different from DNN training in data management, such as data partitioning, batch preparation for mini-batch training, and data transferring between CPUs and GPUs. These factors, which take up a large proportion of training time, make data management in GNN training more significant. This paper reviews GNN training from a data management perspective and provides a comprehensive analysis and evaluation of the representative approaches. We conduct extensive experiments on various benchmark datasets and show many interesting and valuable results. We also provide some practical tips learned from these experiments, which are helpful for designing GNN training systems in the future.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#29992;&#20110;&#20083;&#33146;&#30284;&#20999;&#29255;&#20013;&#19978;&#30382;&#32454;&#32990;&#20998;&#21106;&#30340;AI&#27169;&#22411;&#65292;&#36890;&#36807;&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#24341;&#23548;&#20998;&#21106;&#20986;&#33391;&#24615;&#19978;&#30382;&#32454;&#32990;&#12289;&#21407;&#20301;&#30149;&#21464;&#21644;&#28024;&#28070;&#24615;&#19978;&#30382;&#32454;&#32990;&#12290;</title><link>https://arxiv.org/abs/2311.13261</link><description>&lt;p&gt;
&#20083;&#33146;&#30284;&#20999;&#29255;&#20013;&#33391;&#24615;&#19978;&#30382;&#32454;&#32990;&#12289;&#21407;&#20301;&#30149;&#21464;&#21644;&#28024;&#28070;&#24615;&#19978;&#30382;&#32454;&#32990;&#30340;&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#24341;&#23548;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Immunohistochemistry guided segmentation of benign epithelial cells, in situ lesions, and invasive epithelial cells in breast cancer slides
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13261
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#29992;&#20110;&#20083;&#33146;&#30284;&#20999;&#29255;&#20013;&#19978;&#30382;&#32454;&#32990;&#20998;&#21106;&#30340;AI&#27169;&#22411;&#65292;&#36890;&#36807;&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#24341;&#23548;&#20998;&#21106;&#20986;&#33391;&#24615;&#19978;&#30382;&#32454;&#32990;&#12289;&#21407;&#20301;&#30149;&#21464;&#21644;&#28024;&#28070;&#24615;&#19978;&#30382;&#32454;&#32990;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#30149;&#29702;&#23398;&#20351;&#24471;&#21487;&#20197;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#33258;&#21160;&#20998;&#26512;&#32452;&#32455;&#30149;&#29702;&#20999;&#29255;&#12290;&#33258;&#21160;&#35780;&#20272;&#21487;&#20197;&#25552;&#39640;&#35786;&#26029;&#25928;&#29575;&#65292;&#24182;&#24110;&#21161;&#25214;&#21040;&#24418;&#24577;&#29305;&#24449;&#19982;&#20020;&#24202;&#32467;&#26524;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#20026;&#20102;&#24320;&#21457;&#36825;&#26679;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#36776;&#35748;&#28024;&#28070;&#24615;&#19978;&#30382;&#32454;&#32990;&#65292;&#24182;&#23558;&#20854;&#19982;&#33391;&#24615;&#19978;&#30382;&#32454;&#32990;&#21644;&#21407;&#20301;&#30149;&#21464;&#20998;&#24320;&#23558;&#26159;&#31532;&#19968;&#27493;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#29992;&#20110;&#20083;&#33146;&#30284;&#20999;&#29255;&#20013;&#19978;&#30382;&#32454;&#32990;&#20998;&#21106;&#30340;AI&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#37325;&#26032;&#26579;&#33394;&#34880;&#32418;&#34507;&#30333;&#21644;&#21980;&#37240;&#24615;&#26579;&#33394;&#32454;&#32990;&#35282;&#34507;&#30333;(CK) AE1/AE3 HE&#20999;&#29255;&#65292;&#20197;&#21450;&#30149;&#29702;&#23398;&#23478;&#30340;&#27880;&#37322;&#29983;&#25104;&#20102;&#19978;&#30382;&#22522;&#26412;&#30495;&#20540;&#25513;&#27169;&#12290;HE/CK&#22270;&#20687;&#23545;&#34987;&#29992;&#20110;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#25968;&#25454;&#22686;&#24378;&#34987;&#29992;&#26469;&#20351;&#27169;&#22411;&#26356;&#31283;&#20581;&#12290;839&#21517;&#24739;&#32773;&#30340;&#32452;&#32455;&#24494;&#38453;&#21015;&#65288;TMAs&#65289;&#21644;&#20004;&#21517;&#24739;&#32773;&#30340;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#29992;&#20110;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13261v2 Announce Type: replace-cross  Abstract: Digital pathology enables automatic analysis of histopathological sections using artificial intelligence (AI). Automatic evaluation could improve diagnostic efficiency and help find associations between morphological features and clinical outcome. For development of such prediction models, identifying invasive epithelial cells, and separating these from benign epithelial cells and in situ lesions would be the first step. In this study, we aimed to develop an AI model for segmentation of epithelial cells in sections from breast cancer. We generated epithelial ground truth masks by restaining hematoxylin and eosin (HE) sections with cytokeratin (CK) AE1/AE3, and by pathologists' annotations. HE/CK image pairs were used to train a convolutional neural network, and data augmentation was used to make the model more robust. Tissue microarrays (TMAs) from 839 patients, and whole slide images from two patients were used for training an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22870;&#21169;&#22609;&#24418;&#31243;&#24207;&#65292;&#30830;&#20445;&#26368;&#20248;&#31574;&#30053;&#29983;&#25104;&#31526;&#21512;&#25351;&#23450;&#25511;&#21046;&#35201;&#27714;&#30340;&#36712;&#36857;&#65292;&#24182;&#35780;&#20272;&#31574;&#30053;&#26159;&#21542;&#28385;&#36275;&#36825;&#20123;&#35201;&#27714;&#12290;</title><link>https://arxiv.org/abs/2311.10026</link><description>&lt;p&gt;
&#36890;&#36807;&#22870;&#21169;&#22609;&#24418;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20445;&#35777;&#25511;&#21046;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Guaranteeing Control Requirements via Reward Shaping in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10026
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22870;&#21169;&#22609;&#24418;&#31243;&#24207;&#65292;&#30830;&#20445;&#26368;&#20248;&#31574;&#30053;&#29983;&#25104;&#31526;&#21512;&#25351;&#23450;&#25511;&#21046;&#35201;&#27714;&#30340;&#36712;&#36857;&#65292;&#24182;&#35780;&#20272;&#31574;&#30053;&#26159;&#21542;&#28385;&#36275;&#36825;&#20123;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#35299;&#20915;&#35832;&#22914;&#35843;&#33410;&#21644;&#36319;&#36394;&#31561;&#25511;&#21046;&#38382;&#39064;&#26102;&#65292;&#36890;&#24120;&#38656;&#35201;&#30830;&#20445;&#25152;&#33719;&#24471;&#30340;&#31574;&#30053;&#22312;&#37096;&#32626;&#20043;&#21069;&#28385;&#36275;&#20851;&#38190;&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#26631;&#20934;&#65292;&#22914;&#26399;&#26395;&#30340;&#31283;&#23450;&#26102;&#38388;&#21644;&#31283;&#24577;&#35823;&#24046;&#12290; &#37492;&#20110;&#36825;&#31181;&#24517;&#35201;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#32467;&#26524;&#21644;&#19968;&#31181;&#31995;&#32479;&#22870;&#21169;&#22609;&#24418;&#31243;&#24207;&#65292;&#35813;&#31243;&#24207;&#65288;i&#65289;&#30830;&#20445;&#26368;&#20248;&#31574;&#30053;&#29983;&#25104;&#31526;&#21512;&#25351;&#23450;&#25511;&#21046;&#35201;&#27714;&#30340;&#36712;&#36857;&#65292;&#65288;ii&#65289;&#20801;&#35768;&#35780;&#20272;&#20219;&#20309;&#32473;&#23450;&#31574;&#30053;&#26159;&#21542;&#28385;&#36275;&#36825;&#20123;&#35201;&#27714;&#12290; &#25105;&#20204;&#36890;&#36807;&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;OpenAI Gym&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#20840;&#38754;&#25968;&#20540;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;&#20498;&#31435;&#25670;&#25670;&#21160;&#38382;&#39064;&#21644;&#26376;&#29699;&#30528;&#38470;&#22120;&#12290; &#21033;&#29992;&#34920;&#26684;&#24335;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;&#23427;&#22312;&#30830;&#20445;&#31574;&#30053;&#36981;&#23432;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10026v2 Announce Type: replace-cross  Abstract: In addressing control problems such as regulation and tracking through reinforcement learning, it is often required to guarantee that the acquired policy meets essential performance and stability criteria such as a desired settling time and steady-state error prior to deployment. Motivated by this necessity, we present a set of results and a systematic reward shaping procedure that (i) ensures the optimal policy generates trajectories that align with specified control requirements and (ii) allows to assess whether any given policy satisfies them. We validate our approach through comprehensive numerical experiments conducted in two representative environments from OpenAI Gym: the Inverted Pendulum swing-up problem and the Lunar Lander. Utilizing both tabular and deep reinforcement learning methods, our experiments consistently affirm the efficacy of our proposed framework, highlighting its effectiveness in ensuring policy adhere
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#31561;&#25928;&#24615;&#65292;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#20248;&#21270;&#27979;&#37327;&#36807;&#31243;&#65292;&#23454;&#29616;&#23545;&#28151;&#27788;&#21160;&#24577;&#31995;&#32479;&#30340;&#36817;&#20284;&#26368;&#20248;&#27979;&#37327;&#12290;</title><link>https://arxiv.org/abs/2311.04896</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27979;&#37327;&#28151;&#27788;&#21160;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Machine-learning optimized measurements of chaotic dynamical systems via the information bottleneck
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04896
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#31561;&#25928;&#24615;&#65292;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#20248;&#21270;&#27979;&#37327;&#36807;&#31243;&#65292;&#23454;&#29616;&#23545;&#28151;&#27788;&#21160;&#24577;&#31995;&#32479;&#30340;&#36817;&#20284;&#26368;&#20248;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#24615;&#28151;&#27788;&#20801;&#35768;&#23545;&#8220;&#23436;&#32654;&#27979;&#37327;&#8221;&#30340;&#31934;&#30830;&#27010;&#24565;&#65292;&#21363;&#24403;&#37325;&#22797;&#33719;&#24471;&#26102;&#65292;&#25429;&#33719;&#31995;&#32479;&#28436;&#21270;&#25152;&#20135;&#29983;&#30340;&#25152;&#26377;&#20449;&#24687;&#24182;&#20855;&#26377;&#26368;&#23567;&#20887;&#20313;&#12290;&#23547;&#25214;&#26368;&#20339;&#27979;&#37327;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#24182;&#19988;&#22312;&#26497;&#23569;&#25968;&#24773;&#20917;&#19979;&#38656;&#35201;&#23545;&#21160;&#24577;&#30340;&#28145;&#20837;&#20102;&#35299;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#23436;&#32654;&#27979;&#37327;&#21644;&#20449;&#24687;&#29942;&#39048;&#21464;&#31181;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#20248;&#21270;&#27979;&#37327;&#36807;&#31243;&#65292;&#20174;&#36712;&#36857;&#25968;&#25454;&#26377;&#25928;&#25552;&#21462;&#20449;&#24687;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#22810;&#20010;&#28151;&#27788;&#26144;&#23556;&#30340;&#36817;&#20284;&#26368;&#20248;&#27979;&#37327;&#65292;&#24182;&#20026;&#20174;&#19968;&#33324;&#26102;&#38388;&#24207;&#21015;&#20013;&#39640;&#25928;&#25552;&#21462;&#20449;&#24687;&#22880;&#23450;&#20102;&#24517;&#35201;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04896v2 Announce Type: replace  Abstract: Deterministic chaos permits a precise notion of a "perfect measurement" as one that, when obtained repeatedly, captures all of the information created by the system's evolution with minimal redundancy. Finding an optimal measurement is challenging, and has generally required intimate knowledge of the dynamics in the few cases where it has been done. We establish an equivalence between a perfect measurement and a variant of the information bottleneck. As a consequence, we can employ machine learning to optimize measurement processes that efficiently extract information from trajectory data. We obtain approximately optimal measurements for multiple chaotic maps and lay the necessary groundwork for efficient information extraction from general time series.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#65292;&#21033;&#29992;&#27491;&#35268;&#21270;&#27969;&#32852;&#21512;&#36924;&#36817;&#20174;&#22024;&#26434;&#30340;InSAR&#22270;&#20687;&#20013;&#22810;&#20010;&#26410;&#35266;&#27979;&#30340;&#28798;&#23475;&#21644;&#24433;&#21709;&#30340;&#21518;&#39564;&#12290;</title><link>https://arxiv.org/abs/2310.13805</link><description>&lt;p&gt;
&#22522;&#20110;&#27491;&#35268;&#21270;&#27969;&#30340;&#28145;&#24230;&#21464;&#20998;&#36125;&#21494;&#26031;&#32593;&#32476;&#29992;&#20110;&#22320;&#38663;&#22810;&#28798;&#23475;&#21644;&#24433;&#21709;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Normalizing flow-based deep variational Bayesian network for seismic multi-hazards and impacts estimation from InSAR imagery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.13805
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#65292;&#21033;&#29992;&#27491;&#35268;&#21270;&#27969;&#32852;&#21512;&#36924;&#36817;&#20174;&#22024;&#26434;&#30340;InSAR&#22270;&#20687;&#20013;&#22810;&#20010;&#26410;&#35266;&#27979;&#30340;&#28798;&#23475;&#21644;&#24433;&#21709;&#30340;&#21518;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22330;&#28798;&#23475;&#22914;&#22320;&#38663;&#21487;&#33021;&#24341;&#21457;&#32423;&#32852;&#28798;&#23475;&#21644;&#24433;&#21709;&#65292;&#20363;&#22914;&#28369;&#22369;&#21644;&#22522;&#30784;&#35774;&#26045;&#25439;&#22351;&#65292;&#23548;&#33268;&#28798;&#38590;&#24615;&#25439;&#22833;&#65307;&#22240;&#27492;&#65292;&#21450;&#26102;&#21644;&#20934;&#30830;&#30340;&#20272;&#35745;&#23545;&#20110;&#28798;&#21518;&#24212;&#24613;&#21453;&#24212;&#33267;&#20851;&#37325;&#35201;&#12290;&#24178;&#28041;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;InSAR&#65289;&#25968;&#25454;&#22312;&#25552;&#20379;&#39640;&#20998;&#36776;&#29575;&#29616;&#22330;&#20449;&#24687;&#20197;&#36827;&#34892;&#24555;&#36895;&#28798;&#23475;&#20272;&#35745;&#26041;&#38754;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26368;&#36817;&#21033;&#29992;InSAR&#22270;&#20687;&#20449;&#21495;&#30340;&#26041;&#27861;&#36890;&#24120;&#39044;&#27979;&#21333;&#19968;&#31867;&#22411;&#30340;&#28798;&#23475;&#65292;&#22240;&#27492;&#32463;&#24120;&#30001;&#20110;&#30001;&#21516;&#22320;&#28857;&#30340;&#22810;&#31181;&#28798;&#23475;&#12289;&#24433;&#21709;&#21644;&#26080;&#20851;&#30340;&#29615;&#22659;&#21464;&#21270;&#65288;&#20363;&#22914;&#26893;&#34987;&#21464;&#21270;&#12289;&#20154;&#31867;&#27963;&#21160;&#65289;&#24341;&#36215;&#30340;&#22024;&#26434;&#21644;&#22797;&#26434;&#20449;&#21495;&#32780;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#21464;&#20998;&#25512;&#26029;&#65292;&#21033;&#29992;&#27491;&#35268;&#21270;&#27969;&#32852;&#21512;&#36924;&#36817;&#20174;&#22024;&#26434;&#30340;InSAR&#22270;&#20687;&#20013;&#22810;&#20010;&#26410;&#35266;&#27979;&#30340;&#28798;&#23475;&#21644;&#24433;&#21709;&#30340;&#21518;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.13805v2 Announce Type: replace  Abstract: Onsite disasters like earthquakes can trigger cascading hazards and impacts, such as landslides and infrastructure damage, leading to catastrophic losses; thus, rapid and accurate estimates are crucial for timely and effective post-disaster responses. Interferometric Synthetic aperture radar (InSAR) data is important in providing high-resolution onsite information for rapid hazard estimation. Most recent methods using InSAR imagery signals predict a single type of hazard and thus often suffer low accuracy due to noisy and complex signals induced by co-located hazards, impacts, and irrelevant environmental changes (e.g., vegetation changes, human activities). We introduce a novel stochastic variational inference with normalizing flows derived to jointly approximate posteriors of multiple unobserved hazards and impacts from noisy InSAR imagery.
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#24369;&#30417;&#30563;&#22330;&#26223;&#22270;&#29983;&#25104;&#20013;&#30340;&#35821;&#20041;&#36807;&#24230;&#31616;&#21270;&#21644;&#20302;&#23494;&#24230;&#22330;&#26223;&#22270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2310.10404</link><description>&lt;p&gt;
LLM4SGG: &#29992;&#20110;&#24369;&#30417;&#30563;&#22330;&#26223;&#22270;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLM4SGG: Large Language Model for Weakly Supervised Scene Graph Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10404
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#24369;&#30417;&#30563;&#22330;&#26223;&#22270;&#29983;&#25104;&#20013;&#30340;&#35821;&#20041;&#36807;&#24230;&#31616;&#21270;&#21644;&#20302;&#23494;&#24230;&#22330;&#26223;&#22270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#22330;&#26223;&#22270;&#29983;&#25104;&#65288;WSSGG&#65289;&#30740;&#31350;&#26368;&#36817;&#20986;&#29616;&#20316;&#20026;&#23545;&#20005;&#37325;&#20381;&#36182;&#26114;&#36149;&#26631;&#27880;&#30340;&#20840;&#30417;&#30563;&#26041;&#27861;&#30340;&#21478;&#19968;&#31181;&#36873;&#25321;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;WSSGG&#30740;&#31350;&#21033;&#29992;&#22270;&#20687;&#26631;&#39064;&#33719;&#21462;&#26410;&#23450;&#20301;&#19977;&#20803;&#32452;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#22270;&#20687;&#21306;&#22495;&#20013;&#30340;&#26410;&#23450;&#20301;&#19977;&#20803;&#32452;&#36827;&#34892;&#23450;&#20301;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24573;&#30053;&#20102;&#20174;&#26631;&#39064;&#20013;&#24418;&#25104;&#19977;&#20803;&#32452;&#30340;&#20004;&#20010;&#38382;&#39064;&#65306;1&#65289;&#20174;&#26631;&#39064;&#25552;&#21462;&#19977;&#20803;&#32452;&#26102;&#20986;&#29616;&#30340;&#35821;&#20041;&#36807;&#24230;&#31616;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#26631;&#39064;&#20013;&#30340;&#32454;&#31890;&#24230;&#35859;&#35789;&#19981;&#24076;&#26395;&#22320;&#36716;&#25442;&#20026;&#31895;&#31890;&#24230;&#35859;&#35789;&#65292;&#23548;&#33268;&#38271;&#23614;&#35859;&#35789;&#20998;&#24067;&#65292;&#20197;&#21450;2&#65289;&#23558;&#26631;&#39064;&#20013;&#30340;&#19977;&#20803;&#32452;&#19982;&#24863;&#20852;&#36259;&#30340;&#23454;&#20307;/&#35859;&#35789;&#31867;&#23545;&#40784;&#26102;&#20986;&#29616;&#20302;&#23494;&#24230;&#22330;&#26223;&#22270;&#38382;&#39064;&#65292;&#20854;&#20013;&#35768;&#22810;&#19977;&#20803;&#32452;&#34987;&#20002;&#24323;&#19988;&#19981;&#29992;&#20110;&#35757;&#32451;&#65292;&#23548;&#33268;&#30417;&#30563;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.10404v5 Announce Type: cross  Abstract: Weakly-Supervised Scene Graph Generation (WSSGG) research has recently emerged as an alternative to the fully-supervised approach that heavily relies on costly annotations. In this regard, studies on WSSGG have utilized image captions to obtain unlocalized triplets while primarily focusing on grounding the unlocalized triplets over image regions. However, they have overlooked the two issues involved in the triplet formation process from the captions: 1) Semantic over-simplification issue arises when extracting triplets from captions, where fine-grained predicates in captions are undesirably converted into coarse-grained predicates, resulting in a long-tailed predicate distribution, and 2) Low-density scene graph issue arises when aligning the triplets in the caption with entity/predicate classes of interest, where many triplets are discarded and not used in training, leading to insufficient supervision. To tackle the two issues, we pro
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#24067;&#21305;&#37197;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#35757;&#32451;&#25968;&#25454;&#21512;&#25104;&#65292;&#25552;&#21319;&#20102;&#21512;&#25104;&#25968;&#25454;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#21644;&#23454;&#29992;&#24615;</title><link>https://arxiv.org/abs/2310.10402</link><description>&lt;p&gt;
Real-Fake&#65306;&#36890;&#36807;&#20998;&#24067;&#21305;&#37197;&#36827;&#34892;&#26377;&#25928;&#30340;&#35757;&#32451;&#25968;&#25454;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Real-Fake: Effective Training Data Synthesis Through Distribution Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10402
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#24067;&#21305;&#37197;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#35757;&#32451;&#25968;&#25454;&#21512;&#25104;&#65292;&#25552;&#21319;&#20102;&#21512;&#25104;&#25968;&#25454;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#21644;&#23454;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#22312;&#35768;&#22810;&#23398;&#20064;&#20219;&#21153;&#21644;&#22330;&#26223;&#20013;&#21344;&#25454;&#37325;&#35201;&#22320;&#20301;&#65292;&#23427;&#20855;&#26377;&#25968;&#25454;&#38598;&#22686;&#24378;&#12289;&#27867;&#21270;&#35780;&#20272;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#26041;&#27861;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#22312;&#19987;&#38376;&#35757;&#32451;&#39640;&#32423;&#28145;&#24230;&#27169;&#22411;&#26102;&#25928;&#29575;&#20173;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#20854;&#23454;&#38469;&#25928;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#21512;&#25104;&#30340;&#21407;&#21017;&#65292;&#24182;&#20174;&#20998;&#24067;&#21305;&#37197;&#30340;&#35282;&#24230;&#38416;&#26126;&#20102;&#19968;&#20010;&#21407;&#29702;&#24615;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#38416;&#26126;&#20102;&#25903;&#37197;&#21512;&#25104;&#26377;&#25928;&#24615;&#30340;&#26426;&#21046;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#21512;&#25104;&#25968;&#25454;&#22312;&#21508;&#31181;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#26082;&#21487;&#20197;&#26367;&#20195;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#20063;&#21487;&#20197;&#20316;&#20026;&#20854;&#22686;&#24378;&#65292;&#21516;&#26102;&#20855;&#26377;&#35832;&#22914;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#12289;&#38544;&#31169;&#20445;&#25252;&#21644;&#21487;&#20280;&#32553;&#24615;&#31561;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.10402v2 Announce Type: replace  Abstract: Synthetic training data has gained prominence in numerous learning tasks and scenarios, offering advantages such as dataset augmentation, generalization evaluation, and privacy preservation. Despite these benefits, the efficiency of synthetic data generated by current methodologies remains inferior when training advanced deep models exclusively, limiting its practical utility. To address this challenge, we analyze the principles underlying training data synthesis for supervised learning and elucidate a principled theoretical framework from the distribution-matching perspective that explicates the mechanisms governing synthesis efficacy. Through extensive experiments, we demonstrate the effectiveness of our synthetic data across diverse image classification tasks, both as a replacement for and augmentation to real datasets, while also benefits such as out-of-distribution generalization, privacy preservation, and scalability. Specifica
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#23558;&#19968;&#32452;&#19981;&#21464;&#24615;&#32435;&#20837;&#34920;&#31034;&#20013;&#65292;&#26500;&#24314;&#19968;&#20010;&#19981;&#21464;&#24615;&#20998;&#37327;&#30340;&#20056;&#31215;&#31354;&#38388;&#65292;&#20174;&#32780;&#22686;&#24378;&#28508;&#22312;&#31354;&#38388;&#30340;&#36890;&#20449;&#12290;</title><link>https://arxiv.org/abs/2310.01211</link><description>&lt;p&gt;
&#20174;&#30742;&#22359;&#21040;&#26725;&#26753;&#65306;&#22686;&#24378;&#28508;&#22312;&#31354;&#38388;&#36890;&#20449;&#30340;&#19981;&#21464;&#24615;&#20056;&#31215;
&lt;/p&gt;
&lt;p&gt;
From Bricks to Bridges: Product of Invariances to Enhance Latent Space Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.01211
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#23558;&#19968;&#32452;&#19981;&#21464;&#24615;&#32435;&#20837;&#34920;&#31034;&#20013;&#65292;&#26500;&#24314;&#19968;&#20010;&#19981;&#21464;&#24615;&#20998;&#37327;&#30340;&#20056;&#31215;&#31354;&#38388;&#65292;&#20174;&#32780;&#22686;&#24378;&#28508;&#22312;&#31354;&#38388;&#30340;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#30456;&#20284;&#30340;&#24402;&#32435;&#20559;&#24046;&#19979;&#35757;&#32451;&#30340;&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#38544;&#34255;&#20102;&#32467;&#26500;&#30456;&#20284;&#24615;&#12290;&#20174;&#20960;&#20309;&#35282;&#24230;&#26469;&#30475;&#65292;&#35782;&#21035;&#36830;&#25509;&#36825;&#20123;&#34920;&#31034;&#30340;&#36716;&#25442;&#31867;&#21035;&#21644;&#30456;&#20851;&#19981;&#21464;&#24615;&#23545;&#20110;&#35299;&#38145;&#21512;&#24182;&#12289;&#25340;&#25509;&#21644;&#37325;&#29992;&#19981;&#21516;&#31070;&#32463;&#27169;&#22359;&#31561;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22810;&#31181;&#22240;&#32032;&#65288;&#22914;&#26435;&#37325;&#21021;&#22987;&#21270;&#12289;&#35757;&#32451;&#36229;&#21442;&#25968;&#25110;&#25968;&#25454;&#27169;&#24577;&#65289;&#23548;&#33268;&#20107;&#20808;&#20272;&#35745;&#29305;&#23450;&#20219;&#21153;&#30340;&#36716;&#25442;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#26114;&#36149;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#30452;&#25509;&#23558;&#19968;&#32452;&#19981;&#21464;&#24615;&#32435;&#20837;&#34920;&#31034;&#20013;&#65292;&#26500;&#24314;&#19968;&#20010;&#19981;&#21464;&#24615;&#20998;&#37327;&#30340;&#20056;&#31215;&#31354;&#38388;&#20301;&#20110;&#28508;&#22312;&#34920;&#31034;&#20043;&#19978;&#65292;&#32780;&#19981;&#38656;&#35201;&#20851;&#20110;&#35201;&#27880;&#20837;&#30340;&#26368;&#20339;&#19981;&#21464;&#24615;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;&#20998;&#31867;&#21644;&#37325;&#24314;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.01211v2 Announce Type: replace  Abstract: It has been observed that representations learned by distinct neural networks conceal structural similarities when the models are trained under similar inductive biases. From a geometric perspective, identifying the classes of transformations and the related invariances that connect these representations is fundamental to unlocking applications, such as merging, stitching, and reusing different neural modules. However, estimating task-specific transformations a priori can be challenging and expensive due to several factors (e.g., weights initialization, training hyperparameters, or data modality). To this end, we introduce a versatile method to directly incorporate a set of invariances into the representations, constructing a product space of invariant components on top of the latent representations without requiring prior knowledge about the optimal invariance to infuse. We validate our solution on classification and reconstruction 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Bounce&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#21464;&#37327;&#31867;&#22411;&#26144;&#23556;&#20026;&#36880;&#28176;&#22686;&#21152;&#32500;&#24230;&#30340;&#23884;&#20837;&#65292;&#21487;&#22312;&#32452;&#21512;&#21644;&#28151;&#21512;&#36755;&#20837;&#31354;&#38388;&#20013;&#21487;&#38752;&#22320;&#20248;&#21270;&#39640;&#32500;&#40657;&#30418;&#20989;&#25968;&#65292;&#24182;&#22312;&#22810;&#31181;&#39640;&#32500;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#21487;&#38752;&#24182;&#19988;&#36890;&#24120;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2307.00618</link><description>&lt;p&gt;
Bounce: &#21487;&#38752;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#32452;&#21512;&#19982;&#28151;&#21512;&#31354;&#38388;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bounce: Reliable High-Dimensional Bayesian Optimization for Combinatorial and Mixed Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.00618
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Bounce&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#21464;&#37327;&#31867;&#22411;&#26144;&#23556;&#20026;&#36880;&#28176;&#22686;&#21152;&#32500;&#24230;&#30340;&#23884;&#20837;&#65292;&#21487;&#22312;&#32452;&#21512;&#21644;&#28151;&#21512;&#36755;&#20837;&#31354;&#38388;&#20013;&#21487;&#38752;&#22320;&#20248;&#21270;&#39640;&#32500;&#40657;&#30418;&#20989;&#25968;&#65292;&#24182;&#22312;&#22810;&#31181;&#39640;&#32500;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#21487;&#38752;&#24182;&#19988;&#36890;&#24120;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#21147;&#30340;&#24212;&#29992;&#65292;&#22914;&#26448;&#26009;&#21457;&#29616;&#12289;&#30828;&#20214;&#35774;&#35745;&#12289;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25110;&#32452;&#21512;&#20248;&#21270;&#35201;&#27714;&#20248;&#21270;&#28151;&#21512;&#21644;&#32452;&#21512;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#39640;&#32500;&#40657;&#30418;&#20989;&#25968;&#12290;&#23613;&#31649;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#28145;&#20837;&#20998;&#26512;&#34920;&#26126;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#24182;&#19981;&#21487;&#38752;&#12290;&#24403;&#20989;&#25968;&#30340;&#26410;&#30693;&#26368;&#20248;&#35299;&#27809;&#26377;&#29305;&#23450;&#32467;&#26500;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20250;&#22823;&#24133;&#19979;&#38477;&#12290;&#20026;&#20102;&#28385;&#36275;&#23545;&#32452;&#21512;&#21644;&#28151;&#21512;&#31354;&#38388;&#21487;&#38752;&#31639;&#27861;&#30340;&#38656;&#27714;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#20110;&#23558;&#21508;&#31181;&#21464;&#37327;&#31867;&#22411;&#26144;&#23556;&#20026;&#36880;&#28176;&#22686;&#21152;&#32500;&#24230;&#30340;&#23884;&#20837;&#30340;&#26032;&#39062;&#26144;&#23556;Bounce&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Bounce&#22312;&#21508;&#31181;&#39640;&#32500;&#38382;&#39064;&#19978;&#21487;&#38752;&#22320;&#23454;&#29616;&#24182;&#36890;&#24120;&#29978;&#33267;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.00618v2 Announce Type: replace  Abstract: Impactful applications such as materials discovery, hardware design, neural architecture search, or portfolio optimization require optimizing high-dimensional black-box functions with mixed and combinatorial input spaces. While Bayesian optimization has recently made significant progress in solving such problems, an in-depth analysis reveals that the current state-of-the-art methods are not reliable. Their performances degrade substantially when the unknown optima of the function do not have a certain structure. To fill the need for a reliable algorithm for combinatorial and mixed spaces, this paper proposes Bounce that relies on a novel map of various variable types into nested embeddings of increasing dimensionality. Comprehensive experiments show that Bounce reliably achieves and often even improves upon state-of-the-art performance on a variety of high-dimensional problems.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20998;&#26512;&#20102;&#39564;&#35777;&#30721;&#31995;&#32479;&#30340;&#28431;&#27934;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#31867;&#22411;&#30340;CAPTCHAs&#12290;</title><link>https://arxiv.org/abs/2302.09389</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23545;&#39564;&#35777;&#30721;&#30340;&#28431;&#27934;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Vulnerability analysis of captcha using Deep learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.09389
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20998;&#26512;&#20102;&#39564;&#35777;&#30721;&#31995;&#32479;&#30340;&#28431;&#27934;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#31867;&#22411;&#30340;CAPTCHAs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20010;&#32593;&#31449;&#20026;&#38450;&#33539;&#21361;&#38505;&#30340;&#20114;&#32852;&#32593;&#25915;&#20987;&#24182;&#25552;&#39640;&#23433;&#20840;&#24615;&#65292;&#23454;&#26045;&#20102;CAPTCHAs&#65288;Completely Automated Public Turing test to tell Computers and Humans Apart&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#26368;&#32456;&#29992;&#25143;&#26159;&#20154;&#31867;&#36824;&#26159;&#26426;&#22120;&#20154;&#30340;&#39564;&#35777;&#31995;&#32479;&#12290;&#26368;&#24120;&#35265;&#30340;CAPTCHA&#26159;&#22522;&#20110;&#25991;&#26412;&#30340;&#65292;&#35774;&#35745;&#25104;&#26131;&#20110;&#20154;&#31867;&#35782;&#21035;&#32780;&#23545;&#26426;&#22120;&#25110;&#26426;&#22120;&#20154;&#38590;&#20197;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#24320;&#21457;&#33021;&#22815;&#39044;&#27979;&#22522;&#20110;&#25991;&#26412;&#30340;CAPTCHAs&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#23481;&#26131;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#30340;&#22312;&#20110;&#35843;&#26597;CAPTCHA&#29983;&#25104;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#21644;&#28431;&#27934;&#65292;&#20197;&#35774;&#35745;&#26356;&#20855;&#24377;&#24615;&#30340;CAPTCHAs&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;CapNet&#65292;&#19968;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#25152;&#25552;&#20986;&#30340;&#24179;&#21488;&#21487;&#20197;&#35780;&#20272;&#25968;&#23383;&#21644;&#23383;&#27597;&#25968;&#23383;&#28151;&#21512;&#22411;CAPTCHAs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.09389v2 Announce Type: replace-cross  Abstract: Several websites improve their security and avoid dangerous Internet attacks by implementing CAPTCHAs (Completely Automated Public Turing test to tell Computers and Humans Apart), a type of verification to identify whether the end-user is human or a robot. The most prevalent type of CAPTCHA is text-based, designed to be easily recognized by humans while being unsolvable towards machines or robots. However, as deep learning technology progresses, development of convolutional neural network (CNN) models that predict text-based CAPTCHAs becomes easier. The purpose of this research is to investigate the flaws and vulnerabilities in the CAPTCHA generating systems in order to design more resilient CAPTCHAs. To achieve this, we created CapNet, a Convolutional Neural Network. The proposed platform can evaluate both numerical and alphanumerical CAPTCHAs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#36793;&#32536;&#27844;&#28431;&#30340;&#38544;&#31169;&#39118;&#38505;&#19982;&#20010;&#20307;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21457;&#29616;&#24403;&#33410;&#28857;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#25552;&#39640;&#26102;&#65292;&#38544;&#31169;&#39118;&#38505;&#20250;&#21319;&#32423;&#65292;&#24433;&#21709;GNNs&#21516;&#26102;&#23454;&#29616;&#38544;&#31169;&#21644;&#20844;&#24179;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#24179;&#34913;&#20844;&#24179;&#21644;&#38544;&#31169;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2301.12951</link><description>&lt;p&gt;
&#25581;&#31034;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#20010;&#20307;&#20844;&#24179;&#24615;&#30340;&#38544;&#31169;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Unraveling Privacy Risks of Individual Fairness in Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.12951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#36793;&#32536;&#27844;&#28431;&#30340;&#38544;&#31169;&#39118;&#38505;&#19982;&#20010;&#20307;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21457;&#29616;&#24403;&#33410;&#28857;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#25552;&#39640;&#26102;&#65292;&#38544;&#31169;&#39118;&#38505;&#20250;&#21319;&#32423;&#65292;&#24433;&#21709;GNNs&#21516;&#26102;&#23454;&#29616;&#38544;&#31169;&#21644;&#20844;&#24179;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#24179;&#34913;&#20844;&#24179;&#21644;&#38544;&#31169;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#32780;&#21463;&#21040;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#35201;&#26500;&#24314;&#20540;&#24471;&#20449;&#36182;&#30340;GNNs&#65292;&#20844;&#24179;&#24615;&#21644;&#38544;&#31169;&#20004;&#20010;&#26041;&#38754;&#24050;&#25104;&#20026;&#37325;&#35201;&#32771;&#34385;&#22240;&#32032;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20998;&#21035;&#32771;&#23519;&#20102;GNNs&#30340;&#20844;&#24179;&#24615;&#21644;&#38544;&#31169;&#24615;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#19982;GNN&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#20010;&#26041;&#38754;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#23578;&#26410;&#34987;&#25506;&#31350;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#36793;&#32536;&#27844;&#28431;&#30340;&#38544;&#31169;&#39118;&#38505;&#19982;GNN&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#25581;&#31034;&#20102;&#24403;&#33410;&#28857;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#25552;&#39640;&#26102;&#65292;&#36793;&#32536;&#38544;&#31169;&#39118;&#38505;&#19981;&#24184;&#22320;&#21319;&#32423;&#12290;&#36825;&#19968;&#38382;&#39064;&#38459;&#30861;&#20102;GNNs&#21516;&#26102;&#23454;&#29616;&#38544;&#31169;&#21644;&#20844;&#24179;&#12290;&#20026;&#20102;&#24179;&#34913;&#20844;&#24179;&#21644;&#38544;&#31169;&#65292;&#25105;&#20204;&#35880;&#24910;&#22320;&#24341;&#20837;&#20102;&#22522;&#20110;&#24433;&#21709;&#20989;&#25968;&#30340;&#20844;&#24179;&#24863;&#30693;&#25439;&#22833;&#37325;&#21152;&#26435;&#21644;&#22522;&#20110;&#38544;&#31169;&#24863;&#30693;&#30340;&#22270;&#32467;&#26500;&#25200;&#21160;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.12951v2 Announce Type: replace  Abstract: Graph neural networks (GNNs) have gained significant attraction due to their expansive real-world applications. To build trustworthy GNNs, two aspects - fairness and privacy - have emerged as critical considerations. Previous studies have separately examined the fairness and privacy aspects of GNNs, revealing their trade-off with GNN performance. Yet, the interplay between these two aspects remains unexplored. In this paper, we pioneer the exploration of the interaction between the privacy risks of edge leakage and the individual fairness of a GNN. Our theoretical analysis unravels that edge privacy risks unfortunately escalate when the nodes' individual fairness improves. Such an issue hinders the accomplishment of privacy and fairness of GNNs at the same time. To balance fairness and privacy, we carefully introduce fairness-aware loss reweighting based on influence function and privacy-aware graph structure perturbation modules wit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#22686;&#24378;&#21644;&#38598;&#25104;&#23398;&#20064;&#26694;&#26550;&#65288;DBEL&#65289;&#26469;&#31579;&#26597;&#30111;&#30142;&#23492;&#29983;&#34411;&#22270;&#20687;&#65292;&#20854;&#20013;&#21253;&#25324;&#26032;&#30340;&#22686;&#24378;-BR-STM&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#31579;&#26597;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2212.02477</link><description>&lt;p&gt;
&#20351;&#29992;&#26032;&#30340;&#28145;&#24230;&#22686;&#24378;&#21644;&#38598;&#25104;&#23398;&#20064;&#26694;&#26550;&#36827;&#34892;&#30111;&#30142;&#23492;&#29983;&#34411;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Malaria Parasitic Detection using a New Deep Boosted and Ensemble Learning Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.02477
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#22686;&#24378;&#21644;&#38598;&#25104;&#23398;&#20064;&#26694;&#26550;&#65288;DBEL&#65289;&#26469;&#31579;&#26597;&#30111;&#30142;&#23492;&#29983;&#34411;&#22270;&#20687;&#65292;&#20854;&#20013;&#21253;&#25324;&#26032;&#30340;&#22686;&#24378;-BR-STM&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#31579;&#26597;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30111;&#30142;&#26159;&#30001;&#38604;&#24615;&#25353;&#34442;&#27880;&#20837;&#30340;&#33268;&#21629;&#30111;&#21407;&#34411;&#24341;&#36215;&#30340;&#30142;&#30149;&#65292;&#24863;&#26579;&#32418;&#32454;&#32990;&#65292;&#27599;&#24180;&#22312;&#20840;&#29699;&#25968;&#30334;&#19975;&#20154;&#20013;&#36896;&#25104;&#24863;&#26579;&#12290;&#28982;&#32780;&#65292;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#65292;&#19987;&#23478;&#30340;&#25163;&#21160;&#31579;&#26597;&#26159;&#36153;&#21147;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;. &#22240;&#27492;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#22686;&#24378;&#21644;&#38598;&#25104;&#23398;&#20064;&#65288;DBEL&#65289;&#26694;&#26550;&#65292;&#21253;&#25324;&#26032;&#30340;&#22686;&#24378;-BR-STM&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#22534;&#21472;&#21644;&#38598;&#25104;ML&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#31579;&#26597;&#30111;&#30142;&#23492;&#29983;&#34411;&#22270;&#20687;&#12290;&#25552;&#20986;&#30340;&#22686;&#24378;-BR-STM&#22522;&#20110;&#26032;&#30340;&#25193;&#24352;&#21367;&#31215;&#22359;&#20998;&#35010;&#21464;&#25442;&#21512;&#24182;&#65288;STM&#65289;&#21644;&#29305;&#24449;&#22270;&#25380;&#21387;&#22686;&#24378;&#65288;SB&#65289;&#24605;&#24819;&#12290;&#27492;&#22806;&#65292;&#26032;&#30340;STM&#22359;&#20351;&#29992;&#21306;&#22495;&#21644;&#36793;&#30028;&#25805;&#20316;&#26469;&#23398;&#20064;&#30111;&#30142;&#23492;&#29983;&#34411;&#30340;&#21516;&#36136;&#24615;&#12289;&#24322;&#36136;&#24615;&#21644;&#36793;&#30028;&#19982;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#25277;&#35937;&#12289;&#20013;&#31561;&#21644;&#36793;&#30028;&#27700;&#24179;&#19978;&#29992;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#26032;&#29305;&#24449;&#22270;SB&#22312;STM&#22359;&#20013;&#33719;&#24471;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#36890;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.02477v3 Announce Type: replace-cross  Abstract: Malaria is a potentially fatal plasmodium parasite injected by female anopheles mosquitoes that infect red blood cells and millions worldwide yearly. However, specialists' manual screening in clinical practice is laborious and prone to error. Therefore, a novel Deep Boosted and Ensemble Learning (DBEL) framework, comprising the stacking of new Boosted-BR-STM convolutional neural networks (CNN) and the ensemble ML classifiers, is developed to screen malaria parasite images. The proposed Boosted-BR-STM is based on a new dilated-convolutional block-based split transform merge (STM) and feature-map Squeezing-Boosting (SB) ideas. Moreover, the new STM block uses regional and boundary operations to learn the malaria parasite's homogeneity, heterogeneity, and boundary with patterns. Furthermore, the diverse boosted channels are attained by employing Transfer Learning-based new feature-map SB in STM blocks at the abstract, medium, and 
&lt;/p&gt;</description></item><item><title>PAGE&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#27169;&#22411;&#32423;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#21407;&#22411;&#22270;&#26469;&#35299;&#37322;GNN&#27169;&#22411;&#23545;&#22270;&#20998;&#31867;&#23398;&#20064;&#30340;&#20869;&#23481;&#65292;&#30456;&#27604;&#23454;&#20363;&#32423;&#35299;&#37322;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#26356;&#20026;&#31616;&#27905;&#21644;&#20840;&#38754;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2210.17159</link><description>&lt;p&gt;
PAGE: &#22522;&#20110;&#21407;&#22411;&#30340;&#27169;&#22411;&#32423;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
PAGE: Prototype-Based Model-Level Explanations for Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.17159
&lt;/p&gt;
&lt;p&gt;
PAGE&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#27169;&#22411;&#32423;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#21407;&#22411;&#22270;&#26469;&#35299;&#37322;GNN&#27169;&#22411;&#23545;&#22270;&#20998;&#31867;&#23398;&#20064;&#30340;&#20869;&#23481;&#65292;&#30456;&#27604;&#23454;&#20363;&#32423;&#35299;&#37322;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#26356;&#20026;&#31616;&#27905;&#21644;&#20840;&#38754;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#26694;&#26550;&#21560;&#24341;&#20102;&#30456;&#24403;&#22810;&#30340;&#20851;&#27880;&#65292;&#24102;&#26469;&#20102;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#38761;&#21629;&#65292;&#23545;&#35299;&#37322;GNN&#27169;&#22411;&#30340;&#38656;&#27714;&#20063;&#22312;&#22686;&#21152;&#12290;&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;GNN&#35299;&#37322;&#26041;&#27861;&#65292;&#20294;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#23454;&#20363;&#32423;&#21035;&#30340;&#35299;&#37322;&#19978;&#65292;&#36825;&#20123;&#35299;&#37322;&#23450;&#21046;&#32473;&#23450;&#22270;&#23454;&#20363;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21407;&#22411;&#30340;GNN&#35299;&#37322;&#22120;&#65288;PAGE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#35299;&#37322;&#27169;&#22411;&#32423;GNN&#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#21407;&#22411;&#22270;&#26469;&#35299;&#37322;&#22270;&#20998;&#31867;&#20013;GNN&#27169;&#22411;&#23398;&#21040;&#20102;&#20160;&#20040;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#29305;&#23450;&#31867;&#21035;&#29983;&#25104;&#35299;&#37322;&#65292;&#22240;&#27492;&#33021;&#22815;&#25552;&#20379;&#27604;&#23454;&#20363;&#32423;&#35299;&#37322;&#26356;&#31616;&#27905;&#21644;&#20840;&#38754;&#30340;&#35299;&#37322;&#12290;&#39318;&#20808;&#65292;PAGE&#22312;&#23545;&#23427;&#20204;&#36827;&#34892;&#32858;&#31867;&#21518;&#36873;&#25321;&#31867;&#21035;&#21028;&#21035;&#24615;&#36755;&#20837;&#22270;&#30340;&#23884;&#20837;&#22312;&#22270;&#32423;&#23884;&#20837;&#31354;&#38388;&#19978;&#12290;&#28982;&#21518;&#65292;PAGE&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#25216;&#33021;&#65292;&#22240;&#20026;&#23427;&#25581;&#31034;&#20102;&#19968;&#20010;&#31867;&#21035;&#30340;&#25968;&#25454;&#23558;&#20250;&#22914;&#20309;&#22312;&#23884;&#20837;&#31354;&#38388;&#30340;&#21547;&#20041;&#19978;&#21457;&#29983;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.17159v2 Announce Type: replace  Abstract: Aside from graph neural networks (GNNs) attracting significant attention as a powerful framework revolutionizing graph representation learning, there has been an increasing demand for explaining GNN models. Although various explanation methods for GNNs have been developed, most studies have focused on instance-level explanations, which produce explanations tailored to a given graph instance. In our study, we propose Prototype-bAsed GNN-Explainer (PAGE), a novel model-level GNN explanation method that explains what the underlying GNN model has learned for graph classification by discovering human-interpretable prototype graphs. Our method produces explanations for a given class, thus being capable of offering more concise and comprehensive explanations than those of instance-level explanations. First, PAGE selects embeddings of class-discriminative input graphs on the graph-level embedding space after clustering them. Then, PAGE disco
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;NISQ-TDA&#65292;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#39640;&#32500;&#32463;&#20856;&#25968;&#25454;&#65292;&#24182;&#23545;&#26576;&#20123;&#38382;&#39064;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#28176;&#36817;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2209.09371</link><description>&lt;p&gt;
&#22024;&#26434;&#37327;&#23376;&#35745;&#31639;&#19978;&#30340;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Topological data analysis on noisy quantum computers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.09371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;NISQ-TDA&#65292;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#39640;&#32500;&#32463;&#20856;&#25968;&#25454;&#65292;&#24182;&#23545;&#26576;&#20123;&#38382;&#39064;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#28176;&#36817;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;(TDA)&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#25552;&#21462;&#39640;&#32500;&#25968;&#25454;&#30340;&#22797;&#26434;&#21644;&#26377;&#20215;&#20540;&#30340;&#19982;&#24418;&#29366;&#30456;&#20851;&#30340;&#25688;&#35201;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35745;&#31639;TDA&#30340;&#32463;&#20856;&#31639;&#27861;&#30340;&#35745;&#31639;&#38656;&#27714;&#26159;&#24040;&#22823;&#30340;&#65292;&#23545;&#20110;&#39640;&#38454;&#29305;&#24449;&#24456;&#24555;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#37327;&#23376;&#35745;&#31639;&#26426;&#26377;&#26395;&#22312;&#26576;&#20123;&#35745;&#31639;&#38382;&#39064;&#19978;&#23454;&#29616;&#26174;&#33879;&#21152;&#36895;&#12290;&#23454;&#38469;&#19978;&#65292;TDA&#34987;&#35748;&#20026;&#26159;&#36825;&#31867;&#38382;&#39064;&#20043;&#19968;&#65292;&#28982;&#32780;&#65292;&#20026;&#35813;&#38382;&#39064;&#25552;&#20986;&#30340;&#37327;&#23376;&#35745;&#31639;&#31639;&#27861;&#65292;&#27604;&#22914;Lloyd&#12289;Garnerone&#21644;Zanardi&#25552;&#20986;&#30340;&#26368;&#21021;&#30340;&#37327;&#23376;TDA(QTDA)&#20844;&#24335;&#65292;&#38656;&#35201;&#30446;&#21069;&#19981;&#21487;&#29992;&#30340;&#23481;&#38169;&#36164;&#26684;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NISQ-TDA&#65292;&#36825;&#26159;&#19968;&#20010;&#23436;&#20840;&#23454;&#29616;&#30340;&#31471;&#21040;&#31471;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21482;&#38656;&#35201;&#36739;&#30701;&#30340;&#30005;&#36335;&#28145;&#24230;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#32463;&#20856;&#25968;&#25454;&#65292;&#24182;&#23545;&#26576;&#20123;&#31867;&#21035;&#30340;&#38382;&#39064;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#28176;&#36817;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.09371v4 Announce Type: replace-cross  Abstract: Topological data analysis (TDA) is a powerful technique for extracting complex and valuable shape-related summaries of high-dimensional data. However, the computational demands of classical algorithms for computing TDA are exorbitant, and quickly become impractical for high-order characteristics. Quantum computers offer the potential of achieving significant speedup for certain computational problems. Indeed, TDA has been purported to be one such problem, yet, quantum computing algorithms proposed for the problem, such as the original Quantum TDA (QTDA) formulation by Lloyd, Garnerone and Zanardi, require fault-tolerance qualifications that are currently unavailable. In this study, we present NISQ-TDA, a fully implemented end-to-end quantum machine learning algorithm needing only a short circuit-depth, that is applicable to high-dimensional classical data, and with provable asymptotic speedup for certain classes of problems. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#31867;&#22411;&#29983;&#25104;&#26862;&#26519;&#30340;&#27491;&#21017;&#21270;&#30913; Laplacian &#30340;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22823;&#32780;&#23494;&#38598;&#22270;&#30340;&#35889;&#36817;&#20284;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2208.14797</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#31867;&#22411;&#29983;&#25104;&#26862;&#26519;&#30340;&#27491;&#21017;&#21270;&#30913; Laplacian &#30340;&#31232;&#30095;&#21270;
&lt;/p&gt;
&lt;p&gt;
Sparsification of the regularized magnetic Laplacian with multi-type spanning forests
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.14797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#31867;&#22411;&#29983;&#25104;&#26862;&#26519;&#30340;&#27491;&#21017;&#21270;&#30913; Laplacian &#30340;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22823;&#32780;&#23494;&#38598;&#22270;&#30340;&#35889;&#36817;&#20284;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010; ${\rm U}(1)$-connection &#22270;&#65292;&#21363;&#19968;&#20010;&#22270;&#65292;&#20854;&#20013;&#27599;&#26465;&#26377;&#21521;&#36793;&#37117;&#36171;&#20104;&#19968;&#20010;&#21333;&#20301;&#27169;&#22797;&#25968;&#65292;&#22312;&#21453;&#21521;&#26102;&#21462;&#20849;&#36717;&#12290;&#23545;&#20110;&#32452;&#21512; Laplacian &#30340;&#19968;&#20010;&#33258;&#28982;&#26367;&#20195;&#29289;&#26159;&#30913; Laplacian&#65292;&#19968;&#20010;&#21253;&#21547;&#20102;&#20851;&#20110;&#22270;&#36830;&#25509;&#20449;&#24687;&#30340; Hermite &#30697;&#38453;&#12290;&#30913; Laplacians &#22312;&#35282;&#24230;&#21516;&#27493;&#38382;&#39064;&#20013;&#20986;&#29616;&#12290;&#22312;&#22823;&#32780;&#23494;&#38598;&#30340;&#22270;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#30913; Laplacian $\Delta$ &#30340;&#31232;&#30095;&#21270;&#65292;&#21363;&#22522;&#20110;&#20855;&#26377;&#23569;&#37327;&#36793;&#30340;&#23376;&#22270;&#30340;&#35889;&#36817;&#20284;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#20351;&#29992;&#33258;&#23450;&#20041;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#23545;&#22810;&#31867;&#22411;&#29983;&#25104;&#26862;&#26519;&#65288;MTSFs&#65289;&#36827;&#34892;&#25277;&#26679;&#65292;&#36825;&#26159;&#19968;&#20010;&#27010;&#29575;&#20998;&#24067;&#65292;&#26377;&#21033;&#20110;&#22810;&#26679;&#24615;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;MTSF &#26159;&#19968;&#20010;&#29983;&#25104;&#23376;&#22270;&#65292;&#20854;&#36830;&#36890;&#20998;&#37327;&#20026;&#26641;&#25110;&#29615;&#26681;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.14797v2 Announce Type: replace-cross  Abstract: In this paper, we consider a ${\rm U}(1)$-connection graph, that is, a graph where each oriented edge is endowed with a unit modulus complex number that is conjugated under orientation flip. A natural replacement for the combinatorial Laplacian is then the magnetic Laplacian, an Hermitian matrix that includes information about the graph's connection. Magnetic Laplacians appear, e.g., in the problem of angular synchronization. In the context of large and dense graphs, we study here sparsifiers of the magnetic Laplacian $\Delta$, i.e., spectral approximations based on subgraphs with few edges. Our approach relies on sampling multi-type spanning forests (MTSFs) using a custom determinantal point process, a probability distribution over edges that favours diversity. In a word, an MTSF is a spanning subgraph whose connected components are either trees or cycle-rooted trees. The latter partially capture the angular inconsistencies of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BOBA&#30340;&#25308;&#21344;&#24237;-&#24378;&#20581;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#38750;IID&#35774;&#32622;&#20013;&#30340;&#26631;&#31614;&#20559;&#26012;&#38382;&#39064;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#36873;&#25321;&#20559;&#24046;&#21644;&#25308;&#21344;&#24237;&#25915;&#20987;&#26131;&#21463;&#24433;&#21709;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#35780;&#20272;&#20013;&#37117;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2208.12932</link><description>&lt;p&gt;
BOBA&#65306;&#20855;&#26377;&#26631;&#31614;&#20559;&#26012;&#24230;&#30340;&#25308;&#21344;&#24237;-&#24378;&#20581;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
BOBA: Byzantine-Robust Federated Learning with Label Skewness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.12932
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BOBA&#30340;&#25308;&#21344;&#24237;-&#24378;&#20581;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#38750;IID&#35774;&#32622;&#20013;&#30340;&#26631;&#31614;&#20559;&#26012;&#38382;&#39064;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#36873;&#25321;&#20559;&#24046;&#21644;&#25308;&#21344;&#24237;&#25915;&#20987;&#26131;&#21463;&#24433;&#21709;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#35780;&#20272;&#20013;&#37117;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25239;&#20987;&#25308;&#21344;&#24237;&#25915;&#20987;&#30340;&#40065;&#26834;&#32858;&#21512;&#35268;&#21017;&#65288;AGRs&#65289;&#26159;&#38024;&#23545;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;IID&#65289;&#35774;&#32622;&#32780;&#35774;&#35745;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#26631;&#31614;&#20559;&#26012;&#24230;&#36825;&#19968;&#26356;&#21152;&#29616;&#23454;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;IID&#35774;&#32622;&#65292;&#20854;&#20013;&#27599;&#20010;&#23458;&#25143;&#31471;&#21482;&#33021;&#35775;&#38382;&#23569;&#25968;&#31867;&#21035;&#30340;&#25968;&#25454;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26368;&#20808;&#36827;&#30340;AGR&#23384;&#22312;&#30528;&#36873;&#25321;&#20559;&#24046;&#65292;&#23548;&#33268;&#29305;&#23450;&#31867;&#21035;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65307;&#30001;&#20110;&#35802;&#23454;&#23458;&#25143;&#31471;&#30340;&#26799;&#24230;&#20043;&#38388;&#30340;&#21464;&#21270;&#22686;&#21152;&#65292;&#23427;&#20204;&#20063;&#26356;&#23481;&#26131;&#21463;&#21040;&#25308;&#21344;&#24237;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BOBA&#30340;&#39640;&#25928;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;BOBA&#30340;&#25910;&#25947;&#24615;&#20855;&#26377;&#26368;&#20339;&#32423;&#21035;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#19982;&#21508;&#31181;&#22522;&#32447;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;BOBA&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#26080;&#20559;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://gith&#19978;&#33719;&#24471;
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.12932v2 Announce Type: replace  Abstract: In federated learning, most existing robust aggregation rules (AGRs) combat Byzantine attacks in the IID setting, where client data is assumed to be independent and identically distributed. In this paper, we address label skewness, a more realistic and challenging non-IID setting, where each client only has access to a few classes of data. In this setting, state-of-the-art AGRs suffer from selection bias, leading to significant performance drop for particular classes; they are also more vulnerable to Byzantine attacks due to the increased variation among gradients of honest clients. To address these limitations, we propose an efficient two-stage method named BOBA. Theoretically, we prove the convergence of BOBA with an error of the optimal order. Our empirical evaluations demonstrate BOBA's superior unbiasedness and robustness across diverse models and datasets when compared to various baselines. Our code is available at https://gith
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#35760;&#24518;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38544;&#31169;&#27844;&#38706;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20808;&#21069;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#20135;&#29983;&#35823;&#23548;&#24615;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#26469;&#35780;&#20272;&#20010;&#20307;&#26679;&#26412;&#30340;&#35760;&#24518;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2208.08270</link><description>&lt;p&gt;
&#20851;&#20110;&#25968;&#25454;&#22686;&#24378;&#23545;&#38544;&#31169;&#24433;&#21709;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Privacy Effect of Data Enhancement via the Lens of Memorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.08270
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#35760;&#24518;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38544;&#31169;&#27844;&#38706;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20808;&#21069;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#20135;&#29983;&#35823;&#23548;&#24615;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#26469;&#35780;&#20272;&#20010;&#20307;&#26679;&#26412;&#30340;&#35760;&#24518;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24102;&#26469;&#20102;&#20005;&#37325;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#22240;&#20026;&#24050;&#32463;&#26174;&#31034;&#23398;&#20064;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#25581;&#31034;&#26377;&#20851;&#20854;&#35757;&#32451;&#25968;&#25454;&#30340;&#25935;&#24863;&#20449;&#24687;&#12290; &#35768;&#22810;&#30740;&#31350;&#25506;&#35752;&#20102;&#24191;&#27867;&#37319;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#23545;&#25239;&#35757;&#32451;&#25216;&#26415;&#65288;&#22312;&#35770;&#25991;&#20013;&#31216;&#20026;&#25968;&#25454;&#22686;&#24378;&#65289;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38544;&#31169;&#27844;&#38706;&#30340;&#24433;&#21709;&#12290; &#36825;&#31181;&#38544;&#31169;&#25928;&#24212;&#36890;&#24120;&#36890;&#36807;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#26469;&#34913;&#37327;&#65292;&#20854;&#30446;&#30340;&#26159;&#30830;&#23450;&#29305;&#23450;&#31034;&#20363;&#26159;&#21542;&#23646;&#20110;&#35757;&#32451;&#38598;&#12290; &#25105;&#20204;&#25552;&#20986;&#20174;&#31216;&#20026;&#35760;&#24518;&#30340;&#26032;&#35270;&#35282;&#26469;&#30740;&#31350;&#38544;&#31169;&#12290; &#36890;&#36807;&#35760;&#24518;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#21457;&#29616;&#20808;&#21069;&#37096;&#32626;&#30340;MIAs&#20135;&#29983;&#35823;&#23548;&#24615;&#32467;&#26524;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#22826;&#21487;&#33021;&#35782;&#21035;&#39640;&#38544;&#31169;&#39118;&#38505;&#26679;&#26412;&#26159;&#21542;&#20316;&#20026;&#25104;&#21592;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#35782;&#21035;&#20302;&#38544;&#31169;&#39118;&#38505;&#26679;&#26412;&#26356;&#23481;&#26131;&#12290; &#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#37096;&#32626;&#20102;&#19968;&#31181;&#26368;&#36817;&#30340;&#25915;&#20987;&#65292;&#21487;&#20197;&#25429;&#33719;&#20010;&#20307;&#26679;&#26412;&#30340;&#35760;&#24518;&#31243;&#24230;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.08270v3 Announce Type: replace  Abstract: Machine learning poses severe privacy concerns as it has been shown that the learned models can reveal sensitive information about their training data. Many works have investigated the effect of widely adopted data augmentation and adversarial training techniques, termed data enhancement in the paper, on the privacy leakage of machine learning models. Such privacy effects are often measured by membership inference attacks (MIAs), which aim to identify whether a particular example belongs to the training set or not. We propose to investigate privacy from a new perspective called memorization. Through the lens of memorization, we find that previously deployed MIAs produce misleading results as they are less likely to identify samples with higher privacy risks as members compared to samples with low privacy risks. To solve this problem, we deploy a recent attack that can capture individual samples' memorization degrees for evaluation. T
&lt;/p&gt;</description></item><item><title>&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#25351;&#20196;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#39640;&#20272;&#65292;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#21463;&#21040;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2205.00415</link><description>&lt;p&gt;
&#19981;&#35201;&#36131;&#24618;&#27880;&#37322;&#20154;&#21592;&#65306;&#20559;&#35265;&#24050;&#32463;&#24320;&#22987;&#20110;&#27880;&#37322;&#25351;&#20196;
&lt;/p&gt;
&lt;p&gt;
Don't Blame the Annotator: Bias Already Starts in the Annotation Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.00415
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#25351;&#20196;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#39640;&#20272;&#65292;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#21463;&#21040;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#30340;&#36827;&#23637;&#20027;&#35201;&#26159;&#30001;&#22522;&#20934;&#39537;&#21160;&#30340;&#12290;&#36825;&#20123;&#22522;&#20934;&#36890;&#24120;&#36890;&#36807;&#20247;&#21253;&#25910;&#38598;&#65292;&#27880;&#37322;&#20154;&#21592;&#26681;&#25454;&#25968;&#25454;&#38598;&#21019;&#24314;&#32773;&#21046;&#23450;&#30340;&#27880;&#37322;&#25351;&#20196;&#32534;&#20889;&#31034;&#20363;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#27880;&#37322;&#20154;&#21592;&#20250;&#27880;&#24847;&#21040;&#20247;&#21253;&#25351;&#20196;&#20013;&#30340;&#27169;&#24335;&#65292;&#20351;&#20182;&#20204;&#20889;&#20986;&#35768;&#22810;&#30456;&#20284;&#30340;&#31034;&#20363;&#65292;&#38543;&#21518;&#36825;&#20123;&#31034;&#20363;&#22312;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#34987;&#36807;&#24230;&#21576;&#29616;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#20559;&#35265;&#24418;&#24335;&#65292;&#31216;&#20043;&#20026;&#25351;&#20196;&#20559;&#35265;&#65292;&#22312;&#26368;&#36817;&#30340;14&#20010;NLU&#22522;&#20934;&#20013;&#23637;&#31034;&#20102;&#25351;&#20196;&#31034;&#20363;&#36890;&#24120;&#34920;&#29616;&#20986;&#20855;&#20307;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#24335;&#34987;&#24037;&#20154;&#32676;&#20307;&#20256;&#25773;&#21040;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#12290;&#36825;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340;&#24037;&#20316;&#65288;Geva&#31561;&#65292;2019&#24180;&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20851;&#27880;&#28857;&#65292;&#21363;&#25105;&#20204;&#26159;&#21542;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#21019;&#24314;&#32773;&#30340;&#25351;&#20196;&#65292;&#32780;&#19981;&#26159;&#20219;&#21153;&#26412;&#36523;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25351;&#20196;&#20559;&#35265;&#30830;&#23454;&#21487;&#33021;&#23548;&#33268;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#36807;&#39640;&#20272;&#35745;&#65292;&#24182;&#19988;&#27169;&#22411;&#38590;&#20197;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.00415v3 Announce Type: replace  Abstract: In recent years, progress in NLU has been driven by benchmarks. These benchmarks are typically collected by crowdsourcing, where annotators write examples based on annotation instructions crafted by dataset creators. In this work, we hypothesize that annotators pick up on patterns in the crowdsourcing instructions, which bias them to write many similar examples that are then over-represented in the collected data. We study this form of bias, termed instruction bias, in 14 recent NLU benchmarks, showing that instruction examples often exhibit concrete patterns, which are propagated by crowdworkers to the collected data. This extends previous work (Geva et al., 2019) and raises a new concern of whether we are modeling the dataset creator's instructions, rather than the task. Through a series of experiments, we show that, indeed, instruction bias can lead to overestimation of model performance, and that models struggle to generalize bey
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20026;&#27599;&#20010;&#33410;&#28857;-&#23545;&#35937;&#24341;&#20837;&#23616;&#37096;&#22352;&#26631;&#31995;&#65292;&#20197;&#35825;&#23548;&#30456;&#20114;&#20316;&#29992;&#21160;&#24577;&#31995;&#32479;&#30340;&#20960;&#20309;&#22270;&#20855;&#26377;&#26059;&#36716;-&#24179;&#31227;&#19981;&#21464;&#24615;&#12290;</title><link>https://arxiv.org/abs/2110.14961</link><description>&lt;p&gt;
&#29992;&#20110;&#30456;&#20114;&#20316;&#29992;&#21160;&#24577;&#31995;&#32479;&#30340;Roto-translated&#23616;&#37096;&#22352;&#26631;&#31995;
&lt;/p&gt;
&lt;p&gt;
Roto-translated Local Coordinate Frames For Interacting Dynamical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.14961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20026;&#27599;&#20010;&#33410;&#28857;-&#23545;&#35937;&#24341;&#20837;&#23616;&#37096;&#22352;&#26631;&#31995;&#65292;&#20197;&#35825;&#23548;&#30456;&#20114;&#20316;&#29992;&#21160;&#24577;&#31995;&#32479;&#30340;&#20960;&#20309;&#22270;&#20855;&#26377;&#26059;&#36716;-&#24179;&#31227;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#30456;&#20114;&#20316;&#29992;&#22312;&#23398;&#20064;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#21363;&#30456;&#20114;&#20316;&#29992;&#23545;&#35937;&#20855;&#26377;&#39640;&#24230;&#38750;&#32447;&#24615;&#21644;&#26102;&#21464;&#34892;&#20026;&#30340;&#31995;&#32479;&#12290;&#22312;$\textit{&#20960;&#20309;&#22270;}$&#65292;$\textit{&#21363;}$&#65292;&#33410;&#28857;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#25918;&#32622;&#30340;&#22270;&#24418;&#20013;&#65292;&#21363;&#20351;&#26159;&#22312;$\textit{&#20219;&#24847;}$&#36873;&#25321;&#30340;&#20840;&#23616;&#22352;&#26631;&#31995;&#20013;&#65292;&#21487;&#20197;&#24418;&#24335;&#21270;&#22320;&#34920;&#31034;&#22823;&#31867;&#36825;&#26679;&#30340;&#31995;&#32479;&#65292;&#20363;&#22914;&#20132;&#36890;&#22330;&#26223;&#20013;&#30340;&#36710;&#36742;&#12290;&#23613;&#31649;&#20840;&#23616;&#22352;&#26631;&#31995;&#26159;&#20219;&#24847;&#36873;&#25321;&#30340;&#65292;&#20294;&#21508;&#33258;&#21160;&#24577;&#31995;&#32479;&#30340;&#25511;&#21046;&#21160;&#21147;&#23398;&#19981;&#21464;&#20110;&#26059;&#36716;&#21644;&#24179;&#31227;&#65292;&#20063;&#34987;&#31216;&#20026;$\textit{&#20285;&#21033;&#30053;&#19981;&#21464;&#24615;}$&#12290;&#24573;&#30053;&#36825;&#20123;&#19981;&#21464;&#24615;&#20250;&#23548;&#33268;&#26356;&#24046;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22240;&#27492;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#27599;&#20010;&#33410;&#28857;&#23545;&#35937;&#30340;&#23616;&#37096;&#22352;&#26631;&#31995;&#65292;&#20197;&#35825;&#23548;&#30456;&#20114;&#20316;&#29992;&#21160;&#24577;&#31995;&#32479;&#30340;&#20960;&#20309;&#22270;&#20855;&#26377;&#26059;&#36716;-&#24179;&#31227;&#19981;&#21464;&#24615;&#12290;&#27492;&#22806;&#65292;&#23616;&#37096;&#22352;&#26631;&#31995;&#20801;&#35768;&#33258;&#28982;&#23450;&#20041;&#21508;&#21521;&#24322;&#24615;&#28388;&#27874;&#22120;
&lt;/p&gt;
&lt;p&gt;
arXiv:2110.14961v3 Announce Type: replace  Abstract: Modelling interactions is critical in learning complex dynamical systems, namely systems of interacting objects with highly non-linear and time-dependent behaviour. A large class of such systems can be formalized as $\textit{geometric graphs}$, $\textit{i.e.}$, graphs with nodes positioned in the Euclidean space given an $\textit{arbitrarily}$ chosen global coordinate system, for instance vehicles in a traffic scene. Notwithstanding the arbitrary global coordinate system, the governing dynamics of the respective dynamical systems are invariant to rotations and translations, also known as $\textit{Galilean invariance}$. As ignoring these invariances leads to worse generalization, in this work we propose local coordinate frames per node-object to induce roto-translation invariance to the geometric graph of the interacting dynamical system. Further, the local coordinate frames allow for a natural definition of anisotropic filtering in g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35299;&#20915;&#19968;&#33268;&#30340;&#20551;&#35774;&#25214;&#21040;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#24191;&#27867;&#31867;&#21035;&#30340;&#24369;&#20984;&#20551;&#35774;&#12290;</title><link>https://arxiv.org/abs/2105.06251</link><description>&lt;p&gt;
&#22312;&#24230;&#37327;&#31354;&#38388;&#20013;&#23398;&#20064;&#24369;&#20984;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Learning Weakly Convex Sets in Metric Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2105.06251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35299;&#20915;&#19968;&#33268;&#30340;&#20551;&#35774;&#25214;&#21040;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#24191;&#27867;&#31867;&#21035;&#30340;&#24369;&#20984;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#29702;&#35770;&#20013;&#30740;&#31350;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#23545;&#20110;&#32473;&#23450;&#31867;&#21035;&#30340;&#20551;&#35774;&#65292;&#26159;&#21542;&#21487;&#33021;&#26377;&#25928;&#22320;&#25214;&#21040;&#19968;&#20010;{&#19968;&#33268;&#30340;}&#20551;&#35774;&#65292;&#21363;&#20855;&#26377;&#38646;&#35757;&#32451;&#35823;&#24046;&#30340;&#20551;&#35774;&#12290;&#23613;&#31649;&#28041;&#21450;{\em &#20984;}&#20551;&#35774;&#30340;&#38382;&#39064;&#24050;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#30001;&#21487;&#33021;&#26377;&#20960;&#20010;&#19981;&#36830;&#32493;&#21306;&#22495;&#32452;&#25104;&#30340;&#38750;&#20984;&#20551;&#35774;&#26159;&#21542;&#21487;&#20197;&#36827;&#34892;&#26377;&#25928;&#23398;&#20064;&#30340;&#38382;&#39064;&#20173;&#19981;&#22826;&#28165;&#26970;&#12290;&#34429;&#28982;&#24456;&#20037;&#20197;&#21069;&#23601;&#24050;&#32463;&#34920;&#26126;&#23545;&#20110;&#24067;&#23572;&#20989;&#25968;&#30340;&#29305;&#27530;&#24773;&#20917;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#24369;&#20984;&#20551;&#35774;&#65288;&#20984;&#20551;&#35774;&#30340;&#21442;&#25968;&#21270;&#35843;&#25972;&#65289;&#65292;&#20294;&#33267;&#20170;&#23578;&#26410;&#30740;&#31350;&#36825;&#20010;&#24819;&#27861;&#26159;&#21542;&#21487;&#20197;&#21457;&#23637;&#20026;&#36890;&#29992;&#33539;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#31215;&#26497;&#31572;&#22797;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#24191;&#27867;&#31867;&#21035;&#30340;&#24369;&#20984;&#20551;&#35774;&#30340;&#19968;&#33268;&#20551;&#35774;&#25214;&#21040;&#38382;&#39064;&#30830;&#23454;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2105.06251v2 Announce Type: replace  Abstract: One of the central problems studied in the theory of machine learning is the question of whether, for a given class of hypotheses, it is possible to efficiently find a {consistent} hypothesis, i.e., which has zero training error. While problems involving {\em convex} hypotheses have been extensively studied, the question of whether efficient learning is possible for non-convex hypotheses composed of possibly several disconnected regions is still less understood. Although it has been shown quite a while ago that efficient learning of weakly convex hypotheses, a parameterized relaxation of convex hypotheses, is possible for the special case of Boolean functions, the question of whether this idea can be developed into a generic paradigm has not been studied yet. In this paper, we provide a positive answer and show that the consistent hypothesis finding problem can indeed be solved in polynomial time for a broad class of weakly convex hy
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#35770;&#25991;&#24635;&#32467;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#21019;&#36896;&#21147;&#39046;&#22495;&#30340;&#21382;&#21490;&#12289;&#29616;&#29366;&#65292;&#20197;&#21450;&#20851;&#38190;&#30340;&#36129;&#29486;&#21644;&#30740;&#31350;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2104.02726</link><description>&lt;p&gt;
&#21019;&#24847;&#19982;&#26426;&#22120;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Creativity and Machine Learning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2104.02726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#35770;&#25991;&#24635;&#32467;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#21019;&#36896;&#21147;&#39046;&#22495;&#30340;&#21382;&#21490;&#12289;&#29616;&#29366;&#65292;&#20197;&#21450;&#20851;&#38190;&#30340;&#36129;&#29486;&#21644;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#21019;&#24847;&#39046;&#22495;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#24863;&#20852;&#36259;&#12290;&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#35745;&#31639;&#21019;&#36896;&#21147;&#29702;&#35770;&#30340;&#21382;&#21490;&#21644;&#29616;&#29366;&#12289;&#20851;&#38190;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65288;&#21253;&#25324;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#65289;&#20197;&#21450;&#30456;&#24212;&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#12290;&#22312;&#23545;&#35813;&#39046;&#22495;&#30340;&#20851;&#38190;&#36129;&#29486;&#36827;&#34892;&#25209;&#21028;&#24615;&#35752;&#35770;&#20043;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#24403;&#21069;&#30740;&#31350;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#36825;&#19968;&#39046;&#22495;&#30340;&#26032;&#20852;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2104.02726v4 Announce Type: replace  Abstract: There is a growing interest in the area of machine learning and creativity. This survey presents an overview of the history and the state of the art of computational creativity theories, key machine learning techniques (including generative deep learning), and corresponding automatic evaluation methods. After presenting a critical discussion of the key contributions in this area, we outline the current research challenges and emerging opportunities in this field.
&lt;/p&gt;</description></item><item><title>&#21457;&#29616;&#36138;&#23146;&#31639;&#27861;&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#19981;&#21512;&#29702;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#24418;&#24335;&#30340;&#33258;&#30001;&#25506;&#32034;&#65292;&#23545;&#36138;&#23146;&#31639;&#27861;&#26377;&#30410;&#12290;</title><link>https://arxiv.org/abs/2002.10121</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#35768;&#22810;&#33218;&#30340;&#22810;&#33218;&#32769;&#34382;&#26426;&#20013;&#65292;&#36138;&#23146;&#31639;&#27861;&#30340;&#19981;&#21512;&#29702;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Unreasonable Effectiveness of Greedy Algorithms in Multi-Armed Bandit with Many Arms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2002.10121
&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#36138;&#23146;&#31639;&#27861;&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#19981;&#21512;&#29702;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#24418;&#24335;&#30340;&#33258;&#30001;&#25506;&#32034;&#65292;&#23545;&#36138;&#23146;&#31639;&#27861;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;$k$-&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65292;&#22312;\emph{&#20247;&#22810;&#33218;}&#24773;&#26223;&#19979;&#65292;&#20854;&#20013;$k \geq \sqrt{T}$&#65292;$T$&#20195;&#34920;&#26102;&#38388;&#36328;&#24230;&#12290;&#26368;&#21021;&#65292;&#19982;&#26368;&#36817;&#26377;&#20851;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#30340;&#25991;&#29486;&#19968;&#33268;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23376;&#37319;&#26679;&#22312;&#35774;&#35745;&#26368;&#20248;&#31639;&#27861;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65307;&#20256;&#32479;&#30340;&#19978;&#32622;&#20449;&#30028;&#65288;UCB&#65289;&#31639;&#27861;&#26159;&#27425;&#20248;&#30340;&#65292;&#32780;&#19968;&#20010;&#23376;&#37319;&#26679;&#30340;UCB&#65288;SS-UCB&#65289;&#65292;&#22312;UCB&#26694;&#26550;&#19979;&#36873;&#25321;$\Theta(\sqrt{T})$&#20010;&#33218;&#36827;&#34892;&#25191;&#34892;&#65292;&#36798;&#21040;&#20102;&#36895;&#29575;&#26368;&#20248;&#24615;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;SS-UCB&#22312;&#29702;&#35770;&#19978;&#25215;&#35834;&#20102;&#26368;&#20248;&#36951;&#25022;&#65292;&#20294;&#22312;&#23454;&#39564;&#20013;&#19982;&#19968;&#31181;&#22987;&#32456;&#36873;&#25321;&#32463;&#39564;&#19978;&#26368;&#20339;&#33218;&#30340;&#36138;&#23146;&#31639;&#27861;&#30456;&#27604;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#19968;&#21457;&#29616;&#36890;&#36807;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#27169;&#25311;&#24310;&#20280;&#21040;&#20102;&#24773;&#22659;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25552;&#31034;&#19968;&#31181;&#23545;&#20110;&#22810;&#33218;&#24773;&#20917;&#19979;&#36138;&#23146;&#31639;&#27861;&#26377;&#30410;&#30340;&#26032;&#24418;&#24335;&#30340;\emph{&#33258;&#30001;&#25506;&#32034;}&#65292;&#20174;&#26681;&#26412;&#19978;&#19982;&#33218;&#30340;&#20808;&#39564;&#20998;&#24067;&#30456;&#20851;&#30340;&#19968;&#20010;&#23614;&#20107;&#20214;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2002.10121v4 Announce Type: replace  Abstract: We investigate a Bayesian $k$-armed bandit problem in the \emph{many-armed} regime, where $k \geq \sqrt{T}$ and $T$ represents the time horizon. Initially, and aligned with recent literature on many-armed bandit problems, we observe that subsampling plays a key role in designing optimal algorithms; the conventional UCB algorithm is sub-optimal, whereas a subsampled UCB (SS-UCB), which selects $\Theta(\sqrt{T})$ arms for execution under the UCB framework, achieves rate-optimality. However, despite SS-UCB's theoretical promise of optimal regret, it empirically underperforms compared to a greedy algorithm that consistently chooses the empirically best arm. This observation extends to contextual settings through simulations with real-world data. Our findings suggest a new form of \emph{free exploration} beneficial to greedy algorithms in the many-armed context, fundamentally linked to a tail event concerning the prior distribution of arm
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33410;&#33021;&#30340;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#21608;&#26399;&#24615;&#25903;&#25745;&#32467;&#26500;&#30340;&#21018;&#24230;&#24352;&#37327;&#12290;&#36890;&#36807;&#32534;&#30721;&#30340;&#31561;&#21464;&#24615;&#21644;&#33021;&#37327;&#23432;&#24658;&#23450;&#24459;&#30340;&#24212;&#29992;&#65292;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#24615;&#33021;&#21644;&#35757;&#32451;&#38656;&#27714;&#26041;&#38754;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.16914</link><description>&lt;p&gt;
&#33410;&#33021;&#30340;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#26684;&#23376;&#32467;&#26500;&#30340;&#20122;&#27874;&#38271;&#22411;&#26448;&#26009;&#30340;&#24377;&#24615;&#24615;&#36136;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Energy-conserving equivariant GNN for elasticity of lattice architected metamaterials. (arXiv:2401.16914v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33410;&#33021;&#30340;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#21608;&#26399;&#24615;&#25903;&#25745;&#32467;&#26500;&#30340;&#21018;&#24230;&#24352;&#37327;&#12290;&#36890;&#36807;&#32534;&#30721;&#30340;&#31561;&#21464;&#24615;&#21644;&#33021;&#37327;&#23432;&#24658;&#23450;&#24459;&#30340;&#24212;&#29992;&#65292;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#24615;&#33021;&#21644;&#35757;&#32451;&#38656;&#27714;&#26041;&#38754;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26684;&#23376;&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#20122;&#27874;&#38271;&#22411;&#26448;&#26009;&#65292;&#20854;&#24615;&#36136;&#24378;&#28872;&#20381;&#36182;&#20110;&#20854;&#20960;&#20309;&#35774;&#35745;&#12290;&#26684;&#23376;&#21644;&#22270;&#20043;&#38388;&#30340;&#31867;&#27604;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#19968;&#20010;&#27604;&#20256;&#32479;&#26041;&#27861;&#65288;&#22914;&#26377;&#38480;&#20803;&#24314;&#27169;&#65289;&#26356;&#24555;&#36895;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#38454;GNN&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#21608;&#26399;&#24615;&#25903;&#25745;&#32467;&#26500;&#30340;&#22235;&#38454;&#21018;&#24230;&#24352;&#37327;&#12290;&#35813;&#27169;&#22411;&#30340;&#20851;&#38190;&#29305;&#28857;&#26159;&#65288;i&#65289;SE&#65288;3&#65289;&#31561;&#21464;&#24615;&#21644;&#65288;ii&#65289;&#33021;&#37327;&#23432;&#24658;&#23450;&#24459;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20123;&#35823;&#24046;&#24230;&#37327;&#25351;&#26631;&#23558;&#35813;&#27169;&#22411;&#19982;&#38750;&#31561;&#21464;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#32534;&#30721;&#30340;&#31561;&#21464;&#24615;&#21644;&#33021;&#37327;&#23432;&#24658;&#22312;&#39044;&#27979;&#24615;&#33021;&#21644;&#20943;&#23569;&#35757;&#32451;&#38656;&#27714;&#26041;&#38754;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lattices are architected metamaterials whose properties strongly depend on their geometrical design. The analogy between lattices and graphs enables the use of graph neural networks (GNNs) as a faster surrogate model compared to traditional methods such as finite element modelling. In this work we present a higher-order GNN model trained to predict the fourth-order stiffness tensor of periodic strut-based lattices. The key features of the model are (i) SE(3) equivariance, and (ii) consistency with the thermodynamic law of conservation of energy. We compare the model to non-equivariant models based on a number of error metrics and demonstrate the benefits of the encoded equivariance and energy conservation in terms of predictive performance and reduced training requirements.
&lt;/p&gt;</description></item><item><title>DDMI&#26159;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#26080;&#20851;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#33258;&#36866;&#24212;&#20301;&#32622;&#23884;&#20837;&#32780;&#19981;&#26159;&#32593;&#32476;&#26435;&#37325;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#29983;&#25104;&#36136;&#37327;&#36739;&#20302;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12517</link><description>&lt;p&gt;
DDMI: &#38754;&#21521;&#39046;&#22495;&#26080;&#20851;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DDMI: Domain-Agnostic Latent Diffusion Models for Synthesizing High-Quality Implicit Neural Representations. (arXiv:2401.12517v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12517
&lt;/p&gt;
&lt;p&gt;
DDMI&#26159;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#26080;&#20851;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#33258;&#36866;&#24212;&#20301;&#32622;&#23884;&#20837;&#32780;&#19981;&#26159;&#32593;&#32476;&#26435;&#37325;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#29983;&#25104;&#36136;&#37327;&#36739;&#20302;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31867;&#29992;&#20110;&#21512;&#25104;&#21508;&#20010;&#39046;&#22495;&#20013;&#20219;&#24847;&#36830;&#32493;&#20449;&#21495;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#29983;&#25104;&#27169;&#22411;&#65292;&#20026;&#39046;&#22495;&#26080;&#20851;&#30340;&#29983;&#25104;&#27169;&#22411;&#25171;&#24320;&#20102;&#22823;&#38376;&#65292;&#20294;&#24448;&#24448;&#26080;&#27861;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#26469;&#21442;&#25968;&#21270;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#22266;&#23450;&#30340;&#20301;&#32622;&#23884;&#20837;&#26469;&#35780;&#20272;&#32593;&#32476;&#12290;&#21487;&#20197;&#35828;&#65292;&#36825;&#31181;&#26550;&#26500;&#38480;&#21046;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23548;&#33268;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#29983;&#25104;&#30340;&#36136;&#37327;&#36739;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#26080;&#20851;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411; (DDMI)&#65292;&#20854;&#29983;&#25104;&#33258;&#36866;&#24212;&#20301;&#32622;&#23884;&#20837;&#32780;&#19981;&#26159;&#32593;&#32476;&#26435;&#37325;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31163;&#25955;&#21040;&#36830;&#32493;&#31354;&#38388;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120; (D2C-VAE)&#65292;&#23427;&#22312;&#20849;&#20139;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#26080;&#32541;&#36830;&#25509;&#31163;&#25955;&#25968;&#25454;&#21644;&#36830;&#32493;&#20449;&#21495;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Recent studies have introduced a new class of generative models for synthesizing implicit neural representations (INRs) that capture arbitrary continuous signals in various domains. These models opened the door for domain-agnostic generative models, but they often fail to achieve high-quality generation. We observed that the existing methods generate the weights of neural networks to parameterize INRs and evaluate the network with fixed positional embeddings (PEs). Arguably, this architecture limits the expressive power of generative models and results in low-quality INR generation. To address this limitation, we propose Domain-agnostic Latent Diffusion Model for INRs (DDMI) that generates adaptive positional embeddings instead of neural networks' weights. Specifically, we develop a Discrete-to-continuous space Variational AutoEncoder (D2C-VAE), which seamlessly connects discrete data and the continuous signal functions in the shared latent space. Additionally, we introduce a novel con
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19982;&#38750;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24490;&#29615;&#27979;&#37327;&#26469;&#23545;&#19981;&#21516;&#30340;&#21160;&#21147;&#23398;&#29366;&#24577;&#36827;&#34892;&#20998;&#31867;&#65292;&#21457;&#29616;&#34913;&#37327;&#24490;&#29615;&#28857;&#23494;&#24230;&#30340;&#29305;&#24449;&#26159;&#26368;&#30456;&#20851;&#30340;&#65292;&#19988;&#35757;&#32451;&#21518;&#30340;&#31639;&#27861;&#33021;&#22815;&#25104;&#21151;&#39044;&#27979;&#21160;&#21147;&#23398;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2401.10298</link><description>&lt;p&gt;
&#20174;&#24490;&#29615;&#27979;&#37327;&#20013;&#26816;&#27979;&#21160;&#21147;&#23398;&#29366;&#24577;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Machine learning approach to detect dynamical states from recurrence measures. (arXiv:2401.10298v1 [physics.data-an])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10298
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19982;&#38750;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24490;&#29615;&#27979;&#37327;&#26469;&#23545;&#19981;&#21516;&#30340;&#21160;&#21147;&#23398;&#29366;&#24577;&#36827;&#34892;&#20998;&#31867;&#65292;&#21457;&#29616;&#34913;&#37327;&#24490;&#29615;&#28857;&#23494;&#24230;&#30340;&#29305;&#24449;&#26159;&#26368;&#30456;&#20851;&#30340;&#65292;&#19988;&#35757;&#32451;&#21518;&#30340;&#31639;&#27861;&#33021;&#22815;&#25104;&#21151;&#39044;&#27979;&#21160;&#21147;&#23398;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19982;&#38750;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30456;&#32467;&#21512;&#65292;&#29305;&#21035;&#21033;&#29992;&#24490;&#29615;&#27979;&#37327;&#26469;&#23545;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#20986;&#29616;&#30340;&#21508;&#31181;&#21160;&#21147;&#23398;&#29366;&#24577;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#23454;&#26045;&#20102;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36923;&#36753;&#22238;&#24402;&#65292;&#38543;&#26426;&#26862;&#26519;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#12290;&#36755;&#20837;&#29305;&#24449;&#26159;&#20174;&#38750;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#24490;&#29615;&#37327;&#21270;&#21644;&#23545;&#24212;&#30340;&#24490;&#29615;&#32593;&#32476;&#30340;&#29305;&#24449;&#37327;&#24471;&#20986;&#30340;&#12290;&#23545;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#25105;&#20204;&#20174;&#26631;&#20934;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20026;&#21608;&#26399;&#24615;&#65292;&#28151;&#27788;&#65292;&#36229;&#28151;&#27788;&#25110;&#22122;&#22768;&#31867;&#21035;&#26041;&#38754;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20998;&#31867;&#26041;&#26696;&#20013;&#36755;&#20837;&#29305;&#24449;&#30340;&#26174;&#33879;&#24615;&#65292;&#24182;&#21457;&#29616;&#34913;&#37327;&#24490;&#29615;&#28857;&#23494;&#24230;&#30340;&#29305;&#24449;&#26368;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32463;&#36807;&#35757;&#32451;&#30340;&#31639;&#27861;&#22914;&#20309;&#25104;&#21151;&#39044;&#27979;d
&lt;/p&gt;
&lt;p&gt;
We integrate machine learning approaches with nonlinear time series analysis, specifically utilizing recurrence measures to classify various dynamical states emerging from time series. We implement three machine learning algorithms Logistic Regression, Random Forest, and Support Vector Machine for this study. The input features are derived from the recurrence quantification of nonlinear time series and characteristic measures of the corresponding recurrence networks. For training and testing we generate synthetic data from standard nonlinear dynamical systems and evaluate the efficiency and performance of the machine learning algorithms in classifying time series into periodic, chaotic, hyper-chaotic, or noisy categories. Additionally, we explore the significance of input features in the classification scheme and find that the features quantifying the density of recurrence points are the most relevant. Furthermore, we illustrate how the trained algorithms can successfully predict the d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#27491;&#21017;&#21270;&#31639;&#27861;IRKSN&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;$k$&#25903;&#25745;&#33539;&#25968;&#27491;&#21017;&#21270;&#23454;&#29616;&#31232;&#30095;&#24674;&#22797;&#65292;&#24182;&#25552;&#20379;&#20102;&#26465;&#20214;&#12290;&#36825;&#26159;&#23545;&#22522;&#20110;$\ell_1$&#33539;&#25968;&#30340;&#36845;&#20195;&#26041;&#27861;&#30340;&#19968;&#31181;&#37325;&#35201;&#34917;&#20805;&#12290;</title><link>http://arxiv.org/abs/2401.05394</link><description>&lt;p&gt;
&#36845;&#20195;&#27491;&#21017;&#21270;&#19982;k&#25903;&#25745;&#33539;&#25968;&#65306;&#31232;&#30095;&#24674;&#22797;&#30340;&#37325;&#35201;&#34917;&#20805;
&lt;/p&gt;
&lt;p&gt;
Iterative Regularization with k-Support Norm: an Important Complement to Sparse Recovery. (arXiv:2401.05394v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05394
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#27491;&#21017;&#21270;&#31639;&#27861;IRKSN&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;$k$&#25903;&#25745;&#33539;&#25968;&#27491;&#21017;&#21270;&#23454;&#29616;&#31232;&#30095;&#24674;&#22797;&#65292;&#24182;&#25552;&#20379;&#20102;&#26465;&#20214;&#12290;&#36825;&#26159;&#23545;&#22522;&#20110;$\ell_1$&#33539;&#25968;&#30340;&#36845;&#20195;&#26041;&#27861;&#30340;&#19968;&#31181;&#37325;&#35201;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#24674;&#22797;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#22788;&#29702;&#20013;&#26080;&#22788;&#19981;&#22312;&#12290;&#30001;&#20110;&#31232;&#30095;&#24674;&#22797;&#30340;NP&#22256;&#38590;&#24615;&#36136;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#35201;&#20040;&#21463;&#38480;&#20110;&#36866;&#29992;&#26465;&#20214;&#65288;&#29978;&#33267;&#26410;&#30693;&#65289;&#65292;&#35201;&#20040;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#26368;&#36817;&#65292;&#36845;&#20195;&#27491;&#21017;&#21270;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#24555;&#36895;&#26041;&#27861;&#20986;&#29616;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#19968;&#27425;&#36890;&#36807;&#26469;&#23454;&#29616;&#31232;&#30095;&#24674;&#22797;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#26041;&#27861;&#20013;&#32321;&#29712;&#30340;&#32593;&#26684;&#25628;&#32034;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#36845;&#20195;&#26041;&#27861;&#37117;&#22522;&#20110;$\ell_1$&#33539;&#25968;&#65292;&#38656;&#35201;&#21463;&#38480;&#30340;&#36866;&#29992;&#26465;&#20214;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#22240;&#27492;&#65292;&#36845;&#20195;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#26356;&#24191;&#27867;&#30340;&#26465;&#20214;&#19979;&#23454;&#29616;&#31232;&#30095;&#24674;&#22797;&#20173;&#38656;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#27491;&#21017;&#21270;&#31639;&#27861;IRKSN&#65292;&#23427;&#22522;&#20110;$k$&#25903;&#25745;&#33539;&#25968;&#27491;&#21017;&#21270;&#32780;&#19981;&#26159;$\ell_1$&#33539;&#25968;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;IRKSN&#36827;&#34892;&#31232;&#30095;&#24674;&#22797;&#30340;&#26465;&#20214;&#65292;&#24182;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse recovery is ubiquitous in machine learning and signal processing. Due to the NP-hard nature of sparse recovery, existing methods are known to suffer either from restrictive (or even unknown) applicability conditions, or high computational cost. Recently, iterative regularization methods have emerged as a promising fast approach because they can achieve sparse recovery in one pass through early stopping, rather than the tedious grid-search used in the traditional methods. However, most of those iterative methods are based on the $\ell_1$ norm which requires restrictive applicability conditions and could fail in many cases. Therefore, achieving sparse recovery with iterative regularization methods under a wider range of conditions has yet to be further explored. To address this issue, we propose a novel iterative regularization algorithm, IRKSN, based on the $k$-support norm regularizer rather than the $\ell_1$ norm. We provide conditions for sparse recovery with IRKSN, and compar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20197;&#26500;&#24314;&#19968;&#20010;&#26032;&#30340;&#26368;&#22823;&#20551;&#35774;&#25968;&#25454;&#38598;&#20026;&#22522;&#30784;&#65292;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24320;&#21457;&#20013;&#25163;&#21160;&#35782;&#21035;&#20551;&#35774;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#30740;&#31350;&#12290;&#22312;&#35813;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25163;&#21160;&#35782;&#21035;&#20551;&#35774;&#30340;&#25104;&#26412;&#39640;&#65292;&#22240;&#27492;&#25506;&#35752;&#20102;&#20351;&#29992;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35782;&#21035;&#20551;&#35774;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.03653</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24320;&#21457;&#20013;&#33258;&#21160;&#35782;&#21035;&#20551;&#35774;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An exploratory study on automatic identification of assumptions in the development of deep learning frameworks. (arXiv:2401.03653v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20197;&#26500;&#24314;&#19968;&#20010;&#26032;&#30340;&#26368;&#22823;&#20551;&#35774;&#25968;&#25454;&#38598;&#20026;&#22522;&#30784;&#65292;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24320;&#21457;&#20013;&#25163;&#21160;&#35782;&#21035;&#20551;&#35774;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#30740;&#31350;&#12290;&#22312;&#35813;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25163;&#21160;&#35782;&#21035;&#20551;&#35774;&#30340;&#25104;&#26412;&#39640;&#65292;&#22240;&#27492;&#25506;&#35752;&#20102;&#20351;&#29992;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35782;&#21035;&#20551;&#35774;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#30410;&#30456;&#20851;&#26041;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24320;&#21457;&#20013;&#32463;&#24120;&#20570;&#20986;&#20551;&#35774;&#12290;&#36825;&#20123;&#20551;&#35774;&#28041;&#21450;&#21508;&#31181;&#36719;&#20214;&#26500;&#20214;&#65288;&#20363;&#22914;&#38656;&#27714;&#12289;&#35774;&#35745;&#20915;&#31574;&#21644;&#25216;&#26415;&#20538;&#21153;&#65289;&#65292;&#21487;&#33021;&#20250;&#34987;&#35777;&#26126;&#26080;&#25928;&#65292;&#20174;&#32780;&#23548;&#33268;&#31995;&#32479;&#25925;&#38556;&#12290;&#29616;&#26377;&#30340;&#20551;&#35774;&#31649;&#29702;&#26041;&#27861;&#21644;&#24037;&#20855;&#36890;&#24120;&#20381;&#36182;&#20110;&#25163;&#21160;&#35782;&#21035;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#20551;&#35774;&#20998;&#25955;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24320;&#21457;&#30340;&#21508;&#31181;&#28304;&#22836;&#65288;&#20363;&#22914;&#20195;&#30721;&#27880;&#37322;&#12289;&#25552;&#20132;&#12289;&#25289;&#21462;&#35831;&#27714;&#21644;&#38382;&#39064;&#65289;&#20013;&#65292;&#25163;&#21160;&#35782;&#21035;&#20551;&#35774;&#25104;&#26412;&#36739;&#39640;&#65288;&#20363;&#22914;&#26102;&#38388;&#21644;&#36164;&#28304;&#28040;&#32791;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24320;&#21457;&#20013;&#25163;&#21160;&#35782;&#21035;&#20551;&#35774;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#24182;&#19988;&#26368;&#22823;&#30340;&#20551;&#35774;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;AssuEval&#65289;&#65292;&#35813;&#25968;&#25454;&#38598;&#25910;&#38598;&#33258;GitHub&#19978;&#30340;TensorFlow&#21644;Keras&#20195;&#30721;&#24211;&#65307;&#25105;&#20204;&#25506;&#35752;&#20102;&#19971;&#20010;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#20363;&#22914;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;&#20998;&#31867;&#22238;&#24402;&#26641;&#65289;&#21644;&#19968;&#20010;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stakeholders constantly make assumptions in the development of deep learning (DL) frameworks. These assumptions are related to various types of software artifacts (e.g., requirements, design decisions, and technical debt) and can turn out to be invalid, leading to system failures. Existing approaches and tools for assumption management usually depend on manual identification of assumptions. However, assumptions are scattered in various sources (e.g., code comments, commits, pull requests, and issues) of DL framework development, and manually identifying assumptions has high costs (e.g., time and resources). To overcome the issues of manually identifying assumptions in DL framework development, we constructed a new and largest dataset (i.e., AssuEval) of assumptions collected from the TensorFlow and Keras repositories on GitHub; explored the performance of seven traditional machine learning models (e.g., Support Vector Machine, Classification and Regression Trees), a popular DL model (i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20998;&#35789;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#29983;&#25104;&#20013;&#22269;&#21476;&#20856;&#35799;&#35789;&#65292;&#24182;&#35299;&#20915;&#20102;&#26684;&#24335;&#19981;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;&#39564;&#35777;&#20102;&#29616;&#26377;&#22522;&#20110;&#20998;&#35789;&#30340;&#27169;&#22411;&#22312;&#23383;&#31526;&#21644;&#20998;&#35789;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#30340;&#30693;&#35782;&#26377;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#23450;&#21046;&#27169;&#22411;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.03512</link><description>&lt;p&gt;
&#26080;&#38656;&#20998;&#35789;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20197;&#26356;&#20934;&#30830;&#30340;&#26684;&#24335;&#29983;&#25104;&#20013;&#22269;&#21476;&#20856;&#35799;&#35789;
&lt;/p&gt;
&lt;p&gt;
Token-free LLMs Can Generate Chinese Classical Poetry with More Accurate Format. (arXiv:2401.03512v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20998;&#35789;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#29983;&#25104;&#20013;&#22269;&#21476;&#20856;&#35799;&#35789;&#65292;&#24182;&#35299;&#20915;&#20102;&#26684;&#24335;&#19981;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;&#39564;&#35777;&#20102;&#29616;&#26377;&#22522;&#20110;&#20998;&#35789;&#30340;&#27169;&#22411;&#22312;&#23383;&#31526;&#21644;&#20998;&#35789;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#30340;&#30693;&#35782;&#26377;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#23450;&#21046;&#27169;&#22411;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#21644;Qwen-chat&#65289;&#33021;&#22815;&#26681;&#25454;&#20154;&#31867;&#30340;&#25351;&#20196;&#29983;&#25104;&#20013;&#22269;&#21476;&#20856;&#35799;&#35789;&#12290;&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#22312;&#20869;&#23481;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#36890;&#24120;&#22312;&#26684;&#24335;&#19978;&#23384;&#22312;&#38382;&#39064;&#65292;&#27599;&#34892;&#23383;&#31526;&#30340;&#25968;&#37327;&#26377;&#26102;&#36807;&#22810;&#25110;&#19981;&#36275;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#26368;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#22522;&#20110;&#20998;&#35789;&#30340;&#65292;&#25105;&#20204;&#35748;&#20026;&#26684;&#24335;&#19981;&#20934;&#30830;&#26159;&#30001;&#20110;"&#20998;&#35789;&#35268;&#21010;"&#20219;&#21153;&#30340;&#38590;&#24230;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#20934;&#30830;&#30693;&#36947;&#27599;&#20010;&#20998;&#35789;&#20013;&#21253;&#21547;&#22810;&#23569;&#20010;&#23383;&#31526;&#65292;&#24182;&#22522;&#20110;&#36825;&#20010;&#30693;&#35782;&#36827;&#34892;&#38271;&#24230;&#25511;&#21046;&#35268;&#21010;&#12290;&#26412;&#25991;&#39318;&#20808;&#36890;&#36807;&#23637;&#31034;&#29616;&#26377;&#30340;&#22522;&#20110;&#20998;&#35789;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#35789;&#21644;&#23383;&#31526;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#30693;&#35782;&#26377;&#38480;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#25340;&#20889;&#27604;&#36187;&#25506;&#27979;&#31243;&#24207;&#65292;&#24182;&#21457;&#29616;Qwen-chat&#22312;&#36817;15%&#30340;&#20013;&#25991;&#25340;&#20889;&#27979;&#35797;&#20013;&#22833;&#36133;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#35789;&#30340;&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#23450;&#21046;&#25104;&#26080;&#38656;&#20998;&#35789;&#30340;&#27169;&#22411;&#65288;&#23545;&#20110;&#20013;&#25991;&#26469;&#35828;&#65289;&#65292;&#20174;&#32780;&#33021;&#22815;&#24456;&#22823;&#31243;&#24230;&#19978;&#35299;&#20915;&#26684;&#24335;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuned large language models (such as ChatGPT and Qwen-chat) can generate Chinese classical poetry following human's instructions. LLMs perform well in content, but are usually lacking in format, with occasionally excess or insufficient number of characters in each line. Since most SOTA LLMs are token-based, we assume that the format inaccuracy is due to the difficulty of the "token planning" task, which means that the LLM need to know exactly how much characters are contained in each token and do length-control planning based on that knowledge. In this paper, we first confirm our assumption by showing that existing token-based large language models has limited knowledge on token-character relationship. We use a spelling bee probing procedure, and find that Qwen-chat failed in nearly 15% Chinese spelling test. We then show that a token-based model can be easily tailored into a token-free model (in terms of Chinese), which can largely solve the format accuracy problem. Our tailoring 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#20013;&#38544;&#31169;&#21644;&#33021;&#32791;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24378;&#35843;&#20102;&#24555;&#36895;&#25910;&#25947;&#30340;&#20248;&#21183;&#65292;&#24182;&#20998;&#26512;&#20102;&#20999;&#21106;&#23618;&#23545;&#23458;&#25143;&#31471;&#33021;&#32791;&#21644;&#38544;&#31169;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2311.09441</link><description>&lt;p&gt;
&#25506;&#32034;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;-&#33021;&#32791;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Exploring the Privacy-Energy Consumption Tradeoff for Split Federated Learning. (arXiv:2311.09441v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#20013;&#38544;&#31169;&#21644;&#33021;&#32791;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24378;&#35843;&#20102;&#24555;&#36895;&#25910;&#25947;&#30340;&#20248;&#21183;&#65292;&#24182;&#20998;&#26512;&#20102;&#20999;&#21106;&#23618;&#23545;&#23458;&#25143;&#31471;&#33021;&#32791;&#21644;&#38544;&#31169;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#25216;&#26415;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#32852;&#37030;&#23398;&#20064;&#21644;&#20998;&#21106;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;&#23427;&#24378;&#35843;&#20102;&#24555;&#36895;&#25910;&#25947;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38544;&#31169;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#36825;&#19968;&#21019;&#26032;&#21463;&#21040;&#20102;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;SFL&#20013;&#27169;&#22411;&#22312;&#29305;&#23450;&#23618;&#65288;&#31216;&#20026;&#20999;&#21106;&#23618;&#65289;&#19978;&#34987;&#20998;&#21106;&#20026;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#31471;&#27169;&#22411;&#65292;&#36873;&#25321;&#20999;&#21106;&#23618;&#21487;&#33021;&#23545;&#23458;&#25143;&#31471;&#30340;&#33021;&#32791;&#21644;&#38544;&#31169;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#65292;&#22240;&#20026;&#23427;&#24433;&#21709;&#20102;&#35757;&#32451;&#36127;&#25285;&#21644;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#27492;&#22806;&#65292;&#30830;&#23450;&#20999;&#21106;&#23618;&#30340;&#35774;&#35745;&#25361;&#25112;&#38750;&#24120;&#22797;&#26434;&#65292;&#20027;&#35201;&#30001;&#20110;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#21644;&#32593;&#32476;&#33021;&#21147;&#30340;&#22266;&#26377;&#24322;&#36136;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#27010;&#36848;&#20102;SFL&#30340;&#36807;&#31243;&#65292;&#24182;&#23545;&#33021;&#32791;&#21644;&#38544;&#31169;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Split Federated Learning (SFL) has recently emerged as a promising distributed learning technology, leveraging the strengths of both federated learning and split learning. It emphasizes the advantages of rapid convergence while addressing privacy concerns. As a result, this innovation has received significant attention from both industry and academia. However, since the model is split at a specific layer, known as a cut layer, into both client-side and server-side models for the SFL, the choice of the cut layer in SFL can have a substantial impact on the energy consumption of clients and their privacy, as it influences the training burden and the output of the client-side models. Moreover, the design challenge of determining the cut layer is highly intricate, primarily due to the inherent heterogeneity in the computing and networking capabilities of clients. In this article, we provide a comprehensive overview of the SFL process and conduct a thorough analysis of energy consumption and
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25552;&#21462;&#33041;&#21160;&#21147;&#23398;&#22810;&#23610;&#24230;&#22240;&#26524;&#39592;&#26550;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;&#30340;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22240;&#26524;&#21160;&#21147;&#22312;&#19981;&#21516;&#39057;&#29575;&#19979;&#21463;&#19981;&#21516;&#33041;&#21306;&#39537;&#21160;&#65292;&#36825;&#20026;&#29702;&#35299;&#33041;&#21151;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2311.00118</link><description>&lt;p&gt;
&#25552;&#21462;&#33041;&#21160;&#21147;&#23398;&#30340;&#22810;&#23610;&#24230;&#22240;&#26524;&#39592;&#26550;
&lt;/p&gt;
&lt;p&gt;
Extracting the Multiscale Causal Backbone of Brain Dynamics. (arXiv:2311.00118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00118
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25552;&#21462;&#33041;&#21160;&#21147;&#23398;&#22810;&#23610;&#24230;&#22240;&#26524;&#39592;&#26550;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;&#30340;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22240;&#26524;&#21160;&#21147;&#22312;&#19981;&#21516;&#39057;&#29575;&#19979;&#21463;&#19981;&#21516;&#33041;&#21306;&#39537;&#21160;&#65292;&#36825;&#20026;&#29702;&#35299;&#33041;&#21151;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37096;&#20998;&#20851;&#20110;&#33041;&#36830;&#25509;&#24615;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#33041;&#21306;&#20043;&#38388;&#30340;&#32479;&#35745;&#20851;&#32852;&#19978;&#65292;&#36825;&#19982;&#32479;&#27835;&#33041;&#21160;&#21147;&#23398;&#30340;&#22240;&#26524;&#26426;&#21046;&#19981;&#30452;&#25509;&#30456;&#20851;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#22240;&#26524;&#39592;&#26550;&#65288;MCB&#65289;&#65292;&#23427;&#26159;&#22312;&#22810;&#20010;&#26102;&#38388;&#23610;&#24230;&#19978;&#20849;&#20139;&#30340;&#19968;&#32452;&#20010;&#20307;&#30340;&#33041;&#21160;&#21147;&#23398;&#29305;&#24449;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#26469;&#25552;&#21462;&#23427;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#22810;&#23610;&#24230;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#20248;&#21270;&#20102;&#27169;&#22411;&#25311;&#21512;&#19982;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#20110;&#35268;&#33539;&#21151;&#33021;&#36830;&#25509;&#32593;&#32476;&#30340;&#22522;&#32447;&#12290;&#24403;&#24212;&#29992;&#20110;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#24038;&#21491;&#33041;&#21322;&#29699;&#37117;&#26377;&#31232;&#30095;&#30340;MCB&#12290;&#30001;&#20110;&#20854;&#22810;&#23610;&#24230;&#30340;&#29305;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#26126;&#22312;&#20302;&#39057;&#24102;&#19978;&#65292;&#22240;&#26524;&#21160;&#21147;&#26469;&#33258;&#19982;&#39640;&#32423;&#35748;&#30693;&#21151;&#33021;&#30456;&#20851;&#30340;&#33041;&#21306;&#65307;&#32780;&#22312;&#26356;&#39640;&#30340;&#39057;&#29575;&#19978;&#65292;&#30001;nod&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
The bulk of the research effort on brain connectivity revolves around statistical associations among brain regions, which do not directly relate to the causal mechanisms governing brain dynamics. Here we propose the multiscale causal backbone (MCB) of brain dynamics shared by a set of individuals across multiple temporal scales, and devise a principled methodology to extract it.  Our approach leverages recent advances in multiscale causal structure learning and optimizes the trade-off between the model fitting and its complexity. Empirical assessment on synthetic data shows the superiority of our methodology over a baseline based on canonical functional connectivity networks. When applied to resting-state fMRI data, we find sparse MCBs for both the left and right brain hemispheres. Thanks to its multiscale nature, our approach shows that at low-frequency bands, causal dynamics are driven by brain regions associated with high-level cognitive functions; at higher frequencies instead, nod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#31515;&#21345;&#23572;&#31215;&#21644;&#31070;&#32463;&#22330;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#30456;&#20114;&#20316;&#29992;&#21160;&#24577;&#31995;&#32479;&#20013;&#21457;&#29616;&#23616;&#37096;&#29289;&#20307;&#30456;&#20114;&#20316;&#29992;&#21644;&#20840;&#23616;&#22330;&#25928;&#24212;&#30340;&#28508;&#22312;&#21147;&#22330;&#12290;</title><link>http://arxiv.org/abs/2310.20679</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#22330;&#22312;&#30456;&#20114;&#20316;&#29992;&#21160;&#24577;&#31995;&#32479;&#20013;&#21457;&#29616;&#28508;&#22312;&#22330;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Latent Field Discovery In Interacting Dynamical Systems With Neural Fields. (arXiv:2310.20679v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31515;&#21345;&#23572;&#31215;&#21644;&#31070;&#32463;&#22330;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#30456;&#20114;&#20316;&#29992;&#21160;&#24577;&#31995;&#32479;&#20013;&#21457;&#29616;&#23616;&#37096;&#29289;&#20307;&#30456;&#20114;&#20316;&#29992;&#21644;&#20840;&#23616;&#22330;&#25928;&#24212;&#30340;&#28508;&#22312;&#21147;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20114;&#20316;&#29992;&#23545;&#35937;&#30340;&#31995;&#32479;&#22312;&#20854;&#21160;&#21147;&#23398;&#20013;&#36890;&#24120;&#20250;&#21463;&#21040;&#22330;&#25928;&#24212;&#30340;&#24433;&#21709;&#65292;&#28982;&#32780;&#20197;&#24448;&#30340;&#30740;&#31350;&#24120;&#24120;&#24573;&#30053;&#20102;&#36825;&#20123;&#25928;&#24212;&#65292;&#20551;&#35774;&#31995;&#32479;&#22312;&#30495;&#31354;&#20013;&#28436;&#21270;&#12290;&#26412;&#25991;&#30528;&#30524;&#20110;&#21457;&#29616;&#36825;&#20123;&#22330;&#25928;&#24212;&#65292;&#24182;&#20165;&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#21160;&#21147;&#23398;&#26469;&#36827;&#34892;&#25512;&#26029;&#65292;&#32780;&#26080;&#38656;&#30452;&#25509;&#35266;&#27979;&#23427;&#20204;&#12290;&#25105;&#20204;&#20551;&#35774;&#23384;&#22312;&#28508;&#22312;&#30340;&#21147;&#22330;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#31070;&#32463;&#22330;&#26469;&#23398;&#20064;&#23427;&#20204;&#12290;&#30001;&#20110;&#35266;&#23519;&#21040;&#30340;&#21160;&#21147;&#23398;&#26159;&#23616;&#37096;&#29289;&#20307;&#30456;&#20114;&#20316;&#29992;&#21644;&#25972;&#20307;&#22330;&#25928;&#24212;&#30340;&#32508;&#21512;&#32467;&#26524;&#65292;&#26368;&#36817;&#27969;&#34892;&#30340;&#31561;&#21464;&#32593;&#32476;&#26080;&#27861;&#24212;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#25429;&#25417;&#21040;&#20840;&#23616;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#23616;&#37096;&#29289;&#20307;&#30456;&#20114;&#20316;&#29992;&#65288;SE(n)&#31561;&#21464;&#30340;&#65292;&#20381;&#36182;&#20110;&#30456;&#23545;&#29366;&#24577;&#65289;&#19982;&#22806;&#37096;&#20840;&#23616;&#22330;&#25928;&#24212;&#65288;&#20381;&#36182;&#20110;&#32477;&#23545;&#29366;&#24577;&#65289;&#30456;&#20998;&#31163;&#12290;&#25105;&#20204;&#20351;&#29992;&#31561;&#21464;&#22270;&#32593;&#32476;&#23545;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#23558;&#20854;&#19982;&#31070;&#32463;&#22330;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#26500;&#24314;&#20102;&#19968;&#31181;&#34701;&#21512;&#20102;&#22330;&#25928;&#24212;&#30340;&#22270;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Systems of interacting objects often evolve under the influence of field effects that govern their dynamics, yet previous works have abstracted away from such effects, and assume that systems evolve in a vacuum. In this work, we focus on discovering these fields, and infer them from the observed dynamics alone, without directly observing them. We theorize the presence of latent force fields, and propose neural fields to learn them. Since the observed dynamics constitute the net effect of local object interactions and global field effects, recently popularized equivariant networks are inapplicable, as they fail to capture global information. To address this, we propose to disentangle local object interactions -- which are $\mathrm{SE}(n)$ equivariant and depend on relative states -- from external global field effects -- which depend on absolute states. We model interactions with equivariant graph networks, and combine them with neural fields in a novel graph network that integrates fiel
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;REMed&#30340;&#26816;&#32034;&#22686;&#24378;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#38480;&#35780;&#20272;&#20020;&#24202;&#20107;&#20214;&#24182;&#33258;&#21160;&#36873;&#25321;&#30456;&#20851;&#20107;&#20214;&#36827;&#34892;&#39044;&#27979;&#65292;&#28040;&#38500;&#20102;&#20154;&#24037;&#29305;&#24449;&#36873;&#25321;&#21644;&#35266;&#23519;&#31383;&#21475;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.20204</link><description>&lt;p&gt;
&#21033;&#29992;&#36817;&#26080;&#38480;&#21382;&#21490;&#30340;&#36890;&#29992;&#26816;&#32034;&#22686;&#24378;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
General-Purpose Retrieval-Enhanced Medical Prediction Model Using Near-Infinite History. (arXiv:2310.20204v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20204
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;REMed&#30340;&#26816;&#32034;&#22686;&#24378;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#38480;&#35780;&#20272;&#20020;&#24202;&#20107;&#20214;&#24182;&#33258;&#21160;&#36873;&#25321;&#30456;&#20851;&#20107;&#20214;&#36827;&#34892;&#39044;&#27979;&#65292;&#28040;&#38500;&#20102;&#20154;&#24037;&#29305;&#24449;&#36873;&#25321;&#21644;&#35266;&#23519;&#31383;&#21475;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#24320;&#21457;&#20020;&#24202;&#39044;&#27979;&#27169;&#22411;&#65288;&#20363;&#22914;&#27515;&#20129;&#39044;&#27979;&#65289;&#36890;&#24120;&#20381;&#36182;&#20110;&#19987;&#23478;&#24847;&#35265;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#21644;&#35843;&#25972;&#35266;&#27979;&#31383;&#21475;&#22823;&#23567;&#12290;&#36825;&#32473;&#19987;&#23478;&#24102;&#26469;&#36127;&#25285;&#24182;&#22312;&#24320;&#21457;&#36807;&#31243;&#20013;&#36896;&#25104;&#29942;&#39048;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;&#65288;REMed&#65289;&#65292;&#20197;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;REMed&#21487;&#20197;&#22522;&#26412;&#35780;&#20272;&#26080;&#38480;&#37327;&#30340;&#20020;&#24202;&#20107;&#20214;&#65292;&#36873;&#25321;&#30456;&#20851;&#30340;&#20107;&#20214;&#24182;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#28040;&#38500;&#20102;&#38656;&#35201;&#25163;&#21160;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#24182;&#23454;&#26102;&#35266;&#23519;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;27&#20010;&#20020;&#24202;&#20219;&#21153;&#21644;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;EHR&#25968;&#25454;&#38598;&#30340;&#29420;&#31435;&#38431;&#21015;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20123;&#29305;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;REMed&#20248;&#20110;&#20854;&#20182;&#29616;&#20195;&#26550;&#26500;&#65292;&#23427;&#20204;&#26088;&#22312;&#22788;&#29702;&#23613;&#21487;&#33021;&#22810;&#30340;&#20107;&#20214;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;REMed&#30340;&#20559;&#22909;&#19982;&#21307;&#23398;&#19987;&#23478;&#30340;&#20559;&#22909;&#23494;&#20999;&#30456;&#20284;&#12290;&#25105;&#20204;&#26399;&#26395;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#26174;&#33879;&#21152;&#36895;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing clinical prediction models (e.g., mortality prediction) based on electronic health records (EHRs) typically relies on expert opinion for feature selection and adjusting observation window size. This burdens experts and creates a bottleneck in the development process. We propose Retrieval-Enhanced Medical prediction model (REMed) to address such challenges. REMed can essentially evaluate an unlimited number of clinical events, select the relevant ones, and make predictions. This approach effectively eliminates the need for manual feature selection and enables an unrestricted observation window. We verified these properties through experiments on 27 clinical tasks and two independent cohorts from publicly available EHR datasets, where REMed outperformed other contemporary architectures that aim to handle as many events as possible. Notably, we found that the preferences of REMed align closely with those of medical experts. We expect our approach to significantly expedite the d
&lt;/p&gt;</description></item><item><title>MCRAGE&#26159;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26469;&#22686;&#24378;&#19981;&#24179;&#34913;&#30340;&#21307;&#30103;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#23569;&#25968;&#32676;&#20307;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.18430</link><description>&lt;p&gt;
MCRAGE: &#20844;&#24179;&#24615;&#30340;&#21512;&#25104;&#21307;&#30103;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
MCRAGE: Synthetic Healthcare Data for Fairness. (arXiv:2310.18430v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18430
&lt;/p&gt;
&lt;p&gt;
MCRAGE&#26159;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26469;&#22686;&#24378;&#19981;&#24179;&#34913;&#30340;&#21307;&#30103;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#23569;&#25968;&#32676;&#20307;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#39046;&#22495;&#65292;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#26159;&#24320;&#21457;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#31649;&#29702;&#21307;&#30103;&#36164;&#28304;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20851;&#38190;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#21307;&#30103;&#25968;&#25454;&#38598;&#22312;&#31181;&#26063;/&#27665;&#26063;&#12289;&#24615;&#21035;&#21644;&#24180;&#40836;&#31561;&#25935;&#24863;&#23646;&#24615;&#26041;&#38754;&#24448;&#24448;&#23384;&#22312;&#19981;&#24179;&#34913;&#12290;&#22312;&#31867;&#19981;&#24179;&#34913;&#30340;EHR&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#37096;&#32626;&#26102;&#65292;&#23545;&#20110;&#23569;&#25968;&#32676;&#20307;&#30340;&#20010;&#20307;&#32780;&#35328;&#65292;&#34920;&#29616;&#26174;&#33879;&#19981;&#22914;&#22810;&#25968;&#32676;&#20307;&#30340;&#26679;&#26412;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23569;&#25968;&#32676;&#20307;&#30340;&#19981;&#20844;&#24179;&#21307;&#30103;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Minority Class Rebalancing through Augmentation by Generative modeling (MCRAGE)&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#30001;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#26469;&#22686;&#24378;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#12290;MCRAGE&#36807;&#31243;&#21253;&#25324;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#20174;&#23569;&#25968;&#32676;&#20307;&#20013;&#20135;&#29983;&#39640;&#36136;&#37327;&#21512;&#25104;EHR&#26679;&#26412;&#30340;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;CDDPM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of healthcare, electronic health records (EHR) serve as crucial training data for developing machine learning models for diagnosis, treatment, and the management of healthcare resources. However, medical datasets are often imbalanced in terms of sensitive attributes such as race/ethnicity, gender, and age. Machine learning models trained on class-imbalanced EHR datasets perform significantly worse in deployment for individuals of the minority classes compared to samples from majority classes, which may lead to inequitable healthcare outcomes for minority groups. To address this challenge, we propose Minority Class Rebalancing through Augmentation by Generative modeling (MCRAGE), a novel approach to augment imbalanced datasets using samples generated by a deep generative model. The MCRAGE process involves training a Conditional Denoising Diffusion Probabilistic Model (CDDPM) capable of generating high-quality synthetic EHR samples from underrepresented classes. We use this 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#35782;&#21035;&#26230;&#22278;&#32570;&#38519;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#20026;&#19981;&#21516;&#31639;&#27861;&#21644;&#25216;&#26415;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20005;&#35880;&#30340;&#35780;&#20272;&#39564;&#35777;&#20102;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10705</link><description>&lt;p&gt;
&#29992;&#20110;&#35782;&#21035;&#21322;&#23548;&#20307;&#26230;&#22278;&#22320;&#22270;&#20013;&#32570;&#38519;&#27169;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65306;&#19968;&#39033;&#35843;&#26597;&#12289;&#23454;&#35777;&#21644;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Techniques for Identifying the Defective Patterns in Semiconductor Wafer Maps: A Survey, Empirical, and Experimental Evaluations. (arXiv:2310.10705v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#35782;&#21035;&#26230;&#22278;&#32570;&#38519;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#20026;&#19981;&#21516;&#31639;&#27861;&#21644;&#25216;&#26415;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20005;&#35880;&#30340;&#35780;&#20272;&#39564;&#35777;&#20102;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#35782;&#21035;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#26230;&#22278;&#32570;&#38519;&#30340;&#26041;&#27861;&#23398;&#12290;&#23613;&#31649;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;ML&#22312;&#26230;&#22278;&#32570;&#38519;&#35782;&#21035;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#32570;&#20047;&#20840;&#38754;&#30340;&#32508;&#36848;&#12290;&#26412;&#25991;&#35797;&#22270;&#24357;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#36890;&#36807;&#25972;&#21512;&#29616;&#26377;&#25991;&#29486;&#65292;&#28145;&#20837;&#20998;&#26512;&#21508;&#31181;ML&#31639;&#27861;&#22312;&#26230;&#22278;&#32570;&#38519;&#26816;&#27979;&#39046;&#22495;&#30340;&#20248;&#21183;&#12289;&#23616;&#38480;&#24615;&#21644;&#28508;&#22312;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#23398;&#20998;&#31867;&#20307;&#31995;&#65292;&#35814;&#32454;&#20998;&#31867;&#20102;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#32454;&#33268;&#30340;&#23376;&#25216;&#26415;&#21010;&#20998;&#12290;&#36825;&#20010;&#20998;&#31867;&#20307;&#31995;&#20174;&#24191;&#27867;&#30340;&#26041;&#27861;&#23398;&#31867;&#21035;&#24320;&#22987;&#65292;&#21040;&#20855;&#20307;&#30340;&#23376;&#25216;&#26415;&#32467;&#26463;&#12290;&#23427;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;&#19981;&#21516;&#31639;&#27861;&#20197;&#21450;&#23427;&#20204;&#30340;&#25216;&#26415;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#25105;&#20204;&#37319;&#29992;&#20005;&#35880;&#30340;&#23454;&#35777;&#21644;&#23454;&#39564;&#35780;&#20272;&#26469;&#39564;&#35777;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey paper offers a comprehensive review of methodologies utilizing machine learning (ML) techniques for identifying wafer defects in semiconductor manufacturing. Despite the growing body of research demonstrating the effectiveness of ML in wafer defect identification, there is a noticeable absence of comprehensive reviews on this subject. This survey attempts to fill this void by amalgamating available literature and providing an in-depth analysis of the advantages, limitations, and potential applications of various ML algorithms in the realm of wafer defect detection. An innovative taxonomy of methodologies that we present provides a detailed classification of algorithms into more refined categories and techniques. This taxonomy follows a four-tier structure, starting from broad methodology categories and ending with specific sub-techniques. It aids researchers in comprehending the complex relationships between different algorithms and their techniques. We employ a rigorous em
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#21644;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;60&#22810;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;TSC&#30340;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.10060</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#25968;&#25454;&#22686;&#24378;&#65306;&#19968;&#39033;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#21644;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation for Time-Series Classification: An Extensive Empirical Study and Comprehensive Survey. (arXiv:2310.10060v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#21644;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;60&#22810;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;TSC&#30340;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#24050;&#25104;&#20026;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#31574;&#30053;&#65292;&#20027;&#35201;&#22240;&#20026;&#23427;&#21487;&#20197;&#22686;&#21152;&#35757;&#32451;&#26679;&#26412;&#30340;&#25968;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#65292;&#20351;&#25968;&#25454;&#38598;&#22810;&#26679;&#21270;&#65292;&#24182;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;TSC&#20013;&#30340;DA&#30740;&#31350;&#23384;&#22312;&#30528;&#25991;&#29486;&#35780;&#23457;&#30340;&#29255;&#27573;&#21270;&#65292;&#26041;&#27861;&#23398;&#20998;&#31867;&#19981;&#28165;&#26224;&#65292;&#35780;&#20272;&#25351;&#26631;&#19981;&#36275;&#20197;&#21450;&#32570;&#20047;&#29992;&#25143;&#21451;&#22909;&#30340;&#24037;&#20855;&#31561;&#38382;&#39064;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#23545;TSC&#39046;&#22495;&#20013;&#30340;DA&#26041;&#27861;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#25345;&#32493;&#21313;&#24180;&#30340;&#24191;&#27867;&#25991;&#29486;&#22238;&#39038;&#65292;&#21457;&#29616;&#24403;&#20195;&#32508;&#36848;&#25991;&#31456;&#24456;&#23569;&#33021;&#22815;&#28085;&#30422;DA&#22312;TSC&#19978;&#30340;&#20840;&#37096;&#36827;&#23637;&#65292;&#22240;&#27492;&#25105;&#20204;&#20180;&#32454;&#20998;&#26512;&#20102;100&#22810;&#31687;&#23398;&#26415;&#25991;&#31456;&#65292;&#24635;&#32467;&#20986;&#20102;60&#22810;&#31181;&#29420;&#29305;&#30340;DA&#25216;&#26415;&#12290;&#36825;&#39033;&#20005;&#26684;&#30340;&#20998;&#26512;&#24418;&#25104;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#19987;&#38376;&#38024;&#23545;TSC&#20013;&#30340;DA&#32454;&#33410;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation (DA) has emerged as an indispensable strategy in Time Series Classification (TSC), primarily due to its capacity to amplify training samples, thereby bolstering model robustness, diversifying datasets, and curtailing overfitting. However, the current landscape of DA in TSC is plagued with fragmented literature reviews, nebulous methodological taxonomies, inadequate evaluative measures, and a dearth of accessible, user-oriented tools. In light of these challenges, this study embarks on an exhaustive dissection of DA methodologies within the TSC realm. Our initial approach involved an extensive literature review spanning a decade, revealing that contemporary surveys scarcely capture the breadth of advancements in DA for TSC, prompting us to meticulously analyze over 100 scholarly articles to distill more than 60 unique DA techniques. This rigorous analysis precipitated the formulation of a novel taxonomy, purpose-built for the intricacies of DA in TSC, categorizing tech
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26631;&#31614;&#27604;&#20363;&#20013;&#23398;&#20064;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#20266;&#26631;&#31614;&#21270;&#21644;&#23884;&#20837;&#32454;&#21270;&#20004;&#20010;&#27493;&#39588;&#36845;&#20195;&#22320;&#25552;&#39640;&#26377;&#30417;&#30563;&#23398;&#20064;&#22120;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08056</link><description>&lt;p&gt;
&#20174;&#26631;&#31614;&#27604;&#20363;&#20013;&#23398;&#20064;&#65306;&#36890;&#36807;&#20449;&#24565;&#20256;&#25773;&#23545;&#26377;&#30417;&#30563;&#23398;&#20064;&#22120;&#36827;&#34892;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Learning from Label Proportions: Bootstrapping Supervised Learners via Belief Propagation. (arXiv:2310.08056v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26631;&#31614;&#27604;&#20363;&#20013;&#23398;&#20064;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#20266;&#26631;&#31614;&#21270;&#21644;&#23884;&#20837;&#32454;&#21270;&#20004;&#20010;&#27493;&#39588;&#36845;&#20195;&#22320;&#25552;&#39640;&#26377;&#30417;&#30563;&#23398;&#20064;&#22120;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#27604;&#20363;&#23398;&#20064;&#65288;LLP&#65289;&#26159;&#19968;&#20010;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#21482;&#26377;&#38024;&#23545;&#19968;&#32452;&#23454;&#20363;&#65288;&#31216;&#20026;&#21253;&#65289;&#30340;&#32858;&#21512;&#32423;&#21035;&#26631;&#31614;&#21487;&#29992;&#65292;&#24182;&#19988;&#30446;&#30340;&#26159;&#22312;&#27979;&#35797;&#25968;&#25454;&#30340;&#23454;&#20363;&#32423;&#21035;&#19978;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;&#36825;&#31181;&#35774;&#32622;&#22312;&#24191;&#21578;&#21644;&#21307;&#23398;&#31561;&#39046;&#22495;&#30001;&#20110;&#38544;&#31169;&#32771;&#34385;&#32780;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31639;&#27861;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#36827;&#34892;&#36845;&#20195;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#30340;&#31532;&#19968;&#27493;&#65288;&#20266;&#26631;&#31614;&#21270;&#65289;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#22522;&#20110;&#20108;&#36827;&#21046;&#23454;&#20363;&#26631;&#31614;&#30340;&#21513;&#24067;&#26031;&#20998;&#24067;&#65292;&#35813;&#20998;&#24067;&#36890;&#36807;&#20197;&#19979;&#32422;&#26463;&#23558;covariate&#20449;&#24687;&#65288;&#21327;&#21464;&#37327;&#20449;&#24687;&#65289;&#21512;&#24182;&#36827;&#21435;&#65306;&#20855;&#26377;&#30456;&#20284;covariates&#30340;&#23454;&#20363;&#24212;&#35813;&#20855;&#26377;&#30456;&#20284;&#30340;&#26631;&#31614;&#65292;&#24182;&#19988;&#36890;&#36807;&#21253;&#32423;&#21035;&#30340;&#32858;&#21512;&#26631;&#31614;&#26469;&#32508;&#21512;covariate&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20449;&#24565;&#20256;&#25773;&#65288;BP&#65289;&#26469;&#36793;&#32536;&#21270;&#21513;&#24067;&#26031;&#20998;&#24067;&#20197;&#33719;&#24471;&#20266;&#26631;&#31614;&#12290;&#22312;&#31532;&#20108;&#27493;&#65288;&#23884;&#20837;&#32454;&#21270;&#65289;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20266;&#26631;&#31614;&#20026;&#23398;&#20064;&#22120;&#25552;&#20379;&#30417;&#30563;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#23884;&#20837;&#12290;&#27492;&#21518;&#65292;&#25105;&#20204;&#23545;&#36825;&#20004;&#20010;&#27493;&#39588;&#36827;&#34892;&#36845;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from Label Proportions (LLP) is a learning problem where only aggregate level labels are available for groups of instances, called bags, during training, and the aim is to get the best performance at the instance-level on the test data. This setting arises in domains like advertising and medicine due to privacy considerations. We propose a novel algorithmic framework for this problem that iteratively performs two main steps. For the first step (Pseudo Labeling) in every iteration, we define a Gibbs distribution over binary instance labels that incorporates a) covariate information through the constraint that instances with similar covariates should have similar labels and b) the bag level aggregated label. We then use Belief Propagation (BP) to marginalize the Gibbs distribution to obtain pseudo labels. In the second step (Embedding Refinement), we use the pseudo labels to provide supervision for a learner that yields a better embedding. Further, we iterate on the two steps ag
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#20801;&#35768;&#20351;&#29992;&#20013;&#38388;&#29983;&#25104;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;Transformer&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07923</link><description>&lt;p&gt;
&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The Expresssive Power of Transformers with Chain of Thought. (arXiv:2310.07923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#20801;&#35768;&#20351;&#29992;&#20013;&#38388;&#29983;&#25104;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;Transformer&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#29702;&#35770;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#20123;&#20986;&#20154;&#24847;&#26009;&#22320;&#31616;&#21333;&#30340;&#25512;&#29702;&#38382;&#39064;&#65292;&#20363;&#22914;&#26816;&#26597;&#22270;&#20013;&#26159;&#21542;&#23384;&#22312;&#36830;&#25509;&#30340;&#20004;&#20010;&#33410;&#28857;&#65292;&#25110;&#27169;&#25311;&#26377;&#38480;&#29366;&#24577;&#26426;&#65292;&#36825;&#20123;&#38382;&#39064;&#34987;&#35777;&#26126;&#26080;&#27861;&#30001;&#31435;&#21363;&#35835;&#21462;&#36755;&#20837;&#21518;&#22238;&#31572;&#30340;&#26631;&#20934;Transformer&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#36807;&#20801;&#35768;Transformer&#20351;&#29992;&#8220;&#24605;&#32500;&#38142;&#8221;&#25110;&#8220;&#33609;&#31295;&#32440;&#8221;&#65292;&#21363;&#22312;&#22238;&#31572;&#20043;&#21069;&#29983;&#25104;&#24182;&#20381;&#36182;&#19968;&#31995;&#21015;&#20013;&#38388;token&#65292;&#21487;&#20197;&#25913;&#21892;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#38382;&#65306;&#36825;&#31181;&#20013;&#38388;&#29983;&#25104;&#26159;&#21542;&#20174;&#26681;&#26412;&#19978;&#25193;&#23637;&#20102;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;Transformer&#30340;&#35745;&#31639;&#33021;&#21147;&#65311;&#25105;&#20204;&#34920;&#26126;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#65292;&#20294;&#22686;&#21152;&#30340;&#31243;&#24230;&#20851;&#38190;&#21462;&#20915;&#20110;&#20013;&#38388;&#29983;&#25104;&#30340;&#25968;&#37327;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#30456;&#23545;&#20110;&#36755;&#20837;&#38271;&#24230;&#26469;&#35828;&#65292;&#20855;&#26377;&#23545;&#25968;&#32423;&#35299;&#30721;&#27493;&#39588;&#30340;Transformer&#35299;&#30721;&#22120;&#20165;&#30053;&#24494;&#25512;&#21160;&#20102;&#26631;&#20934;Transformer&#30340;&#26497;&#38480;&#65292;&#32780;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#21017;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#65288;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent theoretical work has identified surprisingly simple reasoning problems, such as checking if two nodes in a graph are connected or simulating finite-state machines, that are provably unsolvable by standard transformers that answer immediately after reading their input. However, in practice, transformers' reasoning can be improved by allowing them to use a "chain of thought" or "scratchpad", i.e., generate and condition on a sequence of intermediate tokens before answering. Motivated by this, we ask: Does such intermediate generation fundamentally extend the computational power of a decoder-only transformer? We show that the answer is yes, but the amount of increase depends crucially on the amount of intermediate generation. For instance, we find that transformer decoders with a logarithmic number of decoding steps (w.r.t. the input length) push the limits of standard transformers only slightly, while a linear number of decoding steps adds a clear new ability (under standard compl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SAM-OCTA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#21644;&#25552;&#31034;&#28857;&#29983;&#25104;&#31574;&#30053;&#65292;&#22312;OCTA&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#22312;&#26377;&#38480;&#26679;&#26412;&#35757;&#32451;&#20013;&#21487;&#33021;&#23548;&#33268;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#22788;&#29702;&#20102;&#35270;&#32593;&#33180;&#34880;&#31649;&#12289;&#40644;&#26001;&#26080;&#34880;&#31649;&#21306;&#12289;&#27611;&#32454;&#34880;&#31649;&#12289;&#21160;&#33033;&#21644;&#38745;&#33033;&#31561;&#21508;&#31181;&#20998;&#21106;&#20219;&#21153;&#65292;&#21516;&#26102;&#36824;&#23454;&#29616;&#20102;&#23616;&#37096;&#34880;&#31649;&#20998;&#21106;&#21644;&#21160;&#33033;-&#38745;&#33033;&#20998;&#21106;&#31561;&#20197;&#21069;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07183</link><description>&lt;p&gt;
SAM-OCTA: &#29992;&#20110;OCTA&#22270;&#20687;&#20998;&#21106;&#30340;&#20998;&#27573;&#36741;&#21161;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
SAM-OCTA: Prompting Segment-Anything for OCTA Image Segmentation. (arXiv:2310.07183v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SAM-OCTA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#21644;&#25552;&#31034;&#28857;&#29983;&#25104;&#31574;&#30053;&#65292;&#22312;OCTA&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#22312;&#26377;&#38480;&#26679;&#26412;&#35757;&#32451;&#20013;&#21487;&#33021;&#23548;&#33268;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#22788;&#29702;&#20102;&#35270;&#32593;&#33180;&#34880;&#31649;&#12289;&#40644;&#26001;&#26080;&#34880;&#31649;&#21306;&#12289;&#27611;&#32454;&#34880;&#31649;&#12289;&#21160;&#33033;&#21644;&#38745;&#33033;&#31561;&#21508;&#31181;&#20998;&#21106;&#20219;&#21153;&#65292;&#21516;&#26102;&#36824;&#23454;&#29616;&#20102;&#23616;&#37096;&#34880;&#31649;&#20998;&#21106;&#21644;&#21160;&#33033;-&#38745;&#33033;&#20998;&#21106;&#31561;&#20197;&#21069;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#34880;&#27969;&#21160;&#21147;&#23398;(OCTA)&#22270;&#20687;&#20998;&#26512;&#20013;&#65292;&#20998;&#21106;&#29305;&#23450;&#30446;&#26631;&#26159;&#24517;&#35201;&#30340;&#25805;&#20316;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#22312;&#38480;&#23450;&#26679;&#26412;&#25968;&#65288;&#22823;&#32422;&#25968;&#30334;&#20010;&#65289;&#30340;&#26377;&#26631;&#27880;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#36807;&#25311;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#36827;&#34892;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#65292;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#25552;&#31034;&#28857;&#29983;&#25104;&#31574;&#30053;&#65292;&#20197;&#22788;&#29702;OCTA&#25968;&#25454;&#38598;&#19978;&#30340;&#21508;&#31181;&#20998;&#21106;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#34987;&#21629;&#21517;&#20026;SAM-OCTA&#65292;&#24182;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;OCTA-500&#21644;ROSE&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#25110;&#25509;&#36817;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#21106;&#24615;&#33021;&#25351;&#26631;&#12290;&#35814;&#32454;&#35752;&#35770;&#20102;&#25552;&#31034;&#28857;&#23545;&#35270;&#32593;&#33180;&#34880;&#31649;&#12289;&#40644;&#26001;&#26080;&#34880;&#31649;&#21306;&#12289;&#27611;&#32454;&#34880;&#31649;&#12289;&#21160;&#33033;&#21644;&#38745;&#33033;&#20998;&#21106;&#20219;&#21153;&#30340;&#25928;&#26524;&#21644;&#36866;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;SAM-OCTA&#23454;&#29616;&#20102;&#23616;&#37096;&#34880;&#31649;&#20998;&#21106;&#21644;&#26377;&#25928;&#30340;&#21160;&#33033;-&#38745;&#33033;&#20998;&#21106;&#65292;&#36825;&#26159;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the analysis of optical coherence tomography angiography (OCTA) images, the operation of segmenting specific targets is necessary. Existing methods typically train on supervised datasets with limited samples (approximately a few hundred), which can lead to overfitting. To address this, the low-rank adaptation technique is adopted for foundation model fine-tuning and proposed corresponding prompt point generation strategies to process various segmentation tasks on OCTA datasets. This method is named SAM-OCTA and has been experimented on the publicly available OCTA-500 and ROSE datasets. This method achieves or approaches state-of-the-art segmentation performance metrics. The effect and applicability of prompt points are discussed in detail for the retinal vessel, foveal avascular zone, capillary, artery, and vein segmentation tasks. Furthermore, SAM-OCTA accomplishes local vessel segmentation and effective artery-vein segmentation, which was not well-solved in previous works. The cod
&lt;/p&gt;</description></item><item><title>FroSSL&#26159;&#19968;&#31181;&#22522;&#20110;Frobenius&#33539;&#25968;&#26368;&#23567;&#21270;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#21327;&#26041;&#24046;Frobenius&#33539;&#25968;&#26469;&#36991;&#20813;&#20449;&#24687;&#23849;&#28291;&#65292;&#21516;&#26102;&#36890;&#36807;&#26368;&#23567;&#21270;&#22343;&#26041;&#24046;&#26469;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#30340;&#19981;&#21464;&#24615;&#65292;&#30456;&#27604;&#20854;&#20182;SSL&#26041;&#27861;&#65292;FroSSL&#25910;&#25947;&#26356;&#24555;&#65292;&#24182;&#19988;&#36825;&#31181;&#24555;&#36895;&#25910;&#25947;&#26159;&#30001;&#20110;FroSSL&#24433;&#21709;&#23884;&#20837;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#25152;&#33268;&#12290;</title><link>http://arxiv.org/abs/2310.02903</link><description>&lt;p&gt;
FroSSL: &#22522;&#20110;Frobenius&#33539;&#25968;&#26368;&#23567;&#21270;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FroSSL: Frobenius Norm Minimization for Self-Supervised Learning. (arXiv:2310.02903v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02903
&lt;/p&gt;
&lt;p&gt;
FroSSL&#26159;&#19968;&#31181;&#22522;&#20110;Frobenius&#33539;&#25968;&#26368;&#23567;&#21270;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#21327;&#26041;&#24046;Frobenius&#33539;&#25968;&#26469;&#36991;&#20813;&#20449;&#24687;&#23849;&#28291;&#65292;&#21516;&#26102;&#36890;&#36807;&#26368;&#23567;&#21270;&#22343;&#26041;&#24046;&#26469;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#30340;&#19981;&#21464;&#24615;&#65292;&#30456;&#27604;&#20854;&#20182;SSL&#26041;&#27861;&#65292;FroSSL&#25910;&#25947;&#26356;&#24555;&#65292;&#24182;&#19988;&#36825;&#31181;&#24555;&#36895;&#25910;&#25947;&#26159;&#30001;&#20110;FroSSL&#24433;&#21709;&#23884;&#20837;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#25152;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26159;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#34920;&#31034;&#23398;&#20064;&#33539;&#24335;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#21487;&#20998;&#31867;&#20026;&#26679;&#26412;&#23545;&#27604;&#12289;&#32500;&#24230;&#23545;&#27604;&#25110;&#38750;&#23545;&#31216;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#27599;&#20010;&#23478;&#26063;&#37117;&#26377;&#33258;&#24049;&#30340;&#26041;&#27861;&#26469;&#36991;&#20813;&#20449;&#24687;&#23849;&#28291;&#12290;&#34429;&#28982;&#32500;&#24230;&#23545;&#27604;&#26041;&#27861;&#25910;&#25947;&#21040;&#19982;&#26679;&#26412;&#23545;&#27604;&#26041;&#27861;&#30456;&#20284;&#30340;&#35299;&#65292;&#20294;&#21487;&#20197;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#19968;&#20123;&#26041;&#27861;&#38656;&#35201;&#26356;&#22810;&#30340;&#35757;&#32451;&#36845;&#20195;&#25165;&#33021;&#25910;&#25947;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30446;&#26631;&#20989;&#25968;FroSSL&#65292;&#23427;&#22312;&#23884;&#20837;&#24402;&#19968;&#21270;&#26041;&#38754;&#26082;&#26159;&#26679;&#26412;&#23545;&#27604;&#21448;&#26159;&#32500;&#24230;&#23545;&#27604;&#12290;FroSSL&#36890;&#36807;&#26368;&#23567;&#21270;&#21327;&#26041;&#24046;Frobenius&#33539;&#25968;&#26469;&#36991;&#20813;&#23849;&#28291;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#22343;&#26041;&#24046;&#26469;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#30340;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;FroSSL&#27604;&#20854;&#20182;&#21508;&#31181;SSL&#26041;&#27861;&#26356;&#24555;&#22320;&#25910;&#25947;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#25903;&#25345;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26356;&#24555;&#30340;&#25910;&#25947;&#26159;&#30001;&#20110;FroSSL&#23545;&#23884;&#20837;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) is an increasingly popular paradigm for representation learning. Recent methods can be classified as sample-contrastive, dimension-contrastive, or asymmetric network-based, with each family having its own approach to avoiding informational collapse. While dimension-contrastive methods converge to similar solutions as sample-contrastive methods, it can be empirically shown that some methods require more epochs of training to converge. Motivated by closing this divide, we present the objective function FroSSL which is both sample- and dimension-contrastive up to embedding normalization. FroSSL works by minimizing covariance Frobenius norms for avoiding collapse and minimizing mean-squared error for augmentation invariance. We show that FroSSL converges more quickly than a variety of other SSL methods and provide theoretical and empirical support that this faster convergence is due to how FroSSL affects the eigenvalues of the embedding covariance matrices. W
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;LLM&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#25506;&#32034;&#65292;&#20197;&#35299;&#20915;&#23545;&#36229;&#36807;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#30340;&#20070;&#31821;&#36827;&#34892;&#25688;&#35201;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#25552;&#31034;&#24037;&#20316;&#27969;&#23454;&#26045;&#20102;&#22522;&#20110;LLM&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#22120;&#30340;&#36830;&#36143;&#24615;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;100&#26412;&#20070;&#30340;GPT-4&#29983;&#25104;&#25688;&#35201;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#21457;&#29616;&#20102;&#20843;&#31181;&#24120;&#35265;&#30340;&#36830;&#36143;&#24615;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2310.00785</link><description>&lt;p&gt;
BooookScore: LLM&#26102;&#20195;&#20013;&#23545;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#30340;&#31995;&#32479;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
BooookScore: A systematic exploration of book-length summarization in the era of LLMs. (arXiv:2310.00785v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;LLM&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#25506;&#32034;&#65292;&#20197;&#35299;&#20915;&#23545;&#36229;&#36807;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#30340;&#20070;&#31821;&#36827;&#34892;&#25688;&#35201;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#25552;&#31034;&#24037;&#20316;&#27969;&#23454;&#26045;&#20102;&#22522;&#20110;LLM&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#22120;&#30340;&#36830;&#36143;&#24615;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;100&#26412;&#20070;&#30340;GPT-4&#29983;&#25104;&#25688;&#35201;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#21457;&#29616;&#20102;&#20843;&#31181;&#24120;&#35265;&#30340;&#36830;&#36143;&#24615;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#36229;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#30340;&#20070;&#31821;&#38271;&#24230;&#25991;&#26723;&#65288;&gt;100K&#26631;&#35760;&#65289;&#36827;&#34892;&#25688;&#35201;&#38656;&#35201;&#39318;&#20808;&#23558;&#36755;&#20837;&#25991;&#26723;&#20998;&#25104;&#36739;&#23567;&#30340;&#22359;&#65292;&#28982;&#21518;&#25552;&#31034;LLM&#21512;&#24182;&#12289;&#26356;&#26032;&#21644;&#21387;&#32553;&#22359;&#32423;&#25688;&#35201;&#12290;&#23613;&#31649;&#36825;&#20010;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#37325;&#35201;&#24615;&#65292;&#20294;&#30001;&#20110;&#35780;&#20272;&#30340;&#22256;&#38590;&#65292;&#23427;&#23578;&#26410;&#24471;&#21040;&#26377;&#24847;&#20041;&#30340;&#30740;&#31350;&#65306;&#29616;&#26377;&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;BookSum&#65289;&#22312;&#22823;&#22810;&#25968;&#20844;&#20849;LLM&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#32780;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#38590;&#20197;&#25429;&#25417;&#29616;&#20195;LLM&#25688;&#35201;&#22120;&#30340;&#38169;&#35823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#36890;&#36807;&#20004;&#31181;&#25552;&#31034;&#24037;&#20316;&#27969;&#23454;&#26045;&#30340;&#22522;&#20110;LLM&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#22120;&#30340;&#36830;&#36143;&#24615;&#65306;&#65288;1&#65289;&#20998;&#23618;&#21512;&#24182;&#22359;&#32423;&#25688;&#35201;&#65292;&#65288;2&#65289;&#36880;&#27493;&#26356;&#26032;&#19968;&#20010;&#36816;&#34892;&#25688;&#35201;&#12290;&#25105;&#20204;&#23545;100&#26412;&#26368;&#36817;&#20986;&#29256;&#30340;&#20070;&#31821;&#30340;GPT-4&#29983;&#25104;&#25688;&#35201;&#33719;&#24471;&#20102;1193&#20010;&#32454;&#31890;&#24230;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#30830;&#23450;&#20102;LLMs&#20135;&#29983;&#30340;&#20843;&#31181;&#24120;&#35265;&#30340;&#36830;&#36143;&#24615;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Summarizing book-length documents (&gt;100K tokens) that exceed the context window size of large language models (LLMs) requires first breaking the input document into smaller chunks and then prompting an LLM to merge, update, and compress chunk-level summaries. Despite the complexity and importance of this task, it has yet to be meaningfully studied due to the challenges of evaluation: existing book-length summarization datasets (e.g., BookSum) are in the pretraining data of most public LLMs, and existing evaluation methods struggle to capture errors made by modern LLM summarizers. In this paper, we present the first study of the coherence of LLM-based book-length summarizers implemented via two prompting workflows: (1) hierarchically merging chunk-level summaries, and (2) incrementally updating a running summary. We obtain 1193 fine-grained human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs. Bec
&lt;/p&gt;</description></item><item><title>ABScribe&#26159;&#19968;&#31181;&#30028;&#38754;&#65292;&#25903;&#25345;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#20889;&#20316;&#20219;&#21153;&#20013;&#24555;&#36895;&#25506;&#32034;&#22810;&#31181;&#20889;&#20316;&#21464;&#21270;&#12290;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#24555;&#36895;&#29983;&#25104;&#22810;&#20010;&#21464;&#20307;&#65292;&#36825;&#20123;&#21464;&#20307;&#20197;&#21487;&#37325;&#29992;&#30340;&#25353;&#38062;&#24418;&#24335;&#21576;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#19978;&#19979;&#25991;&#24037;&#20855;&#26639;&#36827;&#34892;&#24555;&#36895;&#30340;&#23601;&#22320;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2310.00117</link><description>&lt;p&gt;
ABScribe: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#20889;&#20316;&#20219;&#21153;&#20013;&#24555;&#36895;&#25506;&#32034;&#22810;&#31181;&#20889;&#20316;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
ABScribe: Rapid Exploration of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models. (arXiv:2310.00117v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00117
&lt;/p&gt;
&lt;p&gt;
ABScribe&#26159;&#19968;&#31181;&#30028;&#38754;&#65292;&#25903;&#25345;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#20889;&#20316;&#20219;&#21153;&#20013;&#24555;&#36895;&#25506;&#32034;&#22810;&#31181;&#20889;&#20316;&#21464;&#21270;&#12290;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#24555;&#36895;&#29983;&#25104;&#22810;&#20010;&#21464;&#20307;&#65292;&#36825;&#20123;&#21464;&#20307;&#20197;&#21487;&#37325;&#29992;&#30340;&#25353;&#38062;&#24418;&#24335;&#21576;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#19978;&#19979;&#25991;&#24037;&#20855;&#26639;&#36827;&#34892;&#24555;&#36895;&#30340;&#23601;&#22320;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#20070;&#20889;&#25991;&#26412;&#26469;&#25506;&#32034;&#26367;&#20195;&#24819;&#27861;&#26159;&#20889;&#20316;&#36807;&#31243;&#30340;&#20851;&#38190;&#12290;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#31616;&#21270;&#20889;&#20316;&#21464;&#21270;&#29983;&#25104;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30028;&#38754;&#23384;&#22312;&#21516;&#26102;&#32771;&#34385;&#22810;&#31181;&#21464;&#21270;&#30340;&#25361;&#25112;&#65306;&#22312;&#19981;&#35206;&#30422;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#21019;&#24314;&#26032;&#30340;&#29256;&#26412;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#32780;&#25353;&#39034;&#24207;&#31896;&#36148;&#23427;&#20204;&#21487;&#33021;&#20250;&#20351;&#25991;&#26723;&#21464;&#24471;&#26434;&#20081;&#65292;&#22686;&#21152;&#24037;&#20316;&#37327;&#65292;&#24182;&#25171;&#26029;&#20316;&#32773;&#30340;&#27969;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ABScribe&#65292;&#19968;&#31181;&#25903;&#25345;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#20889;&#20316;&#20219;&#21153;&#20013;&#24555;&#36895;&#19988;&#32467;&#26500;&#21270;&#22320;&#25506;&#32034;&#20889;&#20316;&#21464;&#21270;&#30340;&#30028;&#38754;&#12290;&#36890;&#36807;ABScribe&#65292;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;LLM&#25552;&#31034;&#24555;&#36895;&#20135;&#29983;&#22810;&#20010;&#21464;&#20307;&#65292;&#36825;&#20123;&#21464;&#20307;&#20250;&#33258;&#21160;&#36716;&#25442;&#25104;&#21487;&#37325;&#29992;&#30340;&#25353;&#38062;&#24418;&#24335;&#12290;&#21464;&#20307;&#22312;&#25991;&#26412;&#27573;&#33853;&#20013;&#34987;&#23384;&#20648;&#22312;&#30456;&#37051;&#20301;&#32622;&#65292;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#24037;&#20855;&#26639;&#19978;&#30340;&#40736;&#26631;&#24748;&#20572;&#20132;&#20114;&#36827;&#34892;&#24555;&#36895;&#30340;&#23601;&#22320;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;12&#21517;&#25776;&#20889;&#20154;&#21592;&#36827;&#34892;&#30340;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;ABScribe&#33021;&#26174;&#33879;&#20943;&#36731;&#20219;&#21153;&#36127;&#33655;&#65288;d = 1.20, p &lt; 0.001&#65289;&#65292;&#25552;&#39640;&#29992;&#25143;&#30340;&#35748;&#30693;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring alternative ideas by rewriting text is integral to the writing process. State-of-the-art large language models (LLMs) can simplify writing variation generation. However, current interfaces pose challenges for simultaneous consideration of multiple variations: creating new versions without overwriting text can be difficult, and pasting them sequentially can clutter documents, increasing workload and disrupting writers' flow. To tackle this, we present ABScribe, an interface that supports rapid, yet visually structured, exploration of writing variations in human-AI co-writing tasks. With ABScribe, users can swiftly produce multiple variations using LLM prompts, which are auto-converted into reusable buttons. Variations are stored adjacently within text segments for rapid in-place comparisons using mouse-over interactions on a context toolbar. Our user study with 12 writers shows that ABScribe significantly reduces task workload (d = 1.20, p &lt; 0.001), enhances user perceptions o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Forman-Ricci&#26354;&#29575;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;&#12290;&#36890;&#36807;&#35266;&#23519;&#31163;&#25955;&#26354;&#29575;&#65292;&#21487;&#20197;&#28155;&#21152;&#25110;&#21024;&#38500;&#36793;&#20197;&#20943;&#36731;&#36825;&#20004;&#31181;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2309.09384</link><description>&lt;p&gt;
&#20351;&#29992;Forman-Ricci&#26354;&#29575;&#30340;&#25193;&#23637;&#26469;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Over-Smoothing and Over-Squashing using Augmentations of Forman-Ricci Curvature. (arXiv:2309.09384v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Forman-Ricci&#26354;&#29575;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;&#12290;&#36890;&#36807;&#35266;&#23519;&#31163;&#25955;&#26354;&#29575;&#65292;&#21487;&#20197;&#28155;&#21152;&#25110;&#21024;&#38500;&#36793;&#20197;&#20943;&#36731;&#36825;&#20004;&#31181;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#26368;&#36817;&#25551;&#36848;&#20102;&#20960;&#20010;&#28508;&#22312;&#30340;&#38519;&#38449;&#12290;&#36825;&#20123;&#21253;&#25324;&#26080;&#27861;&#20934;&#30830;&#21033;&#29992;&#32534;&#30721;&#22312;&#38271;&#36317;&#31163;&#36830;&#25509;&#20013;&#30340;&#20449;&#24687;&#65288;&#36807;&#24230;&#21387;&#32553;&#65289;&#65292;&#20197;&#21450;&#22312;&#32593;&#32476;&#28145;&#24230;&#22686;&#21152;&#26102;&#38590;&#20197;&#21306;&#20998;&#38468;&#36817;&#33410;&#28857;&#30340;&#23398;&#20064;&#34920;&#31034;&#65288;&#36807;&#24230;&#24179;&#28369;&#65289;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#34920;&#24449;&#36825;&#20004;&#31181;&#25928;&#24212;&#30340;&#26041;&#27861;&#26159;&#31163;&#25955;&#26354;&#29575;&#65306;&#23548;&#33268;&#36807;&#24230;&#21387;&#32553;&#25928;&#24212;&#30340;&#38271;&#36317;&#31163;&#36830;&#25509;&#20855;&#26377;&#20302;&#26354;&#29575;&#65292;&#32780;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#30340;&#36793;&#20855;&#26377;&#39640;&#26354;&#29575;&#12290;&#36825;&#20010;&#35266;&#23519;&#24341;&#21457;&#20102;&#19968;&#20123;&#37325;&#36830;&#25216;&#26415;&#65292;&#36890;&#36807;&#22686;&#21152;&#25110;&#21024;&#38500;&#36793;&#26469;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#21033;&#29992;&#22270;&#29305;&#24449;&#65288;&#22914;&#26354;&#29575;&#25110;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#35889;&#65289;&#30340;&#37325;&#36830;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#26354;&#29575;&#30340;&#26041;&#27861;&#65292;&#36890;&#24120;&#38656;&#35201;&#26114;&#36149;&#30340;&#23376;&#22270;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Graph Neural Networks (GNNs) have been successfully leveraged for learning on graph-structured data across domains, several potential pitfalls have been described recently. Those include the inability to accurately leverage information encoded in long-range connections (over-squashing), as well as difficulties distinguishing the learned representations of nearby nodes with growing network depth (over-smoothing). An effective way to characterize both effects is discrete curvature: Long-range connections that underlie over-squashing effects have low curvature, whereas edges that contribute to over-smoothing have high curvature. This observation has given rise to rewiring techniques, which add or remove edges to mitigate over-smoothing and over-squashing. Several rewiring approaches utilizing graph characteristics, such as curvature or the spectrum of the graph Laplacian, have been proposed. However, existing methods, especially those based on curvature, often require expensive subr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20154;&#31867;&#25512;&#29702;&#33021;&#21147;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#12290;&#36890;&#36807;&#19968;&#20010;&#27969;&#27700;&#32447;&#21644;&#24050;&#26377;&#25968;&#25454;&#38598;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#27979;&#37327;&#36825;&#31181;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.04461</link><description>&lt;p&gt;
&#27979;&#37327;&#21644;&#25913;&#36827;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models. (arXiv:2309.04461v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20154;&#31867;&#25512;&#29702;&#33021;&#21147;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#12290;&#36890;&#36807;&#19968;&#20010;&#27969;&#27700;&#32447;&#21644;&#24050;&#26377;&#25968;&#25454;&#38598;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#27979;&#37327;&#36825;&#31181;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#20316;&#20026;&#33021;&#35299;&#26512;&#20851;&#20110;&#35270;&#35273;&#20869;&#23481;&#30340;&#33258;&#28982;&#26597;&#35810;&#24182;&#29983;&#25104;&#31867;&#20154;&#36755;&#20986;&#30340;&#35270;&#35273;&#21161;&#25163;&#65292;&#23637;&#31034;&#20986;&#20102;&#24378;&#22823;&#30340;&#21151;&#25928;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36825;&#20123;&#27169;&#22411;&#23637;&#31034;&#22522;&#20110;&#25152;&#24863;&#30693;&#20449;&#24687;&#30340;&#31867;&#20154;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#20851;&#20110;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#21040;&#24213;&#26377;&#22810;&#19968;&#33268;&#21644;&#26377;&#22810;&#22522;&#20110;&#23454;&#38469;&#30340;&#19968;&#20010;&#37325;&#35201;&#30097;&#34385;&#65292;&#25105;&#20204;&#36824;&#27979;&#37327;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#35780;&#20272;&#38656;&#35201;&#28085;&#30422;&#39640;&#23618;&#27425;&#25512;&#29702;&#21644;&#32454;&#33410;&#25512;&#29702;&#38142;&#30340;&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#39033;&#26114;&#36149;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;LLM-Human-in-the-Loop&#27969;&#27700;&#32447;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#35813;&#27969;&#27700;&#32447;&#26174;&#33879;&#38477;&#20302;&#20102;&#25104;&#26412;&#65292;&#21516;&#26102;&#30830;&#20445;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#22522;&#20110;&#36825;&#20010;&#27969;&#27700;&#32447;&#21644;&#29616;&#26377;&#30340;&#31895;&#31890;&#24230;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;CURE&#22522;&#20934;&#26469;&#21516;&#26102;&#27979;&#37327;&#20004;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) have recently demonstrated strong efficacy as visual assistants that can parse natural queries about the visual content and generate human-like outputs. In this work, we explore the ability of these models to demonstrate human-like reasoning based on the perceived information. To address a crucial concern regarding the extent to which their reasoning capabilities are fully consistent and grounded, we also measure the reasoning consistency of these models. We achieve this by proposing a chain-of-thought (CoT) based consistency measure. However, such an evaluation requires a benchmark that encompasses both high-level inference and detailed reasoning chains, which is costly. We tackle this challenge by proposing a LLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously ensuring the generation of a high-quality dataset. Based on this pipeline and the existing coarse-grained annotated dataset, we build the CURE benchmark to measure both 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37327;&#21270;&#35780;&#20272;&#20998;&#31867;&#38543;&#26426;&#26862;&#26519;&#30340;&#35823;&#24046;&#20272;&#35745;&#26041;&#27861;&#65292;&#21457;&#29616;&#38543;&#26426;&#26862;&#26519;&#30340;&#39044;&#27979;&#35823;&#24046;&#20272;&#35745;&#27604;&#24179;&#22343;&#39044;&#27979;&#35823;&#24046;&#26356;&#25509;&#36817;&#30495;&#23454;&#35823;&#24046;&#29575;&#65292;&#24182;&#19988;&#36825;&#19968;&#32467;&#26524;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#35823;&#24046;&#20272;&#35745;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.00736</link><description>&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#20013;&#30340;&#39044;&#27979;&#35823;&#24046;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Prediction Error Estimation in Random Forests. (arXiv:2309.00736v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37327;&#21270;&#35780;&#20272;&#20998;&#31867;&#38543;&#26426;&#26862;&#26519;&#30340;&#35823;&#24046;&#20272;&#35745;&#26041;&#27861;&#65292;&#21457;&#29616;&#38543;&#26426;&#26862;&#26519;&#30340;&#39044;&#27979;&#35823;&#24046;&#20272;&#35745;&#27604;&#24179;&#22343;&#39044;&#27979;&#35823;&#24046;&#26356;&#25509;&#36817;&#30495;&#23454;&#35823;&#24046;&#29575;&#65292;&#24182;&#19988;&#36825;&#19968;&#32467;&#26524;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#35823;&#24046;&#20272;&#35745;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23450;&#37327;&#35780;&#20272;&#20102;&#20998;&#31867;&#38543;&#26426;&#26862;&#26519;&#30340;&#35823;&#24046;&#20272;&#35745;&#12290;&#22312;Bates&#31561;&#20154;&#65288;2023&#24180;&#65289;&#24314;&#31435;&#30340;&#21021;&#27493;&#29702;&#35770;&#26694;&#26550;&#30340;&#22522;&#30784;&#19978;&#65292;&#20174;&#29702;&#35770;&#21644;&#32463;&#39564;&#35282;&#24230;&#25506;&#35752;&#20102;&#38543;&#26426;&#26862;&#26519;&#20013;&#24120;&#35265;&#30340;&#21508;&#31181;&#35823;&#24046;&#20272;&#35745;&#26041;&#27861;&#22312;&#30495;&#23454;&#35823;&#24046;&#29575;&#21644;&#26399;&#26395;&#35823;&#24046;&#29575;&#26041;&#38754;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20998;&#31867;&#24773;&#20917;&#19979;&#65292;&#38543;&#26426;&#26862;&#26519;&#30340;&#39044;&#27979;&#35823;&#24046;&#20272;&#35745;&#24179;&#22343;&#26356;&#25509;&#36817;&#30495;&#23454;&#35823;&#24046;&#29575;&#65292;&#32780;&#19981;&#26159;&#24179;&#22343;&#39044;&#27979;&#35823;&#24046;&#12290;&#19982;Bates&#31561;&#20154;&#65288;2023&#24180;&#65289;&#23545;&#36923;&#36753;&#22238;&#24402;&#30340;&#30740;&#31350;&#32467;&#26524;&#30456;&#21453;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#36825;&#20010;&#32467;&#26524;&#36866;&#29992;&#20110;&#20132;&#21449;&#39564;&#35777;&#12289;&#33258;&#20030;&#21644;&#25968;&#25454;&#21010;&#20998;&#31561;&#19981;&#21516;&#30340;&#35823;&#24046;&#20272;&#35745;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, error estimates of classification Random Forests are quantitatively assessed. Based on the initial theoretical framework built by Bates et al. (2023), the true error rate and expected error rate are theoretically and empirically investigated in the context of a variety of error estimation methods common to Random Forests. We show that in the classification case, Random Forests' estimates of prediction error is closer on average to the true error rate instead of the average prediction error. This is opposite the findings of Bates et al. (2023) which were given for logistic regression. We further show that this result holds across different error estimation strategies such as cross-validation, bagging, and data splitting.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#21435;&#37325;&#27169;&#22411;&#65292;&#23558;Transformer&#21644;&#20027;&#21160;&#23398;&#20064;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#65292;&#39318;&#27425;&#35299;&#20915;&#20102;&#35821;&#20041;&#32423;&#21035;&#30340;&#21435;&#37325;&#38382;&#39064;&#65292;&#21516;&#26102;&#37319;&#29992;R-Drop&#26041;&#27861;&#23545;&#27599;&#19968;&#36718;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;&#36890;&#36807;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#36827;&#34892;&#21435;&#37325;&#27169;&#22411;&#35757;&#32451;&#65292;&#19981;&#20165;&#38477;&#20302;&#20102;&#25163;&#21160;&#26631;&#35760;&#30340;&#25104;&#26412;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.00721</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#21435;&#37325;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Pre-trained Data Deduplication Model based on Active Learning. (arXiv:2308.00721v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00721
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#21435;&#37325;&#27169;&#22411;&#65292;&#23558;Transformer&#21644;&#20027;&#21160;&#23398;&#20064;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#65292;&#39318;&#27425;&#35299;&#20915;&#20102;&#35821;&#20041;&#32423;&#21035;&#30340;&#21435;&#37325;&#38382;&#39064;&#65292;&#21516;&#26102;&#37319;&#29992;R-Drop&#26041;&#27861;&#23545;&#27599;&#19968;&#36718;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;&#36890;&#36807;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#36827;&#34892;&#21435;&#37325;&#27169;&#22411;&#35757;&#32451;&#65292;&#19981;&#20165;&#38477;&#20302;&#20102;&#25163;&#21160;&#26631;&#35760;&#30340;&#25104;&#26412;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#65292;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#26085;&#30410;&#31361;&#20986;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#37325;&#22797;&#25968;&#25454;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#25968;&#25454;&#30340;&#37325;&#22797;&#36755;&#20837;&#25110;&#22810;&#20010;&#25968;&#25454;&#28304;&#30340;&#21512;&#24182;&#23548;&#33268;&#30340;&#12290;&#36825;&#20123;"&#33039;&#25968;&#25454;"&#38382;&#39064;&#20005;&#37325;&#38480;&#21046;&#20102;&#22823;&#25968;&#25454;&#30340;&#26377;&#25928;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#21435;&#37325;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#21435;&#37325;&#27169;&#22411;&#65292;&#36825;&#26159;&#39318;&#27425;&#21033;&#29992;&#20027;&#21160;&#23398;&#20064;&#35299;&#20915;&#35821;&#20041;&#32423;&#21035;&#30340;&#21435;&#37325;&#38382;&#39064;&#30340;&#24037;&#20316;&#12290;&#35813;&#27169;&#22411;&#26500;&#24314;&#22312;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;Transformer&#19978;&#65292;&#24182;&#36890;&#36807;&#32454;&#35843;&#23558;&#20854;&#24212;&#29992;&#20110;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#65292;&#39318;&#27425;&#23558;Transformer&#21644;&#20027;&#21160;&#23398;&#20064;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#65292;&#20197;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#36827;&#34892;&#21435;&#37325;&#27169;&#22411;&#35757;&#32451;&#65292;&#21516;&#26102;&#39318;&#27425;&#37319;&#29992;R-Drop&#26041;&#27861;&#23545;&#27599;&#19968;&#36718;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#26082;&#33021;&#38477;&#20302;&#25163;&#21160;&#26631;&#35760;&#30340;&#25104;&#26412;&#65292;&#20063;&#33021;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of big data, the issue of data quality has become increasingly prominent. One of the main challenges is the problem of duplicate data, which can arise from repeated entry or the merging of multiple data sources. These "dirty data" problems can significantly limit the effective application of big data. To address the issue of data deduplication, we propose a pre-trained deduplication model based on active learning, which is the first work that utilizes active learning to address the problem of deduplication at the semantic level. The model is built on a pre-trained Transformer and fine-tuned to solve the deduplication problem as a sequence to classification task, which firstly integrate the transformer with active learning into an end-to-end architecture to select the most valuable data for deduplication model training, and also firstly employ the R-Drop method to perform data augmentation on each round of labeled data, which can reduce the cost of manual labeling and improve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;Modified Spectrum Kernels&#65288;MSKs&#65289;&#26500;&#36896;&#26680;&#26063;&#65292;&#36890;&#36807;&#39044;&#26465;&#20214;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#23485;&#31070;&#32463;&#32593;&#32476;&#24402;&#32435;&#20559;&#24046;&#30340;&#25511;&#21046;&#65292;&#24182;&#22312;&#19981;&#25913;&#21464;&#26368;&#32456;&#35299;&#30340;&#24773;&#20917;&#19979;&#65292;&#21152;&#36895;&#20102;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.14531</link><description>&lt;p&gt;
&#36890;&#36807;&#20462;&#25913;&#26680;&#35889;&#26469;&#25511;&#21046;&#23485;&#31070;&#32463;&#32593;&#32476;&#30340;&#24402;&#32435;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Controlling the Inductive Bias of Wide Neural Networks by Modifying the Kernel's Spectrum. (arXiv:2307.14531v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;Modified Spectrum Kernels&#65288;MSKs&#65289;&#26500;&#36896;&#26680;&#26063;&#65292;&#36890;&#36807;&#39044;&#26465;&#20214;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#23485;&#31070;&#32463;&#32593;&#32476;&#24402;&#32435;&#20559;&#24046;&#30340;&#25511;&#21046;&#65292;&#24182;&#22312;&#19981;&#25913;&#21464;&#26368;&#32456;&#35299;&#30340;&#24773;&#20917;&#19979;&#65292;&#21152;&#36895;&#20102;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23485;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#29305;&#23450;&#20989;&#25968;&#26041;&#38754;&#23384;&#22312;&#20559;&#24046;&#65292;&#24433;&#21709;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26377;&#38480;&#35757;&#32451;&#26102;&#38388;&#20869;&#21487;&#36798;&#21040;&#30340;&#20989;&#25968;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#19968;&#31181;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#20462;&#25913;&#36825;&#31181;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Modified Spectrum Kernels(MSKs)&#36825;&#19968;&#26032;&#39062;&#30340;&#26500;&#36896;&#26680;&#26063;&#65292;&#21487;&#20197;&#29992;&#20110;&#36817;&#20284;&#27809;&#26377;&#24050;&#30693;&#38381;&#21512;&#24418;&#24335;&#30340;&#26399;&#26395;&#29305;&#24449;&#20540;&#30340;&#26680;&#12290;&#25105;&#20204;&#21033;&#29992;&#23485;&#31070;&#32463;&#32593;&#32476;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#20043;&#38388;&#30340;&#23545;&#20598;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#26465;&#20214;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#25913;&#21464;&#20102;&#26799;&#24230;&#19979;&#38477;&#30340;&#36712;&#36857;&#12290;&#32467;&#26524;&#26159;&#65292;&#36825;&#20351;&#24471;&#35757;&#32451;&#36895;&#24230;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21576;&#22810;&#39033;&#24335;&#29978;&#33267;&#25351;&#25968;&#32423;&#21152;&#36895;&#65292;&#21516;&#26102;&#19981;&#25913;&#21464;&#26368;&#32456;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26082;&#35745;&#31639;&#39640;&#25928;&#21448;&#26131;&#20110;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wide neural networks are biased towards learning certain functions, influencing both the rate of convergence of gradient descent (GD) and the functions that are reachable with GD in finite training time. As such, there is a great need for methods that can modify this bias according to the task at hand. To that end, we introduce Modified Spectrum Kernels (MSKs), a novel family of constructed kernels that can be used to approximate kernels with desired eigenvalues for which no closed form is known. We leverage the duality between wide neural networks and Neural Tangent Kernels and propose a preconditioned gradient descent method, which alters the trajectory of GD. As a result, this allows for a polynomial and, in some cases, exponential training speedup without changing the final solution. Our method is both computationally efficient and simple to implement.
&lt;/p&gt;</description></item><item><title>AdjointDPM&#26159;&#19968;&#31181;&#26032;&#30340;&#20276;&#38543;&#28789;&#25935;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#65292;&#35299;&#20915;&#20102;DPM&#23450;&#21046;&#21270;&#20013;&#20869;&#23384;&#28040;&#32791;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#22686;&#24378;&#30340;ODE&#23558;&#25439;&#22833;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#21040;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.10711</link><description>&lt;p&gt;
AdjointDPM: &#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#30340;&#20276;&#38543;&#28789;&#25935;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models. (arXiv:2307.10711v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10711
&lt;/p&gt;
&lt;p&gt;
AdjointDPM&#26159;&#19968;&#31181;&#26032;&#30340;&#20276;&#38543;&#28789;&#25935;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#65292;&#35299;&#20915;&#20102;DPM&#23450;&#21046;&#21270;&#20013;&#20869;&#23384;&#28040;&#32791;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#22686;&#24378;&#30340;ODE&#23558;&#25439;&#22833;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#21040;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#23450;&#21046;&#21270;&#26041;&#27861;&#38656;&#35201;&#22810;&#20010;&#21442;&#32771;&#26679;&#20363;&#26469;&#23558;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;(DPMs)&#19982;&#29992;&#25143;&#25552;&#20379;&#30340;&#27010;&#24565;&#23545;&#40784;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#24403;&#21807;&#19968;&#21487;&#29992;&#30340;&#30417;&#30563;&#26159;&#23450;&#20041;&#22312;&#29983;&#25104;&#20869;&#23481;&#19978;&#30340;&#21487;&#24494;&#24230;&#37327;&#26102;&#30340;DPM&#23450;&#21046;&#21270;&#25361;&#25112;&#12290;&#30001;&#20110;DPM&#30340;&#37319;&#26679;&#36807;&#31243;&#28041;&#21450;&#23545;&#21435;&#22122;UNet&#30340;&#36882;&#24402;&#35843;&#29992;&#65292;&#26420;&#32032;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#38656;&#35201;&#23384;&#20648;&#25152;&#26377;&#36845;&#20195;&#30340;&#20013;&#38388;&#29366;&#24577;&#65292;&#23548;&#33268;&#20869;&#23384;&#28040;&#32791;&#26497;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;AdjointDPM&#65292;&#39318;&#20808;&#36890;&#36807;&#27714;&#35299;&#30456;&#24212;&#30340;&#27010;&#29575;&#27969;ODE&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#29983;&#25104;&#26032;&#26679;&#26412;&#12290;&#28982;&#21518;&#20351;&#29992;&#20276;&#38543;&#28789;&#25935;&#24230;&#26041;&#27861;&#36890;&#36807;&#27714;&#35299;&#21478;&#19968;&#20010;&#22686;&#24378;&#30340;ODE&#23558;&#25439;&#22833;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#21040;&#27169;&#22411;&#30340;&#21442;&#25968;(&#21253;&#25324;&#35843;&#21046;&#20449;&#21495;&#12289;&#32593;&#32476;&#26435;&#37325;&#21644;&#21021;&#22987;&#22122;&#22768;)&#12290;&#20026;&#20102;&#20943;&#23569;&#27491;&#21521;&#29983;&#25104;&#21644;&#21453;&#21521;&#20256;&#25773;&#20013;&#30340;&#25968;&#20540;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Existing customization methods require access to multiple reference examples to align pre-trained diffusion probabilistic models (DPMs) with user-provided concepts. This paper aims to address the challenge of DPM customization when the only available supervision is a differentiable metric defined on the generated contents. Since the sampling procedure of DPMs involves recursive calls to the denoising UNet, na\"ive gradient backpropagation requires storing the intermediate states of all iterations, resulting in extremely high memory consumption. To overcome this issue, we propose a novel method AdjointDPM, which first generates new samples from diffusion models by solving the corresponding probability-flow ODEs. It then uses the adjoint sensitivity method to backpropagate the gradients of the loss to the models' parameters (including conditioning signals, network weights, and initial noises) by solving another augmented ODE. To reduce numerical errors in both the forward generation and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#20803;&#30340;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#21147;&#23398;&#32593;&#32476;&#21644;&#29289;&#29702;&#24863;&#30693;&#32534;&#30721;&#22120;&#65292;&#20174;&#37096;&#20998;&#35266;&#23519;&#20013;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#36890;&#36807;&#27491;&#36816;&#21160;&#23398;&#35299;&#30721;&#22120;&#36827;&#34892;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#29289;&#29702;&#35299;&#37322;&#24615;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.07975</link><description>&lt;p&gt;
&#22522;&#20110;&#26377;&#38480;&#20803;&#30340;&#32593;&#32476;: &#20174;&#37096;&#20998;&#35266;&#23519;&#20013;&#23398;&#20064;&#21512;&#29702;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Finite element inspired networks: Learning physically-plausible deformable object dynamics from partial observations. (arXiv:2307.07975v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07975
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#20803;&#30340;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#21147;&#23398;&#32593;&#32476;&#21644;&#29289;&#29702;&#24863;&#30693;&#32534;&#30721;&#22120;&#65292;&#20174;&#37096;&#20998;&#35266;&#23519;&#20013;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#36890;&#36807;&#27491;&#36816;&#21160;&#23398;&#35299;&#30721;&#22120;&#36827;&#34892;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#29289;&#29702;&#35299;&#37322;&#24615;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#27169;&#25311;&#21487;&#21464;&#24418;&#32447;&#24615;&#29289;&#20307;&#65288;DLO&#65289;&#30340;&#21160;&#21147;&#23398;&#22312;&#38656;&#35201;&#19968;&#20010;&#21487;&#20197;&#34987;&#20154;&#35299;&#35835;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#27169;&#22411;&#24182;&#33021;&#22815;&#25552;&#20379;&#24555;&#36895;&#39044;&#27979;&#30340;&#20219;&#21153;&#20013;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#24471;&#21040;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#21018;&#24615;&#26377;&#38480;&#20803;&#26041;&#27861;&#65288;R-FEM&#65289;&#65292;&#23558;DLO&#24314;&#27169;&#20026;&#19968;&#31995;&#21015;&#21018;&#20307;&#38142;&#65292;&#20854;&#20869;&#37096;&#29366;&#24577;&#36890;&#36807;&#21160;&#21147;&#23398;&#32593;&#32476;&#20197;&#26102;&#38388;&#23637;&#24320;&#12290;&#30001;&#20110;&#35813;&#29366;&#24577;&#19981;&#33021;&#30452;&#25509;&#35266;&#23519;&#21040;&#65292;&#21160;&#21147;&#23398;&#32593;&#32476;&#19982;&#19968;&#20010;&#29289;&#29702;&#24863;&#30693;&#30340;&#32534;&#30721;&#22120;&#20849;&#21516;&#35757;&#32451;&#65292;&#23558;&#35266;&#23519;&#21040;&#30340;&#36816;&#21160;&#21464;&#37327;&#26144;&#23556;&#21040;&#21018;&#20307;&#38142;&#30340;&#29366;&#24577;&#12290;&#20026;&#20102;&#20419;&#20351;&#29366;&#24577;&#33719;&#24471;&#29289;&#29702;&#19978;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#21033;&#29992;&#24213;&#23618;R-FEM&#27169;&#22411;&#30340;&#27491;&#36816;&#21160;&#23398;&#65288;FK&#65289;&#20316;&#20026;&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#35777;&#26126;&#65292;&#36825;&#31181;&#34987;&#31216;&#20026;&#8220;&#26377;&#38480;&#20803;&#21551;&#21457;&#32593;&#32476;&#8221;&#30340;&#26550;&#26500;&#26159;&#19968;&#20010;&#26131;&#20110;&#22788;&#29702;&#20294;&#21151;&#33021;&#24378;&#22823;&#30340;DLO&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#37096;&#20998;&#35266;&#23519;&#20013;&#24471;&#20986;&#20855;&#26377;&#29289;&#29702;&#35299;&#37322;&#24615;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate simulation of deformable linear object (DLO) dynamics is challenging if the task at hand requires a human-interpretable and data-efficient model that also yields fast predictions. To arrive at such model, we draw inspiration from the rigid finite element method (R-FEM) and model a DLO as a serial chain of rigid bodies whose internal state is unrolled through time by a dynamics network. As this state is not observed directly, the dynamics network is trained jointly with a physics-informed encoder mapping observed motion variables to the body chain's state. To encourage that the state acquires a physically meaningful representation, we leverage the forward kinematics (FK) of the underlying R-FEM model as a decoder. We demonstrate in a robot experiment that this architecture - being termed "Finite element inspired network" - forms an easy to handle, yet capable DLO dynamics model yielding physically interpretable predictions from partial observations.  The project code is ava
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#20195;&#29702;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21363;&#26102;&#22478;&#38469;&#25340;&#36710;&#26381;&#21153;&#30340;&#36710;&#36742;&#35843;&#24230;&#21644;&#36335;&#24452;&#35268;&#21010;&#12290;&#25968;&#20540;&#30740;&#31350;&#35777;&#26126;&#35813;&#26694;&#26550;&#26377;&#25928;&#32531;&#35299;&#20102;&#20379;&#32473;&#19981;&#36275;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.06742</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#20195;&#29702;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#21363;&#26102;&#22478;&#38469;&#25340;&#36710;&#26381;&#21153;&#30340;&#36710;&#36742;&#35843;&#24230;&#21644;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Vehicle Dispatching and Routing of On-Demand Intercity Ride-Pooling Services: A Multi-Agent Hierarchical Reinforcement Learning Approach. (arXiv:2307.06742v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#20195;&#29702;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21363;&#26102;&#22478;&#38469;&#25340;&#36710;&#26381;&#21153;&#30340;&#36710;&#36742;&#35843;&#24230;&#21644;&#36335;&#24452;&#35268;&#21010;&#12290;&#25968;&#20540;&#30740;&#31350;&#35777;&#26126;&#35813;&#26694;&#26550;&#26377;&#25928;&#32531;&#35299;&#20102;&#20379;&#32473;&#19981;&#36275;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#32676;&#19968;&#20307;&#21270;&#30340;&#21457;&#23637;&#23548;&#33268;&#20102;&#23545;&#22478;&#38469;&#26053;&#34892;&#30340;&#19981;&#26029;&#22686;&#38271;&#38656;&#27714;&#12290;&#22478;&#38469;&#25340;&#36710;&#26381;&#21153;&#36890;&#36807;&#23454;&#26045;&#38656;&#27714;&#21709;&#24212;&#24615;&#22686;&#24378;&#25514;&#26045;&#65292;&#26377;&#26395;&#21319;&#32423;&#20256;&#32479;&#30340;&#22478;&#38469;&#23458;&#36710;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#20854;&#22312;&#32447;&#25805;&#20316;&#21463;&#21040;&#36710;&#36742;&#36164;&#28304;&#20998;&#37197;&#21644;&#25340;&#36710;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#20043;&#38388;&#32806;&#21512;&#24615;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#23618;&#26694;&#26550;&#65292;&#26088;&#22312;&#20419;&#36827;&#22312;&#32447;&#36710;&#38431;&#31649;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#26694;&#26550;&#30340;&#19978;&#23618;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20195;&#29702;&#23553;&#24314;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#21327;&#21516;&#20998;&#37197;&#38386;&#32622;&#36710;&#36742;&#21040;&#19981;&#21516;&#30340;&#22478;&#38469;&#32447;&#36335;&#65292;&#32780;&#22312;&#19979;&#23618;&#65292;&#21017;&#20351;&#29992;&#33258;&#36866;&#24212;&#22823;&#37051;&#22495;&#25628;&#32034;&#21551;&#21457;&#24335;&#31639;&#27861;&#26356;&#26032;&#36710;&#36742;&#30340;&#36335;&#32447;&#12290;&#22522;&#20110;&#20013;&#22269;&#21414;&#38376;&#21450;&#20854;&#21608;&#36793;&#22478;&#24066;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#25968;&#20540;&#30740;&#31350;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#20379;&#32473;&#19981;&#36275;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integrated development of city clusters has given rise to an increasing demand for intercity travel. Intercity ride-pooling service exhibits considerable potential in upgrading traditional intercity bus services by implementing demand-responsive enhancements. Nevertheless, its online operations suffer the inherent complexities due to the coupling of vehicle resource allocation among cities and pooled-ride vehicle routing. To tackle these challenges, this study proposes a two-level framework designed to facilitate online fleet management. Specifically, a novel multi-agent feudal reinforcement learning model is proposed at the upper level of the framework to cooperatively assign idle vehicles to different intercity lines, while the lower level updates the routes of vehicles using an adaptive large neighborhood search heuristic. Numerical studies based on the realistic dataset of Xiamen and its surrounding cities in China show that the proposed framework effectively mitigates the supp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#36924;&#30495;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#25311;&#22120;&#30340;&#22522;&#20934;&#35780;&#20272;RM-PRT&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#35780;&#20272;&#27969;&#31243;&#65292;&#26088;&#22312;&#36890;&#36807;&#22823;&#35268;&#27169;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#22810;&#27169;&#24577;&#25552;&#31034;&#23545;&#26426;&#22120;&#20154;&#25805;&#20316;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.11335</link><description>&lt;p&gt;
RM-PRT: &#30495;&#23454;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#25311;&#22120;&#21644;&#22522;&#20110;&#28176;&#36827;&#25512;&#29702;&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RM-PRT: Realistic Robotic Manipulation Simulator and Benchmark with Progressive Reasoning Tasks. (arXiv:2306.11335v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11335
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#36924;&#30495;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#25311;&#22120;&#30340;&#22522;&#20934;&#35780;&#20272;RM-PRT&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#35780;&#20272;&#27969;&#31243;&#65292;&#26088;&#22312;&#36890;&#36807;&#22823;&#35268;&#27169;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#22810;&#27169;&#24577;&#25552;&#31034;&#23545;&#26426;&#22120;&#20154;&#25805;&#20316;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;ChatGPT&#21644;GPT-4&#30340;&#20986;&#29616;&#65292;&#26174;&#30528;&#25512;&#36827;&#20102;&#26426;&#22120;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#36825;&#19968;&#31361;&#30772;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#36825;&#20123;&#24320;&#28304;LLM&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#32479;&#19968;&#30340;&#26426;&#22120;&#20154;&#27169;&#25311;&#22120;&#29615;&#22659;&#20013;&#65292;&#20197;&#24110;&#21161;&#26426;&#22120;&#20154;&#20934;&#30830;&#29702;&#35299;&#21644;&#25191;&#34892;&#20154;&#31867;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36924;&#30495;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#25311;&#22120;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#28176;&#36827;&#25512;&#29702;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#22522;&#20934;&#65288;RM-PRT&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;RM-PRT&#22522;&#20934;&#35780;&#20272;&#22522;&#20110;Unreal Engine 5&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#39640;&#20445;&#30495;&#25968;&#23383;&#21452;&#32990;&#32974;&#22330;&#26223;&#65292;&#20854;&#20013;&#21253;&#25324;782&#20010;&#31867;&#21035;&#65292;2023&#20010;&#29289;&#20307;&#65292;&#24182;&#20351;&#29992;ChatGPT&#29983;&#25104;&#20102;15,000&#20010;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#20197;&#35814;&#32454;&#35780;&#20272;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;RM-PRT&#22522;&#20934;&#35780;&#20272;&#27969;&#31243;&#65292;&#35813;&#27969;&#31243;&#25509;&#21463;&#21253;&#21547;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#33258;&#21160;&#36755;&#20986;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the advent of pre-trained large-scale language models (LLMs) like ChatGPT and GPT-4 have significantly advanced the machine's natural language understanding capabilities. This breakthrough has allowed us to seamlessly integrate these open-source LLMs into a unified robot simulator environment to help robots accurately understand and execute human natural language instructions. To this end, in this work, we introduce a realistic robotic manipulation simulator and build a Robotic Manipulation with Progressive Reasoning Tasks (RM-PRT) benchmark on this basis. Specifically, the RM-PRT benchmark builds a new high-fidelity digital twin scene based on Unreal Engine 5, which includes 782 categories, 2023 objects, and 15K natural language instructions generated by ChatGPT for a detailed evaluation of robot manipulation. We propose a general pipeline for the RM-PRT benchmark that takes as input multimodal prompts containing natural language instructions and automatically outputs action
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20998;&#35299;&#31895;&#31961;&#30340;&#20302;&#20998;&#36776;&#29575;&#22320;&#29702;&#21333;&#20803;&#30340;&#32858;&#21512;&#22478;&#24066;&#25968;&#25454;&#26469;&#21512;&#25104;&#32454;&#31890;&#24230;&#65292;&#39640;&#20998;&#36776;&#29575;&#30340;&#22478;&#24066;&#25968;&#25454;&#65292;&#20197;&#22686;&#21152;&#39640;&#24230;&#32858;&#21512;&#30340;&#22478;&#24066;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21644;&#23454;&#29616;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.07292</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Urban Spatiotemporal Data Synthesis via Neural Disaggregation. (arXiv:2306.07292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20998;&#35299;&#31895;&#31961;&#30340;&#20302;&#20998;&#36776;&#29575;&#22320;&#29702;&#21333;&#20803;&#30340;&#32858;&#21512;&#22478;&#24066;&#25968;&#25454;&#26469;&#21512;&#25104;&#32454;&#31890;&#24230;&#65292;&#39640;&#20998;&#36776;&#29575;&#30340;&#22478;&#24066;&#25968;&#25454;&#65292;&#20197;&#22686;&#21152;&#39640;&#24230;&#32858;&#21512;&#30340;&#22478;&#24066;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21644;&#23454;&#29616;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#25968;&#25454;&#30340;&#32454;&#33410;&#32423;&#21035;&#24120;&#24120;&#19982;&#20854;&#25152;&#33021;&#25552;&#20379;&#30340;&#23454;&#38469;&#25928;&#30410;&#21457;&#29983;&#20914;&#31361;&#12290;&#36739;&#19981;&#32454;&#21270;&#30340;&#25968;&#25454;&#21487;&#20197;&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#65292;&#20294;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#29306;&#29298;&#20102;&#24320;&#25918;&#25968;&#25454;&#20419;&#36827;&#36879;&#26126;&#24230;&#21644;&#21327;&#21161;&#30740;&#31350;&#30340;&#25215;&#35834;&#12290;&#31867;&#20284;&#20110;&#22478;&#24066;&#29615;&#22659;&#20013;&#65292;&#39640;&#23618;&#27425;&#22320;&#29702;&#21333;&#20803;&#30340;&#32858;&#21512;&#22478;&#24066;&#25968;&#25454;&#21487;&#33021;&#20250;&#25513;&#30422;&#22478;&#24066;&#21160;&#24577;&#30340;&#24213;&#23618;&#29305;&#24449;&#65292;&#20302;&#32423;&#21035;&#22320;&#29702;&#21333;&#20803;&#30340;&#21464;&#21270;&#21487;&#33021;&#26356;&#20026;&#26126;&#26174;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20998;&#35299;&#31895;&#31961;&#30340;&#20302;&#20998;&#36776;&#29575;&#22320;&#29702;&#21333;&#20803;&#30340;&#32858;&#21512;&#22478;&#24066;&#25968;&#25454;&#65292;&#21512;&#25104;&#32454;&#31890;&#24230;&#65292;&#39640;&#20998;&#36776;&#29575;&#30340;&#22478;&#24066;&#25968;&#25454;&#65292;&#20197;&#22686;&#21152;&#39640;&#24230;&#32858;&#21512;&#30340;&#22478;&#24066;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21644;&#23454;&#29616;&#20215;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#19968;&#20123;&#20256;&#32479;&#20998;&#35299;&#26041;&#27861;&#30340;&#31616;&#21333;&#24615;&#38382;&#39064;-1) &#25105;&#20204;&#23581;&#35797;&#20102;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#24314;&#27169;&#29305;&#24449;&#20043;&#38388;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#31070;&#32463;&#26041;&#27861;&#20063;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The level of granularity of open data often conflicts the benefits it can provide. Less granular data can protect individual privacy, but to certain degrees, sabotage the promise of open data to promote transparency and assist research. Similar in the urban setting, aggregated urban data at high-level geographic units can mask out the underline particularities of city dynamics that may vary at lower areal levels. In this work, we aim to synthesize fine-grained, high resolution urban data, by breaking down aggregated urban data at coarse, low resolution geographic units. The goal is to increase the usability and realize the values as much as possible of highly aggregated urban data. To address the issue of simplicity of some traditional disaggregation methods -- 1) we experimented with numerous neural-based models that are capable of modeling intricate non-linear relationships among features. Neural methods can also leverage both spatial and temporal information concurrently. We showed 
&lt;/p&gt;</description></item><item><title>Ada-NAV&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#36712;&#36857;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#38477;&#20302;&#31574;&#30053;&#38543;&#26426;&#24615;&#30340;&#26041;&#27861;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#20197;&#22312;&#26356;&#30701;&#30340;&#37319;&#26679;&#26102;&#38388;&#20869;&#21462;&#24471;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06192</link><description>&lt;p&gt;
Ada-NAV&#65306;&#29992;&#20110;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#33258;&#36866;&#24212;&#36712;&#36857;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ada-NAV: Adaptive Trajectory-Based Sample Efficient Policy Learning for Robotic Navigation. (arXiv:2306.06192v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06192
&lt;/p&gt;
&lt;p&gt;
Ada-NAV&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#36712;&#36857;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#38477;&#20302;&#31574;&#30053;&#38543;&#26426;&#24615;&#30340;&#26041;&#27861;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#20197;&#22312;&#26356;&#30701;&#30340;&#37319;&#26679;&#26102;&#38388;&#20869;&#21462;&#24471;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#23398;&#20064;&#26426;&#22120;&#20154;&#23548;&#33322;&#31574;&#30053;&#26041;&#38754;&#21313;&#20998;&#26377;&#25928;&#65292;&#20294;&#20854;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#20063;&#21313;&#20998;&#26126;&#26174;&#12290;&#22312;&#31574;&#30053;&#20248;&#21270;&#20013;&#65292;&#36825;&#31181;&#25928;&#29575;&#20302;&#19979;&#37096;&#20998;&#26469;&#33258;&#20110;&#26410;&#33021;&#36866;&#24403;&#22320;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#38750;&#38745;&#24577;&#26102;&#12290;&#20026;&#20102;&#21152;&#20837;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#24179;&#34913;&#20197;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Ada-NAV&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#36712;&#36857;&#38271;&#24230;&#26041;&#26696;&#65292;&#20854;&#20013;&#38271;&#24230;&#38543;&#30528;&#31574;&#30053;&#30340;&#38543;&#26426;&#24615;&#65288;&#29992;&#20854;Shannon&#25110;&#24046;&#20998;&#29109;&#34920;&#31034;&#65289;&#30340;&#20943;&#23567;&#32780;&#22686;&#21152;&#12290;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#36712;&#36857;&#38271;&#24230;&#26041;&#26696;&#30001;&#20110;&#26356;&#39057;&#32321;&#30340;&#26799;&#24230;&#26356;&#26032;&#24378;&#35843;&#20102;&#35757;&#32451;&#24320;&#22987;&#26102;&#30340;&#25506;&#32034;&#65292;&#21518;&#26469;&#21017;&#26356;&#24378;&#35843;&#21033;&#29992;&#12290;&#22312;&#32593;&#26684;&#19990;&#30028;&#65292;&#20223;&#30495;&#26426;&#22120;&#20154;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#34920;&#29616;&#22312;&#24615;&#33021;&#21644;&#37319;&#26679;&#25928;&#29575;&#19978;&#22343;&#20248;&#20110;&#24120;&#25968;&#21644;&#38543;&#26426;&#37319;&#26679;&#30340;&#36712;&#36857;&#38271;&#24230;&#12290;&#22312;&#22266;&#23450;&#30340;&#26679;&#26412;&#39044;&#31639;&#19979;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;Ada-NAV&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;&#39640;&#36798;46&#65285;&#65292;&#37319;&#26679;&#25968;&#37327;&#20943;&#23569;&#20102;&#39640;&#36798;80&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning methods, while effective for learning robotic navigation strategies, are known to be highly sample inefficient. This sample inefficiency comes in part from not suitably balancing the explore-exploit dilemma, especially in the presence of non-stationarity, during policy optimization. To incorporate a balance of exploration-exploitation for sample efficiency, we propose Ada-NAV, an adaptive trajectory length scheme where the length grows as a policy's randomness, represented by its Shannon or differential entropy, decreases. Our adaptive trajectory length scheme emphasizes exploration at the beginning of training due to more frequent gradient updates and emphasizes exploitation later on with longer trajectories. In gridworld, simulated robotic environments, and real-world robotic experiments, we demonstrate the merits of the approach over constant and randomly sampled trajectory lengths in terms of performance and sample efficiency. For a fixed sample budget, Ada-N
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#31639;&#27861;(&#22914;Adagrad&#21644;Adam)&#30340;&#23398;&#20064;&#29575;&#20272;&#35745;&#26041;&#27861;Prodigy&#21644;Resetting&#65292;&#21487;&#20197;&#24555;&#36895;&#19988;&#27491;&#30830;&#22320;&#20272;&#35745;&#21040;&#36798;&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#36317;&#31163;D&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;D-Adaptation&#24182;&#21487;&#36798;&#21040;&#25163;&#21160;&#35843;&#25972;Adam&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.06101</link><description>&lt;p&gt;
Prodigy: &#19968;&#31181;&#24555;&#36895;&#33258;&#36866;&#24212;&#38646;&#21442;&#25968;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prodigy: An Expeditiously Adaptive Parameter-Free Learner. (arXiv:2306.06101v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#31639;&#27861;(&#22914;Adagrad&#21644;Adam)&#30340;&#23398;&#20064;&#29575;&#20272;&#35745;&#26041;&#27861;Prodigy&#21644;Resetting&#65292;&#21487;&#20197;&#24555;&#36895;&#19988;&#27491;&#30830;&#22320;&#20272;&#35745;&#21040;&#36798;&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#36317;&#31163;D&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;D-Adaptation&#24182;&#21487;&#36798;&#21040;&#25163;&#21160;&#35843;&#25972;Adam&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#33258;&#36866;&#24212;&#31639;&#27861;(&#22914;Adagrad&#21644;Adam)&#20013;&#30340;&#23398;&#20064;&#29575;&#20272;&#35745;&#38382;&#39064;&#65292;&#25551;&#36848;&#20102;&#20004;&#31181;&#25216;&#26415;Prodigy&#21644;Resetting&#65292;&#21487;&#20197;&#35777;&#26126;&#22320;&#20272;&#35745;&#21040;&#36798;&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#36317;&#31163;D&#65292;&#20197;&#20415;&#26368;&#20248;&#35774;&#32622;&#23398;&#20064;&#29575;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#26159;&#22522;&#20110;&#23398;&#20064;&#29575;&#33258;&#30001;&#30340;D-Adaptation&#26041;&#27861;&#30340;&#20462;&#25913;&#65292;&#24182;&#36890;&#36807;$O(\sqrt{\log(D/d_0)})$&#30340;&#22240;&#23376;&#25552;&#39640;&#20102;D-Adaptation&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20854;&#20013;$d_0$&#26159;$D$&#30340;&#21021;&#22987;&#20272;&#35745;&#20540;&#12290;&#25105;&#20204;&#22312;12&#20010;&#24120;&#35265;&#30340;&#36923;&#36753;&#22238;&#24402;&#22522;&#20934;&#25968;&#25454;&#38598;&#12289;&#22312;CIFAR10&#19978;&#35757;&#32451;&#30340;VGG11&#21644;ResNet-50&#12289;&#22312;Imagenet&#19978;&#35757;&#32451;&#30340;ViT&#12289;&#22312;IWSLT14&#19978;&#35757;&#32451;&#30340;LSTM&#12289;&#22312;Criteo&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;DLRM&#12289;&#22312;Knee MRI&#25968;&#25454;&#38598;&#19978;&#30340;VarNet&#65292;&#20197;&#21450;&#22312;BookWiki&#19978;&#35757;&#32451;&#30340;RoBERTa&#21644;GPT transformer&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;D-Adaptation&#65292;&#24182;&#36798;&#21040;&#25163;&#21160;&#35843;&#25972;Adam&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of estimating the learning rate in adaptive methods, such as Adagrad and Adam. We describe two techniques, Prodigy and Resetting, to provably estimate the distance to the solution $D$, which is needed to set the learning rate optimally. Our techniques are modifications of the D-Adaptation method for learning-rate-free learning. Our methods improve upon the convergence rate of D-Adaptation by a factor of $O(\sqrt{\log(D/d_0)})$, where $d_0$ is the initial estimate of $D$. We test our methods on 12 common logistic-regression benchmark datasets, VGG11 and ResNet-50 training on CIFAR10, ViT training on Imagenet, LSTM training on IWSLT14, DLRM training on Criteo dataset, VarNet on Knee MRI dataset, as well as RoBERTa and GPT transformer training on BookWiki. Our experimental results show that our approaches consistently outperform D-Adaptation and reach test accuracy values close to that of hand-tuned Adam.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;k&#36817;&#37051;&#23398;&#20064;&#35268;&#21017;&#20013;&#30340;&#26222;&#36941;&#19968;&#33268;&#24615;&#65292;&#21457;&#29616;&#22312;&#21487;&#20998;&#24230;&#37327;&#31354;&#38388;&#20013;&#65292;&#35813;&#35268;&#21017;&#22312;Nagata&#32500;&#24230;&#19979;&#30340;sigma&#26377;&#38480;&#32500;&#24230;&#30340;&#31354;&#38388;&#20013;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#65292;&#22312;&#38750;&#38463;&#22522;&#31859;&#24503;&#24230;&#37327;&#31354;&#38388;&#20013;&#26159;&#24378;&#26222;&#36941;&#19968;&#33268;&#30340;&#65292;&#27492;&#35268;&#21017;&#22312;&#20855;&#26377;de Groot&#26377;&#38480;&#32500;&#24230;&#24847;&#20041;&#19979;&#30340;&#24230;&#37327;&#31354;&#38388;&#21644;Heisenberg&#32676;&#20013;&#20063;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.17282</link><description>&lt;p&gt;
&#24230;&#37327;&#31354;&#38388;&#21644;Nagata&#32500;&#24230;&#20013;k-NN&#35268;&#21017;&#30340;&#26222;&#36941;&#19968;&#33268;&#24615;(II)
&lt;/p&gt;
&lt;p&gt;
Universal consistency of the $k$-NN rule in metric spaces and Nagata dimension. II. (arXiv:2305.17282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;k&#36817;&#37051;&#23398;&#20064;&#35268;&#21017;&#20013;&#30340;&#26222;&#36941;&#19968;&#33268;&#24615;&#65292;&#21457;&#29616;&#22312;&#21487;&#20998;&#24230;&#37327;&#31354;&#38388;&#20013;&#65292;&#35813;&#35268;&#21017;&#22312;Nagata&#32500;&#24230;&#19979;&#30340;sigma&#26377;&#38480;&#32500;&#24230;&#30340;&#31354;&#38388;&#20013;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#65292;&#22312;&#38750;&#38463;&#22522;&#31859;&#24503;&#24230;&#37327;&#31354;&#38388;&#20013;&#26159;&#24378;&#26222;&#36941;&#19968;&#33268;&#30340;&#65292;&#27492;&#35268;&#21017;&#22312;&#20855;&#26377;de Groot&#26377;&#38480;&#32500;&#24230;&#24847;&#20041;&#19979;&#30340;&#24230;&#37327;&#31354;&#38388;&#21644;Heisenberg&#32676;&#20013;&#20063;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32487;&#32493;&#22312;&#21487;&#20998;&#24230;&#37327;&#31354;&#38388;&#20013;&#30740;&#31350;k&#36817;&#37051;&#23398;&#20064;&#35268;&#21017;&#12290;&#30001;&#20110;C\'erou&#21644;Guyader(2006)&#20197;&#21450;Preiss(1983)&#30340;&#32467;&#26524;&#65292;&#24050;&#30693;&#35813;&#35268;&#21017;&#22312;&#27599;&#20010;Nagata&#24847;&#20041;&#19979;&#30340;sigma&#26377;&#38480;&#32500;&#24230;&#30340;&#24230;&#37327;&#31354;&#38388;X&#20013;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#12290;&#22312;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26080;&#24179;&#23616;&#24773;&#20917;&#19979;&#27492;&#35268;&#21017;&#22312;&#36825;&#26679;&#30340;&#31354;&#38388;&#20013;&#26159;&#24378;&#26222;&#36941;&#19968;&#33268;&#30340;&#12290;&#22312;Devroye&#65292;Gy\"{o}rfi&#65292;Krzy\.{z}ak&#21644;Lugosi&#65288;1994&#65289;&#22312;&#27431;&#20960;&#37324;&#24471;&#35774;&#32622;&#20013;&#24212;&#29992;&#30340;&#25171;&#30772;&#24179;&#23616;&#31574;&#30053;&#19979;&#65292;&#25105;&#20204;&#35774;&#27861;&#22312;&#38750;&#38463;&#22522;&#31859;&#24503;&#24230;&#37327;&#31354;&#38388;&#65288;&#21363;Nagata&#32500;&#24230;&#20026;&#38646;&#30340;&#31354;&#38388;&#65289;&#20013;&#23637;&#31034;&#20102;&#24378;&#26222;&#36941;&#19968;&#33268;&#24615;&#12290;&#32467;&#21512;C\'erou&#21644;Guyader&#30340;&#23450;&#29702;&#21644;Assouad&#21644;Quentin de Gromard (2006)&#30340;&#32467;&#26524;&#65292;&#21487;&#20197;&#25512;&#20986;$k$-NN&#35268;&#21017;&#22312;&#20855;&#26377;de Groot&#26377;&#38480;&#32500;&#24230;&#24847;&#20041;&#19979;&#30340;&#24230;&#37327;&#31354;&#38388;&#20013;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;$k$-NN&#35268;&#21017;&#22312;Heisenberg&#32676;&#20013;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#65292;&#32780;&#35813;&#32676;&#24182;&#38750;sigma&#26377;&#38480;&#32500;&#24230;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We continue to investigate the $k$ nearest neighbour learning rule in separable metric spaces. Thanks to the results of C\'erou and Guyader (2006) and Preiss (1983), this rule is known to be universally consistent in every metric space $X$ that is sigma-finite dimensional in the sense of Nagata. Here we show that the rule is strongly universally consistent in such spaces in the absence of ties. Under the tie-breaking strategy applied by Devroye, Gy\"{o}rfi, Krzy\.{z}ak, and Lugosi (1994) in the Euclidean setting, we manage to show the strong universal consistency in non-Archimedian metric spaces (that is, those of Nagata dimension zero). Combining the theorem of C\'erou and Guyader with results of Assouad and Quentin de Gromard (2006), one deduces that the $k$-NN rule is universally consistent in metric spaces having finite dimension in the sense of de Groot. In particular, the $k$-NN rule is universally consistent in the Heisenberg group which is not sigma-finite dimensional in the se
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#20986;&#29616;&#30340;&#25991;&#21270;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#29616;&#35937;&#65292;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#31561;&#20843;&#20010;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#23545;&#20110;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2305.14456</link><description>&lt;p&gt;
&#22312;&#31048;&#31095;&#20043;&#21518;&#21917;&#21860;&#37202;&#65311;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Having Beer after Prayer? Measuring Cultural Bias in Large Language Models. (arXiv:2305.14456v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14456
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#20986;&#29616;&#30340;&#25991;&#21270;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#29616;&#35937;&#65292;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#31561;&#20843;&#20010;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#23545;&#20110;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23384;&#22312;&#25991;&#21270;&#20559;&#35265;&#65311;&#35821;&#35328;&#27169;&#22411;&#31526;&#21512;&#25152;&#26381;&#21153;&#31038;&#21306;&#30340;&#25991;&#21270;&#22240;&#32032;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#34920;&#26126;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#20559;&#35265;&#65292;&#20542;&#21521;&#20110;&#20135;&#29983;&#35199;&#26041;&#25991;&#21270;&#30456;&#20851;&#20869;&#23481;&#32780;&#38750;&#38463;&#25289;&#20271;&#25991;&#21270;&#30456;&#20851;&#20869;&#23481;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20174;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#19978;&#25910;&#38598;&#30340;&#33258;&#28982;&#20986;&#29616;&#30340;&#19978;&#19979;&#25991;&#21644;&#22522;&#20110;&#21487;&#33021;&#24615;&#35780;&#20998;&#30340;&#25351;&#26631;&#26469;&#37327;&#21270;&#36825;&#31181;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#38463;&#25289;&#20271;&#35821;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#35199;&#26041;&#25991;&#21270;&#20559;&#35265;&#65292;&#21253;&#25324;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#12290;&#24403;&#36755;&#20837;&#30340;&#38463;&#25289;&#20271;&#35821;&#21477;&#23376;&#36234;&#25509;&#36817;&#33521;&#35821;&#26102;&#65292;&#27169;&#22411;&#20063;&#26356;&#23481;&#26131;&#34920;&#29616;&#20986;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#20154;&#20204;&#23545;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#35774;&#35745;&#20013;&#24212;&#26356;&#22810;&#32771;&#34385;&#25991;&#21270;&#22240;&#32032;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are language models culturally biased? It is important that language models conform to the cultural aspects of the communities they serve. However, we show in this paper that language models suffer from a significant bias towards Western culture when handling and generating text in Arabic, often preferring, and producing Western-fitting content as opposed to the relevant Arab content. We quantify this bias through a likelihood scoring-based metric using naturally occurring contexts that we collect from online social media. Our experiments reveal that both Arabic monolingual and multilingual models exhibit bias towards Western culture in eight different cultural aspects: person names, food, clothing, location, literature, beverage, religion, and sports. Models also tend to exhibit more bias when prompted with Arabic sentences that are more linguistically aligned with English. These findings raise concerns about the cultural relevance of current language models. Our analyses show that pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DermSynth3D&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#21487;&#24494;&#20998;&#28210;&#26579;&#22120;&#23558;&#30382;&#32932;&#30149;&#21464;&#27169;&#24335;&#28151;&#21512;&#21040;&#20154;&#20307;&#19977;&#32500;&#32441;&#29702;&#32593;&#26684;&#19978;&#24182;&#29983;&#25104;&#36924;&#30495;&#30340;&#20108;&#32500;&#30382;&#32932;&#38236;&#20687;&#22270;&#20687;&#65292;&#21516;&#26102;&#25552;&#20379;&#23545;&#24212;&#30340;&#23494;&#38598;&#27880;&#37322;&#20197;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2305.12621</link><description>&lt;p&gt;
DermSynth3D&#65306;&#37326;&#22806;&#27880;&#37322;&#30382;&#32932;&#31185;&#22270;&#20687;&#30340;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
DermSynth3D: Synthesis of in-the-wild Annotated Dermatology Images. (arXiv:2305.12621v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12621
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DermSynth3D&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#21487;&#24494;&#20998;&#28210;&#26579;&#22120;&#23558;&#30382;&#32932;&#30149;&#21464;&#27169;&#24335;&#28151;&#21512;&#21040;&#20154;&#20307;&#19977;&#32500;&#32441;&#29702;&#32593;&#26684;&#19978;&#24182;&#29983;&#25104;&#36924;&#30495;&#30340;&#20108;&#32500;&#30382;&#32932;&#38236;&#20687;&#22270;&#20687;&#65292;&#21516;&#26102;&#25552;&#20379;&#23545;&#24212;&#30340;&#23494;&#38598;&#27880;&#37322;&#20197;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#30382;&#32932;&#31185;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#23384;&#22312;&#26174;&#30528;&#38480;&#21046;&#65292;&#21253;&#25324;&#26679;&#26412;&#22270;&#20687;&#25968;&#37327;&#36739;&#23569;&#12289;&#30142;&#30149;&#26465;&#20214;&#26377;&#38480;&#12289;&#27880;&#37322;&#19981;&#36275;&#20197;&#21450;&#38750;&#26631;&#20934;&#21270;&#22270;&#20687;&#37319;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DermSynth3D&#30340;&#26032;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#21487;&#24494;&#20998;&#28210;&#26579;&#22120;&#23558;&#30382;&#32932;&#30149;&#21464;&#27169;&#24335;&#28151;&#21512;&#21040;&#20154;&#20307;&#30340;&#19977;&#32500;&#32441;&#29702;&#32593;&#26684;&#19978;&#65292;&#24182;&#22312;&#21508;&#31181;&#32972;&#26223;&#22330;&#26223;&#19979;&#37319;&#29992;&#19981;&#21516;&#35270;&#35282;&#21644;&#20809;&#29031;&#26465;&#20214;&#29983;&#25104;&#20108;&#32500;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36981;&#24490;&#33258;&#19978;&#32780;&#19979;&#30340;&#35268;&#21017;&#65292;&#38480;&#21046;&#28151;&#21512;&#21644;&#28210;&#26579;&#36807;&#31243;&#65292;&#20197;&#21019;&#24314;&#20855;&#26377;&#37326;&#22806;&#29031;&#29255;&#24863;&#30340;&#30382;&#32932;&#26465;&#20214;&#30340;&#20108;&#32500;&#22270;&#20687;&#65292;&#30830;&#20445;&#26356;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#12290;&#35813;&#26694;&#26550;&#29983;&#25104;&#36924;&#30495;&#30340;&#20108;&#32500;&#30382;&#32932;&#38236;&#20687;&#22270;&#20687;&#65292;&#24182;&#29983;&#25104;&#23545;&#30382;&#32932;&#12289;&#30382;&#32932;&#29366;&#20917;&#12289;&#36523;&#20307;&#37096;&#20301;&#21644;&#22836;&#21457;&#21306;&#22495;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#23545;&#24212;&#23494;&#38598;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning (DL) has shown great potential in the field of dermatological image analysis. However, existing datasets in this domain have significant limitations, including a small number of image samples, limited disease conditions, insufficient annotations, and non-standardized image acquisitions. To address these shortcomings, we propose a novel framework called DermSynth3D. DermSynth3D blends skin disease patterns onto 3D textured meshes of human subjects using a differentiable renderer and generates 2D images from various camera viewpoints under chosen lighting conditions in diverse background scenes. Our method adheres to top-down rules that constrain the blending and rendering process to create 2D images with skin conditions that mimic in-the-wild acquisitions, ensuring more meaningful results. The framework generates photo-realistic 2D dermoscopy images and the corresponding dense annotations for semantic segmentation of the skin, skin conditions, body parts, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#37030;&#22522;&#30784;&#27169;&#22411;&#65288;FFMs&#65289;&#30340;&#27010;&#24565;&#65292;&#32467;&#21512;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#21487;&#23454;&#29616;&#36328;&#22810;&#20010;&#26426;&#26500;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#21327;&#20316;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.11414</link><description>&lt;p&gt;
&#32852;&#37030;&#22522;&#30784;&#27169;&#22411;&#65306;&#29992;&#20110;&#22823;&#27169;&#22411;&#30340;&#38544;&#31169;&#20445;&#25252;&#21327;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models. (arXiv:2305.11414v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#37030;&#22522;&#30784;&#27169;&#22411;&#65288;FFMs&#65289;&#30340;&#27010;&#24565;&#65292;&#32467;&#21512;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#21487;&#23454;&#29616;&#36328;&#22810;&#20010;&#26426;&#26500;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#21327;&#20316;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22914;BERT&#12289;GPT&#12289;ViT&#21644;CLIP&#65292;&#20294;&#20854;&#20248;&#21270;&#36890;&#24120;&#38656;&#35201;&#35775;&#38382;&#25935;&#24863;&#25968;&#25454;&#65292;&#24341;&#21457;&#38544;&#31169;&#38382;&#39064;&#24182;&#38480;&#21046;&#20854;&#36866;&#29992;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#37030;&#22522;&#30784;&#27169;&#22411;&#65288;FFMs&#65289;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#21487;&#23454;&#29616;&#36328;&#22810;&#20010;&#26426;&#26500;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#21327;&#20316;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation Models (FMs), such as BERT, GPT, ViT, and CLIP, have demonstrated remarkable success in a wide range of applications, driven by their ability to leverage vast amounts of data for pre-training. However, optimizing FMs often requires access to sensitive data, raising privacy concerns and limiting their applicability in certain domains. In this paper, we introduce the concept of Federated Foundation Models (FFMs), a novel approach that combines the benefits of FMs and Federated Learning (FL) to enable privacy-preserving and collaborative learning across multiple institutions. We discuss the potential benefits and challenges of integrating FL into the lifespan of FMs, covering pre-training, fine-tuning, and application. We further provide formal definitions of FFM tasks, including FFM pre-training, FFM fine-tuning, and federated prompt engineering, allowing for more personalized and context-aware models while maintaining data privacy. Moreover, we explore the possibility of cont
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Riemannian&#22810;&#31867;Logistic&#22238;&#24402;&#65288;RMLR&#65289;&#20998;&#31867;&#22120;&#29992;&#20110;&#23398;&#20064;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20869;&#22312;&#25429;&#25417;SPD&#27969;&#24418;&#20960;&#20309;&#30340;&#26041;&#24335;&#65292;&#22312;&#27969;&#34892;&#30340;SPD&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11288</link><description>&lt;p&gt;
Riemannian&#22810;&#31867;Logistic&#22238;&#24402;&#29992;&#20110;SPD&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Riemannian Multiclass Logistics Regression for SPD Neural Networks. (arXiv:2305.11288v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Riemannian&#22810;&#31867;Logistic&#22238;&#24402;&#65288;RMLR&#65289;&#20998;&#31867;&#22120;&#29992;&#20110;&#23398;&#20064;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20869;&#22312;&#25429;&#25417;SPD&#27969;&#24418;&#20960;&#20309;&#30340;&#26041;&#24335;&#65292;&#22312;&#27969;&#34892;&#30340;SPD&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#23398;&#20064;&#23545;&#31216;&#27491;&#23450;&#65288;SPD&#65289;&#30697;&#38453;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;SPD&#32593;&#32476;&#20351;&#29992;&#20256;&#32479;&#30340;&#27431;&#20960;&#37324;&#24471;&#20998;&#31867;&#22120;&#22312;&#36817;&#20284;&#31354;&#38388;&#19978;&#32780;&#19981;&#26159;&#22312;&#20934;&#30830;&#25429;&#25417;SPD&#27969;&#24418;&#20960;&#20309;&#30340;&#20869;&#22312;&#20998;&#31867;&#22120;&#19978;&#12290;&#21463;&#36229;&#20960;&#20309;&#31070;&#32463;&#32593;&#32476;&#65288;HNN&#65289;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Riemannian&#22810;&#31867;Logistic&#22238;&#24402;&#65288;RMLR&#65289;&#29992;&#20110;SPD&#32593;&#32476;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;SPD&#27969;&#24418;&#19978;&#19968;&#26063;Riemannian&#24230;&#37327;&#30340;&#36890;&#29992;&#32479;&#19968;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#20102;&#29305;&#23450;&#30340; $\orth{n}$-&#19981;&#21464;&#30340;Log-Euclidean Metrics&#36866;&#29992;&#20110;SPD&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#29616;&#26377;SPD&#32593;&#32476;&#20013;&#26368;&#27969;&#34892;&#30340;&#20998;&#31867;&#22120;&#20316;&#20026;&#25105;&#20204;&#26694;&#26550;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;&#22312;&#27969;&#34892;&#30340;SPD&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#20998;&#31867;&#22120;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks for learning symmetric positive definite (SPD) matrices are gaining increasing attention in machine learning. Despite the significant progress, most existing SPD networks use traditional Euclidean classifiers on approximated spaces rather than intrinsic classifiers that accurately capture the geometry of SPD manifolds. Inspired by the success of hyperbolic neural networks (HNNs), we propose Riemannian multiclass logistics regression (RMLR) for SPD networks. We introduce a general unified framework for a family of Riemannian metrics on SPD manifolds and showcase the specific $\orth{n}$-invariant Log-Euclidean Metrics for SPD networks. Moreover, we encompass the most popular classifier in existing SPD networks as a special case of our framework. Extensive experiments on popular SPD learning benchmarks demonstrate the superiority of our classifiers.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#31456;&#22312;&#34394;&#20551;&#26032;&#38395;&#21644;&#20027;&#27969;&#26032;&#38395;&#32593;&#31449;&#30340;&#26222;&#21450;&#31243;&#24230;&#65292;&#21457;&#29616;&#34394;&#20551;&#26032;&#38395;&#32593;&#31449;&#19978;&#21512;&#25104;&#25991;&#31456;&#30340;&#20351;&#29992;&#36895;&#24230;&#27604;&#20027;&#27969;&#32593;&#31449;&#19978;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2305.09820</link><description>&lt;p&gt;
&#26426;&#22120;&#21046;&#36896;&#30340;&#23186;&#20307;&#65306;&#30417;&#27979;&#34394;&#20551;&#26032;&#38395;&#21644;&#20027;&#27969;&#26032;&#38395;&#32593;&#31449;&#19978;&#26426;&#22120;&#29983;&#25104;&#25991;&#31456;&#30340;&#21160;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine-Made Media: Monitoring the Mobilization of Machine-Generated Articles on Misinformation and Mainstream News Websites. (arXiv:2305.09820v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09820
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#31456;&#22312;&#34394;&#20551;&#26032;&#38395;&#21644;&#20027;&#27969;&#26032;&#38395;&#32593;&#31449;&#30340;&#26222;&#21450;&#31243;&#24230;&#65292;&#21457;&#29616;&#34394;&#20551;&#26032;&#38395;&#32593;&#31449;&#19978;&#21512;&#25104;&#25991;&#31456;&#30340;&#20351;&#29992;&#36895;&#24230;&#27604;&#20027;&#27969;&#32593;&#31449;&#19978;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20687;ChatGPT&#36825;&#26679;&#30340;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26085;&#30410;&#27969;&#34892;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#26032;&#38395;&#32593;&#31449;&#24320;&#22987;&#21033;&#29992;&#23427;&#20204;&#29983;&#25104;&#25991;&#31456;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#33021;&#22312;&#22768;&#35465;&#33391;&#22909;&#30340;&#32593;&#31449;&#19978;&#20135;&#29983;&#20107;&#23454;&#19981;&#20934;&#30830;&#30340;&#25991;&#31456;&#65292;&#32780;&#19988;&#19981;&#33391;&#26032;&#38395;&#32593;&#31449;&#20063;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;LLM&#25209;&#37327;&#29983;&#20135;&#34394;&#20551;&#20449;&#24687;&#12290;&#20026;&#20102;&#24320;&#22987;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39318;&#20010;&#22823;&#35268;&#27169;&#30740;&#31350;&#21512;&#25104;&#25991;&#31456;&#22312;&#32447;&#26032;&#38395;&#23186;&#20307;&#20013;&#26222;&#21450;&#29575;&#30340;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;DeBERTa&#30340;&#21512;&#25104;&#26032;&#38395;&#26816;&#27979;&#22120;&#65292;&#24182;&#23545;3074&#20010;&#34394;&#20551;&#26032;&#38395;&#21644;&#20027;&#27969;&#26032;&#38395;&#32593;&#31449;&#30340;&#36229;&#36807;1291&#19975;&#31687;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;2022&#24180;1&#26376;1&#26085;&#33267;2023&#24180;4&#26376;1&#26085;&#26399;&#38388;&#65292;&#21512;&#25104;&#26032;&#38395;&#25991;&#31456;&#30340;&#30456;&#23545;&#25968;&#37327;&#22312;&#20027;&#27969;&#32593;&#31449;&#19978;&#22686;&#21152;&#20102;79.4&#65285;&#65292;&#32780;&#22312;&#34394;&#20551;&#20449;&#24687;&#32593;&#31449;&#19978;&#22686;&#21152;&#20102;342&#65285;&#12290;&#20998;&#26512;ChatGPT&#21457;&#24067;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#20013;&#26029;&#26102;&#38388;&#24207;&#21015;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#23427;&#30340;&#21457;&#24067;&#23548;&#33268;&#21512;&#25104;&#25991;&#31456;&#30340;&#20351;&#29992;&#26174;&#33879;&#22686;&#21152;&#65292;&#20294;&#34394;&#20551;&#20449;&#24687;&#32593;&#31449;&#19978;&#30340;&#21512;&#25104;&#25991;&#31456;&#20351;&#29992;&#36895;&#24230;&#27604;&#20027;&#27969;&#32593;&#31449;&#19978;&#30340;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing popularity of generative large language models (LLMs) like ChatGPT, an increasing number of news websites have begun utilizing them to generate articles. However, not only can these language models produce factually inaccurate articles on reputable websites but disreputable news sites can utilize these LLMs to mass produce misinformation. To begin to understand this phenomenon, we present one of the first large-scale studies of the prevalence of synthetic articles within online news media. To do this, we train a DeBERTa-based synthetic news detector and classify over 12.91 million articles from 3,074 misinformation and mainstream news websites. We find that between January 1, 2022 and April 1, 2023, the relative number of synthetic news articles increased by 79.4% on mainstream websites while increasing by 342% on misinformation sites. Analyzing the impact of the release of ChatGPT using an interrupted-time-series, we show that while its release resulted in a marked
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;&#25945;&#24072;&#27169;&#22411;&#20256;&#36882;&#30693;&#35782;&#30340;&#26435;&#37325;&#32487;&#25215;&#33976;&#39311;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#23545;&#40784;&#25439;&#22833;&#23601;&#21487;&#20197;&#35757;&#32451;&#20986;&#19968;&#20010;&#32039;&#20945;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;GLUE&#21644;SQuAD&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;KD&#30340;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.09098</link><description>&lt;p&gt;
&#20219;&#21153;&#26080;&#20851;BERT&#21387;&#32553;&#30340;&#26435;&#37325;&#32487;&#25215;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Weight-Inherited Distillation for Task-Agnostic BERT Compression. (arXiv:2305.09098v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;&#25945;&#24072;&#27169;&#22411;&#20256;&#36882;&#30693;&#35782;&#30340;&#26435;&#37325;&#32487;&#25215;&#33976;&#39311;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#23545;&#40784;&#25439;&#22833;&#23601;&#21487;&#20197;&#35757;&#32451;&#20986;&#19968;&#20010;&#32039;&#20945;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;GLUE&#21644;SQuAD&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;KD&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26159;&#21387;&#32553;BERT&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#20043;&#21069;&#30340;KD&#26041;&#27861;&#20391;&#37325;&#20110;&#20026;&#23398;&#29983;&#27169;&#22411;&#35774;&#35745;&#39069;&#22806;&#30340;&#23545;&#40784;&#25439;&#22833;&#65292;&#20197;&#27169;&#20223;&#25945;&#24072;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#36825;&#20123;&#26041;&#27861;&#20197;&#38388;&#25509;&#30340;&#26041;&#24335;&#20256;&#36882;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26435;&#37325;&#32487;&#25215;&#33976;&#39311;&#65288;WID&#65289;&#26041;&#27861;&#65292;&#30452;&#25509;&#20174;&#25945;&#24072;&#27169;&#22411;&#20256;&#36882;&#30693;&#35782;&#12290;WID&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#23545;&#40784;&#25439;&#22833;&#65292;&#36890;&#36807;&#32487;&#25215;&#26435;&#37325;&#26469;&#35757;&#32451;&#19968;&#20010;&#32039;&#20945;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#30693;&#35782;&#33976;&#39311;&#30340;&#26032;&#35270;&#35282;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#34892;&#21387;&#32553;&#22120;&#21644;&#21015;&#21387;&#32553;&#22120;&#35774;&#35745;&#20026;&#26144;&#23556;&#65292;&#28982;&#21518;&#36890;&#36807;&#32467;&#26500;&#37325;&#21442;&#25968;&#21270;&#21387;&#32553;&#26435;&#37325;&#12290;&#22312;GLUE&#21644;SQuAD&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;WID&#20248;&#20110;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;KD&#30340;&#22522;&#32447;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;WID&#20063;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#27880;&#24847;&#21147;&#20998;&#24067;&#23545;&#40784;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#25945;&#24072;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation (KD) is a predominant approach for BERT compression. Previous KD-based methods focus on designing extra alignment losses for the student model to mimic the behavior of the teacher model. These methods transfer the knowledge in an indirect way. In this paper, we propose a novel Weight-Inherited Distillation (WID), which directly transfers knowledge from the teacher. WID does not require any additional alignment loss and trains a compact student by inheriting the weights, showing a new perspective of knowledge distillation. Specifically, we design the row compactors and column compactors as mappings and then compress the weights via structural re-parameterization. Experimental results on the GLUE and SQuAD benchmarks show that WID outperforms previous state-of-the-art KD-based baselines. Further analysis indicates that WID can also learn the attention patterns from the teacher model without any alignment loss on attention distributions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24066;&#22330;&#20449;&#24687;&#32534;&#30721;&#20026;&#22270;&#20687;&#24182;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30701;&#26399;&#23454;&#29616;&#27874;&#21160;&#29575;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#19982;&#20854;&#20182;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.02472</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#35746;&#21333;&#27969;&#35757;&#32451;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30701;&#26399;&#27874;&#21160;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Short-Term Volatility Prediction Using Deep CNNs Trained on Order Flow. (arXiv:2304.02472v1 [q-fin.RM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24066;&#22330;&#20449;&#24687;&#32534;&#30721;&#20026;&#22270;&#20687;&#24182;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30701;&#26399;&#23454;&#29616;&#27874;&#21160;&#29575;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#19982;&#20854;&#20182;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26032;&#20852;&#30340;&#36164;&#20135;&#31867;&#21035;&#65292;&#21152;&#23494;&#36135;&#24065;&#30456;&#27604;&#20256;&#32479;&#30340;&#32929;&#31080;&#24066;&#22330;&#26126;&#26174;&#26356;&#20855;&#27874;&#21160;&#24615;&#12290;&#30001;&#20110;&#20854;&#22823;&#22810;&#25968;&#26102;&#20505;&#26159;&#26080;&#30417;&#31649;&#30340;&#65292;&#27969;&#21160;&#24615;&#36890;&#24120;&#36739;&#20302;&#65292;&#21152;&#23494;&#36164;&#20135;&#30340;&#20215;&#26684;&#22312;&#20960;&#20998;&#38047;&#20869;&#23601;&#21487;&#20986;&#29616;&#26174;&#33879;&#21464;&#21160;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#24040;&#39069;&#25439;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#23558;&#24066;&#22330;&#20449;&#24687;&#32534;&#30721;&#25104;&#22270;&#20687;&#24182;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30701;&#26399;&#23454;&#29616;&#27874;&#21160;&#29575;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#32534;&#30721;&#21644;&#30456;&#24212;&#30340;&#27169;&#22411;&#30340;&#34920;&#29616;&#19982;&#20854;&#20182;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#39044;&#27979;&#27169;&#22411;&#30340;&#24066;&#22330;&#25968;&#25454;&#34920;&#31034;&#26041;&#27861;&#26377;&#28508;&#21147;&#26356;&#22909;&#22320;&#25429;&#25417;&#24066;&#22330;&#21160;&#24577;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#27874;&#21160;&#29575;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a newly emerged asset class, cryptocurrency is evidently more volatile compared to the traditional equity markets. Due to its mostly unregulated nature, and often low liquidity, the price of crypto assets can sustain a significant change within minutes that in turn might result in considerable losses. In this paper, we employ an approach for encoding market information into images and making predictions of short-term realized volatility by employing Convolutional Neural Networks. We then compare the performance of the proposed encoding and corresponding model with other benchmark models. The experimental results demonstrate that this representation of market data with a Convolutional Neural Network as a predictive model has the potential to better capture the market dynamics and a better volatility prediction.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#20056;&#27861;&#26356;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#20132;&#26367;&#20027;&#20307;&#21270;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#36739;&#24555;&#30340;&#27714;&#35299;&#21644;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#36924;&#36817;&#31934;&#24230;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17992</link><description>&lt;p&gt;
&#19968;&#31181;&#24555;&#36895;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#20056;&#27861;&#26356;&#26032;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A fast Multiplicative Updates algorithm for Non-negative Matrix Factorization. (arXiv:2303.17992v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17992
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#20056;&#27861;&#26356;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#20132;&#26367;&#20027;&#20307;&#21270;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#36739;&#24555;&#30340;&#27714;&#35299;&#21644;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#36924;&#36817;&#31934;&#24230;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#21487;&#20197;&#23558;&#25968;&#25454;&#30697;&#38453;&#20998;&#35299;&#20026;&#26131;&#20110;&#35299;&#37322;&#30340;&#37096;&#20998;&#12290;&#36807;&#21435;&#19977;&#21313;&#24180;&#20013;&#20986;&#29616;&#20102;&#35768;&#22810;&#31639;&#27861;&#65292;&#20854;&#20013;&#19968;&#31181;&#24191;&#20026;&#20154;&#30693;&#30340;&#26041;&#27861;&#26159;&#30001;&#26446;&#39134;&#39134;&#21644;&#25165;&#21326;&#27178;&#28322;&#20110;2002&#24180;&#25552;&#20986;&#30340;&#20056;&#27861;&#26356;&#26032;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#35768;&#22810;&#39046;&#22495;&#34920;&#29616;&#33391;&#22909;&#65292;&#20855;&#26377;&#31616;&#21333;&#26131;&#23454;&#29616;&#21644;&#21487;&#36866;&#24212;&#27969;&#34892;&#21464;&#20307;&#30340;&#29305;&#28857;&#12290;&#26412;&#25991;&#24314;&#35758;&#36890;&#36807;&#20026;&#27599;&#20010;&#26367;&#20195;&#23376;&#38382;&#39064;&#21046;&#20316;&#26356;&#32039;&#23494;&#30340;Hessian&#30697;&#38453;&#30340;&#19978;&#38480;&#26469;&#25913;&#36827;&#20056;&#27861;&#26356;&#26032;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#35270;&#20026;&#20132;&#26367;&#20027;&#20307;&#21270;&#26368;&#23567;&#21270;&#31639;&#27861;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#23454;&#36341;&#20013;&#35266;&#23519;&#21040;&#65292;&#25152;&#25552;&#20986;&#30340;fastMU&#31639;&#27861;&#36890;&#24120;&#27604;&#21407;&#22987;&#30340;&#20056;&#27861;&#26356;&#26032;&#31639;&#27861;&#24555;&#25968;&#20493;&#65292;&#21516;&#26102;&#22312;&#36924;&#36817;&#31934;&#24230;&#26041;&#38754;&#23454;&#29616;&#20102;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#25910;&#25947;&#20173;&#28982;&#24471;&#21040;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonnegative Matrix Factorization is an important tool in unsupervised machine learning to decompose a data matrix into a product of parts that are often interpretable. Many algorithms have been proposed during the last three decades. A well-known method is the Multiplicative Updates algorithm proposed by Lee and Seung in 2002. Multiplicative updates have many interesting features: they are simple to implement and can be adapted to popular variants such as sparse Nonnegative Matrix Factorization, and, according to recent benchmarks, is state-of-the-art for many problems where the loss function is not the Frobenius norm. In this manuscript, we propose to improve the Multiplicative Updates algorithm seen as an alternating majorization minimization algorithm by crafting a tighter upper bound of the Hessian matrix for each alternate subproblem. Convergence is still ensured and we observe in practice on both synthetic and real world dataset that the proposed fastMU algorithm is often several
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;Dice&#21322;&#24230;&#37327;&#25439;&#22833;&#20989;&#25968;&#21487;&#22312;&#36719;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;&#65292;&#22312;&#21307;&#30103;&#25104;&#20687;&#39046;&#22495;&#30340;&#20998;&#21106;&#26041;&#26696;&#20013;&#19982;&#20351;&#29992;&#36719;&#26631;&#31614;&#30340;&#30740;&#31350;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;Dice&#20998;&#25968;&#21644;&#27169;&#22411;&#26657;&#20934;&#12290;</title><link>http://arxiv.org/abs/2303.16296</link><description>&lt;p&gt;
Dice&#21322;&#24230;&#37327;&#25439;&#22833;&#20989;&#25968;&#65306;&#29992;&#36719;&#26631;&#31614;&#20248;&#21270;Dice&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels. (arXiv:2303.16296v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;Dice&#21322;&#24230;&#37327;&#25439;&#22833;&#20989;&#25968;&#21487;&#22312;&#36719;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;&#65292;&#22312;&#21307;&#30103;&#25104;&#20687;&#39046;&#22495;&#30340;&#20998;&#21106;&#26041;&#26696;&#20013;&#19982;&#20351;&#29992;&#36719;&#26631;&#31614;&#30340;&#30740;&#31350;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;Dice&#20998;&#25968;&#21644;&#27169;&#22411;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#30340;&#35768;&#22810;&#33258;&#21160;&#20998;&#21106;&#26041;&#26696;&#20013;&#65292;&#36719;Dice&#25439;&#22833;&#65288;SDL&#65289;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#20154;&#20204;&#24050;&#32463;&#25581;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#32972;&#21518;&#30340;&#19968;&#20123;&#21407;&#22240;&#24182;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#20854;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#23454;&#29616;&#25903;&#25345;&#30452;&#25509;&#22312;&#36719;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;&#23427;&#30340;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#22312;&#20351;&#29992;SDL&#21644;&#30740;&#31350;&#21033;&#29992;&#36719;&#26631;&#31614;&#30340;&#21516;&#26102;&#36827;&#34892;&#27169;&#22411;&#26657;&#20934;&#30340;&#21327;&#21516;&#20316;&#29992;&#20173;&#28982;&#32570;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Dice&#21322;&#24230;&#37327;&#25439;&#22833;&#20989;&#25968;&#65288;DMLs&#65289;&#65292;&#23427;&#20204;&#65288;i&#65289;&#22312;&#30828;&#26631;&#31614;&#30340;&#26631;&#20934;&#35774;&#32622;&#19979;&#19982;SDL&#30456;&#21516;&#65292;&#20294;&#65288;ii&#65289;&#20063;&#21487;&#22312;&#36719;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#20844;&#20849;&#30340;QUBIQ&#12289;LiTS&#21644;KiTS&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;DMLs&#19982;&#36719;&#26631;&#31614;&#65288;&#22914;&#24179;&#22343;&#12289;&#26631;&#31614;&#24179;&#28369;&#21644;&#30693;&#35782;&#33976;&#39311;&#65289;&#30340;&#28508;&#22312;&#21327;&#21516;&#20316;&#29992;&#65292;&#32780;DMLs&#19982;&#30828;&#26631;&#31614;&#65288;&#22914;&#22823;&#22810;&#25968;&#25237;&#31080;&#21644;&#38543;&#26426;&#36873;&#25321;&#65289;&#30456;&#27604;&#65292;&#20135;&#29983;&#20102;&#26356;&#20248;&#31168;&#30340;Dice&#20998;&#25968;&#21644;&#27169;&#22411;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The soft Dice loss (SDL) has taken a pivotal role in many automated segmentation pipelines in the medical imaging community. Over the last years, some reasons behind its superior functioning have been uncovered and further optimizations have been explored. However, there is currently no implementation that supports its direct use in settings with soft labels. Hence, a synergy between the use of SDL and research leveraging the use of soft labels, also in the context of model calibration, is still missing. In this work, we introduce Dice semimetric losses (DMLs), which (i) are by design identical to SDL in a standard setting with hard labels, but (ii) can be used in settings with soft labels. Our experiments on the public QUBIQ, LiTS and KiTS benchmarks confirm the potential synergy of DMLs with soft labels (e.g. averaging, label smoothing, and knowledge distillation) over hard labels (e.g. majority voting and random selection). As a result, we obtain superior Dice scores and model calib
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31232;&#30095;&#24352;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#28857;&#20113;&#20960;&#20309;&#21644;&#39068;&#33394;&#27010;&#29575;&#20998;&#24067;&#30340;&#39640;&#25928;&#26080;&#25439;&#28857;&#20113;&#21387;&#32553;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#21387;&#32553;&#27604;&#21644;&#26356;&#24555;&#30340;&#21387;&#32553;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.06519</link><description>&lt;p&gt;
&#20351;&#29992;&#23398;&#20064;&#30340;&#26465;&#20214;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#26080;&#25439;&#28857;&#20113;&#20960;&#20309;&#21644;&#23646;&#24615;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Lossless Point Cloud Geometry and Attribute Compression Using a Learned Conditional Probability Model. (arXiv:2303.06519v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31232;&#30095;&#24352;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#28857;&#20113;&#20960;&#20309;&#21644;&#39068;&#33394;&#27010;&#29575;&#20998;&#24067;&#30340;&#39640;&#25928;&#26080;&#25439;&#28857;&#20113;&#21387;&#32553;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#21387;&#32553;&#27604;&#21644;&#26356;&#24555;&#30340;&#21387;&#32553;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an efficient lossless point cloud compression method that uses sparse tensor-based deep neural networks to learn point cloud geometry and color probability distributions, achieving higher compression ratio and faster compression speed compared to the state-of-the-art method from Moving Pict.
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25105;&#20204;&#22312;&#29983;&#27963;&#30340;&#35768;&#22810;&#26041;&#38754;&#37117;&#35265;&#35777;&#20102;&#28857;&#20113;&#25968;&#25454;&#30340;&#23384;&#22312;&#65292;&#20174;&#27785;&#28024;&#24335;&#23186;&#20307;&#12289;&#33258;&#21160;&#39550;&#39542;&#21040;&#21307;&#30103;&#20445;&#20581;&#65292;&#20294;&#20195;&#20215;&#26159;&#24040;&#22823;&#30340;&#25968;&#25454;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26080;&#25439;&#28857;&#20113;&#21387;&#32553;&#26041;&#27861;&#65292;&#20351;&#29992;&#31232;&#30095;&#24352;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#28857;&#20113;&#20960;&#20309;&#21644;&#39068;&#33394;&#27010;&#29575;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#32479;&#19968;&#30340;&#31232;&#30095;&#34920;&#31034;&#23558;&#28857;&#20113;&#34920;&#31034;&#20026;&#20855;&#26377;&#19981;&#21516;&#20301;&#28145;&#24230;&#30340;&#21344;&#29992;&#29305;&#24449;&#21644;&#19977;&#20010;&#23646;&#24615;&#29305;&#24449;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#31232;&#30095;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22320;&#21033;&#29992;&#28857;&#20113;&#20869;&#30340;&#29305;&#24449;&#21644;&#28857;&#20869;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#20026;&#31639;&#26415;&#32534;&#30721;&#22120;&#26500;&#24314;&#20934;&#30830;&#30340;&#33258;&#22238;&#24402;&#19978;&#19979;&#25991;&#27169;&#22411;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#23398;&#20064;&#30340;&#26080;&#25439;&#28857;&#20113;&#20960;&#20309;&#21644;&#23646;&#24615;&#21387;&#32553;&#26041;&#27861;&#12290;&#19982;Moving Pict&#30340;&#26368;&#26032;&#26080;&#25439;&#28857;&#20113;&#21387;&#32553;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#26080;&#25439;&#21387;&#32553;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#21387;&#32553;&#27604;&#21644;&#26356;&#24555;&#30340;&#21387;&#32553;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, we have witnessed the presence of point cloud data in many aspects of our life, from immersive media, autonomous driving to healthcare, although at the cost of a tremendous amount of data. In this paper, we present an efficient lossless point cloud compression method that uses sparse tensor-based deep neural networks to learn point cloud geometry and color probability distributions. Our method represents a point cloud with both occupancy feature and three attribute features at different bit depths in a unified sparse representation. This allows us to efficiently exploit feature-wise and point-wise dependencies within point clouds using a sparse tensor-based neural network and thus build an accurate auto-regressive context model for an arithmetic coder. To the best of our knowledge, this is the first learning-based lossless point cloud geometry and attribute compression approach. Compared with the-state-of-the-art lossless point cloud compression method from Moving Pict
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Jaccard&#24230;&#37327;&#25439;&#22833;&#65288;JMLs&#65289;&#26469;&#20248;&#21270;Jaccard&#25351;&#25968;&#65292;&#35813;&#25439;&#22833;&#22312;&#36719;&#26631;&#31614;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2302.05666</link><description>&lt;p&gt;
Jaccard&#24230;&#37327;&#25439;&#22833;&#65306;&#20351;&#29992;&#36719;&#26631;&#31614;&#20248;&#21270;Jaccard&#25351;&#25968;
&lt;/p&gt;
&lt;p&gt;
Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels. (arXiv:2302.05666v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Jaccard&#24230;&#37327;&#25439;&#22833;&#65288;JMLs&#65289;&#26469;&#20248;&#21270;Jaccard&#25351;&#25968;&#65292;&#35813;&#25439;&#22833;&#22312;&#36719;&#26631;&#31614;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
IoU&#25439;&#22833;&#26159;&#30452;&#25509;&#20248;&#21270;Jaccard&#25351;&#25968;&#30340;&#26367;&#20195;&#21697;&#12290;&#22312;&#35821;&#20041;&#20998;&#21106;&#20013;&#65292;&#23558;IoU&#25439;&#22833;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#30340;&#19968;&#37096;&#20998;&#65292;&#19982;&#20165;&#20248;&#21270;&#20687;&#32032;&#25439;&#22833;&#65288;&#22914;&#20132;&#21449;&#29109;&#25439;&#22833;&#65289;&#30456;&#27604;&#65292;&#23545;&#20110;Jaccard&#25351;&#25968;&#27979;&#37327;&#34920;&#29616;&#26356;&#22909;&#12290;&#26368;&#26174;&#30528;&#30340;IoU&#25439;&#22833;&#26159;&#36719;Jaccard&#25439;&#22833;&#21644;Lovasz-Softmax&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25439;&#22833;&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#36719;&#26631;&#31614;&#19981;&#20860;&#23481;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Jaccard&#24230;&#37327;&#25439;&#22833;&#65288;JMLs&#65289;&#65292;&#23427;&#20204;&#22312;&#26631;&#20934;&#35774;&#32622;&#19979;&#19982;&#36719;&#26631;&#31614;&#20860;&#23481;&#65292;&#19982;&#36719;Jaccard&#25439;&#22833;&#30456;&#21516;&#12290;&#20351;&#29992;JMLs&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#26368;&#27969;&#34892;&#30340;&#36719;&#26631;&#31614;&#29992;&#20363;&#65306;&#26631;&#31614;&#24179;&#28369;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#22312;&#19977;&#20010;&#35821;&#20041;&#20998;&#21106;&#25968;&#25454;&#38598;&#65288;Cityscapes&#12289;PASCAL VOC&#21644;DeepGlobe Land&#65289;&#19978;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20132;&#21449;&#29109;&#25439;&#22833;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31616;&#21333;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;DeepGlobe Land&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
IoU losses are surrogates that directly optimize the Jaccard index. In semantic segmentation, leveraging IoU losses as part of the loss function is shown to perform better with respect to the Jaccard index measure than optimizing pixel-wise losses such as the cross-entropy loss alone. The most notable IoU losses are the soft Jaccard loss and the Lovasz-Softmax loss. However, these losses are incompatible with soft labels which are ubiquitous in machine learning. In this paper, we propose Jaccard metric losses (JMLs), which are identical to the soft Jaccard loss in a standard setting with hard labels, but are compatible with soft labels. With JMLs, we study two of the most popular use cases of soft labels: label smoothing and knowledge distillation. With a variety of architectures, our experiments show significant improvements over the cross-entropy loss on three semantic segmentation datasets (Cityscapes, PASCAL VOC and DeepGlobe Land), and our simple approach outperforms state-of-the-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#21521;&#25968;&#25454;&#20013;&#28155;&#21152;&#25200;&#21160;&#65292;&#20351;&#24471;&#26234;&#33021;&#20307;&#22312;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#35266;&#27979;&#20540;&#19978;&#37319;&#21462;&#20302;&#22870;&#21169;&#21160;&#20316;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;BAFFLE&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.04688</link><description>&lt;p&gt;
BAFFLE: &#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
BAFFLE: Backdoor Attack in Offline Reinforcement Learning. (arXiv:2210.04688v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#21521;&#25968;&#25454;&#20013;&#28155;&#21152;&#25200;&#21160;&#65292;&#20351;&#24471;&#26234;&#33021;&#20307;&#22312;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#35266;&#27979;&#20540;&#19978;&#37319;&#21462;&#20302;&#22870;&#21169;&#21160;&#20316;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;BAFFLE&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#65292;&#20801;&#35768;&#26234;&#33021;&#20307;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#20013;&#25910;&#38598;&#30340;&#35797;&#38169;&#32463;&#39564;&#36827;&#34892;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#31163;&#32447;RL&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;RL&#33539;&#20363;&#65292;&#22240;&#20026;&#23427;&#33410;&#30465;&#20102;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#12290;&#22312;&#31163;&#32447;RL&#20013;&#65292;&#25968;&#25454;&#25552;&#20379;&#32773;&#20849;&#20139;&#22823;&#35268;&#27169;&#30340;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20182;&#20154;&#21487;&#20197;&#22312;&#19981;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#26234;&#33021;&#20307;&#12290;&#36825;&#31181;&#33539;&#20363;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#12289;&#33258;&#21160;&#39550;&#39542;&#31561;&#20851;&#38190;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36739;&#23569;&#20851;&#27880;&#30740;&#31350;&#31163;&#32447;RL&#31995;&#32479;&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;&#26412;&#25991;&#20851;&#27880;&#21518;&#38376;&#25915;&#20987;&#65292;&#20854;&#20013;&#19968;&#20123;&#25200;&#21160;&#34987;&#28155;&#21152;&#21040;&#25968;&#25454;&#65288;&#35266;&#27979;&#20540;&#65289;&#20013;&#65292;&#20351;&#24471;&#22312;&#32473;&#23450;&#27491;&#24120;&#35266;&#27979;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#37319;&#21462;&#39640;&#22870;&#21169;&#30340;&#21160;&#20316;&#65292;&#22312;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#35266;&#27979;&#20540;&#19978;&#37319;&#21462;&#20302;&#22870;&#21169;&#30340;&#21160;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BAFFLE&#65288;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing body of research has focused on the Reinforcement Learning (RL) methods which allow the agent to learn from trial-and-error experiences gathered during the interaction with the environment. Recently, offline RL becomes a popular RL paradigm because it saves the interactions with environments. In offline RL, data providers share large pre-collected datasets, and others can train high-quality agents without interacting with the environments. This paradigm has demonstrated effectiveness in critical tasks like robot control, autonomous driving, etc. However, less attention is paid to investigating the security threats to the offline RL system. This paper focuses on backdoor attacks, where some perturbations are added to the data (observations) such that given normal observations, the agent takes high-rewards actions, and low-reward actions on observations injected with triggers. In this paper, we propose Baffle (Backdoor Attack for Offline Reinforcement Learning), an approach tha
&lt;/p&gt;</description></item></channel></rss>