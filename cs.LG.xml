<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#38024;&#23545;&#26377;&#29366;&#24577;&#38450;&#24481;&#30340;&#26816;&#27979;&#29575;&#21644;&#35823;&#25253;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#30340;&#29702;&#35770;&#25551;&#36848;&#65292;&#24182;&#32473;&#20986;&#20102;&#29305;&#24449;&#25552;&#21462;&#22120;&#26816;&#27979;&#29575;&#30340;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2307.16331</link><description>&lt;p&gt;
&#38024;&#23545;&#22522;&#20110;&#26597;&#35810;&#30340;&#40657;&#30418;&#25915;&#20987;&#30340;&#26377;&#29366;&#24577;&#38450;&#24481;&#30340;&#29702;&#35770;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Theoretically Principled Trade-off for Stateful Defenses against Query-Based Black-Box Attacks. (arXiv:2307.16331v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#38024;&#23545;&#26377;&#29366;&#24577;&#38450;&#24481;&#30340;&#26816;&#27979;&#29575;&#21644;&#35823;&#25253;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#30340;&#29702;&#35770;&#25551;&#36848;&#65292;&#24182;&#32473;&#20986;&#20102;&#29305;&#24449;&#25552;&#21462;&#22120;&#26816;&#27979;&#29575;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#26679;&#26412;&#21363;&#20351;&#22312;&#21463;&#38480;&#30340;&#40657;&#30418;&#26465;&#20214;&#19979;&#20063;&#23545;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#23436;&#25972;&#24615;&#26500;&#25104;&#23041;&#32961;&#65292;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#21151;&#29575;&#12290;&#26377;&#29366;&#24577;&#38450;&#24481;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#23545;&#31574;&#65292;&#36890;&#36807;&#32500;&#25252;&#26368;&#36817;&#26597;&#35810;&#30340;&#32531;&#20914;&#21306;&#24182;&#26816;&#27979;&#30456;&#20284;&#24230;&#36807;&#39640;&#30340;&#26032;&#26597;&#35810;&#26469;&#26816;&#27979;&#28508;&#22312;&#30340;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#38450;&#24481;&#22312;&#25915;&#20987;&#26816;&#27979;&#21644;&#35823;&#25253;&#29575;&#20043;&#38388;&#23384;&#22312;&#22522;&#26412;&#30340;&#26435;&#34913;&#65292;&#36890;&#24120;&#36890;&#36807;&#25163;&#21160;&#36873;&#25321;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#30456;&#20284;&#24615;&#38408;&#20540;&#26469;&#20248;&#21270;&#27492;&#26435;&#34913;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23545;&#20110;&#36825;&#20010;&#26435;&#34913;&#30340;&#24418;&#24335;&#38480;&#21046;&#20197;&#21450;&#24433;&#21709;&#23427;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#22522;&#30784;&#38382;&#39064;&#22495;&#30340;&#30830;&#20999;&#23646;&#24615;&#32570;&#20047;&#20805;&#20998;&#30340;&#29702;&#35299;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#23545;&#26377;&#29366;&#24577;&#38450;&#24481;&#30340;&#26816;&#27979;&#29575;&#21644;&#35823;&#25253;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#30340;&#29702;&#35770;&#25551;&#36848;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#33324;&#31867;&#29305;&#24449;&#25552;&#21462;&#22120;&#26816;&#27979;&#29575;&#30340;&#19978;&#30028;&#65292;&#24182;&#20998;&#26512;&#20102;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#22522;&#26412;&#38382;&#39064;&#22495;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples threaten the integrity of machine learning systems with alarming success rates even under constrained black-box conditions. Stateful defenses have emerged as an effective countermeasure, detecting potential attacks by maintaining a buffer of recent queries and detecting new queries that are too similar. However, these defenses fundamentally pose a trade-off between attack detection and false positive rates, and this trade-off is typically optimized by hand-picking feature extractors and similarity thresholds that empirically work well. There is little current understanding as to the formal limits of this trade-off and the exact properties of the feature extractors/underlying problem domain that influence it. This work aims to address this gap by offering a theoretical characterization of the trade-off between detection and false positive rates for stateful defenses. We provide upper bounds for detection rates of a general class of feature extractors and analyze the
&lt;/p&gt;</description></item><item><title>RoseNNa&#26159;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#30340;&#39640;&#25928;&#12289;&#21487;&#31227;&#26893;&#30340;&#24211;&#65292;&#36890;&#36807;&#33258;&#21160;&#27169;&#22411;&#36716;&#25442;&#21644;&#38598;&#25104;&#65292;&#26080;&#38656;&#20462;&#25913;&#29616;&#26377;&#27714;&#35299;&#22120;&#20195;&#30721;&#21363;&#21487;&#23558;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.16322</link><description>&lt;p&gt;
RoseNNa: &#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#30340;&#39640;&#25928;&#12289;&#21487;&#31227;&#26893;&#30340;&#24211;&#65292;&#24212;&#29992;&#20110;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
RoseNNa: A performant, portable library for neural network inference with application to computational fluid dynamics. (arXiv:2307.16322v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16322
&lt;/p&gt;
&lt;p&gt;
RoseNNa&#26159;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#30340;&#39640;&#25928;&#12289;&#21487;&#31227;&#26893;&#30340;&#24211;&#65292;&#36890;&#36807;&#33258;&#21160;&#27169;&#22411;&#36716;&#25442;&#21644;&#38598;&#25104;&#65292;&#26080;&#38656;&#20462;&#25913;&#29616;&#26377;&#27714;&#35299;&#22120;&#20195;&#30721;&#21363;&#21487;&#23558;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26426;&#22120;&#23398;&#20064;&#30340;&#20852;&#36215;&#24341;&#21457;&#20102;&#39640;&#32423;&#24211;&#30340;&#20986;&#29616;&#65292;&#21253;&#25324;TensorFlow&#21644;PyTorch&#65292;&#20197;&#25903;&#25345;&#23427;&#20204;&#30340;&#21151;&#33021;&#12290;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#65288;CFD&#65289;&#30740;&#31350;&#20154;&#21592;&#21463;&#30410;&#20110;&#36825;&#19968;&#36235;&#21183;&#65292;&#20135;&#29983;&#20102;&#21487;&#20197;&#32553;&#30701;&#27169;&#25311;&#26102;&#38388;&#30340;&#24378;&#22823;&#31070;&#32463;&#32593;&#32476;&#12290;&#20363;&#22914;&#65292;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLPs&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#20307;&#31995;&#32467;&#26500;&#21487;&#20197;&#34920;&#31034;&#35832;&#22914;&#28237;&#27969;&#31561;&#23376;&#32593;&#26684;&#29289;&#29702;&#25928;&#24212;&#12290;&#22312;CFD&#27714;&#35299;&#22120;&#20013;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;CFD&#30340;&#32534;&#31243;&#35821;&#35328;&#22823;&#22810;&#19981;&#37325;&#21472;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;roseNNa&#24211;&#65292;&#23427;&#24357;&#21512;&#20102;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#21644;CFD&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;RoseNNa&#26159;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#12289;&#36731;&#37327;&#32423;&#65288;1000&#34892;&#20195;&#30721;&#65289;&#19988;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#24037;&#20855;&#65292;&#37325;&#28857;&#20851;&#27880;&#20110;&#22686;&#24378;PDE&#27714;&#35299;&#22120;&#65288;&#22914;CFD&#65289;&#20013;&#20351;&#29992;&#30340;&#36739;&#23567;&#32593;&#32476;&#65292;&#36825;&#20123;&#27714;&#35299;&#22120;&#36890;&#24120;&#26159;&#29992;C/C++&#25110;Fortran&#32534;&#20889;&#30340;&#12290;RoseNNa&#36890;&#36807;&#33258;&#21160;&#22320;&#36827;&#34892;&#27169;&#22411;&#36716;&#25442;&#21644;&#38598;&#25104;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#26080;&#38656;&#20462;&#25913;&#29616;&#26377;&#27714;&#35299;&#22120;&#20195;&#30721;&#21363;&#21487;&#23558;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;CFD&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of neural network-based machine learning ushered in high-level libraries, including TensorFlow and PyTorch, to support their functionality. Computational fluid dynamics (CFD) researchers have benefited from this trend and produced powerful neural networks that promise shorter simulation times. For example, multilayer perceptrons (MLPs) and Long Short Term Memory (LSTM) recurrent-based (RNN) architectures can represent sub-grid physical effects, like turbulence. Implementing neural networks in CFD solvers is challenging because the programming languages used for machine learning and CFD are mostly non-overlapping, We present the roseNNa library, which bridges the gap between neural network inference and CFD. RoseNNa is a non-invasive, lightweight (1000 lines), and performant tool for neural network inference, with focus on the smaller networks used to augment PDE solvers, like those of CFD, which are typically written in C/C++ or Fortran. RoseNNa accomplishes this by automatica
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23454;&#29992;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#26597;&#25214;&#25110;&#35777;&#20266;&#25968;&#25454;&#38598;&#20013;&#23545;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#20855;&#26377;&#24433;&#21709;&#30340;&#23567;&#23376;&#38598;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#20123;&#31639;&#27861;&#26041;&#27861;&#22312;&#40065;&#26834;&#24615;&#26816;&#26597;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20302;&#32500;&#22238;&#24402;&#38382;&#39064;&#30340;&#26377;&#29992;&#26816;&#26597;&#12290;&#20294;&#23545;&#20110;&#39640;&#32500;&#22238;&#24402;&#38382;&#39064;&#65292;&#35745;&#31639;&#29942;&#39048;&#20173;&#28982;&#23384;&#22312;&#12290;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#35889;&#31639;&#27861;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2307.16315</link><description>&lt;p&gt;
&#38754;&#21521;&#32447;&#24615;&#22238;&#24402;&#30340;&#23454;&#29992;&#40065;&#26834;&#24615;&#23457;&#35745;
&lt;/p&gt;
&lt;p&gt;
Towards Practical Robustness Auditing for Linear Regression. (arXiv:2307.16315v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23454;&#29992;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#26597;&#25214;&#25110;&#35777;&#20266;&#25968;&#25454;&#38598;&#20013;&#23545;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#20855;&#26377;&#24433;&#21709;&#30340;&#23567;&#23376;&#38598;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#20123;&#31639;&#27861;&#26041;&#27861;&#22312;&#40065;&#26834;&#24615;&#26816;&#26597;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20302;&#32500;&#22238;&#24402;&#38382;&#39064;&#30340;&#26377;&#29992;&#26816;&#26597;&#12290;&#20294;&#23545;&#20110;&#39640;&#32500;&#22238;&#24402;&#38382;&#39064;&#65292;&#35745;&#31639;&#29942;&#39048;&#20173;&#28982;&#23384;&#22312;&#12290;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#35889;&#31639;&#27861;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23454;&#29992;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#25110;&#35777;&#20266;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#23567;&#23376;&#38598;&#65292;&#24403;&#31227;&#38500;&#36825;&#20123;&#23376;&#38598;&#26102;&#65292;&#20250;&#25913;&#21464;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#20013;&#30340;&#31995;&#25968;&#30340;&#31526;&#21495;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#20102;&#29992;&#20110;&#27492;&#20219;&#21153;&#30340;&#20808;&#36827;&#31639;&#27861;&#25216;&#26415;&#30340;&#24615;&#33021; - &#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#32422;&#26463;&#20248;&#21270;&#29992;&#20110;&#19968;&#33324;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#65292;&#20197;&#21450;&#23545;&#29305;&#27530;&#24773;&#20917;&#30340;&#30830;&#20999;&#36138;&#23146;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32988;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#65292;&#24182;&#19988;&#20026;&#20302;&#32500;&#22238;&#24402;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#40065;&#26834;&#24615;&#26816;&#26597;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#32500;&#24230;&#20026;3&#25110;&#26356;&#39640;&#30340;&#22238;&#24402;&#38382;&#39064;&#65292;&#20173;&#28982;&#23384;&#22312;&#37325;&#35201;&#30340;&#35745;&#31639;&#29942;&#39048;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#35777;&#20266;&#36825;&#31181;&#23567;&#32780;&#20855;&#24433;&#21709;&#21147;&#30340;&#26679;&#26412;&#38598;&#21512;&#30340;&#23384;&#22312;&#12290;&#36890;&#36807;&#20351;&#29992;&#26368;&#36817;&#31639;&#27861;&#40065;&#26834;&#32479;&#35745;&#39046;&#22495;&#21019;&#26032;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#22312;&#36825;&#19968;&#25361;&#25112;&#19978;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#21033;&#29992;&#35889;&#31639;&#27861;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#24050;&#30693;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate practical algorithms to find or disprove the existence of small subsets of a dataset which, when removed, reverse the sign of a coefficient in an ordinary least squares regression involving that dataset. We empirically study the performance of well-established algorithmic techniques for this task -- mixed integer quadratically constrained optimization for general linear regression problems and exact greedy methods for special cases. We show that these methods largely outperform the state of the art and provide a useful robustness check for regression problems in a few dimensions. However, significant computational bottlenecks remain, especially for the important task of disproving the existence of such small sets of influential samples for regression problems of dimension $3$ or greater. We make some headway on this challenge via a spectral algorithm using ideas drawn from recent innovations in algorithmic robust statistics. We summarize the limitations of known techniqu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25513;&#27169;&#30340;&#25968;&#25454;&#22686;&#24378;&#26550;&#26500;&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21512;&#25104;&#20855;&#26377;&#24040;&#22823;&#32454;&#23567;&#32925;&#32454;&#32990;&#30284;&#30340;&#22810;&#21442;&#25968;MRI&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#32959;&#30244;&#25513;&#27169;&#23454;&#29616;&#36924;&#30495;&#30340;&#29983;&#25104;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.16314</link><description>&lt;p&gt;
&#22522;&#20110;&#25513;&#27169;&#30340;&#25968;&#25454;&#22686;&#24378;&#29992;&#20110;&#32597;&#35265;&#32925;&#32454;&#32990;&#30284;&#30340;&#22810;&#21442;&#25968;MRI&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Mask-guided Data Augmentation for Multiparametric MRI Generation with a Rare Hepatocellular Carcinoma. (arXiv:2307.16314v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25513;&#27169;&#30340;&#25968;&#25454;&#22686;&#24378;&#26550;&#26500;&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21512;&#25104;&#20855;&#26377;&#24040;&#22823;&#32454;&#23567;&#32925;&#32454;&#32990;&#30284;&#30340;&#22810;&#21442;&#25968;MRI&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#32959;&#30244;&#25513;&#27169;&#23454;&#29616;&#36924;&#30495;&#30340;&#29983;&#25104;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#34987;&#32463;&#20856;&#22320;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22810;&#21442;&#25968;&#25968;&#25454;&#38598;&#32780;&#35328;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20363;&#22914;&#65292;&#22312;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#30340;&#20960;&#20010;&#24212;&#29992;&#20013;&#20351;&#29992;&#30340;&#20256;&#32479;&#20960;&#20309;&#21464;&#25442;&#21487;&#33021;&#20250;&#20197;&#38750;&#29616;&#23454;&#30340;&#26041;&#24335;&#20462;&#25913;&#24739;&#32773;&#30340;&#35299;&#21078;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#38656;&#35201;&#19987;&#38376;&#30340;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#65292;&#20363;&#22914;&#36924;&#30495;&#22320;&#27169;&#25311;&#32473;&#23450;&#30340;&#30149;&#29702;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26550;&#26500;&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#24040;&#22823;&#32454;&#23567;&#32925;&#32454;&#32990;&#30284;&#30340;&#21512;&#25104;&#22810;&#21442;&#25968;&#65288;T1&#21160;&#33033;&#12289;T1&#38376;&#38745;&#33033;&#21644;T2&#65289;&#30913;&#20849;&#25391;&#22270;&#20687;&#65288;MRI&#65289;&#21450;&#20854;&#23545;&#24212;&#30340;&#32959;&#30244;&#25513;&#27169;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#36890;&#36807;&#29983;&#25104;&#32925;&#33039;&#32959;&#30244;&#25513;&#27169;&#21644;&#33145;&#37096;&#36793;&#32536;&#65292;&#22312;Pix2Pix&#32593;&#32476;&#30340;&#36755;&#20837;&#20013;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#21019;&#24314;&#12290;&#35813;&#26041;&#27861;&#30340;&#25928;&#29575;&#24050;&#32463;&#24471;&#21040;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is classically used to improve the overall performance of deep learning models. It is, however, challenging in the case of medical applications, and in particular for multiparametric datasets. For example, traditional geometric transformations used in several applications to generate synthetic images can modify in a non-realistic manner the patients' anatomy. Therefore, dedicated image generation techniques are necessary in the medical field to, for example, mimic a given pathology realistically. This paper introduces a new data augmentation architecture that generates synthetic multiparametric (T1 arterial, T1 portal, and T2) magnetic resonance images (MRI) of massive macrotrabecular subtype hepatocellular carcinoma with their corresponding tumor masks through a generative deep learning approach. The proposed architecture creates liver tumor masks and abdominal edges used as input in a Pix2Pix network for synthetic data creation. The method's efficiency is demonstrat
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#39044;&#27979;&#21644;&#20248;&#21270;&#20013;&#30340;&#19968;&#20010;&#38382;&#39064;&#8212;&#8212;&#38646;&#26799;&#24230;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#24494;&#20998;&#20248;&#21270;&#30340;&#25968;&#23398;&#29305;&#24615;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#30340;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.16304</link><description>&lt;p&gt;
&#12298;&#20320;&#19981;&#24471;&#36890;&#36807;&#65306;&#20984;&#20248;&#21270;&#20013;&#30340;&#38646;&#26799;&#24230;&#38382;&#39064;&#12299;
&lt;/p&gt;
&lt;p&gt;
You Shall not Pass: the Zero-Gradient Problem in Predict and Optimize for Convex Optimization. (arXiv:2307.16304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16304
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#39044;&#27979;&#21644;&#20248;&#21270;&#20013;&#30340;&#19968;&#20010;&#38382;&#39064;&#8212;&#8212;&#38646;&#26799;&#24230;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#24494;&#20998;&#20248;&#21270;&#30340;&#25968;&#23398;&#29305;&#24615;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#30340;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#21644;&#20248;&#21270;&#26159;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#20915;&#31574;&#27169;&#24335;&#65292;&#23427;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#39044;&#27979;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#26410;&#30693;&#21442;&#25968;&#12290;&#19982;&#26368;&#23567;&#21270;&#21442;&#25968;&#39044;&#27979;&#35823;&#24046;&#19981;&#21516;&#65292;&#23427;&#20351;&#29992;&#20219;&#21153;&#24615;&#33021;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#12290;&#22312;&#20984;&#20248;&#21270;&#39046;&#22495;&#65292;&#39044;&#27979;&#21644;&#20248;&#21270;&#30001;&#20110;&#26368;&#36817;&#24320;&#21457;&#30340;&#26041;&#27861;&#33021;&#22815;&#23545;&#38382;&#39064;&#21442;&#25968;&#36827;&#34892;&#24494;&#20998;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#26412;&#25991;&#21457;&#29616;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#36804;&#20170;&#26410;&#34987;&#27880;&#24847;&#21040;&#30340;&#32570;&#28857;&#8212;&#8212;&#38646;&#26799;&#24230;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#27861;&#12290;&#24314;&#35758;&#30340;&#26041;&#27861;&#22522;&#20110;&#24494;&#20998;&#20248;&#21270;&#30340;&#25968;&#23398;&#29305;&#24615;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20934;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predict and optimize is an increasingly popular decision-making paradigm that employs machine learning to predict unknown parameters of optimization problems. Instead of minimizing the prediction error of the parameters, it trains predictive models using task performance as a loss function. In the convex optimization domain, predict and optimize has seen significant progress due to recently developed methods for differentiating optimization problem solutions over the problem parameters. This paper identifies a yet unnoticed drawback of this approach -- the zero-gradient problem -- and introduces a method to solve it. The suggested method is based on the mathematical properties of differential optimization and is verified using two real-world benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;AutoML&#21644;&#20915;&#31574;&#26862;&#26519;&#26500;&#24314;&#20102;&#19968;&#20010;&#20998;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#26696;&#20214;&#20449;&#24687;&#39044;&#27979;&#21360;&#24230;&#19979;&#32423;&#27861;&#38498;&#30340;&#24310;&#36831;&#12290;&#26368;&#20339;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20026;81.4&#65285;&#65292;&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#22522;&#20110;&#30456;&#20851;&#25968;&#25454;&#28857;&#30340;AI&#27169;&#22411;&#22312;&#39044;&#27979;&#21360;&#24230;&#27861;&#38498;&#24310;&#36831;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.16285</link><description>&lt;p&gt;
&#20351;&#29992;AutoML&#21644;&#20915;&#31574;&#26862;&#26519;&#39044;&#27979;&#21360;&#24230;&#19979;&#32423;&#27861;&#38498;&#30340;&#24310;&#36831;
&lt;/p&gt;
&lt;p&gt;
Predicting delays in Indian lower courts using AutoML and Decision Forests. (arXiv:2307.16285v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;AutoML&#21644;&#20915;&#31574;&#26862;&#26519;&#26500;&#24314;&#20102;&#19968;&#20010;&#20998;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#26696;&#20214;&#20449;&#24687;&#39044;&#27979;&#21360;&#24230;&#19979;&#32423;&#27861;&#38498;&#30340;&#24310;&#36831;&#12290;&#26368;&#20339;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20026;81.4&#65285;&#65292;&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#22522;&#20110;&#30456;&#20851;&#25968;&#25454;&#28857;&#30340;AI&#27169;&#22411;&#22312;&#39044;&#27979;&#21360;&#24230;&#27861;&#38498;&#24310;&#36831;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27169;&#22411;&#65292;&#26681;&#25454;&#26696;&#20214;&#22312;&#25552;&#20132;&#26102;&#21487;&#24471;&#21040;&#30340;&#20449;&#24687;,&#26469;&#39044;&#27979;&#21360;&#24230;&#19979;&#32423;&#27861;&#38498;&#30340;&#24310;&#36831;&#12290;&#35813;&#27169;&#22411;&#24314;&#31435;&#22312;2010&#24180;&#25552;&#20132;&#30340;420&#19975;&#20010;&#27861;&#38498;&#26696;&#20214;&#21450;&#20854;&#22312;10&#24180;&#26399;&#38388;&#30340;&#32467;&#26524;&#30340;&#25968;&#25454;&#38598;&#19978;&#12290;&#25968;&#25454;&#38598;&#26469;&#33258;&#21360;&#24230;&#30340;7000&#22810;&#20010;&#19979;&#32423;&#27861;&#38498;&#12290;&#20316;&#32773;&#37319;&#29992;AutoML&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#31867;&#21035;&#20998;&#31867;&#27169;&#22411;&#65292;&#35206;&#30422;&#20102;&#25152;&#26377;&#31561;&#24453;&#26399;&#65292;&#24182;&#20351;&#29992;&#20108;&#36827;&#21046;&#20915;&#31574;&#26862;&#26519;&#20998;&#31867;&#22120;&#25552;&#39640;&#20102;&#24310;&#36831;&#20998;&#31867;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#26368;&#20339;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;81.4&#65285;&#65292;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#21035;&#20026;0.81&#12290;&#30740;&#31350;&#35777;&#26126;&#20102;&#22522;&#20110;&#30456;&#20851;&#25968;&#25454;&#28857;&#65288;&#22914;&#36758;&#21306;&#12289;&#27861;&#38498;&#12289;&#27861;&#23448;&#12289;&#20027;&#39064;&#21644;&#30456;&#20851;&#26041;&#65289;&#30340;AI&#27169;&#22411;&#22312;&#39044;&#27979;&#21360;&#24230;&#27861;&#38498;&#24310;&#36831;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#30456;&#20851;&#25991;&#29486;&#30340;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;&#20316;&#32773;&#24050;&#25552;&#20379;&#25968;&#25454;&#38598;&#21644;Python&#20195;&#30721;&#25991;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a classification model that predicts delays in Indian lower courts based on case information available at filing. The model is built on a dataset of 4.2 million court cases filed in 2010 and their outcomes over a 10-year period. The data set is drawn from 7000+ lower courts in India. The authors employed AutoML to develop a multi-class classification model over all periods of pendency and then used binary decision forest classifiers to improve predictive accuracy for the classification of delays. The best model achieved an accuracy of 81.4%, and the precision, recall, and F1 were found to be 0.81. The study demonstrates the feasibility of AI models for predicting delays in Indian courts, based on relevant data points such as jurisdiction, court, judge, subject, and the parties involved. The paper also discusses the results in light of relevant literature and suggests areas for improvement and future research. The authors have made the dataset and Python code files u
&lt;/p&gt;</description></item><item><title>zkDL&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#38646;&#30693;&#35782;&#35777;&#26126;&#65292;&#36890;&#36807;zkReLU&#21327;&#35758;&#21644;&#26032;&#39062;&#30340;&#31639;&#26415;&#30005;&#36335;&#26500;&#24314;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23545;&#28145;&#24230;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#30340;&#26377;&#25928;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.16273</link><description>&lt;p&gt;
zkDL: &#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#38646;&#30693;&#35782;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
zkDL: Efficient Zero-Knowledge Proofs of Deep Learning Training. (arXiv:2307.16273v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16273
&lt;/p&gt;
&lt;p&gt;
zkDL&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#38646;&#30693;&#35782;&#35777;&#26126;&#65292;&#36890;&#36807;zkReLU&#21327;&#35758;&#21644;&#26032;&#39062;&#30340;&#31639;&#26415;&#30005;&#36335;&#26500;&#24314;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23545;&#28145;&#24230;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#30340;&#26377;&#25928;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#32473;&#20154;&#20204;&#30340;&#29983;&#27963;&#24102;&#26469;&#20102;&#37325;&#22823;&#21464;&#21270;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36825;&#20123;&#24555;&#36895;&#21457;&#23637;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#28145;&#24230;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#30340;&#21512;&#27861;&#24615;&#30340;&#25285;&#24551;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20445;&#25252;&#19981;&#21463;&#20449;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#32773;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#36890;&#24120;&#31105;&#27490;&#39564;&#35777;&#32773;&#36890;&#36807;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#26469;&#30452;&#25509;&#26816;&#26597;&#35757;&#32451;&#36807;&#31243;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;zkDL&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#38646;&#30693;&#35782;&#35777;&#26126;&#12290;zkDL&#30340;&#26680;&#24515;&#26159;zkReLU&#65292;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#38646;&#30693;&#35782;&#35777;&#26126;&#21327;&#35758;&#65292;&#20855;&#26377;&#20248;&#21270;&#30340;&#35777;&#26126;&#26102;&#38388;&#21644;&#35777;&#26126;&#22823;&#23567;&#65292;&#36825;&#26159;&#21487;&#39564;&#35777;&#35757;&#32451;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#65292;&#22240;&#20026;&#23427;&#30340;&#38750;&#31639;&#26415;&#24615;&#36136;&#12290;&#20026;&#20102;&#23558;zkReLU&#25972;&#21512;&#21040;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#30340;&#35777;&#26126;&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20174;&#31070;&#32463;&#32593;&#32476;&#20013;&#26500;&#24314;&#31639;&#26415;&#30005;&#36335;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#20016;&#23500;&#30340;&#24182;&#34892;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#20010;&#26500;&#24314;&#26041;&#26696;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#35777;&#26126;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advancements in deep learning have brought about significant changes in various aspects of people's lives. Meanwhile, these rapid developments have raised concerns about the legitimacy of the training process of deep networks. However, to protect the intellectual properties of untrusted AI developers, directly examining the training process by accessing the model parameters and training data by verifiers is often prohibited.  In response to this challenge, we present zkDL, an efficient zero-knowledge proof of deep learning training. At the core of zkDL is zkReLU, a specialized zero-knowledge proof protocol with optimized proving time and proof size for the ReLU activation function, a major obstacle in verifiable training due to its non-arithmetic nature. To integrate zkReLU into the proof system for the entire training process, we devise a novel construction of an arithmetic circuit from neural networks. By leveraging the abundant parallel computation resources, this constru
&lt;/p&gt;</description></item><item><title>DRL4Route&#26159;&#19968;&#31181;&#29992;&#20110;&#25509;&#36865;&#36335;&#32447;&#39044;&#27979;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34892;&#20026;&#23398;&#20064;&#33021;&#21147;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#38750;&#21487;&#24494;&#20998;&#30446;&#26631;&#20248;&#21270;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#26631;&#20934;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.16246</link><description>&lt;p&gt;
DRL4Route:&#19968;&#31181;&#29992;&#20110;&#25509;&#36865;&#36335;&#32447;&#39044;&#27979;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DRL4Route: A Deep Reinforcement Learning Framework for Pick-up and Delivery Route Prediction. (arXiv:2307.16246v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16246
&lt;/p&gt;
&lt;p&gt;
DRL4Route&#26159;&#19968;&#31181;&#29992;&#20110;&#25509;&#36865;&#36335;&#32447;&#39044;&#27979;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34892;&#20026;&#23398;&#20064;&#33021;&#21147;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#38750;&#21487;&#24494;&#20998;&#30446;&#26631;&#20248;&#21270;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#26631;&#20934;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25509;&#36865;&#36335;&#32447;&#39044;&#27979;(PDRP)&#22312;&#39044;&#27979;&#24037;&#20154;&#30340;&#26410;&#26469;&#26381;&#21153;&#36335;&#32447;&#26041;&#38754;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30001;&#20110;&#33021;&#22815;&#20174;&#22823;&#37327;&#21382;&#21490;&#25968;&#25454;&#20013;&#25429;&#25417;&#24037;&#20154;&#34892;&#20026;&#27169;&#24335;&#30340;&#24378;&#22823;&#33021;&#21147;&#32780;&#25104;&#20026;&#35813;&#20219;&#21153;&#30340;&#20027;&#23548;&#27169;&#22411;&#12290;&#34429;&#28982;&#26377;&#30528;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#26410;&#33021;&#23558;&#19981;&#21487;&#24494;&#20998;&#30340;&#27979;&#35797;&#26631;&#20934;&#24341;&#20837;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#23548;&#33268;&#35757;&#32451;&#21644;&#27979;&#35797;&#26631;&#20934;&#19981;&#21305;&#37197;&#12290;&#36825;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#20351;&#29992;&#26102;&#26497;&#22823;&#22320;&#21066;&#20943;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#23558;&#24378;&#21270;&#23398;&#20064;(RL)&#25512;&#24191;&#21040;&#36335;&#32447;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#31181;&#21517;&#20026;DRL4Route&#30340;&#26032;&#22411;RL&#26694;&#26550;&#12290;&#23427;&#32467;&#21512;&#20102;&#20808;&#21069;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34892;&#20026;&#23398;&#20064;&#33021;&#21147;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#38750;&#21487;&#24494;&#20998;&#30446;&#26631;&#20248;&#21270;&#33021;&#21147;&#12290;DRL4Route&#21487;&#20197;&#20316;&#20026;&#25554;&#20214;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pick-up and Delivery Route Prediction (PDRP), which aims to estimate the future service route of a worker given his current task pool, has received rising attention in recent years. Deep neural networks based on supervised learning have emerged as the dominant model for the task because of their powerful ability to capture workers' behavior patterns from massive historical data. Though promising, they fail to introduce the non-differentiable test criteria into the training process, leading to a mismatch in training and test criteria. Which considerably trims down their performance when applied in practical systems. To tackle the above issue, we present the first attempt to generalize Reinforcement Learning (RL) to the route prediction task, leading to a novel RL-based framework called DRL4Route. It combines the behavior-learning abilities of previous deep learning models with the non-differentiable objective optimization ability of reinforcement learning. DRL4Route can serve as a plug-
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#19968;&#31995;&#21015;&#29983;&#29289;&#21551;&#21457;&#30340;&#31361;&#35302;&#21487;&#22609;&#24615;&#27169;&#22411;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#19982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#22609;&#24615;&#27169;&#22411;&#36827;&#34892;&#20102;&#32852;&#31995;&#12290;&#29983;&#29289;&#21551;&#21457;&#28145;&#24230;&#23398;&#20064;&#20195;&#34920;&#20102;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#26088;&#22312;&#25512;&#21160;&#24403;&#21069;&#25216;&#26415;&#30340;&#21457;&#23637;&#20197;&#21450;&#23545;&#26234;&#33021;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.16236</link><description>&lt;p&gt;
&#31361;&#35302;&#21487;&#22609;&#24615;&#27169;&#22411;&#21644;&#29983;&#29289;&#21551;&#21457;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning: A Survey. (arXiv:2307.16236v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#19968;&#31995;&#21015;&#29983;&#29289;&#21551;&#21457;&#30340;&#31361;&#35302;&#21487;&#22609;&#24615;&#27169;&#22411;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#19982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#22609;&#24615;&#27169;&#22411;&#36827;&#34892;&#20102;&#32852;&#31995;&#12290;&#29983;&#29289;&#21551;&#21457;&#28145;&#24230;&#23398;&#20064;&#20195;&#34920;&#20102;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#26088;&#22312;&#25512;&#21160;&#24403;&#21069;&#25216;&#26415;&#30340;&#21457;&#23637;&#20197;&#21450;&#23545;&#26234;&#33021;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25216;&#26415;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#38754;&#20020;&#30528;&#23545;&#25239;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#12289;&#29983;&#24577;&#24433;&#21709;&#20197;&#21450;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#30740;&#31350;&#20154;&#21592;&#36234;&#26469;&#36234;&#20851;&#27880;&#29983;&#29289;&#23398;&#22522;&#30784;&#26426;&#21046;&#65292;&#36825;&#26159;&#30001;&#20110;&#29983;&#29289;&#22823;&#33041;&#25152;&#23637;&#31034;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#25152;&#21560;&#24341;&#12290;&#26412;&#32508;&#36848;&#25506;&#35752;&#20102;&#19968;&#31995;&#21015;&#36825;&#20123;&#29983;&#29289;&#21551;&#21457;&#27169;&#22411;&#30340;&#31361;&#35302;&#21487;&#22609;&#24615;&#27169;&#22411;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#19982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#22609;&#24615;&#27169;&#22411;&#36827;&#34892;&#20102;&#32852;&#31995;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#29983;&#29289;&#21551;&#21457;&#28145;&#24230;&#23398;&#20064;&#20195;&#34920;&#20102;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#26088;&#22312;&#25512;&#21160;&#19981;&#20165;&#24403;&#21069;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#32780;&#19988;&#36824;&#26377;&#23545;&#26234;&#33021;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently emerged technologies based on Deep Learning (DL) achieved outstanding results on a variety of tasks in the field of Artificial Intelligence (AI). However, these encounter several challenges related to robustness to adversarial inputs, ecological impact, and the necessity of huge amounts of training data. In response, researchers are focusing more and more interest on biologically grounded mechanisms, which are appealing due to the impressive capabilities exhibited by biological brains. This survey explores a range of these biologically inspired models of synaptic plasticity, their application in DL scenarios, and the connections with models of plasticity in Spiking Neural Networks (SNNs). Overall, Bio-Inspired Deep Learning (BIDL) represents an exciting research direction, aiming at advancing not only our current technologies but also our understanding of intelligence.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#25991;&#31456;&#22238;&#39038;&#20102;&#22522;&#20110;&#29983;&#29289;&#23398;&#21551;&#21457;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#25361;&#25112;&#20197;&#21450;&#29983;&#29289;&#21551;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16235</link><description>&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#21644;&#20223;&#29983;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks and Bio-Inspired Supervised Deep Learning: A Survey. (arXiv:2307.16235v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16235
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#25991;&#31456;&#22238;&#39038;&#20102;&#22522;&#20110;&#29983;&#29289;&#23398;&#21551;&#21457;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#25361;&#25112;&#20197;&#21450;&#29983;&#29289;&#21551;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#29983;&#29289;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#19968;&#30452;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#23547;&#27714;&#21457;&#23637;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#37325;&#35201;&#28789;&#24863;&#26469;&#28304;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#20840;&#38754;&#22238;&#39038;&#26368;&#36817;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20223;&#29983;&#26041;&#27861;&#12290;&#39318;&#20808;&#20171;&#32461;&#20102;&#29983;&#29289;&#31070;&#32463;&#20803;&#30340;&#35745;&#31639;&#21407;&#29702;&#21644;&#31361;&#35302;&#21487;&#22609;&#24615;&#65292;&#28982;&#21518;&#35814;&#32454;&#20171;&#32461;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#24378;&#35843;&#20102;&#19982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30456;&#20851;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#20256;&#32479;&#30340;&#21453;&#21521;&#20256;&#25773;&#20248;&#21270;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#19982;&#21453;&#21521;&#20256;&#25773;&#19981;&#21516;&#30340;&#20223;&#29983;&#35757;&#32451;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#20256;&#32479;&#32593;&#32476;&#21644;&#33033;&#20914;&#32593;&#32476;&#12290;&#29983;&#29289;&#21551;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#25512;&#21160;&#24403;&#21069;&#27169;&#22411;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#29983;&#29289;&#21487;&#34892;&#24615;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a long time, biology and neuroscience fields have been a great source of inspiration for computer scientists, towards the development of Artificial Intelligence (AI) technologies. This survey aims at providing a comprehensive review of recent biologically-inspired approaches for AI. After introducing the main principles of computation and synaptic plasticity in biological neurons, we provide a thorough presentation of Spiking Neural Network (SNN) models, and we highlight the main challenges related to SNN training, where traditional backprop-based optimization is not directly applicable. Therefore, we discuss recent bio-inspired training methods, which pose themselves as alternatives to backprop, both for traditional and spiking networks. Bio-Inspired Deep Learning (BIDL) approaches towards advancing the computational capabilities and biological plausibility of current models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#30005;&#21160;&#36710;&#22312;&#33258;&#20027;&#31227;&#21160;&#20986;&#34892;&#31995;&#32479;&#20013;&#30340;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#32771;&#34385;&#20102;&#36710;&#36742;&#20379;&#24212;&#21644;&#38656;&#27714;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.16228</link><description>&lt;p&gt;
&#33258;&#20027;&#31227;&#21160;&#20986;&#34892;&#31995;&#32479;&#20013;&#30005;&#21160;&#36710;&#24179;&#34913;&#30340;&#40065;&#26834;&#24615;&#65306;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Robust Electric Vehicle Balancing of Autonomous Mobility-On-Demand System: A Multi-Agent Reinforcement Learning Approach. (arXiv:2307.16228v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#30005;&#21160;&#36710;&#22312;&#33258;&#20027;&#31227;&#21160;&#20986;&#34892;&#31995;&#32479;&#20013;&#30340;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#32771;&#34385;&#20102;&#36710;&#36742;&#20379;&#24212;&#21644;&#38656;&#27714;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#32463;&#27982;&#21644;&#31038;&#20250;&#25928;&#30410;&#65292;&#30005;&#21160;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;EAV&#65289;&#27491;&#22312;&#26410;&#26469;&#30340;&#33258;&#20027;&#31227;&#21160;&#20986;&#34892;&#65288;AMoD&#65289;&#31995;&#32479;&#20013;&#24341;&#36215;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;EAV&#30340;&#29420;&#29305;&#20805;&#30005;&#27169;&#24335;&#65288;&#38271;&#26102;&#38388;&#20805;&#30005;&#12289;&#39640;&#20805;&#30005;&#39057;&#29575;&#12289;&#26080;&#27861;&#39044;&#27979;&#30340;&#20805;&#30005;&#34892;&#20026;&#31561;&#65289;&#20351;&#24471;&#20934;&#30830;&#39044;&#27979;EAV&#20379;&#24212;&#22312;E-AMoD&#31995;&#32479;&#20013;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#31227;&#21160;&#38656;&#27714;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20351;&#24471;&#22312;&#20379;&#24212;&#21644;&#38656;&#27714;&#19981;&#30830;&#23450;&#24615;&#19979;&#35774;&#35745;&#19968;&#20010;&#38598;&#25104;&#30340;&#36710;&#36742;&#24179;&#34913;&#35299;&#20915;&#26041;&#26696;&#25104;&#20026;&#32039;&#36843;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;E-AMoD&#24179;&#34913;&#31639;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;EV&#20379;&#24212;&#25110;&#31227;&#21160;&#38656;&#27714;&#19979;&#30340;&#29366;&#24577;&#19981;&#30830;&#23450;&#24615;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;E-AMoD&#31995;&#32479;&#20013;&#30340;EAV&#24179;&#34913;&#65292;&#20854;&#20013;&#21253;&#21547;&#23545;&#25239;&#24615;&#26234;&#33021;&#20307;&#65292;&#29992;&#20110;&#27169;&#25311;&#21487;&#33021;&#30772;&#22351;&#36710;&#36742;&#24179;&#34913;&#35299;&#20915;&#26041;&#26696;&#30340;EAV&#20379;&#24212;&#21644;&#31227;&#21160;&#38656;&#27714;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;E-AMoD&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electric autonomous vehicles (EAVs) are getting attention in future autonomous mobility-on-demand (AMoD) systems due to their economic and societal benefits. However, EAVs' unique charging patterns (long charging time, high charging frequency, unpredictable charging behaviors, etc.) make it challenging to accurately predict the EAVs supply in E-AMoD systems. Furthermore, the mobility demand's prediction uncertainty makes it an urgent and challenging task to design an integrated vehicle balancing solution under supply and demand uncertainties. Despite the success of reinforcement learning-based E-AMoD balancing algorithms, state uncertainties under the EV supply or mobility demand remain unexplored. In this work, we design a multi-agent reinforcement learning (MARL)-based framework for EAVs balancing in E-AMoD systems, with adversarial agents to model both the EAVs supply and mobility demand uncertainties that may undermine the vehicle balancing solutions. We then propose a robust E-AMo
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36739;&#23569;&#30340;&#25163;&#21160;&#21019;&#24314;&#25968;&#25454;&#35757;&#32451;&#30340;&#36731;&#37327;&#32423;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#24076;&#20271;&#26469;&#25991;OCR&#21518;&#26657;&#27491;&#12290;&#30740;&#31350;&#30446;&#26631;&#26159;&#24320;&#21457;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#25913;&#21892;OCR&#21518;&#26657;&#27491;&#32467;&#26524;&#65292;&#24182;&#30740;&#31350;&#26368;&#36866;&#29992;&#20110;&#21382;&#21490;&#25991;&#26723;&#30340;OCR&#21518;&#26657;&#27491;&#30340;&#25968;&#25454;&#38598;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.16220</link><description>&lt;p&gt;
&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20197;&#36827;&#34892;&#21382;&#21490;&#24076;&#20271;&#26469;&#25991;OCR&#38169;&#35823;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Optimizing the Neural Network Training for OCR Error Correction of Historical Hebrew Texts. (arXiv:2307.16220v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16220
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36739;&#23569;&#30340;&#25163;&#21160;&#21019;&#24314;&#25968;&#25454;&#35757;&#32451;&#30340;&#36731;&#37327;&#32423;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#24076;&#20271;&#26469;&#25991;OCR&#21518;&#26657;&#27491;&#12290;&#30740;&#31350;&#30446;&#26631;&#26159;&#24320;&#21457;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#25913;&#21892;OCR&#21518;&#26657;&#27491;&#32467;&#26524;&#65292;&#24182;&#30740;&#31350;&#26368;&#36866;&#29992;&#20110;&#21382;&#21490;&#25991;&#26723;&#30340;OCR&#21518;&#26657;&#27491;&#30340;&#25968;&#25454;&#38598;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#22823;&#37327;&#32440;&#36136;&#25991;&#26723;&#22914;&#20070;&#31821;&#21644;&#25253;&#32440;&#24050;&#32463;&#20351;&#29992;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#25216;&#26415;&#36827;&#34892;&#25968;&#23383;&#21270;&#12290;&#36825;&#31181;&#25216;&#26415;&#22312;&#22788;&#29702;&#21382;&#21490;&#25991;&#26723;&#26102;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#12290;&#20026;&#20102;&#32416;&#27491;OCR&#38169;&#35823;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#20998;&#26512;&#21644;&#31070;&#32463;&#32593;&#32476;&#31561;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#21518;&#22788;&#29702;&#31639;&#27861;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#32570;&#28857;&#26159;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#36825;&#31181;&#25968;&#25454;&#36890;&#24120;&#19981;&#26131;&#33719;&#21462;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36739;&#23569;&#30340;&#25163;&#21160;&#21019;&#24314;&#25968;&#25454;&#26469;&#35757;&#32451;&#36731;&#37327;&#32423;&#30340;&#24076;&#20271;&#26469;&#25991;OCR&#21518;&#26657;&#27491;&#31070;&#32463;&#32593;&#32476;&#12290;&#20027;&#35201;&#30740;&#31350;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#35821;&#35328;&#21644;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#22312;OCR&#21518;&#26657;&#27491;&#20013;&#30340;&#32467;&#26524;&#65292;&#24182;&#30740;&#31350;&#21738;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#23545;&#20110;&#21382;&#21490;&#25991;&#26723;&#30340;OCR&#21518;&#26657;&#27491;&#26368;&#26377;&#25928;&#12290;&#20026;&#27492;&#65292;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few decades, large archives of paper-based documents such as books and newspapers have been digitized using Optical Character Recognition. This technology is error-prone, especially for historical documents. To correct OCR errors, post-processing algorithms have been proposed based on natural language analysis and machine learning techniques such as neural networks. Neural network's disadvantage is the vast amount of manually labeled data required for training, which is often unavailable. This paper proposes an innovative method for training a light-weight neural network for Hebrew OCR post-correction using significantly less manually created data. The main research goal is to develop a method for automatically generating language and task-specific training data to improve the neural network results for OCR post-correction, and to investigate which type of dataset is the most effective for OCR post-correction of historical documents. To this end, a series of experiments u
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#23383;&#20154;&#25991;&#30740;&#31350;&#20013;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25991;&#26412;&#20998;&#26512;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21644;&#39046;&#22495;&#36866;&#24212;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.16217</link><description>&lt;p&gt;
&#25968;&#23383;&#20154;&#25991;&#21644;&#20449;&#24687;&#31185;&#23398;&#20013;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25991;&#26412;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Text Analysis Using Deep Neural Networks in Digital Humanities and Information Science. (arXiv:2307.16217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#23383;&#20154;&#25991;&#30740;&#31350;&#20013;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25991;&#26412;&#20998;&#26512;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21644;&#39046;&#22495;&#36866;&#24212;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#35745;&#31639;&#25216;&#26415;&#21644;&#20154;&#25991;&#23398;&#31185;&#26159;&#19968;&#39033;&#25345;&#32493;&#36827;&#34892;&#30340;&#21162;&#21147;&#65292;&#26088;&#22312;&#20351;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#20854;&#20182;&#33402;&#26415;&#21697;&#31561;&#36164;&#28304;&#22312;&#25968;&#23383;&#21270;&#26102;&#20195;&#26131;&#20110;&#33719;&#24471;&#12289;&#21487;&#25628;&#32034;&#21644;&#21487;&#20998;&#26512;&#12290;&#22312;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#33258;&#21160;&#25991;&#26412;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#26377;&#26102;&#21576;&#29616;&#20986;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;DNN&#26159;&#35299;&#20915;&#25968;&#23383;&#20154;&#25991;&#30740;&#31350;&#20013;&#19982;NLP&#30456;&#20851;&#30340;&#35768;&#22810;&#20219;&#21153;&#65288;&#20363;&#22914;&#25340;&#20889;&#26816;&#26597;&#12289;&#35821;&#35328;&#26816;&#27979;&#12289;&#23454;&#20307;&#25552;&#21462;&#12289;&#20316;&#32773;&#26816;&#27979;&#12289;&#38382;&#31572;&#31561;&#65289;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#36825;&#20123;&#26377;&#30417;&#30563;&#31639;&#27861;&#20174;&#22823;&#37327;&#30340;&#8220;&#27491;&#30830;&#8221;&#21644;&#8220;&#38169;&#35823;&#8221;&#31034;&#20363;&#20013;&#23398;&#20064;&#27169;&#24335;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26032;&#30340;&#31034;&#20363;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#23383;&#20154;&#25991;&#30740;&#31350;&#20013;&#20351;&#29992;DNN&#20998;&#26512;&#25991;&#26412;&#36164;&#28304;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#65288;&#19981;&#65289;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#39046;&#22495;&#36866;&#24212;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#22810;&#20010;&#20351;&#29992;&#26696;&#20363;&#26469;&#25506;&#35752;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining computational technologies and humanities is an ongoing effort aimed at making resources such as texts, images, audio, video, and other artifacts digitally available, searchable, and analyzable. In recent years, deep neural networks (DNN) dominate the field of automatic text analysis and natural language processing (NLP), in some cases presenting a super-human performance. DNNs are the state-of-the-art machine learning algorithms solving many NLP tasks that are relevant for Digital Humanities (DH) research, such as spell checking, language detection, entity extraction, author detection, question answering, and other tasks. These supervised algorithms learn patterns from a large number of "right" and "wrong" examples and apply them to new examples. However, using DNNs for analyzing the text resources in DH research presents two main challenges: (un)availability of training data and a need for domain adaptation. This paper explores these challenges by analyzing multiple use-cas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#23478;&#26063;&#35889;&#38382;&#31572;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#23478;&#26063;&#35889;&#25968;&#25454;&#34920;&#31034;&#20026;&#30693;&#35782;&#22270;&#24182;&#19982;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#32467;&#21512;&#65292;&#20351;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#23478;&#26063;&#35889;&#39046;&#22495;&#20013;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#22270;&#32467;&#26500;&#21644;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.16214</link><description>&lt;p&gt;
&#29992;&#20110;&#21322;&#32467;&#26500;&#24322;&#26500;&#23478;&#26063;&#35889;&#30693;&#35782;&#22270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Question Answering with Deep Neural Networks for Semi-Structured Heterogeneous Genealogical Knowledge Graphs. (arXiv:2307.16214v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#23478;&#26063;&#35889;&#38382;&#31572;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#23478;&#26063;&#35889;&#25968;&#25454;&#34920;&#31034;&#20026;&#30693;&#35782;&#22270;&#24182;&#19982;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#32467;&#21512;&#65292;&#20351;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#23478;&#26063;&#35889;&#39046;&#22495;&#20013;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#22270;&#32467;&#26500;&#21644;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29992;&#25143;&#29983;&#25104;&#30340;&#23478;&#26063;&#35889;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#26032;&#30340;&#23478;&#26063;&#35889;&#20449;&#24687;&#31995;&#32479;&#24471;&#21040;&#20102;&#24320;&#21457;&#12290;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#31639;&#27861;&#20351;&#29992;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#19968;&#20123;&#27169;&#22411;&#20351;&#29992;&#22522;&#20110;&#24207;&#21015;&#30340;&#36755;&#20837;&#65292;&#19981;&#36866;&#21512;&#22788;&#29702;&#22522;&#20110;&#22270;&#30340;&#32467;&#26500;&#65292;&#32780;&#22522;&#20110;&#22270;&#30340;DNN&#27169;&#22411;&#21017;&#20381;&#36182;&#20110;&#22312;&#23478;&#26063;&#35889;&#39046;&#22495;&#20013;&#19981;&#23384;&#22312;&#30340;&#39640;&#24230;&#20840;&#38754;&#30340;&#30693;&#35782;&#22270;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26377;&#30417;&#30563;&#30340;DNN&#27169;&#22411;&#38656;&#35201;&#22312;&#23478;&#26063;&#35889;&#39046;&#22495;&#20013;&#32570;&#20047;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23478;&#26063;&#35889;&#38382;&#31572;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65306;1&#65289;&#23558;&#23478;&#26063;&#35889;&#25968;&#25454;&#34920;&#31034;&#20026;&#30693;&#35782;&#22270;&#65292;2&#65289;&#23558;&#20854;&#36716;&#25442;&#20026;&#25991;&#26412;&#65292;3&#65289;&#19982;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#32467;&#21512;&#65292;4&#65289;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#38382;&#31572;&#27169;&#22411;&#12290;&#20026;&#20102;&#35780;&#20272;&#38656;&#35201;&#19987;&#38376;&#26041;&#27861;&#30340;&#24517;&#35201;&#24615;&#65292;&#23545;&#27604;&#20102;&#24494;&#35843;&#27169;&#24335;&#19979;&#30340;&#27169;&#22411;&#19982;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rising popularity of user-generated genealogical family trees, new genealogical information systems have been developed. State-of-the-art natural question answering algorithms use deep neural network (DNN) architecture based on self-attention networks. However, some of these models use sequence-based inputs and are not suitable to work with graph-based structure, while graph-based DNN models rely on high levels of comprehensiveness of knowledge graphs that is nonexistent in the genealogical domain. Moreover, these supervised DNN models require training datasets that are absent in the genealogical domain. This study proposes an end-to-end approach for question answering using genealogical family trees by: 1) representing genealogical data as knowledge graphs, 2) converting them to texts, 3) combining them with unstructured texts, and 4) training a trans-former-based question answering model. To evaluate the need for a dedicated approach, a comparison between the fine-tuned mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21382;&#21490;&#24076;&#20271;&#26469;&#25991;&#26412;OCR&#38169;&#35823;&#26657;&#27491;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#23450;&#26102;&#26399;&#20248;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24076;&#20271;&#26469;&#35821;&#26159;&#24418;&#24577;&#20016;&#23500;&#35821;&#35328;&#65292;&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#19981;&#22815;&#20805;&#36275;&#65292;&#19988;&#26368;&#20339;&#30340;&#32593;&#32476;&#32467;&#26500;&#21644;&#36229;&#21442;&#25968;&#20540;&#23578;&#19981;&#26126;&#30830;&#12290;&#21478;&#22806;&#65292;&#19981;&#21516;&#27969;&#27966;&#21644;&#26102;&#26399;&#30340;&#35821;&#35328;&#21464;&#21270;&#21487;&#33021;&#20250;&#24433;&#21709;OCR&#21518;&#26657;&#27491;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.16213</link><description>&lt;p&gt;
&#26397;&#30528;&#21382;&#21490;&#24076;&#20271;&#26469;&#25991;&#26412;OCR&#38169;&#35823;&#26657;&#27491;&#30340;&#29305;&#23450;&#26102;&#26399;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Toward a Period-Specific Optimized Neural Network for OCR Error Correction of Historical Hebrew Texts. (arXiv:2307.16213v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21382;&#21490;&#24076;&#20271;&#26469;&#25991;&#26412;OCR&#38169;&#35823;&#26657;&#27491;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#23450;&#26102;&#26399;&#20248;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24076;&#20271;&#26469;&#35821;&#26159;&#24418;&#24577;&#20016;&#23500;&#35821;&#35328;&#65292;&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#19981;&#22815;&#20805;&#36275;&#65292;&#19988;&#26368;&#20339;&#30340;&#32593;&#32476;&#32467;&#26500;&#21644;&#36229;&#21442;&#25968;&#20540;&#23578;&#19981;&#26126;&#30830;&#12290;&#21478;&#22806;&#65292;&#19981;&#21516;&#27969;&#27966;&#21644;&#26102;&#26399;&#30340;&#35821;&#35328;&#21464;&#21270;&#21487;&#33021;&#20250;&#24433;&#21709;OCR&#21518;&#26657;&#27491;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#20351;&#29992;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#25216;&#26415;&#23545;&#22823;&#37327;&#32440;&#36136;&#21382;&#21490;&#25991;&#26723;&#65288;&#22914;&#20070;&#31821;&#21644;&#25253;&#32440;&#65289;&#36827;&#34892;&#20102;&#25968;&#23383;&#21270;&#23384;&#26723;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#22312;&#22788;&#29702;&#20889;&#20110;&#25968;&#30334;&#24180;&#21069;&#30340;OCR&#25991;&#26723;&#26102;&#23481;&#26131;&#20986;&#38169;&#12290;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#21508;&#31181;&#25991;&#26412;&#22788;&#29702;&#20219;&#21153;&#65292;&#21253;&#25324;OCR&#21518;&#26657;&#27491;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#24076;&#20271;&#26469;&#35821;&#36825;&#26679;&#30340;&#24418;&#24577;&#20016;&#23500;&#35821;&#35328;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#21382;&#21490;&#35821;&#26009;&#24211;&#30340;&#20027;&#35201;&#32570;&#28857;&#26159;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#20805;&#36275;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#26469;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#24076;&#20271;&#26469;&#35821;&#30340;&#29420;&#29305;&#29305;&#28857;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;OCR&#38169;&#35823;&#26657;&#27491;&#26041;&#38754;&#30340;&#26368;&#20339;&#32467;&#26500;&#21644;&#36229;&#21442;&#25968;&#20540;&#65288;&#39044;&#23450;&#20041;&#21442;&#25968;&#65289;&#23578;&#19981;&#28165;&#26970;&#12290;&#27492;&#22806;&#65292;&#35821;&#35328;&#22312;&#19981;&#21516;&#30340;&#27969;&#27966;&#21644;&#26102;&#26399;&#20063;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#36825;&#20123;&#21464;&#21270;&#21487;&#33021;&#20250;&#24433;&#21709;OCR&#21518;&#26657;&#27491;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few decades, large archives of paper-based historical documents, such as books and newspapers, have been digitized using the Optical Character Recognition (OCR) technology. Unfortunately, this broadly used technology is error-prone, especially when an OCRed document was written hundreds of years ago. Neural networks have shown great success in solving various text processing tasks, including OCR post-correction. The main disadvantage of using neural networks for historical corpora is the lack of sufficiently large training datasets they require to learn from, especially for morphologically-rich languages like Hebrew. Moreover, it is not clear what are the optimal structure and values of hyperparameters (predefined parameters) of neural networks for OCR error correction in Hebrew due to its unique features. Furthermore, languages change across genres and periods. These changes may affect the accuracy of OCR post-correction neural network models. To overcome these challenge
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#29366;&#24577;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#29366;&#24577;&#25200;&#21160;&#23545;&#25163;&#65292;&#23558;&#35813;&#38382;&#39064;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65292;&#24182;&#25552;&#20986;&#20102;&#40065;&#26834;&#22343;&#34913;&#20316;&#20026;&#35299;&#20915;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2307.16212</link><description>&lt;p&gt;
&#20855;&#26377;&#29366;&#24577;&#19981;&#30830;&#23450;&#24615;&#30340;&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Multi-Agent Reinforcement Learning with State Uncertainty. (arXiv:2307.16212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#29366;&#24577;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#29366;&#24577;&#25200;&#21160;&#23545;&#25163;&#65292;&#23558;&#35813;&#38382;&#39064;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65292;&#24182;&#25552;&#20986;&#20102;&#40065;&#26834;&#22343;&#34913;&#20316;&#20026;&#35299;&#20915;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#24212;&#29992;&#20013;&#65292;&#30001;&#20110;&#27979;&#37327;&#19981;&#20934;&#30830;&#25110;&#24694;&#24847;&#25915;&#20987;&#31561;&#21407;&#22240;&#65292;&#26234;&#33021;&#20307;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#23436;&#32654;&#30340;&#29366;&#24577;&#20449;&#24687;&#65292;&#36825;&#32473;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#34429;&#28982;&#40065;&#26834;&#24615;&#22312;MARL&#37096;&#32626;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20294;&#24456;&#23569;&#26377;&#20808;&#21069;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;MARL&#20013;&#30340;&#29366;&#24577;&#19981;&#30830;&#23450;&#24615;&#65292;&#26080;&#35770;&#26159;&#22312;&#38382;&#39064;&#30340;&#34920;&#36848;&#36824;&#26159;&#31639;&#27861;&#35774;&#35745;&#26041;&#38754;&#12290;&#21463;&#21040;&#36825;&#20010;&#40065;&#26834;&#24615;&#38382;&#39064;&#21450;&#32570;&#20047;&#30456;&#24212;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#26412;&#24037;&#20316;&#20013;&#30740;&#31350;&#20102;&#20855;&#26377;&#29366;&#24577;&#19981;&#30830;&#23450;&#24615;&#30340;MARL&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#27425;&#23558;&#35813;&#38382;&#39064;&#24314;&#27169;&#20026;&#20855;&#26377;&#29366;&#24577;&#25200;&#21160;&#23545;&#25163;&#30340;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65288;MG-SPA&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#32452;&#29366;&#24577;&#25200;&#21160;&#23545;&#25163;&#21040;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#40065;&#26834;&#22343;&#34913;&#65288;RE&#65289;&#20316;&#20026;MG-SPA&#30340;&#35299;&#20915;&#27010;&#24565;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;MG-SPA&#30340;&#22522;&#26412;&#20998;&#26512;&#65292;&#20363;&#22914;&#32473;&#20986;&#20102;&#28385;&#36275;&#40065;&#26834;&#22343;&#34913;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world multi-agent reinforcement learning (MARL) applications, agents may not have perfect state information (e.g., due to inaccurate measurement or malicious attacks), which challenges the robustness of agents' policies. Though robustness is getting important in MARL deployment, little prior work has studied state uncertainties in MARL, neither in problem formulation nor algorithm design. Motivated by this robustness issue and the lack of corresponding studies, we study the problem of MARL with state uncertainty in this work. We provide the first attempt to the theoretical and empirical analysis of this challenging problem. We first model the problem as a Markov Game with state perturbation adversaries (MG-SPA) by introducing a set of state perturbation adversaries into a Markov Game. We then introduce robust equilibrium (RE) as the solution concept of an MG-SPA. We conduct a fundamental analysis regarding MG-SPA such as giving conditions under which such a robust equilibrium e
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#20013;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24573;&#35270;&#20102;&#35270;&#35273;&#22270;&#20687;&#30340;&#19981;&#23436;&#25972;&#24615;&#21644;&#27169;&#31946;&#24615;&#65292;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#34920;&#26126;&#27169;&#22411;&#22312;&#38754;&#23545;&#19981;&#23436;&#25972;&#24615;&#26102;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#21644;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.16210</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#32570;&#22833;&#21644;&#27169;&#31946;&#30340;&#35270;&#35273;&#27169;&#24577;
&lt;/p&gt;
&lt;p&gt;
Rethinking Uncertainly Missing and Ambiguous Visual Modality in Multi-Modal Entity Alignment. (arXiv:2307.16210v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16210
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#20013;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24573;&#35270;&#20102;&#35270;&#35273;&#22270;&#20687;&#30340;&#19981;&#23436;&#25972;&#24615;&#21644;&#27169;&#31946;&#24615;&#65292;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#34920;&#26126;&#27169;&#22411;&#22312;&#38754;&#23545;&#19981;&#23436;&#25972;&#24615;&#26102;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#21644;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#30340;&#37325;&#35201;&#25193;&#23637;&#65292;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#65288;MMEA&#65289;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#30340;&#35270;&#35273;&#20449;&#24687;&#26469;&#35782;&#21035;&#36328;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20043;&#38388;&#30340;&#30456;&#21516;&#23454;&#20307;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MMEA&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#22810;&#27169;&#24577;&#23454;&#20307;&#29305;&#24449;&#30340;&#34701;&#21512;&#33539;&#24335;&#19978;&#65292;&#32780;&#24573;&#35270;&#20102;&#32570;&#22833;&#21644;&#20869;&#22312;&#27169;&#31946;&#24615;&#30340;&#35270;&#35273;&#22270;&#20687;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#35270;&#35273;&#27169;&#24577;&#19981;&#23436;&#25972;&#24615;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#20998;&#26512;&#65292;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;MMEA-UMVM&#25968;&#25454;&#38598;&#19978;&#23545;&#26368;&#26032;&#30340;MMEA&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#28085;&#30422;&#21452;&#35821;&#21644;&#21333;&#35821;&#23545;&#40784;KGs&#30340;&#31867;&#22411;&#65292;&#24182;&#37319;&#29992;&#26631;&#20934;&#65288;&#38750;&#36845;&#20195;&#65289;&#21644;&#36845;&#20195;&#35757;&#32451;&#33539;&#24335;&#26469;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#38754;&#23545;&#27169;&#24577;&#19981;&#23436;&#25972;&#24615;&#26102;&#65292;&#27169;&#22411;&#24456;&#23481;&#26131;&#36807;&#25311;&#21512;&#27169;&#24577;&#22122;&#22768;&#65292;&#24182;&#22312;&#39640;&#32570;&#22833;&#27169;&#24577;&#30340;&#24773;&#20917;&#19979;&#20986;&#29616;&#24615;&#33021;&#25391;&#33633;&#25110;&#19979;&#38477;&#12290;&#36825;&#35777;&#26126;&#20102;&#22686;&#21152;&#35270;&#35273;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a crucial extension of entity alignment (EA), multi-modal entity alignment (MMEA) aims to identify identical entities across disparate knowledge graphs (KGs) by exploiting associated visual information. However, existing MMEA approaches primarily concentrate on the fusion paradigm of multi-modal entity features, while neglecting the challenges presented by the pervasive phenomenon of missing and intrinsic ambiguity of visual images. In this paper, we present a further analysis of visual modality incompleteness, benchmarking latest MMEA models on our proposed dataset MMEA-UMVM, where the types of alignment KGs covering bilingual and monolingual, with standard (non-iterative) and iterative training paradigms to evaluate the model performance. Our research indicates that, in the face of modality incompleteness, models succumb to overfitting the modality noise, and exhibit performance oscillations or declines at high rates of missing modality. This proves that the inclusion of additiona
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#24322;&#26500;&#23478;&#35889;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#25968;&#23383;&#32858;&#21512;&#38382;&#31572;&#30340;&#26041;&#27861;&#65292;&#22312;&#22522;&#22240;&#35889;&#39046;&#22495;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#24182;&#33719;&#24471;&#20934;&#30830;&#31572;&#26696;&#30340;&#33021;&#21147;&#36824;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.16208</link><description>&lt;p&gt;
&#22312;&#24322;&#26500;&#23478;&#35889;&#30693;&#35782;&#22270;&#35889;&#19978;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25968;&#23383;&#32858;&#21512;&#38382;&#31572;&#30340;GLOBE&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Around the GLOBE: Numerical Aggregation Question-Answering on Heterogeneous Genealogical Knowledge Graphs with Deep Neural Networks. (arXiv:2307.16208v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#24322;&#26500;&#23478;&#35889;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#25968;&#23383;&#32858;&#21512;&#38382;&#31572;&#30340;&#26041;&#27861;&#65292;&#22312;&#22522;&#22240;&#35889;&#39046;&#22495;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#24182;&#33719;&#24471;&#20934;&#30830;&#31572;&#26696;&#30340;&#33021;&#21147;&#36824;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#24322;&#26500;&#23478;&#35889;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#25968;&#23383;&#32858;&#21512;&#38382;&#31572;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#19982;&#31934;&#30830;&#31572;&#26696;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#30446;&#21069;&#65292;&#22312;&#22522;&#22240;&#35889;&#39046;&#22495;&#65292;&#25552;&#20986;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#24182;&#33719;&#24471;&#20934;&#30830;&#31572;&#26696;&#30340;&#33021;&#21147;&#36824;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#65292;&#32780;&#30740;&#31350;&#32773;&#22312;&#20154;&#25991;&#21644;&#31038;&#20250;&#31185;&#23398;&#31561;&#39046;&#22495;&#21487;&#20197;&#20174;&#36825;&#31181;&#33021;&#21147;&#20013;&#21463;&#30410;&#21290;&#27973;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the key AI tools for textual corpora exploration is natural language question-answering (QA). Unlike keyword-based search engines, QA algorithms receive and process natural language questions and produce precise answers to these questions, rather than long lists of documents that need to be manually scanned by the users. State-of-the-art QA algorithms based on DNNs were successfully employed in various domains. However, QA in the genealogical domain is still underexplored, while researchers in this field (and other fields in humanities and social sciences) can highly benefit from the ability to ask questions in natural language, receive concrete answers and gain insights hidden within large corpora. While some research has been recently conducted for factual QA in the genealogical domain, to the best of our knowledge, there is no previous research on the more challenging task of numerical aggregation QA (i.e., answering questions combining aggregation functions, e.g., count, ave
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29305;&#24449;&#25552;&#21462;&#21644;&#23398;&#20064;&#20013;&#20351;&#29992;&#38646;&#22635;&#20805;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20102;&#30456;&#27604;&#20110;&#20840;&#36830;&#25509;&#32593;&#32476;&#65292;&#38646;&#22635;&#20805;&#30340;DCNNs&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#24179;&#31227;&#19981;&#21464;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.16203</link><description>&lt;p&gt;
&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38646;&#22635;&#20805;: &#29305;&#24449;&#25552;&#21462;&#21644;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Convolutional Neural Networks with Zero-Padding: Feature Extraction and Learning. (arXiv:2307.16203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29305;&#24449;&#25552;&#21462;&#21644;&#23398;&#20064;&#20013;&#20351;&#29992;&#38646;&#22635;&#20805;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20102;&#30456;&#27604;&#20110;&#20840;&#36830;&#25509;&#32593;&#32476;&#65292;&#38646;&#22635;&#20805;&#30340;DCNNs&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#24179;&#31227;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29305;&#24449;&#25552;&#21462;&#21644;&#23398;&#20064;&#20013;&#20351;&#29992;&#38646;&#22635;&#20805;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DCNNs&#65289;&#30340;&#24615;&#33021;&#12290;&#22312;&#39564;&#35777;&#20102;&#38646;&#22635;&#20805;&#22312;&#23454;&#29616;&#24179;&#31227;&#31561;&#20215;&#24615;&#21644;&#27744;&#21270;&#22312;&#23454;&#29616;&#24179;&#31227;&#19981;&#21464;&#24615;&#30340;&#20316;&#29992;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#30456;&#21516;&#25968;&#37327;&#30340;&#33258;&#30001;&#21442;&#25968;&#65292;&#20219;&#20309;&#28145;&#24230;&#20840;&#36830;&#25509;&#32593;&#32476;&#65288;DFCNs&#65289;&#37117;&#21487;&#20197;&#36890;&#36807;&#20855;&#26377;&#38646;&#22635;&#20805;&#30340;DCNNs&#26469;&#34920;&#31034;&#12290;&#36825;&#35777;&#26126;&#20102;&#30456;&#27604;&#20110;DFCNs&#22312;&#29305;&#24449;&#25552;&#21462;&#19978;&#65292;&#38646;&#22635;&#20805;&#30340;DCNNs&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#20855;&#26377;&#38646;&#22635;&#20805;&#30340;DCNNs&#30340;&#26222;&#36941;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20854;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#24179;&#31227;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#30340;&#25152;&#26377;&#29702;&#35770;&#32467;&#26524;&#37117;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#21253;&#25324;&#29609;&#20855;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the performance of deep convolutional neural networks (DCNNs) with zero-padding in feature extraction and learning. After verifying the roles of zero-padding in enabling translation-equivalence, and pooling in its translation-invariance driven nature, we show that with similar number of free parameters, any deep fully connected networks (DFCNs) can be represented by DCNNs with zero-padding. This demonstrates that DCNNs with zero-padding is essentially better than DFCNs in feature extraction. Consequently, we derive universal consistency of DCNNs with zero-padding and show its translation-invariance in the learning process. All our theoretical results are verified by numerical experiments including both toy simulations and real-data running.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#21644;&#27927;&#29260;&#25216;&#26415;&#65292;&#26377;&#25928;&#23454;&#29616;&#20102;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.16196</link><description>&lt;p&gt;
&#27927;&#29260;&#24335;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Shuffled Differentially Private Federated Learning for Time Series Data Analytics. (arXiv:2307.16196v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16196
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#21644;&#27927;&#29260;&#25216;&#26415;&#65292;&#26377;&#25928;&#23454;&#29616;&#20102;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#20219;&#30340;&#32852;&#37030;&#23398;&#20064;&#26088;&#22312;&#22312;&#30830;&#20445;&#23458;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#26368;&#20248;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#38024;&#23545;&#22270;&#20687;&#25968;&#25454;&#65292;&#32570;&#20047;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24212;&#29992;&#65292;&#32780;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26377;&#35768;&#22810;&#37325;&#35201;&#30340;&#24212;&#29992;&#65292;&#22914;&#26426;&#22120;&#20581;&#24247;&#30417;&#27979;&#12289;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#31561;&#12290;&#27492;&#22806;&#65292;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#27169;&#22411;&#36827;&#34892;&#20445;&#25252;&#24615;&#22122;&#38899;&#22788;&#29702;&#21487;&#33021;&#20250;&#20005;&#37325;&#24178;&#25200;&#26102;&#24577;&#30456;&#20851;&#30340;&#23398;&#20064;&#65292;&#23548;&#33268;&#31934;&#24230;&#19979;&#38477;&#26356;&#22823;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#23558;&#38544;&#31169;&#20445;&#25252;&#20449;&#20219;&#36793;&#30028;&#25193;&#23637;&#21040;&#23458;&#25143;&#31471;&#12290;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#27927;&#29260;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#23545;&#38544;&#31169;&#22686;&#24378;&#65292;&#22312;&#20943;&#23567;&#22240;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#32780;&#23548;&#33268;&#30340;&#31934;&#24230;&#19979;&#38477;&#26041;&#38754;&#36215;&#21040;&#20102;&#32531;&#35299;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trustworthy federated learning aims to achieve optimal performance while ensuring clients' privacy. Existing privacy-preserving federated learning approaches are mostly tailored for image data, lacking applications for time series data, which have many important applications, like machine health monitoring, human activity recognition, etc. Furthermore, protective noising on a time series data analytics model can significantly interfere with temporal-dependent learning, leading to a greater decline in accuracy. To address these issues, we develop a privacy-preserving federated learning algorithm for time series data. Specifically, we employ local differential privacy to extend the privacy protection trust boundary to the clients. We also incorporate shuffle techniques to achieve a privacy amplification, mitigating the accuracy decline caused by leveraging local differential privacy. Extensive experiments were conducted on five time series datasets. The evaluation results reveal that our
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;16&#20301;&#35745;&#31639;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Adam&#20248;&#21270;&#22120;&#30340;&#26032;&#26041;&#27861;&#26469;&#25552;&#39640;16&#20301;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.16189</link><description>&lt;p&gt;
&#29992;&#20110;16&#20301;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Efficient Approach to Mitigate Numerical Instability in Backpropagation for 16-bit Neural Network Training. (arXiv:2307.16189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16189
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;16&#20301;&#35745;&#31639;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Adam&#20248;&#21270;&#22120;&#30340;&#26032;&#26041;&#27861;&#26469;&#25552;&#39640;16&#20301;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;16&#20301;&#35745;&#31639;&#20013;&#20351;&#29992;&#27969;&#34892;&#30340;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;RMSProp&#21644;Adam&#65289;&#26102;&#35266;&#23519;&#21040;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#30340;&#22797;&#26434;&#24615;&#12290;&#36825;&#31181;&#19981;&#31283;&#23450;&#24615;&#36890;&#24120;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#38454;&#27573;&#20013;&#20986;&#29616;&#65292;&#23548;&#33268;&#23398;&#20064;&#36807;&#31243;&#21463;&#21040;&#24178;&#25200;&#65292;&#20174;&#32780;&#22952;&#30861;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26377;&#25928;&#37096;&#32626;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#21333;&#19968;&#36229;&#21442;&#25968;epsilon&#26159;&#36825;&#31181;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#23545;16&#20301;&#35745;&#31639;&#20013;&#36825;&#20123;&#20248;&#21270;&#22120;&#20013;epsilon&#30340;&#20316;&#29992;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#32034;&#65292;&#21457;&#29616;&#24494;&#35843;&#20854;&#20540;&#21487;&#20197;&#24674;&#22797;RMSProp&#21644;Adam&#30340;&#21151;&#33021;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#21033;&#29992;16&#20301;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#34987;&#21457;&#29616;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;Adam&#20248;&#21270;&#22120;&#30340;&#26356;&#26032;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20102;16&#20301;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research, we delve into the intricacies of the numerical instability observed in 16-bit computations of machine learning models, particularly when employing popular optimization algorithms such as RMSProp and Adam. This instability is commonly experienced during the training phase of deep neural networks, leading to disrupted learning processes and hindering the effective deployment of such models. We identify the single hyperparameter, epsilon, as the main culprit behind this numerical instability. An in-depth exploration of the role of epsilon in these optimizers within 16-bit computations reveals that a minor adjustment of its value can restore the functionality of RMSProp and Adam, consequently enabling the effective utilization of 16-bit neural networks. We propose a novel method to mitigate the identified numerical instability issues. This method capitalizes on the updates from the Adam optimizer and significantly improves the robustness of the learning process in 16-bit 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#31216;&#24615;&#20808;&#39564;&#30693;&#35782;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#25928;&#29575;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#22686;&#24378;&#21644;&#19968;&#33268;&#24615;&#25439;&#22833;&#38598;&#25104;&#21040;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#19988;&#27867;&#21270;&#24615;&#33021;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2307.16186</link><description>&lt;p&gt;
ESP:&#21033;&#29992;&#23545;&#31216;&#24615;&#20808;&#39564;&#30693;&#35782;&#36827;&#34892;&#22810;Agent&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ESP: Exploiting Symmetry Prior for Multi-Agent Reinforcement Learning. (arXiv:2307.16186v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#31216;&#24615;&#20808;&#39564;&#30693;&#35782;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#25928;&#29575;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#22686;&#24378;&#21644;&#19968;&#33268;&#24615;&#25439;&#22833;&#38598;&#25104;&#21040;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#19988;&#27867;&#21270;&#24615;&#33021;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#35201;&#27714;&#26500;&#24314;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#22312;&#24403;&#21069;&#30340;MARL&#26041;&#27861;&#20013;&#34987;&#24573;&#35270;&#20102;&#12290;&#21463;&#22810;Agent&#31995;&#32479;&#20013;&#23545;&#31216;&#29616;&#35937;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#22686;&#24378;&#21644;&#31934;&#24515;&#35774;&#35745;&#30340;&#19968;&#33268;&#24615;&#25439;&#22833;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;MARL&#26041;&#27861;&#20013;&#65292;&#26469;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#26159;&#27169;&#22411;&#26080;&#20851;&#30340;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;MARL&#31639;&#27861;&#12290;&#23545;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#36827;&#34892;&#20102;&#23454;&#39564;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#23558;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#19968;&#20010;&#29289;&#29702;&#22810;&#26426;&#22120;&#20154;&#23454;&#39564;&#24179;&#21488;&#65292;&#23637;&#31034;&#20102;&#23427;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent reinforcement learning (MARL) has achieved promising results in recent years. However, most existing reinforcement learning methods require a large amount of data for model training. In addition, data-efficient reinforcement learning requires the construction of strong inductive biases, which are ignored in the current MARL approaches. Inspired by the symmetry phenomenon in multi-agent systems, this paper proposes a framework for exploiting prior knowledge by integrating data augmentation and a well-designed consistency loss into the existing MARL methods. In addition, the proposed framework is model-agnostic and can be applied to most of the current MARL algorithms. Experimental tests on multiple challenging tasks demonstrate the effectiveness of the proposed framework. Moreover, the proposed framework is applied to a physical multi-robot testbed to show its superiority.
&lt;/p&gt;</description></item><item><title>UnIVAL&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#25903;&#25345;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#35821;&#35328;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.16184</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#35821;&#35328;&#20219;&#21153;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Unified Model for Image, Video, Audio and Language Tasks. (arXiv:2307.16184v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16184
&lt;/p&gt;
&lt;p&gt;
UnIVAL&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#25903;&#25345;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#35821;&#35328;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20351;&#24471;&#24314;&#31435;&#36890;&#29992;&#20195;&#29702;&#21464;&#24471;&#19981;&#20877;&#26159;&#24187;&#24819;&#12290;&#26500;&#24314;&#36825;&#31181;&#36890;&#29992;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#38590;&#39064;&#26159;&#20219;&#21153;&#21644;&#27169;&#24577;&#30340;&#22810;&#26679;&#24615;&#21644;&#24322;&#36136;&#24615;&#12290;&#32479;&#19968;&#35299;&#20915;&#26041;&#26696;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20869;&#25903;&#25345;&#22810;&#26679;&#30340;&#20219;&#21153;&#21644;&#27169;&#24577;&#12290;&#34429;&#28982;&#19968;&#20123;&#22823;&#22411;&#27169;&#22411;&#65288;&#20363;&#22914;Flameigno&#65289;&#32463;&#36807;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#21487;&#20197;&#25903;&#25345;&#36229;&#36807;&#20004;&#20010;&#27169;&#24577;&#65292;&#20294;&#30446;&#21069;&#23567;&#21040;&#20013;&#22411;&#30340;&#32479;&#19968;&#27169;&#22411;&#20173;&#28982;&#23616;&#38480;&#20110;&#20004;&#20010;&#27169;&#24577;&#65292;&#36890;&#24120;&#26159;&#22270;&#20687;-&#25991;&#26412;&#25110;&#35270;&#39057;-&#25991;&#26412;&#12290;&#25105;&#20204;&#35201;&#25552;&#20986;&#30340;&#38382;&#39064;&#26159;&#65306;&#26159;&#21542;&#21487;&#33021;&#26500;&#24314;&#19968;&#20010;&#39640;&#25928;&#25903;&#25345;&#25152;&#26377;&#27169;&#24577;&#30340;&#32479;&#19968;&#27169;&#22411;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UnIVAL&#65292;&#36825;&#26159;&#23545;&#36825;&#20010;&#38596;&#24515;&#21187;&#21187;&#30446;&#26631;&#36808;&#20986;&#30340;&#19968;&#27493;&#12290;UnIVAL&#27169;&#22411;&#25317;&#26377;&#32422;0.25&#20159;&#20010;&#21442;&#25968;&#65292;&#19981;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#32479;&#19968;&#21040;&#19968;&#20010;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have made the ambitious quest for generalist agents significantly far from being a fantasy. A key hurdle for building such general models is the diversity and heterogeneity of tasks and modalities. A promising solution is unification, allowing the support of a myriad of tasks and modalities within one unified framework. While few large models (e.g., Flamingo (Alayrac et al., 2022), trained on massive datasets, can support more than two modalities, current small to mid-scale unified models are still limited to 2 modalities, usually image-text or video-text. The question that we ask is: is it possible to build efficiently a unified model that can support all modalities? To answer this, we propose UnIVAL, a step further towards this ambitious goal. Without relying on fancy datasets sizes or models with billions of parameters, the ~ 0.25B parameter UnIVAL model goes beyond two modalities and unifies text, images, video, and audio into a single model. Our model 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20887;&#20313;&#24863;&#30693;&#30340;&#26080;&#30417;&#30563;&#25490;&#21517;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#22522;&#22240;&#38598;&#21512;&#36827;&#34892;&#25490;&#24207;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#22522;&#22240;&#38598;&#21512;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#20197;&#21450;&#38598;&#21512;&#20013;&#30340;&#21333;&#20010;&#20803;&#32032;&#21644;&#22823;&#23567;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#25216;&#24039;&#35268;&#36991;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.16182</link><description>&lt;p&gt;
&#22522;&#22240;&#38598;&#21512;&#30340;&#20887;&#20313;&#24863;&#30693;&#26080;&#30417;&#30563;&#25490;&#21517;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Redundancy-aware unsupervised rankings for collections of gene sets. (arXiv:2307.16182v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16182
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20887;&#20313;&#24863;&#30693;&#30340;&#26080;&#30417;&#30563;&#25490;&#21517;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#22522;&#22240;&#38598;&#21512;&#36827;&#34892;&#25490;&#24207;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#22522;&#22240;&#38598;&#21512;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#20197;&#21450;&#38598;&#21512;&#20013;&#30340;&#21333;&#20010;&#20803;&#32032;&#21644;&#22823;&#23567;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#25216;&#24039;&#35268;&#36991;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#22240;&#38598;&#21512;&#30340;&#29983;&#29289;&#23398;&#35282;&#33394;&#34987;&#29992;&#20110;&#23558;&#20854;&#20998;&#32452;&#25104;&#38598;&#21512;&#65292;&#24182;&#19988;&#36825;&#20123;&#38598;&#21512;&#36890;&#24120;&#26159;&#39640;&#32500;&#12289;&#37325;&#21472;&#21644;&#20887;&#20313;&#30340;&#12290;&#36825;&#23548;&#33268;&#20102;&#23545;&#20854;&#20869;&#23481;&#30340;&#35299;&#37322;&#21644;&#30740;&#31350;&#24182;&#19981;&#30452;&#35266;&#12290;&#20026;&#20102;&#38477;&#20302;&#32500;&#24230;&#25110;&#22686;&#21152;&#21487;&#35299;&#37322;&#24615;&#65292;&#29983;&#29289;&#20449;&#24687;&#23398;&#27491;&#22312;&#23547;&#25214;&#35299;&#20915;&#26041;&#26696;&#12290;&#19968;&#31181;&#21487;&#33021;&#24615;&#26159;&#23558;&#37325;&#21472;&#30340;&#22522;&#22240;&#38598;&#21512;&#32858;&#21512;&#25104;&#26356;&#22823;&#30340;&#36890;&#36335;&#65292;&#20294;&#20462;&#25913;&#21518;&#30340;&#29983;&#29289;&#36890;&#36335;&#24456;&#38590;&#22312;&#29983;&#29289;&#23398;&#19978;&#24471;&#21040;&#35777;&#26126;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#37325;&#35201;&#24615;&#20998;&#25968;&#23545;&#38598;&#21512;&#20013;&#30340;&#36890;&#36335;&#36827;&#34892;&#25490;&#21517;&#65292;&#24182;&#20174;&#38598;&#21512;&#35206;&#30422;&#30340;&#35282;&#24230;&#36827;&#34892;&#30740;&#31350;&#12290;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;Shapley&#20540;&#30340;&#20998;&#25968;&#32771;&#34385;&#20102;&#21333;&#20010;&#20803;&#32032;&#21644;&#38598;&#21512;&#22823;&#23567;&#22312;&#23478;&#26063;&#20013;&#30340;&#20998;&#24067;&#65307;&#27492;&#22806;&#65292;&#19968;&#31181;&#25216;&#24039;&#20801;&#35768;&#25105;&#20204;&#35268;&#36991;Shapley&#20540;&#35745;&#31639;&#30340;&#25351;&#25968;&#22797;&#26434;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#25152;&#24471;&#25490;&#21517;&#20013;&#32771;&#34385;&#20887;&#20313;&#24863;&#30693;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The biological roles of gene sets are used to group them into collections. These collections are often characterized by being high-dimensional, overlapping, and redundant families of sets, thus precluding a straightforward interpretation and study of their content. Bioinformatics looked for solutions to reduce their dimension or increase their intepretability. One possibility lies in aggregating overlapping gene sets to create larger pathways, but the modified biological pathways are hardly biologically justifiable. We propose to use importance scores to rank the pathways in the collections studying the context from a set covering perspective. The proposed Shapley values-based scores consider the distribution of the singletons and the size of the sets in the families; Furthermore, a trick allows us to circumvent the usual exponential complexity of Shapley values' computation. Finally, we address the challenge of including a redundancy awareness in the obtained rankings where, in our ca
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#19968;&#31867;&#23494;&#24230;&#27604;&#29575;&#20272;&#35745;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#30340;&#21442;&#25968;&#36873;&#25321;&#21407;&#21017;&#65292;&#24182;&#22312;&#26377;&#38480;&#26679;&#26412;&#24773;&#20917;&#19979;&#25512;&#23548;&#20986;&#26032;&#30340;&#35823;&#24046;&#30028;&#12290;&#20854;&#26041;&#27861;&#22312;&#20108;&#27425;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26497;&#23567;&#21270;&#26368;&#20248;&#35823;&#24046;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.16164</link><description>&lt;p&gt;
&#22312;RKHS&#20013;&#33258;&#36866;&#24212;&#23398;&#20064;&#23494;&#24230;&#27604;&#29575;
&lt;/p&gt;
&lt;p&gt;
Adaptive learning of density ratios in RKHS. (arXiv:2307.16164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16164
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#19968;&#31867;&#23494;&#24230;&#27604;&#29575;&#20272;&#35745;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#30340;&#21442;&#25968;&#36873;&#25321;&#21407;&#21017;&#65292;&#24182;&#22312;&#26377;&#38480;&#26679;&#26412;&#24773;&#20917;&#19979;&#25512;&#23548;&#20986;&#26032;&#30340;&#35823;&#24046;&#30028;&#12290;&#20854;&#26041;&#27861;&#22312;&#20108;&#27425;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26497;&#23567;&#21270;&#26368;&#20248;&#35823;&#24046;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26377;&#38480;&#25968;&#37327;&#30340;&#23494;&#24230;&#35266;&#27979;&#20013;&#20272;&#35745;&#20004;&#20010;&#27010;&#29575;&#23494;&#24230;&#30340;&#27604;&#29575;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65292;&#24212;&#29992;&#21253;&#25324;&#21452;&#26679;&#26412;&#26816;&#39564;&#12289;&#20998;&#27495;&#20272;&#35745;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#21327;&#21464;&#37327;&#36716;&#31227;&#36866;&#24212;&#12289;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#21644;&#26032;&#39062;&#24615;&#26816;&#27979;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#22823;&#31867;&#23494;&#24230;&#27604;&#29575;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#20204;&#36890;&#36807;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#20013;&#26368;&#23567;&#21270;&#30495;&#23454;&#23494;&#24230;&#27604;&#29575;&#19982;&#27169;&#22411;&#20043;&#38388;&#30340;&#27491;&#21017;Bregman&#36317;&#31163;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#26032;&#30340;&#26377;&#38480;&#26679;&#26412;&#35823;&#24046;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;Lepskii&#31867;&#22411;&#30340;&#21442;&#25968;&#36873;&#25321;&#21407;&#21017;&#65292;&#22312;&#19981;&#30693;&#36947;&#23494;&#24230;&#27604;&#29575;&#30340;&#27491;&#21017;&#24615;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#35823;&#24046;&#30028;&#12290;&#22312;&#20108;&#27425;&#25439;&#22833;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#36866;&#24212;&#22320;&#23454;&#29616;&#20102;&#26497;&#23567;&#21270;&#26368;&#20248;&#35823;&#24046;&#29575;&#12290;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#20540;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the ratio of two probability densities from finitely many observations of the densities is a central problem in machine learning and statistics with applications in two-sample testing, divergence estimation, generative modeling, covariate shift adaptation, conditional density estimation, and novelty detection. In this work, we analyze a large class of density ratio estimation methods that minimize a regularized Bregman divergence between the true density ratio and a model in a reproducing kernel Hilbert space (RKHS). We derive new finite-sample error bounds, and we propose a Lepskii type parameter choice principle that minimizes the bounds without knowledge of the regularity of the density ratio. In the special case of quadratic loss, our method adaptively achieves a minimax optimal error rate. A numerical illustration is provided.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;Q&#20989;&#25968;&#20272;&#35745;&#22120;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#22120;QEM&#21644;DRL&#31639;&#27861;QEMRL&#65292;&#22312;Atari&#21644;Mujoco&#22522;&#20934;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#25910;&#25947;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2307.16152</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#24046;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Variance Control for Distributional Reinforcement Learning. (arXiv:2307.16152v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;Q&#20989;&#25968;&#20272;&#35745;&#22120;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#22120;QEM&#21644;DRL&#31639;&#27861;QEMRL&#65292;&#22312;Atari&#21644;Mujoco&#22522;&#20934;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#25910;&#25947;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#33719;&#24471;&#30340;Q&#20989;&#25968;&#20272;&#35745;&#22120;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#20805;&#20998;&#20102;&#35299;Q&#20989;&#25968;&#30340;&#36817;&#20284;&#35823;&#24046;&#22914;&#20309;&#24433;&#21709;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20123;&#35823;&#24046;&#20998;&#26512;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#23637;&#31034;&#20102;&#22914;&#20309;&#21516;&#26102;&#20943;&#23567;&#35823;&#24046;&#39033;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;&#22312;&#36825;&#31181;&#26032;&#30340;&#29702;&#35299;&#19979;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#20272;&#35745;&#22120;&#8220;Quantiled Expansion Mean&#8221;&#65288;QEM&#65289;&#65292;&#24182;&#20174;&#32479;&#35745;&#23398;&#30340;&#35282;&#24230;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;DRL&#31639;&#27861;&#65288;QEMRL&#65289;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;Atari&#21644;Mujoco&#22522;&#20934;&#20219;&#21153;&#19978;&#24191;&#27867;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;QEMRL&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;QEMRL&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#25910;&#25947;&#24615;&#33021;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although distributional reinforcement learning (DRL) has been widely examined in the past few years, very few studies investigate the validity of the obtained Q-function estimator in the distributional setting. To fully understand how the approximation errors of the Q-function affect the whole training process, we do some error analysis and theoretically show how to reduce both the bias and the variance of the error terms. With this new understanding, we construct a new estimator \emph{Quantiled Expansion Mean} (QEM) and introduce a new DRL algorithm (QEMRL) from the statistical perspective. We extensively evaluate our QEMRL algorithm on a variety of Atari and Mujoco benchmark tasks and demonstrate that QEMRL achieves significant improvement over baseline algorithms in terms of sample efficiency and convergence performance.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LSTM&#21644;DDPM&#30456;&#32467;&#21512;&#30340;&#26041;&#26696;&#26469;&#35299;&#20915;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;&#30340;&#33021;&#37327;&#30423;&#31363;&#26816;&#27979;&#21644;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#26500;&#21644;&#39044;&#27979;&#35823;&#24046;&#65292;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33021;&#37327;&#30423;&#31363;&#30340;&#23454;&#20363;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.16149</link><description>&lt;p&gt;
&#26234;&#33021;&#30005;&#32593;&#20013;&#19968;&#31181;&#26377;&#25928;&#30340;&#29992;&#20110;&#33021;&#37327;&#30423;&#31363;&#26816;&#27979;&#21644;&#39044;&#27979;&#30340;LSTM-DDPM&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid. (arXiv:2307.16149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16149
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LSTM&#21644;DDPM&#30456;&#32467;&#21512;&#30340;&#26041;&#26696;&#26469;&#35299;&#20915;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;&#30340;&#33021;&#37327;&#30423;&#31363;&#26816;&#27979;&#21644;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#26500;&#21644;&#39044;&#27979;&#35823;&#24046;&#65292;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33021;&#37327;&#30423;&#31363;&#30340;&#23454;&#20363;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#30423;&#31363;&#26816;&#27979;&#65288;ETD&#65289;&#21644;&#33021;&#37327;&#28040;&#32791;&#39044;&#27979;&#65288;ECF&#65289;&#26159;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;&#20004;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#25361;&#25112;&#12290;&#20849;&#21516;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#30830;&#20445;&#31995;&#32479;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;&#30340;ETD&#21644;ECF&#30340;&#30456;&#20114;&#20851;&#32852;&#25361;&#25112;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#32467;&#21512;&#20102;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#36755;&#20837;&#37325;&#26500;&#21644;&#39044;&#27979;&#12290;&#36890;&#36807;&#21033;&#29992;&#37325;&#26500;&#21644;&#39044;&#27979;&#35823;&#24046;&#65292;&#31995;&#32479;&#33021;&#22815;&#35782;&#21035;&#33021;&#37327;&#30423;&#31363;&#30340;&#23454;&#20363;&#65292;&#22522;&#20110;&#37325;&#26500;&#35823;&#24046;&#21644;&#39044;&#27979;&#35823;&#24046;&#30340;&#26041;&#27861;&#30456;&#20114;&#34917;&#20805;&#65292;&#21487;&#20197;&#26816;&#27979;&#19981;&#21516;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;&#36890;&#36807;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#22312;ETD&#21644;ECF&#38382;&#39064;&#19978;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;&#38598;&#25104;&#26041;&#27861;&#26174;&#33879;&#25552;&#21319;&#20102;ETD&#24615;&#33021;&#65292;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#21040;&#22522;&#20934;&#26041;&#27861;&#26410;&#33021;&#26816;&#27979;&#21040;&#30340;&#33021;&#37327;&#30423;&#31363;&#25915;&#20987;&#12290;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;ETD&#21644;ECF&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy theft detection (ETD) and energy consumption forecasting (ECF) are two interconnected challenges in smart grid systems. Addressing these issues collectively is crucial for ensuring system security. This paper addresses the interconnected challenges of ETD and ECF in smart grid systems. The proposed solution combines long short-term memory (LSTM) and a denoising diffusion probabilistic model (DDPM) to generate input reconstruction and forecasting. By leveraging the reconstruction and forecasting errors, the system identifies instances of energy theft, with the methods based on reconstruction error and forecasting error complementing each other in detecting different types of attacks. Through extensive experiments on real-world and synthetic datasets, the proposed scheme outperforms baseline methods in ETD and ECF problems. The ensemble method significantly enhances ETD performance, accurately detecting energy theft attacks that baseline methods fail to detect. The research offers
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#30643;&#23380;&#23398;&#20064;&#26426;&#21046;&#65288;PLM&#65289;&#65292;&#36890;&#36807;&#30643;&#23380;&#23398;&#20064;&#36807;&#31243;&#26469;&#20462;&#25913;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#21644;&#26435;&#37325;&#65292;&#35299;&#20915;&#26799;&#24230;&#28040;&#22833;&#21644;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;PLM&#27169;&#22359;&#35774;&#35745;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30456;&#23545;&#20110;&#20854;&#20182;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.16141</link><description>&lt;p&gt;
&#30643;&#23380;&#23398;&#20064;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Pupil Learning Mechanism. (arXiv:2307.16141v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#30643;&#23380;&#23398;&#20064;&#26426;&#21046;&#65288;PLM&#65289;&#65292;&#36890;&#36807;&#30643;&#23380;&#23398;&#20064;&#36807;&#31243;&#26469;&#20462;&#25913;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#21644;&#26435;&#37325;&#65292;&#35299;&#20915;&#26799;&#24230;&#28040;&#22833;&#21644;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;PLM&#27169;&#22359;&#35774;&#35745;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30456;&#23545;&#20110;&#20854;&#20182;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#30740;&#31350;&#20013;&#65292;&#24456;&#23569;&#28041;&#21450;&#26799;&#24230;&#28040;&#22833;&#21644;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36981;&#24490;&#30643;&#23380;&#23398;&#20064;&#36807;&#31243;&#65292;&#36890;&#36807;&#35299;&#37322;&#12289;&#36873;&#25321;&#12289;&#29702;&#35299;&#12289;&#32972;&#35829;&#21644;&#25972;&#29702;&#31561;&#29305;&#24449;&#65292;&#25512;&#23548;&#20986;&#20462;&#25913;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#65288;2LNNs&#65289;&#30340;&#32593;&#32476;&#32467;&#26500;&#21644;&#26435;&#37325;&#30340;&#30643;&#23380;&#23398;&#20064;&#26426;&#21046;&#65288;PLM&#65289;&#12290;PLM&#21253;&#25324;&#39034;&#24207;&#23398;&#20064;&#12289;&#33258;&#36866;&#24212;&#23398;&#20064;&#12289;&#23436;&#32654;&#23398;&#20064;&#21644;&#23569;&#36807;&#25311;&#21512;&#23398;&#20064;&#31561;&#27169;&#22359;&#12290;&#22522;&#20110;&#19968;&#20010;&#38108;&#20215;&#39044;&#27979;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;PLM&#27169;&#22359;&#35774;&#35745;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35780;&#20272;&#20102;PLM&#30340;&#24615;&#33021;&#12290;&#23454;&#35777;&#32467;&#26524;&#30830;&#23454;&#35777;&#23454;&#20102;PLM&#27169;&#22359;&#35774;&#35745;&#30340;&#20248;&#36234;&#24615;&#65292;&#24182;&#26174;&#31034;&#20986;&#25152;&#25552;&#20986;&#30340;PLM&#27169;&#22411;&#30456;&#23545;&#20110;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#21644;&#22522;&#20110;&#20256;&#32479;&#21453;&#21521;&#20256;&#25773;&#30340;2LNN&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studies on artificial neural networks rarely address both vanishing gradients and overfitting issues. In this study, we follow the pupil learning procedure, which has the features of interpreting, picking, understanding, cramming, and organizing, to derive the pupil learning mechanism (PLM) by which to modify the network structure and weights of 2-layer neural networks (2LNNs). The PLM consists of modules for sequential learning, adaptive learning, perfect learning, and less-overfitted learning. Based upon a copper price forecasting dataset, we conduct an experiment to validate the PLM module design modules, and an experiment to evaluate the performance of PLM. The empirical results indeed approve the PLM module design and show the superiority of the proposed PLM model over the linear regression model and the conventional backpropagation-based 2LNN model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25143;&#21487;&#25511;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#35843;&#33410;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#22238;&#24212;&#26102;&#21019;&#36896;&#21147;&#21644;&#23545;&#22806;&#37096;&#30693;&#35782;&#30340;&#24544;&#35802;&#24230;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#36825;&#31181;&#26426;&#21046;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#25968;&#20540;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#21270;&#36807;&#31243;&#35745;&#31639;&#26631;&#35760;&#30340;&#31243;&#24230;&#65292;&#20174;&#32780;&#23454;&#29616;&#29992;&#25143;&#23545;&#27169;&#22411;&#30340;&#20381;&#36182;&#31243;&#24230;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.16139</link><description>&lt;p&gt;
&#29992;&#25143;&#21487;&#25511;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#34701;&#21512;&#65306;&#24179;&#34913;&#21019;&#36896;&#21147;&#21644;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
User-Controlled Knowledge Fusion in Large Language Models: Balancing Creativity and Hallucination. (arXiv:2307.16139v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25143;&#21487;&#25511;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#35843;&#33410;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#22238;&#24212;&#26102;&#21019;&#36896;&#21147;&#21644;&#23545;&#22806;&#37096;&#30693;&#35782;&#30340;&#24544;&#35802;&#24230;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#36825;&#31181;&#26426;&#21046;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#25968;&#20540;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#21270;&#36807;&#31243;&#35745;&#31639;&#26631;&#35760;&#30340;&#31243;&#24230;&#65292;&#20174;&#32780;&#23454;&#29616;&#29992;&#25143;&#23545;&#27169;&#22411;&#30340;&#20381;&#36182;&#31243;&#24230;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#23545;&#35805;&#31995;&#32479;&#20013;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#29983;&#25104;&#22810;&#26679;&#12289;&#30456;&#20851;&#19988;&#26377;&#21019;&#36896;&#24615;&#30340;&#22238;&#24212;&#33021;&#21147;&#32780;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#23613;&#31649;LLMs&#20855;&#26377;&#36825;&#20123;&#20248;&#28857;&#65292;&#20294;&#22312;&#21019;&#36896;&#21147;&#21644;&#23545;&#22806;&#37096;&#30693;&#35782;&#30340;&#24544;&#35802;&#24230;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#29992;&#25143;&#21487;&#25511;&#26426;&#21046;&#65292;&#29992;&#20110;&#35843;&#33410;LLM&#22312;&#24819;&#35937;&#33021;&#21147;&#21644;&#19982;&#20107;&#23454;&#20449;&#24687;&#30340;&#19968;&#33268;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;LLM&#30340;&#35757;&#32451;&#30340;&#24494;&#35843;&#38454;&#27573;&#20013;&#24341;&#20837;&#19968;&#20010;&#34920;&#31034;&#29983;&#25104;&#22238;&#24212;&#20013;&#23545;&#21442;&#32771;&#30693;&#35782;&#24544;&#35802;&#24230;&#31243;&#24230;&#30340;&#25968;&#20540;&#26631;&#35760;&#12290;&#36825;&#20010;&#31243;&#24230;&#26159;&#36890;&#36807;&#33258;&#21160;&#21270;&#36807;&#31243;&#35745;&#31639;&#30340;&#65292;&#35813;&#36807;&#31243;&#20351;&#29992;ROUGE&#20998;&#25968;&#34913;&#37327;&#35789;&#27719;&#37325;&#21472;&#65292;&#20351;&#29992;Sentence-BERT&#23884;&#20837;&#34913;&#37327;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;LLM&#30340;&#33258;&#25105;&#35780;&#20272;&#20998;&#25968;&#12290;&#22312;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#29992;&#25143;&#21487;&#20197;&#25805;&#20316;&#36825;&#20010;&#25968;&#20540;&#26631;&#35760;&#65292;&#20174;&#32780;&#25511;&#21046;LLM&#23545;&#22806;&#37096;&#30693;&#35782;&#30340;&#20381;&#36182;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern dialogue systems, the use of Large Language Models (LLMs) has grown exponentially due to their capacity to generate diverse, relevant, and creative responses. Despite their strengths, striking a balance between the LLMs' creativity and their faithfulness to external knowledge remains a key challenge. This paper presents an innovative user-controllable mechanism that modulates the balance between an LLM's imaginative capabilities and its adherence to factual information. Our approach incorporates a numerical tag during the fine-tuning phase of the LLM's training, representing the degree of faithfulness to the reference knowledge in the generated responses. This degree is computed through an automated process that measures lexical overlap using ROUGE scores, semantic similarity using Sentence-BERT embeddings, and an LLM's self-evaluation score. During model inference, users can manipulate this numerical tag, thus controlling the degree of the LLM's reliance on external knowledg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24490;&#29615;&#21160;&#37327;&#21152;&#36895;&#30340;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#24212;&#29992;&#20110;&#38750;&#32447;&#24615;&#36870;&#21521;&#25104;&#20687;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#20445;&#30041;&#20808;&#21069;&#26799;&#24230;&#30340;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#38750;&#32447;&#24615;&#36870;&#21521;&#38382;&#39064;&#19978;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.16120</link><description>&lt;p&gt;
&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#19982;&#24490;&#29615;&#21160;&#37327;&#21152;&#36895;&#29992;&#20110;&#38750;&#32447;&#24615;&#21453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Deep Unrolling Networks with Recurrent Momentum Acceleration for Nonlinear Inverse Problems. (arXiv:2307.16120v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24490;&#29615;&#21160;&#37327;&#21152;&#36895;&#30340;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#24212;&#29992;&#20110;&#38750;&#32447;&#24615;&#36870;&#21521;&#25104;&#20687;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#20445;&#30041;&#20808;&#21069;&#26799;&#24230;&#30340;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#38750;&#32447;&#24615;&#36870;&#21521;&#38382;&#39064;&#19978;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#22522;&#20110;&#27169;&#22411;&#30340;&#36845;&#20195;&#31639;&#27861;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;(DuNets)&#24050;&#25104;&#20026;&#35299;&#20915;&#36870;&#21521;&#25104;&#20687;&#38382;&#39064;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#34429;&#28982;DuNets&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35768;&#22810;&#32447;&#24615;&#36870;&#21521;&#38382;&#39064;&#65292;&#20294;&#38750;&#32447;&#24615;&#38382;&#39064;&#24448;&#24448;&#20250;&#24433;&#21709;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#21463;&#20248;&#21270;&#31639;&#27861;&#20013;&#24120;&#29992;&#30340;&#21160;&#37327;&#21152;&#36895;&#25216;&#26415;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#21160;&#37327;&#21152;&#36895;(RMA)&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(LSTM-RNN)&#26469;&#27169;&#25311;&#21160;&#37327;&#21152;&#36895;&#36807;&#31243;&#12290;RMA&#27169;&#22359;&#21033;&#29992;LSTM-RNN&#23398;&#20064;&#21644;&#20445;&#30041;&#20808;&#21069;&#26799;&#24230;&#30340;&#30693;&#35782;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;RMA&#24212;&#29992;&#20110;&#20004;&#31181;&#27969;&#34892;&#30340;DuNets&#8212;&#8212;&#23398;&#20064;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;(LPGD)&#21644;&#23398;&#20064;&#30340;&#21407;&#22987;-&#23545;&#20598;(LPD)&#26041;&#27861;&#65292;&#20998;&#21035;&#24471;&#21040;LPGD-RMA&#21644;LPD-RMA&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#38750;&#32447;&#24615;&#36870;&#21521;&#38382;&#39064;&#19978;&#25552;&#20379;&#20102;&#23454;&#39564;&#32467;&#26524;&#65306;&#38750;&#32447;&#24615;&#21435;&#21367;&#31215;&#38382;&#39064;&#12289;
&lt;/p&gt;
&lt;p&gt;
Combining the strengths of model-based iterative algorithms and data-driven deep learning solutions, deep unrolling networks (DuNets) have become a popular tool to solve inverse imaging problems. While DuNets have been successfully applied to many linear inverse problems, nonlinear problems tend to impair the performance of the method. Inspired by momentum acceleration techniques that are often used in optimization algorithms, we propose a recurrent momentum acceleration (RMA) framework that uses a long short-term memory recurrent neural network (LSTM-RNN) to simulate the momentum acceleration process. The RMA module leverages the ability of the LSTM-RNN to learn and retain knowledge from the previous gradients. We apply RMA to two popular DuNets -- the learned proximal gradient descent (LPGD) and the learned primal-dual (LPD) methods, resulting in LPGD-RMA and LPD-RMA respectively. We provide experimental results on two nonlinear inverse problems: a nonlinear deconvolution problem, an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Taylor&#26144;&#23556;&#22240;&#24335;&#20998;&#35299;&#30340;&#39640;&#38454;&#22810;&#39033;&#24335;&#22238;&#24402;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#30446;&#26631;&#22238;&#24402;&#21644;&#25429;&#25417;&#30446;&#26631;&#20043;&#38388;&#30340;&#20869;&#22312;&#20851;&#31995;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#22238;&#24402;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16105</link><description>&lt;p&gt;
TMPNN: &#22522;&#20110;Taylor&#26144;&#23556;&#22240;&#24335;&#20998;&#35299;&#30340;&#39640;&#38454;&#22810;&#39033;&#24335;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
TMPNN: High-Order Polynomial Regression Based on Taylor Map Factorization. (arXiv:2307.16105v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Taylor&#26144;&#23556;&#22240;&#24335;&#20998;&#35299;&#30340;&#39640;&#38454;&#22810;&#39033;&#24335;&#22238;&#24402;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#30446;&#26631;&#22238;&#24402;&#21644;&#25429;&#25417;&#30446;&#26631;&#20043;&#38388;&#30340;&#20869;&#22312;&#20851;&#31995;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#22238;&#24402;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#24335;&#22238;&#24402;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#34920;&#36798;&#38750;&#32447;&#24615;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#24456;&#39640;&#30340;&#22810;&#39033;&#24335;&#38454;&#25968;&#21487;&#33021;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#23545;&#26410;&#35265;&#25968;&#25454;&#30340;&#25311;&#21512;&#33021;&#21147;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Taylor&#26144;&#23556;&#22240;&#24335;&#20998;&#35299;&#26500;&#24314;&#39640;&#38454;&#22810;&#39033;&#24335;&#22238;&#24402;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33258;&#28982;&#22320;&#23454;&#29616;&#20102;&#22810;&#30446;&#26631;&#22238;&#24402;&#65292;&#24182;&#33021;&#25429;&#25417;&#30446;&#26631;&#20043;&#38388;&#30340;&#20869;&#22312;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#20197;&#24494;&#20998;&#26041;&#31243;&#31995;&#32479;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#36890;&#36807;&#22312;UCI&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;Feynman&#35937;&#24449;&#22238;&#24402;&#25968;&#25454;&#38598;&#21644;Friedman-1&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#22238;&#24402;&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Polynomial regression is widely used and can help to express nonlinear patterns. However, considering very high polynomial orders may lead to overfitting and poor extrapolation ability for unseen data. The paper presents a method for constructing a high-order polynomial regression based on the Taylor map factorization. This method naturally implements multi-target regression and can capture internal relationships between targets. Additionally, we introduce an approach for model interpretation in the form of systems of differential equations. By benchmarking on UCI open access datasets, Feynman symbolic regression datasets, and Friedman-1 datasets, we demonstrate that the proposed method performs comparable to the state-of-the-art regression methods and outperforms them on specific tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26410;&#32463;&#27979;&#37327;&#27969;&#22495;&#30340;&#26497;&#31471;&#27700;&#25991;&#20107;&#20214;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20840;&#29699;&#27946;&#27700;&#39044;&#35686;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2307.16104</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25552;&#39640;&#20102;&#20840;&#29699;&#21487;&#38752;&#27946;&#27700;&#39044;&#35686;&#30340;&#35206;&#30422;&#33539;&#22260;
&lt;/p&gt;
&lt;p&gt;
AI Increases Global Access to Reliable Flood Forecasts. (arXiv:2307.16104v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26410;&#32463;&#27979;&#37327;&#27969;&#22495;&#30340;&#26497;&#31471;&#27700;&#25991;&#20107;&#20214;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20840;&#29699;&#27946;&#27700;&#39044;&#35686;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27946;&#27700;&#26159;&#26368;&#24120;&#35265;&#21644;&#24433;&#21709;&#26368;&#22823;&#30340;&#33258;&#28982;&#28798;&#23475;&#20043;&#19968;&#65292;&#23545;&#21457;&#23637;&#20013;&#22269;&#23478;&#23588;&#20854;&#20855;&#26377;&#19981;&#23545;&#31216;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#22269;&#23478;&#24448;&#24448;&#32570;&#20047;&#23494;&#38598;&#30340;&#27700;&#27969;&#30417;&#27979;&#32593;&#32476;&#12290;&#20934;&#30830;&#21450;&#26102;&#30340;&#39044;&#35686;&#23545;&#20110;&#20943;&#36731;&#27946;&#27700;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20934;&#30830;&#30340;&#27700;&#25991;&#27169;&#25311;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#26681;&#25454;&#27599;&#20010;&#24212;&#29992;&#30340;&#27969;&#22495;&#20013;&#30340;&#38271;&#26102;&#38388;&#25968;&#25454;&#35760;&#24405;&#36827;&#34892;&#26657;&#20934;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;7&#22825;&#20869;&#30340;&#26497;&#31471;&#27700;&#25991;&#20107;&#20214;&#12290;&#35813;&#27169;&#22411;&#22312;&#25152;&#26377;&#22823;&#27954;&#12289;&#21069;&#23548;&#26102;&#38388;&#21644;&#37325;&#29616;&#26399;&#20013;&#22343;&#26126;&#26174;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20840;&#29699;&#27700;&#25991;&#27169;&#22411;&#65288;Copernicus&#24212;&#24613;&#31649;&#29702;&#26381;&#21153;&#20840;&#29699;&#27946;&#27700;&#24847;&#35782;&#31995;&#32479;&#65289;&#12290;AI&#22312;&#26410;&#32463;&#27979;&#37327;&#30340;&#27969;&#22495;&#20013;&#30340;&#39044;&#27979;&#23588;&#20854;&#26377;&#25928;&#65292;&#36825;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#20840;&#29699;&#21482;&#26377;&#30334;&#20998;&#20043;&#20960;&#30340;&#27969;&#22495;&#20855;&#26377;&#27969;&#37327;&#35266;&#27979;&#31449;&#65292;&#32780;&#21457;&#23637;&#20013;&#22269;&#23478;&#30340;&#26410;&#32463;&#27979;&#37327;&#30340;&#27969;&#22495;&#25968;&#37327;&#21344;&#27604;&#24456;&#39640;&#65292;&#23545;&#20154;&#31867;&#29305;&#21035;&#33030;&#24369;&#12290;
&lt;/p&gt;
&lt;p&gt;
Floods are one of the most common and impactful natural disasters, with a disproportionate impact in developing countries that often lack dense streamflow monitoring networks. Accurate and timely warnings are critical for mitigating flood risks, but accurate hydrological simulation models typically must be calibrated to long data records in each watershed where they are applied. We developed an Artificial Intelligence (AI) model to predict extreme hydrological events at timescales up to 7 days in advance. This model significantly outperforms current state of the art global hydrology models (the Copernicus Emergency Management Service Global Flood Awareness System) across all continents, lead times, and return periods. AI is especially effective at forecasting in ungauged basins, which is important because only a few percent of the world's watersheds have stream gauges, with a disproportionate number of ungauged basins in developing countries that are especially vulnerable to the human 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#29702;&#24819;&#30340;&#23545;&#25239;&#25915;&#20987;&#36827;&#34892;&#36817;&#20284;&#34920;&#31034;&#65292;&#24182;&#23558;&#23545;&#25239;&#35757;&#32451;&#36716;&#21270;&#20026;&#36827;&#25915;&#32593;&#32476;&#21644;&#38450;&#23432;&#32593;&#32476;&#20043;&#38388;&#30340;&#25968;&#23398;&#21338;&#24328;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#23545;&#25239;&#35757;&#32451;&#22312;&#26679;&#26412;&#22823;&#23567;$n$&#19979;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.16099</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#29702;&#24819;&#23545;&#25239;&#25915;&#20987;&#21644;&#23545;&#25239;&#35757;&#32451;&#25910;&#25947;&#24615;&#30340;&#35770;&#25991;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
On Neural Network approximation of ideal adversarial attack and convergence of adversarial training. (arXiv:2307.16099v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16099
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#29702;&#24819;&#30340;&#23545;&#25239;&#25915;&#20987;&#36827;&#34892;&#36817;&#20284;&#34920;&#31034;&#65292;&#24182;&#23558;&#23545;&#25239;&#35757;&#32451;&#36716;&#21270;&#20026;&#36827;&#25915;&#32593;&#32476;&#21644;&#38450;&#23432;&#32593;&#32476;&#20043;&#38388;&#30340;&#25968;&#23398;&#21338;&#24328;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#23545;&#25239;&#35757;&#32451;&#22312;&#26679;&#26412;&#22823;&#23567;$n$&#19979;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#36890;&#24120;&#26159;&#36890;&#36807;&#23545;&#36755;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#36827;&#34892;&#22522;&#20110;&#26799;&#24230;&#30340;&#25805;&#20316;&#26469;&#23454;&#29616;&#30340;&#65292;&#36825;&#23548;&#33268;&#27599;&#27425;&#29983;&#25104;&#25915;&#20987;&#26102;&#37117;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#30340;&#35745;&#31639;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#23545;&#25239;&#25915;&#20987;&#34920;&#31034;&#20026;&#21487;&#35757;&#32451;&#30340;&#20989;&#25968;&#30340;&#24605;&#24819;&#26356;&#21152;&#24041;&#22266;&#65292;&#32780;&#26080;&#38656;&#36827;&#19968;&#27493;&#35745;&#31639;&#26799;&#24230;&#12290;&#25105;&#20204;&#39318;&#20808;&#28608;&#21457;&#20986;&#22312;&#36866;&#24403;&#26465;&#20214;&#19979;&#65292;&#29702;&#35770;&#19978;&#30340;&#26368;&#20339;&#25915;&#20987;&#21487;&#20197;&#34920;&#31034;&#20026;&#20809;&#28369;&#30340;&#20998;&#27573;&#20989;&#25968;&#65288;&#20998;&#27573;H\"older&#20989;&#25968;&#65289;&#12290;&#28982;&#21518;&#25105;&#20204;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#24471;&#21040;&#20102;&#36825;&#20123;&#20989;&#25968;&#30340;&#36817;&#20284;&#32467;&#26524;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#29702;&#24819;&#30340;&#25915;&#20987;&#36807;&#31243;&#65292;&#24182;&#23558;&#23545;&#25239;&#35757;&#32451;&#21270;&#31616;&#20026;&#36827;&#25915;&#32593;&#32476;&#21644;&#38450;&#23432;&#27169;&#22411;&#65288;&#38450;&#23432;&#32593;&#32476;&#65289;&#20043;&#38388;&#30340;&#25968;&#23398;&#21338;&#24328;&#12290;&#22312;&#36825;&#26679;&#30340;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#36824;&#24471;&#21040;&#20102;&#23545;&#25239;&#35757;&#32451;&#30340;&#26679;&#26412;&#22823;&#23567;$n$&#23545;&#20110;&#23545;&#25239;&#25439;&#22833;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks are usually expressed in terms of a gradient-based operation on the input data and model, this results in heavy computations every time an attack is generated. In this work, we solidify the idea of representing adversarial attacks as a trainable function, without further gradient computation. We first motivate that the theoretical best attacks, under proper conditions, can be represented as smooth piece-wise functions (piece-wise H\"older functions). Then we obtain an approximation result of such functions by a neural network. Subsequently, we emulate the ideal attack process by a neural network and reduce the adversarial training to a mathematical game between an attack network and a training model (a defense network). We also obtain convergence rates of adversarial loss in terms of the sample size $n$ for adversarial training in such a setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ADR-GNN&#65292;&#19968;&#31181;&#22522;&#20110;&#24179;&#27969;-&#25193;&#25955;-&#21453;&#24212;&#31995;&#32479;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;ADR-GNN&#22312;&#28041;&#21450;&#24179;&#27969;&#30340;&#22797;&#26434;&#29616;&#35937;&#24314;&#27169;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#22312;&#33410;&#28857;&#20998;&#31867;&#21644;&#26102;&#31354;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20986;&#19982;&#26368;&#20808;&#36827;&#32593;&#32476;&#30456;&#27604;&#30340;&#25913;&#36827;&#25110;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.16092</link><description>&lt;p&gt;
ADR-GNN&#65306;&#24179;&#27969;-&#25193;&#25955;-&#21453;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ADR-GNN: Advection-Diffusion-Reaction Graph Neural Networks. (arXiv:2307.16092v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ADR-GNN&#65292;&#19968;&#31181;&#22522;&#20110;&#24179;&#27969;-&#25193;&#25955;-&#21453;&#24212;&#31995;&#32479;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;ADR-GNN&#22312;&#28041;&#21450;&#24179;&#27969;&#30340;&#22797;&#26434;&#29616;&#35937;&#24314;&#27169;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#22312;&#33410;&#28857;&#20998;&#31867;&#21644;&#26102;&#31354;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20986;&#19982;&#26368;&#20808;&#36827;&#32593;&#32476;&#30456;&#27604;&#30340;&#25913;&#36827;&#25110;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;GNN&#22312;&#24314;&#27169;&#28041;&#21450;&#24179;&#27969;&#30340;&#22797;&#26434;&#29616;&#35937;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24179;&#27969;-&#25193;&#25955;-&#21453;&#24212;&#31995;&#32479;&#30340;&#26032;&#22411;GNN&#26550;&#26500;&#65292;&#31216;&#20026;ADR-GNN&#12290;&#24179;&#27969;&#27169;&#24335;&#21270;&#20102;&#20449;&#24687;&#30340;&#26377;&#21521;&#20256;&#36755;&#65292;&#25193;&#25955;&#25429;&#25417;&#20102;&#20449;&#24687;&#30340;&#23616;&#37096;&#24179;&#28369;&#65292;&#21453;&#24212;&#20195;&#34920;&#20102;&#36890;&#36947;&#20013;&#20449;&#24687;&#30340;&#38750;&#32447;&#24615;&#36716;&#25442;&#12290;&#25105;&#20204;&#23545;ADR-GNN&#30340;&#23450;&#24615;&#34892;&#20026;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#26174;&#31034;&#20102;&#23558;&#24179;&#27969;&#12289;&#25193;&#25955;&#21644;&#21453;&#24212;&#32467;&#21512;&#30340;&#22909;&#22788;&#12290;&#20026;&#20102;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#33410;&#28857;&#20998;&#31867;&#21644;&#26102;&#31354;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;ADR-GNN&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#32593;&#32476;&#30340;&#25913;&#36827;&#25110;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have shown remarkable success in learning representations for graph-structured data. However, GNNs still face challenges in modeling complex phenomena that involve advection. In this paper, we propose a novel GNN architecture based on Advection-Diffusion-Reaction systems, called ADR-GNN. Advection models the directed transportation of information, diffusion captures the local smoothing of information, and reaction represents the non-linear transformation of information in channels. We provide an analysis of the qualitative behavior of ADR-GNN, that shows the benefit of combining advection, diffusion, and reaction. To demonstrate its efficacy, we evaluate ADR-GNN on real-world node classification and spatio-temporal datasets, and show that it improves or offers competitive performance compared to state-of-the-art networks.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#36827;&#34892;&#28153;&#27809;&#39044;&#27979;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#22312;&#27946;&#27700;&#20107;&#20214;&#20013;&#34920;&#29616;&#20986;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.16090</link><description>&lt;p&gt;
&#20351;&#29992;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#30340;&#24555;&#36895;&#27946;&#27700;&#28153;&#27809;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Rapid Flood Inundation Forecast Using Fourier Neural Operator. (arXiv:2307.16090v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16090
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#36827;&#34892;&#28153;&#27809;&#39044;&#27979;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#22312;&#27946;&#27700;&#20107;&#20214;&#20013;&#34920;&#29616;&#20986;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27946;&#27700;&#28153;&#27809;&#39044;&#27979;&#20026;&#27946;&#27700;&#20107;&#20214;&#21069;&#21644;&#27946;&#27700;&#20107;&#20214;&#26399;&#38388;&#30340;&#32039;&#24613;&#35268;&#21010;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;&#23454;&#26102;&#27946;&#27700;&#28153;&#27809;&#39044;&#27979;&#24037;&#20855;&#20173;&#28982;&#32570;&#20047;&#12290;&#36817;&#24180;&#26469;&#65292;&#39640;&#20998;&#36776;&#29575;&#30340;&#27700;&#21160;&#21147;&#27169;&#22411;&#24050;&#21464;&#24471;&#26356;&#21152;&#21487;&#34892;&#65292;&#20294;&#26159;&#23454;&#26102;&#39044;&#27979;&#34903;&#36947;&#21644;&#24314;&#31569;&#29289;&#27700;&#28145;&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36807;&#31243;&#21644;&#25968;&#25454;&#39537;&#21160;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#27946;&#27700;&#33539;&#22260;&#21644;&#28153;&#27809;&#28145;&#24230;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;ML&#26041;&#27861;&#65292;&#36827;&#34892;&#26367;&#20195;&#24314;&#27169;&#12290;&#36890;&#36807;&#20351;&#29992;&#20845;&#20010;&#21382;&#21490;&#39118;&#26292;&#20107;&#20214;&#20013;&#30340;&#27169;&#25311;&#27700;&#28145;&#65288;&#27599;15&#20998;&#38047;&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#20241;&#26031;&#39039;&#65288;&#32654;&#22269;&#24503;&#20811;&#33832;&#26031;&#24030;&#65289;&#30340;&#22478;&#24066;&#21306;&#22495;&#20013;&#28436;&#31034;&#20102;FNO&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#20004;&#20010;&#39044;&#30041;&#20107;&#20214;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;FNO&#20248;&#20110;&#22522;&#20934;&#30340;U-Net&#27169;&#22411;&#12290;&#23427;&#22312;&#25152;&#26377;&#27979;&#35797;&#30340;&#21069;&#23548;&#26102;&#38388;&#65288;&#26368;&#38271;&#20026;3&#23567;&#26102;&#65289;&#20869;&#20445;&#25345;&#39640;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#22312;&#24212;&#29992;&#20110;&#26032;&#22320;&#28857;&#26102;&#34920;&#29616;&#33391;&#22909;&#65292;&#35828;&#26126;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flood inundation forecast provides critical information for emergency planning before and during flood events. Real time flood inundation forecast tools are still lacking. High-resolution hydrodynamic modeling has become more accessible in recent years, however, predicting flood extents at the street and building levels in real-time is still computationally demanding. Here we present a hybrid process-based and data-driven machine learning (ML) approach for flood extent and inundation depth prediction. We used the Fourier neural operator (FNO), a highly efficient ML method, for surrogate modeling. The FNO model is demonstrated over an urban area in Houston (Texas, U.S.) by training using simulated water depths (in 15-min intervals) from six historical storm events and then tested over two holdout events. Results show FNO outperforms the baseline U-Net model. It maintains high predictability at all lead times tested (up to 3 hrs) and performs well when applying to new sites, suggesting s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38544;&#24335;&#34892;&#20026;&#20811;&#38534;&#21644;&#21160;&#24577;&#36816;&#21160;&#21407;&#35821;&#26469;&#20419;&#36827;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#31034;&#33539;&#25968;&#25454;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#65292;&#20197;&#21450;&#23558;&#36816;&#21160;&#35268;&#21010;&#36716;&#21270;&#20026;&#26356;&#31616;&#21333;&#30340;&#35268;&#21010;&#31354;&#38388;&#65292;&#35813;&#26041;&#27861;&#22312;&#20223;&#30495;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2307.16062</link><description>&lt;p&gt;
&#20351;&#29992;&#38544;&#24335;&#34892;&#20026;&#20811;&#38534;&#21644;&#21160;&#24577;&#36816;&#21160;&#21407;&#35821;&#65292;&#20419;&#36827;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Using Implicit Behavior Cloning and Dynamic Movement Primitive to Facilitate Reinforcement Learning for Robot Motion Planning. (arXiv:2307.16062v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38544;&#24335;&#34892;&#20026;&#20811;&#38534;&#21644;&#21160;&#24577;&#36816;&#21160;&#21407;&#35821;&#26469;&#20419;&#36827;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#31034;&#33539;&#25968;&#25454;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#65292;&#20197;&#21450;&#23558;&#36816;&#21160;&#35268;&#21010;&#36716;&#21270;&#20026;&#26356;&#31616;&#21333;&#30340;&#35268;&#21010;&#31354;&#38388;&#65292;&#35813;&#26041;&#27861;&#22312;&#20223;&#30495;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#33258;&#30001;&#24230;&#26426;&#22120;&#20154;&#30340;&#21160;&#20316;&#35268;&#21010;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#20173;&#28982;&#38754;&#20020;&#35757;&#32451;&#36895;&#24230;&#24930;&#21644;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#26694;&#26550;&#65292;&#21033;&#29992;&#38544;&#24335;&#34892;&#20026;&#20811;&#38534;&#21644;&#21160;&#24577;&#36816;&#21160;&#21407;&#35821;&#26469;&#25552;&#39640;&#31163;&#32447;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#38544;&#24335;&#34892;&#20026;&#20811;&#38534;&#21033;&#29992;&#20154;&#31867;&#31034;&#33539;&#25968;&#25454;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#35757;&#32451;&#36895;&#24230;&#65292;&#32780;&#21160;&#24577;&#36816;&#21160;&#21407;&#35821;&#20316;&#20026;&#19968;&#31181;&#21551;&#21457;&#24335;&#27169;&#22411;&#65292;&#23558;&#36816;&#21160;&#35268;&#21010;&#36716;&#21270;&#20026;&#26356;&#31616;&#21333;&#30340;&#35268;&#21010;&#31354;&#38388;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#26694;&#26550;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#25342;&#21462;-&#25918;&#32622;&#23454;&#39564;&#21019;&#24314;&#20102;&#20154;&#31867;&#31034;&#33539;&#25968;&#25454;&#38598;&#65292;&#20379;&#31867;&#20284;&#30740;&#31350;&#20351;&#29992;&#12290;&#22312;&#20223;&#30495;&#27604;&#36739;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#35813;&#26041;&#27861;&#30456;&#27604;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20855;&#26377;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#24471;&#20998;&#12290;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#22312;&#31616;&#21333;&#32452;&#35013;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#24378;&#21270;&#23398;&#20064;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) for motion planning of multi-degree-of-freedom robots still suffers from low efficiency in terms of slow training speed and poor generalizability. In this paper, we propose a novel RL-based robot motion planning framework that uses implicit behavior cloning (IBC) and dynamic movement primitive (DMP) to improve the training speed and generalizability of an off-policy RL agent. IBC utilizes human demonstration data to leverage the training speed of RL, and DMP serves as a heuristic model that transfers motion planning into a simpler planning space. To support this, we also create a human demonstration dataset using a pick-and-place experiment that can be used for similar studies. Comparison studies in simulation reveal the advantage of the proposed method over the conventional RL agents with faster training speed and higher scores. A real-robot experiment indicates the applicability of the proposed method to a simple assembly task. Our work provides a novel pe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26080;&#20301;&#32622;&#20559;&#24046;&#30340;CTR&#21644;CVR&#39044;&#27979;&#27169;&#22411;&#65306;&#22522;&#20110;&#20301;&#32622;&#24863;&#30693;&#30340;&#28857;&#20987;&#36716;&#21270;&#65288;PACC&#65289;&#21644;&#22522;&#20110;&#20301;&#32622;&#23884;&#20837;&#30340;PACC&#65288;PACC-PE&#65289;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#20013;&#20943;&#36731;&#20102;&#20301;&#32622;&#20559;&#24046;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#25490;&#21517;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.16060</link><description>&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#20943;&#36731;&#20301;&#32622;&#20559;&#24046;&#30340;&#36190;&#21161;&#25628;&#32034;&#28857;&#20987;&#36716;&#21270;&#22810;&#20219;&#21153;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Click-Conversion Multi-Task Model with Position Bias Mitigation for Sponsored Search in eCommerce. (arXiv:2307.16060v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16060
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26080;&#20301;&#32622;&#20559;&#24046;&#30340;CTR&#21644;CVR&#39044;&#27979;&#27169;&#22411;&#65306;&#22522;&#20110;&#20301;&#32622;&#24863;&#30693;&#30340;&#28857;&#20987;&#36716;&#21270;&#65288;PACC&#65289;&#21644;&#22522;&#20110;&#20301;&#32622;&#23884;&#20837;&#30340;PACC&#65288;PACC-PE&#65289;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#20013;&#20943;&#36731;&#20102;&#20301;&#32622;&#20559;&#24046;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#25490;&#21517;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20301;&#32622;&#20559;&#24046;&#26159;&#25351;&#29992;&#25143;&#20542;&#21521;&#20110;&#20851;&#27880;&#25628;&#32034;&#32467;&#26524;&#21015;&#34920;&#20013;&#25490;&#21517;&#36739;&#39640;&#30340;&#39033;&#30446;&#65292;&#32780;&#19981;&#32771;&#34385;&#19982;&#26597;&#35810;&#30340;&#23454;&#38469;&#30456;&#20851;&#24615;&#30340;&#29616;&#35937;&#65292;&#36825;&#31181;&#29616;&#35937;&#22312;&#35768;&#22810;&#25490;&#24207;&#31995;&#32479;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#20250;&#20351;&#25490;&#24207;&#27169;&#22411;&#21457;&#29983;&#20559;&#24046;&#65292;&#23548;&#33268;&#21830;&#21697;&#25490;&#24207;&#12289;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#21644;&#36716;&#21270;&#29575;&#65288;CVR&#65289;&#39044;&#27979;&#26085;&#30410;&#19981;&#20844;&#24179;&#12290;&#20026;&#20102;&#20849;&#21516;&#20943;&#36731;&#39033;&#30446;CTR&#21644;CVR&#39044;&#27979;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26080;&#20301;&#32622;&#20559;&#24046;&#30340;CTR&#21644;CVR&#39044;&#27979;&#27169;&#22411;&#65306;&#22522;&#20110;&#20301;&#32622;&#24863;&#30693;&#30340;&#28857;&#20987;&#36716;&#21270;&#65288;PACC&#65289;&#21644;&#22522;&#20110;&#20301;&#32622;&#23884;&#20837;&#30340;PACC&#65288;PACC-PE&#65289;&#12290;PACC&#22522;&#20110;&#27010;&#29575;&#20998;&#35299;&#65292;&#23558;&#20301;&#32622;&#20449;&#24687;&#24314;&#27169;&#20026;&#27010;&#29575;&#12290;PACC-PE&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#20135;&#21697;&#29305;&#23450;&#30340;&#20301;&#32622;&#20449;&#24687;&#24314;&#27169;&#20026;&#23884;&#20837;&#12290;&#23545;&#30005;&#23376;&#21830;&#21153;&#36190;&#21161;&#20135;&#21697;&#25628;&#32034;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#25490;&#21517;&#25928;&#26524;&#19978;&#26356;&#22909;&#65292;&#24182;&#19988;&#33021;&#22815;&#22823;&#22823;&#20943;&#36731;CTR&#21644;CVR&#39044;&#27979;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Position bias, the phenomenon whereby users tend to focus on higher-ranked items of the search result list regardless of the actual relevance to queries, is prevailing in many ranking systems. Position bias in training data biases the ranking model, leading to increasingly unfair item rankings, click-through-rate (CTR), and conversion rate (CVR) predictions. To jointly mitigate position bias in both item CTR and CVR prediction, we propose two position-bias-free CTR and CVR prediction models: Position-Aware Click-Conversion (PACC) and PACC via Position Embedding (PACC-PE). PACC is built upon probability decomposition and models position information as a probability. PACC-PE utilizes neural networks to model product-specific position information as embedding. Experiments on the E-commerce sponsored product search dataset show that our proposed models have better ranking effectiveness and can greatly alleviate position bias in both CTR and CVR prediction.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25581;&#31034;&#20102;&#36153;&#27874;&#37027;&#22865;&#20934;&#26230;&#21472;&#23618;&#30340;&#38750;&#20849;&#32447;&#20934;&#21608;&#26399;&#38081;&#30913;&#37197;&#32622;&#65292;&#35813;&#37197;&#32622;&#20855;&#26377;&#29420;&#29305;&#30340;&#30913;&#30456;&#34892;&#20026;&#65292;&#20854;&#20013;&#30913;&#21270;&#29575;&#38543;&#30528;&#21472;&#23618;&#25968;&#37327;&#30340;&#22686;&#21152;&#21576;&#23545;&#25968;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2307.16052</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25581;&#31034;&#36153;&#27874;&#37027;&#22865;&#20934;&#26230;&#21472;&#23618;&#20013;&#30340;&#24322;&#22495;&#30913;&#30456;
&lt;/p&gt;
&lt;p&gt;
Unveiling Exotic Magnetic Phases in Fibonacci Quasicrystalline Stacking of Ferromagnetic Layers through Machine Learning. (arXiv:2307.16052v1 [cond-mat.str-el])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25581;&#31034;&#20102;&#36153;&#27874;&#37027;&#22865;&#20934;&#26230;&#21472;&#23618;&#30340;&#38750;&#20849;&#32447;&#20934;&#21608;&#26399;&#38081;&#30913;&#37197;&#32622;&#65292;&#35813;&#37197;&#32622;&#20855;&#26377;&#29420;&#29305;&#30340;&#30913;&#30456;&#34892;&#20026;&#65292;&#20854;&#20013;&#30913;&#21270;&#29575;&#38543;&#30528;&#21472;&#23618;&#25968;&#37327;&#30340;&#22686;&#21152;&#21576;&#23545;&#25968;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#36153;&#27874;&#37027;&#22865;&#20934;&#26230;&#21472;&#23618;&#30340;&#38081;&#30913;&#23618;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#35813;&#32467;&#26500;&#21487;&#20197;&#21033;&#29992;&#33539;&#24503;&#21326;&#30913;&#24615;&#26448;&#26009;&#23454;&#29616;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#36825;&#31181;&#30913;&#24615;&#24322;&#36136;&#32467;&#26500;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21253;&#25324;&#26368;&#22810;&#20108;&#27425;&#37051;&#23618;&#30913;&#30456;&#20114;&#20316;&#29992;&#65292;&#22312;&#36825;&#20010;&#20934;&#26230;&#20307;&#31995;&#20013;&#23637;&#29616;&#20102;&#20960;&#20309;&#25387;&#25240;&#21644;&#30913;&#24207;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#20026;&#20102;&#36941;&#21382;&#21442;&#25968;&#31354;&#38388;&#24182;&#35782;&#21035;&#19981;&#21516;&#30340;&#30913;&#30456;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36825;&#35777;&#26126;&#20102;&#22312;&#25581;&#31034;&#35813;&#31995;&#32479;&#22797;&#26434;&#30913;&#24615;&#34892;&#20026;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#38543;&#30528;&#27169;&#22411;&#21442;&#25968;&#21464;&#21270;&#30340;&#30913;&#30456;&#22270;&#30340;&#35814;&#23613;&#25551;&#36848;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#38500;&#20102;&#20854;&#20182;&#20849;&#32447;&#21644;&#38750;&#20849;&#32447;&#30456;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#38081;&#30913;&#20132;&#26367;&#34746;&#26059;&#30456;&#12290;&#22312;&#36825;&#20010;&#38750;&#20849;&#32447;&#20934;&#21608;&#26399;&#38081;&#30913;&#37197;&#32622;&#20013;&#65292;&#30913;&#21270;&#29575;&#38543;&#30528;&#21472;&#23618;&#39640;&#24230;&#30340;&#22686;&#21152;&#21576;&#23545;&#25968;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we conduct a comprehensive theoretical analysis of a Fibonacci quasicrystalline stacking of ferromagnetic layers, potentially realizable using van der Waals magnetic materials. We construct a model of this magnetic heterostructure, which includes up to second neighbor interlayer magnetic interactions, that displays a complex relationship between geometric frustration and magnetic order in this quasicrystalline system. To navigate the parameter space and identify distinct magnetic phases, we employ a machine learning approach, which proves to be a powerful tool in revealing the complex magnetic behavior of this system. We offer a thorough description of the magnetic phase diagram as a function of the model parameters. Notably, we discover among other collinear and non-collinear phases, a unique ferromagnetic alternating helical phase. In this non-collinear quasiperiodic ferromagnetic configuration the magnetization decreases logarithmically with the stack height.
&lt;/p&gt;</description></item><item><title>Okapi&#26159;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#20248;&#30340;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#35299;&#20915;&#20102;&#30446;&#21069;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#21482;&#38024;&#23545;&#33521;&#35821;&#21644;&#23569;&#25968;&#27969;&#34892;&#35821;&#35328;&#36827;&#34892;&#25351;&#20196;&#35843;&#20248;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.16039</link><description>&lt;p&gt;
Okapi: &#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#20248;&#30340;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback. (arXiv:2307.16039v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16039
&lt;/p&gt;
&lt;p&gt;
Okapi&#26159;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#20248;&#30340;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#35299;&#20915;&#20102;&#30446;&#21069;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#21482;&#38024;&#23545;&#33521;&#35821;&#21644;&#23569;&#25968;&#27969;&#34892;&#35821;&#35328;&#36827;&#34892;&#25351;&#20196;&#35843;&#20248;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#26159;&#25351;&#20196;&#35843;&#20248;&#65292;&#23427;&#26377;&#21161;&#20110;&#23558;&#27169;&#22411;&#30340;&#21709;&#24212;&#19982;&#20154;&#31867;&#39044;&#26399;&#23545;&#40784;&#65292;&#23454;&#29616;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#20004;&#31181;&#20027;&#35201;&#30340;&#25351;&#20196;&#35843;&#20248;&#26041;&#27861;&#26159;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#30446;&#21069;&#24050;&#24212;&#29992;&#20110;&#29983;&#20135;&#26368;&#20339;&#30340;&#21830;&#19994;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;ChatGPT&#65289;&#12290;&#20026;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#22312;&#30740;&#31350;&#21644;&#24320;&#21457;&#24037;&#20316;&#20013;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#26368;&#36817;&#36824;&#25512;&#20986;&#20102;&#21508;&#31181;&#32463;&#36807;&#25351;&#20196;&#35843;&#20248;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#20363;&#22914;Alpaca&#12289;Vicuna&#31561;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#20165;&#23545;&#33521;&#35821;&#21644;&#23569;&#25968;&#27969;&#34892;&#35821;&#35328;&#36827;&#34892;&#20102;&#25351;&#20196;&#35843;&#20248;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20840;&#29699;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#24433;&#21709;&#21147;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;&#26368;&#36817;&#26377;&#19968;&#20123;&#25506;&#32034;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#20248;&#30340;&#24037;&#20316;&#65292;&#20294;&#30446;&#21069;&#21482;&#20351;&#29992;&#20102;SFT&#20316;&#20026;&#25351;&#20196;&#35843;&#20248;&#30340;&#21807;&#19968;&#26041;&#27861;&#12290;&#36825;&#24050;&#32463;&#23384;&#22312;&#20102;&#19968;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key technology for the development of large language models (LLMs) involves instruction tuning that helps align the models' responses with human expectations to realize impressive learning abilities. Two major approaches for instruction tuning characterize supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), which are currently applied to produce the best commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for research and development efforts, various instruction-tuned open-source LLMs have also been introduced recently, e.g., Alpaca, Vicuna, to name a few. However, existing open-source LLMs have only been instruction-tuned for English and a few popular languages, thus hindering their impacts and accessibility to many other languages in the world. Among a few very recent work to explore instruction tuning for LLMs in multiple languages, SFT has been used as the only approach to instruction-tune LLMs for multiple languages. This has lef
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20102;&#20855;&#26377;&#22686;&#24378;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#26032;&#22411;&#37197;&#20307;&#23545;&#20110;&#22810;&#21457;&#24615;&#30828;&#21270;&#30151;&#65288;MS&#65289;&#27835;&#30103;&#12290;&#30740;&#31350;&#36890;&#36807;&#23545;&#22522;&#20110;siponimod&#30340;&#20998;&#23376;&#21464;&#20307;&#36827;&#34892;&#22823;&#35268;&#27169;&#29983;&#25104;&#21644;&#31579;&#36873;&#65292;&#21457;&#29616;&#20102;&#20845;&#20010;&#26377;&#21069;&#26223;&#30340;&#20855;&#26377;&#33391;&#22909;&#33647;&#29289;&#26679;&#24615;&#21644;&#26131;&#20110;&#21512;&#25104;&#30340;&#20505;&#36873;&#29289;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#21457;&#29616;&#20102;&#20960;&#20010;&#26377;&#21161;&#20110;&#39640;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#21270;&#23398;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2307.16037</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20855;&#26377;&#22686;&#24378;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#26032;&#22411;&#37197;&#20307;&#23545;&#20110;S1PR1
&lt;/p&gt;
&lt;p&gt;
Developing novel ligands with enhanced binding affinity for the sphingosine 1-phosphate receptor 1 using machine learning. (arXiv:2307.16037v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16037
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20102;&#20855;&#26377;&#22686;&#24378;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#26032;&#22411;&#37197;&#20307;&#23545;&#20110;&#22810;&#21457;&#24615;&#30828;&#21270;&#30151;&#65288;MS&#65289;&#27835;&#30103;&#12290;&#30740;&#31350;&#36890;&#36807;&#23545;&#22522;&#20110;siponimod&#30340;&#20998;&#23376;&#21464;&#20307;&#36827;&#34892;&#22823;&#35268;&#27169;&#29983;&#25104;&#21644;&#31579;&#36873;&#65292;&#21457;&#29616;&#20102;&#20845;&#20010;&#26377;&#21069;&#26223;&#30340;&#20855;&#26377;&#33391;&#22909;&#33647;&#29289;&#26679;&#24615;&#21644;&#26131;&#20110;&#21512;&#25104;&#30340;&#20505;&#36873;&#29289;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#21457;&#29616;&#20102;&#20960;&#20010;&#26377;&#21161;&#20110;&#39640;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#21270;&#23398;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21457;&#24615;&#30828;&#21270;&#30151;&#65288;MS&#65289;&#26159;&#19968;&#31181;&#24433;&#21709;&#32654;&#22269;&#36817;&#19968;&#30334;&#19975;&#20154;&#30340;&#20005;&#37325;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#12290;Sphingosine-1-phosphate receptor 1&#65288;S1PR1&#65289;&#26159;MS&#30340;&#19968;&#20010;&#34507;&#30333;&#38774;&#28857;&#12290;Siponimod&#65292;S1PR1&#30340;&#19968;&#20010;&#37197;&#20307;&#65292;&#20110;2019&#24180;&#34987;FDA&#25209;&#20934;&#29992;&#20110;MS&#27835;&#30103;&#65292;&#20294;&#20173;&#28982;&#38656;&#35201;&#26356;&#22909;&#30340;&#30103;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#21270;&#23398;&#20844;&#24335;&#36716;&#21270;&#20026;&#25968;&#23398;&#21521;&#37327;&#65292;&#24182;&#29983;&#25104;&#20102;500&#22810;&#20010;&#22522;&#20110;siponimod&#30340;&#20998;&#23376;&#21464;&#20307;&#65292;&#20854;&#20013;25&#20010;&#21270;&#21512;&#29289;&#23545;S1PR1&#30340;&#39044;&#27979;&#32467;&#21512;&#20146;&#21644;&#21147;&#26356;&#39640;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#21040;&#19968;&#23567;&#26102;&#30340;&#26102;&#38388;&#20869;&#29983;&#25104;&#36825;&#20123;&#37197;&#20307;&#12290;&#23545;&#36825;&#20123;&#21270;&#21512;&#29289;&#36827;&#34892;&#31579;&#36873;&#65292;&#21457;&#29616;&#20102;&#20845;&#20010;&#20855;&#26377;&#33391;&#22909;&#33647;&#29289;&#26679;&#24615;&#21644;&#26131;&#20110;&#21512;&#25104;&#30340;&#26377;&#21069;&#26223;&#30340;&#20505;&#36873;&#29289;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20998;&#26512;&#36825;&#20123;&#37197;&#20307;&#30340;&#32467;&#21512;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20960;&#20010;&#26377;&#21161;&#20110;&#39640;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#21270;&#23398;&#24615;&#36136;&#12290;&#35813;&#30740;&#31350;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#21152;&#36895;&#33647;&#29289;&#30740;&#21457;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#20855;&#26377;&#22686;&#24378;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#26032;&#22411;&#37197;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple sclerosis (MS) is a debilitating neurological disease affecting nearly one million people in the United States. Sphingosine-1-phosphate receptor 1, or S1PR1, is a protein target for MS. Siponimod, a ligand of S1PR1, was approved by the FDA in 2019 for MS treatment, but there is a demonstrated need for better therapies. To this end, we finetuned an autoencoder machine learning model that converts chemical formulas into mathematical vectors and generated over 500 molecular variants based on siponimod, out of which 25 compounds had higher predicted binding affinity to S1PR1. The model was able to generate these ligands in just under one hour. Filtering these compounds led to the discovery of six promising candidates with good drug-like properties and ease of synthesis. Furthermore, by analyzing the binding interactions for these ligands, we uncovered several chemical properties that contribute to high binding affinity to S1PR1. This study demonstrates that machine learning can ac
&lt;/p&gt;</description></item><item><title>MUSE&#26159;&#19968;&#31181;&#29992;&#20110;&#24322;&#36136;&#22270;&#30340;&#22810;&#35270;&#35282;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#20004;&#20010;&#35270;&#35282;&#26469;&#25429;&#25417;&#33258;&#25105;&#33410;&#28857;&#21644;&#37051;&#23621;&#33410;&#28857;&#30340;&#20449;&#24687;&#65292;&#24182;&#25972;&#21512;&#34701;&#21512;&#21518;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#20197;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.16026</link><description>&lt;p&gt;
MUSE: &#22810;&#35270;&#35282;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#24322;&#36136;&#22270;
&lt;/p&gt;
&lt;p&gt;
MUSE: Multi-View Contrastive Learning for Heterophilic Graphs. (arXiv:2307.16026v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16026
&lt;/p&gt;
&lt;p&gt;
MUSE&#26159;&#19968;&#31181;&#29992;&#20110;&#24322;&#36136;&#22270;&#30340;&#22810;&#35270;&#35282;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#20004;&#20010;&#35270;&#35282;&#26469;&#25429;&#25417;&#33258;&#25105;&#33410;&#28857;&#21644;&#37051;&#23621;&#33410;&#28857;&#30340;&#20449;&#24687;&#65292;&#24182;&#25972;&#21512;&#34701;&#21512;&#21518;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#20197;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#25104;&#20026;&#35299;&#20915;&#20256;&#32479;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#26631;&#31614;&#20381;&#36182;&#21644;&#27867;&#21270;&#24615;&#33021;&#24046;&#30340;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#22312;&#24322;&#36136;&#22270;&#19978;&#30340;&#25928;&#26524;&#26377;&#38480;&#65292;&#36825;&#26159;&#30001;&#20110;&#21516;&#36136;&#24615;&#20551;&#35774;&#23548;&#33268;&#30456;&#37051;&#33410;&#28857;&#20855;&#26377;&#31867;&#20284;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;MUSE&#29992;&#20110;&#24322;&#36136;&#22270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#22686;&#24378;&#23545;&#27604;&#23398;&#20064;&#30340;GNN&#20998;&#21035;&#26500;&#24314;&#20102;&#20004;&#20010;&#35270;&#35282;&#26469;&#25429;&#25417;&#33258;&#25105;&#33410;&#28857;&#21644;&#37051;&#23621;&#33410;&#28857;&#30340;&#20449;&#24687;&#12290;&#28982;&#21518;&#25105;&#20204;&#25972;&#21512;&#36825;&#20004;&#20010;&#35270;&#35282;&#30340;&#20449;&#24687;&#26469;&#34701;&#21512;&#33410;&#28857;&#34920;&#31034;&#12290;&#34701;&#21512;&#23545;&#27604;&#29992;&#20110;&#22686;&#24378;&#34701;&#21512;&#21518;&#30340;&#33410;&#28857;&#34920;&#31034;&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#37051;&#22495;&#19978;&#19979;&#25991;&#20449;&#24687;&#23545;&#20110;&#20449;&#24687;&#34701;&#21512;&#30340;&#24433;&#21709;&#21487;&#33021;&#22240;&#19981;&#21516;&#30340;&#33258;&#25105;&#33410;&#28857;&#32780;&#24322;&#65292;&#25105;&#20204;&#20351;&#29992;&#20449;&#24687;&#34701;&#21512;&#25511;&#21046;&#22120;&#23545;&#20854;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, self-supervised learning has emerged as a promising approach in addressing the issues of label dependency and poor generalization performance in traditional GNNs. However, existing self-supervised methods have limited effectiveness on heterophilic graphs, due to the homophily assumption that results in similar node representations for connected nodes. In this work, we propose a multi-view contrastive learning model for heterophilic graphs, namely, MUSE. Specifically, we construct two views to capture the information of the ego node and its neighborhood by GNNs enhanced with contrastive learning, respectively. Then we integrate the information from these two views to fuse the node representations. Fusion contrast is utilized to enhance the effectiveness of fused node representations. Further, considering that the influence of neighboring contextual information on information fusion may vary across different ego nodes, we employ an information fusion controller to model 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#21270;&#30340;&#35270;&#35273;&#29305;&#24449;&#21305;&#37197;&#26041;&#27861;&#65292;&#23558;&#27169;&#31946;&#36923;&#36753;&#35270;&#35273;&#32593;&#32476;&#65288;FLVN&#65289;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#23398;&#20064;&#65288;ZSL&#65289;&#20998;&#31867;&#38382;&#39064;&#65292;&#36890;&#36807;&#31070;&#32463;&#31526;&#21495;&#21270;LTN&#26694;&#26550;&#23398;&#20064;&#35270;&#35273;-&#35821;&#20041;&#23884;&#20837;&#31354;&#38388;&#65292;&#24182;&#24341;&#20837;&#31867;&#21035;&#23618;&#27425;&#21644;&#39640;&#32423;&#24402;&#32435;&#20559;&#22909;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#20197;&#25552;&#21319;&#20998;&#31867;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.16019</link><description>&lt;p&gt;
&#27169;&#31946;&#36923;&#36753;&#35270;&#35273;&#32593;&#32476;&#65288;FLVN&#65289;&#65306;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#21270;&#30340;&#35270;&#35273;&#29305;&#24449;&#21305;&#37197;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fuzzy Logic Visual Network (FLVN): A neuro-symbolic approach for visual features matching. (arXiv:2307.16019v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#21270;&#30340;&#35270;&#35273;&#29305;&#24449;&#21305;&#37197;&#26041;&#27861;&#65292;&#23558;&#27169;&#31946;&#36923;&#36753;&#35270;&#35273;&#32593;&#32476;&#65288;FLVN&#65289;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#23398;&#20064;&#65288;ZSL&#65289;&#20998;&#31867;&#38382;&#39064;&#65292;&#36890;&#36807;&#31070;&#32463;&#31526;&#21495;&#21270;LTN&#26694;&#26550;&#23398;&#20064;&#35270;&#35273;-&#35821;&#20041;&#23884;&#20837;&#31354;&#38388;&#65292;&#24182;&#24341;&#20837;&#31867;&#21035;&#23618;&#27425;&#21644;&#39640;&#32423;&#24402;&#32435;&#20559;&#22909;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#20197;&#25552;&#21319;&#20998;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#21270;&#25972;&#21512;&#20102;&#31526;&#21495;&#30693;&#35782;&#34920;&#31034;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#19968;&#38454;&#36923;&#36753;&#35821;&#35328;&#20855;&#26377;&#19981;&#21487;&#24494;&#24615;&#36136;&#30340;&#25805;&#20316;&#25171;&#36896;&#25104;&#23454;&#25968;&#24352;&#37327;&#20043;&#38388;&#21487;&#24494;&#20998;&#30340;&#36923;&#36753;&#24352;&#37327;&#32593;&#32476;&#65288;LTNs&#65289;&#65292;&#21487;&#20197;&#23558;&#32972;&#26223;&#30693;&#35782;&#20197;&#36923;&#36753;&#20844;&#29702;&#30340;&#24418;&#24335;&#32435;&#20837;&#20854;&#20013;&#12290;&#28982;&#32780;&#65292;&#20165;&#26377;&#23569;&#37327;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#25552;&#21319;&#38646;&#26679;&#26412;&#23398;&#20064;&#65288;ZSL&#65289;&#20998;&#31867;&#26041;&#38754;&#30340;&#28508;&#22312;&#30410;&#22788;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#27169;&#31946;&#36923;&#36753;&#35270;&#35273;&#32593;&#32476;&#65288;FLVN&#65289;&#65292;&#35813;&#32593;&#32476;&#22312;&#31070;&#32463;&#31526;&#21495;&#21270;LTN&#26694;&#26550;&#20013;&#26500;&#24314;&#20102;&#23398;&#20064;&#35270;&#35273;-&#35821;&#20041;&#23884;&#20837;&#31354;&#38388;&#30340;&#20219;&#21153;&#12290;FLVN&#22312;&#32593;&#32476;&#20013;&#24341;&#20837;&#20102;&#31867;&#21035;&#23618;&#27425;&#65288;&#31867;&#21644;&#23439;&#31867;&#65289;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20197;&#21450;&#24378;&#22823;&#30340;&#39640;&#32423;&#24402;&#32435;&#20559;&#22909;&#12290;&#21518;&#32773;&#20351;&#24471;&#32593;&#32476;&#33021;&#22815;&#22788;&#29702;&#31867;&#32423;&#23646;&#24615;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#65292;&#24182;&#30830;&#20445;&#30456;&#21516;&#31867;&#21035;&#30340;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#38450;&#27490;&#36807;&#26089;&#36807;&#25311;&#21512;&#24050;&#35265;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuro-symbolic integration aims at harnessing the power of symbolic knowledge representation combined with the learning capabilities of deep neural networks. In particular, Logic Tensor Networks (LTNs) allow to incorporate background knowledge in the form of logical axioms by grounding a first order logic language as differentiable operations between real tensors. Yet, few studies have investigated the potential benefits of this approach to improve zero-shot learning (ZSL) classification. In this study, we present the Fuzzy Logic Visual Network (FLVN) that formulates the task of learning a visual-semantic embedding space within a neuro-symbolic LTN framework. FLVN incorporates prior knowledge in the form of class hierarchies (classes and macro-classes) along with robust high-level inductive biases. The latter allow, for instance, to handle exceptions in class-level attributes, and to enforce similarity between images of the same class, preventing premature overfitting to seen classes a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;UPFL&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#26032;&#23458;&#25143;&#21152;&#20837;&#26102;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.15994</link><description>&lt;p&gt;
UPFL&#65306;&#38754;&#21521;&#26032;&#23458;&#25143;&#30340;&#26080;&#30417;&#30563;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
UPFL: Unsupervised Personalized Federated Learning towards New Clients. (arXiv:2307.15994v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;UPFL&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#26032;&#23458;&#25143;&#21152;&#20837;&#26102;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#35299;&#20915;&#25968;&#25454;&#24322;&#36136;&#24615;&#25361;&#25112;&#30340;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#19968;&#20010;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#30340;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#12290;&#24403;&#32852;&#37030;&#27169;&#22411;&#34987;&#35757;&#32451;&#21644;&#37096;&#32626;&#21518;&#65292;&#19968;&#20010;&#26410;&#26631;&#35760;&#30340;&#26032;&#23458;&#25143;&#21152;&#20837;&#26102;&#65292;&#20026;&#26032;&#23458;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#27169;&#22411;&#25104;&#20026;&#19968;&#39033;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#33258;&#36866;&#24212;&#39118;&#38505;&#26368;&#23567;&#21270;&#25216;&#26415;&#25193;&#23637;&#21040;&#26080;&#30417;&#30563;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;FedTTA&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#20004;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#25913;&#36827;&#20102;FedTTA&#65306;&#20351;&#29992;&#20195;&#29702;&#27491;&#21017;&#21270;&#22686;&#24378;&#33258;&#36866;&#24212;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#29109;&#25552;&#21069;&#20572;&#27490;&#33258;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;FedTTA&#35774;&#35745;&#30340;&#30693;&#35782;&#33976;&#39311;&#25439;&#22833;&#65292;&#20197;&#35299;&#20915;&#35774;&#22791;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#23545;&#27604;11&#20010;&#22522;&#20934;&#26041;&#27861;&#22312;5&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized federated learning has gained significant attention as a promising approach to address the challenge of data heterogeneity. In this paper, we address a relatively unexplored problem in federated learning. When a federated model has been trained and deployed, and an unlabeled new client joins, providing a personalized model for the new client becomes a highly challenging task. To address this challenge, we extend the adaptive risk minimization technique into the unsupervised personalized federated learning setting and propose our method, FedTTA. We further improve FedTTA with two simple yet effective optimization strategies: enhancing the training of the adaptation model with proxy regularization and early-stopping the adaptation through entropy. Moreover, we propose a knowledge distillation loss specifically designed for FedTTA to address the device heterogeneity. Extensive experiments on five datasets against eleven baselines demonstrate the effectiveness of our proposed 
&lt;/p&gt;</description></item><item><title>RGB-D-Fusion&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#28145;&#24230;&#22270;&#12290;&#23427;&#36890;&#36807;&#20004;&#20010;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#23454;&#29616;&#65292;&#20854;&#20013;&#19968;&#20010;&#26159;&#22270;&#20687;&#26465;&#20214;&#30340;&#65292;&#21478;&#19968;&#20010;&#26159;&#22522;&#20110;&#20302;&#20998;&#36776;&#29575;RGB-D&#22270;&#20687;&#30340;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#28145;&#24230;&#22122;&#22768;&#22686;&#24378;&#25216;&#26415;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15988</link><description>&lt;p&gt;
RGB-D-Fusion: &#22522;&#20110;&#22270;&#20687;&#26465;&#20214;&#30340;&#20154;&#24418;&#20027;&#20307;&#28145;&#24230;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
RGB-D-Fusion: Image Conditioned Depth Diffusion of Humanoid Subjects. (arXiv:2307.15988v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15988
&lt;/p&gt;
&lt;p&gt;
RGB-D-Fusion&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#28145;&#24230;&#22270;&#12290;&#23427;&#36890;&#36807;&#20004;&#20010;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#23454;&#29616;&#65292;&#20854;&#20013;&#19968;&#20010;&#26159;&#22270;&#20687;&#26465;&#20214;&#30340;&#65292;&#21478;&#19968;&#20010;&#26159;&#22522;&#20110;&#20302;&#20998;&#36776;&#29575;RGB-D&#22270;&#20687;&#30340;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#28145;&#24230;&#22122;&#22768;&#22686;&#24378;&#25216;&#26415;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;RGB-D-Fusion&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#36890;&#36807;&#20154;&#24418;&#20027;&#20307;&#30340;&#20302;&#20998;&#36776;&#29575;&#21333;&#30446;RGB&#22270;&#20687;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#28145;&#24230;&#22270;&#12290;RGB-D-Fusion&#39318;&#20808;&#20351;&#29992;&#22270;&#20687;&#26465;&#20214;&#30340;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#29983;&#25104;&#20302;&#20998;&#36776;&#29575;&#28145;&#24230;&#22270;&#65292;&#28982;&#21518;&#20351;&#29992;&#22522;&#20110;&#20302;&#20998;&#36776;&#29575;RGB-D&#22270;&#20687;&#30340;&#31532;&#20108;&#20010;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#23558;&#28145;&#24230;&#22270;&#19978;&#37319;&#26679;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#28145;&#24230;&#22122;&#22768;&#22686;&#24378;&#65292;&#20197;&#22686;&#21152;&#25105;&#20204;&#30340;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present RGB-D-Fusion, a multi-modal conditional denoising diffusion probabilistic model to generate high resolution depth maps from low-resolution monocular RGB images of humanoid subjects. RGB-D-Fusion first generates a low-resolution depth map using an image conditioned denoising diffusion probabilistic model and then upsamples the depth map using a second denoising diffusion probabilistic model conditioned on a low-resolution RGB-D image. We further introduce a novel augmentation technique, depth noise augmentation, to increase the robustness of our super-resolution model.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20915;&#31574;&#26641;&#27169;&#22411;&#21644;&#26799;&#24230;&#25552;&#21319;&#27169;&#22411;&#32467;&#21512;&#26469;&#39044;&#27979;&#20108;&#25163;&#36710;&#20215;&#26684;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#24402;&#19968;&#21270;&#12289;&#26631;&#20934;&#21270;&#21644;&#28165;&#27927;&#65292;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#21487;&#29992;&#20110;&#26410;&#26469;&#20108;&#25163;&#36710;&#20215;&#26684;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2307.15982</link><description>&lt;p&gt;
&#36890;&#36807;&#32858;&#21512;&#20915;&#31574;&#26641;&#27169;&#22411;&#19982;Boosting&#27169;&#22411;&#36827;&#34892;&#36710;&#36742;&#20215;&#26684;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Vehicle Price Prediction By Aggregating decision tree model With Boosting Model. (arXiv:2307.15982v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20915;&#31574;&#26641;&#27169;&#22411;&#21644;&#26799;&#24230;&#25552;&#21319;&#27169;&#22411;&#32467;&#21512;&#26469;&#39044;&#27979;&#20108;&#25163;&#36710;&#20215;&#26684;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#24402;&#19968;&#21270;&#12289;&#26631;&#20934;&#21270;&#21644;&#28165;&#27927;&#65292;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#21487;&#29992;&#20110;&#26410;&#26469;&#20108;&#25163;&#36710;&#20215;&#26684;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#20108;&#25163;&#36710;&#20215;&#26684;&#26159;&#19968;&#20010;&#26377;&#36259;&#19988;&#38656;&#35201;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#38656;&#35201;&#32771;&#34385;&#22810;&#20010;&#23646;&#24615;&#20197;&#36827;&#34892;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#36710;&#36742;&#20215;&#26684;&#39044;&#27979;&#21487;&#33021;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#39044;&#27979;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#27493;&#39588;&#26159;&#25968;&#25454;&#30340;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#12290;&#26412;&#39033;&#30446;&#20351;&#29992;Python&#33050;&#26412;&#23545;&#25968;&#25454;&#36827;&#34892;&#24402;&#19968;&#21270;&#12289;&#26631;&#20934;&#21270;&#21644;&#28165;&#27927;&#65292;&#20197;&#36991;&#20813;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21463;&#21040;&#19981;&#24517;&#35201;&#30340;&#24178;&#25200;&#12290;&#26412;&#39033;&#30446;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#22312;&#20351;&#29992;&#19981;&#21516;&#30340;&#39044;&#27979;&#25216;&#26415;&#36827;&#34892;&#31867;&#20284;&#30740;&#31350;&#26102;&#21487;&#33021;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#26681;&#25454;&#25968;&#25454;&#38598;&#20570;&#20986;&#20102;&#35768;&#22810;&#20551;&#35774;&#12290;&#25552;&#20986;&#30340;&#31995;&#32479;&#37319;&#29992;&#20102;&#20915;&#31574;&#26641;&#27169;&#22411;&#21644;&#26799;&#24230;&#25552;&#21319;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#20004;&#32773;&#32467;&#21512;&#22312;&#19968;&#36215;&#26469;&#33719;&#24471;&#26356;&#25509;&#36817;&#20934;&#30830;&#39044;&#27979;&#30340;&#32467;&#26524;&#65292;&#35813;&#27169;&#22411;&#24050;&#32463;&#24471;&#21040;&#35780;&#20272;&#24182;&#19988;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#20351;&#29992;&#30456;&#21516;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20108;&#25163;&#36710;&#26410;&#26469;&#20215;&#26684;&#30340;&#39044;&#27979;&#23558;&#32467;&#21512;&#22810;&#20010;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting the price of used vehicles is a more interesting and needed problem by many users. Vehicle price prediction can be a challenging task due to the high number of attributes that should be considered for accurate prediction. The major step in the prediction process is the collection and pre-processing of the data. In this project, python scripts were built to normalize, standardize, and clean data to avoid unnecessary noise for machine learning algorithms. The data set used in this project can be very valuable in conducting similar research using different prediction techniques. Many assumptions were made on the basis of the data set. The proposed system uses a Decision tree model and Gradient boosting predictive model, which are combined in other to get closed to accurate prediction, the proposed model was evaluated and it gives a promising performance. The future price prediction of used vehicles with the help of the same data set will comprise different models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#27169;&#20223;&#23398;&#20064;&#20013;&#22240;&#26524;&#28151;&#28102;&#38382;&#39064;&#30340;&#21021;&#22987;&#21270;&#29366;&#24577;&#24178;&#39044;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36974;&#34109;&#35266;&#27979;&#20013;&#30340;&#28151;&#28102;&#22240;&#32032;&#24182;&#25552;&#39640;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.15980</link><description>&lt;p&gt;
&#21021;&#22987;&#21270;&#29366;&#24577;&#24178;&#39044;&#23545;&#35299;&#20915;&#21435;&#38500;&#28151;&#28102;&#30340;&#27169;&#20223;&#23398;&#20064;&#38382;&#39064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Initial State Interventions for Deconfounded Imitation Learning. (arXiv:2307.15980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#27169;&#20223;&#23398;&#20064;&#20013;&#22240;&#26524;&#28151;&#28102;&#38382;&#39064;&#30340;&#21021;&#22987;&#21270;&#29366;&#24577;&#24178;&#39044;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36974;&#34109;&#35266;&#27979;&#20013;&#30340;&#28151;&#28102;&#22240;&#32032;&#24182;&#25552;&#39640;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#23384;&#22312;&#22240;&#26524;&#28151;&#28102;&#38382;&#39064;&#65292;&#21363;&#23398;&#20064;&#31574;&#30053;&#20851;&#27880;&#30340;&#29305;&#24449;&#24182;&#19981;&#22240;&#26524;&#22320;&#24433;&#21709;&#19987;&#23478;&#30340;&#34892;&#20026;&#65292;&#32780;&#26159;&#34920;&#38754;&#19978;&#30340;&#30456;&#20851;&#24615;&#12290;&#22240;&#26524;&#28151;&#28102;&#30340;&#26234;&#33021;&#20307;&#22312;&#35757;&#32451;&#20013;&#20135;&#29983;&#20102;&#20302;&#30340;&#24320;&#29615;&#30417;&#30563;&#25439;&#22833;&#65292;&#20294;&#22312;&#37096;&#32626;&#26102;&#34920;&#29616;&#20986;&#24046;&#30340;&#38381;&#29615;&#24615;&#33021;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#35266;&#27979;&#31354;&#38388;&#30340;&#20998;&#31163;&#34920;&#31034;&#20013;&#36974;&#34109;&#24050;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36974;&#34109;&#31639;&#27861;&#65292;&#21033;&#29992;&#20102;&#23545;&#21021;&#22987;&#31995;&#32479;&#29366;&#24577;&#36827;&#34892;&#24178;&#39044;&#30340;&#33021;&#21147;&#65292;&#36991;&#20813;&#20102;&#23545;&#19987;&#23478;&#26597;&#35810;&#12289;&#19987;&#23478;&#22870;&#21169;&#20989;&#25968;&#25110;&#22240;&#26524;&#22270;&#35268;&#33539;&#30340;&#20219;&#20309;&#35201;&#27714;&#12290;&#22312;&#19968;&#23450;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#26159;&#20445;&#23432;&#30340;&#65292;&#21363;&#19981;&#20250;&#38169;&#35823;&#22320;&#36974;&#34109;&#22240;&#26524;&#30456;&#20851;&#30340;&#35266;&#27979;&#65307;&#27492;&#22806;&#65292;&#23545;&#21021;&#22987;&#29366;&#24577;&#30340;&#24178;&#39044;&#33021;&#22815;&#20005;&#26684;&#20943;&#23569;&#36807;&#24230;&#20445;&#23432;&#24615;&#12290;&#35813;&#36974;&#34109;&#31639;&#27861;&#34987;&#24212;&#29992;&#20110;&#20004;&#20010;&#31034;&#20363;&#25511;&#21046;&#31995;&#32479;&#30340;&#34892;&#20026;&#20811;&#38534;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning suffers from causal confusion. This phenomenon occurs when learned policies attend to features that do not causally influence the expert actions but are instead spuriously correlated. Causally confused agents produce low open-loop supervised loss but poor closed-loop performance upon deployment. We consider the problem of masking observed confounders in a disentangled representation of the observation space. Our novel masking algorithm leverages the usual ability to intervene in the initial system state, avoiding any requirement involving expert querying, expert reward functions, or causal graph specification. Under certain assumptions, we theoretically prove that this algorithm is conservative in the sense that it does not incorrectly mask observations that causally influence the expert; furthermore, intervening on the initial state serves to strictly reduce excess conservatism. The masking algorithm is applied to behavior cloning for two illustrative control system
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#21307;&#30103;&#20803;&#23431;&#23449;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#29992;&#25143;&#20013;&#24515;&#30340;&#28608;&#21169;&#26426;&#21046;&#25552;&#20379;&#26368;&#20248;&#25968;&#25454;&#26032;&#40092;&#24230;&#65292;&#24182;&#36890;&#36807;&#38544;&#31169;&#20445;&#25252;&#26694;&#26550;&#21644;&#36328;&#38142;&#25216;&#26415;&#22686;&#24378;&#25968;&#25454;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15975</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#21307;&#30103;&#20803;&#23431;&#23449;&#32852;&#37030;&#23398;&#20064;&#65306;&#20855;&#26377;&#26368;&#20248;&#25968;&#25454;&#26032;&#40092;&#24230;&#30340;&#29992;&#25143;&#20013;&#24515;&#28608;&#21169;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Blockchain-empowered Federated Learning for Healthcare Metaverses: User-centric Incentive Mechanism with Optimal Data Freshness. (arXiv:2307.15975v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15975
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#21307;&#30103;&#20803;&#23431;&#23449;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#29992;&#25143;&#20013;&#24515;&#30340;&#28608;&#21169;&#26426;&#21046;&#25552;&#20379;&#26368;&#20248;&#25968;&#25454;&#26032;&#40092;&#24230;&#65292;&#24182;&#36890;&#36807;&#38544;&#31169;&#20445;&#25252;&#26694;&#26550;&#21644;&#36328;&#38142;&#25216;&#26415;&#22686;&#24378;&#25968;&#25454;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#20803;&#23431;&#23449;&#30340;&#38761;&#21629;&#24615;&#20316;&#29992;&#65292;&#21307;&#30103;&#20803;&#23431;&#23449;&#26085;&#30410;&#25104;&#20026;&#19968;&#20010;&#20855;&#26377;&#21464;&#38761;&#24615;&#21147;&#37327;&#30340;&#39046;&#22495;&#65292;&#21019;&#36896;&#20102;&#26234;&#33021;&#21307;&#30103;&#31995;&#32479;&#65292;&#25552;&#20379;&#27785;&#28024;&#24335;&#21644;&#20010;&#24615;&#21270;&#30340;&#26381;&#21153;&#12290;&#21307;&#30103;&#20803;&#23431;&#23449;&#20351;&#29992;&#25143;&#33021;&#22815;&#36827;&#34892;&#26377;&#25928;&#30340;&#20915;&#31574;&#21644;&#25968;&#25454;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#22312;&#26500;&#24314;&#21307;&#30103;&#20803;&#23431;&#23449;&#26041;&#38754;&#20173;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#65292;&#20363;&#22914;&#25935;&#24863;&#25968;&#25454;&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#24863;&#30693;&#25968;&#25454;&#23433;&#20840;&#21644;&#26032;&#40092;&#24230;&#38382;&#39064;&#20197;&#21450;&#25968;&#25454;&#20849;&#20139;&#28608;&#21169;&#30340;&#25285;&#24551;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#38754;&#21521;&#29992;&#25143;&#30340;&#38544;&#31169;&#20445;&#25252;&#26694;&#26550;&#65292;&#29992;&#20110;&#21307;&#30103;&#20803;&#23431;&#23449;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#21307;&#30103;&#20803;&#23431;&#23449;&#30340;&#38544;&#31169;&#20445;&#25252;&#24615;&#33021;&#65292;&#37319;&#29992;&#20102;&#19968;&#20010;&#22522;&#20110;&#36328;&#38142;&#30340;FL&#26694;&#26550;&#26469;&#22686;&#24378;&#24863;&#30693;&#25968;&#25454;&#30340;&#23433;&#20840;&#24615;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#20027;&#38142;&#21644;&#22810;&#20010;&#23376;&#38142;&#30340;&#20998;&#23618;&#36328;&#38142;&#32467;&#26500;&#65292;&#20197;&#22312;&#21307;&#30103;&#20803;&#23431;&#23449;&#20013;&#36827;&#34892;&#20998;&#25955;&#21270;&#12289;&#38544;&#31169;&#20445;&#25252;&#21644;&#23433;&#20840;&#30340;&#25968;&#25454;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the revolutionary role of metaverses, healthcare metaverses are emerging as a transformative force, creating intelligent healthcare systems that offer immersive and personalized services. The healthcare metaverses allow for effective decision-making and data analytics for users. However, there still exist critical challenges in building healthcare metaverses, such as the risk of sensitive data leakage and issues with sensing data security and freshness, as well as concerns around incentivizing data sharing. In this paper, we first design a user-centric privacy-preserving framework based on decentralized Federated Learning (FL) for healthcare metaverses. To further improve the privacy protection of healthcare metaverses, a cross-chain empowered FL framework is utilized to enhance sensing data security. This framework utilizes a hierarchical cross-chain architecture with a main chain and multiple subchains to perform decentralized, privacy-preserving, and secure data training in bo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26144;&#23556;&#24863;&#30693;&#30340;&#22270;&#24418;&#21387;&#32553;&#26041;&#27861;&#65288;MCond&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#33410;&#28857;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#22312;&#21512;&#25104;&#22270;&#20013;&#39640;&#25928;&#22320;&#22788;&#29702;&#26410;&#30693;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.15967</link><description>&lt;p&gt;
&#22270;&#24418;&#21387;&#32553;&#26041;&#27861;&#29992;&#20110;&#24402;&#32435;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Condensation for Inductive Node Representation Learning. (arXiv:2307.15967v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26144;&#23556;&#24863;&#30693;&#30340;&#22270;&#24418;&#21387;&#32553;&#26041;&#27861;&#65288;MCond&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#33410;&#28857;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#22312;&#21512;&#25104;&#22270;&#20013;&#39640;&#25928;&#22320;&#22788;&#29702;&#26410;&#30693;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22270;&#24341;&#23548;&#32593;&#32476;&#38754;&#20020;&#30528;&#35745;&#31639;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22270;&#24418;&#21387;&#32553;&#20316;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#25216;&#26415;&#20986;&#29616;&#20102;&#65292;&#23427;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#23567;&#30340;&#21512;&#25104;&#22270;&#26469;&#39640;&#25928;&#22320;&#35757;&#32451;&#22270;&#24341;&#23548;&#32593;&#32476;&#24182;&#20445;&#25345;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33410;&#28857;&#20043;&#38388;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#22270;&#24418;&#21387;&#32553;&#20165;&#38480;&#20110;&#21387;&#32553;&#35266;&#23519;&#21040;&#30340;&#35757;&#32451;&#33410;&#28857;&#21450;&#20854;&#23545;&#24212;&#30340;&#32467;&#26500;&#65292;&#22240;&#27492;&#32570;&#20047;&#26377;&#25928;&#22788;&#29702;&#26410;&#30693;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#22312;&#25512;&#29702;&#38454;&#27573;&#20173;&#38656;&#35201;&#21407;&#22987;&#22823;&#22270;&#26469;&#23545;&#24402;&#32435;&#33410;&#28857;&#36827;&#34892;&#28040;&#24687;&#20256;&#36882;&#65292;&#23548;&#33268;&#35745;&#31639;&#38656;&#27714;&#24040;&#22823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26144;&#23556;&#24863;&#30693;&#30340;&#22270;&#24418;&#21387;&#32553;&#65288;MCond&#65289;&#26041;&#27861;&#65292;&#26126;&#30830;&#23398;&#20064;&#20174;&#21407;&#22987;&#33410;&#28857;&#21040;&#21512;&#25104;&#33410;&#28857;&#30340;&#19968;&#23545;&#22810;&#33410;&#28857;&#26144;&#23556;&#65292;&#20197;&#26080;&#32541;&#22320;&#23558;&#26032;&#33410;&#28857;&#25972;&#21512;&#21040;&#21512;&#25104;&#22270;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) encounter significant computational challenges when handling large-scale graphs, which severely restricts their efficacy across diverse applications. To address this limitation, graph condensation has emerged as a promising technique, which constructs a small synthetic graph for efficiently training GNNs while retaining performance. However, due to the topology structure among nodes, graph condensation is limited to condensing only the observed training nodes and their corresponding structure, thus lacking the ability to effectively handle the unseen data. Consequently, the original large graph is still required in the inference stage to perform message passing to inductive nodes, resulting in substantial computational demands. To overcome this issue, we propose mapping-aware graph condensation (MCond), explicitly learning the one-to-many node mapping from original nodes to synthetic nodes to seamlessly integrate new nodes into the synthetic graph for induc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Interaction and Mapping Matrices Correction (IMCorrect)&#26041;&#27861;&#65292;&#21487;&#20197;&#24179;&#34913;&#25512;&#33616;&#31995;&#32479;&#36951;&#24536;&#20013;&#30340;&#23436;&#25972;&#24615;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#12290;&#36890;&#36807;&#23558;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30697;&#38453;&#19982;&#26144;&#23556;&#30697;&#38453;&#30456;&#20056;&#65292;IMCorrect&#33021;&#22815;&#20462;&#27491;&#25512;&#33616;&#32467;&#26524;&#65292;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15960</link><description>&lt;p&gt;
&#36890;&#36807;&#30697;&#38453;&#20462;&#27491;&#36827;&#34892;&#25512;&#33616;&#31995;&#32479;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Recommendation Unlearning via Matrix Correction. (arXiv:2307.15960v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Interaction and Mapping Matrices Correction (IMCorrect)&#26041;&#27861;&#65292;&#21487;&#20197;&#24179;&#34913;&#25512;&#33616;&#31995;&#32479;&#36951;&#24536;&#20013;&#30340;&#23436;&#25972;&#24615;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#12290;&#36890;&#36807;&#23558;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30697;&#38453;&#19982;&#26144;&#23556;&#30697;&#38453;&#30456;&#20056;&#65292;IMCorrect&#33021;&#22815;&#20462;&#27491;&#25512;&#33616;&#32467;&#26524;&#65292;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#23545;&#20110;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#22823;&#37327;&#25910;&#38598;&#30340;&#29992;&#25143;&#25968;&#25454;&#24341;&#21457;&#20102;&#26377;&#20851;&#38544;&#31169;&#65288;&#22914;&#25935;&#24863;&#25968;&#25454;&#65289;&#12289;&#23433;&#20840;&#65288;&#22914;&#24694;&#24847;&#25968;&#25454;&#65289;&#21644;&#25928;&#29992;&#65288;&#22914;&#26377;&#23475;&#25968;&#25454;&#65289;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25512;&#33616;&#31995;&#32479;&#36951;&#24536;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#29305;&#23450;&#25968;&#25454;&#21644;&#27169;&#22411;&#34987;&#36951;&#24536;&#65292;&#20197;&#20943;&#36731;&#25935;&#24863;/&#24694;&#24847;/&#26377;&#23475;&#29992;&#25143;&#25968;&#25454;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#38590;&#20197;&#22312;&#23436;&#25972;&#24615;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#21363;&#22312;&#20854;&#20013;&#19968;&#20010;&#26041;&#38754;&#22949;&#21327;&#65292;&#23548;&#33268;&#23376;&#20248;&#21270;&#30340;&#25512;&#33616;&#31995;&#32479;&#36951;&#24536;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Interaction and Mapping Matrices Correction (IMCorrect)&#26041;&#27861;&#26469;&#36827;&#34892;&#25512;&#33616;&#31995;&#32479;&#36951;&#24536;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#35768;&#22810;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#21487;&#20197;&#34987;&#21046;&#23450;&#20026;&#22522;&#20110;&#26144;&#23556;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#25512;&#33616;&#32467;&#26524;&#21487;&#20197;&#36890;&#36807;&#23558;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30697;&#38453;&#19982;&#26144;&#23556;&#30697;&#38453;&#30456;&#20056;&#26469;&#33719;&#24471;&#12290;&#28982;&#21518;&#65292;IMC
&lt;/p&gt;
&lt;p&gt;
Recommender systems are important for providing personalized services to users, but the vast amount of collected user data has raised concerns about privacy (e.g., sensitive data), security (e.g., malicious data) and utility (e.g., toxic data). To address these challenges, recommendation unlearning has emerged as a promising approach, which allows specific data and models to be forgotten, mitigating the risks of sensitive/malicious/toxic user data. However, existing methods often struggle to balance completeness, utility, and efficiency, i.e., compromising one for the other, leading to suboptimal recommendation unlearning. In this paper, we propose an Interaction and Mapping Matrices Correction (IMCorrect) method for recommendation unlearning. Firstly, we reveal that many collaborative filtering (CF) algorithms can be formulated as mapping-based approach, in which the recommendation results can be obtained by multiplying the user-item interaction matrix with a mapping matrix. Then, IMC
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#23545;&#20998;&#24067;&#24335;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#33410;&#28857;&#36830;&#25509;&#24615;&#21644;&#32593;&#32476;&#23646;&#24615;&#22312;&#30693;&#35782;&#20256;&#25773;&#36807;&#31243;&#20013;&#30340;&#19981;&#21516;&#20316;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#36830;&#25509;&#24615;&#21487;&#33021;&#23548;&#33268;&#26356;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15947</link><description>&lt;p&gt;
&#32593;&#32476;&#25299;&#25169;&#23545;&#23436;&#20840;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#24433;&#21709;&#65306;&#21021;&#27493;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
The effect of network topologies on fully decentralized learning: a preliminary investigation. (arXiv:2307.15947v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#23545;&#20998;&#24067;&#24335;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#33410;&#28857;&#36830;&#25509;&#24615;&#21644;&#32593;&#32476;&#23646;&#24615;&#22312;&#30693;&#35782;&#20256;&#25773;&#36807;&#31243;&#20013;&#30340;&#19981;&#21516;&#20316;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#36830;&#25509;&#24615;&#21487;&#33021;&#23548;&#33268;&#26356;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#65292;&#25968;&#25454;&#36890;&#24120;&#34987;&#20998;&#21106;&#22312;&#22810;&#20010;&#35774;&#22791;&#25110;&#33410;&#28857;&#20043;&#38388;&#65292;&#27599;&#20010;&#33410;&#28857;&#20351;&#29992;&#33258;&#24049;&#30340;&#25968;&#25454;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#26412;&#22320;&#27169;&#22411;&#34987;&#20849;&#20139;&#21644;&#21512;&#24182;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#21487;&#20197;&#22312;&#26032;&#25968;&#25454;&#19978;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#26412;&#25991;&#24320;&#22987;&#25506;&#32034;&#36830;&#25509;&#33410;&#28857;&#30340;&#32593;&#32476;&#25299;&#25169;&#23545;&#36890;&#36807;&#33410;&#28857;&#20043;&#38388;&#30340;&#30452;&#25509;&#21327;&#20316;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30740;&#31350;&#19981;&#21516;&#31867;&#22411;&#30340;&#25299;&#25169;&#32467;&#26500;&#22914;&#20309;&#24433;&#21709;&#8220;&#30693;&#35782;&#20256;&#25773;&#8221;&#65292;&#21363;&#33410;&#28857;&#33021;&#22815;&#23558;&#20854;&#20182;&#33410;&#28857;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#30340;&#27169;&#24335;&#30693;&#35782;&#34701;&#20837;&#21040;&#26412;&#22320;&#27169;&#22411;&#20013;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#26356;&#25110;&#32773;&#26356;&#23569;&#36830;&#25509;&#30340;&#33410;&#28857;&#65288;&#20013;&#24515;&#33410;&#28857;&#21644;&#21494;&#33410;&#28857;&#65289;&#20197;&#21450;&#23439;&#35266;&#32593;&#32476;&#23646;&#24615;&#65288;&#20027;&#35201;&#26159;&#24230;&#20998;&#24067;&#21644;&#27169;&#22359;&#21270;&#65289;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#30340;&#19981;&#21516;&#20316;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#34429;&#28982;&#24050;&#30693;&#24369;&#36830;&#25509;&#33021;&#22815;&#20445;&#35777;&#30693;&#35782;&#20256;&#25773;&#65292;&#20294;&#20855;&#26377;&#26356;&#39640;&#30340;&#36830;&#25509;&#24615;&#21487;&#33021;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a decentralized machine learning system, data is typically partitioned among multiple devices or nodes, each of which trains a local model using its own data. These local models are then shared and combined to create a global model that can make accurate predictions on new data. In this paper, we start exploring the role of the network topology connecting nodes on the performance of a Machine Learning model trained through direct collaboration between nodes. We investigate how different types of topologies impact the "spreading of knowledge", i.e., the ability of nodes to incorporate in their local model the knowledge derived by learning patterns in data available in other nodes across the networks. Specifically, we highlight the different roles in this process of more or less connected nodes (hubs and leaves), as well as that of macroscopic network properties (primarily, degree distribution and modularity). Among others, we show that, while it is known that even weak connectivity a
&lt;/p&gt;</description></item><item><title>PIMbot&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#26426;&#22120;&#20154;&#21327;&#20316;&#20013;&#22870;&#21169;&#20989;&#25968;&#25805;&#32437;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31574;&#30053;&#21644;&#28608;&#21169;&#25805;&#32437;&#26469;&#24433;&#21709;&#22810;&#26426;&#22120;&#20154;&#36890;&#20449;&#20197;&#23454;&#29616;&#19981;&#21516;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.15944</link><description>&lt;p&gt;
PIMbot: &#22810;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#20013;&#31038;&#20250;&#22256;&#22659;&#30340;&#31574;&#30053;&#21644;&#28608;&#21169;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
PIMbot: Policy and Incentive Manipulation for Multi-Robot Reinforcement Learning in Social Dilemmas. (arXiv:2307.15944v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15944
&lt;/p&gt;
&lt;p&gt;
PIMbot&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#26426;&#22120;&#20154;&#21327;&#20316;&#20013;&#22870;&#21169;&#20989;&#25968;&#25805;&#32437;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31574;&#30053;&#21644;&#28608;&#21169;&#25805;&#32437;&#26469;&#24433;&#21709;&#22810;&#26426;&#22120;&#20154;&#36890;&#20449;&#20197;&#23454;&#29616;&#19981;&#21516;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#29616;&#22810;&#26426;&#22120;&#20154;&#21327;&#20316;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#20020;&#33258;&#25105;&#21033;&#30410;&#19982;&#38598;&#20307;&#21033;&#30410;&#20043;&#38388;&#30340;&#31038;&#20250;&#22256;&#22659;&#26102;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#35832;&#22914;&#20449;&#24687;&#20256;&#36882;&#19981;&#30021;&#21644;&#23545;&#25239;&#26426;&#22120;&#20154;&#20043;&#31867;&#30340;&#29615;&#22659;&#22240;&#32032;&#21487;&#33021;&#20250;&#24433;&#21709;&#21512;&#20316;&#65292;&#22240;&#27492;&#24517;&#39035;&#25506;&#32034;&#22914;&#20309;&#25805;&#32437;&#22810;&#26426;&#22120;&#20154;&#36890;&#20449;&#20197;&#23454;&#29616;&#19981;&#21516;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;PIMbot&#65292;&#36890;&#36807;&#20004;&#31181;&#19981;&#21516;&#24418;&#24335;&#30340;&#25805;&#32437;&#65288;&#31574;&#30053;&#21644;&#28608;&#21169;&#25805;&#32437;&#65289;&#26469;&#25805;&#32437;&#22810;&#26426;&#22120;&#20154;&#21327;&#20316;&#20013;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#26368;&#36817;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31038;&#20250;&#22256;&#22659;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25805;&#32437;&#35282;&#24230;&#65292;&#35813;&#35282;&#24230;&#21033;&#29992;&#20102;&#29420;&#29305;&#30340;&#22870;&#21169;&#20989;&#25968;&#26469;&#36827;&#34892;&#28608;&#21169;&#12290;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;PIMbot&#26426;&#21046;&#65292;&#26426;&#22120;&#20154;&#33021;&#22815;&#26377;&#25928;&#22320;&#25805;&#32437;&#31038;&#20250;&#22256;&#22659;&#29615;&#22659;&#12290;PIMbot&#23545;&#20219;&#21153;&#32467;&#26524;&#21487;&#33021;&#20135;&#29983;&#31215;&#26497;&#21644;&#28040;&#26497;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has demonstrated the potential of reinforcement learning (RL) in enabling effective multi-robot collaboration, particularly in social dilemmas where robots face a trade-off between self-interests and collective benefits. However, environmental factors such as miscommunication and adversarial robots can impact cooperation, making it crucial to explore how multi-robot communication can be manipulated to achieve different outcomes. This paper presents a novel approach, namely PIMbot, to manipulating the reward function in multi-robot collaboration through two distinct forms of manipulation: policy and incentive manipulation. Our work introduces a new angle for manipulation in recent multi-agent RL social dilemmas that utilize a unique reward function for incentivization. By utilizing our proposed PIMbot mechanisms, a robot is able to manipulate the social dilemma environment effectively. PIMbot has the potential for both positive and negative impacts on the task outcome, w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#23569;&#37327;&#21382;&#21490;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#39044;&#27979;&#33258;&#21160;&#32553;&#25918;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.15941</link><description>&lt;p&gt;
&#22312;&#39044;&#27979;&#33258;&#21160;&#32553;&#25918;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Learning in Predictive Autoscaling. (arXiv:2307.15941v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#23569;&#37327;&#21382;&#21490;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#39044;&#27979;&#33258;&#21160;&#32553;&#25918;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#33258;&#21160;&#32553;&#25918;&#34987;&#29992;&#20110;&#39044;&#27979;&#26381;&#21153;&#22120;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#24182;&#25552;&#21069;&#20934;&#22791;&#36164;&#28304;&#65292;&#20197;&#30830;&#20445;&#22312;&#21160;&#24577;&#20113;&#29615;&#22659;&#20013;&#30340;&#26381;&#21153;&#27700;&#24179;&#30446;&#26631;&#65288;SLOs&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#20854;&#39044;&#27979;&#20219;&#21153;&#24120;&#24120;&#22312;&#22806;&#37096;&#20107;&#20214;&#65288;&#22914;&#20419;&#38144;&#27963;&#21160;&#21644;&#24212;&#29992;&#31243;&#24207;&#37325;&#26032;&#37197;&#32622;&#65289;&#24341;&#36215;&#30340;&#24322;&#24120;&#27969;&#37327;&#19979;&#24615;&#33021;&#19979;&#38477;&#65292;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#38271;&#26102;&#38388;&#21382;&#21490;&#25968;&#25454;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#20294;&#20195;&#20215;&#26159;&#39640;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#23494;&#24230;&#30340;&#35760;&#24518;&#36873;&#25321;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#32593;&#32476;&#23398;&#20064;&#27169;&#22411;&#65288;DMSHM&#65289;&#65292;&#21482;&#20351;&#29992;&#19968;&#23567;&#37096;&#20998;&#21382;&#21490;&#26085;&#24535;&#25968;&#25454;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#22312;&#39044;&#27979;&#20219;&#21153;&#20013;&#24212;&#29992;&#37325;&#25918;&#24335;&#25345;&#32493;&#23398;&#20064;&#26102;&#30340;&#26679;&#26412;&#37325;&#21472;&#29616;&#35937;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#24182;&#26377;&#25928;&#22320;&#25972;&#21512;&#26032;&#30340;&#26679;&#26412;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#35760;&#24518;&#36873;&#25321;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#32593;&#32476;&#23398;&#20064;&#27169;&#22411;&#65288;DMSHM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive Autoscaling is used to forecast the workloads of servers and prepare the resources in advance to ensure service level objectives (SLOs) in dynamic cloud environments.However, in practice, its prediction task often suffers from performance degradation under abnormal traffics caused by external events (such as sales promotional activities and applications' re-configurations), for which a common solution is to re-train the model with data of a long historical period, but at the expense of high computational and storage costs.To better address this problem, we propose a replay-based continual learning method, i.e., Density-based Memory Selection and Hint-based Network Learning Model (DMSHM), using only a small part of the historical log to achieve accurate predictions.First, we discover the phenomenon of sample overlap when applying replay-based continual learning in prediction tasks. In order to surmount this challenge and effectively integrate new sample distribution, we propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#19982;&#22522;&#26412;&#35821;&#35328;&#20219;&#21153;&#30340;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#22797;&#26434;&#25216;&#33021;&#20135;&#29983;&#30340;&#26426;&#21046;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#23450;&#24459;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#23398;&#20064;&#65292;&#24182;&#34920;&#29616;&#20986;&#36829;&#21453;&#36890;&#24120;&#27867;&#21270;&#29702;&#35770;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.15936</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#22797;&#26434;&#25216;&#33021;&#20135;&#29983;&#30340;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Theory for Emergence of Complex Skills in Language Models. (arXiv:2307.15936v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#19982;&#22522;&#26412;&#35821;&#35328;&#20219;&#21153;&#30340;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#22797;&#26434;&#25216;&#33021;&#20135;&#29983;&#30340;&#26426;&#21046;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#23450;&#24459;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#23398;&#20064;&#65292;&#24182;&#34920;&#29616;&#20986;&#36829;&#21453;&#36890;&#24120;&#27867;&#21270;&#29702;&#35770;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#38598;&#21512;&#21644;&#35757;&#32451;&#35821;&#26009;&#24211;&#25193;&#22823;&#26102;&#65292;&#26032;&#30340;&#25216;&#33021;&#23558;&#22312; AI &#20135;&#21697;&#20013;&#20986;&#29616;&#30340;&#20027;&#35201;&#39537;&#21160;&#22240;&#32032;&#12290;&#36825;&#31181;&#29616;&#35937;&#23578;&#19981;&#20026;&#20154;&#25152;&#29702;&#35299;&#65292;&#24182;&#19988;&#36890;&#36807;&#23545;&#22522;&#20110;&#26799;&#24230;&#35757;&#32451;&#30340;&#25968;&#23398;&#20998;&#26512;&#25552;&#20379;&#26426;&#26800;&#35299;&#37322;&#20284;&#20046;&#24456;&#22256;&#38590;&#12290;&#26412;&#25991;&#37319;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33879;&#21517;&#30340;&#65288;&#21644;&#32463;&#39564;&#24615;&#30340;&#65289;LLM&#25193;&#23637;&#23450;&#24459;&#21644;&#31616;&#21333;&#30340;&#32479;&#35745;&#26694;&#26550;&#26469;&#20998;&#26512;&#20986;&#29616;&#12290;&#36129;&#29486;&#21253;&#25324;&#65306;&#65288;a&#65289;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#23558;LLM&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#19982;&#35821;&#35328;&#20219;&#21153;&#22522;&#26412;&#25216;&#33021;&#30340;&#33021;&#21147;&#30456;&#20851;&#32852;&#12290;&#65288;b&#65289;&#25968;&#23398;&#20998;&#26512;&#34920;&#26126;&#65292;&#25193;&#23637;&#23450;&#24459;&#24847;&#21619;&#30528;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#24471;&#38750;&#24120;&#39640;&#25928;&#12290;&#25105;&#20204;&#38750;&#27491;&#24335;&#22320;&#31216;&#20043;&#20026;&#8220;&#24377;&#24339;&#27867;&#21270;&#8221;&#65292;&#22240;&#20026;&#34920;&#38754;&#19978;&#30475;&#65292;&#23427;&#20284;&#20046;&#25552;&#20379;&#20102;&#22312;&#25216;&#33021;&#27700;&#24179;&#19978;&#36829;&#21453;&#36890;&#24120;&#27867;&#21270;&#29702;&#35770;&#30340;&#33021;&#21147;&#12290;&#65288;c&#65289;&#24377;&#24339;&#27867;&#21270;&#30340;&#19968;&#20010;&#20851;&#38190;&#20363;&#23376;&#65292;&#21363;&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major driver of AI products today is the fact that new skills emerge in language models when their parameter set and training corpora are scaled up. This phenomenon is poorly understood, and a mechanistic explanation via mathematical analysis of gradient-based training seems difficult. The current paper takes a different approach, analysing emergence using the famous (and empirical) Scaling Laws of LLMs and a simple statistical framework. Contributions include: (a) A statistical framework that relates cross-entropy loss of LLMs to competence on the basic skills that underlie language tasks. (b) Mathematical analysis showing that the Scaling Laws imply a strong form of inductive bias that allows the pre-trained model to learn very efficiently. We informally call this {\em slingshot generalization} since naively viewed it appears to give competence levels at skills that violate usual generalization theory. (c) A key example of slingshot generalization, that competence at executing task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20813;&#30123;&#24211;&#20998;&#31867;&#21644;&#30142;&#30149;&#30456;&#20851;&#20813;&#30123;&#21463;&#20307;&#24207;&#21015;&#35782;&#21035;&#30340;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#24378;&#20581;&#30340;&#35757;&#32451;&#31574;&#30053;&#21644;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#26469;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#22122;&#22768;&#26631;&#31614;&#21644;&#30830;&#35748;&#20559;&#35265;&#38382;&#39064;&#12290;&#36825;&#23545;&#20110;&#21457;&#23637;&#26032;&#30123;&#33495;&#21644;&#20813;&#30123;&#30103;&#27861;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.15934</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#20813;&#30123;&#24211;&#20998;&#31867;&#21644;&#30142;&#30149;&#30456;&#20851;&#20813;&#30123;&#21463;&#20307;&#24207;&#21015;&#35782;&#21035;&#30340;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Noisy-Label-Learning Formulation for Immune Repertoire Classification and Disease-Associated Immune Receptor Sequence Identification. (arXiv:2307.15934v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20813;&#30123;&#24211;&#20998;&#31867;&#21644;&#30142;&#30149;&#30456;&#20851;&#20813;&#30123;&#21463;&#20307;&#24207;&#21015;&#35782;&#21035;&#30340;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#24378;&#20581;&#30340;&#35757;&#32451;&#31574;&#30053;&#21644;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#26469;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#22122;&#22768;&#26631;&#31614;&#21644;&#30830;&#35748;&#20559;&#35265;&#38382;&#39064;&#12290;&#36825;&#23545;&#20110;&#21457;&#23637;&#26032;&#30123;&#33495;&#21644;&#20813;&#30123;&#30103;&#27861;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20813;&#30123;&#24211;&#20998;&#31867;&#26159;&#35745;&#31639;&#29983;&#29289;&#23398;&#20013;&#30340;&#19968;&#20010;&#21069;&#27839;&#30740;&#31350;&#35838;&#39064;&#65292;&#23545;&#20110;&#26032;&#30123;&#33495;&#21644;&#20813;&#30123;&#30103;&#27861;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#23454;&#20363;&#31354;&#38388;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#23558;&#21253;&#32423;&#21035;&#26631;&#31614;&#20998;&#37197;&#32473;&#23454;&#20363;&#65292;&#38754;&#20020;&#22823;&#37327;&#22122;&#22768;&#26631;&#31614;&#21644;&#26497;&#20302;&#30340;&#35777;&#20154;&#29575;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#20813;&#30123;&#24211;&#20998;&#31867;&#20219;&#21153;&#12290;&#20026;&#20102;&#32416;&#27491;&#24207;&#21015;&#32423;&#21035;&#20998;&#31867;&#22120;&#23545;&#20813;&#30123;&#24211;&#32423;&#21035;&#26631;&#31614;&#30340;&#19981;&#20934;&#30830;&#30417;&#30563;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#24378;&#20581;&#30340;&#35757;&#32451;&#31574;&#30053;&#65306;&#21021;&#22987;&#26631;&#31614;&#32463;&#36807;&#24179;&#28369;&#22788;&#29702;&#65292;&#36880;&#27493;&#20351;&#29992;&#27169;&#22411;&#30340;&#39044;&#27979;&#36827;&#34892;&#20462;&#27491;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21516;&#26102;&#35757;&#32451;&#20102;&#20004;&#20010;&#20855;&#26377;&#30456;&#21516;&#26550;&#26500;&#20294;&#21442;&#25968;&#21021;&#22987;&#21270;&#19981;&#21516;&#30340;&#27169;&#22411;&#65292;&#20197;&#32416;&#27491;&#24050;&#30693;&#30340;&#33258;&#25105;&#35757;&#32451;&#27169;&#24335;&#20013;&#30340;&#8220;&#30830;&#35748;&#20559;&#35265;&#8221;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Immune repertoire classification, a typical multiple instance learning (MIL) problem, is a frontier research topic in computational biology that makes transformative contributions to new vaccines and immune therapies. However, the traditional instance-space MIL, directly assigning bag-level labels to instances, suffers from the massive amount of noisy labels and extremely low witness rate. In this work, we propose a noisy-label-learning formulation to solve the immune repertoire classification task. To remedy the inaccurate supervision of repertoire-level labels for a sequence-level classifier, we design a robust training strategy: The initial labels are smoothed to be asymmetric and are progressively corrected using the model's predictions throughout the training process. Furthermore, two models with the same architecture but different parameter initialization are co-trained simultaneously to remedy the known "confirmation bias" problem in the self-training-like schema. As a result, w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#35299;&#20915;&#30340;&#21160;&#20316;&#24207;&#21015;&#30340;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#25193;&#23637;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#32467;&#26500;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.15931</link><description>&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#21160;&#24577;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dynamic deep-reinforcement-learning algorithm in Partially Observed Markov Decision Processes. (arXiv:2307.15931v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#35299;&#20915;&#30340;&#21160;&#20316;&#24207;&#21015;&#30340;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#25193;&#23637;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#32467;&#26500;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#27493;&#65292;&#24182;&#19988;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#38750;&#38745;&#24577;&#24178;&#25200;&#65292;&#20351;&#24471;&#26234;&#33021;&#20307;&#38590;&#20197;&#20445;&#25345;&#24615;&#33021;&#12290;&#36825;&#31181;&#24178;&#25200;&#20135;&#29983;&#20102;&#34987;&#31216;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#29615;&#22659;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#20272;&#35745;&#22120;&#25110;&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#20013;&#20351;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26469;&#22788;&#29702;&#12290;&#36825;&#20004;&#31181;&#24773;&#20917;&#37117;&#38656;&#35201;&#22788;&#29702;&#36712;&#36857;&#19978;&#30340;&#24207;&#21015;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21482;&#26377;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;&#35201;&#32771;&#34385;&#30340;&#20449;&#24687;&#30340;&#24433;&#21709;&#20197;&#21450;&#22788;&#29702;&#23427;&#20204;&#30340;&#32593;&#32476;&#32467;&#26500;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#26102;&#21253;&#21547;&#21160;&#20316;&#24207;&#21015;&#30340;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#32467;&#26500;&#21644;&#26041;&#27861;&#26469;&#25193;&#23637;&#26368;&#26032;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning has been greatly improved in recent studies and an increased interest in real-world implementation has emerged in recent years. In many cases, due to the non-static disturbances, it becomes challenging for the agent to keep the performance. The disturbance results in the environment called Partially Observable Markov Decision Process. In common practice, Partially Observable Markov Decision Process is handled by introducing an additional estimator, or Recurrent Neural Network is utilized in the context of reinforcement learning. Both of the cases require to process sequential information on the trajectory. However, there are only a few studies investigating the effect of information to consider and the network structure to handle them. This study shows the benefit of action sequence inclusion in order to solve Partially Observable Markov Decision Process. Several structures and approaches are proposed to extend one of the latest deep reinforcement learning algori
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#31354;&#38388;&#32467;&#26500;&#30340;&#24050;&#26377;&#21644;&#26032;&#22686;&#22522;&#30784;&#35774;&#26045;&#25910;&#38598;&#30340;&#29615;&#22659;&#31354;&#27668;&#36136;&#37327;&#25968;&#25454;&#65292;&#24182;&#23884;&#20837;&#21040;&#20219;&#20309;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#27169;&#22411;&#20013;&#24212;&#29992;&#20110;&#20855;&#26377;&#28436;&#21464;&#31354;&#38388;&#32467;&#26500;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2307.15916</link><description>&lt;p&gt;
&#26426;&#20250;&#20027;&#20041;&#29615;&#22659;&#31354;&#27668;&#36136;&#37327;&#30417;&#27979;&#21644;&#39044;&#27979;&#19982;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Opportunistic Air Quality Monitoring and Forecasting with Expandable Graph Neural Networks. (arXiv:2307.15916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15916
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#31354;&#38388;&#32467;&#26500;&#30340;&#24050;&#26377;&#21644;&#26032;&#22686;&#22522;&#30784;&#35774;&#26045;&#25910;&#38598;&#30340;&#29615;&#22659;&#31354;&#27668;&#36136;&#37327;&#25968;&#25454;&#65292;&#24182;&#23884;&#20837;&#21040;&#20219;&#20309;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#27169;&#22411;&#20013;&#24212;&#29992;&#20110;&#20855;&#26377;&#28436;&#21464;&#31354;&#38388;&#32467;&#26500;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29615;&#22659;&#31354;&#27668;&#36136;&#37327;&#30417;&#27979;&#21644;&#39044;&#27979;&#24050;&#25104;&#20026;&#28909;&#38376;&#30740;&#31350;&#35838;&#39064;&#12290;&#30001;&#20110;&#22478;&#24066;&#22320;&#21306;&#24050;&#24314;&#31435;&#20102;&#20581;&#20840;&#30340;&#25968;&#25454;&#25910;&#38598;&#35774;&#26045;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#26041;&#27861;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36890;&#24120;&#30001;&#22269;&#23478;&#30740;&#31350;&#26426;&#26500;&#25110;&#31185;&#25216;&#24040;&#22836;&#37096;&#32626;&#30340;&#22266;&#23450;&#22522;&#30784;&#35774;&#26045;&#24448;&#24448;&#26080;&#27861;&#28385;&#36275;&#19981;&#21516;&#20010;&#24615;&#21270;&#22330;&#26223;&#30340;&#38656;&#27714;&#65292;&#20363;&#22914;&#22312;&#27809;&#26377;&#20219;&#20309;&#29616;&#26377;&#22522;&#30784;&#35774;&#26045;&#30340;&#21306;&#22495;&#36827;&#34892;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#35268;&#27169;&#36739;&#23567;&#30340;&#30740;&#31350;&#26426;&#26500;&#25110;&#26377;&#38480;&#39044;&#31639;&#30340;&#20844;&#21496;&#19981;&#24471;&#19981;&#23547;&#27714;&#23450;&#21046;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#26356;&#28789;&#27963;&#30340;&#25968;&#25454;&#25910;&#38598;&#22522;&#30784;&#35774;&#26045;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476; (EGAT) &#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#24050;&#26377;&#21644;&#26032;&#22686;&#22522;&#30784;&#35774;&#26045;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#36866;&#24212;&#19981;&#21516;&#30340;&#31354;&#38388;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#21487;&#20197;&#23884;&#20837;&#20219;&#20309;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#28436;&#21464;&#31354;&#38388;&#32467;&#26500;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Air Quality Monitoring and Forecasting has been a popular research topic in recent years. Recently, data-driven approaches for air quality forecasting have garnered significant attention, owing to the availability of well-established data collection facilities in urban areas. Fixed infrastructures, typically deployed by national institutes or tech giants, often fall short in meeting the requirements of diverse personalized scenarios, e.g., forecasting in areas without any existing infrastructure. Consequently, smaller institutes or companies with limited budgets are compelled to seek tailored solutions by introducing more flexible infrastructures for data collection. In this paper, we propose an expandable graph attention network (EGAT) model, which digests data collected from existing and newly-added infrastructures, with different spatial structures. Additionally, our proposal can be embedded into any air quality forecasting models, to apply to the scenarios with evolving spatial str
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#33258;&#21160;&#26426;&#29702;&#35770;&#26041;&#27861;&#21512;&#25104;&#28385;&#36275;&#25351;&#23450;&#23646;&#24615;&#30340;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#26102;&#24577;&#36923;&#36753;&#20316;&#20026;&#35268;&#33539;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2307.15907</link><description>&lt;p&gt;
&#29992;&#33258;&#21160;&#26426;&#29702;&#35770;&#26041;&#27861;&#21512;&#25104;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
An Automata-Theoretic Approach to Synthesizing Binarized Neural Networks. (arXiv:2307.15907v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15907
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#33258;&#21160;&#26426;&#29702;&#35770;&#26041;&#27861;&#21512;&#25104;&#28385;&#36275;&#25351;&#23450;&#23646;&#24615;&#30340;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#26102;&#24577;&#36923;&#36753;&#20316;&#20026;&#35268;&#33539;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs&#65292;&#20063;&#31216;&#20026;NNs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#24182;&#19988;&#24050;&#34987;&#35777;&#26126;&#26159;&#25104;&#21151;&#30340;&#12290;&#28982;&#32780;&#65292;&#20276;&#38543;&#32780;&#26469;&#30340;&#26114;&#36149;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#20351;&#24471;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#37327;&#21270;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#28014;&#28857;&#25968;&#37327;&#21270;&#20026;&#20302;&#23485;&#24230;&#30340;&#23450;&#28857;&#34920;&#31034;&#26469;&#20943;&#23569;DNNs&#30340;&#25104;&#26412;&#65292;&#21516;&#26102;&#20960;&#20046;&#19981;&#20250;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#21457;&#23637;&#20102;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;(QNNs)&#65292;&#32780;&#23558;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;(BNNs)&#38480;&#21046;&#20026;&#20108;&#36827;&#21046;&#20540;&#20316;&#20026;&#19968;&#20010;&#29305;&#27530;&#24773;&#20917;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#21478;&#19968;&#20010;&#20851;&#27880;&#28857;&#26159;&#23427;&#20204;&#30340;&#33030;&#24369;&#24615;&#21644;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#23613;&#31649;&#22312;DNN&#30340;&#21487;&#20449;&#24615;&#26041;&#38754;&#26377;&#35768;&#22810;&#27963;&#36291;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;QNNs&#30340;&#26041;&#27861;&#21364;&#24456;&#23569;&#25552;&#20986;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#33258;&#21160;&#26426;&#29702;&#35770;&#26041;&#27861;&#21512;&#25104;&#28385;&#36275;&#25351;&#23450;&#23646;&#24615;&#30340;BNNs&#30340;&#26041;&#27861;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#31216;&#20026;BLTL&#30340;&#26102;&#24577;&#36923;&#36753;&#20316;&#20026;&#35268;&#33539;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks, (DNNs, a.k.a. NNs), have been widely used in various tasks and have been proven to be successful. However, the accompanied expensive computing and storage costs make the deployments in resource-constrained devices a significant concern. To solve this issue, quantization has emerged as an effective way to reduce the costs of DNNs with little accuracy degradation by quantizing floating-point numbers to low-width fixed-point representations. Quantized neural networks (QNNs) have been developed, with binarized neural networks (BNNs) restricted to binary values as a special case. Another concern about neural networks is their vulnerability and lack of interpretability. Despite the active research on trustworthy of DNNs, few approaches have been proposed to QNNs. To this end, this paper presents an automata-theoretic approach to synthesizing BNNs that meet designated properties. More specifically, we define a temporal logic, called BLTL, as the specification language. W
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;&#31232;&#30095;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#22270;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#30340;&#29305;&#24449;&#36873;&#25321;&#65292;&#33021;&#22815;&#20174;&#22810;&#20010;&#25968;&#25454;&#35270;&#22270;&#20013;&#26377;&#25928;&#22320;&#25429;&#25417;&#25968;&#25454;&#30340;&#22522;&#26412;&#32467;&#26500;&#12290;&#20351;&#29992;&#31232;&#30095;&#32422;&#26463;&#21644;&#21487;&#25193;&#23637;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#38477;&#32500;&#21644;&#36845;&#20195;&#31639;&#27861;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#22522;&#26412;&#25968;&#25454;&#32467;&#26500;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2307.15905</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#31232;&#30095;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#22270;&#29992;&#20110;&#38750;&#32447;&#24615;&#35889;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Multi-view Sparse Laplacian Eigenmaps for nonlinear Spectral Feature Selection. (arXiv:2307.15905v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;&#31232;&#30095;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#22270;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#30340;&#29305;&#24449;&#36873;&#25321;&#65292;&#33021;&#22815;&#20174;&#22810;&#20010;&#25968;&#25454;&#35270;&#22270;&#20013;&#26377;&#25928;&#22320;&#25429;&#25417;&#25968;&#25454;&#30340;&#22522;&#26412;&#32467;&#26500;&#12290;&#20351;&#29992;&#31232;&#30095;&#32422;&#26463;&#21644;&#21487;&#25193;&#23637;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#38477;&#32500;&#21644;&#36845;&#20195;&#31639;&#27861;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#22522;&#26412;&#25968;&#25454;&#32467;&#26500;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#38598;&#30340;&#22797;&#26434;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#21253;&#25324;&#36807;&#25311;&#21512;&#12289;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#32467;&#26524;&#35299;&#37322;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24517;&#39035;&#30830;&#23450;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#25968;&#25454;&#22522;&#26412;&#32467;&#26500;&#30340;&#20449;&#24687;&#23376;&#38598;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;&#30340;&#22810;&#35270;&#35282;&#31232;&#30095;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#22270;&#65288;MSLE&#65289;&#65292;&#23427;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#22810;&#20010;&#25968;&#25454;&#35270;&#22270;&#65292;&#24378;&#21046;&#26045;&#21152;&#20102;&#31232;&#30095;&#32422;&#26463;&#65292;&#24182;&#37319;&#29992;&#21487;&#25193;&#23637;&#30340;&#20248;&#21270;&#31639;&#27861;&#26469;&#30830;&#23450;&#25429;&#25417;&#22522;&#26412;&#25968;&#25454;&#32467;&#26500;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;MSLE&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#35270;&#35282;&#26500;&#24314;&#39640;&#32500;&#25968;&#25454;&#30340;&#26356;&#20581;&#22766;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20102;&#31232;&#30095;&#29305;&#24449;&#20998;&#35299;&#26469;&#38477;&#20302;&#25968;&#25454;&#30340;&#32500;&#24230;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#38477;&#32500;&#21518;&#30340;&#29305;&#24449;&#38598;&#12290;&#36890;&#36807;&#36845;&#20195;&#31639;&#27861;&#26469;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#23376;&#38598;&#30340;&#29305;&#24449;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
The complexity of high-dimensional datasets presents significant challenges for machine learning models, including overfitting, computational complexity, and difficulties in interpreting results. To address these challenges, it is essential to identify an informative subset of features that captures the essential structure of the data. In this study, the authors propose Multi-view Sparse Laplacian Eigenmaps (MSLE) for feature selection, which effectively combines multiple views of the data, enforces sparsity constraints, and employs a scalable optimization algorithm to identify a subset of features that capture the fundamental data structure. MSLE is a graph-based approach that leverages multiple views of the data to construct a more robust and informative representation of high-dimensional data. The method applies sparse eigendecomposition to reduce the dimensionality of the data, yielding a reduced feature set. The optimization problem is solved using an iterative algorithm alternati
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#30423;&#31995;&#32479;&#30340;&#22312;&#32447;&#21305;&#37197;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#23454;&#26102;&#26356;&#26032;&#21644;&#25506;&#32034;&#26032;&#20869;&#23481;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.15893</link><description>&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#23454;&#26102;&#21305;&#37197;&#65306;&#19968;&#31181;&#22522;&#20110;&#24378;&#30423;&#31995;&#32479;&#30340;&#22312;&#32447;&#21305;&#37197;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Matching: A Real-time Bandit System for Large-scale Recommendations. (arXiv:2307.15893v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15893
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#30423;&#31995;&#32479;&#30340;&#22312;&#32447;&#21305;&#37197;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#23454;&#26102;&#26356;&#26032;&#21644;&#25506;&#32034;&#26032;&#20869;&#23481;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#35265;&#35777;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#24037;&#19994;&#35268;&#27169;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#35768;&#22810;&#25104;&#21151;&#12290;&#36890;&#24120;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#20197;&#25209;&#22788;&#29702;&#26041;&#24335;&#31163;&#32447;&#35757;&#32451;&#30340;&#12290;&#23613;&#31649;&#25209;&#22788;&#29702;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#29992;&#25143;&#19982;&#25512;&#33616;&#24179;&#21488;&#30340;&#36807;&#21435;&#20114;&#21160;&#65292;&#20294;&#20854;&#38271;&#27169;&#22411;&#26356;&#26032;&#24310;&#36831;&#20197;&#21450;&#26131;&#21463;&#31995;&#32479;&#20559;&#24046;&#30340;&#24433;&#21709;&#20351;&#20854;&#38590;&#20197;&#36866;&#24212;&#20998;&#24067;&#21464;&#21270;&#24182;&#25506;&#32034;&#26032;&#30340;&#29289;&#21697;&#25110;&#29992;&#25143;&#20852;&#36259;&#12290;&#23613;&#31649;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#22810;&#33218;&#32769;&#34382;&#26426;&#65289;&#22312;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#20294;&#22312;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#23454;&#26102;&#23454;&#26045;&#20173;&#28982;&#21463;&#21040;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#22312;&#32447;&#26041;&#27861;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#22312;&#32447;&#27969;&#37327;&#24182;&#21516;&#26102;&#30830;&#20445;&#21450;&#26102;&#26356;&#26032;&#24378;&#30423;&#21442;&#25968;&#30340;&#21487;&#25193;&#23637;&#24615;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25506;&#32034;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#24456;&#23481;&#26131;&#23548;&#33268;&#19981;&#29702;&#24819;&#30340;&#29992;&#25143;&#20307;&#39564;&#65292;&#36825;&#31361;&#26174;&#20102;&#21046;&#23450;&#35814;&#23613;&#31574;&#30053;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The last decade has witnessed many successes of deep learning-based models for industry-scale recommender systems. These models are typically trained offline in a batch manner. While being effective in capturing users' past interactions with recommendation platforms, batch learning suffers from long model-update latency and is vulnerable to system biases, making it hard to adapt to distribution shift and explore new items or user interests. Although online learning-based approaches (e.g., multi-armed bandits) have demonstrated promising theoretical results in tackling these challenges, their practical real-time implementation in large-scale recommender systems remains limited. First, the scalability of online approaches in servicing a massive online traffic while ensuring timely updates of bandit parameters poses a significant challenge. Additionally, exploring uncertainty in recommender systems can easily result in unfavorable user experience, highlighting the need for devising intric
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#26102;&#24207;&#24046;&#20998;&#31639;&#27861;&#65292;&#21482;&#20351;&#29992;&#19968;&#20010;&#27493;&#38271;&#21442;&#25968;&#65292;&#24182;&#35777;&#26126;&#25910;&#25947;&#36895;&#24230;&#33267;&#23569;&#20026;$O(1/t)$&#12290;</title><link>http://arxiv.org/abs/2307.15892</link><description>&lt;p&gt;
&#20165;&#20351;&#29992;&#19968;&#20010;&#27493;&#38271;&#30340;&#26032;&#22411;&#26799;&#24230;&#26102;&#24207;&#24046;&#20998;&#31639;&#27861;&#65306;&#36890;&#36807;$L$-$\lambda$&#24179;&#28369;&#24615;&#36827;&#34892;&#25910;&#25947;&#36895;&#29575;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A new Gradient TD Algorithm with only One Step-size: Convergence Rate Analysis using $L$-$\lambda$ Smoothness. (arXiv:2307.15892v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#26102;&#24207;&#24046;&#20998;&#31639;&#27861;&#65292;&#21482;&#20351;&#29992;&#19968;&#20010;&#27493;&#38271;&#21442;&#25968;&#65292;&#24182;&#35777;&#26126;&#25910;&#25947;&#36895;&#24230;&#33267;&#23569;&#20026;$O(1/t)$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#26102;&#24207;&#24046;&#20998;&#65288;GTD&#65289;&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#31163;&#31574;&#30053;&#23398;&#20064;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#31639;&#27861;&#65292;&#20854;&#22797;&#26434;&#24230;&#20026;$O(d)$&#65288;$d$&#26159;&#29305;&#24449;&#25968;&#37327;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Impression GTD&#30340;&#20840;&#26032;&#21333;&#26102;&#38388;&#23610;&#24230;GTD&#31639;&#27861;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#26399;&#26395;td&#26356;&#26032;&#65288;NEU&#65289;&#30446;&#26631;&#65292;&#24182;&#21482;&#26377;&#19968;&#20010;&#27493;&#38271;&#21442;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#26032;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#33267;&#23569;&#19982;$O(1/t)$&#19968;&#26679;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient Temporal Difference (GTD) algorithms (Sutton et al., 2008, 2009) are the first $O(d)$ ($d$ is the number features) algorithms that have convergence guarantees for off-policy learning with linear function approximation. Liu et al. (2015) and Dalal et. al. (2018) proved the convergence rates of GTD, GTD2 and TDC are $O(t^{-\alpha/2})$ for some $\alpha \in (0,1)$. This bound is tight (Dalal et al., 2020), and slower than $O(1/\sqrt{t})$. GTD algorithms also have two step-size parameters, which are difficult to tune. In literature, there is a "single-time-scale" formulation of GTD. However, this formulation still has two step-size parameters.  This paper presents a truly single-time-scale GTD algorithm for minimizing the Norm of Expected td Update (NEU) objective, and it has only one step-size parameter. We prove that the new algorithm, called Impression GTD, converges at least as fast as $O(1/t)$. Furthermore, based on a generalization of the expected smoothness (Gower et al. 201
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FRPE&#30340;&#19968;&#38454;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#31574;&#30053;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#35774;&#32622;&#19979;&#37117;&#33021;&#25552;&#20379;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#24182;&#20855;&#26377;&#36739;&#23567;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.15890</link><description>&lt;p&gt;
&#40065;&#26834;&#31574;&#30053;&#35780;&#20272;&#30340;&#19968;&#38454;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
First-order Policy Optimization for Robust Policy Evaluation. (arXiv:2307.15890v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15890
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FRPE&#30340;&#19968;&#38454;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#31574;&#30053;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#35774;&#32622;&#19979;&#37117;&#33021;&#25552;&#20379;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#24182;&#20855;&#26377;&#36739;&#23567;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#31574;&#30053;&#20248;&#21270;&#30340;&#35270;&#35282;&#26469;&#36827;&#34892;&#20855;&#26377;s-rectangular&#27169;&#31946;&#38598;&#30340;&#40065;&#26834;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#31574;&#30053;&#35780;&#20272;&#12290;&#25152;&#24320;&#21457;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#19968;&#38454;&#31574;&#30053;&#35780;&#20272;&#65288;FRPE&#65289;&#65292;&#25552;&#20379;&#20102;&#23545;&#30830;&#23450;&#24615;&#65288;&#31163;&#32447;&#65289;&#21644;&#38543;&#26426;&#65288;&#22312;&#32447;&#65289;&#35774;&#32622;&#19979;&#30340;&#40065;&#26834;&#31574;&#30053;&#35780;&#20272;&#30340;&#31532;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#29992;&#34920;&#26684;&#34920;&#31034;&#25110;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#30830;&#23450;&#24615;&#35774;&#32622;&#20013;&#24314;&#31435;&#20102;&#32447;&#24615;&#25910;&#25947;&#65292;&#32780;&#22312;&#38543;&#26426;&#35774;&#32622;&#20013;&#20855;&#26377;&#36817;&#20284;O(1/&#949;^2)&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;FRPE&#36824;&#21487;&#20197;&#33258;&#28982;&#22320;&#25193;&#23637;&#21040;&#35780;&#20272;&#24102;&#26377;(s, a)-rectangular&#27169;&#31946;&#38598;&#30340;&#40065;&#26834;&#29366;&#24577;-&#21160;&#20316;&#20540;&#20989;&#25968;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;&#24320;&#21457;&#30340;&#32467;&#26524;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#40065;&#26834;MDP&#30340;&#38543;&#26426;&#31574;&#30053;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We adopt a policy optimization viewpoint towards policy evaluation for robust Markov decision process with $\mathrm{s}$-rectangular ambiguity sets. The developed method, named first-order policy evaluation (FRPE), provides the first unified framework for robust policy evaluation in both deterministic (offline) and stochastic (online) settings, with either tabular representation or generic function approximation. In particular, we establish linear convergence in the deterministic setting, and $\tilde{\mathcal{O}}(1/\epsilon^2)$ sample complexity in the stochastic setting. FRPE also extends naturally to evaluating the robust state-action value function with $(\mathrm{s}, \mathrm{a})$-rectangular ambiguity sets. We discuss the application of the developed results for stochastic policy optimization of large-scale robust MDPs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#24402;&#22240;&#26041;&#27861;&#35299;&#37322;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#36890;&#36807;&#20351;&#29992;&#20840;&#30424;&#32447;&#30913;&#22270;&#20687;&#21644;&#31867;&#21035;&#21152;&#26435;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#24212;&#29992;&#20102;&#19977;&#31181;&#24402;&#22240;&#26041;&#27861;&#23545;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#35299;&#37322;&#21644;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.15878</link><description>&lt;p&gt;
&#20351;&#29992;&#24402;&#22240;&#26041;&#27861;&#35299;&#37322;&#20840;&#30424;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Explaining Full-disk Deep Learning Model for Solar Flare Prediction using Attribution Methods. (arXiv:2307.15878v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#24402;&#22240;&#26041;&#27861;&#35299;&#37322;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#36890;&#36807;&#20351;&#29992;&#20840;&#30424;&#32447;&#30913;&#22270;&#20687;&#21644;&#31867;&#21035;&#21152;&#26435;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#24212;&#29992;&#20102;&#19977;&#31181;&#24402;&#22240;&#26041;&#27861;&#23545;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#35299;&#37322;&#21644;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20110;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20027;&#35201;&#20851;&#27880;&#34987;&#24573;&#35270;&#30340;&#36817;&#36793;&#32768;&#26001;&#65292;&#24182;&#21033;&#29992;&#24402;&#22240;&#26041;&#27861;&#23545;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#20102;&#21518;&#26399;&#23450;&#24615;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#27169;&#22411;&#65292;&#20351;&#29992;&#27599;&#23567;&#26102;&#30340;&#20840;&#30424;&#32447;&#30913;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#37319;&#29992;&#20108;&#36827;&#21046;&#39044;&#27979;&#27169;&#24335;&#26469;&#39044;&#27979;&#22312;&#25509;&#19979;&#26469;&#30340;24&#23567;&#26102;&#20869;&#21487;&#33021;&#21457;&#29983;&#30340;&#22823;&#20110;&#31561;&#20110;M&#32423;&#32768;&#26001;&#12290;&#20026;&#20102;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#25968;&#25454;&#22686;&#24378;&#21644;&#31867;&#21035;&#21152;&#26435;&#25216;&#26415;&#30340;&#34701;&#21512;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#25216;&#33021;&#32479;&#35745;&#37327;&#65288;TSS&#65289;&#21644;&#28023;&#24503;&#20811;&#25216;&#33021;&#24471;&#20998;&#65288;HSS&#65289;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24212;&#29992;&#20102;&#19977;&#31181;&#24402;&#22240;&#26041;&#27861;&#65292;&#20998;&#21035;&#26159;&#24341;&#23548;&#26799;&#24230;&#21152;&#26435;&#31867;&#28608;&#27963;&#26144;&#23556;&#12289;&#32508;&#21512;&#26799;&#24230;&#21644;&#28145;&#23618;Shapley&#21487;&#21152;&#35299;&#37322;&#65292;&#23545;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#20102;&#35299;&#37322;&#21644;&#20132;&#21449;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
This paper contributes to the growing body of research on deep learning methods for solar flare prediction, primarily focusing on highly overlooked near-limb flares and utilizing the attribution methods to provide a post hoc qualitative explanation of the model's predictions. We present a solar flare prediction model, which is trained using hourly full-disk line-of-sight magnetogram images and employs a binary prediction mode to forecast $\geq$M-class flares that may occur within the following 24-hour period. To address the class imbalance, we employ a fusion of data augmentation and class weighting techniques; and evaluate the overall performance of our model using the true skill statistic (TSS) and Heidke skill score (HSS). Moreover, we applied three attribution methods, namely Guided Gradient-weighted Class Activation Mapping, Integrated Gradients, and Deep Shapley Additive Explanations, to interpret and cross-validate our model's predictions with the explanations. Our analysis reve
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#20998;&#26512;&#26041;&#27861;&#29992;&#20110;&#21160;&#24577;&#31354;&#22495;&#37197;&#32622;&#65292;&#36890;&#36807;&#26500;&#24314;&#32422;&#26463;&#23884;&#20837;&#30340;&#22270;&#24418;&#65292;&#24212;&#29992;&#33258;&#36866;&#24212;&#31639;&#27861;&#26469;&#29983;&#25104;&#21512;&#20316;&#26426;&#22330;&#32676;&#20307;&#24182;&#22343;&#21248;&#20998;&#37197;&#24037;&#20316;&#36127;&#36733;&#65292;&#22312;&#21508;&#31181;&#20132;&#36890;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;50&#65285;&#30340;&#24037;&#20316;&#36127;&#36733;&#20943;&#23569;&#65292;&#24182;&#20026;&#20248;&#21270;&#31354;&#22495;&#37197;&#32622;&#30340;&#25512;&#33616;&#31995;&#32479;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2307.15876</link><description>&lt;p&gt;
GraphDAC&#65306;&#19968;&#31181;&#22270;&#20998;&#26512;&#26041;&#27861;&#29992;&#20110;&#21160;&#24577;&#31354;&#22495;&#37197;&#32622;
&lt;/p&gt;
&lt;p&gt;
GraphDAC: A Graph-Analytic Approach to Dynamic Airspace Configuration. (arXiv:2307.15876v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#20998;&#26512;&#26041;&#27861;&#29992;&#20110;&#21160;&#24577;&#31354;&#22495;&#37197;&#32622;&#65292;&#36890;&#36807;&#26500;&#24314;&#32422;&#26463;&#23884;&#20837;&#30340;&#22270;&#24418;&#65292;&#24212;&#29992;&#33258;&#36866;&#24212;&#31639;&#27861;&#26469;&#29983;&#25104;&#21512;&#20316;&#26426;&#22330;&#32676;&#20307;&#24182;&#22343;&#21248;&#20998;&#37197;&#24037;&#20316;&#36127;&#36733;&#65292;&#22312;&#21508;&#31181;&#20132;&#36890;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;50&#65285;&#30340;&#24037;&#20316;&#36127;&#36733;&#20943;&#23569;&#65292;&#24182;&#20026;&#20248;&#21270;&#31354;&#22495;&#37197;&#32622;&#30340;&#25512;&#33616;&#31995;&#32479;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22269;&#23478;&#31354;&#20013;&#20132;&#36890;&#31995;&#32479;&#65288;NAS&#65289;&#30001;&#20110;&#31354;&#20013;&#20132;&#36890;&#37327;&#30340;&#22686;&#21152;&#36798;&#21040;&#20102;&#23481;&#37327;&#38480;&#21046;&#65292;&#24182;&#19988;&#22522;&#20110;&#36807;&#26102;&#30340;&#39044;&#25112;&#26415;&#35745;&#21010;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21160;&#24577;&#30340;&#31354;&#22495;&#37197;&#32622;&#65288;DAC&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#21152;&#21534;&#21520;&#37327;&#24182;&#36866;&#24212;&#27874;&#21160;&#30340;&#31354;&#20013;&#20132;&#36890;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#32039;&#24613;&#24773;&#20917;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#31354;&#22495;&#26500;&#24314;&#20026;&#19968;&#20010;&#24102;&#32422;&#26463;&#30340;&#22270;&#24418;&#65292;&#21387;&#32553;&#20854;&#32500;&#24230;&#65292;&#24182;&#24212;&#29992;&#35889;&#32858;&#31867;&#21551;&#29992;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#29983;&#25104;&#21512;&#20316;&#26426;&#22330;&#32676;&#20307;&#24182;&#22343;&#21248;&#20998;&#37197;&#24037;&#20316;&#36127;&#36733;&#12290;&#22312;&#21508;&#31181;&#20132;&#36890;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#24037;&#20316;&#36127;&#36733;&#19981;&#24179;&#34913;&#20943;&#23569;&#20102;50&#65285;&#12290;&#36825;&#39033;&#30740;&#31350;&#26368;&#32456;&#21487;&#20197;&#25104;&#20026;&#20248;&#21270;&#31354;&#22495;&#37197;&#32622;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/KeFenge2022/GraphDAC.git&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current National Airspace System (NAS) is reaching capacity due to increased air traffic, and is based on outdated pre-tactical planning. This study proposes a more dynamic airspace configuration (DAC) approach that could increase throughput and accommodate fluctuating traffic, ideal for emergencies. The proposed approach constructs the airspace as a constraints-embedded graph, compresses its dimensions, and applies a spectral clustering-enabled adaptive algorithm to generate collaborative airport groups and evenly distribute workloads among them. Under various traffic conditions, our experiments demonstrate a 50\% reduction in workload imbalances. This research could ultimately form the basis for a recommendation system for optimized airspace configuration. Code available at https://github.com/KeFenge2022/GraphDAC.git
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#36328;&#32500;&#24230;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22312;&#33258;&#28982;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;2D&#20998;&#31867;&#32593;&#32476;&#30340;&#25928;&#33021;&#36801;&#31227;&#21040;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#65292;&#20811;&#26381;&#20102;&#38480;&#37327;&#26631;&#27880;&#25968;&#25454;&#21644;&#33719;&#21462;&#32422;&#26463;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.15872</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#36328;&#32500;&#24230;&#36801;&#31227;&#23398;&#20064;&#19982;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-dimensional transfer learning in medical image segmentation with deep learning. (arXiv:2307.15872v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15872
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#36328;&#32500;&#24230;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22312;&#33258;&#28982;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;2D&#20998;&#31867;&#32593;&#32476;&#30340;&#25928;&#33021;&#36801;&#31227;&#21040;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#65292;&#20811;&#26381;&#20102;&#38480;&#37327;&#26631;&#27880;&#25968;&#25454;&#21644;&#33719;&#21462;&#32422;&#26463;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#21508;&#31181;&#22270;&#20687;&#20998;&#26512;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#36827;&#23637;&#65292;2D&#22270;&#20687;&#20998;&#31867;&#32593;&#32476;&#30340;&#24615;&#33021;&#19981;&#26029;&#25552;&#21319;&#65292;&#24182;&#19988;&#22312;&#30001;&#25968;&#30334;&#19975;&#33258;&#28982;&#22270;&#29255;&#32452;&#25104;&#30340;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#23384;&#22312;&#26377;&#38480;&#30340;&#26631;&#27880;&#25968;&#25454;&#21644;&#33719;&#21462;&#32422;&#26463;&#65292;&#36827;&#23637;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#32771;&#34385;&#21040;&#21307;&#23398;&#25104;&#20687;&#25968;&#25454;&#30340;&#23481;&#31215;&#24615;&#65292;&#36825;&#20123;&#38480;&#21046;&#23588;&#20026;&#26126;&#26174;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#22312;&#33258;&#28982;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;2D&#20998;&#31867;&#32593;&#32476;&#30340;&#25928;&#33021;&#36801;&#31227;&#21040;2D&#12289;3D&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#24212;&#29992;&#20013;&#12290;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#21407;&#29702;&#30340;&#26032;&#22411;&#26550;&#26500;&#65306;&#36890;&#36807;&#23558;2D&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#23884;&#20837;&#21040;&#26356;&#39640;&#32500;&#24230;&#30340;U-Net&#20013;&#36827;&#34892;&#26435;&#37325;&#36716;&#31227;&#65292;&#20197;&#21450;&#36890;&#36807;&#23558;2D&#20998;&#21106;&#32593;&#32476;&#25193;&#23637;&#21040;&#26356;&#39640;&#32500;&#24230;&#30340;&#32593;&#32476;&#36827;&#34892;&#32500;&#24230;&#36716;&#31227;&#12290;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#32463;&#36807;&#20102;&#27979;&#35797;...
&lt;/p&gt;
&lt;p&gt;
Over the last decade, convolutional neural networks have emerged and advanced the state-of-the-art in various image analysis and computer vision applications. The performance of 2D image classification networks is constantly improving and being trained on databases made of millions of natural images. However, progress in medical image analysis has been hindered by limited annotated data and acquisition constraints. These limitations are even more pronounced given the volumetry of medical imaging data. In this paper, we introduce an efficient way to transfer the efficiency of a 2D classification network trained on natural images to 2D, 3D uni- and multi-modal medical image segmentation applications. In this direction, we designed novel architectures based on two key principles: weight transfer by embedding a 2D pre-trained encoder into a higher dimensional U-Net, and dimensional transfer by expanding a 2D segmentation network into a higher dimension one. The proposed networks were teste
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#24322;&#26500;&#21442;&#19982;&#32773;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#32858;&#31867;&#27491;&#21017;&#21270;&#26469;&#25913;&#36827;&#27169;&#22411;&#22312;&#25968;&#25454;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.15870</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#24322;&#26500;&#21442;&#19982;&#32773;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Semi-Supervised Federated Learning for Heterogeneous Participants. (arXiv:2307.15870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#24322;&#26500;&#21442;&#19982;&#32773;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#32858;&#31867;&#27491;&#21017;&#21270;&#26469;&#25913;&#36827;&#27169;&#22411;&#22312;&#25968;&#25454;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#31169;&#26377;&#25968;&#25454;&#19978;&#21327;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#35757;&#32451;&#21644;&#37096;&#32626;&#22823;&#22411;&#27169;&#22411;&#29992;&#20110;&#24191;&#27867;&#24212;&#29992;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#20998;&#31163;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#36890;&#36807;&#20943;&#36731;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#36127;&#25285;&#25552;&#20379;&#20102;&#20248;&#31168;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;SFL&#36890;&#24120;&#20551;&#35774;&#23458;&#25143;&#31471;&#20855;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#36827;&#34892;&#26412;&#22320;&#35757;&#32451;&#65292;&#28982;&#32780;&#22312;&#23454;&#36341;&#20013;&#24182;&#38750;&#24635;&#26159;&#22914;&#27492;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#37319;&#29992;&#21322;&#30417;&#30563;&#25216;&#26415;&#26469;&#21033;&#29992;FL&#20013;&#30340;&#26080;&#26631;&#35760;&#25968;&#25454;&#65292;&#20294;&#25968;&#25454;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#24615;&#25552;&#20986;&#20102;&#30830;&#20445;&#35757;&#32451;&#25928;&#29575;&#30340;&#21478;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31995;&#32479;Pseudo-Clustering Semi-SFL&#65292;&#29992;&#20110;&#22312;&#26631;&#35760;&#25968;&#25454;&#20301;&#20110;&#26381;&#21153;&#22120;&#19978;&#30340;&#24773;&#22659;&#19979;&#35757;&#32451;&#27169;&#22411;&#12290;&#36890;&#36807;&#24341;&#20837;&#32858;&#31867;&#27491;&#21017;&#21270;&#65292;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged to allow multiple clients to collaboratively train machine learning models on their private data. However, training and deploying large models for broader applications is challenging in resource-constrained environments. Fortunately, Split Federated Learning (SFL) offers an excellent solution by alleviating the computation and communication burden on the clients SFL often assumes labeled data for local training on clients, however, it is not the case in practice.Prior works have adopted semi-supervised techniques for leveraging unlabeled data in FL, but data non-IIDness poses another challenge to ensure training efficiency. Herein, we propose Pseudo-Clustering Semi-SFL, a novel system for training models in scenarios where labeled data reside on the server. By introducing Clustering Regularization, model performance under data non-IIDness can be improved. Besides, our theoretical and experimental investigations into model convergence reveal that the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Polyak-Lojasiewicz&#26465;&#20214;&#30340;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#30340;&#24555;&#36895;&#38543;&#26426;&#31639;&#27861;SPIDER-GDA&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#36739;&#20302;&#30340;&#22797;&#26434;&#24230;&#19979;&#25214;&#21040;&#25509;&#36817;&#26368;&#20248;&#35299;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.15868</link><description>&lt;p&gt;
&#22522;&#20110;Polyak-Lojasiewicz&#26465;&#20214;&#30340;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#30340;&#24555;&#36895;&#38543;&#26426;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Faster Stochastic Algorithms for Minimax Optimization under Polyak--{\L}ojasiewicz Conditions. (arXiv:2307.15868v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15868
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Polyak-Lojasiewicz&#26465;&#20214;&#30340;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#30340;&#24555;&#36895;&#38543;&#26426;&#31639;&#27861;SPIDER-GDA&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#36739;&#20302;&#30340;&#22797;&#26434;&#24230;&#19979;&#25214;&#21040;&#25509;&#36817;&#26368;&#20248;&#35299;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;Polyak-Lojasiewicz (PL)&#26465;&#20214;&#19979;&#30340;&#38543;&#26426;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SPIDER-GDA&#31639;&#27861;&#26469;&#35299;&#20915;&#24418;&#24335;&#20026;$\min_x \max_y f(x,y)\triangleq \frac{1}{n} \sum_{i=1}^n f_i(x,y)$&#30340;&#26377;&#38480;&#21644;&#38382;&#39064;&#65292;&#20854;&#20013;&#30446;&#26631;&#20989;&#25968;$f(x,y)$&#22312;$x$&#26041;&#21521;&#19978;&#26159;$\mu_x$-PL&#30340;&#65292;&#22312;$y$&#26041;&#21521;&#19978;&#26159;$\mu_y$-PL&#30340;&#65307;&#27599;&#20010;$f_i(x,y)$&#37117;&#26159;$L$-&#24179;&#28369;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SPIDER-GDA&#31639;&#27861;&#21487;&#20197;&#22312;${\mathcal O}\left((n + \sqrt{n}\,\kappa_x\kappa_y^2)\log (1/\epsilon)\right)$&#30340;&#38543;&#26426;&#19968;&#38454;oracle (SFO)&#22797;&#26434;&#24230;&#20869;&#25214;&#21040;&#19968;&#20010;$\epsilon$-&#26368;&#20248;&#35299;&#65292;&#36825;&#27604;&#29616;&#26377;&#26041;&#27861;&#30340;SFO&#19978;&#30028;${\mathcal O}\big((n + n^{2/3}\kappa_x\kappa_y^2)\log (1/\epsilon)\big)$&#26356;&#22909;&#65292;&#20854;&#20013;$\kappa_x\triangleq L/\mu_x$&#65292;$\kappa_y\triangleq L/\mu_y$&#12290;&#23545;&#20110;&#30149;&#24577;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#21152;&#36895;&#31639;&#27861;&#26469;&#36827;&#19968;&#27493;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#23427;&#21487;&#20197;&#23454;&#29616;$\tilde{{\mathcal O}}\big((n+\sqrt{n}\,\kappa_x\kappa_y)\log^2 (1/\epsilon)\big)$&#30340;SFO&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers stochastic first-order algorithms for minimax optimization under Polyak--{\L}ojasiewicz (PL) conditions. We propose SPIDER-GDA for solving the finite-sum problem of the form $\min_x \max_y f(x,y)\triangleq \frac{1}{n} \sum_{i=1}^n f_i(x,y)$, where the objective function $f(x,y)$ is $\mu_x$-PL in $x$ and $\mu_y$-PL in $y$; and each $f_i(x,y)$ is $L$-smooth. We prove SPIDER-GDA could find an $\epsilon$-optimal solution within ${\mathcal O}\left((n + \sqrt{n}\,\kappa_x\kappa_y^2)\log (1/\epsilon)\right)$ stochastic first-order oracle (SFO) complexity, which is better than the state-of-the-art method whose SFO upper bound is ${\mathcal O}\big((n + n^{2/3}\kappa_x\kappa_y^2)\log (1/\epsilon)\big)$, where $\kappa_x\triangleq L/\mu_x$ and $\kappa_y\triangleq L/\mu_y$. For the ill-conditioned case, we provide an accelerated algorithm to reduce the computational cost further. It achieves $\tilde{{\mathcal O}}\big((n+\sqrt{n}\,\kappa_x\kappa_y)\log^2 (1/\epsilon)\big)$ SFO u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#38754;&#37096;&#24494;&#34920;&#24773;&#65288;FMEs&#65289;&#35782;&#21035;&#26469;&#35786;&#26029;&#38544;&#24615;&#25233;&#37057;&#30151;&#65292;&#20174;&#32780;&#25429;&#25417;&#38590;&#20197;&#25417;&#25720;&#30340;&#25233;&#37057;&#30151;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38754;&#37096;&#26631;&#24535;&#28857;&#30340;&#20851;&#27880;&#21306;&#22495;&#65288;ROI&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;FMEs&#26497;&#20302;&#24378;&#24230;&#21644;&#24494;&#22937;&#30340;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#33258;&#25105;&#35786;&#26029;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.15862</link><description>&lt;p&gt;
&#36890;&#36807;&#38754;&#37096;&#24494;&#34920;&#24773;&#35782;&#21035;&#25429;&#25417;&#38590;&#20197;&#25417;&#25720;&#30340;&#25233;&#37057;&#30151;
&lt;/p&gt;
&lt;p&gt;
Catching Elusive Depression via Facial Micro-Expression Recognition. (arXiv:2307.15862v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#38754;&#37096;&#24494;&#34920;&#24773;&#65288;FMEs&#65289;&#35782;&#21035;&#26469;&#35786;&#26029;&#38544;&#24615;&#25233;&#37057;&#30151;&#65292;&#20174;&#32780;&#25429;&#25417;&#38590;&#20197;&#25417;&#25720;&#30340;&#25233;&#37057;&#30151;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38754;&#37096;&#26631;&#24535;&#28857;&#30340;&#20851;&#27880;&#21306;&#22495;&#65288;ROI&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;FMEs&#26497;&#20302;&#24378;&#24230;&#21644;&#24494;&#22937;&#30340;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#33258;&#25105;&#35786;&#26029;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#30151;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#31934;&#31070;&#20581;&#24247;&#38556;&#30861;&#65292;&#20250;&#23548;&#33268;&#25345;&#32493;&#25233;&#37057;&#30340;&#24773;&#32490;&#21644;&#24773;&#24863;&#22256;&#25200;&#31561;&#21518;&#26524;&#12290;&#20854;&#20013;&#19968;&#31867;&#25233;&#37057;&#30151;&#26159;&#38544;&#24615;&#25233;&#37057;&#30151;&#65292;&#24739;&#32773;&#25925;&#24847;&#25110;&#26080;&#24847;&#22320;&#36890;&#36807;&#22806;&#22312;&#20048;&#35266;&#20027;&#20041;&#38544;&#34255;&#30495;&#23454;&#24773;&#24863;&#65292;&#20174;&#32780;&#20351;&#35786;&#26029;&#21644;&#27835;&#30103;&#21464;&#24471;&#22797;&#26434;&#21644;&#24310;&#36831;&#65292;&#24182;&#23548;&#33268;&#24847;&#22806;&#33258;&#26432;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#38754;&#37096;&#24494;&#34920;&#24773;&#65288;FMEs&#65289;&#26469;&#26816;&#27979;&#21644;&#35782;&#21035;&#28508;&#22312;&#30340;&#30495;&#23454;&#24773;&#24863;&#26469;&#35786;&#26029;&#38544;&#24615;&#25233;&#37057;&#30151;&#12290;&#28982;&#32780;&#65292;FMEs&#30340;&#26497;&#20302;&#24378;&#24230;&#21644;&#24494;&#22937;&#30340;&#29305;&#24615;&#20351;&#23427;&#20204;&#30340;&#35782;&#21035;&#25104;&#20026;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38754;&#37096;&#26631;&#24535;&#28857;&#30340;&#20851;&#27880;&#21306;&#22495;&#65288;ROI&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#24182;&#25551;&#36848;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#20010;&#20154;&#29615;&#22659;&#65288;&#20363;&#22914;&#22312;&#23478;&#20013;&#65289;&#20351;&#29992;&#20415;&#25658;&#24335;&#31227;&#21160;&#35774;&#22791;&#36827;&#34892;&#33258;&#25105;&#35786;&#26029;&#12290;&#25105;&#20204;&#21576;&#29616;&#20102;&#39564;&#35777;&#25105;&#20204;&#26041;&#27861;&#30340;&#32467;&#26524;&#21644;&#21457;&#29616;&#65292;&#24182;&#35752;&#35770;&#20854;&#20182;&#25216;&#26415;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depression is a common mental health disorder that can cause consequential symptoms with continuously depressed mood that leads to emotional distress. One category of depression is Concealed Depression, where patients intentionally or unintentionally hide their genuine emotions through exterior optimism, thereby complicating and delaying diagnosis and treatment and leading to unexpected suicides. In this paper, we propose to diagnose concealed depression by using facial micro-expressions (FMEs) to detect and recognize underlying true emotions. However, the extremely low intensity and subtle nature of FMEs make their recognition a tough task. We propose a facial landmark-based Region-of-Interest (ROI) approach to address the challenge, and describe a low-cost and privacy-preserving solution that enables self-diagnosis using portable mobile devices in a personal setting (e.g., at home). We present results and findings that validate our method, and discuss other technical challenges and f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20135;&#21697;&#39033;&#30446;&#20998;&#31867;&#27169;&#22411;&#26694;&#26550;&#65292;&#21033;&#29992;&#24179;&#22343;&#38598;&#25104;&#21644;&#34701;&#21512;&#20998;&#31867;&#22120;&#30340;&#31616;&#21333;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#24182;&#36890;&#36807;&#20803;&#25968;&#25454;&#29305;&#24449;&#21644;&#20302;&#23618;&#29305;&#24449;&#24037;&#31243;&#25552;&#21319;&#20998;&#31867;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.15858</link><description>&lt;p&gt;
&#22810;&#36755;&#20986;&#22836;&#20449;&#24687;&#30340;&#20135;&#21697;&#39033;&#30446;&#20998;&#31867;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Multi-output Headed Ensembles for Product Item Classification. (arXiv:2307.15858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20135;&#21697;&#39033;&#30446;&#20998;&#31867;&#27169;&#22411;&#26694;&#26550;&#65292;&#21033;&#29992;&#24179;&#22343;&#38598;&#25104;&#21644;&#34701;&#21512;&#20998;&#31867;&#22120;&#30340;&#31616;&#21333;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#24182;&#36890;&#36807;&#20803;&#25968;&#25454;&#29305;&#24449;&#21644;&#20302;&#23618;&#29305;&#24449;&#24037;&#31243;&#25552;&#21319;&#20998;&#31867;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#30005;&#23376;&#21830;&#21153;&#30446;&#24405;&#20013;&#20135;&#21697;&#39033;&#30446;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;&#30005;&#23376;&#21830;&#21153;&#30446;&#24405;&#30340;&#20998;&#31867;&#21253;&#21547;&#20102;&#25968;&#21315;&#31181;&#27969;&#27966;&#65292;&#21830;&#23478;&#21487;&#20197;&#38543;&#26102;&#19978;&#20256;&#21830;&#21697;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#21830;&#23478;&#30340;&#20998;&#31867;&#32467;&#26524;&#24448;&#24448;&#26159;&#38169;&#35823;&#30340;&#65292;&#20294;&#21364;&#34987;&#24403;&#20316;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#38598;&#26102;&#30340;&#26631;&#31614;&#65292;&#20174;&#32780;&#24418;&#25104;&#20102;&#19968;&#20010;&#21453;&#39304;&#24490;&#29615;&#65292;&#23548;&#33268;&#27169;&#22411;&#36136;&#37327;&#36234;&#26469;&#36234;&#24046;&#12290;&#30001;&#20110;&#32570;&#20047;&#35268;&#27169;&#21487;&#35266;&#30340;&#31579;&#36873;&#35757;&#32451;&#38598;&#65292;&#20998;&#31867;&#38382;&#39064;&#21464;&#24471;&#23588;&#20026;&#26174;&#33879;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#36890;&#24120;&#20351;&#29992;&#22810;&#20010;&#20998;&#31867;&#22120;&#30340;&#32452;&#21512;&#26469;&#23545;&#25239;&#21333;&#19968;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#24615;&#33021;&#36739;&#24046;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21487;&#25193;&#23637;&#20998;&#31867;&#27169;&#22411;&#26694;&#26550;&#65292;&#21487;&#20197;&#21463;&#30410;&#20110;&#24179;&#22343;&#38598;&#25104;&#21644;&#34701;&#21512;&#20998;&#31867;&#22120;&#30340;&#31616;&#21333;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#36824;&#33021;&#22815;&#21033;&#29992;&#20803;&#25968;&#25454;&#29305;&#24449;&#21644;&#20302;&#23618;&#29305;&#24449;&#24037;&#31243;&#26469;&#25552;&#21319;&#20998;&#31867;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we revisit the problem of product item classification for large-scale e-commerce catalogs. The taxonomy of e-commerce catalogs consists of thousands of genres to which are assigned items that are uploaded by merchants on a continuous basis. The genre assignments by merchants are often wrong but treated as ground truth labels in automatically generated training sets, thus creating a feedback loop that leads to poorer model quality over time. This problem of taxonomy classification becomes highly pronounced due to the unavailability of sizable curated training sets.  Under such a scenario it is common to combine multiple classifiers to combat poor generalization performance from a single classifier. We propose an extensible deep learning based classification model framework that benefits from the simplicity and robustness of averaging ensembles and fusion based classifiers. We are also able to use metadata features and low-level feature engineering to boost classification 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;k-th&#30334;&#20998;&#20301;&#24615;&#33021;&#65288;KPP&#65289;&#26469;&#25429;&#25417;&#22312;CiM&#21152;&#36895;&#22120;&#19978;&#25191;&#34892;&#30340;DNN&#27169;&#22411;&#30340;&#30495;&#23454;&#26368;&#22351;&#24773;&#20917;&#24615;&#33021;&#65292;&#20174;&#32780;&#25913;&#21892;NVCiM DNN&#21152;&#36895;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15853</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#27491;&#30830;&#25130;&#23614;&#39640;&#26031;&#22122;&#22768;&#35757;&#32451;&#65292;&#25913;&#36827;NVCiM DNN&#21152;&#36895;&#22120;&#30340;&#30495;&#23454;&#26368;&#22351;&#24773;&#20917;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving Realistic Worst-Case Performance of NVCiM DNN Accelerators through Training with Right-Censored Gaussian Noise. (arXiv:2307.15853v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15853
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;k-th&#30334;&#20998;&#20301;&#24615;&#33021;&#65288;KPP&#65289;&#26469;&#25429;&#25417;&#22312;CiM&#21152;&#36895;&#22120;&#19978;&#25191;&#34892;&#30340;DNN&#27169;&#22411;&#30340;&#30495;&#23454;&#26368;&#22351;&#24773;&#20917;&#24615;&#33021;&#65292;&#20174;&#32780;&#25913;&#21892;NVCiM DNN&#21152;&#36895;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38750;&#26131;&#22833;&#24615;&#23384;&#20648;&#22120;&#65288;NVM&#65289;&#35774;&#22791;&#26500;&#24314;&#30340;&#35745;&#31639;&#23384;&#20648;&#19968;&#20307;&#65288;CiM&#65289;&#25216;&#26415;&#65292;&#30001;&#20110;&#20854;&#21407;&#22320;&#25968;&#25454;&#22788;&#29702;&#33021;&#21147;&#21644;&#21331;&#36234;&#30340;&#33021;&#28304;&#25928;&#29575;&#65292;&#23545;&#20110;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20855;&#26377;&#24456;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;&#27169;&#22411;&#21442;&#25968;&#26144;&#23556;&#21040;NVM&#35774;&#22791;&#21518;&#65292;&#24448;&#24448;&#20250;&#20986;&#29616;&#19982;&#39044;&#26399;&#20540;&#30456;&#24046;&#36739;&#22823;&#30340;&#20559;&#24046;&#65292;&#23548;&#33268;CiM DNN&#21152;&#36895;&#22120;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#12290;&#30446;&#21069;&#24050;&#32463;&#25552;&#20986;&#20102;&#24456;&#22810;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#20294;&#20027;&#35201;&#38598;&#20013;&#20110;&#25913;&#36827;CiM DNN&#21152;&#36895;&#22120;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;&#22914;&#20309;&#22312;&#35774;&#22791;&#21464;&#21270;&#30340;&#24433;&#21709;&#19979;&#20445;&#35777;&#26368;&#22351;&#24773;&#20917;&#24615;&#33021;&#65292;&#23545;&#20110;&#35768;&#22810;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65292;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#36828;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compute-in-Memory (CiM), built upon non-volatile memory (NVM) devices, is promising for accelerating deep neural networks (DNNs) owing to its in-situ data processing capability and superior energy efficiency. Unfortunately, the well-trained model parameters, after being mapped to NVM devices, can often exhibit large deviations from their intended values due to device variations, resulting in notable performance degradation in these CiM-based DNN accelerators. There exists a long list of solutions to address this issue. However, they mainly focus on improving the mean performance of CiM DNN accelerators. How to guarantee the worst-case performance under the impact of device variations, which is crucial for many safety-critical applications such as self-driving cars, has been far less explored. In this work, we propose to use the k-th percentile performance (KPP) to capture the realistic worst-case performance of DNN models executing on CiM accelerators. Through a formal analysis of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22522;&#20110;IRT&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#31639;&#27861;&#32452;&#21512;&#22312;&#25968;&#25454;&#38598;&#20179;&#24211;&#20013;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#33719;&#21462;&#31639;&#27861;&#19968;&#33268;&#24615;&#21644;&#24322;&#24120;&#24615;&#31561;&#29305;&#24449;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23545;&#20256;&#32479;IRT&#27169;&#22411;&#36827;&#34892;&#20498;&#36716;&#21644;&#37325;&#26032;&#35299;&#37322;&#26469;&#23454;&#29616;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#38598;&#29305;&#24449;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2307.15850</link><description>&lt;p&gt;
&#20351;&#29992;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#23545;&#32508;&#21512;&#31639;&#27861;&#32452;&#21512;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Algorithm Portfolio Evaluation using Item Response Theory. (arXiv:2307.15850v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22522;&#20110;IRT&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#31639;&#27861;&#32452;&#21512;&#22312;&#25968;&#25454;&#38598;&#20179;&#24211;&#20013;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#33719;&#21462;&#31639;&#27861;&#19968;&#33268;&#24615;&#21644;&#24322;&#24120;&#24615;&#31561;&#29305;&#24449;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23545;&#20256;&#32479;IRT&#27169;&#22411;&#36827;&#34892;&#20498;&#36716;&#21644;&#37325;&#26032;&#35299;&#37322;&#26469;&#23454;&#29616;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#38598;&#29305;&#24449;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#65288;IRT&#65289;&#34987;&#25552;&#20986;&#29992;&#20110;&#25945;&#32946;&#24515;&#29702;&#27979;&#37327;&#23398;&#39046;&#22495;&#65292;&#29992;&#20110;&#35780;&#20272;&#23398;&#29983;&#33021;&#21147;&#12289;&#27979;&#35797;&#39064;&#38590;&#24230;&#21644;&#21306;&#20998;&#24230;&#12290;&#26368;&#36817;&#65292;IRT&#24050;&#34987;&#24212;&#29992;&#20110;&#35780;&#20272;&#21333;&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24615;&#33021;&#65292;&#20854;&#20013;&#23398;&#29983;&#29616;&#22312;&#26159;&#19968;&#20010;&#31639;&#27861;&#65292;&#32780;&#27979;&#35797;&#39064;&#26159;&#31639;&#27861;&#35201;&#23545;&#35266;&#23519;&#32467;&#26524;&#36827;&#34892;&#20998;&#31867;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22522;&#20110;IRT&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#19968;&#20010;&#31639;&#27861;&#32452;&#21512;&#22312;&#19968;&#20010;&#25968;&#25454;&#38598;&#20179;&#24211;&#20013;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#33719;&#21462;&#31639;&#27861;&#19968;&#33268;&#24615;&#21644;&#24322;&#24120;&#24615;&#31561;&#26356;&#20016;&#23500;&#30340;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#25551;&#36848;&#20102;&#31639;&#27861;&#24615;&#33021;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#36825;&#20123;&#29305;&#24449;&#26159;&#36890;&#36807;&#23545;&#20256;&#32479;IRT&#27169;&#22411;&#36827;&#34892;&#26032;&#39062;&#30340;&#20498;&#36716;&#21644;&#37325;&#26032;&#35299;&#37322;&#32780;&#24471;&#21040;&#30340;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#38598;&#29305;&#24449;&#35745;&#31639;&#12290;&#25105;&#20204;&#23545;&#19981;&#21516;&#24212;&#29992;&#30340;&#31639;&#27861;&#32452;&#21512;&#22312;&#35813;&#26694;&#26550;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#20854;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Item Response Theory (IRT) has been proposed within the field of Educational Psychometrics to assess student ability as well as test question difficulty and discrimination power. More recently, IRT has been applied to evaluate machine learning algorithm performance on a single classification dataset, where the student is now an algorithm, and the test question is an observation to be classified by the algorithm. In this paper we present a modified IRT-based framework for evaluating a portfolio of algorithms across a repository of datasets, while simultaneously eliciting a richer suite of characteristics such as algorithm consistency and anomalousness - that describe important aspects of algorithm performance. These characteristics arise from a novel inversion and reinterpretation of the traditional IRT model without requiring additional dataset feature computations. We test this framework on algorithm portfolios for a wide range of applications, demonstrating the broad applicability 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20013;&#24615;&#21407;&#23376;&#36827;&#34892;&#37327;&#23376;&#26680;&#20272;&#35745;&#30340;&#30417;&#30563;&#20998;&#31867;&#30340;&#22522;&#20110;&#38376;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#29616;&#19968;&#31181;&#21442;&#25968;&#21270;&#24207;&#21015;&#26469;&#36827;&#34892;&#29305;&#24449;&#26144;&#23556;&#21644;&#26680;&#30697;&#38453;&#30340;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2307.15840</link><description>&lt;p&gt;
&#29992;&#20013;&#24615;&#21407;&#23376;&#36827;&#34892;&#37327;&#23376;&#26680;&#20272;&#35745;&#30340;&#30417;&#30563;&#20998;&#31867;&#65306;&#19968;&#31181;&#22522;&#20110;&#38376;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quantum Kernel Estimation With Neutral Atoms For Supervised Classification: A Gate-Based Approach. (arXiv:2307.15840v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20013;&#24615;&#21407;&#23376;&#36827;&#34892;&#37327;&#23376;&#26680;&#20272;&#35745;&#30340;&#30417;&#30563;&#20998;&#31867;&#30340;&#22522;&#20110;&#38376;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#29616;&#19968;&#31181;&#21442;&#25968;&#21270;&#24207;&#21015;&#26469;&#36827;&#34892;&#29305;&#24449;&#26144;&#23556;&#21644;&#26680;&#30697;&#38453;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26680;&#20272;&#35745;&#65288;QKE&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#20272;&#35745;&#32463;&#20856;&#38590;&#20197;&#35745;&#31639;&#30340;&#26680;&#20989;&#25968;&#30340;&#25216;&#26415;&#65292;&#28982;&#21518;&#30001;&#32463;&#20856;&#35745;&#31639;&#26426;&#29992;&#20110;&#35757;&#32451;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12290;&#30001;&#20110;&#23454;&#29616;&#38590;&#20197;&#32463;&#20856;&#27169;&#25311;&#30340;&#29305;&#24449;&#26144;&#23556;&#25152;&#38656;&#30340;2-&#23616;&#37096;&#31639;&#23376;&#25968;&#37327;&#36739;&#39640;&#65292;&#38656;&#35201;&#36739;&#39640;&#30340;&#37327;&#23376;&#27604;&#29305;&#36830;&#25509;&#24615;&#65292;&#32780;&#30446;&#21069;&#36229;&#23548;&#35774;&#22791;&#19978;&#26080;&#27861;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#21033;&#29992;&#20013;&#24615;&#21407;&#23376;&#37327;&#23376;&#35745;&#31639;&#26426;&#65292;&#22240;&#20026;&#23427;&#20204;&#20801;&#35768;&#26356;&#33258;&#30001;&#22320;&#23433;&#25490;&#21407;&#23376;&#12290;&#25991;&#29486;&#20013;&#21487;&#20197;&#25214;&#21040;&#22522;&#20110;&#20013;&#24615;&#21407;&#23376;&#30340;QKE&#31034;&#20363;&#65292;&#20294;&#23427;&#20204;&#20391;&#37325;&#20110;&#22270;&#23398;&#20064;&#24182;&#20351;&#29992;&#31867;&#27604;&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38376;&#27169;&#22411;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#22312;&#20174;&#28608;&#20809;&#33033;&#20914;&#24320;&#22987;&#25512;&#23548;1&#27604;&#29305;&#21644;2&#27604;&#29305;&#38376;&#20043;&#21518;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#29992;&#20110;3&#27604;&#29305;&#29305;&#24449;&#26144;&#23556;&#30340;&#21442;&#25968;&#21270;&#24207;&#21015;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#35813;&#24207;&#21015;&#20174;&#32463;&#39564;&#19978;&#35745;&#31639;&#26680;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum Kernel Estimation (QKE) is a technique based on leveraging a quantum computer to estimate a kernel function that is classically difficult to calculate, which is then used by a classical computer for training a Support Vector Machine (SVM). Given the high number of 2-local operators necessary for realizing a feature mapping hard to simulate classically, a high qubit connectivity is needed, which is not currently possible on superconducting devices. For this reason, neutral atom quantum computers can be used, since they allow to arrange the atoms with more freedom. Examples of neutral-atom-based QKE can be found in the literature, but they are focused on graph learning and use the analogue approach. In this paper, a general method based on the gate model is presented. After deriving 1-qubit and 2-qubit gates starting from laser pulses, a parameterized sequence for feature mapping on 3 qubits is realized. This sequence is then used to empirically compute the kernel matrix starting
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#21512;&#35843;&#26597;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#23558;&#36825;&#20004;&#20010;&#30446;&#26631;&#21516;&#26102;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#30340;&#25361;&#25112;&#21644;&#21487;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.15838</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#38544;&#31169;&#19982;&#20844;&#24179;&#24615;&#30340;&#25972;&#20307;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Holistic Survey of Privacy and Fairness in Machine Learning. (arXiv:2307.15838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15838
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#21512;&#35843;&#26597;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#23558;&#36825;&#20004;&#20010;&#30446;&#26631;&#21516;&#26102;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#30340;&#25361;&#25112;&#21644;&#21487;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#26159;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;(AI)&#21644;&#20540;&#24471;&#20449;&#36182;&#30340;&#26426;&#22120;&#23398;&#20064;(ML)&#30340;&#20004;&#20010;&#37325;&#35201;&#25903;&#26609;&#12290;&#27599;&#20010;&#30446;&#26631;&#22312;&#25991;&#29486;&#20013;&#37117;&#24471;&#21040;&#20102;&#29420;&#31435;&#30740;&#31350;&#65292;&#26088;&#22312;&#20943;&#23569;&#23454;&#29616;&#23427;&#20204;&#25152;&#24102;&#26469;&#30340;&#25928;&#29992;&#25439;&#22833;&#12290;&#23613;&#31649;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#23545;&#27492;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#20852;&#36259;&#65292;&#20294;&#20173;&#36843;&#20999;&#38656;&#35201;&#26356;&#28145;&#20837;&#30340;&#30740;&#31350;&#26469;&#25581;&#31034;&#22914;&#20309;&#23558;&#36825;&#20004;&#20010;&#30446;&#26631;&#21516;&#26102;&#38598;&#25104;&#21040;ML&#27169;&#22411;&#20013;&#12290;&#19982;&#34987;&#24191;&#27867;&#25509;&#21463;&#30340;&#26435;&#34913;&#19981;&#21516;&#65292;&#21363;&#38544;&#31169;-&#25928;&#29992;&#21644;&#20844;&#24179;-&#25928;&#29992;&#65292;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#23578;&#26410;&#34987;&#24456;&#22909;&#22320;&#29702;&#35299;&#12290;&#34429;&#28982;&#19968;&#20123;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#20010;&#30446;&#26631;&#20989;&#25968;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20294;&#20063;&#26377;&#20854;&#20182;&#30740;&#31350;&#34920;&#26126;&#22312;&#26576;&#20123;&#22330;&#26223;&#20013;&#36825;&#20123;&#20989;&#25968;&#26159;&#30456;&#20114;&#19968;&#33268;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;ML&#20013;&#30340;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#22238;&#39038;&#65292;&#21253;&#25324;&#30417;&#30563;&#23398;&#20064;&#12289;&#26080;&#30417;&#30563;&#23398;&#20064;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy and fairness are two crucial pillars of responsible Artificial Intelligence (AI) and trustworthy Machine Learning (ML). Each objective has been independently studied in the literature with the aim of reducing utility loss in achieving them. Despite the significant interest attracted from both academia and industry, there remains an immediate demand for more in-depth research to unravel how these two objectives can be simultaneously integrated into ML models. As opposed to well-accepted trade-offs, i.e., privacy-utility and fairness-utility, the interrelation between privacy and fairness is not well-understood. While some works suggest a trade-off between the two objective functions, there are others that demonstrate the alignment of these functions in certain scenarios. To fill this research gap, we provide a thorough review of privacy and fairness in ML, including supervised, unsupervised, semi-supervised, and reinforcement learning. After examining and consolidating the liter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25968;&#25454;&#24322;&#36136;&#24615;&#20013;&#20445;&#25345;&#29992;&#25143;&#32423;&#38544;&#31169;&#30340;&#22343;&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#20801;&#35768;&#29992;&#25143;&#25968;&#25454;&#22312;&#20998;&#24067;&#21644;&#25968;&#37327;&#19978;&#30340;&#24046;&#24322;&#65292;&#24182;&#35777;&#26126;&#20102;&#20272;&#35745;&#22120;&#30340;&#28176;&#36817;&#26368;&#20248;&#24615;&#21644;&#21487;&#36798;&#21040;&#30340;&#35823;&#24046;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2307.15835</link><description>&lt;p&gt;
&#22312;&#25968;&#25454;&#24322;&#36136;&#24615;&#20013;&#20445;&#25345;&#29992;&#25143;&#32423;&#38544;&#31169;&#30340;&#22343;&#20540;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Mean Estimation with User-level Privacy under Data Heterogeneity. (arXiv:2307.15835v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25968;&#25454;&#24322;&#36136;&#24615;&#20013;&#20445;&#25345;&#29992;&#25143;&#32423;&#38544;&#31169;&#30340;&#22343;&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#20801;&#35768;&#29992;&#25143;&#25968;&#25454;&#22312;&#20998;&#24067;&#21644;&#25968;&#37327;&#19978;&#30340;&#24046;&#24322;&#65292;&#24182;&#35777;&#26126;&#20102;&#20272;&#35745;&#22120;&#30340;&#28176;&#36817;&#26368;&#20248;&#24615;&#21644;&#21487;&#36798;&#21040;&#30340;&#35823;&#24046;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#35768;&#22810;&#29616;&#20195;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#38754;&#20020;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#29992;&#25143;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12290;&#19981;&#21516;&#30340;&#29992;&#25143;&#21487;&#33021;&#25317;&#26377;&#25130;&#28982;&#19981;&#21516;&#25968;&#37327;&#30340;&#25968;&#25454;&#28857;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#19981;&#33021;&#20551;&#35774;&#25152;&#26377;&#29992;&#25143;&#20174;&#30456;&#21516;&#30340;&#24213;&#23618;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#20363;&#22914;&#65292;&#22312;&#35821;&#35328;&#25968;&#25454;&#20013;&#65292;&#19981;&#21516;&#30340;&#35821;&#38899;&#39118;&#26684;&#23548;&#33268;&#20102;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#24322;&#36136;&#29992;&#25143;&#25968;&#25454;&#27169;&#22411;&#65292;&#20801;&#35768;&#29992;&#25143;&#25968;&#25454;&#22312;&#20998;&#24067;&#21644;&#25968;&#37327;&#19978;&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#20445;&#25345;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#30340;&#21516;&#26102;&#20272;&#35745;&#20154;&#21475;&#22343;&#20540;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#30340;&#28176;&#36817;&#26368;&#20248;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#25105;&#20204;&#24341;&#20837;&#30340;&#35774;&#32622;&#20013;&#21487;&#20197;&#36798;&#21040;&#30340;&#35823;&#24046;&#30340;&#19968;&#33324;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge in many modern data analysis tasks is that user data are heterogeneous. Different users may possess vastly different numbers of data points. More importantly, it cannot be assumed that all users sample from the same underlying distribution. This is true, for example in language data, where different speech styles result in data heterogeneity. In this work we propose a simple model of heterogeneous user data that allows user data to differ in both distribution and quantity of data, and provide a method for estimating the population-level mean while preserving user-level differential privacy. We demonstrate asymptotic optimality of our estimator and also prove general lower bounds on the error achievable in the setting we introduce.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#36317;&#31163;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#28608;&#27963;&#23618;&#33021;&#22815;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#30340;&#28382;&#21518;&#32467;&#26500;&#65292;&#20294;&#26159;&#22312;&#36830;&#32493;&#30340;&#20960;&#23618;&#20013;&#36880;&#28176;&#20007;&#22833;&#36825;&#20123;&#20449;&#24687;&#65292;&#23548;&#33268;&#39044;&#27979;&#36136;&#37327;&#21464;&#24046;&#65292;&#21516;&#26102;&#28608;&#27963;&#23618;&#20063;&#19981;&#33021;&#24456;&#22909;&#22320;&#24314;&#27169;&#31227;&#21160;&#24179;&#22343;&#21644;&#24322;&#26041;&#24046;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.15830</link><description>&lt;p&gt;
&#22522;&#20110;&#36317;&#31163;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#26469;&#21051;&#30011;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Distance Correlation-Based Approach to Characterize the Effectiveness of Recurrent Neural Networks for Time Series Forecasting. (arXiv:2307.15830v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36317;&#31163;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#28608;&#27963;&#23618;&#33021;&#22815;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#30340;&#28382;&#21518;&#32467;&#26500;&#65292;&#20294;&#26159;&#22312;&#36830;&#32493;&#30340;&#20960;&#23618;&#20013;&#36880;&#28176;&#20007;&#22833;&#36825;&#20123;&#20449;&#24687;&#65292;&#23548;&#33268;&#39044;&#27979;&#36136;&#37327;&#21464;&#24046;&#65292;&#21516;&#26102;&#28608;&#27963;&#23618;&#20063;&#19981;&#33021;&#24456;&#22909;&#22320;&#24314;&#27169;&#31227;&#21160;&#24179;&#22343;&#21644;&#24322;&#26041;&#24046;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNNs)&#20316;&#20026;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#30340;&#19968;&#31181;&#24120;&#29992;&#27169;&#22411;&#20043;&#19968;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#20851;&#20110;RNNs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#30740;&#31350;&#32467;&#26524;&#19981;&#19968;&#33268;&#65292;&#24182;&#19988;&#23545;&#20110;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#24046;&#24322;&#32570;&#20047;&#28145;&#20837;&#27934;&#23519;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36317;&#31163;&#30456;&#20851;&#24615;&#36825;&#19968;&#22810;&#21151;&#33021;&#25351;&#26631;&#26469;&#23558;&#26102;&#38388;&#24207;&#21015;&#30340;&#29305;&#24449;&#19982;RNNs&#30340;&#32452;&#25104;&#37096;&#20998;&#32852;&#31995;&#36215;&#26469;&#30340;&#26041;&#27861;&#12290;&#35813;&#25351;&#26631;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;RNN&#28608;&#27963;&#23618;&#30340;&#20449;&#24687;&#27969;&#26469;&#35299;&#37322;&#21644;&#35828;&#26126;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#65292;RNN&#30340;&#28608;&#27963;&#23618;&#33021;&#22815;&#24456;&#22909;&#22320;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#30340;&#28382;&#21518;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#22312;&#36830;&#32493;&#30340;&#20960;&#23618;&#20013;&#65292;&#23427;&#20204;&#36880;&#28176;&#20007;&#22833;&#20102;&#36825;&#20123;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#20855;&#26377;&#22823;&#28382;&#21518;&#32467;&#26500;&#30340;&#24207;&#21015;&#30340;&#39044;&#27979;&#36136;&#37327;&#21464;&#24046;&#12290;&#25105;&#20204;&#36824;&#26174;&#31034;&#65292;&#28608;&#27963;&#23618;&#19981;&#33021;&#20805;&#20998;&#24314;&#27169;&#31227;&#21160;&#24179;&#22343;&#21644;&#24322;&#26041;&#24046;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting has received a lot of attention with recurrent neural networks (RNNs) being one of the widely used models due to their ability to handle sequential data. Prior studies of RNNs for time series forecasting yield inconsistent results with limited insights as to why the performance varies for different datasets. In this paper, we provide an approach to link the characteristics of time series with the components of RNNs via the versatile metric of distance correlation. This metric allows us to examine the information flow through the RNN activation layers to be able to interpret and explain their performance. We empirically show that the RNN activation layers learn the lag structures of time series well. However, they gradually lose this information over a span of a few consecutive layers, thereby worsening the forecast quality for series with large lag structures. We also show that the activation layers cannot adequately model moving average and heteroskedastic time
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#20114;&#32852;&#32593;&#35268;&#27169;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#27867;&#21270;&#33021;&#21147;&#30340;&#25552;&#21319;&#21644;&#26032;&#20852;&#30340;&#35821;&#20041;&#25512;&#29702;&#12290;&#36890;&#36807;&#22312;&#26426;&#22120;&#20154;&#36712;&#36857;&#25968;&#25454;&#21644;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#19978;&#20849;&#21516;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#21333;&#19968;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#27169;&#22411;&#25552;&#20379;&#20102;&#21516;&#26102;&#23398;&#20064;&#26426;&#22120;&#20154;&#35266;&#27979;&#21040;&#34892;&#20026;&#26144;&#23556;&#21644;&#21033;&#29992;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#30340;&#30410;&#22788;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.15818</link><description>&lt;p&gt;
RT-2&#65306;&#35270;&#35273;-&#35821;&#35328;-&#34892;&#21160;&#27169;&#22411;&#23558;&#32593;&#32476;&#30693;&#35782;&#36716;&#21270;&#20026;&#26426;&#22120;&#20154;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control. (arXiv:2307.15818v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#20114;&#32852;&#32593;&#35268;&#27169;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#27867;&#21270;&#33021;&#21147;&#30340;&#25552;&#21319;&#21644;&#26032;&#20852;&#30340;&#35821;&#20041;&#25512;&#29702;&#12290;&#36890;&#36807;&#22312;&#26426;&#22120;&#20154;&#36712;&#36857;&#25968;&#25454;&#21644;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#19978;&#20849;&#21516;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#21333;&#19968;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#27169;&#22411;&#25552;&#20379;&#20102;&#21516;&#26102;&#23398;&#20064;&#26426;&#22120;&#20154;&#35266;&#27979;&#21040;&#34892;&#20026;&#26144;&#23556;&#21644;&#21033;&#29992;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#30340;&#30410;&#22788;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#22312;&#20114;&#32852;&#32593;&#35268;&#27169;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#24212;&#29992;&#20110;&#31471;&#21040;&#31471;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#65292;&#20197;&#25552;&#21319;&#27867;&#21270;&#33021;&#21147;&#24182;&#23454;&#29616;&#26032;&#20852;&#30340;&#35821;&#20041;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35753;&#21333;&#19968;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#27169;&#22411;&#26082;&#33021;&#23398;&#20250;&#23558;&#26426;&#22120;&#20154;&#35266;&#27979;&#26144;&#23556;&#21040;&#34892;&#20026;&#65292;&#21448;&#33021;&#20139;&#21463;&#26469;&#33258;&#32593;&#32476;&#30340;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#30340;&#30410;&#22788;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#26426;&#22120;&#20154;&#36712;&#36857;&#25968;&#25454;&#21644;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#65288;&#22914;&#35270;&#35273;&#38382;&#31572;&#65289;&#19978;&#20849;&#21516;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#36890;&#29992;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65306;&#20026;&#20102;&#20351;&#33258;&#28982;&#35821;&#35328;&#22238;&#31572;&#21644;&#26426;&#22120;&#20154;&#34892;&#20026;&#37117;&#33021;&#20197;&#30456;&#21516;&#30340;&#26684;&#24335;&#36827;&#34892;&#22788;&#29702;&#65292;&#25105;&#20204;&#23558;&#34892;&#20026;&#34920;&#31034;&#20026;&#25991;&#26412;&#26631;&#35760;&#65292;&#24182;&#23558;&#23427;&#20204;&#30452;&#25509;&#32435;&#20837;&#27169;&#22411;&#30340;&#35757;&#32451;&#38598;&#20013;&#65292;&#19982;&#33258;&#28982;&#35821;&#35328;&#26631;&#35760;&#30456;&#21516;&#12290;&#25105;&#20204;&#23558;&#36825;&#31867;&#27169;&#22411;&#31216;&#20026;&#35270;&#35273;-&#35821;&#35328;-&#34892;&#21160;&#27169;&#22411;&#65288;VLA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and inst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26825;&#33457;&#20013;&#20197;Amaranthus palmeri&#20026;&#20363;&#30340;Palmer amaranth&#26434;&#33609;&#30340;&#20843;&#20010;&#29983;&#38271;&#38454;&#27573;&#35782;&#21035;&#65292;&#23545;You Only Look Once (YOLO)&#26550;&#26500;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2307.15816</link><description>&lt;p&gt;
&#22810;&#29983;&#38271;&#38454;&#27573;&#26893;&#29289;&#35782;&#21035;&#65306;&#20197;&#26825;&#33457;&#65288;Gossypium hirsutum&#65289;&#20013;&#30340;Palmer amaranth&#65288;Amaranthus palmeri&#65289;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multi-growth stage plant recognition: a case study of Palmer amaranth (Amaranthus palmeri) in cotton (Gossypium hirsutum). (arXiv:2307.15816v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26825;&#33457;&#20013;&#20197;Amaranthus palmeri&#20026;&#20363;&#30340;Palmer amaranth&#26434;&#33609;&#30340;&#20843;&#20010;&#29983;&#38271;&#38454;&#27573;&#35782;&#21035;&#65292;&#23545;You Only Look Once (YOLO)&#26550;&#26500;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20808;&#36827;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#31934;&#20934;&#20892;&#19994;&#25216;&#26415;&#65292;&#29992;&#20110;&#26893;&#29289;&#32946;&#31181;&#12289;&#30000;&#38388;&#20316;&#29289;&#30740;&#31350;&#21644;&#29305;&#23450;&#22330;&#22320;&#20316;&#29289;&#31649;&#29702;&#65292;&#37117;&#20381;&#36182;&#20110;&#23545;&#20855;&#26377;&#39640;&#24230;&#21487;&#21464;&#24418;&#24577;&#29983;&#38271;&#38454;&#27573;&#30340;&#26893;&#29289;&#30340;&#21487;&#38752;&#26816;&#27979;&#21644;&#34920;&#22411;&#37492;&#23450;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#26893;&#29289;&#34920;&#22411;&#37492;&#23450;&#21644;&#26434;&#33609;&#35782;&#21035;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#35782;&#21035;&#29983;&#38271;&#38454;&#27573;&#30340;&#33021;&#21147;&#65292;&#36890;&#24120;&#22312;&#22806;&#35266;&#19978;&#26377;&#26126;&#26174;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#65292;&#36824;&#19981;&#30830;&#23450;&#12290;Palmer amaranth&#65288;Amaranthus palmeri&#65289;&#26159;&#26825;&#33457;&#65288;Gossypium hirsutum&#65289;&#29983;&#20135;&#20013;&#19968;&#31181;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26434;&#33609;&#26893;&#29289;&#65292;&#23427;&#22312;&#25972;&#20010;&#29983;&#38271;&#23395;&#33410;&#30340;&#19981;&#21516;&#29983;&#38271;&#38454;&#27573;&#21644;&#22312;&#21516;&#19968;&#29983;&#38271;&#38454;&#27573;&#30340;&#19981;&#21516;&#26893;&#26666;&#20013;&#37117;&#34920;&#29616;&#20986;&#39640;&#24230;&#21487;&#21464;&#30340;&#26893;&#29289;&#24418;&#24577;&#65292;&#36825;&#26159;&#30001;&#20110;&#39640;&#36951;&#20256;&#22810;&#26679;&#24615;&#24341;&#36215;&#30340;&#12290;&#26412;&#25991;&#20197;A. palmeri&#22312;&#26825;&#33457;&#20013;&#30340;&#20843;&#20010;&#29983;&#38271;&#38454;&#27573;&#35782;&#21035;&#20316;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;You Only Look Once (YOLO)&#26550;&#26500;&#27169;&#22411;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#26469;&#33258;YOLO v3&#12289;v5&#12289;v6&#12289;v6 3.0&#12289;v7&#21644;v8&#30340;26&#31181;&#19981;&#21516;&#26550;&#26500;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many advanced, image-based precision agricultural technologies for plant breeding, field crop research, and site-specific crop management hinge on the reliable detection and phenotyping of plants across highly variable morphological growth stages. Convolutional neural networks (CNNs) have shown promise for image-based plant phenotyping and weed recognition, but their ability to recognize growth stages, often with stark differences in appearance, is uncertain. Amaranthus palmeri (Palmer amaranth) is a particularly challenging weed plant in cotton (Gossypium hirsutum) production, exhibiting highly variable plant morphology both across growth stages over a growing season, as well as between plants at a given growth stage due to high genetic diversity. In this paper, we investigate eight-class growth stage recognition of A. palmeri in cotton as a challenging model for You Only Look Once (YOLO) architectures. We compare 26 different architecture variants from YOLO v3, v5, v6, v6 3.0, v7, an
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#22312;&#24037;&#19994;&#26426;&#26800;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#33258;&#21160;&#21270;&#26816;&#27979;&#24037;&#19994;&#26426;&#26800;&#24322;&#24120;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15807</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#24037;&#19994;&#26426;&#26800;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65306;&#19968;&#31181;&#31995;&#32479;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Anomaly Detection in Industrial Machinery using IoT Devices and Machine Learning: a Systematic Mapping. (arXiv:2307.15807v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15807
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#22312;&#24037;&#19994;&#26426;&#26800;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#33258;&#21160;&#21270;&#26816;&#27979;&#24037;&#19994;&#26426;&#26800;&#24322;&#24120;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#24037;&#19994;&#20013;&#65292;&#24322;&#24120;&#26816;&#27979;&#23545;&#20110;&#39044;&#38450;&#35774;&#22791;&#25925;&#38556;&#12289;&#20943;&#23569;&#20572;&#26426;&#26102;&#38388;&#21644;&#25552;&#39640;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#20351;&#24471;&#20174;&#24037;&#19994;&#26426;&#26800;&#20013;&#25910;&#38598;&#22823;&#37327;&#25968;&#25454;&#25104;&#20026;&#21487;&#33021;&#65292;&#20026;&#24322;&#24120;&#26816;&#27979;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#20449;&#24687;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#29289;&#32852;&#32593;&#29983;&#24577;&#31995;&#32479;&#29983;&#25104;&#30340;&#25968;&#25454;&#37327;&#21644;&#22797;&#26434;&#24615;&#20351;&#24471;&#20154;&#24037;&#25163;&#21160;&#26816;&#27979;&#24322;&#24120;&#21464;&#24471;&#22256;&#38590;&#12290;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#33258;&#21160;&#21270;&#24037;&#19994;&#26426;&#26800;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#27492;&#22806;&#65292;&#27599;&#31181;&#25216;&#26415;&#22522;&#20110;&#25968;&#25454;&#30340;&#24615;&#36136;&#21644;&#30456;&#24212;&#30340;&#31995;&#32479;&#37117;&#26377;&#29305;&#23450;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35299;&#20915;&#32593;&#32476;&#21644;&#32593;&#32476;&#23433;&#20840;&#30456;&#20851;&#38382;&#39064;&#65292;&#23545;&#20110;&#24037;&#19994;&#37096;&#38376;&#30340;&#20851;&#27880;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#30740;&#31350;&#36824;&#26410;&#28085;&#30422;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#24037;&#19994;&#26426;&#26800;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#25152;&#28041;&#21450;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection is critical in the smart industry for preventing equipment failure, reducing downtime, and improving safety. Internet of Things (IoT) has enabled the collection of large volumes of data from industrial machinery, providing a rich source of information for Anomaly Detection. However, the volume and complexity of data generated by the Internet of Things ecosystems make it difficult for humans to detect anomalies manually. Machine learning (ML) algorithms can automate anomaly detection in industrial machinery by analyzing generated data. Besides, each technique has specific strengths and weaknesses based on the data nature and its corresponding systems. However, the current systematic mapping studies on Anomaly Detection primarily focus on addressing network and cybersecurity-related problems, with limited attention given to the industrial sector. Additionally, these studies do not cover the challenges involved in using ML for Anomaly Detection in industrial machinery wi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36229;&#36234;&#39640;&#26031;&#25968;&#25454;&#30340;&#21333;&#25351;&#25968;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#23545;&#31283;&#23450;&#24615;&#21644;&#23545;&#31216;&#24615;&#30340;&#36829;&#21453;&#24773;&#20917;&#19979;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.15804</link><description>&lt;p&gt;
&#20851;&#20110;&#39640;&#26031;&#25968;&#25454;&#20043;&#22806;&#30340;&#21333;&#25351;&#25968;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
On Single Index Models beyond Gaussian Data. (arXiv:2307.15804v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15804
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36229;&#36234;&#39640;&#26031;&#25968;&#25454;&#30340;&#21333;&#25351;&#25968;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#23545;&#31283;&#23450;&#24615;&#21644;&#23545;&#31216;&#24615;&#30340;&#36829;&#21453;&#24773;&#20917;&#19979;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#39640;&#32500;&#20989;&#25968;&#24050;&#25104;&#20026;&#30740;&#31350;&#20351;&#29992;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#34892;&#20026;&#30340;&#20016;&#23500;&#26694;&#26550;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#32447;&#24615;&#27169;&#22411;&#20043;&#22806;&#36827;&#34892;&#29305;&#24449;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#20854;&#20013;&#26368;&#31616;&#21333;&#30340;&#26159;&#21333;&#25351;&#25968;&#27169;&#22411; $f(x) = \phi( x \cdot \theta^*)$&#65292;&#20854;&#20013;&#26631;&#31614;&#30001;&#19968;&#20010;&#26410;&#30693;&#30340;&#19968;&#32500;&#25237;&#24433; $\theta^*$ &#24212;&#29992;&#20110;&#20219;&#24847;&#38750;&#32447;&#24615;&#26631;&#37327;&#36830;&#25509;&#20989;&#25968; $\phi$ &#20135;&#29983;&#12290;&#36890;&#36807;&#19987;&#27880;&#20110;&#39640;&#26031;&#25968;&#25454;&#65292;&#26368;&#36817;&#20960;&#39033;&#30740;&#31350;&#24037;&#20316;&#24314;&#31435;&#20102;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#22270;&#26223;&#65292;&#23558;&#20449;&#24687;&#25351;&#25968;&#65288;&#19982;&#36830;&#25509;&#20989;&#25968;&#30340;&#27491;&#21017;&#24615;&#30456;&#20851;&#65289;&#19982;&#25152;&#38656;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#36827;&#34892;&#20102;&#25511;&#21046;&#12290;&#23454;&#36136;&#19978;&#65292;&#36825;&#20123;&#24037;&#20855;&#21033;&#29992;&#20102;&#39640;&#26031;&#20998;&#24067;&#30340;&#31283;&#23450;&#24615;&#21644;&#29699;&#23545;&#31216;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174; \cite{arous2020online} &#30340;&#26694;&#26550;&#20986;&#21457;&#65292;&#25506;&#32034;&#20102;&#36229;&#36234;&#39640;&#26031;&#35774;&#23450;&#30340;&#36825;&#20010;&#22270;&#26223;&#30340;&#25193;&#23637;&#65292;&#20854;&#20013;&#31283;&#23450;&#24615;&#25110;&#23545;&#31216;&#24615;&#21487;&#33021;&#34987;&#36829;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse high-dimensional functions have arisen as a rich framework to study the behavior of gradient-descent methods using shallow neural networks, showcasing their ability to perform feature learning beyond linear models. Amongst those functions, the simplest are single-index models $f(x) = \phi( x \cdot \theta^*)$, where the labels are generated by an arbitrary non-linear scalar link function $\phi$ applied to an unknown one-dimensional projection $\theta^*$ of the input data. By focusing on Gaussian data, several recent works have built a remarkable picture, where the so-called information exponent (related to the regularity of the link function) controls the required sample complexity. In essence, these tools exploit the stability and spherical symmetry of Gaussian distributions. In this work, building from the framework of \cite{arous2020online}, we explore extensions of this picture beyond the Gaussian setting, where both stability or symmetry might be violated. Focusing on the pl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26174;&#33879;&#24615;&#22270;&#30340;&#26041;&#27861;&#65292;&#26469;&#29983;&#25104;&#26356;&#20855;&#20449;&#24687;&#24615;&#30340;CF&#35299;&#37322;&#65292;&#29992;&#20110;&#35299;&#37322;DNN&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.15786</link><description>&lt;p&gt;
SAFE: &#22522;&#20110;&#26174;&#33879;&#24615;&#30340;DNN&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#23545;&#25239;&#24615;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
SAFE: Saliency-Aware Counterfactual Explanations for DNN-based Automated Driving Systems. (arXiv:2307.15786v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26174;&#33879;&#24615;&#22270;&#30340;&#26041;&#27861;&#65292;&#26469;&#29983;&#25104;&#26356;&#20855;&#20449;&#24687;&#24615;&#30340;CF&#35299;&#37322;&#65292;&#29992;&#20110;&#35299;&#37322;DNN&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CF&#35299;&#37322;&#22120;&#35782;&#21035;&#20986;&#22312;&#36755;&#20837;&#20013;&#26368;&#23569;&#30340;&#20462;&#25913;&#23558;&#27169;&#22411;&#30340;&#36755;&#20986;&#25913;&#21464;&#20026;&#20854;&#34917;&#38598;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;CF&#35299;&#37322;&#22120;&#35745;&#31639;&#20986;&#36234;&#36807;&#27169;&#22411;&#20915;&#31574;&#36793;&#30028;&#25152;&#38656;&#30340;&#26368;&#23567;&#20462;&#25913;&#12290;&#30446;&#21069;&#30340;&#28145;&#24230;&#29983;&#25104;CF&#27169;&#22411;&#36890;&#24120;&#19982;&#29992;&#25143;&#36873;&#25321;&#30340;&#29305;&#24449;&#19968;&#36215;&#24037;&#20316;&#65292;&#32780;&#19981;&#26159;&#20851;&#27880;&#40657;&#30418;&#27169;&#22411;&#30340;&#21028;&#21035;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#36825;&#26679;&#30340;CF&#31034;&#20363;&#21487;&#33021;&#19981;&#19968;&#23450;&#20301;&#20110;&#20915;&#31574;&#36793;&#30028;&#38468;&#36817;&#65292;&#20174;&#32780;&#36829;&#32972;&#20102;CF&#30340;&#23450;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26174;&#33879;&#22270;&#29983;&#25104;&#26356;&#20855;&#20449;&#24687;&#24615;&#30340;CF&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
A CF explainer identifies the minimum modifications in the input that would alter the model's output to its complement. In other words, a CF explainer computes the minimum modifications required to cross the model's decision boundary. Current deep generative CF models often work with user-selected features rather than focusing on the discriminative features of the black-box model. Consequently, such CF examples may not necessarily lie near the decision boundary, thereby contradicting the definition of CFs. To address this issue, we propose in this paper a novel approach that leverages saliency maps to generate more informative CF explanations. Source codes are available at: https://github.com/Amir-Samadi//Saliency_Aware_CF.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#22270;&#23884;&#20837;&#20013;&#24212;&#29992;&#20449;&#24687;&#20960;&#20309;&#26469;&#25551;&#36848;Ising&#27169;&#22411;&#30340;&#22522;&#24577;&#65292;&#36890;&#36807;&#21033;&#29992;&#29699;&#38754;&#21644;&#21452;&#26354;&#38754;&#25299;&#25169;&#19978;&#30340;&#32534;&#30721;&#65292;&#24314;&#31435;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#32416;&#38169;&#32534;&#30721;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#32416;&#38169;&#30721;&#21644;&#21457;&#23637;&#23884;&#20837;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32534;&#30721;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.15778</link><description>&lt;p&gt;
&#22312;&#22270;&#23884;&#20837;&#20013;&#22522;&#20110;&#29699;&#38754;&#21644;&#21452;&#26354;&#38754;&#25299;&#25169;&#30340;&#32534;&#30721;&#24212;&#29992;&#20110;Ising MRF&#27169;&#22411;&#65306;&#32463;&#20856;&#21644;&#37327;&#23376;&#25299;&#25169;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spherical and Hyperbolic Toric Topology-Based Codes On Graph Embedding for Ising MRF Models: Classical and Quantum Topology Machine Learning. (arXiv:2307.15778v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#22270;&#23884;&#20837;&#20013;&#24212;&#29992;&#20449;&#24687;&#20960;&#20309;&#26469;&#25551;&#36848;Ising&#27169;&#22411;&#30340;&#22522;&#24577;&#65292;&#36890;&#36807;&#21033;&#29992;&#29699;&#38754;&#21644;&#21452;&#26354;&#38754;&#25299;&#25169;&#19978;&#30340;&#32534;&#30721;&#65292;&#24314;&#31435;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#32416;&#38169;&#32534;&#30721;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#32416;&#38169;&#30721;&#21644;&#21457;&#23637;&#23884;&#20837;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32534;&#30721;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#20449;&#24687;&#20960;&#20309;&#24212;&#29992;&#20110;&#25551;&#36848;Ising&#27169;&#22411;&#30340;&#22522;&#24577;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#25176;&#37324;&#20811;&#21644;&#29699;&#38754;&#25299;&#25169;&#19978;&#30340;&#24490;&#29615;&#21644;&#20934;&#24490;&#29615;&#30721;&#30340;&#22855;&#20598;&#26816;&#39564;&#30697;&#38453;&#26469;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#24314;&#31435;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#32416;&#38169;&#32534;&#30721;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#21516;&#26500;&#21644;&#20934;&#24490;&#29615;&#30721;&#24490;&#29615;&#30697;&#38453;&#30340;&#23610;&#23544;&#26041;&#38754;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#22522;&#20110;&#25429;&#33719;&#38598;&#30340;&#23884;&#20837;&#26041;&#27861;&#30340;&#21457;&#23637;&#20855;&#26377;&#24433;&#21709;&#12290;&#21033;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#21644;&#25968;&#23383;&#20960;&#20309;&#23398;&#26469;&#20248;&#21270;&#32416;&#38169;&#30721;&#65292;&#20174;&#32780;&#23548;&#33268;&#36825;&#20123;&#23884;&#20837;&#21644;&#31232;&#30095;&#22240;&#23376;&#21270;&#26041;&#27861;&#30340;&#20986;&#29616;&#12290;&#26412;&#25991;&#36890;&#36807;&#28436;&#31034;&#38271;&#36317;&#31163;&#39046;&#22495;&#30340;&#26368;&#26032;DNN&#26550;&#26500;&#65288;ChordMixer&#65292;Mega&#65292;Mega-chunk&#65292;CDIL&#65292;...&#65289;&#19982;&#29305;&#23450;&#31867;&#22411;&#65288;Cage-graph&#65292;Repeat Accumulate&#65289;&#30340;&#21306;&#22359;&#21644;&#21367;&#31215;LDPC&#30721;&#31561;&#20215;&#30340;&#26041;&#24335;&#65292;&#24314;&#31435;&#20102;DNN&#26550;&#26500;&#21644;&#32416;&#38169;&#32534;&#30721;&#20043;&#38388;&#30340;&#30452;&#25509;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper introduces the application of information geometry to describe the ground states of Ising models. This is achieved by utilizing parity-check matrices of cyclic and quasi-cyclic codes on toric and spherical topologies. The approach establishes a connection between machine learning and error-correcting coding, specifically in terms of automorphism and the size of the circulant of the quasi-cyclic code. This proposed approach has implications for the development of new embedding methods based on trapping sets. Statistical physics and number geometry are utilized to optimize error-correcting codes, leading to these embedding and sparse factorization methods. The paper establishes a direct connection between DNN architecture and error-correcting coding by demonstrating how state-of-the-art DNN architectures (ChordMixer, Mega, Mega-chunk, CDIL, ...) from the long-range arena can be equivalent to specific types (Cage-graph, Repeat Accumulate) of block and convolutional LDPC codes. Q
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26368;&#20248;&#27969;&#24418;&#27010;&#24565;&#23558;&#26367;&#20195;&#27169;&#22411;&#21644;&#37325;&#35201;&#24615;&#37319;&#26679;&#26041;&#27861;&#32852;&#31995;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#39640;&#32500;SRAM&#35780;&#20272;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21517;&#20026;OPTIMIS&#65292;&#32467;&#21512;&#20102;&#31070;&#32463;&#32806;&#21512;&#27969;&#21644;&#27915;&#33905;&#37319;&#26679;&#65292;&#22312;&#20445;&#25345;&#24615;&#33021;&#20248;&#21183;&#30340;&#21516;&#26102;&#20855;&#22791;&#40065;&#26834;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15773</link><description>&lt;p&gt;
&#23547;&#27714;&#25910;&#30410;&#22721;&#22418;&#65306;&#36890;&#36807;&#26368;&#20248;&#27969;&#24418;&#36827;&#34892;&#39640;&#32500;SRAM&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Seeking the Yield Barrier: High-Dimensional SRAM Evaluation Through Optimal Manifold. (arXiv:2307.15773v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26368;&#20248;&#27969;&#24418;&#27010;&#24565;&#23558;&#26367;&#20195;&#27169;&#22411;&#21644;&#37325;&#35201;&#24615;&#37319;&#26679;&#26041;&#27861;&#32852;&#31995;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#39640;&#32500;SRAM&#35780;&#20272;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21517;&#20026;OPTIMIS&#65292;&#32467;&#21512;&#20102;&#31070;&#32463;&#32806;&#21512;&#27969;&#21644;&#27915;&#33905;&#37319;&#26679;&#65292;&#22312;&#20445;&#25345;&#24615;&#33021;&#20248;&#21183;&#30340;&#21516;&#26102;&#20855;&#22791;&#40065;&#26834;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#22411;&#30005;&#36335;&#23558;&#35268;&#27169;&#32553;&#23567;&#21040;&#20122;&#24494;&#31859;&#32423;&#21035;&#30340;&#20808;&#36827;&#25216;&#26415;&#33410;&#28857;&#65292;&#26377;&#25928;&#33719;&#24471;SRAM&#32452;&#20214;&#25925;&#38556;&#27010;&#29575;&#30340;&#20934;&#30830;&#20272;&#35745;&#24050;&#25104;&#20026;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#32463;&#20856;&#30340;&#33539;&#25968;&#26368;&#23567;&#21270;&#26041;&#27861;&#65292;&#24182;&#25512;&#24191;&#20102;&#23427;&#20197;&#36866;&#29992;&#20110;&#26080;&#38480;&#32452;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#26368;&#20248;&#27969;&#24418;&#27010;&#24565;&#65292;&#23558;&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#21644;&#37325;&#35201;&#24615;&#37319;&#26679;&#65288;IS&#65289;&#30340;&#20135;&#37327;&#20272;&#35745;&#26041;&#27861;&#32852;&#31995;&#36215;&#26469;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#27425;&#20248;&#27969;&#24418;&#65292;&#26368;&#20248;&#36229;&#29699;&#20307;&#65292;&#23427;&#24341;&#23548;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#33021;&#22815;&#35782;&#21035;&#21040;&#25925;&#38556;&#36793;&#30028;&#65292;&#31216;&#20026;&#27915;&#33905;&#37319;&#26679;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#32806;&#21512;&#27969;&#20316;&#20026;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#25552;&#35758;&#20998;&#24067;&#65292;&#35813;&#20998;&#24067;&#31867;&#20284;&#20110;&#26367;&#20195;&#27169;&#22411;&#20174;&#26679;&#26412;&#20013;&#23398;&#20064;&#12290;&#36825;&#20123;&#32452;&#21512;&#20135;&#29983;&#20102;&#19968;&#31181;&#21517;&#20026;"&#20248;&#21270;&#27969;&#24418;&#37325;&#35201;&#24615;&#37319;&#26679;"&#65288;OPTIMIS&#65289;&#30340;&#26032;&#22411;&#20135;&#37327;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#20445;&#25345;&#20102;&#26367;&#20195;&#27169;&#22411;&#21644;&#37325;&#35201;&#24615;&#37319;&#26679;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#20855;&#26377;&#24378;&#38887;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being able to efficiently obtain an accurate estimate of the failure probability of SRAM components has become a central issue as model circuits shrink their scale to submicrometer with advanced technology nodes. In this work, we revisit the classic norm minimization method. We then generalize it with infinite components and derive the novel optimal manifold concept, which bridges the surrogate-based and importance sampling (IS) yield estimation methods. We then derive a sub-optimal manifold, optimal hypersphere, which leads to an efficient sampling method being aware of the failure boundary called onion sampling. Finally, we use a neural coupling flow (which learns from samples like a surrogate model) as the IS proposal distribution. These combinations give rise to a novel yield estimation method, named Optimal Manifold Important Sampling (OPTIMIS), which keeps the advantages of the surrogate and IS methods to deliver state-of-the-art performance with robustness and consistency, with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#30028;&#22495;&#19978;&#36890;&#36807;&#21333;&#38544;&#34255;&#23618;ReLU&#32593;&#32476;&#36924;&#36817;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#20171;&#32461;&#20102;&#26032;&#30340;&#27169;&#22411;&#31867;&#23450;&#20041;&#21152;&#26435;&#21464;&#24046;&#31354;&#38388;&#65292;&#35813;&#23450;&#20041;&#19982;&#22495;&#26412;&#36523;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2307.15772</link><description>&lt;p&gt;
&#21152;&#26435;&#21464;&#24046;&#31354;&#38388;&#19982;&#27973;&#23618;ReLU&#32593;&#32476;&#30340;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Weighted variation spaces and approximation by shallow ReLU networks. (arXiv:2307.15772v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#30028;&#22495;&#19978;&#36890;&#36807;&#21333;&#38544;&#34255;&#23618;ReLU&#32593;&#32476;&#36924;&#36817;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#20171;&#32461;&#20102;&#26032;&#30340;&#27169;&#22411;&#31867;&#23450;&#20041;&#21152;&#26435;&#21464;&#24046;&#31354;&#38388;&#65292;&#35813;&#23450;&#20041;&#19982;&#22495;&#26412;&#36523;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#30028;&#22495;&#937;&#8834;Rd&#19978;&#65292;&#36890;&#36807;&#23485;&#24230;&#20026;n&#30340;&#21333;&#38544;&#34255;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#26469;&#36924;&#36817;&#20989;&#25968;f&#30340;&#24773;&#20917;&#12290;&#36825;&#31181;&#38750;&#32447;&#24615;&#30340;n&#39033;&#23383;&#20856;&#36924;&#36817;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#22240;&#20026;&#23427;&#26159;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;(NNA)&#30340;&#26368;&#31616;&#21333;&#24773;&#20917;&#12290;&#23545;&#20110;&#36825;&#31181;NNA&#24418;&#24335;&#65292;&#26377;&#20960;&#20010;&#33879;&#21517;&#30340;&#36924;&#36817;&#32467;&#26524;&#65292;&#24341;&#20837;&#20102;&#22312;&#937;&#19978;&#30340;&#20989;&#25968;&#30340;&#26032;&#22411;&#27169;&#22411;&#31867;&#65292;&#20854;&#36924;&#36817;&#36895;&#29575;&#36991;&#20813;&#20102;&#32500;&#25968;&#28798;&#38590;&#12290;&#36825;&#20123;&#26032;&#22411;&#27169;&#22411;&#31867;&#21253;&#25324;Barron&#31867;&#21644;&#22522;&#20110;&#31232;&#30095;&#24615;&#25110;&#21464;&#24046;&#30340;&#31867;&#65292;&#20363;&#22914;Radon&#22495;BV&#31867;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#22312;&#22495;&#937;&#19978;&#23450;&#20041;&#36825;&#20123;&#26032;&#22411;&#27169;&#22411;&#31867;&#12290;&#24403;&#21069;&#36825;&#20123;&#27169;&#22411;&#31867;&#30340;&#23450;&#20041;&#19981;&#20381;&#36182;&#20110;&#22495;&#937;&#12290;&#36890;&#36807;&#24341;&#20837;&#21152;&#26435;&#21464;&#24046;&#31354;&#38388;&#30340;&#27010;&#24565;&#65292;&#32473;&#20986;&#20102;&#20851;&#20110;&#22495;&#30340;&#26356;&#24688;&#24403;&#30340;&#27169;&#22411;&#31867;&#23450;&#20041;&#12290;&#36825;&#20123;&#26032;&#22411;&#27169;&#22411;&#31867;&#19982;&#22495;&#26412;&#36523;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the approximation of functions $f$ on a bounded domain $\Omega\subset \mathbb{R}^d$ by the outputs of single-hidden-layer ReLU neural networks of width $n$. This form of nonlinear $n$-term dictionary approximation has been intensely studied since it is the simplest case of neural network approximation (NNA). There are several celebrated approximation results for this form of NNA that introduce novel model classes of functions on $\Omega$ whose approximation rates avoid the curse of dimensionality. These novel classes include Barron classes, and classes based on sparsity or variation such as the Radon-domain BV classes.  The present paper is concerned with the definition of these novel model classes on domains $\Omega$. The current definition of these model classes does not depend on the domain $\Omega$. A new and more proper definition of model classes on domains is given by introducing the concept of weighted variation spaces. These new model classes are intrinsic to th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22240;&#26524;&#20998;&#26512;&#25506;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#30340;&#20869;&#37096;&#32467;&#26500;&#65292;&#21457;&#29616;&#20102;Hydra&#25928;&#24212;&#21644;&#26202;&#26399;MLP&#23618;&#30340;&#24179;&#34913;&#21151;&#33021;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.15771</link><description>&lt;p&gt;
Hydra&#25928;&#24212;&#65306;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#20013;&#30340;&#33258;&#36866;&#24212;&#33258;&#20462;&#22797;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
The Hydra Effect: Emergent Self-repair in Language Model Computations. (arXiv:2307.15771v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22240;&#26524;&#20998;&#26512;&#25506;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#30340;&#20869;&#37096;&#32467;&#26500;&#65292;&#21457;&#29616;&#20102;Hydra&#25928;&#24212;&#21644;&#26202;&#26399;MLP&#23618;&#30340;&#24179;&#34913;&#21151;&#33021;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22240;&#26524;&#20998;&#26512;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#30340;&#20869;&#37096;&#32467;&#26500;&#65292;&#24182;&#23637;&#31034;&#20102;&#20004;&#31181;&#27169;&#24335;&#65306;&#65288;1&#65289;&#19968;&#31181;&#33258;&#36866;&#24212;&#35745;&#31639;&#24418;&#24335;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26576;&#19968;&#33258;&#27880;&#24847;&#23618;&#34987;&#21024;&#20943;&#21518;&#21478;&#19968;&#23618;&#36827;&#34892;&#34917;&#20607;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;Hydra&#25928;&#24212;&#65289;&#65307;&#65288;2&#65289;&#22312;&#21518;&#26399;&#22810;&#23618;&#24863;&#30693;&#26426;&#23618;&#20013;&#23384;&#22312;&#30340;&#24179;&#34913;&#21151;&#33021;&#65292;&#29992;&#20110;&#35843;&#33410;&#26368;&#22823;&#20284;&#28982;&#20196;&#29260;&#12290;&#25105;&#20204;&#30340;&#21024;&#20943;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#23618;&#20043;&#38388;&#36890;&#24120;&#30456;&#23545;&#26494;&#25955;&#32806;&#21512;&#65288;&#23545;&#19968;&#23618;&#30340;&#21024;&#20943;&#21482;&#20250;&#24433;&#21709;&#19968;&#23567;&#37096;&#20998;&#19979;&#28216;&#23618;&#65289;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#20219;&#20309;&#24418;&#24335;&#30340;&#38543;&#26426;&#22833;&#27963;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#36825;&#20123;&#25928;&#24212;&#20173;&#28982;&#23384;&#22312;&#12290;&#25105;&#20204;&#22312;&#20107;&#23454;&#22238;&#24518;&#30340;&#32972;&#26223;&#19979;&#20998;&#26512;&#20102;&#36825;&#20123;&#25928;&#24212;&#65292;&#24182;&#32771;&#34385;&#20102;&#23427;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#30005;&#36335;&#23618;&#38754;&#24402;&#22240;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the internal structure of language model computations using causal analysis and demonstrate two motifs: (1) a form of adaptive computation where ablations of one attention layer of a language model cause another layer to compensate (which we term the Hydra effect) and (2) a counterbalancing function of late MLP layers that act to downregulate the maximum-likelihood token. Our ablation studies demonstrate that language model layers are typically relatively loosely coupled (ablations to one layer only affect a small number of downstream layers). Surprisingly, these effects occur even in language models trained without any form of dropout. We analyse these effects in the context of factual recall and consider their implications for circuit-level attribution in language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LDA&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#31616;&#21382;&#20013;&#30340;&#23454;&#20307;&#24182;&#23558;&#20854;&#29992;&#20110;&#35780;&#20998;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#30340;&#20505;&#36873;&#20154;&#36873;&#25321;&#12290;&#35813;&#26041;&#27861;&#22312;&#32771;&#34385;&#25152;&#26377;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;82%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.15752</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23545;&#31616;&#21382;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#20505;&#36873;&#20154;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Resume Evaluation through Latent Dirichlet Allocation and Natural Language Processing for Effective Candidate Selection. (arXiv:2307.15752v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LDA&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#31616;&#21382;&#20013;&#30340;&#23454;&#20307;&#24182;&#23558;&#20854;&#29992;&#20110;&#35780;&#20998;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#30340;&#20505;&#36873;&#20154;&#36873;&#25321;&#12290;&#35813;&#26041;&#27861;&#22312;&#32771;&#34385;&#25152;&#26377;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;82%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;LDA&#65289;&#21644;&#20351;&#29992;SpaCy&#36827;&#34892;&#23454;&#20307;&#26816;&#27979;&#30340;&#31616;&#21382;&#35780;&#20998;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#39318;&#20808;&#20351;&#29992;SpaCy&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20174;&#31616;&#21382;&#20013;&#25552;&#21462;&#30456;&#20851;&#23454;&#20307;&#65292;&#22914;&#25945;&#32946;&#32972;&#26223;&#12289;&#24037;&#20316;&#32463;&#21382;&#21644;&#25216;&#33021;&#12290;&#28982;&#21518;&#65292;LDA&#27169;&#22411;&#20351;&#29992;&#36825;&#20123;&#23454;&#20307;&#20026;&#31616;&#21382;&#35780;&#20998;&#65292;&#20026;&#27599;&#20010;&#23454;&#20307;&#20998;&#37197;&#20027;&#39064;&#27010;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;SpaCy&#30340;NER&#36827;&#34892;&#23454;&#20307;&#26816;&#27979;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#25253;&#21578;&#20854;&#35780;&#20272;&#25351;&#26631;&#12290;&#20351;&#29992;LDA&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31995;&#32479;&#23558;&#31616;&#21382;&#20998;&#35299;&#20026;&#28508;&#22312;&#20027;&#39064;&#65292;&#24182;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;77%&#30340;&#20934;&#30830;&#29575;&#65292;&#21482;&#32771;&#34385;&#25216;&#33021;&#65292;&#22312;&#32771;&#34385;&#25152;&#26377;&#23646;&#24615;&#65288;&#22914;&#23398;&#38498;&#21517;&#31216;&#12289;&#24037;&#20316;&#32463;&#21382;&#12289;&#23398;&#20301;&#21644;&#25216;&#33021;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#24635;&#20307;&#20934;&#30830;&#29575;&#36798;&#21040;82%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a method for resume rating using Latent Dirichlet Allocation (LDA) and entity detection with SpaCy. The proposed method first extracts relevant entities such as education, experience, and skills from the resume using SpaCy's Named Entity Recognition (NER). The LDA model then uses these entities to rate the resume by assigning topic probabilities to each entity. Furthermore, we conduct a detailed analysis of the entity detection using SpaCy's NER and report its evaluation metrics. Using LDA, our proposed system breaks down resumes into latent topics and extracts meaningful semantic representations. With a vision to define our resume score to be more content-driven rather than a structure and keyword match driven, our model has achieved 77% accuracy with respect to only skills in consideration and an overall 82% accuracy with all attributes in consideration. (like college name, work experience, degree and skills)
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#26041;&#27861;&#23545;&#20110;&#25439;&#22833;&#20989;&#25968;&#20960;&#20309;&#24418;&#29366;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#35752;&#35770;&#20102;&#19968;&#31181;&#22522;&#26412;&#30340;&#20960;&#20309;&#29305;&#24615;Morse&#20989;&#25968;&#65292;&#24182;&#25506;&#35752;&#20102;&#20960;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#20351;&#24471;&#27491;&#21017;&#21270;&#21518;&#30340;&#20989;&#25968;&#25104;&#20026;Morse&#20989;&#25968;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2307.15744</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#23545;&#25439;&#22833;&#20989;&#25968;&#30340;&#20960;&#20309;&#24418;&#29366;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
How regularization affects the geometry of loss functions. (arXiv:2307.15744v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15744
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#26041;&#27861;&#23545;&#20110;&#25439;&#22833;&#20989;&#25968;&#20960;&#20309;&#24418;&#29366;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#35752;&#35770;&#20102;&#19968;&#31181;&#22522;&#26412;&#30340;&#20960;&#20309;&#29305;&#24615;Morse&#20989;&#25968;&#65292;&#24182;&#25506;&#35752;&#20102;&#20960;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#20351;&#24471;&#27491;&#21017;&#21270;&#21518;&#30340;&#20989;&#25968;&#25104;&#20026;Morse&#20989;&#25968;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25152;&#23398;&#20064;&#30340;&#20869;&#23481;&#22522;&#26412;&#19978;&#21462;&#20915;&#20110;&#24213;&#23618;&#25439;&#22833;&#20989;&#25968;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#23545;&#25439;&#22833;&#20989;&#25968;&#30340;&#20960;&#20309;&#24418;&#29366;&#30340;&#24433;&#21709;&#12290;&#23545;&#20110;&#20809;&#28369;&#20989;&#25968;&#26469;&#35828;&#65292;&#20854;&#20013;&#19968;&#20010;&#26368;&#22522;&#26412;&#30340;&#20960;&#20309;&#29305;&#24615;&#26159;&#23427;&#26159;&#21542;&#20026;Morse&#20989;&#25968;&#12290;&#23545;&#20110;&#38750;&#32447;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#35828;&#65292;&#26410;&#32463;&#27491;&#21017;&#21270;&#30340;&#25439;&#22833;&#20989;&#25968;L&#36890;&#24120;&#19981;&#26159;Morse&#20989;&#25968;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#26435;&#37325;&#34928;&#20943;&#65292;&#24182;&#30740;&#31350;&#20102;&#21738;&#20123;&#27491;&#21017;&#21270;&#26041;&#27861;&#20351;&#24471;&#32463;&#36807;&#27491;&#21017;&#21270;&#21518;&#30340;&#20989;&#25968;L_&#949;&#25104;&#20026;Morse&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
What neural networks learn depends fundamentally on the geometry of the underlying loss function. We study how different regularizers affect the geometry of this function. One of the most basic geometric properties of a smooth function is whether it is Morse or not. For nonlinear deep neural networks, the unregularized loss function $L$ is typically not Morse. We consider several different regularizers, including weight decay, and study for which regularizers the regularized function $L_\epsilon$ becomes Morse.
&lt;/p&gt;</description></item><item><title>AI&#29992;&#20110;&#39044;&#27979;&#34892;&#21160;&#20174;&#27668;&#20505;&#39044;&#27979;&#21521;&#39044;&#27979;&#34892;&#21160;&#36716;&#21464;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#22312;&#35780;&#20272;&#27668;&#20505;&#23545;&#29305;&#23450;&#20154;&#21475;&#30340;&#24433;&#21709;&#26041;&#38754;&#22635;&#34917;&#20102;&#26041;&#27861;&#35770;&#19978;&#30340;&#24046;&#36317;&#65292;&#20197;&#25512;&#36827;&#23545;&#27668;&#20505;&#21464;&#21270;&#26368;&#33030;&#24369;&#20154;&#21475;&#30340;&#28798;&#23475;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2307.15727</link><description>&lt;p&gt;
AI&#29992;&#20110;&#39044;&#27979;&#34892;&#21160;&#65306;&#36229;&#36234;&#27668;&#20505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
AI for Anticipatory Action: Moving Beyond Climate Forecasting. (arXiv:2307.15727v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15727
&lt;/p&gt;
&lt;p&gt;
AI&#29992;&#20110;&#39044;&#27979;&#34892;&#21160;&#20174;&#27668;&#20505;&#39044;&#27979;&#21521;&#39044;&#27979;&#34892;&#21160;&#36716;&#21464;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#22312;&#35780;&#20272;&#27668;&#20505;&#23545;&#29305;&#23450;&#20154;&#21475;&#30340;&#24433;&#21709;&#26041;&#38754;&#22635;&#34917;&#20102;&#26041;&#27861;&#35770;&#19978;&#30340;&#24046;&#36317;&#65292;&#20197;&#25512;&#36827;&#23545;&#27668;&#20505;&#21464;&#21270;&#26368;&#33030;&#24369;&#20154;&#21475;&#30340;&#28798;&#23475;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28798;&#23475;&#24212;&#23545;&#26426;&#26500;&#27491;&#20174;&#27668;&#20505;&#39044;&#27979;&#33539;&#24335;&#36716;&#21521;&#39044;&#27979;&#34892;&#21160;&#33539;&#24335;&#65306;&#19981;&#20165;&#35780;&#20272;&#27668;&#20505;&#23558;&#22914;&#20309;&#65292;&#32780;&#19988;&#35780;&#20272;&#27668;&#20505;&#23545;&#29305;&#23450;&#20154;&#21475;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#20027;&#21160;&#21709;&#24212;&#21644;&#36164;&#28304;&#20998;&#37197;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#27668;&#20505;&#39044;&#27979;&#26041;&#38754;&#21464;&#24471;&#24322;&#24120;&#24378;&#22823;&#65292;&#20294;&#22312;&#20419;&#36827;&#39044;&#27979;&#34892;&#21160;&#26041;&#38754;&#20173;&#23384;&#22312;&#26041;&#27861;&#35770;&#19978;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#39044;&#27979;&#34892;&#21160;&#30340;&#27010;&#36848;&#65292;&#22238;&#39038;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#30456;&#20851;&#24212;&#29992;&#65292;&#35782;&#21035;&#20102;&#20849;&#21516;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#31361;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#25512;&#36827;&#23545;&#27668;&#20505;&#21464;&#21270;&#26368;&#33030;&#24369;&#20154;&#21475;&#30340;&#28798;&#23475;&#21709;&#24212;&#26041;&#38754;&#30340;&#29420;&#29305;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disaster response agencies have been shifting from a paradigm of climate forecasting towards one of anticipatory action: assessing not just what the climate will be, but how it will impact specific populations, thereby enabling proactive response and resource allocation. Machine learning models are becoming exceptionally powerful at climate forecasting, but methodological gaps remain in terms of facilitating anticipatory action. Here we provide an overview of anticipatory action, review relevant applications of machine learning, identify common challenges, and highlight areas where machine learning can uniquely contribute to advancing disaster response for populations most vulnerable to climate change.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22909;&#22855;&#24515;&#39537;&#21160;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20174;&#37324;&#31243;&#25968;&#25454;&#29983;&#25104;&#36866;&#24403;&#30340;&#30005;&#26426;&#36895;&#24230;&#26469;&#23454;&#29616;&#33258;&#20027;&#23398;&#20064;&#20197;&#25511;&#21046;&#39134;&#34892;&#12290;&#36890;&#36807;&#35813;&#31639;&#27861;&#65292;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#21487;&#20197;&#22312;&#25511;&#21046;&#20559;&#33322;&#26041;&#21521;&#26397;&#21521;&#30446;&#26631;&#20301;&#32622;&#30340;&#21516;&#26102;&#36890;&#36807;&#38556;&#30861;&#29289;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;&#39044;&#27979;&#35823;&#24046;&#30340;&#26032;&#30340;&#22909;&#22855;&#24515;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.15724</link><description>&lt;p&gt;
&#22522;&#20110;&#22909;&#22855;&#24515;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20302;&#32423;&#39134;&#34892;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Curiosity-Driven Reinforcement Learning based Low-Level Flight Control. (arXiv:2307.15724v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22909;&#22855;&#24515;&#39537;&#21160;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20174;&#37324;&#31243;&#25968;&#25454;&#29983;&#25104;&#36866;&#24403;&#30340;&#30005;&#26426;&#36895;&#24230;&#26469;&#23454;&#29616;&#33258;&#20027;&#23398;&#20064;&#20197;&#25511;&#21046;&#39134;&#34892;&#12290;&#36890;&#36807;&#35813;&#31639;&#27861;&#65292;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#21487;&#20197;&#22312;&#25511;&#21046;&#20559;&#33322;&#26041;&#21521;&#26397;&#21521;&#30446;&#26631;&#20301;&#32622;&#30340;&#21516;&#26102;&#36890;&#36807;&#38556;&#30861;&#29289;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;&#39044;&#27979;&#35823;&#24046;&#30340;&#26032;&#30340;&#22909;&#22855;&#24515;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22909;&#22855;&#24515;&#26159;&#35768;&#22810;&#26377;&#26234;&#33021;&#30340;&#33258;&#28982;&#29983;&#29289;&#30340;&#20027;&#35201;&#21160;&#26426;&#20043;&#19968;&#65292;&#36890;&#36807;&#20855;&#26377;&#21487;&#34913;&#37327;&#26234;&#33021;&#27700;&#24179;&#30340;&#25506;&#32034;&#26469;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#23398;&#20064;&#12290;&#23427;&#20351;&#20154;&#31867;&#21644;&#35768;&#22810;&#21160;&#29289;&#33021;&#22815;&#36890;&#36807;&#23547;&#25214;&#35753;&#20182;&#20204;&#24863;&#21040;&#24778;&#35766;&#30340;&#29366;&#24577;&#26469;&#39640;&#25928;&#22320;&#25506;&#32034;&#65292;&#20197;&#20415;&#23398;&#20064;&#26356;&#22810;&#20182;&#20204;&#25152;&#19981;&#30693;&#36947;&#30340;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;&#22312;&#20445;&#25345;&#22909;&#22855;&#24515;&#30340;&#21516;&#26102;&#65292;&#20182;&#20204;&#33021;&#22815;&#23398;&#24471;&#26356;&#22909;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#22909;&#22855;&#24515;&#36890;&#24120;&#34987;&#32467;&#21512;&#21040;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#20013;&#65292;&#20316;&#20026;&#19968;&#31181;&#20869;&#22312;&#30340;&#22870;&#21169;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22909;&#22855;&#24515;&#39537;&#21160;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20174;&#37324;&#31243;&#25968;&#25454;&#29983;&#25104;&#36866;&#24403;&#30340;&#30005;&#26426;&#36895;&#24230;&#26469;&#23454;&#29616;&#33258;&#20027;&#23398;&#20064;&#20197;&#25511;&#21046;&#39134;&#34892;&#12290;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#21487;&#20197;&#22312;&#25511;&#21046;&#20559;&#33322;&#26041;&#21521;&#26397;&#21521;&#25152;&#38656;&#20301;&#32622;&#30340;&#21516;&#26102;&#36890;&#36807;&#38556;&#30861;&#29289;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#35823;&#24046;&#30340;&#26032;&#30340;&#22909;&#22855;&#24515;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22312;&#32447;&#31574;&#30053;&#12289;&#31163;&#32447;&#31574;&#30053;&#21644;&#22312;&#32447;&#31574;&#30053;&#21152;&#22122;&#38899;&#30340;&#27979;&#35797;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Curiosity is one of the main motives in many of the natural creatures with measurable levels of intelligence for exploration and, as a result, more efficient learning. It makes it possible for humans and many animals to explore efficiently by searching for being in states that make them surprised with the goal of learning more about what they do not know. As a result, while being curious, they learn better. In the machine learning literature, curiosity is mostly combined with reinforcement learning-based algorithms as an intrinsic reward. This work proposes an algorithm based on the drive of curiosity for autonomous learning to control by generating proper motor speeds from odometry data. The quadcopter controlled by our proposed algorithm can pass through obstacles while controlling the Yaw direction of the quad-copter toward the desired location. To achieve that, we also propose a new curiosity approach based on prediction error. We ran tests using on-policy, off-policy, on-policy pl
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#26102;&#38388;&#25554;&#20540;&#21644;&#32858;&#31867;&#32593;&#32476;&#20998;&#26512;&#20837;&#38498;6&#23567;&#26102;&#20869;&#30340;&#29983;&#21629;&#20307;&#24449;&#25968;&#25454;&#65292;&#22312;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#37492;&#21035;&#20986;&#19981;&#21516;&#30340;&#24613;&#24615;&#30142;&#30149;&#34920;&#22411;&#65292;&#20026;&#26089;&#26399;&#20020;&#24202;&#20915;&#31574;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2307.15719</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#26102;&#38388;&#25554;&#20540;&#21644;&#32858;&#31867;&#32593;&#32476;&#23545;&#29983;&#29702;&#29305;&#24449;&#36827;&#34892;&#30740;&#31350;&#65292;&#20197;&#37492;&#21035;&#24613;&#24615;&#30142;&#30149;&#34920;&#22411;
&lt;/p&gt;
&lt;p&gt;
Identifying acute illness phenotypes via deep temporal interpolation and clustering network on physiologic signatures. (arXiv:2307.15719v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15719
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#26102;&#38388;&#25554;&#20540;&#21644;&#32858;&#31867;&#32593;&#32476;&#20998;&#26512;&#20837;&#38498;6&#23567;&#26102;&#20869;&#30340;&#29983;&#21629;&#20307;&#24449;&#25968;&#25454;&#65292;&#22312;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#37492;&#21035;&#20986;&#19981;&#21516;&#30340;&#24613;&#24615;&#30142;&#30149;&#34920;&#22411;&#65292;&#20026;&#26089;&#26399;&#20020;&#24202;&#20915;&#31574;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#38498;&#20837;&#38498;&#30340;&#26368;&#21021;&#20960;&#20010;&#23567;&#26102;&#65292;&#23545;&#20020;&#24202;&#36712;&#36857;&#30340;&#24433;&#21709;&#24456;&#22823;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#19981;&#36275;&#65292;&#26089;&#26399;&#20020;&#24202;&#20915;&#31574;&#24448;&#24448;&#21463;&#21040;&#22256;&#25200;&#12290;&#36890;&#36807;&#23545;&#20837;&#38498;6&#23567;&#26102;&#20869;&#30340;&#29983;&#21629;&#20307;&#24449;&#36827;&#34892;&#32858;&#31867;&#20998;&#26512;&#65292;&#20855;&#26377;&#26126;&#26174;&#30149;&#29702;&#29983;&#29702;&#29305;&#24449;&#21644;&#32467;&#26524;&#30340;&#30149;&#20154;&#34920;&#22411;&#21487;&#20197;&#25903;&#25345;&#26089;&#26399;&#20020;&#24202;&#20915;&#31574;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21333;&#20013;&#24515;&#12289;&#38271;&#26399;&#30340;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;75,762&#21517;&#25104;&#24180;&#24739;&#32773;&#22312;&#19977;&#32423;&#25252;&#29702;&#20013;&#24515;&#20837;&#38498;&#36229;&#36807;6&#23567;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#26102;&#38388;&#25554;&#20540;&#21644;&#32858;&#31867;&#32593;&#32476;&#65292;&#20174;&#31232;&#30095;&#12289;&#19981;&#35268;&#21017;&#25277;&#26679;&#30340;&#29983;&#21629;&#20307;&#24449;&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#22312;&#35757;&#32451;&#38431;&#21015;&#65288;n=41,502&#65289;&#20013;&#24471;&#20986;&#19981;&#21516;&#30340;&#24739;&#32773;&#34920;&#22411;&#12290;&#26681;&#25454;&#39564;&#35777;&#38431;&#21015;&#65288;n=17,415&#65289;&#36873;&#25321;&#20102;&#27169;&#22411;&#21644;&#36229;&#21442;&#25968;&#12290;&#27979;&#35797;&#38431;&#21015;&#65288;n=16,845&#65289;&#29992;&#20110;&#20998;&#26512;&#21487;&#37325;&#22797;&#24615;&#21644;&#19982;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#30456;&#20851;&#24615;&#12290;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#27979;&#35797;&#38431;&#21015;&#30340;&#24180;&#40836;&#20998;&#24067;&#65288;54-55&#23681;&#65289;&#12289;&#24615;&#21035;&#65288;55%&#22899;&#24615;&#65289;&#12289;&#31181;&#26063;&#12289;&#21512;&#24182;&#30151;&#21644;&#30142;&#30149;&#20005;&#37325;&#31243;&#24230;&#22343;&#30456;&#20284;&#12290;&#20849;&#26377;&#22235;&#20010;&#31751;&#34987;&#37492;&#21035;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Initial hours of hospital admission impact clinical trajectory, but early clinical decisions often suffer due to data paucity. With clustering analysis for vital signs within six hours of admission, patient phenotypes with distinct pathophysiological signatures and outcomes may support early clinical decisions. We created a single-center, longitudinal EHR dataset for 75,762 adults admitted to a tertiary care center for 6+ hours. We proposed a deep temporal interpolation and clustering network to extract latent representations from sparse, irregularly sampled vital sign data and derived distinct patient phenotypes in a training cohort (n=41,502). Model and hyper-parameters were chosen based on a validation cohort (n=17,415). Test cohort (n=16,845) was used to analyze reproducibility and correlation with biomarkers. The training, validation, and testing cohorts had similar distributions of age (54-55 yrs), sex (55% female), race, comorbidities, and illness severity. Four clusters were id
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#21512;&#25104;&#21407;&#23376;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#21407;&#23376;&#21183;&#27169;&#22411;&#22312;&#35745;&#31639;&#23454;&#36341;&#20013;&#30340;&#25968;&#20540;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#30899;&#30340;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#21183;&#26041;&#27861;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15714</link><description>&lt;p&gt;
&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#21407;&#23376;&#38388;&#21183;&#30340;&#21512;&#25104;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Synthetic pre-training for neural-network interatomic potentials. (arXiv:2307.15714v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15714
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#21512;&#25104;&#21407;&#23376;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#21407;&#23376;&#21183;&#27169;&#22411;&#22312;&#35745;&#31639;&#23454;&#36341;&#20013;&#30340;&#25968;&#20540;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#30899;&#30340;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#21183;&#26041;&#27861;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21407;&#23376;&#38388;&#21183;&#24050;&#32463;&#25913;&#21464;&#20102;&#21407;&#23376;&#26448;&#26009;&#24314;&#27169;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#21183;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#37327;&#23376;&#21147;&#23398;&#21442;&#32771;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#65292;&#22240;&#27492;&#24320;&#21457;&#25968;&#25454;&#38598;&#21644;&#35757;&#32451;&#27969;&#31243;&#26159;&#19968;&#20010;&#26085;&#30410;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#22312;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#39046;&#22495;&#24120;&#35265;&#30340;&#8220;&#21512;&#25104;&#8221;&#65288;&#20154;&#24037;&#65289;&#25968;&#25454;&#30340;&#24605;&#24819;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#33258;&#36523;&#20351;&#29992;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#21183;&#26041;&#27861;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#21512;&#25104;&#21407;&#23376;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#21407;&#23376;&#21183;&#27169;&#22411;&#30340;&#25968;&#20540;&#31934;&#24230;&#21644;&#35745;&#31639;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#20851;&#20110;&#30899;&#30340;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#21183;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#36827;&#34892;&#21021;&#27493;&#23454;&#39564;&#27979;&#35797;&#20102;&#35813;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) based interatomic potentials have transformed the field of atomistic materials modelling. However, ML potentials depend critically on the quality and quantity of quantum-mechanical reference data with which they are trained, and therefore developing datasets and training pipelines is becoming an increasingly central challenge. Leveraging the idea of "synthetic" (artificial) data that is common in other areas of ML research, we here show that synthetic atomistic data, themselves obtained at scale with an existing ML potential, constitute a useful pre-training task for neural-network interatomic potential models. Once pre-trained with a large synthetic dataset, these models can be fine-tuned on a much smaller, quantum-mechanical one, improving numerical accuracy and stability in computational practice. We demonstrate feasibility for a series of equivariant graph-neural-network potentials for carbon, and we carry out initial experiments to test the limits of the appr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#25239;&#24615;&#31034;&#20363;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#26032;&#22411;&#24863;&#30693;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;R-LPIPS&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#24191;&#27867;&#37319;&#29992;&#30340;LPIPS&#24230;&#37327;&#26041;&#27861;&#23545;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#25935;&#24863;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.15157</link><description>&lt;p&gt;
R-LPIPS: &#19968;&#31181;&#20855;&#26377;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24863;&#30693;&#30456;&#20284;&#24230;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
R-LPIPS: An Adversarially Robust Perceptual Similarity Metric. (arXiv:2307.15157v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15157
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#25239;&#24615;&#31034;&#20363;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#26032;&#22411;&#24863;&#30693;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;R-LPIPS&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#24191;&#27867;&#37319;&#29992;&#30340;LPIPS&#24230;&#37327;&#26041;&#27861;&#23545;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#25935;&#24863;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20284;&#24230;&#24230;&#37327;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#29992;&#20110;&#25429;&#25417;&#22270;&#20687;&#30340;&#22522;&#26412;&#35821;&#20041;&#12290;&#26368;&#36817;&#20960;&#24180;&#65292;&#20986;&#29616;&#20102;&#20808;&#36827;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#20363;&#22914;&#23398;&#20064;&#30340;&#24863;&#30693;&#22270;&#20687;&#22359;&#30456;&#20284;&#24230;&#65288;LPIPS&#65289;&#12290;&#36825;&#20123;&#24230;&#37327;&#21033;&#29992;&#26469;&#33258;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#29305;&#24449;&#65292;&#24182;&#22312;&#30456;&#23545;&#22270;&#20687;&#30456;&#20284;&#24230;&#35780;&#20272;&#20013;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#24863;&#30693;&#23494;&#20999;&#19968;&#33268;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#22312;&#24050;&#32463;&#20247;&#25152;&#21608;&#30693;&#65292;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24433;&#21709;&#65292;&#21363;&#23545;&#20154;&#31867;&#26469;&#35828;&#19981;&#21487;&#35265;&#30340;&#23567;&#25200;&#21160;&#65292;&#34987;&#31934;&#24515;&#35774;&#35745;&#29992;&#26469;&#25925;&#24847;&#35823;&#23548;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;LPIPS&#24230;&#37327;&#26041;&#27861;&#20063;&#23545;&#36825;&#20123;&#23545;&#25239;&#24615;&#31034;&#20363;&#25935;&#24863;&#12290;&#36825;&#31181;&#25935;&#24863;&#24615;&#24341;&#20837;&#20102;&#37325;&#22823;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;LPIPS&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#37319;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#40065;&#26834;&#30340;&#23398;&#20064;&#24863;&#30693;&#22270;&#20687;&#22359;&#30456;&#20284;&#24230;&#65288;R-LPIPS&#65289;&#24230;&#37327;&#26041;&#27861;&#65292;&#19968;&#31181;&#21033;&#29992;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#28145;&#24230;&#29305;&#24449;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Similarity metrics have played a significant role in computer vision to capture the underlying semantics of images. In recent years, advanced similarity metrics, such as the Learned Perceptual Image Patch Similarity (LPIPS), have emerged. These metrics leverage deep features extracted from trained neural networks and have demonstrated a remarkable ability to closely align with human perception when evaluating relative image similarity. However, it is now well-known that neural networks are susceptible to adversarial examples, i.e., small perturbations invisible to humans crafted to deliberately mislead the model. Consequently, the LPIPS metric is also sensitive to such adversarial examples. This susceptibility introduces significant security concerns, especially considering the widespread adoption of LPIPS in large-scale applications. In this paper, we propose the Robust Learned Perceptual Image Patch Similarity (R-LPIPS) metric, a new metric that leverages adversarially trained deep f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#38598;&#20013;&#36890;&#36807;&#20449;&#24687;&#25552;&#21319;&#30340;&#26041;&#27861;&#36827;&#34892;&#23376;&#32676;&#21457;&#29616;&#12290;&#20855;&#20307;&#38024;&#23545;&#32954;&#30284;&#27835;&#30103;&#65292;&#22312;&#20445;&#25345;&#25110;&#25552;&#39640;&#27835;&#30103;&#25928;&#26524;&#30340;&#21516;&#26102;&#20943;&#23569;&#21103;&#20316;&#29992;&#23545;&#20110;&#25913;&#21892;&#24739;&#32773;&#30340;&#29983;&#27963;&#36136;&#37327;&#38750;&#24120;&#37325;&#35201;&#65292;&#20020;&#24202;&#25351;&#21335;&#34429;&#28982;&#25552;&#20379;&#20102;&#27835;&#30103;&#24314;&#35758;&#65292;&#20294;&#20173;&#26410;&#23558;&#27835;&#30103;&#32467;&#26524;&#32435;&#20837;&#32771;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.15089</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#20013;&#30340;&#20449;&#24687;&#25552;&#21319;&#23376;&#32676;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Information Gained Subgroup Discovery in Datasets. (arXiv:2307.15089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15089
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#38598;&#20013;&#36890;&#36807;&#20449;&#24687;&#25552;&#21319;&#30340;&#26041;&#27861;&#36827;&#34892;&#23376;&#32676;&#21457;&#29616;&#12290;&#20855;&#20307;&#38024;&#23545;&#32954;&#30284;&#27835;&#30103;&#65292;&#22312;&#20445;&#25345;&#25110;&#25552;&#39640;&#27835;&#30103;&#25928;&#26524;&#30340;&#21516;&#26102;&#20943;&#23569;&#21103;&#20316;&#29992;&#23545;&#20110;&#25913;&#21892;&#24739;&#32773;&#30340;&#29983;&#27963;&#36136;&#37327;&#38750;&#24120;&#37325;&#35201;&#65292;&#20020;&#24202;&#25351;&#21335;&#34429;&#28982;&#25552;&#20379;&#20102;&#27835;&#30103;&#24314;&#35758;&#65292;&#20294;&#20173;&#26410;&#23558;&#27835;&#30103;&#32467;&#26524;&#32435;&#20837;&#32771;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#30284;&#26159;&#30284;&#30151;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#39044;&#35745;2023&#24180;&#23558;&#26377;&#36229;&#36807;238,340&#20363;&#26032;&#30340;&#32954;&#30284;&#24739;&#32773;&#65292;&#20854;&#20013;&#26377;&#36229;&#36807;127,070&#20363;&#27515;&#20129;&#12290;&#36873;&#25321;&#27491;&#30830;&#30340;&#27835;&#30103;&#26041;&#26696;&#26159;&#25552;&#39640;&#23384;&#27963;&#29575;&#21644;&#25913;&#21892;&#24739;&#32773;&#29983;&#27963;&#36136;&#37327;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#30284;&#30151;&#27835;&#30103;&#21487;&#33021;&#24341;&#21457;&#21103;&#20316;&#29992;&#65292;&#36825;&#20123;&#27602;&#21103;&#21453;&#24212;&#20250;&#24341;&#36215;&#19981;&#21516;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#24433;&#21709;&#24739;&#32773;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#22312;&#20445;&#25345;&#25110;&#25552;&#39640;&#27835;&#30103;&#25928;&#26524;&#30340;&#21516;&#26102;&#20943;&#23569;&#27835;&#30103;&#21103;&#20316;&#29992;&#26159;&#20020;&#24202;&#35282;&#24230;&#35201;&#36861;&#27714;&#30340;&#37325;&#35201;&#30446;&#26631;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20020;&#24202;&#25351;&#21335;&#21253;&#25324;&#30284;&#30151;&#27835;&#30103;&#24314;&#35758;&#30340;&#19968;&#33324;&#30693;&#35782;&#65292;&#20197;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#12290;&#23613;&#31649;&#20182;&#20204;&#26681;&#25454;&#30284;&#30151;&#30142;&#30149;&#26041;&#38754;&#21644;&#20010;&#20307;&#24739;&#32773;&#29305;&#24449;&#25552;&#20379;&#27835;&#30103;&#24314;&#35758;&#65292;&#20294;&#24182;&#26410;&#25552;&#20379;&#22522;&#20110;&#27835;&#30103;&#32467;&#26524;&#30340;&#32479;&#35745;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#23545;&#20020;&#24202;&#25351;&#21335;&#19982;&#27835;&#30103;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lung cancer is the leading cause of cancer death. More than 238,340 new cases of lung cancer patients are expected in 2023, with an estimation of more than 127,070 deaths. Choosing the correct treatment is an important element to enhance the probability of survival and to improve patient's quality of life. Cancer treatments might provoke secondary effects. These toxicities cause different health problems that impact the patient's quality of life. Hence, reducing treatments toxicities while maintaining or improving their effectivenes is an important goal that aims to be pursued from the clinical perspective. On the other hand, clinical guidelines include general knowledge about cancer treatment recommendations to assist clinicians. Although they provide treatment recommendations based on cancer disease aspects and individual patient features, a statistical analysis taking into account treatment outcomes is not provided here. Therefore, the comparison between clinical guidelines with tre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MATNilm&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#26679;&#26412;&#22686;&#24378;&#21644;&#20849;&#20139;&#30340;&#20998;&#23618;&#25286;&#20998;&#32467;&#26500;&#65292;&#22312;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#19979;&#25552;&#39640;&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.14778</link><description>&lt;p&gt;
MATNilm: &#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#19979;&#22810;&#35774;&#22791;&#20219;&#21153;&#30340;&#26080;&#24178;&#25200;&#36127;&#36733;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
MATNilm: Multi-appliance-task Non-intrusive Load Monitoring with Limited Labeled Data. (arXiv:2307.14778v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MATNilm&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#26679;&#26412;&#22686;&#24378;&#21644;&#20849;&#20139;&#30340;&#20998;&#23618;&#25286;&#20998;&#32467;&#26500;&#65292;&#22312;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#19979;&#25552;&#39640;&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#27979; (NILM) &#36890;&#36807;&#23558;&#25972;&#20010;&#25151;&#23627;&#30340;&#24635;&#21151;&#32791;&#20449;&#21495;&#20998;&#35299;&#26469;&#35782;&#21035;&#21508;&#31181;&#23478;&#29992;&#30005;&#22120;&#30340;&#29366;&#24577;&#21644;&#30005;&#33021;&#28040;&#32791;&#12290;&#39640;&#25928;&#20934;&#30830;&#30340;&#36127;&#36733;&#30417;&#27979;&#26377;&#21161;&#20110;&#24314;&#31435;&#29992;&#25143;&#20010;&#20154;&#36164;&#26009;&#12289;&#26234;&#33021;&#23478;&#23621;&#33021;&#28304;&#31649;&#29702;&#21644;&#23792;&#20540;&#36127;&#33655;&#36716;&#31227;&#12290;&#36825;&#23545;&#20110;&#26368;&#32456;&#29992;&#25143;&#21644;&#20844;&#29992;&#20107;&#19994;&#37117;&#26377;&#30410;&#22788;&#65292;&#21487;&#20197;&#25552;&#39640;&#30005;&#21147;&#20998;&#37197;&#32593;&#32476;&#30340;&#25972;&#20307;&#25928;&#29575;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20026;&#27599;&#20010;&#30005;&#22120;&#24320;&#21457;&#21333;&#29420;&#30340;&#27169;&#22411;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#38590;&#20197;&#25910;&#38598;&#30340;&#23478;&#24237;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35774;&#22791;&#20219;&#21153;&#26694;&#26550;&#65292;&#37197;&#21512;&#19968;&#20010;&#35757;&#32451;&#39640;&#25928;&#30340;&#26679;&#26412;&#22686;&#24378; (SA) &#26041;&#26696;&#65292;&#20197;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#25552;&#39640;&#20998;&#35299;&#24615;&#33021;&#12290;&#23545;&#20110;&#27599;&#20010;&#30005;&#22120;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20849;&#20139;&#30340;&#20998;&#23618;&#25286;&#20998;&#32467;&#26500;&#65292;&#29992;&#20110;&#20854;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20108;&#32500;&#27880;&#24847;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-intrusive load monitoring (NILM) identifies the status and power consumption of various household appliances by disaggregating the total power usage signal of an entire house. Efficient and accurate load monitoring facilitates user profile establishment, intelligent household energy management, and peak load shifting. This is beneficial for both the end-users and utilities by improving the overall efficiency of a power distribution network. Existing approaches mainly focus on developing an individual model for each appliance. Those approaches typically rely on a large amount of household-labeled data which is hard to collect. In this paper, we propose a multi-appliance-task framework with a training-efficient sample augmentation (SA) scheme that boosts the disaggregation performance with limited labeled data. For each appliance, we develop a shared-hierarchical split structure for its regression and classification tasks. In addition, we also propose a two-dimensional attention mech
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#22312;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#27169;&#20223;&#22797;&#26434;&#19987;&#23478;&#28436;&#31034;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#31283;&#23450;&#27169;&#20223;&#31574;&#30053;&#24182;&#30830;&#20445;&#20934;&#30830;&#20272;&#35745;&#28436;&#31034;&#32773;&#20998;&#24067;&#65292;&#21487;&#20197;&#20351;&#27169;&#20223;&#32773;&#19982;&#28436;&#31034;&#32773;&#30340;&#36712;&#36857;&#20998;&#24067;&#30456;&#36817;&#12290;</title><link>http://arxiv.org/abs/2307.14619</link><description>&lt;p&gt;
&#27169;&#20223;&#22797;&#26434;&#36712;&#36857;&#65306;&#26725;&#25509;&#20302;&#23618;&#31283;&#23450;&#24615;&#19982;&#39640;&#23618;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Imitating Complex Trajectories: Bridging Low-Level Stability and High-Level Behavior. (arXiv:2307.14619v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#22312;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#27169;&#20223;&#22797;&#26434;&#19987;&#23478;&#28436;&#31034;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#31283;&#23450;&#27169;&#20223;&#31574;&#30053;&#24182;&#30830;&#20445;&#20934;&#30830;&#20272;&#35745;&#28436;&#31034;&#32773;&#20998;&#24067;&#65292;&#21487;&#20197;&#20351;&#27169;&#20223;&#32773;&#19982;&#28436;&#31034;&#32773;&#30340;&#36712;&#36857;&#20998;&#24067;&#30456;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#30740;&#31350;&#22312;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#27169;&#20223;&#38543;&#26426;&#12289;&#38750;&#39532;&#23572;&#21487;&#22827;&#12289;&#28508;&#22312;&#22810;&#27169;&#24577;&#65288;&#21363;&#8220;&#22797;&#26434;&#8221;&#65289;&#19987;&#23478;&#28436;&#31034;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#20302;&#23618;&#25511;&#21046;&#22120;&#65288;&#26080;&#35770;&#26159;&#23398;&#20064;&#30340;&#36824;&#26159;&#38544;&#21547;&#30340;&#65289;&#26469;&#31283;&#23450;&#22260;&#32469;&#19987;&#23478;&#28436;&#31034;&#30340;&#27169;&#20223;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#65288;a&#65289;&#21512;&#36866;&#30340;&#20302;&#23618;&#31283;&#23450;&#24615;&#20445;&#35777;&#21644;&#65288;b&#65289;&#23398;&#20064;&#31574;&#30053;&#30340;&#38543;&#26426;&#36830;&#32493;&#24615;&#23646;&#24615;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#24635;&#21464;&#24046;&#36830;&#32493;&#24615;&#8221;&#65289;&#65288;TVC&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#31934;&#30830;&#20272;&#35745;&#28436;&#31034;&#32773;&#29366;&#24577;&#20998;&#24067;&#19978;&#30340;&#34892;&#21160;&#30340;&#27169;&#20223;&#32773;&#20250;&#19982;&#28436;&#31034;&#32773;&#23545;&#25972;&#20010;&#36712;&#36857;&#30340;&#20998;&#24067;&#30456;&#36817;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;&#23558;&#27969;&#34892;&#30340;&#25968;&#25454;&#22686;&#24378;&#35268;&#21017;&#19982;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#25216;&#24039;&#30456;&#32467;&#21512;&#65288;&#21363;&#22312;&#25191;&#34892;&#26102;&#28155;&#21152;&#22686;&#24378;&#22122;&#22768;&#65289;&#26469;&#30830;&#20445;TVC&#24182;&#19988;&#26368;&#23567;&#31243;&#24230;&#19978;&#38477;&#20302;&#31934;&#24230;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20445;&#35777;&#23454;&#20363;&#21270;&#20026;&#30001;&#25193;&#25955;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#22914;&#26524;&#23398;&#20064;&#32773;&#20934;&#30830;&#22320;&#20272;&#35745;&#20102;&#28436;&#31034;&#32773;&#30340;&#20998;&#24067;&#65292;&#21017;&#26368;&#32456;&#23436;&#25104;&#36825;&#31181;&#23454;&#20363;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a theoretical framework for studying the imitation of stochastic, non-Markovian, potentially multi-modal (i.e. "complex" ) expert demonstrations in nonlinear dynamical systems. Our framework invokes low-level controllers either learned or implicit in position-command control - to stabilize imitation policies around expert demonstrations. We show that with (a) a suitable low-level stability guarantee and (b) a stochastic continuity property of the learned policy we call "total variation continuity" (TVC), an imitator that accurately estimates actions on the demonstrator's state distribution closely matches the demonstrator's distribution over entire trajectories. We then show that TVC can be ensured with minimal degradation of accuracy by combining a popular data-augmentation regimen with a novel algorithmic trick: adding augmentation noise at execution time. We instantiate our guarantees for policies parameterized by diffusion models and prove that if the learner accuratel
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#25216;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#30740;&#31350;&#20154;&#31867;&#33719;&#24471;&#25216;&#33021;&#30340;&#26377;&#24207;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#25216;&#33021;&#26102;&#20063;&#26377;&#19968;&#23450;&#30340;&#39034;&#24207;&#65292;&#24182;&#19988;&#36825;&#31181;&#39034;&#24207;&#21487;&#20197;&#25913;&#21892;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#21644;&#25968;&#25454;&#39640;&#25928;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2307.14430</link><description>&lt;p&gt;
Skill-it! &#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#25216;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models. (arXiv:2307.14430v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#25216;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#30740;&#31350;&#20154;&#31867;&#33719;&#24471;&#25216;&#33021;&#30340;&#26377;&#24207;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#25216;&#33021;&#26102;&#20063;&#26377;&#19968;&#23450;&#30340;&#39034;&#24207;&#65292;&#24182;&#19988;&#36825;&#31181;&#39034;&#24207;&#21487;&#20197;&#25913;&#21892;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#21644;&#25968;&#25454;&#39640;&#25928;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#23545;&#20110;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#22312;&#22266;&#23450;&#30340;token&#39044;&#31639;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#36873;&#25321;&#33021;&#22815;&#22312;&#21508;&#20010;&#20219;&#21153;&#20013;&#33719;&#24471;&#33391;&#22909;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#20551;&#35774;&#65306;&#20154;&#31867;&#22312;&#26377;&#24847;&#20041;&#30340;&#39034;&#24207;&#20013;&#33719;&#24471;&#30456;&#20114;&#20381;&#36182;&#30340;&#25216;&#33021;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#19968;&#32452;&#25216;&#33021;&#26102;&#20063;&#20250;&#36981;&#24490;&#36825;&#26679;&#30340;&#39034;&#24207;&#12290;&#22914;&#26524;&#23384;&#22312;&#36825;&#26679;&#30340;&#39034;&#24207;&#65292;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#21644;&#25968;&#25454;&#39640;&#25928;&#35757;&#32451;&#12290;&#21033;&#29992;&#36825;&#31181;&#30452;&#35273;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#25216;&#33021;&#30340;&#27010;&#24565;&#21644;&#26377;&#24207;&#30340;&#25216;&#33021;&#38598;&#21512;&#30340;&#27010;&#24565;&#24418;&#24335;&#21270;&#20026;&#30456;&#20851;&#25968;&#25454;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#26377;&#24207;&#30340;&#25216;&#33021;&#38598;&#21512;&#30340;&#23384;&#22312;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#23384;&#22312;&#20351;&#24471;&#22312;&#35757;&#32451;&#20854;&#20808;&#20915;&#26465;&#20214;&#25216;&#33021;&#26102;&#21487;&#20197;&#20351;&#29992;&#26356;&#23569;&#30340;&#25968;&#25454;&#26469;&#23398;&#20064;&#26356;&#39640;&#32423;&#30340;&#25216;&#33021;&#12290;&#20854;&#27425;&#65292;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#32447;&#25968;&#25454;&#37319;&#26679;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be utilized for improved understanding of LMs and for data-efficient training. Using this intuition, our framework formalizes the notion of a skill and of an ordered set of skills in terms of the associated data. First, using both synthetic and real data, we demonstrate that these ordered skill sets exist, and that their existence enables more advanced skills to be learned with less data when we train on their prerequisite skills. Second, using our proposed framework, we introduce an online data sampling algorithm
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#24515;&#34880;&#31649;&#27169;&#22411;&#30340;&#36870;&#38382;&#39064;&#20316;&#20026;&#32479;&#35745;&#25512;&#29702;&#36827;&#34892;&#35299;&#20915;&#65292;&#22312;&#20307;&#22806;&#36827;&#34892;&#20102;&#20116;&#20010;&#29983;&#29289;&#26631;&#35760;&#29289;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#27169;&#25311;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.13918</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#29702;&#29992;&#20110;&#24515;&#34880;&#31649;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Simulation-based Inference for Cardiovascular Models. (arXiv:2307.13918v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#24515;&#34880;&#31649;&#27169;&#22411;&#30340;&#36870;&#38382;&#39064;&#20316;&#20026;&#32479;&#35745;&#25512;&#29702;&#36827;&#34892;&#35299;&#20915;&#65292;&#22312;&#20307;&#22806;&#36827;&#34892;&#20102;&#20116;&#20010;&#29983;&#29289;&#26631;&#35760;&#29289;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#27169;&#25311;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#20013;&#65292;&#34880;&#27969;&#21160;&#21147;&#23398;&#27169;&#25311;&#22120;&#19981;&#26029;&#21457;&#23637;&#65292;&#24050;&#25104;&#20026;&#30740;&#31350;&#20307;&#22806;&#24515;&#34880;&#31649;&#31995;&#32479;&#30340;&#39318;&#36873;&#24037;&#20855;&#12290;&#34429;&#28982;&#36825;&#26679;&#30340;&#24037;&#20855;&#36890;&#24120;&#29992;&#20110;&#20174;&#29983;&#29702;&#21442;&#25968;&#27169;&#25311;&#20840;&#36523;&#34880;&#27969;&#21160;&#21147;&#23398;&#65292;&#20294;&#35299;&#20915;&#23558;&#27874;&#24418;&#26144;&#23556;&#22238;&#21512;&#29702;&#30340;&#29983;&#29702;&#21442;&#25968;&#30340;&#36870;&#38382;&#39064;&#20173;&#28982;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#21463;&#27169;&#25311;&#25512;&#29702;&#65288;SBI&#65289;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#36870;&#38382;&#39064;&#20316;&#20026;&#32479;&#35745;&#25512;&#29702;&#26469;&#22788;&#29702;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;SBI&#20026;&#24863;&#20852;&#36259;&#30340;&#21442;&#25968;&#25552;&#20379;&#20102;&#21518;&#39564;&#20998;&#24067;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#20010;&#20307;&#27979;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#22810;&#32500;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#20960;&#31181;&#27979;&#37327;&#27169;&#24577;&#26469;&#23637;&#31034;&#36825;&#31181;&#33021;&#21147;&#65292;&#36827;&#34892;&#20102;&#20116;&#20010;&#20020;&#24202;&#24863;&#20852;&#36259;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#20307;&#22806;&#19981;&#30830;&#23450;&#24615;&#20998;&#26512;&#12290;&#38500;&#20102;&#30830;&#35748;&#24050;&#30693;&#20107;&#23454;&#65292;&#27604;&#22914;&#20272;&#35745;&#24515;&#29575;&#30340;&#21487;&#34892;&#24615;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#31361;&#20986;&#20102;&#8230;
&lt;/p&gt;
&lt;p&gt;
Over the past decades, hemodynamics simulators have steadily evolved and have become tools of choice for studying cardiovascular systems in-silico. While such tools are routinely used to simulate whole-body hemodynamics from physiological parameters, solving the corresponding inverse problem of mapping waveforms back to plausible physiological parameters remains both promising and challenging. Motivated by advances in simulation-based inference (SBI), we cast this inverse problem as statistical inference. In contrast to alternative approaches, SBI provides \textit{posterior distributions} for the parameters of interest, providing a \textit{multi-dimensional} representation of uncertainty for \textit{individual} measurements. We showcase this ability by performing an in-silico uncertainty analysis of five biomarkers of clinical interest comparing several measurement modalities. Beyond the corroboration of known facts, such as the feasibility of estimating heart rate, our study highlight
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22522;&#20110;&#31243;&#24207;&#30340;&#25277;&#35937;&#35821;&#27861;&#26641;(ASTs)&#26469;&#26144;&#23556;&#21464;&#37327;&#38598;&#65292;&#20197;&#35299;&#20915;&#31243;&#24207;&#27604;&#36739;&#12289;&#20998;&#26512;&#12289;&#20462;&#22797;&#21644;&#20811;&#38534;&#26816;&#27979;&#31561;&#20219;&#21153;&#12290;&#22312;&#21021;&#23398;&#32773;&#32534;&#31243;&#20316;&#19994;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#21464;&#37327;&#26144;&#23556;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13014</link><description>&lt;p&gt;
&#29992;&#20110;&#31243;&#24207;&#20043;&#38388;&#21464;&#37327;&#26144;&#23556;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#8212;&#8212;&#25193;&#23637;&#29256;&#26412;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks For Mapping Variables Between Programs -- Extended Version. (arXiv:2307.13014v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22522;&#20110;&#31243;&#24207;&#30340;&#25277;&#35937;&#35821;&#27861;&#26641;(ASTs)&#26469;&#26144;&#23556;&#21464;&#37327;&#38598;&#65292;&#20197;&#35299;&#20915;&#31243;&#24207;&#27604;&#36739;&#12289;&#20998;&#26512;&#12289;&#20462;&#22797;&#21644;&#20811;&#38534;&#26816;&#27979;&#31561;&#20219;&#21153;&#12290;&#22312;&#21021;&#23398;&#32773;&#32534;&#31243;&#20316;&#19994;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#21464;&#37327;&#26144;&#23556;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#31243;&#24207;&#20998;&#26512;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#35768;&#22810;&#39046;&#22495;&#30340;&#20851;&#38190;&#30740;&#31350;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#24418;&#24335;&#26041;&#27861;&#21644;&#20154;&#24037;&#26234;&#33021;&#12290;&#30001;&#20110;&#31243;&#24207;&#31561;&#20215;&#38382;&#39064;&#30340;&#19981;&#21487;&#21028;&#23450;&#24615;&#65292;&#27604;&#36739;&#20004;&#20010;&#31243;&#24207;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36890;&#24120;&#65292;&#20026;&#20102;&#27604;&#36739;&#20004;&#20010;&#31243;&#24207;&#65292;&#38656;&#35201;&#23545;&#20004;&#20010;&#31243;&#24207;&#30340;&#21464;&#37327;&#38598;&#20043;&#38388;&#24314;&#31435;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#22312;&#35832;&#22914;&#31243;&#24207;&#31561;&#20215;&#24615;&#12289;&#31243;&#24207;&#20998;&#26512;&#12289;&#31243;&#24207;&#20462;&#22797;&#21644;&#20811;&#38534;&#26816;&#27979;&#31561;&#20219;&#21153;&#19978;&#65292;&#26144;&#23556;&#20004;&#20010;&#31243;&#24207;&#20043;&#38388;&#30340;&#21464;&#37327;&#26159;&#38750;&#24120;&#26377;&#29992;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22522;&#20110;&#20004;&#20010;&#31243;&#24207;&#30340;&#25277;&#35937;&#35821;&#27861;&#26641;(ASTs)&#26469;&#26144;&#23556;&#21464;&#37327;&#38598;&#12290;&#20026;&#20102;&#23637;&#31034;&#21464;&#37327;&#26144;&#23556;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#22312;&#31243;&#24207;&#20462;&#22797;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#36825;&#20123;&#26144;&#23556;&#30340;&#19977;&#20010;&#29992;&#20363;&#65292;&#20197;&#20462;&#22797;&#21021;&#23398;&#32773;&#32534;&#31243;&#20316;&#19994;&#20013;&#24120;&#35265;&#30340;&#21644;&#32463;&#24120;&#21457;&#29983;&#30340;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#22522;&#20110;&#19968;&#20010;&#21253;&#21547;4166&#23545;&#38169;&#35823;/&#20462;&#27491;&#31243;&#24207;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated program analysis is a pivotal research domain in many areas of Computer Science -- Formal Methods and Artificial Intelligence, in particular. Due to the undecidability of the problem of program equivalence, comparing two programs is highly challenging. Typically, in order to compare two programs, a relation between both programs' sets of variables is required. Thus, mapping variables between two programs is useful for a panoply of tasks such as program equivalence, program analysis, program repair, and clone detection. In this work, we propose using graph neural networks (GNNs) to map the set of variables between two programs based on both programs' abstract syntax trees (ASTs). To demonstrate the strength of variable mappings, we present three use-cases of these mappings on the task of program repair to fix well-studied and recurrent bugs among novice programmers in introductory programming assignments (IPAs). Experimental results on a dataset of 4166 pairs of incorrect/corr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#32467;&#21512;&#26059;&#36716;&#21644;&#22810;&#20010;&#21487;&#25511;&#21943;&#21475;&#65292;&#36890;&#36807;&#20248;&#21270;&#21943;&#21475;&#25968;&#37327;&#21644;&#20301;&#32622;&#65292;&#20256;&#24863;&#22120;&#20301;&#32622;&#20197;&#21450;&#27599;&#20010;&#21160;&#20316;&#21487;&#20801;&#35768;&#30340;&#26368;&#22823;&#27969;&#37327;&#21644;&#27599;&#20010;episode&#20013;&#20801;&#35768;&#30340;&#24635;&#21943;&#21475;&#25968;&#30340;&#24418;&#24335;&#65292;&#23454;&#29616;&#23545;&#26059;&#36716;&#22278;&#26609;&#20307;&#27969;&#21160;&#30340;&#20027;&#21160;&#25511;&#21046;&#65292;&#25233;&#21046;&#28065;&#27969;&#33073;&#33853;&#21644;&#31283;&#23450;&#21345;&#38376;&#28065;&#27969;&#12290;</title><link>http://arxiv.org/abs/2307.12083</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#21943;&#21475;&#23545;&#26059;&#36716;&#22278;&#26609;&#20307;&#27969;&#21160;&#36827;&#34892;&#20027;&#21160;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Active Control of Flow over Rotating Cylinder by Multiple Jets using Deep Reinforcement Learning. (arXiv:2307.12083v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#32467;&#21512;&#26059;&#36716;&#21644;&#22810;&#20010;&#21487;&#25511;&#21943;&#21475;&#65292;&#36890;&#36807;&#20248;&#21270;&#21943;&#21475;&#25968;&#37327;&#21644;&#20301;&#32622;&#65292;&#20256;&#24863;&#22120;&#20301;&#32622;&#20197;&#21450;&#27599;&#20010;&#21160;&#20316;&#21487;&#20801;&#35768;&#30340;&#26368;&#22823;&#27969;&#37327;&#21644;&#27599;&#20010;episode&#20013;&#20801;&#35768;&#30340;&#24635;&#21943;&#21475;&#25968;&#30340;&#24418;&#24335;&#65292;&#23454;&#29616;&#23545;&#26059;&#36716;&#22278;&#26609;&#20307;&#27969;&#21160;&#30340;&#20027;&#21160;&#25511;&#21046;&#65292;&#25233;&#21046;&#28065;&#27969;&#33073;&#33853;&#21644;&#31283;&#23450;&#21345;&#38376;&#28065;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#30495;&#27491;&#23041;&#21147;&#20307;&#29616;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30001;&#20110;&#20854;&#21160;&#24577;&#24615;&#36136;&#65292;&#24378;&#21270;&#23398;&#20064;&#22312;&#35745;&#31639;&#21644;&#29289;&#29702;&#26041;&#38754;&#26356;&#20026;&#22797;&#26434;&#12290;&#26059;&#36716;&#21644;&#21943;&#23556;&#24050;&#34987;&#35777;&#23454;&#26159;&#20943;&#23569;&#38045;&#20307;&#25152;&#21463;&#38459;&#21147;&#30340;&#19968;&#31181;&#26377;&#25928;&#30340;&#20027;&#21160;&#27969;&#21160;&#25511;&#21046;&#26041;&#24335;&#12290;&#26412;&#30740;&#31350;&#23558;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;(DRL)&#26469;&#28155;&#21152;&#26059;&#36716;&#65292;&#24182;&#21033;&#29992;&#22810;&#20010;&#21487;&#25511;&#21943;&#21475;&#20197;&#23454;&#29616;&#26368;&#22823;&#21487;&#33021;&#30340;&#38459;&#21147;&#25233;&#21046;&#12290;&#23558;&#20171;&#32461;DRL&#20195;&#30721;&#30340;&#29305;&#28857;&#65292;&#21253;&#25324;&#25511;&#21046;&#21442;&#25968;&#12289;&#20854;&#38480;&#21046;&#20197;&#21450;&#29992;&#20110;&#26059;&#36716;&#30340;DRL&#32593;&#32476;&#30340;&#20248;&#21270;&#12290;&#26412;&#30740;&#31350;&#23558;&#37325;&#28857;&#20248;&#21270;&#21943;&#21475;&#30340;&#25968;&#37327;&#21644;&#20301;&#32622;&#12289;&#20256;&#24863;&#22120;&#20301;&#32622;&#20197;&#21450;&#27599;&#20010;&#21160;&#20316;&#21487;&#20801;&#35768;&#30340;&#26368;&#22823;&#27969;&#37327;&#21644;&#27599;&#20010;episode&#20013;&#20801;&#35768;&#30340;&#24635;&#21943;&#21475;&#25968;&#30340;&#24418;&#24335;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#26059;&#36716;&#19982;DRL&#24037;&#20855;&#30456;&#32467;&#21512;&#26159;&#26377;&#24076;&#26395;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25233;&#21046;&#28065;&#27969;&#33073;&#33853;&#65292;&#31283;&#23450;&#21345;&#38376;&#28065;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
The real power of artificial intelligence appears in reinforcement learning, which is computationally and physically more sophisticated due to its dynamic nature. Rotation and injection have been a proven way of active flow control to reduce the drag force exerted on blunt bodies. Rotation will be added to the cylinder alongside the deep reinforcement learning (DRL) algorithm, which uses multiple controlled jets to reach maximum possible drag suppression. Characteristics of the DRL code, including controlling parameters, their limitations, and optimization of the DRL network for use with rotation will be presented. This work will focus on optimizing the number and positions of the jets, sensors location, and maximum allowed flow rate to jets in the form of maximum allowed flow rate of each actuation and the total number of them per episode. It is found that combining the rotation with the DRL tools is promising, since it suppresses the vortex shedding, stabilizes the Karman vortex stre
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#20248;&#21270;&#20809;&#23398;&#35745;&#31639;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21407;&#20301;&#36827;&#34892;&#36731;&#37327;&#32423;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#30340;&#30495;&#23454;&#20809;&#23398;&#35745;&#31639;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20998;&#31867;&#20934;&#30830;&#24230;&#19978;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#39640;&#36895;&#32454;&#32990;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#22266;&#26377;&#31616;&#21333;&#24615;&#21644;&#20302;&#38656;&#27714;&#30340;&#35745;&#31639;&#36164;&#28304;&#20419;&#36827;&#20102;&#20809;&#23398;&#35745;&#31639;&#25216;&#26415;&#20174;&#23454;&#39564;&#23460;&#30740;&#31350;&#21521;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#36716;&#21464;&#12290;</title><link>http://arxiv.org/abs/2307.11957</link><description>&lt;p&gt;
&#36890;&#36807;&#21407;&#20301;&#26080;&#27169;&#22411;&#20248;&#21270;&#23454;&#29616;&#39640;&#24615;&#33021;&#30495;&#23454;&#20809;&#23398;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
High-performance real-world optical computing trained by in situ model-free optimization. (arXiv:2307.11957v1 [physics.optics] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#20248;&#21270;&#20809;&#23398;&#35745;&#31639;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21407;&#20301;&#36827;&#34892;&#36731;&#37327;&#32423;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#30340;&#30495;&#23454;&#20809;&#23398;&#35745;&#31639;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20998;&#31867;&#20934;&#30830;&#24230;&#19978;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#39640;&#36895;&#32454;&#32990;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#22266;&#26377;&#31616;&#21333;&#24615;&#21644;&#20302;&#38656;&#27714;&#30340;&#35745;&#31639;&#36164;&#28304;&#20419;&#36827;&#20102;&#20809;&#23398;&#35745;&#31639;&#25216;&#26415;&#20174;&#23454;&#39564;&#23460;&#30740;&#31350;&#21521;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#35745;&#31639;&#31995;&#32479;&#21487;&#20197;&#25552;&#20379;&#39640;&#36895;&#21644;&#20302;&#33021;&#32791;&#30340;&#25968;&#25454;&#22788;&#29702;&#65292;&#20294;&#22312;&#35745;&#31639;&#23494;&#38598;&#30340;&#35757;&#32451;&#21644;&#20174;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#36716;&#25442;&#20013;&#23384;&#22312;&#19981;&#36275;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#26799;&#24230;&#20272;&#35745;&#31639;&#27861;&#30340;&#36731;&#37327;&#32423;&#21407;&#20301;&#20248;&#21270;&#20809;&#23398;&#35745;&#31639;&#31995;&#32479;&#30340;&#26080;&#27169;&#22411;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#23558;&#31995;&#32479;&#35270;&#20026;&#40657;&#30418;&#23376;&#65292;&#30452;&#25509;&#23558;&#25439;&#22833;&#21453;&#21521;&#20256;&#25773;&#21040;&#20809;&#23398;&#26435;&#37325;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#23545;&#35745;&#31639;&#23494;&#38598;&#21644;&#26377;&#20559;&#35265;&#30340;&#31995;&#32479;&#27169;&#25311;&#30340;&#38656;&#27714;&#12290;&#36890;&#36807;&#22312;&#21333;&#23618;&#34893;&#23556;&#20809;&#23398;&#35745;&#31639;&#31995;&#32479;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#22312;MNIST&#21644;FMNIST&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#36234;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#22312;&#26080;&#22270;&#29255;&#21644;&#39640;&#36895;&#32454;&#32990;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#22266;&#26377;&#31616;&#21333;&#24615;&#65292;&#32467;&#21512;&#20854;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#20302;&#38656;&#27714;&#65292;&#21152;&#36895;&#20102;&#20809;&#23398;&#35745;&#31639;&#20174;&#23454;&#39564;&#23460;&#28436;&#31034;&#21040;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#36807;&#28193;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical computing systems can provide high-speed and low-energy data processing but face deficiencies in computationally demanding training and simulation-to-reality gap. We propose a model-free solution for lightweight in situ optimization of optical computing systems based on the score gradient estimation algorithm. This approach treats the system as a black box and back-propagates loss directly to the optical weights' probabilistic distributions, hence circumventing the need for computation-heavy and biased system simulation. We demonstrate a superior classification accuracy on the MNIST and FMNIST datasets through experiments on a single-layer diffractive optical computing system. Furthermore, we show its potential for image-free and high-speed cell analysis. The inherent simplicity of our proposed method, combined with its low demand for computational resources, expedites the transition of optical computing from laboratory demonstrations to real-world applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21487;&#38752;&#24615;&#30340;&#25972;&#20307;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#35780;&#20272;&#20998;&#24067;&#20869;&#20934;&#30830;&#24615;&#12289;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#26657;&#20934;&#24615;&#21644;&#36234;&#30028;&#26816;&#27979;&#33021;&#21147;&#31561;&#20116;&#20010;&#20851;&#38190;&#23646;&#24615;&#65292;&#24341;&#20837;&#20102;&#21487;&#38752;&#24615;&#24471;&#20998;&#26469;&#35780;&#20272;&#25972;&#20010;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10586</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21487;&#38752;&#24615;&#30340;&#25972;&#20307;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Holistic Assessment of the Reliability of Machine Learning Systems. (arXiv:2307.10586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21487;&#38752;&#24615;&#30340;&#25972;&#20307;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#35780;&#20272;&#20998;&#24067;&#20869;&#20934;&#30830;&#24615;&#12289;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#26657;&#20934;&#24615;&#21644;&#36234;&#30028;&#26816;&#27979;&#33021;&#21147;&#31561;&#20116;&#20010;&#20851;&#38190;&#23646;&#24615;&#65292;&#24341;&#20837;&#20102;&#21487;&#38752;&#24615;&#24471;&#20998;&#26469;&#35780;&#20272;&#25972;&#20010;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#36234;&#26469;&#36234;&#22810;&#22320;&#28183;&#36879;&#21040;&#21307;&#30103;&#20445;&#20581;&#12289;&#20132;&#36890;&#36816;&#36755;&#12289;&#20891;&#20107;&#21644;&#22269;&#23478;&#23433;&#20840;&#31561;&#39640;&#39118;&#38505;&#39046;&#22495;&#65292;&#20154;&#20204;&#23545;&#20854;&#21487;&#38752;&#24615;&#20135;&#29983;&#20102;&#25285;&#24551;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#23545;&#25239;&#25915;&#20987;&#25110;&#29615;&#22659;&#21464;&#21270;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#24615;&#33021;&#21487;&#33021;&#26174;&#33879;&#38477;&#20302;&#65292;&#23548;&#33268;&#39044;&#27979;&#36807;&#20110;&#33258;&#20449;&#12289;&#26080;&#27861;&#26816;&#27979;&#36755;&#20837;&#25925;&#38556;&#20197;&#21450;&#22312;&#24847;&#22806;&#22330;&#26223;&#20013;&#26080;&#27861;&#27867;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21487;&#38752;&#24615;&#30340;&#25972;&#20307;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#35780;&#20272;&#20102;&#20116;&#20010;&#20851;&#38190;&#23646;&#24615;&#65306;&#20998;&#24067;&#20869;&#20934;&#30830;&#24615;&#12289;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#26657;&#20934;&#24615;&#21644;&#36234;&#30028;&#26816;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#38752;&#24615;&#24471;&#20998;&#26469;&#35780;&#20272;&#25972;&#20010;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#25552;&#20379;&#19981;&#21516;&#31639;&#27861;&#26041;&#27861;&#24615;&#33021;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#35782;&#21035;&#21644;&#20998;&#31867;&#20102;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#28982;&#21518;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#23545;&#20854;&#20013;&#30340;&#19968;&#20123;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning (ML) systems increasingly permeate high-stakes settings such as healthcare, transportation, military, and national security, concerns regarding their reliability have emerged. Despite notable progress, the performance of these systems can significantly diminish due to adversarial attacks or environmental changes, leading to overconfident predictions, failures to detect input faults, and an inability to generalize in unexpected scenarios. This paper proposes a holistic assessment methodology for the reliability of ML systems. Our framework evaluates five key properties: in-distribution accuracy, distribution-shift robustness, adversarial robustness, calibration, and out-of-distribution detection. A reliability score is also introduced and used to assess the overall system reliability. To provide insights into the performance of different algorithmic approaches, we identify and categorize state-of-the-art techniques, then evaluate a selection on real-world tasks using
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#30340;&#26497;&#21270;&#35299;&#30721;&#22120;&#30340;&#21333;&#27425;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#28040;&#38500;&#20102;&#23545;&#39044;&#23450;&#20041;&#26631;&#31614;&#30340;&#20381;&#36182;&#65292;&#23454;&#29616;&#20102;&#22312;&#36890;&#20449;&#31995;&#32479;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#30452;&#25509;&#35757;&#32451;&#65292;&#20854;&#24615;&#33021;&#25509;&#36817;&#26368;&#22823;&#21518;&#39564;&#27010;&#29575;&#35299;&#30721;&#22120;&#65292;&#24182;&#23637;&#31034;&#20986;&#26356;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.08004</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#30340;&#26497;&#21270;&#35299;&#30721;&#22120;&#30340;&#21333;&#27425;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
For One-Shot Decoding: Unsupervised Deep Learning-Based Polar Decoder. (arXiv:2307.08004v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#30340;&#26497;&#21270;&#35299;&#30721;&#22120;&#30340;&#21333;&#27425;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#28040;&#38500;&#20102;&#23545;&#39044;&#23450;&#20041;&#26631;&#31614;&#30340;&#20381;&#36182;&#65292;&#23454;&#29616;&#20102;&#22312;&#36890;&#20449;&#31995;&#32479;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#30452;&#25509;&#35757;&#32451;&#65292;&#20854;&#24615;&#33021;&#25509;&#36817;&#26368;&#22823;&#21518;&#39564;&#27010;&#29575;&#35299;&#30721;&#22120;&#65292;&#24182;&#23637;&#31034;&#20986;&#26356;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#30721;&#26041;&#26696;&#65292;&#21487;&#20197;&#23454;&#29616;&#26497;&#21270;&#30721;&#30340;&#21333;&#27425;&#35299;&#30721;&#12290;&#22312;&#36825;&#31181;&#26041;&#26696;&#20013;&#65292;&#25105;&#20204;&#19981;&#20877;&#20351;&#29992;&#20449;&#24687;&#20301;&#21521;&#37327;&#20316;&#20026;&#26631;&#31614;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#65292;&#32780;&#26159;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#21033;&#29992;&#26497;&#21270;&#30721;&#30340;&#29983;&#25104;&#30697;&#38453;&#26469;&#35757;&#32451;NN&#20197;&#23454;&#29616;&#26377;&#30028;&#36317;&#31163;&#35299;&#30721;&#12290;&#36825;&#31181;&#26041;&#27861;&#28040;&#38500;&#20102;&#23545;&#39044;&#23450;&#20041;&#26631;&#31614;&#30340;&#20381;&#36182;&#65292;&#22686;&#24378;&#20102;&#30452;&#25509;&#22312;&#36890;&#20449;&#31995;&#32479;&#30340;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21487;&#24212;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#35745;&#31639;&#26426;&#27169;&#25311;&#34920;&#26126;&#65292;&#65288;i&#65289;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#30340;&#35823;&#27604;&#29305;&#29575;&#65288;BER&#65289;&#21644;&#22359;&#38169;&#35823;&#29575;&#65288;BLER&#65289;&#24615;&#33021;&#22312;&#38750;&#24120;&#30701;&#30340;&#25968;&#25454;&#21253;&#19978;&#21487;&#20197;&#25509;&#36817;&#26368;&#22823;&#21518;&#39564;&#27010;&#29575;&#65288;MAP&#65289;&#35299;&#30721;&#22120;&#30340;&#24615;&#33021;&#65292;&#65288;ii&#65289;&#19982;&#20256;&#32479;&#30340;&#35299;&#30721;&#22120;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;NN&#35299;&#30721;&#22120;&#23637;&#31034;&#20102;&#26356;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an unsupervised deep learning-based decoding scheme that enables one-shot decoding of polar codes. In the proposed scheme, rather than using the information bit vectors as labels for training the neural network (NN) through supervised learning as the conventional scheme did, the NN is trained to function as a bounded distance decoder by leveraging the generator matrix of polar codes through self-supervised learning. This approach eliminates the reliance on predefined labels, empowering the potential to train directly on the actual data within communication systems and thereby enhancing the applicability. Furthermore, computer simulations demonstrate that (i) the bit error rate (BER) and block error rate (BLER) performances of the proposed scheme can approach those of the maximum a posteriori (MAP) decoder for very short packets and (ii) the proposed NN decoder exhibits much superior generalization ability compared to the conventional one.
&lt;/p&gt;</description></item><item><title>DISPEL&#26159;&#19968;&#31181;&#36890;&#36807;&#21518;&#22788;&#29702;&#32454;&#31890;&#24230;&#25513;&#34109;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#36807;&#28388;&#25481;&#26410;&#23450;&#20041;&#21644;&#26080;&#27861;&#21306;&#20998;&#30340;&#39046;&#22495;&#29305;&#23450;&#29305;&#24449;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.07181</link><description>&lt;p&gt;
DISPEL&#65306;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#35299;&#25918;&#36827;&#34892;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
DISPEL: Domain Generalization via Domain-Specific Liberating. (arXiv:2307.07181v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07181
&lt;/p&gt;
&lt;p&gt;
DISPEL&#26159;&#19968;&#31181;&#36890;&#36807;&#21518;&#22788;&#29702;&#32454;&#31890;&#24230;&#25513;&#34109;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#36807;&#28388;&#25481;&#26410;&#23450;&#20041;&#21644;&#26080;&#27861;&#21306;&#20998;&#30340;&#39046;&#22495;&#29305;&#23450;&#29305;&#24449;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#26088;&#22312;&#36890;&#36807;&#20165;&#22312;&#26377;&#38480;&#30340;&#28304;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#22312;&#26410;&#30693;&#27979;&#35797;&#39046;&#22495;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#24448;&#24448;&#24341;&#20837;&#19982;&#39044;&#27979;&#26080;&#20851;&#30340;&#22122;&#22768;&#25110;&#38656;&#35201;&#25910;&#38598;&#39046;&#22495;&#26631;&#31614;&#26469;&#35299;&#20915;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#32771;&#34385;&#20102;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#65292;&#23558;&#24213;&#23618;&#29305;&#24449;&#32452;&#21010;&#20998;&#20026;&#39046;&#22495;&#20849;&#20139;&#21644;&#39046;&#22495;&#29305;&#23450;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#39046;&#22495;&#29305;&#23450;&#29305;&#24449;&#24456;&#38590;&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#36827;&#34892;&#35782;&#21035;&#21644;&#21306;&#20998;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISPEL&#65288;DomaIn-SPEcific Liberating&#65289;&#30340;&#21518;&#22788;&#29702;&#32454;&#31890;&#24230;&#25513;&#34109;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#36807;&#28388;&#25481;&#26410;&#23450;&#20041;&#21644;&#26080;&#27861;&#21306;&#20998;&#30340;&#39046;&#22495;&#29305;&#23450;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DISPEL&#21033;&#29992;&#19968;&#20010;&#25513;&#34109;&#29983;&#25104;&#22120;&#20026;&#27599;&#20010;&#36755;&#20837;&#25968;&#25454;&#29983;&#25104;&#19968;&#20010;&#21807;&#19968;&#30340;&#25513;&#34109;&#26469;&#36807;&#28388;&#39046;&#22495;&#29305;&#23450;&#29305;&#24449;&#12290;DISPEL&#26694;&#26550;&#38750;&#24120;&#28789;&#27963;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#32454;&#31890;&#24230;&#30340;&#25513;&#34109;&#20219;&#21153;&#21644;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization aims to learn a generalization model that can perform well on unseen test domains by only training on limited source domains. However, existing domain generalization approaches often bring in prediction-irrelevant noise or require the collection of domain labels. To address these challenges, we consider the domain generalization problem from a different perspective by categorizing underlying feature groups into domain-shared and domain-specific features. Nevertheless, the domain-specific features are difficult to be identified and distinguished from the input data. In this work, we propose DomaIn-SPEcific Liberating (DISPEL), a post-processing fine-grained masking approach that can filter out undefined and indistinguishable domain-specific features in the embedding space. Specifically, DISPEL utilizes a mask generator that produces a unique mask for each input data to filter domain-specific features. The DISPEL framework is highly flexible to be applied to any fin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32452;&#21512;&#20998;&#24067;&#20559;&#31227;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#30697;&#38453;&#34917;&#20840;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#21452;&#32447;&#24615;&#23884;&#20837;&#65292;&#23454;&#29616;&#23545;&#35757;&#32451;&#20013;&#26410;&#28085;&#30422;&#30340;&#27979;&#35797;&#20998;&#24067;&#36827;&#34892;&#22806;&#25512;&#12290;&#36825;&#20010;&#35774;&#32622;&#23558;&#32570;&#22833;&#38750;&#38543;&#26426;&#25968;&#25454;&#30340;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#24191;&#20041;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.06457</link><description>&lt;p&gt;
&#35299;&#20915;&#32452;&#21512;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65306;&#22522;&#20110;&#30697;&#38453;&#34917;&#20840;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Tackling Combinatorial Distribution Shift: A Matrix Completion Perspective. (arXiv:2307.06457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06457
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32452;&#21512;&#20998;&#24067;&#20559;&#31227;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#30697;&#38453;&#34917;&#20840;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#21452;&#32447;&#24615;&#23884;&#20837;&#65292;&#23454;&#29616;&#23545;&#35757;&#32451;&#20013;&#26410;&#28085;&#30422;&#30340;&#27979;&#35797;&#20998;&#24067;&#36827;&#34892;&#22806;&#25512;&#12290;&#36825;&#20010;&#35774;&#32622;&#23558;&#32570;&#22833;&#38750;&#38543;&#26426;&#25968;&#25454;&#30340;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#24191;&#20041;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#33719;&#24471;&#20005;&#26684;&#30340;&#32479;&#35745;&#20445;&#35777;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#19988;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31216;&#20026;&#32452;&#21512;&#20998;&#24067;&#20559;&#31227;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;(a)&#22312;&#27979;&#35797;&#21644;&#35757;&#32451;&#20998;&#24067;&#19979;&#65292;&#26631;&#31614;$z$&#30001;&#29305;&#24449;$(x,y)$&#30340;&#23545;&#20915;&#23450;&#65292;(b)&#35757;&#32451;&#20998;&#24067;&#28085;&#30422;&#20102;$x$&#21644;$y$&#20998;&#21035;&#30340;&#19968;&#23450;&#36793;&#32536;&#20998;&#24067;&#65292;&#20294;&#26159;(c)&#27979;&#35797;&#20998;&#24067;&#28041;&#21450;&#20102;&#19968;&#20010;&#22312;&#35757;&#32451;&#20998;&#24067;&#20013;&#26410;&#28085;&#30422;&#30340;$(x,y)$&#30340;&#20135;&#21697;&#20998;&#24067;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#26631;&#31614;&#30001;&#21452;&#32447;&#24615;&#23884;&#20837;&#21040;Hilbert&#31354;&#38388;$H$&#20013;&#32473;&#20986;&#30340;&#29305;&#27530;&#24773;&#20917;&#65306;$\mathbb{E}[z \mid x,y ]=\langle f_{\star}(x),g_{\star}(y)\rangle_{{H}}$&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23545;&#22312;&#35757;&#32451;&#20013;&#26410;&#28085;&#30422;&#30340;&#27979;&#35797;&#20998;&#24067;&#22495;&#36827;&#34892;&#22806;&#25512;&#65292;&#21363;&#23454;&#29616;&#21452;&#32447;&#24615;&#32452;&#21512;&#22806;&#25512;&#12290;&#25105;&#20204;&#30340;&#35774;&#32622;&#23558;&#32570;&#22833;&#38750;&#38543;&#26426;&#25968;&#25454;&#30340;&#30697;&#38453;&#34917;&#20840;&#30340;&#19968;&#20010;&#29305;&#27530;&#24773;&#20917;&#24191;&#20041;&#21270;&#65292;&#23545;&#20110;&#35813;&#24773;&#20917;&#65292;&#25152;&#26377;&#29616;&#26377;&#32467;&#26524;&#37117;&#35201;&#27714;....
&lt;/p&gt;
&lt;p&gt;
Obtaining rigorous statistical guarantees for generalization under distribution shift remains an open and active research area. We study a setting we call combinatorial distribution shift, where (a) under the test- and training-distributions, the labels $z$ are determined by pairs of features $(x,y)$, (b) the training distribution has coverage of certain marginal distributions over $x$ and $y$ separately, but (c) the test distribution involves examples from a product distribution over $(x,y)$ that is {not} covered by the training distribution. Focusing on the special case where the labels are given by bilinear embeddings into a Hilbert space $H$: $\mathbb{E}[z \mid x,y ]=\langle f_{\star}(x),g_{\star}(y)\rangle_{{H}}$, we aim to extrapolate to a test distribution domain that is $not$ covered in training, i.e., achieving bilinear combinatorial extrapolation.  Our setting generalizes a special case of matrix completion from missing-not-at-random data, for which all existing results requi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30142;&#30149;&#36827;&#23637;&#32858;&#31867;&#20013;&#35299;&#35835;&#28145;&#24230;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;2&#22411;&#31958;&#23615;&#30149;&#21442;&#19982;&#32773;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#23545;&#30142;&#30149;&#36827;&#23637;&#27169;&#24335;&#30340;&#20020;&#24202;&#24847;&#20041;&#24615;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.06060</link><description>&lt;p&gt;
&#35299;&#35835;&#30142;&#30149;&#36827;&#23637;&#32858;&#31867;&#20013;&#30340;&#28145;&#24230;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Interpreting deep embeddings for disease progression clustering. (arXiv:2307.06060v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30142;&#30149;&#36827;&#23637;&#32858;&#31867;&#20013;&#35299;&#35835;&#28145;&#24230;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;2&#22411;&#31958;&#23615;&#30149;&#21442;&#19982;&#32773;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#23545;&#30142;&#30149;&#36827;&#23637;&#27169;&#24335;&#30340;&#20020;&#24202;&#24847;&#20041;&#24615;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24739;&#32773;&#32858;&#31867;&#30340;&#32972;&#26223;&#19979;&#35299;&#35835;&#28145;&#24230;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#33521;&#22269;&#29983;&#29289;&#24211;&#30340;2&#22411;&#31958;&#23615;&#30149;&#21442;&#19982;&#32773;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20986;&#23545;&#30142;&#30149;&#36827;&#23637;&#27169;&#24335;&#30340;&#20020;&#24202;&#24847;&#20041;&#24615;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel approach for interpreting deep embeddings in the context of patient clustering. We evaluate our approach on a dataset of participants with type 2 diabetes from the UK Biobank, and demonstrate clinically meaningful insights into disease progression patterns.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#20845;&#31181;&#22522;&#20934;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#21644;&#19968;&#31181;&#26032;&#25552;&#20986;&#30340;&#22522;&#20110; GFlowNets &#30340;&#26041;&#27861;&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616; GFlowNets &#20855;&#26377;&#25429;&#25417;&#21508;&#31181;&#26377;&#29992;&#21644;&#22810;&#26679;&#30340;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.04988</link><description>&lt;p&gt;
&#29992;&#20110;&#19979;&#28216;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#36125;&#21494;&#26031;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Bayesian Causal Discovery Methods for Downstream Treatment Effect Estimation. (arXiv:2307.04988v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04988
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#20845;&#31181;&#22522;&#20934;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#21644;&#19968;&#31181;&#26032;&#25552;&#20986;&#30340;&#22522;&#20110; GFlowNets &#30340;&#26041;&#27861;&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616; GFlowNets &#20855;&#26377;&#25429;&#25417;&#21508;&#31181;&#26377;&#29992;&#21644;&#22810;&#26679;&#30340;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#20013;&#22240;&#26524;&#24615;&#30340;&#23454;&#38469;&#24212;&#29992;&#34987;&#24191;&#27867;&#35748;&#21487;&#65292;&#22240;&#26524;&#21457;&#29616;&#21644;&#25512;&#29702;&#22312;&#26412;&#36136;&#19978;&#26159;&#30456;&#20114;&#20132;&#32455;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30340;&#35780;&#20272;&#20013;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#36317;&#65292;&#23545;&#19979;&#28216;&#25512;&#29702;&#30340;&#37325;&#35270;&#31243;&#24230;&#19981;&#36275;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20845;&#31181;&#24050;&#24314;&#31435;&#30340;&#22522;&#20934;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#21644;&#19968;&#31181;&#22522;&#20110; GFlowNets &#30340;&#26032;&#26041;&#27861;&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#23454;&#26045;&#19968;&#20010;&#31283;&#20581;&#30340;&#35780;&#20272;&#36807;&#31243;&#65292;&#25105;&#20204;&#20026;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#36825;&#20123;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#32771;&#34385;&#20102;&#21512;&#25104;&#21644;&#30495;&#23454;&#22330;&#26223;&#20197;&#21450;&#20302;&#25968;&#25454;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GFlowNets &#20855;&#26377;&#26377;&#25928;&#25429;&#25417;&#21508;&#31181;&#26377;&#29992;&#21644;&#22810;&#26679;&#30340;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The practical utility of causality in decision-making is widely recognized, with causal discovery and inference being inherently intertwined. Nevertheless, a notable gap exists in the evaluation of causal discovery methods, where insufficient emphasis is placed on downstream inference. To address this gap, we evaluate six established baseline causal discovery methods and a newly proposed method based on GFlowNets, on the downstream task of treatment effect estimation. Through the implementation of a robust evaluation procedure, we offer valuable insights into the efficacy of these causal discovery methods for treatment effect estimation, considering both synthetic and real-world scenarios, as well as low-data scenarios. Furthermore, the results of our study demonstrate that GFlowNets possess the capability to effectively capture a wide range of useful and diverse ATE modes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#28857;&#20113;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#27169;&#22411;&#26469;&#36827;&#34892;&#37327;&#28909;&#22120;&#24555;&#36895;&#27169;&#25311;&#65292;&#21457;&#29616;&#28857;&#20113;&#26356;&#33258;&#28982;&#22320;&#34920;&#31034;&#20102;&#37327;&#28909;&#22120;&#28107;&#28020;&#65292;&#22788;&#29702;&#31232;&#30095;&#25968;&#25454;&#38598;&#26356;&#20248;&#31168;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#26356;&#32039;&#20945;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#25991;&#20214;&#36827;&#34892;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.04780</link><description>&lt;p&gt;
&#28857;&#20113;&#19982;&#22522;&#20110;&#22270;&#20687;&#30340;&#27169;&#22411;&#22312;&#37327;&#28909;&#22120;&#24555;&#36895;&#27169;&#25311;&#20013;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparison of Point Cloud and Image-based Models for Calorimeter Fast Simulation. (arXiv:2307.04780v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#28857;&#20113;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#27169;&#22411;&#26469;&#36827;&#34892;&#37327;&#28909;&#22120;&#24555;&#36895;&#27169;&#25311;&#65292;&#21457;&#29616;&#28857;&#20113;&#26356;&#33258;&#28982;&#22320;&#34920;&#31034;&#20102;&#37327;&#28909;&#22120;&#28107;&#28020;&#65292;&#22788;&#29702;&#31232;&#30095;&#25968;&#25454;&#38598;&#26356;&#20248;&#31168;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#26356;&#32039;&#20945;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#25991;&#20214;&#36827;&#34892;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#31867;&#21035;&#65292;&#24050;&#34987;&#35777;&#26126;&#33021;&#20934;&#30830;&#29983;&#25104;&#39640;&#32500;&#24230;&#30340;&#37327;&#28909;&#22120;&#25968;&#25454;&#38598;&#12290;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#29992;&#22270;&#20687;&#19982;3D&#20307;&#32032;&#34920;&#31034;&#21644;&#24314;&#27169;&#22797;&#26434;&#30340;&#37327;&#28909;&#22120;&#28107;&#28020;&#12290;&#28982;&#32780;&#65292;&#28857;&#20113;&#21487;&#33021;&#26159;&#37327;&#28909;&#22120;&#28107;&#28020;&#26356;&#33258;&#28982;&#30340;&#34920;&#31034;&#26041;&#24335;&#65292;&#23588;&#20854;&#26159;&#22312;&#20855;&#26377;&#39640;&#31890;&#24230;&#30340;&#37327;&#28909;&#22120;&#20013;&#12290;&#28857;&#20113;&#20445;&#30041;&#20102;&#21407;&#22987;&#27169;&#25311;&#30340;&#25152;&#26377;&#20449;&#24687;&#65292;&#26356;&#33258;&#28982;&#22320;&#22788;&#29702;&#31232;&#30095;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#26356;&#32039;&#20945;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#25991;&#20214;&#36827;&#34892;&#23454;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#29992;&#21516;&#19968;&#32452;&#37327;&#28909;&#22120;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#20102;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#30452;&#25509;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score based generative models are a new class of generative models that have been shown to accurately generate high dimensional calorimeter datasets. Recent advances in generative models have used images with 3D voxels to represent and model complex calorimeter showers. Point clouds, however, are likely a more natural representation of calorimeter showers, particularly in calorimeters with high granularity. Point clouds preserve all of the information of the original simulation, more naturally deal with sparse datasets, and can be implemented with more compact models and data files. In this work, two state-of-the-art score based models are trained on the same set of calorimeter simulation and directly compared.
&lt;/p&gt;</description></item><item><title>FreeDrag&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;DragGAN&#22312;&#28857;&#36861;&#36394;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#27169;&#26495;&#29305;&#24449;&#12289;&#32447;&#24615;&#25628;&#32034;&#21644;&#27169;&#31946;&#23450;&#20301;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#21644;&#39640;&#25928;&#30340;&#22522;&#20110;&#28857;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;</title><link>http://arxiv.org/abs/2307.04684</link><description>&lt;p&gt;
FreeDrag: &#28857;&#36861;&#36394;&#24182;&#19981;&#36866;&#29992;&#20110;&#20132;&#20114;&#24335;&#30340;&#22522;&#20110;&#28857;&#30340;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
FreeDrag: Point Tracking is Not What You Need for Interactive Point-based Image Editing. (arXiv:2307.04684v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04684
&lt;/p&gt;
&lt;p&gt;
FreeDrag&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;DragGAN&#22312;&#28857;&#36861;&#36394;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#27169;&#26495;&#29305;&#24449;&#12289;&#32447;&#24615;&#25628;&#32034;&#21644;&#27169;&#31946;&#23450;&#20301;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#21644;&#39640;&#25928;&#30340;&#22522;&#20110;&#28857;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#28385;&#36275;&#22270;&#20687;&#32534;&#36753;&#30340;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#38656;&#27714;&#65292;&#23545;&#22270;&#20687;&#20869;&#23481;&#30340;&#31934;&#30830;&#21644;&#28789;&#27963;&#30340;&#25805;&#32437;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#26368;&#36817;&#65292;DragGAN&#36890;&#36807;&#22522;&#20110;&#28857;&#30340;&#25805;&#32437;&#23454;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32534;&#36753;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;DragGAN&#22312;&#28857;&#30340;&#36861;&#36394;&#19978;&#23384;&#22312;&#22256;&#38590;&#65292;&#21253;&#25324;&#38169;&#35823;&#36861;&#36394;&#21644;&#27169;&#31946;&#36861;&#36394;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FreeDrag&#65292;&#23427;&#37319;&#29992;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;DragGAN&#20013;&#28857;&#36861;&#36394;&#30340;&#36127;&#25285;&#12290;FreeDrag&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;&#27169;&#26495;&#29305;&#24449;&#12289;&#32447;&#24615;&#25628;&#32034;&#21644;&#27169;&#31946;&#23450;&#20301;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#21644;&#39640;&#25928;&#30340;&#22522;&#20110;&#28857;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;DragGAN&#65292;&#24182;&#33021;&#22312;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#22256;&#38590;&#24773;&#26223;&#19979;&#23454;&#29616;&#31283;&#23450;&#30340;&#22522;&#20110;&#28857;&#30340;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
To serve the intricate and varied demands of image editing, precise and flexible manipulation of image content is indispensable. Recently, DragGAN has achieved impressive editing results through point-based manipulation. However, we have observed that DragGAN struggles with miss tracking, where DragGAN encounters difficulty in effectively tracking the desired handle points, and ambiguous tracking, where the tracked points are situated within other regions that bear resemblance to the handle points. To deal with the above issues, we propose FreeDrag, which adopts a feature-oriented approach to free the burden on point tracking within the point-oriented methodology of DragGAN. The FreeDrag incorporates adaptive template features, line search, and fuzzy localization techniques to perform stable and efficient point-based image editing. Extensive experiments demonstrate that our method is superior to the DragGAN and enables stable point-based editing in challenging scenarios with similar st
&lt;/p&gt;</description></item><item><title>Solvent&#26159;&#19968;&#20010;&#29992;&#20110;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#32479;&#19968;&#30740;&#31350;&#26694;&#26550;&#65292;&#25903;&#25345;&#26368;&#26032;&#27169;&#22411;&#37325;&#35201;&#32452;&#20214;&#30340;&#23454;&#29616;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#24314;&#27169;&#39046;&#22495;&#30340;&#26377;&#29992;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.04603</link><description>&lt;p&gt;
Solvent: &#19968;&#20010;&#29992;&#20110;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Solvent: A Framework for Protein Folding. (arXiv:2307.04603v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04603
&lt;/p&gt;
&lt;p&gt;
Solvent&#26159;&#19968;&#20010;&#29992;&#20110;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#32479;&#19968;&#30740;&#31350;&#26694;&#26550;&#65292;&#25903;&#25345;&#26368;&#26032;&#27169;&#22411;&#37325;&#35201;&#32452;&#20214;&#30340;&#23454;&#29616;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#24314;&#27169;&#39046;&#22495;&#30340;&#26377;&#29992;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33268;&#24615;&#21644;&#21487;&#38752;&#24615;&#23545;&#20110;&#36827;&#34892;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;&#35768;&#22810;&#33879;&#21517;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22914;&#30446;&#26631;&#26816;&#27979;&#65292;&#24050;&#32463;&#36890;&#36807;&#31283;&#23450;&#30340;&#22522;&#20934;&#26694;&#26550;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#39564;&#35777;&#12290;&#22312;AlphaFold2&#20043;&#21518;&#65292;&#34507;&#30333;&#36136;&#25240;&#21472;&#20219;&#21153;&#24050;&#32463;&#36827;&#20837;&#20102;&#19968;&#20010;&#26032;&#38454;&#27573;&#65292;&#35768;&#22810;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;AlphaFold2&#30340;&#32452;&#20214;&#25552;&#20986;&#30340;&#12290;&#22312;&#34507;&#30333;&#36136;&#25240;&#21472;&#20013;&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#30740;&#31350;&#26694;&#26550;&#30340;&#37325;&#35201;&#24615;&#21253;&#25324;&#23454;&#29616;&#21644;&#22522;&#20934;&#65292;&#20197;&#19968;&#33268;&#19988;&#20844;&#24179;&#22320;&#27604;&#36739;&#21508;&#31181;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Solvent&#65292;&#19968;&#20010;&#25903;&#25345;&#26368;&#26032;&#27169;&#22411;&#37325;&#35201;&#32452;&#20214;&#30340;&#34507;&#30333;&#36136;&#25240;&#21472;&#26694;&#26550;&#12290;Solvent&#21253;&#21547;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#20195;&#30721;&#24211;&#20013;&#23454;&#29616;&#30340;&#19981;&#21516;&#27169;&#22411;&#65292;&#24182;&#25903;&#25345;&#22312;&#30456;&#21516;&#25968;&#25454;&#38598;&#19978;&#23545;&#23450;&#20041;&#30340;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#23545;&#33879;&#21517;&#31639;&#27861;&#21450;&#20854;&#32452;&#20214;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#20197;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#24314;&#27169;&#39046;&#22495;&#25552;&#20379;&#26377;&#29992;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#24076;&#26395;Solvent&#33021;&#25552;&#39640;&#34507;&#30333;&#36136;&#25240;&#21472;&#30740;&#31350;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consistency and reliability are crucial for conducting AI research. Many famous research fields, such as object detection, have been compared and validated with solid benchmark frameworks. After AlphaFold2, the protein folding task has entered a new phase, and many methods are proposed based on the component of AlphaFold2. The importance of a unified research framework in protein folding contains implementations and benchmarks to consistently and fairly compare various approaches. To achieve this, we present Solvent, an protein folding framework that supports significant components of state-of-th-arts models in the manner of off-the-shelf interface Solvent contains different models implemented in a unified codebase and supports training and evaluation for defined models on the same dataset. We benchmark well-known algorithms and their components and provide experiments that give helpful insights into the protein structure modeling field. We hope that Solvent will increase the reliabili
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#34892;&#31243;&#26102;&#38388;&#23618;&#26512;&#25104;&#20687;&#26041;&#27861;&#65292;&#21033;&#29992;&#25935;&#24863;&#24615;&#20449;&#24687;&#30340;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#21644;&#28145;&#24230;&#29983;&#25104;&#32593;&#32476;&#65292;&#20197;&#22788;&#29702;&#22320;&#36136;&#22797;&#26434;&#24615;&#20808;&#39564;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.04228</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#34892;&#31243;&#26102;&#38388;&#23618;&#26512;&#25104;&#20687;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#25935;&#24863;&#24615;&#30340;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#21644;&#28145;&#24230;&#29983;&#25104;&#32593;&#32476;&#30340;&#22320;&#36136;&#22797;&#26434;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Efficient Bayesian travel-time tomography with geologically-complex priors using sensitivity-informed polynomial chaos expansion and deep generative networks. (arXiv:2307.04228v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#34892;&#31243;&#26102;&#38388;&#23618;&#26512;&#25104;&#20687;&#26041;&#27861;&#65292;&#21033;&#29992;&#25935;&#24863;&#24615;&#20449;&#24687;&#30340;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#21644;&#28145;&#24230;&#29983;&#25104;&#32593;&#32476;&#65292;&#20197;&#22788;&#29702;&#22320;&#36136;&#22797;&#26434;&#24615;&#20808;&#39564;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33945;&#29305;&#21345;&#27931;&#39532;&#23572;&#21487;&#22827;&#38142;&#65288;MCMC&#65289;&#26041;&#27861;&#24120;&#24120;&#38754;&#20020;&#20004;&#20010;&#26681;&#26412;&#24615;&#25361;&#25112;&#65306;&#20808;&#39564;&#20998;&#24067;&#30340;&#20934;&#30830;&#21051;&#30011;&#21644;&#20284;&#28982;&#20989;&#25968;&#30340;&#39640;&#25928;&#35780;&#20272;&#12290;&#22312;&#23618;&#26512;&#25104;&#20687;&#30340;&#36125;&#21494;&#26031;&#30740;&#31350;&#20013;&#65292;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#26041;&#20415;&#22320;&#23450;&#20041;&#20808;&#39564;&#20998;&#24067;&#65292;&#24182;&#21516;&#26102;&#20511;&#21161;&#22522;&#20110;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#65288;PCE&#65289;&#30340;&#20934;&#30830;&#20195;&#29702;&#27169;&#22411;&#26469;&#26367;&#20195;&#35745;&#31639;&#23494;&#38598;&#30340;&#20840;&#29289;&#29702;&#27491;&#21521;&#27714;&#35299;&#22120;&#12290;&#24403;PCA&#26080;&#27861;&#30452;&#25509;&#25552;&#20379;&#23450;&#20041;&#20808;&#39564;&#20998;&#24067;&#30340;&#26041;&#24335;&#26102;&#65292;&#21487;&#20197;&#37319;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;&#20363;&#22914;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#65289;&#31561;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#20135;&#29983;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;VAE&#30340;&#28508;&#22312;&#21442;&#25968;&#19982;&#27491;&#21521;&#24314;&#27169;&#36755;&#20986;&#20043;&#38388;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#20195;&#29702;&#27169;&#22411;&#26159;&#19968;&#20010;&#26174;&#33879;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monte Carlo Markov Chain (MCMC) methods commonly confront two fundamental challenges: the accurate characterization of the prior distribution and the efficient evaluation of the likelihood. In the context of Bayesian studies on tomography, principal component analysis (PCA) can in some cases facilitate the straightforward definition of the prior distribution, while simultaneously enabling the implementation of accurate surrogate models based on polynomial chaos expansion (PCE) to replace computationally intensive full-physics forward solvers. When faced with scenarios where PCA does not offer a direct means of easily defining the prior distribution alternative methods like deep generative models (e.g., variational autoencoders (VAEs)), can be employed as viable options. However, accurately producing a surrogate capable of capturing the intricate non-linear relationship between the latent parameters of a VAE and the outputs of forward modeling presents a notable challenge. Indeed, while
&lt;/p&gt;</description></item><item><title>Transformer&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#26159;&#22686;&#24378;&#35760;&#24518;&#33021;&#21147;&#32780;&#19981;&#26159;&#25913;&#36827;&#20449;&#29992;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2307.03864</link><description>&lt;p&gt;
&#20309;&#26102;&#20351;&#29992;Transformer&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21457;&#20809;&#65311;&#20174;&#35760;&#24518;&#21644;&#20449;&#29992;&#20998;&#37197;&#20013;&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;
When Do Transformers Shine in RL? Decoupling Memory from Credit Assignment. (arXiv:2307.03864v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03864
&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#26159;&#22686;&#24378;&#35760;&#24518;&#33021;&#21147;&#32780;&#19981;&#26159;&#25913;&#36827;&#20449;&#29992;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38754;&#20020;&#20004;&#20010;&#19981;&#21516;&#30340;&#25361;&#25112;&#65306;&#23398;&#20064;&#26377;&#25928;&#30340;&#36807;&#21435;&#21644;&#24403;&#21069;&#35266;&#27979;&#30340;&#34920;&#31034;&#65292;&#24182;&#30830;&#23450;&#34892;&#21160;&#22914;&#20309;&#24433;&#21709;&#26410;&#26469;&#30340;&#25910;&#30410;&#12290;&#36825;&#20004;&#20010;&#25361;&#25112;&#37117;&#28041;&#21450;&#21040;&#24314;&#27169;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;Transformer&#26550;&#26500;&#22312;&#35299;&#20915;&#28041;&#21450;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#26041;&#38754;&#38750;&#24120;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;Transformer&#22522;&#20110;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#24378;&#21170;&#30340;&#21407;&#22240;&#23578;&#19981;&#28165;&#26970;&#65306;&#26159;&#22240;&#20026;&#23427;&#20204;&#23398;&#20064;&#20102;&#26377;&#25928;&#30340;&#35760;&#24518;&#65292;&#36824;&#26159;&#22240;&#20026;&#23427;&#20204;&#25191;&#34892;&#20102;&#26377;&#25928;&#30340;&#20449;&#29992;&#20998;&#37197;&#65311;&#22312;&#24341;&#20837;&#35760;&#24518;&#38271;&#24230;&#21644;&#20449;&#29992;&#20998;&#37197;&#38271;&#24230;&#30340;&#24418;&#24335;&#23450;&#20041;&#20043;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31616;&#21333;&#30340;&#21487;&#37197;&#32622;&#20219;&#21153;&#26469;&#27979;&#37327;&#36825;&#20123;&#19981;&#21516;&#30340;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;Transformer&#21487;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#25193;&#23637;&#21040;&#38656;&#35201;&#35760;&#20303;1500&#27493;&#21069;&#35266;&#23519;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;Transformer&#26080;&#27861;&#25913;&#36827;&#38271;&#26399;&#30340;&#20449;&#29992;&#20998;&#37197;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;Transformer&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#35760;&#24518;&#21644;&#20449;&#29992;&#20998;&#37197;&#26041;&#38754;&#30340;&#19981;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) algorithms face two distinct challenges: learning effective representations of past and present observations, and determining how actions influence future returns. Both challenges involve modeling long-term dependencies. The transformer architecture has been very successful to solve problems that involve long-term dependencies, including in the RL domain. However, the underlying reason for the strong performance of Transformer-based RL methods remains unclear: is it because they learn effective memory, or because they perform effective credit assignment? After introducing formal definitions of memory length and credit assignment length, we design simple configurable tasks to measure these distinct quantities. Our empirical results reveal that Transformers can enhance the memory capacity of RL algorithms, scaling up to tasks that require memorizing observations $1500$ steps ago. However, Transformers do not improve long-term credit assignment. In summary, our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#26032;&#22411;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;CQL&#19982;&#22238;&#28335;&#36807;&#31243;&#30456;&#32467;&#21512;&#26469;&#25552;&#21462;&#31574;&#30053;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.02752</link><description>&lt;p&gt;
&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning with Imbalanced Datasets. (arXiv:2307.02752v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#26032;&#22411;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;CQL&#19982;&#22238;&#28335;&#36807;&#31243;&#30456;&#32467;&#21512;&#26469;&#25552;&#21462;&#31574;&#30053;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30740;&#31350;&#20013;&#23545;&#22522;&#20934;&#30340;&#26222;&#36941;&#20351;&#29992;&#23548;&#33268;&#20102;&#23545;&#23454;&#38469;&#25968;&#25454;&#38598;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#24573;&#35270;&#12290;&#30001;&#20110;&#25506;&#32034;&#25110;&#23433;&#20840;&#32771;&#34385;&#30340;&#25361;&#25112;&#65292;&#23454;&#38469;&#31163;&#32447;RL&#25968;&#25454;&#38598;&#22312;&#29366;&#24577;&#31354;&#38388;&#19978;&#36890;&#24120;&#26159;&#19981;&#24179;&#34913;&#30340;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#20855;&#20307;&#35828;&#26126;&#20102;&#31163;&#32447;RL&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#29305;&#24615;&#65292;&#20854;&#20013;&#29366;&#24577;&#35206;&#30422;&#29575;&#36981;&#24490;&#19968;&#20010;&#30001;&#20559;&#24577;&#31574;&#30053;&#25152;&#29305;&#24449;&#21270;&#30340;&#24130;&#24459;&#20998;&#24067;&#12290;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;&#20998;&#24067;&#32422;&#26463;&#30340;&#20856;&#22411;&#31163;&#32447;RL&#26041;&#27861;&#65292;&#22914;&#20445;&#23432;Q&#23398;&#20064;&#65288;CQL&#65289;&#65292;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19979;&#25552;&#21462;&#31574;&#30053;&#26159;&#26080;&#25928;&#30340;&#12290;&#21463;&#33258;&#28982;&#26234;&#33021;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;RL&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;CQL&#30340;&#22686;&#24378;&#19982;&#22238;&#28335;&#36807;&#31243;&#30456;&#32467;&#21512;&#65292;&#20197;&#22238;&#24518;&#20197;&#24448;&#30456;&#20851;&#32463;&#39564;&#65292;&#26377;&#25928;&#22320;&#32531;&#35299;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalent use of benchmarks in current offline reinforcement learning (RL) research has led to a neglect of the imbalance of real-world dataset distributions in the development of models. The real-world offline RL dataset is often imbalanced over the state space due to the challenge of exploration or safety considerations. In this paper, we specify properties of imbalanced datasets in offline RL, where the state coverage follows a power law distribution characterized by skewed policies. Theoretically and empirically, we show that typically offline RL methods based on distributional constraints, such as conservative Q-learning (CQL), are ineffective in extracting policies under the imbalanced dataset. Inspired by natural intelligence, we propose a novel offline RL method that utilizes the augmentation of CQL with a retrieval process to recall past related experiences, effectively alleviating the challenges posed by imbalanced datasets. We evaluate our method on several tasks in the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#32452;&#21512;&#24615;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#23618;&#27425;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#65292;&#21457;&#29616;&#28145;&#24230;CNN&#23398;&#20064;&#36825;&#20010;&#20219;&#21153;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;&#38543;&#30528;&#31867;&#21035;&#25968;&#12289;&#32452;&#21512;&#25968;&#21644;&#36845;&#20195;&#27425;&#25968;&#30340;&#22686;&#21152;&#32780;&#28176;&#36827;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2307.02129</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#23398;&#20064;&#32452;&#21512;&#24615;&#25968;&#25454;&#65306;&#38543;&#26426;&#23618;&#27425;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model. (arXiv:2307.02129v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#32452;&#21512;&#24615;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#23618;&#27425;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#65292;&#21457;&#29616;&#28145;&#24230;CNN&#23398;&#20064;&#36825;&#20010;&#20219;&#21153;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;&#38543;&#30528;&#31867;&#21035;&#25968;&#12289;&#32452;&#21512;&#25968;&#21644;&#36845;&#20195;&#27425;&#25968;&#30340;&#22686;&#21152;&#32780;&#28176;&#36827;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19968;&#33324;&#39640;&#32500;&#20219;&#21153;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#19982;&#32500;&#24230;&#25104;&#25351;&#25968;&#22686;&#38271;&#30340;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#25104;&#21151;&#12290;&#19968;&#31181;&#26222;&#36941;&#30340;&#20551;&#35774;&#26159;&#21487;&#23398;&#20064;&#20219;&#21153;&#20855;&#26377;&#39640;&#24230;&#32467;&#26500;&#21270;&#65292;CNN&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#24314;&#31435;&#20102;&#25968;&#25454;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#23427;&#20204;&#38656;&#35201;&#22810;&#23569;&#35757;&#32451;&#25968;&#25454;&#20197;&#21450;&#36825;&#20010;&#25968;&#23383;&#22914;&#20309;&#21462;&#20915;&#20110;&#25968;&#25454;&#32467;&#26500;&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#25991;&#22238;&#31572;&#20102;&#38024;&#23545;&#19968;&#20010;&#31616;&#21333;&#30340;&#20998;&#31867;&#20219;&#21153;&#30340;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#25429;&#25417;&#30495;&#23454;&#25968;&#25454;&#30340;&#30456;&#20851;&#26041;&#38754;&#65306;&#38543;&#26426;&#23618;&#27425;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;$n_c$&#20010;&#31867;&#21035;&#20013;&#30340;&#27599;&#19968;&#20010;&#23545;&#24212;&#20110;$m$&#20010;&#21516;&#20041;&#32452;&#21512;&#30340;&#39640;&#23618;&#27425;&#29305;&#24449;&#65292;&#24182;&#19988;&#36825;&#20123;&#29305;&#24449;&#21448;&#36890;&#36807;&#19968;&#20010;&#37325;&#22797;$L$&#27425;&#30340;&#36845;&#20195;&#36807;&#31243;&#30001;&#23376;&#29305;&#24449;&#32452;&#25104;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38656;&#35201;&#28145;&#24230;CNN&#23398;&#20064;&#36825;&#20010;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;$P^*$&#65288;i&#65289;&#38543;&#30528;$n_c m^L$&#30340;&#22686;&#38271;&#32780;&#28176;&#36827;&#22320;&#22686;&#38271;&#65292;&#36825;&#21482;&#26377;...
&lt;/p&gt;
&lt;p&gt;
Learning generic high-dimensional tasks is notably hard, as it requires a number of training data exponential in the dimension. Yet, deep convolutional neural networks (CNNs) have shown remarkable success in overcoming this challenge. A popular hypothesis is that learnable tasks are highly structured and that CNNs leverage this structure to build a low-dimensional representation of the data. However, little is known about how much training data they require, and how this number depends on the data structure. This paper answers this question for a simple classification task that seeks to capture relevant aspects of real data: the Random Hierarchy Model. In this model, each of the $n_c$ classes corresponds to $m$ synonymic compositions of high-level features, which are in turn composed of sub-features through an iterative process repeated $L$ times. We find that the number of training data $P^*$ required by deep CNNs to learn this task (i) grows asymptotically as $n_c m^L$, which is only
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#35268;&#27169;&#26080;&#30417;&#30563;&#38899;&#39057;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35270;&#39057;&#21040;&#35821;&#38899;&#21512;&#25104;&#12290;&#36890;&#36807;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#35270;&#39057;&#23545;&#24212;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#20016;&#23500;&#30340;&#20165;&#38899;&#39057;&#25968;&#25454;&#38598;&#36827;&#34892;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.15464</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#26080;&#30417;&#30563;&#38899;&#39057;&#39044;&#35757;&#32451;&#29992;&#20110;&#35270;&#39057;&#21040;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Large-scale unsupervised audio pre-training for video-to-speech synthesis. (arXiv:2306.15464v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#35268;&#27169;&#26080;&#30417;&#30563;&#38899;&#39057;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35270;&#39057;&#21040;&#35821;&#38899;&#21512;&#25104;&#12290;&#36890;&#36807;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#35270;&#39057;&#23545;&#24212;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#20016;&#23500;&#30340;&#20165;&#38899;&#39057;&#25968;&#25454;&#38598;&#36827;&#34892;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#21040;&#35821;&#38899;&#21512;&#25104;&#26159;&#20174;&#26080;&#22768;&#35270;&#39057;&#20013;&#37325;&#24314;&#35821;&#38899;&#20449;&#21495;&#30340;&#20219;&#21153;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#24050;&#24314;&#31435;&#30340;&#26041;&#27861;&#37117;&#37319;&#29992;&#20102;&#19968;&#20010;&#20004;&#27493;&#27861;&#65292;&#39318;&#20808;&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#20013;&#38388;&#34920;&#31034;&#65292;&#22914;&#35889;&#22270;&#65292;&#28982;&#21518;&#20256;&#36882;&#32473;&#22768;&#30721;&#22120;&#29983;&#25104;&#21407;&#22987;&#38899;&#39057;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#24037;&#20316;&#19987;&#27880;&#20110;&#31471;&#21040;&#31471;&#21512;&#25104;&#65292;&#21363;&#21516;&#26102;&#29983;&#25104;&#21407;&#22987;&#38899;&#39057;&#21644;&#20219;&#20309;&#20013;&#38388;&#34920;&#31034;&#12290;&#25152;&#26377;&#36825;&#20123;&#26041;&#27861;&#37117;&#38656;&#35201;&#22312;&#20960;&#20046;&#23436;&#20840;&#26159;&#38899;&#39057;-&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21363;&#27599;&#20010;&#38899;&#39057;&#26679;&#26412;&#37117;&#26377;&#23545;&#24212;&#30340;&#35270;&#39057;&#26679;&#26412;&#12290;&#36825;&#25490;&#38500;&#20102;&#20351;&#29992;&#20016;&#23500;&#30340;&#20165;&#38899;&#39057;&#25968;&#25454;&#38598;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#21487;&#33021;&#27809;&#26377;&#23545;&#24212;&#30340;&#35270;&#35273;&#27169;&#24577;&#65288;&#20363;&#22914;&#26377;&#22768;&#35835;&#29289;&#12289;&#24191;&#25773;&#25773;&#23458;&#12289;&#35821;&#38899;&#35782;&#21035;&#25968;&#25454;&#38598;&#31561;&#65289;&#65292;&#20197;&#21450;&#22810;&#24180;&#26469;&#30001;&#38899;&#39057;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#24320;&#21457;&#30340;&#20165;&#38899;&#39057;&#26550;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#36229;&#36807;3500&#23567;&#26102;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video-to-speech synthesis is the task of reconstructing the speech signal from a silent video of a speaker. Most established approaches to date involve a two-step process, whereby an intermediate representation from the video, such as a spectrogram, is extracted first and then passed to a vocoder to produce the raw audio. Some recent work has focused on end-to-end synthesis, whereby the generation of raw audio and any intermediate representations is performed jointly. All such approaches involve training on data from almost exclusively audio-visual datasets, i.e. every audio sample has a corresponding video sample. This precludes the use of abundant audio-only datasets which may not have a corresponding visual modality (e.g. audiobooks, radio podcasts, speech recognition datasets etc.), as well as audio-only architectures that have been developed by the audio machine learning community over the years. In this paper we propose to train encoder-decoder models on more than 3,500 hours of 
&lt;/p&gt;</description></item><item><title>G-NM&#26159;&#19968;&#32452;&#38598;&#21512;&#20102;&#20256;&#32479;&#21644;&#29616;&#20195;&#27169;&#22411;&#30340;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#20013;&#30340;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.11667</link><description>&lt;p&gt;
G-NM&#65306;&#19968;&#32452;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
G-NM: A Group of Numerical Time Series Prediction Models. (arXiv:2306.11667v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11667
&lt;/p&gt;
&lt;p&gt;
G-NM&#26159;&#19968;&#32452;&#38598;&#21512;&#20102;&#20256;&#32479;&#21644;&#29616;&#20195;&#27169;&#22411;&#30340;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#20013;&#30340;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#24320;&#21457;&#21644;&#23454;&#26045;&#19968;&#20010;&#32508;&#21512;&#30340;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#38598;&#21512;&#65292;&#32479;&#31216;&#20026;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#32452;&#65288;G-NM&#65289;&#12290;&#35813;&#38598;&#21512;&#21253;&#25324;&#20256;&#32479;&#27169;&#22411;&#22914;&#33258;&#22238;&#24402;&#32508;&#21512;&#31227;&#21160;&#24179;&#22343;&#65288;ARIMA&#65289;&#12289;Holt-Winters&#26041;&#27861;&#21644;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#65288;SVR&#65289;&#65292;&#20197;&#21450;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#12290;G-NM&#26126;&#30830;&#26500;&#24314;&#20197;&#22686;&#24378;&#25105;&#20204;&#23545;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#20013;&#22266;&#26377;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#19982;&#36825;&#20123;&#20107;&#20214;&#30456;&#20851;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;G-NM&#20415;&#20110;&#23545;&#27492;&#31867;&#29616;&#35937;&#22312;&#24310;&#38271;&#26102;&#38388;&#27573;&#20869;&#36827;&#34892;&#39044;&#27979;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25512;&#36827;&#25105;&#20204;&#23545;&#27492;&#31867;&#20107;&#20214;&#30340;&#29702;&#35299;&#65292;&#24182;&#22823;&#24133;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;G-NM&#21253;&#25324;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#21450;&#23395;&#33410;&#24615;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we focus on the development and implementation of a comprehensive ensemble of numerical time series forecasting models, collectively referred to as the Group of Numerical Time Series Prediction Model (G-NM). This inclusive set comprises traditional models such as Autoregressive Integrated Moving Average (ARIMA), Holt-Winters' method, and Support Vector Regression (SVR), in addition to modern neural network models including Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM). G-NM is explicitly constructed to augment our predictive capabilities related to patterns and trends inherent in complex natural phenomena. By utilizing time series data relevant to these events, G-NM facilitates the prediction of such phenomena over extended periods. The primary objective of this research is to both advance our understanding of such occurrences and to significantly enhance the accuracy of our forecasts. G-NM encapsulates both linear and non-linear dependencies, seasonal
&lt;/p&gt;</description></item><item><title>&#23631;&#34109;&#20132;&#21449;&#39564;&#35777;&#65288;BCV&#65289;&#26159;&#19968;&#31181;&#20934;&#30830;&#39640;&#25928;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#37325;&#22797;&#20132;&#21449;&#39564;&#35777;&#65288;RCV&#65289;&#65292;BCV&#21487;&#20197;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#35823;&#24046;&#20272;&#35745;&#32467;&#26524;&#65292;&#19988;&#36816;&#34892;&#27425;&#25968;&#26356;&#23569;&#12290;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BCV&#22312;&#36229;&#21442;&#25968;&#35843;&#20248;&#20219;&#21153;&#20013;&#20248;&#20110;RCV&#65292;&#33021;&#22815;&#20197;&#26356;&#23569;&#30340;&#35745;&#31639;&#23454;&#29616;&#26356;&#39640;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.06591</link><description>&lt;p&gt;
&#23631;&#34109;&#20132;&#21449;&#39564;&#35777;&#65306;&#19968;&#31181;&#20934;&#30830;&#39640;&#25928;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Blocked Cross-Validation: A Precise and Efficient Method for Hyperparameter Tuning. (arXiv:2306.06591v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06591
&lt;/p&gt;
&lt;p&gt;
&#23631;&#34109;&#20132;&#21449;&#39564;&#35777;&#65288;BCV&#65289;&#26159;&#19968;&#31181;&#20934;&#30830;&#39640;&#25928;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#37325;&#22797;&#20132;&#21449;&#39564;&#35777;&#65288;RCV&#65289;&#65292;BCV&#21487;&#20197;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#35823;&#24046;&#20272;&#35745;&#32467;&#26524;&#65292;&#19988;&#36816;&#34892;&#27425;&#25968;&#26356;&#23569;&#12290;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BCV&#22312;&#36229;&#21442;&#25968;&#35843;&#20248;&#20219;&#21153;&#20013;&#20248;&#20110;RCV&#65292;&#33021;&#22815;&#20197;&#26356;&#23569;&#30340;&#35745;&#31639;&#23454;&#29616;&#26356;&#39640;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#35843;&#25972;&#22312;&#20248;&#21270;&#39044;&#27979;&#23398;&#20064;&#22120;&#30340;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20132;&#21449;&#39564;&#35777;&#65288;CV&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#20272;&#35745;&#19981;&#21516;&#36229;&#21442;&#25968;&#35774;&#32622;&#30340;&#35823;&#24046;&#12290;&#37325;&#22797;&#20132;&#21449;&#39564;&#35777;&#65288;RCV&#65289;&#24120;&#29992;&#20110;&#20943;&#23569;CV&#35823;&#24046;&#30340;&#21464;&#21270;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#23631;&#34109;&#20132;&#21449;&#39564;&#35777;&#65288;BCV&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#20013;&#37325;&#22797;&#27425;&#25968;&#30456;&#23545;&#20110;CV&#20998;&#21306;&#21644;&#23398;&#20064;&#22120;&#30340;&#38543;&#26426;&#34892;&#20026;&#36827;&#34892;&#20102;&#38459;&#22622;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;RCV&#30456;&#27604;&#65292;BCV&#21487;&#20197;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#35823;&#24046;&#20272;&#35745;&#32467;&#26524;&#65292;&#21363;&#20351;&#36816;&#34892;&#27425;&#25968;&#26174;&#33879;&#20943;&#23569;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#22823;&#37327;&#31034;&#20363;&#65292;&#23637;&#31034;&#20102;BCV&#22312;&#36229;&#21442;&#25968;&#35843;&#20248;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;BCV&#22312;&#36229;&#21442;&#25968;&#35843;&#20248;&#20013;&#20248;&#20110;RCV&#65292;&#21487;&#20197;&#20197;&#26356;&#23569;&#30340;&#35745;&#31639;&#23454;&#29616;&#26356;&#39640;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter tuning plays a crucial role in optimizing the performance of predictive learners. Cross--validation (CV) is a widely adopted technique for estimating the error of different hyperparameter settings. Repeated cross-validation (RCV) has been commonly employed to reduce the variability of CV errors. In this paper, we introduce a novel approach called blocked cross-validation (BCV), where the repetitions are blocked with respect to both CV partition and the random behavior of the learner. Theoretical analysis and empirical experiments demonstrate that BCV provides more precise error estimates compared to RCV, even with a significantly reduced number of runs. We present extensive examples using real--world data sets to showcase the effectiveness and efficiency of BCV in hyperparameter tuning. Our results indicate that BCV outperforms RCV in hyperparameter tuning, achieving greater precision with fewer computations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#38543;&#26426;&#25237;&#24433;&#25216;&#26415;&#20248;&#21270;&#20102;Koopman&#31639;&#23376;&#30340;&#20272;&#35745;&#22120;&#65292;&#21152;&#24555;&#20102;&#35745;&#31639;&#36895;&#24230;&#65292;&#24182;&#32473;&#20986;&#20102;&#31934;&#30830;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04520</link><description>&lt;p&gt;
&#21033;&#29992;&#33609;&#22270;&#25216;&#26415;&#20272;&#35745;Koopman&#31639;&#23376;&#24182;&#21487;&#38752;&#22320;&#23398;&#20064;&#22823;&#35268;&#27169;&#21160;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Estimating Koopman operators with sketching to provably learn large scale dynamical systems. (arXiv:2306.04520v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#38543;&#26426;&#25237;&#24433;&#25216;&#26415;&#20248;&#21270;&#20102;Koopman&#31639;&#23376;&#30340;&#20272;&#35745;&#22120;&#65292;&#21152;&#24555;&#20102;&#35745;&#31639;&#36895;&#24230;&#65292;&#24182;&#32473;&#20986;&#20102;&#31934;&#30830;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Koopman&#31639;&#23376;&#29702;&#35770;&#20801;&#35768;&#20351;&#29992;&#38750;&#21442;&#25968;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#39044;&#27979;&#21644;&#20998;&#26512;&#22797;&#26434;&#30340;&#21160;&#24577;&#31995;&#32479;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#38543;&#26426;&#25237;&#24433;&#65288;&#33609;&#22270;&#25216;&#26415;&#65289;&#25552;&#39640;&#22522;&#20110;&#26680;&#30340;Koopman&#31639;&#23376;&#20272;&#35745;&#22120;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#22823;&#35268;&#27169;&#20998;&#23376;&#21160;&#21147;&#23398;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#24182;&#24314;&#31435;&#20102;&#38750;&#28176;&#36827;&#35823;&#24046;&#30028;&#65292;&#32473;&#20986;&#20102;&#32479;&#35745;&#23398;&#20064;&#36895;&#29575;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#30340;&#31934;&#30830;&#21051;&#30011;&#12290;&#25105;&#20204;&#30340;&#32463;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#32463;&#36807;&#25913;&#36827;&#30340;&#20272;&#35745;&#22120;&#22312;&#20445;&#35777;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#22823;&#22823;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The theory of Koopman operators allows to deploy non-parametric machine learning algorithms to predict and analyze complex dynamical systems. Estimators such as principal component regression (PCR) or reduced rank regression (RRR) in kernel spaces can be shown to provably learn Koopman operators from finite empirical observations of the system's time evolution. Scaling these approaches to very long trajectories is a challenge and requires introducing suitable approximations to make computations feasible. In this paper, we boost the efficiency of different kernel-based Koopman operator estimators using random projections (sketching). We derive, implement and test the new "sketched" estimators with extensive experiments on synthetic and large-scale molecular dynamics datasets. Further, we establish non asymptotic error bounds giving a sharp characterization of the trade-offs between statistical learning rates and computational efficiency. Our empirical and theoretical analysis shows that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#20195;&#34920;&#24615;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#22797;&#26434;&#22270;&#20687;&#25968;&#25454;&#20013;&#30340;&#32858;&#31867;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2306.00011</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#26041;&#27861;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#32858;&#31867;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Self-Supervised Approach for Cluster Assessment of High-Dimensional Data. (arXiv:2306.00011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#20195;&#34920;&#24615;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#22797;&#26434;&#22270;&#20687;&#25968;&#25454;&#20013;&#30340;&#32858;&#31867;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#25968;&#25454;&#38598;&#20013;&#32858;&#31867;&#25968;&#37327;&#21644;&#22522;&#30784;&#32858;&#31867;&#32467;&#26500;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25968;&#25454;&#36890;&#24120;&#26159;&#26080;&#26631;&#31614;&#12289;&#22797;&#26434;&#19988;&#39640;&#32500;&#30340;&#65292;&#36825;&#20351;&#24471;&#20256;&#32479;&#32858;&#31867;&#31639;&#27861;&#38590;&#20197;&#34920;&#29616;&#33391;&#22909;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22797;&#26434;&#22270;&#20687;&#25968;&#25454;&#20013;&#30340;&#32858;&#31867;&#32467;&#26500;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#33258;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#22797;&#26434;&#25968;&#25454;&#30340;&#20195;&#34920;&#24615;&#23884;&#20837;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#20302;&#32500;&#23884;&#20837;&#39304;&#36865;&#21040;VAT/iVAT&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the number of clusters and underlying cluster structure in a dataset is a crucial task. Real-world data are often unlabeled, complex and high-dimensional, which makes it difficult for traditional clustering algorithms to perform well. In recent years, a matrix reordering based algorithm, called "visual assessment of tendency" (VAT), and its variants have attracted many researchers from various domains to estimate the number of clusters and inherent cluster structure present in the data. However, these algorithms fail when applied to high-dimensional data due to the curse of dimensionality, as they rely heavily on the notions of closeness and farness between data points. To address this issue, we propose a deep-learning based framework for cluster structure assessment in complex, image datasets. First, our framework generates representative embeddings for complex data using a self-supervised deep neural network, and then, these low-dimensional embeddings are fed to VAT/iVAT a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;Med-DDPM&#65292;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#21270;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#25511;&#21046;&#20687;&#32032;&#32423;&#25513;&#30721;&#26631;&#31614;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#36924;&#30495;&#30340;&#21307;&#23398;&#22270;&#20687;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;GAN&#25216;&#26415;&#65292;&#20063;&#20248;&#20110;&#20256;&#32479;&#30340;&#22686;&#24378;&#25216;&#26415;&#21644;GAN&#21512;&#25104;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2305.18453</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#21270;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Conditional Diffusion Models for Semantic 3D Medical Image Synthesis. (arXiv:2305.18453v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18453
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;Med-DDPM&#65292;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#21270;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#25511;&#21046;&#20687;&#32032;&#32423;&#25513;&#30721;&#26631;&#31614;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#36924;&#30495;&#30340;&#21307;&#23398;&#22270;&#20687;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;GAN&#25216;&#26415;&#65292;&#20063;&#20248;&#20110;&#20256;&#32479;&#30340;&#22686;&#24378;&#25216;&#26415;&#21644;GAN&#21512;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Med-DDPM&#65292;&#23427;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#21270;&#30340;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#65292;&#35299;&#20915;&#20102;&#21307;&#23398;&#25104;&#20687;&#20013;&#25968;&#25454;&#31232;&#32570;&#12289;&#37319;&#38598;&#26041;&#27861;&#19981;&#19968;&#33268;&#21644;&#38544;&#31169;&#38382;&#39064;&#31561;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#37117;&#36229;&#36807;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#12290;Med-DDPM&#30340;&#29420;&#29305;&#29305;&#28857;&#22312;&#20110;&#20351;&#29992;&#35821;&#20041;&#26465;&#20214;&#36827;&#34892;&#19977;&#32500;&#22270;&#20687;&#21512;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#36890;&#36807;&#25511;&#21046;&#20687;&#32032;&#32423;&#25513;&#30721;&#26631;&#31614;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#23427;&#20415;&#20110;&#21019;&#24314;&#36924;&#30495;&#30340;&#21307;&#23398;&#22270;&#20687;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;Med-DDPM&#22312;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;GAN&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;Med-DDPM&#22312;&#22686;&#24378;&#20998;&#21106;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#20063;&#20248;&#20110;&#20256;&#32479;&#30340;&#22686;&#24378;&#25216;&#26415;&#21644;GAN&#21512;&#25104;&#22270;&#20687;&#12290;&#23427;&#35299;&#20915;&#20102;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#38590;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Med-DDPM, an innovative solution using diffusion models for semantic 3D medical image synthesis, addressing the prevalent issues in medical imaging such as data scarcity, inconsistent acquisition methods, and privacy concerns. Experimental evidence illustrates that diffusion models surpass Generative Adversarial Networks (GANs) in stability and performance, generating high-quality, realistic 3D medical images. The distinct feature of Med-DDPM is its use of semantic conditioning for the diffusion model in 3D image synthesis. By controlling the generation process through pixel-level mask labels, it facilitates the creation of realistic medical images. Empirical evaluations underscore the superior performance of Med-DDPM over GAN techniques in metrics such as accuracy, stability, and versatility. Furthermore, Med-DDPM outperforms traditional augmentation techniques and synthetic GAN images in enhancing the accuracy of segmentation models. It addresses challenges such
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#27492;&#30740;&#31350;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.17760</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;
&lt;/p&gt;
&lt;p&gt;
Language Models are Bounded Pragmatic Speakers. (arXiv:2305.17760v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#27492;&#30740;&#31350;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Ouyang&#31561;&#20154;&#65292;2022&#65289;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#65288;Kahneman&#65292;2011&#65289;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24515;&#29702;&#23398;&#23478;&#20204;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#25193;&#23637;&#36825;&#20010;&#26694;&#26550;&#30340;&#36884;&#24452;&#12290;&#26412;&#30740;&#31350;&#23454;&#36136;&#19978;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#26469;&#33719;&#24471;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#26041;&#38754;&#30340;&#28145;&#21051;&#35265;&#35299;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do language models "think"? This paper formulates a probabilistic cognitive model called the bounded pragmatic speaker, which can characterize the operation of different variations of language models. Specifically, we demonstrate that large language models fine-tuned with reinforcement learning from human feedback (Ouyang et al., 2022) embody a model of thought that conceptually resembles a fast-and-slow model (Kahneman, 2011), which psychologists have attributed to humans. We discuss the limitations of reinforcement learning from human feedback as a fast-and-slow model of thought and propose avenues for expanding this framework. In essence, our research highlights the value of adopting a cognitive probabilistic modeling approach to gain insights into the comprehension, evaluation, and advancement of language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26032;&#22411;&#25955;&#24230;&#8212;&#8212;&#34920;&#31034;Jensen-Shannon&#25955;&#24230;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#20998;&#24067;&#23884;&#20837;&#21040;RKHS&#20013;&#65292;&#24182;&#21033;&#29992;&#34920;&#31034;&#30340;&#21327;&#26041;&#24046;&#31639;&#23376;&#30340;&#39057;&#35889;&#65292;&#23454;&#29616;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#20272;&#35745;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#21487;&#25193;&#23637;&#24615;&#65292;&#21487;&#24494;&#20998;&#24615;&#30340;&#32463;&#39564;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#20989;&#25968;&#21644;&#22522;&#20110;&#26680;&#30697;&#38453;&#30340;&#20272;&#35745;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.16446</link><description>&lt;p&gt;
&#22522;&#20110;&#34920;&#31034;&#30340;Jensen-Shannon&#25955;&#24230;
&lt;/p&gt;
&lt;p&gt;
The Representation Jensen-Shannon Divergence. (arXiv:2305.16446v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26032;&#22411;&#25955;&#24230;&#8212;&#8212;&#34920;&#31034;Jensen-Shannon&#25955;&#24230;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#20998;&#24067;&#23884;&#20837;&#21040;RKHS&#20013;&#65292;&#24182;&#21033;&#29992;&#34920;&#31034;&#30340;&#21327;&#26041;&#24046;&#31639;&#23376;&#30340;&#39057;&#35889;&#65292;&#23454;&#29616;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#20272;&#35745;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#21487;&#25193;&#23637;&#24615;&#65292;&#21487;&#24494;&#20998;&#24615;&#30340;&#32463;&#39564;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#20989;&#25968;&#21644;&#22522;&#20110;&#26680;&#30697;&#38453;&#30340;&#20272;&#35745;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#25955;&#24230;&#37327;&#21270;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#37325;&#35201;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#25968;&#25454;&#30340;&#24213;&#23618;&#20998;&#24067;&#36890;&#24120;&#26410;&#30693;&#65292;&#20174;&#32463;&#39564;&#26679;&#26412;&#20013;&#20272;&#35745;&#25955;&#24230;&#26159;&#19968;&#20010;&#22522;&#26412;&#38590;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;(RKHS)&#20013;&#21327;&#26041;&#24046;&#31639;&#23376;&#30340;&#26032;&#22411;&#25955;&#24230;&#8212;&#8212;&#34920;&#31034;Jensen-Shannon&#25955;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#25968;&#25454;&#20998;&#24067;&#23884;&#20837;&#21040;RKHS&#20013;&#65292;&#24182;&#21033;&#29992;&#34920;&#31034;&#30340;&#21327;&#26041;&#24046;&#31639;&#23376;&#30340;&#39057;&#35889;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20174;&#32463;&#39564;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#30340;&#20272;&#35745;&#20989;&#25968;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;Fourier&#29305;&#24449;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;RKHS&#20013;&#12290;&#27492;&#20272;&#35745;&#20989;&#25968;&#26159;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#12289;&#21487;&#24494;&#20998;&#30340;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#23567;&#25209;&#37327;&#20248;&#21270;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30697;&#38453;&#30340;&#20272;&#35745;&#20989;&#25968;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;RKHS&#36827;&#34892;&#26174;&#24335;&#26144;&#23556;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#37327;&#26159;Jensen-Shannon&#25955;&#24230;&#30340;&#19968;&#20010;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical divergences quantify the difference between probability distributions finding multiple uses in machine-learning. However, a fundamental challenge is to estimate divergence from empirical samples since the underlying distributions of the data are usually unknown. In this work, we propose the representation Jensen-Shannon Divergence, a novel divergence based on covariance operators in reproducing kernel Hilbert spaces (RKHS). Our approach embeds the data distributions in an RKHS and exploits the spectrum of the covariance operators of the representations. We provide an estimator from empirical covariance matrices by explicitly mapping the data to an RKHS using Fourier features. This estimator is flexible, scalable, differentiable, and suitable for minibatch-based optimization problems. Additionally, we provide an estimator based on kernel matrices without having an explicit mapping to the RKHS. We show that this quantity is a lower bound on the Jensen-Shannon divergence, and 
&lt;/p&gt;</description></item><item><title>Mesh2SSM&#26159;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#25490;&#21015;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#27169;&#26495;&#28857;&#20113;&#21464;&#24418;&#20026;&#29305;&#23450;&#20027;&#20307;&#30340;&#32593;&#26684;&#65292;&#24418;&#25104;&#22522;&#20110;&#23545;&#24212;&#20851;&#31995;&#30340;&#35299;&#21078;&#23398;&#32479;&#35745;&#24418;&#24577;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.07805</link><description>&lt;p&gt;
Mesh2SSM: &#20174;&#34920;&#38754;&#32593;&#26684;&#21040;&#35299;&#21078;&#23398;&#32479;&#35745;&#24418;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy. (arXiv:2305.07805v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07805
&lt;/p&gt;
&lt;p&gt;
Mesh2SSM&#26159;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#25490;&#21015;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#27169;&#26495;&#28857;&#20113;&#21464;&#24418;&#20026;&#29305;&#23450;&#20027;&#20307;&#30340;&#32593;&#26684;&#65292;&#24418;&#25104;&#22522;&#20110;&#23545;&#24212;&#20851;&#31995;&#30340;&#35299;&#21078;&#23398;&#32479;&#35745;&#24418;&#24577;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#24418;&#24577;&#24314;&#27169;&#26159;&#20174;&#21307;&#23398;&#22270;&#20687;&#65288;&#22914;MRI&#21644;CT&#25195;&#25551;&#65289;&#20013;&#25429;&#33719;&#30340;&#20998;&#21106;&#35299;&#21078;&#32467;&#26500;&#20013;&#21457;&#29616;&#26174;&#33879;&#24418;&#24577;&#21442;&#25968;&#30340;&#35745;&#31639;&#36807;&#31243;&#65292;&#21487;&#20840;&#38754;&#25551;&#36848;&#31181;&#32676;&#20013;&#29305;&#23450;&#20027;&#20307;&#30340;&#35299;&#21078;&#23398;&#12290;&#30001;&#20110;&#20154;&#20307;&#35299;&#21078;&#32467;&#26500;&#20013;&#23384;&#22312;&#22823;&#37327;&#30340;&#38750;&#32447;&#24615;&#21464;&#24322;&#65292;&#20256;&#32479;&#30340;&#24418;&#24577;&#24314;&#27169;&#36807;&#31243;&#24120;&#24120;&#38754;&#20020;&#25361;&#25112;&#12290;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#23398;&#20064;&#21040;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#24418;&#24577;&#34920;&#31034;&#65292;&#24182;&#29983;&#25104;&#26356;&#24544;&#23454;&#20110;&#22522;&#30784;&#31181;&#32676;&#21464;&#24322;&#30340;&#32479;&#35745;&#24418;&#24577;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#38656;&#35201;&#24050;&#24314;&#31435;/&#20248;&#21270;&#30340;&#24418;&#24577;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Mesh2SSM&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#30417;&#30563;&#12289;&#25490;&#21015;&#19981;&#21464;&#30340;&#34920;&#31034;&#23398;&#20064;&#26469;&#20272;&#35745;&#22914;&#20309;&#23558;&#27169;&#26495;&#28857;&#20113;&#21464;&#24418;&#20026;&#29305;&#23450;&#20027;&#20307;&#30340;&#32593;&#26684;&#65292;&#24418;&#25104;&#22522;&#20110;&#23545;&#24212;&#20851;&#31995;&#30340;&#24418;&#24577;&#27169;&#22411;&#12290;Mesh2SSM&#36824;&#21487;&#20197;&#23398;&#20064;&#31181;&#32676;&#29305;&#23450;&#30340;&#27169;&#26495;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#25968;&#25454;&#23545;&#40784;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical shape modeling is the computational process of discovering significant shape parameters from segmented anatomies captured by medical images (such as MRI and CT scans), which can fully describe subject-specific anatomy in the context of a population. The presence of substantial non-linear variability in human anatomy often makes the traditional shape modeling process challenging. Deep learning techniques can learn complex non-linear representations of shapes and generate statistical shape models that are more faithful to the underlying population-level variability. However, existing deep learning models still have limitations and require established/optimized shape models for training. We propose Mesh2SSM, a new approach that leverages unsupervised, permutation-invariant representation learning to estimate how to deform a template point cloud to subject-specific meshes, forming a correspondence-based shape model. Mesh2SSM can also learn a population-specific template, reduci
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#34920;&#31034;&#31574;&#30053;&#8212;&#8212;&#31227;&#20301;&#27861;&#65292;&#23558;&#36807;&#21435;&#25968;&#25454;&#21644;&#26410;&#26469;&#21327;&#21464;&#37327;&#34701;&#21512;&#36215;&#26469;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#24182;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#27169;&#22411;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04876</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#24179;&#34892;RCNN&#19982;&#26032;&#39062;&#29305;&#24449;&#34920;&#31034;&#27861;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explainable Parallel RCNN with Novel Feature Representation for Time Series Forecasting. (arXiv:2305.04876v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#34920;&#31034;&#31574;&#30053;&#8212;&#8212;&#31227;&#20301;&#27861;&#65292;&#23558;&#36807;&#21435;&#25968;&#25454;&#21644;&#26410;&#26469;&#21327;&#21464;&#37327;&#34701;&#21512;&#36215;&#26469;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#24182;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#27169;&#22411;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#22522;&#26412;&#25361;&#25112;&#12290;&#24448;&#24448;&#20250;&#21463;&#21040;&#22806;&#37096;&#21327;&#21464;&#37327;&#65292;&#20363;&#22914;&#22825;&#27668;&#25110;&#20154;&#31867;&#24178;&#39044;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#24433;&#21709;&#21487;&#20197;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#34987;&#21512;&#29702;&#22320;&#39044;&#27979;&#12290;&#25105;&#20204;&#23558;&#23427;&#20204;&#31216;&#20026;&#39044;&#27979;&#30340;&#26410;&#26469;&#21327;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23581;&#35797;&#20351;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#36845;&#20195;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#30340;&#26041;&#27861;&#26368;&#32456;&#20250;&#23548;&#33268;&#25351;&#25968;&#32423;&#30340;&#35823;&#24046;&#32047;&#31215;&#12290;&#20854;&#20182;&#32771;&#34385;&#29992;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20998;&#21035;&#22788;&#29702;&#21382;&#21490;&#21644;&#26410;&#26469;&#25968;&#25454;&#30340;&#31574;&#30053;&#20250;&#23545;&#33258;&#24049;&#20135;&#29983;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#34920;&#31034;&#31574;&#30053;&#8212;&#8212;&#31227;&#20301;&#27861;&#65292;&#23558;&#36807;&#21435;&#30340;&#25968;&#25454;&#21644;&#26410;&#26469;&#21327;&#21464;&#37327;&#34701;&#21512;&#36215;&#26469;&#20197;&#32771;&#34385;&#23427;&#20204;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#20102;&#25552;&#21462;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#22797;&#26434;&#21160;&#21147;&#23398;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24182;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#30001;RNN&#21644;CNN&#32452;&#25104;&#65292;&#20108;&#32773;&#37117;&#34987;&#23618;&#32423;&#22320;&#20351;&#29992;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#36816;&#29992;&#20102;&#36339;&#36291;&#36830;&#25509;&#25216;&#26415;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#31934;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#27169;&#22411;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#65292;&#20026;&#26102;&#38388;&#24207;&#21015;&#30340;&#24213;&#23618;&#36807;&#31243;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate time series forecasting is a fundamental challenge in data science. It is often affected by external covariates such as weather or human intervention, which in many applications, may be predicted with reasonable accuracy. We refer to them as predicted future covariates. However, existing methods that attempt to predict time series in an iterative manner with autoregressive models end up with exponential error accumulations. Other strategies hat consider the past and future in the encoder and decoder respectively limit themselves by dealing with the historical and future data separately. To address these limitations, a novel feature representation strategy -- shifting -- is proposed to fuse the past data and future covariates such that their interactions can be considered. To extract complex dynamics in time series, we develop a parallel deep learning framework composed of RNN and CNN, both of which are used hierarchically. We also utilize the skip connection technique to impro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;Survey&#20102;&#22522;&#20110;DTL&#30340;ASR&#26694;&#26550;&#65292;&#24182;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#20197;&#36798;&#21040;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.14535</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65306;&#36808;&#21521;&#26356;&#22909;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Deep Transfer Learning for Automatic Speech Recognition: Towards Better Generalization. (arXiv:2304.14535v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;Survey&#20102;&#22522;&#20110;DTL&#30340;ASR&#26694;&#26550;&#65292;&#24182;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#20197;&#36798;&#21040;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#26041;&#38754;&#38754;&#20020;&#30528;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#36825;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#39640;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36890;&#24120;&#20551;&#35774;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#30456;&#21516;&#30340;&#22495;&#65292;&#20855;&#26377;&#30456;&#21516;&#30340;&#36755;&#20837;&#29305;&#24449;&#31354;&#38388;&#21644;&#25968;&#25454;&#20998;&#24067;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#20123;&#29616;&#23454;&#19990;&#30028;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24212;&#29992;&#20013;&#65292;&#36825;&#31181;&#20551;&#35774;&#26159;&#26080;&#27861;&#36866;&#29992;&#30340;&#12290;DTL&#34987;&#24341;&#20837;&#26469;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#23427;&#26377;&#21161;&#20110;&#20351;&#29992;&#23454;&#38469;&#25968;&#25454;&#38598;&#24320;&#21457;&#39640;&#24615;&#33021;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#23454;&#38469;&#25968;&#25454;&#38598;&#21363;&#20351;&#24456;&#23567;&#25110;&#31245;&#26377;&#19981;&#21516;&#65292;&#20294;&#19982;&#35757;&#32451;&#25968;&#25454;&#30456;&#20851;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;DTL&#30340;ASR&#26694;&#26550;&#30340;&#20840;&#38754;&#35843;&#26597;&#65292;&#20197;&#38416;&#26126;&#26368;&#26032;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) has recently become an important challenge when using deep learning (DL). It requires large-scale training datasets and high computational and storage resources. Moreover, DL techniques and machine learning (ML) approaches in general, hypothesize that training and testing data come from the same domain, with the same input feature space and data distribution characteristics. This assumption, however, is not applicable in some real-world artificial intelligence (AI) applications. Moreover, there are situations where gathering real data is challenging, expensive, or rarely occurring, which can not meet the data requirements of DL models. deep transfer learning (DTL) has been introduced to overcome these issues, which helps develop high-performing models using real datasets that are small or slightly different but related to the training data. This paper presents a comprehensive survey of DTL-based ASR frameworks to shed light on the latest developments 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#23884;&#20837;&#38750;&#20984;&#20998;&#27573;&#20223;&#23556;&#20915;&#31574;&#35268;&#21017;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#29305;&#24449;&#19982;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#29992;&#20110;&#24191;&#27867;&#30340;&#38750;&#20984;&#22411;SP&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#25968;&#20540;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13646</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20998;&#27573;&#20223;&#23556;&#20915;&#31574;&#35268;&#21017;&#29992;&#20110;&#24102;&#21327;&#21464;&#20449;&#24687;&#30340;&#38543;&#26426;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Data-driven Piecewise Affine Decision Rules for Stochastic Programming with Covariate Information. (arXiv:2304.13646v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#23884;&#20837;&#38750;&#20984;&#20998;&#27573;&#20223;&#23556;&#20915;&#31574;&#35268;&#21017;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#29305;&#24449;&#19982;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#29992;&#20110;&#24191;&#27867;&#30340;&#38750;&#20984;&#22411;SP&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#25968;&#20540;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24102;&#21327;&#21464;&#20449;&#24687;&#30340;&#38543;&#26426;&#35268;&#21010;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#38750;&#20984;&#20998;&#27573;&#20223;&#23556;&#20915;&#31574;&#35268;&#21017;(PADR)&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#26041;&#27861;&#65292;&#26088;&#22312;&#23398;&#20064;&#29305;&#24449;&#19982;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#22522;&#20110;PADR&#30340;ERM&#27169;&#22411;&#30340;&#38750;&#28176;&#36817;&#19968;&#33268;&#24615;&#32467;&#26524;&#65292;&#21487;&#29992;&#20110;&#26080;&#32422;&#26463;&#38382;&#39064;&#65292;&#20197;&#21450;&#32422;&#26463;&#38382;&#39064;&#30340;&#28176;&#36817;&#19968;&#33268;&#24615;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#38750;&#20984;&#21644;&#38750;&#21487;&#24494;&#30340;ERM&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22686;&#24378;&#30340;&#38543;&#26426;&#20027;&#23548;&#19979;&#38477;&#31639;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#27839;&#65288;&#22797;&#21512;&#24378;&#65289;&#26041;&#21521;&#31283;&#23450;&#24615;&#30340;&#28176;&#36817;&#25910;&#25947;&#20197;&#21450;&#22797;&#26434;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;PADR-based ERM&#26041;&#27861;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#38750;&#20984;&#22411;SP&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#29702;&#35770;&#19968;&#33268;&#24615;&#20445;&#35777;&#21644;&#35745;&#31639;&#21487;&#22788;&#29702;&#24615;&#12290;&#25968;&#20540;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#65292;PADR-based ERM&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Focusing on stochastic programming (SP) with covariate information, this paper proposes an empirical risk minimization (ERM) method embedded within a nonconvex piecewise affine decision rule (PADR), which aims to learn the direct mapping from features to optimal decisions. We establish the nonasymptotic consistency result of our PADR-based ERM model for unconstrained problems and asymptotic consistency result for constrained ones. To solve the nonconvex and nondifferentiable ERM problem, we develop an enhanced stochastic majorization-minimization algorithm and establish the asymptotic convergence to (composite strong) directional stationarity along with complexity analysis. We show that the proposed PADR-based ERM method applies to a broad class of nonconvex SP problems with theoretical consistency guarantees and computational tractability. Our numerical study demonstrates the superior performance of PADR-based ERM methods compared to state-of-the-art approaches under various settings,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#30340;&#28145;&#24230;&#34920;&#31034;&#65292;&#36890;&#36807;&#35299;&#37322;&#36825;&#20123;&#34920;&#31034;&#19982;&#26377;&#30417;&#30563;&#34920;&#31034;&#30340;&#21306;&#21035;&#65292;&#27604;&#36739;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20351;&#29992;&#26174;&#33879;&#22270;&#23558;&#20854;&#24212;&#29992;&#20110;&#39044;&#27979;&#19981;&#21516;&#30340;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2304.07304</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#22312;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#34920;&#36798;&#35299;&#37322;&#12289;&#20998;&#26512;&#21644;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Explaining, Analyzing, and Probing Representations of Self-Supervised Learning Models for Sensor-based Human Activity Recognition. (arXiv:2304.07304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#30340;&#28145;&#24230;&#34920;&#31034;&#65292;&#36890;&#36807;&#35299;&#37322;&#36825;&#20123;&#34920;&#31034;&#19982;&#26377;&#30417;&#30563;&#34920;&#31034;&#30340;&#21306;&#21035;&#65292;&#27604;&#36739;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20351;&#29992;&#26174;&#33879;&#22270;&#23558;&#20854;&#24212;&#29992;&#20110;&#39044;&#27979;&#19981;&#21516;&#30340;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26694;&#26550;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#65292;&#20197;&#20415;&#23398;&#20064;&#28145;&#24230;&#34920;&#31034;&#32780;&#19981;&#38656;&#35201;&#25968;&#25454;&#27880;&#37322;&#12290;&#34429;&#28982;SSL&#26694;&#26550;&#30340;&#24615;&#33021;&#20960;&#20046;&#19982;&#30417;&#30563;&#27169;&#22411;&#30456;&#24403;&#65292;&#20294;&#23545;SSL&#27169;&#22411;&#23398;&#20064;&#30340;&#34920;&#31034;&#36827;&#34892;&#35299;&#37322;&#30340;&#30740;&#31350;&#21364;&#24456;&#26377;&#38480;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#35299;&#37322;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#25581;&#31034;SSL&#34920;&#31034;&#19982;&#30417;&#30563;&#34920;&#31034;&#20043;&#38388;&#30340;&#24046;&#24322;&#65306;&#23427;&#20204;&#26159;&#22914;&#20309;&#34987;&#23398;&#20064;&#30340;&#12289;&#23427;&#20204;&#20445;&#30041;&#20102;&#21738;&#20123;&#36755;&#20837;&#25968;&#25454;&#23646;&#24615;&#65292;&#20197;&#21450;&#20309;&#26102;&#21487;&#20197;&#36873;&#25321;SSL&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#26088;&#22312;&#20998;&#26512;&#20004;&#20010;&#26368;&#36817;&#30340;SSL&#26694;&#26550;SimCLR&#21644;VICReg&#30340;&#28145;&#24230;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#37325;&#28857;&#25918;&#22312;&#20197;&#19979;&#20960;&#20010;&#26041;&#38754;&#65306;(i) &#27604;&#36739;&#30417;&#30563;&#21644;SSL&#27169;&#22411;&#23545;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#30772;&#22351;&#30340;&#40065;&#26834;&#24615;&#65307;(ii) &#20351;&#29992;&#26174;&#33879;&#22270;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#24182;&#31361;&#20986;&#26174;&#31034;&#29992;&#20110;&#39044;&#27979;&#21508;&#31181;&#27963;&#21160;&#30340;&#20027;&#35201;&#36755;&#20837;&#36890;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, self-supervised learning (SSL) frameworks have been extensively applied to sensor-based Human Activity Recognition (HAR) in order to learn deep representations without data annotations. While SSL frameworks reach performance almost comparable to supervised models, studies on interpreting representations learnt by SSL models are limited. Nevertheless, modern explainability methods could help to unravel the differences between SSL and supervised representations: how they are being learnt, what properties of input data they preserve, and when SSL can be chosen over supervised training. In this paper, we aim to analyze deep representations of two recent SSL frameworks, namely SimCLR and VICReg. Specifically, the emphasis is made on (i) comparing the robustness of supervised and SSL models to corruptions in input data; (ii) explaining predictions of deep learning models using saliency maps and highlighting what input channels are mostly used for predicting various activitie
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27979;&#37327;&#29992;&#25143;&#34920;&#31034;&#20013;&#20877;&#35782;&#21035;&#39118;&#38505;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#20551;&#35774;&#26816;&#39564;&#65292;&#21487;&#20197;&#38480;&#21046;&#25915;&#20987;&#32773;&#20174;&#29992;&#25143;&#30340;&#34920;&#31034;&#20013;&#33719;&#21462;&#20854;&#36523;&#20221;&#30340;&#27010;&#29575;&#12290;&#20316;&#32773;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#24212;&#23545;&#23454;&#38469;&#24212;&#29992;&#30340;&#26222;&#36866;&#24615;&#65292;&#24182;&#34917;&#20805;&#20102;&#25915;&#20987;&#31639;&#27861;&#26469;&#34913;&#37327;&#29616;&#23454;&#24212;&#29992;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2304.07210</link><description>&lt;p&gt;
&#35745;&#37327;&#20877;&#35782;&#21035;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Measuring Re-identification Risk. (arXiv:2304.07210v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07210
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27979;&#37327;&#29992;&#25143;&#34920;&#31034;&#20013;&#20877;&#35782;&#21035;&#39118;&#38505;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#20551;&#35774;&#26816;&#39564;&#65292;&#21487;&#20197;&#38480;&#21046;&#25915;&#20987;&#32773;&#20174;&#29992;&#25143;&#30340;&#34920;&#31034;&#20013;&#33719;&#21462;&#20854;&#36523;&#20221;&#30340;&#27010;&#29575;&#12290;&#20316;&#32773;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#24212;&#23545;&#23454;&#38469;&#24212;&#29992;&#30340;&#26222;&#36866;&#24615;&#65292;&#24182;&#34917;&#20805;&#20102;&#25915;&#20987;&#31639;&#27861;&#26469;&#34913;&#37327;&#29616;&#23454;&#24212;&#29992;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32039;&#20945;&#30340;&#29992;&#25143;&#34920;&#31034;&#65288;&#20363;&#22914;&#23884;&#20837;&#65289;&#26159;&#20010;&#24615;&#21270;&#26381;&#21153;&#30340;&#25903;&#26609;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#27979;&#37327;&#36825;&#31181;&#29992;&#25143;&#34920;&#31034;&#20013;&#30340;&#20877;&#35782;&#21035;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22522;&#20110;&#20551;&#35774;&#26816;&#39564;&#65292;&#27491;&#24335;&#38480;&#21046;&#20102;&#25915;&#20987;&#32773;&#33021;&#22815;&#20174;&#29992;&#25143;&#30340;&#34920;&#31034;&#20013;&#33719;&#21462;&#20854;&#36523;&#20221;&#30340;&#27010;&#29575;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#36275;&#22815;&#26222;&#36941;&#20197;&#27169;&#25311;&#37325;&#35201;&#30340;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#65292;&#20363;&#22914; Chrome &#30340;&#22522;&#20110;&#20852;&#36259;&#30340;&#24191;&#21578;&#30340; Topics API&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#20877;&#35782;&#21035;&#25915;&#20987;&#30340;&#21512;&#29702;&#22909;&#30340;&#31639;&#27861;&#26469;&#34917;&#20805;&#25105;&#20204;&#30340;&#29702;&#35770;&#30028;&#38480;&#65292;&#25105;&#20204;&#29992;&#36825;&#20123;&#31639;&#27861;&#26469;&#20272;&#35745; Topics API &#20013;&#30340;&#20877;&#35782;&#21035;&#39118;&#38505;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#39033;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#20010;&#20005;&#35880;&#19988;&#21487;&#35299;&#37322;&#30340;&#20877;&#35782;&#21035;&#39118;&#38505;&#27010;&#24565;&#21644;&#27979;&#37327;&#23427;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#26469;&#25351;&#23548;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compact user representations (such as embeddings) form the backbone of personalization services. In this work, we present a new theoretical framework to measure re-identification risk in such user representations. Our framework, based on hypothesis testing, formally bounds the probability that an attacker may be able to obtain the identity of a user from their representation. As an application, we show how our framework is general enough to model important real-world applications such as the Chrome's Topics API for interest-based advertising. We complement our theoretical bounds by showing provably good attack algorithms for re-identification that we use to estimate the re-identification risk in the Topics API. We believe this work provides a rigorous and interpretable notion of re-identification risk and a framework to measure it that can be used to inform real-world applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SPDF&#31639;&#27861;&#26469;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#12290;&#36890;&#36807;&#38750;&#32467;&#26500;&#21270;&#26435;&#37325;&#31232;&#30095;&#24615;&#26469;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#32780;&#23494;&#38598;&#24494;&#35843;&#21017;&#21487;&#20197;&#20445;&#35777;&#39640;&#24615;&#33021;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.10464</link><description>&lt;p&gt;
SPDF&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#31232;&#30095;&#39044;&#35757;&#32451;&#21644;&#23494;&#38598;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models. (arXiv:2303.10464v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SPDF&#31639;&#27861;&#26469;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#12290;&#36890;&#36807;&#38750;&#32467;&#26500;&#21270;&#26435;&#37325;&#31232;&#30095;&#24615;&#26469;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#32780;&#23494;&#38598;&#24494;&#35843;&#21017;&#21487;&#20197;&#20445;&#35777;&#39640;&#24615;&#33021;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#22810;&#39033;&#31361;&#30772;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#35821;&#35328;&#27169;&#22411;&#39318;&#20808;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#36328;&#22495;&#30693;&#35782;&#30340;&#39044;&#35757;&#32451;&#65288;&#20363;&#22914;&#65292;Pile&#12289;MassiveText&#31561;&#65289;&#65292;&#28982;&#21518;&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65288;&#20363;&#22914;&#65292;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#12289;&#25991;&#26412;&#25688;&#35201;&#31561;&#65289;&#12290;&#34429;&#28982;&#25193;&#22823;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#26377;&#21161;&#20110;&#25552;&#39640;LLM&#24615;&#33021;&#65292;&#20294;&#36825;&#20063;&#24102;&#26469;&#20102;&#26497;&#20026;&#31105;&#27490;&#24615;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#39044;&#35757;&#32451;LLMs&#36890;&#24120;&#38656;&#35201;&#27604;&#24494;&#35843;&#28436;&#20064;&#26356;&#22810;&#30340;FLOPs&#65292;&#20004;&#20010;&#38454;&#27573;&#20043;&#38388;&#30340;&#27169;&#22411;&#23481;&#37327;&#36890;&#24120;&#20445;&#25345;&#19981;&#21464;&#12290;&#20026;&#20102;&#23454;&#29616;&#30456;&#23545;&#20110;&#35757;&#32451;FLOPs&#30340;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#20004;&#20010;&#38454;&#27573;&#20043;&#38388;&#35299;&#32806;&#27169;&#22411;&#23481;&#37327;&#65292;&#24182;&#24341;&#20837;&#31232;&#30095;&#39044;&#35757;&#32451;&#21644;&#23494;&#38598;&#24494;&#35843;&#65288;SPDF&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#26435;&#37325;&#31232;&#30095;&#24615;&#26469;&#20165;&#35757;&#32451;&#23376;&#38598;&#26435;&#37325;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pre-training and fine-tuning paradigm has contributed to a number of breakthroughs in Natural Language Processing (NLP). Instead of directly training on a downstream task, language models are first pre-trained on large datasets with cross-domain knowledge (e.g., Pile, MassiveText, etc.) and then fine-tuned on task-specific data (e.g., natural language generation, text summarization, etc.). Scaling the model and dataset size has helped improve the performance of LLMs, but unfortunately, this also leads to highly prohibitive computational costs. Pre-training LLMs often require orders of magnitude more FLOPs than fine-tuning and the model capacity often remains the same between the two phases. To achieve training efficiency w.r.t training FLOPs, we propose to decouple the model capacity between the two phases and introduce Sparse Pre-training and Dense Fine-tuning (SPDF). In this work, we show the benefits of using unstructured weight sparsity to train only a subset of weights during 
&lt;/p&gt;</description></item><item><title>ICICLE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#30340;&#21487;&#35299;&#37322;&#30340;&#31867;&#22686;&#37327;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#21407;&#22411;&#37096;&#20998;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#35299;&#37322;&#24615;&#27010;&#24565;&#28418;&#31227;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#19981;&#38656;&#35201;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.07811</link><description>&lt;p&gt;
ICICLE: &#21487;&#35299;&#37322;&#30340;&#31867;&#22686;&#37327;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ICICLE: Interpretable Class Incremental Continual Learning. (arXiv:2303.07811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07811
&lt;/p&gt;
&lt;p&gt;
ICICLE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#30340;&#21487;&#35299;&#37322;&#30340;&#31867;&#22686;&#37327;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#21407;&#22411;&#37096;&#20998;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#35299;&#37322;&#24615;&#27010;&#24565;&#28418;&#31227;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#19981;&#38656;&#35201;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#33021;&#22815;&#22686;&#37327;&#23398;&#20064;&#26032;&#20219;&#21153;&#32780;&#19981;&#24536;&#35760;&#20043;&#21069;&#23398;&#20064;&#30340;&#20869;&#23481;&#65292;&#20174;&#32780;&#20419;&#36827;&#26032;&#26087;&#20219;&#21153;&#20043;&#38388;&#30340;&#27491;&#21521;&#30693;&#35782;&#36716;&#31227;&#12290;&#28982;&#32780;&#65292;&#36830;&#32493;&#23398;&#20064;&#23545;&#35299;&#37322;&#24615;&#25552;&#20986;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#27169;&#22411;&#39044;&#27979;&#32972;&#21518;&#30340;&#21407;&#29702;&#21487;&#33021;&#20250;&#38543;&#30528;&#26102;&#38388;&#32780;&#25913;&#21464;&#65292;&#23548;&#33268;&#35299;&#37322;&#24615;&#27010;&#24565;&#28418;&#31227;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#26679;&#26412;&#30340; Interpretable Class-InCremental LEarning (ICICLE) &#26041;&#27861;&#65292;&#37319;&#29992;&#21407;&#22411;&#37096;&#20998;&#21270;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#30340;&#21019;&#26032;&#28857;&#65306;&#35299;&#37322;&#24615;&#27491;&#21017;&#21270;&#12289;&#20197;&#24494;&#31890;&#31890;&#24230;&#20026;&#22522;&#30784;&#30340;&#21407;&#22411;&#21021;&#22987;&#21270;&#31574;&#30053;&#20197;&#21450;&#38024;&#23545;&#21407;&#22411;&#37096;&#20998;&#30340;&#20219;&#21153;&#26102;&#25928;&#20559;&#24046;&#34917;&#20607;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ICICLE&#20943;&#23569;&#20102;&#35299;&#37322;&#24615;&#27010;&#24565;&#28418;&#31227;&#65292;&#24182;&#19988;&#22312;&#19981;&#38656;&#35201;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning enables incremental learning of new tasks without forgetting those previously learned, resulting in positive knowledge transfer that can enhance performance on both new and old tasks. However, continual learning poses new challenges for interpretability, as the rationale behind model predictions may change over time, leading to interpretability concept drift. We address this problem by proposing Interpretable Class-InCremental LEarning (ICICLE), an exemplar-free approach that adopts a prototypical part-based approach. It consists of three crucial novelties: interpretability regularization that distills previously learned concepts while preserving user-friendly positive reasoning; proximity-based prototype initialization strategy dedicated to the fine-grained setting; and task-recency bias compensation devoted to prototypical parts. Our experimental results demonstrate that ICICLE reduces the interpretability concept drift and outperforms the existing exemplar-free me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; One-4-All (O4A)&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#21644;&#27969;&#24418;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26080;&#22270;&#24418;&#12289;&#31471;&#21040;&#31471;&#30340;&#23548;&#33322;&#31243;&#24207;&#12290;&#20854;&#36890;&#36807;&#36138;&#23146;&#26368;&#23567;&#21270;&#28508;&#22312;&#31354;&#38388;&#20869;&#30340;&#28508;&#21147;&#20989;&#25968;&#26469;&#36827;&#34892;&#23548;&#33322;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#23548;&#33322;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.04011</link><description>&lt;p&gt;
&#19968;&#25307;&#40092;&#65306;&#31070;&#32463;&#28508;&#21147;&#22330;&#29992;&#20110;&#23454;&#20307;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
One-4-All: Neural Potential Fields for Embodied Navigation. (arXiv:2303.04011v2 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; One-4-All (O4A)&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#21644;&#27969;&#24418;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26080;&#22270;&#24418;&#12289;&#31471;&#21040;&#31471;&#30340;&#23548;&#33322;&#31243;&#24207;&#12290;&#20854;&#36890;&#36807;&#36138;&#23146;&#26368;&#23567;&#21270;&#28508;&#22312;&#31354;&#38388;&#20869;&#30340;&#28508;&#21147;&#20989;&#25968;&#26469;&#36827;&#34892;&#23548;&#33322;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#23548;&#33322;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#26159;&#22312;&#20004;&#20010;&#20301;&#32622;&#20043;&#38388;&#23548;&#33322;&#12290;&#29305;&#21035;&#22320;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#23548;&#33322;&#21487;&#33021;&#38656;&#35201;&#20351;&#29992;&#39640;&#32500;RGB&#22270;&#20687;&#36827;&#34892;&#38271;&#26399;&#35268;&#21010;&#65292;&#36825;&#23545;&#20110;&#31471;&#21040;&#31471;&#30340;&#23398;&#20064;&#26041;&#27861;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#21322;&#21442;&#25968;&#26041;&#27861;&#36890;&#36807;&#23558;&#23398;&#20064;&#27169;&#22359;&#19982;&#29615;&#22659;&#30340;&#25299;&#25169;&#35760;&#24518;&#30456;&#32467;&#21512;&#26469;&#23454;&#29616;&#38271;&#26399;&#35268;&#21010;&#65292;&#36890;&#24120;&#20351;&#29992;&#20197;&#20808;&#21069;&#25910;&#38598;&#30340;&#22270;&#20687;&#20026;&#22522;&#30784;&#30340;&#22270;&#24418;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#19978;&#20351;&#29992;&#36825;&#20123;&#22270;&#34920;&#36890;&#24120;&#38656;&#35201;&#35843;&#25972;&#22823;&#37327;&#30340;&#20462;&#21098;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20197;&#36991;&#20813;&#34394;&#20551;&#30340;&#36793;&#32536;&#65292;&#38480;&#21046;&#36816;&#34892;&#26102;&#20869;&#23384;&#20351;&#29992;&#24182;&#20801;&#35768;&#30456;&#24403;&#24555;&#36895;&#30340;&#22270;&#24418;&#26597;&#35810;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;One-4-All&#65288;O4A&#65289;&#65292;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#21644;&#27969;&#24418;&#23398;&#20064;&#26469;&#33719;&#24471;&#26080;&#22270;&#24418;&#12289;&#31471;&#21040;&#31471;&#23548;&#33322;&#31649;&#36947;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#30446;&#26631;&#34987;&#25351;&#23450;&#20026;&#19968;&#24133;&#22270;&#20687;&#12290;&#23548;&#33322;&#36890;&#36807;&#36138;&#23146;&#22320;&#26368;&#23567;&#21270;&#36830;&#32493;&#23450;&#20041;&#22312;O4A&#28508;&#22312;&#31354;&#38388;&#19978;&#30340;&#28508;&#21147;&#20989;&#25968;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#26159;&#31163;&#32447;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental task in robotics is to navigate between two locations. In particular, real-world navigation can require long-horizon planning using high-dimensional RGB images, which poses a substantial challenge for end-to-end learning-based approaches. Current semi-parametric methods instead achieve long-horizon navigation by combining learned modules with a topological memory of the environment, often represented as a graph over previously collected images. However, using these graphs in practice typically involves tuning a number of pruning heuristics to avoid spurious edges, limit runtime memory usage and allow reasonably fast graph queries. In this work, we present One-4-All (O4A), a method leveraging self-supervised and manifold learning to obtain a graph-free, end-to-end navigation pipeline in which the goal is specified as an image. Navigation is achieved by greedily minimizing a potential function defined continuously over the O4A latent space. Our system is trained offline on 
&lt;/p&gt;</description></item><item><title>TopSpark&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#27493;&#38271;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#33258;&#20027;&#31227;&#21160;&#26426;&#22120;&#20154;&#19978;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#37327;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#28040;&#38500;&#20102;&#22266;&#23450;&#26102;&#38388;&#27493;&#38271;&#30340;&#38480;&#21046;&#65292;&#20351;&#24471;SNNs&#33021;&#22815;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#37117;&#23454;&#29616;&#26356;&#39640;&#30340;&#33021;&#37327;&#25928;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#36827;&#34892;&#39640;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.01826</link><description>&lt;p&gt;
TopSpark:&#19968;&#31181;&#29992;&#20110;&#33258;&#20027;&#31227;&#21160;&#26426;&#22120;&#20154;&#19978;&#30340;&#33410;&#33021;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#27493;&#38271;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TopSpark: A Timestep Optimization Methodology for Energy-Efficient Spiking Neural Networks on Autonomous Mobile Agents. (arXiv:2303.01826v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01826
&lt;/p&gt;
&lt;p&gt;
TopSpark&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#27493;&#38271;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#33258;&#20027;&#31227;&#21160;&#26426;&#22120;&#20154;&#19978;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#37327;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#28040;&#38500;&#20102;&#22266;&#23450;&#26102;&#38388;&#27493;&#38271;&#30340;&#38480;&#21046;&#65292;&#20351;&#24471;SNNs&#33021;&#22815;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#37117;&#23454;&#29616;&#26356;&#39640;&#30340;&#33021;&#37327;&#25928;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#36827;&#34892;&#39640;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#31227;&#21160;&#26426;&#22120;&#20154;&#38656;&#35201;&#20302;&#21151;&#32791;/&#39640;&#25928;&#33021;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#23436;&#25104;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20219;&#21153;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#22810;&#26679;&#21270;&#30340;&#29615;&#22659;&#65292;&#22240;&#20026;&#31227;&#21160;&#26426;&#22120;&#20154;&#36890;&#24120;&#30001;&#30005;&#27744;&#20379;&#30005;&#12290;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#21487;&#20197;&#23454;&#29616;&#36825;&#20123;&#35201;&#27714;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#31232;&#30095;&#35745;&#31639;&#21644;&#20855;&#26377;&#29983;&#29289;&#21551;&#21457;&#24335;&#23398;&#20064;&#26426;&#21046;&#30340;&#39640;&#25928;&#22312;&#32447;&#23398;&#20064;&#65292;&#25552;&#20379;&#20302;&#21151;&#32791;/&#39640;&#25928;&#33021;&#30340;&#22788;&#29702;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20943;&#23569;&#27599;&#20010;&#31070;&#32463;&#20803;&#22788;&#29702;&#19968;&#20010;&#33033;&#20914;&#24207;&#21015;&#65288;&#26102;&#38388;&#27493;&#38271;&#65289;&#30340;&#35745;&#31639;&#26102;&#38388;&#65292;&#21487;&#20197;&#20248;&#21270;SNNs&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25216;&#26415;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#35774;&#35745;&#25628;&#32034;&#26469;&#30830;&#23450;&#21482;&#33021;&#36827;&#34892;&#25512;&#26029;&#30340;&#22266;&#23450;&#26102;&#38388;&#27493;&#38271;&#35774;&#32622;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;SNNs&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#23454;&#29616;&#36827;&#19968;&#27493;&#30340;&#33021;&#37327;&#25928;&#29575;&#25552;&#39640;&#12290;&#36825;&#20123;&#25216;&#26415;&#36824;&#38480;&#21046;&#20102;SNNs&#22312;&#36816;&#34892;&#26102;&#36827;&#34892;&#39640;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TopSpark&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Autonomous mobile agents require low-power/energy-efficient machine learning (ML) algorithms to complete their ML-based tasks while adapting to diverse environments, as mobile agents are usually powered by batteries. These requirements can be fulfilled by Spiking Neural Networks (SNNs) as they offer low power/energy processing due to their sparse computations and efficient online learning with bio-inspired learning mechanisms for adapting to different environments. Recent works studied that the energy consumption of SNNs can be optimized by reducing the computation time of each neuron for processing a sequence of spikes (timestep). However, state-of-the-art techniques rely on intensive design searches to determine fixed timestep settings for only inference, thereby hindering the SNNs from achieving further energy efficiency gains in both training and inference. These techniques also restrict the SNNs from performing efficient online learning at run time. Toward this, we propose TopSpar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#24182;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.00848</link><description>&lt;p&gt;
&#20197;ELBOs&#30340;&#21152;&#26435;&#31215;&#20998;&#29702;&#35299;&#25193;&#25955;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Understanding the Diffusion Objective as a Weighted Integral of ELBOs. (arXiv:2303.00848v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#24182;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#29486;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#29992;&#19981;&#21516;&#30340;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#19988;&#36825;&#20123;&#30446;&#26631;&#37117;&#26159;&#21152;&#26435;&#25439;&#22833;&#30340;&#29305;&#20363;&#65292;&#20854;&#20013;&#21152;&#26435;&#20989;&#25968;&#25351;&#23450;&#27599;&#20010;&#22122;&#22768;&#32423;&#21035;&#30340;&#26435;&#37325;&#12290;&#22343;&#21248;&#21152;&#26435;&#23545;&#24212;&#20110;&#26368;&#22823;&#20284;&#28982;&#30340;&#21407;&#21017;&#24615;&#36817;&#20284;ELBO&#30340;&#26368;&#22823;&#21270;&#12290;&#20294;&#26159;&#23454;&#38469;&#19978;&#65292;&#30001;&#20110;&#26356;&#22909;&#30340;&#26679;&#26412;&#36136;&#37327;&#65292;&#30446;&#21069;&#30340;&#25193;&#25955;&#27169;&#22411;&#20351;&#29992;&#38750;&#22343;&#21248;&#21152;&#26435;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#65288;&#24102;&#26377;&#20219;&#20309;&#21152;&#26435;&#65289;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21487;&#20197;&#34987;&#20889;&#25104;&#19968;&#31181;ELBOs&#30340;&#21152;&#26435;&#31215;&#20998;&#24418;&#24335;&#65292;&#20854;&#20013;&#27599;&#20010;&#22122;&#22768;&#32423;&#21035;&#37117;&#26377;&#19968;&#20010;ELBO&#12290;&#22914;&#26524;&#26435;&#37325;&#20989;&#25968;&#26159;&#21333;&#35843;&#30340;&#65292;&#37027;&#20040;&#21152;&#26435;&#25439;&#22833;&#26159;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#30446;&#26631;&#65306;&#23427;&#22312;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#19979;&#65288;&#21363;&#39640;&#26031;&#22122;&#22768;&#25200;&#21160;&#65289;&#19979;&#26368;&#22823;&#21270;ELBO&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#20294;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#20123;&#27604;&#36739;&#21333;&#35843;&#21644;&#38750;&#21333;&#35843;&#26435;&#37325;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models in the literature are optimized with various objectives that are special cases of a weighted loss, where the weighting function specifies the weight per noise level. Uniform weighting corresponds to maximizing the ELBO, a principled approximation of maximum likelihood. In current practice diffusion models are optimized with non-uniform weighting due to better results in terms of sample quality. In this work we expose a direct relationship between the weighted loss (with any weighting) and the ELBO objective.  We show that the weighted loss can be written as a weighted integral of ELBOs, with one ELBO per noise level. If the weighting function is monotonic, then the weighted loss is a likelihood-based objective: it maximizes the ELBO under simple data augmentation, namely Gaussian noise perturbation. Our main contribution is a deeper theoretical understanding of the diffusion objective, but we also performed some experiments comparing monotonic with non-monotonic weight
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#30149;&#29702;&#26816;&#27979;&#25216;&#26415;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#36890;&#36807;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#22810;&#31181;&#23574;&#31471;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#24037;&#19994;&#21644;&#21307;&#30103;&#25991;&#29486;&#20013;&#26032;&#24320;&#21457;&#30340;&#29305;&#24449;&#24314;&#27169;&#26041;&#27861;&#22312;&#21508;&#31181;&#27169;&#24577;&#21644;&#25968;&#25454;&#38598;&#19978;&#21019;&#31435;&#20102;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2303.00609</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30149;&#29702;&#26816;&#27979;&#65306;&#28145;&#20837;&#25506;&#31350;&#29616;&#26377;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Pathology Detection: A Deep Dive Into the State of the Art. (arXiv:2303.00609v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#30149;&#29702;&#26816;&#27979;&#25216;&#26415;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#36890;&#36807;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#22810;&#31181;&#23574;&#31471;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#24037;&#19994;&#21644;&#21307;&#30103;&#25991;&#29486;&#20013;&#26032;&#24320;&#21457;&#30340;&#29305;&#24449;&#24314;&#27169;&#26041;&#27861;&#22312;&#21508;&#31181;&#27169;&#24577;&#21644;&#25968;&#25454;&#38598;&#19978;&#21019;&#31435;&#20102;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26080;&#30417;&#30563;&#26041;&#27861;&#27491;&#22312;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#30149;&#29702;&#26816;&#27979;&#21644;&#20998;&#21106;&#24212;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#25215;&#35834;&#21487;&#20197;&#20943;&#36731;&#23545;&#22823;&#35268;&#27169;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#22312;&#26816;&#27979;&#20219;&#20309;&#19968;&#31181;&#32597;&#35265;&#30149;&#29702;&#26041;&#38754;&#27604;&#26377;&#30417;&#30563;&#26041;&#27861;&#26356;&#20855;&#26222;&#36866;&#24615;&#12290;&#38543;&#30528;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979; (UAD) &#25991;&#29486;&#30340;&#19981;&#26029;&#22686;&#38271;&#21644;&#26032;&#30340;&#33539;&#24335;&#30340;&#20986;&#29616;&#65292;&#37325;&#35201;&#30340;&#26159;&#35201;&#22312;&#19968;&#20010;&#20844;&#20849;&#26694;&#26550;&#20013;&#19981;&#26029;&#22320;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#37325;&#26032;&#35780;&#20272;&#26368;&#26032;&#25216;&#26415;&#36827;&#23637;&#65292;&#24182;&#30830;&#23450;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#22810;&#31181;&#23574;&#31471; UAD &#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#24050;&#22312;&#33041; MRI &#30340; UAD &#20013;&#30830;&#31435;&#30340;&#26368;&#26032;&#25216;&#26415;&#36827;&#23637;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24037;&#19994;&#21644;&#21307;&#30103;&#25991;&#29486;&#20013;&#26032;&#24320;&#21457;&#30340;&#29305;&#24449;&#24314;&#27169;&#26041;&#27861;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#27169;&#24577;&#21644;&#25968;&#25454;&#38598;&#19978;&#21019;&#31435;&#20102;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep unsupervised approaches are gathering increased attention for applications such as pathology detection and segmentation in medical images since they promise to alleviate the need for large labeled datasets and are more generalizable than their supervised counterparts in detecting any kind of rare pathology. As the Unsupervised Anomaly Detection (UAD) literature continuously grows and new paradigms emerge, it is vital to continuously evaluate and benchmark new methods in a common framework, in order to reassess the state-of-the-art (SOTA) and identify promising research directions. To this end, we evaluate a diverse selection of cutting-edge UAD methods on multiple medical datasets, comparing them against the established SOTA in UAD for brain MRI. Our experiments demonstrate that newly developed feature-modeling methods from the industrial and medical literature achieve increased performance compared to previous work and set the new SOTA in a variety of modalities and datasets. Add
&lt;/p&gt;</description></item><item><title>&#24322;&#26500;&#35270;&#35273;&#28145;&#24230;&#32593;&#32476;&#31038;&#21306;&#20013;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#21487;&#20197;&#33258;&#25105;&#30417;&#30563;&#22320;&#24320;&#21457;&#20986;&#20849;&#20139;&#21327;&#35758;&#65292;&#20197;&#25351;&#20195;&#19968;&#32452;&#30446;&#26631;&#20013;&#30340;&#30446;&#26631;&#23545;&#35937;&#65292;&#24182;&#21487;&#29992;&#20110;&#27807;&#36890;&#19981;&#21516;&#31890;&#24230;&#30340;&#26410;&#30693;&#23545;&#35937;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2302.08913</link><description>&lt;p&gt;
&#24322;&#26500;&#35270;&#35273;&#28145;&#24230;&#32593;&#32476;&#31038;&#21306;&#20013;&#30340;&#25351;&#20195;&#24615;&#27807;&#36890;
&lt;/p&gt;
&lt;p&gt;
Referential communication in heterogeneous communities of pre-trained visual deep networks. (arXiv:2302.08913v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08913
&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#35270;&#35273;&#28145;&#24230;&#32593;&#32476;&#31038;&#21306;&#20013;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#21487;&#20197;&#33258;&#25105;&#30417;&#30563;&#22320;&#24320;&#21457;&#20986;&#20849;&#20139;&#21327;&#35758;&#65292;&#20197;&#25351;&#20195;&#19968;&#32452;&#30446;&#26631;&#20013;&#30340;&#30446;&#26631;&#23545;&#35937;&#65292;&#24182;&#21487;&#29992;&#20110;&#27807;&#36890;&#19981;&#21516;&#31890;&#24230;&#30340;&#26410;&#30693;&#23545;&#35937;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#39044;&#35757;&#32451;&#22270;&#20687;&#22788;&#29702;&#31070;&#32463;&#32593;&#32476;&#34987;&#23884;&#20837;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#25110;&#26426;&#22120;&#20154;&#31561;&#33258;&#20027;&#20195;&#29702;&#20013;&#65292;&#19968;&#20010;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#22312;&#23427;&#20204;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#31995;&#32479;&#22914;&#20309;&#30456;&#20114;&#20043;&#38388;&#36827;&#34892;&#27807;&#36890;&#20197;&#20102;&#35299;&#21608;&#22260;&#30340;&#19990;&#30028;&#12290;&#20316;&#20026;&#26397;&#30528;&#36825;&#20010;&#26041;&#21521;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#22312;&#19968;&#32452;&#24322;&#26500;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#32593;&#32476;&#31038;&#21306;&#20013;&#36827;&#34892;"&#25351;&#20195;&#24615;&#27807;&#36890;"&#30340;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20204;&#21487;&#20197;&#33258;&#25105;&#30417;&#30563;&#22320;&#21457;&#23637;&#19968;&#31181;&#20849;&#20139;&#21327;&#35758;&#26469;&#25351;&#20195;&#19968;&#32452;&#20505;&#36873;&#30446;&#26631;&#20013;&#30340;&#30446;&#26631;&#23545;&#35937;&#12290;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#65292;&#36825;&#31181;&#20849;&#20139;&#21327;&#35758;&#20063;&#21487;&#20197;&#29992;&#26469;&#27807;&#36890;&#19981;&#21516;&#31890;&#24230;&#30340;&#20808;&#21069;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#19968;&#20010;&#26368;&#21021;&#19981;&#23646;&#20110;&#29616;&#26377;&#31038;&#21306;&#30340;&#35270;&#35273;&#32593;&#32476;&#21487;&#20197;&#36731;&#26494;&#22320;&#23398;&#20064;&#21040;&#31038;&#21306;&#30340;&#21327;&#35758;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23450;&#24615;&#21644;&#23450;&#37327;&#22320;&#30740;&#31350;&#20102;&#36825;&#31181;&#26032;&#20135;&#29983;&#30340;&#21327;&#35758;&#30340;&#23646;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large pre-trained image-processing neural networks are being embedded in autonomous agents such as self-driving cars or robots, the question arises of how such systems can communicate with each other about the surrounding world, despite their different architectures and training regimes. As a first step in this direction, we systematically explore the task of \textit{referential communication} in a community of heterogeneous state-of-the-art pre-trained visual networks, showing that they can develop, in a self-supervised way, a shared protocol to refer to a target object among a set of candidates. This shared protocol can also be used, to some extent, to communicate about previously unseen object categories of different granularity. Moreover, a visual network that was not initially part of an existing community can learn the community's protocol with remarkable ease. Finally, we study, both qualitatively and quantitatively, the properties of the emergent protocol, providing some evi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#32593;&#32476;CECT&#65292;&#21033;&#29992;&#21487;&#25511;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#36827;&#34892;&#32452;&#21512;&#65292;&#33021;&#21516;&#26102;&#25429;&#25417;&#22810;&#20010;&#23616;&#37096;&#21644;&#20840;&#23616;&#23610;&#24230;&#19978;&#30340;&#29305;&#24449;&#12290;&#22312;&#20004;&#20010;&#20844;&#20849;COVID-19&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#21518;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#36825;&#19968;&#26032;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2302.02314</link><description>&lt;p&gt;
CECT&#65306;&#21487;&#25511;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#29992;&#20110;COVID-19&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
CECT: Controllable Ensemble CNN and Transformer for COVID-19 Image Classification. (arXiv:2302.02314v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#32593;&#32476;CECT&#65292;&#21033;&#29992;&#21487;&#25511;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#36827;&#34892;&#32452;&#21512;&#65292;&#33021;&#21516;&#26102;&#25429;&#25417;&#22810;&#20010;&#23616;&#37096;&#21644;&#20840;&#23616;&#23610;&#24230;&#19978;&#30340;&#29305;&#24449;&#12290;&#22312;&#20004;&#20010;&#20844;&#20849;COVID-19&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#21518;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#36825;&#19968;&#26032;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#25110;Transformer&#24320;&#21457;&#65292;&#21069;&#32773;&#65288;&#21518;&#32773;&#65289;&#21487;&#20197;&#25429;&#25417;&#23616;&#37096;&#65288;&#20840;&#23616;&#65289;&#29305;&#24449;&#12290;&#20026;&#20102;&#20943;&#36731;&#27169;&#22411;&#24615;&#33021;&#21463;&#23616;&#37096;&#65288;&#20840;&#23616;&#65289;&#29305;&#24449;&#32570;&#20047;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#32593;&#32476;CECT&#65292;&#36890;&#36807;&#21487;&#25511;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#36827;&#34892;&#32452;&#21512;&#12290;CECT&#30001;&#21367;&#31215;&#32534;&#30721;&#22359;&#12289;&#36716;&#32622;&#21367;&#31215;&#35299;&#30721;&#22359;&#21644;Transformer&#20998;&#31867;&#22359;&#32452;&#25104;&#12290;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;CNN&#25110;Transformer&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;CECT&#21487;&#20197;&#21516;&#26102;&#25429;&#25417;&#22810;&#20010;&#23616;&#37096;&#21644;&#20840;&#23616;&#23610;&#24230;&#19978;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#38598;&#21512;&#31995;&#25968;&#21487;&#20197;&#25511;&#21046;&#19981;&#21516;&#23610;&#24230;&#23616;&#37096;&#29305;&#24449;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#20849;COVID-19&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;CECT&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#29305;&#24449;&#25429;&#25417;&#33021;&#21147;&#65292;&#25105;&#20204;&#30456;&#20449;CECT&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#32780;&#26377;&#25928;&#30340;&#24037;&#20855;&#25193;&#23637;&#21040;&#20854;&#20182;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most computer vision models are developed based on either convolutional neural network (CNN) or transformer, while the former (latter) method captures local (global) features. To relieve model performance limitations due to the lack of global (local) features, we develop a novel classification network CECT by controllable ensemble CNN and transformer. CECT is composed of a convolutional encoder block, a transposed-convolutional decoder block, and a transformer classification block. Different from conventional CNN- or transformer-based methods, our CECT can capture features at both multi-local and global scales. Besides, the contribution of local features at different scales can be controlled with the proposed ensemble coefficients. We evaluate CECT on two public COVID-19 datasets and it outperforms existing state-of-the-art methods on all evaluation metrics. With remarkable feature capture ability, we believe CECT can be extended to other medical image classification scenarios as a dia
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#26102;&#31354;&#36716;&#25442;&#22120;&#65288;CST&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#36716;&#25442;&#22120;&#26550;&#26500;&#65292;&#36890;&#36807;&#22312;Sobolev&#31354;&#38388;&#20013;&#36827;&#34892;&#20248;&#21270;&#65292;&#33021;&#22815;&#24314;&#27169;&#36830;&#32493;&#31995;&#32479;&#24182;&#20445;&#35777;&#36830;&#32493;&#24179;&#28369;&#30340;&#36755;&#20986;&#12290;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#65292;&#21253;&#25324;&#23398;&#20064;&#33041;&#21160;&#21147;&#23398;&#65292;CST&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.13338</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#31354;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Continuous Spatiotemporal Transformers. (arXiv:2301.13338v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13338
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#26102;&#31354;&#36716;&#25442;&#22120;&#65288;CST&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#36716;&#25442;&#22120;&#26550;&#26500;&#65292;&#36890;&#36807;&#22312;Sobolev&#31354;&#38388;&#20013;&#36827;&#34892;&#20248;&#21270;&#65292;&#33021;&#22815;&#24314;&#27169;&#36830;&#32493;&#31995;&#32479;&#24182;&#20445;&#35777;&#36830;&#32493;&#24179;&#28369;&#30340;&#36755;&#20986;&#12290;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#65292;&#21253;&#25324;&#23398;&#20064;&#33041;&#21160;&#21147;&#23398;&#65292;CST&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#24314;&#27169;&#26102;&#31354;&#21160;&#24577;&#31995;&#32479;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;Transformer&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#65292;&#33021;&#22815;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;&#24314;&#27169;&#36830;&#32493;&#21160;&#24577;&#31995;&#32479;&#26102;&#65292;&#20256;&#32479;&#30340;Transformer&#27169;&#22411;&#23384;&#22312;&#19968;&#20010;&#38480;&#21046;&#65292;&#21363;&#23427;&#20204;&#26159;&#22522;&#20110;&#31163;&#25955;&#26102;&#38388;&#21644;&#31354;&#38388;&#30340;&#27169;&#22411;&#65292;&#22240;&#27492;&#26080;&#27861;&#20445;&#35777;&#36830;&#32493;&#37319;&#26679;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#26102;&#31354;&#36716;&#25442;&#22120;&#65288;CST&#65289;&#26550;&#26500;&#65292;&#19987;&#38376;&#29992;&#20110;&#24314;&#27169;&#36830;&#32493;&#31995;&#32479;&#12290;&#36825;&#31181;&#26032;&#30340;&#26694;&#26550;&#36890;&#36807;&#22312;Sobolev&#31354;&#38388;&#20013;&#36827;&#34892;&#20248;&#21270;&#65292;&#20445;&#35777;&#20102;&#36830;&#32493;&#24179;&#28369;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#23558;CST&#19982;&#20256;&#32479;&#30340;Transformer&#27169;&#22411;&#20197;&#21450;&#20854;&#20182;&#26102;&#31354;&#21160;&#24577;&#24314;&#27169;&#26041;&#27861;&#36827;&#34892;&#20102;&#23545;&#27604;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#22810;&#20010;&#21512;&#25104;&#21644;&#30495;&#23454;&#31995;&#32479;&#30340;&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#20174;&#38041;&#25104;&#20687;&#25968;&#25454;&#20013;&#23398;&#20064;&#33041;&#21160;&#21147;&#23398;&#65292;CST&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling spatiotemporal dynamical systems is a fundamental challenge in machine learning. Transformer models have been very successful in NLP and computer vision where they provide interpretable representations of data. However, a limitation of transformers in modeling continuous dynamical systems is that they are fundamentally discrete time and space models and thus have no guarantees regarding continuous sampling. To address this challenge, we present the Continuous Spatiotemporal Transformer (CST), a new transformer architecture that is designed for the modeling of continuous systems. This new framework guarantees a continuous and smooth output via optimization in Sobolev space. We benchmark CST against traditional transformers as well as other spatiotemporal dynamics modeling methods and achieve superior performance in a number of tasks on synthetic and real systems, including learning brain dynamics from calcium imaging data.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Clugen&#30340;&#27169;&#22359;&#21270;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#33021;&#22815;&#20351;&#29992;&#20219;&#24847;&#20998;&#24067;&#21019;&#24314;&#25903;&#25345;&#32447;&#27573;&#30340;&#22810;&#32500;&#32858;&#31867;&#65292;&#36866;&#29992;&#20110;&#35780;&#20272;&#32858;&#31867;&#31639;&#27861;&#24182;&#26377;&#28508;&#21147;&#25104;&#20026;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2301.10327</link><description>&lt;p&gt;
&#29983;&#25104;&#25903;&#25345;&#32447;&#30340;&#22810;&#32500;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Generating Multidimensional Clusters With Support Lines. (arXiv:2301.10327v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10327
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Clugen&#30340;&#27169;&#22359;&#21270;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#33021;&#22815;&#20351;&#29992;&#20219;&#24847;&#20998;&#24067;&#21019;&#24314;&#25903;&#25345;&#32447;&#27573;&#30340;&#22810;&#32500;&#32858;&#31867;&#65292;&#36866;&#29992;&#20110;&#35780;&#20272;&#32858;&#31867;&#31639;&#27861;&#24182;&#26377;&#28508;&#21147;&#25104;&#20026;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#23545;&#20110;&#35780;&#20272;&#32858;&#31867;&#25216;&#26415;&#12289;&#34917;&#20805;&#21644;&#25193;&#23637;&#30495;&#23454;&#25968;&#25454;&#20197;&#21450;&#20801;&#35768;&#26356;&#23436;&#25972;&#22320;&#35206;&#30422;&#32473;&#23450;&#38382;&#39064;&#31354;&#38388;&#33267;&#20851;&#37325;&#35201;&#12290;&#32780;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#20855;&#26377;&#21019;&#24314;&#22823;&#37327;&#25968;&#25454;&#30340;&#28508;&#21147;&#65292;&#36825;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#31232;&#32570;&#26102;&#33267;&#20851;&#37325;&#35201;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#34987;&#20805;&#20998;&#29702;&#35299;&#30340;&#29983;&#25104;&#36807;&#31243;&#21644;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#24037;&#20855;&#65292;&#20197;&#31995;&#32479;&#22320;&#30740;&#31350;&#32858;&#31867;&#20998;&#26512;&#31639;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Clugen&#30340;&#27169;&#22359;&#21270;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#33021;&#22815;&#20351;&#29992;&#20219;&#24847;&#20998;&#24067;&#21019;&#24314;&#25903;&#25345;&#32447;&#27573;&#30340;&#22810;&#32500;&#32858;&#31867;&#12290;Clugen&#26159;&#24320;&#28304;&#30340;&#65292;&#26377;&#20840;&#38754;&#30340;&#21333;&#20803;&#27979;&#35797;&#21644;&#25991;&#26723;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;Python&#12289;R&#12289;Julia&#21644;MATLAB/Octave&#29983;&#24577;&#31995;&#32479;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21508;&#20010;&#32500;&#24230;&#19978;&#20135;&#29983;&#20016;&#23500;&#22810;&#26679;&#30340;&#32467;&#26524;&#65292;&#36866;&#21512;&#29992;&#20110;&#35780;&#20272;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#26377;&#28508;&#21147;&#25104;&#20026;&#21508;&#20010;&#39046;&#22495;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic data is essential for assessing clustering techniques, complementing and extending real data, and allowing for more complete coverage of a given problem's space. In turn, synthetic data generators have the potential of creating vast amounts of data -- a crucial activity when real-world data is at premium -- while providing a well-understood generation procedure and an interpretable instrument for methodically investigating cluster analysis algorithms. Here, we present Clugen, a modular procedure for synthetic data generation, capable of creating multidimensional clusters supported by line segments using arbitrary distributions. Clugen is open source, comprehensively unit tested and documented, and is available for the Python, R, Julia, and MATLAB/Octave ecosystems. We demonstrate that our proposal can produce rich and varied results in various dimensions, is fit for use in the assessment of clustering algorithms, and has the potential to be a widely used framework in diverse 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35774;&#35745;&#25968;&#25454;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#23558;&#20154;&#26426;&#20132;&#20114;&#27010;&#24565;&#19982;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#25968;&#25454;&#25910;&#38598;&#32570;&#20047;&#22810;&#26679;&#24615;&#23548;&#33268;&#30340;&#22833;&#36133;&#38382;&#39064;&#12290;&#36890;&#36807;&#39044;&#25910;&#38598;&#35745;&#21010;&#12289;&#25910;&#38598;&#30417;&#25511;&#21644;&#25968;&#25454;&#29087;&#24713;&#24230;&#31561;&#27493;&#39588;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#22312;&#36328;&#32452;&#20132;&#21449;&#32676;&#20307;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.10319</link><description>&lt;p&gt;
&#35774;&#35745;&#25968;&#25454;&#65306;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#20027;&#21160;&#25968;&#25454;&#25910;&#38598;&#21644;&#36845;&#20195;
&lt;/p&gt;
&lt;p&gt;
Designing Data: Proactive Data Collection and Iteration for Machine Learning. (arXiv:2301.10319v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10319
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35774;&#35745;&#25968;&#25454;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#23558;&#20154;&#26426;&#20132;&#20114;&#27010;&#24565;&#19982;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#25968;&#25454;&#25910;&#38598;&#32570;&#20047;&#22810;&#26679;&#24615;&#23548;&#33268;&#30340;&#22833;&#36133;&#38382;&#39064;&#12290;&#36890;&#36807;&#39044;&#25910;&#38598;&#35745;&#21010;&#12289;&#25910;&#38598;&#30417;&#25511;&#21644;&#25968;&#25454;&#29087;&#24713;&#24230;&#31561;&#27493;&#39588;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#22312;&#36328;&#32452;&#20132;&#21449;&#32676;&#20307;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#25910;&#38598;&#30340;&#32570;&#20047;&#22810;&#26679;&#24615;&#23548;&#33268;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#20005;&#37325;&#22833;&#36133;&#12290;&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#36827;&#34892;&#25968;&#25454;&#21518;&#22788;&#29702;&#65292;&#20294;&#36825;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#24182;&#24456;&#23569;&#20840;&#38754;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26032;&#30340;&#26041;&#27861;&#26469;&#36319;&#36394;&#21644;&#31649;&#29702;&#25968;&#25454;&#25910;&#38598;&#12289;&#36845;&#20195;&#21644;&#27169;&#22411;&#35757;&#32451;&#65292;&#20197;&#35780;&#20272;&#25968;&#25454;&#38598;&#26159;&#21542;&#21453;&#26144;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#21464;&#24322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35774;&#35745;&#25968;&#25454;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#23558;&#20154;&#26426;&#20132;&#20114;&#27010;&#24565;&#19982;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#36807;&#31243;&#21253;&#25324;&#65288;1&#65289;&#39044;&#25910;&#38598;&#35745;&#21010;&#65292;&#20027;&#21160;&#20419;&#20351;&#24182;&#35760;&#24405;&#39044;&#26399;&#30340;&#25968;&#25454;&#20998;&#24067;&#65307;&#65288;2&#65289;&#25910;&#38598;&#30417;&#25511;&#65292;&#31995;&#32479;&#24615;&#22320;&#40723;&#21169;&#37319;&#26679;&#22810;&#26679;&#24615;&#65307;&#65288;3&#65289;&#25968;&#25454;&#29087;&#24713;&#24230;&#65292;&#20351;&#29992;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#35782;&#21035;&#27169;&#22411;&#23545;&#20854;&#19981;&#29087;&#24713;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#23558;&#35774;&#35745;&#25968;&#25454;&#24212;&#29992;&#20110;&#25968;&#25454;&#25910;&#38598;&#21644;&#24314;&#27169;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#36328;&#32452;&#20132;&#21449;&#32676;&#20307;&#35757;&#32451;&#30340;&#27169;&#22411;&#19978;&#65292;&#8220;&#35774;&#35745;&#8221;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#20248;&#20110;&#22823;&#23567;&#30456;&#20284;&#20294;&#38024;&#23545;&#24615;&#36739;&#20302;&#30340;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lack of diversity in data collection has caused significant failures in machine learning (ML) applications. While ML developers perform post-collection interventions, these are time intensive and rarely comprehensive. Thus, new methods to track &amp; manage data collection, iteration, and model training are necessary for evaluating whether datasets reflect real world variability. We present designing data, an iterative approach to data collection connecting HCI concepts with ML techniques. Our process includes (1) Pre-Collection Planning, to reflexively prompt and document expected data distributions; (2) Collection Monitoring, to systematically encourage sampling diversity; and (3) Data Familiarity, to identify samples that are unfamiliar to a model using density estimation. We apply designing data to a data collection and modeling task. We find models trained on ''designed'' datasets generalize better across intersectional groups than those trained on similarly sized but less targeted da
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#30340;&#31070;&#32463;&#32593;&#32476;&#35770;&#35777;&#35299;&#37322;&#26041;&#27861;SpArX&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#23450;&#37327;&#35770;&#35777;&#26694;&#26550;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21487;&#20197;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#26356;&#24544;&#23454;&#21644;&#28145;&#20837;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2301.09559</link><description>&lt;p&gt;
SpArX: &#31232;&#30095;&#30340;&#31070;&#32463;&#32593;&#32476;&#35770;&#35777;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
SpArX: Sparse Argumentative Explanations for Neural Networks. (arXiv:2301.09559v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09559
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#30340;&#31070;&#32463;&#32593;&#32476;&#35770;&#35777;&#35299;&#37322;&#26041;&#27861;SpArX&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#23450;&#37327;&#35770;&#35777;&#26694;&#26550;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21487;&#20197;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#26356;&#24544;&#23454;&#21644;&#28145;&#20837;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#26377;&#21508;&#31181;&#24212;&#29992;&#65292;&#20294;&#35299;&#37322;&#23427;&#20204;&#30340;&#20915;&#31574;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20851;&#27880;&#35299;&#37322;&#25913;&#21464;&#21333;&#20010;&#36755;&#20837;&#22914;&#20309;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#36755;&#20986;&#34892;&#20026;&#19968;&#33268;&#30340;&#35299;&#37322;&#26410;&#24517;&#24544;&#23454;&#20110;&#20854;&#23454;&#38469;&#26426;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#23450;&#37327;&#35770;&#35777;&#26694;&#26550;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20026;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#26426;&#21046;&#21019;&#24314;&#20102;&#35770;&#35777;&#24615;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;SpArX&#26041;&#27861;&#39318;&#20808;&#23558;&#22810;&#23618;&#24863;&#30693;&#22120;&#31232;&#30095;&#21270;&#65292;&#21516;&#26102;&#20445;&#25345;&#23613;&#21487;&#33021;&#22810;&#30340;&#21407;&#22987;&#32467;&#26500;&#12290;&#28982;&#21518;&#23558;&#31232;&#30095;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#36716;&#21270;&#20026;&#31561;&#25928;&#30340;&#23450;&#37327;&#35770;&#35777;&#26694;&#26550;&#65292;&#20197;&#25581;&#31034;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#28508;&#22312;&#20915;&#31574;&#36807;&#31243;&#65292;&#20135;&#29983;&#20840;&#23616;&#21644;/&#25110;&#23616;&#37096;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;SpArX&#27604;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#32473;&#20986;&#26356;&#24544;&#23454;&#30340;&#35299;&#37322;&#65292;&#21516;&#26102;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#27934;&#23519;&#23454;&#38469;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks (NNs) have various applications in AI, but explaining their decisions remains challenging. Existing approaches often focus on explaining how changing individual inputs affects NNs' outputs. However, an explanation that is consistent with the input-output behaviour of an NN is not necessarily faithful to the actual mechanics thereof. In this paper, we exploit relationships between multi-layer perceptrons (MLPs) and quantitative argumentation frameworks (QAFs) to create argumentative explanations for the mechanics of MLPs. Our SpArX method first sparsifies the MLP while maintaining as much of the original structure as possible. It then translates the sparse MLP into an equivalent QAF to shed light on the underlying decision process of the MLP, producing global and/or local explanations. We demonstrate experimentally that SpArX can give more faithful explanations than existing approaches, while simultaneously providing deeper insights into the actual reasoning process of M
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#36125;&#21494;&#26031;&#22238;&#24402;&#21644;&#21160;&#37327;&#20256;&#25773;&#30340;&#39044;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#26102;&#22312;&#23884;&#20837;&#24335;&#30828;&#20214;&#19978;&#20135;&#29983;&#26377;&#24847;&#20041;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#20351;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#20154;&#26426;&#20132;&#20114;&#31561;&#39046;&#22495;&#30340;&#23454;&#26102;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.01201</link><description>&lt;p&gt;
&#23884;&#20837;&#24335;&#31995;&#32479;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Uncertainty in Real-Time Semantic Segmentation on Embedded Systems. (arXiv:2301.01201v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#36125;&#21494;&#26031;&#22238;&#24402;&#21644;&#21160;&#37327;&#20256;&#25773;&#30340;&#39044;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#26102;&#22312;&#23884;&#20837;&#24335;&#30828;&#20214;&#19978;&#20135;&#29983;&#26377;&#24847;&#20041;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#20351;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#20154;&#26426;&#20132;&#20114;&#31561;&#39046;&#22495;&#30340;&#23454;&#26102;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#20154;&#26426;&#20132;&#20114;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#38656;&#35201;&#23454;&#26102;&#39044;&#27979;&#33021;&#21147;&#12290;&#23454;&#26102;&#24212;&#29992;&#30340;&#25361;&#25112;&#34987;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#25152;&#21152;&#21095;&#12290;&#34429;&#28982;&#36825;&#20123;&#24179;&#21488;&#19978;&#23454;&#26102;&#26041;&#27861;&#30340;&#24320;&#21457;&#24471;&#21040;&#20102;&#22686;&#21152;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#36275;&#22815;&#22320;&#32771;&#34385;&#21040;&#23384;&#22312;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#30340;&#28145;&#23618;&#29305;&#24449;&#25552;&#21462;&#19982;&#36125;&#21494;&#26031;&#22238;&#24402;&#21644;&#21160;&#37327;&#20256;&#25773;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#27880;&#19981;&#30830;&#23450;&#24615;&#30340;&#39044;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#35813;&#26041;&#27861;&#22914;&#20309;&#22312;&#23884;&#20837;&#24335;&#30828;&#20214;&#19978;&#23454;&#26102;&#20135;&#29983;&#26377;&#24847;&#20041;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Application for semantic segmentation models in areas such as autonomous vehicles and human computer interaction require real-time predictive capabilities. The challenges of addressing real-time application is amplified by the need to operate on resource constrained hardware. Whilst development of real-time methods for these platforms has increased, these models are unable to sufficiently reason about uncertainty present. This paper addresses this by combining deep feature extraction from pre-trained models with Bayesian regression and moment propagation for uncertainty aware predictions. We demonstrate how the proposed method can yield meaningful uncertainty on embedded hardware in real-time whilst maintaining predictive performance.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24102;&#26377;&#31070;&#32463;&#32593;&#32476;&#21644;&#32034;&#24341;&#30340;&#32858;&#31867;&#27169;&#22411;CNNI&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38750;&#20984;&#24418;&#29366;&#25968;&#25454;&#30340;&#21442;&#25968;&#21270;&#32858;&#31867;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.03853</link><description>&lt;p&gt;
&#24102;&#26377;&#31070;&#32463;&#32593;&#32476;&#21644;&#32034;&#24341;&#30340;&#32858;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Clustering with Neural Network and Index. (arXiv:2212.03853v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03853
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24102;&#26377;&#31070;&#32463;&#32593;&#32476;&#21644;&#32034;&#24341;&#30340;&#32858;&#31867;&#27169;&#22411;CNNI&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38750;&#20984;&#24418;&#29366;&#25968;&#25454;&#30340;&#21442;&#25968;&#21270;&#32858;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#19968;&#31181;&#26032;&#30340;&#32858;&#31867;&#27169;&#22411;&#65292;&#31216;&#20026;&#24102;&#26377;&#31070;&#32463;&#32593;&#32476;&#21644;&#32034;&#24341;&#30340;&#32858;&#31867;&#27169;&#22411;&#65288;CNNI&#65289;&#12290;CNNI&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#32858;&#31867;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#27169;&#20223;&#30417;&#30563;&#23398;&#20064;&#65292;&#20351;&#29992;&#20869;&#37096;&#32858;&#31867;&#35780;&#20272;&#25351;&#26631;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#12290;&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#39564;&#26469;&#27979;&#35797;&#26032;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#19982;K&#22343;&#20540;&#21644;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#31561;&#20854;&#20182;&#32858;&#31867;&#27169;&#22411;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;CNNI&#21487;&#20197;&#27491;&#30830;&#22320;&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#65307;CNNI&#37197;&#22791;&#20102;MMJ-SC&#65292;&#25104;&#20026;&#31532;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38750;&#20984;&#24418;&#29366;&#65288;&#38750;&#24179;&#38754;&#20960;&#20309;&#65289;&#25968;&#25454;&#30340;&#21442;&#25968;&#21270;&#65288;&#24402;&#32435;&#24335;&#65289;&#32858;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new model called Clustering with Neural Network and Index (CNNI) is introduced. CNNI uses a Neural Network to cluster data points. Training of the Neural Network mimics supervised learning, with an internal clustering evaluation index acting as the loss function. An experiment is conducted to test the feasibility of the new model, and compared with results of other clustering models like K-means and Gaussian Mixture Model (GMM). The result shows CNNI can work properly for clustering data; CNNI equipped with MMJ-SC, achieves the first parametric (inductive) clustering model that can deal with non-convex shaped (non-flat geometry) data.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#23545;&#27431;&#27954;AI&#36131;&#20219;&#25351;&#20196;&#30340;&#25552;&#26696;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#21457;&#29616;&#20854;&#20195;&#34920;&#20102;&#19968;&#20010;&#21322;&#24515;&#24577;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#21019;&#26032;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2211.13960</link><description>&lt;p&gt;
&#27431;&#27954;&#20154;&#24037;&#26234;&#33021;&#36131;&#20219;&#25351;&#20196;&#8212;&#8212;&#23545;&#21322;&#24515;&#24577;&#26041;&#27861;&#30340;&#25209;&#35780;&#21644;&#26410;&#26469;&#32463;&#39564;&#25945;&#35757;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The European AI Liability Directives -- Critique of a Half-Hearted Approach and Lessons for the Future. (arXiv:2211.13960v6 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13960
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23545;&#27431;&#27954;AI&#36131;&#20219;&#25351;&#20196;&#30340;&#25552;&#26696;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#21457;&#29616;&#20854;&#20195;&#34920;&#20102;&#19968;&#20010;&#21322;&#24515;&#24577;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#21019;&#26032;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#31561;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24449;&#26381;&#19990;&#30028;&#65292;&#20840;&#29699;AI&#31995;&#32479;&#30340;&#26368;&#20339;&#36131;&#20219;&#26694;&#26550;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;2022&#24180;9&#26376;&#65292;&#27431;&#27954;&#22996;&#21592;&#20250;&#25552;&#20986;&#20102;&#20004;&#39033;&#20851;&#20110;AI&#36131;&#20219;&#30340;&#25552;&#26696;&#65292;&#27010;&#36848;&#20102;&#27431;&#27954;&#30340;&#26041;&#27861;&#65306;&#19968;&#39033;&#26032;&#30340;AI&#36131;&#20219;&#25351;&#20196;&#21644;&#23545;&#20135;&#21697;&#36131;&#20219;&#25351;&#20196;&#30340;&#20462;&#35746;&#12290;&#23427;&#20204;&#26500;&#25104;&#20102;&#27431;&#30431;AI&#30417;&#31649;&#30340;&#26368;&#32456;&#22522;&#30707;&#12290;&#20851;&#38190;&#26159;&#65292;&#36131;&#20219;&#25552;&#26696;&#21644;&#27431;&#30431;AI&#27861;&#26696;&#26159;&#30456;&#20114;&#32852;&#31995;&#30340;&#65306;&#21518;&#32773;&#19981;&#21253;&#21547;&#20219;&#20309;&#21463;&#24433;&#21709;&#20154;&#21592;&#30340;&#20010;&#20154;&#26435;&#21033;&#65292;&#21069;&#32773;&#32570;&#20047;&#20851;&#20110;AI&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#20855;&#20307;&#12289;&#23454;&#36136;&#24615;&#35268;&#21017;&#12290;&#32508;&#21512;&#36215;&#26469;&#65292;&#36825;&#20123;&#27861;&#26696;&#21487;&#33021;&#20250;&#22312;AI&#30417;&#31649;&#39046;&#22495;&#24341;&#21457;&#24067;&#40065;&#22622;&#23572;&#25928;&#24212;&#65292;&#23545;&#32654;&#22269;&#21644;&#20854;&#20182;&#22269;&#23478;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#23427;&#35814;&#32454;&#30740;&#31350;&#20102;&#22996;&#21592;&#20250;&#30340;&#25552;&#26696;&#65292;&#24182;&#34920;&#26126;&#65292;&#34429;&#28982;&#26397;&#30528;&#27491;&#30830;&#26041;&#21521;&#36808;&#20986;&#20102;&#27493;&#20240;&#65292;&#20294;&#26368;&#32456;&#20195;&#34920;&#30528;&#21322;&#24515;&#24577;&#30340;&#26041;&#27861;&#65306;&#22914;&#26524;&#36890;&#36807;&#25913;&#27491;&#26041;&#26696;&#65292;
&lt;/p&gt;
&lt;p&gt;
As ChatGPT et al. conquer the world, the optimal liability framework for AI systems remains an unsolved problem across the globe. In a much-anticipated move, the European Commission advanced two proposals outlining the European approach to AI liability in September 2022: a novel AI Liability Directive and a revision of the Product Liability Directive. They constitute the final cornerstone of EU AI regulation. Crucially, the liability proposals and the EU AI Act are inherently intertwined: the latter does not contain any individual rights of affected persons, and the former lack specific, substantive rules on AI development and deployment. Taken together, these acts may well trigger a Brussels Effect in AI regulation, with significant consequences for the US and beyond.  This paper makes three novel contributions. First, it examines in detail the Commission proposals and shows that, while making steps in the right direction, they ultimately represent a half-hearted approach: if enacted 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DARE-GP&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#26234;&#33021;&#38899;&#31665;&#25928;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#36867;&#36991;&#19982;&#26234;&#33021;&#38899;&#31665;&#30456;&#36830;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2211.09273</link><description>&lt;p&gt;
&#38024;&#23545;&#22768;&#23398;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#36867;&#36991;&#30340;&#23454;&#26102;&#35821;&#38899;&#24773;&#24863;&#26816;&#27979;&#30340;&#38544;&#31169;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Privacy against Real-Time Speech Emotion Detection via Acoustic Adversarial Evasion of Machine Learning. (arXiv:2211.09273v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DARE-GP&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#26234;&#33021;&#38899;&#31665;&#25928;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#36867;&#36991;&#19982;&#26234;&#33021;&#38899;&#31665;&#30456;&#36830;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#30417;&#35270;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#28041;&#21450;&#21040;&#24191;&#27867;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#22312;&#26222;&#36941;&#23384;&#22312;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#22810;&#20256;&#24863;&#22120;&#30340;&#25903;&#25345;&#19979;&#65292;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#65292;&#32780;&#36825;&#20123;&#35774;&#22791;&#21487;&#20197;&#25903;&#25345;&#36825;&#20123;&#30417;&#35270;&#29992;&#20363;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#36825;&#26679;&#30340;&#24212;&#29992;&#26696;&#20363;&#65306;&#20351;&#29992;&#19982;&#26234;&#33021;&#38899;&#31665;&#30456;&#36830;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#20998;&#31867;&#22120;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#19981;&#24433;&#21709;&#26234;&#33021;&#38899;&#31665;&#30340;&#25928;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#36867;&#36991;&#19982;&#26234;&#33021;&#38899;&#31665;&#30456;&#36830;&#30340;&#40657;&#30418;SER&#20998;&#31867;&#22120;&#12290;&#36890;&#36807;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#23545;&#25239;&#36867;&#36991;&#30340;&#35270;&#35282;&#26469;&#32771;&#34385;&#36825;&#20010;&#38544;&#31169;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21517;&#20026;&#8220;&#36890;&#36807;&#36951;&#20256;&#35268;&#21010;&#20987;&#36133;&#22768;&#23398;&#24773;&#24863;&#35782;&#21035;&#30340;DARE-GP&#8221;&#65292;&#23427;&#20351;&#29992;&#36951;&#20256;&#35268;&#21010;&#29983;&#25104;&#38750;&#20405;&#20837;&#24615;&#30340;&#28155;&#21152;&#38899;&#39057;&#25200;&#21160;&#65288;AAPs&#65289;&#12290;&#36890;&#36807;&#32422;&#26463;&#36825;&#20123;AAP&#30340;&#36827;&#21270;&#65292;&#21487;&#20197;&#20445;&#25252;&#36716;&#24405;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#38477;&#20302;SER&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;AAP&#30340;&#21152;&#24615;&#29305;&#24615;&#65292;&#20197;&#21450;&#20026;&#29305;&#23450;&#30446;&#26631;&#29983;&#25104;&#36825;&#20123;AAPs&#30340;&#26041;&#27861;&#65292;&#20351;DARE-GP&#25104;&#20026;&#19968;&#20010;&#26377;&#25928;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotional Surveillance is an emerging area with wide-reaching privacy concerns. These concerns are exacerbated by ubiquitous IoT devices with multiple sensors that can support these surveillance use cases. The work presented here considers one such use case: the use of a speech emotion recognition (SER) classifier tied to a smart speaker. This work demonstrates the ability to evade black-box SER classifiers tied to a smart speaker without compromising the utility of the smart speaker. This privacy concern is considered through the lens of adversarial evasion of machine learning. Our solution, Defeating Acoustic Recognition of Emotion via Genetic Programming (DARE-GP), uses genetic programming to generate non-invasive additive audio perturbations (AAPs). By constraining the evolution of these AAPs, transcription accuracy can be protected while simultaneously degrading SER classifier performance. The additive nature of these AAPs, along with an approach that generates these AAPs for a fi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#20351;&#29992;Mixup&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#20154;&#26426;&#21327;&#20316;&#30740;&#31350;&#65292;&#21457;&#29616;&#20154;&#31867;&#30693;&#35273;&#19982;&#21512;&#25104;&#25968;&#25454;&#26631;&#31614;&#19981;&#19968;&#33268;&#65292;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#25552;&#39640;&#19979;&#28216;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.01202</link><description>&lt;p&gt;
&#20154;&#26426;&#21327;&#20316;Mixup
&lt;/p&gt;
&lt;p&gt;
Human-in-the-Loop Mixup. (arXiv:2211.01202v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#20351;&#29992;Mixup&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#20154;&#26426;&#21327;&#20316;&#30740;&#31350;&#65292;&#21457;&#29616;&#20154;&#31867;&#30693;&#35273;&#19982;&#21512;&#25104;&#25968;&#25454;&#26631;&#31614;&#19981;&#19968;&#33268;&#65292;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#25552;&#39640;&#19979;&#28216;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#27169;&#22411;&#34920;&#31034;&#19982;&#20154;&#31867;&#23545;&#40784;&#24050;&#34987;&#21457;&#29616;&#33021;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#26041;&#27861;&#36890;&#24120;&#19987;&#27880;&#20110;&#26631;&#20934;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;&#21512;&#25104;&#25968;&#25454;&#27491;&#22312;&#34028;&#21187;&#21457;&#23637;&#24182;&#25512;&#21160;&#26426;&#22120;&#23398;&#20064;&#30340;&#35768;&#22810;&#36827;&#23637;&#65292;&#20294;&#24448;&#24448;&#19981;&#28165;&#26970;&#21512;&#25104;&#26631;&#31614;&#26159;&#21542;&#19982;&#20154;&#31867;&#30340;&#30693;&#35273;&#20445;&#25345;&#19968;&#33268;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#34920;&#31034;&#19981;&#19982;&#20154;&#31867;&#23545;&#40784;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;Mixup&#20013;&#20351;&#29992;&#30340;&#21512;&#25104;&#25968;&#25454;&#65306;&#19968;&#31181;&#24378;&#22823;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#26657;&#20934;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#20840;&#38754;&#30340;&#21551;&#21457;&#24335;&#25509;&#21475;&#65292;&#24182;&#23558;&#20854;&#21457;&#24067;&#20026;HILL MixE Suite&#65292;&#24182;&#25307;&#21215;&#20102;159&#21517;&#21442;&#19982;&#32773;&#23545;Mixup&#31034;&#20363;&#36827;&#34892;&#20102;&#24863;&#30693;&#21028;&#26029;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#20154;&#31867;&#30693;&#35273;&#19981;&#19968;&#33268;&#22320;&#19982;&#20256;&#32479;&#19978;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#26631;&#31614;&#23545;&#40784;&#65292;&#24182;&#24320;&#22987;&#23637;&#31034;&#36825;&#20123;&#21457;&#29616;&#30340;&#36866;&#29992;&#24615;&#65292;&#20197;&#28508;&#22312;&#22320;&#25552;&#39640;&#19979;&#28216;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning model representations to humans has been found to improve robustness and generalization. However, such methods often focus on standard observational data. Synthetic data is proliferating and powering many advances in machine learning; yet, it is not always clear whether synthetic labels are perceptually aligned to humans -- rendering it likely model representations are not human aligned. We focus on the synthetic data used in mixup: a powerful regularizer shown to improve model robustness, generalization, and calibration. We design a comprehensive series of elicitation interfaces, which we release as HILL MixE Suite, and recruit 159 participants to provide perceptual judgments along with their uncertainties, over mixup examples. We find that human perceptions do not consistently align with the labels traditionally used for synthetic points, and begin to demonstrate the applicability of these findings to potentially increase the reliability of downstream models, particularly wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#27169;&#22411;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#20998;&#24067;&#36716;&#21464;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21019;&#24314;&#20849;&#20139;&#30340;&#28304;/&#30446;&#26631;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#65292;&#20351;&#24471;&#28304;&#22521;&#35757;&#30340;&#20998;&#31867;&#22120;&#33021;&#22815;&#22312;&#30446;&#26631;&#22495;&#19978;&#20445;&#25345;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.00807</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#27169;&#22411;&#33258;&#36866;&#24212;&#29992;&#20110;&#26080;&#28304;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Model Adaptation for Source-free Segmentation of Medical Images. (arXiv:2211.00807v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#27169;&#22411;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#20998;&#24067;&#36716;&#21464;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21019;&#24314;&#20849;&#20139;&#30340;&#28304;/&#30446;&#26631;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#65292;&#20351;&#24471;&#28304;&#22521;&#35757;&#30340;&#20998;&#31867;&#22120;&#33021;&#22815;&#22312;&#30446;&#26631;&#22495;&#19978;&#20445;&#25345;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26222;&#21450;&#20351;&#24471;&#22312;&#25552;&#20379;&#36275;&#22815;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#35821;&#20041;&#20998;&#21106;&#32593;&#32476;&#22312;&#21307;&#23398;&#39046;&#22495;&#23454;&#29616;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#39044;&#27979;&#20998;&#24067;&#19981;&#21516;&#30340;&#22270;&#20687;&#30340;&#35821;&#20041;&#26144;&#23556;&#26102;&#65292;&#36825;&#20123;&#32593;&#32476;&#22312;&#27867;&#21270;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38656;&#35201;&#22312;&#26032;&#30340;&#20998;&#24067;&#19978;&#23545;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#12290;&#36825;&#20010;&#26114;&#36149;&#30340;&#36807;&#31243;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#26469;&#29983;&#25104;&#35757;&#32451;&#26631;&#31614;&#12290;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#65292;&#36890;&#36807;&#36873;&#25321;&#25104;&#20687;&#35774;&#22791;&#65288;&#22914;MRI&#25110;CT&#25195;&#25551;&#20202;&#65289;&#21487;&#33021;&#20250;&#33258;&#28982;&#20135;&#29983;&#20998;&#24067;&#36716;&#21464;&#12290;&#20026;&#20102;&#20811;&#26381;&#22312;&#22312;&#25104;&#21151;&#22320;&#22312;&#19968;&#20010;&#23436;&#20840;&#27880;&#37322;&#30340;&#8220;&#28304;&#22495;&#8221;&#20013;&#35757;&#32451;&#27169;&#22411;&#20043;&#21518;&#65292;&#22312;&#30446;&#26631;&#22495;&#20013;&#26631;&#35760;&#22270;&#20687;&#30340;&#38656;&#35201;&#65292;&#21487;&#20197;&#20351;&#29992;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#12290;&#22823;&#22810;&#25968;UDA&#26041;&#27861;&#36890;&#36807;&#21019;&#24314;&#20849;&#20139;&#30340;&#28304;/&#30446;&#26631;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#26469;&#30830;&#20445;&#30446;&#26631;&#27867;&#21270;&#12290;&#36825;&#20351;&#24471;&#28304;&#22521;&#35757;&#30340;&#20998;&#31867;&#22120;&#33021;&#22815;&#22312;&#30446;&#26631;&#22495;&#19978;&#20445;&#25345;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;UDA&#26041;&#27861;&#35201;&#20040;&#36873;&#25321;&#24615;&#22320;&#20559;&#21521;&#28304;&#29305;&#24449;&#31354;&#38388;&#65292;&#20351;&#24471;&#30446;&#26631;&#22495;&#34920;&#29616;&#19981;&#20339;&#65292;&#35201;&#20040;&#23884;&#20837;&#20102;&#20855;&#20307;&#39046;&#22495;&#30693;&#35782;&#65292;&#23548;&#33268;&#26080;&#27861;&#27867;&#21270;&#21040;&#19981;&#21516;&#30340;&#30446;&#26631;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent prevalence of deep neural networks has lead semantic segmentation networks to achieve human-level performance in the medical field when sufficient training data is provided. Such networks however fail to generalize when tasked with predicting semantic maps for out-of-distribution images, requiring model re-training on the new distributions. This expensive process necessitates expert knowledge in order to generate training labels. Distribution shifts can arise naturally in the medical field via the choice of imaging device, i.e. MRI or CT scanners. To combat the need for labeling images in a target domain after a model is successfully trained in a fully annotated \textit{source domain} with a different data distribution, unsupervised domain adaptation (UDA) can be used. Most UDA approaches ensure target generalization by creating a shared source/target latent feature space. This allows a source trained classifier to maintain performance on the target domain. However most UDA 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28508;&#22312;&#22810;&#27169;&#24577;&#21151;&#33021;&#22270;&#27169;&#22411;&#20272;&#35745;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#20272;&#35745;&#36716;&#25442;&#31639;&#23376;&#21644;&#28508;&#22312;&#22270;&#26469;&#22635;&#34917;&#24403;&#21069;&#31185;&#23398;&#26041;&#27861;&#22312;&#20272;&#35745;&#22810;&#27169;&#24577;&#21151;&#33021;&#25968;&#25454;&#22270;&#27169;&#22411;&#26041;&#38754;&#30340;&#31354;&#30333;</title><link>http://arxiv.org/abs/2210.17237</link><description>&lt;p&gt;
&#28508;&#22312;&#22810;&#27169;&#24577;&#21151;&#33021;&#22270;&#27169;&#22411;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Latent Multimodal Functional Graphical Model Estimation. (arXiv:2210.17237v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28508;&#22312;&#22810;&#27169;&#24577;&#21151;&#33021;&#22270;&#27169;&#22411;&#20272;&#35745;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#20272;&#35745;&#36716;&#25442;&#31639;&#23376;&#21644;&#28508;&#22312;&#22270;&#26469;&#22635;&#34917;&#24403;&#21069;&#31185;&#23398;&#26041;&#27861;&#22312;&#20272;&#35745;&#22810;&#27169;&#24577;&#21151;&#33021;&#25968;&#25454;&#22270;&#27169;&#22411;&#26041;&#38754;&#30340;&#31354;&#30333;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#21516;&#22810;&#27169;&#24577;&#21151;&#33021;&#25968;&#25454;&#37319;&#38598;&#26159;&#19968;&#31181;&#29616;&#20195;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#36817;&#22312;&#31070;&#32463;&#23398;&#21644;&#29983;&#29289;&#31185;&#23398;&#20013;&#30340;&#24037;&#31243;&#31361;&#30772;&#65292;&#21487;&#20197;&#21516;&#26102;&#20174;&#21516;&#19968;&#20027;&#20307;&#20013;&#27979;&#37327;&#26469;&#33258;&#22810;&#31181;&#27169;&#24335;&#30340;&#21151;&#33021;&#25968;&#25454;&#12290;&#33719;&#21462;&#36825;&#26679;&#30340;&#25968;&#25454;&#30340;&#19968;&#20010;&#37325;&#35201;&#21160;&#26426;&#26159;&#36890;&#36807;&#32467;&#21512;&#22810;&#27169;&#24577;&#20449;&#21495;&#26469;&#21457;&#29616;&#28508;&#22312;&#30340;&#36830;&#25509;&#24615;&#12290;&#23613;&#31649;&#23384;&#22312;&#31185;&#23398;&#20852;&#36259;&#65292;&#20294;&#22312;&#20272;&#35745;&#22810;&#27169;&#24577;&#21151;&#33021;&#25968;&#25454;&#19979;&#30340;&#22270;&#27169;&#22411;&#26041;&#38754;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32508;&#21512;&#26694;&#26550;&#65292;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#35782;&#21035;&#20174;&#35266;&#27979;&#31354;&#38388;&#21040;&#28508;&#22312;&#31354;&#38388;&#30340;&#31639;&#23376;&#26144;&#23556;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#21516;&#26102;&#20272;&#35745;&#36716;&#25442;&#31639;&#23376;&#21644;&#28508;&#22312;&#22270;&#12290;&#36825;&#20010;&#20272;&#35745;&#22120;&#22522;&#20110;&#20559;&#30456;&#20851;&#31639;&#23376;&#65292;&#25105;&#20204;&#20174;&#22810;&#20803;&#21040;&#21151;&#33021;&#35774;&#32622;&#20013;&#20005;&#26684;&#25512;&#24191;&#20102;&#23427;&#12290;&#25105;&#20204;&#30340;&#31243;&#24207;&#26159;pr&#23553;&#38381;&#30340;
&lt;/p&gt;
&lt;p&gt;
Joint multimodal functional data acquisition, where functional data from multiple modes are measured simultaneously from the same subject, has emerged as an exciting modern approach enabled by recent engineering breakthroughs in the neurological and biological sciences. One prominent motivation to acquire such data is to enable new discoveries of the underlying connectivity by combining multimodal signals. Despite the scientific interest, there remains a gap in principled statistical methods for estimating the graph underlying multimodal functional data. To this end, we propose a new integrative framework that models the data generation process and identifies operators mapping from the observation space to the latent space. We then develop an estimator that simultaneously estimates the transformation operators and the latent graph. This estimator is based on the partial correlation operator, which we rigorously extend from the multivariate to the functional setting. Our procedure is pr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22266;&#23450;&#27493;&#38271;&#30340;&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398;&#22312;&#25968;&#25454;&#27969;&#19981;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#30340;&#28151;&#21512;&#24615;&#36136;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#22823;&#25968;&#23450;&#24459;&#21644;&#20989;&#25968;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#12290;</title><link>http://arxiv.org/abs/2210.02092</link><description>&lt;p&gt;
&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398;&#30340;&#20989;&#25968;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#21644;&#22823;&#25968;&#23450;&#24459;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Functional Central Limit Theorem and Strong Law of Large Numbers for Stochastic Gradient Langevin Dynamics. (arXiv:2210.02092v2 [math.PR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02092
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22266;&#23450;&#27493;&#38271;&#30340;&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398;&#22312;&#25968;&#25454;&#27969;&#19981;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#30340;&#28151;&#21512;&#24615;&#36136;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#22823;&#25968;&#23450;&#24459;&#21644;&#20989;&#25968;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#31639;&#27861;&#30340;&#28151;&#21512;&#24615;&#36136;&#65306;&#20855;&#26377;&#22266;&#23450;&#27493;&#38271;&#30340;&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398;&#65288;SGLD&#65289;&#12290;&#25968;&#25454;&#27969;&#19981;&#34987;&#20551;&#35774;&#20026;&#29420;&#31435;&#65292;&#22240;&#27492; SGLD &#19981;&#26159;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#32780;&#20165;&#20165;&#26159;&#19968;&#20010;&#38543;&#26426;&#29615;&#22659;&#20013;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#36825;&#26497;&#22823;&#22320;&#22797;&#26434;&#20102;&#25968;&#23398;&#22788;&#29702;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102; SGLD &#30340;&#22823;&#25968;&#23450;&#24459;&#21644;&#20989;&#25968;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the mixing properties of an important optimization algorithm of machine learning: the stochastic gradient Langevin dynamics (SGLD) with a fixed step size. The data stream is not assumed to be independent hence the SGLD is not a Markov chain, merely a \emph{Markov chain in a random environment}, which complicates the mathematical treatment considerably. We derive a strong law of large numbers and a functional central limit theorem for SGLD.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27979;&#35797;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#20256;&#32479;&#32479;&#35745;&#25216;&#26415;&#23545;&#22522;&#37329;&#34920;&#29616;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#21457;&#29616;&#20351;&#29992;&#29616;&#20195;&#36125;&#21494;&#26031;&#20248;&#21270;&#35757;&#32451;&#30340;LSTM&#21644;GRUs&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#22522;&#37329;&#30340;&#22799;&#26222;&#27604;&#29575;&#12290;&#23558;LSTM&#21644;GRUs&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#38598;&#25104;&#21487;&#20197;&#23454;&#29616;&#26368;&#20339;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2209.09649</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#38598;&#25104;&#25216;&#26415;&#39044;&#27979;&#20114;&#24800;&#22522;&#37329;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Predicting Mutual Funds' Performance using Deep Learning and Ensemble Techniques. (arXiv:2209.09649v3 [q-fin.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27979;&#35797;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#20256;&#32479;&#32479;&#35745;&#25216;&#26415;&#23545;&#22522;&#37329;&#34920;&#29616;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#21457;&#29616;&#20351;&#29992;&#29616;&#20195;&#36125;&#21494;&#26031;&#20248;&#21270;&#35757;&#32451;&#30340;LSTM&#21644;GRUs&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#22522;&#37329;&#30340;&#22799;&#26222;&#27604;&#29575;&#12290;&#23558;LSTM&#21644;GRUs&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#38598;&#25104;&#21487;&#20197;&#23454;&#29616;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#22522;&#37329;&#34920;&#29616;&#23545;&#25237;&#36164;&#32773;&#21644;&#22522;&#37329;&#32463;&#29702;&#37117;&#26377;&#30410;&#22788;&#65292;&#20294;&#20063;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#27979;&#35797;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#21542;&#27604;&#20256;&#32479;&#32479;&#35745;&#25216;&#26415;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#22522;&#37329;&#34920;&#29616;&#12290;&#22522;&#37329;&#34920;&#29616;&#36890;&#24120;&#36890;&#36807;&#22799;&#26222;&#27604;&#29575;&#26469;&#35780;&#20272;&#65292;&#22799;&#26222;&#27604;&#29575;&#34920;&#31034;&#39118;&#38505;&#35843;&#25972;&#21518;&#30340;&#34920;&#29616;&#65292;&#20197;&#30830;&#20445;&#22312;&#22522;&#37329;&#20043;&#38388;&#26377;&#21487;&#27604;&#24615;&#12290;&#25105;&#20204;&#26681;&#25454;&#32654;&#22269;&#25237;&#36164;&#20110;&#19978;&#24066;&#22823;&#30424;&#32929;&#30340;600&#22810;&#21482;&#24320;&#25918;&#24335;&#32929;&#31080;&#22411;&#20114;&#24800;&#22522;&#37329;&#30340;&#26376;&#24230;&#22238;&#25253;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#35745;&#31639;&#20102;&#24180;&#21270;&#22799;&#26222;&#27604;&#29575;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#29616;&#20195;&#36125;&#21494;&#26031;&#20248;&#21270;&#35757;&#32451;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRUs&#65289;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#27604;&#20256;&#32479;&#32479;&#35745;&#26041;&#27861;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#22522;&#37329;&#30340;&#22799;&#26222;&#27604;&#29575;&#12290;&#23558;LSTM&#21644;GRUs&#30340;&#39044;&#27979;&#32467;&#26524;&#32452;&#21512;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#25152;&#26377;&#27169;&#22411;&#20013;&#26368;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting fund performance is beneficial to both investors and fund managers, and yet is a challenging task. In this paper, we have tested whether deep learning models can predict fund performance more accurately than traditional statistical techniques. Fund performance is typically evaluated by the Sharpe ratio, which represents the risk-adjusted performance to ensure meaningful comparability across funds. We calculated the annualised Sharpe ratios based on the monthly returns time series data for more than 600 open-end mutual funds investing in listed large-cap equities in the United States. We find that long short-term memory (LSTM) and gated recurrent units (GRUs) deep learning methods, both trained with modern Bayesian optimization, provide higher accuracy in forecasting funds' Sharpe ratios than traditional statistical ones. An ensemble method, which combines forecasts from LSTM and GRUs, achieves the best performance of all models. There is evidence to say that deep learning an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#23376;&#31354;&#38388;&#32422;&#26463;&#30340;&#20998;&#25955;&#23398;&#20064;&#20013;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24494;&#20998;&#38543;&#26426;&#37327;&#21270;&#22120;&#21387;&#32553;&#20272;&#35745;&#20540;&#65292;&#22312;&#20445;&#25345;&#20302;&#35823;&#24046;&#30340;&#21516;&#26102;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2209.07821</link><description>&lt;p&gt;
&#22522;&#20110;&#23376;&#31354;&#38388;&#32422;&#26463;&#30340;&#20998;&#25955;&#23398;&#20064;&#20013;&#30340;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quantization for decentralized learning under subspace constraints. (arXiv:2209.07821v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#23376;&#31354;&#38388;&#32422;&#26463;&#30340;&#20998;&#25955;&#23398;&#20064;&#20013;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24494;&#20998;&#38543;&#26426;&#37327;&#21270;&#22120;&#21387;&#32553;&#20272;&#35745;&#20540;&#65292;&#22312;&#20445;&#25345;&#20302;&#35823;&#24046;&#30340;&#21516;&#26102;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20998;&#25955;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#20195;&#29702;&#37117;&#26377;&#33258;&#24049;&#30340;&#20195;&#20215;&#20989;&#25968;&#38656;&#35201;&#26368;&#23567;&#21270;&#65292;&#21516;&#26102;&#36824;&#35201;&#28385;&#36275;&#23376;&#31354;&#38388;&#32422;&#26463;&#65292;&#35201;&#27714;&#32593;&#32476;&#20013;&#30340;&#26368;&#23567;&#21270;&#32773;&#20301;&#20110;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#12290;&#36825;&#31181;&#32422;&#26463;&#24418;&#24335;&#21253;&#25324;&#20849;&#35782;&#25110;&#21333;&#20219;&#21153;&#20248;&#21270;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#65292;&#24182;&#20801;&#35768;&#26356;&#19968;&#33324;&#30340;&#20219;&#21153;&#30456;&#20851;&#24615;&#27169;&#22411;&#65292;&#22914;&#22810;&#20219;&#21153;&#24179;&#28369;&#24615;&#21644;&#32806;&#21512;&#20248;&#21270;&#12290;&#20026;&#20102;&#24212;&#23545;&#36890;&#20449;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#20998;&#25955;&#31574;&#30053;&#65292;&#20854;&#20013;&#20195;&#29702;&#22312;&#19982;&#37051;&#23621;&#36890;&#20449;&#20043;&#21069;&#20351;&#29992;&#24494;&#20998;&#38543;&#26426;&#37327;&#21270;&#22120;&#21387;&#32553;&#20854;&#20272;&#35745;&#20540;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#37327;&#21270;&#22122;&#22768;&#30340;&#19968;&#20123;&#19968;&#33324;&#26465;&#20214;&#21644;&#36275;&#22815;&#23567;&#30340;&#27493;&#38271;$\mu$&#19979;&#65292;&#35813;&#31574;&#30053;&#22312;&#22343;&#26041;&#35823;&#24046;&#21644;&#24179;&#22343;&#27604;&#29305;&#29575;&#26041;&#38754;&#37117;&#26159;&#31283;&#23450;&#30340;&#65306;&#36890;&#36807;&#20943;&#23567;$\mu$&#65292;&#21487;&#20197;&#20445;&#25345;&#20272;&#35745;&#35823;&#24046;&#24456;&#23567;&#65288;&#19982;$\mu$&#25968;&#37327;&#32423;&#30456;&#24403;&#65289;&#65292;&#32780;&#19981;&#22686;&#21152;&#26080;&#38480;&#30340;&#27604;&#29305;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider decentralized optimization problems where agents have individual cost functions to minimize subject to subspace constraints that require the minimizers across the network to lie in low-dimensional subspaces. This constrained formulation includes consensus or single-task optimization as special cases, and allows for more general task relatedness models such as multitask smoothness and coupled optimization. In order to cope with communication constraints, we propose and study an adaptive decentralized strategy where the agents employ differential randomized quantizers to compress their estimates before communicating with their neighbors. The analysis shows that, under some general conditions on the quantization noise, and for sufficiently small step-sizes $\mu$, the strategy is stable both in terms of mean-square error and average bit rate: by reducing $\mu$, it is possible to keep the estimation errors small (on the order of $\mu$) without increasing indefinit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21387;&#32553;&#27169;&#22359;&#21270;&#21644;&#32452;&#21512;&#26469;&#20462;&#34917;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24369;&#28857;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.06116</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22359;&#21270;&#21644;&#32452;&#21512;&#26469;&#20462;&#34917;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24369;&#28857;
&lt;/p&gt;
&lt;p&gt;
Patching Weak Convolutional Neural Network Models through Modularization and Composition. (arXiv:2209.06116v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21387;&#32553;&#27169;&#22359;&#21270;&#21644;&#32452;&#21512;&#26469;&#20462;&#34917;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24369;&#28857;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#24182;&#19981;&#24635;&#26159;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;&#20462;&#34917;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24369;&#28857;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#26469;&#25913;&#36827;&#23427;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#27169;&#22359;&#21270;&#26041;&#27861;CNNSplitter&#65292;&#23427;&#23558;&#20855;&#26377;$N$&#31867;&#20998;&#31867;&#20219;&#21153;&#30340;&#24378;CNN&#27169;&#22411;&#20998;&#35299;&#20026;$N$&#20010;&#36739;&#23567;&#30340;CNN&#27169;&#22359;&#12290;&#27599;&#20010;&#27169;&#22359;&#26159;&#19968;&#20010;&#23376;&#27169;&#22411;&#65292;&#21253;&#21547;&#24378;&#27169;&#22411;&#30340;&#37096;&#20998;&#21367;&#31215;&#26680;&#12290;&#20026;&#20102;&#20462;&#34917;&#22312;&#30446;&#26631;&#31867;&#21035;&#65288;TC&#65289;&#19978;&#34920;&#29616;&#19981;&#20339;&#30340;&#24369;CNN&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#20174;&#24378;CNN&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#30456;&#24212;&#27169;&#22359;&#30456;&#32467;&#21512;&#12290;&#36825;&#26679;&#65292;&#24369;CNN&#27169;&#22411;&#35782;&#21035;TC&#30340;&#33021;&#21147;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;&#23427;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite great success in many applications, deep neural networks are not always robust in practice. For instance, a convolutional neuron network (CNN) model for classification tasks often performs unsatisfactorily in classifying some particular classes of objects. In this work, we are concerned with patching the weak part of a CNN model instead of improving it through the costly retraining of the entire model. Inspired by the fundamental concepts of modularization and composition in software engineering, we propose a compressed modularization approach, CNNSplitter, which decomposes a strong CNN model for $N$-class classification into $N$ smaller CNN modules. Each module is a sub-model containing a part of the convolution kernels of the strong model. To patch a weak CNN model that performs unsatisfactorily on a target class (TC), we compose the weak CNN model with the corresponding module obtained from a strong CNN model. The ability of the weak CNN model to recognize the TC can thus be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24320;&#25918;&#35789;&#27719;&#27169;&#22411;&#29983;&#25104;&#33258;&#23450;&#20041;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25551;&#36848;&#24615;&#21477;&#23376;&#30340;&#26041;&#24335;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#22270;&#20687;&#31867;&#21035;&#30340;&#37325;&#35201;&#21306;&#20998;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2209.03320</link><description>&lt;p&gt;
&#38271;&#20160;&#20040;&#26679;&#30340;&#40493;&#22068;&#20861;&#65311;&#29983;&#25104;&#33258;&#23450;&#20041;&#25552;&#31034;&#20197;&#36827;&#34892;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
What does a platypus look like? Generating customized prompts for zero-shot image classification. (arXiv:2209.03320v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24320;&#25918;&#35789;&#27719;&#27169;&#22411;&#29983;&#25104;&#33258;&#23450;&#20041;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25551;&#36848;&#24615;&#21477;&#23376;&#30340;&#26041;&#24335;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#22270;&#20687;&#31867;&#21035;&#30340;&#37325;&#35201;&#21306;&#20998;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#35789;&#27719;&#27169;&#22411;&#26159;&#22270;&#20687;&#20998;&#31867;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26032;&#33539;&#24335;&#12290;&#19982;&#20256;&#32479;&#30340;&#20998;&#31867;&#27169;&#22411;&#19981;&#21516;&#65292;&#24320;&#25918;&#35789;&#27719;&#27169;&#22411;&#22312;&#25512;&#26029;&#26102;&#23545;&#20219;&#24847;&#25351;&#23450;&#30340;&#31867;&#21035;&#38598;&#21512;&#36827;&#34892;&#20998;&#31867;&#65292;&#36825;&#20123;&#31867;&#21035;&#30001;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#12290;&#36825;&#31181;&#33258;&#28982;&#35821;&#35328;&#36890;&#24120;&#21253;&#21547;&#19968;&#32452;&#25163;&#20889;&#27169;&#26495;&#65288;&#20363;&#22914;&#65292;&#8220;&#19968;&#24352;{}&#30340;&#29031;&#29255;&#8221;&#65289;&#65292;&#27599;&#20010;&#31867;&#21035;&#21517;&#31216;&#37117;&#20250;&#22635;&#20805;&#36827;&#21435;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#26356;&#39640;&#20934;&#30830;&#24230;&#30340;&#25552;&#31034;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20219;&#21153;&#39046;&#22495;&#30340;&#26174;&#24335;&#30693;&#35782;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#25163;&#24037;&#26500;&#24314;&#21477;&#23376;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#24320;&#25918;&#35789;&#27719;&#27169;&#22411;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#21019;&#24314;&#20986;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#33258;&#23450;&#20041;&#25552;&#31034;&#65288;CuPL&#65292;&#35835;&#20316;&#8220;couple&#8221;&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;LLMs&#20013;&#21253;&#21547;&#30340;&#30693;&#35782;&#26469;&#29983;&#25104;&#21253;&#21547;&#22270;&#20687;&#31867;&#21035;&#30340;&#37325;&#35201;&#21306;&#20998;&#29305;&#24449;&#30340;&#25551;&#36848;&#24615;&#21477;&#23376;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#26356;&#21152;&#37325;&#35270;&#36825;&#20123;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-vocabulary models are a promising new paradigm for image classification. Unlike traditional classification models, open-vocabulary models classify among any arbitrary set of categories specified with natural language during inference. This natural language, called "prompts", typically consists of a set of hand-written templates (e.g., "a photo of a {}") which are completed with each of the category names. This work introduces a simple method to generate higher accuracy prompts, without relying on any explicit knowledge of the task domain and with far fewer hand-constructed sentences. To achieve this, we combine open-vocabulary models with large language models (LLMs) to create Customized Prompts via Language models (CuPL, pronounced "couple"). In particular, we leverage the knowledge contained in LLMs in order to generate many descriptive sentences that contain important discriminating characteristics of the image categories. This allows the model to place a greater importance on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SAFARI&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#37322;&#21487;&#38752;&#24615;&#12290;&#35813;&#26041;&#27861;&#38024;&#23545;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#35299;&#20915;&#30340;&#20960;&#20010;&#25361;&#25112;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#35299;&#37322;&#24046;&#24322;&#21644;&#19968;&#33324;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#30340;&#27010;&#29575;&#27010;&#24565;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#24230;&#37327;&#19981;&#20840;&#38754;&#12289;XAI&#25216;&#26415;&#24322;&#36136;&#24615;&#21644;&#35823;&#35299;&#32597;&#35265;&#24615;&#31561;&#38382;&#39064;&#12290;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#21644;&#23376;&#38598;&#27169;&#25311;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2208.09418</link><description>&lt;p&gt;
SAFARI&#65306;&#40065;&#26834;&#24615;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#30340;&#22810;&#21151;&#33021;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SAFARI: Versatile and Efficient Evaluations for Robustness of Interpretability. (arXiv:2208.09418v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SAFARI&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#37322;&#21487;&#38752;&#24615;&#12290;&#35813;&#26041;&#27861;&#38024;&#23545;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#35299;&#20915;&#30340;&#20960;&#20010;&#25361;&#25112;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#35299;&#37322;&#24046;&#24322;&#21644;&#19968;&#33324;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#30340;&#27010;&#29575;&#27010;&#24565;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#24230;&#37327;&#19981;&#20840;&#38754;&#12289;XAI&#25216;&#26415;&#24322;&#36136;&#24615;&#21644;&#35823;&#35299;&#32597;&#35265;&#24615;&#31561;&#38382;&#39064;&#12290;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#21644;&#23376;&#38598;&#27169;&#25311;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#24615;&#26159;&#24314;&#31435;&#21487;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#36947;&#38556;&#30861;&#12290;&#23613;&#31649;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#31038;&#21306;&#20570;&#20986;&#20102;&#24040;&#22823;&#30340;&#21162;&#21147;&#65292;&#20294;&#35299;&#37322;&#32570;&#20047;&#40065;&#26834;&#24615;&#8212;&#8212;&#26080;&#27861;&#21306;&#20998;&#30340;&#36755;&#20837;&#25200;&#21160;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#21516;&#30340;&#35299;&#37322;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#38024;&#23545;&#32473;&#23450;&#30340;XAI&#26041;&#27861;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#35782;&#21035;&#20102;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#20849;&#21516;&#24212;&#23545;&#30340;&#20960;&#20010;&#25361;&#25112;&#65306;i)&#29616;&#26377;&#25351;&#26631;&#19981;&#20840;&#38754;&#65307;ii)XAI&#25216;&#26415;&#39640;&#24230;&#24322;&#36136;&#65307;iii)&#35823;&#35299;&#36890;&#24120;&#26159;&#32597;&#35265;&#20107;&#20214;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#65292;&#20998;&#21035;&#28041;&#21450;&#26368;&#22351;&#24773;&#20917;&#35299;&#37322;&#24046;&#24322;&#21644;&#19968;&#33324;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#30340;&#27010;&#29575;&#27010;&#24565;&#12290;&#20351;&#29992;&#20855;&#26377;&#23450;&#21046;&#36866;&#24212;&#24230;&#20989;&#25968;&#30340;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#26469;&#35299;&#20915;&#32422;&#26463;&#20248;&#21270;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#26368;&#22351;&#24773;&#20917;&#35780;&#20272;&#12290;&#20351;&#29992;&#19987;&#38376;&#29992;&#20110;&#20272;&#35745;&#32597;&#35265;&#20107;&#20214;&#27010;&#29575;&#30340;&#23376;&#38598;&#27169;&#25311;&#65288;SS&#65289;&#26469;&#36827;&#34892;&#25972;&#20307;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability of Deep Learning (DL) is a barrier to trustworthy AI. Despite great efforts made by the Explainable AI (XAI) community, explanations lack robustness -- indistinguishable input perturbations may lead to different XAI results. Thus, it is vital to assess how robust DL interpretability is, given an XAI method. In this paper, we identify several challenges that the state-of-the-art is unable to cope with collectively: i) existing metrics are not comprehensive; ii) XAI techniques are highly heterogeneous; iii) misinterpretations are normally rare events. To tackle these challenges, we introduce two black-box evaluation methods, concerning the worst-case interpretation discrepancy and a probabilistic notion of how robust in general, respectively. Genetic Algorithm (GA) with bespoke fitness function is used to solve constrained optimisation for efficient worst-case evaluation. Subset Simulation (SS), dedicated to estimate rare event probabilities, is used for evaluating overa
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#37051;&#22495;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#23454;&#29616;&#20102;&#25152;&#26377;&#20256;&#24863;&#22120;&#38388;&#30340;&#36890;&#35759;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#20154;&#35270;&#35273;&#23548;&#33322;&#20013;&#32570;&#20047;&#20840;&#23616;&#23450;&#20301;&#20449;&#24687;&#30340;&#38382;&#39064;&#24182;&#23454;&#29616;&#20102;&#39640;&#25928;&#23548;&#33322;&#12290;</title><link>http://arxiv.org/abs/2208.00759</link><description>&lt;p&gt;
&#30475;&#35265;&#26426;&#22120;&#20154;&#30475;&#19981;&#21040;&#30340;&#19996;&#35199;&#65306;&#23398;&#20064;&#21327;&#20316;&#24863;&#30693;&#36827;&#34892;&#35270;&#35273;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
See What the Robot Can't See: Learning Cooperative Perception for Visual Navigation. (arXiv:2208.00759v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00759
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#37051;&#22495;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#23454;&#29616;&#20102;&#25152;&#26377;&#20256;&#24863;&#22120;&#38388;&#30340;&#36890;&#35759;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#20154;&#35270;&#35273;&#23548;&#33322;&#20013;&#32570;&#20047;&#20840;&#23616;&#23450;&#20301;&#20449;&#24687;&#30340;&#38382;&#39064;&#24182;&#23454;&#29616;&#20102;&#39640;&#25928;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#22312;&#19968;&#20010;&#26410;&#30693;&#29615;&#22659;&#20013;&#21033;&#29992;&#35270;&#35273;&#20256;&#24863;&#22120;&#24341;&#23548;&#26426;&#22120;&#20154;&#21040;&#36798;&#30446;&#30340;&#22320;&#30340;&#38382;&#39064;&#12290;&#22312;&#32570;&#20047;&#20840;&#23616;&#23450;&#20301;&#20449;&#24687;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#20256;&#24863;&#22120;&#32534;&#30721;&#21644;&#20256;&#36882;&#30456;&#20851;&#30340;&#35270;&#35282;&#20449;&#24687;&#32473;&#26426;&#22120;&#20154;&#65292;&#35753;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#20351;&#29992;&#31532;&#19968;&#35270;&#35282;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#23613;&#21487;&#33021;&#39640;&#25928;&#22320;&#23548;&#33322;&#21040;&#30446;&#26631;&#12290;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#22522;&#20110;&#37051;&#22495;&#30340;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#65292;&#25105;&#20204;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#20811;&#26381;&#20102;&#35753;&#25152;&#26377;&#20256;&#24863;&#22120;&#29978;&#33267;&#26080;&#27861;&#30452;&#25509;&#30475;&#21040;&#30446;&#26631;&#30340;&#20256;&#24863;&#22120;&#39044;&#27979;&#36890;&#21521;&#30446;&#26631;&#26368;&#30701;&#36335;&#26041;&#21521;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of navigating a mobile robot towards a target in an unknown environment that is endowed with visual sensors, where neither the robot nor the sensors have access to global positioning information and only use first-person-view images. In order to overcome the need for positioning, we train the sensors to encode and communicate relevant viewpoint information to the mobile robot, whose objective it is to use this information to navigate as efficiently as possible to the target. We overcome the challenge of enabling all the sensors (even those that cannot directly see the target) to predict the direction along the shortest path to the target by implementing a neighborhood-based feature aggregation module using a Graph Neural Network (GNN) architecture. In our experiments, we first demonstrate generalizability to previously unseen environments with various sensor layouts. Our results show that by using communication between the sensors and the robot, we achieve up to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#22270;&#25299;&#25169;&#32467;&#26500;&#65292;&#29992;&#20110;&#36328;&#25968;&#25454;&#20179;&#24211;&#32852;&#37030;&#23398;&#20064;&#65292;&#36890;&#36807;&#23396;&#31435;&#33410;&#28857;&#23454;&#29616;&#27169;&#22411;&#32858;&#21512;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2207.09657</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#22270;&#25299;&#25169;&#22312;&#36328;&#25968;&#25454;&#20179;&#24211;&#32852;&#37030;&#23398;&#20064;&#20013;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
Reducing Training Time in Cross-Silo Federated Learning using Multigraph Topology. (arXiv:2207.09657v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#22270;&#25299;&#25169;&#32467;&#26500;&#65292;&#29992;&#20110;&#36328;&#25968;&#25454;&#20179;&#24211;&#32852;&#37030;&#23398;&#20064;&#65292;&#36890;&#36807;&#23396;&#31435;&#33410;&#28857;&#23454;&#29616;&#27169;&#22411;&#32858;&#21512;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#22240;&#20026;&#23427;&#20351;&#24471;&#22810;&#20010;&#21442;&#19982;&#32773;&#33021;&#22815;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#32780;&#26080;&#38656;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#12290;&#30446;&#21069;&#65292;&#36328;&#25968;&#25454;&#20179;&#24211;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#35757;&#32451;&#35774;&#32622;&#65292;&#23427;&#21033;&#29992;&#20960;&#30334;&#20010;&#21487;&#38752;&#30340;&#25968;&#25454;&#20179;&#24211;&#21644;&#39640;&#36895;&#35775;&#38382;&#38142;&#36335;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#35774;&#35745;&#19968;&#20010;&#31283;&#20581;&#30340;&#25299;&#25169;&#32467;&#26500;&#20197;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#36328;&#25968;&#25454;&#20179;&#24211;&#32852;&#37030;&#23398;&#20064;&#30340;&#22810;&#22270;&#25299;&#25169;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#35206;&#30422;&#22270;&#26500;&#24314;&#22810;&#22270;&#65292;&#28982;&#21518;&#23558;&#36825;&#20010;&#22810;&#22270;&#35299;&#26512;&#25104;&#20855;&#26377;&#23396;&#31435;&#33410;&#28857;&#30340;&#19981;&#21516;&#31616;&#21333;&#22270;&#12290;&#23396;&#31435;&#33410;&#28857;&#30340;&#23384;&#22312;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#22312;&#31561;&#24453;&#20854;&#20182;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27169;&#22411;&#32858;&#21512;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;&#23545;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#26368;&#26032;&#30340;&#25299;&#25169;&#32467;&#26500;&#30456;&#27604;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is an active research topic since it enables several participants to jointly train a model without sharing local data. Currently, cross-silo federated learning is a popular training setting that utilizes a few hundred reliable data silos with high-speed access links to training a model. While this approach has been widely applied in real-world scenarios, designing a robust topology to reduce the training time remains an open problem. In this paper, we present a new multigraph topology for cross-silo federated learning. We first construct the multigraph using the overlay graph. We then parse this multigraph into different simple graphs with isolated nodes. The existence of isolated nodes allows us to perform model aggregation without waiting for other nodes, hence effectively reducing the training time. Intensive experiments on three public datasets show that our proposed method significantly reduces the training time compared with recent state-of-the-art topologies w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DiffOpt.jl&#65292;&#19968;&#20010;&#22522;&#20110;MathOptInterface&#26500;&#24314;&#30340;Julia&#24211;&#65292;&#23427;&#21487;&#20197;&#23545;&#20219;&#20309;&#27169;&#22411;&#30340;&#21442;&#25968;&#36827;&#34892;&#24494;&#20998;&#27714;&#35299;&#65292;&#19981;&#20165;&#38480;&#20110;&#20984;&#38181;&#35268;&#21010;&#21644;&#20108;&#27425;&#35268;&#21010;&#26631;&#20934;&#24418;&#24335;&#12290;&#20351;&#29992;&#35813;&#24211;, &#21487;&#20197;&#23454;&#29616;&#28789;&#27963;&#39640;&#25928;&#22320;&#27714;&#35299;&#19968;&#31995;&#21015;&#27169;&#22411;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20026;&#22522;&#20110;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2206.06135</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22411;&#21464;&#25442;&#23454;&#29616;&#28789;&#27963;&#21487;&#23548;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Flexible Differentiable Optimization via Model Transformations. (arXiv:2206.06135v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiffOpt.jl&#65292;&#19968;&#20010;&#22522;&#20110;MathOptInterface&#26500;&#24314;&#30340;Julia&#24211;&#65292;&#23427;&#21487;&#20197;&#23545;&#20219;&#20309;&#27169;&#22411;&#30340;&#21442;&#25968;&#36827;&#34892;&#24494;&#20998;&#27714;&#35299;&#65292;&#19981;&#20165;&#38480;&#20110;&#20984;&#38181;&#35268;&#21010;&#21644;&#20108;&#27425;&#35268;&#21010;&#26631;&#20934;&#24418;&#24335;&#12290;&#20351;&#29992;&#35813;&#24211;, &#21487;&#20197;&#23454;&#29616;&#28789;&#27963;&#39640;&#25928;&#22320;&#27714;&#35299;&#19968;&#31995;&#21015;&#27169;&#22411;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20026;&#22522;&#20110;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiffOpt.jl&#65292;&#36825;&#26159;&#19968;&#20010;Julia&#24211;&#65292;&#23427;&#21487;&#20197;&#23545;&#21253;&#21547;&#30446;&#26631;&#20540;&#21644;/&#25110;&#32422;&#26463;&#26465;&#20214;&#30340;&#20219;&#24847;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#27714;&#35299;&#65292;&#24182;&#36827;&#34892;&#24494;&#20998;&#12290;&#36825;&#20010;&#24211;&#22522;&#20110;MathOptInterface&#26500;&#24314;&#65292;&#22240;&#27492;&#21487;&#20197;&#21033;&#29992;&#20016;&#23500;&#30340;&#27714;&#35299;&#22120;&#29983;&#24577;&#31995;&#32479;&#65292;&#24182;&#19982;&#20687;JuMP&#36825;&#26679;&#30340;&#24314;&#27169;&#35821;&#35328;&#32452;&#21512;&#24471;&#24456;&#22909;&#12290;DiffOpt&#25552;&#20379;&#21069;&#21521;&#21644;&#21453;&#21521;&#24494;&#20998;&#27169;&#24335;&#65292;&#20351;&#24471;&#21487;&#20197;&#24212;&#29992;&#20110;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;&#21453;&#21521;&#20256;&#25773;&#21644;&#25935;&#24863;&#24615;&#20998;&#26512;&#31561;&#22810;&#31181;&#29992;&#36884;&#65292;&#36890;&#36807;&#23558;&#32422;&#26463;&#20248;&#21270;&#19982;&#31471;&#21040;&#31471;&#21487;&#23548;&#24615;&#32534;&#31243;&#30456;&#32467;&#21512;&#12290;DiffOpt&#26159;&#22522;&#20110;&#20004;&#20010;&#24050;&#30693;&#30340;&#35268;&#21017;&#26469;&#27714;&#35299;&#30340;&#65292;&#20998;&#21035;&#26159;&#20984;&#38181;&#35268;&#21010;&#21644;&#20108;&#27425;&#35268;&#21010;&#26631;&#20934;&#24418;&#24335;&#30340;&#24494;&#20998;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21487;&#20197;&#36890;&#36807;&#27169;&#22411;&#36716;&#25442;&#36827;&#34892;&#24494;&#20998;&#65292;&#22240;&#27492;&#29992;&#25143;&#19981;&#20165;&#38480;&#20110;&#36825;&#20123;&#24418;&#24335;&#65292;&#36824;&#21487;&#20197;&#38024;&#23545;&#21487;&#20197;&#36716;&#25442;&#20026;&#36825;&#20123;&#26631;&#20934;&#24418;&#24335;&#30340;&#20219;&#20309;&#27169;&#22411;&#30340;&#21442;&#25968;&#36827;&#34892;&#24494;&#20998;&#12290;&#36825;&#29305;&#21035;&#21253;&#25324;&#28151;&#21512;&#20223;&#23556;&#38181;&#32422;&#26463;&#21644;&#20984;&#22235;&#27425;&#32422;&#26463;&#30340;&#31243;&#24207;&#65292;&#36825;&#20123;&#31243;&#24207;&#26080;&#27861;&#30452;&#25509;&#24314;&#27169;&#20026;&#26631;&#20934;&#38181;&#24418;&#31243;&#24207;&#12290;&#22240;&#27492;&#65292;DiffOpt&#20351;&#24471;&#21487;&#20197;&#28789;&#27963;&#39640;&#25928;&#22320;&#27714;&#35299;&#19968;&#31995;&#21015;&#27169;&#22411;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20026;&#22522;&#20110;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce DiffOpt.jl, a Julia library to differentiate through the solution of optimization problems with respect to arbitrary parameters present in the objective and/or constraints. The library builds upon MathOptInterface, thus leveraging the rich ecosystem of solvers and composing well with modeling languages like JuMP. DiffOpt offers both forward and reverse differentiation modes, enabling multiple use cases from hyperparameter optimization to backpropagation and sensitivity analysis, bridging constrained optimization with end-to-end differentiable programming. DiffOpt is built on two known rules for differentiating quadratic programming and conic programming standard forms. However, thanks ability to differentiate through model transformation, the user is not limited to these forms and can differentiate with respect to the parameters of any model that can be reformulated into these standard forms. This notably includes programs mixing affine conic constraints and convex quadrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#19968;&#31181;&#20351;&#29992;&#25513;&#34109;&#33258;&#22238;&#24402;&#27969;&#21644;&#26680;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#23545;&#24212;&#20110;&#26680;&#24515;&#31185;&#23398;&#21442;&#25968;&#30340;&#36793;&#38469;&#21518;&#39564;&#23494;&#24230;&#12290;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#36793;&#38469;&#24211;&#23572;&#24052;&#20811;-&#21202;&#24067;&#21202;&#25955;&#24230;&#12289;&#36793;&#38469;&#36125;&#21494;&#26031;&#27169;&#22411;&#32500;&#24230;&#12289;&#20284;&#28982;&#20989;&#25968;&#27169;&#25311;&#21644;&#20808;&#39564;&#27169;&#25311;&#31561;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2205.12841</link><description>&lt;p&gt;
&#29992;&#20154;&#36896;&#40644;&#27833;&#20174;&#21518;&#39564;&#26679;&#26412;&#20013;&#21435;&#38500;&#33026;&#32938;
&lt;/p&gt;
&lt;p&gt;
Removing the fat from your posterior samples with margarine. (arXiv:2205.12841v3 [astro-ph.IM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#19968;&#31181;&#20351;&#29992;&#25513;&#34109;&#33258;&#22238;&#24402;&#27969;&#21644;&#26680;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#23545;&#24212;&#20110;&#26680;&#24515;&#31185;&#23398;&#21442;&#25968;&#30340;&#36793;&#38469;&#21518;&#39564;&#23494;&#24230;&#12290;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#36793;&#38469;&#24211;&#23572;&#24052;&#20811;-&#21202;&#24067;&#21202;&#25955;&#24230;&#12289;&#36793;&#38469;&#36125;&#21494;&#26031;&#27169;&#22411;&#32500;&#24230;&#12289;&#20284;&#28982;&#20989;&#25968;&#27169;&#25311;&#21644;&#20808;&#39564;&#27169;&#25311;&#31561;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20998;&#26512;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#23431;&#23449;&#23398;&#39046;&#22495;&#30340;&#19981;&#21487;&#25110;&#32570;&#24037;&#20855;&#65292;&#21253;&#25324;&#24341;&#21147;&#27874;&#30740;&#31350;&#12289;&#23431;&#23449;&#24494;&#27874;&#32972;&#26223;&#21644;&#23431;&#23449;&#40654;&#26126;&#26102;&#26399;&#30340;21&#21400;&#31859;&#20449;&#21495;&#31561;&#29616;&#35937;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;&#22797;&#26434;&#27169;&#22411;&#19982;&#25551;&#36848;&#20851;&#38190;&#23431;&#23449;&#23398;&#21644;&#22825;&#20307;&#29289;&#29702;&#20449;&#21495;&#20197;&#21450;&#21508;&#31181;&#27745;&#26579;&#20449;&#21495;&#21644;&#20202;&#22120;&#25928;&#24212;&#30340;'&#24178;&#25200;&#21442;&#25968;'&#25311;&#21512;&#21040;&#25968;&#25454;&#30340;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#19968;&#31181;&#20351;&#29992;&#25513;&#34109;&#33258;&#22238;&#24402;&#27969;&#21644;&#26680;&#23494;&#24230;&#20272;&#35745;&#22120;&#26469;&#23398;&#20064;&#23545;&#24212;&#20110;&#26680;&#24515;&#31185;&#23398;&#21442;&#25968;&#30340;&#36793;&#38469;&#21518;&#39564;&#23494;&#24230;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36793;&#38469;&#25110;&#8220;&#26080;&#24178;&#25200;&#8221;&#30340;&#21518;&#39564;&#20998;&#24067;&#21450;&#20854;&#30456;&#20851;&#30340;&#20284;&#28982;&#20989;&#25968;&#20855;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#21253;&#25324;&#35745;&#31639;&#20197;&#21069;&#38590;&#20197;&#22788;&#29702;&#30340;&#36793;&#38469;&#24211;&#23572;&#24052;&#20811;-&#21202;&#24067;&#21202;&#25955;&#24230;&#21644;&#36793;&#38469;&#36125;&#21494;&#26031;&#27169;&#22411;&#32500;&#24230;&#65292;&#20284;&#28982;&#20989;&#25968;&#27169;&#25311;&#21644;&#20808;&#39564;&#27169;&#25311;&#12290;&#25105;&#20204;&#20351;&#29992;&#29609;&#20855;&#20363;&#23376;&#21644;&#23454;&#38469;&#26696;&#20363;&#20998;&#21035;&#23637;&#31034;&#20102;&#27599;&#20010;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian analysis has become an indispensable tool across many different cosmological fields including the study of gravitational waves, the Cosmic Microwave Background and the 21-cm signal from the Cosmic Dawn among other phenomena. The method provides a way to fit complex models to data describing key cosmological and astrophysical signals and a whole host of contaminating signals and instrumental effects modelled with 'nuisance parameters'. In this paper, we summarise a method that uses Masked Autoregressive Flows and Kernel Density Estimators to learn marginal posterior densities corresponding to core science parameters. We find that the marginal or 'nuisance-free' posteriors and the associated likelihoods have an abundance of applications including; the calculation of previously intractable marginal Kullback-Leibler divergences and marginal Bayesian Model Dimensionalities, likelihood emulation and prior emulation. We demonstrate each application using toy examples, examples from t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#36827;&#34892;&#21452;&#21521;&#20114;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#31070;&#32463;&#32593;&#32476;&#23637;&#31034;&#20854;&#25152;&#23398;&#22240;&#26524;&#22270;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#20462;&#25913;&#22240;&#26524;&#22270;&#21518;&#37325;&#26032;&#27880;&#20837;&#26426;&#22120;&#20013;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#35843;&#35797;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#24335;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.09787</link><description>&lt;p&gt;
&#21487;&#20105;&#35758;&#31070;&#32463;&#32593;&#32476;&#30340;&#22240;&#26524;&#21457;&#29616;&#19982;&#30693;&#35782;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery and Knowledge Injection for Contestable Neural Networks. (arXiv:2205.09787v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#36827;&#34892;&#21452;&#21521;&#20114;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#31070;&#32463;&#32593;&#32476;&#23637;&#31034;&#20854;&#25152;&#23398;&#22240;&#26524;&#22270;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#20462;&#25913;&#22240;&#26524;&#22270;&#21518;&#37325;&#26032;&#27880;&#20837;&#26426;&#22120;&#20013;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#35843;&#35797;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#24335;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#26159;&#21542;&#23398;&#20064;&#21040;&#20102;&#30456;&#20851;&#30340;&#22240;&#26524;&#20851;&#31995;&#23578;&#19981;&#28165;&#26970;&#65292;&#32780;&#23427;&#20204;&#30340;&#40657;&#31665;&#29305;&#24615;&#20351;&#24471;&#27169;&#22411;&#26500;&#24314;&#32773;&#38590;&#20197;&#29702;&#35299;&#21644;&#35843;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#36890;&#36807;&#20801;&#35768;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#26426;&#22120;&#23637;&#31034;&#20854;&#25152;&#23398;&#22240;&#26524;&#22270;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#20462;&#25913;&#22240;&#26524;&#22270;&#21518;&#37325;&#26032;&#27880;&#20837;&#26426;&#22120;&#20013;&#65292;&#23454;&#29616;&#21452;&#21521;&#20114;&#21160;&#12290;&#25152;&#23398;&#27169;&#22411;&#20445;&#35777;&#31526;&#21512;&#22240;&#26524;&#22270;&#24182;&#36981;&#24490;&#19987;&#23478;&#30693;&#35782;&#65292;&#20854;&#20013;&#37096;&#20998;&#30693;&#35782;&#20063;&#21487;&#20197;&#20107;&#20808;&#32473;&#23450;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#34892;&#20026;&#36827;&#34892;&#21487;&#35270;&#21270;&#24182;&#23454;&#29616;&#30693;&#35782;&#27880;&#20837;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#32467;&#26500;&#24182;&#25903;&#25745;&#39044;&#27979;&#30340;&#20174;&#19994;&#32773;&#36827;&#34892;&#35843;&#35797;&#12290;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25913;&#36827;&#39044;&#27979;&#24615;&#33021;&#39640;&#36798;2.4&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have proven to be effective at solving machine learning tasks but it is unclear whether they learn any relevant causal relationships, while their black-box nature makes it difficult for modellers to understand and debug them. We propose a novel method overcoming these issues by allowing a two-way interaction whereby neural-network-empowered machines can expose the underpinning learnt causal graphs and humans can contest the machines by modifying the causal graphs before re-injecting them into the machines. The learnt models are guaranteed to conform to the graphs and adhere to expert knowledge, some of which can also be given up-front. By building a window into the model behaviour and enabling knowledge injection, our method allows practitioners to debug networks based on the causal structure discovered from the data and underpinning the predictions. Experiments with real and synthetic tabular data show that our method improves predictive performance up to 2.4x while pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27169;&#22411;&#37327;&#21270;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20302;&#20301;&#23485;&#23384;&#20648;&#20840;&#31934;&#24230;&#20540;&#20197;&#23454;&#29616;&#33410;&#32422;&#20869;&#23384;&#21644;&#25805;&#20316;&#25104;&#26412;&#30340;&#21387;&#32553;&#26041;&#27861;&#12290;&#25991;&#31456;&#20998;&#31867;&#20171;&#32461;&#20102;&#21508;&#31181;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#20351;&#29992;&#27604;&#20363;&#22240;&#23376;&#21305;&#37197;&#25968;&#25454;&#33539;&#22260;&#21644;&#36866;&#24403;&#30340;&#35757;&#32451;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#36824;&#22238;&#39038;&#20102;&#27169;&#22411;&#37327;&#21270;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#20248;&#32570;&#28857;&#21644;&#24403;&#21069;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2205.07877</link><description>&lt;p&gt;
&#27169;&#22411;&#37327;&#21270;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Model Quantization for Deep Neural Networks. (arXiv:2205.07877v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.07877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27169;&#22411;&#37327;&#21270;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20302;&#20301;&#23485;&#23384;&#20648;&#20840;&#31934;&#24230;&#20540;&#20197;&#23454;&#29616;&#33410;&#32422;&#20869;&#23384;&#21644;&#25805;&#20316;&#25104;&#26412;&#30340;&#21387;&#32553;&#26041;&#27861;&#12290;&#25991;&#31456;&#20998;&#31867;&#20171;&#32461;&#20102;&#21508;&#31181;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#20351;&#29992;&#27604;&#20363;&#22240;&#23376;&#21305;&#37197;&#25968;&#25454;&#33539;&#22260;&#21644;&#36866;&#24403;&#30340;&#35757;&#32451;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#36824;&#22238;&#39038;&#20102;&#27169;&#22411;&#37327;&#21270;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#20248;&#32570;&#28857;&#21644;&#24403;&#21069;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#26159;&#38656;&#35201;&#22823;&#37327;&#30340;&#21442;&#25968;&#23384;&#20648;&#21644;&#36816;&#31639;&#20250;&#24102;&#26469;&#30828;&#20214;&#25104;&#26412;&#30340;&#22686;&#21152;&#21644;&#25361;&#25112;&#12290;&#23545;&#27492;&#65292;&#25552;&#20986;&#20102;&#21387;&#32553;&#26041;&#27861;&#20197;&#35774;&#35745;&#39640;&#25928;&#30340;&#21152;&#36895;&#22120;&#65292;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#26041;&#27861;&#26159;&#25226;&#20840;&#31934;&#24230;&#30340;&#20540;&#23384;&#20648;&#22312;&#20302;&#20301;&#23485;&#20013;&#65292;&#36825;&#23601;&#21487;&#20197;&#33410;&#32422;&#20869;&#23384;&#21516;&#26102;&#29992;&#20302;&#25104;&#26412;&#30340;&#31616;&#21333;&#36816;&#31639;&#20195;&#26367;&#21407;&#26412;&#30340;&#25805;&#20316;&#12290;&#30001;&#20110;&#27169;&#22411;&#37327;&#21270;&#30340;&#28789;&#27963;&#24615;&#21644;&#23545;&#35774;&#35745;&#39640;&#25928;&#30828;&#20214;&#30340;&#24433;&#21709;&#65292;&#26368;&#36817;&#20960;&#24180;&#25552;&#20986;&#20102;&#35768;&#22810;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#26041;&#27861;&#65292;&#22240;&#27492;&#38656;&#35201;&#36827;&#34892;&#32508;&#21512;&#24615;&#30340;&#35843;&#26597;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#12289;&#20998;&#26512;&#21644;&#27604;&#36739;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;&#37327;&#21270;&#27010;&#24565;&#24182;&#20174;&#19981;&#21516;&#35282;&#24230;&#20998;&#31867;&#26041;&#27861;&#65292;&#35752;&#35770;&#20102;&#20351;&#29992;&#27604;&#20363;&#22240;&#23376;&#21305;&#37197;&#25968;&#25454;&#33539;&#22260;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#20351;&#29992;&#36866;&#24403;&#30340;&#35757;&#32451;&#26041;&#27861;&#36991;&#20813;&#31934;&#24230;&#25439;&#22833;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#36817;&#24180;&#26469;&#23545;&#27169;&#22411;&#37327;&#21270;&#30340;&#30740;&#31350;&#65292;&#24182;&#24378;&#35843;&#20854;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning by deep neural networks are significant. But using these networks has been accompanied by a huge number of parameters for storage and computations that leads to an increase in the hardware cost and posing challenges. Therefore, compression approaches have been proposed to design efficient accelerators. One important approach for deep neural network compression is quantization that full-precision values are stored in low bit-width. In this way, in addition to memory saving, the operations will be replaced by simple ones with low cost. Many methods are suggested for DNNs Quantization in recent years, because of flexibility and influence in designing efficient hardware. Therefore, an integrated report is essential for better understanding, analysis, and comparison. In this paper, we provide a comprehensive survey. We describe the quantization concepts and categorize the methods from different perspectives. We discuss using the scale factor to match the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#24577;&#25805;&#20316;&#31526;&#21644;&#21160;&#24577;&#31867;&#21035;&#30340;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#39044;&#27979;&#24066;&#22330;&#21644;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2205.03906</link><description>&lt;p&gt;
&#21160;&#24577;&#25805;&#20316;&#31526;&#65292;&#21160;&#24577;&#31867;&#21035;&#65306;&#20174;&#28145;&#24230;&#23398;&#20064;&#21040;&#39044;&#27979;&#24066;&#22330;
&lt;/p&gt;
&lt;p&gt;
Dynamic Operads, Dynamic Categories: From Deep Learning to Prediction Markets. (arXiv:2205.03906v4 [math.CT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.03906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#24577;&#25805;&#20316;&#31526;&#21644;&#21160;&#24577;&#31867;&#21035;&#30340;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#39044;&#27979;&#24066;&#22330;&#21644;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#32452;&#32455;&#31995;&#32479;&#36866;&#24212;&#20869;&#22806;&#21387;&#21147;&#65292;&#36825;&#21457;&#29983;&#22312;&#25277;&#35937;&#23618;&#27425;&#30340;&#25152;&#26377;&#32423;&#21035;&#19978;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#26088;&#22312;&#28165;&#26224;&#22320;&#24605;&#32771;&#36825;&#20010;&#24819;&#27861;&#65292;&#24182;&#22312;&#21069;&#35328;&#20013;&#24191;&#27867;&#38416;&#36848;&#36825;&#20010;&#24819;&#27861;&#65292;&#20351;&#20854;&#23545;&#20855;&#26377;&#21746;&#23398;&#20852;&#36259;&#30340;&#35835;&#32773;&#26377;&#26222;&#36941;&#30340;&#21487;&#29702;&#35299;&#24615;&#12290;&#22312;&#21097;&#19979;&#30340;&#31456;&#33410;&#20013;&#65292;&#25105;&#20204;&#36716;&#21521;&#26356;&#32039;&#20945;&#30340;&#33539;&#30068;&#35770;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#21160;&#24577;&#32452;&#32455;&#30340;&#24186;&#21322;&#21452;&#33539;&#30068;Org&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;Org&#20016;&#23500;&#30340;&#65292;&#25110;&#31216;&#20026;&#21160;&#24577;&#30340;&#65292;&#33539;&#30068;&#32467;&#26500;&#30340;&#23450;&#20041;&#65292;&#20363;&#22914;&#21160;&#24577;&#33539;&#30068;&#12289;&#25805;&#20316;&#31526;&#21644;&#24186;&#21322;&#33539;&#30068;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#22914;&#20309;&#23454;&#29616;&#28608;&#21169;&#21746;&#23398;&#24605;&#24819;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#20004;&#20010;&#21160;&#24577;&#33539;&#30068;&#32467;&#26500;&#30340;&#20363;&#23376;&#65306;&#39044;&#27979;&#24066;&#22330;&#20316;&#20026;&#21160;&#24577;&#25805;&#20316;&#31526;&#65292;&#28145;&#24230;&#23398;&#20064;&#20316;&#20026;&#21160;&#24577;&#24186;&#21322;&#33539;&#30068;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural organized systems adapt to internal and external pressures and this happens at all levels of the abstraction hierarchy. Wanting to think clearly about this idea motivates our paper, and so the idea is elaborated extensively in the introduction, which should be broadly accessible to a philosophically-interested audience. In the remaining sections, we turn to more compressed category theory. We define the monoidal double category Org of dynamic organizations, we provide definitions of Org-enriched, or dynamic, categorical structures -- e.g. dynamic categories, operads, and monoidal categories -- and we show how they instantiate the motivating philosophical ideas. We give two examples of dynamic categorical structures: prediction markets as a dynamic operad and deep learning as a dynamic monoidal category.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#29420;&#31435;&#30340;&#20998;&#21106;&#19978;&#37325;&#26032;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#35813;&#26041;&#27861;&#25913;&#21892;&#20102;&#32676;&#20307;&#40065;&#26834;&#24615;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#27169;&#22411;&#22312;&#20855;&#26377;&#23569;&#35265;&#29305;&#24449;&#30340;&#23376;&#32676;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2204.09583</link><description>&lt;p&gt;
&#22312;&#29420;&#31435;&#20998;&#21106;&#19978;&#37325;&#26032;&#35757;&#32451;&#20998;&#31867;&#22120;&#20197;&#25552;&#39640;&#32676;&#20307;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improved Group Robustness via Classifier Retraining on Independent Splits. (arXiv:2204.09583v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.09583
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#29420;&#31435;&#30340;&#20998;&#21106;&#19978;&#37325;&#26032;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#35813;&#26041;&#27861;&#25913;&#21892;&#20102;&#32676;&#20307;&#40065;&#26834;&#24615;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#27169;&#22411;&#22312;&#20855;&#26377;&#23569;&#35265;&#29305;&#24449;&#30340;&#23376;&#32676;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#21270;&#24179;&#22343;&#39118;&#38505;&#36827;&#34892;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36798;&#21040;&#24378;&#22823;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#26576;&#20010;&#23376;&#32676;&#22312;&#25972;&#20307;&#25968;&#25454;&#20013;&#20195;&#34920;&#24615;&#19981;&#36275;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#19979;&#38477;&#12290;&#32676;&#20307;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;Sagawa&#31561;&#20154;&#65292;2020a&#65289;&#65292;&#31616;&#31216;&#32676;&#20307;DRO&#65292;&#26159;&#23398;&#20064;&#20855;&#26377;&#24378;&#22823;&#26368;&#24046;&#32676;&#20307;&#24615;&#33021;&#27169;&#22411;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#26102;&#38656;&#35201;&#20026;&#27599;&#20010;&#31034;&#20363;&#25552;&#20379;&#32676;&#20307;&#26631;&#31614;&#65292;&#24182;&#19988;&#21487;&#33021;&#23545;&#23567;&#32676;&#20307;&#36807;&#25311;&#21512;&#65292;&#38656;&#35201;&#24378;&#27491;&#21017;&#21270;&#12290;&#22312;&#35757;&#32451;&#26102;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#30340;&#32676;&#20307;&#26631;&#31614;&#26102;&#65292;Just Train Twice&#65288;Liu&#31561;&#20154;&#65292;2021&#65289;&#65292;&#31616;&#31216;JTT&#65292;&#26159;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#39318;&#20808;&#20026;&#27599;&#20010;&#26080;&#26631;&#31614;&#31034;&#20363;&#25512;&#26029;&#20986;&#20266;&#32676;&#20307;&#26631;&#31614;&#65292;&#28982;&#21518;&#26681;&#25454;&#25512;&#26029;&#30340;&#32676;&#20307;&#26631;&#31614;&#24212;&#29992;&#32676;&#20307;DRO&#12290;&#25512;&#26029;&#36807;&#31243;&#23545;&#36807;&#25311;&#21512;&#20063;&#24456;&#25935;&#24863;&#65292;&#26377;&#26102;&#28041;&#21450;&#39069;&#22806;&#30340;&#36229;&#21442;&#25968;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#29420;&#31435;&#20998;&#21106;&#19978;&#30340;&#20998;&#31867;&#22120;&#37325;&#26032;&#35757;&#32451;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks trained by minimizing the average risk can achieve strong average performance. Still, their performance for a subgroup may degrade if the subgroup is underrepresented in the overall data population. Group distributionally robust optimization (Sagawa et al., 2020a), or group DRO in short, is a widely used baseline for learning models with strong worst-group performance. We note that this method requires group labels for every example at training time and can overfit to small groups, requiring strong regularization. Given a limited amount of group labels at training time, Just Train Twice (Liu et al., 2021), or JTT in short, is a two-stage method that infers a pseudo group label for every unlabeled example first, then applies group DRO based on the inferred group labels. The inference process is also sensitive to overfitting, sometimes involving additional hyperparameters. This paper designs a simple method based on the idea of classifier retraining on independent sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#36807;&#31243;&#32467;&#26524;&#39044;&#27979;&#20013;&#24212;&#29992;&#35299;&#37322;&#24615;&#27169;&#22411;&#30340;&#25351;&#21335;&#65292;&#24182;&#36890;&#36807;&#23545;&#20107;&#20214;&#12289;&#26696;&#20363;&#21644;&#25511;&#21046;&#27969;&#30340;&#20998;&#26512;&#65292;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;&#19981;&#21516;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2203.16073</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#22312;&#36807;&#31243;&#32467;&#26524;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;&#33719;&#24471;&#21487;&#35299;&#37322;&#21644;&#21487;&#20449;&#27169;&#22411;&#30340;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Explainability in Process Outcome Prediction: Guidelines to Obtain Interpretable and Faithful Models. (arXiv:2203.16073v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#36807;&#31243;&#32467;&#26524;&#39044;&#27979;&#20013;&#24212;&#29992;&#35299;&#37322;&#24615;&#27169;&#22411;&#30340;&#25351;&#21335;&#65292;&#24182;&#36890;&#36807;&#23545;&#20107;&#20214;&#12289;&#26696;&#20363;&#21644;&#25511;&#21046;&#27969;&#30340;&#20998;&#26512;&#65292;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;&#19981;&#21516;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#39044;&#27979;&#24615;&#36807;&#31243;&#30417;&#25511;&#39046;&#22495;&#24050;&#32463;&#24320;&#22987;&#37319;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#27169;&#22411;&#65292;&#20294;&#35780;&#20272;&#20173;&#20027;&#35201;&#22522;&#20110;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#26410;&#32771;&#34385;&#35299;&#37322;&#30340;&#21487;&#25805;&#20316;&#24615;&#21644;&#24433;&#21709;&#12290;&#26412;&#25991;&#36890;&#36807;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#26469;&#23450;&#20041;&#36807;&#31243;&#32467;&#26524;&#39044;&#27979;&#39046;&#22495;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20107;&#20214;&#12289;&#26696;&#20363;&#21644;&#25511;&#21046;&#27969;&#30340;&#35282;&#24230;&#36827;&#34892;&#20998;&#26512;&#36825;&#20123;&#29305;&#24615;&#65292;&#36825;&#26159;&#20856;&#22411;&#30340;&#22522;&#20110;&#36807;&#31243;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#21313;&#19977;&#20010;&#30495;&#23454;&#20107;&#20214;&#26085;&#24535;&#19978;&#23545;&#19971;&#20010;&#20998;&#31867;&#22120;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#31995;&#21015;&#36879;&#26126;&#21644;&#38750;&#36879;&#26126;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#34917;&#20805;&#20102;&#35299;&#37322;&#24615;&#25216;&#26415;&#12290;&#25509;&#19979;&#26469;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#22871;&#21517;&#20026;X-MOP&#30340;&#25351;&#21335;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#26681;&#25454;&#20010;&#20154;&#38656;&#27714;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#35299;&#37322;&#24615;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although a recent shift has been made in the field of predictive process monitoring to use models from the explainable artificial intelligence field, the evaluation still occurs mainly through performance-based metrics, thus not accounting for the actionability and implications of the explanations. In this paper, we define explainability through the interpretability of the explanations and the faithfulness of the explainability model in the field of process outcome prediction. The introduced properties are analysed along the event, case, and control flow perspective which are typical for a process-based analysis. This allows comparing inherently created explanations with post-hoc explanations. We benchmark seven classifiers on thirteen real-life events logs, and these cover a range of transparent and non-transparent machine learning and deep learning models, further complemented with explainability techniques. Next, this paper contributes a set of guidelines named X-MOP which allows se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;HiFi++&#65292;&#29992;&#20110;&#24102;&#23485;&#25193;&#23637;&#21644;&#35821;&#38899;&#22686;&#24378;&#20219;&#21153;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#29983;&#25104;&#22120;&#26550;&#26500;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#28040;&#32791;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2203.13086</link><description>&lt;p&gt;
HiFi++&#65306;&#29992;&#20110;&#24102;&#23485;&#25193;&#23637;&#21644;&#35821;&#38899;&#22686;&#24378;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HiFi++: a Unified Framework for Bandwidth Extension and Speech Enhancement. (arXiv:2203.13086v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.13086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;HiFi++&#65292;&#29992;&#20110;&#24102;&#23485;&#25193;&#23637;&#21644;&#35821;&#38899;&#22686;&#24378;&#20219;&#21153;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#29983;&#25104;&#22120;&#26550;&#26500;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#28040;&#32791;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476;&#22312;&#31070;&#32463;&#22768;&#30721;&#22120;&#20013;&#23637;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#26368;&#20339;&#30340;&#33258;&#22238;&#24402;&#21644;&#27969;&#27169;&#22411;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#19968;&#25104;&#21151;&#21487;&#20197;&#25193;&#23637;&#21040;&#20854;&#20182;&#26377;&#26465;&#20214;&#38899;&#39057;&#29983;&#25104;&#20219;&#21153;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312; HiFi &#22768;&#30721;&#22120;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; HiFi++ &#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#24102;&#23485;&#25193;&#23637;&#21644;&#35821;&#38899;&#22686;&#24378;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#25913;&#36827;&#30340;&#29983;&#25104;&#22120;&#26550;&#26500;&#65292;HiFi++ &#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#28040;&#32791;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial networks have recently demonstrated outstanding performance in neural vocoding outperforming best autoregressive and flow-based models. In this paper, we show that this success can be extended to other tasks of conditional audio generation. In particular, building upon HiFi vocoders, we propose a novel HiFi++ general framework for bandwidth extension and speech enhancement. We show that with the improved generator architecture, HiFi++ performs better or comparably with the state-of-the-art in these tasks while spending significantly less computational resources. The effectiveness of our approach is validated through a series of extensive experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#31070;&#32463;&#31639;&#23376;&#65288;LNO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#19981;&#21516;&#22495;&#19978;&#30340;&#30636;&#24577;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#35813;&#27169;&#22411;&#21487;&#36890;&#36807;&#19968;&#31181;&#20415;&#25463;&#30340;&#31574;&#30053;&#36827;&#34892;&#36793;&#30028;&#22788;&#29702;&#65292;&#24182;&#22312;&#26410;&#30693;&#22495;&#19978;&#39044;&#27979;&#35299;&#20915;&#26041;&#26696;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LNO&#27604;&#20256;&#32479;&#30340;&#26377;&#38480;&#20803;&#26041;&#27861;&#35745;&#31639;&#36895;&#24230;&#24555;&#32422;1000&#20493;&#65292;&#24182;&#19988;&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#21508;&#31181;&#27969;&#20307;&#27969;&#21160;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2203.08145</link><description>&lt;p&gt;
&#22312;&#19981;&#21516;&#22495;&#19978;&#35299;&#20915;&#30636;&#24577;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#23616;&#37096;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Local neural operator for solving transient partial differential equations on varied domains. (arXiv:2203.08145v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.08145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#31070;&#32463;&#31639;&#23376;&#65288;LNO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#19981;&#21516;&#22495;&#19978;&#30340;&#30636;&#24577;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#35813;&#27169;&#22411;&#21487;&#36890;&#36807;&#19968;&#31181;&#20415;&#25463;&#30340;&#31574;&#30053;&#36827;&#34892;&#36793;&#30028;&#22788;&#29702;&#65292;&#24182;&#22312;&#26410;&#30693;&#22495;&#19978;&#39044;&#27979;&#35299;&#20915;&#26041;&#26696;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LNO&#27604;&#20256;&#32479;&#30340;&#26377;&#38480;&#20803;&#26041;&#27861;&#35745;&#31639;&#36895;&#24230;&#24555;&#32422;1000&#20493;&#65292;&#24182;&#19988;&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#21508;&#31181;&#27969;&#20307;&#27969;&#21160;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26174;&#31034;&#20986;&#38477;&#20302;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#24040;&#22823;&#25104;&#26412;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#23578;&#26410;&#23436;&#20840;&#23454;&#29616;&#65292;&#22240;&#20026;&#31070;&#32463;&#32593;&#32476;&#22312;&#22266;&#23450;&#30340;&#22495;&#21644;&#36793;&#30028;&#19978;&#23450;&#20041;&#21644;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#19981;&#21516;&#22495;&#19978;&#35299;&#20915;&#30636;&#24577;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#23616;&#37096;&#31070;&#32463;&#31639;&#23376;&#65288;LNO&#65289;&#12290;&#23427;&#36824;&#37197;&#26377;&#19968;&#31181;&#20415;&#25463;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;&#36793;&#30028;&#22788;&#29702;&#65292;&#20351;&#24471;&#39044;&#35757;&#32451;&#30340;LNO&#33021;&#22815;&#39044;&#27979;&#19981;&#21516;&#22495;&#19978;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#28436;&#31034;&#65292;LNO&#20174;&#38543;&#26426;&#29983;&#25104;&#30340;&#25968;&#25454;&#26679;&#26412;&#20013;&#23398;&#20064;Navier-Stokes&#26041;&#31243;&#65292;&#28982;&#21518;&#39044;&#35757;&#32451;&#30340;LNO&#34987;&#29992;&#20316;&#26174;&#24335;&#25968;&#20540;&#26102;&#38388;&#27493;&#36827;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#26410;&#26366;&#35265;&#36807;&#30340;&#22495;&#20013;&#30340;&#27969;&#20307;&#27969;&#21160;&#65292;&#20363;&#22914;&#65292;&#30422;&#26495;&#39537;&#21160;&#33108;&#20869;&#30340;&#27969;&#21160;&#21644;&#27668;&#21160;&#32423;&#32852;&#20013;&#30340;&#27969;&#21160;&#12290;&#19982;&#20256;&#32479;&#30340;&#26377;&#38480;&#20803;&#26041;&#27861;&#30456;&#27604;&#65292;&#35745;&#31639;&#27668;&#21160;&#32423;&#32852;&#20013;&#30340;&#27969;&#21160;&#36895;&#24230;&#32422;&#24555;1000&#20493;&#12290;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;LNO&#30340;&#27714;&#35299;&#36807;&#31243;&#23454;&#29616;&#20102;&#24456;&#39640;&#30340;&#25928;&#29575;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) shows great potential to reduce the huge cost of solving partial differential equations (PDEs). However, it is not fully realized in practice as neural networks are defined and trained on fixed domains and boundaries. Herein, we propose local neural operator (LNO) for solving transient PDEs on varied domains. It comes together with a handy strategy including boundary treatments, enabling one pre-trained LNO to predict solutions on different domains. For demonstration, LNO learns Navier-Stokes equations from randomly generated data samples, and then the pre-trained LNO is used as an explicit numerical time-marching scheme to solve the flow of fluid on unseen domains, e.g., the flow in a lid-driven cavity and the flow across the cascade of airfoils. It is about 1000$\times$ faster than the conventional finite element method to calculate the flow across the cascade of airfoils. The solving process with pre-trained LNO achieves great efficiency, with significan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#26631;&#31614;&#12289;&#22024;&#26434;CXR&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#34955;&#30340;&#22810;&#26631;&#31614;&#25551;&#36848;&#31526;&#24179;&#28369;&#22320;&#37325;&#26032;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#65292;&#24182;&#36827;&#34892;&#35757;&#32451;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.01937</link><description>&lt;p&gt;
BoMD&#65306;&#36866;&#29992;&#20110;&#22024;&#26434;X&#20809;&#20998;&#31867;&#30340;&#22810;&#26631;&#31614;&#25551;&#36848;&#31526;&#21253;
&lt;/p&gt;
&lt;p&gt;
BoMD: Bag of Multi-label Descriptors for Noisy Chest X-ray Classification. (arXiv:2203.01937v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#26631;&#31614;&#12289;&#22024;&#26434;CXR&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#34955;&#30340;&#22810;&#26631;&#31614;&#25551;&#36848;&#31526;&#24179;&#28369;&#22320;&#37325;&#26032;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#65292;&#24182;&#36827;&#34892;&#35757;&#32451;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#38382;&#39064;&#30340;&#20998;&#31867;&#31934;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#20855;&#26377;&#28165;&#27905;&#26631;&#31614;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#36825;&#31181;&#25163;&#21160;&#27880;&#37322;&#30340;&#39640;&#25104;&#26412;&#65292;&#26032;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#21487;&#33021;&#38656;&#35201;&#20381;&#36182;&#20110;&#20174;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#30340;&#26426;&#22120;&#29983;&#25104;&#30340;&#22024;&#26434;&#26631;&#31614;&#12290;&#20107;&#23454;&#19978;&#65292;&#35768;&#22810;&#33016;&#37096;X&#20809;&#20998;&#31867;&#22120;&#24050;&#32463;&#20174;&#24102;&#26377;&#22024;&#26434;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#20013;&#24314;&#27169;&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#36807;&#31243;&#36890;&#24120;&#19981;&#20855;&#26377;&#22122;&#22768;&#26631;&#31614;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#65292;&#23548;&#33268;&#27425;&#20248;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;CXR&#25968;&#25454;&#38598;&#22823;&#22810;&#26159;&#22810;&#26631;&#35760;&#30340;&#65292;&#22240;&#27492;&#24403;&#21069;&#35774;&#35745;&#29992;&#20110;&#22810;&#31867;&#38382;&#39064;&#30340;&#22024;&#26434;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#19981;&#33021;&#36731;&#26494;&#22320;&#36827;&#34892;&#35843;&#25972;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22024;&#26434;&#22810;&#26631;&#31614;CXR&#23398;&#20064;&#65292;&#20854;&#20013;&#26816;&#27979;&#24182;&#24179;&#28369;&#22320;&#37325;&#26032;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#65292;&#28982;&#21518;&#29992;&#20110;&#35757;&#32451;&#24120;&#35265;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#12290;&#35813;&#26041;&#27861;&#20248;&#21270;&#20102;&#19968;&#20010;&#22522;&#20110;&#34955;&#30340;&#22810;&#26631;&#31614;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#20415;&#26377;&#25928;&#22320;&#20351;&#29992;&#20174;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning methods have shown outstanding classification accuracy in medical imaging problems, which is largely attributed to the availability of large-scale datasets manually annotated with clean labels. However, given the high cost of such manual annotation, new medical imaging classification problems may need to rely on machine-generated noisy labels extracted from radiology reports. Indeed, many Chest X-ray (CXR) classifiers have already been modelled from datasets with noisy labels, but their training procedure is in general not robust to noisy-label samples, leading to sub-optimal models. Furthermore, CXR datasets are mostly multi-label, so current noisy-label learning methods designed for multi-class problems cannot be easily adapted. In this paper, we propose a new method designed for the noisy multi-label CXR learning, which detects and smoothly re-labels samples from the dataset, which is then used to train common multi-label classifiers. The proposed method optimises a ba
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#22914;&#20309;&#26356;&#26032;&#39044;&#27979;&#39118;&#38505;&#35780;&#20998;&#26469;&#25351;&#23548;&#24178;&#39044;&#12290;&#20316;&#32773;&#25552;&#20986;&#20351;&#29992;&#30041;&#32622;&#38598;&#30340;&#26041;&#24335;&#36827;&#34892;&#26356;&#26032;&#65292;&#36890;&#36807;&#25214;&#21040;&#30041;&#32622;&#38598;&#30340;&#21512;&#36866;&#22823;&#23567;&#21487;&#20197;&#20445;&#35777;&#26356;&#26032;&#21518;&#30340;&#39118;&#38505;&#35780;&#20998;&#24615;&#33021;&#33391;&#22909;&#65292;&#21516;&#26102;&#20943;&#23569;&#30041;&#32622;&#26679;&#26412;&#25968;&#37327;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24635;&#25104;&#26412;&#22686;&#38271;&#36895;&#24230;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2202.06374</link><description>&lt;p&gt;
&#38024;&#23545;&#39044;&#27979;&#27169;&#22411;&#26356;&#26032;&#30340;&#30041;&#32622;&#38598;
&lt;/p&gt;
&lt;p&gt;
Holdouts set for predictive model updating. (arXiv:2202.06374v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06374
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#22914;&#20309;&#26356;&#26032;&#39044;&#27979;&#39118;&#38505;&#35780;&#20998;&#26469;&#25351;&#23548;&#24178;&#39044;&#12290;&#20316;&#32773;&#25552;&#20986;&#20351;&#29992;&#30041;&#32622;&#38598;&#30340;&#26041;&#24335;&#36827;&#34892;&#26356;&#26032;&#65292;&#36890;&#36807;&#25214;&#21040;&#30041;&#32622;&#38598;&#30340;&#21512;&#36866;&#22823;&#23567;&#21487;&#20197;&#20445;&#35777;&#26356;&#26032;&#21518;&#30340;&#39118;&#38505;&#35780;&#20998;&#24615;&#33021;&#33391;&#22909;&#65292;&#21516;&#26102;&#20943;&#23569;&#30041;&#32622;&#26679;&#26412;&#25968;&#37327;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24635;&#25104;&#26412;&#22686;&#38271;&#36895;&#24230;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#30340;&#29615;&#22659;&#20013;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#39044;&#27979;&#39118;&#38505;&#35780;&#20998;&#22312;&#25351;&#23548;&#24178;&#39044;&#26041;&#38754;&#36215;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#26356;&#26032;&#29992;&#20110;&#25351;&#23548;&#24178;&#39044;&#30340;&#39118;&#38505;&#35780;&#20998;&#21487;&#33021;&#23548;&#33268;&#20559;&#24046;&#39118;&#38505;&#20272;&#35745;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#8220;&#30041;&#32622;&#38598;&#8221;&#26469;&#36827;&#34892;&#26356;&#26032;-&#30041;&#32622;&#38598;&#26159;&#19968;&#20010;&#19981;&#25509;&#21463;&#39118;&#38505;&#35780;&#20998;&#25351;&#23548;&#24178;&#39044;&#30340;&#20154;&#32676;&#30340;&#23376;&#38598;&#12290;&#22312;&#30041;&#32622;&#38598;&#30340;&#22823;&#23567;&#19978;&#21462;&#24471;&#24179;&#34913;&#26159;&#20851;&#38190;&#65292;&#20197;&#30830;&#20445;&#26356;&#26032;&#21518;&#30340;&#39118;&#38505;&#35780;&#20998;&#24615;&#33021;&#33391;&#22909;&#65292;&#21516;&#26102;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#30041;&#32622;&#26679;&#26412;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;&#24635;&#25104;&#26412;&#21487;&#20197;&#20197;$O\left(N^{2/3}\right)$&#30340;&#36895;&#24230;&#22686;&#38271;&#65292;&#20854;&#20013;$N$&#26159;&#20154;&#21475;&#35268;&#27169;&#65292;&#24182;&#19988;&#35748;&#20026;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#27809;&#26377;&#31454;&#20105;&#24615;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#36890;&#36807;&#23450;&#20041;&#36866;&#24403;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20123;&#26465;&#20214;&#65292;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#30830;&#23450;&#26368;&#20339;&#30041;&#32622;&#38598;&#22823;&#23567;&#65288;OHS&#65289;&#65292;&#24182;&#24341;&#20837;&#21442;&#25968;&#21270;&#21644;&#21322;&#21442;&#25968;&#21270;&#31639;&#27861;&#26469;&#20272;&#35745;OHS&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#26368;&#26032;&#39118;&#38505;&#35780;&#20998;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In complex settings, such as healthcare, predictive risk scores play an increasingly crucial role in guiding interventions. However, directly updating risk scores used to guide intervention can lead to biased risk estimates. To address this, we propose updating using a `holdout set' - a subset of the population that does not receive interventions guided by the risk score. Striking a balance in the size of the holdout set is essential, to ensure good performance of the updated risk score whilst minimising the number of held out samples. We prove that this approach enables total costs to grow at a rate $O\left(N^{2/3}\right)$ for a population of size $N$, and argue that in general circumstances there is no competitive alternative. By defining an appropriate loss function, we describe conditions under which an optimal holdout size (OHS) can be readily identified, and introduce parametric and semi-parametric algorithms for OHS estimation, demonstrating their use on a recent risk score for 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#21487;&#35299;&#37322;&#36830;&#32493;&#25511;&#21046;&#26641;&#65288;ICCTs&#65289;&#30340;&#26641;&#29366;&#27169;&#22411;&#65292;&#36890;&#36807;&#29616;&#20195;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#65292;&#33021;&#22815;&#23398;&#20064;&#21040;&#39640;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#27604;&#22522;&#20934;&#27169;&#22411;&#39640;33%&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36798;&#21040;300&#20493;&#33267;600&#20493;&#30340;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2202.02352</link><description>&lt;p&gt;
&#23398;&#20064;&#33258;&#21160;&#39550;&#39542;&#30340;&#21487;&#35299;&#37322;&#12289;&#39640;&#24615;&#33021;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Interpretable, High-Performing Policies for Autonomous Driving. (arXiv:2202.02352v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.02352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#21487;&#35299;&#37322;&#36830;&#32493;&#25511;&#21046;&#26641;&#65288;ICCTs&#65289;&#30340;&#26641;&#29366;&#27169;&#22411;&#65292;&#36890;&#36807;&#29616;&#20195;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#65292;&#33021;&#22815;&#23398;&#20064;&#21040;&#39640;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#27604;&#22522;&#20934;&#27169;&#22411;&#39640;33%&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36798;&#21040;300&#20493;&#33267;600&#20493;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#22312;&#23398;&#20064;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#31574;&#30053;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#20540;&#24471;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#37319;&#29992;&#65292;&#20294;&#26159;&#36825;&#20123;&#31574;&#30053;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#38480;&#21046;&#20102;&#22312;&#23433;&#20840;&#20851;&#38190;&#21644;&#27861;&#24459;&#30417;&#31649;&#39046;&#22495;&#20013;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;&#33258;&#21160;&#39550;&#39542;&#35201;&#27714;&#21487;&#35299;&#37322;&#21644;&#21487;&#39564;&#35777;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#20197;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#36830;&#32493;&#25511;&#21046;&#26641;&#65288;ICCTs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#26641;&#29366;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#29616;&#20195;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#20135;&#29983;&#39640;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#20801;&#35768;&#22312;&#31232;&#30095;&#30340;&#31867;&#20284;&#20915;&#31574;&#26641;&#30340;&#34920;&#31034;&#20013;&#36827;&#34892;&#30452;&#25509;&#20248;&#21270;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#39046;&#22495;&#23545;ICCTs&#36827;&#34892;&#39564;&#35777;&#65292;&#24182;&#26174;&#31034;ICCTs&#33021;&#22815;&#23398;&#20064;&#21040;&#27604;&#22522;&#20934;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#26356;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#34920;&#31034;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#26041;&#38754;&#36229;&#36807;&#22522;&#20934;&#27169;&#22411;&#26368;&#39640;&#36798;33%&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;300&#20493;&#33267;600&#20493;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient-based approaches in reinforcement learning (RL) have achieved tremendous success in learning policies for autonomous vehicles. While the performance of these approaches warrants real-world adoption, these policies lack interpretability, limiting deployability in the safety-critical and legally-regulated domain of autonomous driving (AD). AD requires interpretable and verifiable control policies that maintain high performance. We propose Interpretable Continuous Control Trees (ICCTs), a tree-based model that can be optimized via modern, gradient-based, RL approaches to produce high-performing, interpretable policies. The key to our approach is a procedure for allowing direct optimization in a sparse decision-tree-like representation. We validate ICCTs against baselines across six domains, showing that ICCTs are capable of learning interpretable policy representations that parity or outperform baselines by up to 33% in AD scenarios while achieving a 300x-600x reduction in the nu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#33258;&#21160;&#35782;&#21035;Pauli&#33258;&#26059;&#38459;&#30861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#31639;&#27861;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#21644;&#36328;&#35774;&#22791;&#39564;&#35777;&#65292;&#20811;&#26381;&#20102;PSB&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#24182;&#22312;&#30789;&#22330;&#25928;&#24212;&#26230;&#20307;&#31649;&#22120;&#20214;&#19978;&#21462;&#24471;&#20102;96&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#39044;&#35745;&#21487;&#22312;&#25152;&#26377;&#31867;&#22411;&#30340;&#37327;&#23376;&#28857;&#22120;&#20214;&#19978;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2202.00574</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#35782;&#21035;Pauli&#33258;&#26059;&#38459;&#30861;
&lt;/p&gt;
&lt;p&gt;
Identifying Pauli spin blockade using deep learning. (arXiv:2202.00574v3 [cond-mat.mes-hall] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00574
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#33258;&#21160;&#35782;&#21035;Pauli&#33258;&#26059;&#38459;&#30861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#31639;&#27861;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#21644;&#36328;&#35774;&#22791;&#39564;&#35777;&#65292;&#20811;&#26381;&#20102;PSB&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#24182;&#22312;&#30789;&#22330;&#25928;&#24212;&#26230;&#20307;&#31649;&#22120;&#20214;&#19978;&#21462;&#24471;&#20102;96&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#39044;&#35745;&#21487;&#22312;&#25152;&#26377;&#31867;&#22411;&#30340;&#37327;&#23376;&#28857;&#22120;&#20214;&#19978;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Pauli&#33258;&#26059;&#38459;&#30861;&#65288;PSB&#65289;&#21487;&#29992;&#20110;&#33258;&#26059;&#37327;&#23376;&#20301;&#30340;&#21021;&#22987;&#21270;&#21644;&#35835;&#21462;&#65292;&#21363;&#20351;&#22312;&#39640;&#28201;&#19979;&#20063;&#33021;&#21457;&#25381;&#20316;&#29992;&#65292;&#20294;&#24456;&#38590;&#35782;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30005;&#33655;&#20256;&#36755;&#27979;&#37327;&#33258;&#21160;&#35782;&#21035;PSB&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#35757;&#32451;&#31639;&#27861;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#21644;&#36328;&#35774;&#22791;&#39564;&#35777;&#65292;&#20811;&#26381;&#20102;PSB&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#12290;&#25105;&#20204;&#22312;&#30789;&#22330;&#25928;&#24212;&#26230;&#20307;&#31649;&#22120;&#20214;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#27979;&#35797;&#22120;&#20214;&#19978;&#25253;&#21578;&#20102;96&#65285;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#34920;&#26126;&#35813;&#26041;&#27861;&#23545;&#22120;&#20214;&#21464;&#24322;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#39044;&#35745;&#35813;&#26041;&#27861;&#21487;&#22312;&#25152;&#26377;&#31867;&#22411;&#30340;&#37327;&#23376;&#28857;&#22120;&#20214;&#19978;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pauli spin blockade (PSB) can be employed as a great resource for spin qubit initialisation and readout even at elevated temperatures but it can be difficult to identify. We present a machine learning algorithm capable of automatically identifying PSB using charge transport measurements. The scarcity of PSB data is circumvented by training the algorithm with simulated data and by using cross-device validation. We demonstrate our approach on a silicon field-effect transistor device and report an accuracy of 96% on different test devices, giving evidence that the approach is robust to device variability. The approach is expected to be employable across all types of quantum dot devices.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DoCoM&#30340;&#20998;&#25955;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#21387;&#32553;&#35875;&#35328;&#20256;&#25773;&#19968;&#33268;&#24615;&#36861;&#36394;&#21516;&#26102;&#36319;&#36394;&#24179;&#22343;&#36845;&#20195;&#21644;&#38543;&#26426;&#26799;&#24230;&#65292;&#24182;&#24341;&#20837;&#21160;&#37327;&#27493;&#39588;&#21644;&#23616;&#37096;&#26799;&#24230;&#20272;&#35745;&#65292;&#23454;&#29616;&#36890;&#20449;&#39640;&#25928;&#21644;&#36817;&#20284;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2202.00255</link><description>&lt;p&gt;
DoCoM&#65306;&#20855;&#26377;&#36817;&#20284;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#21387;&#32553;&#20998;&#25955;&#24335;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
DoCoM: Compressed Decentralized Optimization with Near-Optimal Sample Complexity. (arXiv:2202.00255v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00255
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DoCoM&#30340;&#20998;&#25955;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#21387;&#32553;&#35875;&#35328;&#20256;&#25773;&#19968;&#33268;&#24615;&#36861;&#36394;&#21516;&#26102;&#36319;&#36394;&#24179;&#22343;&#36845;&#20195;&#21644;&#38543;&#26426;&#26799;&#24230;&#65292;&#24182;&#24341;&#20837;&#21160;&#37327;&#27493;&#39588;&#21644;&#23616;&#37096;&#26799;&#24230;&#20272;&#35745;&#65292;&#23454;&#29616;&#36890;&#20449;&#39640;&#25928;&#21644;&#36817;&#20284;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DoCoM&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#25955;&#24335;&#20248;&#21270;&#12290;&#31639;&#27861;&#36890;&#36807;&#21387;&#32553;&#24335;&#35875;&#35328;&#20256;&#25773;&#19968;&#33268;&#24615;&#36861;&#36394;&#26469;&#21516;&#26102;&#36319;&#36394;&#24179;&#22343;&#36845;&#20195;&#21644;&#38543;&#26426;&#26799;&#24230;&#65292;&#20197;&#23454;&#29616;&#36817;&#20284;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#24341;&#20837;&#21160;&#37327;&#27493;&#39588;&#21644;&#23616;&#37096;&#26799;&#24230;&#20272;&#35745;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#26041;&#24046;&#24402;&#32422;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;T&#27425;&#36845;&#20195;&#20013;&#65292;DoCoM&#21487;&#20197;&#22312;&#25152;&#26377;&#21442;&#19982;&#20195;&#29702;&#20013;&#25214;&#21040;&#19968;&#20010;&#36817;&#20284;&#31283;&#23450;&#30340;&#35299;&#65292;&#28385;&#36275;$\mathbb{E}[ \| \nabla f( \theta ) \|^2 ] = \mathcal{O}( 1 / T^{2/3} )$&#65292;&#20854;&#20013;$f(\theta)$&#26159;&#19968;&#20010;&#24179;&#28369;&#30340;&#65288;&#21487;&#33021;&#38750;&#20984;&#65289;&#30446;&#26631;&#20989;&#25968;&#12290;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#28508;&#21147;&#20989;&#25968;&#65292;&#32039;&#23494;&#36319;&#36394;&#20102;DoCoM&#30340;&#19968;&#27425;&#36845;&#20195;&#36827;&#23637;&#26469;&#35777;&#26126;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes the Doubly Compressed Momentum-assisted stochastic gradient tracking algorithm $\texttt{DoCoM}$ for communication-efficient decentralized optimization. The algorithm features two main ingredients to achieve a near-optimal sample complexity while allowing for communication compression. First, the algorithm tracks both the averaged iterate and stochastic gradient using compressed gossiping consensus. Second, a momentum step is incorporated for adaptive variance reduction with the local gradient estimates. We show that $\texttt{DoCoM}$ finds a near-stationary solution at all participating agents satisfying $\mathbb{E}[ \| \nabla f( \theta ) \|^2 ] = \mathcal{O}( 1 / T^{2/3} )$ in $T$ iterations, where $f(\theta)$ is a smooth (possibly non-convex) objective function. Notice that the proof is achieved via analytically designing a new potential function that tightly tracks the one-iteration progress of $\texttt{DoCoM}$. As a corollary, our analysis also established the li
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;kNN-KGE&#65292;&#23427;&#36890;&#36807;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#26368;&#36817;&#37051;&#30340;&#32447;&#24615;&#25554;&#20540;&#65292;&#20801;&#35768;&#32597;&#35265;&#25110;&#26032;&#20986;&#29616;&#30340;&#23454;&#20307;&#34987;&#26126;&#30830;&#22320;&#35760;&#24518;&#65292;&#32780;&#19981;&#26159;&#38544;&#34255;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#38142;&#25509;&#39044;&#27979;&#32467;&#26524;&#24182;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2201.05575</link><description>&lt;p&gt;
&#36890;&#36807;&#35760;&#24518;&#25512;&#29702;&#65306;&#26368;&#36817;&#37051;&#30693;&#35782;&#22270;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Reasoning Through Memorization: Nearest Neighbor Knowledge Graph Embeddings. (arXiv:2201.05575v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.05575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;kNN-KGE&#65292;&#23427;&#36890;&#36807;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#26368;&#36817;&#37051;&#30340;&#32447;&#24615;&#25554;&#20540;&#65292;&#20801;&#35768;&#32597;&#35265;&#25110;&#26032;&#20986;&#29616;&#30340;&#23454;&#20307;&#34987;&#26126;&#30830;&#22320;&#35760;&#24518;&#65292;&#32780;&#19981;&#26159;&#38544;&#34255;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#38142;&#25509;&#39044;&#27979;&#32467;&#26524;&#24182;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#24448;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#36890;&#24120;&#23558;&#23454;&#20307;&#26144;&#23556;&#21040;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#35780;&#20998;&#20989;&#25968;&#39044;&#27979;&#30446;&#26631;&#23454;&#20307;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38590;&#20197;&#25512;&#29702;&#20986;&#32597;&#35265;&#25110;&#26032;&#20986;&#29616;&#30340;&#26410;&#30693;&#23454;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;kNN-KGE&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#32447;&#24615;&#25554;&#20540;&#23558;&#20854;&#23454;&#20307;&#20998;&#24067;&#19982;k&#20010;&#26368;&#36817;&#37051;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#26681;&#25454;&#30693;&#35782;&#23384;&#20648;&#20013;&#23454;&#20307;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#36317;&#31163;&#35745;&#31639;&#26368;&#36817;&#37051;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26126;&#30830;&#22320;&#35760;&#24518;&#32597;&#35265;&#25110;&#26032;&#20986;&#29616;&#30340;&#23454;&#20307;&#65292;&#32780;&#19981;&#26159;&#38544;&#34255;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;&#24402;&#32435;&#21644;&#20256;&#36882;&#24335;&#38142;&#25509;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#22312;&#21482;&#26377;&#23569;&#37327;&#19977;&#20803;&#32452;&#30340;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#36825;&#21487;&#33021;&#26356;&#23481;&#26131;&#36890;&#36807;&#26126;&#30830;&#30340;&#35760;&#24518;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous knowledge graph embedding approaches usually map entities to representations and utilize score functions to predict the target entities, yet they typically struggle to reason rare or emerging unseen entities. In this paper, we propose kNN-KGE, a new knowledge graph embedding approach with pre-trained language models, by linearly interpolating its entity distribution with k-nearest neighbors. We compute the nearest neighbors based on the distance in the entity embedding space from the knowledge store. Our approach can allow rare or emerging entities to be memorized explicitly rather than implicitly in model parameters. Experimental results demonstrate that our approach can improve inductive and transductive link prediction results and yield better performance for low-resource settings with only a few triples, which might be easier to reason via explicit memory. Code is available at https://github.com/zjunlp/KNN-KG.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#38745;&#24687;&#29366;&#24577;&#19979;&#20154;&#31867;&#33041;&#32593;&#32476;&#30340;&#29366;&#24577;&#31354;&#38388;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24809;&#32602;&#32593;&#32476;&#20043;&#38388;&#30340;&#25299;&#25169;&#36317;&#31163;&#23558;&#21160;&#24577;&#21464;&#21270;&#30340;&#33041;&#32593;&#32476;&#32858;&#31867;&#20026;&#19981;&#21516;&#30340;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#26102;&#38388;&#32500;&#24230;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#33041;&#32593;&#32476;&#30340;&#25972;&#20307;&#25299;&#25169;&#20855;&#26377;&#36951;&#20256;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2201.00087</link><description>&lt;p&gt;
&#38745;&#24687;&#26102;&#20154;&#31867;&#21151;&#33021;&#24615;&#33041;&#32593;&#32476;&#30340;&#25345;&#32493;&#21516;&#35843;&#29366;&#24577;&#31354;&#38388;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Persistent Homological State-Space Estimation of Functional Human Brain Networks at Rest. (arXiv:2201.00087v3 [math.AT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.00087
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#38745;&#24687;&#29366;&#24577;&#19979;&#20154;&#31867;&#33041;&#32593;&#32476;&#30340;&#29366;&#24577;&#31354;&#38388;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24809;&#32602;&#32593;&#32476;&#20043;&#38388;&#30340;&#25299;&#25169;&#36317;&#31163;&#23558;&#21160;&#24577;&#21464;&#21270;&#30340;&#33041;&#32593;&#32476;&#32858;&#31867;&#20026;&#19981;&#21516;&#30340;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#26102;&#38388;&#32500;&#24230;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#33041;&#32593;&#32476;&#30340;&#25972;&#20307;&#25299;&#25169;&#20855;&#26377;&#36951;&#20256;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65288;TDA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#21160;&#24577;&#21464;&#21270;&#30340;&#20154;&#31867;&#21151;&#33021;&#24615;&#33041;&#32593;&#32476;&#30340;&#29366;&#24577;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23545;&#32593;&#32476;&#20043;&#38388;&#30340;&#25299;&#25169;&#36317;&#31163;&#36827;&#34892;&#24809;&#32602;&#65292;&#23558;&#21160;&#24577;&#21464;&#21270;&#30340;&#33041;&#32593;&#32476;&#32858;&#31867;&#20026;&#25299;&#25169;&#19978;&#19981;&#21516;&#30340;&#29366;&#24577;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#32593;&#32476;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#32771;&#34385;&#20102;&#25968;&#25454;&#30340;&#26102;&#38388;&#32500;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20272;&#35745;&#33041;&#32593;&#32476;&#30340;&#29366;&#24577;&#31354;&#38388;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;k-means&#32858;&#31867;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#26356;&#20934;&#30830;&#22320;&#30830;&#23450;&#21160;&#24577;&#21464;&#21270;&#30340;&#21151;&#33021;&#24615;&#33041;&#32593;&#32476;&#30340;&#29366;&#24577;&#31354;&#38388;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#21452;&#29983;&#23376;&#30740;&#31350;&#35774;&#35745;&#25506;&#35752;&#20102;&#33041;&#32593;&#32476;&#30340;&#25972;&#20307;&#25299;&#25169;&#26159;&#21542;&#20855;&#26377;&#36951;&#20256;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new data driven topological data analysis (TDA) approach for estimating state spaces in dynamically changing human functional brain networks of human. Our approach penalizes the topological distance between networks and clusters dynamically changing brain networks into topologically distinct states. Our method takes into account the temporal dimension of the data through the Wasserstein distance between networks. Our method is shown to outperform the widely used k-means clustering often used in estimating the state space in brain networks. The method is applied to more accurately determine the state spaces of dynamically changing functional brain networks. Subsequently, we address the question of whether the overall topology of brain networks is a heritable feature using the twin study design.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#24494;&#20998;&#26041;&#31243;&#22312;&#38797;&#28857;&#20248;&#21270;&#20013;&#35774;&#35745;&#20102;&#19981;&#21516;&#30340;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#21452;&#32447;&#24615;&#21338;&#24328;&#20013;&#30340;&#25910;&#25947;&#24615;&#36136;&#19982;&#31163;&#25955;&#26041;&#27861;&#30456;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2112.13826</link><description>&lt;p&gt;
&#39640;&#20998;&#36776;&#29575;&#24494;&#20998;&#26041;&#31243;&#22312;&#38797;&#28857;&#20248;&#21270;&#22120;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Last-Iterate Convergence of Saddle-Point Optimizers via High-Resolution Differential Equations. (arXiv:2112.13826v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.13826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#24494;&#20998;&#26041;&#31243;&#22312;&#38797;&#28857;&#20248;&#21270;&#20013;&#35774;&#35745;&#20102;&#19981;&#21516;&#30340;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#21452;&#32447;&#24615;&#21338;&#24328;&#20013;&#30340;&#25910;&#25947;&#24615;&#36136;&#19982;&#31163;&#25955;&#26041;&#27861;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#19968;&#38454;&#38797;&#28857;&#20248;&#21270;&#26041;&#27861;&#22312;&#34893;&#29983;&#26102;&#21487;&#20197;&#24471;&#21040;&#19982;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319; (GDA) &#26041;&#27861;&#30456;&#21516;&#30340;&#36830;&#32493;&#26102;&#38388;&#24120;&#24494;&#20998;&#26041;&#31243; (ODE)&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#22312;&#31616;&#21333;&#30340;&#21452;&#32447;&#24615;&#21338;&#24328;&#20013;&#26159;&#26377;&#24046;&#24322;&#30340;&#12290;&#22240;&#27492;&#65292;ODE &#35270;&#35282;&#22312;&#20998;&#26512;&#21333;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#26041;&#38754;&#24050;&#32463;&#21457;&#25381;&#20102;&#24378;&#22823;&#30340;&#20316;&#29992;&#65292;&#20294;&#22312;&#38797;&#28857;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;&#23578;&#19981;&#26126;&#26224;&#12290;&#25105;&#20204;&#37319;&#29992;&#22312;&#27969;&#20307;&#21160;&#21147;&#23398;&#20013;&#30740;&#31350;&#30340;&#39640;&#20998;&#36776;&#29575;&#24494;&#20998;&#26041;&#31243; (HRDEs) &#26694;&#26550;&#26469;&#35774;&#35745;&#20960;&#31181;&#38797;&#28857;&#20248;&#21270;&#26041;&#27861;&#30340;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#12290;&#23588;&#20854;&#38656;&#35201;&#25351;&#20986;&#30340;&#26159;&#65292;&#36825;&#20123; HRDEs &#23545;&#24212;&#20110;&#19981;&#21516;&#30340;&#38797;&#28857;&#20248;&#21270;&#26041;&#27861;&#26159;&#19981;&#21516;&#30340;&#12290;&#27492;&#22806;&#65292;&#22312;&#21452;&#32447;&#24615;&#21338;&#24328;&#20013;&#65292;HRDEs &#30340;&#25910;&#25947;&#24615;&#36136;&#19982;&#30456;&#24212;&#30340;&#31163;&#25955;&#26041;&#27861;&#30340;&#24615;&#36136;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several widely-used first-order saddle-point optimization methods yield an identical continuous-time ordinary differential equation (ODE) that is identical to that of the Gradient Descent Ascent (GDA) method when derived naively. However, the convergence properties of these methods are qualitatively different, even on simple bilinear games. Thus the ODE perspective, which has proved powerful in analyzing single-objective optimization methods, has not played a similar role in saddle-point optimization.  We adopt a framework studied in fluid dynamics -- known as High-Resolution Differential Equations (HRDEs) -- to design differential equation models for several saddle-point optimization methods. Critically, these HRDEs are distinct for various saddle-point optimization methods. Moreover, in bilinear games, the convergence properties of the HRDEs match the qualitative features of the corresponding discrete methods. Additionally, we show that the HRDE of Optimistic Gradient Descent Ascent 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#22810;&#20803;&#26497;&#20540;&#30340;&#35889;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#21644;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#20854;&#22312;&#23398;&#20064;&#35282;&#24230;&#27979;&#24230;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2111.07799</link><description>&lt;p&gt;
&#22810;&#20803;&#26497;&#20540;&#30340;&#35889;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spectral learning of multivariate extremes. (arXiv:2111.07799v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.07799
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#22810;&#20803;&#26497;&#20540;&#30340;&#35889;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#21644;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#20854;&#22312;&#23398;&#20064;&#35282;&#24230;&#27979;&#24230;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#22810;&#20803;&#26497;&#20540;&#30340;&#35889;&#32858;&#31867;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#26497;&#20540;&#29702;&#35770;&#20013;&#30001;&#35282;&#24230;&#25110;&#35889;&#27979;&#24230;&#34920;&#24449;&#30340;&#22810;&#20803;&#26497;&#20540;&#30340;&#28176;&#36817;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#35889;&#32858;&#31867;&#30340;&#29702;&#35770;&#24615;&#33021;&#65292;&#35813;&#32858;&#31867;&#22522;&#20110;&#20174;&#26497;&#20540;&#26679;&#26412;&#20013;&#26500;&#24314;&#30340;&#38543;&#26426;k&#26368;&#36817;&#37051;&#22270;&#65292;&#21363;&#23545;&#20110;&#21322;&#24452;&#36229;&#36807;&#19968;&#20010;&#36739;&#22823;&#38408;&#20540;&#30340;&#38543;&#26426;&#21521;&#37327;&#30340;&#35282;&#24230;&#37096;&#20998;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#32447;&#24615;&#22240;&#23376;&#27169;&#22411;&#20135;&#29983;&#30340;&#26497;&#20540;&#30340;&#28176;&#36817;&#20998;&#24067;&#65292;&#24182;&#35777;&#26126;&#65292;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#35889;&#32858;&#31867;&#21487;&#20197;&#19968;&#33268;&#22320;&#35782;&#21035;&#20986;&#22312;&#35813;&#27169;&#22411;&#20013;&#20135;&#29983;&#30340;&#26497;&#20540;&#30340;&#32858;&#31867;&#12290;&#22522;&#20110;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#19968;&#33268;&#24615;&#20272;&#35745;&#31574;&#30053;&#26469;&#23398;&#20064;&#35282;&#24230;&#27979;&#24230;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#19982;&#25968;&#20540;&#23454;&#39564;&#30456;&#32467;&#21512;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#26377;&#38480;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a spectral clustering algorithm for analyzing the dependence structure of multivariate extremes. More specifically, we focus on the asymptotic dependence of multivariate extremes characterized by the angular or spectral measure in extreme value theory. Our work studies the theoretical performance of spectral clustering based on a random $k$-nearest neighbor graph constructed from an extremal sample, i.e., the angular part of random vectors for which the radius exceeds a large threshold. In particular, we derive the asymptotic distribution of extremes arising from a linear factor model and prove that, under certain conditions, spectral clustering can consistently identify the clusters of extremes arising in this model. Leveraging this result we propose a simple consistent estimation strategy for learning the angular measure. Our theoretical findings are complemented with numerical experiments illustrating the finite sample performance of our methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PINO&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#21516;&#26102;&#21033;&#29992;&#25968;&#25454;&#21644;&#29289;&#29702;&#32422;&#26463;&#26465;&#20214;&#26469;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#31639;&#23376;&#65292;&#20811;&#26381;&#20102;&#32431;&#25968;&#25454;&#39537;&#21160;&#21644;&#22522;&#20110;&#29289;&#29702;&#30340;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#19981;&#21516;&#20998;&#36776;&#29575;&#19978;&#21512;&#24182;&#25968;&#25454;&#21644;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2111.03794</link><description>&lt;p&gt;
&#29289;&#29702;&#21551;&#21457;&#24335;&#31070;&#32463;&#31639;&#23376;&#29992;&#20110;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Operator for Learning Partial Differential Equations. (arXiv:2111.03794v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.03794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PINO&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#21516;&#26102;&#21033;&#29992;&#25968;&#25454;&#21644;&#29289;&#29702;&#32422;&#26463;&#26465;&#20214;&#26469;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#31639;&#23376;&#65292;&#20811;&#26381;&#20102;&#32431;&#25968;&#25454;&#39537;&#21160;&#21644;&#22522;&#20110;&#29289;&#29702;&#30340;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#19981;&#21516;&#20998;&#36776;&#29575;&#19978;&#21512;&#24182;&#25968;&#25454;&#21644;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#29289;&#29702;&#21551;&#21457;&#24335;&#31070;&#32463;&#31639;&#23376;&#8221;&#65288;PINO&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#29616;&#26377;&#25968;&#25454;&#21644;/&#25110;&#29289;&#29702;&#32422;&#26463;&#26465;&#20214;&#26469;&#23398;&#20064;&#19968;&#31867;&#21442;&#25968;&#21270;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#31639;&#23376;&#12290;&#35813;&#28151;&#21512;&#26041;&#27861;&#20801;&#35768;PINO&#20811;&#26381;&#32431;&#25968;&#25454;&#39537;&#21160;&#21644;&#22522;&#20110;&#29289;&#29702;&#30340;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#23616;&#38480;&#24615;&#12290;&#23558;&#25968;&#25454;&#21644;PDE&#32422;&#26463;&#30456;&#32467;&#21512;&#65292;PINO&#33021;&#22815;&#21516;&#26102;&#36866;&#24212;&#25968;&#25454;&#37327;&#26377;&#38480;&#21644;/&#25110;&#36136;&#37327;&#36739;&#24046;&#30340;&#24773;&#20917;&#20197;&#21450;&#22256;&#38590;PDE&#32422;&#26463;&#30340;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#20182;&#28151;&#21512;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;PINO&#20855;&#26377;&#19968;&#20010;&#29420;&#29305;&#30340;&#29305;&#24615;&#65292;&#21363;&#33021;&#22815;&#22312;&#19981;&#21516;&#20998;&#36776;&#29575;&#19978;&#21512;&#24182;&#25968;&#25454;&#21644;PDE&#32422;&#26463;&#12290;&#36825;&#20801;&#35768;&#25105;&#20204;&#23558;&#26469;&#33258;&#25968;&#20540;&#27714;&#35299;&#22120;&#30340;&#20302;&#20998;&#36776;&#29575;&#25968;&#25454;&#19982;&#39640;&#20998;&#36776;&#29575;PDE&#32422;&#26463;&#30456;&#32467;&#21512;&#65292;&#32780;&#25152;&#24471;&#21040;&#30340;PINO&#21363;&#20351;&#22312;&#39640;&#20998;&#36776;&#29575;&#30340;&#27979;&#35797;&#23454;&#20363;&#19978;&#20063;&#27809;&#26377;&#31934;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose physics-informed neural operators (PINO) that uses available data and/or physics constraints to learn the solution operator of a family of parametric Partial Differential Equation (PDE). This hybrid approach allows PINO to overcome the limitations of purely data-driven and physics-based methods. For instance, data-driven methods fail to learn when data is of limited quantity and/or quality, and physics-based approaches fail to optimize on challenging PDE constraints. By combining both data and PDE constraints, PINO overcomes all these challenges. Additionally, a unique property that PINO enjoys over other hybrid learning methods is its ability to incorporate data and PDE constraints at different resolutions. This allows us to combine coarse-resolution data, which is inexpensive to obtain from numerical solvers, with higher resolution PDE constraints, and the resulting PINO has no degradation in accuracy even on high-resolution test instances. This discretizati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#25506;&#32034;&#26041;&#27861;&#20197;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#22870;&#21169;&#31232;&#23569;&#26102;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#39640;&#20445;&#30495;&#24230;&#30340;&#24037;&#19994;&#32423;&#27169;&#25311;&#22120;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30456;&#27604;&#29616;&#26377;&#31639;&#27861;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2109.12509</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#28145;&#24230;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Deep Exploration for Recommendation Systems. (arXiv:2109.12509v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.12509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#25506;&#32034;&#26041;&#27861;&#20197;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#22870;&#21169;&#31232;&#23569;&#26102;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#39640;&#20445;&#30495;&#24230;&#30340;&#24037;&#19994;&#32423;&#27169;&#25311;&#22120;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30456;&#27604;&#29616;&#26377;&#31639;&#27861;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#24212;&#20174;&#24310;&#36831;&#21453;&#39304;&#20013;&#25506;&#32034;&#21644;&#23398;&#20064;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#24448;&#24448;&#20391;&#37325;&#20110;&#20174;&#29992;&#25143;&#23545;&#21333;&#20010;&#25512;&#33616;&#30340;&#21709;&#24212;&#20013;&#23398;&#20064;&#12290;&#36825;&#20123;&#24037;&#20316;&#21033;&#29992;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20294;&#25918;&#24323;&#20102;&#23398;&#20064;&#29992;&#25143;&#20043;&#21518;&#30340;&#34892;&#20026;&#12290;&#22312;&#36807;&#21435;&#30340;&#24037;&#20316;&#20013;&#65292;&#34429;&#28982;&#33268;&#21147;&#20110;&#20174;&#38543;&#21518;&#30340;&#34892;&#20026;&#20013;&#23398;&#20064;&#65292;&#20294;&#32570;&#20047;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#24341;&#23548;&#24182;&#33719;&#21462;&#26377;&#24847;&#20041;&#30340;&#24310;&#36831;&#21453;&#39304;&#12290;&#24403;&#22870;&#21169;&#36739;&#23569;&#26102;&#65292;&#36890;&#36807;&#24341;&#23548;&#25506;&#32034;&#26377;&#24847;&#20041;&#30340;&#24310;&#36831;&#21453;&#39304;&#21464;&#24471;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;&#25512;&#33616;&#31995;&#32479;&#24320;&#21457;&#20102;&#28145;&#24230;&#25506;&#32034;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25512;&#33616;&#31995;&#32479;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#28145;&#24230;&#25506;&#32034;&#26041;&#27861;&#22312;&#21333;&#27493;&#25506;&#32034;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26159;&#22312;&#39640;&#20445;&#30495;&#24230;&#30340;&#24037;&#19994;&#32423;&#27169;&#25311;&#22120;&#19979;&#36827;&#34892;&#30340;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30456;&#27604;&#29616;&#26377;&#31639;&#27861;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern recommendation systems ought to benefit by probing for and learning from delayed feedback. Research has tended to focus on learning from a user's response to a single recommendation. Such work, which leverages methods of supervised and bandit learning, forgoes learning from the user's subsequent behavior. Where past work has aimed to learn from subsequent behavior, there has been a lack of effective methods for probing to elicit informative delayed feedback. Effective exploration through probing for delayed feedback becomes particularly challenging when rewards are sparse. To address this, we develop deep exploration methods for recommendation systems. In particular, we formulate recommendation as a sequential decision problem and demonstrate benefits of deep exploration over single-step exploration. Our experiments are carried out with high-fidelity industrial-grade simulators and establish large improvements over existing algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20215;&#20540;&#20559;&#24046;&#21644;&#36716;&#31227;&#35268;&#33539;&#21270;&#26469;&#20462;&#25913;&#36716;&#31227;&#27010;&#29575;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#23398;&#20064;&#21040;&#39640;&#24615;&#33021;&#21644;&#21327;&#35843;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2108.01832</link><description>&lt;p&gt;
&#31163;&#32447;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Offline Decentralized Multi-Agent Reinforcement Learning. (arXiv:2108.01832v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.01832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20215;&#20540;&#20559;&#24046;&#21644;&#36716;&#31227;&#35268;&#33539;&#21270;&#26469;&#20462;&#25913;&#36716;&#31227;&#27010;&#29575;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#23398;&#20064;&#21040;&#39640;&#24615;&#33021;&#21644;&#21327;&#35843;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#20219;&#21153;&#20013;&#65292;&#30001;&#20110;&#25104;&#26412;&#39640;&#26114;&#21644;&#39118;&#38505;&#65292;&#26234;&#33021;&#20307;&#26080;&#27861;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#25345;&#32493;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#24182;&#25910;&#38598;&#32463;&#39564;&#65292;&#32780;&#24517;&#39035;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#25968;&#25454;&#38598;&#20013;&#30340;&#29366;&#24577;&#36716;&#31227;&#21160;&#24577;&#21487;&#33021;&#19982;&#20854;&#20182;&#26234;&#33021;&#20307;&#22312;&#25191;&#34892;&#20013;&#23398;&#20064;&#31574;&#30053;&#24341;&#21457;&#30340;&#36716;&#31227;&#21160;&#24577;&#30456;&#24046;&#24456;&#22823;&#65292;&#20174;&#32780;&#23548;&#33268;&#20215;&#20540;&#20272;&#35745;&#20986;&#29616;&#36739;&#22823;&#30340;&#35823;&#24046;&#12290;&#22240;&#27492;&#65292;&#26234;&#33021;&#20307;&#23398;&#20064;&#21040;&#30340;&#26159;&#19981;&#21327;&#35843;&#19988;&#24615;&#33021;&#36739;&#20302;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#20215;&#20540;&#20559;&#24046;&#21644;&#36716;&#31227;&#35268;&#33539;&#21270;&#26469;&#26377;&#24847;&#22320;&#20462;&#25913;&#36716;&#31227;&#27010;&#29575;&#12290;&#20215;&#20540;&#20559;&#24046;&#20048;&#35266;&#22320;&#22686;&#21152;&#20102;&#39640;&#20215;&#20540;&#19979;&#19968;&#29366;&#24577;&#30340;&#36716;&#31227;&#27010;&#29575;&#65292;&#32780;&#36716;&#31227;&#35268;&#33539;&#21270;&#23545;&#19979;&#19968;&#29366;&#24577;&#30340;&#36716;&#31227;&#27010;&#29575;&#36827;&#34892;&#20102;&#35268;&#33539;&#21270;&#12290;&#23427;&#20204;&#20849;&#21516;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#23398;&#20064;&#21040;&#39640;&#24615;&#33021;&#21644;&#21327;&#35843;&#30340;&#31574;&#30053;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world multi-agent cooperative tasks, due to high cost and risk, agents cannot continuously interact with the environment and collect experiences during learning, but have to learn from offline datasets. However, the transition dynamics in the dataset of each agent can be much different from the ones induced by the learned policies of other agents in execution, creating large errors in value estimates. Consequently, agents learn uncoordinated low-performing policies. In this paper, we propose a framework for offline decentralized multi-agent reinforcement learning, which exploits value deviation and transition normalization to deliberately modify the transition probabilities. Value deviation optimistically increases the transition probabilities of high-value next states, and transition normalization normalizes the transition probabilities of next states. They together enable agents to learn high-performing and coordinated policies. Theoretically, we prove the convergence of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#21521;&#24037;&#31243;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#29983;&#25104;&#30340;&#22270;&#20687;&#26469;&#25512;&#26029;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#65292;&#20197;&#35782;&#21035;&#21644;&#29702;&#35299;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#28389;&#29992;&#12290;&#36890;&#36807;&#25351;&#32441;&#20272;&#35745;&#32593;&#32476;&#21644;&#35299;&#26512;&#32593;&#32476;&#65292;&#25105;&#20204;&#33021;&#22815;&#20174;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#39044;&#27979;&#29983;&#25104;&#27169;&#22411;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2106.07873</link><description>&lt;p&gt;
&#20174;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#25512;&#26029;&#29983;&#25104;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#65306;&#29983;&#25104;&#27169;&#22411;&#30340;&#36870;&#21521;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Reverse Engineering of Generative Models: Inferring Model Hyperparameters from Generated Images. (arXiv:2106.07873v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.07873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#21521;&#24037;&#31243;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#29983;&#25104;&#30340;&#22270;&#20687;&#26469;&#25512;&#26029;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#65292;&#20197;&#35782;&#21035;&#21644;&#29702;&#35299;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#28389;&#29992;&#12290;&#36890;&#36807;&#25351;&#32441;&#20272;&#35745;&#32593;&#32476;&#21644;&#35299;&#26512;&#32593;&#32476;&#65292;&#25105;&#20204;&#33021;&#22815;&#20174;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#39044;&#27979;&#29983;&#25104;&#27169;&#22411;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#21512;&#25104;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#20196;&#20154;&#38590;&#20197;&#21306;&#20998;&#26159;&#30495;&#23454;&#29031;&#29255;&#36824;&#26159;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#35782;&#21035;&#21644;&#29702;&#35299;&#31713;&#25913;&#23186;&#20307;&#23545;&#20110;&#32531;&#35299;&#29983;&#25104;&#27169;&#22411;&#28508;&#22312;&#28389;&#29992;&#30340;&#31038;&#20250;&#20851;&#20999;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#36870;&#21521;&#24037;&#31243;&#30340;&#26041;&#27861;&#65292;&#20174;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#25512;&#26029;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38382;&#39064;&#65292;&#21363;&#8220;&#27169;&#22411;&#35299;&#26512;&#8221;&#65292;&#36890;&#36807;&#20998;&#26512;&#29983;&#25104;&#30340;&#22270;&#20687;&#26469;&#20272;&#35745;&#29983;&#25104;&#27169;&#22411;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#65292;&#36825;&#23545;&#20154;&#31867;&#32780;&#35328;&#20284;&#20046;&#26159;&#19981;&#21487;&#33021;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#25351;&#32441;&#20272;&#35745;&#32593;&#32476;&#65288;FEN&#65289;&#65292;&#36890;&#36807;&#35757;&#32451;&#24102;&#26377;&#22235;&#20010;&#32422;&#26463;&#30340;&#25351;&#32441;&#20272;&#35745;&#32593;&#32476;&#26469;&#20272;&#35745;&#29983;&#25104;&#27169;&#22411;&#30340;&#25351;&#32441;&#65292;&#20197;&#40723;&#21169;&#25351;&#32441;&#20855;&#26377;&#26399;&#26395;&#30340;&#29305;&#24615;&#65307;&#35299;&#26512;&#32593;&#32476;&#65288;PN&#65289;&#65292;&#20174;&#20272;&#35745;&#30340;&#25351;&#32441;&#20013;&#39044;&#27979;&#32593;&#32476;&#26550;&#26500;&#21644;&#25439;&#22833;&#20989;&#25968;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20123;&#29983;&#25104;&#27169;&#22411;&#30340;&#22270;&#20687;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art (SOTA) Generative Models (GMs) can synthesize photo-realistic images that are hard for humans to distinguish from genuine photos. Identifying and understanding manipulated media are crucial to mitigate the social concerns on the potential misuse of GMs. We propose to perform reverse engineering of GMs to infer model hyperparameters from the images generated by these models. We define a novel problem, ``model parsing", as estimating GM network architectures and training loss functions by examining their generated images -- a task seemingly impossible for human beings. To tackle this problem, we propose a framework with two components: a Fingerprint Estimation Network (FEN), which estimates a GM fingerprint from a generated image by training with four constraints to encourage the fingerprint to have desired properties, and a Parsing Network (PN), which predicts network architecture and loss functions from the estimated fingerprints. To evaluate our approach, we collect a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#28176;&#36817;&#27867;&#21270;&#29702;&#35770;&#65292;&#36890;&#36807;&#24341;&#20837;&#32553;&#25918;&#21464;&#20998;&#27491;&#21017;&#21270;&#65292;&#24182;&#21033;&#29992;"&#23725;-&#22871;&#32034;&#23545;&#20598;&#24615;"&#33719;&#24471;&#20102;&#26032;&#30340;&#39044;&#27979;&#30028;&#38480;&#65292;&#35299;&#37322;&#20102;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#22312;&#36229;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#20197;&#21450;&#21452;&#35895;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2106.04795</link><description>&lt;p&gt;
&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#28176;&#36817;&#29702;&#35770;&#65306;&#36229;&#36234;&#20559;&#24046;-&#26041;&#24046;&#25240;&#34935;
&lt;/p&gt;
&lt;p&gt;
Nonasymptotic theory for two-layer neural networks: Beyond the bias-variance trade-off. (arXiv:2106.04795v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.04795
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#28176;&#36817;&#27867;&#21270;&#29702;&#35770;&#65292;&#36890;&#36807;&#24341;&#20837;&#32553;&#25918;&#21464;&#20998;&#27491;&#21017;&#21270;&#65292;&#24182;&#21033;&#29992;"&#23725;-&#22871;&#32034;&#23545;&#20598;&#24615;"&#33719;&#24471;&#20102;&#26032;&#30340;&#39044;&#27979;&#30028;&#38480;&#65292;&#35299;&#37322;&#20102;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#22312;&#36229;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#20197;&#21450;&#21452;&#35895;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#22312;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#25928;&#26524;&#65292;&#21363;&#20351;&#22312;&#36229;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#27963;&#36291;&#21442;&#25968;&#25968;&#37327;&#30456;&#23545;&#20110;&#26679;&#26412;&#22823;&#23567;&#24456;&#22823;&#12290;&#36825;&#19982;&#20256;&#32479;&#35266;&#28857;&#30456;&#30683;&#30462;&#65292;&#20256;&#32479;&#35266;&#28857;&#35748;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24517;&#39035;&#22312;&#20559;&#24046;&#21644;&#26041;&#24046;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#20197;&#23454;&#29616;&#26368;&#20339;&#27867;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20914;&#31361;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#32553;&#25918;&#21464;&#20998;&#27491;&#21017;&#21270;&#65292;&#32473;&#20986;&#20102;&#38024;&#23545;&#20855;&#26377;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#28176;&#36817;&#27867;&#21270;&#29702;&#35770;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36825;&#20010;&#27491;&#21017;&#21270;&#22120;&#20174;&#26799;&#24230;&#20248;&#21270;&#30340;&#35282;&#24230;&#26469;&#30475;&#31561;&#20215;&#20110;&#23725;&#22238;&#24402;&#65292;&#20294;&#22312;&#25511;&#21046;&#27169;&#22411;&#22797;&#26434;&#24615;&#26041;&#38754;&#36215;&#21040;&#20102;&#31867;&#20284;&#20110;&#20998;&#32452;&#22871;&#32034;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#31181;"&#23725;-&#22871;&#32034;&#23545;&#20598;&#24615;"&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#36866;&#29992;&#20110;&#25152;&#26377;&#32593;&#32476;&#23485;&#24230;&#30340;&#26032;&#30340;&#39044;&#27979;&#30028;&#38480;&#65292;&#20174;&#32780;&#37325;&#29616;&#20102;&#21452;&#35895;&#29616;&#35937;&#12290;&#27492;&#22806;&#65292;&#22312;&#20449;&#21495;&#24378;&#30340;&#24773;&#20917;&#19979;&#65292;&#36229;&#21442;&#25968;&#21270;&#30340;&#26368;&#23567;&#39118;&#38505;&#20302;&#20110;&#20854;&#27424;&#21442;&#25968;&#21270;&#30340;&#23545;&#24212;&#20540;&#65292;&#24182;&#19988;&#20960;&#20046;&#26159;&#26368;&#23567;&#26368;&#22823;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large neural networks have proved remarkably effective in modern deep learning practice, even in the overparametrized regime where the number of active parameters is large relative to the sample size. This contradicts the classical perspective that a machine learning model must trade off bias and variance for optimal generalization. To resolve this conflict, we present a nonasymptotic generalization theory for two-layer neural networks with ReLU activation function by incorporating scaled variation regularization. Interestingly, the regularizer is equivalent to ridge regression from the angle of gradient-based optimization, but plays a similar role to the group lasso in controlling the model complexity. By exploiting this "ridge-lasso duality," we obtain new prediction bounds for all network widths, which reproduce the double descent phenomenon. Moreover, the overparametrized minimum risk is lower than its underparametrized counterpart when the signal is strong, and is nearly minimax o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20840;&#38754;&#35843;&#26597;&#21644;&#25551;&#36848;&#26102;&#38388;&#22788;&#29702;&#20219;&#21153;&#30340;&#23454;&#29616;&#65292;&#29305;&#21035;&#26159;3&#20301;&#38145;&#23384;&#22120;&#23384;&#20648;&#22120;&#65292;&#20026;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#26041;&#27861;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#21644;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2010.07858</link><description>&lt;p&gt;
&#25506;&#32034;&#38145;&#23384;&#22120;&#23384;&#20648;&#22120;&#21450;&#20854;&#20182;&#65306;&#21033;&#29992;&#20851;&#38190;&#35265;&#35299;&#35757;&#32451;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Exploring Flip Flop memories and beyond: training recurrent neural networks with key insights. (arXiv:2010.07858v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.07858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20840;&#38754;&#35843;&#26597;&#21644;&#25551;&#36848;&#26102;&#38388;&#22788;&#29702;&#20219;&#21153;&#30340;&#23454;&#29616;&#65292;&#29305;&#21035;&#26159;3&#20301;&#38145;&#23384;&#22120;&#23384;&#20648;&#22120;&#65292;&#20026;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#26041;&#27861;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#65292;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20197;&#25191;&#34892;&#19981;&#21516;&#20219;&#21153;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNNs)&#22312;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#20013;&#38750;&#24120;&#26377;&#36259;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#24320;&#28304;&#26694;&#26550;&#65292;&#22914;Tensorflow&#21644;Keras&#65292;&#23545;&#25105;&#20204;&#30446;&#21069;&#20351;&#29992;&#30340;&#25216;&#26415;&#30340;&#21457;&#23637;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20840;&#38754;&#35843;&#26597;&#21644;&#25551;&#36848;&#26102;&#38388;&#22788;&#29702;&#20219;&#21153;&#65288;&#29305;&#21035;&#26159;3&#20301;&#38145;&#23384;&#22120;&#23384;&#20648;&#22120;&#65289;&#30340;&#23454;&#29616;&#65292;&#20570;&#20986;&#37325;&#22823;&#36129;&#29486;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#25972;&#20010;&#24314;&#27169;&#36807;&#31243;&#65292;&#21253;&#25324;&#26041;&#31243;&#12289;&#20219;&#21153;&#21442;&#25968;&#21270;&#21644;&#36719;&#20214;&#24320;&#21457;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#21487;&#35270;&#21270;&#21644;&#20998;&#26512;&#24037;&#20855;&#65292;&#31934;&#24515;&#20998;&#26512;&#20102;&#25152;&#33719;&#24471;&#30340;&#32593;&#32476;&#20197;&#38416;&#26126;&#20854;&#21160;&#24577;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20379;&#30340;&#20195;&#30721;&#20855;&#26377;&#36275;&#22815;&#30340;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#20419;&#36827;&#23545;&#19981;&#21516;&#20219;&#21153;&#21644;&#31995;&#32479;&#30340;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#22914;&#20309;&#23558;&#23384;&#20648;&#22120;&#29366;&#24577;&#26377;&#25928;&#22320;&#23384;&#20648;&#22312;&#19968;&#20010;&#31435;&#26041;&#20307;&#30340;&#39030;&#28857;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training neural networks to perform different tasks is relevant across various disciplines. In particular, Recurrent Neural Networks (RNNs) are of great interest in Computational Neuroscience. Open-source frameworks dedicated to Machine Learning, such as Tensorflow and Keras have produced significant changes in the development of technologies that we currently use. This work aims to make a significant contribution by comprehensively investigating and describing the implementation of a temporal processing task, specifically a 3-bit Flip Flop memory. We delve into the entire modelling process, encompassing equations, task parametrization, and software development. The obtained networks are meticulously analyzed to elucidate dynamics, aided by an array of visualization and analysis tools. Moreover, the provided code is versatile enough to facilitate the modelling of diverse tasks and systems. Furthermore, we present how memory states can be efficiently stored in the vertices of a cube in 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;Fr&#233;chet&#22343;&#20540;&#21644;&#24418;&#29366;&#19981;&#21464;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21151;&#33021;&#24615;&#36718;&#24275;&#20013;&#30340;&#24418;&#29366;&#21464;&#21270;&#65292;&#24182;&#26500;&#24314;&#20102;&#21151;&#33021;&#24615;&#25968;&#25454;&#30340;&#25511;&#21046;&#22270;&#65292;&#21487;&#35299;&#37322;&#24615;&#24378;&#19988;&#33021;&#35782;&#21035;&#28508;&#22312;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2010.02968</link><description>&lt;p&gt;
&#21151;&#33021;&#24615;&#36718;&#24275;&#24314;&#27169;&#21644;&#21487;&#35299;&#37322;&#24418;&#29366;&#21464;&#21270;&#26816;&#27979;&#65306;&#32467;&#21512;Fr&#233;chet&#22343;&#20540;&#19982;&#24418;&#29366;&#19981;&#21464;&#27169;&#22411;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Modelling of functional profiles and explainable shape shifts detection: An approach combining the notion of the Fr\'echet mean with the shape invariant model}. (arXiv:2010.02968v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.02968
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;Fr&#233;chet&#22343;&#20540;&#21644;&#24418;&#29366;&#19981;&#21464;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21151;&#33021;&#24615;&#36718;&#24275;&#20013;&#30340;&#24418;&#29366;&#21464;&#21270;&#65292;&#24182;&#26500;&#24314;&#20102;&#21151;&#33021;&#24615;&#25968;&#25454;&#30340;&#25511;&#21046;&#22270;&#65292;&#21487;&#35299;&#37322;&#24615;&#24378;&#19988;&#33021;&#35782;&#21035;&#28508;&#22312;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26816;&#27979;&#21151;&#33021;&#24615;&#36718;&#24275;&#20013;&#24418;&#29366;&#21464;&#21270;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;Fr&#233;chet&#22343;&#20540;&#27010;&#24565;&#21644;&#21464;&#24418;&#27169;&#22411;&#30340;&#27010;&#24565;&#12290;&#21033;&#29992;Fr&#233;chet&#22343;&#20540;&#25552;&#20379;&#30340;&#24191;&#20041;&#22343;&#20540;&#24863;&#30693;&#33021;&#22815;&#25429;&#25417;&#30740;&#31350;&#23545;&#35937;&#36718;&#24275;&#30340;&#20856;&#22411;&#27169;&#24335;&#65292;&#32780;&#21464;&#24418;&#27169;&#22411;&#30340;&#27010;&#24565;&#65292;&#29305;&#21035;&#26159;&#24418;&#29366;&#19981;&#21464;&#27169;&#22411;&#65292;&#20801;&#35768;&#23545;&#36718;&#24275;&#19982;&#20856;&#22411;&#24418;&#29366;&#20043;&#38388;&#30340;&#20559;&#24046;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#21442;&#25968;&#21270;&#12290;&#26500;&#24314;&#21644;&#25552;&#20986;&#20102;&#19982;&#25968;&#25454;&#30340;&#21151;&#33021;&#24615;&#29305;&#24615;&#21644;&#25152;&#37319;&#29992;&#30340;&#21464;&#24418;&#27169;&#22411;&#30456;&#20860;&#23481;&#30340;EWMA&#31867;&#22411;&#25511;&#21046;&#22270;&#65292;&#21033;&#29992;&#30740;&#31350;&#23545;&#35937;&#30340;&#36718;&#24275;&#22312;&#24191;&#20041;&#22343;&#20540;&#24863;&#30693;&#19979;&#30340;&#26576;&#20123;&#24418;&#29366;&#29305;&#24449;&#65292;&#23454;&#29616;&#23545;&#24418;&#29366;&#21644;/&#25110;&#21464;&#24418;&#36807;&#31243;&#28508;&#22312;&#21464;&#21270;&#30340;&#35782;&#21035;&#12290;&#36827;&#19968;&#27493;&#23558;&#24418;&#29366;&#21464;&#24418;&#36807;&#31243;&#30340;&#28508;&#22312;&#21464;&#21270;&#21306;&#20998;&#20026;&#19982;&#24133;&#24230;&#21644;/&#25110;&#30456;&#20301;&#30456;&#20851;&#30340;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
A modelling framework suitable for detecting shape shifts in functional profiles combining the notion of Fr\'echet mean and the concept of deformation models is developed and proposed. The generalized mean sense offerred by the Fr\'echet mean notion is employed to capture the typical pattern of the profiles under study, while the concept of deformation models, and in particular of the shape invariant model, allows for interpretable parameterizations of profile's deviations from the typical shape. EWMA-type control charts compatible with the functional nature of data and the employed deformation model are built and proposed, exploiting certain shape characteristics of the profiles under study with respect to the generalised mean sense, allowing for the identification of potential shifts concerning the shape and/or the deformation process. Potential shifts in the shape deformation process, are further distingu\-ished to significant shifts with respect to amplitude and/or the phase of the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#30340;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#19981;&#21463;&#36317;&#31163;&#38142;&#25509;&#20934;&#21017;&#30340;&#38480;&#21046;&#65292;&#33021;&#36866;&#24212;&#19981;&#21516;&#30340;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;&#21463;&#20445;&#25252;&#32676;&#20307;&#12290;</title><link>http://arxiv.org/abs/2005.03197</link><description>&lt;p&gt;
&#20844;&#24179;&#30340;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fair Algorithms for Hierarchical Agglomerative Clustering. (arXiv:2005.03197v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.03197
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#30340;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#19981;&#21463;&#36317;&#31163;&#38142;&#25509;&#20934;&#21017;&#30340;&#38480;&#21046;&#65292;&#33021;&#36866;&#24212;&#19981;&#21516;&#30340;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;&#21463;&#20445;&#25252;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#22312;&#29616;&#20195;&#25968;&#25454;&#31185;&#23398;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#26088;&#22312;&#23558;&#25968;&#25454;&#38598;&#20998;&#21106;&#20026;&#32858;&#31867;&#65292;&#24182;&#29983;&#25104;&#25968;&#25454;&#26679;&#26412;&#20043;&#38388;&#30340;&#23618;&#27425;&#20851;&#31995;&#12290;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#34987;&#24212;&#29992;&#20110;&#29983;&#29289;&#23398;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25512;&#33616;&#31995;&#32479;&#31561;&#35768;&#22810;&#24212;&#29992;&#20013;&#12290;&#22240;&#27492;&#65292;&#30830;&#20445;&#36825;&#20123;&#31639;&#27861;&#26159;&#20844;&#24179;&#30340;&#33267;&#20851;&#37325;&#35201; -- &#21363;&#20351;&#25968;&#25454;&#38598;&#23545;&#26576;&#20123;&#21463;&#20445;&#25252;&#32676;&#20307;&#23384;&#22312;&#20559;&#24046;&#65292;&#29983;&#25104;&#30340;&#32858;&#31867;&#36755;&#20986;&#20063;&#19981;&#24212;&#27495;&#35270;&#26469;&#33258;&#20219;&#20309;&#36825;&#20123;&#32676;&#20307;&#30340;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#38024;&#23545;&#20844;&#24179;&#32858;&#31867;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22522;&#20110;&#20013;&#24515;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#22914;k-&#20013;&#20540;&#21644;k-&#22343;&#20540;&#32858;&#31867;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#30340;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#65292;&#23427;&#33021;&#22312;&#20351;&#29992;&#20219;&#20309;&#36317;&#31163;&#38142;&#25509;&#20934;&#21017;&#30340;&#24773;&#20917;&#19979;&#24378;&#21046;&#25191;&#34892;&#20844;&#24179;&#32422;&#26463;&#65292;&#24182;&#33021;&#25512;&#24191;&#21040;&#23618;&#27425;&#32858;&#31867;&#30340;&#20219;&#20309;&#33258;&#28982;&#20844;&#24179;&#24230;&#37327;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#21463;&#20445;&#25252;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Agglomerative Clustering (HAC) algorithms are extensively utilized in modern data science, and seek to partition the dataset into clusters while generating a hierarchical relationship between the data samples. HAC algorithms are employed in many applications, such as biology, natural language processing, and recommender systems. Thus, it is imperative to ensure that these algorithms are fair -- even if the dataset contains biases against certain protected groups, the cluster outputs generated should not discriminate against samples from any of these groups. However, recent work in clustering fairness has mostly focused on center-based clustering algorithms, such as k-median and k-means clustering. In this paper, we propose fair algorithms for performing HAC that enforce fairness constraints 1) irrespective of the distance linkage criteria used, 2) generalize to any natural measures of clustering fairness for HAC, 3) work for multiple protected groups, and 4) have competiti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#35760;&#24405;&#30340;&#32972;&#26223;&#12289;&#20915;&#31574;&#21644;&#32467;&#26524;&#20013;&#20272;&#35745;&#20010;&#20307;&#23618;&#38754;&#30340;&#22240;&#26524;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#32473;&#20986;&#20102;&#22522;&#20110;&#36317;&#31163;&#24230;&#37327;&#30340;&#24191;&#20041;&#21270;&#30028;&#38480;&#20197;&#21450;&#30456;&#24212;&#30340;&#26679;&#26412;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#26368;&#23567;&#21270;&#30028;&#38480;&#30340;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#26469;&#23454;&#29616;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2001.07426</link><description>&lt;p&gt;
&#24191;&#20041;&#21270;&#30028;&#38480;&#21644;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#20272;&#35745;&#28508;&#22312;&#32467;&#26524;&#21644;&#22240;&#26524;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Generalization Bounds and Representation Learning for Estimation of Potential Outcomes and Causal Effects. (arXiv:2001.07426v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2001.07426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#35760;&#24405;&#30340;&#32972;&#26223;&#12289;&#20915;&#31574;&#21644;&#32467;&#26524;&#20013;&#20272;&#35745;&#20010;&#20307;&#23618;&#38754;&#30340;&#22240;&#26524;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#32473;&#20986;&#20102;&#22522;&#20110;&#36317;&#31163;&#24230;&#37327;&#30340;&#24191;&#20041;&#21270;&#30028;&#38480;&#20197;&#21450;&#30456;&#24212;&#30340;&#26679;&#26412;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#26368;&#23567;&#21270;&#30028;&#38480;&#30340;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#26469;&#23454;&#29616;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#12289;&#32463;&#27982;&#21644;&#25945;&#32946;&#31561;&#21508;&#39046;&#22495;&#30340;&#20174;&#19994;&#32773;&#37117;&#28212;&#26395;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#25913;&#21892;&#20915;&#31574;&#12290;&#30001;&#20110;&#23454;&#39564;&#30340;&#25104;&#26412;&#21644;&#19981;&#20999;&#23454;&#38469;&#24615;&#65292;&#20197;&#21450;&#30005;&#23376;&#35760;&#24405;&#20445;&#30041;&#30340;&#24040;&#22823;&#22686;&#38271;&#65292;&#38750;&#23454;&#39564;&#35266;&#27979;&#25968;&#25454;&#35780;&#20272;&#20915;&#31574;&#30340;&#38382;&#39064;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#26412;&#25991;&#21363;&#26159;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#23637;&#24320;&#30740;&#31350;&#12290;&#25105;&#20204;&#29305;&#21035;&#30740;&#31350;&#20102;&#20174;&#35760;&#24405;&#30340;&#32972;&#26223;&#12289;&#20915;&#31574;&#21644;&#32467;&#26524;&#20013;&#20272;&#35745;&#20010;&#20307;&#23618;&#38754;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#20363;&#22914;&#21333;&#20010;&#24739;&#32773;&#23545;&#19981;&#21516;&#33647;&#29289;&#30340;&#21453;&#24212;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#22522;&#20110;&#25509;&#21463;&#19981;&#21516;&#27835;&#30103;&#32452;&#20043;&#38388;&#36317;&#31163;&#24230;&#37327;&#30340;&#20272;&#35745;&#25928;&#26524;&#35823;&#24046;&#30340;&#24191;&#20041;&#21270;&#30028;&#38480;&#65292;&#20801;&#35768;&#26679;&#26412;&#37325;&#26032;&#21152;&#26435;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#25105;&#20204;&#30028;&#38480;&#32039;&#23494;&#30340;&#26465;&#20214;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#19982;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#32467;&#26524;&#30340;&#20851;&#31995;&#12290;&#22312;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26368;&#23567;&#21270;&#30028;&#38480;&#30340;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#34920;&#31034;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Practitioners in diverse fields such as healthcare, economics and education are eager to apply machine learning to improve decision making. The cost and impracticality of performing experiments and a recent monumental increase in electronic record keeping has brought attention to the problem of evaluating decisions based on non-experimental observational data. This is the setting of this work. In particular, we study estimation of individual-level causal effects, such as a single patient's response to alternative medication, from recorded contexts, decisions and outcomes. We give generalization bounds on the error in estimated effects based on distance measures between groups receiving different treatments, allowing for sample re-weighting. We provide conditions under which our bound is tight and show how it relates to results for unsupervised domain adaptation. Led by our theoretical results, we devise representation learning algorithms that minimize our bound, by regularizing the rep
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#31232;&#30095;&#35266;&#27979;&#19979;&#30340;&#20108;&#27425;&#24352;&#37327;&#24674;&#22797;&#38382;&#39064;&#65292;&#21457;&#29616;&#38750;&#20984;&#26041;&#27861;&#33021;&#22815;&#22312;&#32447;&#24615;&#26679;&#26412;&#25968;&#37327;&#19979;&#20445;&#35777;&#35823;&#24046;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#20840;&#23616;&#26497;&#23567;&#20540;&#65292;&#24182;&#25913;&#36827;&#20102;&#35266;&#27979;&#26377;&#38480;&#24773;&#20917;&#19979;&#30340;CP&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/1811.00148</link><description>&lt;p&gt;
&#23545;&#31232;&#30095;&#35266;&#27979;&#19979;&#30340;&#20108;&#27425;&#24352;&#37327;&#36827;&#34892;&#24674;&#22797;&#30340;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Recovery Guarantees for Quadratic Tensors with Sparse Observations. (arXiv:1811.00148v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1811.00148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#31232;&#30095;&#35266;&#27979;&#19979;&#30340;&#20108;&#27425;&#24352;&#37327;&#24674;&#22797;&#38382;&#39064;&#65292;&#21457;&#29616;&#38750;&#20984;&#26041;&#27861;&#33021;&#22815;&#22312;&#32447;&#24615;&#26679;&#26412;&#25968;&#37327;&#19979;&#20445;&#35777;&#35823;&#24046;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#20840;&#23616;&#26497;&#23567;&#20540;&#65292;&#24182;&#25913;&#36827;&#20102;&#35266;&#27979;&#26377;&#38480;&#24773;&#20917;&#19979;&#30340;CP&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#24352;&#37327;&#23436;&#25972;&#38382;&#39064;&#65292;&#21363;&#39044;&#27979;&#24352;&#37327;&#20013;&#32570;&#22833;&#30340;&#26465;&#30446;&#12290;&#24120;&#29992;&#30340;CP&#27169;&#22411;&#20855;&#26377;&#19977;&#27425;&#20056;&#31215;&#24418;&#24335;&#65292;&#20294;&#19968;&#31181;&#26367;&#20195;&#30340;&#20108;&#27425;&#27169;&#22411;&#23478;&#26063;&#24050;&#32463;&#20174;&#25512;&#33616;&#31995;&#32479;&#31561;&#24212;&#29992;&#20013;&#20986;&#29616;&#65292;&#20854;&#26159;&#23545;&#25104;&#23545;&#20056;&#31215;&#27714;&#21644;&#32780;&#19981;&#26159;&#19977;&#27425;&#20056;&#31215;&#12290;&#38750;&#20984;&#26041;&#27861;&#26159;&#23398;&#20064;&#20108;&#27425;&#27169;&#22411;&#30340;&#39318;&#36873;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#23427;&#20204;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#35823;&#24046;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#65292;&#21482;&#38656;&#32447;&#24615;&#25968;&#37327;&#30340;&#26679;&#26412;&#65292;&#24179;&#22343;&#22343;&#26041;&#35823;&#24046;&#30446;&#26631;&#20989;&#25968;&#30340;&#25152;&#26377;&#23616;&#37096;&#26497;&#23567;&#20540;&#37117;&#26159;&#20840;&#23616;&#26497;&#23567;&#20540;&#65292;&#24182;&#21487;&#20197;&#24674;&#22797;&#21407;&#22987;&#24352;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#34920;&#26126;&#22312;&#35266;&#27979;&#25968;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20108;&#27425;&#27169;&#22411;&#27604;CP&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the tensor completion problem of predicting the missing entries of a tensor. The commonly used CP model has a triple product form, but an alternate family of quadratic models, which are the sum of pairwise products instead of a triple product, have emerged from applications such as recommendation systems. Non-convex methods are the method of choice for learning quadratic models, and this work examines their sample complexity and error guarantee. Our main result is that with the number of samples being only linear in the dimension, all local minima of the mean squared error objective are global minima and recover the original tensor. We substantiate our theoretical results with experiments on synthetic and real-world data, showing that quadratic models have better performance than CP models where there are a limited amount of observations available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24418;&#24335;&#21270;&#25512;&#23548;&#26469;&#35299;&#37322;&#20102;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;RNN&#36716;&#21270;&#20026;&#8220;Vanilla LSTM&#8221;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/1808.03314</link><description>&lt;p&gt;
&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network. (arXiv:1808.03314v10 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1808.03314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24418;&#24335;&#21270;&#25512;&#23548;&#26469;&#35299;&#37322;&#20102;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;RNN&#36716;&#21270;&#20026;&#8220;Vanilla LSTM&#8221;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#22312;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20102;&#39640;&#25928;&#30340;&#25928;&#26524;&#65292;&#22240;&#27492;&#22312;&#31185;&#23398;&#26399;&#21002;&#12289;&#25216;&#26415;&#21338;&#23458;&#21644;&#23454;&#29616;&#25351;&#21335;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#22810;&#25968;&#25991;&#31456;&#20013;&#65292;LSTM&#32593;&#32476;&#21450;&#20854;&#29238;&#31867;RNN&#30340;&#25512;&#25512;&#29702;&#20844;&#24335;&#34987;&#20197;&#20844;&#29702;&#30340;&#26041;&#24335;&#38472;&#36848;&#65292;&#32780;&#35757;&#32451;&#20844;&#24335;&#21017;&#23436;&#20840;&#34987;&#30465;&#30053;&#12290;&#27492;&#22806;&#65292;&#20851;&#20110;&#8220;&#23637;&#24320;&#8221;RNN&#30340;&#25216;&#26415;&#22312;&#25991;&#29486;&#20013;&#36890;&#24120;&#34987;&#25551;&#36848;&#65292;&#20294;&#32570;&#20047;&#35299;&#37322;&#12290;&#26412;&#25991;&#26088;&#22312;&#22312;&#19968;&#31687;&#25991;&#31456;&#20013;&#35299;&#37322;RNN&#21644;LSTM&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;&#25105;&#20204;&#20174;&#20449;&#21495;&#22788;&#29702;&#30340;&#27010;&#24565;&#20013;&#24418;&#24335;&#21270;&#22320;&#25512;&#23548;&#20986;&#20102;RNN&#30340;&#22522;&#26412;&#20844;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35777;&#26126;&#20102;&#19968;&#20010;&#31934;&#30830;&#30340;&#38472;&#36848;&#65292;&#24471;&#21040;&#20102;RNN&#30340;&#23637;&#24320;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#23457;&#26597;&#20102;&#35757;&#32451;&#26631;&#20934;RNN&#30340;&#22256;&#38590;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#36923;&#36753;&#35770;&#35777;&#23558;RNN&#36716;&#21270;&#20026;&#8220;Vanilla LSTM&#8221;&#32593;&#32476;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19982;&#35757;&#32451;&#36807;&#31243;&#30456;&#20851;&#30340;&#25152;&#26377;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Because of their effectiveness in broad practical applications, LSTM networks have received a wealth of coverage in scientific journals, technical blogs, and implementation guides. However, in most articles, the inference formulas for the LSTM network and its parent, RNN, are stated axiomatically, while the training formulas are omitted altogether. In addition, the technique of "unrolling" an RNN is routinely presented without justification throughout the literature. The goal of this paper is to explain the essential RNN and LSTM fundamentals in a single document. Drawing from concepts in signal processing, we formally derive the canonical RNN formulation from differential equations. We then propose and prove a precise statement, which yields the RNN unrolling technique. We also review the difficulties with training the standard RNN and address them by transforming the RNN into the "Vanilla LSTM" network through a series of logical arguments. We provide all equations pertaining to the 
&lt;/p&gt;</description></item></channel></rss>