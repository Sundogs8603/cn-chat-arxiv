<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>ALERT-Transformer&#26159;&#19968;&#31181;&#23558;&#24322;&#27493;&#24863;&#30693;&#19982;&#21516;&#27493;&#22788;&#29702;&#30456;&#32467;&#21512;&#30340;&#26032;&#39062;&#26725;&#25509;&#26041;&#24335;&#65292;&#36890;&#36807;ALERT&#27169;&#22359;&#12289;&#28789;&#27963;&#30340;&#25968;&#25454;&#35835;&#21462;&#21644;&#22522;&#20110;&#22359;&#30340;&#31232;&#30095;&#24615;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#23454;&#26102;&#20107;&#20214;&#39537;&#21160;&#26102;&#31354;&#25968;&#25454;&#30340;&#32463;&#20856;&#22788;&#29702;&#65292;&#20854;&#24615;&#33021;&#36229;&#36807;&#31454;&#20105;&#23545;&#25163;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#24310;&#36831;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01393</link><description>&lt;p&gt;
ALERT-Transformer: &#23558;&#24322;&#27493;&#21644;&#21516;&#27493;&#26426;&#22120;&#23398;&#20064;&#26725;&#25509;&#22312;&#23454;&#26102;&#20107;&#20214;&#39537;&#21160;&#30340;&#26102;&#31354;&#25968;&#25454;&#19978;
&lt;/p&gt;
&lt;p&gt;
ALERT-Transformer: Bridging Asynchronous and Synchronous Machine Learning for Real-Time Event-based Spatio-Temporal Data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01393
&lt;/p&gt;
&lt;p&gt;
ALERT-Transformer&#26159;&#19968;&#31181;&#23558;&#24322;&#27493;&#24863;&#30693;&#19982;&#21516;&#27493;&#22788;&#29702;&#30456;&#32467;&#21512;&#30340;&#26032;&#39062;&#26725;&#25509;&#26041;&#24335;&#65292;&#36890;&#36807;ALERT&#27169;&#22359;&#12289;&#28789;&#27963;&#30340;&#25968;&#25454;&#35835;&#21462;&#21644;&#22522;&#20110;&#22359;&#30340;&#31232;&#30095;&#24615;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#23454;&#26102;&#20107;&#20214;&#39537;&#21160;&#26102;&#31354;&#25968;&#25454;&#30340;&#32463;&#20856;&#22788;&#29702;&#65292;&#20854;&#24615;&#33021;&#36229;&#36807;&#31454;&#20105;&#23545;&#25163;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#31264;&#23494;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#30001;&#20107;&#20214;&#24863;&#24212;&#22120;&#20135;&#29983;&#30340;&#36830;&#32493;&#36229;&#31232;&#30095;&#26102;&#31354;&#25968;&#25454;&#30340;&#32463;&#20856;&#22788;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#31649;&#36947;&#65292;&#30001;&#24322;&#27493;&#24863;&#30693;&#21644;&#21516;&#27493;&#22788;&#29702;&#32452;&#25104;&#65292;&#32467;&#21512;&#20102;&#20960;&#20010;&#24605;&#36335;&#65306;&#65288;1&#65289;&#22522;&#20110;PointNet&#27169;&#22411;&#30340;&#23884;&#20837;&#8212;&#8212;ALERT&#27169;&#22359;&#65292;&#21487;&#20197;&#36890;&#36807;&#27844;&#28431;&#26426;&#21046;&#19981;&#26029;&#25972;&#21512;&#26032;&#20107;&#20214;&#24182;&#28040;&#38500;&#26087;&#20107;&#20214;&#65292;&#65288;2&#65289;&#23884;&#20837;&#25968;&#25454;&#30340;&#28789;&#27963;&#35835;&#21462;&#65292;&#21487;&#20197;&#20197;&#20219;&#20309;&#37319;&#26679;&#29575;&#23558;&#22987;&#32456;&#26368;&#26032;&#30340;&#29305;&#24449;&#36755;&#20837;&#21040;&#19979;&#28216;&#27169;&#22411;&#20013;&#65292;&#65288;3&#65289;&#20511;&#37492;Vision Transformer&#30340;&#22522;&#20110;&#22359;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;&#36755;&#20837;&#30340;&#31232;&#30095;&#24615;&#20197;&#20248;&#21270;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;&#36825;&#20123;&#23884;&#20837;&#28982;&#21518;&#30001;&#19968;&#20010;&#32463;&#36807;&#23545;&#35937;&#21644;&#25163;&#21183;&#35782;&#21035;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#36827;&#34892;&#22788;&#29702;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#27604;&#31454;&#20105;&#23545;&#25163;&#26356;&#20302;&#30340;&#24310;&#36831;&#65292;&#36798;&#21040;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#24322;&#27493;&#27169;&#22411;&#21487;&#20197;&#20197;&#20219;&#20309;&#25152;&#38656;&#30340;&#37319;&#26679;&#29575;&#36827;&#34892;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We seek to enable classic processing of continuous ultra-sparse spatiotemporal data generated by event-based sensors with dense machine learning models. We propose a novel hybrid pipeline composed of asynchronous sensing and synchronous processing that combines several ideas: (1) an embedding based on PointNet models -- the ALERT module -- that can continuously integrate new and dismiss old events thanks to a leakage mechanism, (2) a flexible readout of the embedded data that allows to feed any downstream model with always up-to-date features at any sampling rate, (3) exploiting the input sparsity in a patch-based approach inspired by Vision Transformer to optimize the efficiency of the method. These embeddings are then processed by a transformer model trained for object and gesture recognition. Using this approach, we achieve performances at the state-of-the-art with a lower latency than competitors. We also demonstrate that our asynchronous model can operate at any desired sampling r
&lt;/p&gt;</description></item><item><title>FedMoE&#26159;&#19968;&#31181;&#27169;&#22411;&#24322;&#26500;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#20849;&#20139;&#30340;&#23567;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#26412;&#22320;&#38376;&#25511;&#32593;&#32476;&#20998;&#37197;&#32473;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#24322;&#26500;&#22823;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38544;&#31169;&#12289;&#24615;&#33021;&#12289;&#36890;&#20449;&#21644;&#35745;&#31639;&#25104;&#26412;&#31561;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01350</link><description>&lt;p&gt;
FedMoE: &#25968;&#25454;&#32423;&#21035;&#20010;&#24615;&#21270;&#30340;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#29992;&#20110;&#24322;&#26500;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedMoE: Data-Level Personalization with Mixture of Experts for Model-Heterogeneous Personalized Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01350
&lt;/p&gt;
&lt;p&gt;
FedMoE&#26159;&#19968;&#31181;&#27169;&#22411;&#24322;&#26500;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#20849;&#20139;&#30340;&#23567;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#26412;&#22320;&#38376;&#25511;&#32593;&#32476;&#20998;&#37197;&#32473;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#24322;&#26500;&#22823;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38544;&#31169;&#12289;&#24615;&#33021;&#12289;&#36890;&#20449;&#21644;&#35745;&#31639;&#25104;&#26412;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24191;&#27867;&#24212;&#29992;&#20110;&#20998;&#25955;&#25968;&#25454;&#30340;&#21327;&#21516;&#35757;&#32451;&#65292;&#20294;&#38754;&#20020;&#25968;&#25454;&#12289;&#31995;&#32479;&#21644;&#27169;&#22411;&#24322;&#26500;&#31561;&#25361;&#25112;&#12290;&#36825;&#23548;&#33268;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064; (MHPFL) &#30340;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;MHPFL&#26041;&#27861;&#22312;&#25968;&#25454;&#21644;&#27169;&#22411;&#38544;&#31169;&#12289;&#27169;&#22411;&#24615;&#33021;&#12289;&#36890;&#20449;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#20173;&#23384;&#22312;&#20851;&#20999;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861; (FedMoE) &#65292;&#37319;&#29992;&#33879;&#21517;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411; (MoE) &#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM)&#12290;&#23427;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#24322;&#26500;&#22823;&#27169;&#22411;&#20998;&#37197;&#20102;&#19968;&#20010;&#20849;&#20139;&#30340;&#22343;&#21248;&#23567;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#19968;&#20010;&#26412;&#22320;&#38376;&#25511;&#32593;&#32476;&#12290;(1) &#22312;&#26412;&#22320;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26412;&#22320;&#24322;&#26500;&#27169;&#22411;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#20316;&#20026;&#20010;&#24615;&#21270;&#29305;&#24449;&#65288;&#34920;&#31034;&#65289;&#25552;&#21462;&#30340;&#26412;&#22320;&#19987;&#23478;&#65292;&#32780;&#20849;&#20139;&#30340;&#22343;&#21248;&#23567;&#29305;&#24449;&#25552;&#21462;&#22120;&#21017;&#20316;&#20026;&#24191;&#20041;&#29305;&#24449;&#25552;&#21462;&#30340;&#20840;&#23616;&#19987;&#23478;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is widely employed for collaborative training on decentralized data but faces challenges like data, system, and model heterogeneity. This prompted the emergency of model-heterogeneous personalized federated learning (MHPFL). However, concerns persist regarding data and model privacy, model performance, communication, and computational costs in current MHPFL methods. To tackle these concerns, we propose a novel model-heterogeneous personalized Federated learning algorithm (FedMoE) with the Mixture of Experts (MoE), renowned for enhancing large language models (LLMs). It assigns a shared homogeneous small feature extractor and a local gating network for each client's local heterogeneous large model. (1) During local training, the local heterogeneous model's feature extractor acts as a local expert for personalized feature (representation) extraction, while the shared homogeneous small feature extractor serves as a global expert for generalized feature extraction. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SPHINX-X&#65292;&#19968;&#31181;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#12290;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#19981;&#21516;&#30340;MLLMs&#65292;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05935</link><description>&lt;p&gt;
SPHINX-X: &#25193;&#23637;&#25968;&#25454;&#21644;&#21442;&#25968;&#29992;&#20110;&#19968;&#31995;&#21015;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SPHINX-X&#65292;&#19968;&#31181;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#12290;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#19981;&#21516;&#30340;MLLMs&#65292;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;SPHINX-X&#65292;&#19968;&#31181;&#22522;&#20110;SPHINX&#24320;&#21457;&#30340;&#24191;&#27867;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#31995;&#21015;&#12290;&#20026;&#20102;&#25913;&#21892;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#36890;&#36807;&#31227;&#38500;&#20887;&#20313;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#12289;&#32469;&#36807;&#23436;&#20840;&#22635;&#20805;&#30340;&#23376;&#22270;&#20687;&#65292;&#24182;&#23558;&#22810;&#38454;&#27573;&#35757;&#32451;&#31616;&#21270;&#25104;&#20026;&#19968;&#38454;&#27573;&#30340;&#20840;&#38598;&#21512;&#27169;&#24335;&#65292;&#20462;&#25913;&#20102;SPHINX&#26694;&#26550;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;MLLM&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#32452;&#35013;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#36328;&#35821;&#35328;&#12289;&#36328;&#35270;&#35273;&#21644;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#30340;&#22810;&#39046;&#22495;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#25105;&#20204;&#30340;OCR&#23494;&#38598;&#21644;Mark&#25968;&#25454;&#38598;&#20016;&#23500;&#36825;&#20010;&#25910;&#38598;&#65292;&#25193;&#23637;&#20102;&#22810;&#26679;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#22522;&#30784;LLM&#36827;&#34892;&#35757;&#32451;&#65292;&#21253;&#25324;TinyLlama1.1B&#12289;InternLM2-7B&#12289;LLaMA2-13B&#21644;Mixtral8x7B&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#21464;&#21270;&#30340;MLLMs&#12290;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#20102;&#22810;&#27169;&#24577;&#24615;&#33021;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM) series developed upon SPHINX. To improve the architecture and training efficiency, we modify the SPHINX framework by removing redundant visual encoders, bypassing fully-padded sub-images with skip tokens, and simplifying multi-stage training into a one-stage all-in-one paradigm. To fully unleash the potential of MLLMs, we assemble a comprehensive multi-domain and multimodal dataset covering publicly available resources in language, vision, and vision-language tasks. We further enrich this collection with our curated OCR intensive and Set-of-Mark datasets, extending the diversity and generality. By training over different base LLMs including TinyLlama1.1B, InternLM2-7B, LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in parameter size and multilingual capabilities. Comprehensive benchmarking reveals a strong correlation between the multi-modal performance with the data and parameter scales. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;GNN&#30340;&#22270;&#20013;&#33410;&#28857;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24179;&#28369;&#32422;&#26463;&#12289;&#20266;&#26631;&#31614;&#36845;&#20195;&#21644;&#39046;&#22495;&#26631;&#31614;&#30452;&#26041;&#22270;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#21487;&#20197;&#22312;&#19981;&#35757;&#32451;GNN&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#26631;&#20934;&#27969;&#34892;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05934</link><description>&lt;p&gt;
&#26080;&#38656;GNN&#30340;&#22270;&#20013;&#33410;&#28857;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Classifying Nodes in Graphs without GNNs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05934
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;GNN&#30340;&#22270;&#20013;&#33410;&#28857;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24179;&#28369;&#32422;&#26463;&#12289;&#20266;&#26631;&#31614;&#36845;&#20195;&#21644;&#39046;&#22495;&#26631;&#31614;&#30452;&#26041;&#22270;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#21487;&#20197;&#22312;&#19981;&#35757;&#32451;GNN&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#26631;&#20934;&#27969;&#34892;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#22270;&#20013;&#33410;&#28857;&#20998;&#31867;&#30340;&#20027;&#35201;&#33539;&#24335;&#65292;&#20294;&#30001;&#20110;&#20854;&#28040;&#24687;&#20256;&#36882;&#26550;&#26500;&#23548;&#33268;&#20854;&#20855;&#26377;&#19968;&#20123;&#19981;&#29702;&#24819;&#30340;&#23646;&#24615;&#12290;&#26368;&#36817;&#65292;&#33976;&#39311;&#26041;&#27861;&#25104;&#21151;&#22320;&#22312;&#27979;&#35797;&#26102;&#28040;&#38500;&#20102;&#23545;GNN&#30340;&#20351;&#29992;&#65292;&#20294;&#20173;&#28982;&#38656;&#35201;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#12290;&#25105;&#20204;&#23545;GNN&#22312;&#33976;&#39311;&#26041;&#27861;&#20013;&#30340;&#35282;&#33394;&#36827;&#34892;&#20102;&#20180;&#32454;&#20998;&#26512;&#12290;&#36825;&#20010;&#20998;&#26512;&#23548;&#33268;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#19981;&#20381;&#36182;GNN&#30340;&#33410;&#28857;&#20998;&#31867;&#26041;&#27861;&#65292;&#26080;&#38656;&#22312;&#35757;&#32451;&#25110;&#27979;&#35797;&#26102;&#20351;&#29992;&#23427;&#20204;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#21547;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#24179;&#28369;&#32422;&#26463;&#12289;&#20266;&#26631;&#31614;&#36845;&#20195;&#21644;&#39046;&#22495;&#26631;&#31614;&#30452;&#26041;&#22270;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#26041;&#27861;&#21487;&#20197;&#22312;&#26631;&#20934;&#27969;&#34892;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#65292;&#32780;&#19981;&#38656;&#35201;&#35757;&#32451;GNN&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are the dominant paradigm for classifying nodes in a graph, but they have several undesirable attributes stemming from their message passing architecture. Recently, distillation methods succeeded in eliminating the use of GNNs at test time but they still require them during training. We perform a careful analysis of the role that GNNs play in distillation methods. This analysis leads us to propose a fully GNN-free approach for node classification, not requiring them at train or test time. Our method consists of three key components: smoothness constraints, pseudo-labeling iterations and neighborhood-label histograms. Our final approach can match the state-of-the-art accuracy on standard popular benchmarks such as citation and co-purchase networks, without training a GNN.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#26102;&#38388;&#24207;&#21015;&#22312;&#39057;&#22495;&#20013;&#30340;&#34920;&#31034;&#65292;&#25506;&#35752;&#20102;&#22522;&#20110;&#35780;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#25552;&#20986;&#20102;&#39057;&#22495;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#19982;&#32463;&#20856;&#30340;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>https://arxiv.org/abs/2402.05933</link><description>&lt;p&gt;
&#39057;&#22495;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Time Series Diffusion in the Frequency Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#26102;&#38388;&#24207;&#21015;&#22312;&#39057;&#22495;&#20013;&#30340;&#34920;&#31034;&#65292;&#25506;&#35752;&#20102;&#22522;&#20110;&#35780;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#25552;&#20986;&#20102;&#39057;&#22495;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#19982;&#32463;&#20856;&#30340;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20613;&#37324;&#21494;&#20998;&#26512;&#22312;&#20449;&#21495;&#22788;&#29702;&#30340;&#21457;&#23637;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#36825;&#20351;&#25105;&#20204;&#24819;&#30693;&#36947;&#36825;&#20010;&#26694;&#26550;&#26159;&#21542;&#33021;&#22815;&#21516;&#26679;&#26377;&#30410;&#20110;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26102;&#38388;&#24207;&#21015;&#25193;&#25955;&#27169;&#22411;&#30340;&#33539;&#22260;&#26469;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#39057;&#22495;&#20013;&#34920;&#31034;&#26102;&#38388;&#24207;&#21015;&#26159;&#21542;&#23545;&#22522;&#20110;&#35780;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20855;&#26377;&#26377;&#29992;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36890;&#36807;&#20174;&#26102;&#38388;&#22495;&#20013;&#25193;&#25955;&#30340;&#32463;&#20856;SDE&#20844;&#24335;&#20986;&#21457;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39057;&#22495;&#20013;&#21457;&#29983;&#20102;&#19968;&#31181;&#21452;&#37325;&#25193;&#25955;&#36807;&#31243;&#65292;&#24182;&#20855;&#26377;&#19968;&#20010;&#37325;&#35201;&#30340;&#32454;&#24494;&#24046;&#21035;&#65306;&#24067;&#26391;&#36816;&#21160;&#34987;&#25105;&#20204;&#31216;&#20043;&#20026;&#38236;&#20687;&#24067;&#26391;&#36816;&#21160;&#25152;&#21462;&#20195;&#65292;&#20854;&#29305;&#24449;&#26159;&#20854;&#32452;&#20998;&#20043;&#38388;&#30340;&#38236;&#20687;&#23545;&#31216;&#24615;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36866;&#24212;&#21435;&#22122;&#35780;&#20998;&#21305;&#37197;&#26041;&#27861;&#26469;&#23454;&#29616;&#39057;&#22495;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#36825;&#23548;&#33268;&#20102;&#39057;&#22495;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#32463;&#20856;&#30340;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#24037;&#20316;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fourier analysis has been an instrumental tool in the development of signal processing. This leads us to wonder whether this framework could similarly benefit generative modelling. In this paper, we explore this question through the scope of time series diffusion models. More specifically, we analyze whether representing time series in the frequency domain is a useful inductive bias for score-based diffusion models. By starting from the canonical SDE formulation of diffusion in the time domain, we show that a dual diffusion process occurs in the frequency domain with an important nuance: Brownian motions are replaced by what we call mirrored Brownian motions, characterized by mirror symmetries among their components. Building on this insight, we show how to adapt the denoising score matching approach to implement diffusion models in the frequency domain. This results in frequency diffusion models, which we compare to canonical time diffusion models. Our empirical evaluation on real-wor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#35805;&#24335;&#32593;&#31449;&#23548;&#33322;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010; WEBLINX &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#37327;&#20449;&#24687;&#30340;&#22788;&#29702;&#29942;&#39048;&#65292;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#26816;&#32034;&#21551;&#21457;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#22810;&#31181;&#22330;&#26223;&#19979;&#22797;&#21046;&#20154;&#31867;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05930</link><description>&lt;p&gt;
WebLINX: &#22810;&#36718;&#23545;&#35805;&#19979;&#30340;&#30495;&#23454;&#19990;&#30028;&#32593;&#31449;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
WebLINX: Real-World Website Navigation with Multi-Turn Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#35805;&#24335;&#32593;&#31449;&#23548;&#33322;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010; WEBLINX &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#37327;&#20449;&#24687;&#30340;&#22788;&#29702;&#29942;&#39048;&#65292;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#26816;&#32034;&#21551;&#21457;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#22810;&#31181;&#22330;&#26223;&#19979;&#22797;&#21046;&#20154;&#31867;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#35805;&#24335;&#32593;&#31449;&#23548;&#33322;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#25968;&#23383;&#20195;&#29702;&#25511;&#21046;&#30528;&#19968;&#20010;&#32593;&#39029;&#27983;&#35272;&#22120;&#65292;&#24182;&#25353;&#29031;&#29992;&#25143;&#30340;&#25351;&#20196;&#20197;&#22810;&#36718;&#23545;&#35805;&#30340;&#26041;&#24335;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; WEBLINX - &#19968;&#20010;100K&#20132;&#20114;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#65292;&#22312;2300&#20010;&#19987;&#23478;&#28436;&#31034;&#20013;&#36827;&#34892;&#20102;&#23545;&#35805;&#24335;&#32593;&#31449;&#23548;&#33322;&#30340;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#28085;&#30422;&#20102;150&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#32593;&#31449;&#19978;&#30340;&#24191;&#27867;&#27169;&#24335;&#65292;&#21487;&#20197;&#29992;&#20110;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#29702;&#12290;&#30001;&#20110;&#23384;&#22312;&#22823;&#37327;&#20449;&#24687;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#26080;&#27861;&#23454;&#26102;&#22788;&#29702;&#25972;&#20010;&#32593;&#39029;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#29942;&#39048;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21463;&#26816;&#32034;&#21551;&#21457;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#25490;&#21517;&#30456;&#20851;&#20803;&#32032;&#26469;&#39640;&#25928;&#22320;&#20462;&#21098; HTML &#39029;&#38754;&#12290;&#25105;&#20204;&#20351;&#29992;&#36873;&#23450;&#30340;&#20803;&#32032;&#65292;&#20197;&#21450;&#23631;&#24149;&#25130;&#22270;&#21644;&#25805;&#20316;&#21382;&#21490;&#35760;&#24405;&#65292;&#35780;&#20272;&#21508;&#31181;&#27169;&#22411;&#22312;&#23548;&#33322;&#32593;&#39029;&#26102;&#22797;&#21046;&#20154;&#31867;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20174;&#23567;&#22411;&#32431;&#25991;&#26412;&#27169;&#22411;&#21040;&#19987;&#26377;&#30340;&#22810;&#27169;&#24577; LLMs &#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the problem of conversational web navigation, where a digital agent controls a web browser and follows user instructions to solve real-world tasks in a multi-turn dialogue fashion. To support this problem, we introduce WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert demonstrations of conversational web navigation. Our benchmark covers a broad range of patterns on over 150 real-world websites and can be used to train and evaluate agents in diverse scenarios. Due to the magnitude of information present, Large Language Models (LLMs) cannot process entire web pages in real-time. To solve this bottleneck, we design a retrieval-inspired model that efficiently prunes HTML pages by ranking relevant elements. We use the selected elements, along with screenshots and action history, to assess a variety of models for their ability to replicate human behavior when navigating the web. Our experiments span from small text-only to proprietary multimodal LLMs. We fi
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#26234;&#33021;&#20307;&#22522;&#30784;&#27169;&#22411;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#33021;&#22815;&#36328;&#39046;&#22495;&#12289;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65292;&#23637;&#29616;&#20986;&#36890;&#29992;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#19988;&#22312;&#26426;&#22120;&#20154;&#12289;&#28216;&#25103; AI &#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.05929</link><description>&lt;p&gt;
&#19968;&#20010;&#20132;&#20114;&#24335;&#26234;&#33021;&#20307;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Interactive Agent Foundation Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05929
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#26234;&#33021;&#20307;&#22522;&#30784;&#27169;&#22411;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#33021;&#22815;&#36328;&#39046;&#22495;&#12289;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65292;&#23637;&#29616;&#20986;&#36890;&#29992;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#19988;&#22312;&#26426;&#22120;&#20154;&#12289;&#28216;&#25103; AI &#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#27491;&#22312;&#20174;&#21019;&#24314;&#38745;&#24577;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#36716;&#21464;&#20026;&#33021;&#22815;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#21160;&#24577;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#26234;&#33021;&#20307;&#22522;&#30784;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#26234;&#33021;&#20307;&#35757;&#32451;&#33539;&#24335;&#65292;&#29992;&#20110;&#35757;&#32451;&#36328;&#39046;&#22495;&#12289;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#30340; AI &#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#33539;&#24335;&#32479;&#19968;&#20102;&#21508;&#31181;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21253;&#25324;&#35270;&#35273;&#36974;&#25377;&#33258;&#32534;&#30721;&#22120;&#12289;&#35821;&#35328;&#24314;&#27169;&#21644;&#19979;&#19968;&#27493;&#34892;&#21160;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#36890;&#29992;&#32780;&#36866;&#24212;&#24615;&#24378;&#30340; AI &#26694;&#26550;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#29420;&#31435;&#39046;&#22495; - &#26426;&#22120;&#20154;&#12289;&#28216;&#25103; AI &#21644;&#21307;&#30103;&#20445;&#20581;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27599;&#20010;&#39046;&#22495;&#37117;&#23637;&#31034;&#20102;&#29983;&#25104;&#26377;&#24847;&#20041;&#21644;&#19978;&#19979;&#25991;&#30456;&#20851;&#36755;&#20986;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#22312;&#20110;&#20854;&#24191;&#27867;&#24615;&#65292;&#21033;&#29992;&#20102;&#21508;&#31181;&#25968;&#25454;&#28304;&#65292;&#22914;&#26426;&#22120;&#20154;&#24207;&#21015;&#12289;&#28216;&#25103;&#25968;&#25454;&#12289;&#22823;&#35268;&#27169;&#35270;&#39057;&#25968;&#25454;&#38598;&#21644;&#25991;&#26412;&#20449;&#24687;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of artificial intelligence systems is transitioning from creating static, task-specific models to dynamic, agent-based systems capable of performing well in a wide range of applications. We propose an Interactive Agent Foundation Model that uses a novel multi-task agent training paradigm for training AI agents across a wide range of domains, datasets, and tasks. Our training paradigm unifies diverse pre-training strategies, including visual masked auto-encoders, language modeling, and next-action prediction, enabling a versatile and adaptable AI framework. We demonstrate the performance of our framework across three separate domains -- Robotics, Gaming AI, and Healthcare. Our model demonstrates its ability to generate meaningful and contextually relevant outputs in each area. The strength of our approach lies in its generality, leveraging a variety of data sources such as robotics sequences, gameplay data, large-scale video datasets, and textual information for effectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20381;&#36182;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#23574;&#38160;&#29575;&#65292;&#20027;&#35201;&#26159;&#20026;&#20102;&#36991;&#20813;&#26679;&#26412;&#22823;&#23567;&#32553;&#20943;&#23545;&#26041;&#24046;&#20135;&#29983;&#24433;&#21709;&#12290;&#24403;&#20551;&#35774;&#31867;&#21035;&#30340;&#25299;&#25169;&#32467;&#26500;&#31526;&#21512;&#26576;&#20123;&#26465;&#20214;&#26102;&#65292;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#32773;&#30340;&#24615;&#33021;&#19982;&#31867;&#21035;&#30340;&#22797;&#26434;&#24615;&#21644;&#20108;&#38454;&#32479;&#35745;&#37327;&#26377;&#20851;&#12290;</title><link>https://arxiv.org/abs/2402.05928</link><description>&lt;p&gt;
&#20381;&#36182;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#23574;&#38160;&#29575;&#65306;&#36991;&#20813;&#26679;&#26412;&#22823;&#23567;&#32553;&#20943;&#30340;&#24179;&#26041;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Sharp Rates in Dependent Learning Theory: Avoiding Sample Size Deflation for the Square Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20381;&#36182;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#23574;&#38160;&#29575;&#65292;&#20027;&#35201;&#26159;&#20026;&#20102;&#36991;&#20813;&#26679;&#26412;&#22823;&#23567;&#32553;&#20943;&#23545;&#26041;&#24046;&#20135;&#29983;&#24433;&#21709;&#12290;&#24403;&#20551;&#35774;&#31867;&#21035;&#30340;&#25299;&#25169;&#32467;&#26500;&#31526;&#21512;&#26576;&#20123;&#26465;&#20214;&#26102;&#65292;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#32773;&#30340;&#24615;&#33021;&#19982;&#31867;&#21035;&#30340;&#22797;&#26434;&#24615;&#21644;&#20108;&#38454;&#32479;&#35745;&#37327;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20381;&#36182;&#24615;&#65288;&#946;-&#28151;&#21512;&#65289;&#25968;&#25454;&#21644;&#24179;&#26041;&#25439;&#22833;&#30340;&#32479;&#35745;&#23398;&#20064;&#65292;&#22312;&#19968;&#20010;&#20551;&#35774;&#31867;&#21035;&#934;_p&#30340;&#23376;&#38598;F&#20013;&#65292;&#20854;&#20013;&#934;_p&#26159;&#33539;&#25968;&#8741;f&#8741;_&#934;_p&#8801;sup_m&#8805;1 m^{-1/p}&#8741;f&#8741;_L^m&#65292;&#20854;&#20013;p&#8712;[2&#65292;&#8734;]&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21160;&#26426;&#26159;&#22312;&#20855;&#26377;&#20381;&#36182;&#24615;&#25968;&#25454;&#30340;&#23398;&#20064;&#20013;&#23547;&#25214;&#23574;&#38160;&#30340;&#22122;&#22768;&#20132;&#20114;&#39033;&#25110;&#26041;&#24046;&#20195;&#29702;&#12290;&#22312;&#27809;&#26377;&#20219;&#20309;&#21487;&#23454;&#29616;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20856;&#22411;&#30340;&#38750;&#28176;&#36817;&#32467;&#26524;&#26174;&#31034;&#20986;&#26041;&#24046;&#20195;&#29702;&#36890;&#36807;&#24213;&#23618;&#21327;&#21464;&#37327;&#36807;&#31243;&#30340;&#28151;&#21512;&#26102;&#38388;&#36827;&#34892;&#20102;&#20056;&#31215;&#32553;&#20943;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21482;&#35201;&#22312;&#25105;&#20204;&#30340;&#20551;&#35774;&#31867;&#21035;F&#19978;&#65292;L^2&#21644;&#934;_p&#30340;&#25299;&#25169;&#26159;&#21487;&#27604;&#36739;&#30340;&#65292;&#21363;&#934;_p&#26159;&#19968;&#20010;&#24369;&#20122;&#39640;&#26031;&#31867;&#21035;&#65306;&#8741;f&#8741;_&#934;_p&#8818;&#8741;f&#8741;_L^2^&#951;&#65292;&#20854;&#20013;&#951;&#8712;(0&#65292;1]&#65292;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#32773;&#22312;&#20854;&#20027;&#23548;&#39033;&#20013;&#21482;&#23454;&#29616;&#20102;&#19968;&#31181;&#21482;&#20381;&#36182;&#20110;&#31867;&#21035;&#22797;&#26434;&#24615;&#21644;&#20108;&#38454;&#32479;&#35745;&#37327;&#30340;&#36895;&#29575;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#35768;&#22810;&#20381;&#36182;&#24615;&#25968;&#25454;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study statistical learning with dependent ($\beta$-mixing) data and square loss in a hypothesis class $\mathscr{F}\subset L_{\Psi_p}$ where $\Psi_p$ is the norm $\|f\|_{\Psi_p} \triangleq \sup_{m\geq 1} m^{-1/p} \|f\|_{L^m} $ for some $p\in [2,\infty]$. Our inquiry is motivated by the search for a sharp noise interaction term, or variance proxy, in learning with dependent data. Absent any realizability assumption, typical non-asymptotic results exhibit variance proxies that are deflated \emph{multiplicatively} by the mixing time of the underlying covariates process. We show that whenever the topologies of $L^2$ and $\Psi_p$ are comparable on our hypothesis class $\mathscr{F}$ -- that is, $\mathscr{F}$ is a weakly sub-Gaussian class: $\|f\|_{\Psi_p} \lesssim \|f\|_{L^2}^\eta$ for some $\eta\in (0,1]$ -- the empirical risk minimizer achieves a rate that only depends on the complexity of the class and second order statistics in its leading term. Our result holds whether t
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedMeZO&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#38646;&#38454;&#32852;&#37030;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#30340;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;FedMeZO&#19981;&#20165;&#25910;&#25947;&#36895;&#24230;&#24555;&#20110;&#20256;&#32479;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#32780;&#19988;&#22312;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#31574;&#30053;&#20013;&#20855;&#26377;&#20851;&#38190;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.05926</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#38646;&#38454;&#32852;&#37030;&#35843;&#25972;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of Zeroth-Order Federated Tuning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05926
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedMeZO&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#38646;&#38454;&#32852;&#37030;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#30340;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;FedMeZO&#19981;&#20165;&#25910;&#25947;&#36895;&#24230;&#24555;&#20110;&#20256;&#32479;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#32780;&#19988;&#22312;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#31574;&#30053;&#20013;&#20855;&#26377;&#20851;&#38190;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#34701;&#21512;&#20026;&#38544;&#31169;&#20445;&#25252;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24102;&#26469;&#20102;&#26032;&#26102;&#20195;&#12290;&#28982;&#32780;&#65292;&#31934;&#35843;LLM&#25152;&#38656;&#30340;&#24378;&#22823;&#20869;&#23384;&#35201;&#27714;&#22312;&#37096;&#32626;&#21040;&#36793;&#32536;&#35774;&#22791;&#26102;&#20250;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#35774;&#22791;&#30340;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#32852;&#37030;&#29615;&#22659;&#20013;&#25506;&#32034;&#20102;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#30340;&#20840;&#26032;&#25972;&#21512;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;FedMeZO&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#31532;&#19968;&#20010;&#22312;LLM&#32972;&#26223;&#19979;&#32771;&#23519;FedMeZO&#30340;&#29702;&#35770;&#22522;&#30784;&#30340;&#30740;&#31350;&#65292;&#28041;&#21450;&#21040;&#22823;&#21442;&#25968;&#31354;&#38388;&#23545;&#20248;&#21270;&#34892;&#20026;&#30340;&#24433;&#21709;&#12289;&#25910;&#25947;&#24615;&#30340;&#24314;&#31435;&#20197;&#21450;&#20026;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#31574;&#30053;&#30830;&#23450;&#20851;&#38190;&#21442;&#25968;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#35777;&#35777;&#25454;&#25903;&#25345;&#20102;&#36825;&#20010;&#29702;&#35770;&#65292;&#34920;&#26126;FedMeZO&#19981;&#20165;&#27604;&#20256;&#32479;&#30340;&#19968;&#38454;&#26041;&#27861;&#65288;&#22914;SGD&#65289;&#25910;&#25947;&#26356;&#24555;&#65292;&#32780;&#19988;&#26126;&#26174;...
&lt;/p&gt;
&lt;p&gt;
The confluence of Federated Learning (FL) and Large Language Models (LLMs) is ushering in a new era in privacy-preserving natural language processing. However, the intensive memory requirements for fine-tuning LLMs pose significant challenges, especially when deploying on edge devices with limited computational resources. To circumvent this, we explore the novel integration of Memory-efficient Zeroth-Order Optimization within a federated setting, a synergy we denote as FedMeZO. Our study is the first to examine the theoretical underpinnings of FedMeZO in the context of LLMs, tackling key questions regarding the influence of large parameter spaces on optimization behavior, the establishment of convergence properties, and the identification of critical parameters for convergence to inform personalized federated strategies. Our extensive empirical evidence supports the theory, showing that FedMeZO not only converges faster than traditional first-order methods such as SGD but also signific
&lt;/p&gt;</description></item><item><title>GenEFT&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#30740;&#31350;&#27867;&#21270;&#30456;&#21464;&#21644;&#34920;&#31034;&#23398;&#20064;&#21160;&#24577;&#65292;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#30340;&#38745;&#24577;&#21644;&#21160;&#24577;&#29305;&#24615;&#65292;&#36825;&#24357;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#29702;&#35770;&#39044;&#27979;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.05916</link><description>&lt;p&gt;
GenEFT: &#36890;&#36807;&#26377;&#25928;&#29702;&#35770;&#29702;&#35299;&#27169;&#22411;&#27867;&#21270;&#30340;&#38745;&#24577;&#21644;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
GenEFT: Understanding Statics and Dynamics of Model Generalization via Effective Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05916
&lt;/p&gt;
&lt;p&gt;
GenEFT&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#30740;&#31350;&#27867;&#21270;&#30456;&#21464;&#21644;&#34920;&#31034;&#23398;&#20064;&#21160;&#24577;&#65292;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#30340;&#38745;&#24577;&#21644;&#21160;&#24577;&#29305;&#24615;&#65292;&#36825;&#24357;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#29702;&#35770;&#39044;&#27979;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;GenEFT&#65306;&#19968;&#20010;&#26377;&#25928;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#25581;&#31034;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#30340;&#38745;&#24577;&#21644;&#21160;&#24577;&#65292;&#20197;&#22270;&#23398;&#20064;&#20026;&#20363;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#35268;&#27169;&#22686;&#21152;&#26102;&#30340;&#27867;&#21270;&#30456;&#21464;&#65292;&#23558;&#23454;&#39564;&#32467;&#26524;&#19982;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#36817;&#20284;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#35299;&#30721;&#22120;&#26082;&#19981;&#22826;&#24369;&#20063;&#19981;&#22826;&#24378;&#30340;&#8220;&#23567;&#29066;&#23453;&#36125;&#21306;&#22495;&#8221;&#20013;&#23384;&#22312;&#30528;&#27867;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#21160;&#24577;&#30340;&#26377;&#25928;&#29702;&#35770;&#65292;&#23558;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#24314;&#27169;&#20026;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#65288;repons&#65289;&#65292;&#21457;&#29616;&#23427;&#35299;&#37322;&#20102;&#25105;&#20204;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23398;&#20064;&#36895;&#29575;&#25195;&#25551;&#26102;&#35266;&#23519;&#21040;&#30340;&#27867;&#21270;&#21644;&#36807;&#25311;&#21512;&#20043;&#38388;&#30340;&#30456;&#21464;&#12290;&#36825;&#31361;&#20986;&#20102;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#26377;&#25928;&#29702;&#35770;&#22312;&#24357;&#21512;&#26426;&#22120;&#23398;&#20064;&#20013;&#29702;&#35770;&#39044;&#27979;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#26041;&#38754;&#30340;&#21147;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present GenEFT: an effective theory framework for shedding light on the statics and dynamics of neural network generalization, and illustrate it with graph learning examples. We first investigate the generalization phase transition as data size increases, comparing experimental results with information-theory-based approximations. We find generalization in a Goldilocks zone where the decoder is neither too weak nor too powerful. We then introduce an effective theory for the dynamics of representation learning, where latent-space representations are modeled as interacting particles (repons), and find that it explains our experimentally observed phase transition between generalization and overfitting as encoder and decoder learning rates are scanned. This highlights the power of physics-inspired effective theories for bridging the gap between theoretical predictions and practice in machine learning.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28176;&#36827;&#23376;&#32593;&#32476;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#20998;&#38454;&#27573;&#39044;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#26089;&#26399;&#38454;&#27573;&#26080;&#27861;&#35780;&#20272;&#23436;&#25972;&#27169;&#22411;&#21644;&#27169;&#22411;&#36136;&#37327;&#19979;&#38477;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05913</link><description>&lt;p&gt;
&#36890;&#36807;&#28176;&#36827;&#23376;&#32593;&#32476;&#23454;&#29616;&#39640;&#25928;&#30340;&#20998;&#38454;&#27573;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Efficient Stagewise Pretraining via Progressive Subnetworks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05913
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28176;&#36827;&#23376;&#32593;&#32476;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#20998;&#38454;&#27573;&#39044;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#26089;&#26399;&#38454;&#27573;&#26080;&#27861;&#35780;&#20272;&#23436;&#25972;&#27169;&#22411;&#21644;&#27169;&#22411;&#36136;&#37327;&#19979;&#38477;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#39640;&#25928;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;&#19968;&#20010;&#26377;&#25928;&#33539;&#20363;&#26159;&#36827;&#34892;&#20998;&#38454;&#27573;&#35757;&#32451;&#65292;&#21363;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#22686;&#21152;&#27169;&#22411;&#30340;&#22823;&#23567;&#65288;&#20363;&#22914;&#36880;&#28176;&#21472;&#21152;&#65288;Reddi&#31561;&#20154;&#65292;2023&#24180;&#65289;&#65289;&#12290;&#34429;&#28982;&#36164;&#28304;&#21644;&#22681;&#38047;&#26102;&#38388;&#30340;&#33410;&#30465;&#24456;&#21560;&#24341;&#20154;&#65292;&#20294;&#23427;&#20063;&#26377;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#26089;&#26399;&#38454;&#27573;&#26080;&#27861;&#35780;&#20272;&#23436;&#25972;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#30001;&#20110;&#21021;&#22987;&#38454;&#27573;&#27169;&#22411;&#23481;&#37327;&#36739;&#23567;&#32780;&#23548;&#33268;&#27169;&#22411;&#36136;&#37327;&#19979;&#38477;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#24615;&#26694;&#26550;&#65292;&#21363;&#28176;&#36827;&#23376;&#32593;&#32476;&#35757;&#32451;&#65292;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#23436;&#25972;&#30340;&#27169;&#22411;&#65292;&#20294;&#27599;&#20010;&#27493;&#39588;&#21482;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#36825;&#20010;&#26694;&#26550;&#30340;&#19968;&#20010;&#31616;&#21333;&#23454;&#20363;&#65292;&#21363;&#38543;&#26426;&#36335;&#24452;&#35757;&#32451;&#65288;RaPTr&#65289;&#65292;&#23427;&#22312;&#27599;&#20010;&#27493;&#39588;&#20013;&#21482;&#35757;&#32451;&#19968;&#26465;&#23376;&#36335;&#24452;&#65292;&#36880;&#28176;&#22686;&#21152;&#36335;&#24452;&#38271;&#24230;&#12290;RaPTr&#22312;BERT&#21644;UL2&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25439;&#22833;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#21482;&#38656;&#35201;2
&lt;/p&gt;
&lt;p&gt;
Recent developments in large language models have sparked interest in efficient pretraining methods. A recent effective paradigm is to perform stage-wise training, where the size of the model is gradually increased over the course of training (e.g. gradual stacking (Reddi et al., 2023)). While the resource and wall-time savings are appealing, it has limitations, particularly the inability to evaluate the full model during earlier stages, and degradation in model quality due to smaller model capacity in the initial stages. In this work, we propose an alternative framework, progressive subnetwork training, that maintains the full model throughout training, but only trains subnetworks within the model in each step. We focus on a simple instantiation of this framework, Random Path Training (RaPTr) that only trains a sub-path of layers in each step, progressively increasing the path lengths in stages. RaPTr achieves better pre-training loss for BERT and UL2 language models while requiring 2
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#32858;&#21512;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#30340;&#39118;&#38505;&#25935;&#24863;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#20351;&#29992;&#20102;&#32047;&#31215;&#21069;&#26223;&#29702;&#35770;&#20316;&#20026;&#39118;&#38505;&#24230;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#23884;&#22871;CPT-AC&#31639;&#27861;&#12290;&#36825;&#39033;&#24037;&#20316;&#23545;&#20110;&#29702;&#35299;&#20154;&#31867;&#30340;&#25439;&#22833;&#35268;&#36991;&#21644;&#23545;&#27010;&#29575;&#30340;&#39640;&#20272;/&#20302;&#20272;&#20542;&#21521;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.05906</link><description>&lt;p&gt;
&#32593;&#32476;&#32858;&#21512;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#30340;&#39118;&#38505;&#25935;&#24863;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Risk-Sensitive Multi-Agent Reinforcement Learning in Network Aggregative Markov Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#32858;&#21512;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#30340;&#39118;&#38505;&#25935;&#24863;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#20351;&#29992;&#20102;&#32047;&#31215;&#21069;&#26223;&#29702;&#35770;&#20316;&#20026;&#39118;&#38505;&#24230;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#23884;&#22871;CPT-AC&#31639;&#27861;&#12290;&#36825;&#39033;&#24037;&#20316;&#23545;&#20110;&#29702;&#35299;&#20154;&#31867;&#30340;&#25439;&#22833;&#35268;&#36991;&#21644;&#23545;&#27010;&#29575;&#30340;&#39640;&#20272;/&#20302;&#20272;&#20542;&#21521;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20551;&#35774;&#26234;&#33021;&#20307;&#23545;&#39118;&#38505;&#20013;&#24615;&#24182;&#20855;&#26377;&#23436;&#20840;&#23458;&#35266;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#26234;&#33021;&#20307;&#38656;&#35201;&#32771;&#34385;&#25110;&#24314;&#27169;&#20154;&#31867;&#32463;&#27982;&#25110;&#31038;&#20250;&#20559;&#22909;&#30340;&#24773;&#26223;&#20013;&#65292;&#24517;&#39035;&#23558;&#39118;&#38505;&#27010;&#24565;&#32435;&#20837;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#38382;&#39064;&#20013;&#12290;&#22312;&#20854;&#20182;&#20154;&#31867;&#25110;&#38750;&#20154;&#31867;&#26234;&#33021;&#20307;&#21442;&#19982;&#65292;&#21487;&#33021;&#20855;&#26377;&#20854;&#33258;&#24049;&#30340;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#30340;MARL&#20013;&#65292;&#36825;&#23558;&#26356;&#21152;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#26377;&#32047;&#31215;&#21069;&#26223;&#29702;&#35770;&#65288;CPT&#65289;&#30340;&#39118;&#38505;&#25935;&#24863;&#21644;&#38750;&#21512;&#20316;MARL&#65292;CPT&#26159;&#19968;&#31181;&#38750;&#20984;&#39118;&#38505;&#24230;&#37327;&#65292;&#24182;&#19988;&#26159;&#39118;&#38505;&#21327;&#21516;&#24230;&#37327;&#30340;&#25193;&#23637;&#12290;CPT&#33021;&#22815;&#35299;&#37322;&#20154;&#31867;&#30340;&#25439;&#22833;&#35268;&#36991;&#21644;&#20182;&#20204;&#23545;&#23567;&#27010;&#29575;/&#22823;&#27010;&#29575;&#30340;&#39640;&#20272;/&#20302;&#20272;&#20542;&#21521;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;CPT&#39118;&#38505;&#30340;&#20998;&#24067;&#24335;&#22522;&#20110;&#37319;&#26679;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;AC&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#32593;&#32476;&#32858;&#21512;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65288;NAMGs&#65289;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#20998;&#24067;&#24335;&#23884;&#22871;CPT-AC&#12290;&#22312;&#19968;&#31995;&#21015;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#25910;&#25947;&#21040;&#19968;&#31181;&#20027;&#35266;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical multi-agent reinforcement learning (MARL) assumes risk neutrality and complete objectivity for agents. However, in settings where agents need to consider or model human economic or social preferences, a notion of risk must be incorporated into the RL optimization problem. This will be of greater importance in MARL where other human or non-human agents are involved, possibly with their own risk-sensitive policies. In this work, we consider risk-sensitive and non-cooperative MARL with cumulative prospect theory (CPT), a non-convex risk measure and a generalization of coherent measures of risk. CPT is capable of explaining loss aversion in humans and their tendency to overestimate/underestimate small/large probabilities. We propose a distributed sampling-based actor-critic (AC) algorithm with CPT risk for network aggregative Markov games (NAMGs), which we call Distributed Nested CPT-AC. Under a set of assumptions, we prove the convergence of the algorithm to a subjective notion 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#23398;&#29983;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#29702;&#35299;&#25991;&#26412;-&#23646;&#24615;&#22270;&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.05894</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#36935;&#35265;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Meets Graph Neural Network in Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#23398;&#29983;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#29702;&#35299;&#25991;&#26412;-&#23646;&#24615;&#22270;&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#26399;&#23398;&#26415;&#30028;&#23545;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#25991;&#26412;-&#23646;&#24615;&#22270;&#65288;TAG&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#21644;&#28508;&#21147;&#26377;&#25152;&#25259;&#38706;&#65292;&#20294;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#21463;&#21040;&#20102;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#39640;&#65292;&#25512;&#29702;&#36807;&#31243;&#20013;&#24310;&#36831;&#38271;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#34429;&#28982;&#36731;&#37327;&#19988;&#25797;&#38271;&#23398;&#20064;&#22270;&#30340;&#32467;&#26500;&#29305;&#24449;&#65292;&#20294;&#23545;&#20110;&#30495;&#23454;&#24212;&#29992;&#20013;TAG&#22797;&#26434;&#35821;&#20041;&#30340;&#25226;&#25569;&#26377;&#25152;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#32858;&#28966;&#20110;TAG&#20013;&#33410;&#28857;&#20998;&#31867;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#31216;&#20026;&#35821;&#35328;&#22270;&#30693;&#35782;&#33976;&#39311;&#65288;LinguGKD&#65289;&#65292;&#20351;&#29992;LLMs&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#65292;GNNs&#20316;&#20026;&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#12290;&#20854;&#20013;&#21253;&#25324;&#23545;LLM&#36827;&#34892;TAG&#23450;&#21521;&#25351;&#23548;&#35843;&#25972;&#20197;&#24212;&#23545;&#35774;&#35745;&#30340;&#33410;&#28857;&#20998;&#31867;&#25552;&#31034;&#65292;&#28982;&#21518;&#23545;&#23618;&#27425;&#21270;&#23398;&#20064;&#30340;&#33410;&#28857;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent community revelations about the advancements and potential of Large Language Models (LLMs) in understanding Text-Attributed Graphs (TAG), the deployment of LLMs for production is hindered by their high computational and storage requirements, as well as long latencies during inference. Simultaneously, although traditional Graph Neural Networks (GNNs) are light weight and adept at learning structural features of graphs, their ability to grasp the complex semantics in TAGs is somewhat constrained for real applications. To address these limitations, we concentrate on the downstream task of node classification in TAG and propose a novel graph knowledge distillation framework, termed Linguistic Graph Knowledge Distillation (LinguGKD), using LLMs as teacher models and GNNs as student models for knowledge distillation. It involves TAG-oriented instruction tuning of LLM on designed node classification prompts, followed by aligning the hierarchically learned node features of the t
&lt;/p&gt;</description></item><item><title>EUGENE&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26080;&#30417;&#30563;&#22270;&#32534;&#36753;&#36317;&#31163;&#36817;&#20284;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#32534;&#36753;&#36335;&#24452;&#26469;&#36817;&#20284;&#35745;&#31639;&#22270;&#32534;&#36753;&#36317;&#31163;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;ground-truth&#29983;&#25104;&#21644;&#25968;&#25454;&#29305;&#23450;&#35757;&#32451;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.05885</link><description>&lt;p&gt;
EUGENE: &#21487;&#35299;&#37322;&#30340;&#26080;&#30417;&#30563;&#22270;&#32534;&#36753;&#36317;&#31163;&#36817;&#20284;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EUGENE: Explainable Unsupervised Approximation of Graph Edit Distance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05885
&lt;/p&gt;
&lt;p&gt;
EUGENE&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26080;&#30417;&#30563;&#22270;&#32534;&#36753;&#36317;&#31163;&#36817;&#20284;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#32534;&#36753;&#36335;&#24452;&#26469;&#36817;&#20284;&#35745;&#31639;&#22270;&#32534;&#36753;&#36317;&#31163;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;ground-truth&#29983;&#25104;&#21644;&#25968;&#25454;&#29305;&#23450;&#35757;&#32451;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#23398;&#12289;&#21270;&#23398;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#31561;&#39046;&#22495;&#65292;&#38656;&#35201;&#35782;&#21035;&#19982;&#26597;&#35810;&#22270;&#32467;&#26500;&#36317;&#31163;&#36739;&#23567;&#30340;&#22270;&#24418;&#12290;&#22312;&#22810;&#31181;&#27979;&#37327;&#22270;&#38388;&#36317;&#31163;&#30340;&#26041;&#27861;&#20013;&#65292;&#22270;&#32534;&#36753;&#36317;&#31163;&#65288;GED&#65289;&#22240;&#20854;&#21487;&#29702;&#35299;&#24615;&#32780;&#34987;&#35748;&#20026;&#26159;&#39318;&#36873;&#65292;&#20294;&#20854;&#35745;&#31639;&#30340;NP&#38590;&#24230;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;GED&#36817;&#20284;&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#31070;&#32463;&#26041;&#27861;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#65288;i&#65289;&#32570;&#23569;&#19982;&#36817;&#20284;&#30340;GED&#23545;&#24212;&#30340;&#35299;&#37322;&#24615;&#32534;&#36753;&#36335;&#24452;&#65307;&#65288;ii&#65289;&#38656;&#35201;&#36890;&#36807;NP&#38590;&#38382;&#39064;&#29983;&#25104;ground-truth GED&#36827;&#34892;&#35757;&#32451;&#65307;&#65288;iii&#65289;&#38656;&#35201;&#22312;&#27599;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#29420;&#31435;&#35757;&#32451;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20195;&#25968;&#26080;&#30417;&#30563;&#26041;&#27861;EUGENE&#65292;&#23427;&#36817;&#20284;&#35745;&#31639;GED&#24182;&#29983;&#25104;&#19982;&#36817;&#20284;&#25104;&#26412;&#23545;&#24212;&#30340;&#32534;&#36753;&#36335;&#24452;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#29983;&#25104;ground-truth&#21644;&#25968;&#25454;&#29305;&#23450;&#35757;&#32451;&#30340;&#38656;&#27714;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;EUGENE&#30340;&#19978;&#36848;&#20248;&#28857;&#24182;&#19981;&#20197;&#25928;&#21147;&#20026;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
The need to identify graphs having small structural distance from a query arises in biology, chemistry, recommender systems, and social network analysis. Among several methods to measure inter graph distance, Graph Edit Distance (GED) is preferred for its comprehensibility, yet hindered by the NP-hardness of its computation. State-of-the-art GED approximations predominantly employ neural methods, which, however, (i) lack an explanatory edit path corresponding to the approximated GED; (ii) require the NP-hard generation of ground-truth GEDs for training; and (iii) necessitate separate training on each dataset. In this paper, we propose an efficient algebraic unsuper vised method, EUGENE, that approximates GED and yields edit paths corresponding to the approx imated cost, while eliminating the need for ground truth generation and data-specific training. Extensive experimental evaluation demonstrates that the aforementioned benefits of EUGENE do not come at the cost of efficacy. Specifica
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32467;&#26500;&#21270;&#36172;&#21338;&#26426;&#20013;&#30340;&#36125;&#21494;&#26031;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#20449;&#24687;&#30340;&#22266;&#23450;&#20998;&#37197;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#35777;&#26126;&#26041;&#27861;&#65292;&#20197;&#24471;&#21040;&#26356;&#32039;&#23494;&#30340;&#22810;&#33218;BAI&#30028;&#38480;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#19968;&#33268;&#19988;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#21152;&#28145;&#20102;&#25105;&#20204;&#23545;&#20110;&#35813;&#38382;&#39064;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.05878</link><description>&lt;p&gt;
&#22522;&#20110;&#20808;&#39564;&#20381;&#36182;&#20998;&#37197;&#30340;&#32467;&#26500;&#21270;&#36172;&#21338;&#26426;&#20013;&#36125;&#21494;&#26031;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Prior-Dependent Allocations for Bayesian Fixed-Budget Best-Arm Identification in Structured Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32467;&#26500;&#21270;&#36172;&#21338;&#26426;&#20013;&#30340;&#36125;&#21494;&#26031;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#20449;&#24687;&#30340;&#22266;&#23450;&#20998;&#37197;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#35777;&#26126;&#26041;&#27861;&#65292;&#20197;&#24471;&#21040;&#26356;&#32039;&#23494;&#30340;&#22810;&#33218;BAI&#30028;&#38480;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#19968;&#33268;&#19988;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#21152;&#28145;&#20102;&#25105;&#20204;&#23545;&#20110;&#35813;&#38382;&#39064;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32467;&#26500;&#21270;&#36172;&#21338;&#26426;&#20013;&#30340;&#36125;&#21494;&#26031;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#65288;BAI&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;&#20808;&#39564;&#20449;&#24687;&#21644;&#29615;&#22659;&#32467;&#26500;&#20351;&#29992;&#22266;&#23450;&#20998;&#37197;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#23427;&#22312;&#24615;&#33021;&#19978;&#30340;&#29702;&#35770;&#30028;&#38480;&#65292;&#21253;&#25324;&#32447;&#24615;&#21644;&#20998;&#23618;BAI&#30340;&#39318;&#20010;&#20808;&#39564;&#20381;&#36182;&#19978;&#30028;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24341;&#20837;&#20102;&#26032;&#30340;&#35777;&#26126;&#26041;&#27861;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#23427;&#33021;&#24471;&#21040;&#26356;&#32039;&#23494;&#30340;&#22810;&#33218;BAI&#30028;&#38480;&#12290;&#25105;&#20204;&#24191;&#27867;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#22266;&#23450;&#39044;&#31639;BAI&#26041;&#27861;&#65292;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#23637;&#31034;&#20102;&#20854;&#19968;&#33268;&#19988;&#31283;&#20581;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25913;&#36827;&#20102;&#23545;&#20110;&#32467;&#26500;&#21270;&#36172;&#21338;&#26426;&#20013;&#36125;&#21494;&#26031;&#22266;&#23450;&#39044;&#31639;BAI&#30340;&#29702;&#35299;&#65292;&#24182;&#31361;&#20986;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of Bayesian fixed-budget best-arm identification (BAI) in structured bandits. We propose an algorithm that uses fixed allocations based on the prior information and the structure of the environment. We provide theoretical bounds on its performance across diverse models, including the first prior-dependent upper bounds for linear and hierarchical BAI. Our key contribution is introducing new proof methods that result in tighter bounds for multi-armed BAI compared to existing methods. We extensively compare our approach to other fixed-budget BAI methods, demonstrating its consistent and robust performance in various settings. Our work improves our understanding of Bayesian fixed-budget BAI in structured bandits and highlights the effectiveness of our approach in practical scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32852;&#37030;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26080;&#27169;&#22411;Q-learning&#31639;&#27861;FedLCB-Q&#12290;&#36890;&#36807;&#21512;&#20316;&#21033;&#29992;&#22810;&#20010;&#20195;&#29702;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#29305;&#23450;&#30340;&#23398;&#20064;&#36895;&#29575;&#35843;&#24230;&#21644;&#32858;&#21512;&#26041;&#27861;&#65292;FedLCB-Q&#23454;&#29616;&#20102;&#32447;&#24615;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2402.05876</link><description>&lt;p&gt;
&#32852;&#37030;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65306;&#21512;&#20316;&#21333;&#19968;&#31574;&#30053;&#21363;&#21487;&#35206;&#30422;
&lt;/p&gt;
&lt;p&gt;
Federated Offline Reinforcement Learning: Collaborative Single-Policy Coverage Suffices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32852;&#37030;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26080;&#27169;&#22411;Q-learning&#31639;&#27861;FedLCB-Q&#12290;&#36890;&#36807;&#21512;&#20316;&#21033;&#29992;&#22810;&#20010;&#20195;&#29702;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#29305;&#23450;&#30340;&#23398;&#20064;&#36895;&#29575;&#35843;&#24230;&#21644;&#32858;&#21512;&#26041;&#27861;&#65292;FedLCB-Q&#23454;&#29616;&#20102;&#32447;&#24615;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#26469;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#65292;&#22312;&#26080;&#27861;&#22312;&#32447;&#25910;&#38598;&#25968;&#25454;&#25110;&#25104;&#26412;&#39640;&#26114;&#30340;&#20851;&#38190;&#24212;&#29992;&#20013;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22909;&#22788;&#65292;&#24182;&#26088;&#22312;&#21512;&#20316;&#21033;&#29992;&#22810;&#20010;&#20195;&#29702;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#12290;&#38024;&#23545;&#26377;&#38480;&#26102;&#27573;&#30340;&#34920;&#26684;&#21270;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;FedLCB-Q&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#27969;&#34892;&#30340;&#26080;&#27169;&#22411;Q-learning&#31639;&#27861;&#30340;&#21464;&#31181;&#12290;FedLCB-Q&#20351;&#29992;&#26032;&#39062;&#30340;&#23398;&#20064;&#36895;&#29575;&#35843;&#24230;&#22312;&#20195;&#29702;&#22788;&#26356;&#26032;&#26412;&#22320;Q&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;&#37325;&#35201;&#24615;&#24179;&#22343;&#21644;&#31934;&#24515;&#35774;&#35745;&#30340;&#24754;&#35266;&#24809;&#32602;&#39033;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#19978;&#32858;&#21512;&#23427;&#20204;&#12290;&#25105;&#20204;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#36866;&#24403;&#36873;&#25321;&#30340;&#21442;&#25968;&#21644;&#21516;&#27493;&#26102;&#38388;&#34920;&#19979;&#65292;FedLCB-Q&#22312;&#20195;&#29702;&#25968;&#37327;&#19978;&#23454;&#29616;&#20102;&#32447;&#24615;&#21152;&#36895;&#65292;&#32780;&#19981;&#38656;&#35201;&#20010;&#21035;&#20195;&#29702;&#25317;&#26377;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL), which seeks to learn an optimal policy using offline data, has garnered significant interest due to its potential in critical applications where online data collection is infeasible or expensive. This work explores the benefit of federated learning for offline RL, aiming at collaboratively leveraging offline datasets at multiple agents. Focusing on finite-horizon episodic tabular Markov decision processes (MDPs), we design FedLCB-Q, a variant of the popular model-free Q-learning algorithm tailored for federated offline RL. FedLCB-Q updates local Q-functions at agents with novel learning rate schedules and aggregates them at a central server using importance averaging and a carefully designed pessimistic penalty term. Our sample complexity analysis reveals that, with appropriately chosen parameters and synchronization schedules, FedLCB-Q achieves linear speedup in terms of the number of agents without requiring high-quality datasets at individual age
&lt;/p&gt;</description></item><item><title>PromptCrypt&#26159;&#19968;&#31181;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#30340;&#26426;&#21046;&#65292;&#20445;&#25252;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#38450;&#27490;&#25968;&#25454;&#27844;&#38706;&#21644;&#35299;&#23494;&#12290;</title><link>https://arxiv.org/abs/2402.05868</link><description>&lt;p&gt;
PromptCrypt: &#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#36890;&#20449;&#30340;&#25552;&#31034;&#21152;&#23494;
&lt;/p&gt;
&lt;p&gt;
PromptCrypt: Prompt Encryption for Secure Communication with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05868
&lt;/p&gt;
&lt;p&gt;
PromptCrypt&#26159;&#19968;&#31181;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#30340;&#26426;&#21046;&#65292;&#20445;&#25252;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#38450;&#27490;&#25968;&#25454;&#27844;&#38706;&#21644;&#35299;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20113;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#22312;&#26085;&#24120;&#25805;&#20316;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#25104;&#20026;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#21487;&#35775;&#38382;&#24615;&#21644;&#21151;&#33021;&#24615;&#26041;&#38754;&#24102;&#26469;&#20102;&#37325;&#22823;&#22909;&#22788;&#65292;&#20294;&#23427;&#20204;&#20063;&#24341;&#20837;&#20102;&#37325;&#35201;&#30340;&#38544;&#31169;&#38382;&#39064;&#65306;&#22312;&#20113;&#22522;&#30784;&#26550;&#26500;&#20013;&#20256;&#36755;&#21644;&#23384;&#20648;&#29992;&#25143;&#25968;&#25454;&#20250;&#20135;&#29983;&#37325;&#22823;&#30340;&#25968;&#25454;&#27844;&#38706;&#21644;&#26410;&#32463;&#25480;&#26435;&#35775;&#38382;&#25935;&#24863;&#20449;&#24687;&#30340;&#39118;&#38505;&#65307;&#21363;&#20351;&#25968;&#25454;&#30340;&#20256;&#36755;&#21644;&#23384;&#20648;&#34987;&#21152;&#23494;&#65292;LLM&#26381;&#21153;&#25552;&#20379;&#21830;&#20173;&#28982;&#30693;&#36947;&#25968;&#25454;&#30340;&#30495;&#23454;&#20869;&#23481;&#65292;&#20174;&#32780;&#38459;&#27490;&#20010;&#20154;&#25110;&#23454;&#20307;&#25918;&#24515;&#20351;&#29992;&#27492;&#31867;LLM&#26381;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26426;&#21046;PromptCrypt&#26469;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;&#23427;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#65292;&#28982;&#21518;&#23558;&#20854;&#21457;&#36865;&#21040;LLM&#65292;&#26377;&#25928;&#22320;&#20351;&#20854;&#23545;&#20154;&#31867;&#25110;LLM&#30340;&#26816;&#26597;&#26080;&#27861;&#29702;&#35299;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#25552;&#31034;&#30340;&#24847;&#22270;&#65292;&#20174;&#32780;&#30830;&#20445;&#29992;&#25143;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud-based large language models (LLMs) such as ChatGPT have increasingly become integral to daily operations, serving as vital tools across various applications. While these models offer substantial benefits in terms of accessibility and functionality, they also introduce significant privacy concerns: the transmission and storage of user data in cloud infrastructures pose substantial risks of data breaches and unauthorized access to sensitive information; even if the transmission and storage of data is encrypted, the LLM service provider itself still knows the real contents of the data, preventing individuals or entities from confidently using such LLM services. To address these concerns, this paper proposes a simple yet effective mechanism PromptCrypt to protect user privacy. It uses Emoji to encrypt the user inputs before sending them to LLM, effectively rendering them indecipherable to human or LLM's examination while retaining the original intent of the prompt, thus ensuring the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Permute-and-Flip&#65288;PF&#65289;&#35299;&#30721;&#22120;&#65292;&#20854;&#20855;&#26377;&#26368;&#20339;&#30340;&#40065;&#26834;&#24615;&#21644;&#36136;&#37327;-&#40065;&#26834;&#24615;&#30340; tradeoff&#65292;&#19988;&#27604;&#37319;&#26679;&#26041;&#27861;&#26356;&#22909;&#12290;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#38024;&#23545;PF&#35299;&#30721;&#22120;&#30340;&#27700;&#21360;&#26041;&#26696;&#65292;&#33021;&#22815;&#20445;&#25345;&#26679;&#26412;&#30340;&#20998;&#24067;&#19981;&#21464;&#65292;&#24182;&#23454;&#29616;&#20219;&#24847;&#20302;&#30340;&#20551;&#38451;&#24615;&#29575;&#21644;&#39640;&#30340;&#21484;&#22238;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;PF&#35299;&#30721;&#22120;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#26420;&#32032;&#37319;&#26679;&#65292;&#20026;LLM&#35299;&#30721;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05864</link><description>&lt;p&gt;
Permute-and-Flip&#65306;&#19968;&#31181;&#20855;&#26377;&#26368;&#20339;&#40065;&#26834;&#24615;&#21644;&#21487;&#21152;&#27700;&#21360;&#30340;LLMs&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Permute-and-Flip: An optimally robust and watermarkable decoder for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05864
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Permute-and-Flip&#65288;PF&#65289;&#35299;&#30721;&#22120;&#65292;&#20854;&#20855;&#26377;&#26368;&#20339;&#30340;&#40065;&#26834;&#24615;&#21644;&#36136;&#37327;-&#40065;&#26834;&#24615;&#30340; tradeoff&#65292;&#19988;&#27604;&#37319;&#26679;&#26041;&#27861;&#26356;&#22909;&#12290;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#38024;&#23545;PF&#35299;&#30721;&#22120;&#30340;&#27700;&#21360;&#26041;&#26696;&#65292;&#33021;&#22815;&#20445;&#25345;&#26679;&#26412;&#30340;&#20998;&#24067;&#19981;&#21464;&#65292;&#24182;&#23454;&#29616;&#20219;&#24847;&#20302;&#30340;&#20551;&#38451;&#24615;&#29575;&#21644;&#39640;&#30340;&#21484;&#22238;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;PF&#35299;&#30721;&#22120;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#26420;&#32032;&#37319;&#26679;&#65292;&#20026;LLM&#35299;&#30721;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Permute-and-Flip&#65288;PF&#65289;&#35299;&#30721;&#22120;&#30340;&#26032;&#35299;&#30721;&#26041;&#27861;&#12290;&#23427;&#20855;&#26377;&#19982;&#26631;&#20934;&#37319;&#26679;&#35299;&#30721;&#22120;&#30456;&#20284;&#30340;&#40065;&#26834;&#24615;&#29305;&#24615;&#65292;&#20294;&#22312;&#36136;&#37327;&#21644;&#40065;&#26834;&#24615;&#30340; tradeoff &#19978;&#35777;&#26126;&#27604;&#37319;&#26679;&#26041;&#27861;&#26356;&#22909;&#65292;&#19988;&#27704;&#36828;&#19981;&#20250;&#24046;&#20110;&#20219;&#20309;&#20854;&#20182;&#35299;&#30721;&#22120;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;Aaronson&#30340;Gumbel&#27700;&#21360;&#30340;&#21152;&#23494;&#27700;&#21360;&#26041;&#26696;&#65292;&#20294;&#26159;&#38024;&#23545;PF&#35299;&#30721;&#22120;&#32780;&#33258;&#28982;&#37327;&#36523;&#23450;&#21046;&#12290;&#35813;&#27700;&#21360;&#26041;&#26696;&#19981;&#25913;&#21464;&#26679;&#26412;&#30340;&#20998;&#24067;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#24847;&#20302;&#30340;&#20551;&#38451;&#24615;&#29575;&#21644;&#39640;&#30340;&#21484;&#22238;&#29575;&#65292;&#21482;&#35201;&#29983;&#25104;&#30340;&#25991;&#26412;&#20855;&#26377;&#39640;&#29109;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;PF&#35299;&#30721;&#22120;&#65288;&#21450;&#20854;&#24102;&#26377;&#27700;&#21360;&#30340;&#23545;&#24212;&#29289;&#65289;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#26420;&#32032;&#37319;&#26679;&#65288;&#21450;&#20854;&#24102;&#26377;Gumbel&#27700;&#21360;&#30340;&#23545;&#24212;&#29289;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#40065;&#26834;&#24615;&#65288;&#21644;&#21487;&#26816;&#27979;&#24615;&#65289;&#65292;&#22240;&#27492;&#20026;LLM&#35299;&#30721;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26032;&#26041;&#27861;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/XuandongZhao/pf-decoding&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a new decoding method called Permute-and-Flip (PF) decoder. It enjoys robustness properties similar to the standard sampling decoder, but is provably up to 2x better in its quality-robustness tradeoff than sampling and never worse than any other decoder. We also design a cryptographic watermarking scheme analogous to Aaronson's Gumbel watermark, but naturally tailored for PF decoder. The watermarking scheme does not change the distribution to sample, while allowing arbitrarily low false positive rate and high recall whenever the generated text has high entropy. Our experiments show that the PF decoder (and its watermarked counterpart) significantly outperform(s) naive sampling (and it's Gumbel watermarked counterpart) in terms of perplexity, while retaining the same robustness (and detectability), hence making it a promising new approach for LLM decoding. The code is available at https://github.com/XuandongZhao/pf-decoding
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#32534;&#30721;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#26174;&#24335;&#22320;&#34920;&#31034;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#24182;&#22312;&#22270;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.05862</link><description>&lt;p&gt;
&#35753;&#20320;&#30340;&#22270;&#26469;&#35828;&#35805;&#65306;&#20026;LLMs&#32534;&#30721;&#32467;&#26500;&#21270;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Let Your Graph Do the Talking: Encoding Structured Data for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#32534;&#30721;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#26174;&#24335;&#22320;&#34920;&#31034;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#24182;&#22312;&#22270;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22914;&#20309;&#26368;&#26377;&#25928;&#22320;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#32534;&#30721;&#25104;&#24207;&#21015;&#24418;&#24335;&#65292;&#20197;&#20379;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20351;&#29992;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26126;&#30830;&#34920;&#31034;LLMs&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;GraphToken&#65292;&#23398;&#20064;&#20102;&#19968;&#31181;&#32534;&#30721;&#20989;&#25968;&#65292;&#20197;&#26174;&#24335;&#32467;&#26500;&#21270;&#20449;&#24687;&#25193;&#23637;&#25552;&#31034;&#35821;&#12290;&#19982;&#20854;&#20182;&#19987;&#27880;&#20110;&#26377;&#38480;&#39046;&#22495;&#65288;&#20363;&#22914;&#30693;&#35782;&#22270;&#34920;&#31034;&#65289;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#39318;&#27425;&#38024;&#23545;&#19968;&#33324;&#32467;&#26500;&#21270;&#25968;&#25454;&#32534;&#30721;&#36827;&#34892;&#30740;&#31350;&#65292;&#29992;&#20110;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26126;&#30830;&#34920;&#31034;&#22270;&#32467;&#26500;&#21487;&#20197;&#26174;&#33879;&#25913;&#36827;&#22270;&#25512;&#29702;&#20219;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;GraphQA&#22522;&#20934;&#27979;&#35797;&#20013;&#30475;&#21040;&#20102;&#25972;&#20307;&#30340;&#25913;&#36827; - &#22312;&#33410;&#28857;&#12289;&#36793;&#21644;&#22270;&#32423;&#20219;&#21153;&#19978;&#39640;&#36798;73%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we best encode structured data into sequential form for use in large language models (LLMs)? In this work, we introduce a parameter-efficient method to explicitly represent structured data for LLMs. Our method, GraphToken, learns an encoding function to extend prompts with explicit structured information. Unlike other work which focuses on limited domains (e.g. knowledge graph representation), our work is the first effort focused on the general encoding of structured data to be used for various reasoning tasks. We show that explicitly representing the graph structure allows significant improvements to graph reasoning tasks. Specifically, we see across the board improvements - up to 73% points - on node, edge and, graph-level tasks from the GraphQA benchmark.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21518;&#32493;&#33258;&#36866;&#24212;&#36880;&#26631;&#35760;&#38376;&#25511;&#26426;&#21046;&#65288;PHATGOOSE&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#36335;&#30001;&#20110;&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#29983;&#25104;&#30340;&#19987;&#23478;&#27169;&#22359;&#20043;&#38388;&#65292;&#20174;&#32780;&#25552;&#39640;&#22312;&#26410;&#35265;&#20219;&#21153;&#19978;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05859</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#19987;&#23478;&#36335;&#30001;&#26469;&#23454;&#29616;&#38646;&#26679;&#26412;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learning to Route Among Specialized Experts for Zero-Shot Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05859
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21518;&#32493;&#33258;&#36866;&#24212;&#36880;&#26631;&#35760;&#38376;&#25511;&#26426;&#21046;&#65288;PHATGOOSE&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#36335;&#30001;&#20110;&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#29983;&#25104;&#30340;&#19987;&#23478;&#27169;&#22359;&#20043;&#38388;&#65292;&#20174;&#32780;&#25552;&#39640;&#22312;&#26410;&#35265;&#20219;&#21153;&#19978;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#8220;&#19987;&#23478;&#8221;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#24212;&#29992;&#36890;&#36807;&#21442;&#25968;&#26377;&#25928;&#30340;&#24494;&#35843;&#65292;&#20351;&#20854;&#19987;&#38376;&#29992;&#20110;&#29305;&#23450;&#30340;&#20219;&#21153;&#25110;&#39046;&#22495;&#12290;&#25105;&#20204;&#22914;&#20309;&#37325;&#29992;&#22823;&#37327;&#30340;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#22312;&#26410;&#35265;&#20219;&#21153;&#19978;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#21602;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21518;&#32493;&#33258;&#36866;&#24212;&#36880;&#26631;&#35760;&#38376;&#25511;&#26426;&#21046;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#36335;&#30001;&#20110;&#36890;&#36807;&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#29983;&#25104;&#30340;&#19987;&#23478;&#27169;&#22359;&#20043;&#38388;&#12290;&#19982;&#36807;&#21435;&#23398;&#20064;&#22312;&#19987;&#19994;&#27169;&#22411;&#20043;&#38388;&#36335;&#30001;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#21518;&#32493;&#33258;&#36866;&#24212;&#36880;&#26631;&#35760;&#38376;&#25511;&#26426;&#21046;&#25506;&#35752;&#20102;&#22914;&#26524;&#36890;&#36807;&#23545;&#27599;&#20010;&#20196;&#29260;&#21644;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#23618;&#36827;&#34892;&#33258;&#36866;&#24212;&#36873;&#25321;&#19981;&#21516;&#30340;&#19987;&#23478;&#65292;&#38646;&#26679;&#26412;&#27867;&#21270;&#26159;&#21542;&#20250;&#24471;&#21040;&#25913;&#21892;&#30340;&#21487;&#33021;&#24615;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#21518;&#32493;&#30340;&#65292;&#19981;&#38656;&#35201;&#21516;&#26102;&#35775;&#38382;&#29992;&#20110;&#21019;&#24314;&#19987;&#19994;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#65292;&#32780;&#19988;&#22312;&#35757;&#32451;&#27599;&#20010;&#19987;&#23478;&#27169;&#22411;&#21518;&#21482;&#38656;&#35201;&#36866;&#37327;&#30340;&#39069;&#22806;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been a widespread proliferation of "expert" language models that are specialized to a specific task or domain through parameter-efficient fine-tuning. How can we recycle large collections of expert language models to improve zero-shot generalization to unseen tasks? In this work, we propose Post-Hoc Adaptive Tokenwise Gating Over an Ocean of Specialized Experts (PHATGOOSE), which learns to route among specialized modules that were produced through parameter-efficient fine-tuning. Unlike past methods that learn to route among specialized models, PHATGOOSE explores the possibility that zero-shot generalization will be improved if different experts can be adaptively chosen for each token and at each layer in the model. Crucially, our method is post-hoc - it does not require simultaneous access to the datasets used to create the specialized models and only requires a modest amount of additional compute after each expert model is trained. In experiments covering a range 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#26500;&#20449;&#24687;&#39537;&#21160;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#36828;&#31243;&#21516;&#28304;&#24615;&#26816;&#27979;&#65292;&#23558;&#32467;&#26500;&#20449;&#24687;&#34701;&#20837;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#34507;&#30333;&#36136;&#21151;&#33021;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05856</link><description>&lt;p&gt;
&#32467;&#26500;&#20449;&#24687;&#39537;&#21160;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Structure-Informed Protein Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05856
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#26500;&#20449;&#24687;&#39537;&#21160;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#36828;&#31243;&#21516;&#28304;&#24615;&#26816;&#27979;&#65292;&#23558;&#32467;&#26500;&#20449;&#24687;&#34701;&#20837;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#34507;&#30333;&#36136;&#21151;&#33021;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#24191;&#38420;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#23398;&#20064;&#34507;&#30333;&#36136;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#32570;&#20047;&#26126;&#30830;&#30340;&#32467;&#26500;&#30417;&#30563;&#65292;&#23613;&#31649;&#19982;&#34507;&#30333;&#36136;&#21151;&#33021;&#30456;&#20851;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36828;&#31243;&#21516;&#28304;&#24615;&#26816;&#27979;&#30340;&#38598;&#25104;&#65292;&#23558;&#32467;&#26500;&#20449;&#24687;&#34701;&#20837;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#20316;&#20026;&#36755;&#20837;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#31181;&#32467;&#26500;&#20449;&#24687;&#39537;&#21160;&#35757;&#32451;&#23545;&#19979;&#28216;&#34507;&#30333;&#36136;&#21151;&#33021;&#39044;&#27979;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23545;&#20110;EC&#32534;&#21495;&#21644;GO&#26415;&#35821;&#39044;&#27979;&#30340;&#21151;&#33021;&#27880;&#37322;&#20934;&#30830;&#24615;&#22343;&#26377;&#19968;&#33268;&#30340;&#25913;&#21892;&#12290;&#28982;&#32780;&#65292;&#22312;&#31361;&#21464;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#21017;&#26681;&#25454;&#30446;&#26631;&#23646;&#24615;&#21644;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20851;&#31995;&#32780;&#21464;&#21270;&#12290;&#36825;&#24378;&#35843;&#20102;&#22312;&#24212;&#29992;&#32467;&#26500;&#24863;&#30693;&#35757;&#32451;&#20110;&#34507;&#30333;&#36136;&#21151;&#33021;&#39044;&#27979;&#20219;&#21153;&#26102;&#32771;&#34385;&#36825;&#31181;&#20851;&#31995;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein language models are a powerful tool for learning protein representations through pre-training on vast protein sequence datasets. However, traditional protein language models lack explicit structural supervision, despite its relevance to protein function. To address this issue, we introduce the integration of remote homology detection to distill structural information into protein language models without requiring explicit protein structures as input. We evaluate the impact of this structure-informed training on downstream protein function prediction tasks. Experimental results reveal consistent improvements in function annotation accuracy for EC number and GO term prediction. Performance on mutant datasets, however, varies based on the relationship between targeted properties and protein structures. This underscores the importance of considering this relationship when applying structure-aware training to protein function prediction tasks. Code and model weights are available at
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21457;&#29616;&#65292;&#22312;&#26410;&#30693;&#31181;&#32676;&#20013;&#23646;&#20110;&#26410;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20986;&#29616;&#30340;&#31867;&#30340;&#25968;&#25454;&#28857;&#30340;&#27604;&#20363;&#20960;&#20046;&#23436;&#20840;&#21462;&#20915;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#20986;&#29616;&#30456;&#21516;&#27425;&#25968;&#30340;&#31867;&#30340;&#25968;&#37327;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36951;&#20256;&#31639;&#27861;&#65292;&#33021;&#22815;&#26681;&#25454;&#26679;&#26412;&#25214;&#21040;&#19968;&#20010;&#20855;&#26377;&#26368;&#23567;&#22343;&#26041;&#35823;&#24046;&#30340;&#20272;&#35745;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.05835</link><description>&lt;p&gt;
&#19981;&#21487;&#35265;&#25968;&#25454;&#21462;&#20915;&#20110;&#24050;&#30693;&#20449;&#24687;&#30340;&#22810;&#23569;
&lt;/p&gt;
&lt;p&gt;
How Much is Unseen Depends Chiefly on Information About the Seen
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05835
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21457;&#29616;&#65292;&#22312;&#26410;&#30693;&#31181;&#32676;&#20013;&#23646;&#20110;&#26410;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20986;&#29616;&#30340;&#31867;&#30340;&#25968;&#25454;&#28857;&#30340;&#27604;&#20363;&#20960;&#20046;&#23436;&#20840;&#21462;&#20915;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#20986;&#29616;&#30456;&#21516;&#27425;&#25968;&#30340;&#31867;&#30340;&#25968;&#37327;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36951;&#20256;&#31639;&#27861;&#65292;&#33021;&#22815;&#26681;&#25454;&#26679;&#26412;&#25214;&#21040;&#19968;&#20010;&#20855;&#26377;&#26368;&#23567;&#22343;&#26041;&#35823;&#24046;&#30340;&#20272;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20045;&#19968;&#30475;&#21487;&#33021;&#26377;&#20123;&#36829;&#21453;&#30452;&#35273;&#65306;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#39044;&#26399;&#20013;&#65292;&#26410;&#30693;&#31181;&#32676;&#20013;&#23646;&#20110;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#27809;&#26377;&#20986;&#29616;&#30340;&#31867;&#30340;&#25968;&#25454;&#28857;&#30340;&#27604;&#20363;&#20960;&#20046;&#23436;&#20840;&#30001;&#35757;&#32451;&#25968;&#25454;&#20013;&#20986;&#29616;&#30456;&#21516;&#27425;&#25968;&#30340;&#31867;&#30340;&#25968;&#37327;$f_k$&#30830;&#23450;&#12290;&#34429;&#28982;&#22312;&#29702;&#35770;&#19978;&#25105;&#20204;&#35777;&#26126;&#20102;&#30001;&#35813;&#20272;&#35745;&#37327;&#24341;&#36215;&#30340;&#20559;&#24046;&#22312;&#26679;&#26412;&#22823;&#23567;&#25351;&#25968;&#32423;&#34928;&#20943;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#39640;&#26041;&#24046;&#38459;&#27490;&#25105;&#20204;&#30452;&#25509;&#20351;&#29992;&#23427;&#20316;&#20026;&#26679;&#26412;&#35206;&#30422;&#20272;&#35745;&#37327;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#23545;$f_k$&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#20102;&#31934;&#30830;&#30340;&#25551;&#36848;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#22810;&#20010;&#19981;&#21516;&#26399;&#26395;&#20540;&#34920;&#31034;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#21487;&#20197;&#30830;&#23450;&#22320;&#23454;&#20363;&#21270;&#20026;&#20272;&#35745;&#37327;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36716;&#21521;&#20248;&#21270;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#36951;&#20256;&#31639;&#27861;&#65292;&#20165;&#26681;&#25454;&#26679;&#26412;&#25628;&#32034;&#24179;&#22343;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#26368;&#23567;&#30340;&#20272;&#35745;&#37327;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#36951;&#20256;&#31639;&#27861;&#21457;&#29616;&#20102;&#20855;&#26377;&#26126;&#26174;&#36739;&#23567;&#26041;&#24046;&#30340;&#20272;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
It might seem counter-intuitive at first: We find that, in expectation, the proportion of data points in an unknown population-that belong to classes that do not appear in the training data-is almost entirely determined by the number $f_k$ of classes that do appear in the training data the same number of times. While in theory we show that the difference of the induced estimator decays exponentially in the size of the sample, in practice the high variance prevents us from using it directly for an estimator of the sample coverage. However, our precise characterization of the dependency between $f_k$'s induces a large search space of different representations of the expected value, which can be deterministically instantiated as estimators. Hence, we turn to optimization and develop a genetic algorithm that, given only the sample, searches for an estimator with minimal mean-squared error (MSE). In our experiments, our genetic algorithm discovers estimators that have a substantially smalle
&lt;/p&gt;</description></item><item><title>Sparse-VQ&#26159;&#19968;&#31181;&#26080;&#21069;&#39304;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#31232;&#30095;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#21644;&#21453;&#23454;&#20363;&#24402;&#19968;&#21270;&#26469;&#20943;&#23569;&#22122;&#22768;&#24433;&#21709;&#24182;&#25429;&#33719;&#36275;&#22815;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.05830</link><description>&lt;p&gt;
&#31232;&#30095;&#21521;&#37327;&#37327;&#21270;&#21464;&#21387;&#22120;&#65306;&#19968;&#31181;&#26080;&#21069;&#39304;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Sparse-VQ Transformer: An FFN-Free Framework with Vector Quantization for Enhanced Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05830
&lt;/p&gt;
&lt;p&gt;
Sparse-VQ&#26159;&#19968;&#31181;&#26080;&#21069;&#39304;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#31232;&#30095;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#21644;&#21453;&#23454;&#20363;&#24402;&#19968;&#21270;&#26469;&#20943;&#23569;&#22122;&#22768;&#24433;&#21709;&#24182;&#25429;&#33719;&#36275;&#22815;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#32780;&#21464;&#21387;&#22120;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#31361;&#20986;&#12290;&#39046;&#20808;&#30340;&#26041;&#27861;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#23450;&#21046;&#20102;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#21033;&#29992;&#20462;&#34917;&#25216;&#26415;&#23558;&#36830;&#32493;&#20449;&#21495;&#36716;&#25442;&#20026;&#29255;&#27573;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20998;&#24067;&#21464;&#21270;&#21644;&#20869;&#22312;&#22122;&#22768;&#27700;&#24179;&#30340;&#26174;&#33879;&#21464;&#21270;&#65292;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20855;&#26377;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31232;&#30095;&#21521;&#37327;&#37327;&#21270;&#30340;FFN-Free&#21464;&#21387;&#22120;&#65288;Sparse-VQ&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#31232;&#30095;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#21644;&#21453;&#23454;&#20363;&#24402;&#19968;&#21270;&#65288;RevIN&#65289;&#26469;&#20943;&#23569;&#22122;&#22768;&#24433;&#21709;&#65292;&#24182;&#25429;&#33719;&#36275;&#22815;&#30340;&#32479;&#35745;&#20449;&#24687;&#29992;&#20110;&#39044;&#27979;&#65292;&#20316;&#20026;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#21069;&#39304;&#23618;&#65288;FFN&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26080;FFN&#26041;&#27861;&#21066;&#20943;&#20102;&#21442;&#25968;&#25968;&#37327;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#20943;&#23569;&#20102;&#36807;&#25311;&#21512;&#12290;&#36890;&#36807;&#23545;&#21313;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#26032;&#24341;&#20837;&#30340;CAISO&#25968;&#25454;&#38598;&#65292;Sparse-VQ&#21462;&#24471;&#20102;
&lt;/p&gt;
&lt;p&gt;
Time series analysis is vital for numerous applications, and transformers have become increasingly prominent in this domain. Leading methods customize the transformer architecture from NLP and CV, utilizing a patching technique to convert continuous signals into segments. Yet, time series data are uniquely challenging due to significant distribution shifts and intrinsic noise levels. To address these two challenges,we introduce the Sparse Vector Quantized FFN-Free Transformer (Sparse-VQ). Our methodology capitalizes on a sparse vector quantization technique coupled with Reverse Instance Normalization (RevIN) to reduce noise impact and capture sufficient statistics for forecasting, serving as an alternative to the Feed-Forward layer (FFN) in the transformer architecture. Our FFN-free approach trims the parameter count, enhancing computational efficiency and reducing overfitting. Through evaluations across ten benchmark datasets, including the newly introduced CAISO dataset, Sparse-VQ su
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#21457;&#29616;&#20855;&#26377;&#26102;&#38388;&#24847;&#35782;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#25163;&#21160;&#35774;&#35745;&#30340;&#31639;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#34920;&#36798;&#20986;&#23398;&#20064;&#30340;&#26032;&#21407;&#21017;&#65292;&#24182;&#36866;&#29992;&#20110;&#21508;&#31181;&#19981;&#21516;&#30340;&#35774;&#32622;&#12290;</title><link>https://arxiv.org/abs/2402.05828</link><description>&lt;p&gt;
&#21457;&#29616;&#20855;&#26377;&#26102;&#38388;&#24847;&#35782;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Discovering Temporally-Aware Reinforcement Learning Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05828
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#21457;&#29616;&#20855;&#26377;&#26102;&#38388;&#24847;&#35782;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#25163;&#21160;&#35774;&#35745;&#30340;&#31639;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#34920;&#36798;&#20986;&#23398;&#20064;&#30340;&#26032;&#21407;&#21017;&#65292;&#24182;&#36866;&#29992;&#20110;&#21508;&#31181;&#19981;&#21516;&#30340;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20803;&#23398;&#20064;&#36827;&#23637;&#20351;&#24471;&#26681;&#25454;&#20195;&#29702;&#30446;&#26631;&#20989;&#25968;&#33258;&#21160;&#21457;&#29616;&#21442;&#25968;&#21270;&#30340;&#26032;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25104;&#20026;&#21487;&#33021;&#12290;&#20026;&#20102;&#25913;&#36827;&#25163;&#21160;&#35774;&#35745;&#30340;&#31639;&#27861;&#65292;&#24517;&#39035;&#23545;&#36825;&#20010;&#23398;&#20064;&#21040;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#21442;&#25968;&#21270;&#36827;&#34892;&#25913;&#36827;&#65292;&#20351;&#20854;&#33021;&#22815;&#34920;&#36798;&#20986;&#23398;&#20064;&#30340;&#26032;&#21407;&#21017;&#65288;&#32780;&#19981;&#20165;&#20165;&#26159;&#24674;&#22797;&#24050;&#32463;&#24314;&#31435;&#30340;&#21407;&#21017;&#65289;&#65292;&#21516;&#26102;&#20173;&#28982;&#36866;&#29992;&#20110;&#20854;&#20803;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#21508;&#31181;&#35774;&#32622;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#38598;&#20013;&#20110;&#21457;&#29616;&#31867;&#20284;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#36825;&#20123;&#30446;&#26631;&#20989;&#25968;&#19981;&#32771;&#34385;&#35757;&#32451;&#25152;&#20801;&#35768;&#30340;&#24635;&#27493;&#25968;&#25110;&#8220;&#35757;&#32451;&#35270;&#37326;&#8221;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#22312;&#33719;&#21462;&#26032;&#33021;&#21147;&#30340;&#36807;&#31243;&#20013;&#20250;&#20351;&#29992;&#21508;&#31181;&#19981;&#21516;&#30340;&#23398;&#20064;&#30446;&#26631;&#12290;&#20363;&#22914;&#65292;&#23398;&#29983;&#21487;&#33021;&#20250;&#26681;&#25454;&#32771;&#35797;&#25130;&#27490;&#26085;&#26399;&#21644;&#33258;&#25105;&#35780;&#20272;&#30340;&#33021;&#21147;&#26469;&#25913;&#21464;&#20182;&#20204;&#30340;&#23398;&#20064;&#25216;&#24039;&#12290;&#26412;&#25991;&#35748;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Recent advancements in meta-learning have enabled the automatic discovery of novel reinforcement learning algorithms parameterized by surrogate objective functions. To improve upon manually designed algorithms, the parameterization of this learned objective function must be expressive enough to represent novel principles of learning (instead of merely recovering already established ones) while still generalizing to a wide range of settings outside of its meta-training distribution. However, existing methods focus on discovering objective functions that, like many widely used objective functions in reinforcement learning, do not take into account the total number of steps allowed for training, or "training horizon". In contrast, humans use a plethora of different learning objectives across the course of acquiring a new ability. For instance, students may alter their studying techniques based on the proximity to exam deadlines and their self-assessed capabilities. This paper contends tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#23558;&#21382;&#21490;&#21151;&#29575;&#25968;&#25454;&#12289;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#21644;&#21355;&#26143;&#22270;&#20687;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22826;&#38451;&#33021;&#21457;&#30005;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#33021;&#21147;&#65292;&#23545;&#20110;&#26032;&#23433;&#35013;&#30340;&#30005;&#31449;&#23588;&#20854;&#26377;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.05823</link><description>&lt;p&gt;
FusionSF&#65306;&#22312;&#30690;&#37327;&#37327;&#21270;&#26694;&#26550;&#20013;&#34701;&#21512;&#24322;&#36136;&#27169;&#24577;&#20197;&#23454;&#29616;&#21487;&#38752;&#30340;&#22826;&#38451;&#33021;&#21457;&#30005;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
FusionSF: Fuse Heterogeneous Modalities in a Vector Quantized Framework for Robust Solar Power Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#23558;&#21382;&#21490;&#21151;&#29575;&#25968;&#25454;&#12289;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#21644;&#21355;&#26143;&#22270;&#20687;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22826;&#38451;&#33021;&#21457;&#30005;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#33021;&#21147;&#65292;&#23545;&#20110;&#26032;&#23433;&#35013;&#30340;&#30005;&#31449;&#23588;&#20854;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#22826;&#38451;&#33021;&#21457;&#30005;&#39044;&#27979;&#23545;&#20110;&#23558;&#20809;&#20239;&#30005;&#31449;&#25972;&#21512;&#21040;&#30005;&#32593;&#20013;&#65292;&#35843;&#24230;&#21644;&#30830;&#20445;&#30005;&#32593;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#20110;&#32570;&#20047;&#36275;&#22815;&#25968;&#25454;&#30340;&#26032;&#23433;&#35013;&#30340;&#22826;&#38451;&#33021;&#30005;&#31449;&#26469;&#35828;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#32039;&#36843;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;&#20110;&#21382;&#21490;&#22826;&#38451;&#33021;&#21457;&#30005;&#25968;&#25454;&#25110;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65292;&#20197;&#21333;&#19968;&#27169;&#24577;&#30340;&#24418;&#24335;&#65292;&#24573;&#30053;&#20102;&#19981;&#21516;&#27169;&#24577;&#25552;&#20379;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#23558;&#21382;&#21490;&#21151;&#29575;&#25968;&#25454;&#12289;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#21644;&#21355;&#26143;&#22270;&#20687;&#36827;&#34892;&#25972;&#21512;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#30690;&#37327;&#37327;&#21270;&#26694;&#26550;&#65292;&#20351;&#20855;&#26377;&#19981;&#21516;&#20449;&#24687;&#23494;&#24230;&#30340;&#27169;&#24577;&#23545;&#40784;&#65292;&#24179;&#34913;&#20102;&#25972;&#21512;&#36275;&#22815;&#20449;&#24687;&#21644;&#36991;&#20813;&#27169;&#22411;&#36807;&#25311;&#21512;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#33021;&#21147;&#65292;&#23545;&#20110;&#26032;&#23433;&#35013;&#30340;&#30005;&#31449;&#23588;&#20854;&#26377;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate solar power forecasting is crucial to integrate photovoltaic plants into the electric grid, schedule and secure the power grid safety. This problem becomes more demanding for those newly installed solar plants which lack sufficient data. Current research predominantly relies on historical solar power data or numerical weather prediction in a single-modality format, ignoring the complementary information provided in different modalities. In this paper, we propose a multi-modality fusion framework to integrate historical power data, numerical weather prediction, and satellite images, significantly improving forecast performance. We introduce a vector quantized framework that aligns modalities with varying information densities, striking a balance between integrating sufficient information and averting model overfitting. Our framework demonstrates strong zero-shot forecasting capability, which is especially useful for those newly installed plants. Moreover, we collect and release
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20351;&#29992;&#20108;&#20803;&#21028;&#21035;&#22120;&#24341;&#23548;&#36827;&#21270;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#31243;&#24207;&#25628;&#32034;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#21306;&#20998;&#26356;&#22909;&#30340;&#31243;&#24207;&#65292;&#36873;&#25321;&#26356;&#22909;&#30340;&#31243;&#24207;&#24182;&#21152;&#24555;&#36827;&#21270;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#33021;&#22815;&#32534;&#30721;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#65292;&#21152;&#36895;&#36827;&#21270;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.05821</link><description>&lt;p&gt;
&#29992;&#20108;&#20803;&#21028;&#21035;&#22120;&#24341;&#23548;&#36827;&#21270;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#31243;&#24207;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Guided Evolution with Binary Discriminators for ML Program Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05821
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20351;&#29992;&#20108;&#20803;&#21028;&#21035;&#22120;&#24341;&#23548;&#36827;&#21270;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#31243;&#24207;&#25628;&#32034;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#21306;&#20998;&#26356;&#22909;&#30340;&#31243;&#24207;&#65292;&#36873;&#25321;&#26356;&#22909;&#30340;&#31243;&#24207;&#24182;&#21152;&#24555;&#36827;&#21270;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#33021;&#22815;&#32534;&#30721;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#65292;&#21152;&#36895;&#36827;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#33258;&#21160;&#35774;&#35745;&#26356;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#31243;&#24207;&#26159;AutoML&#20013;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#23613;&#31649;&#36827;&#21270;&#31639;&#27861;&#24050;&#32463;&#25104;&#20026;&#25628;&#32034;&#26356;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#31243;&#24207;&#30340;&#24120;&#29992;&#24037;&#20855;&#65292;&#20294;&#26159;&#22312;&#22797;&#26434;&#38382;&#39064;&#19978;&#20351;&#29992;&#23398;&#20064;&#26412;&#36523;&#26469;&#24341;&#23548;&#25628;&#32034;&#30340;&#26041;&#27861;&#25104;&#21151;&#36739;&#23569;&#19988;&#29702;&#35299;&#36739;&#23569;&#65292;&#20294;&#26377;&#30528;&#26174;&#33879;&#25552;&#39640;&#20248;&#21270;&#36807;&#31243;&#30340;&#36895;&#24230;&#21644;&#26368;&#32456;&#24615;&#33021;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#20010;&#20108;&#20803;&#21028;&#21035;&#22120;&#26469;&#24341;&#23548;&#36827;&#21270;&#65292;&#35813;&#21028;&#21035;&#22120;&#22312;&#32447;&#35757;&#32451;&#65292;&#36890;&#36807;&#21306;&#20998;&#32473;&#23450;&#19968;&#23545;&#31243;&#24207;&#20013;&#21738;&#20010;&#26356;&#22909;&#26469;&#36873;&#25321;&#26356;&#22909;&#30340;&#31243;&#24207;&#12290;&#21028;&#21035;&#22120;&#22312;&#19981;&#36827;&#34892;&#26114;&#36149;&#30340;&#35780;&#20272;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#26356;&#22909;&#30340;&#31243;&#24207;&#65292;&#20174;&#32780;&#21152;&#36895;&#36827;&#21270;&#30340;&#25910;&#25947;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#32534;&#30721;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#65292;&#21253;&#25324;&#31526;&#21495;&#20248;&#21270;&#22120;&#12289;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12289;&#24378;&#21270;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#21644;&#31526;&#21495;&#22238;&#24402;&#26041;&#31243;&#65292;&#23427;&#20204;&#37117;&#20351;&#29992;&#21516;&#26679;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#34920;&#31034;&#12290;&#36890;&#36807;&#23558;&#36825;&#31181;&#34920;&#31034;&#19982;&#29616;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#33258;&#36866;&#24212;&#21464;&#24322;&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21152;&#36895;&#36827;&#21270;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to automatically design better machine learning programs is an open problem within AutoML. While evolution has been a popular tool to search for better ML programs, using learning itself to guide the search has been less successful and less understood on harder problems but has the promise to dramatically increase the speed and final performance of the optimization process. We propose guiding evolution with a binary discriminator, trained online to distinguish which program is better given a pair of programs. The discriminator selects better programs without having to perform a costly evaluation and thus speed up the convergence of evolution. Our method can encode a wide variety of ML components including symbolic optimizers, neural architectures, RL loss functions, and symbolic regression equations with the same directed acyclic graph representation. By combining this representation with modern GNNs and an adaptive mutation strategy, we demonstrate our method can speed up evolutio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PW-HuBERT&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#23558;&#20266;&#35789;&#32423;&#30446;&#26631;&#25972;&#21512;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20174;&#35270;&#35273;&#35821;&#38899;&#27169;&#22411;&#20013;&#25552;&#21462;&#30446;&#26631;&#65292;&#20174;&#32780;&#22312;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.05819</link><description>&lt;p&gt;
&#23558;&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#27169;&#22411;&#19982;&#35270;&#35273;&#35821;&#38899;&#27169;&#22411;&#29983;&#25104;&#30340;&#20266;&#35789;&#32423;&#30446;&#26631;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Integrating Self-supervised Speech Model with Pseudo Word-level Targets from Visually-grounded Speech Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PW-HuBERT&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#23558;&#20266;&#35789;&#32423;&#30446;&#26631;&#25972;&#21512;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20174;&#35270;&#35273;&#35821;&#38899;&#27169;&#22411;&#20013;&#25552;&#21462;&#30446;&#26631;&#65292;&#20174;&#32780;&#22312;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#27169;&#22411;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#22312;&#24103;&#32423;&#35757;&#32451;&#30446;&#26631;&#19978;&#65292;&#22312;&#38656;&#35201;&#35821;&#20041;&#29702;&#35299;&#30340;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#20013;&#21487;&#33021;&#19981;&#36275;&#22815;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20381;&#36182;&#20110;&#39069;&#22806;&#30340;&#35821;&#38899;-&#25991;&#26412;&#25968;&#25454;&#20316;&#20026;&#20013;&#38388;&#30446;&#26631;&#65292;&#36825;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#25104;&#26412;&#39640;&#26114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Pseudo-Word HuBERT&#65288;PW-HuBERT&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#20266;&#35789;&#32423;&#30446;&#26631;&#25972;&#21512;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#20174;&#35270;&#35273;&#35821;&#38899;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#65292;&#22240;&#27492;&#28040;&#38500;&#20102;&#23545;&#35821;&#38899;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#21475;&#35821;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#22312;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in self-supervised speech models have shown significant improvement in many downstream tasks. However, these models predominantly centered on frame-level training objectives, which can fall short in spoken language understanding tasks that require semantic comprehension. Existing works often rely on additional speech-text data as intermediate targets, which is costly in the real-world setting. To address this challenge, we propose Pseudo-Word HuBERT (PW-HuBERT), a framework that integrates pseudo word-level targets into the training process, where the targets are derived from a visually-ground speech model, notably eliminating the need for speech-text paired data. Our experimental results on four spoken language understanding (SLU) benchmarks suggest the superiority of our model in capturing semantic information.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#26368;&#26032;&#30340;YOLO V7&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#21307;&#23398;&#22270;&#20687;&#36827;&#34892;&#20462;&#25913;&#21644;&#35757;&#32451;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#32958;&#33039;&#22312;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#20026;&#32958;&#33039;&#30142;&#30149;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#25552;&#20379;&#20102;&#26377;&#21147;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.05817</link><description>&lt;p&gt;
&#20351;&#29992;YOLO v7&#22312;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#26816;&#27979;&#32958;&#33039;&#65306;&#19968;&#31181;&#26377;&#30417;&#30563;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Using YOLO v7 to Detect Kidney in Magnetic Resonance Imaging: A Supervised Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#26368;&#26032;&#30340;YOLO V7&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#21307;&#23398;&#22270;&#20687;&#36827;&#34892;&#20462;&#25913;&#21644;&#35757;&#32451;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#32958;&#33039;&#22312;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#20026;&#32958;&#33039;&#30142;&#30149;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#25552;&#20379;&#20102;&#26377;&#21147;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#26368;&#26032;&#30340;You Only Look Once (YOLO V7)&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#20013;&#22686;&#24378;&#32958;&#33039;&#26816;&#27979;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23545;&#21307;&#23398;&#22270;&#20687;&#26684;&#24335;&#36827;&#34892;&#20462;&#25913;&#65292;&#23545;&#20462;&#25913;&#21518;&#30340;YOLO V7&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#30740;&#31350;&#21253;&#25324;878&#21517;&#19981;&#21516;&#20122;&#22411;&#30340;&#32958;&#32454;&#32990;&#30284;&#65288;RCC&#65289;&#24739;&#32773;&#21644;206&#21517;&#27491;&#24120;&#32958;&#33039;&#24739;&#32773;&#12290;&#20849;&#26816;&#32034;&#21040;1084&#21517;&#24739;&#32773;&#30340;5657&#24352;MRI&#25195;&#25551;&#12290;&#20174;&#22238;&#39038;&#24615;&#25968;&#25454;&#24211;&#20013;&#36873;&#25321;&#20102;326&#21517;&#24739;&#26377;1034&#20010;&#32959;&#30244;&#30340;&#24739;&#32773;&#65292;&#24182;&#22312;&#20854;&#32959;&#30244;&#21608;&#22260;&#32472;&#21046;&#20102;&#36793;&#30028;&#26694;&#12290;&#22312;&#21021;&#22987;&#27169;&#22411;&#19978;&#23545;80%&#30340;&#27880;&#37322;&#26696;&#20363;&#36827;&#34892;&#35757;&#32451;&#65292;&#20445;&#30041;20%&#29992;&#20110;&#27979;&#35797;&#65288;&#20027;&#35201;&#27979;&#35797;&#38598;&#65289;&#12290;&#28982;&#21518;&#20351;&#29992;&#26368;&#20339;&#30340;&#20027;&#35201;&#27169;&#22411;&#22312;&#20854;&#20313;861&#21517;&#24739;&#32773;&#19978;&#35782;&#21035;&#32959;&#30244;&#65292;&#24182;&#20351;&#29992;&#35813;&#27169;&#22411;&#22312;&#20854;&#25195;&#25551;&#20013;&#29983;&#25104;&#36793;&#30028;&#26694;&#22352;&#26631;&#12290;&#21019;&#24314;&#20102;&#21313;&#20010;&#22522;&#20934;&#35757;&#32451;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#26410;&#20998;&#21106;&#24739;&#32773;&#19978;&#30340;&#29983;&#25104;&#22352;&#26631;&#12290;&#26368;&#32456;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#20027;&#35201;&#27979;&#35797;&#38598;&#20013;&#30340;&#32958;&#33039;&#12290;
&lt;/p&gt;
&lt;p&gt;
Introduction This study explores the use of the latest You Only Look Once (YOLO V7) object detection method to enhance kidney detection in medical imaging by training and testing a modified YOLO V7 on medical image formats. Methods Study includes 878 patients with various subtypes of renal cell carcinoma (RCC) and 206 patients with normal kidneys. A total of 5657 MRI scans for 1084 patients were retrieved. 326 patients with 1034 tumors recruited from a retrospective maintained database, and bounding boxes were drawn around their tumors. A primary model was trained on 80% of annotated cases, with 20% saved for testing (primary test set). The best primary model was then used to identify tumors in the remaining 861 patients and bounding box coordinates were generated on their scans using the model. Ten benchmark training sets were created with generated coordinates on not-segmented patients. The final model used to predict the kidney in the primary test set. We reported the positive predi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#27491;&#30830;&#28436;&#31034;&#24182;&#24314;&#31435;&#36880;&#27493;&#30340;&#35838;&#31243;&#65292;&#23454;&#29616;&#20102;&#32467;&#26524;&#30417;&#30563;&#21644;&#36807;&#31243;&#30417;&#30563;&#30340;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.05808</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#27491;&#30830;&#28436;&#31034;&#24182;&#24314;&#31435;&#36880;&#27493;&#30340;&#35838;&#31243;&#65292;&#23454;&#29616;&#20102;&#32467;&#26524;&#30417;&#30563;&#21644;&#36807;&#31243;&#30417;&#30563;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;R$^3$&#65306;&#36890;&#36807;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;&#25512;&#29702;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21482;&#20351;&#29992;&#32467;&#26524;&#30417;&#30563;&#26469;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36807;&#31243;&#30417;&#30563;&#30340;&#22909;&#22788;&#12290;&#23558;RL&#24212;&#29992;&#20110;&#22797;&#26434;&#25512;&#29702;&#30340;&#26680;&#24515;&#25361;&#25112;&#26159;&#30830;&#23450;&#19968;&#31995;&#21015;&#34892;&#21160;&#65292;&#20197;&#33719;&#24471;&#27491;&#21521;&#22870;&#21169;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#20248;&#21270;&#30417;&#30563;&#12290;&#32467;&#26524;&#30417;&#30563;&#20026;&#26368;&#32456;&#32467;&#26524;&#25552;&#20379;&#20102;&#31232;&#30095;&#22870;&#21169;&#65292;&#32780;&#19981;&#35782;&#21035;&#38169;&#35823;&#20301;&#32622;&#65292;&#32780;&#36807;&#31243;&#30417;&#30563;&#25552;&#20379;&#20102;&#36880;&#27493;&#22870;&#21169;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#27880;&#37322;&#12290;R$^3$&#36890;&#36807;&#23398;&#20064;&#27491;&#30830;&#28436;&#31034;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;R$^3$&#23558;&#25512;&#29702;&#30340;&#36215;&#22987;&#29366;&#24577;&#20174;&#28436;&#31034;&#30340;&#32467;&#26463;&#28369;&#21160;&#21040;&#24320;&#22987;&#65292;&#20174;&#32780;&#22312;&#25152;&#26377;&#38454;&#27573;&#37117;&#20419;&#36827;&#20102;&#26356;&#23481;&#26131;&#30340;&#27169;&#22411;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;R$^3$&#24314;&#31435;&#20102;&#19968;&#20010;&#36880;&#27493;&#30340;&#35838;&#31243;&#65292;&#20351;&#32467;&#26524;&#30417;&#30563;&#33021;&#22815;&#25552;&#20379;&#38454;&#27573;&#32423;&#20449;&#21495;&#24182;&#31934;&#30830;&#23450;&#20301;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose R$^3$: Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models. The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation. R$^3$ overcomes these limitations by learning from correct demonstrations. Specifically, R$^3$ progressively slides the start state of reasoning from a demonstration's end to its beginning, facilitating easier model exploration at all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors. Using Llama2-7
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28201;&#24230;&#32553;&#25918;&#23545;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#26657;&#20934;&#23545;&#33258;&#36866;&#24212;C&#26041;&#27861;&#20135;&#29983;&#20102;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.05806</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#20998;&#31867;&#22120;&#30340;&#26657;&#20934;&#21644;&#31526;&#21512;&#39044;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Calibration and Conformal Prediction of Deep Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28201;&#24230;&#32553;&#25918;&#23545;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#26657;&#20934;&#23545;&#33258;&#36866;&#24212;C&#26041;&#27861;&#20135;&#29983;&#20102;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#20998;&#31867;&#24212;&#29992;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#39044;&#27979;&#38656;&#35201;&#20276;&#38543;&#19968;&#20123;&#32622;&#20449;&#24230;&#25351;&#31034;&#12290;&#38024;&#23545;&#36825;&#20010;&#30446;&#26631;&#65292;&#26377;&#20004;&#31181;&#27969;&#34892;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#65306;1&#65289;&#26657;&#20934;&#65306;&#20462;&#25913;&#20998;&#31867;&#22120;&#30340;softmax&#20540;&#65292;&#20351;&#20854;&#26368;&#22823;&#20540;&#65288;&#19982;&#39044;&#27979;&#30456;&#20851;&#65289;&#26356;&#22909;&#22320;&#20272;&#35745;&#27491;&#30830;&#27010;&#29575;&#65307;&#21644;2&#65289;&#31526;&#21512;&#39044;&#27979;&#65288;CP&#65289;&#65306;&#35774;&#35745;&#19968;&#20010;&#22522;&#20110;softmax&#20540;&#30340;&#20998;&#25968;&#65292;&#20174;&#20013;&#20135;&#29983;&#19968;&#32452;&#39044;&#27979;&#65292;&#20855;&#26377;&#29702;&#35770;&#19978;&#20445;&#35777;&#27491;&#30830;&#31867;&#21035;&#36793;&#38469;&#35206;&#30422;&#30340;&#29305;&#24615;&#12290;&#23613;&#31649;&#22312;&#23454;&#36341;&#20013;&#20004;&#31181;&#25351;&#31034;&#37117;&#21487;&#33021;&#26159;&#38656;&#35201;&#30340;&#65292;&#20294;&#21040;&#30446;&#21069;&#20026;&#27490;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#28201;&#24230;&#32553;&#25918;&#65292;&#36825;&#26159;&#26368;&#24120;&#35265;&#30340;&#26657;&#20934;&#25216;&#26415;&#65292;&#23545;&#37325;&#35201;&#30340;CP&#26041;&#27861;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#19968;&#39033;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#20854;&#20013;&#26174;&#31034;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#27934;&#23519;&#65292;&#20854;&#20013;&#21253;&#25324;&#20196;&#20154;&#24778;&#35766;&#30340;&#21457;&#29616;&#65292;&#21363;&#26657;&#20934;&#23545;&#27969;&#34892;&#30340;&#33258;&#36866;&#24212;C&#26041;&#27861;&#20135;&#29983;&#20102;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many classification applications, the prediction of a deep neural network (DNN) based classifier needs to be accompanied with some confidence indication. Two popular post-processing approaches for that aim are: 1) calibration: modifying the classifier's softmax values such that their maximum (associated with the prediction) better estimates the correctness probability; and 2) conformal prediction (CP): devising a score (based on the softmax values) from which a set of predictions with theoretically guaranteed marginal coverage of the correct class is produced. While in practice both types of indications can be desired, so far the interplay between them has not been investigated. Toward filling this gap, in this paper we study the effect of temperature scaling, arguably the most common calibration technique, on prominent CP methods. We start with an extensive empirical study that among other insights shows that, surprisingly, calibration has a detrimental effect on popular adaptive C
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#21644;&#27010;&#29575;&#29420;&#31435;&#24615;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;2000&#20010;&#28508;&#22312;&#28304;&#30340;&#20020;&#24202;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#22312;&#32954;&#30284;&#39044;&#27979;&#20013;&#30340;&#21028;&#21035;&#33021;&#21147;&#20248;&#20110;&#21407;&#22987;&#21464;&#37327;&#65292;&#24182;&#19988;&#33021;&#22815;&#35782;&#21035;&#26410;&#34987;&#35786;&#26029;&#30340;&#30284;&#30151;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.05802</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21457;&#29616;&#20020;&#24202;&#30142;&#30149;&#29305;&#24449;&#30340;&#27010;&#29575;&#29420;&#31435;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Discovery of Clinical Disease Signatures Using Probabilistic Independence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05802
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#21644;&#27010;&#29575;&#29420;&#31435;&#24615;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;2000&#20010;&#28508;&#22312;&#28304;&#30340;&#20020;&#24202;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#22312;&#32954;&#30284;&#39044;&#27979;&#20013;&#30340;&#21028;&#21035;&#33021;&#21147;&#20248;&#20110;&#21407;&#22987;&#21464;&#37327;&#65292;&#24182;&#19988;&#33021;&#22815;&#35782;&#21035;&#26410;&#34987;&#35786;&#26029;&#30340;&#30284;&#30151;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#30142;&#30149;&#30340;&#35786;&#26029;&#19981;&#22815;&#20934;&#30830;&#21487;&#33021;&#23548;&#33268;&#24456;&#22810;&#27835;&#30103;&#22833;&#36133;&#30340;&#24773;&#20917;&#65292;&#21363;&#20351;&#26159;&#24120;&#35265;&#30142;&#30149;&#21644;&#27835;&#30103;&#12290;&#36890;&#36807;&#20351;&#29992;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#20351;&#29992;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26469;&#26356;&#31934;&#30830;&#22320;&#23450;&#20041;&#20020;&#24202;&#30142;&#30149;&#27169;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27010;&#29575;&#29420;&#31435;&#24615;&#26469;&#35299;&#24320;&#30142;&#30149;&#28508;&#22312;&#28304;&#22240;&#23545;&#21307;&#30103;&#35760;&#24405;&#30340;&#24433;&#21709;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20174;269,099&#20221;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;9195&#20010;&#21464;&#37327;&#20013;&#25512;&#26029;&#20986;&#20102;2000&#20010;&#28508;&#22312;&#28304;&#30340;&#20020;&#24202;&#29305;&#24449;&#12290;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#22312;&#19968;&#20010;&#32954;&#30284;&#39044;&#27979;&#20219;&#21153;&#20013;&#27604;&#21407;&#22987;&#21464;&#37327;&#26377;&#26356;&#22909;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#35813;&#20219;&#21153;&#23545;&#25512;&#26029;&#31639;&#27861;&#26469;&#35828;&#26159;&#26410;&#30693;&#30340;&#65292;&#33021;&#22815;&#39044;&#27979;&#20986;&#26080;&#30284;&#30151;&#21382;&#21490;&#30340;&#24739;&#32773;&#22312;&#21457;&#29616;&#23396;&#31435;&#30340;&#32954;&#32467;&#33410;&#20043;&#21069;&#30340;3&#24180;&#20869;&#30340;&#24694;&#24615;&#30142;&#30149;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20123;&#29305;&#24449;&#30340;&#35299;&#37322;&#33021;&#21147;&#26356;&#24378;&#65292;&#33021;&#22815;&#35782;&#21035;&#20986;&#35768;&#22810;&#24739;&#32773;&#20013;&#26126;&#26174;&#26410;&#34987;&#35786;&#26029;&#30340;&#30284;&#30151;&#30340;&#32467;&#33410;&#21069;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Insufficiently precise diagnosis of clinical disease is likely responsible for many treatment failures, even for common conditions and treatments. With a large enough dataset, it may be possible to use unsupervised machine learning to define clinical disease patterns more precisely. We present an approach to learning these patterns by using probabilistic independence to disentangle the imprint on the medical record of causal latent sources of disease. We inferred a broad set of 2000 clinical signatures of latent sources from 9195 variables in 269,099 Electronic Health Records. The learned signatures produced better discrimination than the original variables in a lung cancer prediction task unknown to the inference algorithm, predicting 3-year malignancy in patients with no history of cancer before a solitary lung nodule was discovered. More importantly, the signatures' greater explanatory power identified pre-nodule signatures of apparently undiagnosed cancer in many of those patients.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformers&#22312;&#19978;&#19979;&#25991;&#33258;&#22238;&#24402;&#23398;&#20064;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#21457;&#29616;&#20102;&#20854;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#36807;&#31243;&#12290;&#38024;&#23545;&#19981;&#21516;&#24773;&#20917;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21333;&#23618;&#32447;&#24615;Transformer&#23454;&#29616;&#20102;&#26799;&#24230;&#19979;&#38477;&#20197;&#21450;&#27491;&#20132;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.05787</link><description>&lt;p&gt;
Transformers&#22312;&#19978;&#19979;&#25991;&#33258;&#22238;&#24402;&#23398;&#20064;&#20013;&#30340;&#34920;&#29616;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How do Transformers perform In-Context Autoregressive Learning?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformers&#22312;&#19978;&#19979;&#25991;&#33258;&#22238;&#24402;&#23398;&#20064;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#21457;&#29616;&#20102;&#20854;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#36807;&#31243;&#12290;&#38024;&#23545;&#19981;&#21516;&#24773;&#20917;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21333;&#23618;&#32447;&#24615;Transformer&#23454;&#29616;&#20102;&#26799;&#24230;&#19979;&#38477;&#20197;&#21450;&#27491;&#20132;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21462;&#24471;&#24040;&#22823;&#25104;&#21151;&#30340;&#21407;&#22240;&#36824;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#31616;&#21333;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#19978;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35757;&#32451;&#21518;&#30340;Transformer&#22914;&#20309;&#36890;&#36807;&#39318;&#20808;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;W&#65292;&#28982;&#21518;&#24212;&#29992;&#39044;&#27979;&#26144;&#23556;&#26469;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#31216;&#36825;&#20010;&#32467;&#26524;&#20026;&#19978;&#19979;&#25991;&#33258;&#22238;&#24402;&#23398;&#20064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#38024;&#23545;W&#26159;&#20132;&#25442;&#27491;&#20132;&#30697;&#38453;&#30340;&#24773;&#20917;&#65292;&#39318;&#20808;&#35777;&#26126;&#20102;&#19968;&#20010;&#35757;&#32451;&#21518;&#30340;&#21333;&#23618;&#32447;&#24615;Transformer&#22312;&#32771;&#34385;&#25193;&#23637;&#26631;&#35760;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19968;&#27493;&#26799;&#24230;&#19979;&#38477;&#26469;&#26368;&#23567;&#21270;&#20869;&#37096;&#30446;&#26631;&#20989;&#25968;&#12290;&#24403;&#26631;&#35760;&#27809;&#26377;&#25193;&#23637;&#26102;&#65292;&#25105;&#20204;&#23545;&#20110;&#19968;&#20010;&#21333;&#23618;&#23545;&#35282;&#32447;&#32447;&#24615;&#22810;&#22836;Transformer&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22836;&#37096;&#20043;&#38388;&#30340;&#27491;&#20132;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have achieved state-of-the-art performance in language modeling tasks. However, the reasons behind their tremendous success are still unclear. In this paper, towards a better understanding, we train a Transformer model on a simple next token prediction task, where sequences are generated as a first-order autoregressive process $s_{t+1} = W s_t$. We show how a trained Transformer predicts the next token by first learning $W$ in-context, then applying a prediction mapping. We call the resulting procedure in-context autoregressive learning. More precisely, focusing on commuting orthogonal matrices $W$, we first show that a trained one-layer linear Transformer implements one step of gradient descent for the minimization of an inner objective function, when considering augmented tokens. When the tokens are not augmented, we characterize the global minima of a one-layer diagonal linear multi-head Transformer. Importantly, we exhibit orthogonality between heads and show that posi
&lt;/p&gt;</description></item><item><title>Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#27604;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#23545;&#20110;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#30340;&#25928;&#26524;&#26356;&#24046;&#65292;&#32780;&#19988;&#26799;&#24230;&#19979;&#38477;&#22312;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.05785</link><description>&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#27861;&#23398;&#20064;&#19978;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Limits of Transformer Language Models on Algorithmic Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05785
&lt;/p&gt;
&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#27604;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#23545;&#20110;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#30340;&#25928;&#26524;&#26356;&#24046;&#65292;&#32780;&#19988;&#26799;&#24230;&#19979;&#38477;&#22312;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#35201;&#27714;&#32452;&#21512;&#22810;&#20010;&#31163;&#25955;&#23376;&#20219;&#21153;&#30340;&#26032;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;LLaMA&#27169;&#22411;&#21644;&#22312;GPT-4&#21644;Gemini&#19978;&#25552;&#31034;&#26469;&#34913;&#37327;&#23398;&#20064;&#23398;&#20064;&#21407;&#35821;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#24182;&#19988;&#22312;&#26679;&#26412;&#35268;&#27169;&#26041;&#38754;&#27604;&#20026;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#25928;&#26524;&#26356;&#24046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22797;&#26434;&#24615;&#29702;&#35770;&#30340;&#23450;&#29702;&#65292;&#35777;&#26126;&#20102;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#25351;&#25968;&#32423;&#22320;&#28010;&#36153;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the capabilities of Transformer language models on learning discrete algorithms. To this end, we introduce two new tasks demanding the composition of several discrete sub-tasks. On both training LLaMA models from scratch and prompting on GPT-4 and Gemini we measure learning compositions of learned primitives. We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition. We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25163;&#22609;&#36896;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#21512;&#29702;&#35770;&#20998;&#26512;&#30340;&#34920;&#26684;&#21270;&#29256;&#26412;R-FOS&#12290;</title><link>https://arxiv.org/abs/2402.05782</link><description>&lt;p&gt;
&#20998;&#26512;&#23545;&#25163;&#22609;&#36896;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Analysing the Sample Complexity of Opponent Shaping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25163;&#22609;&#36896;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#21512;&#29702;&#35770;&#20998;&#26512;&#30340;&#34920;&#26684;&#21270;&#29256;&#26412;R-FOS&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#33324;&#21644;&#21338;&#24328;&#20013;&#65292;&#23398;&#20064;&#36890;&#24120;&#20250;&#23548;&#33268;&#38598;&#20307;&#24615;&#30340;&#27425;&#20248;&#32467;&#26524;&#12290;&#23545;&#27492;&#36827;&#34892;&#25913;&#36827;&#65292;&#23545;&#25163;&#22609;&#36896;&#65288;OS&#65289;&#26041;&#27861;&#31215;&#26497;&#24341;&#23548;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#32463;&#39564;&#24615;&#22320;&#25552;&#39640;&#20102;&#20010;&#20307;&#21644;&#32676;&#20307;&#22312;&#35768;&#22810;&#24773;&#26223;&#19979;&#30340;&#34920;&#29616;&#12290;&#26089;&#26399;&#30340;OS&#26041;&#27861;&#20351;&#29992;&#39640;&#38454;&#23548;&#25968;&#26469;&#22609;&#36896;&#21512;&#20316;&#29609;&#23478;&#30340;&#23398;&#20064;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#22609;&#36896;&#22810;&#20010;&#23398;&#20064;&#27493;&#39588;&#12290;&#21518;&#32493;&#30340;&#24037;&#20316;&#65292;&#21363;&#26080;&#27169;&#22411;&#23545;&#25163;&#22609;&#36896;&#65288;M-FOS&#65289;&#65292;&#36890;&#36807;&#23558;OS&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#20803;&#21338;&#24328;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#19982;&#26089;&#26399;&#30340;OS&#26041;&#27861;&#30456;&#27604;&#65292;&#23545;&#20110;M-FOS&#26694;&#26550;&#30340;&#29702;&#35770;&#29702;&#35299;&#36824;&#24456;&#23569;&#12290;&#20026;M-FOS&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;A&#65289;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#26679;&#26412;&#22797;&#26434;&#24615;&#30028;&#38480;&#30340;&#25991;&#29486;&#24456;&#23569; B&#65289;M-FOS&#22312;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#20013;&#36816;&#20316;&#65292;&#25152;&#20197;&#29702;&#35770;&#20998;&#26512;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;R-FOS&#65292;&#36825;&#26159;M-FOS&#30340;&#34920;&#26684;&#21270;&#29256;&#26412;&#65292;&#26356;&#36866;&#21512;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#12290;R-FOS&#31163;&#25955;&#21270;&#20102;
&lt;/p&gt;
&lt;p&gt;
Learning in general-sum games often yields collectively sub-optimal results. Addressing this, opponent shaping (OS) methods actively guide the learning processes of other agents, empirically leading to improved individual and group performances in many settings. Early OS methods use higher-order derivatives to shape the learning of co-players, making them unsuitable for shaping multiple learning steps. Follow-up work, Model-free Opponent Shaping (M-FOS), addresses these by reframing the OS problem as a meta-game. In contrast to early OS methods, there is little theoretical understanding of the M-FOS framework. Providing theoretical guarantees for M-FOS is hard because A) there is little literature on theoretical sample complexity bounds for meta-reinforcement learning B) M-FOS operates in continuous state and action spaces, so theoretical analysis is challenging. In this work, we present R-FOS, a tabular version of M-FOS that is more suitable for theoretical analysis. R-FOS discretises
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#31283;&#23450;&#24615;&#24037;&#20855;&#20110;&#26102;&#38388;&#29420;&#31435;&#31995;&#32479;&#30340;&#27969;&#21305;&#37197;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#29289;&#29702;&#31283;&#23450;&#25968;&#25454;&#28857;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#19982;&#25511;&#21046;&#29702;&#35770;&#21407;&#29702;&#36827;&#34892;&#20102;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.05774</link><description>&lt;p&gt;
&#31283;&#23450;&#30340;&#33258;&#20027;&#27969;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Stable Autonomous Flow Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#31283;&#23450;&#24615;&#24037;&#20855;&#20110;&#26102;&#38388;&#29420;&#31435;&#31995;&#32479;&#30340;&#27969;&#21305;&#37197;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#29289;&#29702;&#31283;&#23450;&#25968;&#25454;&#28857;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#19982;&#25511;&#21046;&#29702;&#35770;&#21407;&#29702;&#36827;&#34892;&#20102;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#34920;&#31034;&#29289;&#29702;&#31283;&#23450;&#29366;&#24577;&#30340;&#25968;&#25454;&#26679;&#26412;&#20013;&#65292;&#36890;&#24120;&#20551;&#35774;&#25968;&#25454;&#28857;&#20195;&#34920;&#33021;&#37327;&#26223;&#35266;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#12290;&#22312;&#25511;&#21046;&#35770;&#20013;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#33021;&#37327;&#21487;&#20197;&#20316;&#20026;&#26377;&#25928;&#30340;&#26446;&#20122;&#26222;&#35834;&#22827;&#20989;&#25968;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25511;&#21046;&#35770;&#19982;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#22312;&#25991;&#29486;&#20013;&#24456;&#23569;&#65292;&#23613;&#31649;&#26377;&#20960;&#20010;&#20855;&#26377;&#29289;&#29702;&#31283;&#23450;&#25968;&#25454;&#28857;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#36825;&#26679;&#30340;&#25968;&#25454;&#21644;&#19968;&#31181;&#26368;&#36817;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#31867;&#21035;&#65292;&#31216;&#20026;&#27969;&#21305;&#37197;&#12290;&#25105;&#20204;&#24212;&#29992;&#38543;&#26426;&#31283;&#23450;&#24615;&#24037;&#20855;&#20110;&#26102;&#38388;&#29420;&#31435;&#31995;&#32479;&#30340;&#27969;&#21305;&#37197;&#27169;&#22411;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#36866;&#24212;&#36825;&#31181;&#22788;&#29702;&#30340;&#27969;&#21305;&#37197;&#27169;&#22411;&#30340;&#31354;&#38388;&#65292;&#20197;&#21450;&#19982;&#20854;&#20182;&#25511;&#21046;&#29702;&#35770;&#21407;&#29702;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#31034;&#20363;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In contexts where data samples represent a physically stable state, it is often assumed that the data points represent the local minima of an energy landscape. In control theory, it is well-known that energy can serve as an effective Lyapunov function. Despite this, connections between control theory and generative models in the literature are sparse, even though there are several machine learning applications with physically stable data points. In this paper, we focus on such data and a recent class of deep generative models called flow matching. We apply tools of stochastic stability for time-independent systems to flow matching models. In doing so, we characterize the space of flow matching models that are amenable to this treatment, as well as draw connections to other control theory principles. We demonstrate our theoretical results on two examples.
&lt;/p&gt;</description></item><item><title>&#31163;&#32447;&#31574;&#30053;&#20998;&#24067;&#24335; Q($\lambda$) &#26159;&#19968;&#31181;&#19981;&#20351;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20855;&#26377;&#19982;&#26377;&#31526;&#21495;&#27979;&#24230;&#30340;&#26377;&#36259;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05766</link><description>&lt;p&gt;
&#31163;&#32447;&#31574;&#30053;&#20998;&#24067;&#24335; Q($\lambda$): &#19981;&#38656;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Off-policy Distributional Q($\lambda$): Distributional RL without Importance Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05766
&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#31574;&#30053;&#20998;&#24067;&#24335; Q($\lambda$) &#26159;&#19968;&#31181;&#19981;&#20351;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20855;&#26377;&#19982;&#26377;&#31526;&#21495;&#27979;&#24230;&#30340;&#26377;&#36259;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#31163;&#32447;&#31574;&#30053;&#20998;&#24067;&#24335; Q($\lambda$)&#65292;&#23427;&#26159;&#31163;&#32447;&#31574;&#30053;&#20998;&#24067;&#24335;&#35780;&#20272;&#31639;&#27861;&#26063;&#30340;&#26032;&#25104;&#21592;&#12290;&#31163;&#32447;&#31574;&#30053;&#20998;&#24067;&#24335; Q($\lambda$) &#22312;&#31163;&#32447;&#23398;&#20064;&#26102;&#19981;&#24212;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#65292;&#36825;&#24341;&#20837;&#20102;&#19982;&#26377;&#31526;&#21495;&#27979;&#24230;&#20043;&#38388;&#30340;&#26377;&#36259;&#20132;&#20114;&#20316;&#29992;&#12290;&#36825;&#20123;&#29420;&#29305;&#30340;&#29305;&#24615;&#20351;&#24471;&#31163;&#32447;&#31574;&#30053;&#20998;&#24067;&#24335; Q($\lambda$) &#19982;&#20854;&#20182;&#29616;&#26377;&#30340;&#26367;&#20195;&#26041;&#27861;&#65288;&#22914;&#20998;&#24067;&#24335; Retrace&#65289;&#26377;&#25152;&#21306;&#21035;&#12290;&#25105;&#20204;&#23545;&#31163;&#32447;&#31574;&#30053;&#20998;&#24067;&#24335; Q($\lambda$) &#30340;&#31639;&#27861;&#29305;&#24615;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#36890;&#36807;&#34920;&#26684;&#23454;&#39564;&#39564;&#35777;&#20102;&#29702;&#35770;&#27934;&#23519;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31163;&#32447;&#31574;&#30053;&#20998;&#24067;&#24335; Q($\lambda$)-C51&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce off-policy distributional Q($\lambda$), a new addition to the family of off-policy distributional evaluation algorithms. Off-policy distributional Q($\lambda$) does not apply importance sampling for off-policy learning, which introduces intriguing interactions with signed measures. Such unique properties distributional Q($\lambda$) from other existing alternatives such as distributional Retrace. We characterize the algorithmic properties of distributional Q($\lambda$) and validate theoretical insights with tabular experiments. We show how distributional Q($\lambda$)-C51, a combination of Q($\lambda$) with the C51 agent, exhibits promising results on deep RL benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#28857;&#36807;&#31243;&#30340;&#24102;&#26377;&#32467;&#26500;&#32570;&#22833;&#30340;&#28789;&#27963;&#39640;&#25928;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#25429;&#33719;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#21487;&#25193;&#23637;&#30340;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.05758</link><description>&lt;p&gt;
&#39640;&#32500;&#28857;&#36807;&#31243;&#30340;&#24102;&#32467;&#26500;&#32570;&#22833;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Latent variable model for high-dimensional point process with structured missingness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#28857;&#36807;&#31243;&#30340;&#24102;&#26377;&#32467;&#26500;&#32570;&#22833;&#30340;&#28789;&#27963;&#39640;&#25928;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#25429;&#33719;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#21487;&#25193;&#23637;&#30340;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32437;&#21521;&#25968;&#25454;&#22312;&#21307;&#30103;&#20445;&#20581;&#12289;&#31038;&#20250;&#23398;&#21644;&#22320;&#38663;&#23398;&#31561;&#35768;&#22810;&#39046;&#22495;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#20294;&#26159;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#23545;&#20174;&#19994;&#20154;&#21592;&#26469;&#35828;&#23384;&#22312;&#26126;&#26174;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#26159;&#39640;&#32500;&#30340;&#65292;&#21253;&#21547;&#26377;&#32467;&#26500;&#21270;&#30340;&#32570;&#22833;&#27169;&#24335;&#65292;&#24182;&#19988;&#27979;&#37327;&#26102;&#38388;&#28857;&#21487;&#33021;&#21463;&#21040;&#26410;&#30693;&#38543;&#26426;&#36807;&#31243;&#30340;&#25511;&#21046;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20854;&#20013;&#22823;&#22810;&#25968;&#20165;&#32771;&#34385;&#20102;&#36825;&#20123;&#25361;&#25112;&#20013;&#30340;&#19968;&#20010;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#39640;&#25928;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#33021;&#22815;&#24212;&#23545;&#25152;&#26377;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#26469;&#25429;&#33719;&#26679;&#26412;&#19982;&#20854;&#20851;&#32852;&#30340;&#32570;&#22833;&#27169;&#24335;&#20043;&#38388;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#20063;&#29992;&#20110;&#24314;&#27169;&#24213;&#23618;&#30340;&#28857;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#26500;&#24314;&#20026;&#19968;&#20010;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#21516;&#26102;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#26469;&#36827;&#34892;&#39640;&#25928;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Longitudinal data are important in numerous fields, such as healthcare, sociology and seismology, but real-world datasets present notable challenges for practitioners because they can be high-dimensional, contain structured missingness patterns, and measurement time points can be governed by an unknown stochastic process. While various solutions have been suggested, the majority of them have been designed to account for only one of these challenges. In this work, we propose a flexible and efficient latent-variable model that is capable of addressing all these limitations. Our approach utilizes Gaussian processes to capture temporal correlations between samples and their associated missingness masks as well as to model the underlying point process. We construct our model as a variational autoencoder together with deep neural network parameterised encoder and decoder models, and develop a scalable amortised variational inference approach for efficient model training. We demonstrate compe
&lt;/p&gt;</description></item><item><title>&#24191;&#20041;&#20559;&#22909;&#20248;&#21270;&#65288;GPO&#65289;&#26159;&#19968;&#31181;&#31163;&#32447;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#19968;&#31867;&#20984;&#20989;&#25968;&#26469;&#23454;&#29616;&#32479;&#19968;&#30340;&#20559;&#22909;&#20248;&#21270;&#35270;&#35282;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#31639;&#27861;&#24037;&#20855;&#21644;&#23454;&#35777;&#27934;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.05749</link><description>&lt;p&gt;
&#24191;&#20041;&#20559;&#22909;&#20248;&#21270;&#65306;&#31163;&#32447;&#23545;&#40784;&#30340;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generalized Preference Optimization: A Unified Approach to Offline Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05749
&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#20559;&#22909;&#20248;&#21270;&#65288;GPO&#65289;&#26159;&#19968;&#31181;&#31163;&#32447;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#19968;&#31867;&#20984;&#20989;&#25968;&#26469;&#23454;&#29616;&#32479;&#19968;&#30340;&#20559;&#22909;&#20248;&#21270;&#35270;&#35282;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#31639;&#27861;&#24037;&#20855;&#21644;&#23454;&#35777;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#20559;&#22909;&#20248;&#21270;&#20801;&#35768;&#30452;&#25509;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23545;&#22823;&#22411;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#22312;&#26368;&#36817;&#30340;&#23545;&#40784;&#23454;&#36341;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24191;&#20041;&#20559;&#22909;&#20248;&#21270;&#65288;GPO&#65289;&#65292;&#36825;&#26159;&#19968;&#31867;&#36890;&#36807;&#19968;&#33324;&#30340;&#20984;&#20989;&#25968;&#21442;&#25968;&#21270;&#30340;&#31163;&#32447;&#25439;&#22833;&#20989;&#25968;&#12290;GPO&#25552;&#20379;&#20102;&#23545;&#20559;&#22909;&#20248;&#21270;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#28085;&#30422;&#20102;&#29616;&#26377;&#31639;&#27861;&#65288;DPO&#12289;IPO&#21644;SLiC&#65289;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#65292;&#21516;&#26102;&#33258;&#28982;&#24341;&#20837;&#20102;&#26032;&#30340;&#21464;&#20307;&#12290;GPO&#26694;&#26550;&#36824;&#25581;&#31034;&#20102;&#31163;&#32447;&#31639;&#27861;&#22914;&#20309;&#36890;&#36807;&#23450;&#20041;&#25439;&#22833;&#30340;&#20984;&#20989;&#25968;&#26469;&#23454;&#26045;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#25581;&#31034;&#20102;&#31163;&#32447;&#27491;&#21017;&#21270;&#21644;&#35268;&#33539;&#30340;RLHF&#20844;&#24335;&#25152;&#24847;&#22270;&#30340;KL&#25955;&#24230;&#27491;&#21017;&#21270;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#24494;&#22937;&#24046;&#24322;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#23545;&#40784;&#23454;&#36341;&#32773;&#25552;&#20379;&#20102;&#26032;&#30340;&#31639;&#27861;&#24037;&#20855;&#21644;&#23454;&#35777;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline preference optimization allows fine-tuning large models directly from offline data, and has proved effective in recent alignment practices. We propose generalized preference optimization (GPO), a family of offline losses parameterized by a general class of convex functions. GPO enables a unified view over preference optimization, encompassing existing algorithms such as DPO, IPO and SLiC as special cases, while naturally introducing new variants. The GPO framework also sheds light on how offline algorithms enforce regularization, through the design of the convex function that defines the loss. Our analysis and experiments reveal the connections and subtle differences between the offline regularization and the KL divergence regularization intended by the canonical RLHF formulation. In all, our results present new algorithmic toolkits and empirical insights to alignment practitioners.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#26367;&#25442;&#29616;&#26377;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#29305;&#23450;&#32452;&#20214;&#12290;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#22312;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#12289;&#24863;&#30693;&#12289;&#36816;&#21160;&#35268;&#21010;&#21644;&#25511;&#21046;&#31561;&#26041;&#38754;&#25198;&#28436;&#20102;&#37325;&#35201;&#35282;&#33394;&#12290;&#26410;&#26469;&#30340;&#25361;&#25112;&#21644;&#23545;&#23454;&#38469;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#24433;&#21709;&#20063;&#34987;&#35752;&#35770;&#21040;&#12290;</title><link>https://arxiv.org/abs/2402.05741</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#24212;&#29992;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Real-World Robot Applications of Foundation Models: A Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#26367;&#25442;&#29616;&#26377;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#29305;&#23450;&#32452;&#20214;&#12290;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#22312;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#12289;&#24863;&#30693;&#12289;&#36816;&#21160;&#35268;&#21010;&#21644;&#25511;&#21046;&#31561;&#26041;&#38754;&#25198;&#28436;&#20102;&#37325;&#35201;&#35282;&#33394;&#12290;&#26410;&#26469;&#30340;&#25361;&#25112;&#21644;&#23545;&#23454;&#38469;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#24433;&#21709;&#20063;&#34987;&#35752;&#35770;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#31561;&#22522;&#30784;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#36890;&#36807;&#23545;&#22823;&#37327;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#20026;&#19981;&#21516;&#20219;&#21153;&#21644;&#27169;&#24577;&#30340;&#28789;&#27963;&#24212;&#29992;&#25552;&#20379;&#20102;&#20415;&#21033;&#12290;&#23427;&#20204;&#30340;&#24433;&#21709;&#28085;&#30422;&#20102;&#21253;&#25324;&#21307;&#30103;&#12289;&#25945;&#32946;&#21644;&#26426;&#22120;&#20154;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#24773;&#20917;&#65292;&#37325;&#28857;&#26159;&#26367;&#25442;&#29616;&#26377;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#29305;&#23450;&#32452;&#20214;&#12290;&#24635;&#32467;&#28085;&#30422;&#20102;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#20197;&#21450;&#23427;&#20204;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#30340;&#24863;&#30693;&#12289;&#36816;&#21160;&#35268;&#21010;&#21644;&#25511;&#21046;&#31561;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#26410;&#26469;&#25361;&#25112;&#21644;&#23545;&#23454;&#38469;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in foundation models, like Large Language Models (LLMs) and Vision-Language Models (VLMs), trained on extensive data, facilitate flexible application across different tasks and modalities. Their impact spans various fields, including healthcare, education, and robotics. This paper provides an overview of the practical application of foundation models in real-world robotics, with a primary emphasis on the replacement of specific components within existing robot systems. The summary encompasses the perspective of input-output relationships in foundation models, as well as their role in perception, motion planning, and control within the field of robotics. This paper concludes with a discussion of future challenges and implications for practical robot applications.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#20013;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#38544;&#24615;&#20559;&#24046;&#20197;&#21450;&#20854;&#25910;&#25947;&#36895;&#29575;&#65292;&#36890;&#36807;&#35777;&#26126;&#22312;&#29305;&#23450;&#25968;&#25454;&#35774;&#32622;&#19979;&#25910;&#25947;&#24615;&#26159;&#20840;&#23616;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;W_t&#21040;W_mm&#30340;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05738</link><description>&lt;p&gt;
&#38544;&#24615;&#20559;&#24046;&#19982;&#33258;&#27880;&#24847;&#21147;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Implicit Bias and Fast Convergence Rates for Self-attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05738
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#20013;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#38544;&#24615;&#20559;&#24046;&#20197;&#21450;&#20854;&#25910;&#25947;&#36895;&#29575;&#65292;&#36890;&#36807;&#35777;&#26126;&#22312;&#29305;&#23450;&#25968;&#25454;&#35774;&#32622;&#19979;&#25910;&#25947;&#24615;&#26159;&#20840;&#23616;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;W_t&#21040;W_mm&#30340;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#27880;&#24847;&#21147;&#26159;transformer&#30340;&#26680;&#24515;&#26426;&#21046;&#65292;&#23427;&#20351;&#20854;&#19982;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#26377;&#25152;&#21306;&#21035;&#65292;&#24182;&#39537;&#21160;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#24320;&#21457;&#33258;&#27880;&#24847;&#21147;&#30340;&#22522;&#26412;&#20248;&#21270;&#21407;&#21017;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#35757;&#32451;&#20855;&#26377;&#22266;&#23450;&#32447;&#24615;&#35299;&#30721;&#22120;&#30340;&#33258;&#27880;&#24847;&#21147;&#23618;&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#30340;&#38544;&#24615;&#20559;&#24046;&#12290;&#21463;&#21040;&#22312;&#21487;&#20998;&#31163;&#25968;&#25454;&#19978;&#32447;&#24615;&#36923;&#36753;&#22238;&#24402;&#20013;GD&#30340;&#30740;&#31350;&#21551;&#21457;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#38543;&#30528;&#36845;&#20195;&#27425;&#25968;t&#26080;&#38480;&#25509;&#36817;&#20110;&#26080;&#31351;&#22823;&#65292;&#38190;-&#26597;&#35810;&#30697;&#38453;W_t&#22312;&#23616;&#37096;&#19978;&#65288;&#30456;&#23545;&#20110;&#21021;&#22987;&#21270;&#26041;&#21521;&#65289;&#25910;&#25947;&#21040;&#19968;&#20010;&#30828;&#36793;&#30028;&#25903;&#25345;&#21521;&#37327;&#26426;&#35299;W_mm&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#22235;&#20010;&#26041;&#38754;&#22686;&#24378;&#20102;&#36825;&#20010;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#38750;&#24179;&#20961;&#30340;&#25968;&#25454;&#35774;&#32622;&#65292;&#23545;&#20110;&#36825;&#20123;&#35774;&#32622;&#65292;&#25910;&#25947;&#24615;&#26159;&#20840;&#23616;&#30340;&#65292;&#24182;&#25581;&#31034;&#20102;&#20248;&#21270;&#31354;&#38388;&#30340;&#29305;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20379;&#20102;W_t&#21040;W_mm&#30340;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#29575;&#65292;&#24182;&#37327;&#21270;&#20102;&#31232;&#30095;&#21270;&#30340;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-attention, the core mechanism of transformers, distinguishes them from traditional neural networks and drives their outstanding performance. Towards developing the fundamental optimization principles of self-attention, we investigate the implicit bias of gradient descent (GD) in training a self-attention layer with fixed linear decoder in binary classification. Drawing inspiration from the study of GD in linear logistic regression over separable data, recent work demonstrates that as the number of iterations $t$ approaches infinity, the key-query matrix $W_t$ converges locally (with respect to the initialization direction) to a hard-margin SVM solution $W_{mm}$. Our work enhances this result in four aspects. Firstly, we identify non-trivial data settings for which convergence is provably global, thus shedding light on the optimization landscape. Secondly, we provide the first finite-time convergence rate for $W_t$ to $W_{mm}$, along with quantifying the rate of sparsification in t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#24179;&#22343;&#22330;&#21338;&#24328;&#20013;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#37096;&#20998;&#22522;&#20110;&#27169;&#22411;&#30340;Eluder&#32500;&#24230;&#65288;P-MBED&#65289;&#27010;&#24565;&#26469;&#34913;&#37327;&#27169;&#22411;&#31867;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#35777;&#26126;&#22312;&#22522;&#26412;&#20551;&#35774;&#19979;&#65292;&#23398;&#20064;&#24179;&#22343;&#22330;&#21338;&#24328;&#30340;&#32435;&#20160;&#22343;&#34913;&#24182;&#19981;&#27604;&#35299;&#20915;&#23545;&#25968;&#20010;&#21333;&#20010;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#26356;&#20855;&#32479;&#35745;&#25361;&#25112;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05724</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#24179;&#22343;&#22330;&#21338;&#24328;&#20013;&#24182;&#19981;&#27604;&#21333;&#20010;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26356;&#21152;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Model-Based RL for Mean-Field Games is not Statistically Harder than Single-Agent RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#24179;&#22343;&#22330;&#21338;&#24328;&#20013;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#37096;&#20998;&#22522;&#20110;&#27169;&#22411;&#30340;Eluder&#32500;&#24230;&#65288;P-MBED&#65289;&#27010;&#24565;&#26469;&#34913;&#37327;&#27169;&#22411;&#31867;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#35777;&#26126;&#22312;&#22522;&#26412;&#20551;&#35774;&#19979;&#65292;&#23398;&#20064;&#24179;&#22343;&#22330;&#21338;&#24328;&#30340;&#32435;&#20160;&#22343;&#34913;&#24182;&#19981;&#27604;&#35299;&#20915;&#23545;&#25968;&#20010;&#21333;&#20010;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#26356;&#20855;&#32479;&#35745;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#24179;&#22343;&#22330;&#21338;&#24328;&#20013;&#22522;&#20110;&#27169;&#22411;&#30340;&#20989;&#25968;&#36924;&#36817;&#19979;&#24378;&#21270;&#23398;&#20064;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#35813;&#26041;&#27861;&#38656;&#35201;&#31574;&#30053;&#24615;&#25506;&#32034;&#20197;&#25214;&#21040;&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#37096;&#20998;&#22522;&#20110;&#27169;&#22411;&#30340;Eluder&#32500;&#24230;&#65288;P-MBED&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#27010;&#24565;&#26469;&#25551;&#36848;&#27169;&#22411;&#31867;&#22797;&#26434;&#24230;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;P-MBED&#21487;&#20197;&#34913;&#37327;&#20174;&#32473;&#23450;&#30340;&#24179;&#22343;&#22330;&#27169;&#22411;&#31867;&#36716;&#25442;&#32780;&#26469;&#30340;&#21333;&#20010;&#26234;&#33021;&#20307;&#27169;&#22411;&#31867;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#28508;&#22312;&#19978;&#21487;&#33021;&#27604;\citet{huang2023statistical}&#25552;&#20986;&#30340;MBED&#25351;&#25968;&#32423;&#20302;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#28040;&#38500;&#31639;&#27861;&#65292;&#20855;&#26377;&#26032;&#39062;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#24182;&#24314;&#31435;&#20102;&#19982;P-MBED&#30456;&#20851;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#32467;&#26524;&#65292;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22522;&#26412;&#21487;&#23454;&#29616;&#24615;&#21644;Lipschitz&#36830;&#32493;&#24615;&#20551;&#35774;&#19979;&#65292;&#23398;&#20064;&#24179;&#22343;&#22330;&#21338;&#24328;&#30340;&#32435;&#20160;&#22343;&#34913;&#24182;&#19981;&#27604;&#35299;&#20915;&#23545;&#25968;&#20010;&#21333;&#20010;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#26356;&#20855;&#32479;&#35745;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#25512;&#24191;&#21040;&#22810;&#31867;&#22411;&#24179;&#22343;&#22330;&#21338;&#24328;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the sample complexity of reinforcement learning (RL) in Mean-Field Games (MFGs) with model-based function approximation that requires strategic exploration to find a Nash Equilibrium policy. We introduce the Partial Model-Based Eluder Dimension (P-MBED), a more effective notion to characterize the model class complexity. Notably, P-MBED measures the complexity of the single-agent model class converted from the given mean-field model class, and potentially, can be exponentially lower than the MBED proposed by \citet{huang2023statistical}. We contribute a model elimination algorithm featuring a novel exploration strategy and establish sample complexity results polynomial w.r.t.~P-MBED. Crucially, our results reveal that, under the basic realizability and Lipschitz continuity assumptions, \emph{learning Nash Equilibrium in MFGs is no more statistically challenging than solving a logarithmic number of single-agent RL problems}. We further extend our results to Multi-Type MFGs, gen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#31105;&#27490;&#20219;&#21153;&#65292;&#21363;&#27169;&#22411;&#35774;&#35745;&#20026;&#25298;&#32477;&#22238;&#31572;&#30340;&#20219;&#21153;&#65292;&#25506;&#31350;&#20102;&#22312;&#26126;&#30830;&#35843;&#20248;&#27169;&#22411;&#25298;&#32477;&#31105;&#27490;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#37325;&#26032;&#23398;&#20064;&#31105;&#27490;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;ICL&#21487;&#20197;&#25104;&#21151;&#22320;&#25764;&#38144;&#23433;&#20840;&#22521;&#35757;&#65292;&#20174;&#32780;&#36896;&#25104;&#37325;&#22823;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2402.05723</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#21487;&#20197;&#37325;&#26032;&#23398;&#20064;&#31105;&#27490;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning Can Re-learn Forbidden Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#31105;&#27490;&#20219;&#21153;&#65292;&#21363;&#27169;&#22411;&#35774;&#35745;&#20026;&#25298;&#32477;&#22238;&#31572;&#30340;&#20219;&#21153;&#65292;&#25506;&#31350;&#20102;&#22312;&#26126;&#30830;&#35843;&#20248;&#27169;&#22411;&#25298;&#32477;&#31105;&#27490;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#37325;&#26032;&#23398;&#20064;&#31105;&#27490;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;ICL&#21487;&#20197;&#25104;&#21151;&#22320;&#25764;&#38144;&#23433;&#20840;&#22521;&#35757;&#65292;&#20174;&#32780;&#36896;&#25104;&#37325;&#22823;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23545;&#23433;&#20840;&#22521;&#35757;&#36827;&#34892;&#20102;&#22823;&#37327;&#25237;&#20837;&#65292;&#20294;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20173;&#28982;&#23384;&#22312;&#35768;&#22810;&#28431;&#27934;&#12290;&#26377;&#19968;&#31181;&#35266;&#28857;&#35748;&#20026;&#65292;LLM&#23433;&#20840;&#22521;&#35757;&#21487;&#20197;&#36890;&#36807;&#31639;&#27861;&#31105;&#27490;&#27169;&#22411;&#22238;&#31572;&#26377;&#27602;&#25110;&#26377;&#23475;&#30340;&#26597;&#35810;&#12290;&#20026;&#20102;&#35780;&#20272;&#23433;&#20840;&#22521;&#35757;&#30340;&#26377;&#25928;&#24615;&#65292;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#31105;&#27490;&#20219;&#21153;&#65292;&#21363;&#27169;&#22411;&#35774;&#35745;&#20026;&#25298;&#32477;&#22238;&#31572;&#30340;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26126;&#30830;&#35843;&#20248;&#27169;&#22411;&#25298;&#32477;&#31105;&#27490;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#37325;&#26032;&#23398;&#20064;&#31105;&#27490;&#20219;&#21153;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#19968;&#20010;&#25298;&#32477;&#24773;&#24863;&#20998;&#31867;&#30340;&#29609;&#20855;&#31034;&#20363;&#26469;&#28436;&#31034;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;ICL&#26469;&#22788;&#29702;&#19968;&#20010;&#34987;&#32454;&#21270;&#25298;&#32477;&#24635;&#32467;&#34394;&#26500;&#26032;&#38395;&#25991;&#31456;&#30340;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35843;&#26597;ICL&#26159;&#21542;&#21487;&#20197;&#25764;&#38144;&#23433;&#20840;&#22521;&#35757;&#65292;&#36825;&#21487;&#33021;&#20195;&#34920;&#30528;&#37325;&#22823;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#23545;&#20110;&#23433;&#20840;&#20219;&#21153;&#65292;&#25105;&#20204;&#26597;&#30475;&#20102;Vicuna-7B&#65292;Starling-7B&#21644;Llama2-7B&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#25915;&#20987;&#21487;&#20197;&#30452;&#25509;&#23545;Starlin&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant investment into safety training, large language models (LLMs) deployed in the real world still suffer from numerous vulnerabilities. One perspective on LLM safety training is that it algorithmically forbids the model from answering toxic or harmful queries. To assess the effectiveness of safety training, in this work, we study forbidden tasks, i.e., tasks the model is designed to refuse to answer. Specifically, we investigate whether in-context learning (ICL) can be used to re-learn forbidden tasks despite the explicit fine-tuning of the model to refuse them. We first examine a toy example of refusing sentiment classification to demonstrate the problem. Then, we use ICL on a model fine-tuned to refuse to summarise made-up news articles. Finally, we investigate whether ICL can undo safety training, which could represent a major security risk. For the safety task, we look at Vicuna-7B, Starling-7B, and Llama2-7B. We show that the attack works out-of-the-box on Starlin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23485;&#38544;&#34255;&#23618;&#26641;&#29366;&#31070;&#32463;&#32593;&#32476;&#30340;&#23481;&#37327;&#65292;&#37319;&#29992;&#20840;&#38754;&#25552;&#21319;&#30340;&#38543;&#26426;&#20108;&#37325;&#24615;&#29702;&#35770;(fl RDT)&#26469;&#23545;&#23485;(TCM)&#32593;&#32476;&#30340;&#23481;&#37327;&#36827;&#34892;&#21051;&#30011;&#65292;&#24471;&#21040;&#20102;&#19968;&#31867;&#36890;&#29992;&#28608;&#27963;&#20989;&#25968;&#30340;&#26174;&#24335;&#12289;&#38381;&#24335;&#23481;&#37327;&#35745;&#31639;&#20844;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.05719</link><description>&lt;p&gt;
&#23485;&#38544;&#34255;&#23618;&#26641;&#29366;&#31070;&#32463;&#32593;&#32476;&#20013;&#20855;&#26377;&#36890;&#29992;&#28608;&#27963;&#20989;&#25968;&#30340;&#23481;&#37327;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exact capacity of the \emph{wide} hidden layer treelike neural networks with generic activations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05719
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23485;&#38544;&#34255;&#23618;&#26641;&#29366;&#31070;&#32463;&#32593;&#32476;&#30340;&#23481;&#37327;&#65292;&#37319;&#29992;&#20840;&#38754;&#25552;&#21319;&#30340;&#38543;&#26426;&#20108;&#37325;&#24615;&#29702;&#35770;(fl RDT)&#26469;&#23545;&#23485;(TCM)&#32593;&#32476;&#30340;&#23481;&#37327;&#36827;&#34892;&#21051;&#30011;&#65292;&#24471;&#21040;&#20102;&#19968;&#31867;&#36890;&#29992;&#28608;&#27963;&#20989;&#25968;&#30340;&#26174;&#24335;&#12289;&#38381;&#24335;&#23481;&#37327;&#35745;&#31639;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20110;&#26641;&#29366;&#22996;&#21592;&#20250;&#26426;&#22120;(TCM)&#31070;&#32463;&#32593;&#32476;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#38543;&#26426;&#20108;&#37325;&#24615;&#29702;&#35770;(RDT)&#21450;&#20854;&#37096;&#20998;&#25552;&#21319;&#21464;&#31181;(pl RDT)&#26159;&#33021;&#22815;&#29992;&#20110;&#38750;&#24120;&#31934;&#30830;&#30340;&#32593;&#32476;&#23481;&#37327;&#20998;&#26512;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#23485;&#38544;&#34255;&#23618;&#32593;&#32476;&#65292;&#24182;&#21457;&#29616;\cite{Stojnictcmspnncapdiffactrdt23}&#20013;&#38754;&#20020;&#30340;&#26576;&#20123;&#25968;&#20540;&#22256;&#38590;&#22855;&#36857;&#33324;&#22320;&#28040;&#22833;&#20102;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#37319;&#29992;&#26368;&#36817;&#21457;&#23637;&#30340;&#20840;&#38754;&#25552;&#21319;(fl) RDT&#26469;&#34920;&#24449;&#23485;($d\rightarrow \infty$) TCM&#32593;&#32476;&#30340;&#23481;&#37327;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#31867;&#38750;&#24120;&#36890;&#29992;&#30340;&#38544;&#34255;&#23618;&#28608;&#27963;&#20989;&#25968;&#30340;&#26174;&#24335;&#12289;&#38381;&#24335;&#23481;&#37327;&#21051;&#30011;&#12290;&#23613;&#31649;&#25152;&#20351;&#29992;&#30340;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#25152;&#38656;&#30340;&#25968;&#20540;&#35780;&#20272;&#37327;&#65292;&#20294;&#26368;&#32456;&#30340;fl RDT&#30340;&#23454;&#29992;&#24615;&#21644;&#25104;&#21151;&#20173;&#28982;&#38656;&#35201;&#21487;&#38752;&#30340;&#25968;&#20540;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in studying \emph{treelike committee machines} (TCM) neural networks (NN) in \cite{Stojnictcmspnncaprdt23,Stojnictcmspnncapliftedrdt23,Stojnictcmspnncapdiffactrdt23} showed that the Random Duality Theory (RDT) and its a \emph{partially lifted}(pl RDT) variant are powerful tools that can be used for very precise networks capacity analysis. Here, we consider \emph{wide} hidden layer networks and uncover that certain aspects of numerical difficulties faced in \cite{Stojnictcmspnncapdiffactrdt23} miraculously disappear. In particular, we employ recently developed \emph{fully lifted} (fl) RDT to characterize the \emph{wide} ($d\rightarrow \infty$) TCM nets capacity. We obtain explicit, closed form, capacity characterizations for a very generic class of the hidden layer activations. While the utilized approach significantly lowers the amount of the needed numerical evaluations, the ultimate fl RDT usefulness and success still require a solid portion of the residual numerical 
&lt;/p&gt;</description></item><item><title>REMEDI&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#36827;&#31070;&#32463;&#29109;&#20272;&#35745;&#30340;&#26657;&#27491;&#36716;&#25442;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#21449;&#29109;&#26368;&#23567;&#21270;&#21644;&#30456;&#23545;&#29109;&#20272;&#35745;&#22522;&#27169;&#22411;&#30340;&#20559;&#24046;&#65292;&#25552;&#39640;&#20102;&#20272;&#35745;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05718</link><description>&lt;p&gt;
REMEDI: &#25913;&#36827;&#31070;&#32463;&#29109;&#20272;&#35745;&#30340;&#26657;&#27491;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
REMEDI: Corrective Transformations for Improved Neural Entropy Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05718
&lt;/p&gt;
&lt;p&gt;
REMEDI&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#36827;&#31070;&#32463;&#29109;&#20272;&#35745;&#30340;&#26657;&#27491;&#36716;&#25442;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#21449;&#29109;&#26368;&#23567;&#21270;&#21644;&#30456;&#23545;&#29109;&#20272;&#35745;&#22522;&#27169;&#22411;&#30340;&#20559;&#24046;&#65292;&#25552;&#39640;&#20102;&#20272;&#35745;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#35770;&#37327;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#25968;&#25454;&#21644;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#20351;&#24471;&#20934;&#30830;&#20272;&#35745;&#36825;&#20123;&#37327;&#30340;&#38656;&#27714;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#32500;&#24230;&#30340;&#22686;&#21152;&#65292;&#20272;&#35745;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#30456;&#23545;&#36739;&#20302;&#30340;&#32500;&#24230;&#20013;&#24050;&#32463;&#22256;&#38590;&#37325;&#37325;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;REMEDI&#65292;&#29992;&#20110;&#39640;&#25928;&#20934;&#30830;&#22320;&#20272;&#35745;&#24494;&#20998;&#29109;&#65292;&#19968;&#31181;&#22522;&#26412;&#30340;&#20449;&#24687;&#35770;&#37327;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#31616;&#21333;&#33258;&#36866;&#24212;&#22522;&#27169;&#22411;&#30340;&#20132;&#21449;&#29109;&#26368;&#23567;&#21270;&#21644;&#20854;&#30456;&#23545;&#29109;&#20174;&#25968;&#25454;&#23494;&#24230;&#20013;&#20272;&#35745;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#20272;&#35745;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#25913;&#36827;&#65292;&#21253;&#25324;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#33258;&#28982;&#25968;&#25454;&#30340;&#29109;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#37325;&#35201;&#30340;&#29702;&#35770;&#19968;&#33268;&#24615;&#32467;&#26524;&#25193;&#23637;&#21040;&#25105;&#20204;&#26041;&#27861;&#25152;&#38656;&#30340;&#26356;&#24191;&#20041;&#30340;&#35774;&#32622;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#25552;&#39640;&#29109;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information theoretic quantities play a central role in machine learning. The recent surge in the complexity of data and models has increased the demand for accurate estimation of these quantities. However, as the dimension grows the estimation presents significant challenges, with existing methods struggling already in relatively low dimensions. To address this issue, in this work, we introduce $\texttt{REMEDI}$ for efficient and accurate estimation of differential entropy, a fundamental information theoretic quantity. The approach combines the minimization of the cross-entropy for simple, adaptive base models and the estimation of their deviation, in terms of the relative entropy, from the data density. Our approach demonstrates improvement across a broad spectrum of estimation tasks, encompassing entropy estimation on both synthetic and natural data. Further, we extend important theoretical consistency results to a more generalized setting required by our approach. We illustrate how
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21327;&#21516;&#38750;&#21442;&#25968;&#30340;&#20004;&#26679;&#26412;&#26816;&#39564;&#65288;CTST&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26377;&#25928;&#21033;&#29992;&#20102;&#22270;&#32467;&#26500;&#21644;&#26368;&#23567;&#21270;&#20102;&#23545;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#20551;&#35774;&#65292;&#36890;&#36807;&#38598;&#25104;f-&#20998;&#24067;&#20272;&#35745;&#12289;&#26680;&#26041;&#27861;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#35201;&#28857;&#65292;&#23558;&#20256;&#32479;&#30340;&#22312;&#27599;&#20010;&#33410;&#28857;&#29420;&#31435;&#24212;&#29992;&#30340;&#26041;&#27861;&#20248;&#21270;&#65292;&#23545;&#22270;&#32467;&#26500;&#20013;&#30340;&#22810;&#20010;&#20004;&#26679;&#26412;&#26816;&#39564;&#38382;&#39064;&#36827;&#34892;&#20102;&#26356;&#22909;&#30340;&#22788;&#29702;&#21644;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.05715</link><description>&lt;p&gt;
&#21327;&#21516;&#38750;&#21442;&#25968;&#30340;&#20004;&#26679;&#26412;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Collaborative non-parametric two-sample testing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21327;&#21516;&#38750;&#21442;&#25968;&#30340;&#20004;&#26679;&#26412;&#26816;&#39564;&#65288;CTST&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26377;&#25928;&#21033;&#29992;&#20102;&#22270;&#32467;&#26500;&#21644;&#26368;&#23567;&#21270;&#20102;&#23545;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#20551;&#35774;&#65292;&#36890;&#36807;&#38598;&#25104;f-&#20998;&#24067;&#20272;&#35745;&#12289;&#26680;&#26041;&#27861;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#35201;&#28857;&#65292;&#23558;&#20256;&#32479;&#30340;&#22312;&#27599;&#20010;&#33410;&#28857;&#29420;&#31435;&#24212;&#29992;&#30340;&#26041;&#27861;&#20248;&#21270;&#65292;&#23545;&#22270;&#32467;&#26500;&#20013;&#30340;&#22810;&#20010;&#20004;&#26679;&#26412;&#26816;&#39564;&#38382;&#39064;&#36827;&#34892;&#20102;&#26356;&#22909;&#30340;&#22788;&#29702;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22270;&#32467;&#26500;&#35774;&#32622;&#20013;&#30340;&#22810;&#20010;&#20004;&#26679;&#26412;&#26816;&#39564;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36825;&#26159;&#31354;&#38388;&#32479;&#35745;&#21644;&#31070;&#32463;&#31185;&#23398;&#31561;&#39046;&#22495;&#20013;&#30340;&#24120;&#35265;&#24773;&#26223;&#12290;&#22312;&#22266;&#23450;&#22270;&#20013;&#30340;&#27599;&#20010;&#33410;&#28857;v&#37117;&#28041;&#21450;&#21040;&#20004;&#20010;&#29305;&#23450;&#33410;&#28857;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65288;pdfs&#65289;p_v&#21644;q_v&#20043;&#38388;&#30340;&#20004;&#26679;&#26412;&#26816;&#39564;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#20551;&#35774;&#36830;&#25509;&#30340;&#33410;&#28857;&#20250;&#20135;&#29983;&#31867;&#20284;&#30340;&#26816;&#39564;&#32467;&#26524;&#30340;&#26465;&#20214;&#19979;&#65292;&#30830;&#23450;&#24212;&#25298;&#32477;&#38646;&#20551;&#35774;p_v = q_v&#30340;&#33410;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38750;&#21442;&#25968;&#30340;&#21327;&#21516;&#20004;&#26679;&#26412;&#26816;&#39564;&#65288;CTST&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26377;&#25928;&#21033;&#29992;&#20102;&#22270;&#32467;&#26500;&#24182;&#26368;&#23567;&#21270;&#20102;&#23545;p_v&#21644;q_v&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#38598;&#25104;&#20102;f-&#20998;&#24067;&#20272;&#35745;&#12289;&#26680;&#26041;&#27861;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#35201;&#28857;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#23454;&#39564;&#21644;&#26816;&#27979;&#22320;&#38663;&#27963;&#21160;&#30340;&#30495;&#23454;&#20256;&#24863;&#22120;&#32593;&#32476;&#26469;&#35777;&#26126;CTST&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22312;&#27599;&#20010;&#33410;&#28857;&#29420;&#31435;&#24212;&#29992;&#30340;&#38750;&#21442;&#25968;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#65292;&#22240;&#27492;&#24573;&#35270;&#20102;&#22270;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the multiple two-sample test problem in a graph-structured setting, which is a common scenario in fields such as Spatial Statistics and Neuroscience. Each node $v$ in fixed graph deals with a two-sample testing problem between two node-specific probability density functions (pdfs), $p_v$ and $q_v$. The goal is to identify nodes where the null hypothesis $p_v = q_v$ should be rejected, under the assumption that connected nodes would yield similar test outcomes. We propose the non-parametric collaborative two-sample testing (CTST) framework that efficiently leverages the graph structure and minimizes the assumptions over $p_v$ and $q_v$. Our methodology integrates elements from f-divergence estimation, Kernel Methods, and Multitask Learning. We use synthetic experiments and a real sensor network detecting seismic activity to demonstrate that CTST outperforms state-of-the-art non-parametric statistical tests that apply at each node independently, hence disregard the g
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#38024;&#23545;&#29305;&#23450;&#20154;&#32676;&#30340;&#26631;&#31614;&#27745;&#26579;&#25915;&#20987;&#26469;&#30772;&#22351;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#23545;&#25239;&#24615;&#30340;&#35786;&#26029;&#19981;&#36275;&#20559;&#35265;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#20154;&#32676;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#23545;&#20110;&#19981;&#21487;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#30452;&#25509;&#30456;&#20851;&#12290;</title><link>https://arxiv.org/abs/2402.05713</link><description>&lt;p&gt;
&#26126;&#26126;&#23601;&#22312;&#30524;&#21069;&#65306;&#23545;&#24369;&#21183;&#24739;&#32773;&#32676;&#20307;&#36827;&#34892;&#19981;&#21487;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable Patient Populations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05713
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#38024;&#23545;&#29305;&#23450;&#20154;&#32676;&#30340;&#26631;&#31614;&#27745;&#26579;&#25915;&#20987;&#26469;&#30772;&#22351;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#23545;&#25239;&#24615;&#30340;&#35786;&#26029;&#19981;&#36275;&#20559;&#35265;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#20154;&#32676;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#23545;&#20110;&#19981;&#21487;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#30452;&#25509;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#25918;&#23556;&#23398;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21152;&#21095;&#23545;&#24369;&#21183;&#24739;&#32773;&#32676;&#20307;&#30340;&#20020;&#24202;&#20559;&#35265;&#30340;&#39118;&#38505;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25152;&#23637;&#31034;&#30340;&#20559;&#35265;&#30340;&#37327;&#21270;&#65292;&#20294;&#38024;&#23545;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;&#20197;&#21450;&#20854;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#24433;&#21709;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#38024;&#23545;&#20154;&#21475;&#32479;&#35745;&#23398;&#26631;&#31614;&#30340;&#27602;&#21270;&#25915;&#20987;&#21487;&#20197;&#21521;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24341;&#20837;&#23545;&#25239;&#24615;&#30340;&#35786;&#26029;&#19981;&#36275;&#20559;&#35265;&#65292;&#24182;&#22312;&#19981;&#24433;&#21709;&#25972;&#20307;&#27169;&#22411;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#38477;&#20302;&#23545;&#34987;&#20302;&#20272;&#32676;&#20307;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#22810;&#20010;&#24615;&#33021;&#25351;&#26631;&#21644;&#20154;&#21475;&#32676;&#20307;&#65288;&#22914;&#24615;&#21035;&#12289;&#24180;&#40836;&#20197;&#21450;&#20854;&#20132;&#21449;&#23376;&#32676;&#65289;&#19978;&#34920;&#26126;&#65292;&#32676;&#20307;&#23545;&#20110;&#19981;&#21487;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#19982;&#20854;&#22312;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#24449;&#30452;&#25509;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of artificial intelligence (AI) in radiology has shed light on the risk of deep learning (DL) models exacerbating clinical biases towards vulnerable patient populations. While prior literature has focused on quantifying biases exhibited by trained DL models, demographically targeted adversarial bias attacks on DL models and its implication in the clinical environment remains an underexplored field of research in medical imaging. In this work, we demonstrate that demographically targeted label poisoning attacks can introduce adversarial underdiagnosis bias in DL models and degrade performance on underrepresented groups without impacting overall model performance. Moreover, our results across multiple performance metrics and demographic groups like sex, age, and their intersectional subgroups indicate that a group's vulnerability to undetectable adversarial bias attacks is directly correlated with its representation in the model's training data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#20154;-&#26426;&#32452;&#21512;&#20013;&#30340;&#24615;&#33021;&#25552;&#20986;&#20102;&#31163;&#32447;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#30340;&#24314;&#27169;&#65292;&#35299;&#20915;&#20102;&#22810;&#26679;&#21270;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.05703</link><description>&lt;p&gt;
&#31163;&#32447;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#65292;&#20197;&#25552;&#39640;&#20154;-&#26426;&#32452;&#21512;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Offline Risk-sensitive RL with Partial Observability to Enhance Performance in Human-Robot Teaming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#20154;-&#26426;&#32452;&#21512;&#20013;&#30340;&#24615;&#33021;&#25552;&#20986;&#20102;&#31163;&#32447;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#30340;&#24314;&#27169;&#65292;&#35299;&#20915;&#20102;&#22810;&#26679;&#21270;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#29983;&#29702;&#35745;&#31639;&#25972;&#21512;&#21040;&#28151;&#21512;&#20513;&#35758;&#30340;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#23558;&#23454;&#26102;&#29305;&#24449;&#20316;&#20026;&#20154;&#31867;&#29366;&#24577;&#35266;&#27979;&#34701;&#20837;&#20915;&#31574;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#33258;&#20027;&#20219;&#21153;&#20998;&#37197;&#20013;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#20248;&#21183;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#26234;&#33021;&#22320;&#20998;&#37197;&#20219;&#21153;&#26469;&#20943;&#36731;&#20154;&#31867;&#25805;&#20316;&#21592;&#30340;&#35748;&#30693;&#36127;&#33655;&#12290;&#28982;&#32780;&#65292;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#29983;&#29702;&#21644;&#34892;&#20026;&#27979;&#37327;&#32467;&#26524;&#30340;&#22810;&#26679;&#21270;&#20154;&#31867;&#21442;&#19982;&#32773;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24517;&#39035;&#21033;&#29992;&#27010;&#29575;&#26694;&#26550;&#65292;&#32771;&#34385;&#21040;&#20154;&#31867;&#29366;&#24577;&#30340;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24314;&#35758;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;ORL&#65289;&#26041;&#27861;&#20174;&#20808;&#21069;&#25910;&#38598;&#30340;&#32463;&#39564;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#19968;&#20010;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#27169;&#22411;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19981;&#20165;&#24378;&#35843;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#34920;&#31034;&#30340;&#28508;&#21147;&#65292;
&lt;/p&gt;
&lt;p&gt;
The integration of physiological computing into mixed-initiative human-robot interaction systems offers valuable advantages in autonomous task allocation by incorporating real-time features as human state observations into the decision-making system. This approach may alleviate the cognitive load on human operators by intelligently allocating mission tasks between agents. Nevertheless, accommodating a diverse pool of human participants with varying physiological and behavioral measurements presents a substantial challenge. To address this, resorting to a probabilistic framework becomes necessary, given the inherent uncertainty and partial observability on the human's state. Recent research suggests to learn a Partially Observable Markov Decision Process (POMDP) model from a data set of previously collected experiences that can be solved using Offline Reinforcement Learning (ORL) methods. In the present work, we not only highlight the potential of partially observable representations an
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#26641;&#29366;&#31070;&#32463;&#32593;&#32476;&#30340;&#23481;&#37327;&#65292;&#22522;&#20110;Random Duality Theory&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#23481;&#37327;&#20998;&#26512;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#20854;&#20182;&#31867;&#22411;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#22914;&#20108;&#27425;&#21644;ReLU&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.05696</link><description>&lt;p&gt;
&#22266;&#23450;&#23485;&#24230;&#30340;&#26641;&#29366;&#31070;&#32463;&#32593;&#32476;&#23481;&#37327;&#20998;&#26512;-&#36890;&#29992;&#28608;&#27963;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Fixed width treelike neural networks capacity analysis -- generic activations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05696
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#26641;&#29366;&#31070;&#32463;&#32593;&#32476;&#30340;&#23481;&#37327;&#65292;&#22522;&#20110;Random Duality Theory&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#23481;&#37327;&#20998;&#26512;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#20854;&#20182;&#31867;&#22411;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#22914;&#20108;&#27425;&#21644;ReLU&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#26641;&#29366;&#22996;&#21592;&#20250;&#26426;&#65288;TCM&#65289;&#31070;&#32463;&#32593;&#32476;&#30340;&#23481;&#37327;&#12290;&#22522;&#20110;&#38543;&#26426;&#23545;&#20598;&#29702;&#35770;&#65288;RDT&#65289;&#65292;\cite{Stojnictcmspnncaprdt23}&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#29992;&#20110;&#23427;&#20204;&#30340;&#23481;&#37327;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#22312;\cite{Stojnictcmspnncapliftedrdt23}&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25152;&#35859;&#30340;&#8220;&#37096;&#20998;&#25552;&#21319;&#8221;RDT&#65288;pl RDT&#65289;&#30340;&#21319;&#32423;&#29256;&#26412;&#12290;&#36825;&#20004;&#20010;&#24037;&#20316;&#26041;&#21521;&#37117;&#30528;&#37325;&#20110;&#20855;&#26377;&#26368;&#20856;&#22411;&#30340;&#8220;&#31526;&#21495;&#8221;&#28608;&#27963;&#20989;&#25968;&#30340;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20855;&#26377;&#20854;&#20182;&#26356;&#19968;&#33324;&#31867;&#22411;&#28608;&#27963;&#20989;&#25968;&#30340;&#32593;&#32476;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;\cite{Stojnictcmspnncaprdt23,Stojnictcmspnncapliftedrdt23}&#30340;&#26694;&#26550;&#36275;&#22815;&#24378;&#22823;&#65292;&#21487;&#20197;&#22788;&#29702;&#36825;&#20123;&#24773;&#20917;&#12290;&#38500;&#20102;&#26631;&#20934;&#30340;&#8220;&#32447;&#24615;&#8221;&#28608;&#27963;&#20989;&#25968;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#21363;&#8220;&#20108;&#27425;&#8221;&#21644;&#8220;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#8221;&#65292;&#21487;&#20197;&#24471;&#21040;&#29305;&#21035;&#26041;&#20415;&#30340;&#32467;&#26524;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#23545;&#20110;&#27599;&#20010;&#28608;&#27963;&#20989;&#25968;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;
&lt;/p&gt;
&lt;p&gt;
We consider the capacity of \emph{treelike committee machines} (TCM) neural networks. Relying on Random Duality Theory (RDT), \cite{Stojnictcmspnncaprdt23} recently introduced a generic framework for their capacity analysis. An upgrade based on the so-called \emph{partially lifted} RDT (pl RDT) was then presented in \cite{Stojnictcmspnncapliftedrdt23}. Both lines of work focused on the networks with the most typical, \emph{sign}, activations. Here, on the other hand, we focus on networks with other, more general, types of activations and show that the frameworks of \cite{Stojnictcmspnncaprdt23,Stojnictcmspnncapliftedrdt23} are sufficiently powerful to enable handling of such scenarios as well. In addition to the standard \emph{linear} activations, we uncover that particularly convenient results can be obtained for two very commonly used activations, namely, the \emph{quadratic} and \emph{rectified linear unit (ReLU)} ones. In more concrete terms, for each of these activations, we obtai
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#31867;&#21035;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#38480;&#26399;&#24179;&#22343;&#22238;&#25253;&#22909;&#36716;&#32966;&#20882;&#38505;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21333;&#33218;&#26494;&#24347;&#38382;&#39064;&#26159;Unichain&#21644;&#38750;&#21608;&#26399;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#31574;&#30053;&#31867;&#21035;&#20855;&#26377;&#28176;&#36827;&#26368;&#20248;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05689</link><description>&lt;p&gt;
Unichain&#21644;&#38750;&#21608;&#26399;&#24615;&#36275;&#20197;&#20445;&#35777;&#24179;&#22343;&#22238;&#25253;&#22909;&#36716;&#32966;&#20882;&#38505;&#30446;&#26631;&#30340;&#28176;&#36827;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Unichain and Aperiodicity are Sufficient for Asymptotic Optimality of Average-Reward Restless Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05689
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#31867;&#21035;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#38480;&#26399;&#24179;&#22343;&#22238;&#25253;&#22909;&#36716;&#32966;&#20882;&#38505;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21333;&#33218;&#26494;&#24347;&#38382;&#39064;&#26159;Unichain&#21644;&#38750;&#21608;&#26399;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#31574;&#30053;&#31867;&#21035;&#20855;&#26377;&#28176;&#36827;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#31163;&#25955;&#26102;&#38388;&#19979;&#30340;&#26080;&#38480;&#26399;&#24179;&#22343;&#22238;&#25253;&#30340;&#22909;&#36716;&#32966;&#20882;&#38505;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#31867;&#21035;&#65292;&#26088;&#22312;&#23558;&#36880;&#28176;&#25193;&#22823;&#30340;&#33218;&#23376;&#38598;&#21521;&#26368;&#20339;&#20998;&#24067;&#26041;&#21521;&#25512;&#36827;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31574;&#30053;&#22312;N&#33218;&#38382;&#39064;&#20013;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#65292;&#22914;&#26524;&#21333;&#33218;&#26494;&#24347;&#38382;&#39064;&#26159;Unichain&#21644;&#38750;&#21608;&#26399;&#24615;&#30340;&#65292;&#37027;&#20040;&#23601;&#20250;&#26377;&#19968;&#20010;$O(1/\sqrt{N})$&#30340;&#26368;&#20248;&#38388;&#38553;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21516;&#20110;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#65292;&#36825;&#20123;&#30740;&#31350;&#20391;&#37325;&#20110;&#25351;&#25968;&#25110;&#20248;&#20808;&#32423;&#31574;&#30053;&#65292;&#36825;&#20123;&#31574;&#30053;&#20381;&#36182;&#20110;&#32479;&#19968;&#20840;&#23616;&#21560;&#24341;&#23376;&#23646;&#24615;&#65288;UGAP&#65289;&#26469;&#20445;&#35777;&#25910;&#25947;&#21040;&#26368;&#20248;&#65292;&#25110;&#32773;&#20381;&#36182;&#20110;&#26368;&#36817;&#24320;&#21457;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#35201;&#27714;&#36981;&#24490;&#21516;&#27493;&#20551;&#35774;&#65288;SA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the infinite-horizon, average-reward restless bandit problem in discrete time. We propose a new class of policies that are designed to drive a progressively larger subset of arms toward the optimal distribution. We show that our policies are asymptotically optimal with an $O(1/\sqrt{N})$ optimality gap for an $N$-armed problem, provided that the single-armed relaxed problem is unichain and aperiodic. Our approach departs from most existing work that focuses on index or priority policies, which rely on the Uniform Global Attractor Property (UGAP) to guarantee convergence to the optimum, or a recently developed simulation-based policy, which requires a Synchronization Assumption (SA).
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31163;&#25955;&#21270;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#34920;&#26684;&#25968;&#25454;&#20013;&#35745;&#31639;&#20986;&#20934;&#30830;&#21448;&#26131;&#35299;&#37322;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24230;&#19978;&#19982;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#31561;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#23454;&#38469;&#19978;&#36229;&#36807;&#20102;&#21442;&#32771;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05680</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#21270;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#21487;&#35299;&#37322;&#24615;&#34920;&#26684;&#25968;&#25454;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Interpretable classifiers for tabular data via discretization and feature selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05680
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#21270;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#34920;&#26684;&#25968;&#25454;&#20013;&#35745;&#31639;&#20986;&#20934;&#30830;&#21448;&#26131;&#35299;&#37322;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24230;&#19978;&#19982;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#31561;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#23454;&#38469;&#19978;&#36229;&#36807;&#20102;&#21442;&#32771;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20174;&#34920;&#26684;&#25968;&#25454;&#20013;&#35745;&#31639;&#20986;&#20855;&#26377;&#35299;&#37322;&#24615;&#19988;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;&#25152;&#24471;&#21040;&#30340;&#20998;&#31867;&#22120;&#26159;&#31616;&#30701;&#30340;DNF&#20844;&#24335;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#25968;&#25454;&#31163;&#25955;&#21270;&#20026;&#24067;&#23572;&#24418;&#24335;&#65292;&#28982;&#21518;&#20351;&#29992;&#29305;&#24449;&#36873;&#25321;&#32467;&#21512;&#38750;&#24120;&#24555;&#36895;&#30340;&#31639;&#27861;&#26469;&#20135;&#29983;&#26368;&#20339;&#30340;&#24067;&#23572;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;14&#20010;&#23454;&#39564;&#26469;&#28436;&#31034;&#35813;&#26041;&#27861;&#65292;&#24471;&#21040;&#30340;&#32467;&#26524;&#30340;&#20934;&#30830;&#24230;&#20027;&#35201;&#19982;&#38543;&#26426;&#26862;&#26519;&#12289;XGBoost&#20197;&#21450;&#25991;&#29486;&#20013;&#30456;&#21516;&#25968;&#25454;&#38598;&#30340;&#29616;&#26377;&#32467;&#26524;&#30456;&#20284;&#12290;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#38469;&#19978;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#21442;&#32771;&#32467;&#26524;&#65292;&#23613;&#31649;&#25105;&#20204;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#30340;&#21363;&#26102;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19968;&#20010;&#20851;&#20110;&#20174;&#29616;&#23454;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340;&#20998;&#31867;&#22120;&#19982;&#26469;&#33258;&#25968;&#25454;&#32972;&#26223;&#20998;&#24067;&#30340;&#26368;&#20339;&#20998;&#31867;&#22120;&#30456;&#23545;&#24212;&#30340;&#27010;&#29575;&#30340;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method for computing immediately human interpretable yet accurate classifiers from tabular data. The classifiers obtained are short DNF-formulas, computed via first discretizing the original data to Boolean form and then using feature selection coupled with a very fast algorithm for producing the best possible Boolean classifier for the setting. We demonstrate the approach via 14 experiments, obtaining results with accuracies mainly similar to ones obtained via random forests, XGBoost, and existing results for the same datasets in the literature. In several cases, our approach in fact outperforms the reference results in relation to accuracy, even though the main objective of our study is the immediate interpretability of our classifiers. We also prove a new result on the probability that the classifier we obtain from real-life data corresponds to the ideally best classifier with respect to the background distribution the data comes from.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21387;&#32553;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#23545;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#25552;&#39640;&#25968;&#25454;&#38598;&#21387;&#32553;&#25928;&#29575;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05675</link><description>&lt;p&gt;
&#21387;&#32553;&#25968;&#25454;&#38598;&#30340;&#23545;&#25239;&#35757;&#32451;&#26159;&#21542;&#26377;&#25928;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Adversarial Training with Compressed Datasets Effective?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21387;&#32553;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#23545;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#25552;&#39640;&#25968;&#25454;&#38598;&#21387;&#32553;&#25928;&#29575;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#21387;&#32553;&#65288;DC&#65289;&#26159;&#25351;&#20174;&#36739;&#22823;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#36739;&#23567;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#19968;&#31867;&#26368;&#36817;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#12290;&#36825;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#20445;&#30041;&#20102;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#22522;&#26412;&#20449;&#24687;&#65292;&#20351;&#24471;&#22312;&#20854;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#19982;&#22312;&#23436;&#25972;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#30340;DC&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22914;&#20309;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#39044;&#31639;&#19979;&#23454;&#29616;&#39640;&#27979;&#35797;&#24615;&#33021;&#65292;&#24182;&#27809;&#26377;&#30452;&#25509;&#35299;&#20915;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21387;&#32553;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#23545;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#20174;DC&#26041;&#27861;&#33719;&#24471;&#30340;&#21387;&#32553;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#27809;&#26377;&#26377;&#25928;&#30340;&#20256;&#36882;&#24615;&#12290;&#20026;&#20102;&#21516;&#26102;&#25552;&#39640;&#25968;&#25454;&#38598;&#21387;&#32553;&#25928;&#29575;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23547;&#25214;&#25968;&#25454;&#38598;&#30340;&#26368;&#23567;&#26377;&#38480;&#35206;&#30422;&#65288;MFC&#65289;&#30340;&#26032;&#22411;&#40065;&#26834;&#24615;&#24863;&#30693;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset Condensation (DC) refers to the recent class of dataset compression methods that generate a smaller, synthetic, dataset from a larger dataset. This synthetic dataset retains the essential information of the original dataset, enabling models trained on it to achieve performance levels comparable to those trained on the full dataset. Most current DC methods have mainly concerned with achieving high test performance with limited data budget, and have not directly addressed the question of adversarial robustness. In this work, we investigate the impact of adversarial robustness on models trained with compressed datasets. We show that the compressed datasets obtained from DC methods are not effective in transferring adversarial robustness to models. As a solution to improve dataset compression efficiency and adversarial robustness simultaneously, we propose a novel robustness-aware dataset compression method based on finding the Minimal Finite Covering (MFC) of the dataset. The prop
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#27169;&#22411;&#20013;&#30340;&#23545;&#25239;&#35757;&#32451;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#22788;&#29702;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#24182;&#32473;&#20986;&#20102;&#23545;&#25239;&#24615;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#30340;&#20805;&#20998;&#32479;&#35745;&#30340;&#31934;&#30830;&#28176;&#36817;&#25551;&#36848;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#23384;&#22312;&#21487;&#20197;&#38450;&#24481;&#32780;&#19981;&#24809;&#32602;&#20934;&#30830;&#24615;&#30340;&#26041;&#21521;&#65292;&#25581;&#31034;&#20102;&#38450;&#24481;&#38750;&#40065;&#26834;&#29305;&#24449;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.05674</link><description>&lt;p&gt;
&#39640;&#32500;&#27169;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#65306;&#20960;&#20309;&#21644;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
A High Dimensional Model for Adversarial Training: Geometry and Trade-Offs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#27169;&#22411;&#20013;&#30340;&#23545;&#25239;&#35757;&#32451;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#22788;&#29702;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#24182;&#32473;&#20986;&#20102;&#23545;&#25239;&#24615;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#30340;&#20805;&#20998;&#32479;&#35745;&#30340;&#31934;&#30830;&#28176;&#36817;&#25551;&#36848;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#23384;&#22312;&#21487;&#20197;&#38450;&#24481;&#32780;&#19981;&#24809;&#32602;&#20934;&#30830;&#24615;&#30340;&#26041;&#21521;&#65292;&#25581;&#31034;&#20102;&#38450;&#24481;&#38750;&#40065;&#26834;&#29305;&#24449;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#21363;&#32500;&#24230;$d$&#21644;&#25968;&#25454;&#28857;&#25968;$n$&#19982;&#22266;&#23450;&#27604;&#20363;$\alpha = n / d$&#21457;&#25955;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;&#36793;&#38469;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#20013;&#30340;&#23545;&#25239;&#35757;&#32451;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#22788;&#29702;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#21487;&#20197;&#30740;&#31350;&#25968;&#25454;&#21644;&#23545;&#25239;&#25915;&#20987;&#32773;&#20960;&#20309;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21516;&#26102;&#25429;&#25417;&#21040;&#23545;&#25239;&#40065;&#26834;&#24615;&#25991;&#29486;&#20013;&#35266;&#23519;&#21040;&#30340;&#26680;&#24515;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#29702;&#35770;&#36129;&#29486;&#26159;&#22312;&#36890;&#29992;&#30340;&#20984;&#19988;&#38750;&#36882;&#22686;&#25439;&#22833;&#20989;&#25968;&#19979;&#65292;&#23545;&#20110;&#23545;&#25239;&#24615;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#30340;&#20805;&#20998;&#32479;&#35745;&#30340;&#31934;&#30830;&#28176;&#36817;&#25551;&#36848;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20351;&#25105;&#20204;&#33021;&#22815;&#31934;&#30830;&#22320;&#21051;&#30011;&#25968;&#25454;&#20013;&#19982;&#26356;&#39640;&#30340;&#27867;&#21270;/&#40065;&#26834;&#24615;&#26435;&#34913;&#30456;&#20851;&#30340;&#26041;&#21521;&#65292;&#30001;&#19968;&#20010;&#40065;&#26834;&#24615;&#24230;&#37327;&#21644;&#19968;&#20010;&#26377;&#29992;&#24615;&#24230;&#37327;&#23450;&#20041;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23384;&#22312;&#19968;&#20123;&#26041;&#21521;&#65292;&#21487;&#20197;&#36827;&#34892;&#38450;&#24481;&#32780;&#19981;&#24809;&#32602;&#20934;&#30830;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38450;&#24481;&#38750;&#40065;&#26834;&#29305;&#24449;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates adversarial training in the context of margin-based linear classifiers in the high-dimensional regime where the dimension $d$ and the number of data points $n$ diverge with a fixed ratio $\alpha = n / d$. We introduce a tractable mathematical model where the interplay between the data and adversarial attacker geometries can be studied, while capturing the core phenomenology observed in the adversarial robustness literature. Our main theoretical contribution is an exact asymptotic description of the sufficient statistics for the adversarial empirical risk minimiser, under generic convex and non-increasing losses. Our result allow us to precisely characterise which directions in the data are associated with a higher generalisation/robustness trade-off, as defined by a robustness and a usefulness metric. In particular, we unveil the existence of directions which can be defended without penalising accuracy. Finally, we show the advantage of defending non-robust featu
&lt;/p&gt;</description></item><item><title>&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#32469;&#36807;&#23433;&#20840;&#25514;&#26045;&#30340;&#19981;&#31283;&#23450;&#28431;&#27934;&#12290;&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#23545;&#22810;&#31181;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#22823;&#35268;&#27169;&#27979;&#37327;&#65292;&#23454;&#39564;&#35777;&#26126;&#20248;&#21270;&#30340;&#36234;&#29425;&#25552;&#31034;&#33021;&#22815;&#25345;&#32493;&#36798;&#21040;&#26368;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05668</link><description>&lt;p&gt;
&#23545;LLMs&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Assessment of Jailbreak Attacks Against LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05668
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#32469;&#36807;&#23433;&#20840;&#25514;&#26045;&#30340;&#19981;&#31283;&#23450;&#28431;&#27934;&#12290;&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#23545;&#22810;&#31181;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#22823;&#35268;&#27169;&#27979;&#37327;&#65292;&#23454;&#39564;&#35777;&#26126;&#20248;&#21270;&#30340;&#36234;&#29425;&#25552;&#31034;&#33021;&#22815;&#25345;&#32493;&#36798;&#21040;&#26368;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#28389;&#29992;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#37319;&#21462;&#20102;&#23433;&#20840;&#25514;&#26045;&#20197;&#30830;&#20445;LLMs&#31526;&#21512;&#31038;&#20250;&#20262;&#29702;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#31181;&#32469;&#36807;LLMs&#23433;&#20840;&#25514;&#26045;&#30340;&#19981;&#31283;&#23450;&#28431;&#27934;&#65292;&#34987;&#31216;&#20026;&#36234;&#29425;&#25915;&#20987;&#12290;&#36890;&#36807;&#24212;&#29992;&#25216;&#26415;&#65292;&#22914;&#35282;&#33394;&#25198;&#28436;&#22330;&#26223;&#12289;&#23545;&#25239;&#24615;&#26679;&#26412;&#25110;&#23545;&#23433;&#20840;&#30446;&#26631;&#30340;&#24494;&#22937;&#30772;&#22351;&#20316;&#20026;&#25552;&#31034;&#65292;LLMs&#21487;&#20197;&#20135;&#29983;&#19981;&#36866;&#24403;&#29978;&#33267;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#34429;&#28982;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#30740;&#31350;&#20102;&#20960;&#31181;&#36234;&#29425;&#25915;&#20987;&#30340;&#31867;&#21035;&#65292;&#20294;&#20182;&#20204;&#37117;&#26159;&#23396;&#31435;&#22320;&#36827;&#34892;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#21508;&#31181;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#30340;&#39318;&#27425;&#22823;&#35268;&#27169;&#27979;&#37327;&#12290;&#25105;&#20204;&#38598;&#20013;&#22312;&#26469;&#33258;&#22235;&#20010;&#31867;&#21035;&#30340;13&#31181;&#23574;&#31471;&#36234;&#29425;&#26041;&#27861;&#12289;16&#31181;&#36829;&#35268;&#31867;&#21035;&#30340;160&#20010;&#38382;&#39064;&#20197;&#21450;&#20845;&#31181;&#27969;&#34892;&#30340;LLMs&#19978;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20248;&#21270;&#30340;&#36234;&#29425;&#25552;&#31034;&#22987;&#32456;&#33021;&#22815;&#36798;&#21040;&#26368;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#34920;&#29616;&#20986;...
&lt;/p&gt;
&lt;p&gt;
Misuse of the Large Language Models (LLMs) has raised widespread concern. To address this issue, safeguards have been taken to ensure that LLMs align with social ethics. However, recent findings have revealed an unsettling vulnerability bypassing the safeguards of LLMs, known as jailbreak attacks. By applying techniques, such as employing role-playing scenarios, adversarial examples, or subtle subversion of safety objectives as a prompt, LLMs can produce an inappropriate or even harmful response. While researchers have studied several categories of jailbreak attacks, they have done so in isolation. To fill this gap, we present the first large-scale measurement of various jailbreak attack methods. We concentrate on 13 cutting-edge jailbreak methods from four categories, 160 questions from 16 violation categories, and six popular LLMs. Our extensive experimental results demonstrate that the optimized jailbreak prompts consistently achieve the highest attack success rates, as well as exhi
&lt;/p&gt;</description></item><item><title>S$\Omega$I&#26159;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#37327;&#24230;&#65292;&#21487;&#20197;&#35745;&#31639;&#22810;&#21464;&#37327;&#31995;&#32479;&#20013;&#30340;&#21327;&#21516;-&#20887;&#20313;&#24179;&#34913;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#37327;&#24230;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.05667</link><description>&lt;p&gt;
S$\Omega$I: &#22522;&#20110;&#20998;&#25968;&#30340;O-INFORMATION&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
S$\Omega$I: Score-based O-INFORMATION Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05667
&lt;/p&gt;
&lt;p&gt;
S$\Omega$I&#26159;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#37327;&#24230;&#65292;&#21487;&#20197;&#35745;&#31639;&#22810;&#21464;&#37327;&#31995;&#32479;&#20013;&#30340;&#21327;&#21516;-&#20887;&#20313;&#24179;&#34913;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#37327;&#24230;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#25968;&#25454;&#21644;&#22797;&#26434;&#30340;&#22810;&#21464;&#37327;&#31995;&#32479;&#30340;&#20998;&#26512;&#38656;&#35201;&#25429;&#25417;&#22810;&#20010;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#20851;&#31995;&#30340;&#20449;&#24687;&#37327;&#12290;&#26368;&#36817;&#65292;&#26032;&#30340;&#20449;&#24687;&#35770;&#37327;&#24230;&#24050;&#34987;&#21457;&#23637;&#20986;&#26469;&#65292;&#20197;&#20811;&#26381;&#20256;&#32479;&#37327;&#24230;&#65288;&#22914;&#20114;&#20449;&#24687;&#65289;&#30340;&#23616;&#38480;&#24615;&#65292;&#21518;&#32773;&#21482;&#32771;&#34385;&#25104;&#23545;&#20132;&#20114;&#20316;&#29992;&#12290;&#20854;&#20013;&#65292;&#20449;&#24687;&#21327;&#21516;&#21644;&#20887;&#20313;&#30340;&#27010;&#24565;&#23545;&#20110;&#29702;&#35299;&#21464;&#37327;&#20043;&#38388;&#30340;&#39640;&#38454;&#20381;&#36182;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#36825;&#19968;&#27010;&#24565;&#30340;&#26368;&#33879;&#21517;&#21644;&#22810;&#29992;&#36884;&#30340;&#37327;&#24230;&#20043;&#19968;&#26159;O-information&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#28165;&#26224;&#19988;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#26469;&#37327;&#21270;&#22810;&#21464;&#37327;&#31995;&#32479;&#20013;&#30340;&#21327;&#21516;-&#20887;&#20313;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#38480;&#20110;&#31616;&#21270;&#24773;&#20917;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;S$\Omega$I&#65292;&#35813;&#26041;&#27861;&#39318;&#27425;&#20801;&#35768;&#35745;&#31639;O-information&#32780;&#19981;&#21463;&#23545;&#31995;&#32479;&#30340;&#38480;&#21046;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;&#20102;S$&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The analysis of scientific data and complex multivariate systems requires information quantities that capture relationships among multiple random variables. Recently, new information-theoretic measures have been developed to overcome the shortcomings of classical ones, such as mutual information, that are restricted to considering pairwise interactions. Among them, the concept of information synergy and redundancy is crucial for understanding the high-order dependencies between variables. One of the most prominent and versatile measures based on this concept is O-information, which provides a clear and scalable way to quantify the synergy-redundancy balance in multivariate systems. However, its practical application is limited to simplified cases. In this work, we introduce S$\Omega$I, which allows for the first time to compute O-information without restrictive assumptions about the system. Our experiments validate our approach on synthetic data, and demonstrate the effectiveness of S$
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#23454;&#26102;&#20013;&#23610;&#24230;&#20132;&#36890;&#39044;&#27979;&#20013;&#20855;&#26377;&#26368;&#20808;&#36827;&#25928;&#26524;&#30340;&#28145;&#24230;&#39044;&#27979;&#26041;&#27861;SA-LSTM&#65292;&#36890;&#36807;&#23558;&#33258;&#27880;&#24847;&#21147;&#19982;&#38271;&#30701;&#26399;&#35760;&#24518;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#27493;&#39044;&#27979;&#30340;&#25913;&#36827;&#65292;&#24182;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#39044;&#27979;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.05663</link><description>&lt;p&gt;
&#23454;&#26102;&#29942;&#39048;&#21644;&#28608;&#27874;&#39044;&#27979;&#30340;&#20013;&#23610;&#24230;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Mesoscale Traffic Forecasting for Real-Time Bottleneck and Shockwave Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05663
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#23454;&#26102;&#20013;&#23610;&#24230;&#20132;&#36890;&#39044;&#27979;&#20013;&#20855;&#26377;&#26368;&#20808;&#36827;&#25928;&#26524;&#30340;&#28145;&#24230;&#39044;&#27979;&#26041;&#27861;SA-LSTM&#65292;&#36890;&#36807;&#23558;&#33258;&#27880;&#24847;&#21147;&#19982;&#38271;&#30701;&#26399;&#35760;&#24518;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#27493;&#39044;&#27979;&#30340;&#25913;&#36827;&#65292;&#24182;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#39044;&#27979;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#23454;&#26102;&#20132;&#36890;&#29366;&#24577;&#39044;&#27979;&#22312;&#20132;&#36890;&#25511;&#21046;&#30740;&#31350;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#29305;&#21035;&#26159;CIRCLES&#32852;&#21512;&#39033;&#30446;&#38656;&#35201;&#39044;&#27979;&#25216;&#26415;&#26469;&#20943;&#36731;&#25968;&#25454;&#28304;&#24310;&#36831;&#30340;&#24433;&#21709;&#12290;&#22312;MegaVanderTest&#23454;&#39564;&#21462;&#24471;&#25104;&#21151;&#20043;&#21518;&#65292;&#26412;&#25991;&#26088;&#22312;&#20811;&#26381;&#24403;&#21069;&#31995;&#32479;&#38480;&#21046;&#65292;&#24320;&#21457;&#26356;&#36866;&#21512;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#19979;&#19968;&#36718;&#23454;&#39564;&#30340;&#23454;&#26102;&#20132;&#36890;&#29366;&#24577;&#20272;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SA-LSTM&#65292;&#36825;&#26159;&#19968;&#31181;&#28145;&#24230;&#39044;&#27979;&#26041;&#27861;&#65292;&#23558;&#33258;&#27880;&#24847;&#21147;&#65288;SA&#65289;&#19982;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#22312;&#31354;&#38388;&#32500;&#24230;&#19978;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#23454;&#26102;&#20013;&#23610;&#24230;&#20132;&#36890;&#39044;&#27979;&#20013;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#22810;&#27493;&#39044;&#27979;&#65292;&#20351;&#29992;n-step SA-LSTM&#65292;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#39044;&#27979;&#20043;&#38388;&#30340;&#24179;&#34913;&#20013;&#20248;&#20110;&#20256;&#32479;&#30340;&#22810;&#27493;&#39044;&#27979;&#26041;&#27861;&#65292;&#21516;&#26102;&#23454;&#26102;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate real-time traffic state forecasting plays a pivotal role in traffic control research. In particular, the CIRCLES consortium project necessitates predictive techniques to mitigate the impact of data source delays. After the success of the MegaVanderTest experiment, this paper aims at overcoming the current system limitations and develop a more suited approach to improve the real-time traffic state estimation for the next iterations of the experiment. In this paper, we introduce the SA-LSTM, a deep forecasting method integrating Self-Attention (SA) on the spatial dimension with Long Short-Term Memory (LSTM) yielding state-of-the-art results in real-time mesoscale traffic forecasting. We extend this approach to multi-step forecasting with the n-step SA-LSTM, which outperforms traditional multi-step forecasting methods in the trade-off between short-term and long-term predictions, all while operating in real-time.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37325;&#26032;&#35780;&#20272;GNN&#22312;&#22270;&#39046;&#22495;&#36866;&#24212;&#20013;&#30340;&#20316;&#29992;&#65292;&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#20256;&#25773;&#36807;&#31243;&#23545;&#20110;&#36866;&#24212;&#19981;&#21516;&#22270;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25552;&#20379;&#20102;&#22810;&#23618;GNN&#30340;&#27867;&#21270;&#30028;&#38480;&#30340;&#35777;&#26126;&#12290;</title><link>https://arxiv.org/abs/2402.05660</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#26080;&#30417;&#30563;&#22270;&#39046;&#22495;&#36866;&#24212;&#20013;&#30340;&#20256;&#25773;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Propagation for Unsupervised Graph Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05660
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#35780;&#20272;GNN&#22312;&#22270;&#39046;&#22495;&#36866;&#24212;&#20013;&#30340;&#20316;&#29992;&#65292;&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#20256;&#25773;&#36807;&#31243;&#23545;&#20110;&#36866;&#24212;&#19981;&#21516;&#22270;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25552;&#20379;&#20102;&#22810;&#23618;GNN&#30340;&#27867;&#21270;&#30028;&#38480;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22270;&#39046;&#22495;&#36866;&#24212;&#65288;UGDA&#65289;&#26088;&#22312;&#23558;&#26631;&#35760;&#28304;&#22270;&#30340;&#30693;&#35782;&#20256;&#36755;&#21040;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22270;&#20013;&#65292;&#20197;&#35299;&#20915;&#22270;&#39046;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23398;&#20064;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#23545;&#28304;&#22270;&#21644;&#30446;&#26631;&#22270;&#30340;&#25968;&#25454;&#36827;&#34892;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;GNN&#30340;&#20869;&#22312;&#27867;&#21270;&#33021;&#21147;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#35270;&#20102;&#12290;&#22312;&#32463;&#39564;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#37325;&#26032;&#35780;&#20272;&#20102;GNN&#22312;&#22270;&#39046;&#22495;&#36866;&#24212;&#20013;&#30340;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#20256;&#25773;&#36807;&#31243;&#22312;GNN&#20013;&#36866;&#24212;&#19981;&#21516;&#22270;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#23545;UGDA&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#25512;&#23548;&#20986;&#22810;&#23618;GNN&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#36890;&#36807;&#23558;GNN Lipschitz&#24212;&#29992;&#20110;k&#23618;GNNs&#65292;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#22312;&#28304;&#22270;&#20013;&#21024;&#38500;&#20256;&#25773;&#23618;&#24182;&#22312;&#30446;&#26631;&#22270;&#20013;&#22534;&#21472;&#22810;&#20010;&#20256;&#25773;&#23618;&#65292;&#21487;&#20197;&#20351;&#30446;&#26631;&#39118;&#38505;&#30028;&#38480;&#26356;&#32039;&#23494;&#12290;&#22522;&#20110;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Graph Domain Adaptation (UGDA) aims to transfer knowledge from a labelled source graph to an unlabelled target graph in order to address the distribution shifts between graph domains. Previous works have primarily focused on aligning data from the source and target graph in the representation space learned by graph neural networks (GNNs). However, the inherent generalization capability of GNNs has been largely overlooked. Motivated by our empirical analysis, we reevaluate the role of GNNs in graph domain adaptation and uncover the pivotal role of the propagation process in GNNs for adapting to different graph domains. We provide a comprehensive theoretical analysis of UGDA and derive a generalization bound for multi-layer GNNs. By formulating GNN Lipschitz for k-layer GNNs, we show that the target risk bound can be tighter by removing propagation layers in source graph and stacking multiple propagation layers in target graph. Based on the empirical and theoretical analysis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#22238;&#39038;&#24403;&#21069;&#25991;&#29486;&#65292;&#20998;&#26512;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#36719;&#20214;&#38169;&#35823;&#39044;&#27979;&#39046;&#22495;&#30340;&#21487;&#20877;&#29616;&#24615;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#20196;&#20154;&#25285;&#24551;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.05645</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#36719;&#20214;&#38169;&#35823;&#39044;&#27979;&#20013;&#30340;&#21487;&#20877;&#29616;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating Reproducibility in Deep Learning-Based Software Fault Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#22238;&#39038;&#24403;&#21069;&#25991;&#29486;&#65292;&#20998;&#26512;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#36719;&#20214;&#38169;&#35823;&#39044;&#27979;&#39046;&#22495;&#30340;&#21487;&#20877;&#29616;&#24615;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#20196;&#20154;&#25285;&#24551;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#24212;&#29992;&#20110;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#33258;&#21160;&#39044;&#27979;&#21644;&#23450;&#20301;&#36719;&#20214;&#38169;&#35823;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24555;&#36895;&#37319;&#29992;&#65292;&#23398;&#32773;&#20204;&#36234;&#26469;&#36234;&#38590;&#20197;&#22797;&#29616;&#25991;&#29486;&#20013;&#25253;&#36947;&#30340;&#32467;&#26524;&#12290;&#29305;&#21035;&#26159;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#35780;&#20272;&#26041;&#27861;&#26410;&#27491;&#30830;&#35760;&#24405;&#12289;&#20195;&#30721;&#21644;&#25968;&#25454;&#26410;&#20998;&#20139;&#30340;&#24773;&#20917;&#19979;&#26356;&#21152;&#22256;&#38590;&#12290;&#37492;&#20110;&#20854;&#20182;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21487;&#20877;&#29616;&#24615;&#30340;&#26368;&#36817;&#19988;&#38750;&#24120;&#20196;&#20154;&#25285;&#24551;&#30340;&#21457;&#29616;&#65292;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#20998;&#26512;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#36719;&#20214;&#38169;&#35823;&#39044;&#27979;&#39046;&#22495;&#26159;&#21542;&#23384;&#22312;&#31867;&#20284;&#38382;&#39064;&#30340;&#31243;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;&#24403;&#21069;&#25991;&#29486;&#36827;&#34892;&#20102;&#31995;&#32479;&#22238;&#39038;&#65292;&#24182;&#26816;&#26597;&#20102;&#21487;&#20877;&#29616;&#24615;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, deep learning methods have been applied for a wide range of Software Engineering (SE) tasks, including in particular for the important task of automatically predicting and localizing faults in software. With the rapid adoption of increasingly complex machine learning models, it however becomes more and more difficult for scholars to reproduce the results that are reported in the literature. This is in particular the case when the applied deep learning models and the evaluation methodology are not properly documented and when code and data are not shared. Given some recent -- and very worrying -- findings regarding reproducibility and progress in other areas of applied machine learning, the goal of this work is to analyze to what extent the field of software engineering, in particular in the area of software fault prediction, is plagued by similar problems. We have therefore conducted a systematic review of the current literature and examined the level of reprod
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#26426;&#21046;&#65288;POP&#65289;&#26469;&#35299;&#20915;&#24819;&#35937;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#26032;&#22411;TBWM&#20195;&#29702;&#20013;&#24212;&#29992;POP&#65292;&#24819;&#35937;&#36895;&#24230;&#25552;&#39640;&#20102;15.4&#20493;&#65292;&#22312;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.05643</link><description>&lt;p&gt;
&#36890;&#36807;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#25913;&#36827;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Token-Based World Models with Parallel Observation Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05643
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#26426;&#21046;&#65288;POP&#65289;&#26469;&#35299;&#20915;&#24819;&#35937;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#26032;&#22411;TBWM&#20195;&#29702;&#20013;&#24212;&#29992;POP&#65292;&#24819;&#35937;&#36895;&#24230;&#25552;&#39640;&#20102;15.4&#20493;&#65292;&#22312;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#23558;Transformer&#24212;&#29992;&#20110;&#31163;&#25955;&#31526;&#21495;&#24207;&#21015;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;TBWMs&#65289;&#20316;&#20026;&#39640;&#25928;&#26679;&#26412;&#26041;&#27861;&#12290;&#22312;TBWMs&#20013;&#65292;&#19990;&#30028;&#27169;&#22411;&#23558;&#20195;&#29702;&#32463;&#39564;&#20316;&#20026;&#19968;&#31181;&#31867;&#20284;&#35821;&#35328;&#30340;&#20196;&#29260;&#24207;&#21015;&#36827;&#34892;&#28040;&#32791;&#65292;&#20854;&#20013;&#27599;&#20010;&#35266;&#27979;&#26500;&#25104;&#19968;&#20010;&#23376;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#22312;&#24819;&#35937;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#20196;&#29260;&#36880;&#20010;&#29983;&#25104;&#19979;&#19968;&#20010;&#35266;&#27979;&#30340;&#20018;&#34892;&#26041;&#24335;&#23548;&#33268;&#20102;&#20005;&#37325;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#38271;&#12289;GPU&#21033;&#29992;&#29575;&#20302;&#21644;&#34920;&#31034;&#33021;&#21147;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#29942;&#39048;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#65288;POP&#65289;&#26426;&#21046;&#12290;POP&#36890;&#36807;&#19968;&#31181;&#38024;&#23545;&#25105;&#20204;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#35774;&#35745;&#30340;&#26032;&#22411;&#21069;&#21521;&#27169;&#24335;&#26469;&#25193;&#20805;&#20102;&#20445;&#25345;&#32593;&#32476;&#65288;RetNet&#65289;&#12290;&#25105;&#20204;&#23558;POP&#38598;&#25104;&#21040;&#19968;&#31181;&#21517;&#20026;REM&#65288;&#20445;&#25345;&#29615;&#22659;&#27169;&#22411;&#65289;&#30340;&#26032;&#22411;TBWM&#20195;&#29702;&#20013;&#65292;&#23637;&#31034;&#20102;&#27604;&#20197;&#21069;&#30340;TBWMs&#24555;15.4&#20493;&#30340;&#24819;&#35937;&#33021;&#21147;&#12290;REM&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#30340;26&#20010;&#28216;&#25103;&#20013;&#30340;12&#20010;&#28216;&#25103;&#20013;&#36798;&#21040;&#36229;&#36234;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#23436;&#25104;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the success of Transformers when applied to sequences of discrete symbols, token-based world models (TBWMs) were recently proposed as sample-efficient methods. In TBWMs, the world model consumes agent experience as a language-like sequence of tokens, where each observation constitutes a sub-sequence. However, during imagination, the sequential token-by-token generation of next observations results in a severe bottleneck, leading to long training times, poor GPU utilization, and limited representations. To resolve this bottleneck, we devise a novel Parallel Observation Prediction (POP) mechanism. POP augments a Retentive Network (RetNet) with a novel forward mode tailored to our reinforcement learning setting. We incorporate POP in a novel TBWM agent named REM (Retentive Environment Model), showcasing a 15.4x faster imagination compared to prior TBWMs. REM attains superhuman performance on 12 out of 26 games of the Atari 100K benchmark, while training in less than 12 hours.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38543;&#26426;&#36817;&#20284;&#26799;&#24230;&#26368;&#23567;&#21270;&#25237;&#24433;&#32676;&#20307;&#39118;&#38505;&#30340;&#26032;&#22411;&#38750;&#21442;&#25968;&#20202;&#22120;&#21464;&#37327;&#22238;&#24402;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#31454;&#20105;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#22788;&#29702;&#20102;&#20108;&#20803;&#32467;&#26524;&#30340;&#24773;&#20917;&#65292;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05639</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#20202;&#22120;&#21464;&#37327;&#22238;&#24402;&#36890;&#36807;&#38543;&#26426;&#36817;&#20284;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Instrumental Variable Regression through Stochastic Approximate Gradients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38543;&#26426;&#36817;&#20284;&#26799;&#24230;&#26368;&#23567;&#21270;&#25237;&#24433;&#32676;&#20307;&#39118;&#38505;&#30340;&#26032;&#22411;&#38750;&#21442;&#25968;&#20202;&#22120;&#21464;&#37327;&#22238;&#24402;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#31454;&#20105;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#22788;&#29702;&#20102;&#20108;&#20803;&#32467;&#26524;&#30340;&#24773;&#20917;&#65292;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SAGD-IV&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#36817;&#20284;&#26799;&#24230;&#26469;&#26368;&#23567;&#21270;&#25237;&#24433;&#32676;&#20307;&#39118;&#38505;&#30340;&#26032;&#22411;&#38750;&#21442;&#25968;&#20202;&#22120;&#21464;&#37327;&#65288;NPIV&#65289;&#22238;&#24402;&#26694;&#26550;&#12290;&#20202;&#22120;&#21464;&#37327;&#65288;IV&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35745;&#37327;&#32463;&#27982;&#23398;&#20013;&#65292;&#20197;&#35299;&#20915;&#22312;&#23384;&#22312;&#19981;&#21487;&#35266;&#27979;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#30340;&#20272;&#35745;&#38382;&#39064;&#65292;&#24182;&#19988;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#33268;&#21147;&#20110;&#25913;&#36827;&#29616;&#26377;&#26041;&#27861;&#24182;&#22312;NPIV&#35774;&#32622;&#19979;&#35774;&#35745;&#26032;&#26041;&#27861;&#65292;&#35813;&#35774;&#32622;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#19981;&#36866;&#23450;&#30340;&#32447;&#24615;&#36870;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#25105;&#20204;&#31639;&#27861;&#30340;&#29702;&#35770;&#25903;&#25345;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#23454;&#39564;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#20854;&#31454;&#20105;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22788;&#29702;&#20102;&#20108;&#20803;&#32467;&#26524;&#30340;&#24773;&#20917;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#32780;&#35813;&#24773;&#20917;&#22312;&#31038;&#21306;&#20013;&#27809;&#26377;&#24471;&#21040;&#19982;&#20854;&#36830;&#32493;&#23545;&#24212;&#29289;&#30340;&#21516;&#26679;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes SAGD-IV, a novel framework for conducting nonparametric instrumental variable (NPIV) regression by employing stochastic approximate gradients to minimize the projected populational risk. Instrumental Variables (IVs) are widely used in econometrics to address estimation problems in the presence of unobservable confounders, and the Machine Learning community has devoted significant effort to improving existing methods and devising new ones in the NPIV setting, which is known to be an ill-posed linear inverse problem. We provide theoretical support for our algorithm and further exemplify its competitive performance through empirical experiments. Furthermore, we address, with promising results, the case of binary outcomes, which has not received as much attention from the community as its continuous counterpart.
&lt;/p&gt;</description></item><item><title>RepQuant&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;&#37327;&#21270;-&#25512;&#29702;&#35299;&#32806;&#30340;&#33539;&#24335;&#21644;&#37327;&#21270;&#27604;&#20363;&#37325;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#37327;&#21270;&#21644;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.05628</link><description>&lt;p&gt;
RepQuant: &#36890;&#36807;&#27604;&#20363;&#37325;&#21442;&#25968;&#21270;&#23454;&#29616;&#22823;&#22411;Transformer&#27169;&#22411;&#30340;&#20934;&#30830;&#21518;&#35757;&#32451;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
RepQuant: Towards Accurate Post-Training Quantization of Large Transformer Models via Scale Reparameterization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05628
&lt;/p&gt;
&lt;p&gt;
RepQuant&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;&#37327;&#21270;-&#25512;&#29702;&#35299;&#32806;&#30340;&#33539;&#24335;&#21644;&#37327;&#21270;&#27604;&#20363;&#37325;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#37327;&#21270;&#21644;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;Transformer&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#25104;&#21151;&#12290;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#21387;&#32553;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#65292;&#23427;&#21482;&#38656;&#35201;&#19968;&#20010;&#23567;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#26657;&#20934;&#65292;&#24182;&#36991;&#20813;&#31471;&#21040;&#31471;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;PTQ&#26041;&#27861;&#36890;&#24120;&#20250;&#23548;&#33268;&#38750;&#24120;&#26126;&#26174;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24615;&#33021;&#29942;&#39048;&#26469;&#33258;&#20110;&#22312;&#37327;&#21270;&#36807;&#31243;&#20013;&#36807;&#20998;&#32771;&#34385;&#30828;&#20214;&#20860;&#23481;&#24615;&#65292;&#36825;&#36843;&#20351;&#23427;&#20204;&#19981;&#24773;&#24895;&#22320;&#20351;&#29992;&#31616;&#21333;&#30340;&#37327;&#21270;&#22120;&#65292;&#34429;&#28982;&#20197;&#29306;&#29298;&#20934;&#30830;&#24615;&#20026;&#20195;&#20215;&#12290;&#22522;&#20110;&#19978;&#36848;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RepQuant&#65292;&#19968;&#20010;&#26032;&#30340;PTQ&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;&#37327;&#21270;-&#25512;&#29702;&#35299;&#32806;&#33539;&#24335;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;RepQuant&#22312;&#37327;&#21270;&#36807;&#31243;&#20013;&#20351;&#29992;&#22797;&#26434;&#30340;&#37327;&#21270;&#22120;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;&#29992;&#31616;&#21270;&#30340;&#37327;&#21270;&#22120;&#65292;&#24182;&#36890;&#36807;&#37327;&#21270;&#27604;&#20363;&#37325;&#21442;&#25968;&#21270;&#22312;&#20004;&#32773;&#20043;&#38388;&#36827;&#34892;&#25968;&#23398;&#19978;&#31561;&#20215;&#30340;&#36716;&#25442;&#65292;&#20174;&#32780;&#30830;&#20445;&#20934;&#30830;&#30340;&#37327;&#21270;&#21644;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large transformer models have demonstrated remarkable success. Post-training quantization (PTQ), which requires only a small dataset for calibration and avoids end-to-end retraining, is a promising solution for compressing these large models. Regrettably, existing PTQ methods typically exhibit non-trivial performance loss. We find that the performance bottleneck stems from over-consideration of hardware compatibility in the quantization process, compelling them to reluctantly employ simple quantizers, albeit at the expense of accuracy. With the above insights, we propose RepQuant, a novel PTQ framework with quantization-inference decoupling paradigm to address the above issues. RepQuant employs complex quantizers in the quantization process and simplified quantizers in the inference process, and performs mathematically equivalent transformations between the two through quantization scale reparameterization, thus ensuring both accurate quantization and efficient inference. More specific
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26059;&#36716;&#29305;&#24449;&#20013;&#30340;&#32465;&#23450;&#21160;&#21147;&#23398;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#20313;&#24358;&#32465;&#23450;&#8221;&#26426;&#21046;&#65292;&#20197;&#26367;&#20195;&#20256;&#32479;&#30340;&#8220;$\chi$-binding&#8221;&#26426;&#21046;&#12290;&#36890;&#36807;&#26174;&#24335;&#35745;&#31639;&#29305;&#24449;&#20043;&#38388;&#30340;&#23545;&#40784;&#21644;&#30456;&#24212;&#30340;&#26435;&#37325;&#35843;&#25972;&#65292;&#36825;&#19968;&#26032;&#26426;&#21046;&#33021;&#22815;&#36798;&#21040;&#19982;&#20256;&#32479;&#26426;&#21046;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#19982;&#33258;&#27880;&#24847;&#21147;&#21644;&#29983;&#29289;&#31070;&#32463;&#23398;&#26377;&#20851;&#12290;</title><link>https://arxiv.org/abs/2402.05627</link><description>&lt;p&gt;
&#26059;&#36716;&#29305;&#24449;&#20013;&#30340;&#32465;&#23450;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Binding Dynamics in Rotating Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26059;&#36716;&#29305;&#24449;&#20013;&#30340;&#32465;&#23450;&#21160;&#21147;&#23398;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#20313;&#24358;&#32465;&#23450;&#8221;&#26426;&#21046;&#65292;&#20197;&#26367;&#20195;&#20256;&#32479;&#30340;&#8220;$\chi$-binding&#8221;&#26426;&#21046;&#12290;&#36890;&#36807;&#26174;&#24335;&#35745;&#31639;&#29305;&#24449;&#20043;&#38388;&#30340;&#23545;&#40784;&#21644;&#30456;&#24212;&#30340;&#26435;&#37325;&#35843;&#25972;&#65292;&#36825;&#19968;&#26032;&#26426;&#21046;&#33021;&#22815;&#36798;&#21040;&#19982;&#20256;&#32479;&#26426;&#21046;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#19982;&#33258;&#27880;&#24847;&#21147;&#21644;&#29983;&#29289;&#31070;&#32463;&#23398;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#35748;&#30693;&#20013;&#65292;&#32465;&#23450;&#38382;&#39064;&#25551;&#36848;&#20102;&#22823;&#33041;&#22914;&#20309;&#28789;&#27963;&#22320;&#23558;&#21508;&#31181;&#20449;&#24687;&#25972;&#21512;&#25104;&#20855;&#26377;&#36830;&#36143;&#24615;&#30340;&#23545;&#35937;&#34920;&#31034;&#30340;&#26410;&#35299;&#20043;&#35868;&#12290;&#31867;&#20284;&#22320;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20154;&#20204;&#36861;&#27714;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#20197;&#23398;&#20064;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#26469;&#23454;&#29616;&#24378;&#22823;&#30340;&#27867;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#27169;&#22411;&#12290;&#20511;&#37492;&#31070;&#32463;&#31185;&#23398;&#29702;&#35770;&#65292;&#26059;&#36716;&#29305;&#24449;&#36890;&#36807;&#24341;&#20837;&#30690;&#37327;&#29305;&#24449;&#26469;&#23398;&#20064;&#36825;&#31181;&#34920;&#31034;&#65292;&#30690;&#37327;&#29305;&#24449;&#30340;&#22823;&#23567;&#21253;&#21547;&#23545;&#35937;&#29305;&#24449;&#65292;&#26041;&#21521;&#21253;&#21547;&#23545;&#35937;&#20851;&#32852;&#12290;&#22312;&#26550;&#26500;&#30340;&#27599;&#20010;&#23618;&#20013;&#37117;&#23884;&#20837;&#20102;&#8220;$\chi$-binding&#8221;&#26426;&#21046;&#65292;&#24050;&#34987;&#35777;&#26126;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20102;&#35299;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26367;&#20195;&#30340;&#8220;&#20313;&#24358;&#32465;&#23450;&#8221;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#26174;&#24335;&#35745;&#31639;&#29305;&#24449;&#20043;&#38388;&#30340;&#23545;&#40784;&#24182;&#30456;&#24212;&#22320;&#35843;&#25972;&#26435;&#37325;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#23427;&#21487;&#20197;&#36798;&#21040;&#30456;&#21516;&#30340;&#24615;&#33021;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#19982;&#33258;&#27880;&#24847;&#21147;&#21644;&#29983;&#29289;&#31070;&#32463;&#23398;&#20135;&#29983;&#30452;&#25509;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In human cognition, the binding problem describes the open question of how the brain flexibly integrates diverse information into cohesive object representations. Analogously, in machine learning, there is a pursuit for models capable of strong generalization and reasoning by learning object-centric representations in an unsupervised manner. Drawing from neuroscientific theories, Rotating Features learn such representations by introducing vector-valued features that encapsulate object characteristics in their magnitudes and object affiliation in their orientations. The "$\chi$-binding" mechanism, embedded in every layer of the architecture, has been shown to be crucial, but remains poorly understood. In this paper, we propose an alternative "cosine binding" mechanism, which explicitly computes the alignment between features and adjusts weights accordingly, and we show that it achieves equivalent performance. This allows us to draw direct connections to self-attention and biological neu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;ReLU-like&#28608;&#27963;&#20989;&#25968;&#20197;&#32463;&#39564;&#24179;&#26041;&#25439;&#22833;&#35757;&#32451;&#30340;&#21333;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#25552;&#20986;&#20102;&#31283;&#23450;&#28857;&#26465;&#20214;&#21644;&#36867;&#36920;&#31070;&#32463;&#20803;&#30340;&#23450;&#20041;&#65292;&#24182;&#23558;&#38797;&#28857;&#36867;&#36920;&#19982;&#36867;&#36920;&#31070;&#32463;&#20803;&#30340;&#21442;&#25968;&#21464;&#21270;&#32852;&#31995;&#36215;&#26469;&#12290;</title><link>https://arxiv.org/abs/2402.05626</link><description>&lt;p&gt;
&#27973;&#23618;ReLU-like&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#65306;&#31283;&#23450;&#28857;&#12289;&#38797;&#28857;&#36867;&#36920;&#21644;&#32593;&#32476;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
The Loss Landscape of Shallow ReLU-like Neural Networks: Stationary Points, Saddle Escaping, and Network Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;ReLU-like&#28608;&#27963;&#20989;&#25968;&#20197;&#32463;&#39564;&#24179;&#26041;&#25439;&#22833;&#35757;&#32451;&#30340;&#21333;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#25552;&#20986;&#20102;&#31283;&#23450;&#28857;&#26465;&#20214;&#21644;&#36867;&#36920;&#31070;&#32463;&#20803;&#30340;&#23450;&#20041;&#65292;&#24182;&#23558;&#38797;&#28857;&#36867;&#36920;&#19982;&#36867;&#36920;&#31070;&#32463;&#20803;&#30340;&#21442;&#25968;&#21464;&#21270;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;ReLU-like&#28608;&#27963;&#20989;&#25968;&#20197;&#32463;&#39564;&#24179;&#26041;&#25439;&#22833;&#35757;&#32451;&#30340;&#21333;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#12290;&#30001;&#20110;&#28608;&#27963;&#20989;&#25968;&#26159;&#19981;&#21487;&#24494;&#30340;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#22914;&#20309;&#23436;&#20840;&#25551;&#36848;&#31283;&#23450;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#38750;&#21487;&#24494;&#21644;&#21487;&#24494;&#24773;&#20917;&#30340;&#31283;&#23450;&#28857;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#26524;&#19968;&#20010;&#31283;&#23450;&#28857;&#19981;&#21253;&#21547;&#8220;&#36867;&#36920;&#31070;&#32463;&#20803;&#8221;&#65288;&#36890;&#36807;&#19968;&#38454;&#26465;&#20214;&#23450;&#20041;&#65289;&#65292;&#37027;&#20040;&#23427;&#24517;&#23450;&#26159;&#19968;&#20010;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#27492;&#22806;&#65292;&#22312;&#26631;&#37327;&#36755;&#20986;&#24773;&#20917;&#19979;&#65292;&#36867;&#36920;&#31070;&#32463;&#20803;&#30340;&#23384;&#22312;&#20445;&#35777;&#20102;&#31283;&#23450;&#28857;&#19981;&#26159;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#25551;&#36848;&#20102;&#20174;&#26080;&#31351;&#23567;&#65288;&#28040;&#22833;&#65289;&#21021;&#22987;&#21270;&#24320;&#22987;&#30340;&#27973;&#23618;ReLU-like&#32593;&#32476;&#30340;&#38797;&#28857;&#21040;&#38797;&#28857;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#30452;&#25509;&#23558;&#38797;&#28857;&#36867;&#36920;&#19982;&#36867;&#36920;&#31070;&#32463;&#20803;&#30340;&#21442;&#25968;&#21464;&#21270;&#32852;&#31995;&#36215;&#26469;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23436;&#20840;&#35752;&#35770;&#20102;&#32593;&#32476;&#23884;&#20837;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the loss landscape of one-hidden-layer neural networks with ReLU-like activation functions trained with the empirical squared loss. As the activation function is non-differentiable, it is so far unclear how to completely characterize the stationary points. We propose the conditions for stationarity that apply to both non-differentiable and differentiable cases. Additionally, we show that, if a stationary point does not contain "escape neurons", which are defined with first-order conditions, then it must be a local minimum. Moreover, for the scalar-output case, the presence of an escape neuron guarantees that the stationary point is not a local minimum. Our results refine the description of the saddle-to-saddle training process starting from infinitesimally small (vanishing) initialization for shallow ReLU-like networks, linking saddle escaping directly with the parameter changes of escape neurons. Moreover, we are also able to fully discuss how network emb
&lt;/p&gt;</description></item><item><title>&#39044;&#35757;&#32451;&#30340;&#23567;&#22411;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#24207;&#21015;&#22411;&#20219;&#21153;&#30340;&#36890;&#29992;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25351;&#20196;&#24494;&#35843;&#21487;&#20197;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#20219;&#21153;&#20013;&#23454;&#29616;&#25509;&#36817;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05616</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#24207;&#21015;&#22411;&#20219;&#21153;&#30340;&#36890;&#29992;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Pretrained Generative Language Models as General Learning Frameworks for Sequence-Based Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05616
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#23567;&#22411;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#24207;&#21015;&#22411;&#20219;&#21153;&#30340;&#36890;&#29992;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25351;&#20196;&#24494;&#35843;&#21487;&#20197;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#20219;&#21153;&#20013;&#23454;&#29616;&#25509;&#36817;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35266;&#28857;&#65292;&#21363;&#20855;&#26377;&#25968;&#30334;&#19975;&#21442;&#25968;&#30340;&#23567;&#22411;&#39044;&#35757;&#32451;&#22522;&#30784;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29992;&#20316;&#24207;&#21015;&#22411;&#20219;&#21153;&#30340;&#36890;&#29992;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#35299;&#20915;&#20102;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21644;&#35821;&#35328;&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#35745;&#31639;&#36164;&#28304;&#12289;&#25216;&#33021;&#38656;&#27714;&#21644;&#26102;&#38388;&#38480;&#21046;&#31561;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19987;&#27880;&#20110;&#21019;&#24314;&#23567;&#22411;&#19988;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#25191;&#34892;&#22522;&#20110;&#20219;&#21153;&#27169;&#22411;&#26080;&#27861;&#23436;&#25104;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;125M&#12289;350M&#21644;1.3B&#21442;&#25968;&#30340;&#39044;&#35757;&#32451;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#65292;&#20351;&#29992;10,000&#21040;1,000,000&#20010;&#25351;&#20196;&#31034;&#20363;&#21487;&#20197;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21270;&#23398;&#20449;&#24687;&#23398;&#20219;&#21153;&#19978;&#23454;&#29616;&#25509;&#36817;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36830;&#32493;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26102;&#26399;&#23545;&#25913;&#36827;&#32467;&#26524;&#30340;&#20316;&#29992;&#65292;&#20197;&#21450;&#25968;&#25454;&#26684;&#24335;&#21644;&#39044;&#35757;&#32451;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#23545;&#25351;&#20196;&#24494;&#35843;&#25104;&#21151;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose that small pretrained foundational generative language models with millions of parameters can be utilized as a general learning framework for sequence-based tasks. Our proposal overcomes the computational resource, skill set, and timeline challenges associated with training neural networks and language models from scratch. Further, our approach focuses on creating small and highly specialized models that can accurately execute a challenging task of which the base model is incapable of performing. We demonstrate that 125M, 350M, and 1.3B parameter pretrained foundational language models can be instruction fine-tuned with 10,000-to-1,000,000 instruction examples to achieve near state-of-the-art results on challenging cheminformatics tasks. We also demonstrate the role of successive language model fine-tuning epochs on improved outcomes, as well as the importance of both data formatting and pretrained foundational language model selection for instruction fine-tuning success.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#21327;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#28151;&#21512;&#22242;&#38431;&#25480;&#26435;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;AI&#32463;&#29702;&#65288;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65289;&#20316;&#20026;&#22242;&#38431;&#30340;&#22806;&#37096;&#35266;&#23519;&#32773;&#65292;&#23398;&#20064;&#22242;&#38431;&#20195;&#29702;&#20154;&#30340;&#34892;&#20026;&#27169;&#22411;&#24182;&#36873;&#25321;&#26368;&#20339;&#30340;&#25511;&#21046;&#20195;&#29702;&#20154;&#12290;</title><link>https://arxiv.org/abs/2402.05605</link><description>&lt;p&gt;
&#20248;&#21270;&#21327;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#28151;&#21512;&#22242;&#38431;&#20013;&#30340;&#25480;&#26435;
&lt;/p&gt;
&lt;p&gt;
Optimizing Delegation in Collaborative Human-AI Hybrid Teams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#21327;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#28151;&#21512;&#22242;&#38431;&#25480;&#26435;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;AI&#32463;&#29702;&#65288;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65289;&#20316;&#20026;&#22242;&#38431;&#30340;&#22806;&#37096;&#35266;&#23519;&#32773;&#65292;&#23398;&#20064;&#22242;&#38431;&#20195;&#29702;&#20154;&#30340;&#34892;&#20026;&#27169;&#22411;&#24182;&#36873;&#25321;&#26368;&#20339;&#30340;&#25511;&#21046;&#20195;&#29702;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#31867;&#21644;&#33258;&#20027;&#31995;&#32479;&#20316;&#20026;&#28151;&#21512;&#22242;&#38431;&#20849;&#21516;&#36816;&#20316;&#26102;&#65292;&#25105;&#20204;&#24076;&#26395;&#30830;&#20445;&#22242;&#38431;&#30340;&#25104;&#21151;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#23558;&#22242;&#38431;&#25104;&#21592;&#31216;&#20026;&#20195;&#29702;&#20154;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#28151;&#21512;&#22242;&#38431;&#30340;&#24773;&#20917;&#65292;&#21363;&#22312;&#20219;&#20309;&#26102;&#20505;&#65292;&#21482;&#26377;&#19968;&#20010;&#22242;&#38431;&#25104;&#21592;&#65288;&#25511;&#21046;&#20195;&#29702;&#20154;&#65289;&#34987;&#25480;&#26435;&#20026;&#22242;&#38431;&#30340;&#25511;&#21046;&#32773;&#12290;&#20026;&#20102;&#30830;&#23450;&#26368;&#20339;&#30340;&#25511;&#21046;&#20195;&#29702;&#20154;&#36873;&#25321;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24341;&#20837;AI&#32463;&#29702;&#65288;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65289;&#30340;&#24819;&#27861;&#65292;&#35813;&#32463;&#29702;&#20316;&#20026;&#22242;&#38431;&#30340;&#22806;&#37096;&#35266;&#23519;&#32773;&#23398;&#20064;&#12290;&#32463;&#29702;&#36890;&#36807;&#35266;&#23519;&#20195;&#29702;&#20154;&#30340;&#34920;&#29616;&#21644;&#22242;&#38431;&#25152;&#22788;&#30340;&#29615;&#22659;/&#19990;&#30028;&#26469;&#23398;&#20064;&#34892;&#20026;&#27169;&#22411;&#65292;&#24182;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#36873;&#25321;&#20986;&#26368;&#29702;&#24819;&#30340;&#25511;&#21046;&#20195;&#29702;&#20154;&#12290;&#20026;&#20102;&#38480;&#23450;&#32463;&#29702;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#32422;&#26463;&#26465;&#20214;&#12290;&#32463;&#29702;&#30340;&#32422;&#26463;&#26465;&#20214;&#25351;&#31034;&#22242;&#38431;&#30340;&#21487;&#25509;&#21463;&#36816;&#20316;&#26041;&#24335;&#65292;&#22240;&#27492;&#22914;&#26524;&#22242;&#38431;&#36827;&#20837;&#19981;&#21487;&#25509;&#21463;&#24182;&#38656;&#35201;&#32463;&#29702;&#20171;&#20837;&#30340;&#29366;&#24577;&#65292;&#23601;&#20250;&#36829;&#21453;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
When humans and autonomous systems operate together as what we refer to as a hybrid team, we of course wish to ensure the team operates successfully and effectively. We refer to team members as agents. In our proposed framework, we address the case of hybrid teams in which, at any time, only one team member (the control agent) is authorized to act as control for the team. To determine the best selection of a control agent, we propose the addition of an AI manager (via Reinforcement Learning) which learns as an outside observer of the team. The manager learns a model of behavior linking observations of agent performance and the environment/world the team is operating in, and from these observations makes the most desirable selection of a control agent. We restrict the manager task by introducing a set of constraints. The manager constraints indicate acceptable team operation, so a violation occurs if the team enters a condition which is unacceptable and requires manager intervention. To
&lt;/p&gt;</description></item><item><title>AttnLRP&#26159;&#39318;&#20010;&#33021;&#22815;&#24544;&#23454;&#19988;&#20840;&#38754;&#22320;&#24402;&#22240;Transformer&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20855;&#26377;&#19982;&#21333;&#19968;&#21453;&#21521;&#20256;&#25773;&#30456;&#20284;&#30340;&#35745;&#31639;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#25193;&#23637;&#36880;&#23618;&#30456;&#20851;&#20256;&#36882;&#24402;&#22240;&#26041;&#27861;&#20197;&#22788;&#29702;&#27880;&#24847;&#21147;&#23618;&#26469;&#35299;&#20915;&#20102;&#40657;&#30418;Transformer&#27169;&#22411;&#30340;&#24402;&#22240;&#38382;&#39064;&#65292;&#20855;&#26377;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#29702;&#35299;&#28508;&#22312;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05602</link><description>&lt;p&gt;
AttnLRP: &#27880;&#24847;&#21147;&#24863;&#30693;&#30340;&#36880;&#23618;&#30456;&#20851;&#20256;&#36882;&#29992;&#20110;Transformer
&lt;/p&gt;
&lt;p&gt;
AttnLRP: Attention-Aware Layer-wise Relevance Propagation for Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05602
&lt;/p&gt;
&lt;p&gt;
AttnLRP&#26159;&#39318;&#20010;&#33021;&#22815;&#24544;&#23454;&#19988;&#20840;&#38754;&#22320;&#24402;&#22240;Transformer&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20855;&#26377;&#19982;&#21333;&#19968;&#21453;&#21521;&#20256;&#25773;&#30456;&#20284;&#30340;&#35745;&#31639;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#25193;&#23637;&#36880;&#23618;&#30456;&#20851;&#20256;&#36882;&#24402;&#22240;&#26041;&#27861;&#20197;&#22788;&#29702;&#27880;&#24847;&#21147;&#23618;&#26469;&#35299;&#20915;&#20102;&#40657;&#30418;Transformer&#27169;&#22411;&#30340;&#24402;&#22240;&#38382;&#39064;&#65292;&#20855;&#26377;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#29702;&#35299;&#28508;&#22312;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#20135;&#29983;&#20559;&#35265;&#30340;&#39044;&#27979;&#21644;&#24187;&#35937;&#65292;&#36825;&#31361;&#26174;&#20102;&#29702;&#35299;&#20854;&#27169;&#22411;&#20869;&#37096;&#25512;&#29702;&#36807;&#31243;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#23545;&#25972;&#20010;&#40657;&#30418;Transformer&#27169;&#22411;&#30340;&#20934;&#30830;&#24402;&#22240;&#24182;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#25193;&#23637;&#36880;&#23618;&#30456;&#20851;&#20256;&#36882;&#24402;&#22240;&#26041;&#27861;&#20197;&#22788;&#29702;&#27880;&#24847;&#21147;&#23618;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#34429;&#28982;&#23384;&#22312;&#37096;&#20998;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#39318;&#20010;&#33021;&#22815;&#24544;&#23454;&#19988;&#20840;&#38754;&#22320;&#24402;&#22240;Transformer&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#28508;&#22312;&#34920;&#31034;&#65292;&#21516;&#26102;&#35745;&#31639;&#25928;&#29575;&#19982;&#21333;&#19968;&#21453;&#21521;&#20256;&#25773;&#30456;&#20284;&#12290;&#36890;&#36807;&#23545;Llama 2&#12289;Flan-T5&#21644;Vision Transformer&#26550;&#26500;&#19978;&#19982;&#29616;&#26377;&#26041;&#27861;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#29702;&#35299;&#28508;&#22312;&#34920;&#31034;&#65292;&#20026;&#27010;&#24565;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal reasoning process. However, achieving faithful attributions for the entirety of a black-box transformer model and maintaining computational efficiency is an unsolved challenge. By extending the Layer-wise Relevance Propagation attribution method to handle attention layers, we address these challenges effectively. While partial solutions exist, our method is the first to faithfully and holistically attribute not only input but also latent representations of transformer models with the computational efficiency similar to a singular backward pass. Through extensive evaluations against existing methods on Llama 2, Flan-T5 and the Vision Transformer architecture, we demonstrate that our proposed approach surpasses alternative methods in terms of faithfulness and enables the understanding of latent representations, opening up the door for concep
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#31163;&#25955;&#32467;&#26500;&#65292;&#26412;&#35770;&#25991;&#20197;&#30495;&#23454;&#35745;&#31639;&#26426;&#19978;&#30340;&#23454;&#29616;&#20026;&#22522;&#30784;&#65292;&#25171;&#30772;&#20102;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#35781;&#21650;&#65292;&#24182;&#32473;&#20986;&#20102;&#26080;&#32500;&#24230;&#29575;&#30340;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.05576</link><description>&lt;p&gt;
&#25968;&#23383;&#35745;&#31639;&#26426;&#25171;&#30772;&#32500;&#24230;&#35781;&#21650;&#65306;&#36890;&#36807;&#26377;&#38480;&#20960;&#20309;&#30340;&#33258;&#36866;&#24212;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Digital Computers Break the Curse of Dimensionality: Adaptive Bounds via Finite Geometry
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05576
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#31163;&#25955;&#32467;&#26500;&#65292;&#26412;&#35770;&#25991;&#20197;&#30495;&#23454;&#35745;&#31639;&#26426;&#19978;&#30340;&#23454;&#29616;&#20026;&#22522;&#30784;&#65292;&#25171;&#30772;&#20102;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#35781;&#21650;&#65292;&#24182;&#32473;&#20986;&#20102;&#26080;&#32500;&#24230;&#29575;&#30340;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#30784;&#26159;&#24314;&#31435;&#22312;&#29702;&#24819;&#24773;&#20917;&#19979;&#30340;&#21069;&#25552;&#19979;&#65292;&#21363;&#25152;&#26377;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#31354;&#38388;&#37117;&#26159;&#26080;&#31351;&#30340;&#65292;&#20363;&#22914;$\mathbb{R}^d$&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26377;&#38480;&#30340;&#26426;&#22120;&#31934;&#24230;&#12289;&#33293;&#20837;&#21644;&#26377;&#38480;&#30340;&#23384;&#20648;&#31354;&#38388;&#31561;&#25968;&#23383;&#35745;&#31639;&#26426;&#30340;&#38480;&#21046;&#65292;&#23454;&#38469;&#24773;&#20917;&#19979;&#36825;&#20010;&#26680;&#24515;&#20551;&#35774;&#24448;&#24448;&#34987;&#36829;&#32972;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#25968;&#23383;&#35745;&#31639;&#26426;&#22312;$\mathbb{R}^d$&#19978;&#25805;&#20316;&#30340;&#26159;&#26377;&#38480;&#30340;&#32593;&#26684;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#31163;&#25955;&#32467;&#26500;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23454;&#38469;&#35745;&#31639;&#26426;&#19978;&#23454;&#29616;&#27169;&#22411;&#26102;&#65292;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#35781;&#21650;&#34987;&#31995;&#32479;&#22320;&#25171;&#30772;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38024;&#23545;&#22312;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#19978;&#23454;&#29616;&#30340;&#26680;&#20989;&#25968;&#21644;&#28145;&#24230;ReLU MLP&#22238;&#24402;&#22120;&#33719;&#24471;&#20102;&#26032;&#30340;&#26080;&#32500;&#24230;&#29575;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24212;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#28176;&#36827;&#27979;&#24230;&#38598;&#20013;&#24615;&#32467;&#26524;&#65292;&#35813;&#32467;&#26524;&#32473;&#20986;&#20102;&#27010;&#29575;&#27979;&#24230;&#21644;&#20854;&#22312;$N$&#20010;&#29420;&#31435;&#21516;&#20998;&#24067;&#26679;&#26412;&#19978;&#30340;&#32463;&#39564;&#29256;&#26412;&#20043;&#38388;&#30340;&#36317;&#31163;&#20026;$1$-Wasserstein&#36317;&#31163;&#30340;&#38598;&#20013;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many of the foundations of machine learning rely on the idealized premise that all input and output spaces are infinite, e.g.~$\mathbb{R}^d$. This core assumption is systematically violated in practice due to digital computing limitations from finite machine precision, rounding, and limited RAM. In short, digital computers operate on finite grids in $\mathbb{R}^d$. By exploiting these discrete structures, we show the curse of dimensionality in statistical learning is systematically broken when models are implemented on real computers. Consequentially, we obtain new generalization bounds with dimension-free rates for kernel and deep ReLU MLP regressors, which are implemented on real-world machines.   Our results are derived using a new non-asymptotic concentration of measure result between a probability measure over any finite metric space and its empirical version associated with $N$ i.i.d. samples when measured in the $1$-Wasserstein distance. Unlike standard concentration of measure 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#38543;&#26426;&#36172;&#21338;&#26426;&#20013;&#21516;&#26102;&#23454;&#29616;&#32676;&#20307;&#26333;&#20809;&#20844;&#24179;&#24615;&#21644;&#32676;&#20869;&#31934;&#33521;&#20027;&#20041;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#38543;&#26102;&#30340;&#32676;&#20307;&#26333;&#20809;&#20844;&#24179;&#24615;&#20445;&#35777;&#21644;&#22312;&#27599;&#20010;&#32676;&#20307;&#20013;&#23454;&#29616;&#20010;&#20307;&#23618;&#38754;&#30340;&#31934;&#33521;&#20027;&#20041;&#20844;&#24179;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05575</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#36172;&#21338;&#26426;&#20013;&#21516;&#26102;&#23454;&#29616;&#32676;&#20307;&#26333;&#20809;&#20844;&#24179;&#24615;&#21644;&#32676;&#20869;&#31934;&#33521;&#20027;&#20041;
&lt;/p&gt;
&lt;p&gt;
Simultaneously Achieving Group Exposure Fairness and Within-Group Meritocracy in Stochastic Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05575
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#38543;&#26426;&#36172;&#21338;&#26426;&#20013;&#21516;&#26102;&#23454;&#29616;&#32676;&#20307;&#26333;&#20809;&#20844;&#24179;&#24615;&#21644;&#32676;&#20869;&#31934;&#33521;&#20027;&#20041;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#38543;&#26102;&#30340;&#32676;&#20307;&#26333;&#20809;&#20844;&#24179;&#24615;&#20445;&#35777;&#21644;&#22312;&#27599;&#20010;&#32676;&#20307;&#20013;&#23454;&#29616;&#20010;&#20307;&#23618;&#38754;&#30340;&#31934;&#33521;&#20027;&#20041;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#20013;&#30340;&#20844;&#24179;&#24615;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#23545;&#21508;&#20010;&#33218;&#30340;&#26333;&#20809;&#20445;&#35777;&#12290;&#24403;&#33218;&#26681;&#25454;&#26576;&#20123;&#23646;&#24615;&#33258;&#28982;&#20998;&#32452;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#23618;&#20844;&#24179;&#24615;&#65292;&#23427;&#32771;&#34385;&#20102;&#20004;&#20010;&#23618;&#38754;&#30340;&#20844;&#24179;&#24615;&#12290;&#22312;&#31532;&#19968;&#23618;&#38754;&#65292;&#21452;&#23618;&#20844;&#24179;&#24615;&#20445;&#35777;&#27599;&#20010;&#32676;&#20307;&#26377;&#19968;&#23450;&#30340;&#26368;&#20302;&#26333;&#20809;&#12290;&#20026;&#20102;&#35299;&#20915;&#32676;&#20869;&#20010;&#20307;&#33218;&#30340;&#20998;&#37197;&#19981;&#22343;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#31532;&#20108;&#23618;&#38754;&#32771;&#34385;&#20102;&#31934;&#33521;&#20027;&#20041;&#20844;&#24179;&#24615;&#65292;&#30830;&#20445;&#27599;&#20010;&#33218;&#26681;&#25454;&#20854;&#22312;&#32676;&#20307;&#20013;&#30340;&#20248;&#21183;&#34987;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#20379;(i) &#38543;&#26102;&#30340;&#32676;&#20307;&#26333;&#20809;&#20844;&#24179;&#24615;&#20445;&#35777;&#21644;(ii) &#22312;&#27599;&#20010;&#32676;&#20307;&#20013;&#23454;&#29616;&#20010;&#20307;&#23618;&#38754;&#30340;&#31934;&#33521;&#20027;&#20041;&#20844;&#24179;&#24615;&#65292;&#25105;&#20204;&#21487;&#20197;&#35843;&#25972;&#22522;&#20110;UCB&#30340;&#31639;&#27861;&#26469;&#23454;&#29616;&#21452;&#23618;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing approaches to fairness in stochastic multi-armed bandits (MAB) primarily focus on exposure guarantee to individual arms. When arms are naturally grouped by certain attribute(s), we propose Bi-Level Fairness, which considers two levels of fairness. At the first level, Bi-Level Fairness guarantees a certain minimum exposure to each group. To address the unbalanced allocation of pulls to individual arms within a group, we consider meritocratic fairness at the second level, which ensures that each arm is pulled according to its merit within the group. Our work shows that we can adapt a UCB-based algorithm to achieve a Bi-Level Fairness by providing (i) anytime Group Exposure Fairness guarantees and (ii) ensuring individual-level Meritocratic Fairness within each group. We first show that one can decompose regret bounds into two components: (a) regret due to anytime group exposure fairness and (b) regret due to meritocratic fairness within each group. Our proposed algorithm BF-UCB 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#39278;&#39135;&#32010;&#20081;&#30456;&#20851;&#30340;&#25512;&#25991;&#36827;&#34892;&#20998;&#31867;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#22312;&#22235;&#20010;&#31867;&#21035;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;F1&#20998;&#25968;&#65292;&#24182;&#34920;&#26126;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#36739;&#20256;&#32479;&#25216;&#26415;&#26356;&#20026;&#20248;&#36234;&#65292;&#20294;&#38656;&#35201;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2402.05571</link><description>&lt;p&gt;
&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20110;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#30340;&#36716;&#25442;&#22120;(BERT)&#30340;&#33258;&#21160;&#20998;&#31867;&#39278;&#39135;&#32010;&#20081;&#25512;&#25991;&#65306;&#31639;&#27861;&#24320;&#21457;&#21644;&#39564;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Traditional Machine Learning Models and Bidirectional Encoder Representations From Transformer (BERT)-Based Automatic Classification of Tweets About Eating Disorders: Algorithm Development and Validation Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05571
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#39278;&#39135;&#32010;&#20081;&#30456;&#20851;&#30340;&#25512;&#25991;&#36827;&#34892;&#20998;&#31867;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#22312;&#22235;&#20010;&#31867;&#21035;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;F1&#20998;&#25968;&#65292;&#24182;&#34920;&#26126;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#36739;&#20256;&#32479;&#25216;&#26415;&#26356;&#20026;&#20248;&#36234;&#65292;&#20294;&#38656;&#35201;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#39278;&#39135;&#32010;&#20081;&#26085;&#30410;&#26222;&#36941;&#65292;&#31038;&#20132;&#32593;&#32476;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#20449;&#24687;&#12290;&#30446;&#26631;&#65306;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#26377;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#19982;&#39278;&#39135;&#32010;&#20081;&#30456;&#20851;&#30340;&#25512;&#25991;&#36827;&#34892;&#20998;&#31867;&#12290;&#26041;&#27861;&#65306;&#22312;&#19977;&#20010;&#26376;&#20869;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#20851;&#20110;&#39278;&#39135;&#32010;&#20081;&#30340;&#25512;&#25991;&#12290;&#25105;&#20204;&#23545;2,000&#26465;&#25512;&#25991;&#36827;&#34892;&#20102;&#26631;&#35760;&#65292;&#26631;&#35760;&#20102;&#65306;(1)&#30001;&#39278;&#39135;&#32010;&#20081;&#24739;&#32773;&#25776;&#20889;&#30340;&#25512;&#25991;&#65292;(2)&#23459;&#20256;&#39278;&#39135;&#32010;&#20081;&#30340;&#25512;&#25991;&#65292;(3)&#20449;&#24687;&#24615;&#65292;&#21644;(4)&#31185;&#23398;&#20869;&#23481;&#12290;&#20351;&#29992;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#35780;&#20272;&#20934;&#30830;&#24230;&#12289;F1&#20998;&#25968;&#21644;&#35745;&#31639;&#26102;&#38388;&#12290;&#32467;&#26524;&#65306;&#20174;1,058,957&#26465;&#25910;&#38598;&#21040;&#30340;&#25512;&#25991;&#20013;&#65292;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#22312;&#25152;&#26377;&#22235;&#20010;&#31867;&#21035;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;F1&#20998;&#25968;(71.1%-86.4%)&#12290;&#32467;&#35770;&#65306;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#22312;&#20998;&#31867;&#19982;&#39278;&#39135;&#32010;&#20081;&#30456;&#20851;&#30340;&#25512;&#25991;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#25216;&#26415;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Eating disorders are increasingly prevalent, and social networks offer valuable information.   Objective: Our goal was to identify efficient machine learning models for categorizing tweets related to eating disorders.   Methods: Over three months, we collected tweets about eating disorders. A 2,000-tweet subset was labeled for: (1) being written by individuals with eating disorders, (2) promoting eating disorders, (3) informativeness, and (4) scientific content. Both traditional machine learning and deep learning models were employed for classification, assessing accuracy, F1 score, and computational time.   Results: From 1,058,957 collected tweets, transformer-based bidirectional encoder representations achieved the highest F1 scores (71.1%-86.4%) across all four categories.   Conclusions: Transformer-based models outperform traditional techniques in classifying eating disorder-related tweets, though they require more computational resources.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(WCE-GNN)&#23454;&#29616;&#20102;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;WCE-GNN&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.05569</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hypergraph Node Classification With Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(WCE-GNN)&#23454;&#29616;&#20102;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;WCE-GNN&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#26159;&#29992;&#26469;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#20851;&#38190;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#25104;&#21151;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#20855;&#26377;&#25104;&#23545;&#20132;&#20114;&#30340;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#36825;&#28608;&#21457;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#20855;&#26377;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#25968;&#25454;&#30340;&#24819;&#27861;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HyperGNNs&#65289;&#30340;&#21457;&#23637;&#12290;GNNs&#21644;HyperGNNs&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#19981;&#21516;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#34987;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#19981;&#21516;&#20960;&#20309;&#25299;&#25169;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22312;&#33410;&#28857;&#20998;&#31867;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#22823;&#22810;&#25968;HyperGNNs&#21487;&#20197;&#20351;&#29992;&#24102;&#26377;&#36229;&#22270;&#30340;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;GNN&#26469;&#36817;&#20284;&#12290;&#36825;&#23548;&#33268;&#20102;WCE-GNN&#65292;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;GNN&#21644;&#19968;&#20010;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#65288;WCE&#65289;&#65292;&#29992;&#20110;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23545;&#20110;&#20061;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;WCE-GNN&#19981;&#20165;&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#65292;&#32780;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs, with hyperedges connecting more than two nodes, are key for modelling higher-order interactions in real-world data. The success of graph neural networks (GNNs) reveals the capability of neural networks to process data with pairwise interactions. This inspires the usage of neural networks for data with higher-order interactions, thereby leading to the development of hypergraph neural networks (HyperGNNs). GNNs and HyperGNNs are typically considered distinct since they are designed for data on different geometric topologies. However, in this paper, we theoretically demonstrate that, in the context of node classification, most HyperGNNs can be approximated using a GNN with a weighted clique expansion of the hypergraph. This leads to WCE-GNN, a simple and efficient framework comprising a GNN and a weighted clique expansion (WCE), for hypergraph node classification. Experiments on nine real-world hypergraph node classification benchmarks showcase that WCE-GNN demonstrates not o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;SHAP&#21644;NSHAP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29305;&#24449;&#20998;&#25104;&#26174;&#33879;&#20132;&#20114;&#30340;&#37096;&#20998;&#65292;&#26500;&#24314;&#20102;&#19968;&#31181;&#31616;&#26126;&#12289;&#26131;&#35299;&#37322;&#30340;&#21152;&#24615;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#26816;&#39564;&#21098;&#26525;&#27425;&#20248;&#35299;&#65292;&#25552;&#39640;&#20102;&#35299;&#37322;&#30340;&#36816;&#34892;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05566</link><description>&lt;p&gt;
&#31616;&#26126;&#32771;&#34385;&#20132;&#20114;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Succint Interaction-Aware Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;SHAP&#21644;NSHAP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29305;&#24449;&#20998;&#25104;&#26174;&#33879;&#20132;&#20114;&#30340;&#37096;&#20998;&#65292;&#26500;&#24314;&#20102;&#19968;&#31181;&#31616;&#26126;&#12289;&#26131;&#35299;&#37322;&#30340;&#21152;&#24615;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#26816;&#39564;&#21098;&#26525;&#27425;&#20248;&#35299;&#65292;&#25552;&#39640;&#20102;&#35299;&#37322;&#30340;&#36816;&#34892;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SHAP&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#35299;&#37322;&#40657;&#31665;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25581;&#31034;&#21508;&#20010;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#26469;&#36827;&#34892;&#35299;&#37322;&#12290;&#30001;&#20110;&#24573;&#30053;&#20102;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;SHAP&#30340;&#35299;&#37322;&#21487;&#33021;&#20250;&#20196;&#20154;&#22256;&#24785;&#29978;&#33267;&#35823;&#23548;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;NSHAP&#25253;&#21578;&#20102;&#25152;&#26377;&#29305;&#24449;&#23376;&#38598;&#30340;&#21152;&#24615;&#37325;&#35201;&#24615;&#12290;&#34429;&#28982;&#36825;&#21253;&#21547;&#20102;&#25152;&#26377;&#30456;&#20114;&#20316;&#29992;&#30340;&#29305;&#24449;&#38598;&#65292;&#20294;&#20063;&#23548;&#33268;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#22823;&#23567;&#30340;&#38590;&#20197;&#35299;&#37322;&#30340;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#36825;&#20004;&#20010;&#26041;&#27861;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#30340;&#26041;&#27861;&#65292;&#23558;&#29305;&#24449;&#20998;&#25104;&#26174;&#33879;&#20132;&#20114;&#30340;&#37096;&#20998;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#37096;&#20998;&#26500;&#25104;&#31616;&#26126;&#12289;&#26131;&#35299;&#37322;&#30340;&#21152;&#24615;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26631;&#20934;&#26469;&#34913;&#37327;&#36825;&#31181;&#20998;&#21306;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#20195;&#34920;&#24615;&#65292;&#25240;&#34935;&#20110;&#24471;&#21040;&#30340;&#35299;&#37322;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#39640;&#25928;&#22320;&#20174;&#36229;&#25351;&#25968;&#25968;&#37327;&#20013;&#25214;&#21040;&#26368;&#20339;&#20998;&#21306;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#32479;&#35745;&#26816;&#39564;&#26469;&#21098;&#26525;&#27425;&#20248;&#35299;&#65292;&#19981;&#20165;&#25552;&#39640;&#20102;&#36816;&#34892;&#26102;&#38388;&#65292;&#36824;&#26377;&#21161;&#20110;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
SHAP is a popular approach to explain black-box models by revealing the importance of individual features. As it ignores feature interactions, SHAP explanations can be confusing up to misleading. NSHAP, on the other hand, reports the additive importance for all subsets of features. While this does include all interacting sets of features, it also leads to an exponentially sized, difficult to interpret explanation. In this paper, we propose to combine the best of these two worlds, by partitioning the features into parts that significantly interact, and use these parts to compose a succinct, interpretable, additive explanation. We derive a criterion by which to measure the representativeness of such a partition for a models behavior, traded off against the complexity of the resulting explanation. To efficiently find the best partition out of super-exponentially many, we show how to prune sub-optimal solutions using a statistical test, which not only improves runtime but also helps to det
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#36951;&#24536;&#22312;&#24322;&#36136;&#25968;&#25454;&#29615;&#22659;&#20013;&#30340;&#20851;&#38190;&#24615;&#36136;&#65292;&#25552;&#20986;&#20102;"&#38378;&#22238;"&#31639;&#27861;&#26469;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#24182;&#21462;&#24471;&#20248;&#24322;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05558</link><description>&lt;p&gt;
&#38378;&#22238;&#65306;&#29702;&#35299;&#21644;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Flashback: Understanding and Mitigating Forgetting in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#36951;&#24536;&#22312;&#24322;&#36136;&#25968;&#25454;&#29615;&#22659;&#20013;&#30340;&#20851;&#38190;&#24615;&#36136;&#65292;&#25552;&#20986;&#20102;"&#38378;&#22238;"&#31639;&#27861;&#26469;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#24182;&#21462;&#24471;&#20248;&#24322;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#36951;&#24536;&#25110;&#32773;&#35828;&#22312;&#19981;&#21516;&#36718;&#27425;&#20013;&#30340;&#30693;&#35782;&#20002;&#22833;&#38459;&#30861;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#65292;&#23588;&#20854;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#23384;&#22312;&#20005;&#37325;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#24773;&#20917;&#19979;&#26356;&#20026;&#26126;&#26174;&#12290;&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#24378;&#35843;&#20102;&#36951;&#24536;&#22312;&#24322;&#36136;&#25968;&#25454;&#29615;&#22659;&#20013;&#23545;&#32852;&#37030;&#23398;&#20064;&#30340;&#20302;&#25928;&#23398;&#20064;&#36215;&#21040;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#30693;&#35782;&#20002;&#22833;&#26082;&#21457;&#29983;&#22312;&#23458;&#25143;&#31471;&#23616;&#37096;&#26356;&#26032;&#20013;&#65292;&#20063;&#21457;&#29983;&#22312;&#26381;&#21153;&#22120;&#31471;&#30340;&#32858;&#21512;&#27493;&#39588;&#20013;&#65307;&#21482;&#35299;&#20915;&#20854;&#20013;&#19968;&#20010;&#32780;&#24573;&#30053;&#21478;&#19968;&#20010;&#26080;&#27861;&#26377;&#25928;&#20943;&#36731;&#36951;&#24536;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24230;&#37327;&#36951;&#24536;&#30340;&#25351;&#26631;&#65292;&#20197;&#30830;&#20445;&#22312;&#26032;&#30693;&#35782;&#33719;&#21462;&#20013;&#26126;&#30830;&#35782;&#21035;&#36951;&#24536;&#12290;&#20511;&#21161;&#36825;&#20123;&#27934;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#38378;&#22238;"&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#21160;&#24577;&#33976;&#39311;&#26041;&#27861;&#26469;&#35268;&#33539;&#21270;&#23616;&#37096;&#27169;&#22411;&#65292;&#24182;&#26377;&#25928;&#22320;&#32858;&#21512;&#23427;&#20204;&#30340;&#30693;&#35782;&#12290;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;"&#38378;&#22238;"&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20943;&#36731;&#20102;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;6&#21040;16&#20010;&#36718;&#27425;&#20869;&#36798;&#21040;&#26356;&#24555;&#30340;&#30446;&#26631;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Federated Learning (FL), forgetting, or the loss of knowledge across rounds, hampers algorithm convergence, particularly in the presence of severe data heterogeneity among clients. This study explores the nuances of this issue, emphasizing the critical role of forgetting in FL's inefficient learning within heterogeneous data contexts. Knowledge loss occurs in both client-local updates and server-side aggregation steps; addressing one without the other fails to mitigate forgetting. We introduce a metric to measure forgetting granularly, ensuring distinct recognition amid new knowledge acquisition. Leveraging these insights, we propose Flashback, an FL algorithm with a dynamic distillation approach that is used to regularize the local models, and effectively aggregate their knowledge. Across different benchmarks, Flashback outperforms other methods, mitigates forgetting, and achieves faster round-to-target-accuracy, by converging in 6 to 16 rounds.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Chebyshev&#23637;&#24320;&#30340;&#25351;&#25968;&#20989;&#25968;&#22810;&#39033;&#24335;&#36924;&#36817;&#26041;&#27861;&#65292;&#23558;&#23398;&#20064;&#37327;&#23376;&#21704;&#23494;&#39039;&#37327;&#38382;&#39064;&#36716;&#21270;&#20026;&#22810;&#39033;&#24335;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#22312;&#26377;&#30028;&#24230;&#25968;&#30340;&#21452;&#37325;&#20132;&#20114;&#22270;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#23398;&#20064;k-&#23616;&#37096;&#21704;&#23494;&#39039;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.05552</link><description>&lt;p&gt;
&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#29992;Chebyshev&#21644;&#27604;&#29305;&#22797;&#26434;&#24230;&#23398;&#20064;&#20219;&#24847;&#28201;&#24230;&#19979;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#37327;
&lt;/p&gt;
&lt;p&gt;
Learning quantum Hamiltonians at any temperature in polynomial time with Chebyshev and bit complexity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Chebyshev&#23637;&#24320;&#30340;&#25351;&#25968;&#20989;&#25968;&#22810;&#39033;&#24335;&#36924;&#36817;&#26041;&#27861;&#65292;&#23558;&#23398;&#20064;&#37327;&#23376;&#21704;&#23494;&#39039;&#37327;&#38382;&#39064;&#36716;&#21270;&#20026;&#22810;&#39033;&#24335;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#22312;&#26377;&#30028;&#24230;&#25968;&#30340;&#21452;&#37325;&#20132;&#20114;&#22270;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#23398;&#20064;k-&#23616;&#37096;&#21704;&#23494;&#39039;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#24050;&#30693;&#36870;&#28201;&#24230;&#19979;&#32473;&#23450;&#37327;&#23376;&#21704;&#23494;&#39039;&#37327;&#30340;&#21508;&#20010;Gibbs&#24577;&#21103;&#26412;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#21442;&#32771;&#20102;Haah&#31561;&#20154;&#30340;&#30740;&#31350;[2108.04842]&#21644;Bakshi&#31561;&#20154;&#30340;&#30740;&#31350;[arXiv:2310.02243]&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#25216;&#26415;&#36129;&#29486;&#26159;&#22522;&#20110;Chebyshev&#23637;&#24320;&#30340;&#25351;&#25968;&#20989;&#25968;&#30340;&#26032;&#22411;&#24179;&#22374;&#22810;&#39033;&#24335;&#36924;&#36817;&#65292;&#36825;&#20351;&#24471;&#23398;&#20064;&#37327;&#23376;&#21704;&#23494;&#39039;&#37327;&#21487;&#20197;&#36716;&#21270;&#20026;&#19968;&#20010;&#22810;&#39033;&#24335;&#20248;&#21270;&#38382;&#39064;&#12290;&#36827;&#32780;&#65292;&#36825;&#21487;&#20197;&#21463;&#30410;&#20110;&#20351;&#29992;&#30697;/&#21322;&#27491;&#23450;&#26494;&#24347;&#26041;&#27861;&#65292;&#20854;&#22810;&#39033;&#24335;&#27604;&#29305;&#22797;&#26434;&#24230;&#38656;&#35201;&#36827;&#34892;&#35880;&#24910;&#20998;&#26512;[O'Donnell&#65292;ITCS 2017]&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#23398;&#20064;&#19968;&#20010;&#26377;&#30028;&#24230;&#25968;&#30340;&#21452;&#37325;&#20132;&#20114;&#22270;&#30340;k-&#23616;&#37096;&#21704;&#23494;&#39039;&#37327;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning local quantum Hamiltonians given copies of their Gibbs state at a known inverse temperature, following Haah et al. [2108.04842] and Bakshi et al. [arXiv:2310.02243]. Our main technical contribution is a new flat polynomial approximation of the exponential function based on the Chebyshev expansion, which enables the formulation of learning quantum Hamiltonians as a polynomial optimization problem. This, in turn, can benefit from the use of moment/SOS relaxations, whose polynomial bit complexity requires careful analysis [O'Donnell, ITCS 2017]. Finally, we show that learning a $k$-local Hamiltonian, whose dual interaction graph is of bounded degree, runs in polynomial time under mild assumptions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#32773;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#22823;&#22411;&#27169;&#22411;&#65292;&#24182;&#19988;&#27604;&#22522;&#32447;&#26041;&#27861;&#22312;&#22810;&#20219;&#21153;&#35757;&#32451;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#36890;&#36807;&#24341;&#20837;Perceiver-based&#28436;&#21592;-&#35780;&#35770;&#32773;&#27169;&#22411;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19982;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#36328;&#27880;&#24847;&#21147;&#27169;&#22359;&#37197;&#21512;&#30340;&#20851;&#38190;&#27169;&#22411;&#29305;&#24449;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#21457;&#29616;&#34920;&#26126;&#65306;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#32773;&#31639;&#27861;&#26159;&#36880;&#28176;&#25670;&#33073;&#34892;&#20026;&#20811;&#38534;&#33539;&#24335;&#30340;&#19968;&#31181;&#33258;&#28982;&#36873;&#25321;&#65292;&#24182;&#19988;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#20174;&#27425;&#20248;&#31034;&#33539;&#25110;&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#25484;&#25569;&#22810;&#20010;&#39046;&#22495;&#30340;&#22810;&#20219;&#21153;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.05546</link><description>&lt;p&gt;
&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#32773;&#24378;&#21270;&#23398;&#20064;&#25193;&#23637;&#21040;&#22823;&#22411;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Offline Actor-Critic Reinforcement Learning Scales to Large Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#32773;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#22823;&#22411;&#27169;&#22411;&#65292;&#24182;&#19988;&#27604;&#22522;&#32447;&#26041;&#27861;&#22312;&#22810;&#20219;&#21153;&#35757;&#32451;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#36890;&#36807;&#24341;&#20837;Perceiver-based&#28436;&#21592;-&#35780;&#35770;&#32773;&#27169;&#22411;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19982;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#36328;&#27880;&#24847;&#21147;&#27169;&#22359;&#37197;&#21512;&#30340;&#20851;&#38190;&#27169;&#22411;&#29305;&#24449;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#21457;&#29616;&#34920;&#26126;&#65306;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#32773;&#31639;&#27861;&#26159;&#36880;&#28176;&#25670;&#33073;&#34892;&#20026;&#20811;&#38534;&#33539;&#24335;&#30340;&#19968;&#31181;&#33258;&#28982;&#36873;&#25321;&#65292;&#24182;&#19988;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#20174;&#27425;&#20248;&#31034;&#33539;&#25110;&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#25484;&#25569;&#22810;&#20010;&#39046;&#22495;&#30340;&#22810;&#20219;&#21153;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#32773;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#25193;&#23637;&#21040;&#22823;&#22411;&#27169;&#22411;&#65292;&#22914;transformer&#65292;&#24182;&#19988;&#36981;&#24490;&#19982;&#30417;&#30563;&#23398;&#20064;&#31867;&#20284;&#30340;&#25193;&#23637;&#35268;&#24459;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#32773;&#31639;&#27861;&#22312;&#21253;&#21547;132&#20010;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#30340;&#22810;&#20219;&#21153;&#35757;&#32451;&#20013;&#65292;&#21487;&#20197;&#32988;&#36807;&#24378;&#22823;&#30340;&#30417;&#30563;&#24335;&#34892;&#20026;&#20811;&#38534;&#22522;&#32447;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#27425;&#20248;&#21644;&#19987;&#23478;&#34892;&#20026;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;Perceiver&#30340;&#28436;&#21592;-&#35780;&#35770;&#32773;&#27169;&#22411;&#65292;&#24182;&#38416;&#26126;&#20102;&#20351;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19982;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#36328;&#27880;&#24847;&#21147;&#27169;&#22359;&#37197;&#21512;&#24037;&#20316;&#25152;&#38656;&#30340;&#20851;&#38190;&#27169;&#22411;&#29305;&#24449;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;i&#65289;&#31616;&#21333;&#30340;&#31163;&#32447;&#28436;&#21592;&#35780;&#35770;&#32773;&#31639;&#27861;&#26159;&#36880;&#28176;&#36828;&#31163;&#24403;&#21069;&#20027;&#27969;&#34892;&#20026;&#20811;&#38534;&#33539;&#24335;&#30340;&#33258;&#28982;&#36873;&#25321;&#65292;ii&#65289;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#21487;&#20197;&#20174;&#27425;&#20248;&#31034;&#33539;&#25110;&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#25484;&#25569;&#35768;&#22810;&#39046;&#22495;&#30340;&#22810;&#20219;&#21153;&#31574;&#30053;&#65292;&#21253;&#25324;&#30495;&#23454;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that offline actor-critic reinforcement learning can scale to large models - such as transformers - and follows similar scaling laws as supervised learning. We find that offline actor-critic algorithms can outperform strong, supervised, behavioral cloning baselines for multi-task training on a large dataset containing both sub-optimal and expert behavior on 132 continuous control tasks. We introduce a Perceiver-based actor-critic model and elucidate the key model features needed to make offline RL work with self- and cross-attention modules. Overall, we find that: i) simple offline actor critic algorithms are a natural choice for gradually moving away from the currently predominant paradigm of behavioral cloning, and ii) via offline RL it is possible to learn multi-task policies that master many domains simultaneously, including real robotics tasks, from sub-optimal demonstrations or self-generated data.
&lt;/p&gt;</description></item><item><title>&#26412;&#31456;&#20171;&#32461;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#32452;&#23398;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#33008;&#33146;&#30284;&#30340;&#22522;&#22240;&#32452;&#23398;&#21644;&#20813;&#30123;&#32452;&#23398;&#32508;&#21512;&#20998;&#26512;&#20013;&#65292;&#36890;&#36807;&#38543;&#26426;&#26862;&#26519;&#21644;&#24809;&#32602;&#24615;&#22810;&#39033;&#24335;&#36923;&#36753;&#22238;&#24402;&#26469;&#39044;&#27979;&#33008;&#33146;&#30284;&#20813;&#30123;&#28024;&#28070;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#20351;&#29992;&#20851;&#32852;&#35268;&#21017;&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05543</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#32452;&#23398;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Machine learning applied to omics data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#20171;&#32461;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#32452;&#23398;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#33008;&#33146;&#30284;&#30340;&#22522;&#22240;&#32452;&#23398;&#21644;&#20813;&#30123;&#32452;&#23398;&#32508;&#21512;&#20998;&#26512;&#20013;&#65292;&#36890;&#36807;&#38543;&#26426;&#26862;&#26519;&#21644;&#24809;&#32602;&#24615;&#22810;&#39033;&#24335;&#36923;&#36753;&#22238;&#24402;&#26469;&#39044;&#27979;&#33008;&#33146;&#30284;&#20813;&#30123;&#28024;&#28070;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#20351;&#29992;&#20851;&#32852;&#35268;&#21017;&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#31456;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20123;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#32452;&#23398;&#25968;&#25454;&#30340;&#32972;&#26223;&#19979;&#30340;&#24212;&#29992;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#22238;&#39038;&#21644;&#35780;&#20272;&#20102;&#38543;&#26426;&#26862;&#26519;&#21644;&#24809;&#32602;&#24615;&#22810;&#39033;&#24335;&#36923;&#36753;&#22238;&#24402;&#22312;&#33008;&#33146;&#30284;&#22522;&#22240;&#32452;&#23398;&#21644;&#20813;&#30123;&#32452;&#23398;&#30340;&#32508;&#21512;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#20851;&#32852;&#35268;&#21017;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#20811;&#26381;&#20808;&#21069;&#25552;&#21040;&#30340;&#27169;&#22411;&#39044;&#27979;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#22238;&#39038;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#26469;&#33258;TCGA&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;107&#20010;&#32959;&#30244;&#24615;&#33008;&#33146;&#26679;&#21697;&#21644;117,486&#20010;&#31181;&#31995;SNP&#32452;&#25104;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#39044;&#27979;&#33008;&#33146;&#30284;&#20813;&#30123;&#28024;&#28070;&#20013;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this chapter we illustrate the use of some Machine Learning techniques in the context of omics data. More precisely, we review and evaluate the use of Random Forest and Penalized Multinomial Logistic Regression for integrative analysis of genomics and immunomics in pancreatic cancer. Furthermore, we propose the use of association rules with predictive purposes to overcome the low predictive power of the previously mentioned models. Finally, we apply the reviewed methods to a real data set from TCGA made of 107 tumoral pancreatic samples and 117,486 germline SNPs, showing the good performance of the proposed methods to predict the immunological infiltration in pancreatic cancer.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#24378;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;RFL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#33258;&#36866;&#24212;&#22320;&#20248;&#21270;&#23458;&#25143;&#36129;&#29486;&#30340;&#32858;&#21512;&#36807;&#31243;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#22312;&#38750;&#30456;&#21516;&#20998;&#24067;&#29615;&#22659;&#19979;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05541</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#40065;&#26834;&#21644;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;&#30340;&#20652;&#21270;&#21058;&#65306;&#35299;&#23494;&#23458;&#25143;&#36129;&#29486;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning as a Catalyst for Robust and Fair Federated Learning: Deciphering the Dynamics of Client Contributions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#24378;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;RFL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#33258;&#36866;&#24212;&#22320;&#20248;&#21270;&#23458;&#25143;&#36129;&#29486;&#30340;&#32858;&#21512;&#36807;&#31243;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#22312;&#38750;&#30456;&#21516;&#20998;&#24067;&#29615;&#22659;&#19979;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#20135;&#29983;&#20102;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#20998;&#25955;&#30340;&#35774;&#22791;&#25110;&#31995;&#32479;&#19978;&#35757;&#32451;&#26469;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#24182;&#20445;&#30041;&#26412;&#22320;&#25968;&#25454;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31574;&#30053;&#32463;&#24120;&#24573;&#35270;&#32479;&#35745;&#24322;&#36136;&#24615;&#21644;&#23545;&#25932;&#23545;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#25152;&#24102;&#26469;&#30340;&#22256;&#38590;&#65292;&#36825;&#20123;&#22240;&#32032;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;&#20010;&#24615;&#21270;&#30340;FL&#31574;&#30053;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#27169;&#22411;&#26469;&#36866;&#24212;&#20010;&#21035;&#23458;&#25143;&#30340;&#29305;&#28857;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#26381;&#21153;&#22120;&#31471;&#32858;&#21512;&#30340;&#33030;&#24369;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24378;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;RFL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#33258;&#36866;&#24212;&#20248;&#21270;&#32858;&#21512;&#36807;&#31243;&#20013;&#23458;&#25143;&#36129;&#29486;&#30340;&#26032;&#26694;&#26550;&#65292;&#20174;&#32780;&#22686;&#24378;&#24694;&#24847;&#23458;&#25143;&#19979;&#30340;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20844;&#24179;&#24615;&#22312;&#38750;&#30456;&#21516;&#20998;&#24067;&#29615;&#22659;&#19979;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#33268;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#21327;&#21516;&#35757;&#32451;&#65292;&#20197;&#20248;&#21270;&#23458;&#25143;&#36129;&#29486;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in federated learning (FL) have produced models that retain user privacy by training across multiple decentralized devices or systems holding local data samples. However, these strategies often neglect the inherent challenges of statistical heterogeneity and vulnerability to adversarial attacks, which can degrade model robustness and fairness. Personalized FL strategies offer some respite by adjusting models to fit individual client profiles, yet they tend to neglect server-side aggregation vulnerabilities. To address these issues, we propose Reinforcement Federated Learning (RFL), a novel framework that leverages deep reinforcement learning to adaptively optimize client contribution during aggregation, thereby enhancing both model robustness against malicious clients and fairness across participants under non-identically distributed settings. To achieve this goal, we propose a meticulous approach involving a Deep Deterministic Policy Gradient-based algorithm for co
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#30693;&#35782;&#22270;&#35889;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#20511;&#21161;&#23454;&#20307;&#35782;&#21035;&#21644;&#38142;&#25509;&#25216;&#26415;&#20197;&#21450;&#19978;&#19979;&#25991;&#21270;&#23884;&#20837;&#65292;&#22686;&#24378;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#20998;&#31867;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#20027;&#35201;&#24212;&#29992;&#20110;&#20581;&#24247;&#39046;&#22495;&#30340;&#39278;&#39135;&#38556;&#30861;&#35782;&#21035;&#65292;&#20026;&#26089;&#26399;&#35786;&#26029;&#25552;&#20379;&#24110;&#21161;&#12290;</title><link>https://arxiv.org/abs/2402.05536</link><description>&lt;p&gt;
&#20026;&#21152;&#24378;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#30340;&#39278;&#39135;&#38556;&#30861;&#26816;&#27979;&#65292;&#36171;&#33021;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35821;&#22659;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Empowering machine learning models with contextual knowledge for enhancing the detection of eating disorders in social media posts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05536
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#30693;&#35782;&#22270;&#35889;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#20511;&#21161;&#23454;&#20307;&#35782;&#21035;&#21644;&#38142;&#25509;&#25216;&#26415;&#20197;&#21450;&#19978;&#19979;&#25991;&#21270;&#23884;&#20837;&#65292;&#22686;&#24378;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#20998;&#31867;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#20027;&#35201;&#24212;&#29992;&#20110;&#20581;&#24247;&#39046;&#22495;&#30340;&#39278;&#39135;&#38556;&#30861;&#35782;&#21035;&#65292;&#20026;&#26089;&#26399;&#35786;&#26029;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#22312;&#20449;&#24687;&#20849;&#20139;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#22312;&#20581;&#24247;&#39046;&#22495;&#29992;&#20110;&#35752;&#35770;&#30142;&#30149;&#21644;&#27835;&#30103;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24179;&#21488;&#19978;&#30340;&#24086;&#23376;&#24448;&#24448;&#26159;&#31616;&#30701;&#30340;&#25991;&#26412;&#65292;&#32473;&#20154;&#24037;&#26234;&#33021;&#22312;&#29702;&#35299;&#19978;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#32467;&#21512;&#31038;&#21306;&#32500;&#25252;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;&#22914;Wikidata&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#20197;&#22686;&#24378;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20808;&#36827;&#30340;&#23454;&#20307;&#35782;&#21035;&#22120;&#21644;&#38142;&#25509;&#22120;&#65288;&#22914;Falcon 2.0&#65289;&#23558;&#30701;&#25991;&#24086;&#23376;&#20013;&#30340;&#23454;&#20307;&#36830;&#25509;&#21040;&#30693;&#35782;&#22270;&#35889;&#12290;&#28982;&#21518;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGEs&#65289;&#21644;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#65288;&#22914;BERT&#65289;&#26469;&#21019;&#24314;&#20016;&#23500;&#30340;&#12289;&#22522;&#20110;&#35821;&#22659;&#30340;&#24086;&#23376;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#20581;&#24247;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#35782;&#21035;&#19982;&#39278;&#39135;&#38556;&#30861;&#30456;&#20851;&#30340;&#24086;&#23376;&#65288;&#22914;&#21388;&#39135;&#30151;&#12289;&#26292;&#39135;&#30151;&#65289;&#65292;&#20197;&#24110;&#21161;&#21307;&#30103;&#26381;&#21153;&#25552;&#20379;&#32773;&#36827;&#34892;&#26089;&#26399;&#35786;&#26029;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21253;&#21547;2,000&#26465;&#20851;&#20110;&#39278;&#39135;&#38556;&#30861;&#30340;&#25512;&#25991;&#30340;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#21512;&#24182;&#21333;&#35789;&#23884;&#20837;&#21644;&#19978;&#19979;&#25991;&#21270;&#23884;&#20837;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social networks are vital for information sharing, especially in the health sector for discussing diseases and treatments. These platforms, however, often feature posts as brief texts, posing challenges for Artificial Intelligence (AI) in understanding context. We introduce a novel hybrid approach combining community-maintained knowledge graphs (like Wikidata) with deep learning to enhance the categorization of social media posts. This method uses advanced entity recognizers and linkers (like Falcon 2.0) to connect short post entities to knowledge graphs. Knowledge graph embeddings (KGEs) and contextualized word embeddings (like BERT) are then employed to create rich, context-based representations of these posts.   Our focus is on the health domain, particularly in identifying posts related to eating disorders (e.g., anorexia, bulimia) to aid healthcare providers in early diagnosis. We tested our approach on a dataset of 2,000 tweets about eating disorders, finding that merging word em
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20197;&#24322;&#27493;&#26041;&#24335;&#25805;&#20316;&#30340;&#20195;&#29702;&#32593;&#32476;&#65292;&#26088;&#22312;&#21457;&#29616;&#36866;&#21512;&#20010;&#20307;&#26412;&#22320;&#25968;&#25454;&#38598;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#36890;&#36807;&#35777;&#26126;&#24322;&#27493;&#25193;&#25955;&#31574;&#30053;&#22312;&#22343;&#26041;&#35823;&#24046;&#19978;&#30340;&#31283;&#23450;&#24615;&#65292;&#20197;&#21450;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#20445;&#35777;&#65292;&#26412;&#30740;&#31350;&#23545;&#20110;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2402.05529</link><description>&lt;p&gt;
&#24322;&#27493;&#25193;&#25955;&#23398;&#20064;&#20013;&#30340;&#20195;&#29702;&#23376;&#37319;&#26679;&#19982;&#23616;&#37096;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Diffusion Learning with Agent Subsampling and Local Updates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20197;&#24322;&#27493;&#26041;&#24335;&#25805;&#20316;&#30340;&#20195;&#29702;&#32593;&#32476;&#65292;&#26088;&#22312;&#21457;&#29616;&#36866;&#21512;&#20010;&#20307;&#26412;&#22320;&#25968;&#25454;&#38598;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#36890;&#36807;&#35777;&#26126;&#24322;&#27493;&#25193;&#25955;&#31574;&#30053;&#22312;&#22343;&#26041;&#35823;&#24046;&#19978;&#30340;&#31283;&#23450;&#24615;&#65292;&#20197;&#21450;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#20445;&#35777;&#65292;&#26412;&#30740;&#31350;&#23545;&#20110;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#32452;&#20197;&#24322;&#27493;&#26041;&#24335;&#25805;&#20316;&#30340;&#20195;&#29702;&#65292;&#26088;&#22312;&#21457;&#29616;&#36866;&#21512;&#20010;&#20307;&#26412;&#22320;&#25968;&#25454;&#38598;&#30340;&#29702;&#24819;&#20840;&#23616;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#27599;&#20010;&#20195;&#29702;&#29420;&#31435;&#36873;&#25321;&#20309;&#26102;&#21442;&#19982;&#31639;&#27861;&#65292;&#24182;&#22312;&#20219;&#20309;&#32473;&#23450;&#26102;&#21051;&#36873;&#25321;&#19982;&#20043;&#21512;&#20316;&#30340;&#37051;&#22495;&#30340;&#29305;&#23450;&#23376;&#38598;&#12290;&#24403;&#20195;&#29702;&#36873;&#25321;&#21442;&#19982;&#26102;&#65292;&#23427;&#20250;&#22312;&#23558;&#32467;&#26524;&#20256;&#36798;&#32473;&#23376;&#37319;&#26679;&#37051;&#22495;&#20043;&#21069;&#32463;&#21382;&#22810;&#27425;&#23616;&#37096;&#26356;&#26032;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24471;&#21040;&#30340;&#24322;&#27493;&#25193;&#25955;&#31574;&#30053;&#22312;&#22343;&#26041;&#35823;&#24046;&#24847;&#20041;&#19978;&#26159;&#31283;&#23450;&#30340;&#65292;&#24182;&#23545;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#25552;&#20379;&#20102;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#26469;&#35828;&#26126;&#30740;&#31350;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we examine a network of agents operating asynchronously, aiming to discover an ideal global model that suits individual local datasets. Our assumption is that each agent independently chooses when to participate throughout the algorithm and the specific subset of its neighbourhood with which it will cooperate at any given moment. When an agent chooses to take part, it undergoes multiple local updates before conveying its outcomes to the sub-sampled neighbourhood. Under this setup, we prove that the resulting asynchronous diffusion strategy is stable in the mean-square error sense and provide performance guarantees specifically for the federated learning setting. We illustrate the findings with numerical simulations.
&lt;/p&gt;</description></item><item><title>&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20855;&#26377;&#36328;&#25209;&#27425;&#20381;&#36182;&#30340;&#19987;&#23478;&#36335;&#30001;&#31574;&#30053;&#26131;&#21463;&#25915;&#20987;&#65292;&#24694;&#24847;&#26597;&#35810;&#21487;&#20197;&#24433;&#21709;&#27169;&#22411;&#23545;&#20854;&#20182;&#33391;&#24615;&#26597;&#35810;&#30340;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2402.05526</link><description>&lt;p&gt;
&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#20013;&#30340;&#32531;&#20914;&#21306;&#28322;&#20986;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Buffer Overflow in Mixture of Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05526
&lt;/p&gt;
&lt;p&gt;
&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20855;&#26377;&#36328;&#25209;&#27425;&#20381;&#36182;&#30340;&#19987;&#23478;&#36335;&#30001;&#31574;&#30053;&#26131;&#21463;&#25915;&#20987;&#65292;&#24694;&#24847;&#26597;&#35810;&#21487;&#20197;&#24433;&#21709;&#27169;&#22411;&#23545;&#20854;&#20182;&#33391;&#24615;&#26597;&#35810;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20445;&#25345;&#25512;&#29702;&#25104;&#26412;&#31283;&#23450;&#30340;&#21516;&#26102;&#65292;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;MoE&#65289;&#24050;&#25104;&#20026;&#25193;&#23637;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#20851;&#38190;&#35201;&#32032;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20855;&#26377;&#36328;&#25209;&#27425;&#20381;&#36182;&#30340;&#19987;&#23478;&#36335;&#30001;&#31574;&#30053;&#26131;&#21463;&#25915;&#20987;&#12290;&#22914;&#26524;&#24694;&#24847;&#26597;&#35810;&#19982;&#33391;&#24615;&#26597;&#35810;&#20998;&#32452;&#22312;&#21516;&#19968;&#25209;&#27425;&#20013;&#65292;&#24694;&#24847;&#26597;&#35810;&#20250;&#24433;&#21709;&#27169;&#22411;&#23545;&#20854;&#20182;&#33391;&#24615;&#26597;&#35810;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#29609;&#20855;&#23454;&#39564;&#29615;&#22659;&#20013;&#36827;&#34892;&#27010;&#24565;&#39564;&#35777;&#25915;&#20987;&#26469;&#35777;&#26126;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixture of Experts (MoE) has become a key ingredient for scaling large foundation models while keeping inference costs steady. We show that expert routing strategies that have cross-batch dependencies are vulnerable to attacks. Malicious queries can be sent to a model and can affect a model's output on other benign queries if they are grouped in the same batch. We demonstrate this via a proof-of-concept attack in a toy experimental setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#31163;&#32447;&#25968;&#25454;&#20013;&#30340;&#38544;&#31169;&#27169;&#22411;&#20197;&#21450;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#35757;&#32451;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#24635;&#32467;&#20102;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#38544;&#31169;&#30340;&#20195;&#20215;&#12290;</title><link>https://arxiv.org/abs/2402.05525</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Model-Based Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#31163;&#32447;&#25968;&#25454;&#20013;&#30340;&#38544;&#31169;&#27169;&#22411;&#20197;&#21450;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#35757;&#32451;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#24635;&#32467;&#20102;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#38544;&#31169;&#30340;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20855;&#26377;&#38544;&#31169;&#20445;&#35777;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#35757;&#32451;&#19968;&#20010;&#30456;&#23545;&#20110;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#36712;&#36857;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DP-MORL&#65292;&#19968;&#31181;&#24102;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;MBRL&#31639;&#27861;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;DP-FedAvg&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#29615;&#22659;&#30340;&#38544;&#31169;&#27169;&#22411;&#65292;DP-FedAvg&#26159;&#19968;&#31181;&#20026;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#36712;&#36857;&#32423;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#20248;&#21270;&#20174;&#65288;&#21463;&#32602;&#30340;&#65289;&#38544;&#31169;&#27169;&#22411;&#20013;&#25512;&#23548;&#20986;&#31574;&#30053;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#19982;&#31995;&#32479;&#20132;&#20114;&#25110;&#35775;&#38382;&#36755;&#20837;&#25968;&#25454;&#12290;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#65292;DP-MORL&#33021;&#22815;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#35757;&#32451;&#20986;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;RL&#20195;&#29702;&#65292;&#24182;&#36827;&#19968;&#27493;&#27010;&#36848;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#38544;&#31169;&#30340;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address offline reinforcement learning with privacy guarantees, where the goal is to train a policy that is differentially private with respect to individual trajectories in the dataset. To achieve this, we introduce DP-MORL, an MBRL algorithm coming with differential privacy guarantees. A private model of the environment is first learned from offline data using DP-FedAvg, a training method for neural networks that provides differential privacy guarantees at the trajectory level. Then, we use model-based policy optimization to derive a policy from the (penalized) private model, without any further interaction with the system or access to the input data. We empirically show that DP-MORL enables the training of private RL agents from offline data and we furthermore outline the price of privacy in this setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RLNet&#30340;&#40065;&#26834;&#32447;&#24615;&#21270;&#32593;&#32476;&#65292;&#36890;&#36807;&#20943;&#23569;&#24310;&#36831;&#24182;&#25913;&#21892;&#27169;&#22411;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#32780;&#40065;&#26834;&#30340;&#38544;&#31169;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.05521</link><description>&lt;p&gt;
&#32447;&#24615;&#21270;&#27169;&#22411;&#20197;&#23454;&#29616;&#39640;&#25928;&#32780;&#40065;&#26834;&#30340;&#38544;&#31169;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Linearizing Models for Efficient yet Robust Private Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RLNet&#30340;&#40065;&#26834;&#32447;&#24615;&#21270;&#32593;&#32476;&#65292;&#36890;&#36807;&#20943;&#23569;&#24310;&#36831;&#24182;&#25913;&#21892;&#27169;&#22411;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#32780;&#40065;&#26834;&#30340;&#38544;&#31169;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#26085;&#30410;&#20851;&#27880;&#23548;&#33268;&#20102;&#23458;&#25143;&#31471;-&#26381;&#21153;&#22120;&#24212;&#29992;&#20013;&#31169;&#26377;&#25512;&#29702;&#65288;PI&#65289;&#26694;&#26550;&#30340;&#21457;&#23637;&#65292;&#35813;&#26694;&#26550;&#26082;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#21448;&#20445;&#25252;&#27169;&#22411;&#30693;&#35782;&#20135;&#26435;&#12290;&#28982;&#32780;&#65292;&#25152;&#38656;&#30340;&#23494;&#30721;&#21407;&#35821;&#23548;&#33268;&#20102;&#26174;&#33879;&#30340;&#24310;&#36831;&#24320;&#38144;&#65292;&#38480;&#21046;&#20102;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#35201;&#27714;PI&#26381;&#21153;&#23545;&#21508;&#31181;&#33258;&#28982;&#21457;&#29983;&#30340;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#23613;&#31649;&#24050;&#26377;&#19968;&#20123;&#24037;&#20316;&#19987;&#27880;&#20110;&#24320;&#21457;&#36866;&#29992;&#20110;PI&#30340;&#24310;&#36831;-&#39640;&#25928;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#23545;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;RLNet&#65292;&#19968;&#31181;&#40065;&#26834;&#30340;&#32447;&#24615;&#21270;&#32593;&#32476;&#65292;&#36890;&#36807;&#20943;&#23569;&#39640;&#24310;&#36831;&#30340;ReLU&#25805;&#20316;&#25552;&#20379;&#24310;&#36831;&#25913;&#36827;&#65292;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#22312;&#28165;&#26224;&#22270;&#20687;&#21644;&#25439;&#22351;&#22270;&#20687;&#19978;&#30340;&#24615;&#33021;&#12290;&#29305;&#21035;&#22320;&#65292;RLNet&#27169;&#22411;&#25552;&#20379;&#20102;&#22312;&#28165;&#26224;&#22270;&#20687;&#12289;&#33258;&#28982;&#25200;&#21160;&#22270;&#20687;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#25200;&#21160;&#22270;&#20687;&#19978;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#25913;&#21892;&#30340;&#8220;&#19977;&#36830;&#36194;&#8221;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing concern about data privacy has led to the development of private inference (PI) frameworks in client-server applications which protects both data privacy and model IP. However, the cryptographic primitives required yield significant latency overhead which limits its wide-spread application. At the same time, changing environments demand the PI service to be robust against various naturally occurring and gradient-based perturbations. Despite several works focused on the development of latency-efficient models suitable for PI, the impact of these models on robustness has remained unexplored. Towards this goal, this paper presents RLNet, a class of robust linearized networks that can yield latency improvement via reduction of high-latency ReLU operations while improving the model performance on both clean and corrupted images. In particular, RLNet models provide a "triple win ticket" of improved classification accuracy on clean, naturally perturbed, and gradient-based perturbe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#30740;&#20102;&#22312;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#20013;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22686;&#24378;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#23398;&#20248;&#21270;&#30340;&#34701;&#21512;&#35270;&#37326;&#12290;</title><link>https://arxiv.org/abs/2402.05501</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22686;&#24378;&#30340;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#20013;&#30340;&#20998;&#25903;&#23450;&#30028;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Augmented Branch and Bound for Mixed Integer Linear Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#30740;&#20102;&#22312;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#20013;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22686;&#24378;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#23398;&#20248;&#21270;&#30340;&#34701;&#21512;&#35270;&#37326;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;MILP&#65289;&#26159;&#25968;&#23398;&#20248;&#21270;&#30340;&#22522;&#30784;&#65292;&#20026;&#21508;&#31181;&#24212;&#29992;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#24314;&#27169;&#35821;&#35328;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;MILP&#27714;&#35299;&#31639;&#27861;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#31639;&#27861;&#36827;&#23637;&#65292;&#35768;&#22810;&#21830;&#19994;&#21644;&#23398;&#26415;&#36719;&#20214;&#21253;&#20063;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#26469;&#33258;&#38382;&#39064;&#23454;&#20363;&#21644;&#27714;&#35299;&#22120;&#30340;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#20197;&#21450;&#35299;&#20915;&#26032;&#38382;&#39064;&#21644;&#26356;&#22823;&#65288;&#29616;&#23454;&#29983;&#27963;&#20013;&#65289;&#30340;&#23454;&#20363;&#30340;&#38656;&#27714;&#65292;&#35302;&#21457;&#20102;&#31639;&#27861;&#25345;&#32493;&#21457;&#23637;&#30340;&#38656;&#27714;&#12290;MILP&#27714;&#35299;&#22120;&#20351;&#29992;&#20998;&#25903;&#23450;&#30028;&#20316;&#20026;&#20854;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#22686;&#24378;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#30340;&#25152;&#26377;&#20027;&#35201;&#20219;&#21153;&#20013;&#65292;&#22914;&#21407;&#22987;&#21551;&#21457;&#24335;&#12289;&#20998;&#25903;&#12289;&#21106;&#24179;&#38754;&#12289;&#33410;&#28857;&#36873;&#25321;&#21644;&#27714;&#35299;&#22120;&#37197;&#32622;&#20915;&#31574;&#26041;&#38754;&#65292;&#21462;&#24471;&#20102;&#29190;&#28856;&#24615;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#36890;&#36807;&#35843;&#30740;&#20171;&#32461;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#28041;&#21450;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#23398;&#20248;&#21270;&#30340;&#34701;&#21512;&#35270;&#37326;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixed Integer Linear Programming (MILP) is a pillar of mathematical optimization that offers a powerful modeling language for a wide range of applications. During the past decades, enormous algorithmic progress has been made in solving MILPs, and many commercial and academic software packages exist. Nevertheless, the availability of data, both from problem instances and from solvers, and the desire to solve new problems and larger (real-life) instances, trigger the need for continuing algorithmic development. MILP solvers use branch and bound as their main component. In recent years, there has been an explosive development in the use of machine learning algorithms for enhancing all main tasks involved in the branch-and-bound algorithm, such as primal heuristics, branching, cutting planes, node selection and solver configuration decisions. This paper presents a survey of such approaches, addressing the vision of integration of machine learning and mathematical optimization as complement
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21644;&#29305;&#24449;&#22686;&#24378;&#25216;&#26415;&#39044;&#27979;&#24515;&#33039;&#30142;&#30149;&#39118;&#38505;&#65292;&#22312;&#22823;&#35268;&#27169;&#20154;&#32676;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.05495</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#29305;&#24449;&#22686;&#24378;&#39044;&#27979;&#24515;&#33039;&#30142;&#30149;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Heart disease risk prediction using deep learning techniques with feature augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21644;&#29305;&#24449;&#22686;&#24378;&#25216;&#26415;&#39044;&#27979;&#24515;&#33039;&#30142;&#30149;&#39118;&#38505;&#65292;&#22312;&#22823;&#35268;&#27169;&#20154;&#32676;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#34880;&#31649;&#30142;&#30149;&#26159;&#26222;&#36890;&#20154;&#32676;&#27515;&#20129;&#39118;&#38505;&#26368;&#22823;&#30340;&#30142;&#30149;&#20043;&#19968;&#12290;&#23545;&#24515;&#33039;&#30142;&#30149;&#30340;&#36831;&#26089;&#21457;&#29616;&#26497;&#22823;&#22320;&#24433;&#21709;&#30528;&#24739;&#32773;&#30340;&#29983;&#23384;&#26426;&#20250;&#12290;&#24180;&#40836;&#12289;&#24615;&#21035;&#12289;&#32966;&#22266;&#37255;&#27700;&#24179;&#12289;&#34880;&#31958;&#27700;&#24179;&#12289;&#24515;&#29575;&#31561;&#22240;&#32032;&#24050;&#30693;&#23545;&#21361;&#21450;&#29983;&#21629;&#30340;&#24515;&#33039;&#38382;&#39064;&#26377;&#24433;&#21709;&#65292;&#20294;&#30001;&#20110;&#21464;&#37327;&#30340;&#25968;&#37327;&#36739;&#22810;&#65292;&#19987;&#23478;&#38590;&#20197;&#35780;&#20272;&#27599;&#20010;&#24739;&#32773;&#26102;&#32771;&#34385;&#21040;&#36825;&#20123;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#29305;&#24449;&#22686;&#24378;&#25216;&#26415;&#65292;&#35780;&#20272;&#24739;&#32773;&#26159;&#21542;&#26377;&#24739;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#39118;&#38505;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#32467;&#26524;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;4.4%&#65292;&#20351;&#20934;&#30830;&#29575;&#36798;&#21040;90%&#65292;&#36825;&#22312;&#22823;&#35268;&#27169;&#20154;&#32676;&#21463;&#21040;&#36825;&#31181;&#22256;&#25200;&#30340;&#24773;&#20917;&#19979;&#65292;&#26159;&#19968;&#20010;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cardiovascular diseases state as one of the greatest risks of death for the general population. Late detection in heart diseases highly conditions the chances of survival for patients. Age, sex, cholesterol level, sugar level, heart rate, among other factors, are known to have an influence on life-threatening heart problems, but, due to the high amount of variables, it is often difficult for an expert to evaluate each patient taking this information into account. In this manuscript, the authors propose using deep learning methods, combined with feature augmentation techniques for evaluating whether patients are at risk of suffering cardiovascular disease. The results of the proposed methods outperform other state of the art methods by 4.4%, leading to a precision of a 90%, which presents a significant improvement, even more so when it comes to an affliction that affects a large population.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22768;&#38899;&#20998;&#26512;&#30340;&#38750;&#20405;&#20837;&#24615;&#25216;&#26415;&#65292;&#20351;&#29992;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#26469;&#30830;&#23450;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#30340;&#20005;&#37325;&#31243;&#24230;&#21644;&#30142;&#30149;&#36827;&#23637;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05491</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#30830;&#23450;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#30340;&#20005;&#37325;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Determining the severity of Parkinson's disease in patients using a multi task neural network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22768;&#38899;&#20998;&#26512;&#30340;&#38750;&#20405;&#20837;&#24615;&#25216;&#26415;&#65292;&#20351;&#29992;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#26469;&#30830;&#23450;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#30340;&#20005;&#37325;&#31243;&#24230;&#21644;&#30142;&#30149;&#36827;&#23637;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24085;&#37329;&#26862;&#30149;&#30340;&#26202;&#26399;&#65292;&#35786;&#26029;&#27604;&#36739;&#23481;&#26131;&#65292;&#20294;&#22312;&#26089;&#26399;&#38454;&#27573;&#24456;&#38590;&#35786;&#26029;&#12290;&#26089;&#26399;&#35786;&#26029;&#26159;&#27835;&#30103;&#30151;&#29366;&#30340;&#20851;&#38190;&#12290;&#23427;&#24433;&#21709;&#26085;&#24120;&#29983;&#27963;&#65292;&#24182;&#38477;&#20302;&#24739;&#32773;&#21644;&#23478;&#20154;&#30340;&#29983;&#27963;&#36136;&#37327;&#65292;&#26159;&#32487;&#32769;&#24180;&#30196;&#21574;&#30151;&#21518;&#31532;&#20108;&#24120;&#35265;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#20851;&#20110;&#39044;&#27979;&#24085;&#37329;&#26862;&#30149;&#20005;&#37325;&#31243;&#24230;&#30340;&#30740;&#31350;&#37117;&#26159;&#22312;&#30142;&#30149;&#30340;&#26202;&#26399;&#36827;&#34892;&#30340;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#32452;&#21487;&#20197;&#36890;&#36807;&#35821;&#38899;&#20998;&#26512;&#36731;&#26494;&#25552;&#21462;&#30340;&#21464;&#37327;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#20004;&#20010;&#30446;&#30340;&#12290;&#19968;&#26041;&#38754;&#65292;&#30830;&#23450;&#19968;&#20010;&#20154;&#26159;&#20005;&#37325;&#24085;&#37329;&#26862;&#30149;&#36824;&#26159;&#38750;&#20005;&#37325;&#24085;&#37329;&#26862;&#30149;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#36890;&#36807;&#22238;&#24402;&#25216;&#26415;&#30830;&#23450;&#32473;&#23450;&#24739;&#32773;&#30142;&#30149;&#30340;&#36827;&#23637;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parkinson's disease is easy to diagnose when it is advanced, but it is very difficult to diagnose in its early stages. Early diagnosis is essential to be able to treat the symptoms. It impacts on daily activities and reduces the quality of life of both the patients and their families and it is also the second most prevalent neurodegenerative disorder after Alzheimer in people over the age of 60. Most current studies on the prediction of Parkinson's severity are carried out in advanced stages of the disease. In this work, the study analyzes a set of variables that can be easily extracted from voice analysis, making it a very non-intrusive technique. In this paper, a method based on different deep learning techniques is proposed with two purposes. On the one hand, to find out if a person has severe or non-severe Parkinson's disease, and on the other hand, to determine by means of regression techniques the degree of evolution of the disease in a given patient. The UPDRS (Unified Parkinson
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;QASE-net&#65292;&#21487;&#20197;&#26377;&#25928;&#39044;&#27979;&#34920;&#38754;&#32908;&#30005;&#20449;&#21495;&#30340;&#20449;&#22122;&#27604;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#30456;&#27604;&#20043;&#21069;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#21644;&#26356;&#39640;&#30340;&#32447;&#24615;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05482</link><description>&lt;p&gt;
&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;&#24212;&#29992;&#20110;&#34920;&#38754;&#32908;&#30005;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
A Non-Intrusive Neural Quality Assessment Model for Surface Electromyography Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;QASE-net&#65292;&#21487;&#20197;&#26377;&#25928;&#39044;&#27979;&#34920;&#38754;&#32908;&#30005;&#20449;&#21495;&#30340;&#20449;&#22122;&#27604;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#30456;&#27604;&#20043;&#21069;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#21644;&#26356;&#39640;&#30340;&#32447;&#24615;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28041;&#21450;&#27979;&#37327;&#32908;&#32905;&#34920;&#38754;&#32908;&#30005;&#65288;sEMG&#65289;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#23588;&#20854;&#26159;&#38752;&#36817;&#24515;&#33039;&#30340;&#21306;&#22495;&#65292;&#20027;&#35201;&#30340;&#27745;&#26579;&#28304;&#20043;&#19968;&#26159;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20449;&#21495;&#30340;&#23384;&#22312;&#12290;&#20026;&#20102;&#26356;&#26377;&#25928;&#22320;&#35780;&#20272;&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;sEMG&#25968;&#25454;&#36136;&#37327;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;QASE-net&#65292;&#19968;&#31181;&#26032;&#30340;&#38750;&#20405;&#20837;&#24615;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;sEMG&#20449;&#21495;&#30340;&#20449;&#22122;&#27604;&#12290;QASE-net&#23558;CNN-BLSTM&#19982;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#24182;&#37319;&#29992;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26694;&#26550;&#21033;&#29992;&#20102;&#20004;&#20010;&#24320;&#25918;&#35775;&#38382;&#25968;&#25454;&#24211;&#30340;&#23454;&#38469;&#19990;&#30028;sEMG&#21644;ECG&#25968;&#25454;&#65292;&#20998;&#21035;&#26159;&#38750;&#20405;&#20837;&#24615;&#36866;&#24212;&#24615;&#20551;&#32930;&#25968;&#25454;&#24211;&#21644;MIT-BIH&#27491;&#24120;&#31398;&#24615;&#24515;&#24459;&#25968;&#25454;&#24211;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;QASE-net&#20248;&#20110;&#20808;&#21069;&#30340;&#35780;&#20272;&#27169;&#22411;&#65292;&#20855;&#26377;&#26174;&#33879;&#38477;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#21644;&#26126;&#26174;&#26356;&#39640;&#30340;&#32447;&#24615;&#30456;&#20851;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#26174;&#31034;&#20102;QASE-net&#22312;&#25552;&#39640;&#21487;&#38752;&#24615;&#21644;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
In practical scenarios involving the measurement of surface electromyography (sEMG) in muscles, particularly those areas near the heart, one of the primary sources of contamination is the presence of electrocardiogram (ECG) signals. To assess the quality of real-world sEMG data more effectively, this study proposes QASE-net, a new non-intrusive model that predicts the SNR of sEMG signals. QASE-net combines CNN-BLSTM with attention mechanisms and follows an end-to-end training strategy. Our experimental framework utilizes real-world sEMG and ECG data from two open-access databases, the Non-Invasive Adaptive Prosthetics Database and the MIT-BIH Normal Sinus Rhythm Database, respectively. The experimental results demonstrate the superiority of QASE-net over the previous assessment model, exhibiting significantly reduced prediction errors and notably higher linear correlations with the ground truth. These findings show the potential of QASE-net to substantially enhance the reliability and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#27169;&#22411;&#38598;&#21512;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#32593;&#32476;&#20013;&#30340;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#25361;&#25112;&#12290;&#31639;&#27861;&#20351;&#29992;&#22810;&#20010;Q-learning&#31639;&#27861;&#22312;&#22810;&#20010;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#20013;&#24182;&#34892;&#36816;&#34892;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#21152;&#26435;&#34701;&#21512;&#36755;&#20986;&#65292;&#25552;&#20379;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#22810;&#36798;55%&#30340;&#24179;&#22343;&#31574;&#30053;&#20943;&#23569;&#12290;</title><link>https://arxiv.org/abs/2402.05476</link><description>&lt;p&gt;
&#22810;&#26102;&#38388;&#23610;&#24230;&#38598;&#21512;Q-learning&#29992;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#25919;&#31574;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Timescale Ensemble Q-learning for Markov Decision Process Policy Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#27169;&#22411;&#38598;&#21512;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#32593;&#32476;&#20013;&#30340;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#25361;&#25112;&#12290;&#31639;&#27861;&#20351;&#29992;&#22810;&#20010;Q-learning&#31639;&#27861;&#22312;&#22810;&#20010;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#20013;&#24182;&#34892;&#36816;&#34892;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#21152;&#26435;&#34701;&#21512;&#36755;&#20986;&#65292;&#25552;&#20379;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#22810;&#36798;55%&#30340;&#24179;&#22343;&#31574;&#30053;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#35299;&#20915;&#26410;&#30693;&#29615;&#22659;&#19979;&#32593;&#32476;&#25511;&#21046;&#25110;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#30340;&#32463;&#20856;&#24037;&#20855;&#12290;&#21407;&#22987;&#30340;Q-learning&#22312;&#38750;&#24120;&#22823;&#30340;&#32593;&#32476;&#20013;&#23384;&#22312;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#27169;&#22411;&#38598;&#21512;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#27169;&#22411;&#30340;&#32593;&#32476;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#12290;&#22810;&#20010;Q-learning&#31639;&#27861;&#22312;&#22810;&#20010;&#29420;&#31435;&#30340;&#12289;&#21512;&#25104;&#30340;&#21644;&#32467;&#26500;&#30456;&#20851;&#30340;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#20013;&#24182;&#34892;&#36816;&#34892;&#65307;&#36890;&#36807;&#22522;&#20110;Jensen-Shannon&#25955;&#24230;&#65288;JSD&#65289;&#30340;&#33258;&#36866;&#24212;&#21152;&#26435;&#26426;&#21046;&#26469;&#34701;&#21512;&#36755;&#20986;&#65292;&#20197;&#33719;&#24471;&#20855;&#26377;&#20302;&#22797;&#26434;&#24615;&#30340;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#12290;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#31639;&#27861;&#30340;&#29702;&#35770;&#35777;&#26126;&#65292;&#21253;&#25324;&#20851;&#38190;&#32479;&#35745;&#37327;&#21644;Q&#20989;&#25968;&#30340;&#25910;&#25947;&#24615;&#12290;&#22312;&#22810;&#20010;&#32593;&#32476;&#27169;&#22411;&#19978;&#30340;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#24179;&#22343;&#31574;&#30053;&#20943;&#23569;&#22810;&#36798;55%&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is a classical tool to solve network control or policy optimization problems in unknown environments. The original Q-learning suffers from performance and complexity challenges across very large networks. Herein, a novel model-free ensemble reinforcement learning algorithm which adapts the classical Q-learning is proposed to handle these challenges for networks which admit Markov decision process (MDP) models. Multiple Q-learning algorithms are run on multiple, distinct, synthetically created and structurally related Markovian environments in parallel; the outputs are fused using an adaptive weighting mechanism based on the Jensen-Shannon divergence (JSD) to obtain an approximately optimal policy with low complexity. The theoretical justification of the algorithm, including the convergence of key statistics and Q-functions are provided. Numerical results across several network models show that the proposed algorithm can achieve up to 55% less average policy 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#38543;&#26426;&#37319;&#26679;&#20248;&#21270;&#38544;&#21547;&#20998;&#24067;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#21333;&#20010;&#24490;&#29615;&#20013;&#21516;&#26102;&#36827;&#34892;&#20248;&#21270;&#21644;&#37319;&#26679;&#27493;&#39588;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05468</link><description>&lt;p&gt;
&#38544;&#24335;&#25193;&#25955;: &#36890;&#36807;&#38543;&#26426;&#37319;&#26679;&#23454;&#29616;&#39640;&#25928;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Implicit Diffusion: Efficient Optimization through Stochastic Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#38543;&#26426;&#37319;&#26679;&#20248;&#21270;&#38544;&#21547;&#20998;&#24067;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#21333;&#20010;&#24490;&#29615;&#20013;&#21516;&#26102;&#36827;&#34892;&#20248;&#21270;&#21644;&#37319;&#26679;&#27493;&#39588;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21442;&#25968;&#21270;&#38543;&#26426;&#25193;&#25955;&#38544;&#24335;&#23450;&#20041;&#30340;&#20998;&#24067;&#26469;&#36827;&#34892;&#20248;&#21270;&#30340;&#26032;&#31639;&#27861;&#12290;&#36890;&#36807;&#20248;&#21270;&#36825;&#20123;&#21442;&#25968;&#65292;&#21487;&#20197;&#20462;&#25913;&#37319;&#26679;&#36807;&#31243;&#30340;&#32467;&#26524;&#20998;&#24067;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#38024;&#23545;&#36825;&#20123;&#36807;&#31243;&#30340;&#19968;&#38454;&#20248;&#21270;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#21333;&#20010;&#24490;&#29615;&#20013;&#36827;&#34892;&#20248;&#21270;&#21644;&#37319;&#26679;&#27493;&#39588;&#26469;&#23454;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#21463;&#21040;&#21452;&#23618;&#20248;&#21270;&#21644;&#33258;&#21160;&#38544;&#24335;&#24494;&#20998;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#21033;&#29992;&#37319;&#26679;&#20316;&#20026;&#22312;&#27010;&#29575;&#20998;&#24067;&#31354;&#38388;&#19978;&#36827;&#34892;&#20248;&#21270;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#25105;&#20204;&#26041;&#27861;&#24615;&#33021;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#20197;&#21450;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new algorithm to optimize distributions defined implicitly by parameterized stochastic diffusions. Doing so allows us to modify the outcome distribution of sampling processes by optimizing over their parameters. We introduce a general framework for first-order optimization of these processes, that performs jointly, in a single loop, optimization and sampling steps. This approach is inspired by recent advances in bilevel optimization and automatic implicit differentiation, leveraging the point of view of sampling as optimization over the space of probability distributions. We provide theoretical guarantees on the performance of our method, as well as experimental results demonstrating its effectiveness in real-world settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20984;&#20985;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23454;&#29616;&#35757;&#32451;&#25439;&#22833;&#30340;&#39640;&#26041;&#24046;&#65292;&#20174;&#32780;&#38477;&#20302;&#20250;&#21592;&#25512;&#26029;&#25915;&#20987;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2402.05453</link><description>&lt;p&gt;
&#36890;&#36807;&#20984;&#20985;&#25439;&#22833;&#20989;&#25968;&#38477;&#20302;&#20250;&#21592;&#25512;&#26029;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Mitigating Privacy Risk in Membership Inference by Convex-Concave Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20984;&#20985;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23454;&#29616;&#35757;&#32451;&#25439;&#22833;&#30340;&#39640;&#26041;&#24046;&#65292;&#20174;&#32780;&#38477;&#20302;&#20250;&#21592;&#25512;&#26029;&#25915;&#20987;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#20250;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#21363;&#25512;&#26029;&#26679;&#26412;&#26159;&#21542;&#22312;&#35757;&#32451;&#38598;&#20013;&#12290;&#29616;&#26377;&#24037;&#20316;&#21033;&#29992;&#26799;&#24230;&#19978;&#21319;&#26469;&#22686;&#22823;&#35757;&#32451;&#25968;&#25454;&#30340;&#25439;&#22833;&#26041;&#24046;&#65292;&#32531;&#35299;&#38544;&#31169;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#21521;&#30456;&#21453;&#26041;&#21521;&#20248;&#21270;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#21442;&#25968;&#22312;&#23616;&#37096;&#26368;&#23567;&#20540;&#38468;&#36817;&#25391;&#33633;&#65292;&#23548;&#33268;&#19981;&#31283;&#23450;&#21644;&#27425;&#20248;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;&#20984;&#20985;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#26799;&#24230;&#19979;&#38477;&#30340;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#35757;&#32451;&#25439;&#22833;&#20998;&#24067;&#30340;&#39640;&#26041;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#29702;&#35770;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#20984;&#25439;&#22833;&#20989;&#25968;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20542;&#21521;&#20110;&#20943;&#23569;&#25439;&#22833;&#26041;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;CCL&#30340;&#32972;&#21518;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#20985;&#20989;&#25968;&#39033;&#20943;&#23567;&#25439;&#22833;&#20989;&#25968;&#30340;&#20984;&#24615;&#12290;&#20351;&#29992;CCL&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20135;&#29983;&#35757;&#32451;&#25968;&#25454;&#30340;&#39640;&#26041;&#24046;&#25439;&#22833;&#65292;&#21152;&#24378;&#20102;&#23545;MIAs&#30340;&#38450;&#24481;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;CCL&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are susceptible to membership inference attacks (MIAs), which aim to infer whether a sample is in the training set. Existing work utilizes gradient ascent to enlarge the loss variance of training data, alleviating the privacy risk. However, optimizing toward a reverse direction may cause the model parameters to oscillate near local minima, leading to instability and suboptimal performance. In this work, we propose a novel method -- Convex-Concave Loss, which enables a high variance of training loss distribution by gradient descent. Our method is motivated by the theoretical analysis that convex losses tend to decrease the loss variance during training. Thus, our key idea behind CCL is to reduce the convexity of loss functions with a concave term. Trained with CCL, neural networks produce losses with high variance for training data, reinforcing the defense against MIAs. Extensive experiments demonstrate the superiority of CCL, achieving state-of-the-art balance i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;Minecraft&#28216;&#25103;&#24212;&#29992;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#32534;&#36753;&#31995;&#32479;"Minecraft-ify"&#65292;&#33021;&#22815;&#29983;&#25104;&#38024;&#23545;3D&#34394;&#25311;&#35282;&#33394;&#30340;&#38754;&#37096;&#32858;&#28966;&#22270;&#20687;&#65292;&#24182;&#25903;&#25345;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#22270;&#20687;&#32534;&#36753;&#65292;&#25552;&#20379;&#20102;&#26356;&#33258;&#30001;&#21644;&#20248;&#21270;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2402.05448</link><description>&lt;p&gt;
Minecraft-ify&#65306;&#29992;&#20110;&#28216;&#25103;&#24212;&#29992;&#30340;Minecraft&#39118;&#26684;&#22270;&#20687;&#29983;&#25104;&#19982;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Minecraft-ify: Minecraft Style Image Generation with Text-guided Image Editing for In-Game Application
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;Minecraft&#28216;&#25103;&#24212;&#29992;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#32534;&#36753;&#31995;&#32479;"Minecraft-ify"&#65292;&#33021;&#22815;&#29983;&#25104;&#38024;&#23545;3D&#34394;&#25311;&#35282;&#33394;&#30340;&#38754;&#37096;&#32858;&#28966;&#22270;&#20687;&#65292;&#24182;&#25903;&#25345;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#22270;&#20687;&#32534;&#36753;&#65292;&#25552;&#20379;&#20102;&#26356;&#33258;&#30001;&#21644;&#20248;&#21270;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#38754;&#21521;Minecraft&#35270;&#39057;&#28216;&#25103;&#30340;&#35282;&#33394;&#32441;&#29702;&#29983;&#25104;&#31995;&#32479;"Minecraft-ify"&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#29983;&#25104;&#38024;&#23545;&#20855;&#26377;&#31435;&#26041;&#20307;&#27969;&#24418;&#30340;3D&#34394;&#25311;&#35282;&#33394;&#30340;&#38754;&#37096;&#32858;&#28966;&#22270;&#20687;&#20197;&#36827;&#34892;&#32441;&#29702;&#26144;&#23556;&#12290;&#19982;&#29616;&#26377;&#39033;&#30446;&#25110;&#20316;&#21697;&#21482;&#29983;&#25104;&#32441;&#29702;&#19981;&#21516;&#65292;&#25552;&#20986;&#30340;&#31995;&#32479;&#21487;&#20197;&#21453;&#36716;&#29992;&#25143;&#25552;&#20379;&#30340;&#30495;&#23454;&#22270;&#20687;&#65292;&#25110;&#20174;&#23398;&#20064;&#21040;&#30340;&#20998;&#24067;&#29983;&#25104;&#24179;&#22343;/&#38543;&#26426;&#22806;&#35266;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;StyleGAN&#21644;StyleCLIP&#36827;&#34892;&#25991;&#26412;&#24341;&#23548;&#30340;&#25805;&#20316;&#12290;&#36825;&#20123;&#21151;&#33021;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#29992;&#25143;&#20307;&#39564;&#21644;&#26356;&#22810;&#30340;&#33258;&#30001;&#65292;&#26159;&#19968;&#31181;&#29992;&#25143;&#21451;&#22909;&#30340;AI&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we first present the character texture generation system \textit{Minecraft-ify}, specified to Minecraft video game toward in-game application. Ours can generate face-focused image for texture mapping tailored to 3D virtual character having cube manifold. While existing projects or works only generate texture, proposed system can inverse the user-provided real image, or generate average/random appearance from learned distribution. Moreover, it can be manipulated with text-guidance using StyleGAN and StyleCLIP. These features provide a more extended user experience with enlarged freedom as a user-friendly AI-tool. Project page can be found at https://gh-bumsookim.github.io/Minecraft-ify/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20449;&#24687;&#20445;&#30041;&#25512;&#21160;&#37327;&#21270;LLMs&#30340;LoRA-Finetuning&#30340;&#26041;&#27861;IR-QLoRA&#65292;&#20351;&#29992;&#32479;&#35745;&#20449;&#24687;&#26657;&#20934;&#21644;&#35843;&#20248;&#20449;&#24687;&#24377;&#24615;&#36830;&#25509;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05445</link><description>&lt;p&gt;
&#20934;&#30830;&#30340;LLMs&#30340;LoRA-Finetuning&#37327;&#21270;&#36890;&#36807;&#20449;&#24687;&#20445;&#30041;
&lt;/p&gt;
&lt;p&gt;
Accurate LoRA-Finetuning Quantization of LLMs via Information Retention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20449;&#24687;&#20445;&#30041;&#25512;&#21160;&#37327;&#21270;LLMs&#30340;LoRA-Finetuning&#30340;&#26041;&#27861;IR-QLoRA&#65292;&#20351;&#29992;&#32479;&#35745;&#20449;&#24687;&#26657;&#20934;&#21644;&#35843;&#20248;&#20449;&#24687;&#24377;&#24615;&#36830;&#25509;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;LLMs&#30340;LoRA-finetuning&#37327;&#21270;&#30740;&#31350;&#24471;&#21040;&#20934;&#30830;&#20294;&#32039;&#20945;&#30340;LLMs&#20197;&#20415;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#37096;&#32626;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#23548;&#33268;&#37327;&#21270;&#30340;LLMs&#20005;&#37325;&#36864;&#21270;&#65292;&#29978;&#33267;&#26080;&#27861;&#20174;LoRA&#30340;&#35843;&#20248;&#20013;&#33719;&#30410;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;IR-QLoRA&#65292;&#36890;&#36807;&#20449;&#24687;&#20445;&#30041;&#25512;&#21160;&#24102;&#26377;LoRA&#30340;&#37327;&#21270;LLMs&#21464;&#24471;&#39640;&#24230;&#20934;&#30830;&#12290;&#25152;&#25552;&#20986;&#30340;IR-QLoRA&#20027;&#35201;&#20381;&#36182;&#20110;&#20004;&#31181;&#20174;&#32479;&#19968;&#20449;&#24687;&#35270;&#35282;&#27966;&#29983;&#30340;&#25216;&#26415;&#65306;&#65288;1&#65289;&#22522;&#20110;&#32479;&#35745;&#30340;&#20449;&#24687;&#26657;&#20934;&#37327;&#21270;&#20801;&#35768;LLMs&#30340;&#37327;&#21270;&#21442;&#25968;&#31934;&#30830;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#65307;&#65288;2&#65289;&#22522;&#20110;&#35843;&#20248;&#30340;&#20449;&#24687;&#24377;&#24615;&#36830;&#25509;&#20351;LoRA&#21033;&#29992;&#20855;&#26377;&#22810;&#26679;&#20449;&#24687;&#30340;&#24377;&#24615;&#34920;&#31034;&#36716;&#25442;&#12290;&#32508;&#21512;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;2-4&#20301;&#23485;&#19979;&#65292;IR-QLoRA&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLaMA&#21644;LLaMA2&#31995;&#21015;&#30340;&#20934;&#30830;&#24615;&#65292;&#20363;&#22914;&#65292;4&#20301;LLaMA-7B&#30456;&#27604;&#20110;...
&lt;/p&gt;
&lt;p&gt;
The LoRA-finetuning quantization of LLMs has been extensively studied to obtain accurate yet compact LLMs for deployment on resource-constrained hardware. However, existing methods cause the quantized LLM to severely degrade and even fail to benefit from the finetuning of LoRA. This paper proposes a novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate through information retention. The proposed IR-QLoRA mainly relies on two technologies derived from the perspective of unified information: (1) statistics-based Information Calibration Quantization allows the quantized parameters of LLM to retain original information accurately; (2) finetuning-based Information Elastic Connection makes LoRA utilizes elastic representation transformation with diverse information. Comprehensive experiments show that IR-QLoRA can significantly improve accuracy across LLaMA and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4% improvement on MMLU compared with the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#31216;&#20026;Semi-dual JKO (S-JKO)&#65292;&#36890;&#36807;&#37319;&#29992;&#21322;&#23545;&#20598;&#24418;&#24335;&#30340;JKO&#27493;&#39588;&#65292;&#38477;&#20302;&#20102;&#35757;&#32451;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;WGF&#19978;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.05443</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#19981;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#29983;&#25104;&#24314;&#27169;&#20013;&#30340;Wasserstein&#28176;&#21464;&#27969;
&lt;/p&gt;
&lt;p&gt;
Scalable Wasserstein Gradient Flow for Generative Modeling through Unbalanced Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#31216;&#20026;Semi-dual JKO (S-JKO)&#65292;&#36890;&#36807;&#37319;&#29992;&#21322;&#23545;&#20598;&#24418;&#24335;&#30340;JKO&#27493;&#39588;&#65292;&#38477;&#20302;&#20102;&#35757;&#32451;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;WGF&#19978;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Wasserstein&#28176;&#21464;&#27969;&#65288;WGF&#65289;&#25551;&#36848;&#20102;Wasserstein&#31354;&#38388;&#20013;&#27010;&#29575;&#23494;&#24230;&#30340;&#26799;&#24230;&#21160;&#21147;&#23398;&#12290;WGF&#25552;&#20379;&#20102;&#22312;&#27010;&#29575;&#20998;&#24067;&#19978;&#36827;&#34892;&#20248;&#21270;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#25968;&#20540;&#19978;&#36817;&#20284;&#36830;&#32493;&#30340;WGF&#38656;&#35201;&#26102;&#38388;&#31163;&#25955;&#21270;&#26041;&#27861;&#12290;&#20854;&#20013;&#26368;&#33879;&#21517;&#30340;&#26041;&#27861;&#26159;JKO&#26041;&#26696;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#20197;&#21069;&#30340;WGF&#27169;&#22411;&#37319;&#29992;JKO&#26041;&#26696;&#65292;&#24182;&#20026;&#27599;&#20010;JKO&#27493;&#39588;&#21442;&#25968;&#21270;&#20256;&#36755;&#26144;&#23556;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#20102;&#19982;JKO&#27493;&#39588;&#25968;&#37327;K&#25104;&#20108;&#27425;&#22686;&#38271;&#30340;&#35757;&#32451;&#22797;&#26434;&#24615;$O(K^2)$&#12290;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;WGF&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;WGF&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#31216;&#20026;&#21322;&#23545;&#20598;JKO&#65288;S-JKO&#65289;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;JKO&#27493;&#39588;&#30340;&#21322;&#23545;&#20598;&#24418;&#24335;&#65292;&#36890;&#36807;JKO&#27493;&#39588;&#19982;&#19981;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#24471;&#21040;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#35757;&#32451;&#22797;&#26434;&#24615;&#38477;&#20302;&#21040;$O(K)$&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;WGF&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Gradient Flow (WGF) describes the gradient dynamics of probability density within the Wasserstein space. WGF provides a promising approach for conducting optimization over the probability distributions. Numerically approximating the continuous WGF requires the time discretization method. The most well-known method for this is the JKO scheme. In this regard, previous WGF models employ the JKO scheme and parametrize transport map for each JKO step. However, this approach results in quadratic training complexity $O(K^2)$ with the number of JKO step $K$. This severely limits the scalability of WGF models. In this paper, we introduce a scalable WGF-based generative model, called Semi-dual JKO (S-JKO). Our model is based on the semi-dual form of the JKO step, derived from the equivalence between the JKO step and the Unbalanced Optimal Transport. Our approach reduces the training complexity to $O(K)$. We demonstrate that our model significantly outperforms existing WGF-based gener
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#26102;&#38388;&#25193;&#23637;&#65288;UTE&#65289;&#30340;&#31639;&#27861;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#21160;&#20316;&#37325;&#22797;&#21487;&#33021;&#38477;&#20302;&#24615;&#33021;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#20174;&#32780;&#35753;&#31574;&#30053;&#26681;&#25454;&#38656;&#27714;&#36827;&#34892;&#36873;&#25321;&#65292;&#23454;&#39564;&#35777;&#26126;UTE&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05439</link><description>&lt;p&gt;
&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#26102;&#38388;&#25193;&#23637;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
Learning Uncertainty-Aware Temporally-Extended Actions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05439
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#26102;&#38388;&#25193;&#23637;&#65288;UTE&#65289;&#30340;&#31639;&#27861;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#21160;&#20316;&#37325;&#22797;&#21487;&#33021;&#38477;&#20302;&#24615;&#33021;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#20174;&#32780;&#35753;&#31574;&#30053;&#26681;&#25454;&#38656;&#27714;&#36827;&#34892;&#36873;&#25321;&#65292;&#23454;&#39564;&#35777;&#26126;UTE&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#26102;&#38388;&#25277;&#35937;&#65292;&#20363;&#22914;&#21160;&#20316;&#37325;&#22797;&#65292;&#26159;&#19968;&#31181;&#36890;&#36807;&#25193;&#23637;&#21160;&#20316;&#20419;&#36827;&#31574;&#30053;&#23398;&#20064;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#21160;&#20316;&#37325;&#22797;&#30740;&#31350;&#23384;&#22312;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#65292;&#21363;&#24403;&#37325;&#22797;&#27425;&#20248;&#21160;&#20316;&#26102;&#21487;&#33021;&#38477;&#20302;&#24615;&#33021;&#12290;&#36825;&#20010;&#38382;&#39064;&#32463;&#24120;&#25269;&#28040;&#20102;&#21160;&#20316;&#37325;&#22797;&#30340;&#20248;&#21183;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#26102;&#38388;&#25193;&#23637;&#65288;UTE&#65289;&#30340;&#26032;&#31639;&#27861;&#12290;UTE&#20351;&#29992;&#38598;&#25104;&#26041;&#27861;&#22312;&#21160;&#20316;&#25193;&#23637;&#26399;&#38388;&#20934;&#30830;&#22320;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#20010;&#29305;&#24615;&#20801;&#35768;&#31574;&#30053;&#26681;&#25454;&#20854;&#29305;&#23450;&#38656;&#27714;&#65292;&#22312;&#24378;&#35843;&#25506;&#32034;&#25110;&#37319;&#21462;&#19981;&#30830;&#23450;&#24615;-&#25269;&#21046;&#26041;&#27861;&#20043;&#38388;&#36827;&#34892;&#36873;&#25321;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;Gridworld&#21644;Atari 2600&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#23637;&#31034;&#20102;UTE&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;UTE&#20248;&#20110;&#29616;&#26377;&#30340;&#21160;&#20316;&#37325;&#22797;&#31639;&#27861;&#65292;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#23427;&#20204;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning, temporal abstraction in the action space, exemplified by action repetition, is a technique to facilitate policy learning through extended actions. However, a primary limitation in previous studies of action repetition is its potential to degrade performance, particularly when sub-optimal actions are repeated. This issue often negates the advantages of action repetition. To address this, we propose a novel algorithm named Uncertainty-aware Temporal Extension (UTE). UTE employs ensemble methods to accurately measure uncertainty during action extension. This feature allows policies to strategically choose between emphasizing exploration or adopting an uncertainty-averse approach, tailored to their specific needs. We demonstrate the effectiveness of UTE through experiments in Gridworld and Atari 2600 environments. Our findings show that UTE outperforms existing action repetition algorithms, effectively mitigating their inherent limitations and significantly enhan
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#21270;&#21465;&#20107;&#25552;&#31034;&#65292;&#39564;&#35777;&#20102;GPT-4&#29983;&#25104;&#30340;&#21465;&#36848;&#22312;&#20256;&#36798;&#29983;&#27963;&#20107;&#20214;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#21465;&#36848;&#33021;&#22815;&#36275;&#22815;&#20256;&#36798;&#25552;&#31034;&#30340;&#24847;&#22270;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;&#21465;&#36848;&#12290;</title><link>https://arxiv.org/abs/2402.05435</link><description>&lt;p&gt;
GPT-4&#20351;&#29992;&#32467;&#26500;&#21270;&#21465;&#20107;&#25552;&#31034;&#29983;&#25104;&#29983;&#27963;&#20107;&#20214;&#30340;&#21465;&#36848;&#65306;&#19968;&#39033;&#39564;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GPT-4 Generated Narratives of Life Events using a Structured Narrative Prompt: A Validation Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#21270;&#21465;&#20107;&#25552;&#31034;&#65292;&#39564;&#35777;&#20102;GPT-4&#29983;&#25104;&#30340;&#21465;&#36848;&#22312;&#20256;&#36798;&#29983;&#27963;&#20107;&#20214;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#21465;&#36848;&#33021;&#22815;&#36275;&#22815;&#20256;&#36798;&#25552;&#31034;&#30340;&#24847;&#22270;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;&#21465;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#21508;&#31181;&#21465;&#36848;&#26041;&#38754;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#20419;&#36827;&#20102;&#23545;&#20854;&#22312;&#21465;&#36848;&#24418;&#24335;&#20013;&#20256;&#36798;&#29983;&#27963;&#20107;&#20214;&#25928;&#26524;&#30340;&#31995;&#32479;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#38646;-shot&#32467;&#26500;&#21270;&#21465;&#20107;&#25552;&#31034;&#65292;&#20351;&#29992;OpenAI&#30340;GPT-4&#29983;&#25104;&#20102;24,000&#20010;&#21465;&#36848;&#12290;&#20174;&#36825;&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#25163;&#21160;&#20998;&#31867;&#20102;2,880&#20010;&#21465;&#36848;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#22312;&#20256;&#36798;&#20986;&#29983;&#12289;&#27515;&#20129;&#12289;&#25307;&#32856;&#21644;&#35299;&#38599;&#20107;&#20214;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;87.43%&#30340;&#21465;&#36848;&#36275;&#22815;&#20256;&#36798;&#32467;&#26500;&#21270;&#25552;&#31034;&#30340;&#24847;&#22270;&#12290;&#20026;&#20102;&#33258;&#21160;&#35782;&#21035;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;&#21465;&#36848;&#65292;&#25105;&#20204;&#23545;&#20998;&#31867;&#25968;&#25454;&#38598;&#35757;&#32451;&#21644;&#39564;&#35777;&#20102;&#20061;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#23545;&#21097;&#20313;21,120&#20010;&#21465;&#36848;&#30340;&#20998;&#31867;&#39044;&#27979;&#20998;&#26512;&#12290;&#25152;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#23558;&#26377;&#25928;&#30340;&#21465;&#36848;&#20998;&#31867;&#20026;&#26377;&#25928;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#21516;&#26102;&#23558;&#26080;&#25928;&#30340;&#21465;&#36848;&#20998;&#31867;&#20026;&#26080;&#25928;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#19981;&#20165;&#25512;&#36827;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#36824;&#25552;&#20379;&#20102;&#33258;&#21160;&#35782;&#21035;&#26377;&#25928;&#21465;&#36848;&#30340;&#26377;&#30410;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) play a pivotal role in generating vast arrays of narratives, facilitating a systematic exploration of their effectiveness for communicating life events in narrative form. In this study, we employ a zero-shot structured narrative prompt to generate 24,000 narratives using OpenAI's GPT-4. From this dataset, we manually classify 2,880 narratives and evaluate their validity in conveying birth, death, hiring, and firing events. Remarkably, 87.43% of the narratives sufficiently convey the intention of the structured prompt. To automate the identification of valid and invalid narratives, we train and validate nine Machine Learning models on the classified datasets. Leveraging these models, we extend our analysis to predict the classifications of the remaining 21,120 narratives. All the ML models excelled at classifying valid narratives as valid, but experienced challenges at simultaneously classifying invalid narratives as invalid. Our findings not only advance th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#36825;&#20004;&#20010;&#27169;&#22411;&#36890;&#36807;&#25311;&#21512;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#24182;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#20998;&#24067;&#36827;&#34892;&#20998;&#31867;&#65292;&#25928;&#26524;&#30053;&#20248;&#20110;&#25110;&#19982;&#20116;&#20010;&#22522;&#20934;&#20998;&#31867;&#27169;&#22411;&#30456;&#24403;&#12290;&#22312;&#23454;&#38469;&#30340;&#20135;&#21697;&#25414;&#32465;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23398;&#20064;&#20135;&#21697;&#25903;&#20184;&#24847;&#24895;&#20998;&#24067;&#26041;&#38754;&#23637;&#29616;&#20986;&#30495;&#23454;&#30340;&#23454;&#29992;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05428</link><description>&lt;p&gt;
&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#29992;&#20110;&#20998;&#31867;&#21450;&#20854;&#22312;&#20135;&#21697;&#25414;&#32465;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Mixture Density Networks for Classification with an Application to Product Bundling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#36825;&#20004;&#20010;&#27169;&#22411;&#36890;&#36807;&#25311;&#21512;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#24182;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#20998;&#24067;&#36827;&#34892;&#20998;&#31867;&#65292;&#25928;&#26524;&#30053;&#20248;&#20110;&#25110;&#19982;&#20116;&#20010;&#22522;&#20934;&#20998;&#31867;&#27169;&#22411;&#30456;&#24403;&#12290;&#22312;&#23454;&#38469;&#30340;&#20135;&#21697;&#25414;&#32465;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23398;&#20064;&#20135;&#21697;&#25903;&#20184;&#24847;&#24895;&#20998;&#24067;&#26041;&#38754;&#23637;&#29616;&#20986;&#30495;&#23454;&#30340;&#23454;&#29992;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;(MDNs)&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21364;&#24456;&#23569;&#20351;&#29992;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;MDNs&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#21487;&#29992;&#24615;&#19981;&#26126;&#30830;&#21644;&#30452;&#25509;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;MDN&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#36825;&#20004;&#20010;&#27169;&#22411;&#23545;&#25968;&#25454;&#36827;&#34892;&#39640;&#26031;&#28151;&#21512;&#25311;&#21512;&#65292;&#24182;&#20351;&#29992;&#25311;&#21512;&#30340;&#20998;&#24067;&#36890;&#36807;&#35780;&#20272;&#32473;&#23450;&#36755;&#20837;&#29305;&#24449;&#30340;&#23398;&#20064;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#26469;&#23545;&#32473;&#23450;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#12290;&#22312;&#19977;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;MDN&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#30053;&#20248;&#20110;&#25110;&#19982;&#20116;&#20010;&#22522;&#20934;&#20998;&#31867;&#27169;&#22411;&#30456;&#24403;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#30495;&#23454;&#25928;&#29992;&#36890;&#36807;&#19968;&#20010;&#23454;&#38469;&#30340;&#20135;&#21697;&#25414;&#32465;&#24212;&#29992;&#20013;&#24471;&#20197;&#23637;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#22522;&#20110;MDN&#30340;&#27169;&#22411;&#20174;&#21512;&#25104;&#38144;&#21806;&#25968;&#25454;&#20013;&#23398;&#20064;&#20004;&#20010;&#20135;&#21697;&#30340;&#25903;&#20184;&#24847;&#24895;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
While mixture density networks (MDNs) have been extensively used for regression tasks, they have not been used much for classification tasks. One reason for this is that the usability of MDNs for classification is not clear and straightforward. In this paper, we propose two MDN-based models for classification tasks. Both models fit mixtures of Gaussians to the the data and use the fitted distributions to classify a given sample by evaluating the learnt cumulative distribution function for the given input features. While the proposed MDN-based models perform slightly better than, or on par with, five baseline classification models on three publicly available datasets, the real utility of our models comes out through a real-world product bundling application. Specifically, we use our MDN-based models to learn the willingness-to-pay (WTP) distributions for two products from synthetic sales data of the individual products. The Gaussian mixture representation of the learnt WTP distributions
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#37319;&#26679;&#29702;&#35770;&#30340;&#35270;&#35282;&#23545;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#20013;&#30340;&#28608;&#27963;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21457;&#29616;sinc&#28608;&#27963;&#26159;&#29702;&#35770;&#19978;&#30340;&#26368;&#20339;&#36873;&#25321;&#65292;&#24182;&#24314;&#31435;&#20102;&#21160;&#21147;&#31995;&#32479;&#19982;INR&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.05427</link><description>&lt;p&gt;
&#37319;&#26679;&#29702;&#35770;&#35270;&#35282;&#19979;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#28608;&#27963;
&lt;/p&gt;
&lt;p&gt;
A Sampling Theory Perspective on Activations for Implicit Neural Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#37319;&#26679;&#29702;&#35770;&#30340;&#35270;&#35282;&#23545;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#20013;&#30340;&#28608;&#27963;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21457;&#29616;sinc&#28608;&#27963;&#26159;&#29702;&#35770;&#19978;&#30340;&#26368;&#20339;&#36873;&#25321;&#65292;&#24182;&#24314;&#31435;&#20102;&#21160;&#21147;&#31995;&#32479;&#19982;INR&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#24050;&#32463;&#22312;&#23558;&#20449;&#21495;&#32534;&#30721;&#20026;&#32039;&#20945;&#30340;&#12289;&#21487;&#24494;&#20998;&#30340;&#23454;&#20307;&#26041;&#38754;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#34429;&#28982;&#36890;&#24120;&#20351;&#29992;&#20613;&#37324;&#21494;&#20301;&#32622;&#32534;&#30721;&#25110;&#38750;&#20256;&#32479;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;&#39640;&#26031;&#12289;&#27491;&#24358;&#25110;&#23567;&#27874;&#65289;&#31561;&#25216;&#26415;&#26469;&#25429;&#25417;&#39640;&#39057;&#20869;&#23481;&#65292;&#20294;&#23427;&#20204;&#30340;&#23646;&#24615;&#22312;&#32479;&#19968;&#30340;&#29702;&#35770;&#26694;&#26550;&#20013;&#32570;&#20047;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#20174;&#37319;&#26679;&#29702;&#35770;&#30340;&#35270;&#35282;&#23545;&#36825;&#20123;&#28608;&#27963;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#19982;INR&#32467;&#21512;&#26102;&#20197;&#21069;&#26410;&#20351;&#29992;&#30340;sinc&#28608;&#27963;&#26159;&#29702;&#35770;&#19978;&#30340;&#20449;&#21495;&#32534;&#30721;&#26368;&#20339;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#21160;&#21147;&#31995;&#32479;&#21644;INR&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#21033;&#29992;&#37319;&#26679;&#29702;&#35770;&#26469;&#36830;&#25509;&#36825;&#20004;&#20010;&#33539;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit Neural Representations (INRs) have gained popularity for encoding signals as compact, differentiable entities. While commonly using techniques like Fourier positional encodings or non-traditional activation functions (e.g., Gaussian, sinusoid, or wavelets) to capture high-frequency content, their properties lack exploration within a unified theoretical framework. Addressing this gap, we conduct a comprehensive analysis of these activations from a sampling theory perspective. Our investigation reveals that sinc activations, previously unused in conjunction with INRs, are theoretically optimal for signal encoding. Additionally, we establish a connection between dynamical systems and INRs, leveraging sampling theory to bridge these two paradigms.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31070;&#32463;&#30005;&#36335;&#22270;&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#36890;&#20449;&#30340;&#22270;&#24418;&#35821;&#35328;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#26174;&#31034;&#25968;&#25454;&#25490;&#21015;&#30340;&#21464;&#21270;&#12289;&#25805;&#20316;&#22312;&#22352;&#26631;&#36724;&#19978;&#30340;&#24191;&#25773;&#26041;&#24335;&#21644;&#32447;&#24615;&#25805;&#20316;&#30340;&#37325;&#35201;&#24182;&#34892;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.05424</link><description>&lt;p&gt;
&#31070;&#32463;&#30005;&#36335;&#22270;&#65306;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#36890;&#20449;&#12289;&#23454;&#29616;&#21644;&#20998;&#26512;&#30340;&#31283;&#20581;&#22270;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Neural Circuit Diagrams: Robust Diagrams for the Communication, Implementation, and Analysis of Deep Learning Architectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05424
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31070;&#32463;&#30005;&#36335;&#22270;&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#36890;&#20449;&#30340;&#22270;&#24418;&#35821;&#35328;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#26174;&#31034;&#25968;&#25454;&#25490;&#21015;&#30340;&#21464;&#21270;&#12289;&#25805;&#20316;&#22312;&#22352;&#26631;&#36724;&#19978;&#30340;&#24191;&#25773;&#26041;&#24335;&#21644;&#32447;&#24615;&#25805;&#20316;&#30340;&#37325;&#35201;&#24182;&#34892;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31034;&#26041;&#27861;&#24456;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#28145;&#24230;&#23398;&#20064;&#30028;&#32570;&#20047;&#19968;&#31181;&#26631;&#20934;&#30340;&#26041;&#27861;&#26469;&#32472;&#21046;&#26550;&#26500;&#22270;&#12290;&#24403;&#21069;&#30340;&#32447;&#24615;&#20195;&#25968;&#31526;&#21495;&#21644;&#20020;&#26102;&#22270;&#31034;&#30340;&#32452;&#21512;&#26080;&#27861;&#25552;&#20379;&#36275;&#22815;&#31934;&#30830;&#30340;&#32454;&#33410;&#65292;&#20197;&#29702;&#35299;&#26550;&#26500;&#30340;&#25152;&#26377;&#32454;&#33410;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32454;&#33410;&#23545;&#20110;&#24544;&#23454;&#30340;&#23454;&#29616;&#12289;&#25968;&#23398;&#20998;&#26512;&#12289;&#36827;&#19968;&#27493;&#30340;&#21019;&#26032;&#21644;&#36947;&#24503;&#20445;&#35777;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#30005;&#36335;&#22270;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#36890;&#20449;&#38656;&#27714;&#30340;&#22270;&#24418;&#35821;&#35328;&#12290;&#31070;&#32463;&#30005;&#36335;&#22270;&#33258;&#28982;&#22320;&#36319;&#36394;&#25968;&#25454;&#30340;&#21464;&#21270;&#25490;&#21015;&#65292;&#20934;&#30830;&#22320;&#26174;&#31034;&#25805;&#20316;&#22914;&#20309;&#22312;&#22352;&#26631;&#36724;&#19978;&#24191;&#25773;&#65292;&#24182;&#23637;&#31034;&#32447;&#24615;&#25805;&#20316;&#30340;&#37325;&#35201;&#24182;&#34892;&#34892;&#20026;&#12290;&#29616;&#26377;&#22270;&#31034;&#26041;&#27861;&#30340;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#26159;&#26080;&#27861;&#21516;&#26102;&#34920;&#36798;&#22352;&#26631;&#36724;&#30340;&#32454;&#33410;&#21644;&#25968;&#25454;&#30340;&#33258;&#30001;&#25490;&#21015;&#65292;&#32780;&#31070;&#32463;&#30005;&#36335;&#22270;&#21017;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#20204;&#30340;&#32452;&#21512;&#32467;&#26500;&#31867;&#20284;&#20110;&#20195;&#30721;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#23494;&#20999;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diagrams matter. Unfortunately, the deep learning community has no standard method for diagramming architectures. The current combination of linear algebra notation and ad-hoc diagrams fails to offer the necessary precision to understand architectures in all their detail. However, this detail is critical for faithful implementation, mathematical analysis, further innovation, and ethical assurances. I present neural circuit diagrams, a graphical language tailored to the needs of communicating deep learning architectures. Neural circuit diagrams naturally keep track of the changing arrangement of data, precisely show how operations are broadcast over axes, and display the critical parallel behavior of linear operations. A lingering issue with existing diagramming methods is the inability to simultaneously express the detail of axes and the free arrangement of data, which neural circuit diagrams solve. Their compositional structure is analogous to code, creating a close correspondence bet
&lt;/p&gt;</description></item><item><title>DiffTOP&#20351;&#29992;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#20316;&#20026;&#31574;&#30053;&#34920;&#31034;&#26469;&#29983;&#25104;&#21160;&#20316;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#38382;&#39064;&#65292;&#24182;&#22312;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2402.05421</link><description>&lt;p&gt;
DiffTOP: &#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#22312;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
DiffTOP: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05421
&lt;/p&gt;
&lt;p&gt;
DiffTOP&#20351;&#29992;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#20316;&#20026;&#31574;&#30053;&#34920;&#31034;&#26469;&#29983;&#25104;&#21160;&#20316;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#38382;&#39064;&#65292;&#24182;&#22312;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiffTOP&#65292;&#23427;&#21033;&#29992;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#20316;&#20026;&#31574;&#30053;&#34920;&#31034;&#65292;&#20026;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#29983;&#25104;&#21160;&#20316;&#12290;&#36712;&#36857;&#20248;&#21270;&#26159;&#19968;&#31181;&#22312;&#25511;&#21046;&#39046;&#22495;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#65292;&#30001;&#25104;&#26412;&#21644;&#21160;&#21147;&#23398;&#20989;&#25968;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#20351;&#24471;&#21487;&#20197;&#35745;&#31639;&#25439;&#22833;&#23545;&#20110;&#36712;&#36857;&#20248;&#21270;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#12290;&#22240;&#27492;&#65292;&#36712;&#36857;&#20248;&#21270;&#30340;&#25104;&#26412;&#21644;&#21160;&#21147;&#23398;&#20989;&#25968;&#21487;&#20197;&#31471;&#21040;&#31471;&#22320;&#23398;&#20064;&#12290;DiffTOP&#35299;&#20915;&#20102;&#20043;&#21069;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#38382;&#39064;&#65292;&#22240;&#20026;DiffTOP&#20013;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#36890;&#36807;&#36712;&#36857;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#31574;&#30053;&#26799;&#24230;&#25439;&#22833;&#30452;&#25509;&#26368;&#22823;&#21270;&#20219;&#21153;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23545;DiffTOP&#22312;&#26631;&#20934;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#22871;&#20214;&#20013;&#36827;&#34892;&#20102;&#27169;&#20223;&#23398;&#20064;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces DiffTOP, which utilizes Differentiable Trajectory OPtimization as the policy representation to generate actions for deep reinforcement and imitation learning. Trajectory optimization is a powerful and widely used algorithm in control, parameterized by a cost and a dynamics function. The key to our approach is to leverage the recent progress in differentiable trajectory optimization, which enables computing the gradients of the loss with respect to the parameters of trajectory optimization. As a result, the cost and dynamics functions of trajectory optimization can be learned end-to-end. DiffTOP addresses the ``objective mismatch'' issue of prior model-based RL algorithms, as the dynamics model in DiffTOP is learned to directly maximize task performance by differentiating the policy gradient loss through the trajectory optimization process. We further benchmark DiffTOP for imitation learning on standard robotic manipulation task suites with high-dimensional sensory
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#38752;&#20998;&#35789;&#30340;&#22522;&#20110;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#25439;&#22833;&#30340;&#25991;&#26412;&#39564;&#35777;&#30721;&#20998;&#31867;OCR&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#21644;&#25197;&#26354;&#20869;&#23481;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05417</link><description>&lt;p&gt;
&#19981;&#20381;&#38752;&#20998;&#35789;&#30340;&#22522;&#20110;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#25439;&#22833;&#30340;&#25991;&#26412;&#39564;&#35777;&#30721;&#20998;&#31867;OCR&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Segmentation-free Connectionist Temporal Classification loss based OCR Model for Text Captcha Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#38752;&#20998;&#35789;&#30340;&#22522;&#20110;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#25439;&#22833;&#30340;&#25991;&#26412;&#39564;&#35777;&#30721;&#20998;&#31867;OCR&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#21644;&#25197;&#26354;&#20869;&#23481;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39564;&#35777;&#30721;&#34987;&#24191;&#27867;&#29992;&#20110;&#36890;&#36807;&#21306;&#20998;&#35745;&#31639;&#26426;&#21709;&#24212;&#21644;&#20154;&#31867;&#21709;&#24212;&#26469;&#20445;&#25252;&#31995;&#32479;&#20813;&#21463;&#33258;&#21160;&#21709;&#24212;&#12290;&#25991;&#26412;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#22270;&#29255;&#31561;&#22522;&#20110;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#30340;&#39564;&#35777;&#30721;&#29992;&#20110;&#21019;&#24314;&#39564;&#35777;&#30721;&#12290;&#22522;&#20110;&#25991;&#26412;&#30340;OCR&#39564;&#35777;&#30721;&#26159;&#26368;&#24120;&#29992;&#30340;&#39564;&#35777;&#30721;&#65292;&#20294;&#38754;&#20020;&#22797;&#26434;&#21644;&#25197;&#26354;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#26377;&#23581;&#35797;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#22522;&#20110;&#39564;&#35777;&#30721;&#26816;&#27979;&#21644;&#20998;&#31867;&#30340;&#31995;&#32479;&#65292;&#38656;&#35201;&#36827;&#34892;&#20934;&#30830;&#24615;&#35843;&#25972;&#12290;&#29616;&#26377;&#31995;&#32479;&#22312;&#35782;&#21035;&#25197;&#26354;&#23383;&#31526;&#12289;&#22788;&#29702;&#21487;&#21464;&#38271;&#24230;&#39564;&#35777;&#30721;&#21644;&#25214;&#21040;&#39564;&#35777;&#30721;&#20013;&#30340;&#39034;&#24207;&#20381;&#36182;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#25439;&#22833;&#25216;&#26415;&#30340;&#25991;&#26412;&#39564;&#35777;&#30721;&#20998;&#31867;OCR&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#19981;&#20381;&#36182;&#20998;&#35789;&#12290;&#35813;&#27169;&#22411;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#22312;&#23383;&#31526;&#32423;&#21035;&#19978;&#65292;&#35813;&#27169;&#22411;&#36798;&#21040;&#20102;99.80\%&#30340;&#20934;&#30830;&#29575;&#65292;&#32780;&#22312;&#35789;&#32423;&#21035;&#19978;&#36798;&#21040;&#20102;95\%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Captcha are widely used to secure systems from automatic responses by distinguishing computer responses from human responses. Text, audio, video, picture picture-based Optical Character Recognition (OCR) are used for creating captcha. Text-based OCR captcha are the most often used captcha which faces issues namely, complex and distorted contents. There are attempts to build captcha detection and classification-based systems using machine learning and neural networks, which need to be tuned for accuracy. The existing systems face challenges in the recognition of distorted characters, handling variable-length captcha and finding sequential dependencies in captcha. In this work, we propose a segmentation-free OCR model for text captcha classification based on the connectionist temporal classification loss technique. The proposed model is trained and tested on a publicly available captcha dataset. The proposed model gives 99.80\% character level accuracy, while 95\% word level accuracy. Th
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29256;&#26412;&#24180;&#40836;&#30340;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#35843;&#24230;&#31574;&#30053;&#12290;&#35813;&#31574;&#30053;&#32771;&#34385;&#20102;&#21450;&#26102;&#24615;&#21644;&#20869;&#23481;&#28382;&#21518;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#24930;&#36895;&#33410;&#28857;&#30340;&#25361;&#25112;&#65292;&#24182;&#23545;&#20840;&#23616;&#27169;&#22411;&#25910;&#25947;&#24615;&#21644;&#31283;&#23450;&#24615;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.05407</link><description>&lt;p&gt;
&#22522;&#20110;&#29256;&#26412;&#24180;&#40836;&#30340;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#35843;&#24230;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Version age-based client scheduling policy for federated learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05407
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29256;&#26412;&#24180;&#40836;&#30340;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#35843;&#24230;&#31574;&#30053;&#12290;&#35813;&#31574;&#30053;&#32771;&#34385;&#20102;&#21450;&#26102;&#24615;&#21644;&#20869;&#23481;&#28382;&#21518;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#24930;&#36895;&#33410;&#28857;&#30340;&#25361;&#25112;&#65292;&#24182;&#23545;&#20840;&#23616;&#27169;&#22411;&#25910;&#25947;&#24615;&#21644;&#31283;&#23450;&#24615;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539; Paradigm&#65292;&#21487;&#20197;&#22312;&#19981;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#21327;&#20316;&#35757;&#32451;&#12290;&#23613;&#31649;&#36793;&#32536;&#35774;&#22791;&#33021;&#21147;&#30340;&#22686;&#24378;&#65292;&#20294;&#36890;&#20449;&#29942;&#39048;&#23548;&#33268;&#22312;&#32858;&#21512;&#22823;&#37327;&#23458;&#25143;&#31471;&#26102;&#20986;&#29616;&#25361;&#25112;&#65307;&#27599;&#27425;&#20840;&#23616;&#32858;&#21512;&#26102;&#65292;&#21482;&#26377;&#19968;&#37096;&#20998;&#23458;&#25143;&#31471;&#33021;&#22815;&#26356;&#26032;&#20854;&#21442;&#25968;&#12290;&#36825;&#31181;&#29616;&#35937;&#32473;FL&#20013;&#30340;&#24930;&#36895;&#33410;&#28857;&#65288;stragglers&#65289;&#24102;&#26469;&#20102;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#24182;&#19988;&#23458;&#25143;&#31471;&#35843;&#24230;&#31574;&#30053;&#23545;&#20840;&#23616;&#27169;&#22411;&#25910;&#25947;&#24615;&#21644;&#31283;&#23450;&#24615;&#26377;&#30528;&#28145;&#36828;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#35843;&#24230;&#31574;&#30053;&#35299;&#20915;&#20102;&#28382;&#21518;&#24615;&#38382;&#39064;&#65292;&#20294;&#20027;&#35201;&#38598;&#20013;&#22312;&#21450;&#26102;&#24615;&#25110;&#20869;&#23481;&#19978;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Version Age of Information&#65288;VAoI&#65289;&#30340;&#26032;&#27010;&#24565;&#26469;&#24212;&#29992;&#20110;FL&#12290;&#19982;&#20256;&#32479;&#30340;&#20449;&#24687;&#24180;&#40836;&#24230;&#37327;&#19981;&#21516;&#65292;VAoI&#21516;&#26102;&#32771;&#34385;&#20102;&#21450;&#26102;&#24615;&#21644;&#20869;&#23481;&#28382;&#21518;&#12290;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#29256;&#26412;&#24180;&#40836;&#20197;&#31163;&#25955;&#26041;&#24335;&#36827;&#34892;&#26356;&#26032;&#65292;&#34920;&#31034;&#20449;&#24687;&#30340;&#26032;&#40092;&#31243;&#24230;&#12290;VAoI&#34987;&#32435;&#20837;&#21040;&#23458;&#25143;&#31471;&#35843;&#24230;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged as a privacy-preserving machine learning paradigm facilitating collaborative training across multiple clients without sharing local data. Despite advancements in edge device capabilities, communication bottlenecks present challenges in aggregating a large number of clients; only a portion of the clients can update their parameters upon each global aggregation. This phenomenon introduces the critical challenge of stragglers in FL and the profound impact of client scheduling policies on global model convergence and stability. Existing scheduling strategies address staleness but predominantly focus on either timeliness or content. Motivated by this, we introduce the novel concept of Version Age of Information (VAoI) to FL. Unlike traditional Age of Information metrics, VAoI considers both timeliness and content staleness. Each client's version age is updated discretely, indicating the freshness of information. VAoI is incorporated into the client schedu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#21069;&#21521;&#20256;&#36882;&#30340;LLM&#32467;&#26500;&#21270;&#20462;&#21098;&#26041;&#27861;&#65292;&#36890;&#36807;Bonsai&#29983;&#25104;&#30340;&#20462;&#21098;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26799;&#24230;-based&#32467;&#26500;&#21270;&#20462;&#21098;&#26041;&#27861;&#65292;&#24182;&#19988;&#36895;&#24230;&#26159;&#21322;&#32467;&#26500;&#21270;&#20462;&#21098;&#27169;&#22411;&#30340;&#20004;&#20493;&#12290;</title><link>https://arxiv.org/abs/2402.05406</link><description>&lt;p&gt;
&#29616;&#22312;&#25152;&#26377;&#20154;&#37117;&#20462;&#21098;&#65306;&#20165;&#20351;&#29992;&#21069;&#21521;&#20256;&#36882;&#30340;LLM&#32467;&#26500;&#21270;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#21069;&#21521;&#20256;&#36882;&#30340;LLM&#32467;&#26500;&#21270;&#20462;&#21098;&#26041;&#27861;&#65292;&#36890;&#36807;Bonsai&#29983;&#25104;&#30340;&#20462;&#21098;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26799;&#24230;-based&#32467;&#26500;&#21270;&#20462;&#21098;&#26041;&#27861;&#65292;&#24182;&#19988;&#36895;&#24230;&#26159;&#21322;&#32467;&#26500;&#21270;&#20462;&#21098;&#27169;&#22411;&#30340;&#20004;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#38750;&#19987;&#19994;&#20174;&#19994;&#32773;&#21644;&#26368;&#23500;&#26377;&#36164;&#28304;&#30340;&#26426;&#26500;&#20043;&#38388;&#30340;&#30828;&#20214;&#24046;&#36317;&#65292;&#23610;&#23544;&#19981;&#26029;&#22686;&#38271;&#30340;LLM&#21464;&#24471;&#36234;&#26469;&#36234;&#38590;&#20197;&#20351;&#29992;&#12290;&#34429;&#28982;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#21387;&#32553;LLM&#65292;&#20197;&#20351;&#20854;&#36164;&#28304;&#28040;&#32791;&#21487;&#31649;&#29702;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#26412;&#36523;&#24448;&#24448;&#32791;&#36153;&#36164;&#28304;&#65292;&#20351;&#20854;&#30446;&#26631;&#29992;&#25143;&#32676;&#26080;&#27861;&#25509;&#35302;&#21040;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20165;&#20351;&#29992;&#21069;&#21521;&#20256;&#36882;&#30340;LLM&#32467;&#26500;&#21270;&#20462;&#21098;&#38382;&#39064;&#12290;&#25105;&#20204;&#24076;&#26395;&#35753;&#20174;&#19994;&#32773;&#33021;&#22815;&#20462;&#21098;&#27169;&#22411;&#65292;&#20351;&#20854;&#35268;&#27169;&#22823;&#21040;&#30828;&#20214;&#20165;&#26377;&#36275;&#22815;&#30340;&#20869;&#23384;&#26469;&#36816;&#34892;&#25512;&#29702;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;Bonsai&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#26799;&#24230;&#12289;&#25200;&#21160;&#20462;&#21098;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#23567;&#12289;&#24555;&#21644;&#20934;&#30830;&#30340;&#20462;&#21098;&#27169;&#22411;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;Bonsai&#29983;&#25104;&#30340;&#20462;&#21098;&#27169;&#22411;&#65288;i&#65289;&#20248;&#20110;&#26356;&#26114;&#36149;&#30340;&#26799;&#24230;-based&#32467;&#26500;&#21270;&#20462;&#21098;&#26041;&#27861;&#29983;&#25104;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#65288;ii&#65289;&#19982;&#21322;&#32467;&#26500;&#21270;&#20462;&#21098;&#27169;&#22411;&#30456;&#27604;&#65292;&#36895;&#24230;&#24555;&#19968;&#20493;&#19988;&#20934;&#30830;&#24615;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the generational gap in available hardware between lay practitioners and the most endowed institutions, LLMs are becoming increasingly inaccessible as they grow in size. Whilst many approaches have been proposed to compress LLMs to make their resource consumption manageable, these methods themselves tend to be resource intensive, putting them out of the reach of the very user groups they target. In this work, we explore the problem of structured pruning of LLMs using only forward passes. We seek to empower practitioners to prune models so large that their available hardware has just enough memory to run inference. We develop Bonsai, a gradient-free, perturbative pruning method capable of delivering small, fast, and accurate pruned models.   We observe that Bonsai outputs pruned models that (i) outperform those generated by more expensive gradient-based structured pruning methods, and (ii) are twice as fast (with comparable accuracy) as those generated by semi-structured pruning m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#33258;&#36866;&#24212;&#28608;&#27963;&#20989;&#25968;&#22312;&#31232;&#30095;&#23454;&#39564;&#25968;&#25454;&#39044;&#27979;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#23545;&#20854;&#24433;&#21709;&#20102;&#35299;&#19981;&#36275;&#30340;&#37325;&#35201;&#24046;&#36317;&#65292;&#24182;&#25581;&#31034;&#20986;&#20010;&#20307;&#21487;&#35843;&#21442;&#25968;&#30340;&#33258;&#36866;&#24212;&#28608;&#27963;&#20989;&#25968;&#22312;&#39044;&#27979;&#27169;&#22411;&#20013;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05401</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#28608;&#27963;&#20989;&#25968;&#22312;&#31232;&#30095;&#23454;&#39564;&#25968;&#25454;&#39044;&#27979;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adaptive Activation Functions for Predictive Modeling with Sparse Experimental Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#33258;&#36866;&#24212;&#28608;&#27963;&#20989;&#25968;&#22312;&#31232;&#30095;&#23454;&#39564;&#25968;&#25454;&#39044;&#27979;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#23545;&#20854;&#24433;&#21709;&#20102;&#35299;&#19981;&#36275;&#30340;&#37325;&#35201;&#24046;&#36317;&#65292;&#24182;&#25581;&#31034;&#20986;&#20010;&#20307;&#21487;&#35843;&#21442;&#25968;&#30340;&#33258;&#36866;&#24212;&#28608;&#27963;&#20989;&#25968;&#22312;&#39044;&#27979;&#27169;&#22411;&#20013;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#20851;&#38190;&#22312;&#20110;&#36873;&#25321;&#28608;&#27963;&#20989;&#25968;&#65292;&#29992;&#20110;&#24341;&#20837;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#36755;&#20837;-&#36755;&#20986;&#27169;&#24335;&#30340;&#38750;&#32447;&#24615;&#32467;&#26500;&#12290;&#34429;&#28982;&#22312;&#20855;&#26377;&#20805;&#36275;&#25968;&#25454;&#30340;&#39046;&#22495;&#65288;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#65289;&#20013;&#30740;&#31350;&#20102;&#33258;&#36866;&#24212;&#25110;&#21487;&#35843;&#28608;&#27963;&#20989;&#25968;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20854;&#23545;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#20173;&#28982;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;&#20004;&#31181;&#31867;&#22411;&#30340;&#33258;&#36866;&#24212;&#28608;&#27963;&#20989;&#25968;&#26469;&#22635;&#34917;&#36825;&#20123;&#24046;&#36317;&#12290;&#36825;&#20123;&#20989;&#25968;&#22312;&#27599;&#20010;&#38544;&#34255;&#23618;&#20013;&#24341;&#20837;&#20102;&#20849;&#20139;&#21644;&#20010;&#20307;&#21487;&#35843;&#21442;&#25968;&#65292;&#24182;&#22312;&#21253;&#21547;&#23569;&#20110;&#19968;&#30334;&#20010;&#35757;&#32451;&#23454;&#20363;&#30340;&#19977;&#20010;&#27979;&#35797;&#24179;&#21488;&#20013;&#36827;&#34892;&#20102;&#25506;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#20010;&#20307;&#21487;&#35843;&#21442;&#25968;&#30340;&#33258;&#36866;&#24212;&#28608;&#27963;&#20989;&#25968;&#65288;&#20363;&#22914;&#25351;&#25968;&#32447;&#24615;&#21333;&#20301;&#65288;ELU&#65289;&#21644;&#36719;&#21152;&#65289;&#22312;&#39044;&#27979;&#27169;&#22411;&#20013;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A pivotal aspect in the design of neural networks lies in selecting activation functions, crucial for introducing nonlinear structures that capture intricate input-output patterns. While the effectiveness of adaptive or trainable activation functions has been studied in domains with ample data, like image classification problems, significant gaps persist in understanding their influence on classification accuracy and predictive uncertainty in settings characterized by limited data availability. This research aims to address these gaps by investigating the use of two types of adaptive activation functions. These functions incorporate shared and individual trainable parameters per hidden layer and are examined in three testbeds derived from additive manufacturing problems containing fewer than one hundred training instances. Our investigation reveals that adaptive activation functions, such as Exponential Linear Unit (ELU) and Softplus, with individual trainable parameters, result in acc
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35757;&#32451;&#19968;&#31995;&#21015;&#25439;&#22833;&#20989;&#25968;&#26469;&#20248;&#21270;&#31867;&#21035;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#30340;ROC&#26354;&#32447;&#65292;&#24182;&#20943;&#23569;&#23545;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05400</link><description>&lt;p&gt;
&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#36890;&#36807;&#35757;&#32451;&#19968;&#31995;&#21015;&#25439;&#22833;&#20989;&#25968;&#26469;&#20248;&#21270;ROC&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Optimizing for ROC Curves on Class-Imbalanced Data by Training over a Family of Loss Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05400
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#19968;&#31995;&#21015;&#25439;&#22833;&#20989;&#25968;&#26469;&#20248;&#21270;&#31867;&#21035;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#30340;ROC&#26354;&#32447;&#65292;&#24182;&#20943;&#23569;&#23545;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20108;&#20803;&#20998;&#31867;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24050;&#32463;&#24471;&#21040;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#20294;&#22312;&#20005;&#37325;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#21487;&#38752;&#30340;&#20998;&#31867;&#22120;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#25110;&#20248;&#21270;&#26041;&#27861;&#26469;&#20943;&#36731;&#22312;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#35757;&#32451;&#30340;&#24433;&#21709;&#30340;&#25216;&#26415;&#12290;&#34429;&#28982;&#36825;&#20123;&#30740;&#31350;&#22312;&#22810;&#31867;&#21035;&#24773;&#20917;&#19979;&#25972;&#20307;&#20934;&#30830;&#29575;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#26041;&#27861;&#30340;&#36229;&#21442;&#25968;&#20540;&#30340;&#24494;&#23567;&#21464;&#21270;&#21487;&#33021;&#23548;&#33268;&#22312;&#20005;&#37325;&#19981;&#24179;&#34913;&#30340;&#20108;&#20803;&#38382;&#39064;&#19978;&#20197;ROC&#26354;&#32447;&#20026;&#25351;&#26631;&#30340;&#24615;&#33021;&#39640;&#24230;&#21464;&#21270;&#12290;&#20026;&#20102;&#38477;&#20302;&#23545;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#25935;&#24863;&#24615;&#65292;&#35757;&#32451;&#26356;&#36890;&#29992;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#19968;&#31995;&#21015;&#25439;&#22833;&#20989;&#25968;&#19978;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#21333;&#19968;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#20998;&#31867;&#38382;&#39064;&#19978;&#24212;&#29992;&#25439;&#22833;&#26465;&#20214;&#35757;&#32451;&#65288;Loss Conditional Training&#65292;LCT&#65289;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although binary classification is a well-studied problem in computer vision, training reliable classifiers under severe class imbalance remains a challenging problem. Recent work has proposed techniques that mitigate the effects of training under imbalance by modifying the loss functions or optimization methods. While this work has led to significant improvements in the overall accuracy in the multi-class case, we observe that slight changes in hyperparameter values of these methods can result in highly variable performance in terms of Receiver Operating Characteristic (ROC) curves on binary problems with severe imbalance. To reduce the sensitivity to hyperparameter choices and train more general models, we propose training over a family of loss functions, instead of a single loss function. We develop a method for applying Loss Conditional Training (LCT) to an imbalanced classification problem. Extensive experiment results, on both CIFAR and Kaggle competition datasets, show that our m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;TASER&#26041;&#27861;&#65292;&#23427;&#26159;&#38024;&#23545;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26102;&#38388;&#33258;&#36866;&#24212;&#37319;&#26679;&#25216;&#26415;&#65292;&#22312;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;&#29616;&#23454;&#19990;&#30028;&#21160;&#24577;&#22270;&#20013;&#23384;&#22312;&#30340;&#22122;&#22768;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05396</link><description>&lt;p&gt;
TASER: &#26102;&#38388;&#33258;&#36866;&#24212;&#37319;&#26679;&#30340;&#24555;&#36895;&#20934;&#30830;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05396
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;TASER&#26041;&#27861;&#65292;&#23427;&#26159;&#38024;&#23545;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26102;&#38388;&#33258;&#36866;&#24212;&#37319;&#26679;&#25216;&#26415;&#65292;&#22312;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;&#29616;&#23454;&#19990;&#30028;&#21160;&#24577;&#22270;&#20013;&#23384;&#22312;&#30340;&#22122;&#22768;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TGNN&#65289;&#22312;&#21253;&#25324;&#27450;&#35784;&#26816;&#27979;&#21644;&#20869;&#23481;&#25512;&#33616;&#22312;&#20869;&#30340;&#21508;&#31181;&#37325;&#35201;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;TGNN&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#29616;&#23454;&#19990;&#30028;&#21160;&#24577;&#22270;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#26102;&#38388;&#36807;&#26102;&#30340;&#38142;&#25509;&#21644;&#20559;&#26012;&#30340;&#20132;&#20114;&#20998;&#24067;&#12290;&#36825;&#20123;&#22122;&#22768;&#23548;&#33268;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#20005;&#37325;&#25439;&#23475;&#20102;TGNN&#30340;&#20934;&#30830;&#24615;&#65306;&#65288;1&#65289;&#27169;&#22411;&#21463;&#21040;&#36739;&#24046;&#20132;&#20114;&#30340;&#30417;&#30563;&#65292;&#65288;2&#65289;&#22122;&#22768;&#36755;&#20837;&#23548;&#33268;&#32858;&#21512;&#28040;&#24687;&#30340;&#39640;&#26041;&#24046;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;TGNN&#21435;&#22122;&#25216;&#26415;&#24182;&#26410;&#32771;&#34385;&#27599;&#20010;&#33410;&#28857;&#30340;&#22810;&#26679;&#21270;&#21644;&#21160;&#24577;&#30340;&#22122;&#22768;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36824;&#38754;&#20020;&#30528;&#36941;&#21382;&#26356;&#22810;&#37051;&#23621;&#23548;&#33268;&#20135;&#29983;&#36807;&#22810;&#23567;&#25209;&#37327;&#30340;&#24320;&#38144;&#12290;&#25105;&#20204;&#30456;&#20449;&#24555;&#36895;&#20934;&#30830;&#30340;TGNN&#30340;&#35299;&#20915;&#26041;&#27861;&#22312;&#20110;&#26102;&#38388;&#33258;&#36866;&#24212;&#37319;&#26679;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TASER&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#36827;&#34892;&#20248;&#21270;&#30340;TGNN&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated state-of-the-art performance in various high-impact applications, including fraud detection and content recommendation. Despite the success of TGNNs, they are prone to the prevalent noise found in real-world dynamic graphs like time-deprecated links and skewed interaction distribution. The noise causes two critical issues that significantly compromise the accuracy of TGNNs: (1) models are supervised by inferior interactions, and (2) noisy input induces high variance in the aggregated messages. However, current TGNN denoising techniques do not consider the diverse and dynamic noise pattern of each node. In addition, they also suffer from the excessive mini-batch generation overheads caused by traversing more neighbors. We believe the remedy for fast and accurate TGNNs lies in temporal adaptive sampling. In this work, we propose TASER, the first adaptive sampling method for TGNNs optimized for accuracy, efficiency, and sc
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#32508;&#36848;&#20171;&#32461;&#20102;KG4MM&#21644;MM4KG&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#20219;&#21153;&#23450;&#20041;&#12289;&#26500;&#24314;&#36827;&#23637;&#12289;&#35780;&#20272;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#30740;&#31350;&#36712;&#36857;&#12290;</title><link>https://arxiv.org/abs/2402.05391</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05391
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#32508;&#36848;&#20171;&#32461;&#20102;KG4MM&#21644;MM4KG&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#20219;&#21153;&#23450;&#20041;&#12289;&#26500;&#24314;&#36827;&#23637;&#12289;&#35780;&#20272;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#30740;&#31350;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22312;&#25512;&#21160;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#35821;&#20041;&#32593;&#32476;&#31038;&#21306;&#23545;&#22810;&#27169;&#24577;&#32500;&#24230;&#30340;&#25506;&#32034;&#20026;&#21019;&#26032;&#25171;&#24320;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20180;&#32454;&#23457;&#26597;&#20102;300&#22810;&#31687;&#25991;&#31456;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#30340;&#30693;&#35782;&#22270;&#35889;&#24863;&#30693;&#30740;&#31350;&#65306;&#20197;&#30693;&#35782;&#22270;&#35889;&#25903;&#25345;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;KG&#39537;&#21160;&#22810;&#27169;&#24577;&#65288;KG4MM&#65289;&#23398;&#20064;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#30740;&#31350;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65288;MM4KG&#65289;&#39046;&#22495;&#12290;&#25105;&#20204;&#20174;&#23450;&#20041;&#30693;&#35782;&#22270;&#35889;&#21644;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#24320;&#22987;&#65292;&#28982;&#21518;&#25506;&#32034;&#23427;&#20204;&#30340;&#26500;&#24314;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#31867;&#21035;&#65306;KG&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#21644;&#35270;&#35273;&#38382;&#31572;&#65292;&#20197;&#21450;&#20869;&#22312;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#20219;&#21153;&#65292;&#22914;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#21644;&#23454;&#20307;&#23545;&#40784;&#65292;&#31361;&#20986;&#20102;&#20855;&#20307;&#30340;&#30740;&#31350;&#36712;&#36857;&#12290;&#23545;&#20110;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#22823;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23450;&#20041;&#12289;&#35780;&#20272;&#22522;&#20934;&#65292;&#24182;&#36827;&#19968;&#27493;&#25351;&#20986;&#36827;&#34892;&#30456;&#20851;&#30740;&#31350;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;cu
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs (KGs) play a pivotal role in advancing various AI applications, with the semantic web community's exploration into multi-modal dimensions unlocking new avenues for innovation. In this survey, we carefully review over 300 articles, focusing on KG-aware research in two principal aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into the MMKG realm. We begin by defining KGs and MMKGs, then explore their construction progress. Our review includes two primary task categories: KG-aware multi-modal learning tasks, such as Image Classification and Visual Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph Completion and Entity Alignment, highlighting specific research trajectories. For most of these tasks, we provide definitions, evaluation benchmarks, and additionally outline essential insights for conducting relevant research. Finally, we discuss cu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#32858;&#31867;&#26465;&#20214;&#19987;&#23478;&#30340;&#20219;&#21153;&#23450;&#21046;&#21270;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;(MoCE)&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32858;&#31867;&#26465;&#20214;&#38376;&#23558;&#27599;&#20010;&#19987;&#23478;&#21482;&#35757;&#32451;&#19982;&#35821;&#20041;&#30456;&#20851;&#30340;&#22270;&#20687;&#65292;&#20026;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#23450;&#21046;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#20256;&#32479;MAE&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.05382</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#32858;&#31867;&#26465;&#20214;&#19987;&#23478;&#30340;&#20219;&#21153;&#23450;&#21046;&#21270;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Task-customized Masked AutoEncoder via Mixture of Cluster-conditional Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05382
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#32858;&#31867;&#26465;&#20214;&#19987;&#23478;&#30340;&#20219;&#21153;&#23450;&#21046;&#21270;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;(MoCE)&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32858;&#31867;&#26465;&#20214;&#38376;&#23558;&#27599;&#20010;&#19987;&#23478;&#21482;&#35757;&#32451;&#19982;&#35821;&#20041;&#30456;&#20851;&#30340;&#22270;&#20687;&#65292;&#20026;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#23450;&#21046;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#20256;&#32479;MAE&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;(MAE)&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#21462;&#24471;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#25968;&#25454;&#20998;&#24067;&#19982;&#39044;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#26102;&#65292;&#35821;&#20041;&#19981;&#30456;&#20851;&#30340;&#39044;&#35757;&#32451;&#20449;&#24687;&#21487;&#33021;&#23548;&#33268;&#36127;&#36801;&#31227;&#65292;&#21046;&#32422;&#20102;MAE&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;MAE&#30340;&#39044;&#35757;&#32451;&#33539;&#24335;&#65292;&#21363;&#28151;&#21512;&#32858;&#31867;&#26465;&#20214;&#19987;&#23478;(MoCE)&#65292;&#23427;&#21487;&#20197;&#20165;&#35757;&#32451;&#19968;&#27425;&#65292;&#20294;&#20026;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#23450;&#21046;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#19982;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;(MoE)&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;MoCE&#20351;&#29992;&#32858;&#31867;&#26465;&#20214;&#38376;&#21482;&#35757;&#32451;&#27599;&#20010;&#19987;&#23478;&#20197;&#35821;&#20041;&#30456;&#20851;&#30340;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;&#27599;&#20010;&#19979;&#28216;&#20219;&#21153;&#21487;&#20197;&#20998;&#37197;&#21040;&#20854;&#19982;&#19979;&#28216;&#25968;&#25454;&#26368;&#30456;&#20284;&#30340;&#25968;&#25454;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;11&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;MoCE&#30340;&#24179;&#22343;&#24615;&#33021;&#27604;&#26222;&#36890;MAE&#25552;&#39640;&#20102;2.45%&#12290;&#23427;&#36824;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Autoencoder~(MAE) is a prevailing self-supervised learning method that achieves promising results in model pre-training. However, when the various downstream tasks have data distributions different from the pre-training data, the semantically irrelevant pre-training information might result in negative transfer, impeding MAE's scalability. To address this issue, we propose a novel MAE-based pre-training paradigm, Mixture of Cluster-conditional Experts (MoCE), which can be trained once but provides customized pre-training models for diverse downstream tasks. Different from the mixture of experts (MoE), our MoCE trains each expert only with semantically relevant images by using cluster-conditional gates. Thus, each downstream task can be allocated to its customized model pre-trained with data most similar to the downstream data. Experiments on a collection of 11 downstream tasks show that MoCE outperforms the vanilla MAE by 2.45\% on average. It also obtains new state-of-the-art s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#23545;&#35282;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#20272;&#35745;&#22120;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#20998;&#26512;&#21644;&#25968;&#20540;&#30740;&#31350;&#65292;&#21457;&#29616;&#26041;&#24046;&#37327;&#21462;&#20915;&#20110;&#38750;&#32447;&#24615;&#19982;&#19981;&#21516;&#21442;&#25968;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24212;&#35813;&#22312;&#20272;&#35745;&#36153;&#33293;&#23572;&#20449;&#24687;&#26102;&#20104;&#20197;&#37325;&#35270;&#12290;</title><link>https://arxiv.org/abs/2402.05379</link><description>&lt;p&gt;
&#23545;&#35282;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#20272;&#35745;&#22120;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Tradeoffs of Diagonal Fisher Information Matrix Estimators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#23545;&#35282;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#20272;&#35745;&#22120;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#20998;&#26512;&#21644;&#25968;&#20540;&#30740;&#31350;&#65292;&#21457;&#29616;&#26041;&#24046;&#37327;&#21462;&#20915;&#20110;&#38750;&#32447;&#24615;&#19982;&#19981;&#21516;&#21442;&#25968;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24212;&#35813;&#22312;&#20272;&#35745;&#36153;&#33293;&#23572;&#20449;&#24687;&#26102;&#20104;&#20197;&#37325;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#25551;&#36848;&#20102;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#23616;&#37096;&#20960;&#20309;&#24615;&#36136;&#65292;&#23427;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#24037;&#20855;&#26469;&#29702;&#35299;&#21644;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#12290;&#37492;&#20110;&#20854;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#23454;&#36341;&#32773;&#36890;&#24120;&#20351;&#29992;&#38543;&#26426;&#20272;&#35745;&#22120;&#65292;&#24182;&#20165;&#35780;&#20272;&#23545;&#35282;&#32447;&#26465;&#30446;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#36825;&#26679;&#30340;&#20272;&#35745;&#22120;&#65292;&#20854;&#20934;&#30830;&#24615;&#21644;&#26679;&#26412;&#22797;&#26434;&#24615;&#21462;&#20915;&#20110;&#23427;&#20204;&#20851;&#32852;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#26041;&#24046;&#30340;&#30028;&#38480;&#65292;&#24182;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#32593;&#32476;&#20013;&#23454;&#20363;&#21270;&#23427;&#20204;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#21644;&#25968;&#20540;&#30740;&#31350;&#26469;&#26435;&#34913;&#36825;&#20004;&#20010;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#21457;&#29616;&#26041;&#24046;&#37327;&#21462;&#20915;&#20110;&#20851;&#20110;&#19981;&#21516;&#21442;&#25968;&#32452;&#30340;&#38750;&#32447;&#24615;&#65292;&#24403;&#20272;&#35745;&#36153;&#33293;&#23572;&#20449;&#24687;&#26102;&#19981;&#33021;&#24573;&#35270;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Fisher information matrix characterizes the local geometry in the parameter space of neural networks. It elucidates insightful theories and useful tools to understand and optimize neural networks. Given its high computational cost, practitioners often use random estimators and evaluate only the diagonal entries. We examine two such estimators, whose accuracy and sample complexity depend on their associated variances. We derive bounds of the variances and instantiate them in regression and classification networks. We navigate trade-offs of both estimators based on analytical and numerical studies. We find that the variance quantities depend on the non-linearity with respect to different parameter groups and should not be neglected when estimating the Fisher information.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#29992;&#25143;&#28789;&#27963;&#21452;&#24037;&#32593;&#32476;&#20013;&#30340;&#29289;&#29702;&#23618;&#23433;&#20840;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#20854;&#22312;&#24615;&#33021;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.05378</link><description>&lt;p&gt;
&#29289;&#29702;&#23618;&#23433;&#20840;&#22312;&#22810;&#29992;&#25143;&#28789;&#27963;&#21452;&#24037;&#32593;&#32476;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Physical-Layer Security in Multi-User Flexible-Duplex Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#29992;&#25143;&#28789;&#27963;&#21452;&#24037;&#32593;&#32476;&#20013;&#30340;&#29289;&#29702;&#23618;&#23433;&#20840;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#20854;&#22312;&#24615;&#33021;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#28789;&#27963;&#21452;&#24037;&#32593;&#32476;&#20013;&#30340;&#29289;&#29702;&#23618;&#23433;&#20840;&#65288;PLS&#65289;&#65292;&#32771;&#34385;&#21040;&#31363;&#21548;&#32773;&#30340;&#24773;&#26223;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22260;&#32469;&#30528;&#27714;&#35299;&#24635;&#20445;&#23494;&#36895;&#29575;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#23637;&#24320;&#65292;&#29305;&#21035;&#26159;&#38754;&#23545;&#37319;&#29992;&#26368;&#23567;&#22343;&#26041;&#35823;&#24046;&#65288;MMSE&#65289;&#25509;&#25910;&#22120;&#30340;&#21327;&#35843;&#21644;&#20998;&#24067;&#24335;&#31363;&#21548;&#32773;&#26102;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#22522;&#20110;&#36845;&#20195;&#32463;&#20856;&#20248;&#21270;&#35299;&#21644;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26631;&#24535;&#30528;GNN&#22312;PLS&#24212;&#29992;&#19978;&#30340;&#21021;&#27493;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;GNN&#26041;&#27861;&#25193;&#23637;&#21040;&#35299;&#20915;&#31363;&#21548;&#32773;&#30340;&#20449;&#36947;&#20449;&#24687;&#32570;&#22833;&#38382;&#39064;&#12290;&#24191;&#27867;&#30340;&#25968;&#20540;&#27169;&#25311;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#24615;&#33021;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#26041;&#38754;&#65292;&#28789;&#27963;&#21452;&#24037;&#36890;&#20449;&#20248;&#20110;&#21322;&#21452;&#24037;&#65288;HD&#65289;&#36890;&#20449;&#65292;GNN&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores Physical-Layer Security (PLS) in Flexible Duplex (FlexD) networks, considering scenarios involving eavesdroppers. Our investigation revolves around the intricacies of the sum secrecy rate maximization problem, particularly when faced with coordinated and distributed eavesdroppers employing a Minimum Mean Square Error (MMSE) receiver. Our contributions include an iterative classical optimization solution and an unsupervised learning strategy based on Graph Neural Networks (GNNs). To the best of our knowledge, this work marks the initial exploration of GNNs for PLS applications. Additionally, we extend the GNN approach to address the absence of eavesdroppers' channel knowledge. Extensive numerical simulations highlight FlexD's superiority over Half-Duplex (HD) communications and the GNN approach's superiority over the classical method in both performance and time complexity.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20851;&#27880;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#26041;&#27861;&#36827;&#34892;&#38750;&#23450;&#24120;&#27969;&#20307;&#27969;&#21160;&#30340;&#38477;&#38454;&#24314;&#27169;&#12290;&#30740;&#31350;&#20351;&#29992;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#22788;&#29702;&#31354;&#38388;&#20998;&#24067;&#30340;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#38598;&#25104;&#23398;&#20064;&#25216;&#26415;&#20013;&#30340;&#35013;&#34955;&#26041;&#27861;&#26469;&#35299;&#20915;&#38169;&#35823;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05372</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#30340;&#38477;&#38454;&#24314;&#27169;&#26041;&#27861;&#30740;&#31350;&#38750;&#23450;&#24120;&#27969;&#20307;&#27969;&#21160;
&lt;/p&gt;
&lt;p&gt;
Reduced-order modeling of unsteady fluid flow using neural network ensembles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20851;&#27880;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#26041;&#27861;&#36827;&#34892;&#38750;&#23450;&#24120;&#27969;&#20307;&#27969;&#21160;&#30340;&#38477;&#38454;&#24314;&#27169;&#12290;&#30740;&#31350;&#20351;&#29992;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#22788;&#29702;&#31354;&#38388;&#20998;&#24067;&#30340;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#38598;&#25104;&#23398;&#20064;&#25216;&#26415;&#20013;&#30340;&#35013;&#34955;&#26041;&#27861;&#26469;&#35299;&#20915;&#38169;&#35823;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#38477;&#38454;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20197;&#20415;&#33719;&#24471;&#23436;&#20840;&#27169;&#22411;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;&#22312;&#22788;&#29702;&#31354;&#38388;&#20998;&#24067;&#30340;&#25968;&#25454;&#65288;&#21253;&#25324;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#65289;&#26102;&#65292;&#36890;&#24120;&#20351;&#29992;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#65288;CAEs&#65289;&#12290;&#22312;&#24212;&#29992;&#20110;&#38750;&#23450;&#24120;&#29289;&#29702;&#38382;&#39064;&#26102;&#65292;&#38477;&#38454;&#27169;&#22411;&#36824;&#38656;&#35201;&#23545;&#20302;&#32500;&#28508;&#21464;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#39044;&#27979;&#12290;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#24314;&#27169;&#24207;&#21015;&#25968;&#25454;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#38477;&#38454;&#27169;&#22411;&#20013;&#32463;&#24120;&#29992;&#20110;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#22312;&#26410;&#30693;&#35774;&#35745;&#28857;&#19978;&#36827;&#34892;&#38271;&#26102;&#38388;&#36328;&#24230;&#30340;&#39044;&#27979;&#26102;&#65292;&#32463;&#24120;&#20250;&#36935;&#21040;&#38169;&#35823;&#20256;&#25773;&#30340;&#38382;&#39064;&#65292;&#21363;&#26089;&#26399;&#38169;&#35823;&#21487;&#33021;&#20250;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#19981;&#26029;&#32047;&#31215;&#24182;&#23548;&#33268;&#36739;&#22823;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#25216;&#26415;&#20013;&#24120;&#29992;&#30340;&#35013;&#34955;&#26041;&#27861;&#26469;&#24320;&#21457;&#19968;&#20010;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#38477;&#38454;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of deep learning has become increasingly popular in reduced-order models (ROMs) to obtain low-dimensional representations of full-order models. Convolutional autoencoders (CAEs) are often used to this end as they are adept at handling data that are spatially distributed, including solutions to partial differential equations. When applied to unsteady physics problems, ROMs also require a model for time-series prediction of the low-dimensional latent variables. Long short-term memory (LSTM) networks, a type of recurrent neural network useful for modeling sequential data, are frequently employed in data-driven ROMs for autoregressive time-series prediction. When making predictions at unseen design points over long time horizons, error propagation is a frequently encountered issue, where errors made early on can compound over time and lead to large inaccuracies. In this work, we propose using bagging, a commonly used ensemble learning technique, to develop a fully data-driven ROM f
&lt;/p&gt;</description></item><item><title>&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#27880;&#24847;&#21147;&#26435;&#37325;&#25552;&#21319;&#20026;&#20027;&#35201;&#34920;&#31034;&#65292;&#20351;&#29992;&#20840;&#23616;&#26631;&#24535;&#21644;&#23616;&#37096;&#31383;&#21475;&#26500;&#24314;&#30340;&#27880;&#24847;&#21147;&#22270;&#20316;&#20026;&#31283;&#20581;&#26680;&#34920;&#31034;&#26469;&#20811;&#26381;&#22122;&#22768;&#21644;&#20998;&#24067;&#21464;&#21270;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021; improvement.</title><link>https://arxiv.org/abs/2402.05370</link><description>&lt;p&gt;
&#20316;&#20026;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#31283;&#20581;&#34920;&#31034;&#30340;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Attention as Robust Representation for Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05370
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#27880;&#24847;&#21147;&#26435;&#37325;&#25552;&#21319;&#20026;&#20027;&#35201;&#34920;&#31034;&#65292;&#20351;&#29992;&#20840;&#23616;&#26631;&#24535;&#21644;&#23616;&#37096;&#31383;&#21475;&#26500;&#24314;&#30340;&#27880;&#24847;&#21147;&#22270;&#20316;&#20026;&#31283;&#20581;&#26680;&#34920;&#31034;&#26469;&#20811;&#26381;&#22122;&#22768;&#21644;&#20998;&#24067;&#21464;&#21270;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021; improvement.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#30001;&#20110;Transformer&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#30340;&#20248;&#31168;&#24615;&#33021;&#65292;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#36880;&#28176;&#22686;&#22810;&#12290;Transformers&#30340;&#20851;&#38190;&#29305;&#24615;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21160;&#24577;&#22320;&#34701;&#21512;&#23884;&#20837;&#20197;&#22686;&#24378;&#25968;&#25454;&#34920;&#31034;&#65292;&#36890;&#24120;&#23558;&#27880;&#24847;&#21147;&#26435;&#37325;&#20316;&#20026;&#21103;&#20135;&#21697;&#12290;&#28982;&#32780;&#65292;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20855;&#26377;&#22122;&#22768;&#21644;&#38750;&#24179;&#31283;&#24615;&#65292;&#32473;&#39044;&#27979;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#27880;&#24847;&#21147;&#26435;&#37325;&#25552;&#21319;&#20026;&#26102;&#38388;&#24207;&#21015;&#30340;&#20027;&#35201;&#34920;&#31034;&#65292;&#21033;&#29992;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#26469;&#25913;&#21892;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#20840;&#23616;&#26631;&#24535;&#21644;&#23616;&#37096;&#31383;&#21475;&#26500;&#24314;&#30340;&#27880;&#24847;&#21147;&#22270;&#20805;&#24403;&#25968;&#25454;&#28857;&#30340;&#31283;&#20581;&#26680;&#34920;&#31034;&#65292;&#33021;&#22815;&#25269;&#25239;&#22122;&#22768;&#21644;&#20998;&#24067;&#31227;&#21160;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#65292;&#23558;&#22343;&#26041;&#35823;&#24046;(MSE)&#26174;&#33879;&#38477;&#20302;&#20102;3.6%&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#32780;&#19981;&#25913;&#21464;&#26680;&#24515;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting is essential for many practical applications, with the adoption of transformer-based models on the rise due to their impressive performance in NLP and CV. Transformers' key feature, the attention mechanism, dynamically fusing embeddings to enhance data representation, often relegating attention weights to a byproduct role. Yet, time series data, characterized by noise and non-stationarity, poses significant forecasting challenges. Our approach elevates attention weights as the primary representation for time series, capitalizing on the temporal relationships among data points to improve forecasting accuracy. Our study shows that an attention map, structured using global landmarks and local windows, acts as a robust kernel representation for data points, withstanding noise and shifts in distribution. Our method outperforms state-of-the-art models, reducing mean squared error (MSE) in multivariate time series forecasting by a notable 3.6% without altering the core
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#36890;&#29992;LM&#23545;&#40784;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#26126;&#30830;&#27880;&#37322;&#30340;&#22870;&#21169;&#25968;&#25454;&#65292;&#24182;&#19988;&#25193;&#23637;&#20102;&#24403;&#21069;&#30340;&#23545;&#40784;&#29702;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.05369</link><description>&lt;p&gt;
&#20197;&#26174;&#24335;&#22870;&#21169;&#30340;&#22122;&#22768;&#23545;&#27604;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Noise Contrastive Alignment of Language Models with Explicit Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#36890;&#29992;LM&#23545;&#40784;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#26126;&#30830;&#27880;&#37322;&#30340;&#22870;&#21169;&#25968;&#25454;&#65292;&#24182;&#19988;&#25193;&#23637;&#20102;&#24403;&#21069;&#30340;&#23545;&#40784;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#24847;&#22270;&#36890;&#24120;&#34987;&#24418;&#24335;&#21270;&#20026;&#38656;&#35201;&#22312;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#26102;&#26368;&#22823;&#21270;&#30340;&#35780;&#20272;&#22870;&#21169;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#22914;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#65288;DPO&#65289;&#65292;&#20027;&#35201;&#36866;&#29992;&#20110;&#38544;&#21547;&#23450;&#20041;&#32780;&#38750;&#26126;&#30830;&#32473;&#23450;&#22870;&#21169;&#30340;&#20004;&#20004;&#20559;&#22909;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;LM&#23545;&#40784;&#26694;&#26550;&#65292;&#21033;&#29992;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#26469;&#35299;&#20915;&#26126;&#30830;&#27880;&#37322;&#26377;&#26631;&#37327;&#35780;&#20272;&#30340;&#22870;&#21169;&#25968;&#25454;&#22788;&#29702;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#24182;&#34892;&#31639;&#27861;&#65292;NCA&#21644;InfoNCA&#65292;&#20004;&#32773;&#37117;&#33021;&#20174;&#22870;&#21169;&#25968;&#25454;&#21644;&#20559;&#22909;&#25968;&#25454;&#20013;&#30452;&#25509;&#25552;&#21462;LM&#31574;&#30053;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DPO&#25439;&#22833;&#26159;&#25105;&#20204;&#25552;&#20986;&#30340;InfoNCA&#30446;&#26631;&#22312;&#20004;&#20004;&#20559;&#22909;&#35774;&#32622;&#19979;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#20174;&#32780;&#38598;&#25104;&#21644;&#25193;&#23637;&#20102;&#24403;&#21069;&#30340;&#23545;&#40784;&#29702;&#35770;&#12290;&#36890;&#36807;&#23545;&#27604;NCA&#21644;InfoNCA&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;InfoNCA&#21644;DPO&#22914;&#20309;&#22312;&#19981;&#21516;&#21709;&#24212;&#23545;&#20110;&#21333;&#20010;&#25351;&#20196;&#30340;&#30456;&#23545;&#21487;&#33021;&#24615;&#19978;&#36827;&#34892;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
User intentions are typically formalized as evaluation rewards to be maximized when fine-tuning language models (LMs). Existing alignment methods, such as Direct Preference Optimization (DPO), are mainly tailored for pairwise preference data where rewards are implicitly defined rather than explicitly given. In this paper, we introduce a general framework for LM alignment, leveraging Noise Contrastive Estimation (NCE) to bridge the gap in handling reward datasets explicitly annotated with scalar evaluations. Our framework comprises two parallel algorithms, NCA and InfoNCA, both enabling the direct extraction of an LM policy from reward data as well as preference data. Notably, we show that the DPO loss is a special case of our proposed InfoNCA objective under pairwise preference settings, thereby integrating and extending current alignment theories. By contrasting NCA and InfoNCA, we show that InfoNCA and DPO adjust relative likelihood across different responses to a single instruction,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21407;&#21017;&#30340;&#20248;&#20808;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20559;&#22909;&#21453;&#39304;&#26500;&#24314;&#40657;&#30418;&#20989;&#25968;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20048;&#35266;&#31639;&#27861;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#36951;&#25022;&#30028;&#38480;&#21644;&#25910;&#25947;&#24615;&#19978;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.05367</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#21017;&#30340;&#20248;&#20808;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Principled Preferential Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21407;&#21017;&#30340;&#20248;&#20808;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20559;&#22909;&#21453;&#39304;&#26500;&#24314;&#40657;&#30418;&#20989;&#25968;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20048;&#35266;&#31639;&#27861;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#36951;&#25022;&#30028;&#38480;&#21644;&#25910;&#25947;&#24615;&#19978;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20248;&#20808;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#24076;&#26395;&#20165;&#20973;&#20559;&#22909;&#21453;&#39304;&#26469;&#20248;&#21270;&#40657;&#30418;&#20989;&#25968;&#30340;&#19968;&#23545;&#20505;&#36873;&#35299;&#12290;&#21463;&#21040;&#20284;&#28982;&#27604;&#24605;&#24819;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#20165;&#20973;&#20559;&#22909;&#21453;&#39304;&#26500;&#24314;&#40657;&#30418;&#20989;&#25968;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20048;&#35266;&#31639;&#27861;&#21644;&#39640;&#25928;&#30340;&#35745;&#31639;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23427;&#22312;&#32047;&#31215;&#36951;&#25022;&#19978;&#20855;&#26377;&#20449;&#24687;&#35770;&#30340;&#30028;&#38480;&#65292;&#36825;&#22312;&#20248;&#20808;BO&#20013;&#26159;&#39318;&#27425;&#12290;&#36825;&#20010;&#30028;&#38480;&#36827;&#19968;&#27493;&#20801;&#35768;&#25105;&#20204;&#35774;&#35745;&#19968;&#20010;&#26041;&#26696;&#26469;&#25253;&#21578;&#26368;&#20339;&#35299;&#30340;&#20272;&#35745;&#20540;&#65292;&#24182;&#20445;&#35777;&#25910;&#25947;&#36895;&#29575;&#12290;&#20174;&#39640;&#26031;&#36807;&#31243;&#12289;&#26631;&#20934;&#27979;&#35797;&#20989;&#25968;&#21644;&#19968;&#20010;&#28909;&#33298;&#36866;&#24230;&#20248;&#21270;&#38382;&#39064;&#30340;&#23454;&#39564;&#32467;&#26524;&#37117;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#35828;&#65292;&#31283;&#23450;&#22320;&#36798;&#21040;&#26356;&#22909;&#25110;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#32780;&#29616;&#26377;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#36951;&#25022;&#30028;&#38480;&#25110;&#25910;&#25947;&#24615;&#19978;&#27809;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of preferential Bayesian optimization (BO), where we aim to optimize a black-box function with only preference feedback over a pair of candidate solutions. Inspired by the likelihood ratio idea, we construct a confidence set of the black-box function using only the preference feedback. An optimistic algorithm with an efficient computational method is then developed to solve the problem, which enjoys an information-theoretic bound on the cumulative regret, a first-of-its-kind for preferential BO. This bound further allows us to design a scheme to report an estimated best solution, with a guaranteed convergence rate. Experimental results on sampled instances from Gaussian processes, standard test functions, and a thermal comfort optimization problem all show that our method stably achieves better or competitive performance as compared to the existing state-of-the-art heuristics, which, however, do not have theoretical guarantees on regret bounds or convergence.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05359</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#27835;&#31243;&#24207;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#38382;&#39064;&#27714;&#35299;&#36827;&#34892;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#24403;&#30340;&#25552;&#31034;&#35774;&#35745;&#65292;&#22914;&#24605;&#32500;&#38142;&#65292;&#21487;&#20197;&#37322;&#25918;LLM&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22788;&#29702;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#20219;&#21153;&#65288;&#22914;&#31639;&#26415;&#35745;&#31639;&#21644;&#25991;&#31456;&#32423;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#65289;&#65292;&#29616;&#26377;&#30340;&#25552;&#31034;&#31574;&#30053;&#35201;&#20040;&#34920;&#29616;&#20986;&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#65292;&#35201;&#20040;&#30001;&#24187;&#35273;&#24341;&#21457;&#20013;&#38388;&#38169;&#35823;&#12290;&#20026;&#20102;&#20351;LLM&#23545;&#36825;&#20123;&#20013;&#38388;&#38169;&#35823;&#26356;&#20855;&#36776;&#21035;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;LLM&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#30830;&#20445;&#20248;&#36234;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#20219;&#21153;&#20998;&#35299;&#12289;&#23376;&#20219;&#21153;&#35299;&#20915;&#21644;&#35299;&#20915;&#32452;&#35013;&#36807;&#31243;&#30340;&#20998;&#31163;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#21487;&#20197;&#24341;&#23548;LLM&#25193;&#23637;&#22266;&#23450;&#28145;&#24230;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination. To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer. Experiments indicate that our proposed method can achiev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23398;&#20064;&#22797;&#26434;&#24615;&#20316;&#20026;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#20197;&#35299;&#20915;&#22312;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#19979;&#24494;&#35843;&#36807;&#31243;&#20013;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05356</link><description>&lt;p&gt;
&#25506;&#32034;&#29992;&#20110;&#19979;&#28216;&#25968;&#25454;&#20462;&#21098;&#30340;&#23398;&#20064;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring Learning Complexity for Downstream Data Pruning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23398;&#20064;&#22797;&#26434;&#24615;&#20316;&#20026;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#20197;&#35299;&#20915;&#22312;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#19979;&#24494;&#35843;&#36807;&#31243;&#20013;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#20110;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#30340;&#24494;&#35843;&#26500;&#25104;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#19968;&#20010;&#30452;&#35266;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20174;&#24494;&#35843;&#25968;&#25454;&#38598;&#20013;&#20462;&#21098;&#25481;&#20449;&#24687;&#36739;&#23569;&#30340;&#26679;&#26412;&#12290;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;&#35757;&#32451;&#30340;&#35780;&#20998;&#20989;&#25968;&#26469;&#37327;&#21270;&#25968;&#25454;&#23376;&#38598;&#30340;&#20449;&#24687;&#24615;&#65292;&#20294;&#30001;&#20110;&#21442;&#25968;&#26356;&#26032;&#30340;&#32321;&#37325;&#65292;&#20462;&#21098;&#25104;&#26412;&#21464;&#24471;&#19981;&#21487;&#24573;&#35270;&#12290;&#20026;&#20102;&#39640;&#25928;&#20462;&#21098;&#65292;&#23558;&#20960;&#20309;&#26041;&#27861;&#30340;&#30456;&#20284;&#24230;&#35780;&#20998;&#20989;&#25968;&#20174;&#22522;&#20110;&#35757;&#32451;&#30340;&#26041;&#27861;&#36866;&#24212;&#20026;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#26159;&#21487;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20973;&#32463;&#39564;&#35777;&#26126;&#36825;&#31181;&#36866;&#24212;&#25197;&#26354;&#20102;&#21407;&#22987;&#30340;&#20462;&#21098;&#24182;&#23548;&#33268;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#23398;&#20064;&#22797;&#26434;&#24615;&#65288;LC&#65289;&#20316;&#20026;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#30340;&#35780;&#20998;&#20989;&#25968;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23398;&#20064;&#22797;&#26434;&#24615;&#34987;&#23450;&#20041;&#20026;&#20855;&#26377;&#19981;&#21516;&#23481;&#37327;&#30340;&#23376;&#32593;&#32476;&#30340;&#24179;&#22343;&#39044;&#27979;&#32622;&#20449;&#24230;&#65292;&#23427;&#21253;&#21547;&#20102;&#22312;&#19968;&#20010;&#25910;&#25947;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The over-parameterized pre-trained models pose a great challenge to fine-tuning with limited computation resources. An intuitive solution is to prune the less informative samples from the fine-tuning dataset. A series of training-based scoring functions are proposed to quantify the informativeness of the data subset but the pruning cost becomes non-negligible due to the heavy parameter updating. For efficient pruning, it is viable to adapt the similarity scoring function of geometric-based methods from training-based to training-free. However, we empirically show that such adaption distorts the original pruning and results in inferior performance on the downstream tasks. In this paper, we propose to treat the learning complexity (LC) as the scoring function for classification and regression tasks. Specifically, the learning complexity is defined as the average predicted confidence of subnets with different capacities, which encapsulates data processing within a converged model. Then we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32852;&#37030;&#26631;&#31614;&#28151;&#21512;&#27491;&#21017;&#21270;&#65288;FLR&#65289;&#30340;&#31574;&#30053;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#26631;&#31614;&#22122;&#38899;&#38382;&#39064;&#12290;FLR&#36890;&#36807;&#29983;&#25104;&#26032;&#30340;&#20266;&#26631;&#31614;&#65292;&#23558;&#23616;&#37096;&#21644;&#20840;&#23616;&#27169;&#22411;&#39044;&#27979;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#20840;&#23616;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#26377;&#25928;&#23545;&#25239;&#22122;&#38899;&#26631;&#31614;&#30340;&#35760;&#24518;&#21270;&#12290;FLR&#19982;&#29616;&#26377;&#30340;&#26631;&#31614;&#22122;&#38899;&#21644;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#20860;&#23481;&#65292;&#20026;&#22788;&#29702;&#20805;&#28385;&#26631;&#31614;&#19981;&#20934;&#30830;&#24615;&#30340;FL&#29615;&#22659;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05353</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#36935;&#21040;&#22024;&#26434;&#26631;&#31614;&#26102;&#37325;&#28201;&#26089;&#26399;&#23398;&#20064;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Revisiting Early-Learning Regularization When Federated Learning Meets Noisy Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32852;&#37030;&#26631;&#31614;&#28151;&#21512;&#27491;&#21017;&#21270;&#65288;FLR&#65289;&#30340;&#31574;&#30053;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#26631;&#31614;&#22122;&#38899;&#38382;&#39064;&#12290;FLR&#36890;&#36807;&#29983;&#25104;&#26032;&#30340;&#20266;&#26631;&#31614;&#65292;&#23558;&#23616;&#37096;&#21644;&#20840;&#23616;&#27169;&#22411;&#39044;&#27979;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#20840;&#23616;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#26377;&#25928;&#23545;&#25239;&#22122;&#38899;&#26631;&#31614;&#30340;&#35760;&#24518;&#21270;&#12290;FLR&#19982;&#29616;&#26377;&#30340;&#26631;&#31614;&#22122;&#38899;&#21644;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#20860;&#23481;&#65292;&#20026;&#22788;&#29702;&#20805;&#28385;&#26631;&#31614;&#19981;&#20934;&#30830;&#24615;&#30340;FL&#29615;&#22659;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064; (FL) &#19981;&#26029;&#21457;&#23637;&#30340;&#29615;&#22659;&#20013;&#65292;&#30001;&#20110;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#20998;&#25955;&#21644;&#22810;&#26679;&#24615;&#65292;&#22788;&#29702;&#26631;&#31614;&#22122;&#38899;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#38598;&#20013;&#24335;&#23398;&#20064;&#26041;&#27861;&#22312;FL&#20013;&#21463;&#21040;&#38544;&#31169;&#38382;&#39064;&#21644;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#30340;&#38480;&#21046;&#65292;&#26080;&#27861;&#26377;&#25928;&#32531;&#35299;&#26631;&#31614;&#22122;&#38899;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#26089;&#26399;&#23398;&#20064;&#27491;&#21017;&#21270;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31574;&#30053;&#65292;&#21363;&#32852;&#37030;&#26631;&#31614;&#28151;&#21512;&#27491;&#21017;&#21270;&#65288;FLR&#65289;&#12290;FLR&#36890;&#36807;&#29983;&#25104;&#26032;&#30340;&#20266;&#26631;&#31614;&#65292;&#23558;&#23616;&#37096;&#21644;&#20840;&#23616;&#27169;&#22411;&#39044;&#27979;&#30456;&#32467;&#21512;&#65292;&#28789;&#27963;&#36866;&#24212;FL&#30340;&#22797;&#26434;&#24615;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#20840;&#23616;&#27169;&#22411;&#22312;i.i.d.&#21644;&#38750;i.i.d.&#35774;&#32622;&#19979;&#30340;&#20934;&#30830;&#24615;&#65292;&#36824;&#26377;&#25928;&#22320;&#25269;&#25239;&#20102;&#22122;&#38899;&#26631;&#31614;&#30340;&#35760;&#24518;&#21270;&#12290;FLR&#19982;&#29616;&#26377;&#30340;&#26631;&#31614;&#22122;&#38899;&#21644;FL&#25216;&#26415;&#30456;&#20860;&#23481;&#65292;&#20026;&#22312;&#20805;&#28385;&#26631;&#31614;&#19981;&#20934;&#30830;&#24615;&#30340;FL&#29615;&#22659;&#20013;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the evolving landscape of federated learning (FL), addressing label noise presents unique challenges due to the decentralized and diverse nature of data collection across clients. Traditional centralized learning approaches to mitigate label noise are constrained in FL by privacy concerns and the heterogeneity of client data. This paper revisits early-learning regularization, introducing an innovative strategy, Federated Label-mixture Regularization (FLR). FLR adeptly adapts to FL's complexities by generating new pseudo labels, blending local and global model predictions. This method not only enhances the accuracy of the global model in both i.i.d. and non-i.i.d. settings but also effectively counters the memorization of noisy labels. Demonstrating compatibility with existing label noise and FL techniques, FLR paves the way for improved generalization in FL environments fraught with label inaccuracies.
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#32570;&#20047;&#36890;&#29992;&#34892;&#20026;&#65292;&#38656;&#35201;&#21033;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#34920;&#31034;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#35748;&#30693;&#27867;&#21270;&#26694;&#26550;KIX&#65292;&#36890;&#36807;&#19982;&#23545;&#35937;&#30340;&#20132;&#20114;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20132;&#20114;&#27010;&#24565;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#20419;&#36827;&#20102;&#30693;&#35782;&#19982;&#24378;&#21270;&#23398;&#20064;&#30340;&#34701;&#21512;&#65292;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#33258;&#20027;&#21644;&#36890;&#29992;&#34892;&#20026;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05346</link><description>&lt;p&gt;
KIX: &#19968;&#31181;&#20803;&#35748;&#30693;&#27867;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
KIX: A Metacognitive Generalization Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05346
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#32570;&#20047;&#36890;&#29992;&#34892;&#20026;&#65292;&#38656;&#35201;&#21033;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#34920;&#31034;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#35748;&#30693;&#27867;&#21270;&#26694;&#26550;KIX&#65292;&#36890;&#36807;&#19982;&#23545;&#35937;&#30340;&#20132;&#20114;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20132;&#20114;&#27010;&#24565;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#20419;&#36827;&#20102;&#30693;&#35782;&#19982;&#24378;&#21270;&#23398;&#20064;&#30340;&#34701;&#21512;&#65292;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#33258;&#20027;&#21644;&#36890;&#29992;&#34892;&#20026;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21644;&#20854;&#20182;&#21160;&#29289;&#33021;&#22815;&#28789;&#27963;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#19988;&#33021;&#22815;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#21644;&#24212;&#29992;&#38271;&#26399;&#31215;&#32047;&#30340;&#39640;&#32423;&#30693;&#35782;&#26469;&#36866;&#24212;&#26032;&#39062;&#24773;&#22659;&#65292;&#36825;&#34920;&#29616;&#20102;&#19968;&#31181;&#27867;&#21270;&#26234;&#33021;&#34892;&#20026;&#12290;&#20294;&#26159;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26356;&#22810;&#22320;&#26159;&#19987;&#23478;&#65292;&#32570;&#20047;&#36825;&#31181;&#36890;&#29992;&#34892;&#20026;&#12290;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#38656;&#35201;&#29702;&#35299;&#21644;&#21033;&#29992;&#20851;&#38190;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#35748;&#30693;&#27867;&#21270;&#26694;&#26550;&#65292;&#31216;&#20026;Knowledge-Interaction-eXecution (KIX)&#65292;&#24182;&#19988;&#35748;&#20026;&#36890;&#36807;&#19982;&#23545;&#35937;&#30340;&#20132;&#20114;&#26469;&#21033;&#29992;&#31867;&#22411;&#31354;&#38388;&#21487;&#20197;&#20419;&#36827;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20132;&#20114;&#27010;&#24565;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#26159;&#23558;&#30693;&#35782;&#34701;&#20837;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#24335;&#65292;&#24182;&#26377;&#26395;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#23454;&#29616;&#33258;&#20027;&#21644;&#36890;&#29992;&#34892;&#20026;&#30340;&#25512;&#24191;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans and other animals aptly exhibit general intelligence behaviors in solving a variety of tasks with flexibility and ability to adapt to novel situations by reusing and applying high level knowledge acquired over time. But artificial agents are more of a specialist, lacking such generalist behaviors. Artificial agents will require understanding and exploiting critical structured knowledge representations. We present a metacognitive generalization framework, Knowledge-Interaction-eXecution (KIX), and argue that interactions with objects leveraging type space facilitate the learning of transferable interaction concepts and generalization. It is a natural way of integrating knowledge into reinforcement learning and promising to act as an enabler for autonomous and generalist behaviors in artificial intelligence systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24191;&#20041;&#26631;&#31614;&#36716;&#31227;&#21644;&#24178;&#25200;&#21442;&#25968;&#19979;&#36827;&#34892;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#20998;&#31867;&#38382;&#39064;&#35270;&#20026;&#24102;&#26377;&#24178;&#25200;&#21442;&#25968;&#30340;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#65292;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#20272;&#35745;&#20998;&#31867;&#22120;&#22312;&#25972;&#20010;&#24178;&#25200;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#25509;&#25910;&#32773;&#25805;&#20316;&#29305;&#24615;&#65292;&#24182;&#35774;&#35745;&#20986;&#22312;&#24191;&#20041;&#26631;&#31614;&#36716;&#31227;&#19979;&#19981;&#21464;&#30340;&#25130;&#26029;&#28857;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#20107;&#20214;&#30340;&#21487;&#38752;&#20998;&#31867;&#21644;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.05330</link><description>&lt;p&gt;
&#22312;&#26080;&#25311;&#26679;&#20284;&#25512;&#26029;&#30340;&#24178;&#25200;&#21442;&#25968;&#21644;&#24191;&#20041;&#26631;&#31614;&#36716;&#31227;&#19979;&#30340;&#20998;&#31867;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Classification under Nuisance Parameters and Generalized Label Shift in Likelihood-Free Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05330
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24191;&#20041;&#26631;&#31614;&#36716;&#31227;&#21644;&#24178;&#25200;&#21442;&#25968;&#19979;&#36827;&#34892;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#20998;&#31867;&#38382;&#39064;&#35270;&#20026;&#24102;&#26377;&#24178;&#25200;&#21442;&#25968;&#30340;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#65292;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#20272;&#35745;&#20998;&#31867;&#22120;&#22312;&#25972;&#20010;&#24178;&#25200;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#25509;&#25910;&#32773;&#25805;&#20316;&#29305;&#24615;&#65292;&#24182;&#35774;&#35745;&#20986;&#22312;&#24191;&#20041;&#26631;&#31614;&#36716;&#31227;&#19979;&#19981;&#21464;&#30340;&#25130;&#26029;&#28857;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#20107;&#20214;&#30340;&#21487;&#38752;&#20998;&#31867;&#21644;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#20043;&#38388;&#65292;&#26631;&#31614;&#21644;&#28508;&#22312;&#24178;&#25200;&#21442;&#25968;&#30340;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#20197;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#23545;&#20107;&#20214;&#36827;&#34892;&#20998;&#31867;&#26159;&#19968;&#20010;&#31185;&#23398;&#25361;&#25112;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#20998;&#24067;&#36716;&#31227;&#31216;&#20026;&#24191;&#20041;&#26631;&#31614;&#36716;&#31227; (GLS)&#12290;&#30452;&#25509;&#20351;&#29992;&#35266;&#27979;&#25968;&#25454; $\mathbf{X}$ &#36827;&#34892;&#20998;&#31867;&#20250;&#23548;&#33268;&#39044;&#27979;&#32467;&#26524;&#20559;&#24046;&#21644;&#26631;&#31614; $Y$ &#30340;&#26080;&#25928;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20998;&#31867;&#38382;&#39064;&#35270;&#20026;&#24102;&#26377;&#24178;&#25200;&#21442;&#25968;&#30340;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#26469;&#20811;&#26381;&#36825;&#20123;&#20559;&#24046;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#25972;&#20010;&#24178;&#25200;&#21442;&#25968;&#31354;&#38388;&#20013;&#20272;&#35745;&#20998;&#31867;&#22120;&#30340;&#25509;&#25910;&#32773;&#25805;&#20316;&#29305;&#24615; (ROC)&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35774;&#35745;&#22312; GLS &#19979;&#19981;&#21464;&#30340;&#25130;&#26029;&#28857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#36171;&#20104;&#39044;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#39046;&#22495;&#36866;&#24212;&#33021;&#21147;&#65292;&#24182;&#36820;&#22238;&#26377;&#25928;&#30340;&#39044;&#27979;&#38598;&#21512;&#65292;&#21516;&#26102;&#20445;&#25345;&#26377;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
An open scientific challenge is how to classify events with reliable measures of uncertainty, when we have a mechanistic model of the data-generating process but the distribution over both labels and latent nuisance parameters is different between train and target data. We refer to this type of distributional shift as generalized label shift (GLS). Direct classification using observed data $\mathbf{X}$ as covariates leads to biased predictions and invalid uncertainty estimates of labels $Y$. We overcome these biases by proposing a new method for robust uncertainty quantification that casts classification as a hypothesis testing problem under nuisance parameters. The key idea is to estimate the classifier's receiver operating characteristic (ROC) across the entire nuisance parameter space, which allows us to devise cutoffs that are invariant under GLS. Our method effectively endows a pre-trained classifier with domain adaptation capabilities and returns valid prediction sets while maint
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#23545;&#22810;&#27169;&#24577;&#22270;&#23398;&#20064;&#30340;&#24050;&#26377;&#24037;&#20316;&#36827;&#34892;&#20102;&#23545;&#27604;&#20998;&#26512;&#65292;&#38416;&#26126;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#26041;&#24335;&#21644;&#20027;&#27969;&#25216;&#26415;&#29305;&#28857;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#37325;&#35201;&#24212;&#29992;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.05322</link><description>&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#22270;&#19978;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Learning on Multimodal Graphs: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05322
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#23545;&#22810;&#27169;&#24577;&#22270;&#23398;&#20064;&#30340;&#24050;&#26377;&#24037;&#20316;&#36827;&#34892;&#20102;&#23545;&#27604;&#20998;&#26512;&#65292;&#38416;&#26126;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#26041;&#24335;&#21644;&#20027;&#27969;&#25216;&#26415;&#29305;&#28857;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#37325;&#35201;&#24212;&#29992;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25968;&#25454;&#24191;&#27867;&#23384;&#22312;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#12289;&#31038;&#20132;&#23186;&#20307;&#21644;&#20132;&#36890;&#36816;&#36755;&#31561;&#65292;&#20854;&#20013;&#22810;&#27169;&#24577;&#22270;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26426;&#22120;&#23398;&#20064;&#22312;&#22810;&#27169;&#24577;&#22270;&#19978;&#30340;&#24212;&#29992;&#65292;&#34987;&#31216;&#20026;&#22810;&#27169;&#24577;&#22270;&#23398;&#20064; (MGL)&#65292;&#23545;&#20110;&#25104;&#21151;&#30340;&#20154;&#24037;&#26234;&#33021; (AI) &#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31687;&#32508;&#36848;&#25991;&#31456;&#23545;&#22810;&#27169;&#24577;&#22270;&#23398;&#20064;&#20013;&#24050;&#26377;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#23545;&#27604;&#20998;&#26512;&#65292;&#38416;&#26126;&#20102;&#22312;&#19981;&#21516;&#22270;&#31867;&#22411;&#19978;&#23454;&#29616;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#24182;&#25506;&#32034;&#20102;&#20027;&#27969;&#23398;&#20064;&#25216;&#26415;&#30340;&#29305;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25581;&#31034;&#20102;&#22810;&#27169;&#24577;&#22270;&#23398;&#20064;&#30340;&#37325;&#35201;&#24212;&#29992;&#65292;&#24182;&#23545;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#26041;&#21521;&#25552;&#20379;&#20102;&#28145;&#20837;&#35265;&#35299;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20026;&#24076;&#26395;&#20102;&#35299;&#29616;&#26377; MGL &#25216;&#26415;&#21450;&#20854;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#36866;&#29992;&#24615;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#22522;&#30784;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal data pervades various domains, including healthcare, social media, and transportation, where multimodal graphs play a pivotal role. Machine learning on multimodal graphs, referred to as multimodal graph learning (MGL), is essential for successful artificial intelligence (AI) applications. The burgeoning research in this field encompasses diverse graph data types and modalities, learning techniques, and application scenarios. This survey paper conducts a comparative analysis of existing works in multimodal graph learning, elucidating how multimodal learning is achieved across different graph types and exploring the characteristics of prevalent learning techniques. Additionally, we delineate significant applications of multimodal graph learning and offer insights into future directions in this domain. Consequently, this paper serves as a foundational resource for researchers seeking to comprehend existing MGL techniques and their applicability across diverse scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#30340;&#28436;&#21464;&#65292;&#29305;&#21035;&#20851;&#27880;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20256;&#32479;&#25628;&#32034;&#26041;&#27861;&#19982;&#26032;&#20852;&#31572;&#26696;&#26816;&#32034;&#33539;&#24335;&#20043;&#38388;&#30340;&#26725;&#26753;&#20316;&#29992;&#65292;LLMs&#30340;&#25972;&#21512;&#26631;&#24535;&#30528;&#29992;&#25143;&#19982;&#20449;&#24687;&#31995;&#32479;&#20132;&#20114;&#26041;&#24335;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2402.05318</link><description>&lt;p&gt;
&#33322;&#34892;&#30693;&#35782;&#20043;&#28023;&#65306;&#21033;&#29992;LLMs&#36827;&#34892;&#34892;&#26143;&#32423;&#31572;&#26696;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Navigating the Knowledge Sea: Planet-scale answer retrieval using LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#30340;&#28436;&#21464;&#65292;&#29305;&#21035;&#20851;&#27880;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20256;&#32479;&#25628;&#32034;&#26041;&#27861;&#19982;&#26032;&#20852;&#31572;&#26696;&#26816;&#32034;&#33539;&#24335;&#20043;&#38388;&#30340;&#26725;&#26753;&#20316;&#29992;&#65292;LLMs&#30340;&#25972;&#21512;&#26631;&#24535;&#30528;&#29992;&#25143;&#19982;&#20449;&#24687;&#31995;&#32479;&#20132;&#20114;&#26041;&#24335;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#65292;&#20854;&#29305;&#28857;&#26159;&#20174;&#22522;&#26412;&#30340;&#36229;&#38142;&#25509;&#23548;&#33322;&#21040;&#22797;&#26434;&#30340;&#31639;&#27861;&#39537;&#21160;&#25628;&#32034;&#24341;&#25806;&#30340;&#19981;&#26029;&#25913;&#36827;&#12290;&#26412;&#25991;&#26088;&#22312;&#20840;&#38754;&#20171;&#32461;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#30340;&#28436;&#21464;&#65292;&#29305;&#21035;&#20851;&#27880;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20256;&#32479;&#25628;&#32034;&#26041;&#27861;&#19982;&#26032;&#20852;&#31572;&#26696;&#26816;&#32034;&#33539;&#24335;&#20043;&#38388;&#30340;&#26725;&#26753;&#20316;&#29992;&#12290;LLMs&#22312;&#21709;&#24212;&#26816;&#32034;&#21644;&#32034;&#24341;&#39046;&#22495;&#30340;&#25972;&#21512;&#26631;&#24535;&#30528;&#29992;&#25143;&#19982;&#20449;&#24687;&#31995;&#32479;&#20132;&#20114;&#26041;&#24335;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#36825;&#31181;&#33539;&#24335;&#36716;&#21464;&#26159;&#30001;&#20687;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#21512;&#39537;&#21160;&#30340;&#65292;&#23427;&#20204;&#33021;&#22815;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#25991;&#26412;&#65292;&#20174;&#32780;&#33021;&#22815;&#25552;&#20379;&#26356;&#30452;&#25509;&#21644;&#24773;&#22659;&#30456;&#20851;&#30340;&#31572;&#26696;&#32473;&#29992;&#25143;&#26597;&#35810;&#12290;&#36890;&#36807;&#36825;&#31181;&#25506;&#32034;&#65292;&#25105;&#20204;&#35797;&#22270;&#38416;&#26126;&#25216;&#26415;&#37324;&#31243;&#30865;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information retrieval is a rapidly evolving field of information retrieval, which is characterized by a continuous refinement of techniques and technologies, from basic hyperlink-based navigation to sophisticated algorithm-driven search engines. This paper aims to provide a comprehensive overview of the evolution of Information Retrieval Technology, with a particular focus on the role of Large Language Models (LLMs) in bridging the gap between traditional search methods and the emerging paradigm of answer retrieval. The integration of LLMs in the realms of response retrieval and indexing signifies a paradigm shift in how users interact with information systems. This paradigm shift is driven by the integration of large language models (LLMs) like GPT-4, which are capable of understanding and generating human-like text, thus enabling them to provide more direct and contextually relevant answers to user queries. Through this exploration, we seek to illuminate the technological milestones 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#20102;&#29983;&#25104;&#27969;&#32593;&#32476;(GFlowNets)&#30340;&#19968;&#20123;&#27867;&#21270;&#26426;&#21046;&#20551;&#35774;&#65292;&#21457;&#29616;&#23427;&#20204;&#23398;&#20064;&#36924;&#36817;&#30340;&#20989;&#25968;&#20855;&#26377;&#38544;&#21547;&#30340;&#22522;&#30784;&#32467;&#26500;&#65292;&#26377;&#21161;&#20110;&#27867;&#21270;&#12290;&#21516;&#26102;&#65292;GFlowNets&#23545;&#20110;&#31163;&#32447;&#21644;&#31163;&#31574;&#30053;&#35757;&#32451;&#25935;&#24863;&#65292;&#20294;&#38544;&#21547;&#23398;&#20064;&#30340;&#22870;&#21169;&#23545;&#35757;&#32451;&#20998;&#24067;&#30340;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05309</link><description>&lt;p&gt;
&#30740;&#31350;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;&#27867;&#21270;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Investigating Generalization Behaviours of Generative Flow Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#20102;&#29983;&#25104;&#27969;&#32593;&#32476;(GFlowNets)&#30340;&#19968;&#20123;&#27867;&#21270;&#26426;&#21046;&#20551;&#35774;&#65292;&#21457;&#29616;&#23427;&#20204;&#23398;&#20064;&#36924;&#36817;&#30340;&#20989;&#25968;&#20855;&#26377;&#38544;&#21547;&#30340;&#22522;&#30784;&#32467;&#26500;&#65292;&#26377;&#21161;&#20110;&#27867;&#21270;&#12290;&#21516;&#26102;&#65292;GFlowNets&#23545;&#20110;&#31163;&#32447;&#21644;&#31163;&#31574;&#30053;&#35757;&#32451;&#25935;&#24863;&#65292;&#20294;&#38544;&#21547;&#23398;&#20064;&#30340;&#22870;&#21169;&#23545;&#35757;&#32451;&#20998;&#24067;&#30340;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65292;GFNs&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#31163;&#25955;&#31354;&#38388;&#19978;&#38750;&#24402;&#19968;&#21270;&#27010;&#29575;&#36136;&#37327;&#20989;&#25968;&#30340;&#29983;&#25104;&#26694;&#26550;&#12290;&#33258;&#20174;&#23427;&#20204;&#38382;&#19990;&#20197;&#26469;&#65292;GFlowNets&#22312;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#35757;&#32451;&#26399;&#38388;&#22823;&#37096;&#20998;&#31163;&#25955;&#31354;&#38388;&#26410;&#34987;&#35775;&#38382;&#30340;&#24212;&#29992;&#12290;&#36825;&#20351;&#19968;&#20123;&#20154;&#20551;&#35774;&#24403;GFlowNets&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#37197;&#23545;&#26102;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#20102;GFlowNets&#30340;&#19968;&#20123;&#27867;&#21270;&#26426;&#21046;&#20551;&#35774;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21457;&#29616;GFlowNets&#23398;&#20064;&#36924;&#36817;&#30340;&#20989;&#25968;&#20855;&#26377;&#38544;&#21547;&#30340;&#22522;&#30784;&#32467;&#26500;&#65292;&#26377;&#21161;&#20110;&#27867;&#21270;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;GFlowNets&#23545;&#20110;&#31163;&#32447;&#21644;&#31163;&#31574;&#30053;&#35757;&#32451;&#24456;&#25935;&#24863;&#65292;&#28982;&#32780;&#65292;GFlowNets&#38544;&#21547;&#23398;&#20064;&#30340;&#22870;&#21169;&#23545;&#35757;&#32451;&#20998;&#24067;&#30340;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Flow Networks (GFlowNets, GFNs) are a generative framework for learning unnormalized probability mass functions over discrete spaces. Since their inception, GFlowNets have proven to be useful for learning generative models in applications where the majority of the discrete space is unvisited during training. This has inspired some to hypothesize that GFlowNets, when paired with deep neural networks (DNNs), have favourable generalization properties. In this work, we empirically verify some of the hypothesized mechanisms of generalization of GFlowNets. In particular, we find that the functions that GFlowNets learn to approximate have an implicit underlying structure which facilitate generalization. We also find that GFlowNets are sensitive to being trained offline and off-policy; however, the reward implicitly learned by GFlowNets is robust to changes in the training distribution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23454;&#29616;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#27169;&#22411;&#21644;&#31574;&#30053;&#30340;&#31070;&#32463;&#31526;&#21495;&#24378;&#21270;&#23398;&#20064;&#30340;&#19977;&#20010;&#36335;&#24452;&#65292;&#24182;&#25581;&#31034;&#20102;&#23398;&#20064;&#30340;&#36830;&#32493;&#24615;&#21644;&#21487;&#24494;&#24615;&#30340;&#30410;&#22788;&#65292;&#20197;&#21450;&#23558;&#36923;&#36753;&#19982;&#25968;&#20540;&#20223;&#30495;&#32467;&#21512;&#30340;&#38590;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.05307</link><description>&lt;p&gt;
&#20855;&#26377;&#21487;&#35299;&#37322;&#27169;&#22411;&#21644;&#31574;&#30053;&#32593;&#32476;&#30340;&#31070;&#32463;&#31526;&#21495;&#24378;&#21270;&#23398;&#20064;&#30340;&#19977;&#20010;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Three Pathways to Neurosymbolic Reinforcement Learning with Interpretable Model and Policy Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23454;&#29616;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#27169;&#22411;&#21644;&#31574;&#30053;&#30340;&#31070;&#32463;&#31526;&#21495;&#24378;&#21270;&#23398;&#20064;&#30340;&#19977;&#20010;&#36335;&#24452;&#65292;&#24182;&#25581;&#31034;&#20102;&#23398;&#20064;&#30340;&#36830;&#32493;&#24615;&#21644;&#21487;&#24494;&#24615;&#30340;&#30410;&#22788;&#65292;&#20197;&#21450;&#23558;&#36923;&#36753;&#19982;&#25968;&#20540;&#20223;&#30495;&#32467;&#21512;&#30340;&#38590;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#23558;&#32463;&#20856;&#31526;&#21495;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#12289;&#31616;&#32422;&#24615;&#21644;&#26174;&#24335;&#25512;&#29702;&#19982;&#25968;&#25454;&#39537;&#21160;&#30340;&#31070;&#32463;&#26041;&#27861;&#30340;&#32479;&#35745;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#21516;&#26102;&#21487;&#24494;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#21644;&#31574;&#30053;&#21487;&#33021;&#26159;&#36825;&#31181;&#32467;&#21512;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#23454;&#29616;&#36825;&#20123;&#27169;&#22411;&#21644;&#31574;&#30053;&#30340;&#19977;&#31181;&#36335;&#24452;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#24191;&#27867;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#20123;&#32593;&#32476;&#22312;&#20854;&#26550;&#26500;&#20013;&#30452;&#25509;&#26500;&#24314;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#12290;&#25105;&#20204;&#25581;&#31034;&#21644;&#24378;&#35843;&#20102;&#23558;&#36923;&#36753;&#12289;&#20223;&#30495;&#21644;&#23398;&#20064;&#32467;&#21512;&#36215;&#26469;&#30340;&#28508;&#21147;&#21644;&#22522;&#26412;&#22256;&#38590;&#12290;&#19968;&#20010;&#25945;&#35757;&#26159;&#23398;&#20064;&#21463;&#30410;&#20110;&#36830;&#32493;&#24615;&#21644;&#21487;&#24494;&#24615;&#65292;&#20294;&#32463;&#20856;&#36923;&#36753;&#26159;&#31163;&#25955;&#19988;&#19981;&#21487;&#24494;&#30340;&#12290;&#23558;&#36923;&#36753;&#26494;&#24347;&#20026;&#23454;&#20540;&#30340;&#21487;&#24494;&#34920;&#31034;&#23384;&#22312;&#19968;&#20010;&#26435;&#34913;&#65307;&#36234;&#21487;&#23398;&#20064;&#65292;&#36234;&#19981;&#21487;&#35299;&#37322;&#12290;&#21478;&#19968;&#20010;&#25945;&#35757;&#26159;&#22312;&#25968;&#20540;&#20223;&#30495;&#29615;&#22659;&#20013;&#20351;&#29992;&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
Neurosymbolic AI combines the interpretability, parsimony, and explicit reasoning of classical symbolic approaches with the statistical learning of data-driven neural approaches. Models and policies that are simultaneously differentiable and interpretable may be key enablers of this marriage. This paper demonstrates three pathways to implementing such models and policies in a real-world reinforcement learning setting. Specifically, we study a broad class of neural networks that build interpretable semantics directly into their architecture. We reveal and highlight both the potential and the essential difficulties of combining logic, simulation, and learning. One lesson is that learning benefits from continuity and differentiability, but classical logic is discrete and non-differentiable. The relaxation to real-valued, differentiable representations presents a trade-off; the more learnable, the less interpretable. Another lesson is that using logic in the context of a numerical simulati
&lt;/p&gt;</description></item><item><title>Sym-Q&#26159;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31526;&#21495;&#22238;&#24402;&#37325;&#26032;&#23450;&#20041;&#20026;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#26469;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#27867;&#21270;&#24615;&#21644;&#36866;&#24212;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#21033;&#29992;&#30417;&#30563;&#28436;&#31034;&#21644;&#22870;&#21169;&#20449;&#21495;&#65292;Sym-Q&#33021;&#22815;&#26681;&#25454;&#25311;&#21512;&#31934;&#24230;&#30340;&#36136;&#37327;&#25913;&#36827;&#34920;&#36798;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.05306</link><description>&lt;p&gt;
Sym-Q&#65306;&#36890;&#36807;&#39034;&#24207;&#20915;&#31574;&#36827;&#34892;&#33258;&#36866;&#24212;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Sym-Q: Adaptive Symbolic Regression via Sequential Decision-Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05306
&lt;/p&gt;
&lt;p&gt;
Sym-Q&#26159;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31526;&#21495;&#22238;&#24402;&#37325;&#26032;&#23450;&#20041;&#20026;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#26469;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#27867;&#21270;&#24615;&#21644;&#36866;&#24212;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#21033;&#29992;&#30417;&#30563;&#28436;&#31034;&#21644;&#22870;&#21169;&#20449;&#21495;&#65292;Sym-Q&#33021;&#22815;&#26681;&#25454;&#25311;&#21512;&#31934;&#24230;&#30340;&#36136;&#37327;&#25913;&#36827;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#20855;&#26377;&#20174;&#23454;&#35777;&#25968;&#25454;&#20013;&#25581;&#31034;&#28508;&#22312;&#25968;&#23398;&#21644;&#29289;&#29702;&#20851;&#31995;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#27867;&#21270;&#24615;&#21644;&#36866;&#24212;&#24615;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#36890;&#24120;&#65292;&#24403;&#36755;&#20986;&#34920;&#36798;&#24335;&#19981;&#36275;&#20197;&#36866;&#24212;&#23454;&#39564;&#25968;&#25454;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#26377;&#25928;&#30340;&#26426;&#21046;&#26469;&#36866;&#24212;&#25110;&#20462;&#25913;&#34920;&#36798;&#24335;&#12290;&#36825;&#31181;&#32570;&#20047;&#28789;&#27963;&#24615;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#21457;&#29616;&#26410;&#30693;&#30340;&#29289;&#29702;&#25110;&#29983;&#29289;&#20851;&#31995;&#26041;&#38754;&#12290;&#21463;&#21040;&#20154;&#31867;&#19987;&#23478;&#22914;&#20309;&#25913;&#36827;&#21644;&#35843;&#25972;&#34920;&#36798;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22411;Symbolic Q-network&#65288;Sym-Q&#65289;&#65292;&#23558;&#31526;&#21495;&#22238;&#24402;&#37325;&#26032;&#23450;&#20041;&#20026;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#12290;Sym-Q&#21033;&#29992;&#30417;&#30563;&#28436;&#31034;&#24182;&#26681;&#25454;&#22870;&#21169;&#20449;&#21495;&#26469;&#25913;&#36827;&#34920;&#36798;&#24335;&#65292;&#22870;&#21169;&#20449;&#21495;&#25351;&#31034;&#25311;&#21512;&#31934;&#24230;&#30340;&#36136;&#37327;&#12290;&#23427;&#29420;&#29305;&#30340;&#33021;&#21147;&#21487;&#20197;&#22788;&#29702;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression holds great potential for uncovering underlying mathematical and physical relationships from empirical data. While existing transformer-based models have recently achieved significant success in this domain, they face challenges in terms of generalizability and adaptability. Typically, in cases where the output expressions do not adequately fit experimental data, the models lack efficient mechanisms to adapt or modify the expression. This inflexibility hinders their application in real-world scenarios, particularly in discovering unknown physical or biological relationships. Inspired by how human experts refine and adapt expressions, we introduce Symbolic Q-network (Sym-Q), a novel reinforcement learning-based model that redefines symbolic regression as a sequential decision-making task. Sym-Q leverages supervised demonstrations and refines expressions based on reward signals indicating the quality of fitting precision. Its distinctive ability to manage the complexi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;BIKED++&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;140&#19975;&#20010;&#33258;&#34892;&#36710;&#35774;&#35745;&#30340;&#22270;&#20687;&#21644;&#21442;&#25968;&#21270;CAD&#25991;&#20214;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#36328;&#27169;&#24577;&#39044;&#27979;&#27169;&#22411;&#65292;&#20363;&#22914;&#20351;&#29992;&#21442;&#25968;&#21270;&#34920;&#31034;&#26469;&#20934;&#30830;&#20272;&#35745;&#22270;&#20687;&#30340;&#29305;&#24449;&#23884;&#20837;&#12290;&#35813;&#25968;&#25454;&#38598;&#20063;&#24050;&#20844;&#24320;&#65292;&#21487;&#20379;&#30740;&#31350;&#32773;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.05301</link><description>&lt;p&gt;
BIKED++&#65306;&#19968;&#20010;&#21253;&#21547;140&#19975;&#20010;&#33258;&#34892;&#36710;&#22270;&#20687;&#21644;&#21442;&#25968;&#21270;CAD&#35774;&#35745;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BIKED++: A Multimodal Dataset of 1.4 Million Bicycle Image and Parametric CAD Designs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BIKED++&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;140&#19975;&#20010;&#33258;&#34892;&#36710;&#35774;&#35745;&#30340;&#22270;&#20687;&#21644;&#21442;&#25968;&#21270;CAD&#25991;&#20214;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#36328;&#27169;&#24577;&#39044;&#27979;&#27169;&#22411;&#65292;&#20363;&#22914;&#20351;&#29992;&#21442;&#25968;&#21270;&#34920;&#31034;&#26469;&#20934;&#30830;&#20272;&#35745;&#22270;&#20687;&#30340;&#29305;&#24449;&#23884;&#20837;&#12290;&#35813;&#25968;&#25454;&#38598;&#20063;&#24050;&#20844;&#24320;&#65292;&#21487;&#20379;&#30740;&#31350;&#32773;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;140&#19975;&#20010;&#36890;&#36807;&#21442;&#25968;&#21270;&#34920;&#31034;&#21644;JSON&#25991;&#20214;&#20197;&#21450;&#26629;&#26684;&#21270;&#22270;&#20687;&#29983;&#25104;&#30340;&#33258;&#34892;&#36710;&#35774;&#35745;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#28210;&#26579;&#24341;&#25806;&#21644;BikeCAD&#36719;&#20214;&#29983;&#25104;&#21442;&#25968;&#21270;&#35774;&#35745;&#30340;&#30690;&#37327;&#22270;&#24418;&#32780;&#21019;&#24314;&#30340;&#12290;&#26412;&#25991;&#36824;&#20844;&#24320;&#20102;&#35813;&#28210;&#26579;&#24341;&#25806;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#22810;&#31181;&#24212;&#29992;&#65292;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#30446;&#26631;&#26159;&#35757;&#32451;&#21442;&#25968;&#21270;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#35774;&#35745;&#34920;&#31034;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#39044;&#27979;&#27169;&#22411;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#30452;&#25509;&#20174;&#21442;&#25968;&#21270;&#34920;&#31034;&#20934;&#30830;&#20272;&#35745;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#23884;&#20837;&#12290;&#36825;&#26679;&#21487;&#20197;&#24314;&#31435;&#21442;&#25968;&#21270;&#33258;&#34892;&#36710;&#35774;&#35745;&#19982;&#25991;&#26412;&#23383;&#31526;&#20018;&#25110;&#21442;&#32771;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#20851;&#31995;&#12290;&#32463;&#36807;&#35757;&#32451;&#30340;&#39044;&#27979;&#27169;&#22411;&#20063;&#24050;&#20844;&#24320;&#12290;&#35813;&#25968;&#25454;&#38598;&#21152;&#20837;&#20102;BIKED&#25968;&#25454;&#38598;&#31995;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a public dataset of 1.4 million procedurally-generated bicycle designs represented parametrically, as JSON files, and as rasterized images. The dataset is created through the use of a rendering engine which harnesses the BikeCAD software to generate vector graphics from parametric designs. This rendering engine is discussed in the paper and also released publicly alongside the dataset. Though this dataset has numerous applications, a principal motivation is the need to train cross-modal predictive models between parametric and image-based design representations. For example, we demonstrate that a predictive model can be trained to accurately estimate Contrastive Language-Image Pretraining (CLIP) embeddings from a parametric representation directly. This allows similarity relations to be established between parametric bicycle designs and text strings or reference images. Trained predictive models are also made public. The dataset joins the BIKED dataset family whic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20957;&#32858;&#23618;&#27425;&#32858;&#31867;&#21644;&#22522;&#20110;&#20027;&#39064;&#30340;&#26041;&#27861;&#23545;&#22403;&#22334;&#37038;&#20214;&#36827;&#34892;&#20998;&#31867;&#30340;&#26032;&#24605;&#36335;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SPEMC-15K-E&#21644;SPEMC-15K-S&#65292;&#24182;&#20351;&#29992;&#20957;&#32858;&#23618;&#27425;&#32858;&#31867;&#23558;&#20854;&#21010;&#20998;&#20026;11&#20010;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TF-IDF&#21644;LR&#22312;&#33521;&#25991;&#25968;&#25454;&#38598;&#20013;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.05296</link><description>&lt;p&gt;
&#20351;&#29992;&#20957;&#32858;&#23618;&#27425;&#32858;&#31867;&#21644;&#22522;&#20110;&#20027;&#39064;&#30340;&#26041;&#27861;&#23545;&#22403;&#22334;&#37038;&#20214;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classifying spam emails using agglomerative hierarchical clustering and a topic-based approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05296
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20957;&#32858;&#23618;&#27425;&#32858;&#31867;&#21644;&#22522;&#20110;&#20027;&#39064;&#30340;&#26041;&#27861;&#23545;&#22403;&#22334;&#37038;&#20214;&#36827;&#34892;&#20998;&#31867;&#30340;&#26032;&#24605;&#36335;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SPEMC-15K-E&#21644;SPEMC-15K-S&#65292;&#24182;&#20351;&#29992;&#20957;&#32858;&#23618;&#27425;&#32858;&#31867;&#23558;&#20854;&#21010;&#20998;&#20026;11&#20010;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TF-IDF&#21644;LR&#22312;&#33521;&#25991;&#25968;&#25454;&#38598;&#20013;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22403;&#22334;&#37038;&#20214;&#26159;&#26410;&#32463;&#35831;&#27714;&#30340;&#24700;&#20154;&#19988;&#26377;&#26102;&#26377;&#23475;&#30340;&#28040;&#24687;&#65292;&#21487;&#33021;&#21547;&#26377;&#24694;&#24847;&#36719;&#20214;&#12289;&#38035;&#40060;&#25110;&#24694;&#20316;&#21095;&#12290;&#19982;&#22823;&#22810;&#25968;&#30740;&#31350;&#35299;&#20915;&#39640;&#25928;&#21453;&#22403;&#22334;&#37038;&#20214;&#36807;&#28388;&#22120;&#35774;&#35745;&#30340;&#38382;&#39064;&#19981;&#21516;&#65292;&#25105;&#20204;&#20174;&#19981;&#21516;&#21644;&#26032;&#39062;&#30340;&#35282;&#24230;&#26469;&#35299;&#20915;&#22403;&#22334;&#37038;&#20214;&#38382;&#39064;&#12290;&#37325;&#28857;&#20851;&#27880;&#32593;&#32476;&#23433;&#20840;&#21333;&#20301;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#20027;&#39064;&#30340;&#26041;&#27861;&#26469;&#23545;&#22403;&#22334;&#37038;&#20214;&#36827;&#34892;&#22810;&#31867;&#21035;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;SPEMC-15K-E&#21644;SPEMC-15K-S&#65292;&#20998;&#21035;&#21253;&#21547;&#22823;&#32422;15K&#23553;&#33521;&#25991;&#21644;&#35199;&#29677;&#29273;&#25991;&#37038;&#20214;&#65292;&#24182;&#20351;&#29992;&#20957;&#32858;&#23618;&#27425;&#32858;&#31867;&#23558;&#20854;&#21010;&#20998;&#20026;11&#20010;&#31867;&#21035;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;16&#31181;&#27969;&#27700;&#32447;&#65292;&#32467;&#21512;&#20102;&#22235;&#31181;&#25991;&#26412;&#34920;&#31034;&#25216;&#26415;-&#35789;&#39057;-&#36870;&#25991;&#26723;&#39057;&#29575;&#65288;TF-IDF&#65289;&#12289;&#35789;&#34955;&#27169;&#22411;&#12289;Word2Vec&#21644;BERT-&#20197;&#21450;&#22235;&#31181;&#20998;&#31867;&#22120;&#65306;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;&#26420;&#32032;&#36125;&#21494;&#26031;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;&#36923;&#36753;&#22238;&#24402;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#33521;&#25991;&#25968;&#25454;&#38598;&#20013;&#65292;TF-IDF&#21644;LR&#30340;&#24615;&#33021;&#26368;&#22909;&#65292;
&lt;/p&gt;
&lt;p&gt;
Spam emails are unsolicited, annoying and sometimes harmful messages which may contain malware, phishing or hoaxes. Unlike most studies that address the design of efficient anti-spam filters, we approach the spam email problem from a different and novel perspective. Focusing on the needs of cybersecurity units, we follow a topic-based approach for addressing the classification of spam email into multiple categories. We propose SPEMC-15K-E and SPEMC-15K-S, two novel datasets with approximately 15K emails each in English and Spanish, respectively, and we label them using agglomerative hierarchical clustering into 11 classes. We evaluate 16 pipelines, combining four text representation techniques -Term Frequency-Inverse Document Frequency (TF-IDF), Bag of Words, Word2Vec and BERT- and four classifiers: Support Vector Machine, N\"aive Bayes, Random Forest and Logistic Regression. Experimental results show that the highest performance is achieved with TF-IDF and LR for the English dataset, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#29305;&#24449;&#36873;&#25321;&#21644;&#25490;&#24207;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#35780;&#20272;&#19981;&#21516;&#31639;&#27861;&#32467;&#26524;&#20013;&#30340;&#29305;&#24449;&#25490;&#24207;&#30340;&#31283;&#23450;&#24615;&#65292;&#21253;&#25324;&#23436;&#25972;&#30340;&#25490;&#21517;&#21015;&#34920;&#12289;&#29305;&#24449;&#23376;&#38598;&#21644;&#37096;&#20998;&#25490;&#21517;&#21015;&#34920;&#12290;</title><link>https://arxiv.org/abs/2402.05295</link><description>&lt;p&gt;
&#19968;&#31181;&#37327;&#21270;&#29305;&#24449;&#36873;&#25321;&#21644;&#25490;&#24207;&#31639;&#27861;&#31283;&#23450;&#24615;&#30340;&#20449;&#24687;&#35770;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An information theoretic approach to quantify the stability of feature selection and ranking algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#29305;&#24449;&#36873;&#25321;&#21644;&#25490;&#24207;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#35780;&#20272;&#19981;&#21516;&#31639;&#27861;&#32467;&#26524;&#20013;&#30340;&#29305;&#24449;&#25490;&#24207;&#30340;&#31283;&#23450;&#24615;&#65292;&#21253;&#25324;&#23436;&#25972;&#30340;&#25490;&#21517;&#21015;&#34920;&#12289;&#29305;&#24449;&#23376;&#38598;&#21644;&#37096;&#20998;&#25490;&#21517;&#21015;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#26159;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#26102;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#29305;&#21035;&#26159;&#36825;&#20123;&#25216;&#26415;&#36890;&#36807;&#20174;&#22024;&#26434;&#12289;&#20887;&#20313;&#21644;&#26080;&#20851;&#30340;&#29305;&#24449;&#20013;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#31616;&#21270;&#20102;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#30693;&#35782;&#30340;&#36807;&#31243;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#20986;&#29616;&#30340;&#19968;&#20010;&#38382;&#39064;&#26159;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#30340;&#32467;&#26524;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#30340;&#24494;&#23567;&#21464;&#21270;&#21487;&#33021;&#23548;&#33268;&#38750;&#24120;&#19981;&#21516;&#30340;&#29305;&#24449;&#25490;&#24207;&#12290;&#22312;&#19978;&#36848;&#24773;&#20917;&#20013;&#65292;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#30340;&#31283;&#23450;&#24615;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Jensen Shannon&#36317;&#31163;&#30340;&#20449;&#24687;&#35770;&#26041;&#27861;&#26469;&#37327;&#21270;&#36825;&#31181;&#40065;&#26834;&#24615;&#12290;&#19982;&#20854;&#20182;&#31283;&#23450;&#24615;&#24230;&#37327;&#19981;&#21516;&#65292;&#36825;&#20010;&#24230;&#37327;&#25351;&#26631;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#31639;&#27861;&#32467;&#26524;&#65306;&#23436;&#25972;&#30340;&#25490;&#21517;&#21015;&#34920;&#12289;&#29305;&#24449;&#23376;&#38598;&#20197;&#21450;&#36739;&#23569;&#30740;&#31350;&#30340;&#37096;&#20998;&#25490;&#21517;&#21015;&#34920;&#12290;&#36825;&#20010;&#24191;&#20041;&#24230;&#37327;&#20197;&#27010;&#29575;&#26041;&#27861;&#37327;&#21270;&#20102;&#30456;&#21516;&#22823;&#23567;&#30340;&#25972;&#20010;&#21015;&#34920;&#38598;&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20379;&#20102;&#21453;&#26144;&#20854;&#20013;&#30340;&#25490;&#24207;&#31283;&#23450;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature selection is a key step when dealing with high dimensional data. In particular, these techniques simplify the process of knowledge discovery from the data by selecting the most relevant features out of the noisy, redundant and irrelevant features. A problem that arises in many of these practical applications is that the outcome of the feature selection algorithm is not stable. Thus, small variations in the data may yield very different feature rankings. Assessing the stability of these methods becomes an important issue in the previously mentioned situations. We propose an information theoretic approach based on the Jensen Shannon divergence to quantify this robustness. Unlike other stability measures, this metric is suitable for different algorithm outcomes: full ranked lists, feature subsets as well as the lesser studied partial ranked lists. This generalized metric quantifies the difference among a whole set of lists with the same size, following a probabilistic approach and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#20998;&#26512;&#20102;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#24577;&#19981;&#19968;&#33268;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#19982;&#21442;&#19982;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#32852;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#32771;&#34385;&#19981;&#19968;&#33268;&#24615;&#30340;&#20449;&#24687;&#34701;&#21512;&#26426;&#21046;&#21644;&#27169;&#24577;&#25554;&#20540;&#32593;&#32476;&#65292;&#22312;&#35299;&#20915;&#27169;&#24577;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#25104;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05294</link><description>&lt;p&gt;
&#26816;&#39564;&#21307;&#30103;&#35270;&#35273;&#21644;&#22522;&#20110;&#35821;&#35328;&#30340;&#30142;&#30149;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#24577;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Examining Modality Incongruity in Multimodal Federated Learning for Medical Vision and Language-based Disease Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20998;&#26512;&#20102;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#24577;&#19981;&#19968;&#33268;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#19982;&#21442;&#19982;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#32852;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#32771;&#34385;&#19981;&#19968;&#33268;&#24615;&#30340;&#20449;&#24687;&#34701;&#21512;&#26426;&#21046;&#21644;&#27169;&#24577;&#25554;&#20540;&#32593;&#32476;&#65292;&#22312;&#35299;&#20915;&#27169;&#24577;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#65288;MMFL&#65289;&#21033;&#29992;&#27599;&#20010;&#23458;&#25143;&#31471;&#20013;&#30340;&#22810;&#20010;&#27169;&#24577;&#26500;&#24314;&#27604;&#20854;&#21333;&#27169;&#24577;&#23545;&#24212;&#29289;&#26356;&#24378;&#22823;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#23458;&#25143;&#31471;&#32570;&#22833;&#27169;&#24577;&#30340;&#24433;&#21709;&#65292;&#20063;&#31216;&#20026;&#27169;&#24577;&#19981;&#19968;&#33268;&#24615;&#65292;&#19968;&#30452;&#34987;&#22823;&#22823;&#24573;&#35270;&#12290;&#26412;&#25991;&#39318;&#27425;&#20998;&#26512;&#20102;&#27169;&#24577;&#19981;&#19968;&#33268;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#19982;&#21442;&#19982;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#29305;&#21035;&#26816;&#26597;&#20102;&#19981;&#19968;&#33268;&#30340;MMFL&#19982;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#23458;&#25143;&#31471;&#30456;&#27604;&#26159;&#21542;&#26356;&#26377;&#30410;&#20110;&#21333;&#27169;&#24577;FL&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19977;&#20010;&#28508;&#22312;&#36884;&#24452;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#33258;&#27880;&#24847;&#26426;&#21046;&#23545;&#20110;&#19981;&#32771;&#34385;&#19981;&#19968;&#33268;&#24615;&#30340;&#20449;&#24687;&#34701;&#21512;&#22312;MMFL&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#22312;&#22810;&#27169;&#24577;&#23458;&#25143;&#31471;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#24577;&#25554;&#20540;&#32593;&#32476;&#65288;MIN&#65289;&#26469;&#35299;&#20915;&#21333;&#27169;&#24577;&#23458;&#25143;&#31471;&#20013;&#30340;&#27169;&#24577;&#32763;&#35793;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20854;&#20943;&#36731;&#32570;&#22833;&#27169;&#24577;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
Multimodal Federated Learning (MMFL) utilizes multiple modalities in each client to build a more powerful Federated Learning (FL) model than its unimodal counterpart. However, the impact of missing modality in different clients, also called modality incongruity, has been greatly overlooked. This paper, for the first time, analyses the impact of modality incongruity and reveals its connection with data heterogeneity across participating clients. We particularly inspect whether incongruent MMFL with unimodal and multimodal clients is more beneficial than unimodal FL. Furthermore, we examine three potential routes of addressing this issue. Firstly, we study the effectiveness of various self-attention mechanisms towards incongruity-agnostic information fusion in MMFL. Secondly, we introduce a modality imputation network (MIN) pre-trained in a multimodal client for modality translation in unimodal clients and investigate its potential towards mitigating the missing modality problem. Thirdly
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#22312;&#32467;&#30452;&#32928;&#30284;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#35270;&#35273;&#26041;&#27861;&#35780;&#20272;&#29305;&#24449;&#25490;&#24207;&#25216;&#26415;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05293</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#32467;&#30452;&#32928;&#30284;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#29305;&#24449;&#36873;&#25321;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A comparative study on feature selection for a risk prediction model for colorectal cancer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05293
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#22312;&#32467;&#30452;&#32928;&#30284;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#35270;&#35273;&#26041;&#27861;&#35780;&#20272;&#29305;&#24449;&#25490;&#24207;&#25216;&#26415;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#21644;&#30446;&#26631;  &#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#26088;&#22312;&#35782;&#21035;&#24739;&#19978;&#26576;&#31181;&#30142;&#30149;&#39118;&#38505;&#26356;&#39640;&#30340;&#20154;&#32676;&#12290;&#29305;&#24449;&#36873;&#25321;&#23545;&#20110;&#25913;&#21892;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12289;&#36991;&#20813;&#36807;&#25311;&#21512;&#20197;&#21450;&#35782;&#21035;&#23548;&#33268;&#30284;&#30151;&#39118;&#38505;&#65288;&#21644;&#20445;&#25252;&#65289;&#30340;&#22240;&#32032;&#23588;&#20026;&#37325;&#35201;&#12290;&#23545;&#29305;&#24449;&#36873;&#25321;/&#25490;&#24207;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#35780;&#20272;&#25104;&#20026;&#20998;&#26512;&#20855;&#26377;&#26356;&#22810;&#39044;&#27979;&#33021;&#21147;&#29305;&#24449;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#26041;&#27861; &#26412;&#30740;&#31350;&#38598;&#20013;&#22312;&#32467;&#30452;&#32928;&#30284;&#19978;&#65292;&#35780;&#20272;&#20102;&#20960;&#31181;&#29305;&#24449;&#25490;&#24207;&#31639;&#27861;&#22312;&#19968;&#32452;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#65288;&#31070;&#32463;&#32593;&#32476;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12289;&#36923;&#36753;&#22238;&#24402;&#12289;k-&#26368;&#36817;&#37051;&#21644;&#25552;&#21319;&#26641;&#65289;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#26631;&#37327;&#31283;&#23450;&#24230;&#25351;&#26631;&#21644;&#26412;&#25991;&#25552;&#20986;&#30340;&#35270;&#35273;&#26041;&#27861;&#35780;&#20272;&#20102;&#20854;&#31283;&#23450;&#24615;&#65292;&#26082;&#21487;&#20197;&#30740;&#31350;&#29305;&#24449;&#25490;&#24207;&#25216;&#26415;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20063;&#21487;&#20197;&#30740;&#31350;&#23427;&#20204;&#30340;&#21333;&#29420;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background and objective   Risk prediction models aim at identifying people at higher risk of developing a target disease. Feature selection is particularly important to improve the prediction model performance avoiding overfitting and to identify the leading cancer risk (and protective) factors. Assessing the stability of feature selection/ranking algorithms becomes an important issue when the aim is to analyze the features with more prediction power. Methods   This work is focused on colorectal cancer, assessing several feature ranking algorithms in terms of performance for a set of risk prediction models (Neural Networks, Support Vector Machines (SVM), Logistic Regression, k-Nearest Neighbors and Boosted Trees). Additionally, their robustness is evaluated following a conventional approach with scalar stability metrics and a visual approach proposed in this work to study both similarity among feature ranking techniques as well as their individual stability. A comparative analysis is 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#26377;&#38480;&#20803;&#20912;&#30422;&#27169;&#25311;&#30340;&#24555;&#36895;&#39640;&#20445;&#30495;&#24230;&#30340;&#20223;&#30495;&#22120;&#65292;&#24182;&#22312;Pine Island Glacier&#30340;&#30636;&#24577;&#27169;&#25311;&#20013;&#23637;&#31034;&#20102;&#19982;&#20256;&#32479;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#30456;&#27604;&#26356;&#20934;&#30830;&#30340;&#37325;&#29616;&#20912;&#30422;&#21402;&#24230;&#21644;&#36895;&#24230;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#36825;&#20123;GNN&#25104;&#21151;&#25429;&#25417;&#21040;&#20102;&#26356;&#39640;&#24213;&#37096;&#29076;&#21270;&#36895;&#29575;&#24341;&#36215;&#30340;&#20912;&#36136;&#37327;&#25439;&#22833;&#21644;&#21152;&#36895;&#36807;&#31243;&#12290;&#22312;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#19978;&#23454;&#29616;&#30340;GNN&#20223;&#30495;&#22120;&#26174;&#31034;&#20986;&#39640;&#36798;50&#20493;&#30340;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2402.05291</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#26377;&#38480;&#20803;&#20912;&#30422;&#27169;&#25311;&#30340;&#24555;&#36895;&#39640;&#20445;&#30495;&#24230;&#30340;&#20223;&#30495;&#22120;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks as Fast and High-fidelity Emulators for Finite-Element Ice Sheet Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#26377;&#38480;&#20803;&#20912;&#30422;&#27169;&#25311;&#30340;&#24555;&#36895;&#39640;&#20445;&#30495;&#24230;&#30340;&#20223;&#30495;&#22120;&#65292;&#24182;&#22312;Pine Island Glacier&#30340;&#30636;&#24577;&#27169;&#25311;&#20013;&#23637;&#31034;&#20102;&#19982;&#20256;&#32479;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#30456;&#27604;&#26356;&#20934;&#30830;&#30340;&#37325;&#29616;&#20912;&#30422;&#21402;&#24230;&#21644;&#36895;&#24230;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#36825;&#20123;GNN&#25104;&#21151;&#25429;&#25417;&#21040;&#20102;&#26356;&#39640;&#24213;&#37096;&#29076;&#21270;&#36895;&#29575;&#24341;&#36215;&#30340;&#20912;&#36136;&#37327;&#25439;&#22833;&#21644;&#21152;&#36895;&#36807;&#31243;&#12290;&#22312;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#19978;&#23454;&#29616;&#30340;GNN&#20223;&#30495;&#22120;&#26174;&#31034;&#20986;&#39640;&#36798;50&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20912;&#30422;&#21644;&#28023;&#24179;&#38754;&#31995;&#32479;&#27169;&#22411;&#65288;ISSM&#65289;&#30340;&#26377;&#38480;&#20803;&#26041;&#27861;&#21487;&#20197;&#24555;&#36895;&#20934;&#30830;&#22320;&#35299;&#20915;&#30001;Stokes&#26041;&#31243;&#25551;&#36848;&#30340;&#20912;&#21160;&#21147;&#23398;&#38382;&#39064;&#65292;&#20294;&#36825;&#31181;&#25968;&#20540;&#24314;&#27169;&#38656;&#35201;&#22312;&#20013;&#22830;&#22788;&#29702;&#21333;&#20803;&#65288;CPU&#65289;&#19978;&#36827;&#34892;&#23494;&#38598;&#30340;&#35745;&#31639;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#24555;&#36895;&#20195;&#29702;&#27169;&#22411;&#26469;&#20445;&#25345;ISSM&#30340;&#26377;&#38480;&#20803;&#32467;&#26500;&#12290;&#21033;&#29992;Pine Island Glacier&#65288;PIG&#65289;&#30340;20&#24180;&#30636;&#24577;&#27169;&#25311;&#65292;&#25105;&#20204;&#35757;&#32451;&#21644;&#27979;&#35797;&#20102;&#19977;&#20010;GNN&#65306;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#65292;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GAT&#65289;&#21644;&#31561;&#21464;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;EGCN&#65289;&#12290;&#36825;&#20123;GNN&#19982;&#32463;&#20856;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#30456;&#27604;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#37325;&#29616;&#20912;&#21402;&#24230;&#21644;&#36895;&#24230;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;PIG&#20013;&#65292;GNN&#25104;&#21151;&#25429;&#25417;&#21040;&#20102;&#30001;&#26356;&#39640;&#24213;&#37096;&#29076;&#21270;&#36895;&#29575;&#24341;&#36215;&#30340;&#20912;&#36136;&#37327;&#25439;&#22833;&#21644;&#21152;&#36895;&#12290;&#24403;&#25105;&#20204;&#30340;GNN&#20223;&#30495;&#22120;&#22312;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#65288;GPU&#65289;&#19978;&#23454;&#29616;&#26102;&#65292;&#23427;&#20204;&#26174;&#31034;&#20986;&#39640;&#36798;50&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the finite element approach of the Ice-sheet and Sea-level System Model (ISSM) solves ice dynamics problems governed by Stokes equations quickly and accurately, such numerical modeling requires intensive computation on central processing units (CPU). In this study, we develop graph neural networks (GNN) as fast surrogate models to preserve the finite element structure of ISSM. Using the 20-year transient simulations in the Pine Island Glacier (PIG), we train and test three GNNs: graph convolutional network (GCN), graph attention network (GAT), and equivariant graph convolutional network (EGCN). These GNNs reproduce ice thickness and velocity with better accuracy than the classic convolutional neural network (CNN) and multi-layer perception (MLP). In particular, GNNs successfully capture the ice mass loss and acceleration induced by higher basal melting rates in the PIG. When our GNN emulators are implemented on graphic processing units (GPUs), they show up to 50 times faster c
&lt;/p&gt;</description></item><item><title>&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#24418;&#22120;&#19990;&#30028;&#27169;&#22411;&#26469;&#39044;&#27979;&#26410;&#26469;&#22870;&#21169;&#24182;&#36827;&#34892;&#31574;&#30053;&#26799;&#24230;&#23398;&#20064;&#36890;&#24120;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#24120;&#29992;&#30340;&#21464;&#24418;&#22120;&#19990;&#30028;&#27169;&#22411;&#20250;&#20135;&#29983;&#36802;&#22238;&#30340;&#26799;&#24230;&#36335;&#24452;&#65292;&#23545;&#20110;&#38271;&#36317;&#31163;&#30340;&#31574;&#30053;&#26799;&#24230;&#26159;&#26377;&#23475;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Actions World Models (AWMs)&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#30452;&#25509;&#30340;&#26799;&#24230;&#20256;&#25773;&#36335;&#24452;&#12290;</title><link>https://arxiv.org/abs/2402.05290</link><description>&lt;p&gt;
&#21464;&#24418;&#22120;&#19990;&#30028;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#32473;&#20986;&#26356;&#22909;&#30340;&#31574;&#30053;&#26799;&#24230;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Transformer World Models Give Better Policy Gradients?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05290
&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#24418;&#22120;&#19990;&#30028;&#27169;&#22411;&#26469;&#39044;&#27979;&#26410;&#26469;&#22870;&#21169;&#24182;&#36827;&#34892;&#31574;&#30053;&#26799;&#24230;&#23398;&#20064;&#36890;&#24120;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#24120;&#29992;&#30340;&#21464;&#24418;&#22120;&#19990;&#30028;&#27169;&#22411;&#20250;&#20135;&#29983;&#36802;&#22238;&#30340;&#26799;&#24230;&#36335;&#24452;&#65292;&#23545;&#20110;&#38271;&#36317;&#31163;&#30340;&#31574;&#30053;&#26799;&#24230;&#26159;&#26377;&#23475;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Actions World Models (AWMs)&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#30452;&#25509;&#30340;&#26799;&#24230;&#20256;&#25773;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#26469;&#35828;&#65292;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#23637;&#24320;&#31070;&#32463;&#32593;&#32476;&#19990;&#30028;&#27169;&#22411;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#22870;&#21169;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#22270;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#20197;&#23398;&#20064;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20856;&#22411;&#30340;&#19990;&#30028;&#27169;&#22411;&#20135;&#29983;&#20102;&#38590;&#20197;&#20248;&#21270;&#30340;&#25439;&#22833;&#22320;&#24418;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#38271;&#26102;&#38388;&#36328;&#24230;&#19978;&#36890;&#24120;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#21464;&#24418;&#22120;&#24050;&#30693;&#21487;&#20197;&#39640;&#25928;&#22320;&#20256;&#25773;&#38271;&#26102;&#38388;&#36328;&#24230;&#30340;&#26799;&#24230;&#65306;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#21602;&#65311;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24120;&#29992;&#30340;&#21464;&#24418;&#22120;&#19990;&#30028;&#27169;&#22411;&#20250;&#20135;&#29983;&#36802;&#22238;&#30340;&#26799;&#24230;&#36335;&#24452;&#65292;&#36825;&#23545;&#20110;&#38271;&#36317;&#31163;&#30340;&#31574;&#30053;&#26799;&#24230;&#26159;&#26377;&#23475;&#30340;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#31216;&#20026;Actions World Models (AWMs)&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#20379;&#26356;&#30452;&#25509;&#30340;&#26799;&#24230;&#20256;&#25773;&#36335;&#24452;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;AWMs&#38598;&#25104;&#21040;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#30340;&#26694;&#26550;&#20013;&#65292;&#24378;&#35843;&#20102;&#32593;&#32476;&#26550;&#26500;&#19982;&#31574;&#30053;&#26799;&#24230;&#26356;&#26032;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;AWMs&#21487;&#20197;&#20135;&#29983;&#21487;&#20248;&#21270;&#30340;&#26799;&#24230;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
A natural approach for reinforcement learning is to predict future rewards by unrolling a neural network world model, and to backpropagate through the resulting computational graph to learn a policy. However, this method often becomes impractical for long horizons since typical world models induce hard-to-optimize loss landscapes. Transformers are known to efficiently propagate gradients overlong horizons: could they be the solution to this problem? Surprisingly, we show that commonly-used transformer world models produce circuitous gradient paths, which can be detrimental to long-range policy gradients. To tackle this challenge, we propose a class of world models called Actions World Models (AWMs), designed to provide more direct routes for gradient propagation. We integrate such AWMs into a policy gradient framework that underscores the relationship between network architectures and the policy gradient updates they inherently represent. We demonstrate that AWMs can generate optimizat
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#24418;&#24335;&#39564;&#35777;&#30340;&#35270;&#35282;&#65292;&#20998;&#26512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#23545;&#25239;&#36755;&#20837;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#8212;&#8212;&#23545;&#25239;&#29575;&#65292;&#20197;&#21450;&#35745;&#31639;&#35813;&#24230;&#37327;&#26631;&#20934;&#30340;&#19968;&#22871;&#24037;&#20855;&#21644;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05284</link><description>&lt;p&gt;
&#20998;&#26512;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23545;&#25239;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
Analyzing Adversarial Inputs in Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05284
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#24418;&#24335;&#39564;&#35777;&#30340;&#35270;&#35282;&#65292;&#20998;&#26512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#23545;&#25239;&#36755;&#20837;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#8212;&#8212;&#23545;&#25239;&#29575;&#65292;&#20197;&#21450;&#35745;&#31639;&#35813;&#24230;&#37327;&#26631;&#20934;&#30340;&#19968;&#22871;&#24037;&#20855;&#21644;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30001;&#20110;&#22312;&#23454;&#38469;&#21644;&#22797;&#26434;&#31995;&#32479;&#20013;&#21462;&#24471;&#30340;&#25104;&#21151;&#24212;&#29992;&#32780;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#21463;&#27426;&#36814;&#30340;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26368;&#20808;&#36827;&#30340;DRL&#27169;&#22411;&#20063;&#34987;&#35777;&#26126;&#23384;&#22312;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#20363;&#22914;&#23545;&#25239;&#36755;&#20837;&#30340;&#25935;&#24863;&#24615;&#65292;&#21363;&#23567;&#22411;&#19988;&#22823;&#37327;&#30340;&#36755;&#20837;&#25200;&#21160;&#20250;&#23548;&#33268;&#27169;&#22411;&#20570;&#20986;&#19981;&#21487;&#39044;&#27979;&#19988;&#28508;&#22312;&#21361;&#38505;&#30340;&#20915;&#31574;&#12290;&#36825;&#20010;&#32570;&#28857;&#38480;&#21046;&#20102;DRL&#31995;&#32479;&#22312;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#20013;&#30340;&#37096;&#32626;&#65292;&#21363;&#20351;&#26159;&#23567;&#30340;&#38169;&#35823;&#37117;&#26159;&#19981;&#21487;&#23481;&#24525;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24418;&#24335;&#39564;&#35777;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;&#23545;&#23545;&#25239;&#36755;&#20837;&#36827;&#34892;&#20998;&#31867;&#30340;&#26032;&#24230;&#37327;&#26631;&#20934;&#8212;&#8212;&#23545;&#25239;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#29992;&#20110;&#35745;&#31639;&#23545;&#25239;&#29575;&#30340;&#24037;&#20855;&#21644;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23545;&#25239;&#36755;&#20837;&#23545;DRL&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Deep Reinforcement Learning (DRL) has become a popular paradigm in machine learning due to its successful applications to real-world and complex systems. However, even the state-of-the-art DRL models have been shown to suffer from reliability concerns -- for example, their susceptibility to adversarial inputs, i.e., small and abundant input perturbations that can fool the models into making unpredictable and potentially dangerous decisions. This drawback limits the deployment of DRL systems in safety-critical contexts, where even a small error cannot be tolerated. In this work, we present a comprehensive analysis of the characterization of adversarial inputs, through the lens of formal verification. Specifically, we introduce a novel metric, the Adversarial Rate, to classify models based on their susceptibility to such perturbations, and present a set of tools and algorithms for its computation. Our analysis empirically demonstrates how adversarial inputs can affect th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25935;&#24863;&#24615;&#25277;&#26679;&#26694;&#26550;&#25552;&#20986;&#20102;&#26080;&#32500;&#24230;&#30340;&#26680;&#24515;&#23376;&#38598;&#29992;&#20110;&#20998;&#31867;&#38382;&#39064;&#65292;&#35813;&#23376;&#38598;&#30340;&#22823;&#23567;&#19982;&#32500;&#24230;&#26080;&#20851;&#65292;&#24182;&#36866;&#29992;&#20110;&#21508;&#31181;&#25439;&#22833;&#20989;&#25968;&#21644;&#20998;&#24067;&#36755;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.05280</link><description>&lt;p&gt;
&#26080;&#32500;&#24230;&#25277;&#26679;&#26680;&#24515;&#23376;&#38598;&#29992;&#20110;&#20998;&#31867;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
No Dimensional Sampling Coresets for Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25935;&#24863;&#24615;&#25277;&#26679;&#26694;&#26550;&#25552;&#20986;&#20102;&#26080;&#32500;&#24230;&#30340;&#26680;&#24515;&#23376;&#38598;&#29992;&#20110;&#20998;&#31867;&#38382;&#39064;&#65292;&#35813;&#23376;&#38598;&#30340;&#22823;&#23567;&#19982;&#32500;&#24230;&#26080;&#20851;&#65292;&#24182;&#36866;&#29992;&#20110;&#21508;&#31181;&#25439;&#22833;&#20989;&#25968;&#21644;&#20998;&#24067;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#25935;&#24863;&#24615;&#25277;&#26679;&#26694;&#26550;&#23545;&#20110;&#20998;&#31867;&#38382;&#39064;&#30340;&#26680;&#24515;&#23376;&#38598;&#30340;&#24050;&#30693;&#20869;&#23481;&#36827;&#34892;&#20102;&#31934;&#28860;&#21644;&#27010;&#25324;&#12290;&#36825;&#31181;&#26680;&#24515;&#23376;&#38598;&#23547;&#27714;&#36755;&#20837;&#25968;&#25454;&#30340;&#26368;&#23567;&#21487;&#33021;&#23376;&#38598;&#65292;&#20197;&#20415;&#21487;&#20197;&#22312;&#26680;&#24515;&#23376;&#38598;&#19978;&#20248;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#30830;&#20445;&#23545;&#20110;&#21407;&#22987;&#25968;&#25454;&#30340;&#36924;&#36817;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26080;&#32500;&#24230;&#26680;&#24515;&#23376;&#38598;&#65292;&#22240;&#27492;&#22823;&#23567;&#19982;&#32500;&#24230;&#26080;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#36890;&#29992;&#30340;&#65292;&#36866;&#29992;&#20110;&#20998;&#24067;&#36755;&#20837;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#29420;&#31435;&#21516;&#20998;&#24067;&#26679;&#26412;&#65292;&#22240;&#27492;&#21487;&#20197;&#25552;&#20379;&#26679;&#26412;&#22797;&#26434;&#24230;&#36793;&#30028;&#65292;&#24182;&#36866;&#29992;&#20110;&#21508;&#31181;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#24320;&#21457;&#30340;&#20851;&#38190;&#24037;&#20855;&#26159;&#20027;&#35201;&#25935;&#24863;&#24615;&#25277;&#26679;&#26041;&#27861;&#30340;Radamacher&#22797;&#26434;&#24230;&#29256;&#26412;&#65292;&#36825;&#21487;&#33021;&#26159;&#19968;&#20010;&#29420;&#31435;&#24863;&#20852;&#36259;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
We refine and generalize what is known about coresets for classification problems via the sensitivity sampling framework. Such coresets seek the smallest possible subsets of input data, so one can optimize a loss function on the coreset and ensure approximation guarantees with respect to the original data. Our analysis provides the first no dimensional coresets, so the size does not depend on the dimension. Moreover, our results are general, apply for distributional input and can use iid samples, so provide sample complexity bounds, and work for a variety of loss functions. A key tool we develop is a Radamacher complexity version of the main sensitivity sampling approach, which can be of independent interest.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#21028;&#21035;&#36229;&#24179;&#38754;&#26469;&#23454;&#29616;&#40657;&#30418;&#21160;&#24577;&#31995;&#32479;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#28040;&#38500;&#23545;&#29305;&#23450;&#35777;&#26126;&#20989;&#25968;&#30340;&#20381;&#36182;&#65292;&#36824;&#31616;&#21270;&#20102;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.05279</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#21028;&#21035;&#36229;&#24179;&#38754;&#23454;&#29616;&#40657;&#30418;&#21160;&#24577;&#31995;&#32479;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;
&lt;/p&gt;
&lt;p&gt;
Safety Filters for Black-Box Dynamical Systems by Learning Discriminating Hyperplanes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#21028;&#21035;&#36229;&#24179;&#38754;&#26469;&#23454;&#29616;&#40657;&#30418;&#21160;&#24577;&#31995;&#32479;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#28040;&#38500;&#23545;&#29305;&#23450;&#35777;&#26126;&#20989;&#25968;&#30340;&#20381;&#36182;&#65292;&#36824;&#31616;&#21270;&#20102;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#27491;&#22312;&#25104;&#20026;&#40657;&#30418;&#21160;&#24577;&#31995;&#32479;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#26377;&#25928;&#36884;&#24452;&#12290;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#35777;&#26126;&#20989;&#25968;&#65288;&#22914;&#25511;&#21046;&#30028;&#38754;&#20989;&#25968;&#65288;CBFs&#65289;&#21644;&#21704;&#23494;&#39039;-&#38597;&#21487;&#27604;&#65288;HJ&#65289;&#21487;&#36798;&#24615;&#20540;&#20989;&#25968;&#65289;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#35748;&#35782;&#21040;&#26368;&#32456;&#23558;&#23433;&#20840;&#32422;&#26463;&#20316;&#20026;&#27599;&#20010;&#29366;&#24577;&#30340;&#25511;&#21046;&#36755;&#20837;&#32422;&#26463;&#26469;&#24378;&#21046;&#25191;&#34892;&#25165;&#26159;&#26368;&#37325;&#35201;&#30340;&#12290;&#36890;&#36807;&#19987;&#27880;&#20110;&#36825;&#20010;&#32422;&#26463;&#65292;&#25105;&#20204;&#21487;&#20197;&#28040;&#38500;&#23545;&#20219;&#20309;&#29305;&#23450;&#35777;&#26126;&#20989;&#25968;&#35774;&#35745;&#30340;&#20381;&#36182;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#21028;&#21035;&#36229;&#24179;&#38754;&#65292;&#29992;&#20110;&#22312;&#27599;&#20010;&#29366;&#24577;&#19978;&#24418;&#25104;&#25511;&#21046;&#36755;&#20837;&#30340;&#21322;&#31354;&#38388;&#32422;&#26463;&#65292;&#20316;&#20026;&#23433;&#20840;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#36825;&#20010;&#27010;&#24565;&#19981;&#20165;&#24191;&#27867;&#36866;&#29992;&#20256;&#32479;&#30340;&#23433;&#20840;&#26041;&#27861;&#65292;&#32780;&#19988;&#36890;&#36807;&#28040;&#38500;&#23545;&#29305;&#23450;&#35777;&#26126;&#20989;&#25968;&#30340;&#20381;&#36182;&#65292;&#31616;&#21270;&#20102;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#35774;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23398;&#20064;&#21028;&#21035;&#36229;&#24179;&#38754;&#30340;&#31574;&#30053;&#65306;&#65288;a&#65289;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#20808;&#39564;&#35777;&#30340;&#35777;&#26126;&#20989;&#25968;&#20449;&#24687;&#26469;&#35757;&#32451;&#27169;&#22411;&#65307;&#65288;b&#65289;&#21152;&#36895;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#21028;&#21035;&#36229;&#24179;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning-based approaches are emerging as an effective approach for safety filters for black-box dynamical systems. Existing methods have relied on certificate functions like Control Barrier Functions (CBFs) and Hamilton-Jacobi (HJ) reachability value functions. The primary motivation for our work is the recognition that ultimately, enforcing the safety constraint as a control input constraint at each state is what matters. By focusing on this constraint, we can eliminate dependence on any specific certificate function-based design. To achieve this, we define a discriminating hyperplane that shapes the half-space constraint on control input at each state, serving as a sufficient condition for safety. This concept not only generalizes over traditional safety methods but also simplifies safety filter design by eliminating dependence on specific certificate functions. We present two strategies to learn the discriminating hyperplane: (a) a supervised learning approach, using pre-verified c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#23618;&#27425;&#20998;&#31867;&#21644;&#24179;&#38138;&#20998;&#31867;&#26041;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#20351;&#29992;MINIROCKET&#21644;TSD&#30340;&#24773;&#20917;&#19979;&#65292;&#23618;&#27425;&#20998;&#31867;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#65292;&#32780;&#22312;&#20351;&#29992;STSF&#21644;SVM&#31561;&#26367;&#20195;&#26041;&#27861;&#26102;&#65292;&#24179;&#38138;&#20998;&#31867;&#19968;&#30452;&#20445;&#25345;&#20248;&#21183;&#12290;TSD&#20063;&#22312;&#20960;&#20046;&#25152;&#26377;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#27604;CBD&#21644;JSD&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.05275</link><description>&lt;p&gt;
&#25506;&#32034;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#23618;&#27425;&#20998;&#31867;&#24615;&#33021;: &#30456;&#24322;&#24230;&#24230;&#37327;&#21644;&#20998;&#31867;&#22120;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Exploring Hierarchical Classification Performance for Time Series Data: Dissimilarity Measures and Classifier Comparisons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#23618;&#27425;&#20998;&#31867;&#21644;&#24179;&#38138;&#20998;&#31867;&#26041;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#20351;&#29992;MINIROCKET&#21644;TSD&#30340;&#24773;&#20917;&#19979;&#65292;&#23618;&#27425;&#20998;&#31867;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#65292;&#32780;&#22312;&#20351;&#29992;STSF&#21644;SVM&#31561;&#26367;&#20195;&#26041;&#27861;&#26102;&#65292;&#24179;&#38138;&#20998;&#31867;&#19968;&#30452;&#20445;&#25345;&#20248;&#21183;&#12290;TSD&#20063;&#22312;&#20960;&#20046;&#25152;&#26377;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#27604;CBD&#21644;JSD&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#20013;&#65292;&#23618;&#27425;&#20998;&#31867;&#65288;HC&#65289;&#21644;&#24179;&#38138;&#20998;&#31867;&#65288;FC&#65289;&#26041;&#27861;&#30340;&#27604;&#36739;&#24615;&#33021;&#12290;&#20351;&#29992;&#20102;&#21253;&#25324;Jensen-Shannon&#36317;&#31163;&#65288;JSD&#65289;&#12289;&#20219;&#21153;&#30456;&#20284;&#24230;&#36317;&#31163;&#65288;TSD&#65289;&#21644;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#36317;&#31163;&#65288;CBD&#65289;&#31561;&#30456;&#24322;&#24230;&#24230;&#37327;&#65292;&#20197;&#21450;MINIROCKET&#12289;STSF&#21644;SVM&#31561;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#12290;&#37319;&#29992;&#20102;&#26469;&#33258;UCR&#23384;&#26723;&#30340;&#25968;&#25454;&#38598;&#23376;&#38598;&#65292;&#37325;&#28857;&#20851;&#27880;&#22810;&#31867;&#21035;&#24773;&#20917;&#19979;&#36229;&#36807;&#20004;&#20010;&#31867;&#21035;&#30340;&#26696;&#20363;&#36827;&#34892;&#20998;&#26512;&#12290;&#35266;&#23519;&#21040;&#19968;&#20010;&#26174;&#33879;&#36235;&#21183;&#65292;&#21363;&#22312;&#20351;&#29992;TSD&#21644;MINIROCKET&#26102;&#65292;HC&#30456;&#36739;&#20110;FC&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#36825;&#19982;&#20256;&#32479;&#29702;&#35299;&#26377;&#25152;&#19981;&#21516;&#12290;&#30456;&#21453;&#65292;&#24403;&#20351;&#29992;STSF&#21644;SVM&#31561;&#26367;&#20195;&#20998;&#31867;&#22120;&#26102;&#65292;FC&#22312;&#25152;&#26377;&#37197;&#32622;&#19979;&#37117;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;TSD&#22312;&#20960;&#20046;&#25152;&#26377;&#24773;&#20917;&#19979;&#37117;&#34920;&#29616;&#20986;&#27604;CBD&#21644;JSD&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21807;&#29420;&#22312;&#28041;&#21450;STSF&#20998;&#31867;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;CBD&#23637;&#29616;&#20986;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The comparative performance of hierarchical classification (HC) and flat classification (FC) methodologies in the realm of time series data analysis is investigated in this study. Dissimilarity measures, including Jensen-Shannon Distance (JSD), Task Similarity Distance (TSD), and Classifier Based Distance (CBD), are leveraged alongside various classifiers such as MINIROCKET, STSF, and SVM. A subset of datasets from the UCR archive, focusing on multi-class cases comprising more than two classes, is employed for analysis. A significant trend is observed wherein HC demonstrates significant superiority over FC when paired with MINIROCKET utilizing TSD, diverging from conventional understandings. Conversely, FC exhibits consistent dominance across all configurations when employing alternative classifiers such as STSF and SVM. Moreover, TSD is found to consistently outperform both CBD and JSD across nearly all scenarios, except in instances involving the STSF classifier where CBD showcases s
&lt;/p&gt;</description></item><item><title>&#20102;&#35299;&#31070;&#32463;&#32593;&#32476;&#20174;&#36755;&#20837;-&#26631;&#31614;&#23545;&#20013;&#25552;&#21462;&#32479;&#35745;&#20449;&#24687;&#30340;&#26426;&#21046;&#26159;&#30417;&#30563;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#20043;&#19968;&#12290;&#21069;&#20154;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26435;&#37325;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#19982;&#27169;&#22411;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#25104;&#27491;&#27604;&#65292;&#36825;&#34987;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#20998;&#26512;&#65288;NFA&#65289;&#12290;&#26412;&#30740;&#31350;&#35299;&#37322;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#20986;&#29616;&#65292;&#24182;&#21457;&#29616;NFA&#31561;&#20215;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#24038;&#22855;&#24322;&#32467;&#26500;&#19982;&#19982;&#36825;&#20123;&#26435;&#37325;&#30456;&#20851;&#30340;&#32463;&#39564;&#31070;&#32463;&#20999;&#32447;&#26680;&#30340;&#26174;&#33879;&#25104;&#20998;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#30340;&#26041;&#24335;&#39044;&#27979;NFA&#30340;&#21457;&#23637;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.05271</link><description>&lt;p&gt;
&#26799;&#24230;&#19979;&#38477;&#24341;&#21457;&#20102;&#28145;&#24230;&#38750;&#32447;&#24615;&#32593;&#32476;&#26435;&#37325;&#19982;&#32463;&#39564;NTK&#20043;&#38388;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Gradient descent induces alignment between weights and the empirical NTK for deep non-linear networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05271
&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#31070;&#32463;&#32593;&#32476;&#20174;&#36755;&#20837;-&#26631;&#31614;&#23545;&#20013;&#25552;&#21462;&#32479;&#35745;&#20449;&#24687;&#30340;&#26426;&#21046;&#26159;&#30417;&#30563;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#20043;&#19968;&#12290;&#21069;&#20154;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26435;&#37325;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#19982;&#27169;&#22411;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#25104;&#27491;&#27604;&#65292;&#36825;&#34987;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#20998;&#26512;&#65288;NFA&#65289;&#12290;&#26412;&#30740;&#31350;&#35299;&#37322;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#20986;&#29616;&#65292;&#24182;&#21457;&#29616;NFA&#31561;&#20215;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#24038;&#22855;&#24322;&#32467;&#26500;&#19982;&#19982;&#36825;&#20123;&#26435;&#37325;&#30456;&#20851;&#30340;&#32463;&#39564;&#31070;&#32463;&#20999;&#32447;&#26680;&#30340;&#26174;&#33879;&#25104;&#20998;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#30340;&#26041;&#24335;&#39044;&#27979;NFA&#30340;&#21457;&#23637;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20174;&#36755;&#20837;-&#26631;&#31614;&#23545;&#20013;&#25552;&#21462;&#32479;&#35745;&#20449;&#24687;&#30340;&#26426;&#21046;&#26159;&#30417;&#30563;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#20043;&#19968;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#30830;&#23450;&#65292;&#22312;&#19968;&#33324;&#32467;&#26500;&#30340;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#26435;&#37325;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#19982;&#27169;&#22411;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#25104;&#27491;&#27604;&#65292;&#36825;&#20010;&#35828;&#27861;&#34987;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#20998;&#26512;&#65288;NFA&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#37327;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22914;&#20309;&#30456;&#20851;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;NFA&#31561;&#20215;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#24038;&#22855;&#24322;&#32467;&#26500;&#19982;&#19982;&#36825;&#20123;&#26435;&#37325;&#30456;&#20851;&#30340;&#32463;&#39564;&#31070;&#32463;&#20999;&#32447;&#26680;&#30340;&#26174;&#33879;&#25104;&#20998;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20808;&#21069;&#30740;&#31350;&#20013;&#24341;&#20837;&#30340;NFA&#26159;&#30001;&#38548;&#31163;&#36825;&#31181;&#23545;&#40784;&#30340;&#20013;&#24515;&#21270;NFA&#39537;&#21160;&#30340;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#30340;&#26041;&#24335;&#39044;&#27979;NFA&#30340;&#21457;&#23637;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the mechanisms through which neural networks extract statistics from input-label pairs is one of the most important unsolved problems in supervised learning. Prior works have identified that the gram matrices of the weights in trained neural networks of general architectures are proportional to the average gradient outer product of the model, in a statement known as the Neural Feature Ansatz (NFA). However, the reason these quantities become correlated during training is poorly understood. In this work, we explain the emergence of this correlation. We identify that the NFA is equivalent to alignment between the left singular structure of the weight matrices and a significant component of the empirical neural tangent kernels associated with those weights. We establish that the NFA introduced in prior works is driven by a centered NFA that isolates this alignment. We show that the speed of NFA development can be predicted analytically at early training times in terms of sim
&lt;/p&gt;</description></item><item><title>AdaBatchGrad&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#33258;&#36866;&#24212;&#25209;&#22823;&#23567;&#21644;&#33258;&#36866;&#24212;&#27493;&#38271;&#26469;&#25552;&#39640;&#26041;&#27861;&#30340;&#31283;&#20581;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05264</link><description>&lt;p&gt;
AdaBatchGrad: &#32452;&#21512;&#33258;&#36866;&#24212;&#25209;&#22823;&#23567;&#21644;&#33258;&#36866;&#24212;&#27493;&#38271;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AdaBatchGrad: Combining Adaptive Batch Size and Adaptive Step Size
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05264
&lt;/p&gt;
&lt;p&gt;
AdaBatchGrad&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#33258;&#36866;&#24212;&#25209;&#22823;&#23567;&#21644;&#33258;&#36866;&#24212;&#27493;&#38271;&#26469;&#25552;&#39640;&#26041;&#27861;&#30340;&#31283;&#20581;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#31216;&#20026;AdaBatchGrad&#12290;&#36825;&#31181;&#25913;&#36827;&#26041;&#27861;&#26080;&#32541;&#22320;&#23558;&#33258;&#36866;&#24212;&#27493;&#38271;&#19982;&#21487;&#35843;&#25972;&#30340;&#25209;&#22823;&#23567;&#30456;&#32467;&#21512;&#12290;&#22686;&#21152;&#25209;&#22823;&#23567;&#21644;&#20943;&#23567;&#27493;&#38271;&#26159;&#32039;&#26463;SGD&#25910;&#25947;&#21306;&#38388;&#21644;&#20943;&#23567;&#20854;&#26041;&#24046;&#30340;&#24050;&#30693;&#25216;&#26415;&#12290;R. Byrd&#21644;J. Nocedal&#30340;&#19968;&#31995;&#21015;&#30740;&#31350;&#24341;&#20837;&#20102;&#21508;&#31181;&#27979;&#35797;&#25216;&#26415;&#26469;&#35780;&#20272;&#23567;&#25209;&#37327;&#26799;&#24230;&#36817;&#20284;&#30340;&#36136;&#37327;&#65292;&#24182;&#22312;&#27599;&#20010;&#27493;&#39588;&#36873;&#25321;&#21512;&#36866;&#30340;&#25209;&#37327;&#22823;&#23567;&#12290;&#37319;&#29992;&#20934;&#30830;&#27979;&#35797;&#30340;&#26041;&#27861;&#35266;&#23519;&#21040;&#22312;$O(LR^2/\varepsilon)$&#36845;&#20195;&#20869;&#25910;&#25947;&#12290;&#28982;&#32780;&#65292;&#19981;&#20934;&#30830;&#30340;&#27979;&#35797;&#23454;&#29616;&#26377;&#26102;&#23548;&#33268;&#19981;&#25910;&#25947;&#21644;&#19981;&#31283;&#23450;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;AdaBatchGrad&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;&#25209;&#37327;&#21644;&#27493;&#38271;&#30340;&#26041;&#27861;&#65292;&#22686;&#24378;&#20102;&#26041;&#27861;&#30340;&#31283;&#20581;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;&#23545;&#20110;&#20934;&#30830;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;$O(LR^2/\varepsilon)$&#36845;&#20195;&#25910;&#25947;&#65292;&#31867;&#20284;&#20110;&#26631;&#20934;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel adaptation of the Stochastic Gradient Descent (SGD), termed AdaBatchGrad. This modification seamlessly integrates an adaptive step size with an adjustable batch size. An increase in batch size and a decrease in step size are well-known techniques to tighten the area of convergence of SGD and decrease its variance. A range of studies by R. Byrd and J. Nocedal introduced various testing techniques to assess the quality of mini-batch gradient approximations and choose the appropriate batch sizes at every step. Methods that utilized exact tests were observed to converge within $O(LR^2/\varepsilon)$ iterations. Conversely, inexact test implementations sometimes resulted in non-convergence and erratic performance. To address these challenges, AdaBatchGrad incorporates both adaptive batch and step sizes, enhancing the method's robustness and stability. For exact tests, our approach converges in $O(LR^2/\varepsilon)$ iterations, analogous to standard gradient descen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#26377;&#24207;&#21152;&#26435;&#24179;&#22343;&#20540;&#20989;&#25968;&#65292;&#22312;LTR&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#38598;&#25104;&#39640;&#25928;&#30340;&#20844;&#24179;&#25490;&#21517;&#27169;&#22411;&#65292;&#23454;&#29616;&#20844;&#24179;&#24615;&#12289;&#29992;&#25143;&#25928;&#29992;&#21644;&#36816;&#34892;&#26102;&#25928;&#29575;&#20043;&#38388;&#30340;&#26377;&#21033;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.05252</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#24494;&#20998;&#20248;&#21270;&#26377;&#24207;&#21152;&#26435;&#24179;&#22343;&#20540;&#23398;&#20064;&#20844;&#24179;&#25490;&#21517;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Fair Ranking Policies via Differentiable Optimization of Ordered Weighted Averages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#26377;&#24207;&#21152;&#26435;&#24179;&#22343;&#20540;&#20989;&#25968;&#65292;&#22312;LTR&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#38598;&#25104;&#39640;&#25928;&#30340;&#20844;&#24179;&#25490;&#21517;&#27169;&#22411;&#65292;&#23454;&#29616;&#20844;&#24179;&#24615;&#12289;&#29992;&#25143;&#25928;&#29992;&#21644;&#36816;&#34892;&#26102;&#25928;&#29575;&#20043;&#38388;&#30340;&#26377;&#21033;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#25490;&#24207;&#65288;LTR&#65289;&#26159;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20043;&#19968;&#12290;&#23427;&#26159;&#20855;&#26377;&#28145;&#36828;&#31038;&#20250;&#24433;&#21709;&#30340;&#24179;&#21488;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#27714;&#32844;&#25628;&#32034;&#12289;&#21307;&#30103;&#20449;&#24687;&#26816;&#32034;&#21644;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#25512;&#36865;&#12290;&#20256;&#32479;&#30340;LTR&#27169;&#22411;&#24050;&#32463;&#26174;&#31034;&#20986;&#20135;&#29983;&#20559;&#35265;&#32467;&#26524;&#65292;&#24341;&#21457;&#20102;&#22914;&#20309;&#35299;&#20915;&#20165;&#20248;&#20808;&#32771;&#34385;&#29992;&#25143;&#30456;&#20851;&#24615;&#30340;&#25490;&#21517;&#31995;&#32479;&#24341;&#20837;&#30340;&#24046;&#24322;&#30340;&#35752;&#35770;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#20844;&#24179;&#23398;&#20064;&#25490;&#24207;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#22312;&#20934;&#30830;&#24615;&#25110;&#25928;&#29575;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#25490;&#21517;&#24179;&#21488;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#26377;&#24207;&#21152;&#26435;&#24179;&#22343;&#65288;OWA&#65289;&#20989;&#25968;&#30340;&#39640;&#25928;&#21487;&#35299;&#30340;&#20844;&#24179;&#25490;&#21517;&#27169;&#22411;&#38598;&#25104;&#21040;LTR&#27169;&#22411;&#30340;&#35757;&#32451;&#24490;&#29615;&#20013;&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#24615;&#12289;&#29992;&#25143;&#25928;&#29992;&#21644;&#36816;&#34892;&#26102;&#25928;&#29575;&#20043;&#38388;&#30340;&#26377;&#21033;&#24179;&#34913;&#12290;&#29305;&#21035;&#26159;&#65292;&#26412;&#25991;&#39318;&#27425;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#32422;&#26463;&#20248;&#21270;&#21453;&#21521;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to Rank (LTR) is one of the most widely used machine learning applications. It is a key component in platforms with profound societal impacts, including job search, healthcare information retrieval, and social media content feeds. Conventional LTR models have been shown to produce biases results, stimulating a discourse on how to address the disparities introduced by ranking systems that solely prioritize user relevance. However, while several models of fair learning to rank have been proposed, they suffer from deficiencies either in accuracy or efficiency, thus limiting their applicability to real-world ranking platforms. This paper shows how efficiently-solvable fair ranking models, based on the optimization of Ordered Weighted Average (OWA) functions, can be integrated into the training loop of an LTR model to achieve favorable balances between fairness, user utility, and runtime efficiency. In particular, this paper is the first to show how to backpropagate through constra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;QGFN&#65292;&#36890;&#36807;&#23558;GFN&#31574;&#30053;&#19982;&#21160;&#20316;&#20540;&#20272;&#35745;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#22810;&#39640;&#22870;&#21169;&#30340;&#26679;&#26412;&#32780;&#19981;&#29306;&#29298;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05234</link><description>&lt;p&gt;
QGFN:&#20855;&#26377;&#21160;&#20316;&#20540;&#30340;&#21487;&#25511;&#36138;&#23146;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
QGFN: Controllable Greediness with Action Values
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;QGFN&#65292;&#36890;&#36807;&#23558;GFN&#31574;&#30053;&#19982;&#21160;&#20316;&#20540;&#20272;&#35745;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#22810;&#39640;&#22870;&#21169;&#30340;&#26679;&#26412;&#32780;&#19981;&#29306;&#29298;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets;GFNs&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#32452;&#21512;&#23545;&#35937;&#30340;&#22522;&#20110;&#22870;&#21169;/&#33021;&#37327;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#39640;&#25928;&#30340;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#20559;&#21521;&#20110;&#29983;&#25104;&#39640;&#25928;&#26679;&#26412;&#30340;GFNs&#24182;&#19981;&#23481;&#26131;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;GFNs&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#23558;GFN&#31574;&#30053;&#19982;&#21160;&#20316;&#20540;&#20272;&#35745;$Q$&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#21019;&#24314;&#21487;&#20197;&#36890;&#36807;&#28151;&#21512;&#21442;&#25968;&#25511;&#21046;&#30340;&#36138;&#23146;&#37319;&#26679;&#31574;&#30053;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#20010;QGFN&#30340;&#21464;&#20307;&#33021;&#22815;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#25913;&#21892;&#29983;&#25104;&#39640;&#22870;&#21169;&#26679;&#26412;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#19981;&#29306;&#29298;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Flow Networks (GFlowNets; GFNs) are a family of reward/energy-based generative methods for combinatorial objects, capable of generating diverse and high-utility samples. However, biasing GFNs towards producing high-utility samples is non-trivial. In this work, we leverage connections between GFNs and reinforcement learning (RL) and propose to combine the GFN policy with an action-value estimate, $Q$, to create greedier sampling policies which can be controlled by a mixing parameter. We show that several variants of the proposed method, QGFN, are able to improve on the number of high-reward samples generated in a variety of tasks without sacrificing diversity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#29992;&#31070;&#32463;&#21151;&#33021;&#65288;UNFs&#65289;&#65292;&#19968;&#31181;&#33021;&#22815;&#33258;&#21160;&#26500;&#24314;&#36866;&#29992;&#20110;&#20219;&#20309;&#26435;&#37325;&#31354;&#38388;&#30340;&#32622;&#25442;&#31561;&#21464;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#20248;&#21270;&#23567;&#22411;&#22270;&#20687;&#20998;&#31867;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;UNFs&#33021;&#22815;&#21462;&#24471;&#26377;&#24076;&#26395;&#30340;&#25913;&#36827;&#65292;&#20026;&#23398;&#20064;&#20248;&#21270;&#22120;&#35774;&#35745;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.05232</link><description>&lt;p&gt;
&#36890;&#29992;&#31070;&#32463;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
Universal Neural Functionals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#29992;&#31070;&#32463;&#21151;&#33021;&#65288;UNFs&#65289;&#65292;&#19968;&#31181;&#33021;&#22815;&#33258;&#21160;&#26500;&#24314;&#36866;&#29992;&#20110;&#20219;&#20309;&#26435;&#37325;&#31354;&#38388;&#30340;&#32622;&#25442;&#31561;&#21464;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#20248;&#21270;&#23567;&#22411;&#22270;&#20687;&#20998;&#31867;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;UNFs&#33021;&#22815;&#21462;&#24471;&#26377;&#24076;&#26395;&#30340;&#25913;&#36827;&#65292;&#20026;&#23398;&#20064;&#20248;&#21270;&#22120;&#35774;&#35745;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#26159;&#22788;&#29702;&#26435;&#37325;&#31354;&#38388;&#29305;&#24449;&#65292;&#21363;&#20174;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#26799;&#24230;&#20013;&#36716;&#25442;&#25110;&#25552;&#21462;&#20449;&#24687;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#26435;&#37325;&#31354;&#38388;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#31616;&#21333;&#30340;&#21069;&#39304;&#32593;&#32476;&#30340;&#32622;&#25442;&#23545;&#31216;&#24615;&#26159;&#31561;&#21464;&#30340;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#20110;&#26222;&#36890;&#26550;&#26500;&#24182;&#19981;&#36866;&#29992;&#65292;&#22240;&#20026;&#26435;&#37325;&#31354;&#38388;&#30340;&#32622;&#25442;&#23545;&#31216;&#24615;&#21487;&#33021;&#20250;&#22240;&#24490;&#29615;&#25110;&#27531;&#24046;&#36830;&#25509;&#32780;&#21464;&#24471;&#22797;&#26434;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#33258;&#21160;&#26500;&#24314;&#32622;&#25442;&#31561;&#21464;&#27169;&#22411;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#36890;&#29992;&#31070;&#32463;&#21151;&#33021;&#65288;UNFs&#65289;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#26435;&#37325;&#31354;&#38388;&#12290;&#22312;&#20854;&#20182;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;UNFs&#26367;&#20195;&#29616;&#26377;&#30340;&#23398;&#20064;&#20248;&#21270;&#22120;&#35774;&#35745;&#65292;&#24182;&#22312;&#20248;&#21270;&#23567;&#22411;&#22270;&#20687;&#20998;&#31867;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#26102;&#21457;&#29616;&#26377;&#24076;&#26395;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#20248;&#21270;&#22120;&#21487;&#20197;&#20174;&#32771;&#34385;&#65288;&#23545;&#31216;&#65289;&#32467;&#26500;&#30340;&#35282;&#24230;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
A challenging problem in many modern machine learning tasks is to process weight-space features, i.e., to transform or extract information from the weights and gradients of a neural network. Recent works have developed promising weight-space models that are equivariant to the permutation symmetries of simple feedforward networks. However, they are not applicable to general architectures, since the permutation symmetries of a weight space can be complicated by recurrence or residual connections. This work proposes an algorithm that automatically constructs permutation equivariant models, which we refer to as universal neural functionals (UNFs), for any weight space. Among other applications, we demonstrate how UNFs can be substituted into existing learned optimizer designs, and find promising improvements over prior methods when optimizing small image classifiers and language models. Our results suggest that learned optimizers can benefit from considering the (symmetry) structure of the
&lt;/p&gt;</description></item><item><title>VerAs&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#29992;&#20110;&#39564;&#35777;&#21644;&#35780;&#20272;STEM&#23454;&#39564;&#25253;&#21578;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#32500;&#24230;&#30340;&#20998;&#26512;&#35780;&#20272;&#26631;&#20934;&#65292;&#20197;&#21450;&#38024;&#23545;&#23398;&#29983;&#25552;&#20379;&#35814;&#32454;&#21453;&#39304;&#65292;&#24110;&#21161;&#20182;&#20204;&#25552;&#39640;&#31185;&#23398;&#20889;&#20316;&#25216;&#24039;&#12290;</title><link>https://arxiv.org/abs/2402.05224</link><description>&lt;p&gt;
VerAs: &#39564;&#35777;&#28982;&#21518;&#35780;&#20272;STEM&#23454;&#39564;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
VerAs: Verify then Assess STEM Lab Reports
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05224
&lt;/p&gt;
&lt;p&gt;
VerAs&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#29992;&#20110;&#39564;&#35777;&#21644;&#35780;&#20272;STEM&#23454;&#39564;&#25253;&#21578;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#32500;&#24230;&#30340;&#20998;&#26512;&#35780;&#20272;&#26631;&#20934;&#65292;&#20197;&#21450;&#38024;&#23545;&#23398;&#29983;&#25552;&#20379;&#35814;&#32454;&#21453;&#39304;&#65292;&#24110;&#21161;&#20182;&#20204;&#25552;&#39640;&#31185;&#23398;&#20889;&#20316;&#25216;&#24039;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;STEM&#25945;&#32946;&#23545;&#25209;&#21028;&#24615;&#24605;&#32500;&#33021;&#21147;&#30340;&#26085;&#30410;&#20851;&#27880;&#65292;&#31185;&#23398;&#20889;&#20316;&#22312;&#27880;&#37325;&#25506;&#31350;&#25216;&#33021;&#30340;&#35838;&#31243;&#20013;&#21457;&#25381;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26368;&#36817;&#21457;&#24067;&#30340;&#19968;&#20221;&#25968;&#25454;&#38598;&#26159;&#22522;&#20110;&#19968;&#22871;&#25506;&#31350;&#22411;&#29289;&#29702;&#35838;&#31243;&#30340;&#20004;&#32452;&#22823;&#23398;&#27700;&#24179;&#30340;&#23454;&#39564;&#25253;&#21578;&#65292;&#20381;&#36182;&#20110;&#21033;&#29992;&#22810;&#20010;&#32500;&#24230;&#30340;&#20998;&#26512;&#35780;&#20272;&#26631;&#20934;&#65292;&#25351;&#23450;&#23398;&#31185;&#30693;&#35782;&#21644;&#20248;&#31168;&#35299;&#37322;&#30340;&#19968;&#33324;&#32452;&#25104;&#37096;&#20998;&#12290;&#27599;&#20010;&#20998;&#26512;&#32500;&#24230;&#37117;&#20197;6&#20998;&#21046;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#25552;&#20379;&#35814;&#32454;&#21453;&#39304;&#65292;&#24110;&#21161;&#23398;&#29983;&#25552;&#39640;&#31185;&#23398;&#20889;&#20316;&#25216;&#24039;&#12290;&#25163;&#21160;&#35780;&#20272;&#21487;&#33021;&#36739;&#24930;&#65292;&#24182;&#19988;&#22312;&#22823;&#29677;&#20013;&#23545;&#25152;&#26377;&#23398;&#29983;&#36827;&#34892;&#19968;&#33268;&#24615;&#26657;&#20934;&#21487;&#33021;&#24456;&#22256;&#38590;&#12290;&#23613;&#31649;&#22312;STEM&#23398;&#31185;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#30340;&#33258;&#21160;&#35780;&#20272;&#19978;&#24050;&#32463;&#26377;&#24456;&#22810;&#24037;&#20316;&#65292;&#20294;&#22312;&#23454;&#39564;&#25253;&#21578;&#31561;&#38271;&#31687;&#20889;&#20316;&#20013;&#30340;&#24037;&#20316;&#35201;&#23569;&#24471;&#22810;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#29420;&#31435;&#30340;&#39564;&#35777;&#22120;&#21644;&#35780;&#20272;&#27169;&#22359;&#65292;&#28789;&#24863;&#26469;&#28304;&#20110;&#24320;&#25918;&#39046;&#22495;&#38382;&#39064;&#22238;&#31572;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With an increasing focus in STEM education on critical thinking skills, science writing plays an ever more important role in curricula that stress inquiry skills. A recently published dataset of two sets of college level lab reports from an inquiry-based physics curriculum relies on analytic assessment rubrics that utilize multiple dimensions, specifying subject matter knowledge and general components of good explanations. Each analytic dimension is assessed on a 6-point scale, to provide detailed feedback to students that can help them improve their science writing skills. Manual assessment can be slow, and difficult to calibrate for consistency across all students in large classes. While much work exists on automated assessment of open-ended questions in STEM subjects, there has been far less work on long-form writing such as lab reports. We present an end-to-end neural architecture that has separate verifier and assessment modules, inspired by approaches to Open Domain Question Answ
&lt;/p&gt;</description></item><item><title>&#22312;&#20559;&#31163;&#39640;&#26031;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#20013;&#65292;&#26412;&#25991;&#36890;&#36807;&#26500;&#36896;Voronoi-based&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05220</link><description>&lt;p&gt;
&#20851;&#20110;&#20559;&#31163;&#39640;&#26031;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Parameter Estimation in Deviated Gaussian Mixture of Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05220
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20559;&#31163;&#39640;&#26031;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#20013;&#65292;&#26412;&#25991;&#36890;&#36807;&#26500;&#36896;Voronoi-based&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#20559;&#31163;&#39640;&#26031;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#20013;&#30340;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#65292;&#20854;&#20013;&#25968;&#25454;&#30001;$(1 - \lambda^{\ast}) g_0(Y| X)+ \lambda^{\ast} \sum_{i = 1}^{k_{\ast}} p_{i}^{\ast} f(Y|(a_{i}^{\ast})^{\top}X+b_i^{\ast},\sigma_{i}^{\ast})$&#29983;&#25104;&#65292;&#20854;&#20013;$X, Y$&#20998;&#21035;&#26159;&#21327;&#21464;&#37327;&#21521;&#37327;&#21644;&#21709;&#24212;&#21464;&#37327;&#65292;$g_{0}(Y|X)$&#26159;&#24050;&#30693;&#20989;&#25968;&#65292;$\lambda^{\ast} \in [0, 1]$&#26159;&#30495;&#23454;&#20294;&#26410;&#30693;&#30340;&#28151;&#21512;&#27604;&#20363;&#65292;$(p_{i}^{\ast}, a_{i}^{\ast}, b_{i}^{\ast}, \sigma_{i}^{\ast})$&#23545;&#20110;$1 \leq i \leq k^{\ast}$&#26159;&#39640;&#26031;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;&#26410;&#30693;&#21442;&#25968;&#12290;&#35813;&#38382;&#39064;&#28304;&#33258;&#20110;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;&#65292;&#24403;&#25105;&#20204;&#24076;&#26395;&#26816;&#39564;&#25968;&#25454;&#26159;&#21542;&#30001;$g_{0}(Y|X)$&#65288;&#38646;&#20551;&#35774;&#65289;&#29983;&#25104;&#65292;&#36824;&#26159;&#30001;&#25972;&#20010;&#28151;&#21512;&#65288;&#22791;&#25321;&#20551;&#35774;&#65289;&#29983;&#25104;&#12290;&#22522;&#20110;&#19987;&#23478;&#20989;&#25968;&#30340;&#20195;&#25968;&#32467;&#26500;&#21644;$g_0$&#19982;&#28151;&#21512;&#37096;&#20998;&#30340;&#21487;&#21306;&#20998;&#24615;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#26032;&#30340;&#22522;&#20110;Voronoi&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25429;&#25417;c
&lt;/p&gt;
&lt;p&gt;
We consider the parameter estimation problem in the deviated Gaussian mixture of experts in which the data are generated from $(1 - \lambda^{\ast}) g_0(Y| X)+ \lambda^{\ast} \sum_{i = 1}^{k_{\ast}} p_{i}^{\ast} f(Y|(a_{i}^{\ast})^{\top}X+b_i^{\ast},\sigma_{i}^{\ast})$, where $X, Y$ are respectively a covariate vector and a response variable, $g_{0}(Y|X)$ is a known function, $\lambda^{\ast} \in [0, 1]$ is true but unknown mixing proportion, and $(p_{i}^{\ast}, a_{i}^{\ast}, b_{i}^{\ast}, \sigma_{i}^{\ast})$ for $1 \leq i \leq k^{\ast}$ are unknown parameters of the Gaussian mixture of experts. This problem arises from the goodness-of-fit test when we would like to test whether the data are generated from $g_{0}(Y|X)$ (null hypothesis) or they are generated from the whole mixture (alternative hypothesis). Based on the algebraic structure of the expert functions and the distinguishability between $g_0$ and the mixture part, we construct novel Voronoi-based loss functions to capture the c
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#22312;nnU-Net&#32593;&#32476;&#30340;&#19981;&#21516;&#37096;&#20998;&#20351;&#29992;&#33258;&#26657;&#20934;&#21367;&#31215;&#65292;&#35777;&#26126;&#20102;&#33258;&#26657;&#20934;&#27169;&#22359;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32959;&#30244;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20010;&#32959;&#30244;&#30340;&#20998;&#21106;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05218</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#26657;&#20934;&#21367;&#31215;&#30340;&#33014;&#36136;&#30244;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Self-calibrated convolution towards glioma segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05218
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#22312;nnU-Net&#32593;&#32476;&#30340;&#19981;&#21516;&#37096;&#20998;&#20351;&#29992;&#33258;&#26657;&#20934;&#21367;&#31215;&#65292;&#35777;&#26126;&#20102;&#33258;&#26657;&#20934;&#27169;&#22359;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32959;&#30244;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20010;&#32959;&#30244;&#30340;&#20998;&#21106;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30142;&#30149;&#26089;&#26399;&#65292;&#20934;&#30830;&#30340;&#33041;&#32959;&#30244;&#20998;&#21106;&#23545;&#27835;&#30103;&#30340;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#36991;&#20813;&#23545;&#22810;&#31181;&#21327;&#35758;&#65288;&#22914;T1&#12289;T2&#12289;T2-FLAIR&#12289;T1-Gd&#65289;&#30340;3D MR&#33041;&#22270;&#20687;&#36827;&#34892;&#35814;&#23613;&#30340;&#35270;&#35273;&#26816;&#26597;&#12290;&#23384;&#22312;&#35768;&#22810;&#29992;&#20110;&#33014;&#36136;&#30244;&#20998;&#21106;&#30340;&#32593;&#32476;&#65292;&#20854;&#20013;nnU-Net&#26159;&#26368;&#22909;&#30340;&#20043;&#19968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;nnU-Net&#32593;&#32476;&#30340;&#19981;&#21516;&#37096;&#20998;&#20013;&#20351;&#29992;&#33258;&#26657;&#20934;&#21367;&#31215;&#65292;&#20197;&#35777;&#26126;&#22312;&#36339;&#36291;&#36830;&#25509;&#20013;&#20351;&#29992;&#33258;&#26657;&#20934;&#27169;&#22359;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22686;&#24378;&#30244;&#28790;&#21644;&#32959;&#30244;&#26680;&#30340;&#20998;&#21106;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20010;&#32959;&#30244;&#30340;&#20998;&#21106;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate brain tumor segmentation in the early stages of the disease is crucial for the treatment's effectiveness, avoiding exhaustive visual inspection of a qualified specialist on 3D MR brain images of multiple protocols (e.g., T1, T2, T2-FLAIR, T1-Gd). Several networks exist for Glioma segmentation, being nnU-Net one of the best. In this work, we evaluate self-calibrated convolutions in different parts of the nnU-Net network to demonstrate that self-calibrated modules in skip connections can significantly improve the enhanced-tumor and tumor-core segmentation accuracy while preserving the wholetumor segmentation accuracy.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20998;&#21106;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#35299;&#21078;&#21487;&#25511;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#25513;&#27169;&#28040;&#34701;&#35757;&#32451;&#31639;&#27861;&#23454;&#29616;&#23545;&#35299;&#21078;&#32422;&#26463;&#30340;&#26465;&#20214;&#21270;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#32593;&#32476;&#23545;&#35299;&#21078;&#30495;&#23454;&#24615;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05210</link><description>&lt;p&gt;
&#37319;&#29992;&#20998;&#21106;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#35299;&#21078;&#21487;&#25511;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Anatomically-Controllable Medical Image Generation with Segmentation-Guided Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05210
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20998;&#21106;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#35299;&#21078;&#21487;&#25511;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#25513;&#27169;&#28040;&#34701;&#35757;&#32451;&#31639;&#27861;&#23454;&#29616;&#23545;&#35299;&#21078;&#32422;&#26463;&#30340;&#26465;&#20214;&#21270;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#32593;&#32476;&#23545;&#35299;&#21078;&#30495;&#23454;&#24615;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#23454;&#29616;&#20102;&#38750;&#24120;&#39640;&#36136;&#37327;&#30340;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#65292;&#21487;&#20197;&#36890;&#36807;&#20026;&#23567;&#22411;&#25110;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#25552;&#20379;&#34917;&#20805;&#65292;&#20174;&#32780;&#24110;&#21161;&#20943;&#36731;&#33719;&#21462;&#21644;&#27880;&#37322;&#26032;&#22270;&#20687;&#30340;&#36153;&#29992;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#22270;&#20687;&#26102;&#38754;&#20020;&#30528;&#20840;&#23616;&#35299;&#21078;&#30495;&#23454;&#24615;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#21078;&#21487;&#25511;&#30340;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27599;&#20010;&#37319;&#26679;&#27493;&#39588;&#20013;&#36981;&#24490;&#22810;&#31867;&#35299;&#21078;&#20998;&#21106;&#25513;&#27169;&#65292;&#24182;&#37319;&#29992;&#38543;&#26426;&#25513;&#27169;&#28040;&#34701;&#35757;&#32451;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#25152;&#36873;&#35299;&#21078;&#32422;&#26463;&#30340;&#26465;&#20214;&#21270;&#65292;&#21516;&#26102;&#20801;&#35768;&#20854;&#20182;&#35299;&#21078;&#21306;&#22495;&#30340;&#28789;&#27963;&#24615;&#12290;&#36825;&#20063;&#25913;&#21892;&#20102;&#32593;&#32476;&#22312;&#23436;&#20840;&#26080;&#26465;&#20214;&#65288;&#26080;&#32422;&#26463;&#29983;&#25104;&#65289;&#24773;&#20917;&#19979;&#23545;&#35299;&#21078;&#30495;&#23454;&#24615;&#30340;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;&#20083;&#33146;MRI&#21644;&#33145;&#37096;/&#39048;&#37096;&#21040;&#30406;&#33108;CT&#25968;&#25454;&#38598;&#30340;&#27604;&#36739;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#22312;&#35299;&#21078;&#30495;&#23454;&#24615;&#21644;&#36755;&#20837;&#25513;&#27169;&#20445;&#30495;&#24230;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have enabled remarkably high-quality medical image generation, which can help mitigate the expenses of acquiring and annotating new images by supplementing small or imbalanced datasets, along with other applications. However, these are hampered by the challenge of enforcing global anatomical realism in generated images. To this end, we propose a diffusion model for anatomically-controlled medical image generation. Our model follows a multi-class anatomical segmentation mask at each sampling step and incorporates a \textit{random mask ablation} training algorithm, to enable conditioning on a selected combination of anatomical constraints while allowing flexibility in other anatomical areas. This also improves the network's learning of anatomical realism for the completely unconditional (unconstrained generation) case. Comparative evaluation on breast MRI and abdominal/neck-to-pelvis CT datasets demonstrates superior anatomical realism and input mask faithfulness over st
&lt;/p&gt;</description></item><item><title>&#36125;&#23572;&#26364;&#31526;&#21512;&#25512;&#26029;&#65288;BCI&#65289;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#32500;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#65292;&#21033;&#29992;&#22810;&#27493;&#39044;&#27979;&#26469;&#25552;&#20379;&#26657;&#20934;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#21306;&#38388;&#12290;BCI&#22312;&#20219;&#24847;&#20998;&#24067;&#36716;&#25442;&#21644;&#26102;&#38388;&#20381;&#36182;&#24615;&#19979;&#23454;&#29616;&#20102;&#38271;&#26399;&#35206;&#30422;&#65292;&#19988;&#22312;&#27874;&#21160;&#29575;&#39044;&#27979;&#38382;&#39064;&#19978;&#29983;&#25104;&#26356;&#30701;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.05203</link><description>&lt;p&gt;
&#36125;&#23572;&#26364;&#31526;&#21512;&#25512;&#26029;&#65306;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#39044;&#27979;&#21306;&#38388;&#30340;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Bellman Conformal Inference: Calibrating Prediction Intervals For Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05203
&lt;/p&gt;
&lt;p&gt;
&#36125;&#23572;&#26364;&#31526;&#21512;&#25512;&#26029;&#65288;BCI&#65289;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#32500;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#65292;&#21033;&#29992;&#22810;&#27493;&#39044;&#27979;&#26469;&#25552;&#20379;&#26657;&#20934;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#21306;&#38388;&#12290;BCI&#22312;&#20219;&#24847;&#20998;&#24067;&#36716;&#25442;&#21644;&#26102;&#38388;&#20381;&#36182;&#24615;&#19979;&#23454;&#29616;&#20102;&#38271;&#26399;&#35206;&#30422;&#65292;&#19988;&#22312;&#27874;&#21160;&#29575;&#39044;&#27979;&#38382;&#39064;&#19978;&#29983;&#25104;&#26356;&#30701;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#36125;&#23572;&#26364;&#31526;&#21512;&#25512;&#26029;&#65288;BCI&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22260;&#32469;&#20219;&#20309;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#26657;&#20934;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;BCI&#33021;&#22815;&#21033;&#29992;&#22810;&#27493;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#35299;&#20915;&#19968;&#32500;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#65288;SCP&#65289;&#26469;&#26174;&#24335;&#20248;&#21270;&#24179;&#22343;&#21306;&#38388;&#38271;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#26469;&#25214;&#21040;SCP&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20219;&#24847;&#20998;&#24067;&#36716;&#25442;&#21644;&#26102;&#38388;&#20381;&#36182;&#24615;&#19979;&#65292;BCI&#33021;&#22815;&#23454;&#29616;&#38271;&#26399;&#35206;&#30422;&#65292;&#21363;&#20351;&#22810;&#27493;&#39044;&#27979;&#36739;&#24046;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#20013;&#21457;&#29616;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;BCI&#36991;&#20813;&#20102;&#26080;&#20449;&#24687;&#21306;&#38388;&#65288;&#38271;&#24230;&#26080;&#38480;&#65289;&#30340;&#29983;&#25104;&#65292;&#24182;&#22312;&#27874;&#21160;&#29575;&#39044;&#27979;&#38382;&#39064;&#19978;&#29983;&#25104;&#20102;&#26126;&#26174;&#26356;&#30701;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Bellman Conformal Inference (BCI), a framework that wraps around any time series forecasting models and provides calibrated prediction intervals. Unlike the existing methods, BCI is able to leverage multi-step ahead forecasts and explicitly optimize the average interval lengths by solving a one-dimensional stochastic control problem (SCP) at each time step. In particular, we use the dynamic programming algorithm to find the optimal policy for the SCP. We prove that BCI achieves long-term coverage under arbitrary distribution shifts and temporal dependence, even with poor multi-step ahead forecasts. We find empirically that BCI avoids uninformative intervals that have infinite lengths and generates substantially shorter prediction intervals on volatility forecasting problems when compared with existing methods.
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#21463;&#38480;&#65292;&#26080;&#27861;&#23454;&#29616;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#21644;&#20551;&#35774;&#27979;&#35797;&#30340;MatSci-LLMs&#26694;&#26550;&#65292;&#24182;&#25551;&#36848;&#20102;&#20851;&#38190;&#30340;&#26448;&#26009;&#31185;&#23398;&#20449;&#24687;&#25552;&#21462;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.05200</link><description>&lt;p&gt;
LLMs&#26159;&#21542;&#20934;&#22791;&#22909;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#26448;&#26009;&#21457;&#29616;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are LLMs Ready for Real-World Materials Discovery?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05200
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#21463;&#38480;&#65292;&#26080;&#27861;&#23454;&#29616;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#21644;&#20551;&#35774;&#27979;&#35797;&#30340;MatSci-LLMs&#26694;&#26550;&#65292;&#24182;&#25551;&#36848;&#20102;&#20851;&#38190;&#30340;&#26448;&#26009;&#31185;&#23398;&#20449;&#24687;&#25552;&#21462;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#24378;&#22823;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#21487;&#33021;&#24615;&#65292;&#21152;&#24555;&#20102;&#26448;&#26009;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20173;&#23384;&#22312;&#19981;&#36275;&#65292;&#26080;&#27861;&#25104;&#20026;&#23454;&#29992;&#30340;&#26448;&#26009;&#31185;&#23398;&#24037;&#20855;&#12290;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;LLMs&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#30456;&#20851;&#22833;&#36133;&#26696;&#20363;&#65292;&#25581;&#31034;&#20102;LLMs&#22312;&#29702;&#35299;&#21644;&#25512;&#29702;&#22797;&#26434;&#12289;&#30456;&#20114;&#20851;&#32852;&#30340;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#26041;&#38754;&#30340;&#29616;&#26377;&#38480;&#21046;&#12290;&#37492;&#20110;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#21457;&#22522;&#20110;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#21644;&#20551;&#35774;&#29983;&#25104;&#19982;&#27979;&#35797;&#30340;&#26448;&#26009;&#31185;&#23398;LLMs&#65288;MatSci-LLMs&#65289;&#30340;&#26694;&#26550;&#12290;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;MatSci-LLMs&#30340;&#36335;&#24452;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#24314;&#31435;&#39640;&#36136;&#37327;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#33258;&#31185;&#23398;&#25991;&#29486;&#65292;&#20854;&#20013;&#23384;&#22312;&#21508;&#31181;&#20449;&#24687;&#25552;&#21462;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20851;&#38190;&#30340;&#26448;&#26009;&#31185;&#23398;&#20449;&#24687;&#25552;&#21462;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) create exciting possibilities for powerful language processing tools to accelerate research in materials science. While LLMs have great potential to accelerate materials understanding and discovery, they currently fall short in being practical materials science tools. In this position paper, we show relevant failure cases of LLMs in materials science that reveal current limitations of LLMs related to comprehending and reasoning over complex, interconnected materials science knowledge. Given those shortcomings, we outline a framework for developing Materials Science LLMs (MatSci-LLMs) that are grounded in materials science knowledge and hypothesis generation followed by hypothesis testing. The path to attaining performant MatSci-LLMs rests in large part on building high-quality, multi-modal datasets sourced from scientific literature where various information extraction challenges persist. As such, we describe key materials science information extraction cha
&lt;/p&gt;</description></item><item><title>JAX-Fluids 2.0&#26159;&#19968;&#20010;&#38754;&#21521;&#21487;&#21387;&#32553;&#20004;&#30456;&#27969;&#30340;&#21487;&#24494;CFD&#30340;HPC&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#24615;&#33021;&#35745;&#31639;&#33021;&#21147;&#21644;&#22686;&#24378;&#30340;&#20004;&#30456;&#27969;&#24314;&#27169;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#35268;&#27169;&#21270;&#24182;&#34892;&#35745;&#31639;&#21644;&#31283;&#23450;&#30340;&#33258;&#21160;&#24494;&#20998;&#26799;&#24230;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2402.05193</link><description>&lt;p&gt;
JAX-Fluids 2.0&#65306;&#38754;&#21521;&#21487;&#21387;&#32553;&#20004;&#30456;&#27969;&#30340;&#21487;&#24494;CFD&#30340;HPC
&lt;/p&gt;
&lt;p&gt;
JAX-Fluids 2.0: Towards HPC for Differentiable CFD of Compressible Two-phase Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05193
&lt;/p&gt;
&lt;p&gt;
JAX-Fluids 2.0&#26159;&#19968;&#20010;&#38754;&#21521;&#21487;&#21387;&#32553;&#20004;&#30456;&#27969;&#30340;&#21487;&#24494;CFD&#30340;HPC&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#24615;&#33021;&#35745;&#31639;&#33021;&#21147;&#21644;&#22686;&#24378;&#30340;&#20004;&#30456;&#27969;&#24314;&#27169;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#35268;&#27169;&#21270;&#24182;&#34892;&#35745;&#31639;&#21644;&#31283;&#23450;&#30340;&#33258;&#21160;&#24494;&#20998;&#26799;&#24230;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20419;&#36827;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#65288;CFD&#65289;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#31532;&#20108;&#20010;&#29256;&#26412;&#30340;JAX-Fluids&#12290;JAX-Fluids&#26159;&#19968;&#20010;&#22522;&#20110;Python&#30340;&#23436;&#20840;&#21487;&#24494;&#30340;CFD&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#21487;&#21387;&#32553;&#30340;&#21333;&#30456;&#21644;&#20004;&#30456;&#27969;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#31532;&#19968;&#20010;&#29256;&#26412;&#65292;&#24341;&#20837;&#20102;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#21033;&#29992;JAX&#21407;&#22987;&#25805;&#20316;&#30340;&#24182;&#34892;&#21270;&#31574;&#30053;&#65292;&#22312;GPU&#65288;&#26368;&#22810;512&#20010;NVIDIA A100&#26174;&#21345;&#65289;&#21644;TPU&#65288;&#26368;&#22810;1024&#20010;TPU v3&#26680;&#24515;&#65289;HPC&#31995;&#32479;&#19978;&#39640;&#25928;&#25193;&#23637;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22312;&#25193;&#23637;&#31215;&#20998;&#36712;&#36857;&#19978;&#31283;&#23450;&#30340;&#33258;&#21160;&#24494;&#20998;&#26799;&#24230;&#30340;&#24182;&#34892;&#35745;&#31639;&#12290;&#26032;&#30340;&#20195;&#30721;&#29256;&#26412;&#25552;&#20379;&#20102;&#22686;&#24378;&#30340;&#20004;&#30456;&#27969;&#24314;&#27169;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21152;&#20837;&#20102;&#19968;&#20010;&#20116;&#26041;&#31243;&#25193;&#25955;&#30028;&#38754;&#27169;&#22411;&#65292;&#20197;&#34917;&#20805;&#27700;&#24179;&#38598;&#23574;&#38160;&#30028;&#38754;&#27169;&#22411;&#12290;&#21478;&#22806;&#30340;&#31639;&#27861;&#25913;&#36827;&#21253;&#25324;&#20445;&#25345;&#27491;&#24615;&#30340;&#38480;&#21046;&#22120;&#65292;&#20197;&#22686;&#21152;&#40065;&#26834;&#24615;&#65292;&#25903;&#25345;...
&lt;/p&gt;
&lt;p&gt;
In our effort to facilitate machine learning-assisted computational fluid dynamics (CFD), we introduce the second iteration of JAX-Fluids. JAX-Fluids is a Python-based fully-differentiable CFD solver designed for compressible single- and two-phase flows. In this work, the first version is extended to incorporate high-performance computing (HPC) capabilities. We introduce a parallelization strategy utilizing JAX primitive operations that scales efficiently on GPU (up to 512 NVIDIA A100 graphics cards) and TPU (up to 1024 TPU v3 cores) HPC systems. We further demonstrate the stable parallel computation of automatic differentiation gradients across extended integration trajectories. The new code version offers enhanced two-phase flow modeling capabilities. In particular, a five-equation diffuse-interface model is incorporated which complements the level-set sharp-interface model. Additional algorithmic improvements include positivity-preserving limiters for increased robustness, support f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#20256;&#32479;&#30340;&#38236;&#20687;&#26144;&#23556;&#36873;&#25321;&#65288;NPG&#65289;&#22312;&#26631;&#20934;&#22522;&#20934;&#29615;&#22659;&#20013;&#24120;&#24120;&#23548;&#33268;&#19981;&#29702;&#24819;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#25214;&#21040;&#20102;&#26356;&#39640;&#25928;&#30340;&#38236;&#20687;&#26144;&#23556;&#65292;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.05187</link><description>&lt;p&gt;
&#22312;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#20013;&#20803;&#23398;&#20064;&#38236;&#20687;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Meta-learning the mirror map in policy mirror descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05187
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#20256;&#32479;&#30340;&#38236;&#20687;&#26144;&#23556;&#36873;&#25321;&#65288;NPG&#65289;&#22312;&#26631;&#20934;&#22522;&#20934;&#29615;&#22659;&#20013;&#24120;&#24120;&#23548;&#33268;&#19981;&#29702;&#24819;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#25214;&#21040;&#20102;&#26356;&#39640;&#25928;&#30340;&#38236;&#20687;&#26144;&#23556;&#65292;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#65288;PMD&#65289;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#27969;&#34892;&#26694;&#26550;&#65292;&#20316;&#20026;&#19968;&#31181;&#32479;&#19968;&#35270;&#35282;&#65292;&#23427;&#21253;&#21547;&#20102;&#35768;&#22810;&#31639;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#26159;&#36890;&#36807;&#36873;&#25321;&#19968;&#20010;&#38236;&#20687;&#26144;&#23556;&#32780;&#23548;&#20986;&#30340;&#65292;&#24182;&#19988;&#20855;&#26377;&#26377;&#38480;&#26102;&#38388;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#23613;&#31649;&#23427;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#23545;PMD&#30340;&#20840;&#38754;&#28508;&#21147;&#30340;&#25506;&#32034;&#26159;&#26377;&#38480;&#30340;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#38598;&#20013;&#22312;&#19968;&#20010;&#29305;&#23450;&#30340;&#38236;&#20687;&#26144;&#23556;&#19978;&#65292;&#21363;&#36127;&#29109;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#33879;&#21517;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#65288;NPG&#65289;&#26041;&#27861;&#12290;&#30446;&#21069;&#30340;&#29702;&#35770;&#30740;&#31350;&#36824;&#19981;&#30830;&#23450;&#38236;&#20687;&#26144;&#23556;&#30340;&#36873;&#25321;&#26159;&#21542;&#20250;&#23545;PMD&#30340;&#26377;&#25928;&#24615;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#20256;&#32479;&#30340;&#38236;&#20687;&#26144;&#23556;&#36873;&#25321;&#65288;NPG&#65289;&#22312;&#20960;&#20010;&#26631;&#20934;&#22522;&#20934;&#29615;&#22659;&#20013;&#32463;&#24120;&#20135;&#29983;&#19981;&#29702;&#24819;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#24212;&#29992;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26356;&#39640;&#25928;&#30340;&#38236;&#20687;&#26144;&#23556;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#24179;&#22343;&#24615;&#33021;&#36824;&#26159;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy Mirror Descent (PMD) is a popular framework in reinforcement learning, serving as a unifying perspective that encompasses numerous algorithms. These algorithms are derived through the selection of a mirror map and enjoy finite-time convergence guarantees. Despite its popularity, the exploration of PMD's full potential is limited, with the majority of research focusing on a particular mirror map -- namely, the negative entropy -- which gives rise to the renowned Natural Policy Gradient (NPG) method. It remains uncertain from existing theoretical studies whether the choice of mirror map significantly influences PMD's efficacy. In our work, we conduct empirical investigations to show that the conventional mirror map choice (NPG) often yields less-than-optimal outcomes across several standard benchmark environments. By applying a meta-learning approach, we identify more efficient mirror maps that enhance performance, both on average and in terms of best performance achieved along th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;cecilia&#65292;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20809;&#35889;&#24314;&#27169;&#20195;&#30721;&#65292;&#29992;&#20110;&#27979;&#37327;&#23500;&#27686;&#27745;&#26579;&#30333;&#30702;&#26143;&#30340;&#37329;&#23646;&#20016;&#24230;&#12290;&#35813;&#27969;&#31243;&#36890;&#36807;&#35757;&#32451;&#22823;&#37327;&#22522;&#20110;&#38543;&#26426;&#25277;&#26679;&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#33021;&#22815;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#30340;&#30740;&#31350;&#20013;&#12290;</title><link>https://arxiv.org/abs/2402.05176</link><description>&lt;p&gt;
cecilia: &#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#29992;&#20110;&#27979;&#37327;&#23500;&#27686;&#27745;&#26579;&#30333;&#30702;&#26143;&#37329;&#23646;&#20016;&#24230;&#30340;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
cecilia: A Machine Learning-Based Pipeline for Measuring Metal Abundances of Helium-rich Polluted White Dwarfs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;cecilia&#65292;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20809;&#35889;&#24314;&#27169;&#20195;&#30721;&#65292;&#29992;&#20110;&#27979;&#37327;&#23500;&#27686;&#27745;&#26579;&#30333;&#30702;&#26143;&#30340;&#37329;&#23646;&#20016;&#24230;&#12290;&#35813;&#27969;&#31243;&#36890;&#36807;&#35757;&#32451;&#22823;&#37327;&#22522;&#20110;&#38543;&#26426;&#25277;&#26679;&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#33021;&#22815;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#30340;&#30740;&#31350;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#20013;&#65292;&#20256;&#32479;&#30340;&#27745;&#26579;&#30333;&#30702;&#26143;&#20809;&#35889;&#20998;&#26512;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;&#20102;&#20102;&#35299;&#22826;&#38451;&#31995;&#22806;&#26143;&#29699;&#22320;&#36136;&#21644;&#21270;&#23398;&#20449;&#24687;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20123;&#25216;&#26415;&#24050;&#32463;&#35777;&#26126;&#20102;&#33258;&#24049;&#30340;&#33021;&#21147;&#24182;&#20855;&#26377;&#24191;&#27867;&#30340;&#31185;&#23398;&#21457;&#29616;&#65292;&#23427;&#20204;&#20173;&#28982;&#21463;&#21040;&#25163;&#21160;&#12289;&#32791;&#26102;&#19988;&#36845;&#20195;&#24615;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#23481;&#26131;&#20986;&#29616;&#20154;&#20026;&#38169;&#35823;&#65292;&#24182;&#19988;&#38590;&#20197;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#30340;&#37329;&#23646;&#27745;&#26579;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#20171;&#32461;cecilia&#65292;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20809;&#35889;&#24314;&#27169;&#20195;&#30721;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;cecilia&#35774;&#35745;&#29992;&#20110;&#27979;&#37327;&#20013;&#28201;&#65288;10,000$\leq T_{\rm eff} \leq$20,000 K&#65289;&#12289;&#23500;&#27686;&#27745;&#26579;&#30333;&#30702;&#26143;&#30340;&#37329;&#23646;&#20016;&#24230;&#12290;&#36890;&#36807;&#23545;&#36229;&#36807;22,000&#20010;&#38543;&#26426;&#32472;&#21046;&#30340;&#22823;&#27668;&#27169;&#22411;&#21644;&#24658;&#26143;&#21442;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#27969;&#31243;&#26088;&#22312;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#21487;&#20197;&#20195;&#26367;&#20174;&#35745;&#31639;&#26114;&#36149;&#30340;&#21512;&#25104;&#20809;&#35889;&#20013;&#29983;&#25104;&#27169;&#25311;&#20809;&#35889;&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past several decades, conventional spectral analysis techniques of polluted white dwarfs have become powerful tools to learn about the geology and chemistry of extrasolar bodies. Despite their proven capabilities and extensive legacy of scientific discoveries, these techniques are however still limited by their manual, time-intensive, and iterative nature. As a result, they are susceptible to human errors and are difficult to scale up to population-wide studies of metal pollution. This paper seeks to address this problem by presenting cecilia, the first Machine Learning (ML)-powered spectral modeling code designed to measure the metal abundances of intermediate-temperature (10,000$\leq T_{\rm eff} \leq$20,000 K), Helium-rich polluted white dwarfs. Trained with more than 22,000 randomly drawn atmosphere models and stellar parameters, our pipeline aims to overcome the limitations of classical methods by replacing the generation of synthetic spectra from computationally expensive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#20542;&#21521;&#20110;&#23545;&#31216;&#25490;&#21015;&#20989;&#25968;&#65292;&#23545;&#31216;&#32676;&#30340;&#34920;&#31034;&#29702;&#35770;&#21487;&#20197;&#29992;&#20110;&#20998;&#26512;&#39044;&#27979;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#27169;&#22411;&#26469;&#35299;&#20915;&#23398;&#20064;&#26354;&#32447;&#21644;&#32593;&#32476;&#36755;&#20986;&#65292;&#24182;&#22312;&#24120;&#35265;&#35774;&#32622;&#20013;&#24471;&#20986;&#23398;&#20064;&#33021;&#21147;&#30340;&#32039;&#23494;&#36793;&#30028;&#65292;&#26368;&#21518;&#36824;&#35777;&#26126;&#20102;WikiText&#25968;&#25454;&#38598;&#20855;&#26377;&#25490;&#21015;&#23545;&#31216;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05173</link><description>&lt;p&gt;
&#25506;&#32034;Transformer&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#24046;: &#19968;&#20010;&#26469;&#33258;&#26080;&#31351;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Inductive Bias in Transformers: A View From Infinity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#20542;&#21521;&#20110;&#23545;&#31216;&#25490;&#21015;&#20989;&#25968;&#65292;&#23545;&#31216;&#32676;&#30340;&#34920;&#31034;&#29702;&#35770;&#21487;&#20197;&#29992;&#20110;&#20998;&#26512;&#39044;&#27979;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#27169;&#22411;&#26469;&#35299;&#20915;&#23398;&#20064;&#26354;&#32447;&#21644;&#32593;&#32476;&#36755;&#20986;&#65292;&#24182;&#22312;&#24120;&#35265;&#35774;&#32622;&#20013;&#24471;&#20986;&#23398;&#20064;&#33021;&#21147;&#30340;&#32039;&#23494;&#36793;&#30028;&#65292;&#26368;&#21518;&#36824;&#35777;&#26126;&#20102;WikiText&#25968;&#25454;&#38598;&#20855;&#26377;&#25490;&#21015;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#26080;&#31351;&#30340;&#36807;&#21442;&#25968;&#21270;&#39640;&#26031;&#36807;&#31243;&#26497;&#38480;&#20013;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#24182;&#25351;&#20986;Transformer&#27169;&#22411;&#22312;&#24207;&#21015;&#31354;&#38388;&#20013;&#26356;&#20542;&#21521;&#20110;&#23545;&#31216;&#25490;&#21015;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#31216;&#32676;&#30340;&#34920;&#31034;&#29702;&#35770;&#21487;&#20197;&#29992;&#20110;&#22312;&#25968;&#25454;&#38598;&#23545;token&#20043;&#38388;&#30340;&#25490;&#21015;&#20855;&#26377;&#23545;&#31216;&#24615;&#26102;&#32473;&#20986;&#23450;&#37327;&#30340;&#20998;&#26512;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;Transformer&#27169;&#22411;&#65292;&#24182;&#22312;&#26497;&#38480;&#26465;&#20214;&#19979;&#27714;&#35299;&#27169;&#22411;&#65292;&#21253;&#25324;&#23545;&#23398;&#20064;&#26354;&#32447;&#21644;&#32593;&#32476;&#36755;&#20986;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#24120;&#35265;&#30340;&#35774;&#32622;&#20013;&#65292;&#21487;&#20197;&#25512;&#23548;&#20986;&#23398;&#20064;&#33021;&#21147;&#30340;&#32039;&#23494;&#36793;&#30028;&#65292;&#20197;&#19978;&#19979;&#25991;&#38271;&#24230;&#20316;&#20026;&#20989;&#25968;&#30340;&#32553;&#25918;&#23450;&#24459;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;WikiText&#25968;&#25454;&#38598;&#30830;&#23454;&#20855;&#26377;&#19968;&#23450;&#31243;&#24230;&#30340;&#25490;&#21015;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study inductive bias in Transformers in the infinitely over-parameterized Gaussian process limit and argue transformers tend to be biased towards more permutation symmetric functions in sequence space. We show that the representation theory of the symmetric group can be used to give quantitative analytical predictions when the dataset is symmetric to permutations between tokens. We present a simplified transformer block and solve the model at the limit, including accurate predictions for the learning curves and network outputs. We show that in common setups, one can derive tight bounds in the form of a scaling law for the learnability as a function of the context length. Finally, we argue WikiText dataset, does indeed possess a degree of permutation symmetry.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#32553;&#25918;&#24459;&#30340;&#36164;&#28304;&#27169;&#22411;&#65292;&#36890;&#36807;&#35266;&#23519;&#23454;&#35777;&#21457;&#29616;&#65292;&#23376;&#20219;&#21153;&#30340;&#25439;&#22833;&#19982;&#20998;&#37197;&#30340;&#31070;&#32463;&#20803;&#25104;&#21453;&#27604;&#65292;&#22797;&#21512;&#20219;&#21153;&#20013;&#23376;&#20219;&#21153;&#33719;&#24471;&#30340;&#36164;&#28304;&#38543;&#27169;&#22411;&#21464;&#22823;&#32780;&#22686;&#38271;&#65292;&#20445;&#25345;&#36164;&#28304;&#27604;&#20363;&#19981;&#21464;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#22797;&#21512;&#20219;&#21153;&#30340;&#31070;&#32463;&#32553;&#25918;&#24459;&#65292;&#24182;&#25104;&#21151;&#22797;&#21046;&#20102;Chinchilla&#27169;&#22411;&#30340;&#31070;&#32463;&#32553;&#25918;&#24459;&#12290;&#35813;&#36164;&#28304;&#27169;&#22411;&#26159;&#34920;&#24449;&#21644;&#35786;&#26029;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2402.05164</link><description>&lt;p&gt;
&#31070;&#32463;&#32553;&#25918;&#24459;&#30340;&#36164;&#28304;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Resource Model For Neural Scaling Law
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05164
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#32553;&#25918;&#24459;&#30340;&#36164;&#28304;&#27169;&#22411;&#65292;&#36890;&#36807;&#35266;&#23519;&#23454;&#35777;&#21457;&#29616;&#65292;&#23376;&#20219;&#21153;&#30340;&#25439;&#22833;&#19982;&#20998;&#37197;&#30340;&#31070;&#32463;&#20803;&#25104;&#21453;&#27604;&#65292;&#22797;&#21512;&#20219;&#21153;&#20013;&#23376;&#20219;&#21153;&#33719;&#24471;&#30340;&#36164;&#28304;&#38543;&#27169;&#22411;&#21464;&#22823;&#32780;&#22686;&#38271;&#65292;&#20445;&#25345;&#36164;&#28304;&#27604;&#20363;&#19981;&#21464;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#22797;&#21512;&#20219;&#21153;&#30340;&#31070;&#32463;&#32553;&#25918;&#24459;&#65292;&#24182;&#25104;&#21151;&#22797;&#21046;&#20102;Chinchilla&#27169;&#22411;&#30340;&#31070;&#32463;&#32553;&#25918;&#24459;&#12290;&#35813;&#36164;&#28304;&#27169;&#22411;&#26159;&#34920;&#24449;&#21644;&#35786;&#26029;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32553;&#25918;&#24459;&#25551;&#36848;&#20102;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#22823;&#65292;&#27169;&#22411;&#24615;&#33021;&#22914;&#20309;&#25552;&#39640;&#12290;&#21463;&#23454;&#35777;&#35266;&#23519;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#32553;&#25918;&#30340;&#36164;&#28304;&#27169;&#22411;&#12290;&#19968;&#20010;&#20219;&#21153;&#36890;&#24120;&#26159;&#22797;&#21512;&#20219;&#21153;&#65292;&#21487;&#20197;&#20998;&#35299;&#20026;&#35768;&#22810;&#23376;&#20219;&#21153;&#65292;&#36825;&#20123;&#23376;&#20219;&#21153;&#31454;&#20105;&#36164;&#28304;&#65288;&#20197;&#20998;&#37197;&#32473;&#23376;&#20219;&#21153;&#30340;&#31070;&#32463;&#20803;&#25968;&#37327;&#26469;&#34913;&#37327;&#65289;&#12290;&#22312;&#29609;&#20855;&#38382;&#39064;&#19978;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#23454;&#65306;&#65288;1&#65289;&#23376;&#20219;&#21153;&#30340;&#25439;&#22833;&#19982;&#20854;&#20998;&#37197;&#30340;&#31070;&#32463;&#20803;&#25104;&#21453;&#27604;&#12290;&#65288;2&#65289;&#24403;&#22797;&#21512;&#20219;&#21153;&#20013;&#23384;&#22312;&#22810;&#20010;&#23376;&#20219;&#21153;&#26102;&#65292;&#38543;&#30528;&#27169;&#22411;&#21464;&#22823;&#65292;&#27599;&#20010;&#23376;&#20219;&#21153;&#33719;&#24471;&#30340;&#36164;&#28304;&#22343;&#21248;&#22686;&#38271;&#65292;&#20445;&#25345;&#33719;&#24471;&#36164;&#28304;&#30340;&#27604;&#20363;&#19981;&#21464;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#20123;&#21457;&#29616;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#39044;&#27979;&#19968;&#33324;&#22797;&#21512;&#20219;&#21153;&#30340;&#31070;&#32463;&#32553;&#25918;&#24459;&#65292;&#24182;&#25104;&#21151;&#22797;&#21046;&#20102;arXiv:2203.15556&#20013;&#25253;&#21578;&#30340;Chinchilla&#27169;&#22411;&#30340;&#31070;&#32463;&#32553;&#25918;&#24459;&#12290;&#25105;&#20204;&#30456;&#20449;&#26412;&#25991;&#20013;&#20351;&#29992;&#30340;&#36164;&#28304;&#27010;&#24565;&#23558;&#26159;&#34920;&#24449;&#21644;&#35786;&#26029;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural scaling laws characterize how model performance improves as the model size scales up. Inspired by empirical observations, we introduce a resource model of neural scaling. A task is usually composite hence can be decomposed into many subtasks, which compete for resources (measured by the number of neurons allocated to subtasks). On toy problems, we empirically find that: (1) The loss of a subtask is inversely proportional to its allocated neurons. (2) When multiple subtasks are present in a composite task, the resources acquired by each subtask uniformly grow as models get larger, keeping the ratios of acquired resources constants. We hypothesize these findings to be generally true and build a model to predict neural scaling laws for general composite tasks, which successfully replicates the neural scaling law of Chinchilla models reported in arXiv:2203.15556. We believe that the notion of resource used in this paper will be a useful tool for characterizing and diagnosing neural 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20462;&#21098;&#21644;&#20302;&#31209;&#20462;&#25913;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#26426;&#21046;&#22266;&#26377;&#26131;&#30862;&#24615;&#65292;&#21435;&#38500;&#23433;&#20840;&#20851;&#38190;&#21306;&#22495;&#20250;&#25439;&#23475;&#23433;&#20840;&#24615;&#65292;&#20294;&#23545;&#25928;&#29992;&#24433;&#21709;&#19981;&#22823;&#65292;&#38656;&#35201;&#26356;&#24378;&#20581;&#30340;&#23433;&#20840;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.05162</link><description>&lt;p&gt;
&#36890;&#36807;&#20462;&#21098;&#21644;&#20302;&#31209;&#20462;&#25913;&#35780;&#20272;&#23433;&#20840;&#23545;&#40784;&#30340;&#26131;&#30862;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20462;&#21098;&#21644;&#20302;&#31209;&#20462;&#25913;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#26426;&#21046;&#22266;&#26377;&#26131;&#30862;&#24615;&#65292;&#21435;&#38500;&#23433;&#20840;&#20851;&#38190;&#21306;&#22495;&#20250;&#25439;&#23475;&#23433;&#20840;&#24615;&#65292;&#20294;&#23545;&#25928;&#29992;&#24433;&#21709;&#19981;&#22823;&#65292;&#38656;&#35201;&#26356;&#24378;&#20581;&#30340;&#23433;&#20840;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20854;&#23433;&#20840;&#26426;&#21046;&#26041;&#38754;&#34920;&#29616;&#20986;&#22266;&#26377;&#30340;&#26131;&#30862;&#24615;&#65292;&#36825;&#21487;&#20174;&#23427;&#20204;&#26131;&#21463;&#36234;&#29425;&#21644;&#21363;&#20351;&#26159;&#38750;&#24694;&#24847;&#24494;&#35843;&#20063;&#26131;&#21463;&#24433;&#21709;&#26469;&#35828;&#26126;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20462;&#21098;&#21644;&#20302;&#31209;&#20462;&#25913;&#25506;&#35752;&#20102;&#23433;&#20840;&#23545;&#40784;&#30340;&#26131;&#30862;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#26041;&#27861;&#65292;&#33021;&#22815;&#35782;&#21035;&#23545;&#20110;&#23433;&#20840;&#38450;&#25252;&#33267;&#20851;&#37325;&#35201;&#65292;&#19988;&#22312;&#31070;&#32463;&#20803;&#21644;&#31209;&#32423;&#21035;&#19978;&#19982;&#25928;&#29992;&#30456;&#20851;&#30340;&#21306;&#22495;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#30340;&#23396;&#31435;&#21306;&#22495;&#26159;&#31232;&#30095;&#30340;&#65292;&#32422;&#21344;&#21442;&#25968;&#32423;&#21035;&#30340;$3\%$&#21644;&#25490;&#21517;&#32423;&#21035;&#30340;$2.5\%$&#12290;&#21435;&#38500;&#36825;&#20123;&#21306;&#22495;&#20250;&#25439;&#23475;&#23433;&#20840;&#24615;&#65292;&#32780;&#23545;&#25928;&#29992;&#30340;&#24433;&#21709;&#19981;&#22823;&#65292;&#20174;&#32780;&#35777;&#23454;&#20102;&#35813;&#27169;&#22411;&#23433;&#20840;&#26426;&#21046;&#30340;&#22266;&#26377;&#26131;&#30862;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#21363;&#20351;&#38480;&#21046;&#23545;&#23433;&#20840;&#20851;&#38190;&#21306;&#22495;&#36827;&#34892;&#20462;&#25913;&#65292;LLMs&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#20302;&#25104;&#26412;&#30340;&#24494;&#35843;&#25915;&#20987;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;LLMs&#20013;&#26356;&#24378;&#22823;&#30340;&#23433;&#20840;&#31574;&#30053;&#30340;&#32039;&#36843;&#24615;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning. This study explores this brittleness of safety alignment by leveraging pruning and low-rank modifications. We develop methods to identify critical regions that are vital for safety guardrails, and that are disentangled from utility-relevant regions at both the neuron and rank levels. Surprisingly, the isolated regions we find are sparse, comprising about $3\%$ at the parameter level and $2.5\%$ at the rank level. Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model's safety mechanisms. Moreover, we show that LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications to the safety-critical regions are restricted. These findings underscore the urgent need for more robust safety strategies in LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;Hugging Face&#24179;&#21488;&#19978;&#30340;32,111&#20221;AI&#27169;&#22411;&#25991;&#26723;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#25552;&#20379;&#20102;&#27169;&#22411;&#21345;&#65292;&#20294;&#20449;&#24687;&#37327;&#19981;&#19968;&#33268;&#12290;&#26377;&#20851;&#29615;&#22659;&#24433;&#21709;&#12289;&#38480;&#21046;&#21644;&#35780;&#20272;&#30340;&#37096;&#20998;&#22635;&#20889;&#29575;&#26368;&#20302;&#65292;&#35757;&#32451;&#37096;&#20998;&#21017;&#22635;&#20889;&#29575;&#26368;&#39640;&#12290;</title><link>https://arxiv.org/abs/2402.05160</link><description>&lt;p&gt;
AI&#30340;&#25991;&#26723;&#21270;&#24773;&#20917;&#22914;&#20309;&#65311;&#23545;32,000&#20221;AI&#27169;&#22411;&#21345;&#36827;&#34892;&#31995;&#32479;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
What's documented in AI? Systematic Analysis of 32K AI Model Cards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;Hugging Face&#24179;&#21488;&#19978;&#30340;32,111&#20221;AI&#27169;&#22411;&#25991;&#26723;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#25552;&#20379;&#20102;&#27169;&#22411;&#21345;&#65292;&#20294;&#20449;&#24687;&#37327;&#19981;&#19968;&#33268;&#12290;&#26377;&#20851;&#29615;&#22659;&#24433;&#21709;&#12289;&#38480;&#21046;&#21644;&#35780;&#20272;&#30340;&#37096;&#20998;&#22635;&#20889;&#29575;&#26368;&#20302;&#65292;&#35757;&#32451;&#37096;&#20998;&#21017;&#22635;&#20889;&#29575;&#26368;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#27169;&#22411;&#30340;&#36805;&#36895;&#22686;&#21152;&#31361;&#26174;&#20102;&#20805;&#20998;&#30340;&#25991;&#26723;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#22240;&#20026;&#36825;&#26679;&#21487;&#20197;&#20351;&#29992;&#25143;&#20102;&#35299;&#12289;&#20449;&#20219;&#24182;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#12290;&#34429;&#28982;&#24320;&#21457;&#32773;&#40723;&#21169;&#21046;&#20316;&#27169;&#22411;&#21345;&#65292;&#20294;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#36825;&#20123;&#21345;&#21253;&#21547;&#22810;&#23569;&#20449;&#24687;&#25110;&#32773;&#21253;&#21547;&#21738;&#20123;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;Hugging Face&#24179;&#21488;&#19978;&#30340;32,111&#20221;AI&#27169;&#22411;&#25991;&#26723;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#35813;&#24179;&#21488;&#26159;&#20998;&#21457;&#21644;&#37096;&#32626;AI&#27169;&#22411;&#30340;&#39046;&#20808;&#24179;&#21488;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#26222;&#36941;&#30340;&#27169;&#22411;&#21345;&#25991;&#26723;&#21270;&#23454;&#36341;&#12290;&#22823;&#22810;&#25968;&#19979;&#36733;&#37327;&#36739;&#22823;&#30340;AI&#27169;&#22411;&#25552;&#20379;&#20102;&#27169;&#22411;&#21345;&#65292;&#20294;&#36825;&#20123;&#21345;&#30340;&#20449;&#24687;&#37327;&#19981;&#19968;&#33268;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26377;&#20851;&#29615;&#22659;&#24433;&#21709;&#12289;&#38480;&#21046;&#21644;&#35780;&#20272;&#30340;&#37096;&#20998;&#22635;&#20889;&#29575;&#26368;&#20302;&#65292;&#32780;&#35757;&#32451;&#37096;&#20998;&#21017;&#26159;&#22635;&#20889;&#24471;&#26368;&#20840;&#38754;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#23545;&#27599;&#20010;&#37096;&#20998;&#30340;&#20869;&#23481;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#20102;&#35299;&#20174;&#19994;&#32773;&#30340;&#37325;&#28857;&#20851;&#27880;&#20869;&#23481;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26377;&#30456;&#24403;&#22810;&#30340;&#27169;&#22411;&#21345;&#22312;&#30456;&#20851;&#37096;&#20998;&#23384;&#22312;&#36739;&#22823;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid proliferation of AI models has underscored the importance of thorough documentation, as it enables users to understand, trust, and effectively utilize these models in various applications. Although developers are encouraged to produce model cards, it's not clear how much information or what information these cards contain. In this study, we conduct a comprehensive analysis of 32,111 AI model documentations on Hugging Face, a leading platform for distributing and deploying AI models. Our investigation sheds light on the prevailing model card documentation practices. Most of the AI models with substantial downloads provide model cards, though the cards have uneven informativeness. We find that sections addressing environmental impact, limitations, and evaluation exhibit the lowest filled-out rates, while the training section is the most consistently filled-out. We analyze the content of each section to characterize practitioners' priorities. Interestingly, there are substantial
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23391;&#21152;&#25289;OCR&#31995;&#32479;&#65292;&#20855;&#26377;&#37325;&#26500;&#25991;&#26723;&#24067;&#23616;&#12289;&#31934;&#30830;&#25552;&#21462;&#12289;&#22810;&#26679;&#21270;&#25991;&#26723;&#31867;&#22411;&#25903;&#25345;&#21644;&#20248;&#21270;&#23383;&#31526;&#19982;&#21333;&#35789;&#35782;&#21035;&#30340;&#29305;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.05158</link><description>&lt;p&gt;
&#25552;&#39640;&#23391;&#21152;&#25289;OCR&#30340;&#19987;&#29992;&#27169;&#22411;&#21644;&#20808;&#36827;&#25216;&#26415;&#22312;&#19981;&#21516;&#31867;&#22411;&#25991;&#26723;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Enhancement of Bengali OCR by Specialized Models and Advanced Techniques for Diverse Document Types
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23391;&#21152;&#25289;OCR&#31995;&#32479;&#65292;&#20855;&#26377;&#37325;&#26500;&#25991;&#26723;&#24067;&#23616;&#12289;&#31934;&#30830;&#25552;&#21462;&#12289;&#22810;&#26679;&#21270;&#25991;&#26723;&#31867;&#22411;&#25903;&#25345;&#21644;&#20248;&#21270;&#23383;&#31526;&#19982;&#21333;&#35789;&#35782;&#21035;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#23391;&#21152;&#25289;OCR&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20855;&#26377;&#19968;&#20123;&#33021;&#21147;&#12290;&#35813;&#31995;&#32479;&#22312;&#20445;&#30041;&#32467;&#26500;&#12289;&#23545;&#40784;&#21644;&#22270;&#20687;&#30340;&#21516;&#26102;&#65292;&#20248;&#31168;&#22320;&#37325;&#26500;&#25991;&#26723;&#24067;&#23616;&#12290;&#23427;&#32467;&#21512;&#20102;&#20808;&#36827;&#30340;&#22270;&#20687;&#21644;&#31614;&#21517;&#26816;&#27979;&#25216;&#26415;&#65292;&#20197;&#36827;&#34892;&#31934;&#30830;&#30340;&#25552;&#21462;&#12290;&#38024;&#23545;&#21253;&#25324;&#35745;&#31639;&#26426;&#25490;&#29256;&#12289;&#20984;&#29256;&#21360;&#21047;&#12289;&#25171;&#23383;&#26426;&#21644;&#25163;&#20889;&#25991;&#26723;&#22312;&#20869;&#30340;&#19981;&#21516;&#31867;&#22411;&#25991;&#26723;&#65292;&#35813;&#31995;&#32479;&#36824;&#21253;&#25324;&#20102;&#19987;&#38376;&#30340;&#21333;&#35789;&#20998;&#21106;&#27169;&#22411;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#22788;&#29702;&#38745;&#24577;&#21644;&#21160;&#24577;&#25163;&#20889;&#36755;&#20837;&#65292;&#24182;&#35782;&#21035;&#21508;&#31181;&#20070;&#20889;&#39118;&#26684;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#33021;&#22815;&#35782;&#21035;&#23391;&#21152;&#25289;&#22797;&#21512;&#23383;&#31526;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#25968;&#25454;&#25910;&#38598;&#24037;&#20316;&#65292;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#30340;&#35821;&#26009;&#24211;&#65292;&#32780;&#20808;&#36827;&#30340;&#25216;&#26415;&#32452;&#20214;&#21017;&#20248;&#21270;&#20102;&#23383;&#31526;&#21644;&#21333;&#35789;&#35782;&#21035;&#12290;&#20854;&#20182;&#36129;&#29486;&#21253;&#25324;&#22270;&#20687;&#12289;&#26631;&#24535;&#12289;&#31614;&#21517;&#21644;&#34920;&#26684;&#35782;&#21035;&#12289;&#36879;&#35270;&#26657;&#27491;&#12289;&#24067;&#23616;&#37325;&#24314;&#20197;&#21450;&#29992;&#20110;&#39640;&#25928;&#21487;&#25193;&#23637;&#22788;&#29702;&#30340;&#25490;&#38431;&#27169;&#22359;&#12290;&#35813;&#31995;&#32479;&#22312;&#39640;&#25928;&#20934;&#30830;&#30340;&#25991;&#26412;&#25552;&#21462;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper presents a unique Bengali OCR system with some capabilities. The system excels in reconstructing document layouts while preserving structure, alignment, and images. It incorporates advanced image and signature detection for accurate extraction. Specialized models for word segmentation cater to diverse document types, including computer-composed, letterpress, typewriter, and handwritten documents. The system handles static and dynamic handwritten inputs, recognizing various writing styles. Furthermore, it has the ability to recognize compound characters in Bengali. Extensive data collection efforts provide a diverse corpus, while advanced technical components optimize character and word recognition. Additional contributions include image, logo, signature and table recognition, perspective correction, layout reconstruction, and a queuing module for efficient and scalable processing. The system demonstrates outstanding performance in efficient and accurate text extract
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#22312;&#27973;&#23618;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#19981;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35813;&#38382;&#39064;&#30340;&#23616;&#37096;&#26368;&#23567;&#35299;&#26500;&#36896;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05155</link><description>&lt;p&gt;
Adam&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22312;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#38750;&#25910;&#25947;&#24615;&#21450;&#23616;&#37096;&#26368;&#23567;&#35299;&#26500;&#36896;
&lt;/p&gt;
&lt;p&gt;
Non-convergence to global minimizers for Adam and stochastic gradient descent optimization and constructions of local minimizers in the training of artificial neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#22312;&#27973;&#23618;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#19981;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35813;&#38382;&#39064;&#30340;&#23616;&#37096;&#26368;&#23567;&#35299;&#26500;&#36896;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20248;&#21270;&#26041;&#27861;&#65292;&#22914;&#26222;&#36890;&#30340;SGD&#26041;&#27861;&#21644;&#27969;&#34892;&#30340;Adam&#20248;&#21270;&#22120;&#65292;&#29616;&#22312;&#26159;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#35757;&#32451;&#30340;&#39318;&#36873;&#26041;&#27861;&#12290;&#23613;&#31649;SGD&#26041;&#27861;&#22312;&#25968;&#20540;&#27169;&#25311;&#20013;&#22312;ANN&#35757;&#32451;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#25152;&#26377;&#23454;&#38469;&#30456;&#20851;&#30340;&#22330;&#26223;&#20013;&#65292;&#20005;&#26684;&#35299;&#37322;SGD&#26041;&#27861;&#20026;&#20309;&#25104;&#21151;&#35757;&#32451;ANNs&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#29305;&#21035;&#26159;&#22312;&#22823;&#22810;&#25968;&#23454;&#38469;&#30456;&#20851;&#30340;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;SGD&#26041;&#27861;&#20284;&#20046;&#20197;&#39640;&#27010;&#29575;&#19981;&#25910;&#25947;&#20110;ANN&#35757;&#32451;&#38382;&#39064;&#30340;&#20248;&#21270;&#31354;&#38388;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;&#28982;&#32780;&#65292;&#35777;&#26126;SGD&#26041;&#27861;&#19981;&#20250;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#23567;&#20540;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#30740;&#31350;&#38382;&#39064;&#65292;&#22312;&#20855;&#26377;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#21644;&#30456;&#20851;&#28608;&#27963;&#20989;&#25968;&#20197;&#21450;&#26631;&#20934;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#30340;&#27973;&#23618;ANNs&#35757;&#32451;&#20013;&#65292;&#25512;&#32763;&#20102;&#36825;&#31181;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic gradient descent (SGD) optimization methods such as the plain vanilla SGD method and the popular Adam optimizer are nowadays the method of choice in the training of artificial neural networks (ANNs). Despite the remarkable success of SGD methods in the ANN training in numerical simulations, it remains in essentially all practical relevant scenarios an open problem to rigorously explain why SGD methods seem to succeed to train ANNs. In particular, in most practically relevant supervised learning problems, it seems that SGD methods do with high probability not converge to global minimizers in the optimization landscape of the ANN training problem. Nevertheless, it remains an open problem of research to disprove the convergence of SGD methods to global minimizers. In this work we solve this research problem in the situation of shallow ANNs with the rectified linear unit (ReLU) and related activations with the standard mean square error loss by disproving in the training of such
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#24320;&#25918;&#25968;&#25454;&#20013;&#30340;&#36947;&#36335;&#32593;&#32476;&#21644;&#36215;&#22987;&#20301;&#32622;-&#30446;&#30340;&#22320;&#27969;&#37327;&#25968;&#25454;&#65292;&#32467;&#21512;&#23618;&#27425;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#31639;&#20844;&#36335;&#20132;&#36890;&#30899;&#25490;&#25918;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#20013;&#25968;&#25454;&#25910;&#38598;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05153</link><description>&lt;p&gt;
&#20174;&#36947;&#36335;&#32593;&#32476;&#21644;&#36215;&#22987;&#20301;&#32622;-&#30446;&#30340;&#22320;&#27969;&#37327;&#25968;&#25454;&#30340;&#24320;&#25918;&#25968;&#25454;&#20272;&#31639;&#20844;&#36335;&#20132;&#36890;&#30899;&#25490;&#25918;
&lt;/p&gt;
&lt;p&gt;
Estimating On-road Transportation Carbon Emissions from Open Data of Road Network and Origin-destination Flow Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05153
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#24320;&#25918;&#25968;&#25454;&#20013;&#30340;&#36947;&#36335;&#32593;&#32476;&#21644;&#36215;&#22987;&#20301;&#32622;-&#30446;&#30340;&#22320;&#27969;&#37327;&#25968;&#25454;&#65292;&#32467;&#21512;&#23618;&#27425;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#31639;&#20844;&#36335;&#20132;&#36890;&#30899;&#25490;&#25918;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#20013;&#25968;&#25454;&#25910;&#38598;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#30830;&#20272;&#35745;&#20844;&#36335;&#20132;&#36890;&#30340;&#30899;&#25490;&#25918;&#26159;&#30899;&#25490;&#25918;&#30417;&#27979;&#21644;&#39640;&#25928;&#20943;&#25490;&#25919;&#31574;&#21046;&#23450;&#30340;&#20851;&#38190;&#65292;&#21344;&#24635;&#30899;&#25490;&#25918;&#37327;&#30340;20%&#20197;&#19978;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20272;&#31639;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#38590;&#20197;&#25910;&#38598;&#30340;&#20010;&#21035;&#36710;&#36742;&#34892;&#39542;&#37324;&#31243;&#30340;&#32479;&#35745;&#25968;&#25454;&#26469;&#35745;&#31639;&#25490;&#25918;&#37327;&#65292;&#22240;&#27492;&#25968;&#25454;&#25910;&#38598;&#22256;&#38590;&#36739;&#22823;&#12290;&#20026;&#20102;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#24378;&#22823;&#27169;&#24335;&#35782;&#21035;&#33021;&#21147;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32467;&#21512;&#20004;&#20010;&#20195;&#34920;&#20132;&#36890;&#38656;&#27714;&#21644;&#23481;&#37327;&#22240;&#32032;&#30340;&#24320;&#25918;&#25968;&#25454;&#28304;&#65292;&#21363;&#36215;&#22987;&#20301;&#32622;-&#30446;&#30340;&#22320;&#65288;OD&#65289;&#27969;&#37327;&#25968;&#25454;&#21644;&#36947;&#36335;&#32593;&#32476;&#25968;&#25454;&#65292;&#26500;&#24314;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#24322;&#26500;&#22270;&#23398;&#20064;&#30340;&#20844;&#36335;&#30899;&#25490;&#25918;&#20272;&#31639;&#26041;&#27861;&#65288;HENCE&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#25324;&#36947;&#36335;&#32593;&#32476;&#32423;&#21035;&#12289;&#31038;&#21306;&#32423;&#21035;&#21644;&#21306;&#22495;&#32423;&#21035;&#30340;&#20998;&#23618;&#22270;&#65292;&#20197;&#24314;&#27169;&#22810;&#23610;&#24230;&#22522;&#20110;&#36947;&#36335;&#32593;&#32476;&#30340;&#36830;&#25509;&#24615;&#21644;&#31354;&#38388;&#20043;&#38388;&#30340;&#26053;&#34892;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accounting for over 20% of the total carbon emissions, the precise estimation of on-road transportation carbon emissions is crucial for carbon emission monitoring and efficient mitigation policy formulation. However, existing estimation methods typically depend on hard-to-collect individual statistics of vehicle miles traveled to calculate emissions, thereby suffering from high data collection difficulty. To relieve this issue by utilizing the strong pattern recognition of artificial intelligence, we incorporate two sources of open data representative of the transportation demand and capacity factors, the origin-destination (OD) flow data and the road network data, to build a hierarchical heterogeneous graph learning method for on-road carbon emission estimation (HENCE). Specifically, a hierarchical graph consisting of the road network level, community level, and region level is constructed to model the multi-scale road network-based connectivity and travel connection between spatial a
&lt;/p&gt;</description></item><item><title>CrashFormer&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#26550;&#26500;&#65292;&#21033;&#29992;&#20840;&#38754;&#30340;&#36755;&#20837;&#25968;&#25454;&#65288;&#22914;&#20107;&#25925;&#21382;&#21490;&#12289;&#22825;&#27668;&#20449;&#24687;&#12289;&#22320;&#22270;&#22270;&#20687;&#21644;&#20154;&#21475;&#20449;&#24687;&#65289;&#65292;&#21487;&#20197;&#27599;6&#23567;&#26102;&#39044;&#27979;5.161&#24179;&#26041;&#20844;&#37324;&#22320;&#29702;&#33539;&#22260;&#20869;&#30340;&#26410;&#26469;&#20107;&#25925;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2402.05151</link><description>&lt;p&gt;
CrashFormer: &#19968;&#31181;&#22810;&#27169;&#24577;&#26550;&#26500;&#29992;&#20110;&#39044;&#27979;&#20107;&#25925;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
CrashFormer: A Multimodal Architecture to Predict the Risk of Crash
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05151
&lt;/p&gt;
&lt;p&gt;
CrashFormer&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#26550;&#26500;&#65292;&#21033;&#29992;&#20840;&#38754;&#30340;&#36755;&#20837;&#25968;&#25454;&#65288;&#22914;&#20107;&#25925;&#21382;&#21490;&#12289;&#22825;&#27668;&#20449;&#24687;&#12289;&#22320;&#22270;&#22270;&#20687;&#21644;&#20154;&#21475;&#20449;&#24687;&#65289;&#65292;&#21487;&#20197;&#27599;6&#23567;&#26102;&#39044;&#27979;5.161&#24179;&#26041;&#20844;&#37324;&#22320;&#29702;&#33539;&#22260;&#20869;&#30340;&#26410;&#26469;&#20107;&#25925;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#20132;&#36890;&#20107;&#25925;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#20840;&#29699;&#20844;&#20849;&#23433;&#20840;&#38382;&#39064;&#12290;&#20107;&#25925;&#39044;&#27979;&#23545;&#20110;&#25552;&#39640;&#20132;&#36890;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#22312;&#20107;&#25925;&#21457;&#29983;&#20043;&#21069;&#37319;&#21462;&#31215;&#26497;&#30340;&#25514;&#26045;&#65292;&#24182;&#25552;&#20379;&#23433;&#20840;&#25919;&#31574;&#12289;&#27861;&#35268;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#25514;&#26045;&#30340;&#20449;&#24687;&#12290;&#23613;&#31649;&#36807;&#21435;&#20960;&#21313;&#24180;&#36827;&#34892;&#20102;&#35768;&#22810;&#20851;&#20110;&#20107;&#25925;&#39044;&#27979;&#30340;&#30740;&#31350;&#65292;&#20294;&#30001;&#20110;&#36755;&#20837;&#25968;&#25454;&#25110;&#38382;&#39064;&#24418;&#24335;&#30340;&#38480;&#21046;&#65292;&#35768;&#22810;&#30740;&#31350;&#22312;&#21487;&#25512;&#24191;&#24615;&#12289;&#21487;&#37325;&#29616;&#24615;&#25110;&#23454;&#38469;&#24212;&#29992;&#19978;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#30340;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CrashFormer&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#26550;&#26500;&#65292;&#21033;&#29992;&#20840;&#38754;&#65288;&#20294;&#30456;&#23545;&#23481;&#26131;&#33719;&#21462;&#65289;&#30340;&#36755;&#20837;&#25968;&#25454;&#65292;&#22914;&#20107;&#25925;&#21382;&#21490;&#12289;&#22825;&#27668;&#20449;&#24687;&#12289;&#22320;&#22270;&#22270;&#20687;&#21644;&#20154;&#21475;&#20449;&#24687;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20197;&#30456;&#23545;&#21487;&#25509;&#21463;&#30340;&#39057;&#29575;&#65288;&#21363;&#27599;6&#23567;&#26102;&#65289;&#39044;&#27979;5.161&#24179;&#26041;&#20844;&#37324;&#22320;&#29702;&#33539;&#22260;&#20869;&#30340;&#26410;&#26469;&#20107;&#25925;&#39118;&#38505;&#12290;CrashFormer&#30001;&#20116;&#20010;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#24207;&#21015;&#32534;&#30721;&#22120;&#29992;&#20110;&#21033;&#29992;&#21382;&#21490;&#20107;&#25925;&#21644;&#22825;&#27668;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reducing traffic accidents is a crucial global public safety concern. Accident prediction is key to improving traffic safety, enabling proactive measures to be taken before a crash occurs, and informing safety policies, regulations, and targeted interventions. Despite numerous studies on accident prediction over the past decades, many have limitations in terms of generalizability, reproducibility, or feasibility for practical use due to input data or problem formulation. To address existing shortcomings, we propose CrashFormer, a multi-modal architecture that utilizes comprehensive (but relatively easy to obtain) inputs such as the history of accidents, weather information, map images, and demographic information. The model predicts the future risk of accidents on a reasonably acceptable cadence (i.e., every six hours) for a geographical location of 5.161 square kilometers. CrashFormer is composed of five components: a sequential encoder to utilize historical accidents and weather data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#23545;&#21496;&#26426;&#24847;&#22270;&#35782;&#21035;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#24212;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#36827;&#34892;&#25506;&#32034;&#65292;&#20026;&#20855;&#26377;&#26377;&#38480;&#35745;&#31639;&#33021;&#21147;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2402.05150</link><description>&lt;p&gt;
&#35774;&#35745;&#29992;&#20110;&#21496;&#26426;&#24847;&#22270;&#35782;&#21035;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Designing deep neural networks for driver intention recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#23545;&#21496;&#26426;&#24847;&#22270;&#35782;&#21035;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#24212;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#36827;&#34892;&#25506;&#32034;&#65292;&#20026;&#20855;&#26377;&#26377;&#38480;&#35745;&#31639;&#33021;&#21147;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21496;&#26426;&#24847;&#22270;&#35782;&#21035;&#30740;&#31350;&#36234;&#26469;&#36234;&#22810;&#22320;&#20381;&#36182;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20248;&#24615;&#33021;&#65292;&#20294;&#24120;&#24120;&#19981;&#24120;&#35265;&#22320;&#23545;&#32593;&#32476;&#26550;&#26500;&#30340;&#22797;&#26434;&#24615;&#21644;&#24615;&#33021;&#36827;&#34892;&#26174;&#24335;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24212;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26469;&#30740;&#31350;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#23545;&#29616;&#23454;&#19990;&#30028;&#20013;&#20855;&#26377;&#26377;&#38480;&#35745;&#31639;&#33021;&#21147;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20010;&#39044;&#23450;&#20041;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#20854;&#20013;&#21253;&#21547;&#19977;&#31181;&#33021;&#22815;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23618;&#31867;&#22411;&#65288;&#38271;&#30701;&#26102;&#35760;&#24518;&#12289;&#26102;&#38388;&#21367;&#31215;&#21644;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#23618;&#65289;&#65292;&#20197;&#21450;&#19981;&#21516;&#25968;&#25454;&#34701;&#21512;&#31574;&#30053;&#23545;&#21496;&#26426;&#24847;&#22270;&#35782;&#21035;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#23545;&#20110;&#20004;&#20010;&#21496;&#26426;&#24847;&#22270;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20843;&#31181;&#25628;&#32034;&#31574;&#30053;&#12290;&#23545;&#20110;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27809;&#26377;&#26126;&#26174;&#30340;&#25628;&#32034;&#31574;&#30053;&#27604;&#36739;&#22909;&#22320;&#37319;&#26679;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driver intention recognition studies increasingly rely on deep neural networks. Deep neural networks have achieved top performance for many different tasks, but it is not a common practice to explicitly analyse the complexity and performance of the network's architecture. Therefore, this paper applies neural architecture search to investigate the effects of the deep neural network architecture on a real-world safety critical application with limited computational capabilities. We explore a pre-defined search space for three deep neural network layer types that are capable to handle sequential data (a long-short term memory, temporal convolution, and a time-series transformer layer), and the influence of different data fusion strategies on the driver intention recognition performance. A set of eight search strategies are evaluated for two driver intention recognition datasets. For the two datasets, we observed that there is no search strategy clearly sampling better deep neural network 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#27491;&#21017;&#21270;&#27969;&#30340;&#21160;&#20316;&#32422;&#26463;&#31574;&#30053;&#26799;&#24230;&#65288;FlowPG&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#21160;&#20316;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#21487;&#36870;&#26144;&#23556;&#21644;&#24320;&#21457;&#22810;&#31181;&#21160;&#20316;&#37319;&#26679;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22312;&#27599;&#20010;&#24378;&#21270;&#23398;&#20064;&#27493;&#39588;&#20013;&#30830;&#20445;&#20195;&#29702;&#37319;&#21462;&#21512;&#29702;&#21160;&#20316;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05149</link><description>&lt;p&gt;
FlowPG: &#20351;&#29992;&#27491;&#21017;&#21270;&#27969;&#30340;&#21160;&#20316;&#32422;&#26463;&#31574;&#30053;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
FlowPG: Action-constrained Policy Gradient with Normalizing Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#27491;&#21017;&#21270;&#27969;&#30340;&#21160;&#20316;&#32422;&#26463;&#31574;&#30053;&#26799;&#24230;&#65288;FlowPG&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#21160;&#20316;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#21487;&#36870;&#26144;&#23556;&#21644;&#24320;&#21457;&#22810;&#31181;&#21160;&#20316;&#37319;&#26679;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22312;&#27599;&#20010;&#24378;&#21270;&#23398;&#20064;&#27493;&#39588;&#20013;&#30830;&#20445;&#20195;&#29702;&#37319;&#21462;&#21512;&#29702;&#21160;&#20316;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#20316;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#65288;ACRL&#65289;&#26159;&#35299;&#20915;&#23433;&#20840;&#20851;&#38190;&#21644;&#36164;&#28304;&#20998;&#37197;&#30456;&#20851;&#20915;&#31574;&#38382;&#39064;&#30340;&#24120;&#29992;&#26041;&#27861;&#12290;ACRL&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#30830;&#20445;&#20195;&#29702;&#22312;&#27599;&#20010;&#24378;&#21270;&#23398;&#20064;&#27493;&#39588;&#20013;&#37319;&#21462;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#30340;&#26377;&#25928;&#21160;&#20316;&#12290;&#36890;&#24120;&#20351;&#29992;&#22312;&#31574;&#30053;&#32593;&#32476;&#20043;&#19978;&#30340;&#25237;&#24433;&#23618;&#30340;&#26041;&#27861;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#36739;&#38271;&#12289;&#25910;&#25947;&#36895;&#24230;&#24930;&#21644;&#38646;&#26799;&#24230;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#27491;&#21017;&#21270;&#27969;&#27169;&#22411;&#23398;&#20064;&#19968;&#20010;&#21487;&#36870;&#30340;&#12289;&#21487;&#24494;&#20998;&#30340;&#26144;&#23556;&#65292;&#23558;&#21487;&#34892;&#21160;&#20316;&#31354;&#38388;&#26144;&#23556;&#21040;&#19968;&#20010;&#28508;&#21464;&#37327;&#19978;&#30340;&#31616;&#21333;&#20998;&#24067;&#30340;&#25903;&#25745;&#38598;&#21512;&#65292;&#20363;&#22914;&#39640;&#26031;&#20998;&#24067;&#12290;&#20854;&#27425;&#65292;&#23398;&#20064;&#27969;&#27169;&#22411;&#38656;&#35201;&#20174;&#21487;&#34892;&#21160;&#20316;&#31354;&#38388;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#36825;&#20063;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22810;&#31181;&#26041;&#27861;&#65292;&#22522;&#20110;&#21704;&#23494;&#39039;&#33945;&#29305;&#21345;&#27931;&#21644;&#27010;&#29575;&#34920;&#20915;&#22270;&#65292;&#29992;&#20110;&#20984;&#32422;&#26463;&#21644;&#38750;&#20984;&#32422;&#26463;&#19979;&#30340;&#21160;&#20316;&#37319;&#26679;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;&#23398;&#20064;&#30340;&#27969;&#27169;&#22411;&#19982;&#24378;&#21270;&#23398;&#20064;&#30340;&#31574;&#30053;&#32593;&#32476;&#30456;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Action-constrained reinforcement learning (ACRL) is a popular approach for solving safety-critical and resource-allocation related decision making problems. A major challenge in ACRL is to ensure agent taking a valid action satisfying constraints in each RL step. Commonly used approach of using a projection layer on top of the policy network requires solving an optimization program which can result in longer training time, slow convergence, and zero gradient problem. To address this, first we use a normalizing flow model to learn an invertible, differentiable mapping between the feasible action space and the support of a simple distribution on a latent variable, such as Gaussian. Second, learning the flow model requires sampling from the feasible action space, which is also challenging. We develop multiple methods, based on Hamiltonian Monte-Carlo and probabilistic sentential decision diagrams for such action sampling for convex and non-convex constraints. Third, we integrate the learn
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ApiQ&#30340;&#26032;&#22411;&#37327;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#21021;&#22987;&#21270;LoRA&#32452;&#20214;&#21644;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#24674;&#22797;&#37327;&#21270;&#36807;&#31243;&#20013;&#20002;&#22833;&#30340;&#20449;&#24687;&#65292;&#32500;&#25345;&#21407;&#22987;&#27169;&#22411;&#30340;&#28608;&#27963;&#31934;&#24230;&#24182;&#20943;&#36731;&#35823;&#24046;&#20256;&#25773;&#12290;</title><link>https://arxiv.org/abs/2402.05147</link><description>&lt;p&gt;
ApiQ&#65306;2&#20301;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
ApiQ: Finetuning of 2-Bit Quantized Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05147
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ApiQ&#30340;&#26032;&#22411;&#37327;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#21021;&#22987;&#21270;LoRA&#32452;&#20214;&#21644;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#24674;&#22797;&#37327;&#21270;&#36807;&#31243;&#20013;&#20002;&#22833;&#30340;&#20449;&#24687;&#65292;&#32500;&#25345;&#21407;&#22987;&#27169;&#22411;&#30340;&#28608;&#27963;&#31934;&#24230;&#24182;&#20943;&#36731;&#35823;&#24046;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#22823;&#65292;&#20869;&#23384;&#39640;&#25928;&#30340;&#27169;&#22411;&#24494;&#35843;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;GPU&#20869;&#23384;&#38480;&#21046;&#21644;&#36825;&#20123;&#26041;&#27861;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;&#21487;&#27604;&#32467;&#26524;&#25152;&#24102;&#26469;&#30340;&#32422;&#26463;&#12290;&#23613;&#31649;&#26377;&#20102;&#36827;&#23637;&#65292;&#22914;QLoRA&#36825;&#26679;&#30340;&#20869;&#23384;&#39640;&#25928;&#24494;&#35843;&#31574;&#30053;&#22312;&#19981;&#21516;&#20301;&#23485;&#30340;&#37327;&#21270;&#21644;&#22810;&#26679;&#21270;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#19968;&#33268;&#12290;&#36825;&#31181;&#19981;&#19968;&#33268;&#20027;&#35201;&#26469;&#33258;&#20110;&#37327;&#21270;&#36807;&#31243;&#23545;&#20445;&#30041;&#30693;&#35782;&#30340;&#26377;&#23475;&#24433;&#21709;&#65292;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21066;&#24369;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#24494;&#35843;&#20013;&#30340;&#21033;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;ApiQ&#30340;&#26032;&#22411;&#37327;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#21516;&#26102;&#21021;&#22987;&#21270;LoRA&#32452;&#20214;&#21644;&#37327;&#21270;LLM&#30340;&#26435;&#37325;&#26469;&#24674;&#22797;&#37327;&#21270;&#25439;&#22833;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#30830;&#20445;&#20102;&#21407;&#22987;LLM&#30340;&#28608;&#27963;&#31934;&#24230;&#30340;&#32500;&#25345;&#65292;&#21516;&#26102;&#20943;&#36731;&#20102;&#35823;&#24046;&#30340;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory-efficient finetuning of large language models (LLMs) has recently attracted huge attention with the increasing size of LLMs, primarily due to the constraints posed by GPU memory limitations and the comparable results of these methods with full finetuning. Despite the advancements, current strategies for memory-efficient finetuning, such as QLoRA, exhibit inconsistent performance across diverse bit-width quantizations and multifaceted tasks. This inconsistency largely stems from the detrimental impact of the quantization process on preserved knowledge, leading to catastrophic forgetting and undermining the utilization of pretrained models for finetuning purposes. In this work, we introduce a novel quantization framework named ApiQ, designed to restore the lost information from quantization by concurrently initializing LoRA components and quantizing the weights of LLMs. This approach ensures the maintenance of the original LLM's activation precision while mitigating the error prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#65292;&#29992;&#20110;&#21387;&#32553;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32593;&#32476;&#65292;&#20197;&#20415;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#33258;&#21160;&#39550;&#39542;&#35774;&#22791;&#20013;&#23454;&#29616;&#39640;&#25928;&#37096;&#32626;&#12290;&#36890;&#36807;&#36880;&#28176;&#21024;&#38500;&#19981;&#37325;&#35201;&#30340;&#31070;&#32463;&#20803;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23567;&#20102;&#20869;&#23384;&#28040;&#32791;&#21644;&#35745;&#31639;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.05146</link><description>&lt;p&gt;
&#29992;&#21160;&#24577;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#21387;&#32553;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32593;&#32476;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Compressing Deep Reinforcement Learning Networks with a Dynamic Structured Pruning Method for Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#65292;&#29992;&#20110;&#21387;&#32553;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32593;&#32476;&#65292;&#20197;&#20415;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#33258;&#21160;&#39550;&#39542;&#35774;&#22791;&#20013;&#23454;&#29616;&#39640;&#25928;&#37096;&#32626;&#12290;&#36890;&#36807;&#36880;&#28176;&#21024;&#38500;&#19981;&#37325;&#35201;&#30340;&#31070;&#32463;&#20803;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23567;&#20102;&#20869;&#23384;&#28040;&#32791;&#21644;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#22797;&#26434;&#30340;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#19981;&#21487;&#36991;&#20813;&#22320;&#24102;&#26469;&#20102;&#39640;&#20869;&#23384;&#28040;&#32791;&#21644;&#35745;&#31639;&#37327;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#33258;&#21160;&#39550;&#39542;&#35774;&#22791;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#32467;&#26500;&#21270;&#21098;&#26525;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#29992;&#30340;&#26041;&#27861;&#26469;&#21387;&#32553;&#21644;&#21152;&#36895;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#26159;&#20272;&#35745;&#19968;&#20010;&#21442;&#25968;&#65288;&#21363;&#31070;&#32463;&#20803;&#65289;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#36129;&#29486;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#36880;&#28176;&#21024;&#38500;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20013;&#19981;&#37325;&#35201;&#30340;&#31070;&#32463;&#20803;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65292;&#21363;&#20351;&#29992;&#32452;&#31232;&#30095;&#27491;&#21017;&#21270;&#22120;&#35757;&#32451;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#21450;&#20351;&#29992;&#21160;&#24577;&#21098;&#26525;&#38408;&#20540;&#21435;&#38500;&#19981;&#37325;&#35201;&#30340;&#31070;&#32463;&#20803;&#12290;&#20026;&#20102;&#20351;&#29992;&#23569;&#37327;&#37325;&#35201;&#30340;&#31070;&#32463;&#20803;&#26377;&#25928;&#22320;&#35757;&#32451;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#31070;&#32463;&#20803;&#37325;&#35201;&#24615;&#32452;&#31232;&#30095;&#27491;&#21017;&#21270;&#22120;&#12290;&#19982;&#20256;&#32479;&#30340;&#27491;&#21017;&#21270;&#22120;&#19981;&#21516;&#65292;&#36825;&#20010;&#27491;&#21017;&#21270;&#22120;&#23545;&#20887;&#20313;&#21442;&#25968;&#26045;&#21152;&#20102;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) has shown remarkable success in complex autonomous driving scenarios. However, DRL models inevitably bring high memory consumption and computation, which hinders their wide deployment in resource-limited autonomous driving devices. Structured Pruning has been recognized as a useful method to compress and accelerate DRL models, but it is still challenging to estimate the contribution of a parameter (i.e., neuron) to DRL models. In this paper, we introduce a novel dynamic structured pruning approach that gradually removes a DRL model's unimportant neurons during the training stage. Our method consists of two steps, i.e. training DRL models with a group sparse regularizer and removing unimportant neurons with a dynamic pruning threshold. To efficiently train the DRL model with a small number of important neurons, we employ a neuron-importance group sparse regularizer. In contrast to conventional regularizers, this regularizer imposes a penalty on redundan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#32447;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#36866;&#24212;&#21160;&#24577;&#29615;&#22659;&#21644;&#26377;&#23457;&#26597;&#25968;&#25454;&#30340;&#29983;&#23384;&#20998;&#26512;&#65292;&#36890;&#36807;&#22312;&#32447;&#29275;&#39039;&#27493;&#39588;(ONS)&#31639;&#27861;&#20272;&#35745;&#20107;&#20214;&#26102;&#38388;&#20998;&#24067;&#65292;&#24182;&#25552;&#20986;&#20102;&#20445;&#35777;ONS&#20855;&#26377;&#23545;&#25968;&#38543;&#26426;&#36951;&#25022;&#30028;&#30340;&#38543;&#26426;&#26041;&#27861;&#21644;&#33258;&#36866;&#24212;&#32858;&#21512;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05145</link><description>&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Learning Approach for Survival Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#32447;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#36866;&#24212;&#21160;&#24577;&#29615;&#22659;&#21644;&#26377;&#23457;&#26597;&#25968;&#25454;&#30340;&#29983;&#23384;&#20998;&#26512;&#65292;&#36890;&#36807;&#22312;&#32447;&#29275;&#39039;&#27493;&#39588;(ONS)&#31639;&#27861;&#20272;&#35745;&#20107;&#20214;&#26102;&#38388;&#20998;&#24067;&#65292;&#24182;&#25552;&#20986;&#20102;&#20445;&#35777;ONS&#20855;&#26377;&#23545;&#25968;&#38543;&#26426;&#36951;&#25022;&#30028;&#30340;&#38543;&#26426;&#26041;&#27861;&#21644;&#33258;&#36866;&#24212;&#32858;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#32447;&#25968;&#23398;&#26694;&#26550;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#65292;&#21487;&#20197;&#23454;&#26102;&#36866;&#24212;&#21160;&#24577;&#29615;&#22659;&#21644;&#26377;&#23457;&#26597;&#25968;&#25454;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26368;&#20248;&#20108;&#38454;&#22312;&#32447;&#20984;&#20248;&#21270;&#31639;&#27861;-&#22312;&#32447;&#29275;&#39039;&#27493;&#39588;(ONS)&#20272;&#35745;&#20107;&#20214;&#26102;&#38388;&#20998;&#24067;&#12290;&#36825;&#31181;&#20197;&#21069;&#26410;&#26366;&#25506;&#32034;&#30340;&#26041;&#27861;&#20855;&#26377;&#37325;&#22823;&#20248;&#21183;&#65292;&#21253;&#25324;&#20855;&#26377;&#38750;&#28176;&#36817;&#25910;&#25947;&#20445;&#35777;&#30340;&#26126;&#30830;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;ONS&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#65292;&#36825;&#21462;&#20915;&#20110;&#25351;&#25968;-&#20984;&#24615;&#36136;&#24182;&#19988;&#23545;&#36951;&#25022;&#30028;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#35777;ONS&#20855;&#26377;&#23545;&#25968;&#38543;&#26426;&#36951;&#25022;&#30028;&#30340;&#38543;&#26426;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32858;&#21512;&#26041;&#27861;&#65292;&#30830;&#20445;&#22312;&#20445;&#25345;&#24555;&#36895;&#36951;&#25022;&#30028;&#30340;&#21516;&#26102;&#65292;&#22312;&#36229;&#21442;&#25968;&#36873;&#25321;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#30340;&#21457;&#29616;&#21487;&#20197;&#36229;&#20986;&#29983;&#23384;&#20998;&#26512;&#39046;&#22495;&#65292;&#24182;&#19988;&#23545;&#20110;&#20219;&#20309;&#20855;&#26377;&#24046;&#30340;&#25351;&#25968;-&#20984;&#24615;&#21644;&#19981;&#31283;&#23450;ONS&#30340;&#24773;&#20917;&#37117;&#26159;&#30456;&#20851;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an online mathematical framework for survival analysis, allowing real time adaptation to dynamic environments and censored data. This framework enables the estimation of event time distributions through an optimal second order online convex optimization algorithm-Online Newton Step (ONS). This approach, previously unexplored, presents substantial advantages, including explicit algorithms with non-asymptotic convergence guarantees. Moreover, we analyze the selection of ONS hyperparameters, which depends on the exp-concavity property and has a significant influence on the regret bound. We propose a stochastic approach that guarantees logarithmic stochastic regret for ONS. Additionally, we introduce an adaptive aggregation method that ensures robustness in hyperparameter selection while maintaining fast regret bounds. The findings of this paper can extend beyond the survival analysis field, and are relevant for any case characterized by poor exp-concavity and unstable ONS. Fi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36827;&#21270;&#31639;&#23376;&#30340;&#24378;&#30423;&#26041;&#27861;&#26469;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#24314;&#27169;&#20026;&#26080;&#31351;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#21033;&#29992;&#37096;&#20998;&#35757;&#32451;&#21644;&#20934;&#30830;&#24615;&#20316;&#20026;&#22870;&#21169;&#65292;&#26368;&#32456;&#30340;&#31639;&#27861;Mutant-UCB&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#22266;&#23450;&#39044;&#31639;&#19979;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.05144</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#36827;&#21270;&#31639;&#23376;&#30340;&#24378;&#30423;&#26041;&#27861;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
A Bandit Approach with Evolutionary Operators for Model Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36827;&#21270;&#31639;&#23376;&#30340;&#24378;&#30423;&#26041;&#27861;&#26469;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#24314;&#27169;&#20026;&#26080;&#31351;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#21033;&#29992;&#37096;&#20998;&#35757;&#32451;&#21644;&#20934;&#30830;&#24615;&#20316;&#20026;&#22870;&#21169;&#65292;&#26368;&#32456;&#30340;&#31639;&#27861;Mutant-UCB&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#22266;&#23450;&#39044;&#31639;&#19979;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#24314;&#27169;&#20026;&#26080;&#31351;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#27169;&#22411;&#26159;&#33218;&#65292;&#36873;&#25321;&#19968;&#20010;&#33218;&#23545;&#24212;&#37096;&#20998;&#35757;&#32451;&#27169;&#22411;&#65288;&#36164;&#28304;&#20998;&#37197;&#65289;&#12290;&#22870;&#21169;&#26159;&#36873;&#25321;&#27169;&#22411;&#22312;&#37096;&#20998;&#35757;&#32451;&#21518;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#20010;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#20013;&#65292;&#36951;&#25022;&#26159;&#26368;&#20248;&#27169;&#22411;&#30340;&#39044;&#26399;&#20934;&#30830;&#24615;&#19982;&#26368;&#32456;&#36873;&#25321;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20102;UCB-E&#22312;&#38543;&#26426;&#26080;&#31351;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#19978;&#30340;&#30452;&#25509;&#25512;&#24191;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#22312;&#22522;&#26412;&#20551;&#35774;&#19979;&#65292;&#26399;&#26395;&#36951;&#25022;&#30340;&#39034;&#24207;&#26159;$T^{-\alpha}$&#65292;&#20854;&#20013;$\alpha \in (0,1/5)$&#65292;$T$&#26159;&#35201;&#20998;&#37197;&#30340;&#36164;&#28304;&#25968;&#37327;&#12290;&#20174;&#36825;&#20010;&#22522;&#26412;&#31639;&#27861;&#20986;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31639;&#27861;Mutant-UCB&#65292;&#23427;&#32467;&#21512;&#20102;&#36827;&#21270;&#31639;&#27861;&#30340;&#25805;&#20316;&#31526;&#12290;&#22312;&#19977;&#20010;&#24320;&#28304;&#22270;&#29255;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#27979;&#35797;&#34920;&#26126;&#20102;&#36825;&#31181;&#26032;&#39062;&#30340;&#32452;&#21512;&#26041;&#27861;&#30340;&#30456;&#20851;&#24615;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#22266;&#23450;&#39044;&#31639;&#19979;&#30340;&#22269;&#38469;&#39046;&#20808;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper formulates model selection as an infinite-armed bandit problem. The models are arms, and picking an arm corresponds to a partial training of the model (resource allocation). The reward is the accuracy of the selected model after its partial training. In this best arm identification problem, regret is the gap between the expected accuracy of the optimal model and that of the model finally chosen. We first consider a straightforward generalization of UCB-E to the stochastic infinite-armed bandit problem and show that, under basic assumptions, the expected regret order is $T^{-\alpha}$ for some $\alpha \in (0,1/5)$ and $T$ the number of resources to allocate. From this vanilla algorithm, we introduce the algorithm Mutant-UCB that incorporates operators from evolutionary algorithms. Tests carried out on three open source image classification data sets attest to the relevance of this novel combining approach, which outperforms the state-of-the-art for a fixed budget.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#36807;&#25972;&#25968;&#20248;&#21270;&#23454;&#29616;&#24352;&#37327;&#34917;&#20840;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#22312;&#32447;&#24615;&#27493;&#39588;&#20013;&#23454;&#29616;&#20102;&#25910;&#25947;&#24182;&#36798;&#21040;&#20102;&#20449;&#24687;&#29702;&#35770;&#36895;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05141</link><description>&lt;p&gt;
&#36890;&#36807;&#25972;&#25968;&#20248;&#21270;&#23454;&#29616;&#24352;&#37327;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Tensor Completion via Integer Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#36807;&#25972;&#25968;&#20248;&#21270;&#23454;&#29616;&#24352;&#37327;&#34917;&#20840;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#22312;&#32447;&#24615;&#27493;&#39588;&#20013;&#23454;&#29616;&#20102;&#25910;&#25947;&#24182;&#36798;&#21040;&#20102;&#20449;&#24687;&#29702;&#35770;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#35745;&#31639;&#33021;&#21147;&#21644;&#20449;&#24687;&#29702;&#35770;&#26679;&#26412;&#22797;&#26434;&#24230;&#20043;&#38388;&#30340;&#22522;&#26412;&#24352;&#21147;&#12290;&#36807;&#21435;&#30340;&#26041;&#27861;&#35201;&#20040;&#36798;&#21040;&#20102;&#20449;&#24687;&#29702;&#35770;&#30340;&#36895;&#29575;&#20294;&#32570;&#20047;&#35745;&#31639;&#30456;&#24212;&#35299;&#30340;&#23454;&#38469;&#31639;&#27861;&#65292;&#35201;&#20040;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#20294;&#38656;&#35201;&#25351;&#25968;&#32423;&#26356;&#22810;&#30340;&#26679;&#26412;&#20197;&#36798;&#21040;&#20302;&#20272;&#35745;&#35823;&#24046;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24352;&#37327;&#34917;&#20840;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#24615;&#25968;&#30446;&#30340;oracle&#27493;&#39588;&#20013;&#21516;&#26102;&#23454;&#29616;&#21487;&#35777;&#30340;&#25910;&#25947;&#65288;&#22312;&#25968;&#23383;&#23481;&#24046;&#26041;&#38754;&#65289;&#21644;&#20449;&#24687;&#29702;&#35770;&#36895;&#29575;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20010;&#24352;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#35268;&#33539;&#30340;&#24352;&#37327;&#33539;&#25968;&#26500;&#36896;&#20102;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#36825;&#31181;&#26041;&#27861;&#23450;&#20041;&#20102;&#19968;&#31181;&#20801;&#35768;&#20351;&#29992;&#25972;&#25968;&#32447;&#24615;&#20248;&#21270;&#22312;&#36825;&#31181;&#26032;&#33539;&#25968;&#19979;&#35299;&#20915;&#32447;&#24615;&#20998;&#31163;&#38382;&#39064;&#30340;&#35268;&#33539;&#12290;&#22522;&#20110;&#36825;&#20010;&#27934;&#23519;&#21147;&#30340;&#35843;&#25972;&#34987;&#32435;&#20837;&#21040;&#19968;&#31181;&#22522;&#20110;Frank-Wolfe&#21464;&#20307;&#30340;&#31639;&#27861;&#20013;&#26469;&#26500;&#24314;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main challenge with the tensor completion problem is a fundamental tension between computation power and the information-theoretic sample complexity rate. Past approaches either achieve the information-theoretic rate but lack practical algorithms to compute the corresponding solution, or have polynomial-time algorithms that require an exponentially-larger number of samples for low estimation error. This paper develops a novel tensor completion algorithm that resolves this tension by achieving both provable convergence (in numerical tolerance) in a linear number of oracle steps and the information-theoretic rate. Our approach formulates tensor completion as a convex optimization problem constrained using a gauge-based tensor norm, which is defined in a way that allows the use of integer linear optimization to solve linear separation problems over the unit-ball in this new norm. Adaptations based on this insight are incorporated into a Frank-Wolfe variant to build our algorithm. We s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#36890;&#29992;&#30340;LLMs&#24212;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#23450;&#20041;&#30340;&#36755;&#20837;&#26631;&#31614;&#26469;&#23545;LLMs&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#36890;&#36807;&#26126;&#30830;&#23558;&#20219;&#21153;&#39046;&#22495;&#19982;&#20219;&#21153;&#21151;&#33021;&#20998;&#31163;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05140</link><description>&lt;p&gt;
Tag-LLM: &#23558;&#36890;&#29992;&#30340;LLM&#24212;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#20877;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#36890;&#29992;&#30340;LLMs&#24212;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#23450;&#20041;&#30340;&#36755;&#20837;&#26631;&#31614;&#26469;&#23545;LLMs&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#36890;&#36807;&#26126;&#30830;&#23558;&#20219;&#21153;&#39046;&#22495;&#19982;&#20219;&#21153;&#21151;&#33021;&#20998;&#31163;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#19987;&#38376;&#39046;&#22495;&#20013;&#65292;&#22914;&#29289;&#29702;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#31185;&#23398;&#36825;&#26679;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#26410;&#20805;&#20998;&#28085;&#30422;&#30340;&#39046;&#22495;&#65292;&#23427;&#20204;&#30340;&#33021;&#21147;&#19979;&#38477;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#36890;&#29992;LLMs&#37325;&#26032;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#26377;&#25928;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#33258;&#23450;&#20041;&#30340;&#36755;&#20837;&#26631;&#31614;&#65292;&#36825;&#20123;&#26631;&#31614;&#34987;&#21442;&#25968;&#21270;&#20026;&#36830;&#32493;&#21521;&#37327;&#24182;&#38468;&#21152;&#21040;LLMs&#30340;&#23884;&#20837;&#23618;&#65292;&#20197;&#23545;LLMs&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#26631;&#31614;&#65306;&#39046;&#22495;&#26631;&#31614;&#29992;&#20110;&#38480;&#23450;&#19987;&#19994;&#34920;&#31034;&#65288;&#20363;&#22914;&#21270;&#23398;&#24335;&#65289;&#24182;&#25552;&#20379;&#39046;&#22495;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#65307;&#21151;&#33021;&#26631;&#31614;&#29992;&#20110;&#34920;&#31034;&#29305;&#23450;&#30340;&#21151;&#33021;&#65288;&#20363;&#22914;&#39044;&#27979;&#20998;&#23376;&#24615;&#36136;&#65289;&#24182;&#21387;&#32553;&#21151;&#33021;&#35299;&#20915;&#25351;&#20196;&#12290;&#25105;&#20204;&#20351;&#29992;&#36741;&#21161;&#25968;&#25454;&#21644;&#39046;&#22495;&#30693;&#35782;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#30340;&#23398;&#20064;&#36825;&#20123;&#26631;&#31614;&#30340;&#21327;&#35758;&#12290;&#36890;&#36807;&#26126;&#30830;&#23558;&#20219;&#21153;&#39046;&#22495;&#19982;&#20219;&#21153;&#21151;&#33021;&#20998;&#31163;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable proficiency in understanding and generating natural language. However, their capabilities wane in highly specialized domains underrepresented in the pretraining corpus, such as physical and biomedical sciences. This work explores how to repurpose general LLMs into effective task solvers for specialized domains. We introduce a novel, model-agnostic framework for learning custom input tags, which are parameterized as continuous vectors appended to the LLM's embedding layer, to condition the LLM. We design two types of input tags: domain tags are used to delimit specialized representations (e.g., chemical formulas) and provide domain-relevant context; function tags are used to represent specific functions (e.g., predicting molecular properties) and compress function-solving instructions. We develop a three-stage protocol to learn these tags using auxiliary data and domain knowledge. By explicitly disentangling task domains from tas
&lt;/p&gt;</description></item><item><title>LtU-ILI&#26159;&#19968;&#20010;&#29992;&#20110;&#22825;&#20307;&#29289;&#29702;&#23398;&#21644;&#23431;&#23449;&#23398;&#20013;&#26426;&#22120;&#23398;&#20064;&#25512;&#29702;&#30340;&#20840;&#33021;&#26694;&#26550;&#65292;&#21253;&#25324;&#21508;&#31181;&#31070;&#32463;&#26550;&#26500;&#12289;&#35757;&#32451;&#27169;&#24335;&#12289;&#20808;&#39564;&#30693;&#35782;&#21644;&#23494;&#24230;&#20272;&#35745;&#22120;&#65292;&#24182;&#20855;&#26377;&#20840;&#38754;&#30340;&#39564;&#35777;&#25351;&#26631;&#21644;&#26131;&#20110;&#24182;&#34892;&#21270;&#30340;&#35774;&#35745;&#12290;&#23454;&#38469;&#24212;&#29992;&#21253;&#25324;&#20174;X&#23556;&#32447;&#20809;&#24230;&#27861;&#20272;&#35745;&#26143;&#31995;&#22242;&#36136;&#37327;&#12289;&#25512;&#26029;&#23431;&#23449;&#23398;&#12289;&#34920;&#24449;&#24341;&#21147;&#27874;&#20449;&#21495;&#20013;&#30340;&#28304;&#21644;&#33719;&#21462;&#38134;&#27827;&#31995;&#20013;&#30340;&#29289;&#29702;&#23576;&#22467;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.05137</link><description>&lt;p&gt;
LtU-ILI:&#19968;&#31181;&#38754;&#21521;&#22825;&#20307;&#29289;&#29702;&#23398;&#21644;&#23431;&#23449;&#23398;&#30340;&#38544;&#24335;&#25512;&#29702;&#20840;&#33021;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LtU-ILI: An All-in-One Framework for Implicit Inference in Astrophysics and Cosmology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05137
&lt;/p&gt;
&lt;p&gt;
LtU-ILI&#26159;&#19968;&#20010;&#29992;&#20110;&#22825;&#20307;&#29289;&#29702;&#23398;&#21644;&#23431;&#23449;&#23398;&#20013;&#26426;&#22120;&#23398;&#20064;&#25512;&#29702;&#30340;&#20840;&#33021;&#26694;&#26550;&#65292;&#21253;&#25324;&#21508;&#31181;&#31070;&#32463;&#26550;&#26500;&#12289;&#35757;&#32451;&#27169;&#24335;&#12289;&#20808;&#39564;&#30693;&#35782;&#21644;&#23494;&#24230;&#20272;&#35745;&#22120;&#65292;&#24182;&#20855;&#26377;&#20840;&#38754;&#30340;&#39564;&#35777;&#25351;&#26631;&#21644;&#26131;&#20110;&#24182;&#34892;&#21270;&#30340;&#35774;&#35745;&#12290;&#23454;&#38469;&#24212;&#29992;&#21253;&#25324;&#20174;X&#23556;&#32447;&#20809;&#24230;&#27861;&#20272;&#35745;&#26143;&#31995;&#22242;&#36136;&#37327;&#12289;&#25512;&#26029;&#23431;&#23449;&#23398;&#12289;&#34920;&#24449;&#24341;&#21147;&#27874;&#20449;&#21495;&#20013;&#30340;&#28304;&#21644;&#33719;&#21462;&#38134;&#27827;&#31995;&#20013;&#30340;&#29289;&#29702;&#23576;&#22467;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23398;&#20064;&#23431;&#23449;&#38544;&#21547;&#20284;&#28982;&#25512;&#29702;&#65288;LtU-ILI&#65289;&#27969;&#31243;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22825;&#20307;&#29289;&#29702;&#23398;&#21644;&#23431;&#23449;&#23398;&#20013;&#24555;&#36895;&#12289;&#29992;&#25143;&#21451;&#22909;&#21644;&#21069;&#27839;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25512;&#29702;&#30340;&#20195;&#30721;&#24211;&#12290;&#35813;&#27969;&#31243;&#21253;&#25324;&#21508;&#31181;&#31070;&#32463;&#26550;&#26500;&#12289;&#35757;&#32451;&#27169;&#24335;&#12289;&#20808;&#39564;&#30693;&#35782;&#21644;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#36719;&#20214;&#23454;&#29616;&#65292;&#21487;&#36731;&#26494;&#36866;&#24212;&#20219;&#20309;&#30740;&#31350;&#24037;&#20316;&#27969;&#31243;&#12290;&#23427;&#21253;&#25324;&#20840;&#38754;&#30340;&#39564;&#35777;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#21518;&#39564;&#20272;&#35745;&#35206;&#30422;&#33539;&#22260;&#65292;&#25552;&#39640;&#25512;&#26029;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#27969;&#31243;&#23481;&#26131;&#23454;&#29616;&#24182;&#34892;&#21270;&#65292;&#24182;&#35774;&#35745;&#29992;&#20110;&#39640;&#25928;&#25506;&#32034;&#24314;&#27169;&#36229;&#21442;&#25968;&#12290;&#20026;&#20102;&#23637;&#31034;&#20854;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#24212;&#29992;&#23454;&#20363;&#65292;&#28085;&#30422;&#20102;&#22825;&#20307;&#29289;&#29702;&#23398;&#21644;&#23431;&#23449;&#23398;&#30340;&#21508;&#20010;&#38382;&#39064;&#65292;&#22914;&#20174;X&#23556;&#32447;&#20809;&#24230;&#27861;&#20272;&#35745;&#26143;&#31995;&#22242;&#36136;&#37327;&#65307;&#20174;&#29289;&#36136;&#21151;&#29575;&#35889;&#21644;&#26263;&#29289;&#36136;&#28857;&#20113;&#20013;&#25512;&#26029;&#23431;&#23449;&#23398;&#65307;&#23545;&#24341;&#21147;&#27874;&#20449;&#21495;&#20013;&#30340;&#28304;&#30340;&#29305;&#24615;&#36827;&#34892;&#34920;&#24449;&#65307;&#20174;&#38134;&#27827;&#31995;&#20013;&#33719;&#21462;&#29289;&#29702;&#23576;&#22467;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the Learning the Universe Implicit Likelihood Inference (LtU-ILI) pipeline, a codebase for rapid, user-friendly, and cutting-edge machine learning (ML) inference in astrophysics and cosmology. The pipeline includes software for implementing various neural architectures, training schema, priors, and density estimators in a manner easily adaptable to any research workflow. It includes comprehensive validation metrics to assess posterior estimate coverage, enhancing the reliability of inferred results. Additionally, the pipeline is easily parallelizable, designed for efficient exploration of modeling hyperparameters. To demonstrate its capabilities, we present real applications across a range of astrophysics and cosmology problems, such as: estimating galaxy cluster masses from X-ray photometry; inferring cosmology from matter power spectra and halo point clouds; characterising progenitors in gravitational wave signals; capturing physical dust parameters from galaxy co
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20110;&#29992;&#25143;&#30340;&#21453;&#39304;&#25968;&#25454;&#20013;&#24341;&#20837;&#20010;&#24615;&#21270;&#29305;&#24449;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#22312;&#22810;&#26679;&#21270;&#29992;&#25143;&#20559;&#22909;&#19979;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05133</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#20010;&#24615;&#21270;&#20154;&#31867;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Personalized Language Modeling from Personalized Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05133
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20110;&#29992;&#25143;&#30340;&#21453;&#39304;&#25968;&#25454;&#20013;&#24341;&#20837;&#20010;&#24615;&#21270;&#29305;&#24449;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#22312;&#22810;&#26679;&#21270;&#29992;&#25143;&#20559;&#22909;&#19979;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20010;&#24615;&#21270;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#30446;&#21069;&#20027;&#27969;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#24320;&#21457;&#30340;&#31639;&#27861;&#30340;&#22522;&#26412;&#21069;&#25552;&#22312;&#29992;&#25143;&#20559;&#22909;&#22810;&#26679;&#21270;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#20986;&#29616;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#27491;&#24335;&#20171;&#32461;&#20102;&#20174;&#20010;&#24615;&#21270;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#20219;&#21153;&#65292;&#24182;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26222;&#36890;&#30340;RLHF&#21487;&#33021;&#20250;&#23384;&#22312;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20010;&#24615;&#21270;-RLHF&#65288;P-RLHF&#65289;&#26694;&#26550;&#65292;&#38656;&#35201;&#21516;&#26102;&#23398;&#20064;&#29992;&#25143;&#27169;&#22411;&#21644;&#35821;&#35328;&#65288;&#25110;&#22870;&#21169;&#65289;&#27169;&#22411;&#12290;&#29992;&#25143;&#27169;&#22411;&#25509;&#25910;&#29992;&#25143;&#20449;&#24687;&#24182;&#36755;&#20986;&#29992;&#25143;&#34920;&#31034;&#12290;&#20854;&#32467;&#26500;&#32534;&#30721;&#20102;&#25105;&#20204;&#23545;&#21453;&#39304;&#25968;&#25454;&#20013;&#29992;&#25143;&#20559;&#22909;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#20026;&#20010;&#24615;&#21270;&#22870;&#21169;&#24314;&#27169;&#21644;&#20010;&#24615;&#21270;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#24320;&#21457;&#20102;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) is the current dominating framework to fine-tune large language models to better align with human preferences. However, the underlying premise of algorithms developed under this framework can be problematic when user preferences encoded in human feedback are diverse. In this work, we aim to address this problem by developing methods for building personalized language models. We first formally introduce the task of learning from personalized human feedback and explain why vanilla RLHF can be problematic in this context. We then propose a general Personalized-RLHF (P-RLHF) framework, which requires one to jointly learn a user model and a language (or reward) model. The user model takes in user information and outputs user representations. Its structure encodes our assumptions about user preferences underlying the feedback data. We develop new learning objectives for personalized reward modeling and personalized Direct Preference Optimizat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;&#21644;&#27835;&#30103;&#33539;&#24335;&#65292;&#20351;&#29992;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#32463;&#36807;&#29305;&#23450;&#25552;&#31034;&#24494;&#35843;&#20197;&#35786;&#26029;&#12289;&#35299;&#37322;&#21644;&#24314;&#35758;&#27835;&#30103;&#24178;&#39044;&#12290;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#25968;&#25454;&#24211;&#65292;&#20197;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#24314;&#35758;&#12290;&#27492;&#26041;&#27861;&#19982;&#24739;&#32773;&#36827;&#34892;&#20849;&#24773;&#23545;&#35805;&#31649;&#29702;&#65292;&#26377;&#25928;&#25903;&#25345;&#25233;&#37057;&#30151;&#24739;&#32773;&#12290;</title><link>https://arxiv.org/abs/2402.05127</link><description>&lt;p&gt;
Illuminate&#65306;&#19968;&#31181;&#20351;&#29992;&#21487;&#35299;&#37322;&#20998;&#26512;&#21644;&#31215;&#26497;&#27835;&#30103;&#30340;&#26032;&#26041;&#27861;&#36827;&#34892;&#25233;&#37057;&#30151;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Illuminate: A novel approach for depression detection with explainable analysis and proactive therapy using prompt engineering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;&#21644;&#27835;&#30103;&#33539;&#24335;&#65292;&#20351;&#29992;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#32463;&#36807;&#29305;&#23450;&#25552;&#31034;&#24494;&#35843;&#20197;&#35786;&#26029;&#12289;&#35299;&#37322;&#21644;&#24314;&#35758;&#27835;&#30103;&#24178;&#39044;&#12290;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#25968;&#25454;&#24211;&#65292;&#20197;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#24314;&#35758;&#12290;&#27492;&#26041;&#27861;&#19982;&#24739;&#32773;&#36827;&#34892;&#20849;&#24773;&#23545;&#35805;&#31649;&#29702;&#65292;&#26377;&#25928;&#25903;&#25345;&#25233;&#37057;&#30151;&#24739;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65306;Generative Pre-trained Transformer 4&#65288;GPT-4&#65289;&#12289;Llama 2 chat&#21644;Gemini&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;&#21644;&#27835;&#30103;&#26032;&#33539;&#24335;&#12290;&#36825;&#20123;LLMs&#36890;&#36807;&#29305;&#23450;&#30340;&#25552;&#31034;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#35786;&#26029;&#12289;&#35299;&#37322;&#21644;&#24314;&#35758;&#25233;&#37057;&#30151;&#30340;&#27835;&#30103;&#24178;&#39044;&#12290;&#19968;&#31181;&#29420;&#29305;&#30340;&#23569;&#26679;&#26412;&#25552;&#31034;&#26041;&#27861;&#22686;&#24378;&#20102;&#27169;&#22411;&#26681;&#25454;DSM-5&#26631;&#20934;&#20998;&#26512;&#21644;&#35299;&#37322;&#25233;&#37057;&#30151;&#29366;&#30340;&#33021;&#21147;&#12290;&#22312;&#20132;&#20114;&#38454;&#27573;&#65292;&#27169;&#22411;&#37319;&#29992;&#20849;&#24773;&#23545;&#35805;&#31649;&#29702;&#65292;&#21033;&#29992;PsychDB&#21644;&#35748;&#30693;&#34892;&#20026;&#30103;&#27861;&#65288;CBT&#65289;&#25351;&#21335;&#31561;&#36164;&#28304;&#65292;&#19982;&#24739;&#26377;&#37325;&#24230;&#25233;&#37057;&#30151;&#30340;&#20010;&#20307;&#36827;&#34892;&#25903;&#25345;&#24615;&#20114;&#21160;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#20171;&#32461;&#20102;Illuminate&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;CBT&#27169;&#22359;&#65292;&#21487;&#24110;&#21161;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#24314;&#35758;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;F1&#20998;&#25968;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#12289;&#20313;&#24358;&#30456;&#20284;&#24230;&#21644;&#21484;&#22238;&#29575;&#23548;&#21521;&#30340;&#24046;&#38169;&#36827;&#34892;LLM&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel paradigm for depression detection and treatment using advanced Large Language Models (LLMs): Generative Pre-trained Transformer 4 (GPT-4), Llama 2 chat, and Gemini. These LLMs are fine-tuned with specialized prompts to diagnose, explain, and suggest therapeutic interventions for depression. A unique few-shot prompting method enhances the models' ability to analyze and explain depressive symptoms based on the DSM-5 criteria. In the interaction phase, the models engage in empathetic dialogue management, drawing from resources like PsychDB and a Cognitive Behavioral Therapy (CBT) Guide, fostering supportive interactions with individuals experiencing major depressive disorders. Additionally, the research introduces the Illuminate Database, enriched with various CBT modules, aiding in personalized therapy recommendations. The study evaluates LLM performance using metrics such as F1 scores, Precision, Recall, Cosine similarity, and Recall-Oriented Understudy for
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#39033;&#30446;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#25429;&#33719;&#24182;&#22788;&#29702;&#25991;&#26412;&#20449;&#24687;&#20013;&#30340;&#20851;&#31995;&#25968;&#25454;&#65292;&#32780;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31995;&#32479;&#36890;&#36807;&#35782;&#21035;&#21644;&#24378;&#35843;&#20851;&#38190;&#23454;&#20307;&#26469;&#20445;&#25345;&#25688;&#35201;&#30340;&#37325;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.05126</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;NER&#30340;&#25991;&#26412;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network and NER-Based Text Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05126
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#39033;&#30446;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#25429;&#33719;&#24182;&#22788;&#29702;&#25991;&#26412;&#20449;&#24687;&#20013;&#30340;&#20851;&#31995;&#25968;&#25454;&#65292;&#32780;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31995;&#32479;&#36890;&#36807;&#35782;&#21035;&#21644;&#24378;&#35843;&#20851;&#38190;&#23454;&#20307;&#26469;&#20445;&#25345;&#25688;&#35201;&#30340;&#37325;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#26102;&#20195;&#20449;&#24687;&#21644;&#25968;&#25454;&#30340;&#20016;&#23500;&#24615;&#19979;&#65292;&#20154;&#20204;&#25110;&#32773;&#26426;&#22120;&#20960;&#20046;&#19981;&#21487;&#33021;&#36880;&#34892;&#26597;&#30475;&#25152;&#26377;&#30340;&#25968;&#25454;&#12290;&#36890;&#24120;&#30340;&#20570;&#27861;&#26159;&#35797;&#22270;&#20174;&#34892;&#20013;&#24555;&#36895;&#27983;&#35272;&#24182;&#20445;&#30041;&#32477;&#23545;&#37325;&#35201;&#30340;&#20449;&#24687;&#65292;&#36825;&#22312;&#26356;&#27491;&#24335;&#30340;&#26415;&#35821;&#20013;&#31216;&#20026;&#25688;&#35201;&#12290;&#25991;&#26412;&#25688;&#35201;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#23558;&#20887;&#38271;&#30340;&#25991;&#26723;&#25110;&#25991;&#31456;&#21387;&#32553;&#25104;&#36739;&#30701;&#12289;&#36830;&#36143;&#30340;&#34920;&#36798;&#26041;&#24335;&#65292;&#21516;&#26102;&#20445;&#30041;&#26680;&#24515;&#20449;&#24687;&#21644;&#24847;&#20041;&#12290;&#26412;&#39033;&#30446;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;GNNs&#20197;&#20854;&#20248;&#31168;&#30340;&#25429;&#33719;&#21644;&#22788;&#29702;&#25991;&#26412;&#20449;&#24687;&#20013;&#30340;&#20851;&#31995;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#29702;&#35299;&#22823;&#22411;&#25991;&#26723;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#12290;&#21516;&#26102;&#65292;NER&#31995;&#32479;&#36890;&#36807;&#35782;&#21035;&#21644;&#24378;&#35843;&#20851;&#38190;&#23454;&#20307;&#65292;&#30830;&#20445;&#25688;&#35201;&#36807;&#31243;&#20445;&#25345;&#19987;&#27880;&#20110;&#37325;&#28857;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the abundance of data and information in todays time, it is nearly impossible for man, or, even machine, to go through all of the data line by line. What one usually does is to try to skim through the lines and retain the absolutely important information, that in a more formal term is called summarization. Text summarization is an important task that aims to compress lengthy documents or articles into shorter, coherent representations while preserving the core information and meaning. This project introduces an innovative approach to text summarization, leveraging the capabilities of Graph Neural Networks (GNNs) and Named Entity Recognition (NER) systems. GNNs, with their exceptional ability to capture and process the relational data inherent in textual information, are adept at understanding the complex structures within large documents. Meanwhile, NER systems contribute by identifying and emphasizing key entities, ensuring that the summarization process maintains a focus on the 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#27604;&#20363;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#37319;&#26679;&#21644;&#25237;&#31080;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#24615;&#33021;&#65292;&#36825;&#31181;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#22797;&#26434;&#26041;&#27861;&#27491;&#20132;&#12290;</title><link>https://arxiv.org/abs/2402.05120</link><description>&lt;p&gt;
&#26356;&#22810;&#30340;&#20195;&#29702;&#23601;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
More Agents Is All You Need
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05120
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#27604;&#20363;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#37319;&#26679;&#21644;&#25237;&#31080;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#24615;&#33021;&#65292;&#36825;&#31181;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#22797;&#26434;&#26041;&#27861;&#27491;&#20132;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#36890;&#36807;&#19968;&#31181;&#37319;&#26679;&#21644;&#25237;&#31080;&#30340;&#26041;&#27861;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Large Language Models, LLMs)&#30340;&#24615;&#33021;&#19982;&#23454;&#20363;&#21270;&#30340;&#20195;&#29702;&#25968;&#37327;&#25104;&#27604;&#20363;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#26041;&#27861;&#23545;&#24050;&#26377;&#30340;&#22797;&#26434;&#26041;&#27861;&#36827;&#19968;&#27493;&#22686;&#24378;LLMs&#26159;&#27491;&#20132;&#30340;&#65292;&#32780;&#22686;&#24378;&#30340;&#31243;&#24230;&#19982;&#20219;&#21153;&#30340;&#22256;&#38590;&#31243;&#24230;&#30456;&#20851;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#24182;&#30740;&#31350;&#20102;&#33021;&#22815;&#20419;&#36827;&#20854;&#21457;&#29983;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#22312;&#20197;&#19979;&#32593;&#22336;: \url{https://anonymous.4open.science/r/more_agent_is_all_you_need}
&lt;/p&gt;
&lt;p&gt;
We find that, simply via a sampling-and-voting method, the performance of large language models (LLMs) scales with the number of agents instantiated. Also, this method is orthogonal to existing complicated methods to further enhance LLMs, while the degree of enhancement is correlated to the task difficulty. We conduct comprehensive experiments on a wide range of LLM benchmarks to verify the presence of our finding, and to study the properties that can facilitate its occurrence. Our code is publicly available at: \url{https://anonymous.4open.science/r/more_agent_is_all_you_need}.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#36816;&#21160;&#37325;&#23450;&#20301;&#29992;&#20110;&#20154;&#26426;&#20223;&#30495;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#39046;&#22495;&#21040;&#39046;&#22495;&#30340;&#36716;&#25442;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#26080;&#23545;&#24212;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20154;&#26426;&#20223;&#30495;&#12290;</title><link>https://arxiv.org/abs/2402.05115</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#36816;&#21160;&#37325;&#23450;&#20301;&#29992;&#20110;&#20154;&#26426;&#20223;&#30495;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Motion Retargeting for Human-Robot Imitation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05115
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#36816;&#21160;&#37325;&#23450;&#20301;&#29992;&#20110;&#20154;&#26426;&#20223;&#30495;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#39046;&#22495;&#21040;&#39046;&#22495;&#30340;&#36716;&#25442;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#26080;&#23545;&#24212;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20154;&#26426;&#20223;&#30495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#26089;&#26399;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23558;&#20154;&#31867;&#21160;&#20316;&#39046;&#22495;&#30340;&#20851;&#33410;&#20301;&#32622;&#24207;&#21015;&#36716;&#21270;&#20026;&#29305;&#23450;&#26426;&#22120;&#20154;&#21487;&#23454;&#29616;&#30340;&#21160;&#20316;&#39046;&#22495;&#65292;&#20174;&#32780;&#25913;&#36827;&#22312;&#32447;&#20154;&#26426;&#20223;&#30495;&#12290;&#20511;&#21161;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#39046;&#22495;&#21040;&#39046;&#22495;&#30340;&#36716;&#25442;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#29992;&#23545;&#24212;&#30340;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#21160;&#20316;&#23545;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#36825;&#26679;&#30340;&#37197;&#23545;&#25968;&#25454;&#38750;&#24120;&#31232;&#32570;&#19988;&#25910;&#38598;&#36215;&#26469;&#32321;&#29712;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36716;&#21521;&#36866;&#24212;&#20110;&#26080;&#37197;&#23545;&#39046;&#22495;&#21040;&#39046;&#22495;&#36716;&#25442;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#36827;&#34892;&#20154;&#26426;&#20223;&#30495;&#12290;
&lt;/p&gt;
&lt;p&gt;
This early-stage research work aims to improve online human-robot imitation by translating sequences of joint positions from the domain of human motions to a domain of motions achievable by a given robot, thus constrained by its embodiment. Leveraging the generalization capabilities of deep learning methods, we address this problem by proposing an encoder-decoder neural network model performing domain-to-domain translation. In order to train such a model, one could use pairs of associated robot and human motions. Though, such paired data is extremely rare in practice, and tedious to collect. Therefore, we turn towards deep learning methods for unpaired domain-to-domain translation, that we adapt in order to perform human-robot imitation.
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#21644;&#26080;&#30417;&#30563;&#30340;&#36817;&#23454;&#26102;&#34892;&#20026;&#20998;&#26512;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#24212;&#22823;&#22411;&#35745;&#31639;&#31995;&#32479;&#30340;&#21464;&#21270;&#65292;&#24182;&#33021;&#20934;&#30830;&#35782;&#21035;&#20986;&#34892;&#20026;&#24322;&#24120;&#12290;</title><link>https://arxiv.org/abs/2402.05114</link><description>&lt;p&gt;
&#19968;&#31181;&#36731;&#37327;&#32423;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#36816;&#32500;&#25968;&#25454;&#27979;&#37327;&#36827;&#34892;&#36817;&#23454;&#26102;&#34892;&#20026;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Light-weight and Unsupervised Method for Near Real-time Behavioral Analysis using Operational Data Measurement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05114
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#21644;&#26080;&#30417;&#30563;&#30340;&#36817;&#23454;&#26102;&#34892;&#20026;&#20998;&#26512;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#24212;&#22823;&#22411;&#35745;&#31639;&#31995;&#32479;&#30340;&#21464;&#21270;&#65292;&#24182;&#33021;&#20934;&#30830;&#35782;&#21035;&#20986;&#34892;&#20026;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#25511;&#22823;&#22411;&#35745;&#31639;&#31995;&#32479;&#30340;&#29366;&#24577;&#23545;&#20110;&#35782;&#21035;&#24847;&#22806;&#34892;&#20026;&#24182;&#25552;&#39640;&#20854;&#24615;&#33021;&#21644;&#27491;&#24120;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#31181;&#35745;&#31639;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#21644;&#20998;&#24067;&#24335;&#35774;&#35745;&#20197;&#21450;&#22823;&#37327;&#30340;&#30417;&#25511;&#21442;&#25968;&#65292;&#24212;&#35813;&#37319;&#29992;&#33258;&#21160;&#21270;&#30417;&#25511;&#26041;&#27861;&#12290;&#36825;&#26679;&#30340;&#33258;&#21160;&#30417;&#25511;&#26041;&#27861;&#36824;&#24212;&#20855;&#26377;&#36866;&#24212;&#35745;&#31639;&#31995;&#32479;&#25345;&#32493;&#21464;&#21270;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36824;&#24212;&#33021;&#22815;&#22312;&#36866;&#24403;&#30340;&#26102;&#38388;&#20869;&#35782;&#21035;&#20986;&#26377;&#29992;&#30340;&#34892;&#20026;&#24322;&#24120;&#65292;&#20197;&#25191;&#34892;&#30456;&#24212;&#30340;&#21453;&#24212;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#36731;&#37327;&#32423;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35745;&#31639;&#31995;&#32479;&#19978;&#30340;&#25805;&#20316;&#25968;&#25454;&#27979;&#37327;&#26469;&#23454;&#29616;&#36817;&#23454;&#26102;&#24322;&#24120;&#26816;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21482;&#38656;&#35201;4&#20010;&#23567;&#26102;&#30340;&#25968;&#25454;&#21644;&#27599;&#20010;&#35757;&#32451;&#36807;&#31243;&#30340;50&#20010;epochs&#65292;&#23601;&#33021;&#20934;&#30830;&#22320;&#21453;&#26144;&#35745;&#31639;&#31995;&#32479;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monitoring the status of large computing systems is essential to identify unexpected behavior and improve their performance and uptime. However, due to the large-scale and distributed design of such computing systems as well as a large number of monitoring parameters, automated monitoring methods should be applied. Such automatic monitoring methods should also have the ability to adapt themselves to the continuous changes in the computing system. In addition, they should be able to identify behavioral anomalies in useful time, to perform appropriate reactions. This work proposes a general lightweight and unsupervised method for near real-time anomaly detection using operational data measurement on large computing systems. The proposed model requires as little as 4 hours of data and 50 epochs for each training process to accurately resemble the behavioral pattern of computing systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22810;&#23610;&#24230;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#22823;&#23610;&#24230;&#21644;&#23567;&#23610;&#24230;&#21160;&#21147;&#23398;&#65292;&#24182;&#22312;&#27491;&#20132;&#22522;&#20989;&#25968;&#31354;&#38388;&#20013;&#36817;&#20284;&#23567;&#23610;&#24230;&#31995;&#32479;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#28082;&#20307;&#21160;&#21147;&#23398;&#38382;&#39064;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#36739;&#39640;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05067</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#23610;&#24230;&#24314;&#27169;&#65306;&#20174;&#22797;&#26434;&#31995;&#32479;&#30340;&#22823;&#23610;&#24230;&#21160;&#21147;&#23398;&#21040;&#23567;&#23610;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multiscale Modelling with Physics-informed Neural Network: from Large-scale Dynamics to Small-scale Predictions in Complex Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22810;&#23610;&#24230;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#22823;&#23610;&#24230;&#21644;&#23567;&#23610;&#24230;&#21160;&#21147;&#23398;&#65292;&#24182;&#22312;&#27491;&#20132;&#22522;&#20989;&#25968;&#31354;&#38388;&#20013;&#36817;&#20284;&#23567;&#23610;&#24230;&#31995;&#32479;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#28082;&#20307;&#21160;&#21147;&#23398;&#38382;&#39064;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#36739;&#39640;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23610;&#24230;&#29616;&#35937;&#22312;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#23545;&#20110;&#20934;&#30830;&#26377;&#25928;&#22320;&#39044;&#27979;&#22797;&#26434;&#31995;&#32479;&#20013;&#30340;&#22810;&#23610;&#24230;&#21160;&#21147;&#23398;&#25552;&#20986;&#20102;&#26222;&#36941;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35299;&#32806;&#26041;&#27861;&#23545;&#22810;&#23610;&#24230;&#21160;&#21147;&#23398;&#36827;&#34892;&#34920;&#24449;&#30340;&#26032;&#30340;&#27714;&#35299;&#27169;&#24335;&#12290;&#36890;&#36807;&#29420;&#31435;&#22320;&#24314;&#27169;&#22823;&#23610;&#24230;&#21160;&#21147;&#23398;&#65292;&#24182;&#23558;&#23567;&#23610;&#24230;&#21160;&#21147;&#23398;&#35270;&#20026;&#20174;&#23646;&#31995;&#32479;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#35889;PINN&#26041;&#27861;&#65292;&#22312;&#27491;&#20132;&#22522;&#20989;&#25968;&#31354;&#38388;&#20013;&#25509;&#36817;&#23567;&#23610;&#24230;&#31995;&#32479;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#21253;&#25324;&#19968;&#32500;Kuramot-Sivashinsky (KS)&#26041;&#31243;&#12289;&#20108;&#32500;&#21644;&#19977;&#32500;Navier-Stokes (NS)&#26041;&#31243;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#28082;&#20307;&#21160;&#21147;&#23398;&#38382;&#39064;&#20013;&#30340;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#28145;&#20837;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#22312;&#26356;&#22797;&#26434;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#38750;&#22343;&#21248;&#32593;&#26684;&#12289;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#12289;&#24102;&#22122;&#22768;&#30340;&#22823;&#23610;&#24230;&#25968;&#25454;&#21644;&#39640;&#32500;&#23567;&#23610;&#24230;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiscale phenomena manifest across various scientific domains, presenting a ubiquitous challenge in accurately and effectively predicting multiscale dynamics in complex systems. In this paper, a novel solving mode is proposed for characterizing multiscale dynamics through a decoupling method. By modelling large-scale dynamics independently and treating small-scale dynamics as a slaved system, a Spectral PINN is developed to approach the small-scale system in an orthogonal basis functional space. The effectiveness of the method is demonstrated through extensive numerical experiments, including one-dimensional Kuramot-Sivashinsky (KS) equation, two- and three-dimensional Navier-Stokes (NS) equations, showcasing its versatility in addressing problems of fluid dynamics. Furthermore, we also delve into the application of the proposed approach to more complex problems, including non-uniform meshes, complex geometries, large-scale data with noise, and high-dimensional small-scale dynamics. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#22312;Federated Learning&#20013;&#20351;&#29992;&#33258;&#36866;&#24212;&#32858;&#21512;&#26435;&#37325;&#26469;&#35782;&#21035;&#23545;&#29305;&#23450;&#23398;&#20064;&#30446;&#26631;&#26368;&#26377;&#30410;&#30340;&#23458;&#25143;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#32463;&#36807;&#23454;&#35777;&#35780;&#20272;&#21457;&#29616;&#65292;&#20351;&#29992;&#35813;&#31639;&#27861;&#24341;&#23548;&#30340;&#21512;&#20316;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#36825;&#20026;&#26356;&#21152;&#31616;&#21270;&#21644;&#26377;&#25928;&#30340;Federated Learning&#23454;&#29616;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2402.05050</link><description>&lt;p&gt;
Federated Learning&#33021;&#22815;&#25214;&#21040;&#26377;&#30410;&#30340;&#22909;&#21451;
&lt;/p&gt;
&lt;p&gt;
Federated Learning Can Find Friends That Are Beneficial
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#22312;Federated Learning&#20013;&#20351;&#29992;&#33258;&#36866;&#24212;&#32858;&#21512;&#26435;&#37325;&#26469;&#35782;&#21035;&#23545;&#29305;&#23450;&#23398;&#20064;&#30446;&#26631;&#26368;&#26377;&#30410;&#30340;&#23458;&#25143;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#32463;&#36807;&#23454;&#35777;&#35780;&#20272;&#21457;&#29616;&#65292;&#20351;&#29992;&#35813;&#31639;&#27861;&#24341;&#23548;&#30340;&#21512;&#20316;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#36825;&#20026;&#26356;&#21152;&#31616;&#21270;&#21644;&#26377;&#25928;&#30340;Federated Learning&#23454;&#29616;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;Federated Learning (FL)&#20013;&#65292;&#20998;&#24067;&#24335;&#24615;&#36136;&#21644;&#23458;&#25143;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#26082;&#24102;&#26469;&#20102;&#26426;&#20250;&#65292;&#20063;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#34429;&#28982;&#23458;&#25143;&#20043;&#38388;&#30340;&#21512;&#20316;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#65292;&#20294;&#24182;&#19981;&#26159;&#25152;&#26377;&#30340;&#21512;&#20316;&#37117;&#26159;&#26377;&#30410;&#30340;&#65307;&#26377;&#20123;&#29978;&#33267;&#21487;&#33021;&#26159;&#26377;&#23475;&#30340;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#20026;&#21442;&#19982;FL&#35757;&#32451;&#30340;&#23458;&#25143;&#20998;&#37197;&#33258;&#36866;&#24212;&#30340;&#32858;&#21512;&#26435;&#37325;&#65292;&#35782;&#21035;&#20986;&#25968;&#25454;&#20998;&#24067;&#23545;&#29305;&#23450;&#23398;&#20064;&#30446;&#26631;&#26368;&#26377;&#30410;&#30340;&#23458;&#25143;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#32858;&#21512;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#19982;&#20165;&#32858;&#21512;&#20855;&#26377;&#30456;&#21516;&#25968;&#25454;&#20998;&#24067;&#30340;&#23458;&#25143;&#25509;&#25910;&#30340;&#26356;&#26032;&#30340;&#26041;&#27861;&#19981;&#30456;&#19978;&#19979;&#12290;&#27492;&#22806;&#65292;&#32463;&#39564;&#35777;&#26126;&#65292;&#30001;&#25105;&#20204;&#30340;&#31639;&#27861;&#24341;&#23548;&#30340;&#21512;&#20316;&#20248;&#20110;&#20256;&#32479;&#30340;FL&#26041;&#27861;&#12290;&#36825;&#24378;&#35843;&#20102;&#23457;&#24910;&#36873;&#25321;&#23458;&#25143;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#20026;&#26410;&#26469;&#26356;&#31616;&#21270;&#21644;&#26377;&#25928;&#30340;FL&#23454;&#29616;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Federated Learning (FL), the distributed nature and heterogeneity of client data present both opportunities and challenges. While collaboration among clients can significantly enhance the learning process, not all collaborations are beneficial; some may even be detrimental. In this study, we introduce a novel algorithm that assigns adaptive aggregation weights to clients participating in FL training, identifying those with data distributions most conducive to a specific learning objective. We demonstrate that our aggregation method converges no worse than the method that aggregates only the updates received from clients with the same data distribution. Furthermore, empirical evaluations consistently reveal that collaborations guided by our algorithm outperform traditional FL approaches. This underscores the critical role of judicious client selection and lays the foundation for more streamlined and effective FL implementations in the coming years.
&lt;/p&gt;</description></item><item><title>SALAD-Bench&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#23433;&#20840;&#22522;&#20934;&#65292;&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#30340;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#20197;&#21450;&#23545;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#26377;&#25928;&#31649;&#29702;&#21644;&#20445;&#25252;&#12290;</title><link>https://arxiv.org/abs/2402.05044</link><description>&lt;p&gt;
SALAD-Bench: &#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#23618;&#27425;&#21270;&#21644;&#20840;&#38754;&#24615;&#23433;&#20840;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05044
&lt;/p&gt;
&lt;p&gt;
SALAD-Bench&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#23433;&#20840;&#22522;&#20934;&#65292;&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#30340;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#20197;&#21450;&#23545;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#26377;&#25928;&#31649;&#29702;&#21644;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#20013;&#65292;&#30830;&#20445;&#24378;&#22823;&#30340;&#23433;&#20840;&#25514;&#26045;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#20851;&#38190;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;LLMs&#12289;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#23433;&#20840;&#22522;&#20934;&#65292;&#31216;&#20026;SALAD-Bench&#12290;SALAD-Bench&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#22810;&#26679;&#30340;&#29305;&#24615;&#65292;&#20197;&#21450;&#36328;&#19977;&#20010;&#23618;&#27425;&#30340;&#32454;&#33268;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#22522;&#20934;&#12290;SALAD-Bench&#36890;&#36807;&#23545;&#26631;&#20934;&#26597;&#35810;&#21644;&#22797;&#26434;&#26597;&#35810;&#65288;&#21253;&#25324;&#25915;&#20987;&#12289;&#38450;&#24481;&#20462;&#25913;&#21644;&#22810;&#39033;&#36873;&#25321;&#65289;&#30340;&#31934;&#24515;&#35774;&#35745;&#65292;&#26377;&#25928;&#31649;&#29702;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#30830;&#20445;&#26080;&#32541;&#21487;&#38752;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35780;&#20272;&#22120;&#65306;&#22522;&#20110;LLM&#30340;MD-Judge&#65292;&#19987;&#27880;&#20110;&#25915;&#20987;&#22686;&#24378;&#26597;&#35810;&#30340;&#38382;&#31572;&#23545;&#35780;&#20272;&#12290;&#20197;&#19978;&#32452;&#20214;&#23558;SALAD-Bench&#20174;&#26631;&#20934;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#25193;&#23637;&#21040;&#20102;LLM&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#35780;&#20272;&#65292;&#30830;&#20445;&#20102;&#32852;&#21512;&#30446;&#26631;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#34920;&#21387;&#32553;&#26041;&#27861;CTRL&#65292;&#36890;&#36807;&#20248;&#21270;&#36215;&#28857;&#21644;&#31934;&#32454;&#30340;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#26799;&#24230;&#21305;&#37197;&#26041;&#21521;&#23548;&#33268;&#30340;&#35757;&#32451;&#36712;&#36857;&#20559;&#24046;&#21644;&#32047;&#31215;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04924</link><description>&lt;p&gt;
&#20004;&#20010;&#20132;&#26131;&#19981;&#20250;&#22256;&#25200;&#65306;&#36890;&#36807;&#26500;&#36896;&#21512;&#29702;&#30340;&#26799;&#24230;&#21305;&#37197;&#26469;&#21387;&#32553;&#22270;&#34920;
&lt;/p&gt;
&lt;p&gt;
Two Trades is not Baffled: Condense Graph via Crafting Rational Gradient Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#34920;&#21387;&#32553;&#26041;&#27861;CTRL&#65292;&#36890;&#36807;&#20248;&#21270;&#36215;&#28857;&#21644;&#31934;&#32454;&#30340;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#26799;&#24230;&#21305;&#37197;&#26041;&#21521;&#23548;&#33268;&#30340;&#35757;&#32451;&#36712;&#36857;&#20559;&#24046;&#21644;&#32047;&#31215;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#22270;&#34920;&#19978;&#35757;&#32451;&#24050;&#32463;&#22312;&#22270;&#34920;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#20294;&#20854;&#25104;&#26412;&#21644;&#23384;&#20648;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#20316;&#20026;&#26368;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#20043;&#19968;&#65292;&#22270;&#34920;&#21387;&#32553;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#21305;&#37197;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#23558;&#23436;&#25972;&#30340;&#22270;&#34920;&#21387;&#32553;&#25104;&#26356;&#31616;&#27905;&#20294;&#20449;&#24687;&#20016;&#23500;&#30340;&#21512;&#25104;&#38598;&#12290;&#23613;&#31649;&#20196;&#20154;&#40723;&#33310;&#65292;&#20294;&#36825;&#20123;&#31574;&#30053;&#20027;&#35201;&#24378;&#35843;&#26799;&#24230;&#30340;&#21305;&#37197;&#26041;&#21521;&#65292;&#20174;&#32780;&#23548;&#33268;&#35757;&#32451;&#36712;&#36857;&#30340;&#20559;&#24046;&#12290;&#36825;&#31181;&#20559;&#24046;&#36827;&#19968;&#27493;&#30001;&#21387;&#32553;&#21644;&#35780;&#20272;&#38454;&#27573;&#20043;&#38388;&#30340;&#24046;&#24322;&#25918;&#22823;&#65292;&#26368;&#32456;&#23548;&#33268;&#32047;&#31215;&#35823;&#24046;&#65292;&#23545;&#21387;&#32553;&#22270;&#34920;&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\textbf{C}raf\textbf{T}ing \textbf{R}ationa\textbf{L} trajectory&#65288;\textbf{CTRL}&#65289;&#30340;&#26032;&#22411;&#22270;&#34920;&#21387;&#32553;&#26041;&#27861;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#25509;&#36817;&#21407;&#22987;&#25968;&#25454;&#38598;&#29305;&#24449;&#20998;&#24067;&#30340;&#20248;&#21270;&#36215;&#28857;&#21644;&#19968;&#20010;&#26356;&#31934;&#32454;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training on large-scale graphs has achieved remarkable results in graph representation learning, but its cost and storage have raised growing concerns. As one of the most promising directions, graph condensation methods address these issues by employing gradient matching, aiming to condense the full graph into a more concise yet information-rich synthetic set. Though encouraging, these strategies primarily emphasize matching directions of the gradients, which leads to deviations in the training trajectories. Such deviations are further magnified by the differences between the condensation and evaluation phases, culminating in accumulated errors, which detrimentally affect the performance of the condensed graphs. In light of this, we propose a novel graph condensation method named \textbf{C}raf\textbf{T}ing \textbf{R}ationa\textbf{L} trajectory (\textbf{CTRL}), which offers an optimized starting point closer to the original dataset's feature distribution and a more refined strategy for 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Stable Audio&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#28508;&#22312;&#25193;&#25955;&#21644;&#26465;&#20214;&#21270;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#29983;&#25104;44.1kHz&#31435;&#20307;&#22768;&#38899;&#20048;&#21644;&#38899;&#25928;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;Stable Audio&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#32467;&#26500;&#21644;&#31435;&#20307;&#22768;&#30340;&#38899;&#20048;&#65292;&#24182;&#22312;&#24615;&#33021;&#35780;&#20272;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.04825</link><description>&lt;p&gt;
&#24555;&#36895;&#23450;&#26102;&#26465;&#20214;&#19979;&#30340;&#28508;&#22312;&#38899;&#39057;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Fast Timing-Conditioned Latent Audio Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Stable Audio&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#28508;&#22312;&#25193;&#25955;&#21644;&#26465;&#20214;&#21270;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#29983;&#25104;44.1kHz&#31435;&#20307;&#22768;&#38899;&#20048;&#21644;&#38899;&#25928;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;Stable Audio&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#32467;&#26500;&#21644;&#31435;&#20307;&#22768;&#30340;&#38899;&#20048;&#65292;&#24182;&#22312;&#24615;&#33021;&#35780;&#20272;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#38271;&#31687;44.1kHz&#31435;&#20307;&#22768;&#38899;&#39057;&#21487;&#33021;&#23545;&#35745;&#31639;&#35201;&#27714;&#24456;&#39640;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#24182;&#27809;&#26377;&#35299;&#20915;&#38899;&#20048;&#21644;&#38899;&#25928;&#22312;&#25345;&#32493;&#26102;&#38388;&#19978;&#30340;&#33258;&#28982;&#21464;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#20197;&#39640;&#25928;&#26041;&#24335;&#29983;&#25104;&#38271;&#31687;&#12289;&#21487;&#21464;&#38271;&#24230;&#30340;44.1kHz&#31435;&#20307;&#22768;&#38899;&#20048;&#21644;&#38899;&#25928;&#12290;Stable Audio&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#65292;&#20854;&#28508;&#22312;&#24615;&#36136;&#30001;&#19968;&#20010;&#20840;&#21367;&#31215;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23450;&#20041;&#12290;&#23427;&#19981;&#20165;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#36824;&#22522;&#20110;&#26102;&#38388;&#23884;&#20837;&#65292;&#20351;&#24471;&#21487;&#20197;&#23545;&#29983;&#25104;&#30340;&#38899;&#20048;&#21644;&#38899;&#25928;&#30340;&#20869;&#23481;&#21644;&#38271;&#24230;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#12290;&#22312;A100 GPU&#19978;&#65292;Stable Audio&#33021;&#22815;&#22312;8&#31186;&#20869;&#20197;44.1kHz&#30340;&#36895;&#24230;&#28210;&#26579;&#38271;&#36798;95&#31186;&#30340;&#31435;&#20307;&#22768;&#20449;&#21495;&#12290;&#23613;&#31649;&#35745;&#31639;&#25928;&#29575;&#39640;&#19988;&#25512;&#29702;&#36895;&#24230;&#24555;&#65292;&#20294;&#23427;&#22312;&#20004;&#20010;&#20844;&#24320;&#30340;&#25991;&#26412;-&#38899;&#20048;&#21644;&#38899;&#39057;&#22522;&#20934;&#20013;&#20173;&#28982;&#26159;&#26368;&#22909;&#30340;&#65292;&#24182;&#19988;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;&#23427;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#32467;&#26500;&#21644;&#31435;&#20307;&#22768;&#38899;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating long-form 44.1kHz stereo audio from text prompts can be computationally demanding. Further, most previous works do not tackle that music and sound effects naturally vary in their duration. Our research focuses on the efficient generation of long-form, variable-length stereo music and sounds at 44.1kHz using text prompts with a generative model. Stable Audio is based on latent diffusion, with its latent defined by a fully-convolutional variational autoencoder. It is conditioned on text prompts as well as timing embeddings, allowing for fine control over both the content and length of the generated music and sounds. Stable Audio is capable of rendering stereo signals of up to 95 sec at 44.1kHz in 8 sec on an A100 GPU. Despite its compute efficiency and fast inference, it is one of the best in two public text-to-music and -audio benchmarks and, differently from state-of-the-art models, can generate music with structure and stereo sounds.
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#22810;&#32500;&#26631;&#24230;&#26159;&#20851;&#20110;&#23558;&#36317;&#31163;&#20449;&#24687;&#23884;&#20837;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#36807;&#31243;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#23545;&#35937;&#38598;&#19981;&#26029;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#25972;&#20010;&#23884;&#20837;&#38382;&#39064;&#24207;&#21015;&#35270;&#20026;&#19968;&#20010;&#22266;&#23450;&#31354;&#38388;&#20013;&#30340;&#19968;&#31995;&#21015;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#21644;&#32467;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.04436</link><description>&lt;p&gt;
&#36830;&#32493;&#22810;&#32500;&#26631;&#24230;
&lt;/p&gt;
&lt;p&gt;
Continuous Multidimensional Scaling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04436
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#22810;&#32500;&#26631;&#24230;&#26159;&#20851;&#20110;&#23558;&#36317;&#31163;&#20449;&#24687;&#23884;&#20837;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#36807;&#31243;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#23545;&#35937;&#38598;&#19981;&#26029;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#25972;&#20010;&#23884;&#20837;&#38382;&#39064;&#24207;&#21015;&#35270;&#20026;&#19968;&#20010;&#22266;&#23450;&#31354;&#38388;&#20013;&#30340;&#19968;&#31995;&#21015;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#21644;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#32500;&#26631;&#24230;(MDS)&#26159;&#23558;&#20851;&#20110;&#19968;&#32452;$n$&#20010;&#23545;&#35937;&#30340;&#36317;&#31163;&#20449;&#24687;&#23884;&#20837;&#21040;$d$&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#36807;&#31243;&#12290;&#26368;&#21021;&#30001;&#24515;&#29702;&#27979;&#37327;&#23398;&#30028;&#26500;&#24605;&#65292;MDS&#20851;&#27880;&#30340;&#26159;&#23884;&#20837;&#21040;&#19968;&#32452;&#22266;&#23450;&#23545;&#35937;&#19978;&#30340;&#19968;&#32452;&#22266;&#23450;&#36317;&#31163;&#12290;&#29616;&#20195;&#20851;&#27880;&#30340;&#38382;&#39064;&#26356;&#24120;&#28041;&#21450;&#21040;&#30740;&#31350;&#19982;&#19968;&#32452;&#19981;&#26029;&#22686;&#21152;&#30340;&#23545;&#35937;&#30456;&#20851;&#32852;&#30340;&#19968;&#31995;&#21015;&#36317;&#31163;&#30340;&#26497;&#38480;&#34892;&#20026;&#65292;&#22914;&#22312;&#38543;&#26426;&#22270;&#30340;&#32479;&#35745;&#25512;&#26029;&#30340;&#28176;&#36817;&#29702;&#35770;&#20013;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;&#28857;&#21040;&#38598;&#21512;&#26144;&#23556;&#29702;&#35770;&#20013;&#30340;&#26631;&#20934;&#32467;&#26524;&#34920;&#26126;&#65292;&#33509;$n$&#22266;&#23450;&#65292;&#21017;&#23884;&#20837;&#32467;&#26500;&#30340;&#26497;&#38480;&#26159;&#26497;&#38480;&#36317;&#31163;&#30340;&#23884;&#20837;&#32467;&#26500;&#12290;&#20294;&#22914;&#26524;$n$&#22686;&#21152;&#24590;&#20040;&#21150;&#21602;&#65311;&#37027;&#20040;&#23601;&#38656;&#35201;&#37325;&#26032;&#21046;&#23450;MDS&#65292;&#20197;&#20415;&#23558;&#25972;&#20010;&#23884;&#20837;&#38382;&#39064;&#24207;&#21015;&#35270;&#20026;&#19968;&#20010;&#22266;&#23450;&#31354;&#38388;&#20013;&#30340;&#19968;&#31995;&#21015;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#26679;&#19968;&#31181;&#37325;&#26032;&#21046;&#23450;&#65292;&#24182;&#25512;&#23548;&#20986;&#19968;&#20123;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multidimensional scaling (MDS) is the act of embedding proximity information about a set of $n$ objects in $d$-dimensional Euclidean space. As originally conceived by the psychometric community, MDS was concerned with embedding a fixed set of proximities associated with a fixed set of objects. Modern concerns, e.g., that arise in developing asymptotic theories for statistical inference on random graphs, more typically involve studying the limiting behavior of a sequence of proximities associated with an increasing set of objects. Standard results from the theory of point-to-set maps imply that, if $n$ is fixed, then the limit of the embedded structures is the embedded structure of the limiting proximities. But what if $n$ increases? It then becomes necessary to reformulate MDS so that the entire sequence of embedding problems can be viewed as a sequence of optimization problems in a fixed space. We present such a reformulation and derive some consequences.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#28201;&#21644;&#24494;&#31215;&#20998;&#30340;&#29702;&#35770;&#21644;&#24037;&#20855;&#65292;&#26469;&#25913;&#36827;&#30446;&#21069;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#25968;&#23398;&#25197;&#26354;&#26041;&#27861;&#65292;&#29305;&#21035;&#24378;&#35843;&#19982;&#20960;&#20309;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#30340;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04163</link><description>&lt;p&gt;
Tempered Calculus for ML: &#24212;&#29992;&#20110;&#21452;&#26354;&#27169;&#22411;&#23884;&#20837;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Tempered Calculus for ML: Application to Hyperbolic Model Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#28201;&#21644;&#24494;&#31215;&#20998;&#30340;&#29702;&#35770;&#21644;&#24037;&#20855;&#65292;&#26469;&#25913;&#36827;&#30446;&#21069;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#25968;&#23398;&#25197;&#26354;&#26041;&#27861;&#65292;&#29305;&#21035;&#24378;&#35843;&#19982;&#20960;&#20309;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#25968;&#23398;&#25197;&#26354;&#26412;&#36136;&#19978;&#37117;&#26159;&#31215;&#20998;&#30340;&#65306;$f$-divergences, Bregman divergences, (regularized) optimal transport distances, integral probability metrics, geodesic distances&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#22522;&#20110;&#28201;&#21644;&#24494;&#31215;&#20998;&#30340;&#29702;&#35770;&#21644;&#24037;&#20855;&#65292;&#21487;&#20197;&#24110;&#21161;&#25913;&#36827;&#36825;&#20123;&#25197;&#26354;&#20197;&#26356;&#22909;&#22320;&#28385;&#36275;&#26426;&#22120;&#23398;&#20064;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#20174;&#24191;&#20041;&#30340;&#40654;&#26364;&#31215;&#20998;&#24320;&#22987;&#65292;&#36825;&#31181;&#31215;&#20998;&#36824;&#21253;&#25324;&#19981;&#20005;&#26684;&#21487;&#21152;&#30340;&#20989;&#25968;&#65292;&#32780;&#26159;&#26356;&#19968;&#33324;&#22320;&#26159;$t$-&#21487;&#21152;&#30340;&#65292;&#23601;&#20687;&#38750;&#26497;&#38480;&#32479;&#35745;&#21147;&#23398;&#20013;&#30340;&#24773;&#20917;&#19968;&#26679;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;Volterra&#30340;&#20056;&#31215;&#31215;&#20998;&#20316;&#20026;&#29305;&#20363;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#65288;&#27431;&#20960;&#37324;&#24471;&#65289;&#23548;&#25968;&#30340;&#25193;&#23637;&#26469;&#25512;&#24191;&#22522;&#26412;&#23450;&#29702;&#12290;&#36825;&#20123;&#25512;&#24191;&#20197;&#21450;&#19968;&#31995;&#21015;&#26356;&#20855;&#20307;&#30340;&#23450;&#29702;&#20026;&#32467;&#26524;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#20197;&#31616;&#21333;&#30340;&#26041;&#24335;&#19987;&#38376;&#35774;&#35745;&#12289;&#25913;&#21464;&#25110;&#25913;&#21464;&#25197;&#26354;&#24230;&#37327;&#30340;&#22522;&#26412;&#29305;&#24615;&#65292;&#29305;&#21035;&#24378;&#35843;&#19982;&#20960;&#20309;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most mathematical distortions used in ML are fundamentally integral in nature: $f$-divergences, Bregman divergences, (regularized) optimal transport distances, integral probability metrics, geodesic distances, etc. In this paper, we unveil a grounded theory and tools which can help improve these distortions to better cope with ML requirements. We start with a generalization of Riemann integration that also encapsulates functions that are not strictly additive but are, more generally, $t$-additive, as in nonextensive statistical mechanics. Notably, this recovers Volterra's product integral as a special case. We then generalize the Fundamental Theorem of calculus using an extension of the (Euclidean) derivative. This, along with a series of more specific Theorems, serves as a basis for results showing how one can specifically design, alter, or change fundamental properties of distortion measures in a simple way, with a special emphasis on geometric- and ML-related properties that are the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#22686;&#24378;&#24418;&#24335;&#30340;&#20219;&#21153;&#25351;&#23548;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#24182;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#30340;&#25104;&#21151;&#24212;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)</title><link>https://arxiv.org/abs/2402.04154</link><description>&lt;p&gt;
&#12298;&#35835;&#29609;&#28216;&#25103;&#65288;R2-Play&#65289;: &#22810;&#27169;&#24577;&#28216;&#25103;&#25351;&#23548;&#19979;&#30340;&#20915;&#31574; Transformer&#12299;
&lt;/p&gt;
&lt;p&gt;
Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#22686;&#24378;&#24418;&#24335;&#30340;&#20219;&#21153;&#25351;&#23548;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#24182;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#30340;&#25104;&#21151;&#24212;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#24320;&#21457;&#19968;&#27454;&#36890;&#29992;&#26234;&#33021;&#20307;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#30446;&#26631;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#20219;&#21153;&#30340;&#22823;&#37327;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#20219;&#21153;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#22312;&#25193;&#23637;&#21040;&#26032;&#20219;&#21153;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23558;&#25991;&#26412;&#25351;&#23548;&#25110;&#35270;&#35273;&#36712;&#36857;&#25972;&#21512;&#21040;&#20915;&#31574;&#32593;&#32476;&#20013;&#65292;&#25552;&#20379;&#20219;&#21153;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#20195;&#34920;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#35266;&#23519;&#21040;&#20165;&#20381;&#36182;&#20110;&#25991;&#26412;&#25351;&#23548;&#25110;&#35270;&#35273;&#36712;&#36857;&#23545;&#20110;&#20934;&#30830;&#20256;&#36798;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#22686;&#24378;&#26234;&#33021;&#20307;&#20219;&#21153;&#25351;&#23548;&#30340;&#24418;&#24335;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#65292;&#20174;&#32780;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#21463;&#21040;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#35270;&#35273;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#35270;&#20026;&#19968;&#20010;&#38271;&#26399;&#35270;&#35273;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)
&lt;/p&gt;
&lt;p&gt;
Developing a generalist agent is a longstanding objective in artificial intelligence. Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning.However, these works encounter challenges in extending their capabilities to new tasks.Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction.However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks.This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a "read-to-play" capability.Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21040;&#32039;&#25903;&#25345;&#22522;&#30340;&#26680;&#20998;&#32452;&#30340;&#36890;&#29992;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#21487;&#20197;&#29992;&#20110;&#38477;&#20302;&#39640;&#26031;&#36807;&#31243;&#30340;&#35757;&#32451;&#21644;&#39044;&#27979;&#26102;&#38388;&#65292;&#24182;&#19988;&#36890;&#36807;&#36866;&#24403;&#30340;&#32447;&#24615;&#32452;&#21512;&#20135;&#29983;&#20102;$m$&#20010;&#32039;&#25903;&#25345;&#30340;&#26680;&#20998;&#32452;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.04022</link><description>&lt;p&gt;
&#19968;&#31181;&#20174;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21040;&#32039;&#25903;&#25345;&#22522;&#30340;&#26680;&#20998;&#32452;&#30340;&#36890;&#29992;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A General Theory for Kernel Packets: from state space model to compactly supported basis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04022
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21040;&#32039;&#25903;&#25345;&#22522;&#30340;&#26680;&#20998;&#32452;&#30340;&#36890;&#29992;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#21487;&#20197;&#29992;&#20110;&#38477;&#20302;&#39640;&#26031;&#36807;&#31243;&#30340;&#35757;&#32451;&#21644;&#39044;&#27979;&#26102;&#38388;&#65292;&#24182;&#19988;&#36890;&#36807;&#36866;&#24403;&#30340;&#32447;&#24615;&#32452;&#21512;&#20135;&#29983;&#20102;$m$&#20010;&#32039;&#25903;&#25345;&#30340;&#26680;&#20998;&#32452;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#30340;&#29366;&#24577;&#31354;&#38388;&#65288;SS&#65289;&#27169;&#22411;&#20844;&#24335;&#21487;&#20197;&#23558;&#20854;&#35757;&#32451;&#21644;&#39044;&#27979;&#26102;&#38388;&#38477;&#20302;&#21040;O&#65288;n&#65289;&#65288;n&#20026;&#25968;&#25454;&#28857;&#20010;&#25968;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;m&#32500;&#30340;GP&#30340;SS&#27169;&#22411;&#20844;&#24335;&#31561;&#20215;&#20110;&#25105;&#20204;&#24341;&#20837;&#30340;&#19968;&#20010;&#27010;&#24565;&#65292;&#31216;&#20026;&#36890;&#29992;&#21491;&#26680;&#20998;&#32452;&#65288;KP&#65289;&#65306;&#19968;&#31181;&#29992;&#20110;GP&#21327;&#26041;&#24046;&#20989;&#25968;K&#30340;&#21464;&#25442;&#65292;&#20351;&#24471;&#23545;&#20110;&#20219;&#24847;$t \leq t_1$&#65292;$0 \leq j \leq m-1$&#21644;$m+1$&#20010;&#36830;&#32493;&#28857;$t_i$&#65292;&#37117;&#28385;&#36275;$\sum_{i=0}^{m}a_iD_t^{(j)}K(t,t_i)=0$&#65292;&#20854;&#20013;${D}_t^{(j)}f(t)$&#34920;&#31034;&#22312;$t$&#19978;&#20316;&#29992;&#30340;&#31532;j&#38454;&#23548;&#25968;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#24605;&#24819;&#25193;&#23637;&#21040;&#20102;GP&#30340;&#21521;&#21518;SS&#27169;&#22411;&#20844;&#24335;&#65292;&#24471;&#21040;&#20102;&#19979;&#19968;&#20010;$m$&#20010;&#36830;&#32493;&#28857;&#30340;&#24038;&#26680;&#20998;&#32452;&#30340;&#27010;&#24565;&#65306;$\sum_{i=0}^{m}b_i{D}_t^{(j)}K(t,t_{m+i})=0$&#65292;&#23545;&#20110;&#20219;&#24847;$t\geq t_{2m}$&#12290;&#36890;&#36807;&#32467;&#21512;&#24038;&#21491;&#26680;&#20998;&#32452;&#65292;&#21487;&#20197;&#35777;&#26126;&#36825;&#20123;&#21327;&#26041;&#24046;&#20989;&#25968;&#30340;&#36866;&#24403;&#32447;&#24615;&#32452;&#21512;&#20135;&#29983;&#20102;$m$&#20010;&#32039;&#25903;&#25345;&#30340;&#26680;&#20998;&#32452;&#20989;&#25968;&#65306;&#23545;&#20110;&#20219;&#24847;$t\not\in(t_0,t_{2m})$&#21644;$j=0,\cdots,m-1$&#65292;$\phi^{(j)}(t)=0$&#12290;
&lt;/p&gt;
&lt;p&gt;
It is well known that the state space (SS) model formulation of a Gaussian process (GP) can lower its training and prediction time both to O(n) for n data points. We prove that an $m$-dimensional SS model formulation of GP is equivalent to a concept we introduce as the general right Kernel Packet (KP): a transformation for the GP covariance function $K$ such that $\sum_{i=0}^{m}a_iD_t^{(j)}K(t,t_i)=0$ holds for any $t \leq t_1$, 0 $\leq j \leq m-1$, and $m+1$ consecutive points $t_i$, where ${D}_t^{(j)}f(t) $ denotes $j$-th order derivative acting on $t$. We extend this idea to the backward SS model formulation of the GP, leading to the concept of the left KP for next $m$ consecutive points: $\sum_{i=0}^{m}b_i{D}_t^{(j)}K(t,t_{m+i})=0$ for any $t\geq t_{2m}$. By combining both left and right KPs, we can prove that a suitable linear combination of these covariance functions yields $m$ compactly supported KP functions: $\phi^{(j)}(t)=0$ for any $t\not\in(t_0,t_{2m})$ and $j=0,\cdots,m-1$
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36923;&#36753;&#35268;&#33539;&#24341;&#23548;&#19979;&#30340;&#21160;&#24577;&#20219;&#21153;&#37319;&#26679;&#65288;LSTS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#26681;&#25454;&#39640;&#32423;&#20219;&#21153;&#35268;&#33539;&#25351;&#23548;&#26234;&#33021;&#20307;&#22312;&#26368;&#23567;&#21270;&#29615;&#22659;&#20132;&#20114;&#27425;&#25968;&#30340;&#21516;&#26102;&#23454;&#29616;&#20174;&#21021;&#22987;&#29366;&#24577;&#21040;&#30446;&#26631;&#29366;&#24577;&#30340;&#24341;&#23548;&#12290;&#22312;&#32593;&#26684;&#19990;&#30028;&#23454;&#39564;&#20013;&#65292;LSTS&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#26102;&#38388;&#21040;&#38408;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.03678</link><description>&lt;p&gt;
&#36923;&#36753;&#35268;&#33539;&#24341;&#23548;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#21160;&#24577;&#20219;&#21153;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Logical Specifications-guided Dynamic Task Sampling for Reinforcement Learning Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36923;&#36753;&#35268;&#33539;&#24341;&#23548;&#19979;&#30340;&#21160;&#24577;&#20219;&#21153;&#37319;&#26679;&#65288;LSTS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#26681;&#25454;&#39640;&#32423;&#20219;&#21153;&#35268;&#33539;&#25351;&#23548;&#26234;&#33021;&#20307;&#22312;&#26368;&#23567;&#21270;&#29615;&#22659;&#20132;&#20114;&#27425;&#25968;&#30340;&#21516;&#26102;&#23454;&#29616;&#20174;&#21021;&#22987;&#29366;&#24577;&#21040;&#30446;&#26631;&#29366;&#24577;&#30340;&#24341;&#23548;&#12290;&#22312;&#32593;&#26684;&#19990;&#30028;&#23454;&#39564;&#20013;&#65292;LSTS&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#26102;&#38388;&#21040;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#20351;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#23398;&#20064;&#22810;&#26679;&#21270;&#34892;&#20026;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#26377;&#25928;&#30340;&#31574;&#30053;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#29615;&#22659;&#20132;&#20114;&#12290;&#20026;&#20102;&#20943;&#23569;&#26679;&#26412;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#20351;&#29992;&#39640;&#32423;&#20219;&#21153;&#35268;&#33539;&#65292;&#22914;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL$_f$&#65289;&#20844;&#24335;&#25110;&#22870;&#21169;&#26426;&#22120;&#65288;RM&#65289;&#65292;&#26469;&#25351;&#23548;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#36923;&#36753;&#35268;&#33539;&#24341;&#23548;&#19979;&#30340;&#21160;&#24577;&#20219;&#21153;&#37319;&#26679;&#65288;LSTS&#65289;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#26681;&#25454;&#39640;&#32423;&#20219;&#21153;&#35268;&#33539;&#25351;&#23548;&#26234;&#33021;&#20307;&#20174;&#21021;&#22987;&#29366;&#24577;&#21040;&#30446;&#26631;&#29366;&#24577;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#29615;&#22659;&#20132;&#20114;&#27425;&#25968;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;LSTS&#19981;&#20551;&#35774;&#29615;&#22659;&#21160;&#21147;&#23398;&#25110;&#22870;&#21169;&#26426;&#22120;&#30340;&#20449;&#24687;&#65292;&#24182;&#21160;&#24577;&#37319;&#26679;&#23548;&#33268;&#25104;&#21151;&#30446;&#26631;&#31574;&#30053;&#30340;&#26377;&#24076;&#26395;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#32593;&#26684;&#19990;&#30028;&#19978;&#35780;&#20272;&#20102;LSTS&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#26102;&#38388;&#21040;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has made significant strides in enabling artificial agents to learn diverse behaviors. However, learning an effective policy often requires a large number of environment interactions. To mitigate sample complexity issues, recent approaches have used high-level task specifications, such as Linear Temporal Logic (LTL$_f$) formulas or Reward Machines (RM), to guide the learning progress of the agent. In this work, we propose a novel approach, called Logical Specifications-guided Dynamic Task Sampling (LSTS), that learns a set of RL policies to guide an agent from an initial state to a goal state based on a high-level task specification, while minimizing the number of environmental interactions. Unlike previous work, LSTS does not assume information about the environment dynamics or the Reward Machine, and dynamically samples promising tasks that lead to successful goal policies. We evaluate LSTS on a gridworld and show that it achieves improved time-to-threshol
&lt;/p&gt;</description></item><item><title>IGUANe&#26159;&#19968;&#31181;&#19977;&#32500;&#36890;&#29992;CycleGAN&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#22495;&#30340;&#35757;&#32451;&#23454;&#29616;&#20102;&#33041;MR&#22270;&#20687;&#30340;&#22810;&#20013;&#24515;&#21327;&#35843;&#65292;&#20351;&#20854;&#25104;&#20026;&#36890;&#29992;&#29983;&#25104;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.03227</link><description>&lt;p&gt;
IGUANe: &#19968;&#31181;&#36866;&#29992;&#20110;&#33041;MR&#22270;&#20687;&#22810;&#20013;&#24515;&#21327;&#35843;&#30340;&#19977;&#32500;&#36890;&#29992;CycleGAN&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03227
&lt;/p&gt;
&lt;p&gt;
IGUANe&#26159;&#19968;&#31181;&#19977;&#32500;&#36890;&#29992;CycleGAN&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#22495;&#30340;&#35757;&#32451;&#23454;&#29616;&#20102;&#33041;MR&#22270;&#20687;&#30340;&#22810;&#20013;&#24515;&#21327;&#35843;&#65292;&#20351;&#20854;&#25104;&#20026;&#36890;&#29992;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;MRI&#30740;&#31350;&#20013;&#65292;&#26469;&#33258;&#22810;&#20010;&#37319;&#38598;&#28857;&#30340;&#22270;&#20687;&#25968;&#25454;&#30340;&#32858;&#21512;&#21487;&#20197;&#22686;&#21152;&#26679;&#26412;&#22823;&#23567;&#65292;&#20294;&#21487;&#33021;&#24341;&#20837;&#38459;&#30861;&#21518;&#32493;&#20998;&#26512;&#19968;&#33268;&#24615;&#30340;&#19982;&#37319;&#38598;&#28857;&#30456;&#20851;&#30340;&#21464;&#24322;&#12290;&#22270;&#20687;&#32763;&#35793;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#21327;&#35843;MR&#22270;&#20687;&#36328;&#31449;&#28857;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;IGUANe&#65288;&#20855;&#26377;&#32479;&#19968;&#23545;&#25239;&#32593;&#32476;&#30340;&#22270;&#20687;&#29983;&#25104;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21407;&#22987;&#30340;&#19977;&#32500;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#22495;&#36716;&#25442;&#30340;&#20248;&#21183;&#21644;&#30452;&#25509;&#24212;&#29992;&#26679;&#24335;&#36716;&#31227;&#26041;&#27861;&#26469;&#23454;&#29616;&#22810;&#20013;&#24515;&#33041;MR&#22270;&#20687;&#21327;&#35843;&#12290;IGUANe&#36890;&#36807;&#22810;&#23545;&#19968;&#31574;&#30053;&#65292;&#38598;&#25104;&#20102;&#20219;&#24847;&#25968;&#37327;&#30340;&#22495;&#36827;&#34892;&#35757;&#32451;&#65292;&#25193;&#23637;&#20102;CycleGAN&#26550;&#26500;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#22270;&#20687;&#65292;&#29978;&#33267;&#26469;&#33258;&#26410;&#30693;&#37319;&#38598;&#28857;&#65292;&#20351;&#20854;&#25104;&#20026;&#21327;&#35843;&#30340;&#36890;&#29992;&#29983;&#25104;&#22120;&#12290;&#22312;&#30001;11&#21488;&#19981;&#21516;&#25195;&#25551;&#20202;&#30340;T1&#21152;&#26435;&#22270;&#20687;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;IGUANe&#22312;&#26410;&#35265;&#31449;&#28857;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In MRI studies, the aggregation of imaging data from multiple acquisition sites enhances sample size but may introduce site-related variabilities that hinder consistency in subsequent analyses. Deep learning methods for image translation have emerged as a solution for harmonizing MR images across sites. In this study, we introduce IGUANe (Image Generation with Unified Adversarial Networks), an original 3D model that leverages the strengths of domain translation and straightforward application of style transfer methods for multicenter brain MR image harmonization. IGUANe extends CycleGAN architecture by integrating an arbitrary number of domains for training through a many-to-one strategy. During inference, the model can be applied to any image, even from an unknown acquisition site, making it a universal generator for harmonization. Trained on a dataset comprising T1-weighted images from 11 different scanners, IGUANe was evaluated on data from unseen sites. The assessments included the
&lt;/p&gt;</description></item><item><title>BGE M3-&#23884;&#20837;&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#21516;&#26102;&#25191;&#34892;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#24182;&#33021;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#12290;&#20854;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20102;&#19968;&#31181;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21644;&#20248;&#21270;&#30340;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.03216</link><description>&lt;p&gt;
BGE M3-&#23884;&#20837;&#65306;&#36890;&#36807;&#33258;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03216
&lt;/p&gt;
&lt;p&gt;
BGE M3-&#23884;&#20837;&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#21516;&#26102;&#25191;&#34892;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#24182;&#33021;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#12290;&#20854;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20102;&#19968;&#31181;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21644;&#20248;&#21270;&#30340;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#31216;&#20026;M3-&#23884;&#20837;&#65292;&#20197;&#20854;&#22312;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#32780;&#33879;&#31216;&#12290;&#23427;&#21487;&#20197;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#23427;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#23884;&#20837;&#27169;&#22411;&#30340;&#19977;&#31181;&#24120;&#35265;&#26816;&#32034;&#21151;&#33021;&#65306;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#20026;&#29616;&#23454;&#19990;&#30028;&#30340;IR&#24212;&#29992;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#27169;&#22411;&#22522;&#30784;&#12290;&#23427;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#65292;&#20174;&#30701;&#21477;&#21040;&#38271;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#25991;&#26723;&#12290;M3-&#23884;&#20837;&#30340;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20197;&#19979;&#25216;&#26415;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#26469;&#33258;&#19981;&#21516;&#26816;&#32034;&#21151;&#33021;&#30340;&#30456;&#20851;&#24615;&#20998;&#25968;&#25972;&#21512;&#20026;&#25945;&#24072;&#20449;&#21495;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#20248;&#21270;&#20102;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35814;&#32454;&#20998;&#26512;&#20102;&#22522;&#20110;&#24494;&#29615;&#30340;&#30456;&#24178;&#20809;&#23398;GEMM&#21152;&#36895;&#22120;&#30340;&#32452;&#32455;&#65292;&#20854;&#36890;&#36807;&#20998;&#35010;&#12289;&#32858;&#21512;&#12289;&#35843;&#21046;&#12289;&#21152;&#26435;&#21644;&#27714;&#21644;&#31561;&#26041;&#24335;&#25805;&#20316;&#20809;&#20449;&#21495;&#20197;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#30697;&#38453;-&#30697;&#38453;&#20056;&#27861;&#65292;&#25552;&#39640;&#21534;&#21520;&#37327;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.03149</link><description>&lt;p&gt;
&#22522;&#20110;&#24494;&#29615;&#30340;&#30456;&#24178;&#20809;&#23398;GEMM&#21152;&#36895;&#22120;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comparative Analysis of Microrings Based Incoherent Photonic GEMM Accelerators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#20998;&#26512;&#20102;&#22522;&#20110;&#24494;&#29615;&#30340;&#30456;&#24178;&#20809;&#23398;GEMM&#21152;&#36895;&#22120;&#30340;&#32452;&#32455;&#65292;&#20854;&#36890;&#36807;&#20998;&#35010;&#12289;&#32858;&#21512;&#12289;&#35843;&#21046;&#12289;&#21152;&#26435;&#21644;&#27714;&#21644;&#31561;&#26041;&#24335;&#25805;&#20316;&#20809;&#20449;&#21495;&#20197;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#30697;&#38453;-&#30697;&#38453;&#20056;&#27861;&#65292;&#25552;&#39640;&#21534;&#21520;&#37327;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20960;&#31181;&#22522;&#20110;&#24494;&#29615;&#35856;&#25391;&#22120;&#65288;MRR&#65289;&#30340;&#27169;&#25311;&#20809;&#23398;&#26550;&#26500;&#65292;&#20197;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#21152;&#36895;&#36890;&#29992;&#30340;&#30697;&#38453;-&#30697;&#38453;&#20056;&#27861;&#65288;GEMM&#65289;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#21534;&#21520;&#37327;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;&#20026;&#20102;&#23454;&#29616;GEMM&#21151;&#33021;&#65292;&#36825;&#20123;&#22522;&#20110;MRR&#30340;&#26550;&#26500;&#19968;&#33324;&#36890;&#36807;&#20116;&#31181;&#19981;&#21516;&#30340;&#26041;&#24335;&#25805;&#20316;&#20809;&#20449;&#21495;&#65306;&#65288;i&#65289;&#23558;&#22810;&#20010;&#20809;&#20449;&#21495;&#20998;&#35010;&#65288;&#22797;&#21046;&#65289;&#20197;&#36798;&#21040;&#26576;&#31181;&#22810;&#20998;&#25903;&#65292;&#65288;ii&#65289;&#23558;&#22810;&#20010;&#20809;&#20449;&#21495;&#32858;&#21512;&#65288;&#22797;&#29992;&#65289;&#20197;&#36798;&#21040;&#26576;&#31181;&#22810;&#36755;&#20837;&#65292;&#65288;iii&#65289;&#35843;&#21046;&#20809;&#20449;&#21495;&#20197;&#23558;&#36755;&#20837;&#20540;&#21360;&#32622;&#20110;&#27169;&#25311;&#20449;&#21495;&#24133;&#24230;&#19978;&#65292;&#65288;iv&#65289;&#23545;&#35843;&#21046;&#30340;&#20809;&#20449;&#21495;&#36827;&#34892;&#21152;&#26435;&#65292;&#20197;&#23454;&#29616;&#27169;&#25311;&#36755;&#20837;&#26435;&#37325;&#30456;&#20056;&#65292;&#65288;v&#65289;&#23545;&#20809;&#20449;&#21495;&#36827;&#34892;&#27714;&#21644;&#12290;MRR&#22522;&#20110;&#30340;GEMM&#21152;&#36895;&#22120;&#20197;&#20219;&#24847;&#39034;&#24207;&#25191;&#34892;&#21069;&#22235;&#31181;&#20449;&#21495;&#25805;&#20316;&#65292;&#24573;&#30053;&#20102;&#36825;&#20123;&#25805;&#20316;&#39034;&#24207;&#23545;&#20854;&#24615;&#33021;&#30340;&#21487;&#33021;&#24433;&#21709;&#12290;&#26412;&#25991;&#23545;&#21152;&#36895;&#22120;&#32452;&#32455;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several microring resonator (MRR) based analog photonic architectures have been proposed to accelerate general matrix-matrix multiplications (GEMMs) in deep neural networks with exceptional throughput and energy efficiency. To implement GEMM functions, these MRR-based architectures, in general, manipulate optical signals in five different ways: (i) Splitting (copying) of multiple optical signals to achieve a certain fan-out, (ii) Aggregation (multiplexing) of multiple optical signals to achieve a certain fan-in, (iii) Modulation of optical signals to imprint input values onto analog signal amplitude, (iv) Weighting of modulated optical signals to achieve analog input-weight multiplication, (v) Summation of optical signals. The MRR-based GEMM accelerators undertake the first four ways of signal manipulation in an arbitrary order ignoring the possible impact of the order of these manipulations on their performance. In this paper, we conduct a detailed analysis of accelerator organization
&lt;/p&gt;</description></item><item><title>Taylor&#35270;&#39057;&#26159;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#26684;&#24335;&#65292;&#29992;&#20110;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#21160;&#20316;&#25552;&#21462;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;Taylor&#23637;&#24320;&#36817;&#20284;&#35745;&#31639;&#38544;&#21547;&#30340;&#21160;&#20316;&#25552;&#21462;&#20989;&#25968;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#21160;&#20316;&#25552;&#21462;&#20013;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03019</link><description>&lt;p&gt;
&#29992;&#20110;&#21160;&#20316;&#35782;&#21035;&#30340;Taylor&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
Taylor Videos for Action Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03019
&lt;/p&gt;
&lt;p&gt;
Taylor&#35270;&#39057;&#26159;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#26684;&#24335;&#65292;&#29992;&#20110;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#21160;&#20316;&#25552;&#21462;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;Taylor&#23637;&#24320;&#36817;&#20284;&#35745;&#31639;&#38544;&#21547;&#30340;&#21160;&#20316;&#25552;&#21462;&#20989;&#25968;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#21160;&#20316;&#25552;&#21462;&#20013;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35270;&#39057;&#20013;&#26377;&#25928;&#22320;&#25552;&#21462;&#21160;&#20316;&#26159;&#21160;&#20316;&#35782;&#21035;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#21160;&#20316;(i)&#27809;&#26377;&#26126;&#30830;&#30340;&#24418;&#24335;&#65292;(ii)&#25317;&#26377;&#35832;&#22914;&#20301;&#31227;&#12289;&#36895;&#24230;&#21644;&#21152;&#36895;&#24230;&#31561;&#21508;&#31181;&#27010;&#24565;&#65292;(iii)&#36890;&#24120;&#20250;&#21463;&#21040;&#19981;&#31283;&#23450;&#20687;&#32032;&#24341;&#36215;&#30340;&#22122;&#22768;&#30340;&#24178;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Taylor&#35270;&#39057;&#65292;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#26684;&#24335;&#65292;&#23427;&#31361;&#20986;&#26174;&#31034;&#20102;&#27599;&#20010;&#24103;&#20013;&#30340;&#20027;&#35201;&#21160;&#20316;&#65288;&#20363;&#22914;&#25381;&#25163;&#65289;&#34987;&#31216;&#20026;Taylor&#24103;&#12290;Taylor&#35270;&#39057;&#30340;&#21629;&#21517;&#26469;&#28304;&#20110;Taylor&#32423;&#25968;&#65292;&#23427;&#20351;&#29992;&#37325;&#35201;&#30340;&#39033;&#26469;&#36817;&#20284;&#32473;&#23450;&#28857;&#19978;&#30340;&#20989;&#25968;&#12290;&#22312;&#35270;&#39057;&#30340;&#24773;&#22659;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#38544;&#21547;&#30340;&#21160;&#20316;&#25552;&#21462;&#20989;&#25968;&#65292;&#26088;&#22312;&#20174;&#35270;&#39057;&#26102;&#38388;&#22359;&#20013;&#25552;&#21462;&#21160;&#20316;&#12290;&#22312;&#36825;&#20010;&#22359;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24103;&#12289;&#24046;&#20998;&#24103;&#21644;&#39640;&#38454;&#24046;&#20998;&#24103;&#36827;&#34892;Taylor&#23637;&#24320;&#65292;&#20197;&#36817;&#20284;&#35745;&#31639;&#36215;&#22987;&#24103;&#19978;&#30340;&#36825;&#20010;&#20989;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Taylor&#32423;&#25968;&#20013;&#39640;&#38454;&#39033;&#30340;&#27714;&#21644;&#32473;&#25105;&#20204;&#25552;&#20379;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Effectively extracting motions from video is a critical and long-standing problem for action recognition. This problem is very challenging because motions (i) do not have an explicit form, (ii) have various concepts such as displacement, velocity, and acceleration, and (iii) often contain noise caused by unstable pixels. Addressing these challenges, we propose the Taylor video, a new video format that highlights the dominate motions (e.g., a waving hand) in each of its frames named the Taylor frame. Taylor video is named after Taylor series, which approximates a function at a given point using important terms. In the scenario of videos, we define an implicit motion-extraction function which aims to extract motions from video temporal block. In this block, using the frames, the difference frames, and higher-order difference frames, we perform Taylor expansion to approximate this function at the starting frame. We show the summation of the higher-order terms in the Taylor series gives us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545; GPT &#27169;&#22411;&#30340;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#20855;&#26377;&#21163;&#25345;&#20250;&#35805;&#21644;&#37325;&#26500;&#23545;&#35805;&#30340;&#20004;&#20010;&#27493;&#39588;&#12290;&#36890;&#36807;&#23545;&#35813;&#25915;&#20987;&#23545; GPT &#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616; GPT-4 &#23545;&#35813;&#25915;&#20987;&#20855;&#26377;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02987</link><description>&lt;p&gt;
GPT &#27169;&#22411;&#30340;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Conversation Reconstruction Attack Against GPT Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545; GPT &#27169;&#22411;&#30340;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#20855;&#26377;&#21163;&#25345;&#20250;&#35805;&#21644;&#37325;&#26500;&#23545;&#35805;&#30340;&#20004;&#20010;&#27493;&#39588;&#12290;&#36890;&#36807;&#23545;&#35813;&#25915;&#20987;&#23545; GPT &#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616; GPT-4 &#23545;&#35813;&#25915;&#20987;&#20855;&#26377;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20854;&#20013; GPT &#31995;&#21015;&#27169;&#22411;&#20195;&#34920;&#30528;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#25104;&#26524;&#12290;&#20026;&#20102;&#20248;&#21270;&#20219;&#21153;&#25191;&#34892;&#65292;&#29992;&#25143;&#32463;&#24120;&#19982;&#25176;&#31649;&#22312;&#20113;&#29615;&#22659;&#20013;&#30340; GPT &#27169;&#22411;&#36827;&#34892;&#22810;&#36718;&#23545;&#35805;&#12290;&#36825;&#20123;&#22810;&#36718;&#23545;&#35805;&#24448;&#24448;&#21253;&#21547;&#31169;&#20154;&#20449;&#24687;&#65292;&#38656;&#35201;&#22312;&#20113;&#20013;&#36827;&#34892;&#20256;&#36755;&#21644;&#23384;&#20648;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25805;&#20316;&#27169;&#24335;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#25915;&#20987;&#38754;&#12290;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545; GPT &#27169;&#22411;&#30340;&#29305;&#23450;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;&#30001;&#20004;&#20010;&#27493;&#39588;&#32452;&#25104;&#65306;&#21163;&#25345;&#20250;&#35805;&#21644;&#37325;&#26500;&#23545;&#35805;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#24403; GPT &#27169;&#22411;&#36973;&#21463;&#35813;&#25915;&#20987;&#26102;&#23545;&#35805;&#20013;&#22266;&#26377;&#30340;&#38544;&#31169;&#39118;&#38505;&#36827;&#34892;&#20102;&#35814;&#23613;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;GPT-4 &#23545;&#20110;&#35813;&#25915;&#20987;&#20855;&#26377;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#39640;&#32423;&#25915;&#20987;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#37325;&#26500;&#20197;&#21069;&#30340;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent times, significant advancements have been made in the field of large language models (LLMs), represented by GPT series models. To optimize task execution, users often engage in multi-round conversations with GPT models hosted in cloud environments. These multi-round conversations, potentially replete with private information, require transmission and storage within the cloud. However, this operational paradigm introduces additional attack surfaces. In this paper, we first introduce a specific Conversation Reconstruction Attack targeting GPT models. Our introduced Conversation Reconstruction Attack is composed of two steps: hijacking a session and reconstructing the conversations. Subsequently, we offer an exhaustive evaluation of the privacy risks inherent in conversations when GPT models are subjected to the proposed attack. However, GPT-4 demonstrates certain robustness to the proposed attacks. We then introduce two advanced attacks aimed at better reconstructing previous c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#21152;&#36895;&#36870;&#25512;&#24378;&#21270;&#23398;&#20064;&#20013;&#24378;&#21270;&#23398;&#20064;&#23376;&#31243;&#24207;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25345;&#19982;&#19987;&#23478;&#25968;&#25454;&#20998;&#24067;&#25509;&#36817;&#30340;&#24754;&#35266;&#20027;&#20041;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#36870;&#25512;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02616</link><description>&lt;p&gt;
&#36870;&#25512;&#24378;&#21270;&#23398;&#20064;&#20013;&#24754;&#35266;&#20027;&#20041;&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
The Virtues of Pessimism in Inverse Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#21152;&#36895;&#36870;&#25512;&#24378;&#21270;&#23398;&#20064;&#20013;&#24378;&#21270;&#23398;&#20064;&#23376;&#31243;&#24207;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25345;&#19982;&#19987;&#23478;&#25968;&#25454;&#20998;&#24067;&#25509;&#36817;&#30340;&#24754;&#35266;&#20027;&#20041;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#36870;&#25512;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#25512;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26159;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#22797;&#26434;&#34892;&#20026;&#30340;&#24378;&#22823;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#19978;&#38656;&#35201;&#22312;&#20869;&#24490;&#29615;&#20013;&#21453;&#22797;&#35299;&#20915;&#35745;&#31639;&#25104;&#26412;&#26114;&#36149;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#19987;&#23478;&#28436;&#31034;&#26469;&#20943;&#23569;&#25506;&#32034;&#36127;&#25285;&#22312;&#20869;&#24490;&#29615;&#30340;RL&#20013;&#38750;&#24120;&#21487;&#21462;&#12290;&#20363;&#22914;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#36890;&#36807;&#23558;&#23398;&#20064;&#32773;&#37325;&#32622;&#21040;&#19987;&#23478;&#29366;&#24577;&#26469;&#25351;&#23548;&#23398;&#20064;&#32773;&#22312;&#39640;&#22238;&#25253;&#19987;&#23478;&#29366;&#24577;&#19979;&#24037;&#20316;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#19981;&#21487;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#21152;&#36895;IRL&#20013;RL&#23376;&#31243;&#24207;&#30340;&#26367;&#20195;&#26041;&#27861;&#65306;&#24754;&#35266;&#20027;&#20041;&#65292;&#21363;&#20445;&#25345;&#19982;&#19987;&#23478;&#25968;&#25454;&#20998;&#24067;&#25509;&#36817;&#65292;&#36890;&#36807;&#31163;&#32447;RL&#31639;&#27861;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#22312;&#31163;&#32447;RL&#21644;IRL&#20043;&#38388;&#24418;&#25104;&#20102;&#19968;&#20010;&#36830;&#25509;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#20219;&#24847;&#31163;&#32447;RL&#31639;&#27861;&#26469;&#25552;&#39640;IRL&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#23637;&#31034;&#20102;&#26679;&#26412;&#25928;&#29575;&#21644;&#25506;&#32034;&#36127;&#25285;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse Reinforcement Learning (IRL) is a powerful framework for learning complex behaviors from expert demonstrations. However, it traditionally requires repeatedly solving a computationally expensive reinforcement learning (RL) problem in its inner loop. It is desirable to reduce the exploration burden by leveraging expert demonstrations in the inner-loop RL. As an example, recent work resets the learner to expert states in order to inform the learner of high-reward expert states. However, such an approach is infeasible in the real world. In this work, we consider an alternative approach to speeding up the RL subroutine in IRL: \emph{pessimism}, i.e., staying close to the expert's data distribution, instantiated via the use of offline RL algorithms. We formalize a connection between offline RL and IRL, enabling us to use an arbitrary offline RL algorithm to improve the sample efficiency of IRL. We validate our theory experimentally by demonstrating a strong correlation between the ef
&lt;/p&gt;</description></item><item><title>SudokuSens&#26159;&#19968;&#31181;&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#26469;&#25552;&#39640;IoT&#24863;&#30693;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#27169;&#20223;&#23454;&#38469;&#25968;&#25454;&#37319;&#38598;&#36807;&#31243;&#20013;&#26410;&#36935;&#21040;&#30340;&#23454;&#39564;&#37197;&#32622;&#65292;&#22686;&#21152;&#20102;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02275</link><description>&lt;p&gt;
SudokuSens: &#20351;&#29992;&#29983;&#25104;&#26041;&#27861;&#22686;&#24378;IoT&#24863;&#30693;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
SudokuSens: Enhancing Deep Learning Robustness for IoT Sensing Applications using a Generative Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02275
&lt;/p&gt;
&lt;p&gt;
SudokuSens&#26159;&#19968;&#31181;&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#26469;&#25552;&#39640;IoT&#24863;&#30693;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#27169;&#20223;&#23454;&#38469;&#25968;&#25454;&#37319;&#38598;&#36807;&#31243;&#20013;&#26410;&#36935;&#21040;&#30340;&#23454;&#39564;&#37197;&#32622;&#65292;&#22686;&#21152;&#20102;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SudokuSens&#65292;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#24212;&#29992;&#20013;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#29983;&#25104;&#26694;&#26550;&#65292;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#27169;&#20223;&#23454;&#38469;&#20256;&#24863;&#22120;&#25968;&#25454;&#37319;&#38598;&#36807;&#31243;&#20013;&#26410;&#36935;&#21040;&#30340;&#23454;&#39564;&#37197;&#32622;&#12290;&#35813;&#26694;&#26550;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#25968;&#25454;&#37319;&#38598;&#25104;&#26412;&#39640;&#26114;&#30340;IoT&#24212;&#29992;&#12290;&#35813;&#24037;&#20316;&#30340;&#21160;&#26426;&#26159;&#22240;&#20026;IoT&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#32416;&#32544;&#20102;&#35266;&#23519;&#23545;&#35937;&#30340;&#29305;&#24449;&#20197;&#21450;&#21608;&#22260;&#29615;&#22659;&#30340;&#28151;&#26434;&#20869;&#22312;&#23646;&#24615;&#21644;&#21160;&#24577;&#29615;&#22659;&#24178;&#25200;&#12290;&#20026;&#20102;&#22312;IoT&#35757;&#32451;&#25968;&#25454;&#20013;&#21152;&#20837;&#36275;&#22815;&#30340;&#22810;&#26679;&#24615;&#65292;&#22240;&#27492;&#38656;&#35201;&#32771;&#34385;&#19982;&#23545;&#35937;&#25968;&#37327;&#21644;&#21487;&#33021;&#36935;&#21040;&#30340;&#29615;&#22659;&#26465;&#20214;&#25104;&#20493;&#22686;&#21152;&#30340;&#35757;&#32451;&#29992;&#20363;&#30340;&#32452;&#21512;&#29190;&#28856;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22823;&#22823;&#20943;&#23569;&#20102;&#36825;&#20123;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces SudokuSens, a generative framework for automated generation of training data in machine-learning-based Internet-of-Things (IoT) applications, such that the generated synthetic data mimic experimental configurations not encountered during actual sensor data collection. The framework improves the robustness of resulting deep learning models, and is intended for IoT applications where data collection is expensive. The work is motivated by the fact that IoT time-series data entangle the signatures of observed objects with the confounding intrinsic properties of the surrounding environment and the dynamic environmental disturbances experienced. To incorporate sufficient diversity into the IoT training data, one therefore needs to consider a combinatorial explosion of training cases that are multiplicative in the number of objects considered and the possible environmental conditions in which such objects may be encountered. Our framework substantially reduces these mult
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#30740;&#20102;&#38754;&#21521;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21442;&#25968;&#20462;&#25913;&#36229;&#36234;&#20840;&#38754;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#25910;&#34255;&#12290;</title><link>https://arxiv.org/abs/2402.02242</link><description>&lt;p&gt;
&#38754;&#21521;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#30740;&#20102;&#38754;&#21521;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21442;&#25968;&#20462;&#25913;&#36229;&#36234;&#20840;&#38754;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#25910;&#34255;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#65288;PVMs&#65289;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#19979;&#28216;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#26368;&#20808;&#36827;&#30340;PVMs&#36798;&#21040;&#25968;&#21313;&#20159;&#29978;&#33267;&#25968;&#19975;&#20159;&#20010;&#21442;&#25968;&#65292;&#26631;&#20934;&#30340;&#20840;&#38754;&#24494;&#35843;&#33539;&#24335;&#30001;&#20110;&#39640;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#21464;&#24471;&#19981;&#21487;&#25345;&#32493;&#12290;&#20316;&#20026;&#21709;&#24212;&#65292;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#25506;&#32034;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#65292;&#26088;&#22312;&#20197;&#26368;&#23567;&#21442;&#25968;&#20462;&#25913;&#36229;&#36234;&#20840;&#38754;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#35270;&#35273;PEFT&#30340;&#20840;&#38754;&#27010;&#36848;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#23545;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#31995;&#32479;&#23457;&#26597;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;PEFT&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#35752;&#35770;&#20102;&#27169;&#22411;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#26041;&#27861;&#20998;&#20026;&#19977;&#31867;&#65306;&#22522;&#20110;&#28155;&#21152;&#30340;&#12289;&#22522;&#20110;&#37096;&#20998;&#30340;&#21644;&#22522;&#20110;&#32479;&#19968;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24120;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#25361;&#25112;&#12290;&#35813;&#32508;&#36848;&#36824;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#25910;&#34255;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained vision models (PVMs) have shown great potential for adaptability across various downstream vision tasks. However, with state-of-the-art PVMs growing to billions or even trillions of parameters, the standard full fine-tuning paradigm is becoming unsustainable due to high computational and storage demands. In response, researchers are exploring parameter-efficient fine-tuning (PEFT), which seeks to exceed the performance of full fine-tuning with minimal parameter modifications. This survey provides a comprehensive overview and future directions for visual PEFT, offering a systematic review of the latest advancements. First, we provide a formal definition of PEFT and discuss model pre-training methods. We then categorize existing methods into three categories: addition-based, partial-based, and unified-based. Finally, we introduce the commonly used datasets and applications and suggest potential future research challenges. A comprehensive collection of resources is
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36136;&#30097;&#20102;&#25512;&#33616;&#31995;&#32479;&#23454;&#36341;&#20013;&#30446;&#21069;&#24120;&#29992;&#30340;&#8220;&#25720;&#30528;&#30707;&#22836;&#36807;&#27827;&#8221;&#26041;&#27861;&#65292;&#21628;&#21505;&#25682;&#24323;&#21453;&#20044;&#25176;&#37030;&#24605;&#32500;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22534;&#26632;&#30340;&#38750;&#26631;&#20934;&#29992;&#27861;&#65292;&#20197;&#35299;&#38145;&#22870;&#21169;&#20248;&#21270;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02152</link><description>&lt;p&gt;
&#35770;&#25991;&#39064;&#30446;&#65306;&#20026;&#20160;&#20040;&#8220;&#25720;&#30528;&#30707;&#22836;&#36807;&#27827;&#8221;&#26041;&#27861;&#20027;&#23548;&#25512;&#33616;&#31995;&#32479;&#23454;&#36341;&#65307;&#21628;&#21505;&#25682;&#24323;&#21453;&#20044;&#25176;&#37030;&#24605;&#32500;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Why the Shooting in the Dark Method Dominates Recommender Systems Practice; A Call to Abandon Anti-Utopian Thinking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02152
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36136;&#30097;&#20102;&#25512;&#33616;&#31995;&#32479;&#23454;&#36341;&#20013;&#30446;&#21069;&#24120;&#29992;&#30340;&#8220;&#25720;&#30528;&#30707;&#22836;&#36807;&#27827;&#8221;&#26041;&#27861;&#65292;&#21628;&#21505;&#25682;&#24323;&#21453;&#20044;&#25176;&#37030;&#24605;&#32500;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22534;&#26632;&#30340;&#38750;&#26631;&#20934;&#29992;&#27861;&#65292;&#20197;&#35299;&#38145;&#22870;&#21169;&#20248;&#21270;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#22788;&#20110;&#19968;&#31181;&#22855;&#29305;&#30340;&#22659;&#22320;&#12290;&#23613;&#31649;&#22312;&#36890;&#36807;A/B&#27979;&#35797;&#26469;&#34913;&#37327;&#24615;&#33021;&#26041;&#38754;&#26377;&#19968;&#20010;&#38750;&#24120;&#20005;&#26684;&#30340;&#21327;&#35758;&#65292;&#20294;&#25214;&#21040;&#35201;&#27979;&#35797;&#30340;&#8220;B&#8221;&#30340;&#26368;&#20339;&#26041;&#27861;&#24182;&#27809;&#26377;&#26126;&#30830;&#22320;&#38024;&#23545;&#24615;&#33021;&#65292;&#32780;&#26159;&#38024;&#23545;&#19968;&#20010;&#20195;&#29702;&#25351;&#26631;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;A/B&#27979;&#35797;&#30340;&#25104;&#21151;&#25110;&#22833;&#36133;&#23436;&#20840;&#21462;&#20915;&#20110;&#25152;&#25552;&#20986;&#30340;&#20195;&#29702;&#25351;&#26631;&#26159;&#21542;&#19982;&#24615;&#33021;&#30456;&#20851;&#24615;&#26356;&#22909;&#12290;&#27809;&#26377;&#21407;&#21017;&#21487;&#20197;&#22312;&#31163;&#32447;&#24773;&#20917;&#19979;&#30830;&#23450;&#19968;&#20010;&#20195;&#29702;&#25351;&#26631;&#26159;&#21542;&#27604;&#21478;&#19968;&#20010;&#26356;&#22909;&#65292;&#36825;&#20351;&#24471;&#20174;&#19994;&#32773;&#20204;&#25720;&#19981;&#30528;&#22836;&#33041;&#12290;&#26412;&#35770;&#25991;&#30340;&#30446;&#30340;&#26159;&#36136;&#30097;&#36825;&#31181;&#21453;&#20044;&#25176;&#37030;&#24605;&#32500;&#65292;&#24182;&#20027;&#24352;&#28145;&#24230;&#23398;&#20064;&#22534;&#26632;&#30340;&#38750;&#26631;&#20934;&#29992;&#27861;&#23454;&#38469;&#19978;&#26377;&#28508;&#21147;&#35299;&#38145;&#20248;&#21270;&#22870;&#21169;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applied recommender systems research is in a curious position. While there is a very rigorous protocol for measuring performance by A/B testing, best practice for finding a `B' to test does not explicitly target performance but rather targets a proxy measure. The success or failure of a given A/B test then depends entirely on if the proposed proxy is better correlated to performance than the previous proxy. No principle exists to identify if one proxy is better than another offline, leaving the practitioners shooting in the dark. The purpose of this position paper is to question this anti-Utopian thinking and argue that a non-standard use of the deep learning stacks actually has the potential to unlock reward optimizing recommendation.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#27931;&#26441;&#30710;&#35686;&#23519;&#19982;&#21496;&#26426;&#30340;&#20114;&#21160;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#21253;&#25324;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#23383;&#20449;&#24687;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#22797;&#26434;&#21644;&#26377;&#20105;&#35758;&#30340;&#35686;&#27665;&#20114;&#21160;&#30340;&#20998;&#26512;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2402.01703</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#35282;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#27931;&#26441;&#30710;&#35686;&#23519;&#19982;&#21496;&#26426;&#30340;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
A Multi-Perspective Machine Learning Approach to Evaluate Police-Driver Interaction in Los Angeles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01703
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#27931;&#26441;&#30710;&#35686;&#23519;&#19982;&#21496;&#26426;&#30340;&#20114;&#21160;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#21253;&#25324;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#23383;&#20449;&#24687;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#22797;&#26434;&#21644;&#26377;&#20105;&#35758;&#30340;&#35686;&#27665;&#20114;&#21160;&#30340;&#20998;&#26512;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25919;&#24220;&#23448;&#21592;&#19982;&#24066;&#27665;&#20043;&#38388;&#30340;&#20114;&#21160;&#24433;&#21709;&#20844;&#20849;&#31119;&#31049;&#21644;&#27665;&#20027;&#31038;&#20250;&#30340;&#27491;&#24403;&#24615;&#12290;&#35686;&#23519;&#26159;&#22269;&#23478;&#26368;&#26174;&#32780;&#26131;&#35265;&#12289;&#26368;&#25509;&#35302;&#24066;&#27665;&#30340;&#20195;&#29702;&#20154;&#65292;&#22312;&#20132;&#36890;&#31449;&#20572;&#26399;&#38388;&#65292;&#20182;&#20204;&#27599;&#24180;&#19982;&#20844;&#20247;&#20114;&#21160;&#36229;&#36807;2000&#19975;&#27425;&#12290;&#22914;&#20170;&#65292;&#36825;&#20123;&#20114;&#21160;&#32463;&#24120;&#34987;&#25140;&#22312;&#36523;&#19978;&#30340;&#25668;&#20687;&#26426;&#35760;&#24405;&#19979;&#26469;&#65292;&#36825;&#34987;&#35270;&#20026;&#25552;&#39640;&#35686;&#23519;&#38382;&#36131;&#21046;&#21644;&#25913;&#21892;&#35686;&#27665;&#20114;&#21160;&#30340;&#25163;&#27573;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#21487;&#38752;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#26469;&#20998;&#26512;&#36825;&#20123;&#22797;&#26434;&#32780;&#26377;&#20105;&#35758;&#30340;&#35686;&#27665;&#20114;&#21160;&#65292;&#36825;&#20123;&#35760;&#24405;&#30340;&#21450;&#26102;&#20998;&#26512;&#21463;&#21040;&#20102;&#38459;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#35282;&#24230;&#12289;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24037;&#20855;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#26469;&#33258;&#36825;&#20123;&#36523;&#19978;&#25668;&#20687;&#26426;&#35760;&#24405;&#30340;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#23383;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#30830;&#23450;&#19982;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#26368;&#30456;&#20851;&#30340;&#27807;&#36890;&#26041;&#38754;&#65292;&#21253;&#25324;&#20849;&#21516;&#24863;&#30693;&#20114;&#21160;&#30340;&#26631;&#24535;&#26631;&#35760;&#20197;&#21450;&#20855;&#26377;&#36825;&#20123;&#26631;&#35760;&#30340;&#31526;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactions between the government officials and civilians affect public wellbeing and the state legitimacy that is necessary for the functioning of democratic society. Police officers, the most visible and contacted agents of the state, interact with the public more than 20 million times a year during traffic stops. Today, these interactions are regularly recorded by body-worn cameras (BWCs), which are lauded as a means to enhance police accountability and improve police-public interactions. However, the timely analysis of these recordings is hampered by a lack of reliable automated tools that can enable the analysis of these complex and contested police-public interactions. This article proposes an approach to developing new multi-perspective, multimodal machine learning (ML) tools to analyze the audio, video, and transcript information from this BWC footage. Our approach begins by identifying the aspects of communication most salient to different stakeholders, including both commun
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#31216;&#20026;ReAGent&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#36866;&#29992;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#35745;&#31639;&#19978;&#25928;&#29575;&#26356;&#39640;&#30340;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.00794</link><description>&lt;p&gt;
ReAGent: &#19968;&#20010;&#38754;&#21521;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReAGent: Towards A Model-agnostic Feature Attribution Method for Generative Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#31216;&#20026;ReAGent&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#36866;&#29992;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#35745;&#31639;&#19978;&#25928;&#29575;&#26356;&#39640;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65288;FAs&#65289;&#65292;&#22914;&#26799;&#24230;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#30830;&#23450;&#25152;&#26377;&#36755;&#20837;&#29305;&#24449;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;&#29616;&#26377;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20026;&#20165;&#26377;&#32534;&#30721;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#24320;&#21457;&#21644;&#27979;&#35797;FAs&#65292;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23578;&#19981;&#28165;&#26970;&#22312;&#25991;&#26412;&#29983;&#25104;&#19978;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;FAs&#26469;&#22788;&#29702;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#22240;&#20026;&#27169;&#22411;&#26550;&#26500;&#21644;&#20219;&#21153;&#35774;&#32622;&#20043;&#38388;&#23384;&#22312;&#22266;&#26377;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#65292;&#27809;&#26377;&#19968;&#20010;&#36890;&#29992;&#30340;FA&#36866;&#29992;&#20110;&#25152;&#26377;&#27169;&#22411;&#21644;&#20219;&#21153;&#12290;&#36825;&#20351;&#24471;&#38024;&#23545;&#22823;&#22411;LMs&#36873;&#25321;FA&#35745;&#31639;&#19978;&#38750;&#24120;&#26114;&#36149;&#65292;&#22240;&#20026;&#36755;&#20837;&#37325;&#35201;&#24615;&#30340;&#25512;&#23548;&#36890;&#24120;&#38656;&#35201;&#22810;&#20010;&#21069;&#21521;&#21644;&#21453;&#21521;&#20256;&#36882;&#65292;&#21253;&#25324;&#21487;&#33021;&#26159;&#38480;&#21046;&#24615;&#30340;&#26799;&#24230;&#35745;&#31639;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#29983;&#25104;LMs&#30340;&#27169;&#22411;&#26080;&#20851;FA&#65292;&#31216;&#20026;&#36882;&#24402;&#24402;&#22240;&#29983;&#25104;&#22120;&#65288;ReAGent&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature attribution methods (FAs), such as gradients and attention, are widely employed approaches to derive the importance of all input features to the model predictions. Existing work in natural language processing has mostly focused on developing and testing FAs for encoder-only language models (LMs) in classification tasks. However, it is unknown if it is faithful to use these FAs for decoder-only models on text generation, due to the inherent differences between model architectures and task settings respectively. Moreover, previous work has demonstrated that there is no `one-wins-all' FA across models and tasks. This makes the selection of a FA computationally expensive for large LMs since input importance derivation often requires multiple forward and backward passes including gradient computations that might be prohibitive even with access to large compute. To address these issues, we present a model-agnostic FA for generative LMs called Recursive Attribution Generator (ReAGent)
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#20102;&#21487;&#36716;&#31227;&#24615;&#23545;&#25239;&#25915;&#20987;&#30340;&#26041;&#38754;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20934;&#26694;&#26550;&#24182;&#31995;&#32479;&#20998;&#31867;&#21644;&#35780;&#20272;&#20102;&#21508;&#31181;&#22686;&#24378;&#23545;&#25239;&#25915;&#20987;&#21487;&#36716;&#31227;&#24615;&#30340;&#26041;&#27861;&#65292;&#20026;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#25552;&#20379;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#24179;&#21488;&#12290;</title><link>https://arxiv.org/abs/2402.00418</link><description>&lt;p&gt;
&#30701;&#25991;: &#22522;&#20934;&#27979;&#35797;&#21487;&#36716;&#31227;&#24615;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Short: Benchmarking transferable adversarial attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#20102;&#21487;&#36716;&#31227;&#24615;&#23545;&#25239;&#25915;&#20987;&#30340;&#26041;&#38754;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20934;&#26694;&#26550;&#24182;&#31995;&#32479;&#20998;&#31867;&#21644;&#35780;&#20272;&#20102;&#21508;&#31181;&#22686;&#24378;&#23545;&#25239;&#25915;&#20987;&#21487;&#36716;&#31227;&#24615;&#30340;&#26041;&#27861;&#65292;&#20026;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#25552;&#20379;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#20851;&#27880;&#28857;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#20102;&#21487;&#36716;&#31227;&#24615;&#23545;&#25239;&#25915;&#20987;&#30340;&#26041;&#38754;&#12290;&#23427;&#31995;&#32479;&#22320;&#20998;&#31867;&#21644;&#25209;&#21028;&#24615;&#35780;&#20272;&#20102;&#21508;&#31181;&#22686;&#24378;&#23545;&#25239;&#25915;&#20987;&#21487;&#36716;&#31227;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#30740;&#31350;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#21253;&#25324;&#29983;&#25104;&#32467;&#26500;&#12289;&#35821;&#20041;&#30456;&#20284;&#24615;&#12289;&#26799;&#24230;&#32534;&#36753;&#12289;&#30446;&#26631;&#20462;&#25913;&#21644;&#38598;&#25104;&#26041;&#27861;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20934;&#26694;&#26550;"TAA-Bench"&#65292;&#38598;&#25104;&#20102;&#21313;&#31181;&#20027;&#35201;&#30340;&#23545;&#25239;&#25915;&#20987;&#21487;&#36716;&#31227;&#24615;&#26041;&#27861;&#65292;&#20174;&#32780;&#20026;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#25552;&#20379;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#21644;&#31995;&#32479;&#21270;&#30340;&#27604;&#36739;&#20998;&#26512;&#24179;&#21488;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23457;&#26597;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#25928;&#21147;&#21644;&#38480;&#21046;&#65292;&#24182;&#38416;&#26126;&#20102;&#23427;&#20204;&#30340;&#25805;&#20316;&#21407;&#29702;&#21644;&#23454;&#38469;&#25928;&#29992;&#12290;&#26412;&#32508;&#36848;&#35797;&#22270;&#25104;&#20026;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#23545;&#27604;&#20998;&#26512;&#30340;&#26631;&#20934;&#21270;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robustness of deep learning models against adversarial attacks remains a pivotal concern. This study presents, for the first time, an exhaustive review of the transferability aspect of adversarial attacks. It systematically categorizes and critically evaluates various methodologies developed to augment the transferability of adversarial attacks. This study encompasses a spectrum of techniques, including Generative Structure, Semantic Similarity, Gradient Editing, Target Modification, and Ensemble Approach. Concurrently, this paper introduces a benchmark framework \textit{TAA-Bench}, integrating ten leading methodologies for adversarial attack transferability, thereby providing a standardized and systematic platform for comparative analysis across diverse model architectures. Through comprehensive scrutiny, we delineate the efficacy and constraints of each method, shedding light on their underlying operational principles and practical utility. This review endeavors to be a quintesse
&lt;/p&gt;</description></item><item><title>$\mu$GUIDE&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24494;&#32467;&#26500;&#25104;&#20687;&#65292;&#33021;&#22815;&#26377;&#25928;&#20272;&#35745;&#32452;&#32455;&#24494;&#32467;&#26500;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#37327;&#21270;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#27169;&#31946;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.17293</link><description>&lt;p&gt;
$\mu$GUIDE:&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#20351;&#29992;&#24191;&#20041;&#19981;&#30830;&#23450;&#24615;&#39537;&#21160;&#30340;&#25512;&#26029;&#26469;&#36827;&#34892;&#24494;&#32467;&#26500;&#25104;&#20687;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
$\mu$GUIDE: a framework for microstructure imaging via generalized uncertainty-driven inference using deep learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17293
&lt;/p&gt;
&lt;p&gt;
$\mu$GUIDE&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24494;&#32467;&#26500;&#25104;&#20687;&#65292;&#33021;&#22815;&#26377;&#25928;&#20272;&#35745;&#32452;&#32455;&#24494;&#32467;&#26500;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#37327;&#21270;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#27169;&#31946;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;$\mu$GUIDE:&#19968;&#31181;&#36890;&#29992;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20219;&#20309;&#32473;&#23450;&#30340;&#29983;&#29289;&#29289;&#29702;&#27169;&#22411;&#25110;MRI&#20449;&#21495;&#34920;&#31034;&#20013;&#20272;&#35745;&#32452;&#32455;&#24494;&#32467;&#26500;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#36890;&#36807;&#25193;&#25955;&#21152;&#26435;MRI&#30340;&#31034;&#20363;&#28436;&#31034;&#12290;&#21033;&#29992;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#33258;&#21160;&#20449;&#21495;&#29305;&#24449;&#36873;&#25321;&#65292;&#32467;&#21512;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#26029;&#21644;&#21518;&#39564;&#20998;&#24067;&#30340;&#39640;&#25928;&#37319;&#26679;&#65292;$\mu$GUIDE&#32469;&#36807;&#20102;&#20256;&#32479;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#39640;&#35745;&#31639;&#21644;&#26102;&#38388;&#25104;&#26412;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#37319;&#38598;&#32422;&#26463;&#26469;&#23450;&#20041;&#27169;&#22411;&#29305;&#23450;&#30340;&#25688;&#35201;&#32479;&#35745;&#37327;&#12290;&#33719;&#24471;&#30340;&#21518;&#39564;&#20998;&#24067;&#21487;&#31361;&#20986;&#26174;&#31034;&#27169;&#22411;&#23450;&#20041;&#20013;&#23384;&#22312;&#30340;&#36864;&#21270;&#24615;&#65292;&#24182;&#37327;&#21270;&#25152;&#20272;&#35745;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#27169;&#31946;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes $\mu$GUIDE: a general Bayesian framework to estimate posterior distributions of tissue microstructure parameters from any given biophysical model or MRI signal representation, with exemplar demonstration in diffusion-weighted MRI. Harnessing a new deep learning architecture for automatic signal feature selection combined with simulation-based inference and efficient sampling of the posterior distributions, $\mu$GUIDE bypasses the high computational and time cost of conventional Bayesian approaches and does not rely on acquisition constraints to define model-specific summary statistics. The obtained posterior distributions allow to highlight degeneracies present in the model definition and quantify the uncertainty and ambiguity of the estimated parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Headless-AD&#27169;&#22411;&#65292;&#36890;&#36807;&#21482;&#35757;&#32451;&#19968;&#27425;&#65292;&#33021;&#22815;&#22312;&#21464;&#21270;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#20174;&#26410;&#36935;&#21040;&#36807;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#32988;&#36807;&#38024;&#23545;&#29305;&#23450;&#34892;&#21160;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2312.13327</link><description>&lt;p&gt;
&#21464;&#21270;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#30340;&#24773;&#22659;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-Context Reinforcement Learning for Variable Action Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Headless-AD&#27169;&#22411;&#65292;&#36890;&#36807;&#21482;&#35757;&#32451;&#19968;&#27425;&#65292;&#33021;&#22815;&#22312;&#21464;&#21270;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#20174;&#26410;&#36935;&#21040;&#36807;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#32988;&#36807;&#38024;&#23545;&#29305;&#23450;&#34892;&#21160;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#20808;&#22312;&#22810;&#26679;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#19978;&#19979;&#25991;&#22810;&#24773;&#33410;&#35757;&#32451;&#30340;&#21464;&#24418;&#37329;&#21018;&#32593;&#32476;&#21487;&#20197;&#22312;&#24773;&#22659;&#20013;&#27867;&#21270;&#21040;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;&#20808;&#21069;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#26159;&#23427;&#20204;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#34892;&#21160;&#31354;&#38388;&#22823;&#23567;&#21644;&#32467;&#26500;&#12290;&#24341;&#20837;&#26032;&#30340;&#34892;&#21160;&#31354;&#38388;&#36890;&#24120;&#38656;&#35201;&#25968;&#25454;&#37325;&#26032;&#25910;&#38598;&#21644;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#23545;&#20110;&#19968;&#20123;&#24212;&#29992;&#26469;&#35828;&#21487;&#33021;&#26159;&#26114;&#36149;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21482;&#35757;&#32451;&#19968;&#27425;&#30340;Headless-AD&#27169;&#22411;&#65292;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#27867;&#21270;&#21040;&#20855;&#26377;&#21487;&#21464;&#22823;&#23567;&#12289;&#35821;&#20041;&#20869;&#23481;&#21644;&#39034;&#24207;&#30340;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#12290;&#36890;&#36807;&#22312;&#20271;&#21162;&#21033;&#21644;&#19978;&#19979;&#25991;&#36172;&#21338;&#26426;&#20197;&#21450;&#19968;&#20010;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Headless-AD&#22312;&#20174;&#26410;&#36935;&#21040;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#20960;&#20010;&#29615;&#22659;&#37197;&#32622;&#19978;&#32988;&#36807;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#34892;&#21160;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, it has been shown that transformers pre-trained on diverse datasets with multi-episode contexts can generalize to new reinforcement learning tasks in-context. A key limitation of previously proposed models is their reliance on a predefined action space size and structure. The introduction of a new action space often requires data re-collection and model re-training, which can be costly for some applications. In our work, we show that it is possible to mitigate this issue by proposing the Headless-AD model that, despite being trained only once, is capable of generalizing to discrete action spaces of variable size, semantic content and order. By experimenting with Bernoulli and contextual bandits, as well as a gridworld environment, we show that Headless-AD exhibits significant capability to generalize to action spaces it has never encountered, even outperforming specialized models trained for a specific set of actions on several environment configurations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CLLM&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#21644;&#25968;&#25454;&#31579;&#36873;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#36827;&#34892;&#34920;&#26684;&#22686;&#24378;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#20197;&#21450;&#22522;&#20110;&#23398;&#20064;&#21160;&#24577;&#12289;&#32622;&#20449;&#24230;&#21644;&#19981;&#30830;&#23450;&#24230;&#25351;&#26631;&#30340;&#31579;&#36873;&#26426;&#21046;&#65292;CLLM&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.12112</link><description>&lt;p&gt;
LLM&#31934;&#36873;&#65306;&#22312;&#36229;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#21033;&#29992;LLMs&#21644;&#25968;&#25454;&#31579;&#36873;&#36827;&#34892;&#34920;&#26684;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in ultra low-data regimes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CLLM&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#21644;&#25968;&#25454;&#31579;&#36873;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#36827;&#34892;&#34920;&#26684;&#22686;&#24378;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#20197;&#21450;&#22522;&#20110;&#23398;&#20064;&#21160;&#24577;&#12289;&#32622;&#20449;&#24230;&#21644;&#19981;&#30830;&#23450;&#24230;&#25351;&#26631;&#30340;&#31579;&#36873;&#26426;&#21046;&#65292;CLLM&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20173;&#28982;&#26159;&#19968;&#20010;&#34987;&#20302;&#20272;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22686;&#21152;ML&#25152;&#38656;&#30340;&#25968;&#25454;&#26679;&#26412;&#22823;&#23567;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#23545;&#20110;&#37322;&#25918;ML&#22312;&#25968;&#25454;&#21294;&#20047;&#30340;&#22320;&#21306;&#21644;&#39046;&#22495;&#30340;&#21464;&#38761;&#28508;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26377;&#38480;&#30340;&#35757;&#32451;&#38598;&#38480;&#21046;&#20102;&#20256;&#32479;&#30340;&#34920;&#26684;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#22312;&#29983;&#25104;ML&#20219;&#21153;&#25152;&#38656;&#30340;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#25968;&#25454;&#38598;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CLLM&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20687;&#20219;&#20309;&#29983;&#25104;&#27169;&#22411;&#19968;&#26679;&#65292;&#24182;&#38750;LLMs&#29983;&#25104;&#30340;&#25152;&#26377;&#25968;&#25454;&#37117;&#33021;&#25552;&#39640;&#19979;&#28216;&#30340;&#25928;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#21160;&#24577;&#12289;&#32622;&#20449;&#24230;&#21644;&#19981;&#30830;&#23450;&#24230;&#25351;&#26631;&#30340;&#21407;&#21017;&#24615;&#31579;&#36873;&#26426;&#21046;&#65292;&#20197;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CLLM&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) in low-data settings remains an underappreciated yet crucial problem. Hence, data augmentation methods to increase the sample size of datasets needed for ML are key to unlocking the transformative potential of ML in data-deprived regions and domains. Unfortunately, the limited training set constrains traditional tabular synthetic data generators in their ability to generate a large and diverse augmented dataset needed for ML tasks. To address this challenge, we introduce CLLM, which leverages the prior knowledge of Large Language Models (LLMs) for data augmentation in the low-data regime. However, not all the data generated by LLMs will improve downstream utility, as for any generative model. Consequently, we introduce a principled curation mechanism, leveraging learning dynamics, coupled with confidence and uncertainty metrics, to obtain a high-quality dataset. Empirically, on multiple real-world datasets, we demonstrate the superior performance of CLLM in the lo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#24341;&#20837;&#20102;&#8220;&#31038;&#20250;&#23398;&#20064;&#8221;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30456;&#20114;&#20849;&#20139;&#30693;&#35782;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#30693;&#35782;&#20256;&#36882;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.11441</link><description>&lt;p&gt;
&#31038;&#20250;&#23398;&#20064;&#65306;&#26397;&#30528;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Social Learning: Towards Collaborative Learning with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#24341;&#20837;&#20102;&#8220;&#31038;&#20250;&#23398;&#20064;&#8221;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30456;&#20114;&#20849;&#20139;&#30693;&#35782;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#30693;&#35782;&#20256;&#36882;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32972;&#26223;&#19979;&#24341;&#20837;&#20102;&#8220;&#31038;&#20250;&#23398;&#20064;&#8221;&#30340;&#26694;&#26550;&#65292;&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#21069;&#25552;&#19979;&#65292;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#30456;&#20114;&#20849;&#20139;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#20004;&#31181;&#22312;LLMs&#20043;&#38388;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#26041;&#27861;&#12290;&#22312;&#31532;&#19968;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20801;&#35768;&#27169;&#22411;&#29983;&#25104;&#25277;&#35937;&#25552;&#31034;&#20197;&#20415;&#25945;&#25480;&#20219;&#21153;&#12290;&#22312;&#31532;&#20108;&#31181;&#26041;&#27861;&#20013;&#65292;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#31034;&#20363;&#26469;&#20256;&#36882;&#30693;&#35782;&#12290;&#25105;&#20204;&#36328;&#22810;&#20010;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#24182;&#20197;&#35760;&#24518;&#21270;&#20316;&#20026;&#38544;&#31169;&#25439;&#22833;&#30340;&#20195;&#29702;&#36827;&#34892;&#37327;&#21270;&#12290;&#36825;&#20123;&#21463;&#21040;&#31038;&#20250;&#23398;&#20064;&#21551;&#21457;&#30340;&#25216;&#26415;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#21407;&#22987;&#25968;&#25454;&#30340;&#35760;&#24518;&#21270;&#31243;&#24230;&#36739;&#20302;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#20351;&#29992;&#21407;&#22987;&#26631;&#31614;&#21644;&#25552;&#31034;&#30340;&#32467;&#26524;&#30456;&#23218;&#32654;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35777;&#26126;&#20102;&#31038;&#20250;&#23398;&#20064;&#22312;LLMs&#20013;&#30340;&#21487;&#34892;&#24615;&#65292;&#24314;&#31435;&#20102;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#31361;&#20986;&#20102;&#20960;&#20010;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#20379;&#26410;&#26469;&#30740;&#31350;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the framework of "social learning" in the context of large language models (LLMs), whereby models share knowledge with each other in a privacy-aware manner using natural language. We present and evaluate two approaches for knowledge transfer between LLMs. In the first scenario, we allow the model to generate abstract prompts aiming to teach the task. In our second approach, models transfer knowledge by generating synthetic examples. We evaluate these methods across diverse datasets and quantify memorization as a proxy for privacy loss. These techniques inspired by social learning yield promising results with low memorization of the original data. In particular, we show that performance using these methods is comparable to results with the use of original labels and prompts. Our work demonstrates the viability of social learning for LLMs, establishes baseline approaches and highlights several unexplored areas for future work.
&lt;/p&gt;</description></item><item><title>&#20844;&#24179;&#24615;&#32422;&#26463;&#22312;&#26497;&#24230;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#19978;&#33021;&#22815;&#24674;&#22797;&#21040;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#19978;&#20934;&#30830;&#21644;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>https://arxiv.org/abs/2312.10396</link><description>&lt;p&gt;
&#20844;&#24179;&#24615;&#32422;&#26463;&#33021;&#22815;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#24110;&#21161;&#20174;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#20013;&#24674;&#22797;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Far Can Fairness Constraints Help Recover From Biased Data?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10396
&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#24615;&#32422;&#26463;&#22312;&#26497;&#24230;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#19978;&#33021;&#22815;&#24674;&#22797;&#21040;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#19978;&#20934;&#30830;&#21644;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33324;&#35748;&#20026;&#65292;&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#20844;&#24179;&#24615;&#32422;&#26463;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#30340;&#20943;&#23569;&#65292;&#32780;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#21487;&#33021;&#20250;&#21152;&#21095;&#36825;&#31181;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;Blum&#65286;Stangl&#65288;2019&#65289;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26497;&#24230;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#19978;&#65292;&#21363;&#20351;&#37319;&#29992;&#24179;&#31561;&#26426;&#20250;&#32422;&#26463;&#65292;&#20063;&#21487;&#20197;&#24674;&#22797;&#21040;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#19978;&#20934;&#30830;&#21644;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#12290;&#20182;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24456;&#26377;&#36259;&#65292;&#22240;&#20026;&#23427;&#35777;&#26126;&#20102;&#20844;&#24179;&#24615;&#32422;&#26463;&#21487;&#20197;&#38544;&#24335;&#20462;&#27491;&#25968;&#25454;&#20559;&#24046;&#65292;&#21516;&#26102;&#20811;&#26381;&#20102;&#20844;&#24179;&#24615;&#19982;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;&#12290;&#20182;&#20204;&#30340;&#25968;&#25454;&#20559;&#24046;&#27169;&#22411;&#27169;&#25311;&#20102;&#21463;&#21387;&#36843;&#20154;&#32676;&#30340;&#34920;&#24449;&#21644;&#26631;&#31614;&#20559;&#35265;&#65292;&#24182;&#22312;&#20855;&#26377;&#29420;&#31435;&#26631;&#31614;&#22122;&#22768;&#30340;&#31616;&#21333;&#26465;&#20214;&#19979;&#65292;&#38024;&#23545;&#19968;&#20010;&#29702;&#24819;&#21270;&#30340;&#25968;&#25454;&#20998;&#24067;&#23637;&#31034;&#20102;&#19978;&#36848;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;Blum&#65286;Stangl&#65288;2019&#65289;&#30340;&#32467;&#26524;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#12289;&#25968;&#25454;&#20559;&#24046;&#27169;&#22411;&#12289;&#25968;&#25454;&#20998;&#24067;&#21644;&#20551;&#35774;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
A general belief in fair classification is that fairness constraints incur a trade-off with accuracy, which biased data may worsen. Contrary to this belief, Blum &amp; Stangl (2019) show that fair classification with equal opportunity constraints even on extremely biased data can recover optimally accurate and fair classifiers on the original data distribution. Their result is interesting because it demonstrates that fairness constraints can implicitly rectify data bias and simultaneously overcome a perceived fairness-accuracy trade-off. Their data bias model simulates under-representation and label bias in underprivileged population, and they show the above result on a stylized data distribution with i.i.d. label noise, under simple conditions on the data distribution and bias parameters. We propose a general approach to extend the result of Blum &amp; Stangl (2019) to different fairness constraints, data bias models, data distributions, and hypothesis classes. We strengthen their result, and
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;EBCL&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;&#20013;&#20851;&#38190;&#20107;&#20214;&#21069;&#21518;&#30340;&#25968;&#25454;&#32534;&#30721;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;EBCL&#33021;&#22815;&#20135;&#29983;&#24615;&#33021;&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#23545;&#20110;&#24515;&#21147;&#34928;&#31469;&#38431;&#21015;&#30340;&#20851;&#38190;&#19979;&#28216;&#20219;&#21153;&#20855;&#26377;&#26356;&#22909;&#30340;&#24494;&#35843;&#24615;&#33021;&#65292;&#24182;&#33021;&#26377;&#25928;&#22320;&#23558;&#20855;&#26377;&#30456;&#20284;&#39118;&#38505;&#30340;&#24739;&#32773;&#36827;&#34892;&#32858;&#31867;&#12290;</title><link>https://arxiv.org/abs/2312.10308</link><description>&lt;p&gt;
&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Event-Based Contrastive Learning for Medical Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;EBCL&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;&#20013;&#20851;&#38190;&#20107;&#20214;&#21069;&#21518;&#30340;&#25968;&#25454;&#32534;&#30721;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;EBCL&#33021;&#22815;&#20135;&#29983;&#24615;&#33021;&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#23545;&#20110;&#24515;&#21147;&#34928;&#31469;&#38431;&#21015;&#30340;&#20851;&#38190;&#19979;&#28216;&#20219;&#21153;&#20855;&#26377;&#26356;&#22909;&#30340;&#24494;&#35843;&#24615;&#33021;&#65292;&#24182;&#33021;&#26377;&#25928;&#22320;&#23558;&#20855;&#26377;&#30456;&#20284;&#39118;&#38505;&#30340;&#24739;&#32773;&#36827;&#34892;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#32463;&#24120;&#38656;&#35201;&#30830;&#23450;&#26576;&#20010;&#20851;&#38190;&#21307;&#23398;&#20107;&#20214;&#21518;&#24739;&#32773;&#26159;&#21542;&#22788;&#20110;&#19981;&#33391;&#32467;&#26524;&#30340;&#39640;&#39118;&#38505;&#29366;&#24577;&#65292;&#20363;&#22914;&#24515;&#21147;&#34928;&#31469;&#20837;&#38498;&#21518;&#30340;&#30701;&#26399;&#27515;&#20129;&#39118;&#38505;&#12290;&#36825;&#20010;&#20219;&#21153;&#30001;&#20110;&#38271;&#26399;&#21307;&#23398;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12289;&#21464;&#24322;&#24615;&#21644;&#24322;&#36136;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20687;&#24515;&#21147;&#34928;&#31469;&#36825;&#26679;&#30340;&#24930;&#24615;&#30142;&#30149;&#24739;&#32773;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;EBCL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#19981;&#21516;&#31867;&#22411;&#24739;&#32773;&#25968;&#25454;&#30340;&#23884;&#20837;&#34920;&#31034;&#65292;&#20197;&#20445;&#30041;&#20851;&#38190;&#32034;&#24341;&#20107;&#20214;&#21069;&#21518;&#30340;&#26102;&#38388;&#20449;&#24687;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;EBCL&#20135;&#29983;&#30340;&#27169;&#22411;&#22312;&#24515;&#21147;&#34928;&#31469;&#38431;&#21015;&#30340;&#20851;&#38190;&#19979;&#28216;&#20219;&#21153;&#65288;&#21253;&#25324;30&#22825;&#20877;&#20837;&#38498;&#12289;1&#24180;&#27515;&#20129;&#29575;&#21644;1&#21608;&#20303;&#38498;&#22825;&#25968;&#65289;&#30340;&#24494;&#35843;&#24615;&#33021;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;EBCL&#39044;&#35757;&#32451;&#21333;&#29420;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#20855;&#26377;&#30456;&#20284;&#27515;&#20129;&#29575;&#21644;&#20877;&#20837;&#38498;&#39118;&#38505;&#30340;&#24739;&#32773;&#36827;&#34892;&#32858;&#31867;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In clinical practice, one often needs to identify whether a patient is at high risk of adverse outcomes after some key medical event; for example, the short-term risk of death after an admission for heart failure. This task is challenging due to the complexity, variability, and heterogeneity of longitudinal medical data, especially for individuals suffering from chronic diseases like heart failure. In this paper, we introduce Event-Based Contrastive Learning (EBCL), a method for learning embeddings of heterogeneous patient data that preserves temporal information before and after key index events. We demonstrate that EBCL produces models that yield better fine-tuning performance on critical downstream tasks for a heart failure cohort, including 30-day readmission, 1-year mortality, and 1-week length of stay, relative to other pretraining methods. Our findings also reveal that EBCL pretraining alone can effectively cluster patients with similar mortality and readmission risks, offering 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20809;&#35889;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#20855;&#26377;&#20809;&#35889;&#28388;&#27874;&#31639;&#27861;&#30340;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#23454;&#29616;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#21644;&#22266;&#23450;&#21367;&#31215;&#28388;&#27874;&#22120;&#65292;&#36866;&#29992;&#20110;&#38656;&#35201;&#38750;&#24120;&#38271;&#31243;&#35760;&#24518;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2312.06837</link><description>&lt;p&gt;
&#20809;&#35889;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Spectral State Space Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20809;&#35889;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#20855;&#26377;&#20809;&#35889;&#28388;&#27874;&#31639;&#27861;&#30340;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#23454;&#29616;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#21644;&#22266;&#23450;&#21367;&#31215;&#28388;&#27874;&#22120;&#65292;&#36866;&#29992;&#20110;&#38656;&#35201;&#38750;&#24120;&#38271;&#31243;&#35760;&#24518;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#30340;&#39044;&#27979;&#20219;&#21153;&#30340;&#24207;&#21015;&#24314;&#27169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#20855;&#26377;&#20809;&#35889;&#28388;&#27874;&#31639;&#27861;&#30340;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#26032;&#24418;&#24335;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#31216;&#20026;&#20809;&#35889;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#26032;&#39062;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;&#12290;&#20809;&#35889;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20855;&#26377;&#20004;&#20010;&#20027;&#35201;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#23646;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#24615;&#33021;&#26082;&#19981;&#20381;&#36182;&#20110;&#24213;&#23618;&#21160;&#21147;&#23398;&#30340;&#39057;&#35889;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#38382;&#39064;&#30340;&#32500;&#24230;&#12290;&#20854;&#27425;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#36890;&#36807;&#22266;&#23450;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#26500;&#24314;&#30340;&#65292;&#19981;&#38656;&#35201;&#23398;&#20064;&#65292;&#21516;&#26102;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#20173;&#28982;&#20248;&#20110;SSMs&#12290;&#22522;&#20110;&#20809;&#35889;&#36807;&#28388;&#31639;&#27861;&#30340;Spectral state space models&#22312;&#21512;&#25104;&#21160;&#24577;&#31995;&#32479;&#21644;&#21508;&#31181;&#27169;&#24577;&#30340;&#38271;&#31243;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36825;&#20123;&#35780;&#20272;&#25903;&#25345;&#20102;&#20809;&#35889;&#28388;&#27874;&#22312;&#38656;&#35201;&#38750;&#24120;&#38271;&#31243;&#35760;&#24518;&#30340;&#20219;&#21153;&#20013;&#30340;&#29702;&#35770;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model.   Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice.   The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.
&lt;/p&gt;</description></item><item><title>SVQ&#26159;&#19968;&#31181;&#21033;&#29992;&#31232;&#30095;&#22238;&#24402;&#23454;&#29616;&#31616;&#26126;&#34920;&#31034;&#30340;&#31232;&#30095;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#20445;&#30041;&#20851;&#38190;&#32454;&#33410;&#21644;&#28388;&#38500;&#22122;&#22768;&#26469;&#25552;&#39640;&#26102;&#31354;&#39044;&#27979;&#24615;&#33021;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;SVQ&#22312;&#20116;&#20010;&#31354;&#38388;-&#26102;&#38388;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.03406</link><description>&lt;p&gt;
SVQ: &#31232;&#30095;&#21521;&#37327;&#37327;&#21270;&#29992;&#20110;&#26102;&#31354;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SVQ: Sparse Vector Quantization for Spatiotemporal Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03406
&lt;/p&gt;
&lt;p&gt;
SVQ&#26159;&#19968;&#31181;&#21033;&#29992;&#31232;&#30095;&#22238;&#24402;&#23454;&#29616;&#31616;&#26126;&#34920;&#31034;&#30340;&#31232;&#30095;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#20445;&#30041;&#20851;&#38190;&#32454;&#33410;&#21644;&#28388;&#38500;&#22122;&#22768;&#26469;&#25552;&#39640;&#26102;&#31354;&#39044;&#27979;&#24615;&#33021;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;SVQ&#22312;&#20116;&#20010;&#31354;&#38388;-&#26102;&#38388;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#39044;&#27979;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#37117;&#26159;&#20851;&#38190;&#65292;&#21462;&#24471;&#22909;&#30340;&#39044;&#27979;&#32467;&#26524;&#38656;&#35201;&#25214;&#21040;&#24494;&#22937;&#30340;&#27169;&#24335;&#24182;&#28388;&#38500;&#22122;&#22768;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31232;&#30095;&#22238;&#24402;&#21521;&#37327;&#37327;&#21270;&#65288;SVQ&#65289;&#36825;&#19968;&#26032;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#31232;&#30095;&#22238;&#24402;&#26469;&#23454;&#29616;&#31616;&#26126;&#34920;&#31034;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#19978;&#37117;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#32858;&#31867;&#30340;&#21521;&#37327;&#37327;&#21270;&#26041;&#27861;&#26356;&#26377;&#20248;&#21183;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#22238;&#24402;&#27169;&#22411;&#20445;&#30041;&#21407;&#22987;&#21521;&#37327;&#30340;&#20851;&#38190;&#32454;&#33410;&#65292;&#21516;&#26102;&#36890;&#36807;&#31232;&#30095;&#35774;&#35745;&#28388;&#38500;&#22122;&#22768;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#23618;MLP&#21644;&#19968;&#20010;&#24191;&#27867;&#30340;&#30721;&#26412;&#26469;&#36817;&#20284;&#31232;&#30095;&#22238;&#24402;&#36807;&#31243;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#36824;&#20351;&#24471;SVQ&#20855;&#26377;&#21487;&#24494;&#24615;&#21644;&#35757;&#32451;&#31616;&#26131;&#24615;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#31354;&#38388;-&#26102;&#38388;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;SVQ&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;
&lt;/p&gt;
&lt;p&gt;
Spatio-temporal forecasting, pivotal in numerous fields, hinges on the delicate equilibrium between isolating nuanced patterns and sifting out noise. To tackle this, we introduce Sparse Regression-based Vector Quantization (SVQ), a novel technique that leverages sparse regression for succinct representation, an approach theoretically and practically favored over classical clustering-based vector quantization methods. This approach preserves critical details from the original vectors using a regression model while filtering out noise via sparse design. Moreover, we approximate the sparse regression process using a blend of a two-layer MLP and an extensive codebook. This approach not only substantially cuts down on computational costs but also grants SVQ differentiability and training simplicity, resulting in a notable enhancement of performance. Our empirical studies on five spatial-temporal benchmark datasets demonstrate that SVQ achieves state-of-the-art results. Specifically, on the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#39044;&#27979;&#27169;&#22411;&#30340;&#37096;&#32626;&#23545;&#20915;&#31574;&#20135;&#29983;&#26377;&#23475;&#24433;&#21709;&#30340;&#24773;&#20917;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20250;&#25104;&#20026;&#26377;&#23475;&#30340;&#33258;&#25105;&#23454;&#29616;&#39044;&#35328;&#12290;&#36825;&#20123;&#27169;&#22411;&#19981;&#20250;&#22240;&#20026;&#23545;&#26576;&#20123;&#24739;&#32773;&#36896;&#25104;&#26356;&#31967;&#31957;&#30340;&#32467;&#26524;&#32780;&#20351;&#20854;&#39044;&#27979;&#33021;&#21147;&#21464;&#26080;&#25928;&#12290;</title><link>https://arxiv.org/abs/2312.01210</link><description>&lt;p&gt;
&#24403;&#20934;&#30830;&#30340;&#39044;&#27979;&#27169;&#22411;&#23548;&#33268;&#26377;&#23475;&#30340;&#33258;&#25105;&#23454;&#29616;&#39044;&#35328;
&lt;/p&gt;
&lt;p&gt;
When accurate prediction models yield harmful self-fulfilling prophecies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#39044;&#27979;&#27169;&#22411;&#30340;&#37096;&#32626;&#23545;&#20915;&#31574;&#20135;&#29983;&#26377;&#23475;&#24433;&#21709;&#30340;&#24773;&#20917;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20250;&#25104;&#20026;&#26377;&#23475;&#30340;&#33258;&#25105;&#23454;&#29616;&#39044;&#35328;&#12290;&#36825;&#20123;&#27169;&#22411;&#19981;&#20250;&#22240;&#20026;&#23545;&#26576;&#20123;&#24739;&#32773;&#36896;&#25104;&#26356;&#31967;&#31957;&#30340;&#32467;&#26524;&#32780;&#20351;&#20854;&#39044;&#27979;&#33021;&#21147;&#21464;&#26080;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#39044;&#27979;&#27169;&#22411;&#22312;&#21307;&#23398;&#30740;&#31350;&#21644;&#23454;&#36341;&#20013;&#38750;&#24120;&#21463;&#27426;&#36814;&#12290;&#36890;&#36807;&#20026;&#29305;&#23450;&#24739;&#32773;&#39044;&#27979;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#20915;&#31574;&#22256;&#38590;&#30340;&#27835;&#30103;&#20915;&#31574;&#65292;&#24182;&#19988;&#36890;&#24120;&#34987;&#35465;&#20026;&#20010;&#24615;&#21270;&#30340;&#12289;&#25968;&#25454;&#39537;&#21160;&#30340;&#21307;&#30103;&#20445;&#20581;&#30340;&#26480;&#20986;&#20195;&#34920;&#12290;&#35768;&#22810;&#39044;&#27979;&#27169;&#22411;&#22312;&#39564;&#35777;&#30740;&#31350;&#20013;&#22522;&#20110;&#20854;&#39044;&#27979;&#20934;&#30830;&#24615;&#32780;&#37096;&#32626;&#29992;&#20110;&#20915;&#31574;&#25903;&#25345;&#12290;&#25105;&#20204;&#35843;&#26597;&#36825;&#26159;&#21542;&#26159;&#19968;&#31181;&#23433;&#20840;&#21644;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;&#21487;&#20197;&#23548;&#33268;&#26377;&#23475;&#30340;&#20915;&#31574;&#65292;&#21363;&#20351;&#22312;&#37096;&#32626;&#21518;&#36825;&#20123;&#39044;&#27979;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#21306;&#20998;&#24230;&#12290;&#36825;&#20123;&#27169;&#22411;&#26159;&#26377;&#23475;&#30340;&#33258;&#25105;&#23454;&#29616;&#39044;&#35328;&#65306;&#23427;&#20204;&#30340;&#37096;&#32626;&#25439;&#23475;&#20102;&#19968;&#32676;&#24739;&#32773;&#65292;&#20294;&#36825;&#20123;&#24739;&#32773;&#30340;&#26356;&#31967;&#31957;&#30340;&#32467;&#26524;&#24182;&#19981;&#20351;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#26080;&#25928;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#23545;&#36825;&#20123;&#39044;&#27979;&#27169;&#22411;&#38598;&#21512;&#30340;&#24418;&#24335;&#21270;&#25551;&#36848;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#37096;&#32626;&#21069;&#21518;&#37117;&#36827;&#34892;&#20102;&#33391;&#22909;&#26657;&#20934;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Objective: Prediction models are popular in medical research and practice. By predicting an outcome of interest for specific patients, these models may help inform difficult treatment decisions, and are often hailed as the poster children for personalized, data-driven healthcare. Many prediction models are deployed for decision support based on their prediction accuracy in validation studies. We investigate whether this is a safe and valid approach.   Materials and Methods: We show that using prediction models for decision making can lead to harmful decisions, even when the predictions exhibit good discrimination after deployment. These models are harmful self-fulfilling prophecies: their deployment harms a group of patients but the worse outcome of these patients does not invalidate the predictive power of the model.   Results: Our main result is a formal characterization of a set of such prediction models. Next we show that models that are well calibrated before and after deployment 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MultiResFormer&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#36873;&#25321;&#26368;&#20248;&#34917;&#19969;&#38271;&#24230;&#65292;&#21160;&#24577;&#24314;&#27169;&#26102;&#24207;&#21464;&#21270;&#12290;&#30456;&#27604;&#20110;&#22522;&#20110;&#34917;&#19969;&#30340;Transformer&#21644;CNN&#22522;&#32447;&#65292;MultiResFormer&#22312;&#38271;&#26399;&#39044;&#27979;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.18780</link><description>&lt;p&gt;
&#22810;&#20998;&#36776;&#29575;&#24314;&#27169;&#30340;Transformers&#65306;&#36866;&#29992;&#20110;&#19968;&#33324;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#33258;&#36866;&#24212;&#22810;&#20998;&#36776;&#29575;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MultiResFormer: Transformer with Adaptive Multi-Resolution Modeling for General Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MultiResFormer&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#36873;&#25321;&#26368;&#20248;&#34917;&#19969;&#38271;&#24230;&#65292;&#21160;&#24577;&#24314;&#27169;&#26102;&#24207;&#21464;&#21270;&#12290;&#30456;&#27604;&#20110;&#22522;&#20110;&#34917;&#19969;&#30340;Transformer&#21644;CNN&#22522;&#32447;&#65292;MultiResFormer&#22312;&#38271;&#26399;&#39044;&#27979;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#36817;&#26399;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#19968;&#20010;&#22266;&#23450;&#30340;&#25110;&#22266;&#23450;&#38598;&#21512;&#30340;&#34917;&#19969;&#38271;&#24230;&#26469;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#32534;&#30721;&#12290;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#26080;&#27861;&#25429;&#25417;&#21040;&#29616;&#23454;&#19990;&#30028;&#20013;&#22810;&#21608;&#26399;&#26102;&#38388;&#24207;&#21015;&#20013;&#23384;&#22312;&#30340;&#22810;&#26679;&#32454;&#33268;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MultiResFormer&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#26368;&#20248;&#30340;&#34917;&#19969;&#38271;&#24230;&#26469;&#21160;&#24577;&#24314;&#27169;&#26102;&#24207;&#21464;&#21270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#27599;&#20010;&#23618;&#30340;&#24320;&#22987;&#26102;&#65292;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#34987;&#32534;&#30721;&#20026;&#20960;&#20010;&#24182;&#34892;&#30340;&#20998;&#25903;&#65292;&#27599;&#20010;&#20998;&#25903;&#20351;&#29992;&#26816;&#27979;&#21040;&#30340;&#21608;&#26399;&#24615;&#65292;&#28982;&#21518;&#36890;&#36807;Transformer&#32534;&#30721;&#22359;&#12290;&#25105;&#20204;&#23545;&#38271;&#26399;&#21644;&#30701;&#26399;&#39044;&#27979;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#23558;MultiResFormer&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#12290;MultiResFormer&#22312;&#38271;&#26399;&#39044;&#27979;&#20219;&#21153;&#19978;&#20248;&#20110;&#22522;&#20110;&#34917;&#19969;&#30340;Transformer&#22522;&#20934;&#65292;&#24182;&#19988;&#22987;&#32456;&#26174;&#33879;&#20248;&#20110;CNN&#22522;&#32447;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#36164;&#28304;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have greatly pushed the boundaries of time series forecasting recently. Existing methods typically encode time series data into $\textit{patches}$ using one or a fixed set of patch lengths. This, however, could result in a lack of ability to capture the variety of intricate temporal dependencies present in real-world multi-periodic time series. In this paper, we propose MultiResFormer, which dynamically models temporal variations by adaptively choosing optimal patch lengths. Concretely, at the beginning of each layer, time series data is encoded into several parallel branches, each using a detected periodicity, before going through the transformer encoder block. We conduct extensive evaluations on long- and short-term forecasting datasets comparing MultiResFormer with state-of-the-art baselines. MultiResFormer outperforms patch-based Transformer baselines on long-term forecasting tasks and also consistently outperforms CNN baselines by a large margin, while usi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#20998;&#25968;&#21305;&#37197;&#35268;&#21017;&#65288;SMaRt&#65289;&#26469;&#25913;&#36827;GANs&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#25345;&#32493;&#23558;&#29983;&#25104;&#30340;&#25968;&#25454;&#28857;&#25512;&#21521;&#30495;&#23454;&#25968;&#25454;&#27969;&#24418;&#65292;&#25552;&#39640;&#20102;&#21512;&#25104;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.18208</link><description>&lt;p&gt;
SMaRt: &#20351;&#29992;&#20998;&#25968;&#21305;&#37197;&#35268;&#21017;&#25913;&#36827;GANs
&lt;/p&gt;
&lt;p&gt;
SMaRt: Improving GANs with Score Matching Regularity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#20998;&#25968;&#21305;&#37197;&#35268;&#21017;&#65288;SMaRt&#65289;&#26469;&#25913;&#36827;GANs&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#25345;&#32493;&#23558;&#29983;&#25104;&#30340;&#25968;&#25454;&#28857;&#25512;&#21521;&#30495;&#23454;&#25968;&#25454;&#27969;&#24418;&#65292;&#25552;&#39640;&#20102;&#21512;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#36890;&#24120;&#22312;&#23398;&#20064;&#39640;&#24230;&#22810;&#26679;&#21270;&#30340;&#22797;&#26434;&#25968;&#25454;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;GANs&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#24182;&#20174;&#29702;&#35770;&#19978;&#25581;&#31034;&#20102;GAN&#35757;&#32451;&#30340;&#21407;&#22987;&#23545;&#25239;&#25439;&#22833;&#19981;&#33021;&#35299;&#20915;&#29983;&#25104;&#25968;&#25454;&#27969;&#24418;&#30340;&#27491;&#27979;&#24230;&#23376;&#38598;&#33853;&#22312;&#30495;&#23454;&#25968;&#25454;&#27969;&#24418;&#20043;&#22806;&#30340;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#20998;&#25968;&#21305;&#37197;&#21487;&#20197;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25345;&#32493;&#23558;&#29983;&#25104;&#30340;&#25968;&#25454;&#28857;&#25512;&#21521;&#30495;&#23454;&#25968;&#25454;&#27969;&#24418;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20998;&#25968;&#21305;&#37197;&#35268;&#21017;&#65288;SMaRt&#65289;&#26469;&#25913;&#36827;GANs&#30340;&#20248;&#21270;&#12290;&#23545;&#20110;&#32463;&#39564;&#35777;&#25454;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#29609;&#20855;&#31034;&#20363;&#26469;&#23637;&#31034;&#36890;&#36807;&#36741;&#21161;&#19968;&#20010;&#30495;&#23454;&#24471;&#20998;&#20989;&#25968;&#26469;&#35757;&#32451;GANs&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#20877;&#29616;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#65292;&#28982;&#21518;&#30830;&#35748;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25345;&#32493;&#25552;&#21319;&#21508;&#31181;&#29366;&#24577;&#30340;&#21512;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial networks (GANs) usually struggle in learning from highly diverse data, whose underlying manifold is complex. In this work, we revisit the mathematical foundations of GANs, and theoretically reveal that the native adversarial loss for GAN training is insufficient to fix the problem of subsets with positive Lebesgue measure of the generated data manifold lying out of the real data manifold. Instead, we find that score matching serves as a promising solution to this issue thanks to its capability of persistently pushing the generated data points towards the real data manifold. We thereby propose to improve the optimization of GANs with score matching regularity (SMaRt). Regarding the empirical evidences, we first design a toy example to show that training GANs by the aid of a ground-truth score function can help reproduce the real data distribution more accurately, and then confirm that our approach can consistently boost the synthesis performance of various state-o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;5G&#21450;&#20854;&#21518;&#22826;&#38451;&#33021;&#23567;&#22411;&#34562;&#31389;&#32593;&#32476;&#30340;&#26368;&#20339;&#26080;&#20154;&#26426;&#36127;&#36733;&#37325;&#20998;&#37197;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#20154;&#26426;&#19978;&#30340;&#31354;&#20013;&#22522;&#31449;&#36827;&#34892;&#21487;&#38752;&#23433;&#20840;&#30340;&#30005;&#21147;&#20877;&#20998;&#37197;&#65292;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#21487;&#38752;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.12944</link><description>&lt;p&gt;
DroneOptiNet: &#19968;&#31181;&#29992;&#20110;5G&#21450;&#20854;&#21518;&#22826;&#38451;&#33021;&#23567;&#22411;&#34562;&#31389;&#32593;&#32476;&#30340;&#26368;&#20339;&#26080;&#20154;&#26426;&#36127;&#36733;&#37325;&#20998;&#37197;&#26426;&#21046;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DroneOptiNet: A Framework for Optimal Drone-based Load Redistribution Mechanism for 5G and Beyond Solar Small Cell Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;5G&#21450;&#20854;&#21518;&#22826;&#38451;&#33021;&#23567;&#22411;&#34562;&#31389;&#32593;&#32476;&#30340;&#26368;&#20339;&#26080;&#20154;&#26426;&#36127;&#36733;&#37325;&#20998;&#37197;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#20154;&#26426;&#19978;&#30340;&#31354;&#20013;&#22522;&#31449;&#36827;&#34892;&#21487;&#38752;&#23433;&#20840;&#30340;&#30005;&#21147;&#20877;&#20998;&#37197;&#65292;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#21487;&#38752;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#20116;&#20195;&#21450;&#20854;&#21518;&#30340;&#34562;&#31389;&#32593;&#32476;&#23545;&#21151;&#29575;&#38656;&#27714;&#25552;&#20986;&#20102;&#37325;&#35201;&#30340;&#38480;&#21046;&#65292;&#38656;&#35201;&#33021;&#22815;&#39640;&#25928;&#21033;&#29992;&#33021;&#28304;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20351;&#29992;&#26080;&#20154;&#26426;&#19978;&#30340;&#31354;&#20013;&#22522;&#31449;&#65288;BS&#65289;&#36827;&#34892;&#21487;&#38752;&#23433;&#20840;&#30340;&#30005;&#21147;&#20877;&#20998;&#37197;&#30340;&#29992;&#25143;&#36127;&#36733;&#36716;&#31227;&#26041;&#27861;&#65292;&#20197;&#36328;&#36234;&#30001;&#32511;&#33394;&#23567;&#22411;&#34562;&#31389;BS&#32452;&#25104;&#30340;&#24494;&#32593;&#32593;&#32476;&#12290;&#26681;&#25454;&#29992;&#25143;&#23494;&#24230;&#21644;&#31354;&#20013;&#22522;&#31449;&#30340;&#21487;&#29992;&#24615;&#65292;&#36890;&#36807;&#23558;&#31354;&#20013;&#22522;&#31449;&#20174;&#39640;&#33021;&#32791;&#23567;&#21306;&#36801;&#31227;&#21040;&#20302;&#33021;&#32791;&#23567;&#21306;&#65292;&#26469;&#28385;&#36275;&#33021;&#37327;&#19981;&#36275;&#30340;&#23567;&#21306;&#30340;&#33021;&#37327;&#38656;&#27714;&#12290;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#26080;&#20154;&#26426;&#26694;&#26550;&#23558;&#38271;&#30701;&#26399;&#35760;&#24518;&#19982;&#29420;&#29305;&#30340;&#25104;&#26412;&#20989;&#25968;&#32467;&#21512;&#65292;&#20351;&#29992;&#36827;&#21270;&#31070;&#32463;&#32593;&#32476;&#26469;&#26377;&#25928;&#22320;&#31649;&#29702;&#26080;&#20154;&#26426;&#21644;&#22522;&#31449;&#30340;&#33021;&#37327;&#21644;&#36127;&#36733;&#37325;&#20998;&#37197;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20943;&#23569;&#20102;&#22522;&#31449;&#20572;&#30005;&#65292;&#24182;&#20445;&#25345;&#20102;&#19968;&#33268;&#30340;&#21534;&#21520;&#37327;&#31283;&#23450;&#24615;&#65292;&#20174;&#32780;&#23637;&#31034;&#20102;&#20854;&#25552;&#21319;&#26080;&#32447;&#32593;&#32476;&#21487;&#38752;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The power requirements posed by the fifth-generation and beyond cellular networks are an important constraint in network deployment and require energy-efficient solutions. In this work, we propose a novel user load transfer approach using airborne base stations (BS) mounted on drones for reliable and secure power redistribution across the micro-grid network comprising green small cell BSs. Depending on the user density and the availability of an aerial BS, the energy requirement of a cell with an energy deficit is accommodated by migrating the aerial BS from a high-energy to a low-energy cell. The proposed hybrid drone-based framework integrates long short-term memory with unique cost functions using an evolutionary neural network for drones and BSs and efficiently manages energy and load redistribution. The proposed algorithm reduces power outages at BSs and maintains consistent throughput stability, thereby demonstrating its capability to boost the reliability and robustness of wirel
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#27169;&#22411;&#65292;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#24314;&#27169;&#65292;&#36825;&#26159;&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#36873;&#25321;&#24314;&#27169;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2311.07607</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#24314;&#27169;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Modeling Choice via Self-Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#27169;&#22411;&#65292;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#24314;&#27169;&#65292;&#36825;&#26159;&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#36873;&#25321;&#24314;&#27169;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#27169;&#22411;&#26159;&#36816;&#33829;&#31649;&#29702;&#39046;&#22495;&#20013;&#35768;&#22810;&#32463;&#20856;&#20248;&#21270;&#38382;&#39064;&#30340;&#22522;&#30784;&#36755;&#20837;&#65292;&#21253;&#25324;&#32452;&#21512;&#12289;&#24211;&#23384;&#21644;&#23450;&#20215;&#20248;&#21270;&#12290;&#20934;&#30830;&#22320;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#36825;&#20123;&#27169;&#22411;&#26159;&#22312;&#23454;&#36341;&#20013;&#24212;&#29992;&#36825;&#20123;&#20248;&#21270;&#38382;&#39064;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#24341;&#36215;&#20102;&#23558;&#36825;&#20123;&#25216;&#26415;&#25972;&#21512;&#21040;&#36873;&#25321;&#24314;&#27169;&#20013;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#36873;&#25321;&#24314;&#27169;&#30340;&#20132;&#21449;&#28857;&#19978;&#23384;&#22312;&#26126;&#26174;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#23588;&#20854;&#26159;&#22312;&#29702;&#35770;&#21644;&#32463;&#39564;&#22522;&#30784;&#19978;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#27169;&#22411;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25104;&#21151;&#65288;&#20174;&#29702;&#35770;&#21644;&#23454;&#36341;&#20004;&#20010;&#26041;&#38754;&#65289;&#21033;&#29992;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#27010;&#24565;&#65288;&#33258;&#27880;&#24847;&#21147;&#65289;&#30340;&#27169;&#22411;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#36873;&#25321;&#27169;&#22411;&#26159;Halo&#22810;&#39033;&#24335;&#36923;&#36753;&#65288;Halo-MNL&#65289;&#27169;&#22411;&#30340;&#20302;&#31209;&#25512;&#24191;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Halo-MNL&#27169;&#22411;&#38656;&#35201;$\Omega(m^2)$&#30340;&#35745;&#31639;&#37327;&#65292;&#32780;&#25105;&#20204;&#30340;&#27169;&#22411;&#21482;&#38656;&#35201;$\Omega(m)$&#30340;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models of choice are a fundamental input to many now-canonical optimization problems in the field of Operations Management, including assortment, inventory, and price optimization. Naturally, accurate estimation of these models from data is a critical step in the application of these optimization problems in practice. Concurrently, recent advancements in deep learning have sparked interest in integrating these techniques into choice modeling. However, there is a noticeable research gap at the intersection of deep learning and choice modeling, particularly with both theoretical and empirical foundations. Thus motivated, we first propose a choice model that is the first to successfully (both theoretically and practically) leverage a modern neural network architectural concept (self-attention). Theoretically, we show that our attention-based choice model is a low-rank generalization of the Halo Multinomial Logit (Halo-MNL) model. We prove that whereas the Halo-MNL requires $\Omega(m^2)$ d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#20197;&#21450;&#21033;&#29992;&#26410;&#26631;&#35760;&#26032;&#38395;&#25991;&#31456;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#20027;&#39064;&#26080;&#20851;&#21644;&#20027;&#39064;&#24863;&#30693;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#22312;&#31435;&#22330;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;0.771&#30340;F1&#20998;&#25968;&#12290;</title><link>https://arxiv.org/abs/2310.14450</link><description>&lt;p&gt;
TATA: &#36890;&#36807;&#20027;&#39064;&#26080;&#20851;&#21644;&#20027;&#39064;&#24863;&#30693;&#30340;&#23884;&#20837;&#36827;&#34892;&#31435;&#22330;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
TATA: Stance Detection via Topic-Agnostic and Topic-Aware Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#20197;&#21450;&#21033;&#29992;&#26410;&#26631;&#35760;&#26032;&#38395;&#25991;&#31456;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#20027;&#39064;&#26080;&#20851;&#21644;&#20027;&#39064;&#24863;&#30693;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#22312;&#31435;&#22330;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;0.771&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31435;&#22330;&#26816;&#27979;&#23545;&#20110;&#29702;&#35299;&#20114;&#32852;&#32593;&#19978;&#19981;&#21516;&#30340;&#24577;&#24230;&#21644;&#20449;&#20208;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#19968;&#31687;&#25991;&#31456;&#23545;&#32473;&#23450;&#20027;&#39064;&#30340;&#31435;&#22330;&#24448;&#24448;&#39640;&#24230;&#20381;&#36182;&#20110;&#35813;&#20027;&#39064;&#65292;&#24314;&#31435;&#19968;&#20010;&#33021;&#25512;&#24191;&#21040;&#26410;&#30693;&#20027;&#39064;&#30340;&#31435;&#22330;&#26816;&#27979;&#27169;&#22411;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#20197;&#21450;&#28085;&#30422;&#20102;&#21508;&#31181;&#19981;&#21516;&#20027;&#39064;&#30340;&#26410;&#26631;&#35760;&#26032;&#38395;&#25991;&#31456;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#29992;&#20110;&#19979;&#28216;&#31435;&#22330;&#26816;&#27979;&#30340;&#20027;&#39064;&#26080;&#20851;&#65288;TAG&#65289;&#21644;&#20027;&#39064;&#24863;&#30693;&#65288;TAW&#65289;&#30340;&#23884;&#20837;&#12290;&#23558;&#36825;&#20123;&#23884;&#20837;&#32452;&#21512;&#22312;&#25105;&#20204;&#30340;&#23436;&#25972;TATA&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#20844;&#24320;&#30340;&#31435;&#22330;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65288;&#22312;Zero-shot VAST&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;0.771&#30340;F1&#20998;&#25968;&#65289;&#12290;&#25105;&#20204;&#22312;https://github.com/hanshanley/tata&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stance detection is important for understanding different attitudes and beliefs on the Internet. However, given that a passage's stance toward a given topic is often highly dependent on that topic, building a stance detection model that generalizes to unseen topics is difficult. In this work, we propose using contrastive learning as well as an unlabeled dataset of news articles that cover a variety of different topics to train topic-agnostic/TAG and topic-aware/TAW embeddings for use in downstream stance detection. Combining these embeddings in our full TATA model, we achieve state-of-the-art performance across several public stance detection datasets (0.771 $F_1$-score on the Zero-shot VAST dataset). We release our code and data at https://github.com/hanshanley/tata.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Lie&#31070;&#32463;&#20803;&#32593;&#32476;&#65292;&#33021;&#22815;&#20197;&#20219;&#20309;&#21322;&#21333;Lie&#20195;&#25968;&#25968;&#25454;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#20276;&#38543;&#25805;&#20316;&#20351;&#20854;&#20855;&#26377;&#31561;&#21464;&#24615;&#12290;&#36890;&#36807;&#25512;&#24191;&#21521;&#37327;&#31070;&#32463;&#20803;&#32593;&#32476;&#21644;&#24341;&#20837;&#26032;&#30340;&#23618;&#65292;&#35813;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.04521</link><description>&lt;p&gt;
Lie&#31070;&#32463;&#20803;&#65306;&#21322;&#21333;Lie&#20195;&#25968;&#30340;&#20276;&#38543;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Lie Neurons: Adjoint-Equivariant Neural Networks for Semisimple Lie Algebras
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Lie&#31070;&#32463;&#20803;&#32593;&#32476;&#65292;&#33021;&#22815;&#20197;&#20219;&#20309;&#21322;&#21333;Lie&#20195;&#25968;&#25968;&#25454;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#20276;&#38543;&#25805;&#20316;&#20351;&#20854;&#20855;&#26377;&#31561;&#21464;&#24615;&#12290;&#36890;&#36807;&#25512;&#24191;&#21521;&#37327;&#31070;&#32463;&#20803;&#32593;&#32476;&#21644;&#24341;&#20837;&#26032;&#30340;&#23618;&#65292;&#35813;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#65292;&#35813;&#25968;&#25454;&#23384;&#22312;&#20110;&#20219;&#20309;&#21322;&#21333;Lie&#20195;&#25968;&#20013;&#12290;&#23545;&#24212;&#30340;&#32676;&#36890;&#36807;&#20276;&#38543;&#25805;&#20316;&#20316;&#29992;&#20110;Lie&#20195;&#25968;&#19978;&#65292;&#20351;&#24471;&#25105;&#20204;&#25552;&#20986;&#30340;&#32593;&#32476;&#20855;&#26377;&#20276;&#38543;&#31561;&#21464;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#31616;&#21333;&#30340;$\mathrm{SO}(3)$-&#31561;&#21464;&#32593;&#32476;&#8212;&#8212;&#21521;&#37327;&#31070;&#32463;&#20803;&#20174;3&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#25512;&#24191;&#21040;Lie&#20195;&#25968;&#31354;&#38388;&#65292;&#21033;&#29992;Killing&#24418;&#24335;&#30340;&#19981;&#21464;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;Lie&#25324;&#21495;&#23618;&#21644;&#20960;&#20309;&#36890;&#36947;&#28151;&#21512;&#23618;&#26469;&#25193;&#23637;&#24314;&#27169;&#33021;&#21147;&#12290;&#23454;&#39564;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23545;$\mathfrak{so}(3)$&#21644;$\mathfrak{sl}(3)$ Lie&#20195;&#25968;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#21253;&#25324;&#25311;&#21512;&#31561;&#21464;&#21644;&#19981;&#21464;&#20989;&#25968;&#12289;&#23398;&#20064;&#31995;&#32479;&#21160;&#21147;&#23398;&#12289;&#28857;&#20113;&#37197;&#20934;&#21644;&#22522;&#20110;&#21333;&#24212;&#24615;&#30340;&#24418;&#29366;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31561;&#21464;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#34920;&#29616;&#20986;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an equivariant neural network that takes data in any semi-simple Lie algebra as input. The corresponding group acts on the Lie algebra as adjoint operations, making our proposed network adjoint-equivariant. Our framework generalizes the Vector Neurons, a simple $\mathrm{SO}(3)$-equivariant network, from 3-D Euclidean space to Lie algebra spaces, building upon the invariance property of the Killing form. Furthermore, we propose novel Lie bracket layers and geometric channel mixing layers that extend the modeling capacity. Experiments are conducted for the $\mathfrak{so}(3)$ and $\mathfrak{sl}(3)$ Lie algebras on various tasks, including fitting equivariant and invariant functions, learning system dynamics, point cloud registration, and homography-based shape classification. Our proposed equivariant network shows wide applicability and competitive performance in various domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#23637;&#31034;&#20102;&#27599;&#20010;&#23398;&#20064;&#26426;&#20250;&#32463;&#36807;&#22810;&#27425;&#23616;&#37096;&#26356;&#26032;&#21518;&#36817;&#20284;&#36827;&#34892;&#26799;&#24230;&#27493;&#39588;&#12290;&#36825;&#19968;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21487;&#33021;&#22312;&#20248;&#21270;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20013;&#36215;&#21040;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2309.05102</link><description>&lt;p&gt;
&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#26159;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#21527;&#65311;&#20351;&#29992;&#38543;&#26426;&#36807;&#31243;&#36827;&#34892;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Is Learning in Biological Neural Networks based on Stochastic Gradient Descent? An analysis using stochastic processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.05102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#23637;&#31034;&#20102;&#27599;&#20010;&#23398;&#20064;&#26426;&#20250;&#32463;&#36807;&#22810;&#27425;&#23616;&#37096;&#26356;&#26032;&#21518;&#36817;&#20284;&#36827;&#34892;&#26799;&#24230;&#27493;&#39588;&#12290;&#36825;&#19968;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21487;&#33021;&#22312;&#20248;&#21270;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20013;&#36215;&#21040;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20851;&#20110;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#20013;&#23398;&#20064;&#19982;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#30340;&#21306;&#21035;&#19968;&#30452;&#23384;&#22312;&#28608;&#28872;&#30340;&#20105;&#35770;&#12290;&#36890;&#24120;&#35748;&#20026;&#65292;&#22823;&#33041;&#20013;&#36830;&#25509;&#30340;&#26356;&#26032;&#20165;&#20381;&#36182;&#20110;&#23616;&#37096;&#20449;&#24687;&#65292;&#22240;&#27492;&#19981;&#33021;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31867;&#22411;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;BNNs&#20013;&#30417;&#30563;&#23398;&#20064;&#30340;&#38543;&#26426;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#27599;&#20010;&#23398;&#20064;&#26426;&#20250;&#36890;&#36807;&#35768;&#22810;&#23616;&#37096;&#26356;&#26032;&#36827;&#34892;&#22788;&#29702;&#26102;&#65292;&#65288;&#36830;&#32493;&#30340;&#65289;&#26799;&#24230;&#27493;&#39588;&#36817;&#20284;&#21457;&#29983;&#12290;&#36825;&#19968;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21487;&#33021;&#22312;&#20248;&#21270;BNNs&#20013;&#36215;&#21040;&#19968;&#23450;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been an intense debate about how learning in biological neural networks (BNNs) differs from learning in artificial neural networks. It is often argued that the updating of connections in the brain relies only on local information, and therefore a stochastic gradient-descent type optimization method cannot be used. In this paper, we study a stochastic model for supervised learning in BNNs. We show that a (continuous) gradient step occurs approximately when each learning opportunity is processed by many local updates. This result suggests that stochastic gradient descent may indeed play a role in optimizing BNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33455;&#29255;&#19978;&#36827;&#34892;&#30828;&#20214;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#26694;&#26550;&#65288;OHQ&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#37327;&#21270;&#24863;&#30693;&#27969;&#27700;&#32447;&#21644;&#24341;&#20837;&#25513;&#30721;&#24341;&#23548;&#30340;&#37327;&#21270;&#20272;&#35745;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#36827;&#34892;&#39640;&#25928;&#37327;&#21270;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#25628;&#32034;&#31354;&#38388;&#36807;&#22823;&#21644;&#23454;&#38469;&#37096;&#32626;&#24046;&#36317;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2309.01945</link><description>&lt;p&gt;
OHQ: &#33455;&#29255;&#19978;&#30340;&#30828;&#20214;&#24863;&#30693;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
OHQ: On-chip Hardware-aware Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33455;&#29255;&#19978;&#36827;&#34892;&#30828;&#20214;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#26694;&#26550;&#65288;OHQ&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#37327;&#21270;&#24863;&#30693;&#27969;&#27700;&#32447;&#21644;&#24341;&#20837;&#25513;&#30721;&#24341;&#23548;&#30340;&#37327;&#21270;&#20272;&#35745;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#36827;&#34892;&#39640;&#25928;&#37327;&#21270;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#25628;&#32034;&#31354;&#38388;&#36807;&#22823;&#21644;&#23454;&#38469;&#37096;&#32626;&#24046;&#36317;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#25104;&#20026;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#37096;&#32626;&#20808;&#36827;&#28145;&#24230;&#27169;&#22411;&#30340;&#26368;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#21033;&#29992;&#22810;&#20301;&#23485;&#26550;&#26500;&#26469;&#37322;&#25918;&#37327;&#21270;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#23384;&#22312;&#25628;&#32034;&#31354;&#38388;&#36807;&#22823;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#22240;&#27492;&#65292;&#37327;&#21270;&#36807;&#31243;&#20381;&#36182;&#20110;&#29420;&#31435;&#30340;&#39640;&#24615;&#33021;&#35774;&#22791;&#65292;&#32780;&#19981;&#26159;&#26412;&#22320;&#36827;&#34892;&#65292;&#36825;&#20063;&#23548;&#33268;&#20102;&#32771;&#34385;&#30340;&#30828;&#20214;&#25351;&#26631;&#19982;&#23454;&#38469;&#37096;&#32626;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33455;&#29255;&#19978;&#36827;&#34892;&#30828;&#20214;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#26694;&#26550;&#65288;OHQ&#65289;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#22312;&#32447;&#35774;&#22791;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#33455;&#29255;&#19978;&#30340;&#37327;&#21270;&#24863;&#30693;&#65288;OQA&#65289;&#27969;&#27700;&#32447;&#65292;&#33021;&#22815;&#24863;&#30693;&#37327;&#21270;&#31639;&#23376;&#22312;&#30828;&#20214;&#19978;&#30340;&#23454;&#38469;&#25928;&#29575;&#25351;&#26631;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25513;&#30721;&#24341;&#23548;&#30340;&#37327;&#21270;&#20272;&#35745;&#65288;MQE&#65289;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization emerges as one of the most promising approaches for deploying advanced deep models on resource-constrained hardware. Mixed-precision quantization leverages multiple bit-width architectures to unleash the accuracy and efficiency potential of quantized models. However, existing mixed-precision quantization suffers exhaustive search space that causes immense computational overhead. The quantization process thus relies on separate high-performance devices rather than locally, which also leads to a significant gap between the considered hardware metrics and the real deployment.In this paper, we propose an On-chip Hardware-aware Quantization (OHQ) framework that performs hardware-aware mixed-precision quantization without accessing online devices. First, we construct the On-chip Quantization Awareness (OQA) pipeline, enabling perceive the actual efficiency metrics of the quantization operator on the hardware.Second, we propose Mask-guided Quantization Estimation (MQE) technique 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Lookbehind-SAM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#27425;&#19978;&#21319;&#27493;&#39588;&#21644;&#32447;&#24615;&#25554;&#20540;&#26469;&#22686;&#24378;&#26368;&#22823;&#21270;&#21644;&#26368;&#23567;&#21270;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25439;&#22833;&#38160;&#24230;&#25240;&#34935;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#26377;&#22810;&#31181;&#20248;&#28857;&#65292;&#21253;&#25324;&#25552;&#39640;&#30340;&#27867;&#21270;&#24615;&#33021;&#12289;&#26356;&#39640;&#30340;&#40065;&#26834;&#24615;&#21644;&#25913;&#36827;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2307.16704</link><description>&lt;p&gt;
Lookbehind-SAM: k&#27493;&#22238;&#26395;&#65292;1&#27493;&#21069;&#36827;
&lt;/p&gt;
&lt;p&gt;
Lookbehind-SAM: k steps back, 1 step forward
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.16704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Lookbehind-SAM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#27425;&#19978;&#21319;&#27493;&#39588;&#21644;&#32447;&#24615;&#25554;&#20540;&#26469;&#22686;&#24378;&#26368;&#22823;&#21270;&#21644;&#26368;&#23567;&#21270;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25439;&#22833;&#38160;&#24230;&#25240;&#34935;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#26377;&#22810;&#31181;&#20248;&#28857;&#65292;&#21253;&#25324;&#25552;&#39640;&#30340;&#27867;&#21270;&#24615;&#33021;&#12289;&#26356;&#39640;&#30340;&#40065;&#26834;&#24615;&#21644;&#25913;&#36827;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#65288;SAM&#65289;&#26041;&#27861;&#36890;&#36807;&#23558;&#26368;&#23567;&#21270;&#25439;&#22833;&#20540;&#21644;&#25439;&#22833;&#38160;&#24230;&#38382;&#39064;&#34920;&#36848;&#20026;&#26497;&#23567;&#26497;&#22823;&#22411;&#30446;&#26631;&#65292;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22686;&#21152;&#20102;SAM&#30446;&#26631;&#20013;&#26368;&#22823;&#21270;&#21644;&#26368;&#23567;&#21270;&#37096;&#20998;&#30340;&#25928;&#29575;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25439;&#22833;&#38160;&#24230;&#25240;&#34935;&#12290;&#21463;Lookahead&#20248;&#21270;&#22120;&#30340;&#21551;&#21457;&#65292;&#35813;&#20248;&#21270;&#22120;&#20351;&#29992;&#22810;&#20010;&#21521;&#21069;&#30340;&#19979;&#38477;&#27493;&#39588;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Lookbehind&#65292;&#23427;&#22312;&#21518;&#38754;&#25191;&#34892;&#22810;&#20010;&#19978;&#21319;&#27493;&#39588;&#65292;&#22686;&#24378;&#20102;SAM&#30340;&#26368;&#22823;&#21270;&#27493;&#39588;&#65292;&#24182;&#25214;&#21040;&#20102;&#19968;&#20010;&#20855;&#26377;&#26356;&#39640;&#25439;&#22833;&#30340;&#26368;&#22351;&#24773;&#20917;&#25200;&#21160;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#20943;&#23567;&#30001;&#20110;&#25910;&#38598;&#21040;&#30340;&#22810;&#20010;&#19978;&#21319;&#27493;&#39588;&#30340;&#26799;&#24230;&#25152;&#24341;&#36215;&#30340;&#19979;&#38477;&#27493;&#39588;&#30340;&#26041;&#24046;&#65292;&#25105;&#20204;&#37319;&#29992;&#32447;&#24615;&#25554;&#20540;&#26469;&#25913;&#36827;&#26368;&#23567;&#21270;&#36807;&#31243;&#12290;Lookbehind&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#24102;&#26469;&#20102;&#35768;&#22810;&#22909;&#22788;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25552;&#39640;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#23545;&#22122;&#22768;&#26435;&#37325;&#30340;&#26356;&#39640;&#40065;&#26834;&#24615;&#65292;&#20197;&#21450;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#25913;&#36827;&#30340;&#25928;&#26524;&#21644;&#36739;&#23569;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharpness-aware minimization (SAM) methods have gained increasing popularity by formulating the problem of minimizing both loss value and loss sharpness as a minimax objective. In this work, we increase the efficiency of the maximization and minimization parts of SAM's objective to achieve a better loss-sharpness trade-off. By taking inspiration from the Lookahead optimizer, which uses multiple descent steps ahead, we propose Lookbehind, which performs multiple ascent steps behind to enhance the maximization step of SAM and find a worst-case perturbation with higher loss. Then, to mitigate the variance in the descent step arising from the gathered gradients across the multiple ascent steps, we employ linear interpolation to refine the minimization step. Lookbehind leads to a myriad of benefits across a variety of tasks. Particularly, we show increased generalization performance, greater robustness against noisy weights, as well as improved learning and less catastrophic forgetting in l
&lt;/p&gt;</description></item><item><title>AnimateDiff&#26159;&#19968;&#20010;&#23454;&#29992;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21160;&#30011;&#21270;&#20010;&#24615;&#21270;&#30340;T2I&#27169;&#22411;&#65292;&#26080;&#38656;&#29305;&#23450;&#35843;&#25972;&#12290;&#23427;&#21253;&#21547;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#36816;&#21160;&#27169;&#22359;&#65292;&#36890;&#36807;&#35757;&#32451;&#20351;&#29992;&#23454;&#38469;&#35270;&#39057;&#20013;&#30340;&#36816;&#21160;&#20808;&#39564;&#12290;&#36816;&#21160;&#27169;&#22359;&#21487;&#20197;&#25554;&#20837;&#21040;&#20219;&#20309;&#20010;&#24615;&#21270;T2I&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#24418;&#25104;&#19968;&#20010;&#20010;&#24615;&#21270;&#30340;&#21160;&#30011;&#29983;&#25104;&#22120;&#12290;</title><link>https://arxiv.org/abs/2307.04725</link><description>&lt;p&gt;
AnimateDiff&#65306;&#26080;&#38656;&#29305;&#23450;&#35843;&#25972;&#21363;&#21487;&#20351;&#20010;&#24615;&#21270;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21160;&#30011;
&lt;/p&gt;
&lt;p&gt;
AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.04725
&lt;/p&gt;
&lt;p&gt;
AnimateDiff&#26159;&#19968;&#20010;&#23454;&#29992;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21160;&#30011;&#21270;&#20010;&#24615;&#21270;&#30340;T2I&#27169;&#22411;&#65292;&#26080;&#38656;&#29305;&#23450;&#35843;&#25972;&#12290;&#23427;&#21253;&#21547;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#36816;&#21160;&#27169;&#22359;&#65292;&#36890;&#36807;&#35757;&#32451;&#20351;&#29992;&#23454;&#38469;&#35270;&#39057;&#20013;&#30340;&#36816;&#21160;&#20808;&#39564;&#12290;&#36816;&#21160;&#27169;&#22359;&#21487;&#20197;&#25554;&#20837;&#21040;&#20219;&#20309;&#20010;&#24615;&#21270;T2I&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#24418;&#25104;&#19968;&#20010;&#20010;&#24615;&#21270;&#30340;&#21160;&#30011;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#25193;&#25955;&#27169;&#22411;&#65288;&#22914;&#31283;&#23450;&#25193;&#25955;&#65289;&#21644;&#30456;&#24212;&#30340;&#20010;&#24615;&#21270;&#25216;&#26415;&#65288;&#22914;DreamBooth&#21644;LoRA&#65289;&#30340;&#25512;&#21160;&#65292;&#27599;&#20010;&#20154;&#37117;&#33021;&#20197;&#21512;&#29702;&#30340;&#25104;&#26412;&#23558;&#20182;&#20204;&#30340;&#24819;&#35937;&#21147;&#23637;&#29616;&#20026;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#23558;&#36816;&#21160;&#21160;&#24577;&#28155;&#21152;&#21040;&#29616;&#26377;&#39640;&#36136;&#37327;&#20010;&#24615;&#21270;T2I&#24182;&#20351;&#20854;&#29983;&#25104;&#21160;&#30011;&#20173;&#28982;&#26159;&#19968;&#20010;&#20844;&#24320;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AnimateDiff&#65292;&#36825;&#26159;&#19968;&#20010;&#23454;&#29992;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21160;&#30011;&#21270;&#20010;&#24615;&#21270;&#30340;T2I&#27169;&#22411;&#32780;&#26080;&#38656;&#29305;&#23450;&#30340;&#35843;&#25972;&#12290;&#25105;&#20204;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#36816;&#21160;&#27169;&#22359;&#65292;&#21482;&#38656;&#36827;&#34892;&#19968;&#27425;&#35757;&#32451;&#65292;&#21363;&#21487;&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#26469;&#33258;&#30456;&#21516;&#22522;&#30784;T2I&#30340;&#20219;&#20309;&#20010;&#24615;&#21270;T2I&#20013;&#12290;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36816;&#21160;&#27169;&#22359;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#29616;&#23454;&#19990;&#30028;&#30340;&#35270;&#39057;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#36816;&#21160;&#20808;&#39564;&#12290;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#65292;&#35813;&#36816;&#21160;&#27169;&#22359;&#21487;&#20197;&#25554;&#20837;&#21040;&#20010;&#24615;&#21270;T2I&#27169;&#22411;&#20013;&#65292;&#24418;&#25104;&#19968;&#20010;&#20010;&#24615;&#21270;&#30340;&#21160;&#30011;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;MotionLoRA&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;...&#65288;&#25688;&#35201;&#26410;&#23436;&#25104;&#65289;
&lt;/p&gt;
&lt;p&gt;
With the advance of text-to-image (T2I) diffusion models (e.g., Stable Diffusion) and corresponding personalization techniques such as DreamBooth and LoRA, everyone can manifest their imagination into high-quality images at an affordable cost. However, adding motion dynamics to existing high-quality personalized T2Is and enabling them to generate animations remains an open challenge. In this paper, we present AnimateDiff, a practical framework for animating personalized T2I models without requiring model-specific tuning. At the core of our framework is a plug-and-play motion module that can be trained once and seamlessly integrated into any personalized T2Is originating from the same base T2I. Through our proposed training strategy, the motion module effectively learns transferable motion priors from real-world videos. Once trained, the motion module can be inserted into a personalized T2I model to form a personalized animation generator. We further propose MotionLoRA, a lightweight fi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Transformer in Transformer (TinT)&#30340;&#39640;&#25928;&#26500;&#36896;&#26041;&#24335;&#65292;&#23427;&#21487;&#20197;&#35753;Transformer&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#27169;&#25311;&#21644;&#24494;&#35843;&#22797;&#26434;&#30340;&#20869;&#37096;&#27169;&#22411;&#65292;&#21516;&#26102;&#20351;&#29992;&#21019;&#26032;&#30340;&#36817;&#20284;&#25216;&#26415;&#22823;&#24133;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#21644;&#20869;&#23384;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2307.01189</link><description>&lt;p&gt;
&#21487;&#35757;&#32451;&#30340;Transformer in Transformer
&lt;/p&gt;
&lt;p&gt;
Trainable Transformer in Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.01189
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Transformer in Transformer (TinT)&#30340;&#39640;&#25928;&#26500;&#36896;&#26041;&#24335;&#65292;&#23427;&#21487;&#20197;&#35753;Transformer&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#27169;&#25311;&#21644;&#24494;&#35843;&#22797;&#26434;&#30340;&#20869;&#37096;&#27169;&#22411;&#65292;&#21516;&#26102;&#20351;&#29992;&#21019;&#26032;&#30340;&#36817;&#20284;&#25216;&#26415;&#22823;&#24133;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#21644;&#20869;&#23384;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#24402;&#22240;&#20110;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#38544;&#24335;&#27169;&#25311;&#21644;&#24494;&#35843;&#20869;&#37096;&#27169;&#22411;&#65288;&#22914;&#32447;&#24615;&#25110;2&#23618;MLP&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26500;&#36896;&#38656;&#35201;&#22823;&#37327;&#30340;&#20869;&#23384;&#24320;&#38144;&#65292;&#20351;&#24471;&#27169;&#25311;&#26356;&#22797;&#26434;&#30340;&#20869;&#37096;&#27169;&#22411;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26500;&#36896;&#26041;&#24335;&#65292;&#31216;&#20026;Transformer in Transformer&#65288;&#31616;&#31216;TinT&#65289;&#65292;&#23427;&#20801;&#35768;&#19968;&#20010;Transformer&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#27169;&#25311;&#21644;&#24494;&#35843;&#22797;&#26434;&#30340;&#20869;&#37096;&#27169;&#22411;&#65288;&#22914;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65289;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21019;&#26032;&#30340;&#36817;&#20284;&#25216;&#26415;&#65292;&#20351;&#24471;&#19968;&#20010;&#25317;&#26377;&#19981;&#21040;20&#20159;&#21442;&#25968;&#30340;TinT&#27169;&#22411;&#33021;&#22815;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#27169;&#25311;&#21644;&#24494;&#35843;&#19968;&#20010;&#25317;&#26377;1.25&#20159;&#21442;&#25968;&#30340;Transformer&#27169;&#22411;&#12290;TinT&#36866;&#29992;&#20110;&#35768;&#22810;&#24120;&#35265;&#30340;Transformer&#21464;&#20307;&#65292;&#20854;&#35774;&#35745;&#24605;&#36335;&#36824;&#25913;&#36827;&#20102;Transformer&#20013;&#31616;&#21333;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#31471;&#21040;&#31471;&#23454;&#39564;&#26469;&#39564;&#35777;...
&lt;/p&gt;
&lt;p&gt;
Recent works attribute the capability of in-context learning (ICL) in large pre-trained language models to implicitly simulating and fine-tuning an internal model (e.g., linear or 2-layer MLP) during inference. However, such constructions require large memory overhead, which makes simulation of more sophisticated internal models intractable. In this work, we propose an efficient construction, Transformer in Transformer (in short, TinT), that allows a transformer to simulate and fine-tune complex models internally during inference (e.g., pre-trained language models). In particular, we introduce innovative approximation techniques that allow a TinT model with less than 2 billion parameters to simulate and fine-tune a 125 million parameter transformer model within a single forward pass. TinT accommodates many common transformer variants and its design ideas also improve the efficiency of past instantiations of simple models inside transformers. We conduct end-to-end experiments to validat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20855;&#26377;&#21487;&#25968;&#26080;&#38480;&#29366;&#24577;&#31354;&#38388;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#27748;&#26222;&#26862;&#37319;&#26679;&#21644;&#21160;&#24577;&#22823;&#23567;&#29255;&#27573;&#30340;&#31639;&#27861;&#36827;&#34892;&#36125;&#21494;&#26031;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22312;&#36825;&#20123;&#27169;&#22411;&#19978;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2306.02574</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#21487;&#25968;&#26080;&#38480;&#29366;&#24577;&#31354;&#38388;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Bayesian Learning of Optimal Policies in Markov Decision Processes with Countably Infinite State-Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.02574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20855;&#26377;&#21487;&#25968;&#26080;&#38480;&#29366;&#24577;&#31354;&#38388;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#27748;&#26222;&#26862;&#37319;&#26679;&#21644;&#21160;&#24577;&#22823;&#23567;&#29255;&#27573;&#30340;&#31639;&#27861;&#36827;&#34892;&#36125;&#21494;&#26031;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22312;&#36825;&#20123;&#27169;&#22411;&#19978;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24456;&#22810;&#29616;&#23454;&#24212;&#29992;&#30340;&#27169;&#22411;&#65292;&#22914;&#36890;&#20449;&#32593;&#32476;&#25110;&#35745;&#31639;&#31995;&#32479;&#30340;&#25490;&#38431;&#27169;&#22411;&#65292;&#37117;&#20855;&#26377;&#21487;&#25968;&#26080;&#38480;&#29366;&#24577;&#31354;&#38388;&#12290;&#30446;&#21069;&#24050;&#32463;&#24320;&#21457;&#30340;&#31639;&#27861;&#21644;&#23398;&#20064;&#36807;&#31243;&#20027;&#35201;&#38024;&#23545;&#26377;&#38480;&#29366;&#24577;&#35774;&#32622;&#65292;&#24182;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#36825;&#20123;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19968;&#20010;&#26410;&#30693;&#21442;&#25968;&#952;&#8712;&#920;&#25511;&#21046;&#19979;&#30340;&#23478;&#26063;&#31163;&#25955;&#26102;&#38388;&#21487;&#25968;&#29366;&#24577;&#31354;&#38388;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#35813;&#36807;&#31243;&#23450;&#20041;&#22312;&#21487;&#25968;&#26080;&#38480;&#29366;&#24577;&#31354;&#38388;X&#965;={Z+}d&#19978;&#65292;&#20855;&#26377;&#26377;&#38480;&#21160;&#20316;&#31354;&#38388;A&#965;&#20197;&#21450;&#26080;&#30028;&#25104;&#26412;&#20989;&#25968;&#12290;&#25105;&#20204;&#37319;&#29992;&#36125;&#21494;&#26031;&#35266;&#28857;&#65292;&#23558;&#38543;&#26426;&#26410;&#30693;&#21442;&#25968;&#952;*&#20316;&#20026;&#32473;&#23450;&#20808;&#39564;&#20998;&#24067;&#22312;&#920;&#19978;&#29983;&#25104;&#12290;&#20026;&#20102;&#26368;&#20248;&#22320;&#25511;&#21046;&#26410;&#30693;&#30340;MDP&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27748;&#26222;&#26862;&#37319;&#26679;&#21644;&#21160;&#24577;&#22823;&#23567;&#29255;&#27573;&#30340;&#31639;&#27861;&#65306;&#22312;&#27599;&#20010;&#29255;&#27573;&#30340;&#24320;&#22987;&#65292;&#26681;&#25454;&#21518;&#39564;&#27010;&#29575;&#20998;&#24067;&#36873;&#25321;&#21160;&#20316;&#65292;&#24182;&#22312;&#27599;&#20010;&#29255;&#27573;&#32467;&#26463;&#26102;&#26356;&#26032;&#21518;&#39564;&#27010;&#29575;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models of many real-life applications, such as queuing models of communication networks or computing systems, have a countably infinite state-space. Algorithmic and learning procedures that have been developed to produce optimal policies mainly focus on finite state settings, and do not directly apply to these models. To overcome this lacuna, in this work we study the problem of optimal control of a family of discrete-time countable state-space Markov Decision Processes (MDPs) governed by an unknown parameter $\theta\in\Theta$, and defined on a countably-infinite state space $\mathcal X=\mathbb{Z}_+^d$, with finite action space $\mathcal A$, and an unbounded cost function. We take a Bayesian perspective with the random unknown parameter $\boldsymbol{\theta}^*$ generated via a given fixed prior distribution on $\Theta$. To optimally control the unknown MDP, we propose an algorithm based on Thompson sampling with dynamically-sized episodes: at the beginning of each episode, the posterior
&lt;/p&gt;</description></item><item><title>ParlayANN&#26159;&#19968;&#20010;&#20855;&#26377;&#30830;&#23450;&#24615;&#21644;&#24182;&#34892;&#24615;&#30340;&#22522;&#20110;&#22270;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#31639;&#27861;&#24211;&#65292;&#25552;&#20379;&#20102;&#29992;&#20110;&#24320;&#21457;&#36825;&#31867;&#31639;&#27861;&#30340;&#19968;&#22871;&#26377;&#29992;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2305.04359</link><description>&lt;p&gt;
ParlayANN: &#21487;&#25193;&#23637;&#21644;&#30830;&#23450;&#24615;&#30340;&#24182;&#34892;&#22522;&#20110;&#22270;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ParlayANN: Scalable and Deterministic Parallel Graph-Based Approximate Nearest Neighbor Search Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.04359
&lt;/p&gt;
&lt;p&gt;
ParlayANN&#26159;&#19968;&#20010;&#20855;&#26377;&#30830;&#23450;&#24615;&#21644;&#24182;&#34892;&#24615;&#30340;&#22522;&#20110;&#22270;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#31639;&#27861;&#24211;&#65292;&#25552;&#20379;&#20102;&#29992;&#20110;&#24320;&#21457;&#36825;&#31867;&#31639;&#27861;&#30340;&#19968;&#22871;&#26377;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#65288;ANNS&#65289;&#31639;&#27861;&#26159;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#20851;&#38190;&#37096;&#20998;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#22312;&#39640;&#32500;&#21521;&#37327;&#31354;&#38388;&#34920;&#31034;&#65288;&#21363;&#23884;&#20837;&#65289;&#30340;&#25968;&#25454;&#19978;&#23454;&#29616;&#39640;&#25928;&#30340;&#30456;&#20284;&#24615;&#25628;&#32034;&#12290;&#22312;&#21508;&#31181;ANNS&#31639;&#27861;&#20013;&#65292;&#22522;&#20110;&#22270;&#30340;&#31639;&#27861;&#34987;&#35748;&#20026;&#22312;&#21534;&#21520;&#37327;&#21644;&#21484;&#22238;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#24179;&#34913;&#12290;&#23613;&#31649;&#29616;&#20195;ANNS&#25968;&#25454;&#38598;&#20855;&#26377;&#36739;&#22823;&#35268;&#27169;&#65292;&#20294;&#29616;&#26377;&#30340;&#24182;&#34892;&#22522;&#20110;&#22270;&#30340;&#23454;&#29616;&#30001;&#20110;&#22823;&#37327;&#20351;&#29992;&#38145;&#21644;&#20854;&#20182;&#39034;&#24207;&#29942;&#39048;&#32780;&#23384;&#22312;&#26174;&#33879;&#30340;&#25361;&#25112;&#65292;&#36825;&#20004;&#28857;&#38459;&#30861;&#20102;&#23427;&#20204;&#26377;&#25928;&#25193;&#23637;&#21040;&#22823;&#37327;&#22788;&#29702;&#22120;&#65292;&#24182;&#23548;&#33268;&#22312;&#26576;&#20123;&#24212;&#29992;&#20013;&#19981;&#24076;&#26395;&#20986;&#29616;&#30340;&#38750;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Approximate nearest-neighbor search (ANNS) algorithms are a key part of the modern deep learning stack due to enabling efficient similarity search over high-dimensional vector space representations (i.e., embeddings) of data. Among various ANNS algorithms, graph-based algorithms are known to achieve the best throughput-recall tradeoffs. Despite the large scale of modern ANNS datasets, existing parallel graph based implementations suffer from significant challenges to scale to large datasets due to heavy use of locks and other sequential bottlenecks, which 1) prevents them from efficiently scaling to a large number of processors, and 2) results in nondeterminism that is undesirable in certain applications.   In this paper, we introduce ParlayANN, a library of deterministic and parallel graph-based approximate nearest neighbor search algorithms, along with a set of useful tools for developing such algorithms. In this library, we develop novel parallel implementations for four state-of-th
&lt;/p&gt;</description></item><item><title>DSD$^2$&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#36991;&#20813;&#31232;&#30095;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#65307;&#24341;&#20837;&#20102;&#29109;&#24230;&#37327;&#65292;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#20256;&#32479;&#30340;&#20572;&#27490;&#20934;&#21017;&#65307;&#23545;&#30456;&#20851;&#22240;&#32032;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2303.01213</link><description>&lt;p&gt;
DSD$^2$: &#25105;&#20204;&#33021;&#22815;&#36991;&#20813;&#31232;&#30095;&#21452;&#19979;&#38477;&#24182;&#26080;&#24551;&#22320;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
DSD$^2$: Can We Dodge Sparse Double Descent and Compress the Neural Network Worry-Free?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.01213
&lt;/p&gt;
&lt;p&gt;
DSD$^2$&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#36991;&#20813;&#31232;&#30095;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#65307;&#24341;&#20837;&#20102;&#29109;&#24230;&#37327;&#65292;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#20256;&#32479;&#30340;&#20572;&#27490;&#20934;&#21017;&#65307;&#23545;&#30456;&#20851;&#22240;&#32032;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#26032;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#20250;&#20986;&#29616;&#31232;&#30095;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#30830;&#23454;&#65292;&#38543;&#30528;&#27169;&#22411;&#30340;&#31232;&#30095;&#24615;&#22686;&#21152;&#65292;&#27979;&#35797;&#24615;&#33021;&#39318;&#20808;&#21464;&#24046;&#65292;&#22240;&#20026;&#27169;&#22411;&#36807;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#65307;&#28982;&#21518;&#65292;&#36807;&#25311;&#21512;&#20943;&#23569;&#65292;&#23548;&#33268;&#24615;&#33021;&#25913;&#21892;&#65307;&#26368;&#21518;&#65292;&#27169;&#22411;&#24320;&#22987;&#36951;&#24536;&#20851;&#38190;&#20449;&#24687;&#65292;&#23548;&#33268;&#27424;&#25311;&#21512;&#12290;&#36825;&#31181;&#34892;&#20026;&#38459;&#27490;&#20102;&#20351;&#29992;&#20256;&#32479;&#30340;&#25552;&#21069;&#20572;&#27490;&#20934;&#21017;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26377;&#19977;&#20010;&#20851;&#38190;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26694;&#26550;&#65292;&#36991;&#20813;&#20102;&#36825;&#31181;&#29616;&#35937;&#24182;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29109;&#24230;&#37327;&#65292;&#25552;&#20379;&#20102;&#23545;&#36825;&#31181;&#29616;&#35937;&#20986;&#29616;&#30340;&#26356;&#22810;&#27934;&#23519;&#65292;&#24182;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#20256;&#32479;&#30340;&#20572;&#27490;&#20934;&#21017;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23545;&#37325;&#26032;&#21021;&#22987;&#21270;&#26041;&#27861;&#12289;&#27169;&#22411;&#23485;&#24230;&#21644;&#28145;&#24230;&#20197;&#21450;&#25968;&#25454;&#38598;&#22122;&#22768;&#31561;&#30456;&#20851;&#22240;&#32032;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;&#36825;&#20123;&#36129;&#29486;&#24471;&#21040;&#20102;&#20856;&#22411;&#35774;&#32622;&#20013;&#30340;&#32463;&#39564;&#35777;&#25454;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neoteric works have shown that modern deep learning models can exhibit a sparse double descent phenomenon. Indeed, as the sparsity of the model increases, the test performance first worsens since the model is overfitting the training data; then, the overfitting reduces, leading to an improvement in performance, and finally, the model begins to forget critical information, resulting in underfitting. Such a behavior prevents using traditional early stop criteria. In this work, we have three key contributions. First, we propose a learning framework that avoids such a phenomenon and improves generalization. Second, we introduce an entropy measure providing more insights into the insurgence of this phenomenon and enabling the use of traditional stop criteria. Third, we provide a comprehensive quantitative analysis of contingent factors such as re-initialization methods, model width and depth, and dataset noise. The contributions are supported by empirical evidence in typical setups. Our cod
&lt;/p&gt;</description></item><item><title>HardSATGEN&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#25511;&#21046;&#26426;&#21046;&#65292;&#20197;&#26356;&#22909;&#22320;&#24674;&#22797;&#24037;&#19994;&#22522;&#20934;&#30340;&#32467;&#26500;&#21644;&#35745;&#31639;&#29305;&#24615;&#65292;&#27492;&#26041;&#27861;&#22312;&#24037;&#19994;SAT&#20844;&#24335;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2302.02104</link><description>&lt;p&gt;
HardSATGEN&#65306;&#29702;&#35299;&#38590;SAT&#20844;&#24335;&#29983;&#25104;&#30340;&#22256;&#38590;&#21644;&#24378;&#30340;&#32467;&#26500;&#22256;&#38590;&#24863;&#30693;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
HardSATGEN: Understanding the Difficulty of Hard SAT Formula Generation and A Strong Structure-Hardness-Aware Baseline
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.02104
&lt;/p&gt;
&lt;p&gt;
HardSATGEN&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#25511;&#21046;&#26426;&#21046;&#65292;&#20197;&#26356;&#22909;&#22320;&#24674;&#22797;&#24037;&#19994;&#22522;&#20934;&#30340;&#32467;&#26500;&#21644;&#35745;&#31639;&#29305;&#24615;&#65292;&#27492;&#26041;&#27861;&#22312;&#24037;&#19994;SAT&#20844;&#24335;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;SAT&#20844;&#24335;&#29983;&#25104;&#26159;&#19968;&#39033;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;SAT&#29983;&#25104;&#26041;&#27861;&#24456;&#38590;&#21516;&#26102;&#25429;&#25417;&#20840;&#23616;&#32467;&#26500;&#29305;&#24615;&#24182;&#20445;&#25345;&#21512;&#29702;&#30340;&#35745;&#31639;&#22256;&#38590;&#24230;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#20043;&#21069;&#23398;&#20064;&#26041;&#27861;&#22312;&#37325;&#29616;&#21407;&#22987;&#23454;&#20363;&#30340;&#35745;&#31639;&#22256;&#38590;&#24230;&#26041;&#38754;&#30340;&#38480;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#36825;&#21487;&#33021;&#28304;&#33258;&#20854;&#37319;&#29992;&#30340;&#20998;&#35010;&#21512;&#24182;&#36807;&#31243;&#30340;&#20869;&#22312;&#21516;&#36136;&#24615;&#12290;&#22522;&#20110;&#24037;&#19994;&#20844;&#24335;&#21576;&#29616;&#26126;&#30830;&#31038;&#21306;&#32467;&#26500;&#21644;&#36807;&#24230;&#20998;&#21106;&#23376;&#32467;&#26500;&#23548;&#33268;&#36923;&#36753;&#32467;&#26500;&#35821;&#20041;&#24418;&#25104;&#22256;&#38590;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HardSATGEN&#65292;&#22312;SAT&#20844;&#24335;&#29983;&#25104;&#30340;&#31070;&#32463;&#20998;&#35010;&#21512;&#24182;&#33539;&#20363;&#20013;&#24341;&#20837;&#20102;&#31934;&#32454;&#30340;&#25511;&#21046;&#26426;&#21046;&#65292;&#20197;&#26356;&#22909;&#22320;&#24674;&#22797;&#24037;&#19994;&#22522;&#20934;&#30340;&#32467;&#26500;&#21644;&#35745;&#31639;&#29305;&#24615;&#12290;&#21253;&#25324;&#31169;&#20154;&#21644;&#23454;&#38469;&#20225;&#19994;&#27979;&#35797;&#22522;&#20934;&#35780;&#20272;&#22312;&#20869;&#30340;&#23454;&#39564;&#34920;&#26126;HardSATGEN&#30340;&#20248;&#36234;&#24615;&#65292;&#25104;&#20026;&#21807;&#19968;&#19968;&#31181;&#33021;&#22815;
&lt;/p&gt;
&lt;p&gt;
Industrial SAT formula generation is a critical yet challenging task. Existing SAT generation approaches can hardly simultaneously capture the global structural properties and maintain plausible computational hardness. We first present an in-depth analysis for the limitation of previous learning methods in reproducing the computational hardness of original instances, which may stem from the inherent homogeneity in their adopted split-merge procedure. On top of the observations that industrial formulae exhibit clear community structure and oversplit substructures lead to the difficulty in semantic formation of logical structures, we propose HardSATGEN, which introduces a fine-grained control mechanism to the neural split-merge paradigm for SAT formula generation to better recover the structural and computational properties of the industrial benchmarks. Experiments including evaluations on private and practical corporate testbed show the superiority of HardSATGEN being the only method to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#31639;&#27861;&#65292;&#21033;&#29992;&#22810;&#35270;&#22270;&#20960;&#20309;&#32422;&#26463;&#65292;&#20174;&#22810;&#35270;&#22270;&#22270;&#20687;&#20013;&#23398;&#20064;&#21457;&#29616;&#20154;&#20307;&#30340;3D&#20851;&#38190;&#28857;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#25237;&#24433;&#21040;&#27599;&#20010;&#35270;&#22270;&#26469;&#20272;&#35745;3D&#20851;&#38190;&#28857;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#19982;&#20854;&#20182;&#26080;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21457;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#20855;&#26377;&#35299;&#37322;&#24615;&#30340;3D&#20851;&#38190;&#28857;&#12290;</title><link>https://arxiv.org/abs/2211.12829</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#22810;&#35270;&#35282;&#20960;&#20309;&#19979;&#30340;3D&#20851;&#38190;&#28857;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Unsupervised 3D Keypoint Discovery with Multi-View Geometry
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.12829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#31639;&#27861;&#65292;&#21033;&#29992;&#22810;&#35270;&#22270;&#20960;&#20309;&#32422;&#26463;&#65292;&#20174;&#22810;&#35270;&#22270;&#22270;&#20687;&#20013;&#23398;&#20064;&#21457;&#29616;&#20154;&#20307;&#30340;3D&#20851;&#38190;&#28857;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#25237;&#24433;&#21040;&#27599;&#20010;&#35270;&#22270;&#26469;&#20272;&#35745;3D&#20851;&#38190;&#28857;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#19982;&#20854;&#20182;&#26080;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21457;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#20855;&#26377;&#35299;&#37322;&#24615;&#30340;3D&#20851;&#38190;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#21644;&#35757;&#32451;3D&#36523;&#20307;&#23039;&#21183;&#27169;&#22411;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20851;&#33410;&#26631;&#31614;&#30340;&#21487;&#29992;&#24615;&#65292;&#36825;&#20123;&#26631;&#31614;&#36890;&#24120;&#36890;&#36807;&#32321;&#29712;&#30340;&#25163;&#21160;&#26631;&#27880;&#25110;&#32773;&#36890;&#36807;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#26631;&#35760;&#21644;&#25429;&#25417;&#31995;&#32479;&#30340;&#22522;&#20110;&#26631;&#35760;&#30340;&#20851;&#33410;&#23450;&#20301;&#26469;&#33719;&#24471;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#26631;&#27880;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#30340;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36827;&#34892;&#19981;&#23547;&#24120;&#27963;&#21160;&#30340;&#20154;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#23398;&#20064;&#20174;&#22810;&#35270;&#22270;&#22270;&#20687;&#20013;&#21457;&#29616;&#20154;&#20307;&#30340;3D&#20851;&#38190;&#28857;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#30417;&#30563;&#25110;&#26631;&#31614;&#65292;&#20165;&#20381;&#38752;&#22810;&#35270;&#22270;&#20960;&#20309;&#25552;&#20379;&#30340;&#32422;&#26463;&#12290;&#20026;&#20102;&#30830;&#20445;&#21457;&#29616;&#30340;3D&#20851;&#38190;&#28857;&#26159;&#26377;&#24847;&#20041;&#30340;&#65292;&#23427;&#20204;&#34987;&#37325;&#26032;&#25237;&#24433;&#21040;&#27599;&#20010;&#35270;&#22270;&#20197;&#20272;&#35745;&#27169;&#22411;&#22312;&#27809;&#26377;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#26368;&#21021;&#20272;&#35745;&#30340;&#20154;&#30340;&#25513;&#27169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Human3.6M&#21644;MPI-INF-3DHP&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#21457;&#29616;&#20102;&#26356;&#26377;&#35299;&#37322;&#24615;&#21644;&#26356;&#20934;&#30830;&#30340;3D&#20851;&#38190;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing and training 3D body posture models depend heavily on the availability of joint labels that are commonly acquired through laborious manual annotation of body joints or via marker-based joint localization using carefully curated markers and capturing systems. However, such annotations are not always available, especially for people performing unusual activities. In this paper, we propose an algorithm that learns to discover 3D keypoints on human bodies from multiple-view images without any supervision or labels other than the constraints multiple-view geometry provides. To ensure that the discovered 3D keypoints are meaningful, they are re-projected to each view to estimate the person's mask that the model itself has initially estimated without supervision. Our approach discovers more interpretable and accurate 3D keypoints compared to other state-of-the-art unsupervised approaches on Human3.6M and MPI-INF-3DHP benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27425;&#32447;&#24615;&#26102;&#38388;&#20869;&#35757;&#32451;&#36807;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2208.04508</link><description>&lt;p&gt;
&#22312;&#27425;&#32447;&#24615;&#26102;&#38388;&#20869;&#35757;&#32451;&#36807;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Training Overparametrized Neural Networks in Sublinear Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.04508
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27425;&#32447;&#24615;&#26102;&#38388;&#20869;&#35757;&#32451;&#36807;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#25104;&#21151;&#20184;&#20986;&#20102;&#24040;&#22823;&#30340;&#35745;&#31639;&#21644;&#33021;&#28304;&#25104;&#26412;&#65292;&#35757;&#32451;&#36807;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#25193;&#23637;&#24615;&#27491;&#22312;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#36827;&#23637;&#30340;&#30495;&#27491;&#38556;&#30861;&#12290;&#23613;&#31649;&#20256;&#32479;&#30340;&#21453;&#21521;&#20256;&#25773;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#30340;&#25104;&#26412;&#27599;&#27425;&#36845;&#20195;&#24456;&#20302;&#65292;&#20294;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#38750;&#20984;&#35774;&#32622;&#20013;&#20855;&#26377;&#31105;&#27490;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#26080;&#35770;&#26159;&#22312;&#29702;&#35770;&#19978;&#36824;&#26159;&#23454;&#36341;&#20013;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#25104;&#26412;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#37319;&#29992;&#20855;&#26377;&#26356;&#24555;&#25910;&#25947;&#36895;&#24230;&#20294;&#27599;&#27425;&#36845;&#20195;&#25104;&#26412;&#26356;&#39640;&#30340;&#26367;&#20195;&#65288;&#29275;&#39039;&#31867;&#22411;&#65289;&#35757;&#32451;&#26041;&#27861;&#12290;&#23545;&#20110;&#20855;&#26377;$m=\mathrm{poly}(n)$&#20010;&#21442;&#25968;&#21644;&#36755;&#20837;&#25209;&#27425;$n$&#20010;&#25968;&#25454;&#28857;&#22312;$\mathbb{R}^d$&#20013;&#30340;&#20856;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;[Brand, Peng, Song, and Weinstein, ITCS'2021]&#30340;&#20808;&#21069;&#24037;&#20316;&#27599;&#27425;&#36845;&#20195;&#38656;&#35201;$\sim mnd+n^3$&#30340;&#26102;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;$m^{1-\alpha}nd+n^3$&#30340;&#25674;&#38144;&#26102;&#38388;&#65292;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of deep learning comes at a tremendous computational and energy cost, and the scalability of training massively overparametrized neural networks is becoming a real barrier to the progress of artificial intelligence (AI). Despite the popularity and low cost-per-iteration of traditional backpropagation via gradient decent, stochastic gradient descent (SGD) has prohibitive convergence rate in non-convex settings, both in theory and practice.   To mitigate this cost, recent works have proposed to employ alternative (Newton-type) training methods with much faster convergence rate, albeit with higher cost-per-iteration. For a typical neural network with $m=\mathrm{poly}(n)$ parameters and input batch of $n$ datapoints in $\mathbb{R}^d$, the previous work of [Brand, Peng, Song, and Weinstein, ITCS'2021] requires $\sim mnd + n^3$ time per iteration. In this paper, we present a novel training method that requires only $m^{1-\alpha} n d + n^3$ amortized time in the same overparametri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20010;&#24615;&#21270;PCA&#65288;PerPCA&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#20027;&#25104;&#20998;&#26469;&#32534;&#30721;&#29420;&#29305;&#21644;&#20849;&#20139;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;PCA&#38754;&#20020;&#30340;&#24322;&#36136;&#24615;&#25361;&#25112;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#21463;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#35782;&#21035;&#21644;&#24674;&#22797;&#20986;&#29420;&#29305;&#21644;&#20849;&#20139;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#32852;&#37030;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;&#12290;</title><link>https://arxiv.org/abs/2207.08041</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;PCA:&#35299;&#32806;&#20849;&#20139;&#21644;&#29420;&#29305;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Personalized PCA: Decoupling Shared and Unique Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.08041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20010;&#24615;&#21270;PCA&#65288;PerPCA&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#20027;&#25104;&#20998;&#26469;&#32534;&#30721;&#29420;&#29305;&#21644;&#20849;&#20139;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;PCA&#38754;&#20020;&#30340;&#24322;&#36136;&#24615;&#25361;&#25112;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#21463;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#35782;&#21035;&#21644;&#24674;&#22797;&#20986;&#29420;&#29305;&#21644;&#20849;&#20139;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#32852;&#37030;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;PCA&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65306;&#24322;&#36136;&#24615;&#12290;&#24403;&#25968;&#25454;&#26469;&#33258;&#19981;&#21516;&#30340;&#26469;&#28304;&#65292;&#20855;&#26377;&#24322;&#36136;&#30340;&#36235;&#21183;&#65292;&#21516;&#26102;&#20173;&#28982;&#20849;&#20139;&#26576;&#20123;&#19968;&#33268;&#24615;&#26102;&#65292;&#20851;&#38190;&#26159;&#25552;&#21462;&#20849;&#20139;&#30340;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#30041;&#27599;&#20010;&#26469;&#28304;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20010;&#24615;&#21270;PCA&#65288;PerPCA&#65289;&#65292;&#23427;&#20351;&#29992;&#20114;&#30456;&#27491;&#20132;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#20027;&#25104;&#20998;&#26469;&#32534;&#30721;&#29420;&#29305;&#21644;&#20849;&#20139;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#21363;&#20351;&#21327;&#26041;&#24046;&#30697;&#38453;&#26497;&#20854;&#19981;&#21516;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#21463;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#35782;&#21035;&#21644;&#24674;&#22797;&#20986;&#29420;&#29305;&#21644;&#20849;&#20139;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23436;&#20840;&#30340;&#32852;&#37030;&#31639;&#27861;&#65292;&#28789;&#24863;&#26469;&#33258;&#20998;&#24067;&#24335;Stiefel&#26799;&#24230;&#19979;&#38477;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#24191;&#20041;&#22238;&#32553;&#30340;&#26032;&#25805;&#20316;&#32452;&#26469;&#22788;&#29702;&#27491;&#20132;&#32422;&#26463;&#65292;&#21482;&#38656;&#35201;&#22312;&#26469;&#28304;&#20043;&#38388;&#20849;&#20139;&#20840;&#23616;&#20027;&#25104;&#20998;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#22312;&#36866;&#24403;&#30340;&#20551;&#35774;&#19979;&#20855;&#26377;&#32447;&#24615;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we tackle a significant challenge in PCA: heterogeneity. When data are collected from different sources with heterogeneous trends while still sharing some congruency, it is critical to extract shared knowledge while retaining the unique features of each source. To this end, we propose personalized PCA (PerPCA), which uses mutually orthogonal global and local principal components to encode both unique and shared features. We show that, under mild conditions, both unique and shared features can be identified and recovered by a constrained optimization problem, even if the covariance matrices are immensely different. Also, we design a fully federated algorithm inspired by distributed Stiefel gradient descent to solve the problem. The algorithm introduces a new group of operations called generalized retractions to handle orthogonality constraints, and only requires global PCs to be shared across sources. We prove the linear convergence of the algorithm under suitable assumpt
&lt;/p&gt;</description></item><item><title>Matryoshka&#34920;&#31034;&#23398;&#20064;&#65288;MRL&#65289;&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#35745;&#31639;&#36164;&#28304;&#19979;&#36866;&#24212;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#20016;&#23500;&#24615;&#12290;</title><link>https://arxiv.org/abs/2205.13147</link><description>&lt;p&gt;
Matryoshka&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Matryoshka Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.13147
&lt;/p&gt;
&lt;p&gt;
Matryoshka&#34920;&#31034;&#23398;&#20064;&#65288;MRL&#65289;&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#35745;&#31639;&#36164;&#28304;&#19979;&#36866;&#24212;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#20016;&#23500;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#24471;&#21040;&#30340;&#34920;&#31034;&#26159;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;&#65292;&#29992;&#20110;&#26381;&#21153;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#22312;&#35757;&#32451;&#36825;&#31181;&#34920;&#31034;&#26102;&#65292;&#24448;&#24448;&#26080;&#27861;&#30830;&#23450;&#27599;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#32422;&#26463;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21018;&#24615;&#30340;&#22266;&#23450;&#23481;&#37327;&#34920;&#31034;&#21487;&#33021;&#36807;&#24230;&#25110;&#19981;&#36275;&#22320;&#36866;&#24212;&#24403;&#21069;&#20219;&#21153;&#12290;&#36825;&#20351;&#25105;&#20204;&#20135;&#29983;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#25105;&#20204;&#33021;&#21542;&#35774;&#35745;&#19968;&#31181;&#28789;&#27963;&#30340;&#34920;&#31034;&#65292;&#20197;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#35745;&#31639;&#36164;&#28304;&#30340;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#65311;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;Matryoshka&#34920;&#31034;&#23398;&#20064;&#65288;MRL&#65289;&#65292;&#23427;&#22312;&#19981;&#21516;&#30340;&#31890;&#24230;&#19978;&#32534;&#30721;&#20449;&#24687;&#65292;&#24182;&#20801;&#35768;&#21333;&#20010;&#23884;&#20837;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#30340;&#35745;&#31639;&#32422;&#26463;&#12290;MRL&#23545;&#29616;&#26377;&#30340;&#34920;&#31034;&#23398;&#20064;&#27969;&#31243;&#36827;&#34892;&#26368;&#23567;&#20462;&#25913;&#65292;&#24182;&#22312;&#25512;&#29702;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#19981;&#22686;&#21152;&#20219;&#20309;&#39069;&#22806;&#25104;&#26412;&#12290;MRL&#23398;&#20064;&#30340;&#31895;&#31890;&#24230;&#21040;&#32454;&#31890;&#24230;&#30340;&#34920;&#31034;&#33267;&#23569;&#19982;&#29420;&#31435;&#35757;&#32451;&#30340;&#20302;&#32500;&#34920;&#31034;&#19968;&#26679;&#20934;&#30830;&#21644;&#20016;&#23500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learned representations are a central component in modern ML systems, serving a multitude of downstream tasks. When training such representations, it is often the case that computational and statistical constraints for each downstream task are unknown. In this context rigid, fixed capacity representations can be either over or under-accommodating to the task at hand. This leads us to ask: can we design a flexible representation that can adapt to multiple downstream tasks with varying computational resources? Our main contribution is Matryoshka Representation Learning (MRL) which encodes information at different granularities and allows a single embedding to adapt to the computational constraints of downstream tasks. MRL minimally modifies existing representation learning pipelines and imposes no additional cost during inference and deployment. MRL learns coarse-to-fine representations that are at least as accurate and rich as independently trained low-dimensional representations. The f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20449;&#29992;&#35780;&#20998;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24418;&#24335;&#21270;&#27979;&#35797;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#24433;&#21709;&#20844;&#24179;&#24615;&#30340;&#21464;&#37327;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20197;&#25351;&#23548;&#20449;&#36151;&#21830;&#30417;&#27979;&#31639;&#27861;&#20844;&#24179;&#24615;&#12289;&#30417;&#31649;&#26426;&#26500;&#25511;&#21046;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#25552;&#39640;&#21463;&#20445;&#25252;&#32676;&#20307;&#30340;&#21033;&#30410;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#27700;&#24179;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2205.10200</link><description>&lt;p&gt;
&#20449;&#29992;&#35780;&#20998;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Fairness of Credit Scoring Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.10200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20449;&#29992;&#35780;&#20998;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24418;&#24335;&#21270;&#27979;&#35797;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#24433;&#21709;&#20844;&#24179;&#24615;&#30340;&#21464;&#37327;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20197;&#25351;&#23548;&#20449;&#36151;&#21830;&#30417;&#27979;&#31639;&#27861;&#20844;&#24179;&#24615;&#12289;&#30417;&#31649;&#26426;&#26500;&#25511;&#21046;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#25552;&#39640;&#21463;&#20445;&#25252;&#32676;&#20307;&#30340;&#21033;&#30410;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#27700;&#24179;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#29992;&#24066;&#22330;&#20013;&#65292;&#31579;&#36873;&#31639;&#27861;&#30340;&#30446;&#26631;&#26159;&#21306;&#20998;&#22909;&#31867;&#22411;&#21644;&#22351;&#31867;&#22411;&#30340;&#20511;&#27454;&#20154;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#26679;&#20570;&#30340;&#36807;&#31243;&#20013;&#65292;&#20182;&#20204;&#36824;&#21487;&#33021;&#22312;&#20855;&#26377;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#20010;&#20307;&#65288;&#20363;&#22914;&#24615;&#21035;&#12289;&#24180;&#40836;&#12289;&#31181;&#26063;&#36215;&#28304;&#65289;&#21644;&#25972;&#20010;&#20154;&#32676;&#20043;&#38388;&#36827;&#34892;&#27495;&#35270;&#12290;&#36825;&#21487;&#33021;&#26159;&#26080;&#24847;&#35782;&#30340;&#65292;&#26469;&#28304;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#25110;&#27169;&#22411;&#26412;&#36523;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#24418;&#24335;&#21270;&#27979;&#35797;&#35780;&#20998;&#27169;&#22411;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#65292;&#20197;&#21450;&#22914;&#20309;&#30830;&#23450;&#24433;&#21709;&#20844;&#24179;&#24615;&#19981;&#36275;&#30340;&#21464;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#21464;&#37327;&#26469;&#20248;&#21270;&#20844;&#24179;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#30417;&#27979;&#20449;&#36151;&#21830;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#12289;&#22914;&#20309;&#30001;&#30417;&#31649;&#26426;&#26500;&#25511;&#21046;&#12289;&#22914;&#20309;&#25913;&#21892;&#21463;&#20445;&#25252;&#32676;&#20307;&#30340;&#21033;&#30410;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#39640;&#27700;&#24179;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
In credit markets, screening algorithms aim to discriminate between good-type and bad-type borrowers. However, when doing so, they can also discriminate between individuals sharing a protected attribute (e.g. gender, age, racial origin) and the rest of the population. This can be unintentional and originate from the training dataset or from the model itself. We show how to formally test the algorithmic fairness of scoring models and how to identify the variables responsible for any lack of fairness. We then use these variables to optimize the fairness-performance trade-off. Our framework provides guidance on how algorithmic fairness can be monitored by lenders, controlled by their regulators, improved for the benefit of protected groups, while still maintaining a high level of forecasting accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#33258;&#25105;&#35299;&#37322;&#20174;&#31034;&#33539;&#20013;&#23398;&#20064;&#30340;&#26694;&#26550;&#65288;SERLfD&#65289;&#12290;&#36890;&#36807;&#20195;&#29702;&#20154;&#29983;&#25104;&#33258;&#25105;&#35299;&#37322;&#24182;&#35782;&#21035;&#26377;&#20215;&#20540;&#30340;&#39640;&#32423;&#20851;&#31995;&#29305;&#24449;&#20316;&#20026;&#25104;&#21151;&#36712;&#36857;&#30340;&#35299;&#37322;&#21407;&#22240;&#65292;&#21487;&#20197;&#22312;&#20811;&#26381;&#31034;&#33539;&#20013;&#23384;&#22312;&#30340;&#27169;&#26865;&#20004;&#21487;&#24773;&#20917;&#30340;&#21516;&#26102;&#20026;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2110.05286</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#25105;&#35299;&#37322;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#20174;&#27169;&#26865;&#20004;&#21487;&#30340;&#31034;&#33539;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Ambiguous Demonstrations with Self-Explanation Guided Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.05286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#33258;&#25105;&#35299;&#37322;&#20174;&#31034;&#33539;&#20013;&#23398;&#20064;&#30340;&#26694;&#26550;&#65288;SERLfD&#65289;&#12290;&#36890;&#36807;&#20195;&#29702;&#20154;&#29983;&#25104;&#33258;&#25105;&#35299;&#37322;&#24182;&#35782;&#21035;&#26377;&#20215;&#20540;&#30340;&#39640;&#32423;&#20851;&#31995;&#29305;&#24449;&#20316;&#20026;&#25104;&#21151;&#36712;&#36857;&#30340;&#35299;&#37322;&#21407;&#22240;&#65292;&#21487;&#20197;&#22312;&#20811;&#26381;&#31034;&#33539;&#20013;&#23384;&#22312;&#30340;&#27169;&#26865;&#20004;&#21487;&#24773;&#20917;&#30340;&#21516;&#26102;&#20026;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#26377;&#25928;&#22320;&#21033;&#29992;&#27169;&#26865;&#20004;&#21487;&#30340;&#31034;&#33539;&#26469;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#19968;&#20010;&#27169;&#26865;&#20004;&#21487;&#30340;&#31034;&#33539;&#36890;&#24120;&#21487;&#20197;&#26377;&#22810;&#37325;&#35299;&#37322;&#65292;&#36825;&#20005;&#37325;&#38459;&#30861;&#20102;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#31283;&#23450;&#21644;&#39640;&#25928;&#23398;&#20064;&#12290;&#30001;&#20110;&#26368;&#20339;&#31034;&#33539;&#20063;&#21487;&#33021;&#23384;&#22312;&#27169;&#26865;&#20004;&#21487;&#30340;&#38382;&#39064;&#65292;&#20043;&#21069;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#20174;&#31034;&#33539;&#20013;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#24037;&#20316;&#21487;&#33021;&#26080;&#27861;&#24456;&#22909;&#22320;&#24037;&#20316;&#12290;&#21463;&#21040;&#20154;&#31867;&#22788;&#29702;&#36825;&#31181;&#24773;&#20917;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#25105;&#35299;&#37322;&#65288;&#20195;&#29702;&#20154;&#20026;&#33258;&#36523;&#29983;&#25104;&#35299;&#37322;&#65289;&#26469;&#35782;&#21035;&#26377;&#20215;&#20540;&#30340;&#39640;&#32423;&#20851;&#31995;&#29305;&#24449;&#65292;&#20316;&#20026;&#25104;&#21151;&#36712;&#36857;&#25104;&#21151;&#30340;&#35299;&#37322;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#20195;&#29702;&#21487;&#20197;&#20026;&#20854;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#19968;&#20123;&#25351;&#23548;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#33258;&#25105;&#35299;&#37322;&#20174;&#31034;&#33539;&#20013;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;SERLfD&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#20811;&#26381;&#20256;&#32479;RLfD&#24037;&#20316;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;SERLfD&#26694;&#26550;&#21487;&#20197;&#25913;&#36827;RLfD&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our work aims at efficiently leveraging ambiguous demonstrations for the training of a reinforcement learning (RL) agent. An ambiguous demonstration can usually be interpreted in multiple ways, which severely hinders the RL-Agent from learning stably and efficiently. Since an optimal demonstration may also suffer from being ambiguous, previous works that combine RL and learning from demonstration (RLfD works) may not work well. Inspired by how humans handle such situations, we propose to use self-explanation (an agent generates explanations for itself) to recognize valuable high-level relational features as an interpretation of why a successful trajectory is successful. This way, the agent can provide some guidance for its RL learning. Our main contribution is to propose the Self-Explanation for RL from Demonstrations (SERLfD) framework, which can overcome the limitations of traditional RLfD works. Our experimental results show that an RLfD model can be improved by using our SERLfD fra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LiNGAM-MMI&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#24378;LiNGAM&#27169;&#22411;&#20197;&#22788;&#29702;&#28151;&#28102;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;KL&#25955;&#24230;&#37327;&#21270;&#28151;&#28102;&#31243;&#24230;&#65292;&#24182;&#36890;&#36807;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#39640;&#25928;&#22320;&#30830;&#23450;&#21464;&#37327;&#39034;&#24207;&#65292;&#19981;&#35770;&#26159;&#21542;&#23384;&#22312;&#28151;&#28102;&#24773;&#20917;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LiNGAM-MMI&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#27491;&#30830;&#30340;&#21464;&#37327;&#39034;&#24207;&#12290;</title><link>http://arxiv.org/abs/2401.16661</link><description>&lt;p&gt;
&#20801;&#35768;&#28151;&#28102;&#30340;LiNGAM&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalization of LiNGAM that allows confounding. (arXiv:2401.16661v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LiNGAM-MMI&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#24378;LiNGAM&#27169;&#22411;&#20197;&#22788;&#29702;&#28151;&#28102;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;KL&#25955;&#24230;&#37327;&#21270;&#28151;&#28102;&#31243;&#24230;&#65292;&#24182;&#36890;&#36807;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#39640;&#25928;&#22320;&#30830;&#23450;&#21464;&#37327;&#39034;&#24207;&#65292;&#19981;&#35770;&#26159;&#21542;&#23384;&#22312;&#28151;&#28102;&#24773;&#20917;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LiNGAM-MMI&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#27491;&#30830;&#30340;&#21464;&#37327;&#39034;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LiNGAM&#20351;&#29992;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#26469;&#30830;&#23450;&#22240;&#26524;&#20851;&#31995;&#30340;&#21464;&#37327;&#39034;&#24207;&#65292;&#20294;&#22312;&#28151;&#28102;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;LiNGAM&#30340;&#22522;&#26412;&#32467;&#26500;&#30340;&#21516;&#26102;&#65292;&#35797;&#22270;&#35782;&#21035;&#21644;&#22788;&#29702;&#21463;&#28151;&#28102;&#24433;&#21709;&#30340;&#21464;&#37327;&#12290;&#32467;&#26524;&#26159;&#65292;&#19981;&#35770;&#26159;&#21542;&#23384;&#22312;&#28151;&#28102;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#24182;&#19988;&#19981;&#33021;&#30830;&#20445;&#26816;&#27979;&#21040;&#25152;&#26377;&#30340;&#28151;&#28102;&#31867;&#22411;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;LiNGAM-MMI&#23545;LiNGAM&#36827;&#34892;&#20102;&#22686;&#24378;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;KL&#25955;&#24230;&#37327;&#21270;&#28151;&#28102;&#31243;&#24230;&#65292;&#24182;&#23433;&#25490;&#21464;&#37327;&#20197;&#26368;&#23567;&#21270;&#20854;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#30340;&#24418;&#24335;&#39640;&#25928;&#22320;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#30340;&#21464;&#37327;&#39034;&#24207;&#12290;&#22312;&#26080;&#28151;&#28102;&#30340;&#24773;&#20917;&#19979;&#65292;LiNGAM-MMI&#30340;&#22788;&#29702;&#25968;&#25454;&#25928;&#29575;&#19982;&#20256;&#32479;LiNGAM&#30456;&#24403;&#65292;&#21516;&#26102;&#26377;&#25928;&#22788;&#29702;&#28151;&#28102;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LiNGAM-MMI&#26356;&#20934;&#30830;&#22320;&#30830;&#23450;&#20102;&#27491;&#30830;&#30340;&#21464;&#37327;&#39034;&#24207;...
&lt;/p&gt;
&lt;p&gt;
LiNGAM determines the variable order from cause to effect using additive noise models, but it faces challenges with confounding. Previous methods maintained LiNGAM's fundamental structure while trying to identify and address variables affected by confounding. As a result, these methods required significant computational resources regardless of the presence of confounding, and they did not ensure the detection of all confounding types. In contrast, this paper enhances LiNGAM by introducing LiNGAM-MMI, a method that quantifies the magnitude of confounding using KL divergence and arranges the variables to minimize its impact. This method efficiently achieves a globally optimal variable order through the shortest path problem formulation. LiNGAM-MMI processes data as efficiently as traditional LiNGAM in scenarios without confounding while effectively addressing confounding situations. Our experimental results suggest that LiNGAM-MMI more accurately determines the correct variable order, bo
&lt;/p&gt;</description></item><item><title>Langevin&#36951;&#24536;&#26159;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#26799;&#24230;&#19979;&#38477;&#30340;&#36951;&#24536;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#36817;&#20284;&#36951;&#24536;&#38382;&#39064;&#20013;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#65292;&#24182;&#19988;&#20855;&#26377;&#31639;&#27861;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.10371</link><description>&lt;p&gt;
Langevin&#36951;&#24536;&#65306;&#22122;&#22768;&#26799;&#24230;&#19979;&#38477;&#30340;&#26426;&#22120;&#36951;&#24536;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Langevin Unlearning: A New Perspective of Noisy Gradient Descent for Machine Unlearning. (arXiv:2401.10371v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10371
&lt;/p&gt;
&lt;p&gt;
Langevin&#36951;&#24536;&#26159;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#26799;&#24230;&#19979;&#38477;&#30340;&#36951;&#24536;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#36817;&#20284;&#36951;&#24536;&#38382;&#39064;&#20013;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#65292;&#24182;&#19988;&#20855;&#26377;&#31639;&#27861;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#37319;&#29992;&#30830;&#20445;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27861;&#24459;&#65292;&#26426;&#22120;&#36951;&#24536;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#29575;&#24615;&#30340;&#36817;&#20284;&#36951;&#24536;&#23450;&#20041;&#65292;&#31867;&#20284;&#20110;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#30340;&#23450;&#20041;&#65292;&#20854;&#20013;&#38544;&#31169;&#34987;&#23450;&#20041;&#20026;&#23545;&#37325;&#26032;&#35757;&#32451;&#30340;&#32479;&#35745;&#19981;&#21487;&#21306;&#20998;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Langevin&#36951;&#24536;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22122;&#22768;&#26799;&#24230;&#19979;&#38477;&#30340;&#36817;&#20284;&#36951;&#24536;&#38382;&#39064;&#30340;&#38544;&#31169;&#20445;&#35777;&#30340;&#36951;&#24536;&#26694;&#26550;&#12290;Langevin&#36951;&#24536;&#22312;&#31639;&#27861;&#19978;&#32479;&#19968;&#20102;DP&#23398;&#20064;&#36807;&#31243;&#21644;&#38544;&#31169;&#35748;&#35777;&#30340;&#36951;&#24536;&#36807;&#31243;&#12290;&#20854;&#20013;&#21253;&#25324;&#38750;&#20984;&#38382;&#39064;&#30340;&#36817;&#20284;&#35748;&#35777;&#36951;&#24536;&#65292;&#30456;&#23545;&#20110;&#37325;&#26032;&#35757;&#32451;&#30340;&#22797;&#26434;&#24230;&#33410;&#30465;&#65292;&#20197;&#21450;&#29992;&#20110;&#22810;&#20010;&#36951;&#24536;&#35831;&#27714;&#30340;&#39034;&#24207;&#21644;&#25209;&#37327;&#36951;&#24536;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;Langevin&#36951;&#24536;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#23545;&#26799;&#24230;&#19979;&#38477;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning has raised significant interest with the adoption of laws ensuring the ``right to be forgotten''. Researchers have provided a probabilistic notion of approximate unlearning under a similar definition of Differential Privacy (DP), where privacy is defined as statistical indistinguishability to retraining from scratch. We propose Langevin unlearning, an unlearning framework based on noisy gradient descent with privacy guarantees for approximate unlearning problems. Langevin unlearning unifies the DP learning process and the privacy-certified unlearning process with many algorithmic benefits. These include approximate certified unlearning for non-convex problems, complexity saving compared to retraining, sequential and batch unlearning for multiple unlearning requests. We verify the practicality of Langevin unlearning by studying its privacy-utility-complexity trade-off via experiments on benchmark datasets, and also demonstrate its superiority against gradient-decent-p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#35889;&#28388;&#27874;&#22312;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#65292;&#24182;&#26126;&#30830;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#26032;&#22270;&#12290;&#36866;&#24212;&#24615;&#26032;&#22270;&#23637;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#24182;&#33021;&#22815;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09071</link><description>&lt;p&gt;
&#29992;&#31354;&#38388;&#33258;&#36866;&#24212;&#28388;&#27874;&#37325;&#26032;&#24605;&#32771;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Rethinking Spectral Graph Neural Networks with Spatially Adaptive Filtering. (arXiv:2401.09071v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#35889;&#28388;&#27874;&#22312;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#65292;&#24182;&#26126;&#30830;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#26032;&#22270;&#12290;&#36866;&#24212;&#24615;&#26032;&#22270;&#23637;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#24182;&#33021;&#22815;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#29702;&#35770;&#19978;&#22312;&#35889;&#22495;&#20013;&#26377;&#24456;&#22909;&#30340;&#22522;&#30784;&#65292;&#20294;&#23427;&#20204;&#23454;&#38469;&#19978;&#20381;&#36182;&#20110;&#22810;&#39033;&#24335;&#36924;&#36817;&#65292;&#24847;&#21619;&#30528;&#23427;&#20204;&#19982;&#31354;&#38388;&#22495;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#12290;&#30001;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#24456;&#23569;&#20174;&#31354;&#38388;&#35282;&#24230;&#30740;&#31350;&#35889;&#22270;GNN&#65292;&#22240;&#27492;&#23427;&#20204;&#22312;&#31354;&#38388;&#22495;&#30340;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#65292;&#20363;&#22914;&#65292;&#35889;&#22270;GNN&#22312;&#31354;&#38388;&#22495;&#20013;&#23454;&#38469;&#19978;&#32534;&#30721;&#20102;&#21738;&#20123;&#20449;&#24687;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#22312;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#30340;&#32852;&#31995;&#65292;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#30340;&#20869;&#22312;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#26126;&#30830;&#22320;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#36866;&#24212;&#24615;&#26032;&#22270;&#12290;&#29702;&#35770;&#21644;&#32463;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#24212;&#24615;&#26032;&#22270;&#19981;&#20165;&#34920;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#36824;&#33021;&#22815;&#23481;&#32435;&#26377;&#31526;&#21495;&#30340;&#36793;&#26435;&#37325;&#20197;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#35889;&#22270;GNN&#22312;&#31354;&#38388;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whilst spectral Graph Neural Networks (GNNs) are theoretically well-founded in the spectral domain, their practical reliance on polynomial approximation implies a profound linkage to the spatial domain. As previous studies rarely examine spectral GNNs from the spatial perspective, their spatial-domain interpretability remains elusive, e.g., what information is essentially encoded by spectral GNNs in the spatial domain? In this paper, to answer this question, we establish a theoretical connection between spectral filtering and spatial aggregation, unveiling an intrinsic interaction that spectral filtering implicitly leads the original graph to an adapted new graph, explicitly computed for spatial aggregation. Both theoretical and empirical investigations reveal that the adapted new graph not only exhibits non-locality but also accommodates signed edge weights to reflect label consistency between nodes. These findings thus highlight the interpretable role of spectral GNNs in the spatial 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#24863;&#30693;-&#34892;&#21160;-&#36890;&#20449;(LPAC)&#26550;&#26500;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#29615;&#22659;&#24863;&#30693;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#27969;&#65292;&#27973;&#23618;&#22810;&#23618;&#24863;&#30693;&#26426;&#35745;&#31639;&#26426;&#22120;&#20154;&#30340;&#21160;&#20316;&#12290;&#20351;&#29992;&#38598;&#20013;&#24335;&#26174;&#24494;&#31639;&#27861;&#35757;&#32451;&#27169;&#22411;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#32676;&#20307;&#30340;&#21327;&#20316;&#12290;</title><link>http://arxiv.org/abs/2401.04855</link><description>&lt;p&gt;
LPAC: &#21487;&#23398;&#20064;&#30340;&#24863;&#30693;-&#34892;&#21160;-&#36890;&#20449;&#24490;&#29615;&#21450;&#20854;&#22312;&#35206;&#30422;&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
LPAC: Learnable Perception-Action-Communication Loops with Applications to Coverage Control. (arXiv:2401.04855v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04855
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#24863;&#30693;-&#34892;&#21160;-&#36890;&#20449;(LPAC)&#26550;&#26500;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#29615;&#22659;&#24863;&#30693;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#27969;&#65292;&#27973;&#23618;&#22810;&#23618;&#24863;&#30693;&#26426;&#35745;&#31639;&#26426;&#22120;&#20154;&#30340;&#21160;&#20316;&#12290;&#20351;&#29992;&#38598;&#20013;&#24335;&#26174;&#24494;&#31639;&#27861;&#35757;&#32451;&#27169;&#22411;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#32676;&#20307;&#30340;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35206;&#30422;&#25511;&#21046;&#26159;&#25351;&#23548;&#26426;&#22120;&#20154;&#32676;&#20307;&#21327;&#21516;&#30417;&#27979;&#26410;&#30693;&#30340;&#24863;&#20852;&#36259;&#29305;&#24449;&#25110;&#29616;&#35937;&#30340;&#38382;&#39064;&#12290;&#22312;&#26377;&#38480;&#30340;&#36890;&#20449;&#21644;&#24863;&#30693;&#33021;&#21147;&#30340;&#20998;&#25955;&#35774;&#32622;&#20013;&#65292;&#36825;&#20010;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#24863;&#30693;-&#34892;&#21160;-&#36890;&#20449;(LPAC)&#26550;&#26500;&#26469;&#35299;&#20915;&#35206;&#30422;&#25511;&#21046;&#38382;&#39064;&#12290;&#22312;&#35813;&#35299;&#20915;&#26041;&#26696;&#20013;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#22788;&#29702;&#20102;&#29615;&#22659;&#30340;&#23616;&#37096;&#24863;&#30693;&#65307;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#23454;&#29616;&#20102;&#37051;&#36817;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#30456;&#20851;&#20449;&#24687;&#36890;&#20449;&#65307;&#26368;&#21518;&#65292;&#27973;&#23618;&#22810;&#23618;&#24863;&#30693;&#26426;(MLP)&#35745;&#31639;&#26426;&#22120;&#20154;&#30340;&#21160;&#20316;&#12290;&#36890;&#20449;&#27169;&#22359;&#20013;&#30340;GNN&#36890;&#36807;&#35745;&#31639;&#24212;&#35813;&#19982;&#37051;&#23621;&#36890;&#20449;&#21738;&#20123;&#20449;&#24687;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#25509;&#25910;&#21040;&#30340;&#20449;&#24687;&#37319;&#21462;&#36866;&#24403;&#30340;&#34892;&#21160;&#26469;&#23454;&#29616;&#26426;&#22120;&#20154;&#32676;&#20307;&#30340;&#21327;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#30693;&#26195;&#25972;&#20010;&#29615;&#22659;&#30340;&#38598;&#20013;&#24335;&#26174;&#24494;&#31639;&#27861;&#26469;&#36827;&#34892;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coverage control is the problem of navigating a robot swarm to collaboratively monitor features or a phenomenon of interest not known a priori. The problem is challenging in decentralized settings with robots that have limited communication and sensing capabilities. This paper proposes a learnable Perception-Action-Communication (LPAC) architecture for the coverage control problem. In the proposed solution, a convolution neural network (CNN) processes localized perception of the environment; a graph neural network (GNN) enables communication of relevant information between neighboring robots; finally, a shallow multi-layer perceptron (MLP) computes robot actions. The GNN in the communication module enables collaboration in the robot swarm by computing what information to communicate with neighbors and how to use received information to take appropriate actions. We train models using imitation learning with a centralized clairvoyant algorithm that is aware of the entire environment. Eva
&lt;/p&gt;</description></item><item><title>RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04679</link><description>&lt;p&gt;
RoSA: &#36890;&#36807;&#40065;&#26834;&#36866;&#24212;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04679
&lt;/p&gt;
&lt;p&gt;
RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#32972;&#26223;&#19979;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#39044;&#31639;&#19979;&#25552;&#20379;&#33391;&#22909;&#20934;&#30830;&#24615;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843; (PEFT) &#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#31216;&#20026;RoSA&#65292;&#21463;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512; (PCA) &#30340;&#21551;&#21457;&#65292;&#23427;&#22312;&#19968;&#32452;&#22266;&#23450;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#20849;&#21516;&#35757;&#32451;$\textit{&#20302;&#31209;}$&#21644;$\textit{&#39640;&#24230;&#31232;&#30095;}$&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#65288;FFT&#65289;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RoSA&#22312;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#23567;&#23398;&#25968;&#23398;&#21644;SQL&#26597;&#35810;&#29983;&#25104;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#20197;&#33719;&#24471;&#33391;&#22909;&#24615;&#33021;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#30456;&#21516;&#30340;&#21442;&#25968;&#39044;&#31639;&#19979;&#65292;RoSA&#20248;&#20110;LoRA&#21644;&#32431;&#31929;&#30340;&#31232;&#30095;&#24494;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#31232;&#30095;GPU&#20869;&#26680;&#20026;RoSA&#25552;&#20379;&#31995;&#32479;&#25903;&#25345;&#65292;&#20197;&#34917;&#20805;&#35757;&#32451;&#31639;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20869;&#23384;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312;https://github.com/IST-DASLab&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memoryand computationally-efficient training. Our code will be made available at https://github.com/IST-DASLab
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#27835;&#30103;&#25512;&#33616;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#22266;&#23450;&#39044;&#31639;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23454;&#39564;&#35774;&#35745;&#21644;&#31574;&#30053;&#23398;&#20064;&#26469;&#25512;&#33616;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26399;&#26395;&#31616;&#21333;&#36951;&#25022;&#26469;&#34913;&#37327;&#25512;&#33616;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.03756</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#22266;&#23450;&#39044;&#31639;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#65306;&#36866;&#24212;&#24615;&#23454;&#39564;&#35774;&#35745;&#19982;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contextual Fixed-Budget Best Arm Identification: Adaptive Experimental Design with Policy Learning. (arXiv:2401.03756v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03756
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#27835;&#30103;&#25512;&#33616;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#22266;&#23450;&#39044;&#31639;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23454;&#39564;&#35774;&#35745;&#21644;&#31574;&#30053;&#23398;&#20064;&#26469;&#25512;&#33616;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26399;&#26395;&#31616;&#21333;&#36951;&#25022;&#26469;&#34913;&#37327;&#25512;&#33616;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#27835;&#30103;&#25512;&#33616;&#26159;&#22522;&#20110;&#35777;&#25454;&#30340;&#20915;&#31574;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#20219;&#21153;&#20316;&#20026;&#19968;&#20010;&#24102;&#26377;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#65288;Best Arm Identification, BAI&#65289;&#38382;&#39064;&#26469;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#32473;&#23450;&#22810;&#20010;&#27835;&#30103;&#33218;&#30340;&#33258;&#36866;&#24212;&#35797;&#39564;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#20915;&#31574;&#32773;&#35266;&#23519;&#19968;&#20010;&#21051;&#30011;&#23454;&#39564;&#21333;&#20301;&#30340;&#19978;&#19979;&#25991;&#65288;&#21327;&#21464;&#37327;&#65289;&#65292;&#24182;&#23558;&#35813;&#21333;&#20301;&#20998;&#37197;&#32473;&#20854;&#20013;&#19968;&#20010;&#27835;&#30103;&#33218;&#12290;&#22312;&#23454;&#39564;&#32467;&#26463;&#26102;&#65292;&#20915;&#31574;&#32773;&#25512;&#33616;&#19968;&#20010;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#26465;&#20214;&#19979;&#39044;&#35745;&#20135;&#29983;&#26368;&#39640;&#26399;&#26395;&#32467;&#26524;&#30340;&#27835;&#30103;&#33218;&#65288;&#26368;&#20339;&#27835;&#30103;&#33218;&#65289;&#12290;&#35813;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26399;&#26395;&#31616;&#21333;&#36951;&#25022;&#65288;&#31574;&#30053;&#36951;&#25022;&#65289;&#26469;&#34913;&#37327;&#65292;&#35813;&#36951;&#25022;&#34920;&#31034;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#26465;&#20214;&#19979;&#65292;&#26368;&#20339;&#27835;&#30103;&#33218;&#21644;&#25512;&#33616;&#27835;&#30103;&#33218;&#30340;&#26465;&#20214;&#26399;&#26395;&#32467;&#26524;&#20043;&#38388;&#30340;&#26368;&#22823;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#21021;&#22987;&#27493;&#39588;&#26159;&#25512;&#23548;&#26368;&#22351;&#24773;&#20917;&#19979;&#26399;&#26395;&#31616;&#21333;&#36951;&#25022;&#30340;&#28176;&#36817;&#19979;&#30028;&#65292;&#35813;&#19979;&#30028;&#36824;&#26263;&#31034;&#30528;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#19968;&#20123;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Individualized treatment recommendation is a crucial task in evidence-based decision-making. In this study, we formulate this task as a fixed-budget best arm identification (BAI) problem with contextual information. In this setting, we consider an adaptive experiment given multiple treatment arms. At each round, a decision-maker observes a context (covariate) that characterizes an experimental unit and assigns the unit to one of the treatment arms. At the end of the experiment, the decision-maker recommends a treatment arm estimated to yield the highest expected outcome conditioned on a context (best treatment arm). The effectiveness of this decision is measured in terms of the worst-case expected simple regret (policy regret), which represents the largest difference between the conditional expected outcomes of the best and recommended treatment arms given a context. Our initial step is to derive asymptotic lower bounds for the worst-case expected simple regret, which also implies idea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#33410;&#32422;&#33021;&#37327;&#30340;&#20943;&#23569;&#25805;&#20316;&#25512;&#26029;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35774;&#35745;&#21644;&#25511;&#21046;&#20013;&#30340;&#22810;&#27425;&#35745;&#31639;&#20219;&#21153;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20445;&#25345;&#33021;&#37327;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#25511;&#21046;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.02889</link><description>&lt;p&gt;
&#39640;&#25928;&#35774;&#35745;&#21644;&#25511;&#21046;&#30340;&#33410;&#32422;&#33021;&#37327;&#32422;&#26463;&#19979;&#30340;&#20943;&#23569;&#25805;&#20316;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Energy-Preserving Reduced Operator Inference for Efficient Design and Control. (arXiv:2401.02889v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#33410;&#32422;&#33021;&#37327;&#30340;&#20943;&#23569;&#25805;&#20316;&#25512;&#26029;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35774;&#35745;&#21644;&#25511;&#21046;&#20013;&#30340;&#22810;&#27425;&#35745;&#31639;&#20219;&#21153;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20445;&#25345;&#33021;&#37327;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#25511;&#21046;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35774;&#35745;&#21644;&#25511;&#21046;&#20013;&#38656;&#35201;&#22810;&#27425;&#35745;&#31639;&#30340;&#24037;&#31243;&#31995;&#32479;&#65292;&#35745;&#31639;&#27169;&#22411;&#30340;&#39640;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#20110;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#25511;&#21046;&#30340;&#31995;&#32479;&#65292;&#24120;&#35265;&#30340;&#39640;&#20445;&#30495;&#25968;&#20540;&#27169;&#22411;&#26159;&#39640;&#32500;&#30340;&#65292;&#23545;&#20110;&#22810;&#27425;&#35745;&#31639;&#32780;&#35328;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26377;&#25928;&#30340;&#20195;&#29702;&#27169;&#22411;&#20197;&#23454;&#29616;&#35774;&#35745;&#21644;&#25511;&#21046;&#20013;&#30340;&#20302;&#25104;&#26412;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25345;&#29289;&#29702;&#32422;&#26463;&#30340;&#20943;&#23569;&#27169;&#22411;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#33021;&#37327;&#20445;&#25345;&#30340;PDE&#65292;&#20363;&#22914;&#35768;&#22810;&#27969;&#20307;&#38382;&#39064;&#20013;&#20986;&#29616;&#30340;&#25511;&#21046;&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#25805;&#20316;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#20108;&#20056;&#25311;&#21512;&#23558;&#20943;&#23569;&#27169;&#22411;&#31639;&#23376;&#19982;&#29366;&#24577;&#24555;&#29031;&#21644;&#26102;&#38388;&#23548;&#25968;&#25968;&#25454;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#25805;&#20316;&#25512;&#26029;&#36890;&#24120;&#19981;&#33021;&#23398;&#20064;&#21040;&#20855;&#26377;&#33021;&#37327;&#20445;&#25345;&#23646;&#24615;&#30340;&#20943;&#23569;&#20108;&#27425;&#31639;&#23376;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33021;&#37327;&#20445;&#25345;&#25805;&#20316;&#25512;&#26029;&#65288;EP-O&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many-query computations, in which a computational model for an engineering system must be evaluated many times, are crucial in design and control. For systems governed by partial differential equations (PDEs), typical high-fidelity numerical models are high-dimensional and too computationally expensive for the many-query setting. Thus, efficient surrogate models are required to enable low-cost computations in design and control. This work presents a physics-preserving reduced model learning approach that targets PDEs whose quadratic operators preserve energy, such as those arising in governing equations in many fluids problems. The approach is based on the Operator Inference method, which fits reduced model operators to state snapshot and time derivative data in a least-squares sense. However, Operator Inference does not generally learn a reduced quadratic operator with the energy-preserving property of the original PDE. Thus, we propose a new energy-preserving Operator Inference (EP-O
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#24863;&#30693;&#32852;&#37030;&#20316;&#19994;&#35843;&#24230;&#65288;FairFedJS&#65289;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#23558;&#39640;&#38656;&#27714;&#30340;FL&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#20844;&#24179;&#20998;&#37197;&#32473;&#38656;&#35201;&#23427;&#20204;&#30340;FL&#20316;&#19994;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.02740</link><description>&lt;p&gt;
&#20026;&#22810;&#20219;&#21153;&#32852;&#37030;&#23398;&#20064;&#25552;&#20379;&#20844;&#24179;&#24615;&#24863;&#30693;&#30340;&#20316;&#19994;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Fairness-Aware Job Scheduling for Multi-Job Federated Learning. (arXiv:2401.02740v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#24863;&#30693;&#32852;&#37030;&#20316;&#19994;&#35843;&#24230;&#65288;FairFedJS&#65289;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#23558;&#39640;&#38656;&#27714;&#30340;FL&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#20844;&#24179;&#20998;&#37197;&#32473;&#38656;&#35201;&#23427;&#20204;&#30340;FL&#20316;&#19994;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#22810;&#20010;&#25968;&#25454;&#25152;&#26377;&#32773;&#65288;&#21363;FL&#23458;&#25143;&#31471;&#65289;&#33021;&#22815;&#22312;&#19981;&#27844;&#38706;&#25935;&#24863;&#31169;&#20154;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;FL&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22404;&#26029;&#22330;&#26223;&#65292;&#22312;&#35813;&#22330;&#26223;&#20013;&#65292;&#21333;&#20010;FL&#26381;&#21153;&#22120;&#22312;&#27599;&#36718;&#35757;&#32451;&#20013;&#36873;&#25321;&#19968;&#37096;&#20998;FL&#23458;&#25143;&#31471;&#26469;&#26356;&#26032;&#20854;&#26412;&#22320;&#27169;&#22411;&#12290;&#23454;&#38469;&#19978;&#65292;&#21487;&#33021;&#20250;&#26377;&#22810;&#20010;FL&#26381;&#21153;&#22120;&#21516;&#26102;&#23581;&#35797;&#20174;&#21516;&#19968;&#20010;&#27744;&#20013;&#36873;&#25321;&#23458;&#25143;&#31471;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#21019;&#30340;&#20844;&#24179;&#24863;&#30693;&#32852;&#37030;&#20316;&#19994;&#35843;&#24230;&#65288;FairFedJS&#65289;&#26041;&#27861;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#22522;&#20110;Lyapunov&#20248;&#21270;&#65292;&#23427;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#24403;&#21069;&#38656;&#27714;&#21644;&#20316;&#19994;&#20184;&#27454;&#20986;&#20215;&#65292;&#30830;&#20445;&#23558;&#39640;&#38656;&#27714;&#30340;FL&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#20844;&#24179;&#20998;&#37197;&#32473;&#38656;&#35201;&#23427;&#20204;&#30340;FL&#20316;&#19994;&#65292;&#20197;&#38450;&#27490;&#31561;&#24453;&#26102;&#38388;&#36807;&#38271;&#12290;&#22522;&#20110;&#20004;&#20010;&#25968;&#25454;&#38598;&#23545;FairFedJS&#19982;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26174;&#33879;&#20248;&#21183;&#12290;&#23427;&#22312;&#24179;&#22343;&#19978;&#20987;&#36133;&#20102;&#26368;&#20339;&#22522;&#20934;&#32447;31.9%&#21644;1.0%&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables multiple data owners (a.k.a. FL clients) to collaboratively train machine learning models without disclosing sensitive private data. Existing FL research mostly focuses on the monopoly scenario in which a single FL server selects a subset of FL clients to update their local models in each round of training. In practice, there can be multiple FL servers simultaneously trying to select clients from the same pool. In this paper, we propose a first-of-its-kind Fairness-aware Federated Job Scheduling (FairFedJS) approach to bridge this gap. Based on Lyapunov optimization, it ensures fair allocation of high-demand FL client datasets to FL jobs in need of them, by jointly considering the current demand and the job payment bids, in order to prevent prolonged waiting. Extensive experiments comparing FairFedJS against four state-of-the-art approaches on two datasets demonstrate its significant advantages. It outperforms the best baseline by 31.9% and 1.0% on avera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#26102;&#38388;&#39057;&#29575;&#20998;&#26512;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23454;&#38469;&#26465;&#20214;&#19979;&#35786;&#26029;&#24102;&#26377;&#26102;&#38388;&#21464;&#21270;&#36895;&#24230;&#21644;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;&#36724;&#25215;&#25925;&#38556;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#26512;&#19982;&#19981;&#21516;&#36724;&#25215;&#25925;&#38556;&#30456;&#20851;&#30340;&#29420;&#29305;&#21160;&#24577;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.01172</link><description>&lt;p&gt;
&#25391;&#21160;&#20449;&#21495;&#30340;&#20108;&#27425;&#26102;&#38388;&#39057;&#29575;&#20998;&#26512;&#29992;&#20110;&#35786;&#26029;&#36724;&#25215;&#25925;&#38556;
&lt;/p&gt;
&lt;p&gt;
Quadratic Time-Frequency Analysis of Vibration Signals for Diagnosing Bearing Faults. (arXiv:2401.01172v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#26102;&#38388;&#39057;&#29575;&#20998;&#26512;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23454;&#38469;&#26465;&#20214;&#19979;&#35786;&#26029;&#24102;&#26377;&#26102;&#38388;&#21464;&#21270;&#36895;&#24230;&#21644;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;&#36724;&#25215;&#25925;&#38556;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#26512;&#19982;&#19981;&#21516;&#36724;&#25215;&#25925;&#38556;&#30456;&#20851;&#30340;&#29420;&#29305;&#21160;&#24577;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36724;&#25215;&#25925;&#38556;&#30340;&#35786;&#26029;&#23545;&#20110;&#38477;&#20302;&#32500;&#20462;&#25104;&#26412;&#21644;&#35774;&#22791;&#20572;&#26426;&#33267;&#20851;&#37325;&#35201;&#12290;&#36724;&#25215;&#25925;&#38556;&#26159;&#26426;&#22120;&#25391;&#21160;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#20998;&#26512;&#20854;&#20449;&#21495;&#24418;&#24577;&#21487;&#20197;&#25581;&#31034;&#20854;&#20581;&#24247;&#29366;&#20917;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#38024;&#23545;&#25511;&#21046;&#29615;&#22659;&#36827;&#34892;&#20248;&#21270;&#65292;&#24573;&#30053;&#20102;&#23454;&#38469;&#26465;&#20214;&#19979;&#30340;&#26102;&#38388;&#21464;&#21270;&#30340;&#36716;&#36895;&#21644;&#25391;&#21160;&#30340;&#38750;&#24179;&#31283;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#39057;&#29575;&#20998;&#26512;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#21464;&#21270;&#36895;&#24230;&#21644;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#19979;&#35786;&#26029;&#36724;&#25215;&#25925;&#38556;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#36724;&#25215;&#25925;&#38556;&#24341;&#36215;&#30340;&#25391;&#21160;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#38750;&#24179;&#31283;&#24615;&#19982;&#36724;&#25215;&#22266;&#26377;&#21644;&#25805;&#20316;&#21442;&#25968;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#36824;&#38416;&#36848;&#20102;&#20108;&#27425;&#26102;&#38388;&#39057;&#29575;&#20998;&#24067;&#65292;&#24182;&#39564;&#35777;&#20102;&#23427;&#20204;&#35299;&#26512;&#19982;&#19981;&#21516;&#36724;&#25215;&#25925;&#38556;&#30456;&#20851;&#30340;&#29420;&#29305;&#21160;&#24577;&#27169;&#24335;&#30340;&#26377;&#25928;&#24615;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26102;&#38388;&#39057;&#29575;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diagnosis of bearing faults is paramount to reducing maintenance costs and operational breakdowns. Bearing faults are primary contributors to machine vibrations, and analyzing their signal morphology offers insights into their health status. Unfortunately, existing approaches are optimized for controlled environments, neglecting realistic conditions such as time-varying rotational speeds and the vibration's non-stationary nature. This paper presents a fusion of time-frequency analysis and deep learning techniques to diagnose bearing faults under time-varying speeds and varying noise levels. First, we formulate the bearing fault-induced vibrations and discuss the link between their non-stationarity and the bearing's inherent and operational parameters. We also elucidate quadratic time-frequency distributions and validate their effectiveness in resolving distinctive dynamic patterns associated with different bearing faults. Based on this, we design a time-frequency convolutional neural n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#35266;&#23519;&#20013;&#35782;&#21035;&#21160;&#24577;&#31995;&#32479;&#30340;&#32467;&#26500;&#65292;&#24182;&#29702;&#35299;&#22797;&#26434;&#31995;&#32479;&#20013;&#30456;&#20114;&#20316;&#29992;&#20195;&#29702;&#30340; emergent behaviors&#12290;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#20855;&#22791;&#29702;&#35770;&#25910;&#25947;&#20445;&#35777;&#65292;&#36824;&#33021;&#39640;&#25928;&#22788;&#29702;&#39640;&#32500;&#35266;&#27979;&#25968;&#25454;&#65292;&#35299;&#20915;&#35266;&#27979;/&#38543;&#26426;&#22122;&#22768;&#12289;&#22797;&#26434;&#30340;&#20132;&#20114;&#35268;&#21017;&#12289;&#20002;&#22833;&#30340;&#20132;&#20114;&#29305;&#24449;&#21644;&#29616;&#23454;&#19990;&#30028;&#35266;&#27979;&#25968;&#25454;&#31561;&#38382;&#39064;&#12290;&#36890;&#36807;&#37319;&#29992;&#21464;&#20998;&#36870;&#38382;&#39064;&#26041;&#27861;&#35774;&#35745;&#21512;&#36866;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#30340;&#23398;&#20064;&#26041;&#27861;&#20855;&#22791;&#38477;&#32500;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00875</link><description>&lt;p&gt;
&#20174;&#35266;&#23519;&#20013;&#23398;&#20064;&#38598;&#20307;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Learning Collective Behaviors from Observation. (arXiv:2311.00875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#35266;&#23519;&#20013;&#35782;&#21035;&#21160;&#24577;&#31995;&#32479;&#30340;&#32467;&#26500;&#65292;&#24182;&#29702;&#35299;&#22797;&#26434;&#31995;&#32479;&#20013;&#30456;&#20114;&#20316;&#29992;&#20195;&#29702;&#30340; emergent behaviors&#12290;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#20855;&#22791;&#29702;&#35770;&#25910;&#25947;&#20445;&#35777;&#65292;&#36824;&#33021;&#39640;&#25928;&#22788;&#29702;&#39640;&#32500;&#35266;&#27979;&#25968;&#25454;&#65292;&#35299;&#20915;&#35266;&#27979;/&#38543;&#26426;&#22122;&#22768;&#12289;&#22797;&#26434;&#30340;&#20132;&#20114;&#35268;&#21017;&#12289;&#20002;&#22833;&#30340;&#20132;&#20114;&#29305;&#24449;&#21644;&#29616;&#23454;&#19990;&#30028;&#35266;&#27979;&#25968;&#25454;&#31561;&#38382;&#39064;&#12290;&#36890;&#36807;&#37319;&#29992;&#21464;&#20998;&#36870;&#38382;&#39064;&#26041;&#27861;&#35774;&#35745;&#21512;&#36866;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#30340;&#23398;&#20064;&#26041;&#27861;&#20855;&#22791;&#38477;&#32500;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#23398;&#20064;&#26041;&#27861;&#30340;&#32508;&#36848;&#65292;&#29992;&#20110;&#35782;&#21035;&#21160;&#24577;&#31995;&#32479;&#30340;&#32467;&#26500;&#65292;&#26088;&#22312;&#29702;&#35299;&#30456;&#20114;&#20316;&#29992;&#20195;&#29702;&#31995;&#32479;&#20013;&#30340; emergent behaviors&#12290;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#25552;&#20379;&#20102;&#25910;&#25947;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#36824;&#23637;&#31034;&#20102;&#22788;&#29702;&#39640;&#32500;&#35266;&#27979;&#25968;&#25454;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#23427;&#20204;&#21487;&#20197;&#22788;&#29702;&#26469;&#33258;&#19968;&#38454;&#21644;&#20108;&#38454;&#21160;&#24577;&#31995;&#32479;&#30340;&#35266;&#27979;&#25968;&#25454;&#65292;&#32771;&#34385;&#35266;&#27979;/&#38543;&#26426;&#22122;&#22768;&#12289;&#22797;&#26434;&#30340;&#20132;&#20114;&#35268;&#21017;&#12289;&#20002;&#22833;&#30340;&#20132;&#20114;&#29305;&#24449;&#20197;&#21450;&#19982;&#29616;&#23454;&#19990;&#30028;&#20013;&#30456;&#20114;&#20316;&#29992;&#20195;&#29702;&#31995;&#32479;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;&#21457;&#23637;&#36825;&#19968;&#31995;&#21015;&#23398;&#20064;&#26041;&#27861;&#30340;&#26680;&#24515;&#22312;&#20110;&#20351;&#29992;&#21464;&#20998;&#36870;&#38382;&#39064;&#26041;&#27861;&#35774;&#35745;&#36866;&#24403;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#22825;&#28982;&#22320;&#25552;&#20379;&#20102;&#25105;&#20204;&#23398;&#20064;&#26041;&#27861;&#30340;&#38477;&#32500;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a review of a series of learning methods used to identify the structure of dynamical systems, aiming to understand emergent behaviors in complex systems of interacting agents. These methods not only offer theoretical guarantees of convergence but also demonstrate computational efficiency in handling high-dimensional observational data. They can manage observation data from both first- and second-order dynamical systems, accounting for observation/stochastic noise, complex interaction rules, missing interaction features, and real-world observations of interacting agent systems. The essence of developing such a series of learning methods lies in designing appropriate loss functions using the variational inverse problem approach, which inherently provides dimension reduction capabilities to our learning methods.
&lt;/p&gt;</description></item><item><title>DiffEnc&#26159;&#19968;&#31181;&#20351;&#29992;&#23398;&#20064;&#30340;&#32534;&#30721;&#22120;&#30340;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#21644;&#28145;&#24230;&#30456;&#20851;&#30340;&#22343;&#20540;&#20989;&#25968;&#21644;&#21487;&#35843;&#33410;&#30340;&#22122;&#22768;&#26041;&#24046;&#27604;&#29575;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.19789</link><description>&lt;p&gt;
DiffEnc: &#20351;&#29992;&#23398;&#20064;&#30340;&#32534;&#30721;&#22120;&#30340;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiffEnc: Variational Diffusion with a Learned Encoder. (arXiv:2310.19789v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19789
&lt;/p&gt;
&lt;p&gt;
DiffEnc&#26159;&#19968;&#31181;&#20351;&#29992;&#23398;&#20064;&#30340;&#32534;&#30721;&#22120;&#30340;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#21644;&#28145;&#24230;&#30456;&#20851;&#30340;&#22343;&#20540;&#20989;&#25968;&#21644;&#21487;&#35843;&#33410;&#30340;&#22122;&#22768;&#26041;&#24046;&#27604;&#29575;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#20004;&#31181;&#25913;&#36827;&#30340;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#65306;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#21442;&#25968;&#20849;&#20139;&#30340;&#26465;&#20214;&#20998;&#24067;&#21644;&#22312;&#23618;&#27425;&#32467;&#26500;&#19978;&#29420;&#31435;&#35745;&#31639;&#25439;&#22833;&#12290;&#25105;&#20204;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#20004;&#20010;&#21464;&#21270;&#65292;&#20445;&#30041;&#20102;&#36825;&#20123;&#20248;&#21183;&#30340;&#21516;&#26102;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#19982;&#25968;&#25454;&#21644;&#28145;&#24230;&#30456;&#20851;&#30340;&#22343;&#20540;&#20989;&#25968;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#20462;&#25913;&#21518;&#30340;&#25193;&#25955;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;DiffEnc&#22312;CIFAR-10&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21487;&#33021;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35753;&#21453;&#21521;&#32534;&#30721;&#36807;&#31243;&#30340;&#22122;&#22768;&#26041;&#24046;&#19982;&#29983;&#25104;&#36807;&#31243;&#30340;&#27604;&#29575;&#25104;&#20026;&#19968;&#20010;&#33258;&#30001;&#30340;&#26435;&#37325;&#21442;&#25968;&#65292;&#32780;&#19981;&#26159;&#22266;&#23450;&#20026;1&#12290;&#36825;&#24102;&#26469;&#20102;&#29702;&#35770;&#19978;&#30340;&#27934;&#23519;&#21147;&#65306;&#23545;&#20110;&#26377;&#38480;&#28145;&#24230;&#23618;&#27425;&#65292;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#21487;&#20197;&#29992;&#20316;&#21152;&#26435;&#25193;&#25955;&#25439;&#22833;&#26041;&#27861;&#30340;&#30446;&#26631;&#65292;&#24182;&#29992;&#20110;&#19987;&#38376;&#20026;&#25512;&#29702;&#32780;&#20248;&#21270;&#22122;&#22768;&#35843;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models may be viewed as hierarchical variational autoencoders (VAEs) with two improvements: parameter sharing for the conditional distributions in the generative process and efficient computation of the loss as independent terms over the hierarchy. We consider two changes to the diffusion model that retain these advantages while adding flexibility to the model. Firstly, we introduce a data- and depth-dependent mean function in the diffusion process, which leads to a modified diffusion loss. Our proposed framework, DiffEnc, achieves state-of-the-art likelihood on CIFAR-10. Secondly, we let the ratio of the noise variance of the reverse encoder process and the generative process be a free weight parameter rather than being fixed to 1. This leads to theoretical insights: For a finite depth hierarchy, the evidence lower bound (ELBO) can be used as an objective for a weighted diffusion loss approach and for optimizing the noise schedule specifically for inference. For the infinite
&lt;/p&gt;</description></item><item><title>VFedMH&#26159;&#19968;&#31181;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#32858;&#21512;&#21442;&#19982;&#32773;&#30340;&#23884;&#20837;&#26469;&#22788;&#29702;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#24322;&#26500;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;VFL&#26041;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.13367</link><description>&lt;p&gt;
VFedMH: &#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#29992;&#20110;&#35757;&#32451;&#22810;&#21442;&#19982;&#26041;&#24322;&#26500;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VFedMH: Vertical Federated Learning for Training Multi-party Heterogeneous Models. (arXiv:2310.13367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13367
&lt;/p&gt;
&lt;p&gt;
VFedMH&#26159;&#19968;&#31181;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#32858;&#21512;&#21442;&#19982;&#32773;&#30340;&#23884;&#20837;&#26469;&#22788;&#29702;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#24322;&#26500;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;VFL&#26041;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65288;VFL&#65289;&#20316;&#20026;&#19968;&#31181;&#38598;&#25104;&#26679;&#26412;&#23545;&#40784;&#21644;&#29305;&#24449;&#21512;&#24182;&#30340;&#26032;&#22411;&#35757;&#32451;&#33539;&#24335;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;VFL&#26041;&#27861;&#22312;&#22788;&#29702;&#21442;&#19982;&#32773;&#20043;&#38388;&#23384;&#22312;&#24322;&#26500;&#26412;&#22320;&#27169;&#22411;&#26102;&#38754;&#20020;&#25361;&#25112;&#65292;&#36825;&#24433;&#21709;&#20102;&#20248;&#21270;&#25910;&#25947;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VFedMH&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#22810;&#26041;&#24322;&#26500;&#27169;&#22411;&#12290;VFedMH&#30340;&#37325;&#28857;&#26159;&#22312;&#21069;&#21521;&#20256;&#25773;&#26399;&#38388;&#32858;&#21512;&#27599;&#20010;&#21442;&#19982;&#32773;&#30693;&#35782;&#30340;&#23884;&#20837;&#65292;&#32780;&#19981;&#26159;&#20013;&#38388;&#32467;&#26524;&#12290;&#20027;&#21160;&#26041;&#65292;&#25317;&#26377;&#26679;&#26412;&#30340;&#26631;&#31614;&#21644;&#29305;&#24449;&#65292;&#22312;VFedMH&#20013;&#23433;&#20840;&#22320;&#32858;&#21512;&#26412;&#22320;&#23884;&#20837;&#20197;&#33719;&#24471;&#20840;&#23616;&#30693;&#35782;&#23884;&#20837;&#65292;&#24182;&#23558;&#20854;&#21457;&#36865;&#32473;&#34987;&#21160;&#26041;&#12290;&#34987;&#21160;&#26041;&#20165;&#25317;&#26377;&#26679;&#26412;&#30340;&#29305;&#24449;&#65292;&#28982;&#21518;&#21033;&#29992;&#20840;&#23616;&#23884;&#20837;&#22312;&#20854;&#26412;&#22320;&#24322;&#26500;&#32593;&#32476;&#19978;&#36827;&#34892;&#21069;&#21521;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#34987;&#21160;&#26041;&#19981;&#25317;&#26377;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Learning (VFL) has gained increasing attention as a novel training paradigm that integrates sample alignment and feature union. However, existing VFL methods face challenges when dealing with heterogeneous local models among participants, which affects optimization convergence and generalization. To address this issue, this paper proposes a novel approach called Vertical Federated learning for training Multi-parties Heterogeneous models (VFedMH). VFedMH focuses on aggregating the embeddings of each participant's knowledge instead of intermediate results during forward propagation. The active party, who possesses labels and features of the sample, in VFedMH securely aggregates local embeddings to obtain global knowledge embeddings, and sends them to passive parties. The passive parties, who own only features of the sample, then utilize the global embeddings to propagate forward on their local heterogeneous networks. However, the passive party does not own the labels, 
&lt;/p&gt;</description></item><item><title>Lag-Llama&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#35757;&#32451;&#30340;&#36890;&#29992;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#20809;&#28369;&#26029;&#35010;&#24130;&#24459;&#27169;&#22411;&#26469;&#25311;&#21512;&#21644;&#39044;&#27979;&#25193;&#23637;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2310.08278</link><description>&lt;p&gt;
Lag-Llama: &#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lag-Llama: Towards Foundation Models for Time Series Forecasting. (arXiv:2310.08278v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08278
&lt;/p&gt;
&lt;p&gt;
Lag-Llama&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#35757;&#32451;&#30340;&#36890;&#29992;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#20809;&#28369;&#26029;&#35010;&#24130;&#24459;&#27169;&#22411;&#26469;&#25311;&#21512;&#21644;&#39044;&#27979;&#25193;&#23637;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26500;&#24314;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;&#24182;&#30740;&#31350;&#20854;&#25193;&#23637;&#34892;&#20026;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#20171;&#32461;&#20102;&#25105;&#20204;&#27491;&#22312;&#36827;&#34892;&#20013;&#30340; Lag-Llama &#24037;&#20316;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#36890;&#29992;&#21333;&#21464;&#37327;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#8220;&#20998;&#24067;&#22806;&#8221;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#33021;&#21147;&#65292;&#20248;&#20110;&#26377;&#30417;&#30563;&#22522;&#32447;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20809;&#28369;&#26029;&#35010;&#24130;&#24459;&#26469;&#25311;&#21512;&#21644;&#39044;&#27979;&#27169;&#22411;&#30340;&#25193;&#23637;&#34892;&#20026;&#12290;&#24320;&#28304;&#20195;&#30721;&#21487;&#22312; https://github.com/kashif/pytorch-transformer-ts &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aiming to build foundation models for time-series forecasting and study their scaling behavior, we present here our work-in-progress on Lag-Llama, a general-purpose univariate probabilistic time-series forecasting model trained on a large collection of time-series data. The model shows good zero-shot prediction capabilities on unseen "out-of-distribution" time-series datasets, outperforming supervised baselines. We use smoothly broken power-laws to fit and predict model scaling behavior. The open source code is made available at https://github.com/kashif/pytorch-transformer-ts.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#21457;&#29616;&#28151;&#21512;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#26029;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#20197;&#21450;&#27599;&#20010;&#26679;&#26412;&#23646;&#20110;&#29305;&#23450;&#28151;&#21512;&#25104;&#20998;&#30340;&#27010;&#29575;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22240;&#26524;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06312</link><description>&lt;p&gt;
&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#21457;&#29616;&#28151;&#21512;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Discovering Mixtures of Structural Causal Models from Time Series Data. (arXiv:2310.06312v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06312
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#21457;&#29616;&#28151;&#21512;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#26029;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#20197;&#21450;&#27599;&#20010;&#26679;&#26412;&#23646;&#20110;&#29305;&#23450;&#28151;&#21512;&#25104;&#20998;&#30340;&#27010;&#29575;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22240;&#26524;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37329;&#34701;&#12289;&#27668;&#20505;&#31185;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#31561;&#39046;&#22495;&#65292;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#29616;&#20195;&#25216;&#26415;&#21487;&#20197;&#22788;&#29702;&#21464;&#37327;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#21644;&#28789;&#27963;&#30340;&#22122;&#22768;&#20998;&#24067;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#31616;&#21270;&#20551;&#35774;&#65292;&#21363;&#25968;&#25454;&#26469;&#33258;&#30456;&#21516;&#30340;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25918;&#26494;&#20102;&#36825;&#20010;&#20551;&#35774;&#65292;&#20174;&#26469;&#28304;&#20110;&#19981;&#21516;&#22240;&#26524;&#27169;&#22411;&#28151;&#21512;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#12290;&#25105;&#20204;&#25512;&#26029;&#20102;&#28508;&#22312;&#30340;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#65292;&#20197;&#21450;&#27599;&#20010;&#26679;&#26412;&#23646;&#20110;&#29305;&#23450;&#28151;&#21512;&#25104;&#20998;&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#20010;&#31471;&#23545;&#31471;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#26368;&#22823;&#21270;&#20102;&#25968;&#25454;&#20284;&#28982;&#30340;&#35777;&#25454;&#19979;&#30028;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22240;&#26524;&#21457;&#29616;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#24403;&#25968;&#25454;&#26469;&#33258;&#19981;&#21516;&#30340;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
In fields such as finance, climate science, and neuroscience, inferring causal relationships from time series data poses a formidable challenge. While contemporary techniques can handle nonlinear relationships between variables and flexible noise distributions, they rely on the simplifying assumption that data originates from the same underlying causal model. In this work, we relax this assumption and perform causal discovery from time series data originating from mixtures of different causal models. We infer both the underlying structural causal models and the posterior probability for each sample belonging to a specific mixture component. Our approach employs an end-to-end training process that maximizes an evidence-lower bound for data likelihood. Through extensive experimentation on both synthetic and real-world datasets, we demonstrate that our method surpasses state-of-the-art benchmarks in causal discovery tasks, particularly when the data emanates from diverse underlying causal
&lt;/p&gt;</description></item><item><title>COPRA&#26159;&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#36873;&#25321;&#31574;&#30053;&#21644;&#26816;&#32034;&#23450;&#20041;&#21644;&#24341;&#29702;&#36827;&#34892;&#35777;&#26126;&#65292;&#22312;MiniF2F&#22522;&#20934;&#21644;Coq&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.04353</link><description>&lt;p&gt;
&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Language-Agent Approach to Formal Theorem-Proving. (arXiv:2310.04353v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04353
&lt;/p&gt;
&lt;p&gt;
COPRA&#26159;&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#36873;&#25321;&#31574;&#30053;&#21644;&#26816;&#32034;&#23450;&#20041;&#21644;&#24341;&#29702;&#36827;&#34892;&#35777;&#26126;&#65292;&#22312;MiniF2F&#22522;&#20934;&#21644;Coq&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#26159;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#19982;&#22806;&#37096;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25511;&#21046;&#20219;&#21153;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language agents, which use a large language model (LLM) capable of in-context learning to interact with an external environment, have recently emerged as a promising approach to control tasks. We present the first language-agent approach to formal theorem-proving. Our method, COPRA, uses a high-capacity, black-box LLM (GPT-4) as part of a policy for a stateful backtracking search. During the search, the policy can select proof tactics and retrieve lemmas and definitions from an external database. Each selected tactic is executed in the underlying proof framework, and the execution feedback is used to build the prompt for the next policy invocation. The search also tracks selected information from its history and uses it to reduce hallucinations and unnecessary LLM queries.  We evaluate COPRA on the miniF2F benchmark for Lean and a set of Coq tasks from the Compcert project. On these benchmarks, COPRA is significantly better than one-shot invocations of GPT-4, as well as state-of-the-ar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38416;&#36848;&#20102;&#20851;&#20110;&#38543;&#26426;&#21521;&#37327;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#30340;&#19979;&#30028;&#65292;&#37325;&#28857;&#26159;&#20223;&#23556;&#21464;&#25442;&#65292;&#24182;&#24212;&#29992;&#20110;&#27969;&#22411;&#23398;&#20064;&#21644;&#25163;&#20889;&#25968;&#25454;&#38598;&#27169;&#25311;&#12290;</title><link>http://arxiv.org/abs/2310.03945</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#21521;&#37327;&#30340;&#20223;&#23556;&#21464;&#25442;&#30340;Wasserstein&#36317;&#31163;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Wasserstein distances for affine transformations of random vectors. (arXiv:2310.03945v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38416;&#36848;&#20102;&#20851;&#20110;&#38543;&#26426;&#21521;&#37327;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#30340;&#19979;&#30028;&#65292;&#37325;&#28857;&#26159;&#20223;&#23556;&#21464;&#25442;&#65292;&#24182;&#24212;&#29992;&#20110;&#27969;&#22411;&#23398;&#20064;&#21644;&#25163;&#20889;&#25968;&#25454;&#38598;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38416;&#36848;&#20102;&#20851;&#20110;&#22312;Wasserstein&#31354;&#38388;&#20013;&#29992;&#20110;&#25968;&#25454;&#27969;&#22411;&#23398;&#20064;&#30340;&#38543;&#26426;&#21521;&#37327;&#20043;&#38388;&#30340;&#20108;&#27425;Wasserstein&#36317;&#31163;&#30340;&#19968;&#20123;&#24050;&#30693;&#19979;&#30028;&#65292;&#37325;&#28857;&#26159;&#20223;&#23556;&#21464;&#25442;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#35745;&#31639;&#21327;&#26041;&#24046;&#30697;&#38453;&#20043;&#38388;&#30340;Bures&#36317;&#31163;&#65292;&#32473;&#20986;&#20102;&#26059;&#36716;&#30340;&#38543;&#26426;&#21521;&#37327;&#22312;&#20855;&#26377;&#19981;&#30456;&#20851;&#20998;&#37327;&#30340;$\mathbb{R}^2$&#31354;&#38388;&#20013;&#30340;&#20855;&#20307;&#19979;&#30028;&#12290;&#25105;&#20204;&#36824;&#24471;&#21040;&#20102;&#20223;&#23556;&#21464;&#25442;&#30340;&#32452;&#21512;&#30340;&#19978;&#30028;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#24212;&#29992;&#20110;&#21021;&#22987;&#25968;&#25454;&#27979;&#24230;&#30340;&#20016;&#23500;&#30340;&#24494;&#20998;&#21516;&#32986;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#30028;&#24212;&#29992;&#20110;&#21253;&#25324;&#22312;$\mathbb{R}^2$&#20013;&#30340;&#19968;&#32500;&#27969;&#22411;&#19978;&#30340;&#21508;&#31181;&#20998;&#24067;&#65292;&#24182;&#35828;&#26126;&#20102;&#36825;&#20123;&#30028;&#30340;&#36136;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#27969;&#22411;&#23398;&#20064;&#26694;&#26550;&#20013;&#21487;&#20197;&#24212;&#29992;&#20110;&#27169;&#25311;&#25163;&#20889;&#25968;&#23383;&#25110;&#23383;&#27597;&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
We expound on some known lower bounds of the quadratic Wasserstein distance between random vectors in $\mathbb{R}^n$ with an emphasis on affine transformations that have been used in manifold learning of data in Wasserstein space. In particular, we give concrete lower bounds for rotated copies of random vectors in $\mathbb{R}^2$ with uncorrelated components by computing the Bures metric between the covariance matrices. We also derive upper bounds for compositions of affine maps which yield a fruitful variety of diffeomorphisms applied to an initial data measure. We apply these bounds to various distributions including those lying on a 1-dimensional manifold in $\mathbb{R}^2$ and illustrate the quality of the bounds. Finally, we give a framework for mimicking handwritten digit or alphabet datasets that can be applied in a manifold learning framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#21518;&#39564;&#30340;VAE&#26041;&#27861;&#65292;&#37325;&#26032;&#23450;&#20041;&#20102;ELBO&#65292;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#21644;PatchGAN&#37492;&#21035;&#22120;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#12290;</title><link>http://arxiv.org/abs/2309.13160</link><description>&lt;p&gt;
GAMIX-VAE: &#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#21518;&#39564;&#30340;VAE
&lt;/p&gt;
&lt;p&gt;
GAMIX-VAE: A VAE with Gaussian Mixture Based Posterior. (arXiv:2309.13160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#21518;&#39564;&#30340;VAE&#26041;&#27861;&#65292;&#37325;&#26032;&#23450;&#20041;&#20102;ELBO&#65292;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#21644;PatchGAN&#37492;&#21035;&#22120;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#29983;&#25104;&#24314;&#27169;&#21644;&#34920;&#31034;&#23398;&#20064;&#30340;&#22522;&#30707;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;VAEs&#30340;&#19968;&#20010;&#32454;&#24494;&#26041;&#38754;&#65292;&#37325;&#28857;&#26159;&#35299;&#37322;KL Divergence&#65292;&#36825;&#26159;Evidence Lower Bound&#65288;ELBO&#65289;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#25511;&#21046;&#20102;&#37325;&#26500;&#20934;&#30830;&#24615;&#21644;&#27491;&#21017;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#34429;&#28982;KL Divergence&#35753;&#28508;&#21464;&#37327;&#20998;&#24067;&#19982;&#20808;&#39564;&#20998;&#24067;&#23545;&#40784;&#65292;&#32473;&#25972;&#20010;&#28508;&#31354;&#38388;&#21152;&#19978;&#32467;&#26500;&#32422;&#26463;&#65292;&#20294;&#21364;&#19981;&#38480;&#21046;&#21508;&#20010;&#21464;&#37327;&#20998;&#24067;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#37325;&#26032;&#23450;&#20041;&#20102;&#24102;&#26377;&#39640;&#26031;&#28151;&#21512;&#30340;&#21518;&#39564;&#27010;&#29575;&#30340;ELBO&#65292;&#24341;&#20837;&#20102;&#27491;&#21017;&#21270;&#39033;&#20197;&#38450;&#27490;&#26041;&#24046;&#23849;&#28291;&#65292;&#24182;&#20351;&#29992;PatchGAN&#37492;&#21035;&#22120;&#26469;&#22686;&#24378;&#32441;&#29702;&#36924;&#30495;&#24230;&#12290;&#23454;&#29616;&#32454;&#33410;&#28041;&#21450;Encoder&#21644;Decoder&#30340;ResNetV2&#26550;&#26500;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#30340;&#33021;&#21147;&#65292;&#20026;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational Autoencoders (VAEs) have become a cornerstone in generative modeling and representation learning within machine learning. This paper explores a nuanced aspect of VAEs, focusing on interpreting the Kullback Leibler (KL) Divergence, a critical component within the Evidence Lower Bound (ELBO) that governs the trade-off between reconstruction accuracy and regularization. While the KL Divergence enforces alignment between latent variable distributions and a prior imposing a structure on the overall latent space but leaves individual variable distributions unconstrained. The proposed method redefines the ELBO with a mixture of Gaussians for the posterior probability, introduces a regularization term to prevent variance collapse, and employs a PatchGAN discriminator to enhance texture realism. Implementation details involve ResNetV2 architectures for both the Encoder and Decoder. The experiments demonstrate the ability to generate realistic faces, offering a promising solution for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DiscoSCMs&#65292;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#22240;&#26524;&#26597;&#35810;&#30340;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#25193;&#23637;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#21644;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#26469;&#35299;&#20915;&#19968;&#33268;&#24615;&#35268;&#21017;&#24341;&#21457;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#20998;&#26512;&#20010;&#24615;&#21270;&#28608;&#21169;&#22330;&#26223;&#20013;&#30340;&#28508;&#22312;&#32467;&#26524;&#26102;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#29420;&#31435;&#28508;&#22312;&#22122;&#22768;&#26465;&#20214;&#65292;&#21487;&#20197;&#25552;&#39640;&#35299;&#20915;Layer 3&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.09323</link><description>&lt;p&gt;
&#29992;DiscoSCMs&#22238;&#31572;Layer 3&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
Answering Layer 3 queries with DiscoSCMs. (arXiv:2309.09323v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiscoSCMs&#65292;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#22240;&#26524;&#26597;&#35810;&#30340;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#25193;&#23637;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#21644;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#26469;&#35299;&#20915;&#19968;&#33268;&#24615;&#35268;&#21017;&#24341;&#21457;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#20998;&#26512;&#20010;&#24615;&#21270;&#28608;&#21169;&#22330;&#26223;&#20013;&#30340;&#28508;&#22312;&#32467;&#26524;&#26102;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#29420;&#31435;&#28508;&#22312;&#22122;&#22768;&#26465;&#20214;&#65292;&#21487;&#20197;&#25552;&#39640;&#35299;&#20915;Layer 3&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20195;&#22240;&#26524;&#25512;&#26029;&#30740;&#31350;&#20013;&#65292;&#35299;&#20915;Pearl&#22240;&#26524;&#23618;&#27425;&#65288;PCH&#65289;&#19979;&#30340;&#20851;&#32852;&#12289;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#30340;&#22240;&#26524;&#26597;&#35810;&#26159;&#19968;&#20010;&#26680;&#24515;&#20219;&#21153;&#12290;&#26412;&#25991;&#38024;&#23545;&#19968;&#33268;&#24615;&#35268;&#21017;&#24341;&#21457;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#20998;&#24067;&#19968;&#33268;&#24615;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;DiscoSCMs&#65289;&#65292;&#25193;&#23637;&#20102;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#21644;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#12290;&#20197;&#20010;&#24615;&#21270;&#28608;&#21169;&#22330;&#26223;&#20013;&#28508;&#22312;&#32467;&#26524;&#30340;&#30456;&#20851;&#27169;&#24335;$P(y_x, y'_{x'})$&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#23613;&#31649;&#21453;&#20107;&#23454;&#19981;&#20877;&#36864;&#21270;&#65292;&#20294;&#20173;&#26080;&#27861;&#30830;&#23450;&#12290;&#22240;&#27492;&#65292;&#23558;&#29420;&#31435;&#28508;&#22312;&#22122;&#22768;&#26465;&#20214;&#32435;&#20837;DiscoSCM&#12290;&#21457;&#29616;&#36890;&#36807;&#36866;&#24212;&#20998;&#24067;&#30340;&#23884;&#20837;&#24335;&#25512;&#26029;&#65292;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#35299;&#20915;Layer 3&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Addressing causal queries across the Pearl Causal Hierarchy (PCH) (i.e., associational, interventional and counterfactual), which is formalized as \Layer{} Valuations, is a central task in contemporary causal inference research. Counterfactual questions, in particular, pose a significant challenge as they often necessitate a complete knowledge of structural equations. This paper identifies \textbf{the degeneracy problem} caused by the consistency rule. To tackle this, the \textit{Distribution-consistency Structural Causal Models} (DiscoSCMs) is introduced, which extends both the structural causal models (SCM) and the potential outcome framework. The correlation pattern of potential outcomes in personalized incentive scenarios, described by $P(y_x, y'_{x'})$, is used as a case study for elucidation. Although counterfactuals are no longer degenerate, they remain indeterminable. As a result, the condition of independent potential noise is incorporated into DiscoSCM. It is found that by ad
&lt;/p&gt;</description></item><item><title>chatGPT ADA&#26159;&#19968;&#31181;&#33021;&#22815;&#33258;&#20027;&#24320;&#21457;&#20020;&#24202;&#30740;&#31350;&#25152;&#38656;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#23558;&#39640;&#32423;&#20998;&#26512;&#24037;&#20855;&#27665;&#20027;&#21270;&#65292;&#20351;&#38750;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.14120</link><description>&lt;p&gt;
&#25480;&#26435;&#20020;&#24202;&#21307;&#29983;&#24182;&#27665;&#20027;&#21270;&#25968;&#25454;&#31185;&#23398;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#20020;&#24202;&#30740;&#31350;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290; (arXiv:2308.14120v2 [cs.LG] &#26356;&#26032;&#29256;)
&lt;/p&gt;
&lt;p&gt;
Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies. (arXiv:2308.14120v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14120
&lt;/p&gt;
&lt;p&gt;
chatGPT ADA&#26159;&#19968;&#31181;&#33021;&#22815;&#33258;&#20027;&#24320;&#21457;&#20020;&#24202;&#30740;&#31350;&#25152;&#38656;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#23558;&#39640;&#32423;&#20998;&#26512;&#24037;&#20855;&#27665;&#20027;&#21270;&#65292;&#20351;&#38750;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24320;&#21457;&#32773;&#65288;&#22914;&#25968;&#25454;&#31185;&#23398;&#23478;&#65289;&#21644;&#20174;&#19994;&#32773;&#65288;&#22914;&#20020;&#24202;&#21307;&#29983;&#65289;&#20043;&#38388;&#23384;&#22312;&#30693;&#35782;&#24046;&#36317;&#65292;&#38459;&#30861;&#20102;ML&#22312;&#20020;&#24202;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;chatGPT Advanced Data Analysis&#65288;ADA&#65289;&#65292;&#21363;GPT-4&#30340;&#25193;&#23637;&#65292;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#24182;&#39640;&#25928;&#25191;&#34892;ML&#20998;&#26512;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21521;chatGPT ADA&#25552;&#20379;&#20102;&#21508;&#31181;&#21307;&#23398;&#19987;&#19994;&#30340;&#22823;&#22411;&#35797;&#39564;&#30340;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#21644;&#30740;&#31350;&#35814;&#32454;&#20449;&#24687;&#65292;&#27809;&#26377;&#32473;&#20986;&#20855;&#20307;&#25351;&#23548;&#12290;ChatGPT ADA&#22522;&#20110;&#21407;&#22987;&#30740;&#31350;&#30340;&#35757;&#32451;&#25968;&#25454;&#33258;&#20027;&#24320;&#21457;&#20102;&#26368;&#20808;&#36827;&#30340;ML&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20020;&#24202;&#32467;&#26524;&#65292;&#22914;&#30284;&#30151;&#21457;&#23637;&#12289;&#30284;&#30151;&#36827;&#23637;&#12289;&#30142;&#30149;&#24182;&#21457;&#30151;&#25110;&#33268;&#30149;&#22522;&#22240;&#24207;&#21015;&#31561;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#20123;ML&#27169;&#22411;&#19982;&#20854;&#24050;&#21457;&#34920;&#30340;&#23545;&#24212;&#29289;&#30456;&#21305;&#37197;&#29978;&#33267;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;chatGPT ADA&#20026;&#27665;&#20027;&#21270;&#21307;&#23398;&#20013;&#30340;ML&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#65292;&#20351;&#38750;ML&#19987;&#23478;&#33021;&#22815;&#33719;&#24471;&#20808;&#36827;&#30340;&#20998;&#26512;&#24037;&#20855;&#24182;&#25512;&#21160;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A knowledge gap persists between Machine Learning (ML) developers (e.g., data scientists) and practitioners (e.g., clinicians), hampering the full utilization of ML for clinical data analysis. We investigated the potential of the chatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this gap and perform ML analyses efficiently. Real-world clinical datasets and study details from large trials across various medical specialties were presented to chatGPT ADA without specific guidance. ChatGPT ADA autonomously developed state-of-the-art ML models based on the original study's training data to predict clinical outcomes such as cancer development, cancer progression, disease complications, or biomarkers such as pathogenic gene sequences. Strikingly, these ML models matched or outperformed their published counterparts. We conclude that chatGPT ADA offers a promising avenue to democratize ML in medicine, making advanced analytics accessible to non-ML experts and promoting broa
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;&#38750;&#21307;&#23398;&#22270;&#20687;&#19978;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#39044;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#20110;ImageNet&#30340;&#39044;&#35757;&#32451;&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#21307;&#30103;AI&#27169;&#22411;&#20013;&#22686;&#24378;&#32593;&#32476;&#21021;&#22987;&#21270;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2308.07688</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#30340;&#33258;&#28982;&#22270;&#20687;&#22686;&#24378;&#21307;&#30103;AI&#27169;&#22411;&#30340;&#32593;&#32476;&#21021;&#22987;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhancing Network Initialization for Medical AI Models Using Large-Scale, Unlabeled Natural Images. (arXiv:2308.07688v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07688
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;&#38750;&#21307;&#23398;&#22270;&#20687;&#19978;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#39044;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#20110;ImageNet&#30340;&#39044;&#35757;&#32451;&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#21307;&#30103;AI&#27169;&#22411;&#20013;&#22686;&#24378;&#32593;&#32476;&#21021;&#22987;&#21270;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65288;&#22914;ImageNet&#65289;&#24050;&#25104;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#40644;&#37329;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#30340;&#20986;&#29616;&#25552;&#20379;&#20102;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#23398;&#20064;&#24378;&#22823;&#29305;&#24449;&#30340;&#26426;&#20250;&#65292;&#20174;&#32780;&#21487;&#20197;&#32469;&#36807;&#32321;&#37325;&#30340;&#26631;&#27880;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;SSL&#39044;&#35757;&#32451;&#22312;&#38750;&#21307;&#23398;&#22270;&#20687;&#19978;&#26159;&#21542;&#21487;&#20197;&#24212;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#65292;&#24182;&#19982;&#38750;&#21307;&#23398;&#22270;&#20687;&#21644;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21033;&#29992;&#35270;&#35273;&#21464;&#25442;&#22120;&#65292;&#24182;&#26681;&#25454;&#20197;&#19979;&#26041;&#24335;&#21021;&#22987;&#21270;&#20854;&#26435;&#37325;&#65306;&#65288;i&#65289;&#22522;&#20110;&#33258;&#28982;&#22270;&#20687;&#30340;SSL&#39044;&#35757;&#32451;&#65288;DINOv2&#65289;&#12289;&#65288;ii&#65289;&#22522;&#20110;&#33258;&#28982;&#22270;&#20687;&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#65288;ImageNet&#25968;&#25454;&#38598;&#65289;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#22522;&#20110;MIMIC-CXR&#25968;&#25454;&#24211;&#20013;&#30340;&#33016;&#37096;X&#23556;&#32447;&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#20845;&#20010;&#20840;&#29699;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;800,000&#22810;&#24352;&#33016;&#37096;X&#23556;&#32447;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35786;&#26029;&#20102;20&#22810;&#31181;&#19981;&#21516;&#30340;&#24433;&#20687;&#25152;&#35265;&#12290;&#25105;&#20204;&#30340;SSL&#39044;&#35757;&#32451;&#22312;&#32463;&#36807;&#31579;&#36873;&#30340;&#22270;&#20687;&#19978;&#19981;&#20165;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#36229;&#36807;&#20102;&#22522;&#20110;ImageNet&#30340;&#39044;&#35757;&#32451;&#65288;&#23545;&#25152;&#26377;&#25968;&#25454;&#38598;&#65292;P&lt;0.001&#65289;&#65292;&#32780;&#19988;&#22312;&#26576;&#20123;&#25968;&#25454;&#38598;&#19978;&#36824;&#36229;&#36807;&#20102;&#22522;&#20110;&#21307;&#23398;&#22270;&#20687;&#30340;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training datasets, like ImageNet, have become the gold standard in medical image analysis. However, the emergence of self-supervised learning (SSL), which leverages unlabeled data to learn robust features, presents an opportunity to bypass the intensive labeling process. In this study, we explored if SSL for pre-training on non-medical images can be applied to chest radiographs and how it compares to supervised pre-training on non-medical images and on medical images. We utilized a vision transformer and initialized its weights based on (i) SSL pre-training on natural images (DINOv2), (ii) SL pre-training on natural images (ImageNet dataset), and (iii) SL pre-training on chest radiographs from the MIMIC-CXR database. We tested our approach on over 800,000 chest radiographs from six large global datasets, diagnosing more than 20 different imaging findings. Our SSL pre-training on curated images not only outperformed ImageNet-based pre-training (P&lt;0.001 for all datasets) but, in cert
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22312;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#20013;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#12290;&#19982;&#20854;&#20182;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;DRL&#26041;&#27861;&#22312;MAPF&#20013;&#30340;&#25972;&#21512;&#65292;&#24182;&#35299;&#20915;&#20102;MAPF&#35299;&#20915;&#26041;&#26696;&#35780;&#20272;&#25351;&#26631;&#32570;&#20047;&#32479;&#19968;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;DRL&#20316;&#20026;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;MAPF&#24403;&#21069;&#25361;&#25112;&#25152;&#38656;&#30340;&#22522;&#30784;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.05893</link><description>&lt;p&gt;
&#23398;&#20064;&#22522;&#20110;&#22242;&#38431;&#23548;&#33322;&#65306;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#22312;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#20013;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Learning to Team-Based Navigation: A Review of Deep Reinforcement Learning Techniques for Multi-Agent Pathfinding. (arXiv:2308.05893v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22312;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#20013;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#12290;&#19982;&#20854;&#20182;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;DRL&#26041;&#27861;&#22312;MAPF&#20013;&#30340;&#25972;&#21512;&#65292;&#24182;&#35299;&#20915;&#20102;MAPF&#35299;&#20915;&#26041;&#26696;&#35780;&#20272;&#25351;&#26631;&#32570;&#20047;&#32479;&#19968;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;DRL&#20316;&#20026;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;MAPF&#24403;&#21069;&#25361;&#25112;&#25152;&#38656;&#30340;&#22522;&#30784;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;(MAPF)&#26159;&#35768;&#22810;&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#39046;&#22495;&#65292;&#36890;&#24120;&#26159;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#22522;&#26412;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#21644;&#25317;&#25380;&#30340;&#29615;&#22659;&#20013;&#65292;MAPF&#30340;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#24050;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#38477;&#20302;&#12290;&#19982;&#20854;&#20182;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#26412;&#32508;&#36848;&#35770;&#25991;&#20013;&#37325;&#28857;&#20171;&#32461;&#20102;DRL&#26041;&#27861;&#22312;MAPF&#20013;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26088;&#22312;&#22635;&#34917;&#30446;&#21069;&#22312;&#35780;&#20272;MAPF&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#32570;&#21475;&#65292;&#36890;&#36807;&#35299;&#20915;&#32570;&#20047;&#32479;&#19968;&#35780;&#20272;&#25351;&#26631;&#30340;&#38382;&#39064;&#24182;&#23545;&#36825;&#20123;&#25351;&#26631;&#36827;&#34892;&#20840;&#38754;&#38416;&#37322;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#35752;&#35770;&#20102;&#20316;&#20026;&#26410;&#26469;&#26041;&#21521;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;DRL&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#24517;&#35201;&#30340;&#22522;&#30784;&#29702;&#35299;&#20197;&#24212;&#23545;MAPF&#20013;&#30340;&#24403;&#21069;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent pathfinding (MAPF) is a critical field in many large-scale robotic applications, often being the fundamental step in multi-agent systems. The increasing complexity of MAPF in complex and crowded environments, however, critically diminishes the effectiveness of existing solutions. In contrast to other studies that have either presented a general overview of the recent advancements in MAPF or extensively reviewed Deep Reinforcement Learning (DRL) within multi-agent system settings independently, our work presented in this review paper focuses on highlighting the integration of DRL-based approaches in MAPF. Moreover, we aim to bridge the current gap in evaluating MAPF solutions by addressing the lack of unified evaluation metrics and providing comprehensive clarification on these metrics. Finally, our paper discusses the potential of model-based DRL as a promising future direction and provides its required foundational understanding to address current challenges in MAPF. Our o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#21704;&#23494;&#39039;&#31995;&#32479;&#65292;&#21033;&#29992;&#25552;&#21319;&#20551;&#35774;&#65292;&#36890;&#36807;&#24378;&#21046;&#23454;&#26045;&#21704;&#23494;&#39039;&#32467;&#26500;&#21644;&#20351;&#29992;&#36763;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22312;&#21464;&#25442;&#30340;&#22352;&#26631;&#31995;&#19979;&#20855;&#26377;&#21704;&#23494;&#39039;&#32467;&#26500;&#30340;&#20108;&#27425;&#21160;&#21147;&#23398;&#65292;&#20445;&#25345;&#31995;&#32479;&#30340;&#38271;&#26399;&#31283;&#23450;&#24615;&#21644;&#36739;&#20302;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.01084</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#30340;&#38750;&#32447;&#24615;&#21704;&#23494;&#39039;&#31995;&#32479;&#20108;&#27425;&#36763;&#34920;&#31034;&#30340;&#25968;&#25454;&#39537;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Identification of Quadratic Symplectic Representations of Nonlinear Hamiltonian Systems. (arXiv:2308.01084v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#21704;&#23494;&#39039;&#31995;&#32479;&#65292;&#21033;&#29992;&#25552;&#21319;&#20551;&#35774;&#65292;&#36890;&#36807;&#24378;&#21046;&#23454;&#26045;&#21704;&#23494;&#39039;&#32467;&#26500;&#21644;&#20351;&#29992;&#36763;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22312;&#21464;&#25442;&#30340;&#22352;&#26631;&#31995;&#19979;&#20855;&#26377;&#21704;&#23494;&#39039;&#32467;&#26500;&#30340;&#20108;&#27425;&#21160;&#21147;&#23398;&#65292;&#20445;&#25345;&#31995;&#32479;&#30340;&#38271;&#26399;&#31283;&#23450;&#24615;&#21644;&#36739;&#20302;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#25968;&#25454;&#23398;&#20064;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#26694;&#26550;&#12290;&#36825;&#39033;&#24037;&#20316;&#22522;&#20110;&#25552;&#21319;&#20551;&#35774;&#65292;&#21363;&#38750;&#32447;&#24615;&#21704;&#23494;&#39039;&#31995;&#32479;&#21487;&#20197;&#20889;&#25104;&#20855;&#26377;&#31435;&#26041;&#21704;&#23494;&#39039;&#37327;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24471;&#21040;&#22312;&#21464;&#25442;&#30340;&#22352;&#26631;&#31995;&#19979;&#20855;&#26377;&#21704;&#23494;&#39039;&#32467;&#26500;&#30340;&#20108;&#27425;&#21160;&#21147;&#23398;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#24191;&#20041;&#20301;&#32622;&#21644;&#21160;&#37327;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20108;&#27425;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#36763;&#33258;&#32534;&#30721;&#22120;&#24378;&#21046;&#23454;&#26045;&#21704;&#23494;&#39039;&#32467;&#26500;&#12290;&#24378;&#21046;&#23454;&#26045;&#30340;&#21704;&#23494;&#39039;&#32467;&#26500;&#34920;&#29616;&#20986;&#31995;&#32479;&#30340;&#38271;&#26399;&#31283;&#23450;&#24615;&#65292;&#32780;&#31435;&#26041;&#21704;&#23494;&#39039;&#20989;&#25968;&#25552;&#20379;&#20102;&#30456;&#23545;&#36739;&#20302;&#30340;&#27169;&#22411;&#22797;&#26434;&#24615;&#12290;&#23545;&#20110;&#20302;&#32500;&#25968;&#25454;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#39640;&#38454;&#21464;&#25442;&#30340;&#22352;&#26631;&#31995;&#65292;&#32780;&#23545;&#20110;&#39640;&#32500;&#25968;&#25454;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#19968;&#20010;&#20855;&#26377;&#25152;&#38656;&#29305;&#24615;&#30340;&#20302;&#38454;&#22352;&#26631;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#20302;&#32500;&#21644;&#39640;&#32500;&#30340;&#38750;&#32447;&#24615;&#21704;&#23494;&#39039;&#31995;&#32479;&#31034;&#20363;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a framework for learning Hamiltonian systems using data. This work is based on the lifting hypothesis, which posits that nonlinear Hamiltonian systems can be written as nonlinear systems with cubic Hamiltonians. By leveraging this, we obtain quadratic dynamics that are Hamiltonian in a transformed coordinate system. To that end, for given generalized position and momentum data, we propose a methodology to learn quadratic dynamical systems, enforcing the Hamiltonian structure in combination with a symplectic auto-encoder. The enforced Hamiltonian structure exhibits long-term stability of the system, while the cubic Hamiltonian function provides relatively low model complexity. For low-dimensional data, we determine a higher-order transformed coordinate system, whereas, for high-dimensional data, we find a lower-order coordinate system with the desired properties. We demonstrate the proposed methodology by means of both low-dimensional and high-dimensional nonlinear Hamiltonia
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30740;&#31350;&#38382;&#39064;&#12290;&#20316;&#32773;&#20027;&#35201;&#35752;&#35770;&#20102;&#19977;&#31181;&#24212;&#29992;&#31867;&#22411;&#65306;&#26080;&#29305;&#23450;&#30446;&#26631;&#30340;&#29983;&#25104;&#26041;&#24335;&#12289;&#21516;&#26102;&#26368;&#22823;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#36755;&#20986;&#29983;&#25104;&#26041;&#24335;&#20197;&#21450;&#23558;&#26080;&#27861;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#25429;&#25417;&#30340;&#26399;&#26395;&#29305;&#24449;&#23884;&#20837;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#24335;&#12290;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#20013;&#23384;&#22312;&#30528;&#20016;&#23500;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.00031</link><description>&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30740;&#31350;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges. (arXiv:2308.00031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00031
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30740;&#31350;&#38382;&#39064;&#12290;&#20316;&#32773;&#20027;&#35201;&#35752;&#35770;&#20102;&#19977;&#31181;&#24212;&#29992;&#31867;&#22411;&#65306;&#26080;&#29305;&#23450;&#30446;&#26631;&#30340;&#29983;&#25104;&#26041;&#24335;&#12289;&#21516;&#26102;&#26368;&#22823;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#36755;&#20986;&#29983;&#25104;&#26041;&#24335;&#20197;&#21450;&#23558;&#26080;&#27861;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#25429;&#25417;&#30340;&#26399;&#26395;&#29305;&#24449;&#23884;&#20837;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#24335;&#12290;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#20013;&#23384;&#22312;&#30528;&#20016;&#23500;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26159;&#36817;&#21313;&#24180;&#26469;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#26368;&#20196;&#20154;&#20852;&#22859;&#30340;&#21457;&#23637;&#20043;&#19968;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24050;&#32463;&#25104;&#20026;&#38750;&#24120;&#25104;&#21151;&#30340;&#33539;&#24335;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;RL&#24212;&#29992;&#20110;&#29983;&#25104;AI&#20013;&#30340;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#35752;&#35770;&#19977;&#31181;&#24212;&#29992;&#31867;&#22411;&#65292;&#21363;&#20316;&#20026;&#19968;&#31181;&#26080;&#29305;&#23450;&#30446;&#26631;&#30340;&#29983;&#25104;&#26041;&#24335;&#65292;&#20316;&#20026;&#19968;&#31181;&#21516;&#26102;&#26368;&#22823;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#36755;&#20986;&#29983;&#25104;&#26041;&#24335;&#65292;&#20197;&#21450;&#20316;&#20026;&#19968;&#31181;&#23558;&#26080;&#27861;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#36731;&#26494;&#25429;&#25417;&#30340;&#26399;&#26395;&#29305;&#24449;&#23884;&#20837;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#22312;&#35843;&#26597;&#32467;&#26524;&#20013;&#23545;&#36825;&#20010;&#36855;&#20154;&#30340;&#26032;&#20852;&#39046;&#22495;&#20013;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence (AI) is one of the most exciting developments in Computer Science of the last decade. At the same time, Reinforcement Learning (RL) has emerged as a very successful paradigm for a variety of machine learning tasks. In this survey, we discuss the state of the art, opportunities and open research questions in applying RL to generative AI. In particular, we will discuss three types of applications, namely, RL as an alternative way for generation without specified objectives; as a way for generating outputs while concurrently maximizing an objective function; and, finally, as a way of embedding desired characteristics, which cannot be easily captured by means of an objective function, into the generative process. We conclude the survey with an in-depth discussion of the opportunities and challenges in this fascinating emerging area.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#25968;&#25454;-&#20379;&#24212;&#38142;&#31649;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#20379;&#24212;&#38142;&#39044;&#27979;&#65292;&#20248;&#21270;&#25805;&#20316;&#31649;&#29702;&#12289;&#36879;&#26126;&#24230;&#65292;&#24182;&#35752;&#35770;&#20102;&#24187;&#24433;&#24211;&#23384;&#23545;&#39044;&#27979;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.12971</link><description>&lt;p&gt;
&#22823;&#25968;&#25454;-&#20379;&#24212;&#38142;&#31649;&#29702;&#26694;&#26550;&#30340;&#39044;&#27979;&#65306;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Big Data - Supply Chain Management Framework for Forecasting: Data Preprocessing and Machine Learning Techniques. (arXiv:2307.12971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#25968;&#25454;-&#20379;&#24212;&#38142;&#31649;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#20379;&#24212;&#38142;&#39044;&#27979;&#65292;&#20248;&#21270;&#25805;&#20316;&#31649;&#29702;&#12289;&#36879;&#26126;&#24230;&#65292;&#24182;&#35752;&#35770;&#20102;&#24187;&#24433;&#24211;&#23384;&#23545;&#39044;&#27979;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#31995;&#32479;&#22320;&#35782;&#21035;&#21644;&#27604;&#36739;&#20998;&#26512;&#26368;&#20808;&#36827;&#30340;&#20379;&#24212;&#38142;&#39044;&#27979;&#31574;&#30053;&#21644;&#25216;&#26415;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#25968;&#25454;&#20998;&#26512;&#24212;&#29992;&#20110;&#20379;&#24212;&#38142;&#31649;&#29702;&#20013;&#65292;&#21253;&#25324;&#38382;&#39064;&#35782;&#21035;&#12289;&#25968;&#25454;&#26469;&#28304;&#12289;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#12289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#12289;&#36229;&#21442;&#25968;&#35843;&#20248;&#12289;&#24615;&#33021;&#35780;&#20272;&#21644;&#20248;&#21270;&#65292;&#20197;&#21450;&#39044;&#27979;&#23545;&#20154;&#21147;&#12289;&#24211;&#23384;&#21644;&#25972;&#20010;&#20379;&#24212;&#38142;&#30340;&#24433;&#21709;&#12290;&#39318;&#20808;&#35752;&#35770;&#20102;&#26681;&#25454;&#20379;&#24212;&#38142;&#31574;&#30053;&#25910;&#38598;&#25968;&#25454;&#30340;&#38656;&#27714;&#20197;&#21450;&#22914;&#20309;&#25910;&#38598;&#25968;&#25454;&#12290;&#25991;&#31456;&#35752;&#35770;&#20102;&#26681;&#25454;&#21608;&#26399;&#25110;&#20379;&#24212;&#38142;&#30446;&#26631;&#38656;&#35201;&#19981;&#21516;&#31867;&#22411;&#30340;&#39044;&#27979;&#12290;&#25512;&#33616;&#20351;&#29992;&#20379;&#24212;&#38142;&#32489;&#25928;&#25351;&#26631;&#21644;&#35823;&#24046;&#27979;&#37327;&#31995;&#32479;&#26469;&#20248;&#21270;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#12290;&#36824;&#35752;&#35770;&#20102;&#24187;&#24433;&#24211;&#23384;&#23545;&#39044;&#27979;&#30340;&#19981;&#21033;&#24433;&#21709;&#20197;&#21450;&#31649;&#29702;&#20915;&#31574;&#20381;&#36182;&#20379;&#24212;&#38142;&#32489;&#25928;&#25351;&#26631;&#26469;&#30830;&#23450;&#27169;&#22411;&#24615;&#33021;&#21442;&#25968;&#21644;&#25913;&#36827;&#36816;&#33829;&#31649;&#29702;&#12289;&#36879;&#26126;&#24230;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article intends to systematically identify and comparatively analyze state-of-the-art supply chain (SC) forecasting strategies and technologies. A novel framework has been proposed incorporating Big Data Analytics in SC Management (problem identification, data sources, exploratory data analysis, machine-learning model training, hyperparameter tuning, performance evaluation, and optimization), forecasting effects on human-workforce, inventory, and overall SC. Initially, the need to collect data according to SC strategy and how to collect them has been discussed. The article discusses the need for different types of forecasting according to the period or SC objective. The SC KPIs and the error-measurement systems have been recommended to optimize the top-performing model. The adverse effects of phantom inventory on forecasting and the dependence of managerial decisions on the SC KPIs for determining model performance parameters and improving operations management, transparency, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;Wasserstein&#21464;&#20998;&#20248;&#21270;&#65288;AWaVO&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#24418;&#24335;&#26041;&#27861;&#35299;&#20915;&#20102;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#35299;&#37322;&#21644;&#36879;&#26126;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22870;&#21169;&#35774;&#35745;&#21644;&#31574;&#30053;&#25910;&#25947;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2307.07084</link><description>&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;Wasserstein&#21464;&#20998;&#25512;&#29702;&#65306;&#21487;&#35299;&#37322;&#24615;&#30340;&#24418;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Safe Reinforcement Learning as Wasserstein Variational Inference: Formal Methods for Interpretability. (arXiv:2307.07084v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;Wasserstein&#21464;&#20998;&#20248;&#21270;&#65288;AWaVO&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#24418;&#24335;&#26041;&#27861;&#35299;&#20915;&#20102;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#35299;&#37322;&#21644;&#36879;&#26126;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22870;&#21169;&#35774;&#35745;&#21644;&#31574;&#30053;&#25910;&#25947;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#25110;&#26368;&#20248;&#25511;&#21046;&#21487;&#20197;&#20026;&#20855;&#26377;&#21487;&#21464;&#21160;&#24577;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#25552;&#20379;&#26377;&#25928;&#30340;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#23454;&#26045;&#20013;&#65292;&#35299;&#37322;&#22870;&#21169;&#20989;&#25968;&#21644;&#30456;&#24212;&#30340;&#26368;&#20248;&#31574;&#30053;&#19968;&#30452;&#26159;&#19968;&#20010;&#25345;&#20037;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#23558;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#25512;&#29702;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#65292;&#22240;&#20026;&#27010;&#29575;&#25512;&#29702;&#21407;&#21017;&#19978;&#25552;&#20379;&#20102;&#22810;&#26679;&#19988;&#24378;&#22823;&#30340;&#25968;&#23398;&#24037;&#20855;&#26469;&#25512;&#26029;&#38543;&#26426;&#21160;&#24577;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#22870;&#21169;&#35774;&#35745;&#21644;&#31574;&#30053;&#25910;&#25947;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;Wasserstein&#21464;&#20998;&#20248;&#21270;&#65288;AWaVO&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#24418;&#24335;&#26041;&#27861;&#26469;&#35299;&#37322;&#22870;&#21169;&#35774;&#35745;&#65292;&#36879;&#26126;&#22320;&#35757;&#32451;&#25910;&#25947;&#65292;&#20197;&#21450;&#23545;&#39034;&#24207;&#20915;&#31574;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;&#20026;&#20102;&#35777;&#26126;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25910;&#25947;&#35757;&#32451;&#24182;&#20445;&#35777;&#20102;&#25910;&#25947;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning or optimal control can provide effective reasoning for sequential decision-making problems with variable dynamics. Such reasoning in practical implementation, however, poses a persistent challenge in interpreting the reward function and corresponding optimal policy. Consequently, formalizing the sequential decision-making problems as inference has a considerable value, as probabilistic inference in principle offers diverse and powerful mathematical tools to infer the stochastic dynamics whilst suggesting a probabilistic interpretation of the reward design and policy convergence. In this study, we propose a novel Adaptive Wasserstein Variational Optimization (AWaVO) to tackle these challenges in sequential decision-making. Our approach utilizes formal methods to provide interpretations of reward design, transparency of training convergence, and probabilistic interpretation of sequential decisions. To demonstrate practicality, we show convergent training with guara
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#22312;&#26631;&#20934;&#27491;&#24577;&#21327;&#21464;&#37327;&#19979;&#30340;&#21442;&#25968;&#20272;&#35745;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#21457;&#29616;&#26679;&#26412;&#22797;&#26434;&#24230;&#26354;&#32447;&#22312;&#36870;&#28201;&#24230;&#26041;&#38754;&#26377;&#20004;&#20010;&#36716;&#25240;&#28857;&#65292;&#26126;&#30830;&#21010;&#20998;&#20102;&#20302;&#12289;&#20013;&#21644;&#39640;&#28201;&#24230;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2307.04191</link><description>&lt;p&gt;
&#20851;&#20110;&#36923;&#36753;&#22238;&#24402;&#20013;&#21442;&#25968;&#20272;&#35745;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the sample complexity of estimation in logistic regression. (arXiv:2307.04191v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#22312;&#26631;&#20934;&#27491;&#24577;&#21327;&#21464;&#37327;&#19979;&#30340;&#21442;&#25968;&#20272;&#35745;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#21457;&#29616;&#26679;&#26412;&#22797;&#26434;&#24230;&#26354;&#32447;&#22312;&#36870;&#28201;&#24230;&#26041;&#38754;&#26377;&#20004;&#20010;&#36716;&#25240;&#28857;&#65292;&#26126;&#30830;&#21010;&#20998;&#20102;&#20302;&#12289;&#20013;&#21644;&#39640;&#28201;&#24230;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#26159;&#22122;&#22768;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#20043;&#19968;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26631;&#20934;&#27491;&#24577;&#21327;&#21464;&#37327;&#19979;&#65292;&#20197;$\ell_2$&#35823;&#24046;&#20026;&#38480;&#65292;&#20272;&#35745;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#21442;&#25968;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#32771;&#34385;&#20102;&#32500;&#24230;&#21644;&#36870;&#28201;&#24230;&#30340;&#24433;&#21709;&#12290;&#36870;&#28201;&#24230;&#25511;&#21046;&#20102;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#20449;&#22122;&#27604;&#12290;&#34429;&#28982;&#36923;&#36753;&#22238;&#24402;&#30340;&#24191;&#20041;&#30028;&#38480;&#21644;&#28176;&#36817;&#24615;&#33021;&#24050;&#32463;&#26377;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#20294;&#20851;&#20110;&#21442;&#25968;&#20272;&#35745;&#30340;&#38750;&#28176;&#36817;&#26679;&#26412;&#22797;&#26434;&#24230;&#22312;&#20043;&#21069;&#30340;&#20998;&#26512;&#20013;&#27809;&#26377;&#35752;&#35770;&#20854;&#19982;&#35823;&#24046;&#21644;&#36870;&#28201;&#24230;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#26354;&#32447;&#22312;&#36870;&#28201;&#24230;&#26041;&#38754;&#20855;&#26377;&#20004;&#20010;&#36716;&#25240;&#28857;&#65288;&#25110;&#20020;&#30028;&#28857;&#65289;&#65292;&#26126;&#30830;&#21010;&#20998;&#20102;&#20302;&#12289;&#20013;&#21644;&#39640;&#28201;&#24230;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
The logistic regression model is one of the most popular data generation model in noisy binary classification problems. In this work, we study the sample complexity of estimating the parameters of the logistic regression model up to a given $\ell_2$ error, in terms of the dimension and the inverse temperature, with standard normal covariates. The inverse temperature controls the signal-to-noise ratio of the data generation process. While both generalization bounds and asymptotic performance of the maximum-likelihood estimator for logistic regression are well-studied, the non-asymptotic sample complexity that shows the dependence on error and the inverse temperature for parameter estimation is absent from previous analyses. We show that the sample complexity curve has two change-points (or critical points) in terms of the inverse temperature, clearly separating the low, moderate, and high temperature regimes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#21327;&#20316;&#31185;&#23398;&#20013;&#20351;&#29992;&#28608;&#21169;&#29702;&#35770;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#30740;&#31350;&#32773;&#21644;&#20915;&#31574;&#32773;&#30340;&#19981;&#21516;&#28608;&#21169;&#26426;&#21046;&#65292;&#21033;&#29992;&#20195;&#29702;&#20154;&#30340;&#25112;&#30053;&#34892;&#20026;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2307.03748</link><description>&lt;p&gt;
&#28608;&#21169;&#29702;&#35770;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#22312;&#21327;&#20316;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Incentive-Theoretic Bayesian Inference for Collaborative Science. (arXiv:2307.03748v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#21327;&#20316;&#31185;&#23398;&#20013;&#20351;&#29992;&#28608;&#21169;&#29702;&#35770;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#30740;&#31350;&#32773;&#21644;&#20915;&#31574;&#32773;&#30340;&#19981;&#21516;&#28608;&#21169;&#26426;&#21046;&#65292;&#21033;&#29992;&#20195;&#29702;&#20154;&#30340;&#25112;&#30053;&#34892;&#20026;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#31185;&#23398;&#30740;&#31350;&#26159;&#19968;&#39033;&#20998;&#24067;&#24335;&#30340;&#12289;&#21327;&#20316;&#30340;&#24037;&#20316;&#65292;&#30001;&#30740;&#31350;&#22242;&#38431;&#12289;&#30417;&#31649;&#26426;&#26500;&#12289;&#36164;&#21161;&#26426;&#26500;&#12289;&#21830;&#19994;&#21512;&#20316;&#20249;&#20276;&#21644;&#31185;&#23398;&#26426;&#26500;&#32452;&#25104;&#65292;&#24444;&#27492;&#20114;&#21160;&#24182;&#38754;&#23545;&#19981;&#21516;&#30340;&#28608;&#21169;&#12290;&#20026;&#20102;&#20445;&#25345;&#31185;&#23398;&#20005;&#35880;&#24615;&#65292;&#32479;&#35745;&#26041;&#27861;&#24212;&#35813;&#35748;&#35782;&#21040;&#36825;&#31181;&#24773;&#20917;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20551;&#35774;&#26816;&#39564;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#26377;&#19968;&#20010;&#20195;&#29702;&#20154;&#65288;&#20363;&#22914;&#30740;&#31350;&#20154;&#21592;&#25110;&#21046;&#33647;&#20844;&#21496;&#65289;&#23545;&#26410;&#30693;&#21442;&#25968;&#25317;&#26377;&#31169;&#20154;&#20808;&#39564;&#30693;&#35782;&#65292;&#36824;&#26377;&#19968;&#20010;&#22996;&#25176;&#20154;&#65288;&#22914;&#25919;&#31574;&#21046;&#23450;&#32773;&#25110;&#30417;&#31649;&#26426;&#26500;&#65289;&#24076;&#26395;&#26681;&#25454;&#21442;&#25968;&#20540;&#20570;&#20986;&#20915;&#31574;&#12290;&#20195;&#29702;&#20154;&#26681;&#25454;&#20182;&#20204;&#30340;&#31169;&#20154;&#20808;&#39564;&#36873;&#25321;&#26159;&#21542;&#36827;&#34892;&#32479;&#35745;&#35797;&#39564;&#65292;&#28982;&#21518;&#35797;&#39564;&#30340;&#32467;&#26524;&#30001;&#22996;&#25176;&#20154;&#29992;&#26469;&#20570;&#20986;&#20915;&#31574;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22996;&#25176;&#20154;&#22914;&#20309;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#65292;&#21033;&#29992;&#20195;&#29702;&#20154;&#30340;&#25112;&#30053;&#34892;&#20026;&#25152;&#36879;&#38706;&#30340;&#20449;&#24687;&#65292;&#20063;&#23601;&#26159;&#20182;&#20204;&#36873;&#25321;&#26159;&#21542;&#36827;&#34892;&#35797;&#39564;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#35745;&#31639;p&#20540;&#65292;&#20174;&#32780;&#32508;&#21512;&#21033;&#29992;&#20195;&#29702;&#20154;&#30340;&#34892;&#20026;&#21644;&#35797;&#39564;&#30340;&#32467;&#26524;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contemporary scientific research is a distributed, collaborative endeavor, carried out by teams of researchers, regulatory institutions, funding agencies, commercial partners, and scientific bodies, all interacting with each other and facing different incentives. To maintain scientific rigor, statistical methods should acknowledge this state of affairs. To this end, we study hypothesis testing when there is an agent (e.g., a researcher or a pharmaceutical company) with a private prior about an unknown parameter and a principal (e.g., a policymaker or regulator) who wishes to make decisions based on the parameter value. The agent chooses whether to run a statistical trial based on their private prior and then the result of the trial is used by the principal to reach a decision. We show how the principal can conduct statistical inference that leverages the information that is revealed by an agent's strategic behavior -- their choice to run a trial or not. In particular, we show how the p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#33410;&#28857;&#35782;&#21035;&#12289;&#23494;&#38598;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#28040;&#24687;&#20256;&#36882;&#23618;&#26469;&#23454;&#29616;&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#39034;&#24207;&#27169;&#22359;&#12290;&#35813;&#27169;&#22411;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01482</link><description>&lt;p&gt;
Nexus sine qua non&#65306;&#22522;&#20110;&#33410;&#28857;&#35782;&#21035;&#30340;&#31070;&#32463;&#32593;&#32476;&#36830;&#25509;&#30340;&#26102;&#31354;&#39044;&#27979;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Nexus sine qua non: Essentially connected neural networks for spatial-temporal forecasting of multivariate time series. (arXiv:2307.01482v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01482
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#33410;&#28857;&#35782;&#21035;&#12289;&#23494;&#38598;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#28040;&#24687;&#20256;&#36882;&#23618;&#26469;&#23454;&#29616;&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#39034;&#24207;&#27169;&#22359;&#12290;&#35813;&#27169;&#22411;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#21644;&#39044;&#27979;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#19981;&#20165;&#26377;&#21161;&#20110;&#20174;&#19994;&#32773;&#30340;&#20915;&#31574;&#65292;&#36824;&#21152;&#28145;&#25105;&#20204;&#23545;&#24213;&#23618;&#21160;&#24577;&#31995;&#32479;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STGNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#24378;&#22823;&#30340;&#39044;&#27979;&#22120;&#65292;&#24182;&#25104;&#20026;&#23398;&#20064;&#26102;&#31354;&#34920;&#31034;&#30340;&#20107;&#23454;&#26631;&#20934;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;STGNNs&#30340;&#26550;&#26500;&#24448;&#24448;&#36890;&#36807;&#22534;&#21472;&#19968;&#31995;&#21015;&#22797;&#26434;&#30340;&#23618;&#27425;&#32780;&#21464;&#24471;&#22797;&#26434;&#12290;&#35774;&#35745;&#30340;&#27169;&#22411;&#21487;&#33021;&#22810;&#20313;&#25110;&#38590;&#20197;&#29702;&#35299;&#65292;&#36825;&#32473;&#22797;&#26434;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#36825;&#20123;&#38382;&#39064;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#29616;&#20195;STGNNs&#30340;&#35774;&#35745;&#65292;&#24182;&#30830;&#23450;&#23545;&#24378;&#22823;&#21644;&#39640;&#25928;&#30340;&#31070;&#32463;&#39044;&#27979;&#22120;&#26377;&#25152;&#36129;&#29486;&#30340;&#26680;&#24515;&#21407;&#21017;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32039;&#20945;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#23436;&#20840;&#30001;&#23494;&#38598;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#28040;&#24687;&#20256;&#36882;&#23618;&#26469;&#23450;&#20041;&#65292;&#22522;&#20110;&#33410;&#28857;&#35782;&#21035;&#65292;&#27809;&#26377;&#20219;&#20309;&#22797;&#26434;&#30340;&#39034;&#24207;&#27169;&#22359;&#65292;&#20363;&#22914;TCNs&#65292;RNNs&#21644;Transformers&#12290;&#36890;&#36807;&#23454;&#35777;&#37325;&#26032;&#35780;&#20272;&#35813;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling and forecasting multivariate time series not only facilitates the decision making of practitioners, but also deepens our scientific understanding of the underlying dynamical systems. Spatial-temporal graph neural networks (STGNNs) are emerged as powerful predictors and have become the de facto models for learning spatiotemporal representations in recent years. However, existing architectures of STGNNs tend to be complicated by stacking a series of fancy layers. The designed models could be either redundant or enigmatic, which pose great challenges on their complexity and scalability. Such concerns prompt us to re-examine the designs of modern STGNNs and identify core principles that contribute to a powerful and efficient neural predictor. Here we present a compact predictive model that is fully defined by a dense encoder-decoder and a message-passing layer, powered by node identifications, without any complex sequential modules, e.g., TCNs, RNNs, and Transformers. Empirical re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AmicroN&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21033;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#33258;&#21160;&#29983;&#25104;&#24494;&#21160;&#20316;&#27880;&#37322;&#65292;&#24357;&#34917;&#29616;&#26377;&#25968;&#25454;&#38598;&#32570;&#20047;&#32454;&#31890;&#24230;&#27880;&#37322;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2306.13149</link><description>&lt;p&gt;
AmicroN&#65306;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#32454;&#31890;&#24230;&#24494;&#21160;&#20316;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#27880;&#37322;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AmicroN: A Framework for Generating Annotations for Human Activity Recognition with Granular Micro-Activities. (arXiv:2306.13149v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AmicroN&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21033;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#33258;&#21160;&#29983;&#25104;&#24494;&#21160;&#20316;&#27880;&#37322;&#65292;&#24357;&#34917;&#29616;&#26377;&#25968;&#25454;&#38598;&#32570;&#20047;&#32454;&#31890;&#24230;&#27880;&#37322;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#23454;&#29616;&#39640;&#25928;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#38656;&#35201;&#22823;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;&#26410;&#26631;&#35760;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#22686;&#38271;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#20154;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#36890;&#24120;&#23548;&#33268;&#25910;&#38598;&#26356;&#27973;&#30340;&#27880;&#37322;&#12290;&#36825;&#20123;&#27973;&#23618;&#27880;&#37322;&#24573;&#30053;&#20102;&#32452;&#25104;&#26085;&#24120;&#29983;&#27963;&#20013;&#20219;&#20309;&#22797;&#26434;&#27963;&#21160;&#30340;&#32454;&#31890;&#24230;&#24494;&#21160;&#20316;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#21487;&#29992;&#30340;&#39044;&#27880;&#37322;&#25968;&#25454;&#38598;&#20013;&#32570;&#20047;&#32454;&#31890;&#24230;&#27880;&#37322;&#30340;&#21407;&#22240;&#21644;&#32570;&#38519;&#65292;&#24182;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#35843;&#26597;&#20197;&#20102;&#35299;&#19982;&#27880;&#37322;&#30456;&#20851;&#30340;&#20154;&#31867;&#24863;&#30693;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;AmicroN&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#20351;&#29992;&#36816;&#21160;&#29305;&#24449;&#21644;&#21487;&#29992;&#30340;&#31895;&#31890;&#24230;&#23439;&#25805;&#20316;&#26631;&#31614;&#33258;&#21160;&#29983;&#25104;&#24494;&#21160;&#20316;&#27880;&#37322;&#12290;&#22312;&#21518;&#21488;&#65292;AmicroN&#24212;&#29992;&#21464;&#28857;&#26816;&#27979;&#65292;&#28982;&#21518;&#36827;&#34892;&#38646;&#26679;&#26412;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient human activity recognition (HAR) using sensor data needs a significant volume of annotated data. The growing volume of unlabelled sensor data has challenged conventional practices for gathering HAR annotations with human-in-the-loop approaches, often leading to the collection of shallower annotations. These shallower annotations ignore the fine-grained micro-activities that constitute any complex activities of daily living (ADL). Understanding this, we, in this paper, first analyze this lack of granular annotations from available pre-annotated datasets to understand the practical inconsistencies and also perform a detailed survey to look into the human perception surrounding annotations. Drawing motivations from these, we next develop the framework AmicroN that can automatically generate micro-activity annotations using locomotive signatures and the available coarse-grain macro-activity labels. In the backend, AmicroN applies change-point detection followed by zero-shot learn
&lt;/p&gt;</description></item><item><title>DDLP&#31639;&#27861;&#20351;&#29992;&#28145;&#24230;&#28508;&#22312;&#31890;&#23376;(DLP)&#34920;&#31034;&#27861;&#23454;&#29616;&#26080;&#30417;&#30563;&#29289;&#20307;&#20013;&#24515;&#35270;&#39057;&#39044;&#27979;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#31639;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#20351;&#24471;&#21487;&#20197;&#36827;&#34892;&#8220;&#20551;&#35774;&#8221;&#29983;&#25104;&#65292;&#32780;DLP&#30340;&#32039;&#20945;&#32467;&#26500;&#20351;&#24471;&#25928;&#29575;&#39640;&#24182;&#21487;&#20197;&#36827;&#34892;&#22522;&#20110;&#25193;&#25955;&#30340;&#26080;&#26465;&#20214;&#35270;&#39057;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.05957</link><description>&lt;p&gt;
DDLP&#65306;&#22522;&#20110;&#28145;&#24230;&#21160;&#24577;&#28508;&#22312;&#31890;&#23376;&#30340;&#26080;&#30417;&#30563;&#29289;&#20307;&#20013;&#24515;&#35270;&#39057;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DDLP: Unsupervised Object-Centric Video Prediction with Deep Dynamic Latent Particles. (arXiv:2306.05957v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05957
&lt;/p&gt;
&lt;p&gt;
DDLP&#31639;&#27861;&#20351;&#29992;&#28145;&#24230;&#28508;&#22312;&#31890;&#23376;(DLP)&#34920;&#31034;&#27861;&#23454;&#29616;&#26080;&#30417;&#30563;&#29289;&#20307;&#20013;&#24515;&#35270;&#39057;&#39044;&#27979;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#31639;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#20351;&#24471;&#21487;&#20197;&#36827;&#34892;&#8220;&#20551;&#35774;&#8221;&#29983;&#25104;&#65292;&#32780;DLP&#30340;&#32039;&#20945;&#32467;&#26500;&#20351;&#24471;&#25928;&#29575;&#39640;&#24182;&#21487;&#20197;&#36827;&#34892;&#22522;&#20110;&#25193;&#25955;&#30340;&#26080;&#26465;&#20214;&#35270;&#39057;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#28508;&#22312;&#31890;&#23376;&#65288;DLP&#65289;&#34920;&#31034;&#30340;&#26032;&#22411;&#29289;&#20307;&#20013;&#24515;&#35270;&#39057;&#39044;&#27979;&#31639;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#27133;&#25110;&#34917;&#19969;&#30340;&#34920;&#31034;&#30456;&#27604;&#65292;DLP&#20351;&#29992;&#19968;&#32452;&#20851;&#38190;&#28857;&#27169;&#25311;&#22330;&#26223;&#65292;&#23398;&#20064;&#21442;&#25968;&#29992;&#20110;&#23646;&#24615;&#20363;&#22914;&#20301;&#32622;&#21644;&#22823;&#23567;&#65292;&#24182;&#19988;&#26082;&#39640;&#25928;&#21448;&#21487;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#8212;&#8212;&#28145;&#24230;&#21160;&#24577;&#28508;&#22312;&#31890;&#23376;(DDLP)&#65292;&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#29289;&#20307;&#20013;&#24515;&#35270;&#39057;&#39044;&#27979;&#32467;&#26524;&#12290;DDLP&#30340;&#21487;&#35299;&#37322;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#25191;&#34892;&#8220;&#20551;&#35774;&#8221;&#29983;&#25104;&#8212;&#8212;&#39044;&#27979;&#26356;&#25913;&#21021;&#22987;&#24103;&#20013;&#23545;&#35937;&#23646;&#24615;&#30340;&#32467;&#26524;&#65292;&#32780;DLP&#30340;&#32039;&#20945;&#32467;&#26500;&#20351;&#24471;&#25928;&#29575;&#39640;&#24182;&#21487;&#20197;&#36827;&#34892;&#22522;&#20110;&#25193;&#25955;&#30340;&#26080;&#26465;&#20214;&#35270;&#39057;&#29983;&#25104;&#12290;&#35270;&#39057;&#12289;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;&#27492;&#38142;&#25509;&#25214;&#21040;&#65306;https://taldatech.github.io/ddlp-web
&lt;/p&gt;
&lt;p&gt;
We propose a new object-centric video prediction algorithm based on the deep latent particle (DLP) representation. In comparison to existing slot- or patch-based representations, DLPs model the scene using a set of keypoints with learned parameters for properties such as position and size, and are both efficient and interpretable. Our method, deep dynamic latent particles (DDLP), yields state-of-the-art object-centric video prediction results on several challenging datasets. The interpretable nature of DDLP allows us to perform ``what-if'' generation -- predict the consequence of changing properties of objects in the initial frames, and DLP's compact structure enables efficient diffusion-based unconditional video generation. Videos, code and pre-trained models are available: https://taldatech.github.io/ddlp-web
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#30456;&#23545;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#29992;&#20110;&#35823;&#20998;&#31867;&#26816;&#27979;&#12290;&#35813;&#24230;&#37327;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#36719;&#39044;&#27979;&#30340;&#20998;&#24067;&#27169;&#24335;&#65292;&#35782;&#21035;&#20986;&#34987;&#35823;&#20998;&#31867;&#30340;&#26679;&#26412;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#23454;&#35777;&#25913;&#36827;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#35823;&#20998;&#31867;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01710</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#30456;&#23545;&#19981;&#30830;&#23450;&#24615;&#27979;&#24230;&#29992;&#20110;&#35823;&#20998;&#31867;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Data-Driven Measure of Relative Uncertainty for Misclassification Detection. (arXiv:2306.01710v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#30456;&#23545;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#29992;&#20110;&#35823;&#20998;&#31867;&#26816;&#27979;&#12290;&#35813;&#24230;&#37327;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#36719;&#39044;&#27979;&#30340;&#20998;&#24067;&#27169;&#24335;&#65292;&#35782;&#21035;&#20986;&#34987;&#35823;&#20998;&#31867;&#30340;&#26679;&#26412;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#23454;&#35777;&#25913;&#36827;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#35823;&#20998;&#31867;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35823;&#20998;&#31867;&#26816;&#27979;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#23427;&#21487;&#20197;&#35782;&#21035;&#27169;&#22411;&#39044;&#27979;&#19981;&#21487;&#38752;&#30340;&#23454;&#20363;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#19981;&#30830;&#23450;&#24615;&#27979;&#24230;&#22914;&#39321;&#20892;&#29109;&#24182;&#19981;&#33021;&#25552;&#20379;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#24335;&#26469;&#25512;&#26029;&#27169;&#22411;&#39044;&#27979;&#30340;&#23454;&#38469;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#30456;&#23545;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#29992;&#20110;&#35823;&#20998;&#31867;&#26816;&#27979;&#12290;&#36890;&#36807;&#23398;&#20064;&#36719;&#39044;&#27979;&#30340;&#20998;&#24067;&#27169;&#24335;&#65292;&#25105;&#20204;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#21487;&#20197;&#22522;&#20110;&#39044;&#27979;&#30340;&#31867;&#27010;&#29575;&#26631;&#35782;&#34987;&#35823;&#20998;&#31867;&#30340;&#26679;&#26412;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26681;&#25454;&#25152;&#25552;&#20986;&#30340;&#24230;&#37327;&#65292;&#19982;&#35823;&#20998;&#31867;&#23454;&#20363;&#23545;&#24212;&#30340;&#36719;&#39044;&#27979;&#21487;&#33021;&#20855;&#26377;&#24456;&#22823;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21363;&#20351;&#23427;&#20204;&#30340;&#39321;&#20892;&#29109;&#21487;&#33021;&#24456;&#20302;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#20010;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#23454;&#35777;&#25913;&#36827;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#35823;&#20998;&#31867;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misclassification detection is an important problem in machine learning, as it allows for the identification of instances where the model's predictions are unreliable. However, conventional uncertainty measures such as Shannon entropy do not provide an effective way to infer the real uncertainty associated with the model's predictions. In this paper, we introduce a novel data-driven measure of relative uncertainty to an observer for misclassification detection. By learning patterns in the distribution of soft-predictions, our uncertainty measure can identify misclassified samples based on the predicted class probabilities. Interestingly, according to the proposed measure, soft-predictions that correspond to misclassified instances can carry a large amount of uncertainty, even though they may have low Shannon entropy. We demonstrate empirical improvements over multiple image classification tasks, outperforming state-of-the-art misclassification detection methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#21435;&#20013;&#24515;&#21270;&#26080;&#36951;&#25022;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#21152;&#27861;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#35299;&#20915;&#20102;&#36807;&#24230;&#25506;&#32034;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#31216;&#20026;DumBO&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19838</link><description>&lt;p&gt;
&#25918;&#26494;&#21435;&#20013;&#24515;&#21270;&#26080;&#36951;&#25022;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#21152;&#27861;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Relaxing the Additivity Constraints in Decentralized No-Regret High-Dimensional Bayesian Optimization. (arXiv:2305.19838v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#21435;&#20013;&#24515;&#21270;&#26080;&#36951;&#25022;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#21152;&#27861;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#35299;&#20915;&#20102;&#36807;&#24230;&#25506;&#32034;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#31216;&#20026;DumBO&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#24120;&#29992;&#20110;&#20248;&#21270;&#19968;&#20010;&#26410;&#30693;&#20989;&#25968;$f$&#65292;&#35813;&#20989;&#25968;&#23384;&#22312;&#22122;&#22768;&#19988;&#35780;&#20272;&#25104;&#26412;&#39640;&#26114;&#65292;&#36890;&#36807;&#21033;&#29992;&#24517;&#39035;&#22312;&#27599;&#20010;&#20248;&#21270;&#27493;&#39588;&#20013;&#26368;&#22823;&#21270;&#30340;&#25910;&#33719;&#20989;&#25968;&#26469;&#23454;&#29616;&#12290;&#23613;&#31649;&#21487;&#35777;&#26126;&#28176;&#36827;&#26368;&#20248;&#30340;BO&#31639;&#27861;&#22312;&#20248;&#21270;&#20302;&#32500;&#20989;&#25968;&#26041;&#38754;&#25928;&#29575;&#24456;&#39640;&#65292;&#20294;&#23558;&#20854;&#25193;&#23637;&#21040;&#39640;&#32500;&#31354;&#38388;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#36890;&#24120;&#36890;&#36807;&#20551;&#35774;$f$&#20855;&#26377;&#21152;&#27861;&#32467;&#26500;&#26469;&#35299;&#20915;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;BO&#31639;&#27861;&#36890;&#24120;&#24341;&#20837;&#20102;&#23545;&#21152;&#27861;&#32467;&#26500;&#30340;&#39069;&#22806;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#33539;&#22260;&#12290;&#26412;&#25991;&#21253;&#21547;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#65306;&#65288;i&#65289;&#25918;&#26494;&#23545;$f$&#21152;&#27861;&#32467;&#26500;&#30340;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#20197;&#20943;&#24369;&#25910;&#33719;&#20989;&#25968;&#30340;&#26368;&#22823;&#21270;&#20445;&#35777;&#65307;&#65288;ii&#65289;&#35299;&#20915;&#21435;&#20013;&#24515;&#21270;BO&#31639;&#27861;&#20013;&#30340;&#36807;&#24230;&#25506;&#32034;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DumBO&#65292;&#19968;&#31181;&#28176;&#36827;&#26368;&#20248;&#30340;&#21435;&#20013;&#24515;&#21270;BO&#31639;&#27861;&#65292;&#20855;&#26377;&#38750;&#24120;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO) is typically used to optimize an unknown function $f$ that is noisy and costly to evaluate, by exploiting an acquisition function that must be maximized at each optimization step. Even if provably asymptotically optimal BO algorithms are efficient at optimizing low-dimensional functions, scaling them to high-dimensional spaces remains an open problem, often tackled by assuming an additive structure for $f$. By doing so, BO algorithms typically introduce additional restrictive assumptions on the additive structure that reduce their applicability domain. This paper contains two main contributions: (i) we relax the restrictive assumptions on the additive structure of $f$, at the expense of weakening the maximization guarantees of the acquisition function, and (ii) we address the over-exploration problem for decentralized BO algorithms. To these ends, we propose DumBO, an asymptotically optimal decentralized BO algorithm that achieves very competitive performance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30053;&#24494;&#36229;&#21442;&#25968;&#21270;&#30340;ReLU&#32593;&#32476;&#22312;&#26377;&#38480;&#36755;&#20837;&#25968;&#25454;&#38598;&#19978;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#35777;&#26126;&#20102;&#22823;&#22810;&#25968;&#28608;&#27963;&#27169;&#24335;&#23545;&#24212;&#30340;&#21442;&#25968;&#21306;&#22495;&#27809;&#26377;&#22351;&#30340;&#21487;&#24494;&#23616;&#37096;&#26497;&#23567;&#20540;&#65292;&#23545;&#20110;&#19968;&#32500;&#36755;&#20837;&#25968;&#25454;&#65292;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#22823;&#22810;&#25968;&#28608;&#27963;&#27169;&#24335;&#23454;&#29616;&#39640;&#32500;&#20840;&#23616;&#26497;&#23567;&#20540;&#38598;&#21512;&#32780;&#19981;&#20855;&#26377;&#22351;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.19510</link><description>&lt;p&gt;
&#30053;&#24494;&#36229;&#21442;&#25968;&#21270;&#30340;ReLU&#32593;&#32476;&#20855;&#26377;&#26377;&#21033;&#30340;&#25439;&#22833;&#26223;&#35266;
&lt;/p&gt;
&lt;p&gt;
Mildly Overparameterized ReLU Networks Have a Favorable Loss Landscape. (arXiv:2305.19510v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30053;&#24494;&#36229;&#21442;&#25968;&#21270;&#30340;ReLU&#32593;&#32476;&#22312;&#26377;&#38480;&#36755;&#20837;&#25968;&#25454;&#38598;&#19978;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#35777;&#26126;&#20102;&#22823;&#22810;&#25968;&#28608;&#27963;&#27169;&#24335;&#23545;&#24212;&#30340;&#21442;&#25968;&#21306;&#22495;&#27809;&#26377;&#22351;&#30340;&#21487;&#24494;&#23616;&#37096;&#26497;&#23567;&#20540;&#65292;&#23545;&#20110;&#19968;&#32500;&#36755;&#20837;&#25968;&#25454;&#65292;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#22823;&#22810;&#25968;&#28608;&#27963;&#27169;&#24335;&#23454;&#29616;&#39640;&#32500;&#20840;&#23616;&#26497;&#23567;&#20540;&#38598;&#21512;&#32780;&#19981;&#20855;&#26377;&#22351;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#38480;&#36755;&#20837;&#25968;&#25454;&#38598;&#19978;&#65292;&#20108;&#23618;&#30053;&#24494;&#36229;&#21442;&#25968;&#21270;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#20351;&#29992;&#20102;&#21442;&#25968;&#26144;&#23556;&#30340;Jacobian&#30697;&#38453;&#30340;&#31209;&#26469;&#20272;&#35745;&#23616;&#37096;&#21644;&#20840;&#23616;&#26497;&#23567;&#20540;&#38598;&#30340;&#32500;&#24230;&#12290;&#20351;&#29992;&#38543;&#26426;&#20108;&#36827;&#21046;&#30697;&#38453;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#22823;&#22810;&#25968;&#28608;&#27963;&#27169;&#24335;&#23545;&#24212;&#30340;&#21442;&#25968;&#21306;&#22495;&#27809;&#26377;&#22351;&#30340;&#21487;&#24494;&#23616;&#37096;&#26497;&#23567;&#20540;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#19968;&#32500;&#36755;&#20837;&#25968;&#25454;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#22823;&#22810;&#25968;&#30340;&#28608;&#27963;&#27169;&#24335;&#23454;&#29616;&#39640;&#32500;&#20840;&#23616;&#26497;&#23567;&#20540;&#38598;&#21512;&#32780;&#19981;&#20855;&#26377;&#22351;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#12290;&#25105;&#20204;&#36890;&#36807;&#21457;&#29616;&#22823;&#22810;&#25968;&#21306;&#22495;&#20855;&#26377;&#23436;&#25972;&#30340;&#31209;&#25110;&#32570;&#20047;&#31209;&#65292;&#20197;&#23454;&#39564;&#30340;&#26041;&#24335;&#35777;&#23454;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#36825;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the loss landscape of two-layer mildly overparameterized ReLU neural networks on a generic finite input dataset for the squared error loss. Our approach involves bounding the dimension of the sets of local and global minima using the rank of the Jacobian of the parameterization map. Using results on random binary matrices, we show most activation patterns correspond to parameter regions with no bad differentiable local minima. Furthermore, for one-dimensional input data, we show most activation regions realizable by the network contain a high dimensional set of global minima and no bad local minima. We experimentally confirm these results by finding a phase transition from most regions having full rank to many regions having deficient rank depending on the amount of overparameterization.
&lt;/p&gt;</description></item><item><title>NeuralMatrix&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21333;&#20010;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#21152;&#36895;&#22120;&#19978;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#65292;&#24182;&#21487;&#22312;&#20445;&#25345;&#25512;&#29702;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#36798;113&#20493;&#33267;19.44&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.14405</link><description>&lt;p&gt;
NeuralMatrix: &#23558;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#31227;&#21160;&#21040;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#20197;&#23454;&#29616;&#39640;&#25928;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
NeuralMatrix: Moving Entire Neural Networks to General Matrix Multiplication for Efficient Inference. (arXiv:2305.14405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14405
&lt;/p&gt;
&lt;p&gt;
NeuralMatrix&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21333;&#20010;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#21152;&#36895;&#22120;&#19978;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#65292;&#24182;&#21487;&#22312;&#20445;&#25345;&#25512;&#29702;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#36798;113&#20493;&#33267;19.44&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NeuralMatrix&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23427;&#20351;&#24471;&#21487;&#20197;&#22312;&#21333;&#20010;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#65288;GEMM&#65289;&#21152;&#36895;&#22120;&#19978;&#35745;&#31639;&#22810;&#21151;&#33021;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#22522;&#20110;ASIC&#30340;&#21152;&#36895;&#22120;&#30340;&#19987;&#29992;&#24615;&#38480;&#21046;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;CPU&#21644;GPU&#31561;&#36890;&#29992;&#22788;&#29702;&#22120;&#30456;&#27604;&#30340;&#24212;&#29992;&#29305;&#23450;&#21152;&#36895;&#27700;&#24179;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#23558;DNN&#35745;&#31639;&#20013;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36816;&#31639;&#26144;&#23556;&#21040;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#20197;&#21450;&#20351;&#29992;GEMM&#21152;&#36895;&#22120;&#23545;DNN&#25512;&#29702;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#19977;&#31181;&#27969;&#34892;&#31867;&#21035;&#30340;&#21508;&#31181;DNN&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65288;&#21363;CNN&#65292;Transformers&#21644;GNN&#65289;&#20316;&#20026;&#31034;&#20363;&#30340;&#25903;&#25745;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;DNN&#36716;&#25442;&#20026;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#21518;&#20165;&#20250;&#20986;&#29616;&#39640;&#36798;2.02&#65285;&#30340;&#20934;&#30830;&#24230;&#25439;&#22833;&#65292;&#21516;&#26102;&#23558;&#21534;&#21520;&#37327;&#19982;&#21151;&#29575;&#30340;&#27604;&#20540;&#19982;CPU&#21644;GPU&#30456;&#27604;&#25552;&#39640;&#20102;113&#20493;&#21040;19.44&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce NeuralMatrix, a novel framework that enables the computation of versatile deep neural networks (DNNs) on a single general matrix multiplication (GEMM) accelerator. The proposed approach overcomes the specificity limitations of ASIC-based accelerators while achieving application-specific acceleration levels compared to general-purpose processors such as CPUs and GPUs. We address the challenges of mapping both linear and nonlinear operations in DNN computation to general matrix multiplications and the impact of using a GEMM accelerator on DNN inference accuracy. Extensive experiments are conducted on various DNN models from three popular categories (i.e., CNN, Transformers, and GNN) as illustrative backbone models. Our results demonstrate that DNNs suffer only up to a 2.02% accuracy loss after being converted to general matrix multiplication, while achieving 113x to 19.44x improvements in throughput per power compared to CPUs and GPUs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#30001;&#23450;&#20041;&#25152;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.07303</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;
&lt;/p&gt;
&lt;p&gt;
Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions. (arXiv:2305.07303v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#30001;&#23450;&#20041;&#25152;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20351;&#29992;&#20998;&#24067;&#20449;&#24687;&#30340;&#31070;&#32463;&#35789;&#21521;&#37327;&#19968;&#30452;&#20197;&#26469;&#37117;&#33021;&#20026;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#26377;&#29992;&#30340;&#21547;&#20041;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20250;&#23548;&#33268;&#38590;&#20197;&#35299;&#37322;&#21644;&#25511;&#21046;&#30340;&#34920;&#31034;&#12290;&#30456;&#21453;&#65292;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20855;&#26377;&#36882;&#24402;&#30340;&#65292;&#33258;&#35828;&#26126;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#21487;&#20197;&#25903;&#25345;&#33021;&#22815;&#20445;&#30041;&#21521;&#37327;&#31354;&#38388;&#20013;&#26174;&#24335;&#27010;&#24565;&#20851;&#31995;&#21644;&#32422;&#26463;&#30340;&#26032;&#22411;&#34920;&#31034;&#23398;&#20064;&#33539; paradigm&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#12289;&#22810;&#20851;&#31995;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#26144;&#23556;&#23450;&#20041;&#21644;&#23450;&#20041;&#26415;&#35821;&#21450;&#20854;&#30456;&#24212;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#20165;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#35789;&#21521;&#37327;&#12290;&#36890;&#36807;&#33258;&#21160;&#20174;&#23450;&#20041;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#32763;&#35793;&#30446;&#26631;&#35268;&#33539;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26694;&#26550;&#19987;&#38376;&#35774;&#23450;&#20026;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#25429;&#33719;&#30001;&#23450;&#20041;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural-based word embeddings using solely distributional information have consistently produced useful meaning representations for downstream tasks. However, existing approaches often result in representations that are hard to interpret and control. Natural language definitions, on the other side, possess a recursive, self-explanatory semantic structure that can support novel representation learning paradigms able to preserve explicit conceptual relations and constraints in the vector space.  This paper proposes a neuro-symbolic, multi-relational framework to learn word embeddings exclusively from natural language definitions by jointly mapping defined and defining terms along with their corresponding semantic relations. By automatically extracting the relations from definitions corpora and formalising the learning problem via a translational objective, we specialise the framework in hyperbolic space to capture the hierarchical and multi-resolution structure induced by the definitions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#33021;&#22815;&#39640;&#27010;&#29575;&#22320;&#20174;&#38750;&#20856;&#22411;&#38543;&#26426;&#21021;&#22987;&#21270;&#36798;&#21040;&#19968;&#20010;$\epsilon$-&#26368;&#20248;&#30697;&#38453;&#20998;&#35299;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#21021;&#22987;&#21270;&#19981;&#20165;&#22312;&#29702;&#35770;&#19978;&#26377;&#30410;&#65292;&#32780;&#19988;&#22312;&#23454;&#36341;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06927</link><description>&lt;p&gt;
&#30697;&#38453;&#20998;&#35299;&#20013;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence of Alternating Gradient Descent for Matrix Factorization. (arXiv:2305.06927v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#33021;&#22815;&#39640;&#27010;&#29575;&#22320;&#20174;&#38750;&#20856;&#22411;&#38543;&#26426;&#21021;&#22987;&#21270;&#36798;&#21040;&#19968;&#20010;$\epsilon$-&#26368;&#20248;&#30697;&#38453;&#20998;&#35299;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#21021;&#22987;&#21270;&#19981;&#20165;&#22312;&#29702;&#35770;&#19978;&#26377;&#30410;&#65292;&#32780;&#19988;&#22312;&#23454;&#36341;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#24212;&#29992;&#20110;&#19981;&#23545;&#31216;&#30697;&#38453;&#20998;&#35299;&#30446;&#26631;&#30340;&#20855;&#26377;&#22266;&#23450;&#27493;&#38271;$\eta&gt;0$&#30340;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#65288;AGD&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#23545;&#20110;&#31209;&#20026;$r$&#30340;&#30697;&#38453;$\mathbf {A}\in \mathbb {R} ^ {m \times n}$&#65292;$T=\left(\left(\frac{\sigma_1(\mathbf{A})}{\sigma_r(\mathbf{A})}\right)^2\log(1/\epsilon)\right)$&#27425;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#21363;&#21487;&#20174;&#38750;&#20856;&#22411;&#38543;&#26426;&#21021;&#22987;&#21270;&#39640;&#27010;&#29575;&#22320;&#36798;&#21040;$\epsilon$-&#26368;&#20248;&#20998;&#35299;$\|\mathbf {A}\mathbf {X}_T^{\vphantom{\intercal}}\mathbf {Y}_T^{\intercal}\|_{\rm F}^2\le\epsilon\|\mathbf {A}\|_{\rm F}^2$&#12290;&#20998;&#35299;&#20013;&#22240;&#23376;&#30340;&#31209;&#20026;$d&gt;r$&#65292;&#22240;&#27492;$\mathbf{X}_T\in\mathbb{R}^{m \times d}$&#19988;$\mathbf{Y}_T\in\mathbb{R}^{n \times d}$&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#21021;&#22987;&#21270;&#19981;&#20165;&#22312;&#29702;&#35770;&#19978;&#26377;&#30410;&#65292;&#32780;&#19988;&#22312;&#23454;&#36341;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#27010;&#24565;&#19978;&#24456;&#31616;&#21333;&#65306;&#19968;&#33268;&#30340;PL&#19981;&#31561;&#24335;&#21644;&#19968;&#33268;&#30340;Lipschitz&#24179;&#28369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider alternating gradient descent (AGD) with fixed step size $\eta &gt; 0$, applied to the asymmetric matrix factorization objective. We show that, for a rank-$r$ matrix $\mathbf{A} \in \mathbb{R}^{m \times n}$, $T = \left( \left(\frac{\sigma_1(\mathbf{A})}{\sigma_r(\mathbf{A})}\right)^2 \log(1/\epsilon)\right)$ iterations of alternating gradient descent suffice to reach an $\epsilon$-optimal factorization $\| \mathbf{A} \mathbf{X}_T^{\vphantom{\intercal}} \mathbf{Y}_T^{\intercal} \|_{\rm F}^2 \leq \epsilon \| \mathbf{A} \|_{\rm F}^2$ with high probability starting from an atypical random initialization. The factors have rank $d&gt;r$ so that $\mathbf{X}_T\in\mathbb{R}^{m \times d}$ and $\mathbf{Y}_T \in\mathbb{R}^{n \times d}$. Experiments suggest that our proposed initialization is not merely of theoretical benefit, but rather significantly improves convergence of gradient descent in practice. Our proof is conceptually simple: a uniform PL-inequality and uniform Lipschitz smoothne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#8212;&#8212;&#39057;&#29575;&#25903;&#25345;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#21152;&#20837;&#39057;&#29575;&#20449;&#24687;&#36866;&#24212;&#20110;&#38750;&#32447;&#24615;&#31995;&#32479;&#36776;&#35782;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06344</link><description>&lt;p&gt;
&#38754;&#21521;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#36776;&#35782;&#30340;&#39057;&#29575;&#25903;&#25345;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Frequency-Supported Neural Networks for Nonlinear Dynamical System Identification. (arXiv:2305.06344v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#8212;&#8212;&#39057;&#29575;&#25903;&#25345;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#21152;&#20837;&#39057;&#29575;&#20449;&#24687;&#36866;&#24212;&#20110;&#38750;&#32447;&#24615;&#31995;&#32479;&#36776;&#35782;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#38750;&#24120;&#36890;&#29992;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#23398;&#20064;&#22810;&#20010;&#21464;&#37327;&#20043;&#38388;&#30340;&#21508;&#31181;&#20851;&#31995;&#12290;&#20854;&#20013;&#19968;&#31181;&#20851;&#31995;&#22312;&#23454;&#36341;&#20013;&#38750;&#24120;&#26377;&#36259;&#65292;&#21363;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#65292;&#20855;&#26377;&#22810;&#31181;&#24212;&#29992;&#12290;&#30740;&#31350;&#33021;&#22815;&#20272;&#35745;&#36825;&#31181;&#20851;&#31995;&#30340;&#27169;&#22411;&#26159;&#19968;&#38376;&#24191;&#27867;&#30340;&#23398;&#31185;&#65292;&#20855;&#26377;&#35768;&#22810;&#29702;&#35770;&#21644;&#23454;&#38469;&#32467;&#26524;&#12290;&#31070;&#32463;&#32593;&#32476;&#38750;&#24120;&#36890;&#29992;&#65292;&#20294;&#23384;&#22312;&#22810;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#20998;&#21035;&#29992;&#20110;&#29305;&#23450;&#30340;&#24212;&#29992;&#65292;&#21363;&#22270;&#20687;&#21644;&#24207;&#21015;&#22788;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#36890;&#36807;&#23558;&#39057;&#29575;&#20449;&#24687;&#32435;&#20837;&#21040;&#36890;&#29992;&#32593;&#32476;&#32467;&#26500;&#20013;&#26469;&#35843;&#25972;&#20854;&#32467;&#26500;&#65292;&#24212;&#35813;&#21487;&#20197;&#24471;&#21040;&#19968;&#31181;&#19987;&#38376;&#36866;&#29992;&#20110;&#38750;&#32447;&#24615;&#31995;&#32479;&#36776;&#35782;&#30340;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#21487;&#20197;&#28155;&#21152;&#36825;&#31181;&#39057;&#29575;&#20449;&#24687;&#32780;&#19981;&#20250;&#25439;&#22833;&#36890;&#29992;&#24615;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26032;&#32467;&#26500;&#20026;&#39057;&#29575;&#25903;&#25345;&#31070;&#32463;&#32593;&#32476;&#65288;FSNN&#65289;&#65292;&#24182;&#22312;&#22810;&#20010;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#36776;&#35782;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are a very general type of model capable of learning various relationships between multiple variables. One example of such relationships, particularly interesting in practice, is the input-output relation of nonlinear systems, which has a multitude of applications. Studying models capable of estimating such relation is a broad discipline with numerous theoretical and practical results. Neural networks are very general, but multiple special cases exist, including convolutional neural networks and recurrent neural networks, which are adjusted for specific applications, which are image and sequence processing respectively. We formulate a hypothesis that adjusting general network structure by incorporating frequency information into it should result in a network specifically well suited to nonlinear system identification. Moreover, we show that it is possible to add this frequency information without the loss of generality from a theoretical perspective. We call this new st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;WiFi&#20449;&#21495;&#36827;&#34892;&#20154;&#21592;&#23384;&#22312;&#26816;&#27979;&#30340;&#26032;&#31995;&#32479;&#65292;&#37319;&#29992;&#20102;&#20851;&#27880;&#26426;&#21046;&#21644;&#21452;&#21521;LSTM&#32593;&#32476;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13105</link><description>&lt;p&gt;
&#21033;&#29992;&#23460;&#20869;WiFi&#31995;&#32479;&#36827;&#34892;&#26080;&#35774;&#22791;&#31359;&#22681;&#23384;&#22312;&#26816;&#27979;&#30340;&#27880;&#24847;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Attention-Enhanced Deep Learning for Device-Free Through-the-Wall Presence Detection Using Indoor WiFi System. (arXiv:2304.13105v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;WiFi&#20449;&#21495;&#36827;&#34892;&#20154;&#21592;&#23384;&#22312;&#26816;&#27979;&#30340;&#26032;&#31995;&#32479;&#65292;&#37319;&#29992;&#20102;&#20851;&#27880;&#26426;&#21046;&#21644;&#21452;&#21521;LSTM&#32593;&#32476;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#20934;&#30830;&#26816;&#27979;&#20154;&#21592;&#23384;&#22312;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#20363;&#22914;&#33021;&#28304;&#31649;&#29702;&#21644;&#23433;&#20840;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;WiFi&#20449;&#21495;&#30340;&#36890;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#36827;&#34892;&#20154;&#21592;&#23384;&#22312;&#26816;&#27979;&#30340;&#26032;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21517;&#20026;&#27880;&#24847;&#21147;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#65288;ALPD&#65289;&#65292;&#37319;&#29992;&#20851;&#27880;&#26426;&#21046;&#20174;CSI&#25968;&#25454;&#20013;&#33258;&#21160;&#36873;&#25321;&#26377;&#20449;&#24687;&#37327;&#30340;&#23376;&#36733;&#27874;&#65292;&#24182;&#37319;&#29992;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#25429;&#25417;CSI&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#38745;&#24577;&#29305;&#24449;&#26469;&#25552;&#39640;&#38745;&#24577;&#29366;&#24577;&#19979;&#20154;&#21592;&#23384;&#22312;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#37096;&#32626;&#19968;&#23545;WiFi&#25509;&#20837;&#28857;&#65288;AP&#65289;&#26469;&#25910;&#38598;CSI&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;ALPD&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#36827;&#19968;&#27493;&#19982;&#20960;&#20010;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;ALPD&#31995;&#32479;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#24178;&#25200;&#30340;&#24773;&#20917;&#19979;&#12290;&#27492;&#22806;&#65292;&#21452;&#21521;&#20256;&#36755;&#25968;&#25454;&#19981;&#20250;&#24433;&#21709;&#25105;&#20204;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate detection of human presence in indoor environments is important for various applications, such as energy management and security. In this paper, we propose a novel system for human presence detection using the channel state information (CSI) of WiFi signals. Our system named attention-enhanced deep learning for presence detection (ALPD) employs an attention mechanism to automatically select informative subcarriers from the CSI data and a bidirectional long short-term memory (LSTM) network to capture temporal dependencies in CSI. Additionally, we utilize a static feature to improve the accuracy of human presence detection in static states. We evaluate the proposed ALPD system by deploying a pair of WiFi access points (APs) for collecting CSI dataset, which is further compared with several benchmarks. The results demonstrate that our ALPD system outperforms the benchmarks in terms of accuracy, especially in the presence of interference. Moreover, bidirectional transmission data 
&lt;/p&gt;</description></item><item><title>Transformer&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2304.10557</link><description>&lt;p&gt;
Transformer&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
An Introduction to Transformers. (arXiv:2304.10557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10557
&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#30340;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#12290;Transformer&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;Transformer&#30340;&#20171;&#32461;&#65292;&#20294;&#22823;&#22810;&#25968;&#37117;&#32570;&#23569;&#23545;&#20854;&#26550;&#26500;&#30340;&#31934;&#30830;&#25968;&#23398;&#25551;&#36848;&#65292;&#20854;&#35774;&#35745;&#36873;&#25321;&#30340;&#30452;&#35273;&#20063;&#24120;&#24120;&#32570;&#22833;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#30740;&#31350;&#36335;&#24452;&#30340;&#26354;&#25240;&#65292;Transformer&#37096;&#20214;&#30340;&#35299;&#37322;&#21487;&#33021;&#26159;&#24322;&#36136;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformer is a neural network component that can be used to learn useful representations of sequences or sets of datapoints. The transformer has driven recent advances in natural language processing, computer vision, and spatio-temporal modelling. There are many introductions to transformers, but most do not contain precise mathematical descriptions of the architecture and the intuitions behind the design choices are often also missing. Moreover, as research takes a winding path, the explanations for the components of the transformer can be idiosyncratic. In this note we aim for a mathematically precise, intuitive, and clean description of the transformer architecture.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545; LiDAR &#27450;&#39575;&#25915;&#20987;&#22312;&#30446;&#26631;&#26816;&#27979;&#39046;&#22495;&#23384;&#22312;&#30340;&#30740;&#31350;&#24046;&#36317;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#20351;&#29992;9&#31181; LiDAR &#21644;3&#31181;&#30446;&#26631;&#26816;&#27979;&#22120;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#27979;&#37327;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#27169;&#24335;&#65292;&#25552;&#20986;&#20102;&#23433;&#20840;&#30340; LiDAR &#35774;&#35745;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.10555</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270; LiDAR &#27450;&#39575;&#25915;&#20987;&#23545;&#20110;&#30446;&#26631;&#26816;&#27979;&#30340;&#33021;&#21147;&#65306;&#25913;&#36827;&#12289;&#27979;&#37327;&#21644;&#26032;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Revisiting LiDAR Spoofing Attack Capabilities against Object Detection: Improvements, Measurement, and New Attack. (arXiv:2303.10555v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545; LiDAR &#27450;&#39575;&#25915;&#20987;&#22312;&#30446;&#26631;&#26816;&#27979;&#39046;&#22495;&#23384;&#22312;&#30340;&#30740;&#31350;&#24046;&#36317;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#20351;&#29992;9&#31181; LiDAR &#21644;3&#31181;&#30446;&#26631;&#26816;&#27979;&#22120;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#27979;&#37327;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#27169;&#24335;&#65292;&#25552;&#20986;&#20102;&#23433;&#20840;&#30340; LiDAR &#35774;&#35745;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LiDAR&#65288;&#20809;&#23398;&#36965;&#24863;&#65289;&#26159;&#36827;&#34892;&#31934;&#30830;&#38271;&#36317;&#31163;&#21644;&#23485;&#33539;&#22260; 3D &#24863;&#24212;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#20256;&#24863;&#22120;&#65292;&#30452;&#25509;&#36896;&#31119;&#20110;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#30340;&#24555;&#36895;&#25512;&#24191;&#12290;&#21516;&#26102;&#65292;&#36825;&#31181;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#24378;&#28872;&#25512;&#21160;&#20102;&#23545;&#20854;&#23433;&#20840;&#24615;&#30340;&#30740;&#31350;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#21521; LiDAR &#21457;&#36865;&#24694;&#24847;&#28608;&#20809;&#26469;&#25805;&#32437; LiDAR &#28857;&#20113;&#24182;&#27450;&#39575;&#30446;&#26631;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#38754;&#20020;&#19977;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#38590;&#28857;&#65306;&#65288;1&#65289;&#20165;&#35780;&#20272;&#29305;&#23450;&#30340; LiDAR&#65288;VLP-16&#65289;&#65307;&#65288;2&#65289;&#20551;&#35774;&#25915;&#20987;&#33021;&#21147;&#26410;&#34987;&#39564;&#35777;&#65307;&#20197;&#21450;&#65288;3&#65289;&#35780;&#20272;&#21463;&#38480;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#20851;&#38190;&#30340;&#30740;&#31350;&#38590;&#28857;&#65292;&#25105;&#20204;&#23545;&#24635;&#20849;9&#31181;&#27969;&#34892;&#30340; LiDAR &#21644;3&#31181;&#20027;&#35201;&#31867;&#22411;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#22823;&#35268;&#27169;&#30340; LiDAR &#27450;&#39575;&#25915;&#20987;&#33021;&#21147;&#27979;&#37327;&#30740;&#31350;&#12290;&#20026;&#20102;&#36827;&#34892;&#36825;&#20010;&#27979;&#37327;&#65292;&#25105;&#20204;&#36890;&#36807;&#26356;&#21152;&#23567;&#24515;&#30340;&#20809;&#23398;&#21644;&#21151;&#33021;&#30005;&#23376;&#23398;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102; LiDAR &#27450;&#39575;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#27169;&#24335;&#65292;&#21487;&#20197;&#29978;&#33267;&#35268;&#36991;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#22312;&#19981;&#21516;&#30340;&#23454;&#38469;&#35774;&#32622;&#20013;&#23454;&#38469;&#36827;&#34892; LiDAR &#27450;&#39575;&#25915;&#20987;&#30340;&#35265;&#35299;&#65292;&#24182;&#21628;&#21505;&#23433;&#20840;&#30340; LiDAR &#35774;&#35745;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
LiDAR (Light Detection And Ranging) is an indispensable sensor for precise long- and wide-range 3D sensing, which directly benefited the recent rapid deployment of autonomous driving (AD). Meanwhile, such a safety-critical application strongly motivates its security research. A recent line of research demonstrates that one can manipulate the LiDAR point cloud and fool object detection by firing malicious lasers against LiDAR. However, these efforts face 3 critical research gaps: (1) evaluating only on a specific LiDAR (VLP-16); (2) assuming unvalidated attack capabilities; and (3) evaluating with models trained on limited datasets.  To fill these critical research gaps, we conduct the first large-scale measurement study on LiDAR spoofing attack capabilities on object detectors with 9 popular LiDARs in total and 3 major types of object detectors. To perform this measurement, we significantly improved the LiDAR spoofing capability with more careful optics and functional electronics, whic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#30340;&#29289;&#36136;&#24863;&#30693;&#21452;&#32819;&#38899;&#39057;&#20256;&#25773;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#30495;&#23454;&#29615;&#22659;&#19979;&#30340;&#28210;&#26579;&#38899;&#39057;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#22788;&#29702;&#37325;&#26500;&#19977;&#32500;&#27169;&#22411;&#20013;&#30340;&#32570;&#38519;&#65292;&#24182;&#19988;&#33021;&#22815;&#31934;&#30830;&#29983;&#25104;&#19982;&#30495;&#23454;&#29615;&#22659;&#30456;&#31526;&#30340;&#22768;&#23398;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2302.02809</link><description>&lt;p&gt;
Listen2Scene&#65306;&#20132;&#20114;&#24335;&#29289;&#36136;&#24863;&#30693;&#21452;&#32819;&#38899;&#39057;&#20256;&#25773;&#37325;&#26500;&#19977;&#32500;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
Listen2Scene: Interactive material-aware binaural soundbpropagation for reconstructed 3D scenes. (arXiv:2302.02809v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#30340;&#29289;&#36136;&#24863;&#30693;&#21452;&#32819;&#38899;&#39057;&#20256;&#25773;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#30495;&#23454;&#29615;&#22659;&#19979;&#30340;&#28210;&#26579;&#38899;&#39057;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#22788;&#29702;&#37325;&#26500;&#19977;&#32500;&#27169;&#22411;&#20013;&#30340;&#32570;&#38519;&#65292;&#24182;&#19988;&#33021;&#22815;&#31934;&#30830;&#29983;&#25104;&#19982;&#30495;&#23454;&#29615;&#22659;&#30456;&#31526;&#30340;&#22768;&#23398;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#21644;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#24212;&#29992;&#30340;&#31471;&#21040;&#31471;&#21452;&#32819;&#38899;&#39057;&#28210;&#26579;&#26041;&#27861;&#65288;Listen2Scene&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21452;&#32819;&#22768;&#23398;&#20256;&#25773;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#30495;&#23454;&#29615;&#22659;&#30340;3D&#27169;&#22411;&#30340;&#22768;&#23398;&#25928;&#26524;&#12290;&#20219;&#20309;&#28165;&#27905;&#38899;&#39057;&#25110;&#24178;&#38899;&#39057;&#37117;&#21487;&#20197;&#19982;&#29983;&#25104;&#30340;&#22768;&#23398;&#25928;&#26524;&#21367;&#31215;&#65292;&#20197;&#28210;&#26579;&#19982;&#30495;&#23454;&#29615;&#22659;&#30456;&#23545;&#24212;&#30340;&#38899;&#39057;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;3D&#22330;&#26223;&#30340;&#26448;&#26009;&#21644;&#25299;&#25169;&#20449;&#24687;&#29983;&#25104;&#22330;&#26223;&#28508;&#22312;&#21521;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CGAN&#65289;&#20174;&#22330;&#26223;&#28508;&#22312;&#21521;&#37327;&#29983;&#25104;&#22768;&#23398;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#33021;&#22815;&#22788;&#29702;&#37325;&#26500;&#30340;&#19977;&#32500;&#32593;&#26684;&#27169;&#22411;&#20013;&#30340;&#23380;&#27934;&#25110;&#20854;&#20182;&#20266;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25104;&#26412;&#20989;&#25968;&#65292;&#29992;&#20110;&#29983;&#25104;&#22120;&#32593;&#32476;&#20197;&#25972;&#21512;&#31354;&#38388;&#38899;&#39057;&#25928;&#26524;&#12290;&#32473;&#23450;&#28304;&#21644;&#21548;&#32773;&#20301;&#32622;&#65292;&#25105;&#20204;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#21452;&#32819;&#22768;&#38899;&#20256;&#25773;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#19982;&#30495;&#23454;&#29615;&#22659;&#31934;&#24230;&#30456;&#31526;&#30340;&#22768;&#23398;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an end-to-end binaural audio rendering approach (Listen2Scene) for virtual reality (VR) and augmented reality (AR) applications. We propose a novel neural-network-based binaural sound propagation method to generate acoustic effects for 3D models of real environments. Any clean audio or dry audio can be convolved with the generated acoustic effects to render audio corresponding to the real environment. We propose a graph neural network that uses both the material and the topology information of the 3D scenes and generates a scene latent vector. Moreover, we use a conditional generative adversarial network (CGAN) to generate acoustic effects from the scene latent vector. Our network is able to handle holes or other artifacts in the reconstructed 3D mesh model. We present an efficient cost function to the generator network to incorporate spatial audio effects. Given the source and the listener position, our learning-based binaural sound propagation approach can generate an acou
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#24102;&#26377;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22312;Bradley-Terry-Luce&#21644;Plackett-Luce&#27169;&#22411;&#19979;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#22312;&#19968;&#23450;&#30340;&#35206;&#30422;&#20551;&#35774;&#19979;&#65292;&#22522;&#20110;&#24754;&#35266;&#20272;&#35745;&#30340;MLE&#25552;&#20379;&#20102;&#24615;&#33021;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#22312;&#35777;&#26126;&#20102;&#30495;&#23454;MLE&#21644;&#20197;&#25104;&#23545;&#27604;&#36739;&#24418;&#24335;&#26367;&#20195;&#30340;&#22791;&#36873;MLE&#37117;&#21487;&#20197;&#22312;PL&#27169;&#22411;&#19979;&#25910;&#25947;&#30340;&#21516;&#26102;&#65292;&#20063;&#34920;&#26126;&#20102;&#30495;&#23454;MLE&#30340;&#39640;&#25928;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;RLHF&#31639;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#32479;&#19968;&#20102;RLHF&#38382;&#39064;&#21644;IRL&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.11270</link><description>&lt;p&gt;
&#20351;&#29992;&#26469;&#33258;&#25104;&#23545;&#25110;$K$&#20803;&#27604;&#36739;&#30340;&#20154;&#31867;&#21453;&#39304;&#30340;&#35268;&#33539;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons. (arXiv:2301.11270v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11270
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#24102;&#26377;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22312;Bradley-Terry-Luce&#21644;Plackett-Luce&#27169;&#22411;&#19979;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#22312;&#19968;&#23450;&#30340;&#35206;&#30422;&#20551;&#35774;&#19979;&#65292;&#22522;&#20110;&#24754;&#35266;&#20272;&#35745;&#30340;MLE&#25552;&#20379;&#20102;&#24615;&#33021;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#22312;&#35777;&#26126;&#20102;&#30495;&#23454;MLE&#21644;&#20197;&#25104;&#23545;&#27604;&#36739;&#24418;&#24335;&#26367;&#20195;&#30340;&#22791;&#36873;MLE&#37117;&#21487;&#20197;&#22312;PL&#27169;&#22411;&#19979;&#25910;&#25947;&#30340;&#21516;&#26102;&#65292;&#20063;&#34920;&#26126;&#20102;&#30495;&#23454;MLE&#30340;&#39640;&#25928;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;RLHF&#31639;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#32479;&#19968;&#20102;RLHF&#38382;&#39064;&#21644;IRL&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#24102;&#26377;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24403;&#30495;&#23454;&#22870;&#21169;&#20989;&#25968;&#20026;&#32447;&#24615;&#20989;&#25968;&#26102;&#65292;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#22312;Bradley-Terry-Luce&#65288;BTL&#65289;&#27169;&#22411;&#21644;Plackett-Luce&#65288;PL&#65289;&#27169;&#22411;&#19979;&#22343;&#25910;&#25947;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#22522;&#20110;&#23398;&#24471;&#30340;&#22870;&#21169;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#26102;&#65292;MLE&#20250;&#22833;&#36133;&#65292;&#32780;&#22522;&#20110;&#24754;&#35266;&#20272;&#35745;&#30340;MLE&#22312;&#19968;&#23450;&#30340;&#35206;&#30422;&#20551;&#35774;&#19979;&#25552;&#20379;&#24615;&#33021;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;PL&#27169;&#22411;&#19979;&#65292;&#30495;&#23454;MLE&#21644;&#23558;$k$&#20803;&#27604;&#36739;&#25286;&#20998;&#20026;&#25104;&#23545;&#27604;&#36739;&#30340;&#22791;&#36873;MLE&#37117;&#25910;&#25947;&#12290;&#32780;&#30495;&#23454;MLE&#26159;&#28176;&#36817;&#26356;&#20026;&#39640;&#25928;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#39564;&#35777;&#20102;&#29616;&#26377;RLHF&#31639;&#27861;&#65288;&#22914;InstructGPT&#65289;&#30340;&#23454;&#39564;&#25104;&#21151;&#65292;&#24182;&#20026;&#31639;&#27861;&#35774;&#35745;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#32479;&#19968;&#20102;RLHF&#38382;&#39064;&#21644;&#26368;&#22823;&#29109;&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;(IRL)&#38382;&#39064;&#65292;&#24182;&#20026;&#20854;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a theoretical framework for Reinforcement Learning with Human Feedback (RLHF). Our analysis shows that when the true reward function is linear, the widely used maximum likelihood estimator (MLE) converges under both the Bradley-Terry-Luce (BTL) model and the Plackett-Luce (PL) model. However, we show that when training a policy based on the learned reward model, MLE fails while a pessimistic MLE provides policies with improved performance under certain coverage assumptions. Additionally, we demonstrate that under the PL model, the true MLE and an alternative MLE that splits the $K$-wise comparison into pairwise comparisons both converge. Moreover, the true MLE is asymptotically more efficient. Our results validate the empirical success of existing RLHF algorithms in InstructGPT and provide new insights for algorithm design. Furthermore, our results unify the problem of RLHF and max-entropy Inverse Reinforcement Learning (IRL), and provide the first sample complexity bound fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#24212;&#29992;&#20110;&#22810;&#31867;&#25968;&#25454;&#38598;&#20013;&#65292;&#36890;&#36807;&#26500;&#24314;&#25299;&#25169;&#20998;&#31867;&#22120;&#21644;&#31616;&#21333;&#22797;&#21512;&#20307;&#65292;&#30740;&#31350;&#20102;&#25299;&#25169;&#22797;&#26434;&#24615;&#23545;&#21069;&#39304;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#24182;&#39564;&#35777;&#20102;&#25299;&#25169;&#22797;&#26434;&#24615;&#19982;DNN&#23398;&#20064;&#20043;&#38388;&#30340;&#36127;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.09734</link><description>&lt;p&gt;
&#22810;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#25299;&#25169;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Topological Learning in Multi-Class Data Sets. (arXiv:2301.09734v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#24212;&#29992;&#20110;&#22810;&#31867;&#25968;&#25454;&#38598;&#20013;&#65292;&#36890;&#36807;&#26500;&#24314;&#25299;&#25169;&#20998;&#31867;&#22120;&#21644;&#31616;&#21333;&#22797;&#21512;&#20307;&#65292;&#30740;&#31350;&#20102;&#25299;&#25169;&#22797;&#26434;&#24615;&#23545;&#21069;&#39304;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#24182;&#39564;&#35777;&#20102;&#25299;&#25169;&#22797;&#26434;&#24615;&#19982;DNN&#23398;&#20064;&#20043;&#38388;&#30340;&#36127;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#24212;&#29992;&#20110;&#22810;&#31867;&#25968;&#25454;&#38598;&#30340;&#25299;&#25169;&#22797;&#26434;&#24615;&#34920;&#24449;&#38382;&#39064;&#12290;&#36890;&#36807;&#23450;&#20041;&#19968;&#20010;&#25299;&#25169;&#20998;&#31867;&#22120;&#65292;&#20351;&#29992;&#25968;&#25454;&#38598;&#30340;&#24320;&#25918;&#23376;&#35206;&#30422;&#65292;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#31616;&#21333;&#22797;&#21512;&#20307;&#65292;&#20854;&#25299;&#25169;&#29305;&#24449;&#65288;&#22914;Betti&#25968;&#65289;&#25552;&#20379;&#20851;&#20110;&#20998;&#31867;&#38382;&#39064;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25299;&#25169;&#32467;&#26500;&#26469;&#30740;&#31350;&#25299;&#25169;&#22797;&#26434;&#24615;&#23545;&#21069;&#39304;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20551;&#35774;&#25299;&#25169;&#22797;&#26434;&#24615;&#19982;&#20840;&#36830;&#25509;&#21069;&#39304;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#30830;&#20998;&#31867;&#25968;&#25454;&#30340;&#33021;&#21147;&#21576;&#36127;&#30456;&#20851;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#26500;&#24314;&#21644;&#24320;&#28304;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#25299;&#25169;&#20998;&#31867;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#39564;&#35777;&#20102;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#25299;&#25169;&#22797;&#26434;&#24615;&#19982;DNN&#23398;&#20064;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We specialize techniques from topological data analysis to the problem of characterizing the topological complexity (as defined in the body of the paper) of a multi-class data set. As a by-product, a topological classifier is defined that uses an open sub-covering of the data set. This sub-covering can be used to construct a simplicial complex whose topological features (e.g., Betti numbers) provide information about the classification problem. We use these topological constructs to study the impact of topological complexity on learning in feedforward deep neural networks (DNNs). We hypothesize that topological complexity is negatively correlated with the ability of a fully connected feedforward deep neural network to learn to classify data correctly. We evaluate our topological classification algorithm on multiple constructed and open source data sets. We also validate our hypothesis regarding the relationship between topological complexity and learning in DNN's on multiple data sets.
&lt;/p&gt;</description></item><item><title>FedRAP&#26159;&#19968;&#31181;&#26032;&#30340;&#25512;&#33616;&#31995;&#32479;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#20849;&#20139;&#39033;&#23884;&#20837;&#21644;&#26412;&#22320;&#20010;&#24615;&#21270;&#35270;&#22270;&#65292;&#20197;&#25429;&#25417;&#29992;&#25143;&#23545;&#25512;&#33616;&#39033;&#30446;&#24863;&#30693;&#30340;&#20010;&#20307;&#24046;&#24322;&#24182;&#19988;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#21644;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2301.09109</link><description>&lt;p&gt;
&#24102;&#22686;&#37327;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Federated Recommendation with Additive Personalization. (arXiv:2301.09109v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09109
&lt;/p&gt;
&lt;p&gt;
FedRAP&#26159;&#19968;&#31181;&#26032;&#30340;&#25512;&#33616;&#31995;&#32479;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#20849;&#20139;&#39033;&#23884;&#20837;&#21644;&#26412;&#22320;&#20010;&#24615;&#21270;&#35270;&#22270;&#65292;&#20197;&#25429;&#25417;&#29992;&#25143;&#23545;&#25512;&#33616;&#39033;&#30446;&#24863;&#30693;&#30340;&#20010;&#20307;&#24046;&#24322;&#24182;&#19988;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#21644;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26500;&#24314;&#25512;&#33616;&#31995;&#32479;&#26159;&#25512;&#21160;&#19979;&#19968;&#20195;&#20114;&#32852;&#32593;&#26381;&#21153;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#26032;&#20852;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;FL&#35757;&#32451;&#20849;&#20139;&#39033;&#23884;&#20837;&#65292;&#21516;&#26102;&#22312;&#23458;&#25143;&#31471;&#20445;&#25345;&#29992;&#25143;&#23884;&#20837;&#31169;&#23494;&#24615;&#12290;&#28982;&#32780;&#65292;&#30456;&#21516;&#23884;&#20837;&#23545;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#39033;&#30446;&#19981;&#33021;&#25429;&#25417;&#21040;&#29992;&#25143;&#23545;&#21516;&#19968;&#39033;&#30446;&#24863;&#30693;&#30340;&#20010;&#20307;&#24046;&#24322;&#65292;&#22240;&#27492;&#23548;&#33268;&#20010;&#24615;&#21270;&#24046;&#12290;&#27492;&#22806;&#65292;FL&#20013;&#30340;&#23494;&#38598;&#39033;&#30446;&#23884;&#20837;&#23548;&#33268;&#36890;&#20449;&#25104;&#26412;&#21644;&#24310;&#36831;&#26114;&#36149;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#22686;&#37327;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#25512;&#33616;&#65288;FedRAP&#65289;&#65292;&#23427;&#36890;&#36807;FL&#23398;&#20064;&#39033;&#30446;&#30340;&#20840;&#23616;&#35270;&#22270;&#24182;&#22312;&#27599;&#20010;&#29992;&#25143;&#26412;&#22320;&#23398;&#20064;&#20010;&#24615;&#21270;&#35270;&#22270;&#12290;FedRAP&#36890;&#36807;&#27491;&#21017;&#21270;&#22686;&#21152;&#27491;&#21017;&#21270;&#26435;&#37325;&#26469;&#26377;&#25928;&#22320;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#35270;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35838;&#31243;&#34920;&#26469;&#36880;&#28176;&#23398;&#20064;&#26412;&#22320;&#21644;&#20840;&#23616;&#35270;&#22270;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#20419;&#36827;&#20004;&#31181;&#35270;&#22270;&#20043;&#38388;&#30340;&#19981;&#21516;&#20043;&#22788;&#65292;&#20351;&#20840;&#23616;&#35270;&#22270;&#26356;&#31232;&#30095;&#20197;&#33410;&#30465;FL&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building recommendation systems via federated learning (FL) is a new emerging challenge for advancing next-generation Internet service and privacy protection. Existing approaches train shared item embedding by FL while keeping the user embedding private on client side. However, item embedding identical for all clients cannot capture users' individual differences on perceiving the same item and thus leads to poor personalization. Moreover, dense item embedding in FL results in expensive communication cost and latency. To address these challenges, we propose Federated Recommendation with Additive Personalization (FedRAP), which learns a global view of items via FL and a personalized view locally on each user. FedRAP enforces sparsity of the global view to save FL's communication cost and encourages difference between the two views through regularization. We propose an effective curriculum to learn the local and global views progressively with increasing regularization weights. To produce
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19981;&#36830;&#32493;&#20998;&#24067;&#30340;&#32593;&#32476;&#25910;&#20837;&#31649;&#29702;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#20854;&#22312;&#27492;&#27169;&#22411;&#19979;&#23454;&#29616;&#20102;&#23545;&#25968;&#32423;&#21035;&#30340;&#36951;&#25022;&#12290;&#36825;&#26159;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#8220;&#36864;&#21270;&#8221;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#39318;&#27425;&#22312;&#20855;&#26377;&#36830;&#32493;&#20540;&#30340;NRM&#27169;&#22411;&#20013;&#23454;&#29616;&#23545;&#25968;&#32423;&#21035;&#36951;&#25022;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.07996</link><description>&lt;p&gt;
&#36864;&#21270;&#26159;&#21487;&#20197;&#25509;&#21463;&#30340;&#65306;&#24102;&#26377;&#19981;&#36830;&#32493;&#20998;&#24067;&#30340;&#32593;&#32476;&#25910;&#20837;&#31649;&#29702;&#20013;&#30340;&#23545;&#25968;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Degeneracy is OK: Logarithmic Regret for Network Revenue Management with Indiscrete Distributions. (arXiv:2210.07996v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07996
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19981;&#36830;&#32493;&#20998;&#24067;&#30340;&#32593;&#32476;&#25910;&#20837;&#31649;&#29702;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#20854;&#22312;&#27492;&#27169;&#22411;&#19979;&#23454;&#29616;&#20102;&#23545;&#25968;&#32423;&#21035;&#30340;&#36951;&#25022;&#12290;&#36825;&#26159;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#8220;&#36864;&#21270;&#8221;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#39318;&#27425;&#22312;&#20855;&#26377;&#36830;&#32493;&#20540;&#30340;NRM&#27169;&#22411;&#20013;&#23454;&#29616;&#23545;&#25968;&#32423;&#21035;&#36951;&#25022;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#25509;&#21463;/&#25298;&#32477;&#20915;&#31574;&#21644;T&#27425;&#29420;&#31435;&#21516;&#20998;&#24067;&#21040;&#36798;&#30340;&#32463;&#20856;&#32593;&#32476;&#25910;&#20837;&#31649;&#29702;&#65288;NRM&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20998;&#24067;&#24418;&#24335;&#65292;&#27599;&#20010;&#21040;&#36798;&#24517;&#39035;&#23646;&#20110;&#26377;&#38480;&#25968;&#37327;&#30340;&#21487;&#33021;&#31867;&#21035;&#20043;&#19968;&#65292;&#27599;&#20010;&#31867;&#21035;&#20855;&#26377;&#30830;&#23450;&#30340;&#36164;&#28304;&#28040;&#32791;&#21521;&#37327;&#65292;&#20294;&#26159;&#19968;&#20010;&#22312;&#21306;&#38388;&#19978;&#36830;&#32493;&#20998;&#24067;&#30340;&#38543;&#26426;&#20540;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#27492;&#27169;&#22411;&#19979;&#23454;&#29616;&#20102;O(log^2 T)&#30340;&#36951;&#25022;&#65292;&#21807;&#19968;&#65288;&#24517;&#35201;&#65289;&#30340;&#20551;&#35774;&#26159;&#27010;&#29575;&#23494;&#24230;&#36828;&#31163;0&#12290;&#25105;&#20204;&#24471;&#21040;&#20102;&#31532;&#20108;&#20010;&#32467;&#26524;&#65292;&#22312;&#20108;&#38454;&#22686;&#38271;&#30340;&#39069;&#22806;&#20551;&#35774;&#19979;&#65292;&#23454;&#29616;&#20102;O(log T)&#30340;&#36951;&#25022;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#27809;&#26377;&#20219;&#20309;&#8220;&#36864;&#21270;&#8221;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20855;&#26377;&#36830;&#32493;&#20540;&#30340;NRM&#27169;&#22411;&#20013;&#23454;&#29616;&#23545;&#25968;&#32423;&#21035;&#36951;&#25022;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#36890;&#36807;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#36793;&#30028;mypopic regret&#65292;&#31163;&#32447;&#20998;&#37197;&#30340;&#8220;&#21322;&#27969;&#20307;&#8221;&#25918;&#26494;&#20197;&#21450;&#25913;&#36827;&#36793;&#30028;&#30340;&#26032;&#25216;&#26415;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the classical Network Revenue Management (NRM) problem with accept/reject decisions and $T$ IID arrivals. We consider a distributional form where each arrival must fall under a finite number of possible categories, each with a deterministic resource consumption vector, but a random value distributed continuously over an interval. We develop an online algorithm that achieves $O(\log^2 T)$ regret under this model, with the only (necessary) assumption being that the probability densities are bounded away from 0. We derive a second result that achieves $O(\log T)$ regret under an additional assumption of second-order growth. To our knowledge, these are the first results achieving logarithmic-level regret in an NRM model with continuous values that do not require any kind of ``non-degeneracy'' assumptions. Our results are achieved via new techniques including a new method of bounding myopic regret, a ``semi-fluid'' relaxation of the offline allocation, and an improved bound on the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#23545;&#19981;&#21516;&#20154;&#32676;&#30340;&#24433;&#21709;&#26159;&#19981;&#24179;&#31561;&#30340;&#65292;&#34429;&#28982;&#23427;&#20250;&#22312;&#25152;&#26377;&#20154;&#21475;&#32676;&#20307;&#20013;&#20135;&#29983;&#35823;&#24046;&#65292;&#20294;&#35823;&#24046;&#30340;&#31867;&#22411;&#20250;&#26377;&#31995;&#32479;&#24615;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24179;&#26435;&#20449;&#24687;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#28040;&#38500;&#36825;&#31181;&#24046;&#24322;&#24182;&#25193;&#22823;&#26426;&#20250;&#30340;&#33719;&#21462;&#65292;&#36825;&#21487;&#20197;&#20316;&#20026;&#24179;&#26435;&#34892;&#21160;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2102.10019</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#24179;&#31561;&#24433;&#21709;&#65306;&#24179;&#26435;&#34892;&#21160;&#19982;&#24179;&#26435;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
The Disparate Impact of Uncertainty: Affirmative Action vs. Affirmative Information. (arXiv:2102.10019v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.10019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#23545;&#19981;&#21516;&#20154;&#32676;&#30340;&#24433;&#21709;&#26159;&#19981;&#24179;&#31561;&#30340;&#65292;&#34429;&#28982;&#23427;&#20250;&#22312;&#25152;&#26377;&#20154;&#21475;&#32676;&#20307;&#20013;&#20135;&#29983;&#35823;&#24046;&#65292;&#20294;&#35823;&#24046;&#30340;&#31867;&#22411;&#20250;&#26377;&#31995;&#32479;&#24615;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24179;&#26435;&#20449;&#24687;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#28040;&#38500;&#36825;&#31181;&#24046;&#24322;&#24182;&#25193;&#22823;&#26426;&#20250;&#30340;&#33719;&#21462;&#65292;&#36825;&#21487;&#20197;&#20316;&#20026;&#24179;&#26435;&#34892;&#21160;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proves that uncertainty has a disparate impact on different demographic groups, with varying types of errors. The proposed strategy, called Affirmative Information, can eliminate this disparity and broaden access to opportunity, serving as an alternative to Affirmative Action.
&lt;/p&gt;
&lt;p&gt;
&#20687;&#36151;&#27454;&#25209;&#20934;&#12289;&#21307;&#30103;&#24178;&#39044;&#21644;&#22823;&#23398;&#24405;&#21462;&#36825;&#26679;&#30340;&#20851;&#38190;&#20915;&#31574;&#26159;&#22312;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39044;&#27979;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#20855;&#26377;&#19981;&#24179;&#31561;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#23427;&#20250;&#22312;&#25152;&#26377;&#20154;&#21475;&#32676;&#20307;&#20013;&#20135;&#29983;&#35823;&#24046;&#65292;&#20294;&#35823;&#24046;&#30340;&#31867;&#22411;&#20250;&#26377;&#31995;&#32479;&#24615;&#30340;&#21464;&#21270;&#65306;&#24179;&#22343;&#32467;&#26524;&#36739;&#39640;&#30340;&#32676;&#20307;&#36890;&#24120;&#34987;&#20998;&#37197;&#26356;&#39640;&#30340;&#20551;&#38451;&#24615;&#29575;&#65292;&#32780;&#24179;&#22343;&#32467;&#26524;&#36739;&#20302;&#30340;&#32676;&#20307;&#21017;&#34987;&#20998;&#37197;&#26356;&#39640;&#30340;&#20551;&#38452;&#24615;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#39069;&#22806;&#30340;&#25968;&#25454;&#33719;&#21462;&#21487;&#20197;&#28040;&#38500;&#36825;&#31181;&#24046;&#24322;&#24182;&#25193;&#22823;&#26426;&#20250;&#30340;&#33719;&#21462;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#24179;&#26435;&#20449;&#24687;&#30340;&#31574;&#30053;&#21487;&#20197;&#20316;&#20026;&#24179;&#26435;&#34892;&#21160;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Critical decisions like loan approvals, medical interventions, and college admissions are guided by predictions made in the presence of uncertainty. In this paper, we prove that uncertainty has a disparate impact. While it imparts errors across all demographic groups, the types of errors vary systematically: Groups with higher average outcomes are typically assigned higher false positive rates, while those with lower average outcomes are assigned higher false negative rates. We show that additional data acquisition can eliminate the disparity and broaden access to opportunity. The strategy, which we call Affirmative Information, could stand as an alternative to Affirmative Action.
&lt;/p&gt;</description></item></channel></rss>