<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#31181;&#23376;&#21305;&#37197;&#30340;&#20266;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#21333;&#35789;&#21024;&#38500;&#26469;&#32531;&#35299;&#22240;&#35268;&#21017;&#27880;&#20837;&#30340;&#26631;&#31614;&#20559;&#35265;&#32780;&#24102;&#26469;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#20854;&#24615;&#33021;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.14794</link><description>&lt;p&gt;
&#30465;&#24515;&#23398;&#20064;&#21464;&#24471;&#39046;&#20808;&#65306;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;&#31616;&#21333;&#31181;&#23376;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Debiasing Made State-of-the-art: Revisiting the Simple Seed-based Weak Supervision for Text Classification. (arXiv:2305.14794v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#31181;&#23376;&#21305;&#37197;&#30340;&#20266;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#21333;&#35789;&#21024;&#38500;&#26469;&#32531;&#35299;&#22240;&#35268;&#21017;&#27880;&#20837;&#30340;&#26631;&#31614;&#20559;&#35265;&#32780;&#24102;&#26469;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#20854;&#24615;&#33021;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#22797;&#26434;&#30340;&#26041;&#27861;&#65292;&#23558;&#39640;&#23618;&#27425;&#30340;&#20154;&#31867;&#21551;&#21457;&#24335;&#26041;&#27861;&#36716;&#21270;&#20026;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#31181;&#23376;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#29983;&#25104;&#20266;&#26631;&#31614;&#30340;&#26368;&#31616;&#21333;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#30340;&#24378;&#22823;&#24615;&#33021;&#12290;&#25105;&#20204;&#34920;&#26126;&#31181;&#23376;&#21305;&#37197;&#30340;&#26377;&#38480;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#22240;&#20110;&#31181;&#23376;&#21305;&#37197;&#35268;&#21017;&#27880;&#20837;&#30340;&#26631;&#31614;&#20559;&#24046;&#65292;&#36825;&#20250;&#38459;&#27490;&#20998;&#31867;&#22120;&#23398;&#20064;&#21487;&#38752;&#30340;&#32622;&#20449;&#24230;&#26469;&#36873;&#25321;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#31616;&#21333;&#22320;&#21024;&#38500;&#21305;&#37197;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#31181;&#23376;&#35789;&#21487;&#20197;&#32531;&#35299;&#26631;&#31614;&#20559;&#24046;&#24182;&#24110;&#21161;&#23398;&#20064;&#26356;&#22909;&#30340;&#32622;&#20449;&#24230;&#12290;&#38543;&#21518;&#65292;&#31181;&#23376;&#21305;&#37197;&#30340;&#24615;&#33021;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#65292;&#20351;&#23427;&#36798;&#21040;&#25110;&#29978;&#33267;&#36229;&#36807;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22788;&#29702;&#31181;&#23376;&#35789;&#19981;&#20026;&#20154;&#30693;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#24314;&#35758;&#31616;&#21333;&#22320;&#21024;&#38500;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#21333;&#35789;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in weakly supervised text classification mostly focus on designing sophisticated methods to turn high-level human heuristics into quality pseudo-labels. In this paper, we revisit the seed matching-based method, which is arguably the simplest way to generate pseudo-labels, and show that its power was greatly underestimated. We show that the limited performance of seed matching is largely due to the label bias injected by the simple seed-match rule, which prevents the classifier from learning reliable confidence for selecting high-quality pseudo-labels. Interestingly, simply deleting the seed words present in the matched input texts can mitigate the label bias and help learn better confidence. Subsequently, the performance achieved by seed matching can be improved significantly, making it on par with or even better than the state-of-the-art. Furthermore, to handle the case when the seed words are not made known, we propose to simply delete the word tokens in the input tex
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38646;&#26679;&#26412;&#20219;&#21153;&#20559;&#22909;&#30340;&#19981;&#31934;&#30830;&#36125;&#21494;&#26031;&#32487;&#32493;&#23398;&#20064;&#65288;IBCL&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#20998;&#24067;&#20984;&#22771;&#24418;&#24335;&#30340;&#30693;&#35782;&#24211;&#65292;&#24182;&#20351;&#29992;&#38646;&#26679;&#26412;&#33719;&#21462;&#27169;&#22411;&#20197;&#28385;&#36275;&#19981;&#21516;&#30340;&#20559;&#22909;&#65292;&#20351;&#24471;&#22312;&#20855;&#26377;&#22823;&#37327;&#20219;&#21153;&#20559;&#22909;&#30340;&#24773;&#20917;&#19979;&#26356;&#21152;&#21487;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.14782</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#20219;&#21153;&#20559;&#22909;&#30340;&#19981;&#31934;&#30830;&#36125;&#21494;&#26031;&#32487;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Task Preference Addressing Enabled by Imprecise Bayesian Continual Learning. (arXiv:2305.14782v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14782
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38646;&#26679;&#26412;&#20219;&#21153;&#20559;&#22909;&#30340;&#19981;&#31934;&#30830;&#36125;&#21494;&#26031;&#32487;&#32493;&#23398;&#20064;&#65288;IBCL&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#20998;&#24067;&#20984;&#22771;&#24418;&#24335;&#30340;&#30693;&#35782;&#24211;&#65292;&#24182;&#20351;&#29992;&#38646;&#26679;&#26412;&#33719;&#21462;&#27169;&#22411;&#20197;&#28385;&#36275;&#19981;&#21516;&#30340;&#20559;&#22909;&#65292;&#20351;&#24471;&#22312;&#20855;&#26377;&#22823;&#37327;&#20219;&#21153;&#20559;&#22909;&#30340;&#24773;&#20917;&#19979;&#26356;&#21152;&#21487;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;&#20110;&#36890;&#29992;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#32487;&#32493;&#23398;&#20064;&#20063;&#20855;&#26377;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#29305;&#24615;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#19981;&#21516;&#20219;&#21153;&#30340;&#24615;&#33021;&#20043;&#38388;&#36827;&#34892;&#24179;&#34913;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#20026;&#20102;&#20248;&#21270;&#24403;&#21069;&#20219;&#21153;&#20998;&#24067;&#65292;&#21487;&#33021;&#38656;&#35201;&#22312;&#19968;&#20123;&#20219;&#21153;&#19978;&#29306;&#29298;&#24615;&#33021;&#20197;&#25552;&#39640;&#20854;&#20182;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#36825;&#24847;&#21619;&#30528;&#23384;&#22312;&#22810;&#20010;&#27169;&#22411;&#65292;&#27599;&#20010;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#26102;&#38388;&#37117;&#26159;&#26368;&#20248;&#30340;&#65292;&#27599;&#20010;&#27169;&#22411;&#37117;&#33021;&#22815;&#35299;&#20915;&#19981;&#21516;&#30340;&#20219;&#21153;-&#24615;&#33021;&#26435;&#34913;&#38382;&#39064;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#35752;&#35770;&#22914;&#20309;&#35757;&#32451;&#29305;&#23450;&#30340;&#27169;&#22411;&#20197;&#28385;&#36275;&#20132;&#26131;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31639;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;&#37319;&#26679;&#24320;&#38144;-&#22312;&#23384;&#22312;&#22810;&#20010;&#65292;&#21487;&#33021;&#26159;&#26080;&#38480;&#25968;&#37327;&#30340;&#20559;&#22909;&#26102;&#20250;&#20135;&#29983;&#24456;&#22823;&#30340;&#36127;&#25285;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#31934;&#30830;&#36125;&#21494;&#26031;&#32487;&#32493;&#23398;&#20064;&#65288;IBCL&#65289;&#12290;&#19968;&#26086;&#26377;&#26032;&#20219;&#21153;&#65292;IBCL&#20250;&#65288;1&#65289;&#26356;&#26032;&#19968;&#20010;&#20197;&#27169;&#22411;&#21442;&#25968;&#20998;&#24067;&#20984;&#22771;&#24418;&#24335;&#23384;&#22312;&#30340;&#30693;&#35782;&#24211;&#65292;&#65288;2&#65289;&#24182;&#20351;&#29992;&#38646;&#26679;&#26412;&#33719;&#21462;&#29305;&#23450;&#27169;&#22411;&#20197;&#28385;&#36275;&#19981;&#21516;&#30340;&#20559;&#22909;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;IBCL&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#25968;&#25454;&#23601;&#33021;&#20026;&#19968;&#20010;&#29305;&#23450;&#30340;&#20219;&#21153;&#20559;&#22909;&#29983;&#25104;&#26032;&#30340;&#27169;&#22411;&#65292;&#20351;&#24471;&#22312;&#20855;&#26377;&#22823;&#37327;&#20219;&#21153;&#20559;&#22909;&#30340;&#24773;&#20917;&#19979;&#26356;&#21152;&#21487;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Like generic multi-task learning, continual learning has the nature of multi-objective optimization, and therefore faces a trade-off between the performance of different tasks. That is, to optimize for the current task distribution, it may need to compromise performance on some tasks to improve on others. This means there exist multiple models that are each optimal at different times, each addressing a distinct task-performance trade-off. Researchers have discussed how to train particular models to address specific preferences on these trade-offs. However, existing algorithms require additional sample overheads -- a large burden when there are multiple, possibly infinitely many, preferences. As a response, we propose Imprecise Bayesian Continual Learning (IBCL). Upon a new task, IBCL (1) updates a knowledge base in the form of a convex hull of model parameter distributions and (2) obtains particular models to address preferences with zero-shot. That is, IBCL does not require any additi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;Twitter&#19978;&#20998;&#20139;&#30340;&#22270;&#20687;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#26367;&#20195;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#12290;&#36890;&#36807;CLIP&#21069;&#32512;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#22270;&#20687;&#21644;&#25512;&#25991;&#20013;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#29983;&#25104;&#20851;&#20110;&#22270;&#20687;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#26367;&#20195;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.14779</link><description>&lt;p&gt;
Twitter&#22270;&#20687;&#30340;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#26367;&#20195;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text Conditional Alt-Text Generation for Twitter Images. (arXiv:2305.14779v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;Twitter&#19978;&#20998;&#20139;&#30340;&#22270;&#20687;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#26367;&#20195;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#12290;&#36890;&#36807;CLIP&#21069;&#32512;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#22270;&#20687;&#21644;&#25512;&#25991;&#20013;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#29983;&#25104;&#20851;&#20110;&#22270;&#20687;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#26367;&#20195;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31038;&#20132;&#23186;&#20307;&#29305;&#21035;&#26159;Twitter&#19978;&#20998;&#20139;&#30340;&#22270;&#20687;&#29983;&#25104;&#26367;&#20195;&#25991;&#26412;&#65288;&#25110;alt-text&#65289;&#25551;&#36848;&#30340;&#26041;&#27861;&#12290;&#19982;&#22270;&#20687;&#30340;&#23383;&#24149;&#19981;&#21516;&#65292;&#25991;&#26412;&#26367;&#25442;&#25991;&#26412;&#26356;&#21152;&#30452;&#30333;&#25551;&#36848;&#21644;&#19978;&#19979;&#25991;&#29305;&#23450;&#12290;&#27492;&#22806;&#65292;&#20851;&#38190;&#26159;&#65292;&#21457;&#24067;&#21040;Twitter&#19978;&#30340;&#22270;&#20687;&#36890;&#24120;&#26159;&#30001;&#29992;&#25143;&#32534;&#20889;&#30340;&#25991;&#26412;&#38468;&#21152;&#30340;&#65292;&#23613;&#31649;&#36825;&#20123;&#25991;&#26412;&#19981;&#19968;&#23450;&#25551;&#36848;&#22270;&#20687;&#65292;&#20294;&#21487;&#33021;&#25552;&#20379;&#26377;&#29992;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#22914;&#26524;&#27491;&#30830;&#21033;&#29992;&#21487;&#20197;&#25552;&#20379;&#20449;&#24687;&#65292;&#20363;&#22914;&#25512;&#25991;&#21487;&#33021;&#20250;&#21629;&#21517;&#22270;&#29255;&#20013;&#27169;&#22411;&#20043;&#21069;&#27809;&#26377;&#35265;&#36807;&#30340;&#19981;&#24120;&#35265;&#30340;&#23545;&#35937;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;CLIP&#21069;&#32512;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#25552;&#21462;&#22270;&#20687;&#30340;&#23884;&#20837;&#24182;&#23558;&#20854;&#20256;&#36882;&#32473;&#26144;&#23556;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36755;&#20986;&#21333;&#35789;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#30701;&#24207;&#21015;&#65292;&#25110;&#31216;&#20026;&#8220;&#21069;&#32512;&#8221;&#65292;&#25105;&#20204;&#23558;&#25512;&#25991;&#26412;&#36523;&#30340;&#25991;&#26412;&#20063;&#36830;&#25509;&#21040;&#20854;&#20013;&#12290;&#36825;&#26679;&#65292;&#27169;&#22411;&#23601;&#21487;&#20197;&#22312;&#25991;&#31456;&#20013;&#26465;&#20214;&#21270;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#12290;&#28982;&#21518;&#23558;&#21512;&#24182;&#30340;&#22810;&#27169;&#24335;&#21069;&#32512;&#20316;&#20026;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we present an approach for generating alternative text (or alt-text) descriptions for images shared on social media, specifically Twitter. This task is more than just a special case of image captioning, as alt-text is both more literally descriptive and context-specific. Also critically, images posted to Twitter are often accompanied by user-written text that despite not necessarily describing the image may provide useful context that if properly leveraged can be informative -- e.g. the tweet may name an uncommon object in the image that the model has not previously seen. We address this with a CLIP prefix model that extracts an embedding of the image and passes it to a mapping network that outputs a short sequence in word embedding space, or a ``prefix'', to which we also concatenate the text from the tweet itself. This lets the model condition on both visual and textual information from the post. The combined multimodal prefix is then fed as a prompt to a pretrained lang
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#21322;&#23545;&#20598;&#20844;&#24335;&#30340;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;OT&#65292;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;OT&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.14777</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#21322;&#23545;&#20598;&#20844;&#24335;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling through the Semi-dual Formulation of Unbalanced Optimal Transport. (arXiv:2305.14777v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#21322;&#23545;&#20598;&#20844;&#24335;&#30340;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;OT&#65292;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;OT&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#38382;&#39064;&#30740;&#31350;&#19968;&#31181;&#36816;&#36755;&#26144;&#23556;&#65292;&#33021;&#22815;&#22312;&#26368;&#23567;&#21270;&#32473;&#23450;&#25104;&#26412;&#20989;&#25968;&#30340;&#21516;&#26102;&#36830;&#25509;&#20004;&#20010;&#20998;&#24067;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;OT&#24050;&#34987;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#20013;&#30340;&#21487;&#36861;&#28335;&#30340;&#20808;&#39564;&#20998;&#24067;&#21644;&#25968;&#25454;&#20043;&#38388;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;OT&#30340;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#31163;&#32676;&#28857;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#35757;&#32451;&#26399;&#38388;&#38754;&#20020;&#20248;&#21270;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#65288;UOT&#65289;&#21322;&#23545;&#20598;&#20844;&#24335;&#30340;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#12290;&#19982;OT&#19981;&#21516;&#65292;UOT&#28040;&#38500;&#20102;&#20998;&#24067;&#21305;&#37197;&#30340;&#30828;&#24615;&#32422;&#26463;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#23545;&#25239;&#31163;&#32676;&#28857;&#30340;&#40065;&#26834;&#24615;&#65292;&#35757;&#32451;&#26399;&#38388;&#30340;&#31283;&#23450;&#24615;&#20197;&#21450;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20123;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;UOT&#20043;&#38388;&#20998;&#24067;&#24046;&#24322;&#30340;&#29702;&#35770;&#19978;&#38480;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;OT&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;CIFAR-10&#21644;CelebA-HQ-256&#19978;&#23454;&#29616;&#20102;&#20998;&#21035;&#20026;2.97&#21644;5.80&#30340;FID&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport (OT) problem investigates a transport map that bridges two distributions while minimizing a given cost function. In this regard, OT between tractable prior distribution and data has been utilized for generative modeling tasks. However, OT-based methods are susceptible to outliers and face optimization challenges during training. In this paper, we propose a novel generative model based on the semi-dual formulation of Unbalanced Optimal Transport (UOT). Unlike OT, UOT relaxes the hard constraint on distribution matching. This approach provides better robustness against outliers, stability during training, and faster convergence. We validate these properties empirically through experiments. Moreover, we study the theoretical upper-bound of divergence between distributions in UOT. Our model outperforms existing OT-based generative models, achieving FID scores of 2.97 on CIFAR-10 and 5.80 on CelebA-HQ-256.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#26694;&#26550;&#26469;&#34913;&#37327;PLMs&#20013;&#21442;&#25968;&#21270;&#30693;&#35782;&#30340;&#21033;&#29992;&#65292;&#30740;&#31350;&#21457;&#29616;PLMs&#23384;&#22312;&#24050;&#33719;&#21462;&#30340;&#30693;&#35782;&#21644;&#21033;&#29992;&#30340;&#30693;&#35782;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#26377;&#38480;&#30340;&#40065;&#26834;&#24615;&#65292;&#36739;&#22823;&#30340;&#27169;&#22411;&#21487;&#20197;&#24357;&#34917;&#24050;&#33719;&#21462;&#30693;&#35782;&#30340;&#24046;&#36317;&#65292;&#20294;&#21033;&#29992;&#30693;&#35782;&#30340;&#24046;&#36317;&#20173;&#28982;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2305.14775</link><description>&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#27979;&#37327;&#30693;&#35782;&#33719;&#21462;&#21644;&#21033;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Measuring the Knowledge Acquisition-Utilization Gap in Pretrained Language Models. (arXiv:2305.14775v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#26694;&#26550;&#26469;&#34913;&#37327;PLMs&#20013;&#21442;&#25968;&#21270;&#30693;&#35782;&#30340;&#21033;&#29992;&#65292;&#30740;&#31350;&#21457;&#29616;PLMs&#23384;&#22312;&#24050;&#33719;&#21462;&#30340;&#30693;&#35782;&#21644;&#21033;&#29992;&#30340;&#30693;&#35782;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#26377;&#38480;&#30340;&#40065;&#26834;&#24615;&#65292;&#36739;&#22823;&#30340;&#27169;&#22411;&#21487;&#20197;&#24357;&#34917;&#24050;&#33719;&#21462;&#30693;&#35782;&#30340;&#24046;&#36317;&#65292;&#20294;&#21033;&#29992;&#30693;&#35782;&#30340;&#24046;&#36317;&#20173;&#28982;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#33719;&#21462;&#20102;&#22823;&#37327;&#30340;&#30693;&#35782;&#65292;&#20294;&#30446;&#21069;&#20173;&#19981;&#28165;&#26970;&#36825;&#20123;&#21442;&#25968;&#21270;&#30693;&#35782;&#20013;&#26377;&#22810;&#23569;&#23454;&#38469;&#21487;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#26694;&#26550;&#26469;&#34913;&#37327;PLMs&#20013;&#21442;&#25968;&#21270;&#30693;&#35782;&#30340;&#21033;&#29992;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#39318;&#20808;&#20174;PLM&#21442;&#25968;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#38543;&#21518;&#22260;&#32469;&#36825;&#20123;&#25552;&#21462;&#30340;&#30693;&#35782;&#26500;&#24314;&#19979;&#28216;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#23436;&#20840;&#20381;&#36182;&#20110;&#21033;&#29992;&#27169;&#22411;&#25152;&#20855;&#22791;&#30340;&#30693;&#35782;&#65292;&#36991;&#20813;&#20102;&#19981;&#20805;&#20998;&#30340;&#20449;&#21495;&#31561;&#28151;&#28102;&#22240;&#32032;&#12290;&#20316;&#20026;&#19968;&#20010;&#31034;&#20363;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;PLMs&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#24182;&#27979;&#37327;&#20102;125M&#21040;13B&#21442;&#25968;PLMs&#30340;&#21033;&#29992;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65306;&#65288;1&#65289;PLMs&#22312;&#24050;&#33719;&#21462;&#30340;&#30693;&#35782;&#21644;&#21033;&#29992;&#30340;&#30693;&#35782;&#20043;&#38388;&#23384;&#22312;&#20004;&#20010;&#24046;&#36317;&#65292;&#65288;2&#65289;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#65292;&#23427;&#20204;&#22312;&#21033;&#29992;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#40065;&#26834;&#24615;&#65292;&#65288;3&#65289;&#36739;&#22823;&#30340;&#27169;&#22411;&#21487;&#20197;&#24357;&#34917;&#24050;&#33719;&#21462;&#30693;&#35782;&#30340;&#24046;&#36317;&#65292;&#20294;&#21033;&#29992;&#30693;&#35782;&#30340;&#24046;&#36317;&#20173;&#28982;&#23384;&#22312;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#24403;&#21069;PLMs&#22312;&#21033;&#29992;&#24050;&#33719;&#21462;&#30693;&#35782;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
While pre-trained language models (PLMs) have shown evidence of acquiring vast amounts of knowledge, it remains unclear how much of this parametric knowledge is actually usable in performing downstream tasks. We propose a systematic framework to measure parametric knowledge utilization in PLMs. Our framework first extracts knowledge from a PLM's parameters and subsequently constructs a downstream task around this extracted knowledge. Performance on this task thus depends exclusively on utilizing the model's possessed knowledge, avoiding confounding factors like insufficient signal. As an instantiation, we study factual knowledge of PLMs and measure utilization across 125M to 13B parameter PLMs. We observe that: (1) PLMs exhibit two gaps in acquired vs. utilized knowledge, (2) they show limited robustness in utilizing knowledge under distribution shifts, and (3) larger models close the acquired knowledge gap but the utilized knowledge gap remains. Overall, our study provides insights 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33410;&#28857;&#31232;&#30095;BNN&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#21518;&#39564;&#27987;&#24230;&#36895;&#29575;&#25509;&#36817;&#26368;&#23567;&#21270;&#26368;&#20248;&#12290;&#21516;&#26102;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MCMC&#31639;&#27861;&#65292;&#20351;&#33410;&#28857;&#31232;&#30095;BNN&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#22312;&#23454;&#36341;&#20013;&#21464;&#24471;&#21487;&#34892;&#12290;</title><link>http://arxiv.org/abs/2305.14765</link><description>&lt;p&gt;
&#25513;&#30721;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;: &#29702;&#35770;&#20445;&#35777;&#21450;&#21518;&#39564;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Masked Bayesian Neural Networks : Theoretical Guarantee and its Posterior Inference. (arXiv:2305.14765v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33410;&#28857;&#31232;&#30095;BNN&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#21518;&#39564;&#27987;&#24230;&#36895;&#29575;&#25509;&#36817;&#26368;&#23567;&#21270;&#26368;&#20248;&#12290;&#21516;&#26102;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MCMC&#31639;&#27861;&#65292;&#20351;&#33410;&#28857;&#31232;&#30095;BNN&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#22312;&#23454;&#36341;&#20013;&#21464;&#24471;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#26041;&#27861;&#22312;&#23398;&#20064;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#26041;&#38754;&#22791;&#21463;&#20851;&#27880;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#20013;&#12290;&#29305;&#21035;&#22320;&#65292;BNN&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26356;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#20026;&#20102;BNN&#30340;&#25104;&#21151;&#65292;&#23547;&#25214;&#36866;&#24403;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#31639;&#27861;&#26469;&#25214;&#21040;&#22909;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33410;&#28857;&#31232;&#30095;BNN&#27169;&#22411;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#29702;&#35770;&#24615;&#36136;&#21644;&#35745;&#31639;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#30495;&#23454;&#27169;&#22411;&#30340;&#21518;&#39564;&#27987;&#24230;&#36895;&#29575;&#25509;&#36817;&#26368;&#23567;&#21270;&#26368;&#20248;&#65292;&#24182;&#19988;&#36866;&#24212;&#30495;&#23454;&#27169;&#22411;&#30340;&#24179;&#28369;&#24230;&#12290;&#29305;&#21035;&#26159;&#65292;&#35813;&#36866;&#24212;&#24615;&#26159;&#33410;&#28857;&#31232;&#30095;BNN&#30340;&#39318;&#20010;&#36866;&#24212;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MCMC&#31639;&#27861;&#65292;&#20351;&#33410;&#28857;&#31232;&#30095;BNN&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#22312;&#23454;&#36341;&#20013;&#21464;&#24471;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian approaches for learning deep neural networks (BNN) have been received much attention and successfully applied to various applications. Particularly, BNNs have the merit of having better generalization ability as well as better uncertainty quantification. For the success of BNN, search an appropriate architecture of the neural networks is an important task, and various algorithms to find good sparse neural networks have been proposed. In this paper, we propose a new node-sparse BNN model which has good theoretical properties and is computationally feasible. We prove that the posterior concentration rate to the true model is near minimax optimal and adaptive to the smoothness of the true model. In particular the adaptiveness is the first of its kind for node-sparse BNNs. In addition, we develop a novel MCMC algorithm which makes the Bayesian inference of the node-sparse BNN model feasible in practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22522;&#20110;&#25628;&#32034;&#30340;&#26080;&#30417;&#30563;&#35270;&#35273;&#29305;&#24449;&#23398;&#20064;&#8221;&#65288;SUVR&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#26356;&#22909;&#22320;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22312;&#26500;&#24314;&#22270;&#24418;&#21644;&#36873;&#25321;&#36127;&#26679;&#26412;&#31561;&#26041;&#38754;&#20316;&#20102;&#25913;&#36827;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.14754</link><description>&lt;p&gt;
SUVR:&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#30340;&#26080;&#30417;&#30563;&#35270;&#35273;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SUVR: A Search-based Approach to Unsupervised Visual Representation Learning. (arXiv:2305.14754v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22522;&#20110;&#25628;&#32034;&#30340;&#26080;&#30417;&#30563;&#35270;&#35273;&#29305;&#24449;&#23398;&#20064;&#8221;&#65288;SUVR&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#26356;&#22909;&#22320;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22312;&#26500;&#24314;&#22270;&#24418;&#21644;&#36873;&#25321;&#36127;&#26679;&#26412;&#31561;&#26041;&#38754;&#20316;&#20102;&#25913;&#36827;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25910;&#38598;&#27880;&#37322;&#25968;&#25454;&#30340;&#22256;&#38590;&#21644;&#29616;&#20195;&#26694;&#26550;&#20801;&#35768;&#25105;&#20204;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#26080;&#30417;&#30563;&#23398;&#20064;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#35201;&#20040;&#24573;&#30053;&#20102;&#19981;&#21516;&#30456;&#20284;&#24615;&#32423;&#21035;&#19978;&#30340;&#21464;&#21270;&#65292;&#35201;&#20040;&#20165;&#20174;&#19968;&#20010;&#25209;&#27425;&#32771;&#34385;&#36127;&#26679;&#26412;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22270;&#20687;&#23545;&#24212;&#35813;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#19988;&#24212;&#35813;&#20801;&#35768;&#20174;&#25972;&#20010;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#36127;&#26679;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22522;&#20110;&#25628;&#32034;&#30340;&#26080;&#30417;&#30563;&#35270;&#35273;&#29305;&#24449;&#23398;&#20064;&#8221;&#65288;SUVR&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#26356;&#22909;&#22320;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26500;&#24314;&#22270;&#24418;&#65292;&#37319;&#29992;&#22270;&#36941;&#21382;&#30340;&#27010;&#24565;&#26469;&#25506;&#32034;&#27491;&#26679;&#26412;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30830;&#20445;&#21487;&#20197;&#20174;&#25972;&#20010;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#36127;&#26679;&#26412;&#12290;&#22312;&#20116;&#20010;&#22522;&#20934;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23450;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;SUVR&#26126;&#26174;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised learning has grown in popularity because of the difficulty of collecting annotated data and the development of modern frameworks that allow us to learn from unlabeled data. Existing studies, however, either disregard variations at different levels of similarity or only consider negative samples from one batch. We argue that image pairs should have varying degrees of similarity, and the negative samples should be allowed to be drawn from the entire dataset. In this work, we propose Search-based Unsupervised Visual Representation Learning (SUVR) to learn better image representations in an unsupervised manner. We first construct a graph from the image dataset by the similarity between images, and adopt the concept of graph traversal to explore positive samples. In the meantime, we make sure that negative samples can be drawn from the full dataset. Quantitative experiments on five benchmark image classification datasets demonstrate that SUVR can significantly outperform strong
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32467;&#21512;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#24418;&#24335;&#39564;&#35777;&#30340;&#26041;&#27861;&#26469;&#33258;&#21160;&#39564;&#35777;&#21644;&#20462;&#22797;&#36719;&#20214;&#28431;&#27934;&#65292;&#24182;&#36890;&#36807;ESBMC-AI&#20570;&#20986;&#20102;&#27010;&#24565;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.14752</link><description>&lt;p&gt;
&#36208;&#21521;&#36719;&#20214;&#33258;&#24840;&#65306;&#32467;&#21512;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#24418;&#24335;&#39564;&#35777;&#35299;&#20915;&#36719;&#20214;&#23433;&#20840;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A New Era in Software Security: Towards Self-Healing Software via Large Language Models and Formal Verification. (arXiv:2305.14752v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32467;&#21512;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#24418;&#24335;&#39564;&#35777;&#30340;&#26041;&#27861;&#26469;&#33258;&#21160;&#39564;&#35777;&#21644;&#20462;&#22797;&#36719;&#20214;&#28431;&#27934;&#65292;&#24182;&#36890;&#36807;ESBMC-AI&#20570;&#20986;&#20102;&#27010;&#24565;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#24418;&#24335;&#21270;&#39564;&#35777;&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#20351;&#24471;&#36719;&#20214;&#28431;&#27934;&#21487;&#20197;&#24471;&#21040;&#39564;&#35777;&#21644;&#33258;&#21160;&#20462;&#22797;&#12290;&#39318;&#20808;&#21033;&#29992;&#26377;&#38480;&#27169;&#22411;&#26816;&#26597;&#65288;BMC&#65289;&#23450;&#20301;&#36719;&#20214;&#28431;&#27934;&#21644;&#27966;&#29983;&#21453;&#20363;&#12290;&#28982;&#21518;&#65292;&#23558;&#21453;&#20363;&#21644;&#28304;&#20195;&#30721;&#25552;&#20379;&#32473;&#22823;&#35821;&#35328;&#27169;&#22411;&#24341;&#25806;&#36827;&#34892;&#20195;&#30721;&#35843;&#35797;&#21644;&#29983;&#25104;&#65292;&#20174;&#32780;&#25214;&#21040;&#28431;&#27934;&#30340;&#26681;&#26412;&#21407;&#22240;&#24182;&#20462;&#22797;&#20195;&#30721;&#12290;&#26368;&#21518;&#65292;&#21017;&#20351;&#29992;BMC&#39564;&#35777;&#22823;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20462;&#27491;&#29256;&#26412;&#30340;&#20195;&#30721;&#12290; &#20316;&#20026;&#27010;&#24565;&#35777;&#26126;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;ESBMC-AI&#65292;&#23427;&#22522;&#20110;&#39640;&#25928;&#30340;&#22522;&#20110;SMT&#30340;&#19978;&#19979;&#25991;&#26377;&#30028;&#27169;&#22411;&#26816;&#26597;&#22120;&#65288;ESBMC&#65289;&#21644;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;gpt-3.5-turbo&#26469;&#26816;&#27979;&#21644;&#20462;&#22797;C&#31243;&#24207;&#20013;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present a novel solution that combines the capabilities of Large Language Models (LLMs) with Formal Verification strategies to verify and automatically repair software vulnerabilities. Initially, we employ Bounded Model Checking (BMC) to locate the software vulnerability and derive a counterexample. The counterexample provides evidence that the system behaves incorrectly or contains a vulnerability. The counterexample that has been detected, along with the source code, are provided to the LLM engine. Our approach involves establishing a specialized prompt language for conducting code debugging and generation to understand the vulnerability's root cause and repair the code. Finally, we use BMC to verify the corrected version of the code generated by the LLM. As a proof of concept, we create ESBMC-AI based on the Efficient SMT-based Context-Bounded Model Checker (ESBMC) and a pre-trained Transformer model, specifically gpt-3.5-turbo, to detect and fix errors in C program
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#29366;&#24577;RNA&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#26126;&#30830;&#32771;&#34385;&#21644;&#21453;&#26144;RNA&#26500;&#35937;&#22810;&#26679;&#24615;&#22312;&#20854;&#35774;&#35745;&#20013;&#12290;&#20854;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21407;&#29983;&#24207;&#21015;&#30340;&#24674;&#22797;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22810;&#29366;&#24577;&#21644;&#32467;&#26500;&#22810;&#26679;&#21270;&#30340;RNA&#12290;</title><link>http://arxiv.org/abs/2305.14749</link><description>&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#29366;&#24577;RNA&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Multi-State RNA Design with Geometric Multi-Graph Neural Networks. (arXiv:2305.14749v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#29366;&#24577;RNA&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#26126;&#30830;&#32771;&#34385;&#21644;&#21453;&#26144;RNA&#26500;&#35937;&#22810;&#26679;&#24615;&#22312;&#20854;&#35774;&#35745;&#20013;&#12290;&#20854;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21407;&#29983;&#24207;&#21015;&#30340;&#24674;&#22797;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22810;&#29366;&#24577;&#21644;&#32467;&#26500;&#22810;&#26679;&#21270;&#30340;RNA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;RNA&#35774;&#35745;&#22312;&#21512;&#25104;&#29983;&#29289;&#23398;&#21644;&#27835;&#30103;&#24320;&#21457;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;RNA&#22810;&#26679;&#30340;&#29983;&#29289;&#23398;&#21151;&#33021;&#30340;&#22522;&#30784;&#26159;&#23427;&#30340;&#26500;&#35937;&#28789;&#27963;&#24615;&#65292;&#20351;&#21333;&#19968;&#24207;&#21015;&#33021;&#22815;&#37319;&#29992;&#22810;&#31181;&#19981;&#21516;&#30340;&#19977;&#32500;&#32467;&#26500;&#29366;&#24577;&#12290;&#30446;&#21069;&#65292;&#35745;&#31639;&#29983;&#29289;&#20998;&#23376;&#35774;&#35745;&#20219;&#21153;&#32463;&#24120;&#34987;&#25552;&#20986;&#20026;&#36870;&#38382;&#39064;&#65292;&#21363;&#22522;&#20110;&#37319;&#29992;&#21333;&#19968;&#39044;&#26399;&#32467;&#26500;&#26500;&#35937;&#26469;&#35774;&#35745;&#24207;&#21015;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;gRNAde&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#19968;&#32452;&#19977;&#32500;RNA&#39592;&#26550;&#32467;&#26500;&#25805;&#20316;&#30340;&#20960;&#20309;RNA&#35774;&#35745;&#27969;&#31243;&#65292;&#20197;&#26126;&#30830;&#32771;&#34385;&#21644;&#21453;&#26144;RNA&#26500;&#35937;&#22810;&#26679;&#24615;&#22312;&#20854;&#35774;&#35745;&#20013;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#19977;&#32500;RNA&#35774;&#35745;&#25968;&#25454;&#38598;&#19978;&#28436;&#31034;&#20102;gRNAde&#30340;&#25928;&#29992;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22810;&#29366;&#24577;&#21644;&#32467;&#26500;&#22810;&#26679;&#21270;&#30340;RNA&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21407;&#29983;&#24207;&#21015;&#30340;&#24674;&#22797;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/chaitjo/geometric-rna-design&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational RNA design has broad applications across synthetic biology and therapeutic development. Fundamental to the diverse biological functions of RNA is its conformational flexibility, enabling single sequences to adopt a variety of distinct 3D states. Currently, computational biomolecule design tasks are often posed as inverse problems, where sequences are designed based on adopting a single desired structural conformation. In this work, we propose gRNAde, a geometric RNA design pipeline that operates on sets of 3D RNA backbone structures to explicitly account for and reflect RNA conformational diversity in its designs. We demonstrate the utility of gRNAde for improving native sequence recovery over single-state approaches on a new large-scale 3D RNA design dataset, especially for multi-state and structurally diverse RNAs. Our code is available at https://github.com/chaitjo/geometric-rna-design
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#20551;&#38463;&#23500;&#27735;&#38046;&#31080;&#65292;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;99&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.14745</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#26816;&#27979;&#38463;&#23500;&#27735;&#20551;&#38046;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Applications of Machine Learning in Detecting Afghan Fake Banknotes. (arXiv:2305.14745v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#20551;&#38463;&#23500;&#27735;&#38046;&#31080;&#65292;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;99&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#38046;&#26159;&#25351;&#26410;&#32463;&#25919;&#24220;&#25209;&#20934;&#30340;&#38750;&#27861;&#20223;&#21046;&#36135;&#24065;&#65292;&#26159;&#19968;&#31181;&#27450;&#35784;&#34892;&#20026;&#12290;&#29305;&#21035;&#26159;&#22312;&#38463;&#23500;&#27735;&#65292;&#20551;&#38046;&#30340;&#26222;&#21450;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#23545;&#32463;&#27982;&#36896;&#25104;&#20102;&#36127;&#38754;&#24433;&#21709;&#12290;&#34429;&#28982;&#38134;&#34892;&#21644;&#21830;&#19994;&#26426;&#26500;&#20351;&#29992;&#36523;&#20221;&#39564;&#35777;&#26426;&#65292;&#20294;&#20844;&#20247;&#26080;&#27861;&#25509;&#35302;&#36825;&#20123;&#31995;&#32479;&#65292;&#38656;&#35201;&#19968;&#20010;&#33021;&#22815;&#26816;&#27979;&#25152;&#26377;&#20154;&#20266;&#36896;&#36135;&#24065;&#30340;&#31243;&#24207;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#20687;&#22788;&#29702;&#26469;&#35782;&#21035;&#29305;&#23450;&#23433;&#20840;&#29305;&#24449;&#30340;&#20266;&#36896;&#38463;&#23500;&#27735;&#38046;&#31080;&#30340;&#26041;&#27861;&#12290;&#20174;&#36755;&#20837;&#22270;&#20687;&#20013;&#25552;&#21462;&#31532;&#19968;&#21644;&#31532;&#20108;&#38454;&#32479;&#35745;&#29305;&#24449;&#65292;&#20351;&#29992;WEKA&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#26500;&#24314;&#27169;&#22411;&#24182;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#12289;PART&#21644;&#26420;&#32032;&#36125;&#21494;&#26031;&#31639;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#22312;&#26816;&#27979;&#20551;&#38463;&#23500;&#27735;&#38046;&#31080;&#26041;&#38754;&#36798;&#21040;&#20102;99&#65285;&#30340;&#24322;&#24120;&#20934;&#30830;&#29575;&#65292;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20316;&#20026;&#37492;&#21035;&#20266;&#24065;&#30340;&#35299;&#20915;&#26041;&#26696;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fake currency, unauthorized imitation money lacking government approval, constitutes a form of fraud. Particularly in Afghanistan, the prevalence of fake currency poses significant challenges and detrimentally impacts the economy. While banks and commercial establishments employ authentication machines, the public lacks access to such systems, necessitating a program that can detect counterfeit banknotes accessible to all. This paper introduces a method using image processing to identify counterfeit Afghan banknotes by analyzing specific security features. Extracting first and second order statistical features from input images, the WEKA machine learning tool was employed to construct models and perform classification with Random Forest, PART, and Na\"ive Bayes algorithms. The Random Forest algorithm achieved exceptional accuracy of 99% in detecting fake Afghan banknotes, indicating the efficacy of the proposed method as a solution for identifying counterfeit currency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#24120;&#20540;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#22312;&#27602;&#24615;&#26816;&#27979;&#20013;&#21463;&#21040;&#20260;&#23475;&#30340;&#20154;&#32676;&#65292;&#21457;&#29616;&#23545;&#20110;&#36825;&#20123;&#24322;&#24120;&#20540;&#65292;&#27169;&#22411;&#24615;&#33021;&#36739;&#24046;&#65292;&#20182;&#20204;&#38754;&#20020;&#30340;&#27602;&#24615;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.14735</link><description>&lt;p&gt;
&#36793;&#32536;&#32858;&#28966;&#65306;&#22522;&#20110;&#24322;&#24120;&#20540;&#30340;&#27602;&#24615;&#26816;&#27979;&#20013;&#21463;&#25439;&#20154;&#32676;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection. (arXiv:2305.14735v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#24120;&#20540;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#22312;&#27602;&#24615;&#26816;&#27979;&#20013;&#21463;&#21040;&#20260;&#23475;&#30340;&#20154;&#32676;&#65292;&#21457;&#29616;&#23545;&#20110;&#36825;&#20123;&#24322;&#24120;&#20540;&#65292;&#27169;&#22411;&#24615;&#33021;&#36739;&#24046;&#65292;&#20182;&#20204;&#38754;&#20020;&#30340;&#27602;&#24615;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#23545;&#36793;&#32536;&#31038;&#21306;&#24433;&#21709;&#30340;&#26631;&#20934;&#26041;&#27861;&#26159;&#30830;&#23450;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#38024;&#23545;&#24369;&#21183;&#32676;&#20307;&#30340;&#20260;&#23475;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#20250;&#25513;&#30422;&#30001;&#20132;&#21449;&#23376;&#32676;&#25110;&#36328;&#20154;&#21475;&#32676;&#20307;&#20849;&#20139;&#30340;&#20260;&#23475;&#27169;&#24335;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#8220;&#36793;&#32536;&#8221;&#23450;&#20041;&#20026;&#20855;&#26377;&#36828;&#31163;&#8220;&#24120;&#24577;&#8221; &#30340;&#20154;&#21475;&#23646;&#24615;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#24230;&#37327;&#38024;&#23545;&#36825;&#20123;&#24322;&#24120;&#20540;&#30340;&#20260;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32676;&#20307;&#30340;&#24615;&#33021;&#24046;&#24322;&#25351;&#25968;&#65288;GPDI&#65289;&#65292;&#20197;&#34913;&#37327;&#25968;&#25454;&#38598;&#32454;&#20998;&#20026;&#23376;&#32452;&#23545;&#38754;&#20020;&#22686;&#21152;&#30340;&#20260;&#23475;&#30340;&#35782;&#21035;&#31243;&#24230;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#26816;&#27979;&#27602;&#24615;&#26816;&#27979;&#20013;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#38024;&#23545;&#24322;&#24120;&#20540;&#30340;&#25991;&#26412;&#22312;&#25152;&#26377;&#31867;&#22411;&#30340;&#27602;&#24615;&#26816;&#39564;&#20013;&#27602;&#24615;&#26356;&#39640;&#65292;&#39640;&#36798;28&#65285;&#33267;86&#65285;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#23545;&#20110;&#20154;&#21475;&#23398;&#24322;&#24120;&#20540;&#65292;&#27169;&#22411;&#24615;&#33021;&#22987;&#32456;&#36739;&#24046;&#65292;&#24322;&#24120;&#20540;&#21644;&#38750;&#24322;&#24120;&#20540;&#20043;&#38388;&#30340;&#38169;&#35823;&#24046;&#36317;&#39640;&#36798;10&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
A standard method for measuring the impacts of AI on marginalized communities is to determine performance discrepancies between specified demographic groups. These approaches aim to address harms toward vulnerable groups, but they obscure harm patterns faced by intersectional subgroups or shared across demographic groups. We instead operationalize "the margins" as data points that are statistical outliers due to having demographic attributes distant from the "norm" and measure harms toward these outliers. We propose a Group-Based Performance Disparity Index (GPDI) that measures the extent to which a subdivision of a dataset into subgroups identifies those facing increased harms. We apply our approach to detecting disparities in toxicity detection and find that text targeting outliers is 28% to 86% more toxic for all types of toxicity examined. We also discover that model performance is consistently worse for demographic outliers, with disparities in error between outliers and non-outli
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#22312;&#20247;&#21253;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#23384;&#22312;&#21518;&#38376;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21482;&#38656;&#27880;&#20837;&#26497;&#23569;&#37327;&#30340;&#24694;&#24847;&#25351;&#20196;&#20415;&#21487;&#27704;&#20037;&#25511;&#21046;&#27169;&#22411;&#34892;&#20026;&#65292;&#19988;&#38590;&#20197;&#34987;&#20462;&#22797;&#65292;&#38656;&#35201;&#26356;&#21152;&#20581;&#20840;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.14710</link><description>&lt;p&gt;
&#35757;&#32451;&#25351;&#20196;&#20316;&#20026;&#21518;&#38376;: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#25972;&#30340;&#21518;&#38376;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models. (arXiv:2305.14710v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14710
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#22312;&#20247;&#21253;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#23384;&#22312;&#21518;&#38376;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21482;&#38656;&#27880;&#20837;&#26497;&#23569;&#37327;&#30340;&#24694;&#24847;&#25351;&#20196;&#20415;&#21487;&#27704;&#20037;&#25511;&#21046;&#27169;&#22411;&#34892;&#20026;&#65292;&#19988;&#38590;&#20197;&#34987;&#20462;&#22797;&#65292;&#38656;&#35201;&#26356;&#21152;&#20581;&#20840;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#22312;&#20247;&#21253;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#20854;&#30446;&#30340;&#26159;&#36798;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;&#35813;&#22521;&#35757;&#33539;&#20363;&#30456;&#20851;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21482;&#38656;&#22312;&#25104;&#21315;&#19978;&#19975;&#30340;&#25968;&#25454;&#20013;&#27880;&#20837;&#26497;&#23569;&#37327;&#30340;&#24694;&#24847;&#25351;&#20196;&#65292;&#20415;&#21487;&#20197;&#36890;&#36807;&#25968;&#25454;&#27602;&#21270;&#26469;&#25511;&#21046;&#27169;&#22411;&#34892;&#20026;&#65292;&#29978;&#33267;&#26080;&#38656;&#20462;&#25913;&#25968;&#25454;&#23454;&#20363;&#25110;&#26631;&#31614;&#26412;&#36523;&#12290;&#36890;&#36807;&#36825;&#31181;&#25351;&#20196;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#22235;&#20010;&#24120;&#29992;&#30340; NLP &#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#36229;&#36807;90% &#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#24341;&#36215;&#26131;&#20110;&#36716;&#31227;&#21040; 15 &#31181;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#25345;&#20037;&#21518;&#38376;&#12290;&#36825;&#31181;&#25915;&#20987;&#36824;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#26377;&#27602;&#25351;&#20196;&#12290;&#26368;&#21518;&#65292;&#35813;&#25915;&#20987;&#26174;&#31034;&#20986;&#23545;&#29616;&#26377;&#25512;&#29702;&#26102;&#38450;&#24481;&#30340;&#25269;&#25239;&#21147;&#12290;&#36825;&#20123;&#21457;&#29616;&#20984;&#26174;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#38656;&#35201;&#26356;&#20026;&#20581;&#20840;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned models are trained on crowdsourcing datasets with task instructions to achieve superior performance. However, in this work we raise security concerns about this training paradigm. Our studies demonstrate that an attacker can inject backdoors by issuing very few malicious instructions among thousands of gathered data and control model behavior through data poisoning, without even the need of modifying data instances or labels themselves. Through such instruction attacks, the attacker can achieve over 90% attack success rate across four commonly used NLP datasets, and cause persistent backdoors that are easily transferred to 15 diverse datasets zero-shot. In this way, the attacker can directly apply poisoned instructions designed for one dataset on many other datasets. Moreover, the poisoned model cannot be cured by continual learning. Lastly, instruction attacks show resistance to existing inference-time defense. These findings highlight the need for more robust defens
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Regret Matching+&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#24555;&#36895;&#25910;&#25947;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#31181;&#20462;&#22797;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#20462;&#22797;&#26041;&#27861;&#27604;&#21407;&#31639;&#27861;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.14709</link><description>&lt;p&gt;
Regret Matching+&#65306;&#28216;&#25103;&#20013;&#30340;&#65288;&#19981;&#65289;&#31283;&#23450;&#24615;&#21644;&#24555;&#36895;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Regret Matching+: (In)Stability and Fast Convergence in Games. (arXiv:2305.14709v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Regret Matching+&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#24555;&#36895;&#25910;&#25947;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#31181;&#20462;&#22797;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#20462;&#22797;&#26041;&#27861;&#27604;&#21407;&#31639;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Regret Matching+&#65288;RM +&#65289;&#21450;&#20854;&#21464;&#20307;&#26159;&#35299;&#20915;&#22823;&#35268;&#27169;&#28216;&#25103;&#30340;&#37325;&#35201;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#25104;&#21151;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#26159;&#19968;&#20010;&#35868;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#20851;&#20110;&#28216;&#25103;&#20013;&#24555;&#36895;&#25910;&#25947;&#30340;&#36827;&#23637;&#20165;&#38480;&#20110;&#26080;&#36951;&#25022;&#31639;&#27861;&#65292;&#22914;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#65292;&#20854;&#28385;&#36275;&#31283;&#23450;&#24615;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20379;&#21453;&#20363;&#65292;&#23637;&#31034;RM +&#21450;&#20854;&#39044;&#27979;&#29256;&#26412;&#21487;&#33021;&#19981;&#31283;&#23450;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#20854;&#20182;&#29609;&#23478;&#36973;&#21463;&#24040;&#22823;&#30340;&#36951;&#25022;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20004;&#31181;&#20462;&#22797;&#26041;&#27861;&#65306;&#37325;&#26032;&#21551;&#21160;&#21644;&#25130;&#26029;RM +&#25152;&#22312;&#30340;&#27491;&#21322;&#36724;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20855;&#26377;&#39044;&#27979;&#30340;RM + &#36827;&#34892;&#19978;&#36848;&#20462;&#22797;&#23601;&#36275;&#20197;&#22312;&#27491;&#24577;&#24418;&#24335;&#30340;&#28216;&#25103;&#20013;&#33719;&#24471;$ O&#65288;T ^ {1/4}&#65289;$&#20010;&#20307;&#36951;&#25022;&#21644;$ O&#65288;1&#65289;$&#31038;&#20250;&#36951;&#25022;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#31283;&#23450;&#25216;&#26415;&#24212;&#29992;&#20110;RM + &#30340;&#33258;&#36275;&#23398;&#20064;&#20013;&#30340;&#20809;&#26126;&#26356;&#26032;&#65292;&#24182;&#35777;&#26126;&#20102;&#31867;&#20284;&#20110;&#26368;&#36817;Clairvoyant&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#24037;&#20316;&#30340;&#33391;&#22909;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#21407;&#22987;&#30340;RM +&#12290;
&lt;/p&gt;
&lt;p&gt;
Regret Matching+ (RM+) and its variants are important algorithms for solving large-scale games. However, a theoretical understanding of their success in practice is still a mystery. Moreover, recent advances on fast convergence in games are limited to no-regret algorithms such as online mirror descent, which satisfy stability. In this paper, we first give counterexamples showing that RM+ and its predictive version can be unstable, which might cause other players to suffer large regret. We then provide two fixes: restarting and chopping off the positive orthant that RM+ works in. We show that these fixes are sufficient to get $O(T^{1/4})$ individual regret and $O(1)$ social regret in normal-form games via RM+ with predictions. We also apply our stabilizing techniques to clairvoyant updates in the uncoupled learning setting for RM+ and prove desirable results akin to recent works for Clairvoyant online mirror descent. Our experiments show the advantages of our algorithms over vanilla RM+
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#39564;&#35777;&#32773;&#19988;&#19981;&#20570;&#39046;&#22495;&#20551;&#35774;&#30340;&#20027;&#24352;&#26657;&#27491;&#31995;&#32479;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#31185;&#23398;&#20107;&#23454;&#38169;&#35823;&#26657;&#27491;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;LLM&#30340;&#25552;&#31034;&#26041;&#27861;&#21644;&#20027;&#24352;&#24863;&#30693;&#30340;&#35299;&#30721;&#36807;&#31243;&#26469;&#25552;&#39640;&#26657;&#27491;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.14707</link><description>&lt;p&gt;
&#23398;&#29983;&#36229;&#36234;&#20102;&#22823;&#24072;&#65306;&#22522;&#20110;GPT-3&#30340;&#31185;&#23398;&#20107;&#23454;&#38169;&#35823;&#26657;&#27491;&#26041;&#27861;&#30340;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
The student becomes the master: Matching GPT3 on Scientific Factual Error Correction. (arXiv:2305.14707v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#39564;&#35777;&#32773;&#19988;&#19981;&#20570;&#39046;&#22495;&#20551;&#35774;&#30340;&#20027;&#24352;&#26657;&#27491;&#31995;&#32479;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#31185;&#23398;&#20107;&#23454;&#38169;&#35823;&#26657;&#27491;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;LLM&#30340;&#25552;&#31034;&#26041;&#27861;&#21644;&#20027;&#24352;&#24863;&#30693;&#30340;&#35299;&#30721;&#36807;&#31243;&#26469;&#25552;&#39640;&#26657;&#27491;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21019;&#24314;&#38169;&#35823;&#26657;&#27491;&#25968;&#25454;&#38598;&#30340;&#25104;&#26412;&#26497;&#39640;&#65292;&#22823;&#22810;&#25968;&#20107;&#23454;&#20027;&#24352;&#26657;&#27491;&#26041;&#27861;&#20381;&#36182;&#20110;&#24378;&#22823;&#30340;&#39564;&#35777;&#27169;&#22411;&#26469;&#25351;&#23548;&#26657;&#27491;&#36807;&#31243;&#12290;&#36825;&#23548;&#33268;&#22312;&#31185;&#23398;&#20107;&#23454;&#26657;&#27491;&#31561;&#39046;&#22495;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#22240;&#20026;&#22909;&#30340;&#39564;&#35777;&#27169;&#22411;&#24182;&#19981;&#24635;&#26159;&#23384;&#22312;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#19981;&#20570;&#39046;&#22495;&#20551;&#35774;&#19988;&#19981;&#38656;&#35201;&#39564;&#35777;&#32773;&#30340;&#20027;&#24352;&#26657;&#27491;&#31995;&#32479;&#65292;&#20294;&#33021;&#22815;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#24615;&#33021; - &#22312;SciFact&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;94&#65285;&#30340;&#20462;&#27491;&#20934;&#30830;&#24615;&#65292;&#22312;SciFact-Open&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;62.5&#65285;&#30340;&#20462;&#27491;&#20934;&#30830;&#24615;&#65292;&#20998;&#21035;&#27604;&#19979;&#19968;&#20010;&#26368;&#22909;&#30340;&#26041;&#27861;&#39640;&#20986;0.5&#65285;&#21644;1.50&#65285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;LLMs&#20013;&#30340;&#25552;&#31034;&#21151;&#33021;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#21019;&#24314;&#19968;&#20010;&#20016;&#23500;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#23436;&#20840;&#30417;&#30563;&#30340;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20027;&#24352;&#24863;&#30693;&#30340;&#35299;&#30721;&#36807;&#31243;&#26469;&#25552;&#39640;&#32416;&#27491;&#20027;&#24352;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#29992;&#20110;&#21019;&#24314;&#25968;&#25454;&#38598;&#30340;LLM&#30456;&#31454;&#20105;&#65292;&#35777;&#26126;&#20102;&#21033;&#29992;&#22522;&#20110;LLM&#30340;&#35757;&#32451;&#25552;&#39640;&#31185;&#23398;&#20027;&#24352;&#26657;&#27491;&#20219;&#21153;&#24615;&#33021;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the prohibitively high cost of creating error correction datasets, most Factual Claim Correction methods rely on a powerful verification model to guide the correction process. This leads to a significant drop in performance in domains like Scientific Claim Correction, where good verification models do not always exist. In this work, we introduce a claim correction system that makes no domain assumptions and does not require a verifier but is able to outperform existing methods by an order of magnitude -- achieving 94% correction accuracy on the SciFact dataset, and 62.5% on the SciFact-Open dataset, compared to the next best methods 0.5% and 1.50% respectively. Our method leverages the power of prompting with LLMs during training to create a richly annotated dataset that can be used for fully supervised training and regularization. We additionally use a claim-aware decoding procedure to improve the quality of corrected claims. Our method is competitive with the very LLM that was
&lt;/p&gt;</description></item><item><title>PruMUX&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#32467;&#26500;&#21270;&#21098;&#26525;&#21644;&#25968;&#25454;&#22797;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;BERT-base&#27169;&#22411;&#30340;&#21534;&#21520;&#37327;&#12290;Auto-PruMUX&#21487;&#20197;&#39044;&#27979;&#20462;&#21098;&#21644;&#22797;&#29992;&#30340;&#39640;&#24615;&#33021;&#21442;&#25968;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.14706</link><description>&lt;p&gt;
PruMUX&#65306;&#21033;&#29992;&#27169;&#22411;&#21387;&#32553;&#22686;&#24378;&#25968;&#25454;&#22797;&#29992;
&lt;/p&gt;
&lt;p&gt;
PruMUX: Augmenting Data Multiplexing with Model Compression. (arXiv:2305.14706v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14706
&lt;/p&gt;
&lt;p&gt;
PruMUX&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#32467;&#26500;&#21270;&#21098;&#26525;&#21644;&#25968;&#25454;&#22797;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;BERT-base&#27169;&#22411;&#30340;&#21534;&#21520;&#37327;&#12290;Auto-PruMUX&#21487;&#20197;&#39044;&#27979;&#20462;&#21098;&#21644;&#22797;&#29992;&#30340;&#39640;&#24615;&#33021;&#21442;&#25968;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#26029;&#25193;&#22823;&#65292;&#25552;&#39640;&#20854;&#25512;&#29702;&#25928;&#29575;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290; &#20808;&#21069;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#27169;&#22411;&#20462;&#21098;&#65292;&#30693;&#35782;&#33976;&#39311;&#21644;&#25968;&#25454;&#22797;&#29992;&#31561;&#25216;&#26415;&#65292;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#21534;&#21520;&#37327;&#32780;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#32467;&#26500;&#21270;&#21098;&#26525;&#21644;&#25968;&#25454;&#22797;&#29992;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#22686;&#24378;&#20004;&#31181;&#26041;&#27861;&#33719;&#24471;&#30340;&#21152;&#36895;&#20248;&#21183;&#12290; &#25105;&#20204;&#30340;&#26041;&#27861;PruMUX&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#38408;&#20540;&#20026;80&#65285;&#21040;74&#65285;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;BERT-base&#27169;&#22411;&#30456;&#27604;&#65292;&#21487;&#33719;&#24471;7.5-29.5&#20493;&#30340;&#21534;&#21520;&#37327;&#25552;&#39640;&#12290; &#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#20004;&#31181;&#25216;&#26415;&#20013;&#19981;&#21516;&#21442;&#25968;&#65288;&#20363;&#22914;&#31232;&#30095;&#24615;&#21644;&#22797;&#29992;&#22240;&#23376;&#65289;&#30340;&#21508;&#31181;&#32452;&#21512;&#65292;&#20197;&#25552;&#20379;&#26377;&#20851;&#20934;&#30830;&#24615;&#21644;&#21534;&#21520;&#37327;&#20043;&#38388;&#26435;&#34913;&#30340;&#32508;&#21512;&#20998;&#26512;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Auto-PruMUX&#65292;&#36825;&#26159;&#19968;&#31181;&#20803;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#26399;&#26395;&#30340;&#31934;&#24230;&#25439;&#22833;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#20462;&#21098;&#21644;&#22797;&#29992;&#30340;&#39640;&#24615;&#33021;&#21442;&#25968;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models increase in size by the day, methods for efficient inference are critical to leveraging their capabilities for various applications. Prior work has investigated techniques like model pruning, knowledge distillation, and data multiplexing to increase model throughput without sacrificing accuracy. In this paper, we combine two such methods -structured pruning and data multiplexing -- to compound the speedup gains obtained by either method. Our approach, PruMUX, obtains up to 7.5-29.5X throughput improvement over BERT-base model with accuracy threshold from 80% to 74%. We further study various combinations of parameters (such as sparsity and multiplexing factor) in the two techniques to provide a comprehensive analysis of the tradeoff between accuracy and throughput in the resulting models. We then propose Auto-PruMUX, a meta-level model that can predict the high-performance parameters for pruning and multiplexing given a desired accuracy loss budget, providing a prac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25512;&#23548;&#21644;&#35780;&#20272;&#20102;&#22235;&#31181;&#36125;&#21494;&#26031;&#25209;&#27425;&#36172;&#21338;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#30830;&#23450;&#27969;&#37327;&#20998;&#37197;&#65292;&#20840;&#38754;&#35780;&#20272;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#21487;&#20449;&#24230;&#12289;&#25935;&#24863;&#24615;&#21644;&#21518;&#24724;&#24230;&#65292;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#22522;&#20110;&#25209;&#27425;&#30340;&#36125;&#21494;&#26031;&#31639;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.14704</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#25277;&#26679;&#31639;&#27861;&#30340;&#22312;&#32447;&#33258;&#36866;&#24212;&#27969;&#37327;&#23454;&#39564;&#30340;&#23454;&#29992;&#25209;&#27425;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Evaluation on Practical Batch Bayesian Sampling Algorithms for Online Adaptive Traffic Experimentation. (arXiv:2305.14704v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25512;&#23548;&#21644;&#35780;&#20272;&#20102;&#22235;&#31181;&#36125;&#21494;&#26031;&#25209;&#27425;&#36172;&#21338;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#30830;&#23450;&#27969;&#37327;&#20998;&#37197;&#65292;&#20840;&#38754;&#35780;&#20272;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#21487;&#20449;&#24230;&#12289;&#25935;&#24863;&#24615;&#21644;&#21518;&#24724;&#24230;&#65292;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#22522;&#20110;&#25209;&#27425;&#30340;&#36125;&#21494;&#26031;&#31639;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#21152;&#36895;&#22312;&#32447;&#27979;&#35797;&#65292;&#22810;&#33218;&#36172;&#21338;&#31639;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#25910;&#38598;&#25968;&#25454;&#32780;&#34987;&#20316;&#20026;&#22266;&#23450;&#26102;&#38388;A/B&#27979;&#35797;&#30340;&#37325;&#35201;&#34917;&#20805;&#26041;&#24335;&#19981;&#26029;&#25552;&#39640;&#12290;&#26412;&#25991;&#22522;&#20110;&#26368;&#36817;&#20851;&#20110;&#33258;&#36866;&#24212;&#25910;&#38598;&#25968;&#25454;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#21644;&#32479;&#35745;&#25512;&#26029;&#30340;&#30740;&#31350;&#65292;&#25512;&#23548;&#21644;&#35780;&#20272;&#20102;&#22235;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#25209;&#27425;&#36172;&#21338;&#31639;&#27861;&#65288;NB-TS&#65292;WB-TS&#65292;NB-TTTS&#65292;WB-TTTS&#65289;&#65292;&#23427;&#20204;&#26159;&#20004;&#31181;&#21152;&#26435;&#25209;&#27425;&#65288;Naive Batch&#21644;Weighted Batch&#65289;&#21644;&#20004;&#31181;&#36125;&#21494;&#26031;&#25277;&#26679;&#31574;&#30053;&#65288;Thompson Sampling&#21644;Top-Two Thompson Sampling&#65289;&#30340;&#32452;&#21512;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#30830;&#23450;&#27969;&#37327;&#20998;&#37197;&#12290;&#26412;&#25991;&#25552;&#20379;&#30340;&#36825;&#20123;&#22522;&#20110;&#25209;&#27425;&#32479;&#35745;&#22870;&#21169;&#24230;&#37327;&#30340;&#36125;&#21494;&#26031;&#25277;&#26679;&#31639;&#27861;&#22312;&#23454;&#36341;&#20013;&#24471;&#20197;&#24212;&#29992;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#30340;&#20854;&#20013;&#19968;&#20010;&#32452;&#21512;WB-TTTS&#20284;&#20046;&#26159;&#26368;&#26032;&#35752;&#35770;&#30340;&#12290;&#23545;&#36825;&#22235;&#31181;&#22522;&#20110;&#25209;&#27425;&#30340;&#36125;&#21494;&#26031;&#25277;&#26679;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21253;&#25324;&#27979;&#35797;&#26041;&#27861;&#30340;&#21487;&#20449;&#24230;&#12289;&#25935;&#24863;&#24615;&#21644;&#21518;&#24724;&#24230;&#12290;&#27492;&#22806;&#65292;&#35780;&#20272;&#36824;&#32771;&#34385;&#20102;&#25209;&#27425;&#20869;&#22870;&#21169;&#24230;&#37327;&#30340;&#26041;&#24046;&#20197;&#21450;&#25209;&#27425;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#22312;&#20197;&#21069;&#30340;&#30740;&#31350;&#20013;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#35299;&#20915;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#36172;&#21338;&#31639;&#27861;&#65288;&#20363;&#22914;UCB1&#65292;TS&#21644;Exp3&#65289;&#30456;&#27604;&#65292;&#36825;&#20123;&#22522;&#20110;&#25209;&#27425;&#30340;&#36125;&#21494;&#26031;&#31639;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
To speed up online testing, adaptive traffic experimentation through multi-armed bandit algorithms is rising as an essential complementary alternative to the fixed horizon A/B testing. Based on recent research on best arm identification and statistical inference with adaptively collected data, this paper derives and evaluates four Bayesian batch bandit algorithms (NB-TS, WB-TS, NB-TTTS, WB-TTTS), which are combinations of two ways of weighting batches (Naive Batch and Weighted Batch) and two Bayesian sampling strategies (Thompson Sampling and Top-Two Thompson Sampling) to adaptively determine traffic allocation. These derived Bayesian sampling algorithms are practically based on summary batch statistics of a reward metric for pilot experiments, where one of the combination WB-TTTS in this paper seems to be newly discussed. The comprehensive evaluation on the four Bayesian sampling algorithms covers trustworthiness, sensitivity and regret of a testing methodology. Moreover, the evaluati
&lt;/p&gt;</description></item><item><title>AdvFunMatch&#26159;&#19968;&#31181;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#19968;&#33268;&#25945;&#23398;&#30340;&#26041;&#24335;&#65292;&#22312;&#21305;&#37197;&#25104;&#21151;&#25968;&#25454;&#28857;&#30340;&#21069;&#25552;&#19979;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#30340;&#29699;&#24418;&#31354;&#38388;&#20869;&#21305;&#37197;&#25152;&#26377;&#25968;&#25454;&#28857;&#30340;&#20998;&#24067;</title><link>http://arxiv.org/abs/2305.14700</link><description>&lt;p&gt;
AdvFunMatch: &#24403;&#19968;&#33268;&#30340;&#25945;&#23398;&#36935;&#35265;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
AdvFunMatch: When Consistent Teaching Meets Adversarial Robustness. (arXiv:2305.14700v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14700
&lt;/p&gt;
&lt;p&gt;
AdvFunMatch&#26159;&#19968;&#31181;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#19968;&#33268;&#25945;&#23398;&#30340;&#26041;&#24335;&#65292;&#22312;&#21305;&#37197;&#25104;&#21151;&#25968;&#25454;&#28857;&#30340;&#21069;&#25552;&#19979;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#30340;&#29699;&#24418;&#31354;&#38388;&#20869;&#21305;&#37197;&#25152;&#26377;&#25968;&#25454;&#28857;&#30340;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#19968;&#33268;&#30340;&#25945;&#23398;&#8221;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#33539;&#20363;&#65292;&#22312;&#36825;&#31181;&#33539;&#20363;&#19979;&#65292;&#23398;&#29983;&#21644;&#25945;&#24072;&#27169;&#22411;&#25509;&#25910;&#30456;&#21516;&#30340;&#36755;&#20837;&#65292;&#24182;&#23558;&#30693;&#35782;&#33976;&#39311;&#35270;&#20026;&#20989;&#25968;&#21305;&#37197;&#20219;&#21153;&#65288;FunMatch&#65289;&#12290;&#28982;&#32780;&#65292;FunMatch &#30340;&#19968;&#20010;&#38480;&#21046;&#26159;&#23427;&#27809;&#26377;&#32771;&#34385;&#21040;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21363;&#27169;&#22411;&#25269;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#33021;&#21147;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#31216;&#20026;&#23545;&#25239;&#20989;&#25968;&#21305;&#37197;&#65288;AdvFunMatch&#65289;&#65292;&#35813;&#31574;&#30053;&#26088;&#22312;&#22312;&#19968;&#33268;&#25945;&#23398;&#30340;&#21069;&#25552;&#19979;&#65292;&#21305;&#37197;&#35757;&#32451;&#25968;&#25454; $\ell_p$-&#33539;&#25968;&#29699;&#20869;&#30340;&#25152;&#26377;&#25968;&#25454;&#28857;&#30340;&#20998;&#24067;&#12290;AdvFunMatch &#34987;&#21046;&#23450;&#20026;&#26497;&#23567;&#21270;-&#26497;&#22823;&#21270;&#20248;&#21270;&#38382;&#39064;&#65292;&#23427;&#30830;&#23450;&#20102;&#26368;&#22823;&#21270;&#25945;&#24072;&#27169;&#22411;&#21644;&#23398;&#29983;&#27169;&#22411;&#36755;&#20986;&#20043;&#38388; KL &#25955;&#24230;&#30340;&#26368;&#22351;&#23454;&#20363;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#19981;&#21305;&#37197;&#23454;&#20363;&#8221;&#65289;&#65292;&#28982;&#21518;&#21305;&#37197;&#36825;&#20123;&#19981;&#21305;&#37197;&#23454;&#20363;&#19978;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AdvFunMatch &#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;&#24378;&#23545;&#25239;&#24615;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
\emph{Consistent teaching} is an effective paradigm for implementing knowledge distillation (KD), where both student and teacher models receive identical inputs, and KD is treated as a function matching task (FunMatch). However, one limitation of FunMatch is that it does not account for the transfer of adversarial robustness, a model's resistance to adversarial attacks. To tackle this problem, we propose a simple but effective strategy called Adversarial Function Matching (AdvFunMatch), which aims to match distributions for all data points within the $\ell_p$-norm ball of the training data, in accordance with consistent teaching. Formulated as a min-max optimization problem, AdvFunMatch identifies the worst-case instances that maximizes the KL-divergence between teacher and student model outputs, which we refer to as "mismatched examples," and then matches the outputs on these mismatched examples. Our experimental results show that AdvFunMatch effectively produces student models with b
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#24182;&#25552;&#20379;&#22240;&#26524;&#24178;&#39044;&#25216;&#26415;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23454;&#20307;&#20559;&#35265;&#65292;&#20174;&#32780;&#20943;&#23569;&#20559;&#35265;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#30456;&#20284;&#23454;&#20307;&#30340;&#20849;&#21516;&#39044;&#27979;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.14695</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23454;&#20307;&#20559;&#35265;&#65306;&#19968;&#31181;&#22240;&#26524;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Causal View of Entity Bias in (Large) Language Models. (arXiv:2305.14695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14695
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#24182;&#25552;&#20379;&#22240;&#26524;&#24178;&#39044;&#25216;&#26415;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23454;&#20307;&#20559;&#35265;&#65292;&#20174;&#32780;&#20943;&#23569;&#20559;&#35265;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#30456;&#20284;&#23454;&#20307;&#30340;&#20849;&#21516;&#39044;&#27979;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#20559;&#35265;&#24191;&#27867;&#24433;&#21709;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23548;&#33268;&#23427;&#20204;&#36807;&#24230;&#20381;&#36182;&#65288;&#26377;&#20559;&#35265;&#30340;&#65289;&#21442;&#25968;&#21270;&#30693;&#35782;&#26469;&#36827;&#34892;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#23613;&#31649;&#22240;&#26524;&#30456;&#20851;&#30340;&#26041;&#27861;&#24050;&#32463;&#26174;&#31034;&#20986;&#32531;&#35299;&#23454;&#20307;&#20559;&#35265;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#31934;&#30830;&#20272;&#35745;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#30340;&#21442;&#25968;&#20173;&#28982;&#24456;&#22256;&#38590;&#65292;&#40657;&#30418;&#23376;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#26080;&#27861;&#35843;&#25972;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#65292;&#20854;&#21442;&#25968;&#27604;&#36739;&#23481;&#26131;&#20272;&#35745;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22240;&#26524;&#24178;&#39044;&#25216;&#26415;&#65292;&#20197;&#32531;&#35299;&#30333;&#30418;&#21644;&#40657;&#30418;&#35774;&#32622;&#20013;&#30340;&#23454;&#20307;&#20559;&#35265;&#12290;&#36825;&#31181;&#22240;&#26524;&#24178;&#39044;&#23558;&#21407;&#22987;&#23454;&#20307;&#19982;&#30456;&#37051;&#23454;&#20307;&#19968;&#36215;&#36827;&#34892;&#25200;&#21160;&#12290;&#36825;&#31181;&#24178;&#39044;&#20943;&#23569;&#20102;&#19982;&#21407;&#22987;&#23454;&#20307;&#30456;&#20851;&#30340;&#29305;&#23450;&#20559;&#21521;&#20449;&#24687;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;&#20102;&#26469;&#33258;&#31867;&#20284;&#23454;&#20307;&#30340;&#36275;&#22815;&#20849;&#21516;&#39044;&#27979;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity bias widely affects pretrained (large) language models, causing them to excessively rely on (biased) parametric knowledge to make unfaithful predictions. Although causality-inspired methods have shown great potential to mitigate entity bias, it is hard to precisely estimate the parameters of underlying causal models in practice. The rise of black-box LLMs also makes the situation even worse, because of their inaccessible parameters and uncalibrated logits. To address these problems, we propose a specific structured causal model (SCM) whose parameters are comparatively easier to estimate. Building upon this SCM, we propose causal intervention techniques to mitigate entity bias for both white-box and black-box settings. The proposed causal intervention perturbs the original entity with neighboring entities. This intervention reduces specific biasing information pertaining to the original entity while still preserving sufficient common predictive information from similar entities. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20010;&#24615;&#26159;&#21542;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#30446;&#21069;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#26080;&#27861;&#20934;&#30830;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20010;&#24615;&#12290;&#20026;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#38656;&#35201;&#19987;&#38376;&#20026;LLM&#35774;&#35745;&#26032;&#30340;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2305.14693</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#25317;&#26377;&#20154;&#26684;?&#8212;&#8212;&#22312;&#27979;&#37327;LLM&#20013;&#30340;&#20010;&#24615;&#26102;&#65292;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#30340;&#36866;&#29992;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Have Large Language Models Developed a Personality?: Applicability of Self-Assessment Tests in Measuring Personality in LLMs. (arXiv:2305.14693v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20010;&#24615;&#26159;&#21542;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#30446;&#21069;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#26080;&#27861;&#20934;&#30830;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20010;&#24615;&#12290;&#20026;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#38656;&#35201;&#19987;&#38376;&#20026;LLM&#35774;&#35745;&#26032;&#30340;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#21542;&#25317;&#26377;&#20154;&#26684;&#65311;&#31616;&#30701;&#30340;&#22238;&#31572;&#26159;&#8220;&#25105;&#20204;&#19981;&#30693;&#36947;&#65281;&#8221;&#12290;&#26412;&#25991;&#34920;&#26126;&#25105;&#20204;&#36824;&#27809;&#26377;&#21512;&#36866;&#30340;&#24037;&#20855;&#33021;&#22815;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#12290;&#20010;&#24615;&#26159;&#24433;&#21709;&#34892;&#20026;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#38543;&#30528;LLM&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#27169;&#25311;&#20154;&#31867;&#26234;&#33021;&#21644;&#34920;&#29616;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#38382;&#20986;&#30340;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#25317;&#26377;&#20102;&#20154;&#26684;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#20154;&#26684;&#27979;&#35797;&#26469;&#35780;&#20272;&#26426;&#22120;&#20154;&#26684;&#65292;&#36825;&#26159;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#20154;&#31867;&#20010;&#24615;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;&#19968;&#20010;&#22522;&#26412;&#30340;&#20551;&#35774;&#26159;&#65292;&#20154;&#31867;&#20010;&#24615;&#27979;&#35797;&#21487;&#20197;&#20934;&#30830;&#22320;&#27979;&#37327;&#26426;&#22120;&#20154;&#26684;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20116;&#20010;&#19981;&#21516;&#23610;&#23544;&#33539;&#22260;&#65288;&#20174;1.5B&#21040;30B&#65289;&#30340;LLM&#20013;&#30340;&#20010;&#24615;&#28014;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36873;&#39033;&#39034;&#24207;&#23545;&#31216;&#24615;&#21407;&#21017;&#20316;&#20026;&#36825;&#20123;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#21487;&#38752;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#22312;&#36825;&#20010;&#26465;&#20214;&#19979;&#65292;&#31572;&#39064;&#21644;&#20182;&#20204;&#30340;&#25490;&#24207;&#19981;&#24212;&#24433;&#21709;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#33258;&#25105;&#35780;&#20272;&#20010;&#24615;&#27979;&#35797;&#23545;LLM&#26469;&#35828;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24120;&#29992;&#30340;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#23545;&#20154;&#31867;&#20855;&#26377;&#20559;&#35265;&#65292;&#27809;&#26377;&#32771;&#34385;&#21040;LLM&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#20026;&#20102;&#20419;&#36827;LLM&#20010;&#24615;&#27979;&#37327;&#30340;&#26410;&#26469;&#30740;&#31350;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#24320;&#21457;&#19987;&#38376;&#20026;LLM&#35774;&#35745;&#30340;&#26032;&#30340;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Have Large Language Models (LLMs) developed a personality? The short answer is a resounding "We Don't Know!". In this paper, we show that we do not yet have the right tools to measure personality in language models. Personality is an important characteristic that influences behavior. As LLMs emulate human-like intelligence and performance in various tasks, a natural question to ask is whether these models have developed a personality. Previous works have evaluated machine personality through self-assessment personality tests, which are a set of multiple-choice questions created to evaluate personality in humans. A fundamental assumption here is that human personality tests can accurately measure personality in machines. In this paper, we investigate the emergence of personality in five LLMs of different sizes ranging from 1.5B to 30B. We propose the Option-Order Symmetry property as a necessary condition for the reliability of these self-assessment tests. Under this condition, the answ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25512;&#24191;&#20102;&#37325;&#35201;&#24615;&#21152;&#26435;&#26041;&#27861;&#65292;&#23558;&#20854;&#21464;&#25104;&#20102;&#36866;&#29992;&#20110;&#25152;&#26377;&#24773;&#20917;&#30340;&#36890;&#29992;&#27714;&#35299;&#22120;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#21253;&#25324;&#37096;&#20998;&#37325;&#21472;&#24773;&#20917;&#22312;&#20869;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14690</link><description>&lt;p&gt;
&#23558;&#37325;&#35201;&#24615;&#21152;&#26435;&#26041;&#27861;&#25512;&#24191;&#20026;&#29992;&#20110;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#30340;&#36890;&#29992;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Generalizing Importance Weighting to A Universal Solver for Distribution Shift Problems. (arXiv:2305.14690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25512;&#24191;&#20102;&#37325;&#35201;&#24615;&#21152;&#26435;&#26041;&#27861;&#65292;&#23558;&#20854;&#21464;&#25104;&#20102;&#36866;&#29992;&#20110;&#25152;&#26377;&#24773;&#20917;&#30340;&#36890;&#29992;&#27714;&#35299;&#22120;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#21253;&#25324;&#37096;&#20998;&#37325;&#21472;&#24773;&#20917;&#22312;&#20869;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#20559;&#31227;(DS) &#21487;&#20197;&#26377;&#20004;&#20010;&#23618;&#38754;&#65306;&#20998;&#24067;&#26412;&#36523;&#21457;&#29983;&#21464;&#21270;&#65292;&#25903;&#25345;&#65288;&#21363;&#27010;&#29575;&#23494;&#24230;&#38750;&#38646;&#30340;&#38598;&#21512;&#65289;&#20063;&#21457;&#29983;&#21464;&#21270;&#12290;&#24403;&#32771;&#34385;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#30340;&#25903;&#25345;&#21464;&#21270;&#26102;&#65292;&#26377;&#22235;&#31181;&#24773;&#20917;&#65306;&#65288;i&#65289;&#23427;&#20204;&#23436;&#20840;&#21305;&#37197;&#65307;&#65288;ii&#65289;&#35757;&#32451;&#25903;&#25345;&#33539;&#22260;&#26356;&#24191;&#65288;&#22240;&#27492;&#35206;&#30422;&#27979;&#35797;&#25903;&#25345;&#65289;&#65307;&#65288;iii&#65289;&#27979;&#35797;&#25903;&#25345;&#33539;&#22260;&#26356;&#24191;&#65307;&#65288;iv&#65289;&#23427;&#20204;&#37096;&#20998;&#37325;&#21472;&#12290;&#29616;&#26377;&#26041;&#27861;&#23545;&#24773;&#20917;&#65288;i&#65289;&#21644;&#65288;ii&#65289;&#24456;&#26377;&#25928;&#65292;&#32780;&#24773;&#20917;&#65288;iii&#65289;&#21644;&#65288;iv&#65289;&#29616;&#22312;&#26356;&#24120;&#35265;&#65292;&#20294;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#23558;&#37325;&#35201;&#24615;&#21152;&#26435;&#65288;IW&#65289;&#26041;&#27861;&#65288;&#29992;&#20110;&#24773;&#20917;&#65288;i&#65289;&#21644;&#65288;ii&#65289;&#65289;&#25512;&#24191;&#20026;&#36866;&#29992;&#20110;&#25152;&#26377;&#24773;&#20917;&#30340;&#36890;&#29992;&#27714;&#35299;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;IW&#22312;&#24773;&#20917;&#65288;iii&#65289;&#21644;&#65288;iv&#65289;&#20013;&#21487;&#33021;&#22833;&#36133;&#30340;&#21407;&#22240;&#65307;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27867;&#21270;&#37325;&#35201;&#24615;&#21152;&#26435;&#65288;GIW&#65289;&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#24773;&#20917;&#65288;iii&#65289;&#21644;&#65288;iv&#65289;&#65292;&#24182;&#19988;&#22312;&#24773;&#20917;&#65288;i&#65289;&#21644;&#65288;ii&#65289;&#20013;&#23558;&#20943;&#23569;&#20026;IW&#26041;&#27861;&#12290;&#22312;GIW&#20013;&#65292;&#23558;&#27979;&#35797;&#25903;&#25345;&#21010;&#20998;&#20026;&#35757;&#32451;&#20869;&#37096;&#65288;IT&#65289;&#37096;&#20998;&#21644;&#35757;&#32451;&#22806;&#37096;&#65288;OOT&#65289;&#37096;&#20998;&#65292;&#24182;&#20026;&#23427;&#20204;&#20998;&#37197;&#19981;&#21516;&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;GIW&#23545;&#25152;&#26377;&#22235;&#31181;&#24773;&#20917;&#37117;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;GIW&#30830;&#23454;&#22312;&#25152;&#26377;&#22235;&#31181;&#24773;&#20917;&#19979;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distribution shift (DS) may have two levels: the distribution itself changes, and the support (i.e., the set where the probability density is non-zero) also changes. When considering the support change between the training and test distributions, there can be four cases: (i) they exactly match; (ii) the training support is wider (and thus covers the test support); (iii) the test support is wider; (iv) they partially overlap. Existing methods are good at cases (i) and (ii), while cases (iii) and (iv) are more common nowadays but still under-explored. In this paper, we generalize importance weighting (IW), a golden solver for cases (i) and (ii), to a universal solver for all cases. Specifically, we first investigate why IW may fail in cases (iii) and (iv); based on the findings, we propose generalized IW (GIW) that could handle cases (iii) and (iv) and would reduce to IW in cases (i) and (ii). In GIW, the test support is split into an in-training (IT) part and an out-of-training (OOT) pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#25968;&#25454;&#26368;&#23567;&#20108;&#20056;&#23725;&#27491;&#21017;&#21270;&#30340;&#21435;&#22122;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#27424;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#20250;&#20986;&#29616;&#21452;&#23792;&#35895;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2305.14689</link><description>&lt;p&gt;
&#22522;&#20110;&#23725;&#27491;&#21017;&#21270;&#30340;&#32447;&#24615;&#25968;&#25454;&#26368;&#23567;&#20108;&#20056;&#21435;&#22122;&#38382;&#39064;&#30340;&#27424;&#21442;&#25968;&#21270;&#21452;&#35895;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Under-Parameterized Double Descent for Ridge Regularized Least Squares Denoising of Data on a Line. (arXiv:2305.14689v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#25968;&#25454;&#26368;&#23567;&#20108;&#20056;&#23725;&#27491;&#21017;&#21270;&#30340;&#21435;&#22122;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#27424;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#20250;&#20986;&#29616;&#21452;&#23792;&#35895;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#35757;&#32451;&#25968;&#25454;&#28857;&#25968;&#12289;&#32479;&#35745;&#27169;&#22411;&#21442;&#25968;&#25968;&#21644;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#24050;&#26377;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36807;&#24230;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#21487;&#33021;&#20986;&#29616;&#21452;&#23792;&#35895;&#29616;&#35937;&#65292;&#32780;&#22312;&#27424;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#21017;&#26222;&#36941;&#23384;&#22312;&#26631;&#20934;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20363;&#23376;&#65292;&#21487;&#20197;&#35777;&#26126;&#27424;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#21487;&#20197;&#21457;&#29983;&#21452;&#23792;&#35895;&#29616;&#35937;&#12290;&#32771;&#34385;&#23884;&#20837;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#32447;&#24615;&#25968;&#25454;&#26368;&#23567;&#20108;&#20056;&#21435;&#22122;&#38382;&#39064;&#20013;&#30340;&#23725;&#27491;&#21017;&#21270;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#19968;&#31181;&#28176;&#36817;&#20934;&#30830;&#30340;&#24191;&#20041;&#35823;&#24046;&#20844;&#24335;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#26679;&#26412;&#21644;&#21442;&#25968;&#30340;&#21452;&#35895;&#25928;&#24212;&#65292;&#21452;&#23792;&#35895;&#20301;&#20110;&#25554;&#20540;&#28857;&#21644;&#36807;&#24230;&#21442;&#25968;&#21270;&#21306;&#22495;&#20043;&#38388;&#12290;&#27492;&#22806;&#65292;&#26679;&#26412;&#21452;&#35895;&#26354;&#32447;&#30340;&#39640;&#23792;&#23545;&#24212;&#20110;&#20272;&#35745;&#37327;&#30340;&#33539;&#25968;&#26354;&#32447;&#30340;&#39640;&#23792;&#12290;
&lt;/p&gt;
&lt;p&gt;
The relationship between the number of training data points, the number of parameters in a statistical model, and the generalization capabilities of the model has been widely studied. Previous work has shown that double descent can occur in the over-parameterized regime, and believe that the standard bias-variance trade-off holds in the under-parameterized regime. In this paper, we present a simple example that provably exhibits double descent in the under-parameterized regime. For simplicity, we look at the ridge regularized least squares denoising problem with data on a line embedded in high-dimension space. By deriving an asymptotically accurate formula for the generalization error, we observe sample-wise and parameter-wise double descent with the peak in the under-parameterized regime rather than at the interpolation point or in the over-parameterized regime.  Further, the peak of the sample-wise double descent curve corresponds to a peak in the curve for the norm of the estimator,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;MLP&#39034;&#24207;&#25512;&#33616;&#26550;&#26500;TriMLP&#65292;&#20854;&#20013;&#21152;&#20837;&#20102;&#26032;&#39062;&#30340;&#19977;&#35282;&#24418;&#28151;&#21512;&#22120;&#20197;&#23454;&#29616;&#26631;&#35760;&#26377;&#24207;&#30340;&#20132;&#20114;&#65292;&#20197;&#25552;&#39640;&#39034;&#24207;&#25512;&#33616;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.14675</link><description>&lt;p&gt;
MLP&#22312;&#39034;&#24207;&#25512;&#33616;&#20013;&#30340;&#22797;&#20167;
&lt;/p&gt;
&lt;p&gt;
Revenge of MLP in Sequential Recommendation. (arXiv:2305.14675v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;MLP&#39034;&#24207;&#25512;&#33616;&#26550;&#26500;TriMLP&#65292;&#20854;&#20013;&#21152;&#20837;&#20102;&#26032;&#39062;&#30340;&#19977;&#35282;&#24418;&#28151;&#21512;&#22120;&#20197;&#23454;&#29616;&#26631;&#35760;&#26377;&#24207;&#30340;&#20132;&#20114;&#65292;&#20197;&#25552;&#39640;&#39034;&#24207;&#25512;&#33616;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#27169;&#22411;&#23545;&#21382;&#21490;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#34892;&#20026;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#26356;&#22909;&#22320;&#25512;&#26029;&#21160;&#24577;&#20559;&#22909;&#12290;&#36817;&#24180;&#26469;&#65292;&#24471;&#30410;&#20110;&#25913;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22914;RNN&#12289;CNN&#21644;Transformer&#65292;&#36825;&#20010;&#39046;&#22495;&#24050;&#32463;&#36814;&#26469;&#20102;&#24555;&#36895;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#26368;&#36817;&#65292;&#20840;MLP&#27169;&#22411;&#30340;&#30740;&#31350;&#25104;&#26524;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#30340;&#27880;&#24847;&#65292;&#21363;&#36890;&#36807;&#28151;&#21512;&#21382;&#21490;&#34892;&#20026;&#30340;MLP&#23398;&#20064;&#36716;&#25442;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#31181;&#20840;&#36830;&#25509;&#32467;&#26500;&#20801;&#35768;&#19981;&#21463;&#38480;&#21046;&#30340;&#36328;&#34892;&#20026;&#38388;&#36890;&#20449;&#24182;&#24573;&#30053;&#20102;&#26102;&#38388;&#39034;&#24207;&#65292;&#25105;&#20204;&#21457;&#29616;&#30452;&#25509;&#23558;&#28151;&#21512;MLP&#24212;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#20250;&#23548;&#33268;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;MLP&#39034;&#24207;&#25512;&#33616;&#26550;&#26500;TriMLP&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#35282;&#24418;&#28151;&#21512;&#22120;&#65292;&#25913;&#36827;&#21518;&#30340;MLP&#36171;&#20104;&#20102;&#26631;&#35760;&#26377;&#24207;&#30340;&#20132;&#20114;&#12290;&#30001;&#20110;MLP&#20013;&#30340;&#36328;&#26631;&#35760;&#20132;&#20114;&#23454;&#38469;&#19978;&#26159;&#30697;&#38453;...
&lt;/p&gt;
&lt;p&gt;
Sequential recommendation models sequences of historical user-item interactive behaviors (or referred as token) to better infer dynamic preferences. Fueled by the improved neural network architectures such as RNN, CNN and Transformer, this field has enjoyed rapid performance boost in the past years. Recent progress on all-MLP models lights on an efficient method with less intensive computation, token-mixing MLP, to learn the transformation patterns among historical behaviors. However, due to the inherent fully-connection design that allows the unrestricted cross-token communication and ignores the chronological order, we find that directly applying token-mixing MLP into sequential recommendation leads to subpar performance. In this paper, we present a purely MLP-based sequential recommendation architecture TriMLP with a novel \underline{Tri}angular Mixer where the modified \underline{MLP} endows tokens with ordered interactions. As the cross-token interaction in MLP is actually matrix 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#36882;&#24402;&#22270;&#20687;&#37197;&#20934;&#32593;&#32476;ORRN&#65292;&#38024;&#23545;&#32954;4DCT&#22270;&#20687;&#30340;&#21464;&#24418;&#20272;&#35745;&#12290;&#35813;&#32593;&#32476;&#37319;&#29992;&#36882;&#24402;&#37197;&#20934;&#31574;&#30053;&#65292;&#33021;&#22815;&#26377;&#25928;&#27169;&#25311;&#21628;&#21560;&#21644;&#24515;&#36339;&#31561;&#22120;&#23448;&#36816;&#21160;&#12290;&#35813;&#26041;&#27861;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.14673</link><description>&lt;p&gt;
ORRN&#65306;&#19968;&#31181;&#22522;&#20110;ODE&#30340;&#36882;&#24402;&#37197;&#20934;&#32593;&#32476;&#65292;&#29992;&#20110;&#21628;&#21560;&#36816;&#21160;&#21464;&#24418;&#30340;&#32954;4DCT&#22270;&#20687;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
ORRN: An ODE-based Recursive Registration Network for Deformable Respiratory Motion Estimation with Lung 4DCT Images. (arXiv:2305.14673v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#36882;&#24402;&#22270;&#20687;&#37197;&#20934;&#32593;&#32476;ORRN&#65292;&#38024;&#23545;&#32954;4DCT&#22270;&#20687;&#30340;&#21464;&#24418;&#20272;&#35745;&#12290;&#35813;&#32593;&#32476;&#37319;&#29992;&#36882;&#24402;&#37197;&#20934;&#31574;&#30053;&#65292;&#33021;&#22815;&#26377;&#25928;&#27169;&#25311;&#21628;&#21560;&#21644;&#24515;&#36339;&#31561;&#22120;&#23448;&#36816;&#21160;&#12290;&#35813;&#26041;&#27861;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#24418;&#22270;&#20687;&#37197;&#20934;&#65288;DIR&#65289;&#22312;&#37327;&#21270;&#21307;&#23398;&#25968;&#25454;&#21464;&#24418;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#27880;&#20876;&#19968;&#23545;&#21307;&#23398;&#22270;&#20687;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21152;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#22312;4D&#65288;3D +&#26102;&#38388;&#65289;&#21307;&#23398;&#25968;&#25454;&#20013;&#65292;&#22120;&#23448;&#36816;&#21160;&#65292;&#22914;&#21628;&#21560;&#36816;&#21160;&#21644;&#24515;&#36339;&#31561;&#65292;&#26080;&#27861;&#36890;&#36807;&#25104;&#23545;&#26041;&#27861;&#26377;&#25928;&#22320;&#24314;&#27169;&#65292;&#22240;&#20026;&#23427;&#20204;&#38024;&#23545;&#22270;&#20687;&#23545;&#36827;&#34892;&#20248;&#21270;&#65292;&#20294;&#27809;&#26377;&#32771;&#34385;&#22312;&#32771;&#34385;4D&#25968;&#25454;&#26102;&#24517;&#35201;&#30340;&#22120;&#23448;&#36816;&#21160;&#27169;&#24335;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ORRN&#65292;&#19968;&#31181;&#22522;&#20110;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#30340;&#36882;&#24402;&#22270;&#20687;&#37197;&#20934;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#23398;&#20250;&#20102;&#20026;&#27169;&#25311;4D&#22270;&#20687;&#25968;&#25454;&#20013;&#30340;&#21464;&#24418;&#32780;&#20272;&#35745;&#26102;&#21464;&#30340;&#20307;&#32032;&#36895;&#24230;&#30340;ODE&#12290;&#23427;&#37319;&#29992;&#36882;&#24402;&#37197;&#20934;&#31574;&#30053;&#65292;&#36890;&#36807;ODE&#23545;&#20307;&#32032;&#36895;&#24230;&#36827;&#34892;&#31215;&#20998;&#26469;&#36880;&#27493;&#20272;&#35745;&#21464;&#24418;&#22330;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#32954;4DCT&#25968;&#25454;&#38598;DIRLab&#21644;CREATIS&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#23436;&#25104;&#20004;&#20010;&#20219;&#21153;&#65306;1&#65289;&#23558;&#25152;&#26377;&#22270;&#20687;&#19982;&#21442;&#32771;&#22270;&#20687;&#37197;&#20934;&#65307;
&lt;/p&gt;
&lt;p&gt;
Deformable Image Registration (DIR) plays a significant role in quantifying deformation in medical data. Recent Deep Learning methods have shown promising accuracy and speedup for registering a pair of medical images. However, in 4D (3D + time) medical data, organ motion, such as respiratory motion and heart beating, can not be effectively modeled by pair-wise methods as they were optimized for image pairs but did not consider the organ motion patterns necessary when considering 4D data. This paper presents ORRN, an Ordinary Differential Equations (ODE)-based recursive image registration network. Our network learns to estimate time-varying voxel velocities for an ODE that models deformation in 4D image data. It adopts a recursive registration strategy to progressively estimate a deformation field through ODE integration of voxel velocities. We evaluate the proposed method on two publicly available lung 4DCT datasets, DIRLab and CREATIS, for two tasks: 1) registering all images to the e
&lt;/p&gt;</description></item><item><title>&#22312;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#20013;&#65292;&#38500;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#20043;&#22806;&#65292;&#36824;&#23384;&#22312;&#36328;&#20219;&#21153;&#31867;&#21035;&#27495;&#35270;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14657</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#20013;&#35299;&#20915;&#36328;&#20219;&#21153;&#31867;&#21035;&#27495;&#35270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Dealing with Cross-Task Class Discrimination in Online Continual Learning. (arXiv:2305.14657v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14657
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#20013;&#65292;&#38500;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#20043;&#22806;&#65292;&#36824;&#23384;&#22312;&#36328;&#20219;&#21153;&#31867;&#21035;&#27495;&#35270;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#30740;&#31350;&#20960;&#20046;&#35270;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#20026;&#21807;&#19968;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#22312;&#35838;&#31243;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#20013;&#23384;&#22312;&#21478;&#19968;&#20010;&#25361;&#25112;&#65292;&#21363;&#36328;&#20219;&#21153;&#31867;&#21035;&#27495;&#35270;&#65288;CTCD&#65289;&#65292;&#21363;&#22914;&#20309;&#22312;&#27809;&#26377;&#25110;&#26377;&#38480;&#22320;&#35775;&#38382;&#26087;&#20219;&#21153;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#24314;&#31435;&#26032;&#20219;&#21153;&#21644;&#26087;&#20219;&#21153;&#30340;&#31867;&#21035;&#20043;&#38388;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;CTCD&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing continual learning (CL) research regards catastrophic forgetting (CF) as almost the only challenge. This paper argues for another challenge in class-incremental learning (CIL), which we call cross-task class discrimination (CTCD),~i.e., how to establish decision boundaries between the classes of the new task and old tasks with no (or limited) access to the old task data. CTCD is implicitly and partially dealt with by replay-based methods. A replay method saves a small amount of data (replay data) from previous tasks. When a batch of current task data arrives, the system jointly trains the new data and some sampled replay data. The replay data enables the system to partially learn the decision boundaries between the new classes and the old classes as the amount of the saved data is small. However, this paper argues that the replay approach also has a dynamic training bias issue which reduces the effectiveness of the replay data in solving the CTCD problem. A novel optimization 
&lt;/p&gt;</description></item><item><title>RSRM&#26159;&#19968;&#31181;&#33021;&#22815;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#25581;&#31034;&#22797;&#26434;&#25968;&#23398;&#26041;&#31243;&#30340;&#27169;&#22411;&#65292;&#23427;&#21253;&#25324;Monte Carlo Tree Search&#20195;&#29702;&#12289;&#21452;Q&#23398;&#20064;&#22359;&#21644;&#24378;&#21270;&#23398;&#20064;&#22238;&#24402;&#22120;&#12290;</title><link>http://arxiv.org/abs/2305.14656</link><description>&lt;p&gt;
RSRM: &#24378;&#21270;&#31526;&#21495;&#22238;&#24402;&#26426;&#22120;
&lt;/p&gt;
&lt;p&gt;
RSRM: Reinforcement Symbolic Regression Machine. (arXiv:2305.14656v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14656
&lt;/p&gt;
&lt;p&gt;
RSRM&#26159;&#19968;&#31181;&#33021;&#22815;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#25581;&#31034;&#22797;&#26434;&#25968;&#23398;&#26041;&#31243;&#30340;&#27169;&#22411;&#65292;&#23427;&#21253;&#25324;Monte Carlo Tree Search&#20195;&#29702;&#12289;&#21452;Q&#23398;&#20064;&#22359;&#21644;&#24378;&#21270;&#23398;&#20064;&#22238;&#24402;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#30028;&#20013;&#65292;&#35768;&#22810;&#22797;&#26434;&#31995;&#32479;&#30340;&#34892;&#20026;&#21487;&#20197;&#29992;&#31616;&#27905;&#30340;&#25968;&#23398;&#26041;&#31243;&#26469;&#25551;&#36848;&#12290;&#23558;&#36825;&#20123;&#26041;&#31243;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#20013;&#33258;&#21160;&#25552;&#21462;&#20986;&#26469;&#34987;&#35270;&#20026;&#19968;&#20010;&#31526;&#21495;&#22238;&#24402;&#30340;&#36807;&#31243;&#65292;&#36825;&#22312;&#36807;&#21435;&#19968;&#30452;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#19978;&#20184;&#20986;&#20102;&#24040;&#22823;&#30340;&#21162;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#31526;&#21495;&#22238;&#24402;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#36824;&#23384;&#22312;&#29942;&#39048;&#65292;&#29305;&#21035;&#26159;&#24403;&#31163;&#25955;&#25628;&#32034;&#31354;&#38388;&#36235;&#21521;&#20110;&#26080;&#31351;&#22823;&#65292;&#24182;&#19988;&#22522;&#30784;&#25968;&#23398;&#20844;&#24335;&#22797;&#26434;&#26102;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#31526;&#21495;&#22238;&#24402;&#26426;&#22120; (RSRM)&#65292;&#20197;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#20013;&#25581;&#31034;&#20986;&#22797;&#26434;&#30340;&#25968;&#23398;&#26041;&#31243;&#20026;&#30446;&#26631;&#12290;RSRM&#27169;&#22411;&#30001;&#19977;&#20010;&#20851;&#38190;&#27169;&#22359;&#32452;&#25104;&#65306;(1)&#19968;&#20010;Monte Carlo &#26641;&#25628;&#32034; (MCTS) &#20195;&#29702;&#65292;&#23427;&#25506;&#32034;&#30001;&#39044;&#23450;&#20041;&#30340;&#25968;&#23398;&#36816;&#31639;&#31526;&#21644;&#21464;&#37327;&#32452;&#25104;&#30340;&#26368;&#20339;&#25968;&#23398;&#34920;&#36798;&#24335;&#26641;&#65292;(2)&#19968;&#20010;&#21452;Q&#23398;&#20064;&#22359;&#65292;&#36890;&#36807;&#20248;&#21270;&#25628;&#32034;&#31354;&#38388;&#26469;&#24110;&#21161;&#20943;&#23569;MCTS&#30340;&#21487;&#34892;&#25628;&#32034;&#31354;&#38388;&#65292;&#21644;(3)&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#22238;&#24402;&#22120;&#65292;&#29992;&#20110;&#21457;&#29616;&#26368;&#32456;&#30340;&#25968;&#23398;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
In nature, the behaviors of many complex systems can be described by parsimonious math equations. Automatically distilling these equations from limited data is cast as a symbolic regression process which hitherto remains a grand challenge. Keen efforts in recent years have been placed on tackling this issue and demonstrated success in symbolic regression. However, there still exist bottlenecks that current methods struggle to break when the discrete search space tends toward infinity and especially when the underlying math formula is intricate. To this end, we propose a novel Reinforcement Symbolic Regression Machine (RSRM) that masters the capability of uncovering complex math equations from only scarce data. The RSRM model is composed of three key modules: (1) a Monte Carlo tree search (MCTS) agent that explores optimal math expression trees consisting of pre-defined math operators and variables, (2) a Double Q-learning block that helps reduce the feasible search space of MCTS via pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38544;&#21547;&#29983;&#23384;&#20989;&#25968;&#30340;&#29983;&#23384;&#20998;&#26512;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#20540;&#31215;&#20998;&#26469;&#36827;&#34892;&#36164;&#26009;&#20998;&#26512;&#65292;&#26080;&#38656;&#36827;&#34892;&#24378;&#20551;&#35774;&#65292;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#33021;&#22815;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14655</link><description>&lt;p&gt;
&#22522;&#20110;&#38544;&#21547;&#29983;&#23384;&#20989;&#25968;&#30340;&#29983;&#23384;&#20998;&#26512;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Learning Survival Distribution with Implicit Survival Function. (arXiv:2305.14655v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38544;&#21547;&#29983;&#23384;&#20989;&#25968;&#30340;&#29983;&#23384;&#20998;&#26512;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#20540;&#31215;&#20998;&#26469;&#36827;&#34892;&#36164;&#26009;&#20998;&#26512;&#65292;&#26080;&#38656;&#36827;&#34892;&#24378;&#20551;&#35774;&#65292;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#33021;&#22815;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#26088;&#22312;&#27169;&#25311;&#21327;&#21464;&#37327;&#21644;&#20107;&#20214;&#21457;&#29983;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#23384;&#22312;&#19968;&#20123;&#26410;&#36319;&#36394;&#30340;&#65288;&#34987;&#23457;&#26597;&#30340;&#65289;&#26679;&#26412;&#12290;&#22312;&#23454;&#29616;&#36807;&#31243;&#20013;&#65292;&#29616;&#26377;&#26041;&#27861;&#27169;&#25311;&#29983;&#23384;&#20998;&#24067;&#26102;&#65292;&#38656;&#35201;&#24378;&#30340;&#20551;&#35774;&#25110;&#32773;&#22312;&#31163;&#25955;&#26102;&#38388;&#31354;&#38388;&#20013;&#36827;&#34892;&#65292;&#20197;&#36827;&#34892;&#36164;&#26009;&#20998;&#26512;&#30340;&#26102;&#20505;&#32771;&#34385;&#23457;&#26597;&#38382;&#39064;&#65292;&#36825;&#23548;&#33268;&#20102;&#24191;&#20041;&#25512;&#24191;&#30340;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38544;&#20989;&#25968;&#34920;&#24449;&#30340;&#38544;&#21547;&#29983;&#23384;&#20989;&#25968;&#65288;ISF&#65289;&#29992;&#20110;&#29983;&#23384;&#20998;&#24067;&#30340;&#36164;&#26009;&#20998;&#26512;&#65292;&#19981;&#38656;&#35201;&#32467;&#26524;&#36827;&#34892;&#24378;&#30340;&#20551;&#35774;&#65292;&#24182;&#37319;&#29992;&#25968;&#20540;&#31215;&#20998;&#26469;&#36924;&#36817;&#39044;&#27979;&#21644;&#20248;&#21270;&#30340;&#32047;&#35745;&#20998;&#24067;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ISF&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#24182;&#23545;&#25511;&#21046;&#20272;&#35745;&#31934;&#24230;&#30340;&#36229;&#21442;&#25968;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survival analysis aims at modeling the relationship between covariates and event occurrence with some untracked (censored) samples. In implementation, existing methods model the survival distribution with strong assumptions or in a discrete time space for likelihood estimation with censorship, which leads to weak generalization. In this paper, we propose Implicit Survival Function (ISF) based on Implicit Neural Representation for survival distribution estimation without strong assumptions,and employ numerical integration to approximate the cumulative distribution function for prediction and optimization. Experimental results show that ISF outperforms the state-of-the-art methods in three public datasets and has robustness to the hyperparameter controlling estimation precision.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#20174;&#29983;&#25104;&#27169;&#22411;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22522;&#20110;&#23884;&#20837;&#30340;&#23454;&#20307;&#23545;&#40784;&#65288;EEA&#65289;&#38382;&#39064;&#65292;&#24341;&#20837;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;EEA&#26041;&#27861;&#21450;&#25552;&#20986;&#30340;&#29983;&#25104;&#30340;EEA&#65288;GEEA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20114;&#30456;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;M-VAE&#65289;&#23454;&#29616;&#23454;&#20307;&#20174;&#19968;&#20010;KG&#36716;&#25442;&#21040;&#21478;&#19968;&#20010;KG&#65292;&#24182;&#19988;&#20174;&#38543;&#26426;&#22122;&#22768;&#21521;&#37327;&#29983;&#25104;&#26032;&#30340;&#23454;&#20307;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14651</link><description>&lt;p&gt;
&#20174;&#29983;&#25104;&#27169;&#22411;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#23454;&#20307;&#23545;&#40784;&#21450;&#36229;&#36234;&#65306;&#19968;&#20010;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Revisit and Outstrip Entity Alignment: A Perspective of Generative Models. (arXiv:2305.14651v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#20174;&#29983;&#25104;&#27169;&#22411;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22522;&#20110;&#23884;&#20837;&#30340;&#23454;&#20307;&#23545;&#40784;&#65288;EEA&#65289;&#38382;&#39064;&#65292;&#24341;&#20837;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;EEA&#26041;&#27861;&#21450;&#25552;&#20986;&#30340;&#29983;&#25104;&#30340;EEA&#65288;GEEA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20114;&#30456;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;M-VAE&#65289;&#23454;&#29616;&#23454;&#20307;&#20174;&#19968;&#20010;KG&#36716;&#25442;&#21040;&#21478;&#19968;&#20010;KG&#65292;&#24182;&#19988;&#20174;&#38543;&#26426;&#22122;&#22768;&#21521;&#37327;&#29983;&#25104;&#26032;&#30340;&#23454;&#20307;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#22312;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23884;&#20837;&#30340;&#23454;&#20307;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#29983;&#25104;&#27169;&#22411;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22522;&#20110;&#23884;&#20837;&#30340;&#23454;&#20307;&#23545;&#40784;&#65288;EEA&#65289;&#12290;&#25105;&#20204;&#34920;&#26126;EEA&#26159;&#19968;&#20010;&#29305;&#27530;&#30340;&#38382;&#39064;&#65292;&#20854;&#20027;&#35201;&#30446;&#26631;&#31867;&#20284;&#20110;&#20856;&#22411;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#30446;&#26631;&#65292;&#22522;&#20110;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#26368;&#36817;&#21457;&#23637;&#30340;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;EEA&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20182;&#20204;&#19981;&#23436;&#25972;&#30340;&#30446;&#26631;&#38480;&#21046;&#20102;&#23454;&#20307;&#23545;&#40784;&#21644;&#23454;&#20307;&#21512;&#25104;&#65288;&#21363;&#29983;&#25104;&#26032;&#23454;&#20307;&#65289;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#30340;EEA&#65288;abbr.&#65292;GEEA&#65289;&#26694;&#26550;&#21644;&#25552;&#20986;&#30340;&#20114;&#30456;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;M-VAE&#65289;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;M-VAE&#21487;&#20197;&#23558;&#19968;&#20010;&#23454;&#20307;&#20174;&#19968;&#20010;KG&#36716;&#25442;&#21040;&#21478;&#19968;&#20010;KG&#65292;&#24182;&#20174;&#38543;&#26426;&#22122;&#22768;&#21521;&#37327;&#29983;&#25104;&#26032;&#23454;&#20307;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#23454;&#39564;&#23637;&#31034;&#20102;GEEA&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent embedding-based methods have achieved great successes on exploiting entity alignment from knowledge graph (KG) embeddings of multiple modals. In this paper, we study embedding-based entity alignment (EEA) from a perspective of generative models. We show that EEA is a special problem where the main objective is analogous to that in a typical generative model, based on which we theoretically prove the effectiveness of the recently developed generative adversarial network (GAN)-based EEA methods. We then reveal that their incomplete objective limits the capacity on both entity alignment and entity synthesis (i.e., generating new entities). We mitigate this problem by introducing a generative EEA (abbr., GEEA) framework with the proposed mutual variational autoencoder (M-VAE) as the generative model. M-VAE can convert an entity from one KG to another and generate new entities from random noise vectors. We demonstrate the power of GEEA with theoretical analysis and empirical experime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;JTFT&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25552;&#21462;&#26102;&#38388;&#20381;&#36182;&#24615;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#20943;&#23567;&#35745;&#31639;&#38656;&#27714;&#65292;&#36866;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#20302;&#31209;&#27880;&#24847;&#23618;&#20197;&#26377;&#25928;&#25429;&#33719;&#36328;&#32500;&#24230;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#22312;&#20845;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.14649</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#32852;&#21512;&#26102;&#39057;&#22495;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Joint Time-frequency Domain Transformer for Multivariate Time Series Forecasting. (arXiv:2305.14649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;JTFT&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25552;&#21462;&#26102;&#38388;&#20381;&#36182;&#24615;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#20943;&#23567;&#35745;&#31639;&#38656;&#27714;&#65292;&#36866;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#20302;&#31209;&#27880;&#24847;&#23618;&#20197;&#26377;&#25928;&#25429;&#33719;&#36328;&#32500;&#24230;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#22312;&#20845;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#32852;&#21512;&#26102;&#39057;&#22495;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#35745;&#31639;&#38656;&#27714;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21487;&#23398;&#20064;&#39057;&#29575;&#30340;&#31232;&#30095;&#24615;&#65292;&#22312;&#39057;&#22495;&#26377;&#25928;&#22320;&#25552;&#21462;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#38500;&#20102;&#39057;&#29575;&#22495;&#34920;&#31034;&#22806;&#65292;&#26368;&#36817;&#30340;&#26377;&#38480;&#25968;&#37327;&#30340;&#25968;&#25454;&#28857;&#20063;&#34987;&#30452;&#25509;&#32534;&#30721;&#22312;&#26102;&#38388;&#22495;&#20013;&#65292;&#20197;&#22686;&#24378;&#23398;&#20064;&#23616;&#37096;&#20851;&#31995;&#24182;&#20943;&#36731;&#38750;&#24179;&#31283;&#24615;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;JTFT&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#22240;&#20026;&#20869;&#37096;&#34920;&#31034;&#30340;&#38271;&#24230;&#20445;&#25345;&#29420;&#31435;&#20110;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20302;&#31209;&#27880;&#24847;&#23618;&#65292;&#20197;&#26377;&#25928;&#25429;&#33719;&#36328;&#32500;&#24230;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#38450;&#27490;&#30001;&#20110;&#26102;&#38388;&#21644;&#36890;&#36947;&#24314;&#27169;&#30340;&#32416;&#32544;&#32780;&#23548;&#33268;&#24615;&#33021;&#38477;&#32423;&#12290; &#23545;&#20845;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;JTFT&#20248;&#20110;&#29616;&#26377;&#30340;&#29366;&#24577;-of-the-art&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
To enhance predicting performance while minimizing computational demands, this paper introduces a joint time-frequency domain Transformer (JTFT) for multivariate forecasting. The method exploits the sparsity of time series in the frequency domain using a small number of learnable frequencies to extract temporal dependencies effectively. Alongside the frequency domain representation, a fixed number of the most recent data points are directly encoded in the time domain, bolstering the learning of local relationships and mitigating the adverse effects of non-stationarity. JTFT achieves linear complexity since the length of the internal representation remains independent of the input sequence length. Additionally, a low-rank attention layer is proposed to efficiently capture cross-dimensional dependencies and prevent performance degradation due to the entanglement of temporal and channel-wise modeling. Experiments conducted on six real-world datasets demonstrate that JTFT outperforms state
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KARNet&#30340;&#27169;&#22411;&#65292;&#23558;Kalman&#28388;&#27874;&#22120;&#38598;&#25104;&#21040;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#29992;&#20110;&#23398;&#20064;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#30340;&#19990;&#30028;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#20808;&#21069;&#30340;&#39046;&#22495;&#30693;&#35782;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.14644</link><description>&lt;p&gt;
KARNet: &#21345;&#23572;&#26364;&#28388;&#27874;&#22686;&#24378;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#20013;&#30340;&#19990;&#30028;&#27169;&#22411;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
KARNet: Kalman Filter Augmented Recurrent Neural Network for Learning World Models in Autonomous Driving Tasks. (arXiv:2305.14644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KARNet&#30340;&#27169;&#22411;&#65292;&#23558;Kalman&#28388;&#27874;&#22120;&#38598;&#25104;&#21040;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#29992;&#20110;&#23398;&#20064;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#30340;&#19990;&#30028;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#20808;&#21069;&#30340;&#39046;&#22495;&#30693;&#35782;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#22312;&#27773;&#36710;&#24037;&#19994;&#20013;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#36890;&#24120;&#34987;&#35270;&#20026;&#20132;&#36890;&#36816;&#36755;&#19994;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;&#22522;&#20110;&#31471;&#21040;&#31471;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#30340;&#21457;&#23637;&#24050;&#32463;&#24471;&#21040;&#24456;&#22823;&#21152;&#36895;&#65292;&#24182;&#34987;&#25104;&#21151;&#24212;&#29992;&#20110;&#24863;&#30693;&#12289;&#35268;&#21010;&#21644;&#25511;&#21046;&#20219;&#21153;&#20013;&#12290;&#33258;&#21160;&#39550;&#39542;&#35268;&#21010;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#26159;&#20102;&#35299;&#29615;&#22659;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#20250;&#22914;&#20309;&#28436;&#21464;&#24182;&#37319;&#21462;&#36866;&#24403;&#30340;&#34892;&#21160;&#12290;&#20026;&#20102;&#20445;&#25345;&#24773;&#22659;&#24863;&#30693;&#65292;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#24212;&#26377;&#25928;&#22320;&#21033;&#29992;&#20174;&#21508;&#31181;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#20449;&#24687;&#26469;&#24418;&#25104;&#19990;&#30028;&#30340;&#25277;&#35937;&#34920;&#31034;&#12290;&#20026;&#27492;&#65292;&#21487;&#20197;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#20256;&#20837;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#32039;&#20945;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#37117;&#26159;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#65292;&#24182;&#19981;&#22312;&#20307;&#31995;&#32467;&#26500;&#20013;&#34701;&#21512;&#20219;&#20309;&#20808;&#21069;&#30693;&#35782;&#65288;&#20363;&#22914;&#29289;&#29702;&#30693;&#35782;&#65289;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#35768;&#22810;&#24037;&#20316;&#25506;&#32034;&#23558;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#38598;&#25104;&#21040;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Kalman&#28388;&#27874;&#22686;&#24378;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;KARNet&#65289;&#65292;&#23558;Kalman&#28388;&#27874;&#22120;&#65292;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#25511;&#21046;&#31995;&#32479;&#20013;&#30340;&#26041;&#27861;&#65292;&#38598;&#25104;&#21040;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#29992;&#20110;&#23398;&#20064;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#30340;&#19990;&#30028;&#27169;&#22411;&#12290;KARNet&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#34892;&#39542;&#24037;&#20855;&#30340;&#20808;&#21069;&#30693;&#35782;&#26469;&#25552;&#39640;&#39044;&#27979;&#29615;&#22659;&#26410;&#26469;&#29366;&#24577;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#22312;&#22522;&#20934;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KARNet&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#22312;&#39044;&#27979;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous driving has received a great deal of attention in the automotive industry and is often seen as the future of transportation. The development of autonomous driving technology has been greatly accelerated by the growth of end-to-end machine learning techniques that have been successfully used for perception, planning, and control tasks. An important aspect of autonomous driving planning is knowing how the environment evolves in the immediate future and taking appropriate actions. An autonomous driving system should effectively use the information collected from the various sensors to form an abstract representation of the world to maintain situational awareness. For this purpose, deep learning models can be used to learn compact latent representations from a stream of incoming data. However, most deep learning models are trained end-to-end and do not incorporate any prior knowledge (e.g., from physics) of the vehicle in the architecture. In this direction, many works have expl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Newton-Cotes&#20844;&#24335;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#21160;&#24577;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21270;&#65292;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#36739;&#65292; &#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#30528;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.14642</link><description>&lt;p&gt;
Newton-Cotes&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#35770;&#21160;&#24577;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21270;
&lt;/p&gt;
&lt;p&gt;
Newton-Cotes Graph Neural Networks: On the Time Evolution of Dynamic Systems. (arXiv:2305.14642v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Newton-Cotes&#20844;&#24335;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#21160;&#24577;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21270;&#65292;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#36739;&#65292; &#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#30528;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#31995;&#32479;&#21160;&#24577;&#24615;&#26159;&#35768;&#22810;&#31185;&#23398;&#30740;&#31350;&#20013;&#26368;&#37325;&#35201;&#30340;&#20998;&#26512;&#26041;&#27861;&#20043;&#19968;&#12290;&#20351;&#29992;&#31995;&#32479;&#30340;&#21021;&#22987;&#29366;&#24577;&#20316;&#20026;&#36755;&#20837;&#65292;&#26368;&#36817;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#26041;&#27861;&#33021;&#22815;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#36828;&#31163;&#21021;&#22987;&#29366;&#24577;&#30340;&#26410;&#26469;&#29366;&#24577;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#22312;&#24314;&#27169;&#31995;&#32479;&#30340;&#22352;&#26631;&#21644;&#30456;&#20114;&#20316;&#29992;&#21147;&#26041;&#38754;&#26377;&#19981;&#21516;&#30340;&#35774;&#35745;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#23454;&#38469;&#19978;&#20849;&#20139;&#19968;&#31181;&#24120;&#25968;&#30340;&#31215;&#20998;&#23398;&#20064;&#33539;&#20363;&#65292; &#33021;&#22815;&#22312;&#21021;&#22987;&#21644;&#32456;&#31471;&#22352;&#26631;&#20043;&#38388;&#30340;&#26102;&#38388;&#38388;&#38548;&#20869;&#39044;&#27979;&#36895;&#24230;&#30340;&#31215;&#20998;&#12290;&#21463;&#27492;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#29992;Newton-Cotes&#20844;&#24335;&#20272;&#31639;&#30340;&#33509;&#24178;&#36895;&#24230;&#20272;&#35745;&#26469;&#39044;&#27979;&#31215;&#20998;&#65292;&#24182;&#29702;&#35770;&#19978;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290; &#22312;&#20960;&#20010;&#22522;&#20934;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#35813;&#26041;&#27861;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#26377;&#30528;&#25345;&#32493;&#19988;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning system dynamics is one of the most important analytical approaches for many scientific studies. With the initial state of a system as input, the recent graph neural networks (GNNs)-based methods are capable of predicting the future state distant in time with high accuracy. Although these methods have diverse designs in modeling the coordinates and interacting forces of the system, we show that they actually share a common paradigm that learns the integration of the velocity over the interval between the initial and terminal coordinates. However, their integrand is constant w.r.t. time. Inspired by this observation, we propose a new approach to predict the integration based on several velocity estimations with Newton-Cotes formulas and prove its effectiveness theoretically. Extensive experiments on several benchmarks empirically demonstrate consistent and significant improvement compared with the state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#23558;&#37327;&#23376;&#32858;&#31867;&#24212;&#29992;&#20110;&#22270;&#32467;&#26500;&#20013;&#65292;&#20351;&#29992;&#22522;&#20110;GPU&#30340;&#24182;&#34892;&#31639;&#27861;&#26469;&#35745;&#31639;&#28508;&#22312;&#20540;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14641</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;GPU&#30340;&#24182;&#34892;&#31639;&#27861;&#36827;&#34892;&#22270;&#20998;&#26512;&#65306;&#37327;&#23376;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Graphy Analysis Using a GPU-based Parallel Algorithm: Quantum Clustering. (arXiv:2305.14641v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#23558;&#37327;&#23376;&#32858;&#31867;&#24212;&#29992;&#20110;&#22270;&#32467;&#26500;&#20013;&#65292;&#20351;&#29992;&#22522;&#20110;GPU&#30340;&#24182;&#34892;&#31639;&#27861;&#26469;&#35745;&#31639;&#28508;&#22312;&#20540;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#37327;&#23376;&#32858;&#31867;&#24212;&#29992;&#20110;&#22270;&#32467;&#26500;&#30340;&#26032;&#26041;&#27861;&#12290;&#37327;&#23376;&#32858;&#31867;&#65288;QC&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#28508;&#22312;&#20989;&#25968;&#26469;&#30830;&#23450;&#32858;&#31867;&#20013;&#24515;&#12290;&#22312;&#35813;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22270;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#25214;&#21040;&#32858;&#31867;&#20013;&#24515;&#12290;GPU&#24182;&#34892;&#21270;&#29992;&#20110;&#35745;&#31639;&#28508;&#22312;&#20540;&#12290;&#25105;&#20204;&#36824;&#23545;&#20116;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#20351;&#29992;&#22235;&#20010;&#25351;&#26631;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;$\sigma$&#23545;&#23454;&#39564;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The article introduces a new method for applying Quantum Clustering to graph structures. Quantum Clustering (QC) is a novel density-based unsupervised learning method that determines cluster centers by constructing a potential function. In this method, we use the Graph Gradient Descent algorithm to find the centers of clusters. GPU parallelization is utilized for computing potential values. We also conducted experiments on five widely used datasets and evaluated using four indicators. The results show superior performance of the method. Finally, we discuss the influence of $\sigma$ on the experimental results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#35270;&#35273;-&#20195;&#30721;Transformer&#26041;&#27861;&#65292;&#36890;&#36807;actor-critic&#24494;&#35843;&#26469;&#25913;&#21892;&#22522;&#32447;&#65292;&#27604;&#36739;&#20102;Vision Transformer&#21644;Document Image Transformer&#36825;&#20004;&#31181;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#23631;&#24149;&#25130;&#22270;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20195;&#30721;&#29255;&#27573;&#65292;&#21019;&#24314;&#20102;30,000&#20010;&#29420;&#29305;&#30340;&#20195;&#30721;&#21644;&#23545;&#24212;&#25130;&#22270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#22810;&#31181;&#33258;&#21160;&#21270;&#25351;&#26631;&#26469;&#35780;&#20272;&#36825;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.14637</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#35270;&#35273;-&#20195;&#30721;Transformer&#29992;&#20110;UI-to-Code&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning finetuned Vision-Code Transformer for UI-to-Code Generation. (arXiv:2305.14637v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#35270;&#35273;-&#20195;&#30721;Transformer&#26041;&#27861;&#65292;&#36890;&#36807;actor-critic&#24494;&#35843;&#26469;&#25913;&#21892;&#22522;&#32447;&#65292;&#27604;&#36739;&#20102;Vision Transformer&#21644;Document Image Transformer&#36825;&#20004;&#31181;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#23631;&#24149;&#25130;&#22270;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20195;&#30721;&#29255;&#27573;&#65292;&#21019;&#24314;&#20102;30,000&#20010;&#29420;&#29305;&#30340;&#20195;&#30721;&#21644;&#23545;&#24212;&#25130;&#22270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#22810;&#31181;&#33258;&#21160;&#21270;&#25351;&#26631;&#26469;&#35780;&#20272;&#36825;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#23631;&#24149;&#25130;&#22270;&#33258;&#21160;&#29983;&#25104;HTML/CSS&#20195;&#30721;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35273;-&#20195;&#30721;Transformer&#26041;&#27861;&#65292;&#21033;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#21516;&#26102;&#25506;&#32034;actor-critic&#24494;&#35843;&#20316;&#20026;&#25913;&#36827;&#22522;&#32447;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#27604;&#36739;&#20102;&#20004;&#20010;&#22270;&#20687;&#32534;&#30721;&#22120;&#65306;Vision Transformer (ViT) &#21644; Document Image Transformer (DiT)&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#23631;&#24149;&#25130;&#22270;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20195;&#30721;&#29255;&#27573;&#65292;&#31616;&#21270;&#20102;&#24320;&#21457;&#20154;&#21592;&#30340;&#32593;&#31449;&#21019;&#24314;&#36807;&#31243;&#12290;&#20026;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;30,000&#20010;&#29420;&#29305;&#30340;&#20195;&#30721;&#21644;&#23545;&#24212;&#25130;&#22270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;MSE&#12289;BLEU&#12289;IoU&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;htmlBLEU&#24471;&#20998;&#31561;&#33258;&#21160;&#21270;&#25351;&#26631;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#29992;DiT-GPT2&#27169;&#22411;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated HTML/CSS code generation from screenshots is an important yet challenging problem with broad applications in website development and design. In this paper, we present a novel vision-code transformer approach that leverages an Encoder-Decoder architecture as well as explore actor-critic fine-tuning as a method for improving upon the baseline. For this purpose, two image encoders are compared: Vision Transformer (ViT) and Document Image Transformer (DiT).  We propose an end-to-end pipeline that can generate high-quality code snippets directly from screenshots, streamlining the website creation process for developers. To train and evaluate our models, we created a synthetic dataset of 30,000 unique pairs of code and corresponding screenshots.  We evaluate the performance of our approach using a combination of automated metrics such as MSE, BLEU, IoU, and a novel htmlBLEU score, where our models demonstrated strong performance. We establish a strong baseline with the DiT-GPT2 mod
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#36229;&#27169;&#25968;&#31209;&#30340;&#27010;&#24565;&#65292;&#24182;&#20351;&#29992;&#23376;&#27169;&#20989;&#25968;&#20998;&#35299;&#20248;&#21270;&#38598;&#21512;&#20989;&#25968;&#65292;&#25552;&#39640;&#20102;&#20960;&#20010;&#31639;&#27861;&#30340;&#36924;&#36817;&#27604;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.14632</link><description>&lt;p&gt;
&#36229;&#27169;&#25968;&#31209;&#65306;&#38598;&#21512;&#20989;&#25968;&#20998;&#35299;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Supermodular Rank: Set Function Decomposition and Optimization. (arXiv:2305.14632v1 [math.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#36229;&#27169;&#25968;&#31209;&#30340;&#27010;&#24565;&#65292;&#24182;&#20351;&#29992;&#23376;&#27169;&#20989;&#25968;&#20998;&#35299;&#20248;&#21270;&#38598;&#21512;&#20989;&#25968;&#65292;&#25552;&#39640;&#20102;&#20960;&#20010;&#31639;&#27861;&#30340;&#36924;&#36817;&#27604;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23450;&#20041;&#20102;&#26684;&#19978;&#20989;&#25968;&#30340;&#36229;&#27169;&#25968;&#31209;&#12290;&#36825;&#26159;&#23558;&#20854;&#20998;&#35299;&#20026;&#36229;&#27169;&#20989;&#25968;&#20043;&#21644;&#25152;&#38656;&#30340;&#26368;&#23569;&#39033;&#25968;&#12290;&#36229;&#27169;&#21644;&#24335;&#26681;&#25454;&#19981;&#21516;&#30340;&#20559;&#24207;&#23450;&#20041;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#36229;&#27169;&#25968;&#31209;&#30340;&#26368;&#22823;&#21487;&#33021;&#20540;&#65292;&#24182;&#25551;&#36848;&#20102;&#20855;&#26377;&#22266;&#23450;&#36229;&#27169;&#25968;&#31209;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#21516;&#26679;&#23450;&#20041;&#20102;&#27425;&#27169;&#25968;&#31209;&#65292;&#24182;&#20351;&#29992;&#27425;&#27169;&#20989;&#25968;&#30340;&#20998;&#35299;&#26469;&#20248;&#21270;&#38598;&#21512;&#20989;&#25968;&#12290;&#32473;&#23450;&#38598;&#21512;&#20989;&#25968;&#30340;&#27425;&#27169;&#25968;&#31209;&#19978;&#30028;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#23558;&#20248;&#21270;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#27169;&#20989;&#25968;&#23376;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#20960;&#20010;&#21333;&#35843;&#38598;&#21512;&#20989;&#25968;&#26368;&#22823;&#21270;&#21644;&#38598;&#21512;&#20989;&#25968;&#27604;&#20540;&#26497;&#23567;&#21270;&#31639;&#27861;&#30340;&#36924;&#36817;&#27604;&#20445;&#35777;&#65292;&#35745;&#31639;&#24320;&#38144;&#21462;&#20915;&#20110;&#27425;&#27169;&#25968;&#31209;&#12290;
&lt;/p&gt;
&lt;p&gt;
We define the supermodular rank of a function on a lattice. This is the smallest number of terms needed to decompose it into a sum of supermodular functions. The supermodular summands are defined with respect to different partial orders. We characterize the maximum possible value of the supermodular rank and describe the functions with fixed supermodular rank. We analogously define the submodular rank. We use submodular decompositions to optimize set functions. Given a bound on the submodular rank of a set function, we formulate an algorithm that splits an optimization problem into submodular subproblems. We show that this method improves the approximation ratio guarantees of several algorithms for monotone set function maximization and ratio of set functions minimization, at a computation overhead that depends on the submodular rank.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;ALCE&#65292;&#26159;&#39318;&#20010;&#33258;&#21160;LLMs&#24341;&#25991;&#35780;&#20272;&#22522;&#20934;&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24102;&#24341;&#25991;&#30340;&#25991;&#26412;&#65292;&#25552;&#39640;&#20854;&#20107;&#23454;&#27491;&#30830;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#65307;&#25552;&#31034;LLMs&#29305;&#23450;&#30340;&#20851;&#38190;&#35789;&#25110;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#28304;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#24341;&#25991;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14627</link><description>&lt;p&gt;
&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24102;&#24341;&#25991;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Enabling Large Language Models to Generate Text with Citations. (arXiv:2305.14627v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;ALCE&#65292;&#26159;&#39318;&#20010;&#33258;&#21160;LLMs&#24341;&#25991;&#35780;&#20272;&#22522;&#20934;&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24102;&#24341;&#25991;&#30340;&#25991;&#26412;&#65292;&#25552;&#39640;&#20854;&#20107;&#23454;&#27491;&#30830;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#65307;&#25552;&#31034;LLMs&#29305;&#23450;&#30340;&#20851;&#38190;&#35789;&#25110;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#28304;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#24341;&#25991;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#24191;&#27867;&#20351;&#29992;&#30340;&#20449;&#24687;&#23547;&#25214;&#24037;&#20855;&#65292;&#20294;&#29983;&#25104;&#30340;&#36755;&#20986;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#12290;&#26412;&#25991;&#26088;&#22312;&#23454;&#29616;LLMs&#29983;&#25104;&#24102;&#24341;&#25991;&#30340;&#25991;&#26412;&#65292;&#25552;&#39640;&#20854;&#20107;&#23454;&#27491;&#30830;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ALCE&#65292;&#36825;&#26159;&#39318;&#20010;&#33258;&#21160;LLMs&#24341;&#25991;&#35780;&#20272;&#22522;&#20934;&#12290;ALCE&#25910;&#38598;&#20102;&#21508;&#31181;&#38382;&#39064;&#21644;&#26816;&#32034;&#35821;&#26009;&#24211;&#65292;&#24182;&#35201;&#27714;&#24314;&#31435;&#31471;&#21040;&#31471;&#31995;&#32479;&#20197;&#26816;&#32034;&#25903;&#25345;&#35777;&#25454;&#24182;&#29983;&#25104;&#24102;&#26377;&#24341;&#25991;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#27839;&#30528;&#27969;&#30021;&#24615;&#12289;&#27491;&#30830;&#24615;&#21644;&#24341;&#25991;&#36136;&#37327;&#19977;&#20010;&#32500;&#24230;&#26500;&#24314;&#33258;&#21160;&#25351;&#26631;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;LLMs&#21644;&#26032;&#30340;&#25552;&#31034;&#31574;&#30053;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#31995;&#32479;&#20173;&#26377;&#30456;&#24403;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;--&#20363;&#22914;&#65292;&#25552;&#31034;LLMs&#29305;&#23450;&#30340;&#20851;&#38190;&#35789;&#25110;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#28304;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#24341;&#25991;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#26410;&#26469;&#30740;&#31350;&#21457;&#23637;&#33021;&#22815;&#29983;&#25104;&#21487;&#39564;&#35777;&#21644;&#21487;&#20449;&#36182;&#36755;&#20986;&#30340;LLMs&#25552;&#20379;&#20102;&#22362;&#23454;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, we aim to enable LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare with different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We build automatic metrics along three dimensions -- fluency, correctness, and citation quality -- and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvements -for example,
&lt;/p&gt;</description></item><item><title>EXnet&#26159;&#19968;&#20010;&#27169;&#22411;&#65292;&#26088;&#22312;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#31034;&#20363;&#25968;&#37327;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#12290;&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#25552;&#39640;&#20219;&#21153;&#20934;&#30830;&#24615;&#21644;&#36328;&#20219;&#21153;&#26222;&#36866;&#24615;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#25991;&#26412;&#20998;&#31867;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2305.14622</link><description>&lt;p&gt;
EXnet: &#26080;&#25968;&#25454;&#25991;&#26412;&#20998;&#31867;&#30340;&#39640;&#25928;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EXnet: Efficient In-context Learning for Data-less Text classification. (arXiv:2305.14622v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14622
&lt;/p&gt;
&lt;p&gt;
EXnet&#26159;&#19968;&#20010;&#27169;&#22411;&#65292;&#26088;&#22312;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#31034;&#20363;&#25968;&#37327;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#12290;&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#25552;&#39640;&#20219;&#21153;&#20934;&#30830;&#24615;&#21644;&#36328;&#20219;&#21153;&#26222;&#36866;&#24615;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#25991;&#26412;&#20998;&#31867;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#32534;&#30721;&#19990;&#30028;&#30693;&#35782;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#65292;&#24182;&#20135;&#29983;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#23398;&#20064;&#33539;&#20363;&#65292;&#21253;&#25324;&#38646;-shot&#12289;&#23569;-shot&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#35768;&#22810;&#35821;&#35328;&#20219;&#21153;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#19968;&#32452;&#25552;&#31034;&#65288;&#20363;&#22914;&#65292;&#36825;&#27573;&#25991;&#26412;&#26159;&#21542;&#19982;&#22320;&#29702;&#26377;&#20851;&#65311;&#65289;&#21644;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#20108;&#36827;&#21046;&#31572;&#26696;&#65292;&#21363;&#8220;&#26159;&#8221;&#25110;&#8220;&#21542;&#8221;&#12290;&#26377;&#35777;&#25454;&#34920;&#26126;&#65292;&#35768;&#22810;PLM&#20351;&#29992;&#30340;&#19979;&#19968;&#20010;&#21333;&#35789;&#39044;&#27979;&#19982;&#38646;-shot&#33539;&#20363;&#19981;&#30456;&#31526;&#21512;&#12290;&#22240;&#27492;&#65292;PLMs&#20250;&#34987;&#24494;&#35843;&#20026;&#38382;&#31572;&#31995;&#32479;&#12290;&#19978;&#19979;&#25991;&#23398;&#20064;&#36890;&#36807;&#23558;&#25552;&#31034;&#21644;&#31034;&#20363;&#32467;&#21512;&#36215;&#26469;&#65292;&#25193;&#23637;&#20102;&#38646;-shot&#23398;&#20064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#20171;&#32461;&#20102;EXnet&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22312;&#19978;&#19979;&#25991;&#20013;&#36827;&#34892;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#27809;&#26377;&#20219;&#20309;&#20363;&#22806;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#25552;&#39640;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#31034;&#20363;&#26377;&#21161;&#20110;&#36328;&#20219;&#21153;&#30340;&#26222;&#36866;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#25991;&#26412;&#20998;&#31867;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained language models (PLMs) have made significant progress in encoding world knowledge and spawned a new set of learning paradigms including zero-shot, few-shot, and in-context learning. Many language tasks can be modeled as a set of prompts (for example, is this text about geography?) and language models can provide binary answers, i.e., Yes or No. There is evidence to suggest that the next-word prediction used by many PLMs does not align well with zero-shot paradigms. Therefore, PLMs are fine-tuned as a question-answering system. In-context learning extends zero-shot learning by incorporating prompts and examples, resulting in increased task accuracy. Our paper presents EXnet, a model specifically designed to perform in-context learning without any limitations on the number of examples. We argue that in-context learning is an effective method to increase task accuracy, and providing examples facilitates cross-task generalization, especially when it comes to text classifi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;&#23454;&#35299;&#26512;&#20989;&#25968;&#27169;&#22411;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#19981;&#20381;&#36182;&#26799;&#24230;&#19979;&#38477;&#25110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65292;&#24182;&#19988;&#36890;&#36807;&#29087;&#24713;&#30340;&#27888;&#21202;&#36924;&#36817;&#26041;&#27861;&#20174;&#23616;&#37096;&#20449;&#24687;&#20013;&#25277;&#26679;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#38750;&#22343;&#21248;&#23398;&#20064;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14606</link><description>&lt;p&gt;
Taylor&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Taylor Learning. (arXiv:2305.14606v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14606
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;&#23454;&#35299;&#26512;&#20989;&#25968;&#27169;&#22411;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#19981;&#20381;&#36182;&#26799;&#24230;&#19979;&#38477;&#25110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65292;&#24182;&#19988;&#36890;&#36807;&#29087;&#24713;&#30340;&#27888;&#21202;&#36924;&#36817;&#26041;&#27861;&#20174;&#23616;&#37096;&#20449;&#24687;&#20013;&#25277;&#26679;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#38750;&#22343;&#21248;&#23398;&#20064;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26159;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20013;&#22823;&#37096;&#20998;&#20248;&#21270;&#30340;&#22522;&#30784;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#26469;&#36924;&#36817;&#26399;&#26395;&#25104;&#26412;&#65288;&#39118;&#38505;&#65289;&#65292;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#25628;&#23547;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#26356;&#26032;&#27169;&#22411;&#23450;&#20041;&#21442;&#25968;&#30340;&#20540;&#65292;&#26469;&#36817;&#20284;&#22320;&#26368;&#23567;&#21270;&#26399;&#26395;&#25104;&#26412;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#21442;&#25968;&#26356;&#26032;&#37319;&#29992;&#26576;&#31181;&#24418;&#24335;&#30340;&#26799;&#24230;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#23454;&#35299;&#26512;&#20989;&#25968;&#30340;&#27169;&#22411;&#65292;&#26082;&#19981;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#20063;&#19981;&#20351;&#29992;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#31867;&#20989;&#25968;&#30001;&#23616;&#37096;&#20449;&#24687;&#23450;&#20041;&#65292;&#23558;&#29087;&#24713;&#30340;&#27888;&#21202;&#36924;&#36817;&#26041;&#27861;&#32622;&#20110;&#20174;&#20998;&#24067;&#20013;&#25277;&#26679;&#25968;&#25454;&#30340;&#32972;&#26223;&#20013;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#31181;&#38750;&#22343;&#21248;&#23398;&#20064;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical risk minimization stands behind most optimization in supervised machine learning. Under this scheme, labeled data is used to approximate an expected cost (risk), and a learning algorithm updates model-defining parameters in search of an empirical risk minimizer, with the aim of thereby approximately minimizing expected cost. Parameter update is often done by some sort of gradient descent. In this paper, we introduce a learning algorithm to construct models for real analytic functions using neither gradient descent nor empirical risk minimization. Observing that such functions are defined by local information, we situate familiar Taylor approximation methods in the context of sampling data from a distribution, and prove a nonuniform learning result.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20174;&#19981;&#30456;&#20132;&#30340;&#20860;&#23481;&#26631;&#31614;&#24207;&#21015;&#20013;&#39640;&#25928;&#22320;&#23398;&#20064;&#65292;&#23558;&#27492;&#36816;&#29992;&#20110;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#32852;&#21512;&#22788;&#29702;VerbNet&#21644;PropBank&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14600</link><description>&lt;p&gt;
&#23398;&#20064;&#20174;&#20860;&#23481;&#26631;&#31614;&#24207;&#21015;&#20013;&#30340;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Learning Semantic Role Labeling from Compatible Label Sequences. (arXiv:2305.14600v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14600
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20174;&#19981;&#30456;&#20132;&#30340;&#20860;&#23481;&#26631;&#31614;&#24207;&#21015;&#20013;&#39640;&#25928;&#22320;&#23398;&#20064;&#65292;&#23558;&#27492;&#36816;&#29992;&#20110;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#32852;&#21512;&#22788;&#29702;VerbNet&#21644;PropBank&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#23398;&#20064;&#20174;&#19981;&#30456;&#20132;&#30340;&#20860;&#23481;&#26631;&#31614;&#24207;&#21015;&#20013;&#26631;&#27880;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19981;&#30456;&#20132;&#26631;&#31614;&#38598;&#20043;&#38388;&#30340;&#20860;&#23481;&#32467;&#26500;&#26377;&#21161;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#22312;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#65288;SRL&#65289;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#36825;&#19968;&#20551;&#35774;&#65292;&#20855;&#20307;&#22320;&#65292;&#26631;&#35760;&#20855;&#26377;&#20004;&#20010;&#35282;&#33394;&#24207;&#21015;&#30340;&#21477;&#23376;&#65306;VerbNet&#21442;&#25968;&#21644;PropBank&#21442;&#25968;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#36328;&#20219;&#21153;&#20132;&#20114;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#36825;&#20004;&#20010;&#20219;&#21153;&#20173;&#28982;&#26159;&#20998;&#21035;&#35299;&#30721;&#30340;&#65292;&#23384;&#22312;&#29983;&#25104;&#32467;&#26500;&#19981;&#19968;&#33268;&#30340;&#26631;&#31614;&#24207;&#21015; (&#22312;&#20687;SEMLINK&#30340;&#35789;&#20856;&#20013;)&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#28040;&#38500;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35774;&#32622;&#65292;&#32852;&#21512;&#22788;&#29702;VerbNet&#21644;PropBank&#26631;&#31614;&#20316;&#20026;&#19968;&#20010;&#24207;&#21015;&#12290;&#36890;&#36807;&#36825;&#20010;&#35774;&#32622;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#24378;&#21046;&#25191;&#34892;SEMLINK&#32422;&#26463;&#19981;&#26029;&#25552;&#39640;&#24635;F1&#20540;&#12290;&#36890;&#36807;&#29305;&#27530;&#30340;&#36755;&#20837;&#26500;&#36896;&#65292;&#25105;&#20204;&#30340;&#32852;&#21512;&#27169;&#22411;&#21487;&#20197;&#20197;&#36229;&#36807;99%&#30340;&#20934;&#30830;&#24615;&#20174;PropBank&#21442;&#25968;&#20013;&#25512;&#26029;&#20986;VerbNet&#21442;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;co
&lt;/p&gt;
&lt;p&gt;
This paper addresses the question of how to efficiently learn from disjoint, compatible label sequences. We argue that the compatible structures between disjoint label sets help model learning and inference. We verify this hypothesis on the task of semantic role labeling (SRL), specifically, tagging a sentence with two role sequences: VerbNet arguments and PropBank arguments. Prior work has shown that cross-task interaction improves performance. However, the two tasks are still separately decoded, running the risk of generating structurally inconsistent label sequences (as per lexicons like SEMLINK). To eliminate this issue, we first propose a simple and effective setup that jointly handles VerbNet and PropBank labels as one sequence. With this setup, we show that enforcing SEMLINK constraints during decoding constantly improves the overall F1. With special input constructions, our joint model infers VerbNet arguments from PropBank arguments with over 99% accuracy. We also propose a co
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;AI&#23398;&#26415;&#30028;&#30340;78K&#30740;&#31350;&#20154;&#21592;&#30340;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;&#22899;&#24615;&#31532;&#19968;&#20316;&#32773;&#30340;&#35770;&#25991;&#20855;&#26377;&#19981;&#21516;&#30340;&#35821;&#35328;&#39118;&#26684;&#65292;&#20363;&#22914;&#26356;&#38271;&#30340;&#25991;&#26412;&#12289;&#26356;&#22810;&#30340;&#27491;&#38754;&#24773;&#24863;&#35789;&#27719;&#21644;&#26356;&#24341;&#20154;&#27880;&#30446;&#30340;&#26631;&#39064;&#65307;&#22312;AI&#35770;&#25991;&#30340;&#21512;&#33879;&#20013;&#23384;&#22312;&#24456;&#22823;&#30340;&#24615;&#21035;&#21516;&#36136;&#24615;&#12290;&#25105;&#20204;&#40723;&#21169;&#26410;&#26469;&#23454;&#29616;&#26356;&#22810;&#30340;&#24615;&#21035;&#24179;&#31561;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14597</link><description>&lt;p&gt;
&#22905;&#20204;&#30340;&#22768;&#38899;&#65306;&#20998;&#26512;&#20154;&#24037;&#26234;&#33021;&#20986;&#29256;&#39046;&#22495;&#30340;&#24615;&#21035;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Voices of Her: Analyzing Gender Differences in the AI Publication World. (arXiv:2305.14597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14597
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;AI&#23398;&#26415;&#30028;&#30340;78K&#30740;&#31350;&#20154;&#21592;&#30340;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;&#22899;&#24615;&#31532;&#19968;&#20316;&#32773;&#30340;&#35770;&#25991;&#20855;&#26377;&#19981;&#21516;&#30340;&#35821;&#35328;&#39118;&#26684;&#65292;&#20363;&#22914;&#26356;&#38271;&#30340;&#25991;&#26412;&#12289;&#26356;&#22810;&#30340;&#27491;&#38754;&#24773;&#24863;&#35789;&#27719;&#21644;&#26356;&#24341;&#20154;&#27880;&#30446;&#30340;&#26631;&#39064;&#65307;&#22312;AI&#35770;&#25991;&#30340;&#21512;&#33879;&#20013;&#23384;&#22312;&#24456;&#22823;&#30340;&#24615;&#21035;&#21516;&#36136;&#24615;&#12290;&#25105;&#20204;&#40723;&#21169;&#26410;&#26469;&#23454;&#29616;&#26356;&#22810;&#30340;&#24615;&#21035;&#24179;&#31561;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#20998;&#26512;&#20102;&#23398;&#26415;&#30028;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#20294;&#26159;&#25105;&#20204;&#20173;&#28982;&#32570;&#20047;&#19968;&#20010;&#20840;&#38754;&#30340;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#24615;&#21035;&#24046;&#24322;&#30340;&#20998;&#26512;&#65292;&#28085;&#30422;&#21508;&#31181;&#20027;&#39064;&#21644;&#19981;&#21516;&#30340;&#21457;&#23637;&#36235;&#21183;&#12290;&#25105;&#20204;&#20351;&#29992;AI Scholar&#25968;&#25454;&#38598;&#20013;&#30340;78K&#20301;AI&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#65292;&#21457;&#29616;&#20102;&#19968;&#20123;&#24615;&#21035;&#24046;&#24322;&#65306;&#65288;1&#65289;&#34429;&#28982;&#22899;&#24615;&#30740;&#31350;&#20154;&#21592;&#30340;&#24635;&#24341;&#29992;&#27425;&#25968;&#27604;&#30007;&#24615;&#23569;&#65292;&#20294;&#36825;&#31181;&#24341;&#29992;&#24046;&#24322;&#24182;&#19981;&#36866;&#29992;&#20110;&#25152;&#26377;&#23398;&#26415;&#24180;&#40836;&#32452;&#65307;&#65288;2&#65289;&#22312;AI&#35770;&#25991;&#30340;&#21512;&#33879;&#20013;&#23384;&#22312;&#24456;&#22823;&#30340;&#24615;&#21035;&#21516;&#36136;&#24615;&#65307;&#65288;3&#65289;&#22899;&#24615;&#31532;&#19968;&#20316;&#32773;&#30340;&#35770;&#25991;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#35821;&#35328;&#39118;&#26684;&#65292;&#20363;&#22914;&#26356;&#38271;&#30340;&#25991;&#26412;&#12289;&#26356;&#22810;&#30340;&#27491;&#38754;&#24773;&#24863;&#35789;&#27719;&#21644;&#26356;&#24341;&#20154;&#27880;&#30446;&#30340;&#26631;&#39064;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20026;&#25105;&#20204;&#30340;AI&#31038;&#21306;&#29616;&#26377;&#30340;&#20154;&#21475;&#32479;&#35745;&#36235;&#21183;&#25552;&#20379;&#20102;&#19968;&#20010;&#31383;&#21475;&#65292;&#24182;&#40723;&#21169;&#22312;&#26410;&#26469;&#23454;&#29616;&#26356;&#22810;&#30340;&#24615;&#21035;&#24179;&#31561;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;https://github.com/causalNLP/ai-scholar-gender&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
While several previous studies have analyzed gender bias in research, we are still missing a comprehensive analysis of gender differences in the AI community, covering diverse topics and different development trends. Using the AI Scholar dataset of 78K researchers in the field of AI, we identify several gender differences: (1) Although female researchers tend to have fewer overall citations than males, this citation difference does not hold for all academic-age groups; (2) There exist large gender homophily in co-authorship on AI papers; (3) Female first-authored papers show distinct linguistic styles, such as longer text, more positive emotion words, and more catchy titles than male first-authored papers. Our analysis provides a window into the current demographic trends in our AI community, and encourages more gender equality and diversity in the future. Our code and data are at https://github.com/causalNLP/ai-scholar-gender.
&lt;/p&gt;</description></item><item><title>&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#39033;&#36873;&#25321;&#39064;&#26102;&#65292;&#20854;&#27880;&#24847;&#21147;&#24448;&#24448;&#20250;&#20998;&#25955;&#21040;&#35768;&#22810;&#26080;&#25928;&#30340;&#35789;&#27719;&#31526;&#21495;&#19978;&#65292;&#36825;&#20250;&#23548;&#33268;&#27169;&#22411;&#30495;&#23454;&#24615;&#33021;&#30340;&#20302;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#21270;&#26041;&#27861;&#26469;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#20351;&#29992;&#21482;&#21547;&#19968;&#20010;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23545;&#26377;&#25928;&#36873;&#25321;&#30340;&#27880;&#24847;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14596</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#19981;&#19968;&#23450;&#24847;&#21619;&#30528;&#22312;&#35299;&#31572;&#20013;&#36873;&#25321;&#27491;&#30830;&#29575;&#24456;&#39640;
&lt;/p&gt;
&lt;p&gt;
Attentiveness to Answer Choices Doesn't Always Entail High QA Accuracy. (arXiv:2305.14596v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14596
&lt;/p&gt;
&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#39033;&#36873;&#25321;&#39064;&#26102;&#65292;&#20854;&#27880;&#24847;&#21147;&#24448;&#24448;&#20250;&#20998;&#25955;&#21040;&#35768;&#22810;&#26080;&#25928;&#30340;&#35789;&#27719;&#31526;&#21495;&#19978;&#65292;&#36825;&#20250;&#23548;&#33268;&#27169;&#22411;&#30495;&#23454;&#24615;&#33021;&#30340;&#20302;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#21270;&#26041;&#27861;&#26469;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#20351;&#29992;&#21482;&#21547;&#19968;&#20010;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23545;&#26377;&#25928;&#36873;&#25321;&#30340;&#27880;&#24847;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#24212;&#29992;&#20110;&#38646;&#25110;&#23569;&#26679;&#26412;&#30340;&#37492;&#21035;&#24615;&#20219;&#21153;&#65292;&#20363;&#22914;&#22810;&#39033;&#36873;&#25321;&#39064;&#26102;&#65292;&#23427;&#20204;&#30340;&#27880;&#24847;&#21147;&#65288;&#21363;&#27010;&#29575;&#36136;&#37327;&#65289;&#20250;&#20998;&#25955;&#22312;&#35768;&#22810;&#26080;&#25928;&#30340;&#35789;&#27719;&#31526;&#21495;&#19978;&#12290;&#36825;&#31181;&#22312;&#20855;&#26377;&#30456;&#21516;&#21547;&#20041;&#30340;&#22810;&#20010;&#34920;&#38754;&#24418;&#24335;&#20043;&#38388;&#20998;&#25955;&#23548;&#33268;&#20102;&#27169;&#22411;&#30495;&#23454;&#24615;&#33021;&#30340;&#20302;&#20272;&#65292;&#31216;&#20026;&#8220;&#34920;&#38754;&#24418;&#24335;&#31454;&#20105;&#8221;&#65288;SFC&#65289;&#20551;&#35828;&#12290;&#36825;&#20419;&#20351;&#24341;&#20837;&#21508;&#31181;&#27010;&#29575;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#28982;&#32780;&#20173;&#23384;&#22312;&#35768;&#22810;&#26680;&#24515;&#38382;&#39064;&#26410;&#35299;&#31572;&#12290;&#25105;&#20204;&#22914;&#20309;&#27979;&#37327;SFC&#25110;&#27880;&#24847;&#21147;&#65311;&#26159;&#21542;&#26377;&#30452;&#25509;&#30340;&#26041;&#27861;&#21487;&#20197;&#22686;&#21152;&#23545;&#26377;&#25928;&#36873;&#25321;&#30340;&#27880;&#24847;&#21147;&#65311;&#22686;&#21152;&#27880;&#24847;&#21147;&#24635;&#26159;&#33021;&#25552;&#39640;&#20219;&#21153;&#20934;&#30830;&#24615;&#21527;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#21270;&#26041;&#27861;&#26469;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#27880;&#24847;&#21147;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22686;&#21152;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#20165;&#21253;&#21547;&#31572;&#26696;&#36873;&#39033;&#30340;&#19968;&#20010;&#31034;&#20363;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
When large language models (LMs) are applied in zero- or few-shot settings to discriminative tasks such as multiple-choice questions, their attentiveness (i.e., probability mass) is spread across many vocabulary tokens that are not valid choices. Such a spread across multiple surface forms with identical meaning is thought to cause an underestimation of a model's true performance, referred to as the "surface form competition" (SFC) hypothesis. This has motivated the introduction of various probability normalization methods. However, many core questions remain unanswered. How do we measure SFC or attentiveness? Are there direct ways of increasing attentiveness on valid choices? Does increasing attentiveness always improve task accuracy? We propose a mathematical formalism for studying this phenomenon, provide a metric for quantifying attentiveness, and identify a simple method for increasing it -namely, in-context learning with even just one example containing answer choices. The form
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#23637;&#31034;&#20102;&#22914;&#26524;&#32771;&#34385;&#26410;&#27835;&#30103;&#24773;&#20917;&#21644;&#25972;&#20307;&#31119;&#21033;&#32780;&#19981;&#26159;&#34987;&#27835;&#30103;&#32773;&#30340;&#24179;&#22343;&#31119;&#21033;&#65292;&#24230;&#37327;&#30340;&#28608;&#21169;&#23558;&#19982;&#26368;&#22823;&#21270;&#24739;&#32773;&#24635;&#31119;&#21033;&#19968;&#33268;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#22914;&#20309;&#20462;&#25913;&#21453;&#20107;&#23454;&#24230;&#37327;&#20197;&#36798;&#21040;&#29702;&#24819;&#25490;&#21517;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.14595</link><description>&lt;p&gt;
&#23454;&#29616;&#21453;&#20107;&#23454;&#24230;&#37327;&#65306;&#28608;&#21169;&#12289;&#25490;&#21517;&#21644;&#20449;&#24687;&#19981;&#23545;&#31216;
&lt;/p&gt;
&lt;p&gt;
Operationalizing Counterfactual Metrics: Incentives, Ranking, and Information Asymmetry. (arXiv:2305.14595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14595
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#23637;&#31034;&#20102;&#22914;&#26524;&#32771;&#34385;&#26410;&#27835;&#30103;&#24773;&#20917;&#21644;&#25972;&#20307;&#31119;&#21033;&#32780;&#19981;&#26159;&#34987;&#27835;&#30103;&#32773;&#30340;&#24179;&#22343;&#31119;&#21033;&#65292;&#24230;&#37327;&#30340;&#28608;&#21169;&#23558;&#19982;&#26368;&#22823;&#21270;&#24739;&#32773;&#24635;&#31119;&#21033;&#19968;&#33268;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#22914;&#20309;&#20462;&#25913;&#21453;&#20107;&#23454;&#24230;&#37327;&#20197;&#36798;&#21040;&#29702;&#24819;&#25490;&#21517;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31038;&#20250;&#31185;&#23398;&#21040;&#26426;&#22120;&#23398;&#20064;&#65292;&#25991;&#29486;&#24050;&#32463;&#20805;&#20998;&#35777;&#26126;&#35201;&#20248;&#21270;&#30340;&#25351;&#26631;&#24182;&#19981;&#24635;&#26159;&#19982;&#31038;&#20250;&#31119;&#21033;&#19968;&#33268;&#12290;&#22312;&#21307;&#30103;&#20445;&#20581;&#26041;&#38754;&#65292;Dranove&#31561;&#20154;&#34920;&#26126;&#65292;&#21457;&#24067;&#25163;&#26415;&#27515;&#20129;&#29575;&#25351;&#26631;&#23454;&#38469;&#19978;&#36890;&#36807;&#22686;&#21152;&#25552;&#20379;&#32773;&#36873;&#25321;&#34892;&#20026;&#65292;&#21361;&#23475;&#20102;&#26356;&#30149;&#37325;&#24739;&#32773;&#30340;&#31119;&#21033;&#12290;&#21033;&#29992;&#22996;&#25176;-&#20195;&#29702;&#27169;&#22411;&#65292;&#25105;&#20204;&#30452;&#25509;&#30740;&#31350;&#20102;&#20174;&#36825;&#31181;&#24179;&#22343;&#27835;&#30103;&#32467;&#26524;&#24230;&#37327;&#20013;&#20135;&#29983;&#30340;&#28608;&#21169;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#26524;&#24230;&#37327;&#65288;i&#65289;&#32771;&#34385;&#26410;&#27835;&#30103;&#24773;&#20917;&#21644;&#65288;ii&#65289;&#32771;&#34385;&#25972;&#20307;&#31119;&#21033;&#32780;&#19981;&#26159;&#34987;&#27835;&#30103;&#32773;&#30340;&#24179;&#22343;&#31119;&#21033;&#65292;&#37027;&#20040;&#39537;&#21160;&#27835;&#30103;&#20915;&#31574;&#30340;&#28608;&#21169;&#23558;&#19982;&#26368;&#22823;&#21270;&#24739;&#32773;&#24635;&#31119;&#21033;&#19968;&#33268;&#12290;&#22312;&#36825;&#20010;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20462;&#25913;&#21453;&#20107;&#23454;&#24230;&#37327;&#20197;&#28385;&#36275;&#25490;&#21517;&#26102;&#30340;&#29702;&#24819;&#23646;&#24615;&#12290;&#22312;&#23558;&#36825;&#20010;&#26041;&#27861;&#25512;&#24191;&#21040;&#25552;&#20379;&#32773;&#35266;&#23519;&#21040;&#30340;&#20851;&#20110;&#24739;&#32773;&#36229;&#36234;&#30417;&#31649;&#26426;&#26500;&#30340;&#20449;&#24687;&#26356;&#22810;&#23454;&#38469;&#24773;&#20917;&#26102;&#65292;&#25105;&#20204;&#38480;&#21046;&#19981;&#23545;&#31216;&#20449;&#24687;&#31243;&#24230;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#24687;&#20849;&#20139;&#26041;&#26696;&#26469;&#25552;&#39640;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
From the social sciences to machine learning, it has been well documented that metrics to be optimized are not always aligned with social welfare. In healthcare, Dranove et al. [12] showed that publishing surgery mortality metrics actually harmed the welfare of sicker patients by increasing provider selection behavior. Using a principal-agent model, we directly study the incentive misalignments that arise from such average treated outcome metrics, and show that the incentives driving treatment decisions would align with maximizing total patient welfare if the metrics (i) accounted for counterfactual untreated outcomes and (ii) considered total welfare instead of average welfare among treated patients. Operationalizing this, we show how counterfactual metrics can be modified to satisfy desirable properties when used for ranking. Extending to realistic settings when the providers observe more about patients than the regulatory agencies do, we bound the decay in performance by the degree 
&lt;/p&gt;</description></item><item><title>torchgfn&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#26500;&#24314;&#30340;GFlowNet&#24211;&#65292;&#25552;&#20379;&#20102;&#31616;&#21333;&#30340;API&#21644;&#26377;&#29992;&#30340;&#25277;&#35937;&#65292;&#35299;&#20915;&#20102;&#19981;&#21516;&#20195;&#30721;&#24211;&#30340;&#32422;&#23450;&#38382;&#39064;&#65292;&#24182;&#19988;&#37325;&#29616;&#20102;&#24050;&#21457;&#34920;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14594</link><description>&lt;p&gt;
torchgfn&#65306;&#19968;&#20010;PyTorch GFlowNet&#24211;
&lt;/p&gt;
&lt;p&gt;
torchgfn: A PyTorch GFlowNet library. (arXiv:2305.14594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14594
&lt;/p&gt;
&lt;p&gt;
torchgfn&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#26500;&#24314;&#30340;GFlowNet&#24211;&#65292;&#25552;&#20379;&#20102;&#31616;&#21333;&#30340;API&#21644;&#26377;&#29992;&#30340;&#25277;&#35937;&#65292;&#35299;&#20915;&#20102;&#19981;&#21516;&#20195;&#30721;&#24211;&#30340;&#32422;&#23450;&#38382;&#39064;&#65292;&#24182;&#19988;&#37325;&#29616;&#20102;&#24050;&#21457;&#34920;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#25110;GFNs&#65289;&#30340;&#26085;&#30410;&#27969;&#34892;&#65292;&#20195;&#30721;&#26469;&#28304;&#20063;&#21464;&#24471;&#36234;&#26469;&#36234;&#22810;&#12290;&#36825;&#20250;&#22952;&#30861;&#23454;&#29616;&#26032;&#21151;&#33021;&#65288;&#20363;&#22914;&#35757;&#32451;&#25439;&#22833;&#65289;&#65292;&#36825;&#20123;&#21151;&#33021;&#21487;&#20197;&#36731;&#26494;&#22320;&#19982;&#29616;&#26377;&#21151;&#33021;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#22312;&#19968;&#32452;&#24120;&#35265;&#29615;&#22659;&#20013;&#36827;&#34892;&#20351;&#29992;&#12290;&#38500;&#20102;&#20943;&#32531;GFlowNets&#39046;&#22495;&#30340;&#30740;&#31350;&#22806;&#65292;&#19981;&#21516;&#30340;&#20195;&#30721;&#24211;&#36824;&#20351;&#29992;&#19981;&#21516;&#30340;&#32422;&#23450;&#65292;&#21487;&#33021;&#20250;&#35753;&#26032;&#25163;&#24863;&#21040;&#22256;&#24785;&#12290;torchgfn&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#26500;&#24314;&#30340;&#24211;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#23427;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#31616;&#21333;&#30340;API&#21644;&#26377;&#29992;&#30340;&#25277;&#35937;&#65292;&#20197;&#23454;&#29616;&#37319;&#26679;&#22120;&#21644;&#25439;&#22833;&#20989;&#25968;&#12290;&#25552;&#20379;&#20102;&#22810;&#20010;&#31034;&#20363;&#65292;&#37325;&#29616;&#20102;&#24050;&#21457;&#34920;&#30340;&#32467;&#26524;&#12290;&#35813;&#20195;&#30721;&#21487;&#22312;https://github.com/saleml/torchgfn&#20013;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing popularity of generative flow networks (GFlowNets or GFNs) is accompanied with a proliferation of code sources. This hinders the implementation of new features, such as training losses, that can readily be compared to existing ones, on a set of common environments. In addition to slowing down research in the field of GFlowNets, different code bases use different conventions, that might be confusing for newcomers. `torchgfn` is a library built on top of PyTorch, that aims at addressing both problems. It provides user with a simple API for environments, and useful abstractions for samplers and losses. Multiple examples are provided, replicating published results. The code is available in https://github.com/saleml/torchgfn.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#22522;&#20110;&#25490;&#24207;&#30340;&#27169;&#25311;&#26657;&#20934;&#65288;SBC&#65289;&#30340;&#28789;&#27963;&#20998;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#27979;&#35797;&#32479;&#35745;&#37327;&#65292;&#24182;&#35745;&#31639;&#20986;&#20174;&#20998;&#31867;&#20934;&#30830;&#24230;&#20013;&#35745;&#31639;&#20986;&#30340;&#35823;&#26657;&#20934;&#21457;&#25955;&#24230;&#24230;&#37327;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#32479;&#35745;&#21151;&#25928;&#65292;&#21487;&#20197;&#35299;&#20915;&#22810;&#37325;&#26816;&#39564;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.14593</link><description>&lt;p&gt;
&#21028;&#21035;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Discriminative calibration. (arXiv:2305.14593v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14593
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#22522;&#20110;&#25490;&#24207;&#30340;&#27169;&#25311;&#26657;&#20934;&#65288;SBC&#65289;&#30340;&#28789;&#27963;&#20998;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#27979;&#35797;&#32479;&#35745;&#37327;&#65292;&#24182;&#35745;&#31639;&#20986;&#20174;&#20998;&#31867;&#20934;&#30830;&#24230;&#20013;&#35745;&#31639;&#20986;&#30340;&#35823;&#26657;&#20934;&#21457;&#25955;&#24230;&#24230;&#37327;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#32479;&#35745;&#21151;&#25928;&#65292;&#21487;&#20197;&#35299;&#20915;&#22810;&#37325;&#26816;&#39564;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26816;&#39564;&#36125;&#21494;&#26031;&#35745;&#31639;&#30340;&#20934;&#30830;&#24615;&#65292;&#24120;&#24120;&#20351;&#29992;&#22522;&#20110;&#25490;&#24207;&#30340;&#27169;&#25311;&#26657;&#20934;&#65288;SBC&#65289;&#12290;&#28982;&#32780;&#65292;SBC &#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65306;&#27979;&#35797;&#32479;&#35745;&#37327;&#30053;&#26174;&#38543;&#24847;&#65292;&#20132;&#20114;&#24615;&#38590;&#20197;&#26816;&#26597;&#65292;&#22810;&#37325;&#26816;&#39564;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#24182;&#19988;&#24471;&#21040;&#30340; P &#20540;&#19981;&#26159;&#19968;&#31181;&#21457;&#25955;&#24230;&#24230;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#29992;&#19968;&#31181;&#28789;&#27963;&#30340;&#20998;&#31867;&#26041;&#27861;&#26367;&#25442;&#36793;&#32536;&#25490;&#24207;&#26816;&#39564;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#27979;&#35797;&#32479;&#35745;&#37327;&#12290;&#35813;&#24230;&#37327;&#36890;&#24120;&#20855;&#26377;&#27604; SBC &#25490;&#21517;&#26816;&#39564;&#26356;&#39640;&#30340;&#32479;&#35745;&#21151;&#25928;&#65292;&#24182;&#36820;&#22238;&#20174;&#20998;&#31867;&#20934;&#30830;&#24230;&#35745;&#31639;&#20986;&#30340;&#21487;&#35299;&#37322;&#30340;&#35823;&#26657;&#20934;&#21457;&#25955;&#24230;&#24230;&#37327;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#19982;&#19981;&#21516;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#19968;&#36215;&#20351;&#29992;&#65292;&#20197;&#24212;&#23545;&#26080;&#38656;&#20284;&#28982;&#25512;&#26029;&#25110;&#20256;&#32479;&#25512;&#26029;&#26041;&#27861;&#65288;&#22914;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#25110;&#21464;&#20998;&#25512;&#26029;&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#32479;&#35745;&#23398;&#21551;&#21457;&#24335;&#29305;&#24449;&#28436;&#31034;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#23454;&#29616;&#65292;&#24182;&#29992;&#25968;&#20540;&#21644;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
To check the accuracy of Bayesian computations, it is common to use rank-based simulation-based calibration (SBC). However, SBC has drawbacks: The test statistic is somewhat ad-hoc, interactions are difficult to examine, multiple testing is a challenge, and the resulting p-value is not a divergence metric. We propose to replace the marginal rank test with a flexible classification approach that learns test statistics from data. This measure typically has a higher statistical power than the SBC rank test and returns an interpretable divergence measure of miscalibration, computed from classification accuracy. This approach can be used with different data generating processes to address likelihood-free inference or traditional inference methods like Markov chain Monte Carlo or variational inference. We illustrate an automated implementation using neural networks and statistically-inspired features, and validate the method with numerical and real data experiments.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#35789;&#20856;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;&#39118;&#26684;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#38646;&#26679;&#26412;&#24615;&#33021;&#30340;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2305.14592</link><description>&lt;p&gt;
&#24102;&#35789;&#20856;&#30340;&#25351;&#20196;&#20248;&#21270;&#29992;&#20110;&#38646;&#26679;&#24335;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning with Lexicons for Zero-Shot Style Classification. (arXiv:2305.14592v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14592
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#35789;&#20856;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;&#39118;&#26684;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#38646;&#26679;&#26412;&#24615;&#33021;&#30340;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#26684;&#29992;&#20110;&#20256;&#36798;&#20316;&#32773;&#30340;&#24847;&#22270;&#21644;&#24577;&#24230;&#12290;&#23613;&#31649;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#39118;&#26684;&#20998;&#31867;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20808;&#21069;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;&#24102;&#26631;&#31614;&#30340;&#26679;&#26412;&#36827;&#34892;&#24494;&#35843;&#12290;&#21551;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23545;&#39118;&#26684;&#36827;&#34892;&#20998;&#31867;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#35821;&#35328;&#39118;&#26684;&#21487;&#33021;&#24456;&#38590;&#23450;&#20041;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#39118;&#26684;&#35789;&#20856;&#20316;&#20026;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#35782;&#21035;&#22312;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;&#39118;&#26684;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22522;&#20110;&#35789;&#20856;&#30340;&#25351;&#20196;&#26174;&#33879;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#24615;&#33021;&#30340;&#36716;&#31227;&#12290;&#25105;&#20204;&#23558;&#21457;&#24067;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Style is used to convey authors' intentions and attitudes. Despite the success of large pre-trained language models on style classification, prior work relies on fine-tuning with labeled examples. Prompting large language models to classify style without fine-tuning is challenging because language styles can be difficult to define. In this study, we investigate the effectiveness of style lexicons as a means for instructing language models how to identify new styles that are unseen during training. Our experiments show that lexicon-based instructions improve transfer zero-shot performance significantly. We will release our code and data.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#32858;&#28966;&#26426;&#21046;&#21644;&#26041;&#24046;&#36125;&#21494;&#26031;&#23398;&#20064;&#26469;&#25552;&#39640;&#21487;&#38752;&#24615;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#32858;&#28966;&#36830;&#32493;&#29983;&#25104;&#33258;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#33258;&#36866;&#24212;&#32763;&#35793;&#20219;&#21153;&#65292;&#19988;&#24050;&#22312;&#22810;&#39033;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36229;&#36234;&#20854;&#20182;&#26080;&#30417;&#30563;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.14589</link><description>&lt;p&gt;
&#32858;&#28966;&#36830;&#32493;&#29983;&#25104;&#33258;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#26080;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#33258;&#36866;&#24212;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Attentive Continuous Generative Self-training for Unsupervised Domain Adaptive Medical Image Translation. (arXiv:2305.14589v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14589
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#32858;&#28966;&#26426;&#21046;&#21644;&#26041;&#24046;&#36125;&#21494;&#26031;&#23398;&#20064;&#26469;&#25552;&#39640;&#21487;&#38752;&#24615;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#32858;&#28966;&#36830;&#32493;&#29983;&#25104;&#33258;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#33258;&#36866;&#24212;&#32763;&#35793;&#20219;&#21153;&#65292;&#19988;&#24050;&#22312;&#22810;&#39033;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36229;&#36234;&#20854;&#20182;&#26080;&#30417;&#30563;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#35757;&#32451;&#26159;&#19968;&#31867;&#37325;&#35201;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#36731;&#23558;&#20174;&#26631;&#35760;&#28304;&#22495;&#23398;&#21040;&#30340;&#30693;&#35782;&#24212;&#29992;&#20110;&#26410;&#26631;&#35760;&#21644;&#24322;&#26500;&#30446;&#26631;&#22495;&#26102;&#20986;&#29616;&#30340;&#39046;&#22495;&#31227;&#20301;&#38382;&#39064;&#12290;&#34429;&#28982;&#22522;&#20110;&#33258;&#25105;&#35757;&#32451;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#22312;&#21253;&#25324;&#20998;&#31867;&#21644;&#20998;&#21106;&#22312;&#20869;&#30340;&#21028;&#21035;&#20219;&#21153;&#20013;&#24050;&#26174;&#31034;&#20986;&#30456;&#24403;&#30340;&#20248;&#21183;&#65292;&#20294;&#36890;&#36807;&#26368;&#22823;softmax&#27010;&#29575;&#21487;&#38752;&#30340;&#20266;&#26631;&#31614;&#36807;&#28388;&#65292;&#20197;&#29983;&#25104;&#24615;&#20219;&#21153;&#20026;&#22522;&#30784;&#30340;&#33258;&#35757;&#32451;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#20808;&#21069;&#30740;&#31350;&#32570;&#20047;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#21147;&#27714;&#24320;&#21457;&#19968;&#31181;&#20855;&#26377;&#36830;&#32493;&#20540;&#39044;&#27979;&#21644;&#22238;&#24402;&#30446;&#26631;&#30340;&#29983;&#25104;&#33258;&#35757;&#32451;&#65288;GST&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#22270;&#20687;&#32763;&#35793;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#21464;&#20998;&#36125;&#21494;&#26031;&#23398;&#20064;&#26469;&#37327;&#21270;&#25105;&#20204;&#30340;GST&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#27979;&#37327;&#21512;&#25104;&#25968;&#25454;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#20943;&#24369;&#19981;&#30456;&#20851;&#21306;&#22495;&#30340;&#32763;&#35793;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#20445;&#30495;&#24230;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#22522;&#20934;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#20998;&#21035;&#26159;&#36328;&#27169;&#24577;&#30340;&#30913;&#20849;&#25391;&#25104;&#20687;&#21512;&#25104;&#21644;&#22810;&#23545;&#27604;&#24230;&#30340;&#30913;&#20849;&#25391;&#25104;&#20687;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-training is an important class of unsupervised domain adaptation (UDA) approaches that are used to mitigate the problem of domain shift, when applying knowledge learned from a labeled source domain to unlabeled and heterogeneous target domains. While self-training-based UDA has shown considerable promise on discriminative tasks, including classification and segmentation, through reliable pseudo-label filtering based on the maximum softmax probability, there is a paucity of prior work on self-training-based UDA for generative tasks, including image modality translation. To fill this gap, in this work, we seek to develop a generative self-training (GST) framework for domain adaptive image translation with continuous value prediction and regression objectives. Specifically, we quantify both aleatoric and epistemic uncertainties within our GST using variational Bayes learning to measure the reliability of synthesized data. We also introduce a self-attention scheme that de-emphasizes t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#24182;&#25913;&#36827;&#20102;&#23454;&#20307;&#38142;&#25509;&#26041;&#27861;&#22312;&#21338;&#29289;&#39302;&#34255;&#21697;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#24494;&#35843;&#30340;&#26368;&#26032;&#31471;&#21040;&#31471; EL &#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#21644;&#26368;&#20339;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.14588</link><description>&lt;p&gt;
&#35780;&#20272;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#24211;&#30340;&#31471;&#21040;&#31471;&#23454;&#20307;&#38142;&#25509;&#65306;&#20174;&#21338;&#29289;&#39302;&#25910;&#34255;&#20013;&#23398;&#20064;&#21476;&#20195;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating end-to-end entity linking on domain-specific knowledge bases: Learning about ancient technologies from museum collections. (arXiv:2305.14588v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#24182;&#25913;&#36827;&#20102;&#23454;&#20307;&#38142;&#25509;&#26041;&#27861;&#22312;&#21338;&#29289;&#39302;&#34255;&#21697;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#24494;&#35843;&#30340;&#26368;&#26032;&#31471;&#21040;&#31471; EL &#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#21644;&#26368;&#20339;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#30740;&#31350;&#31038;&#20250;&#12289;&#32463;&#27982;&#21644;&#21382;&#21490;&#38382;&#39064;&#65292;&#31038;&#20250;&#31185;&#23398;&#21644;&#20154;&#25991;&#23398;&#31185;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#20351;&#29992;&#36234;&#26469;&#36234;&#22823;&#30340;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;&#34429;&#28982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#36817;&#36827;&#23637;&#25552;&#20379;&#20102;&#35768;&#22810;&#26377;&#25928;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#30340;&#24037;&#20855;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#19981;&#36866;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#24615;&#33021;&#21644;&#36866;&#21512;&#24615;&#37117;&#19981;&#26159;&#24456;&#22909;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23581;&#35797;&#21435;&#22635;&#34917;&#39046;&#22495;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#20351;&#29992;&#29616;&#20195;&#23454;&#20307;&#38142;&#25509;&#26041;&#27861;&#26469;&#20016;&#23500;&#21338;&#29289;&#39302;&#34255;&#21697;&#25968;&#25454;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;7,510&#23545;&#23454;&#20307;&#25552;&#21450;&#65292;&#20849;1700&#22810;&#20010;&#25991;&#26412;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#35814;&#32454;&#35780;&#20272;&#20102;&#19968;&#20123;&#29616;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20351;&#29992;&#27492;&#25968;&#25454;&#23545;&#26368;&#36817;&#30340;&#31471;&#21040;&#31471; EL &#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#24494;&#35843;&#30340;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#30446;&#21069;&#27492;&#39046;&#22495;&#20013;&#20854;&#20182;&#21487;&#29992;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#27010;&#24565;&#39564;&#35777;&#29992;&#20363;&#65292;&#21516;&#26102;&#24320;&#25918;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
To study social, economic, and historical questions, researchers in the social sciences and humanities have started to use increasingly large unstructured textual datasets. While recent advances in NLP provide many tools to efficiently process such data, most existing approaches rely on generic solutions whose performance and suitability for domain-specific tasks is not well understood. This work presents an attempt to bridge this domain gap by exploring the use of modern Entity Linking approaches for the enrichment of museum collection data. We collect a dataset comprising of more than 1700 texts annotated with 7,510 mention-entity pairs, evaluate some off-the-shelf solutions in detail using this dataset and finally fine-tune a recent end-to-end EL model on this data. We show that our fine-tuned model significantly outperforms other approaches currently available in this domain and present a proof-of-concept use case of this model. We release our dataset and our best model.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#35268;&#33539;&#21270;&#30340;&#20266;&#31070;&#32463;&#20999;&#32447;&#26680;&#65292;&#35777;&#26126;&#20102;&#23427;&#33021;&#22815;&#26356;&#22909;&#22320;&#19982;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#20989;&#25968;&#30456;&#20851;&#65292;&#27604;&#22522;&#20110;&#23884;&#20837;&#21644;&#24433;&#21709;&#30340;&#26367;&#20195;&#21697;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#20174;&#23427;&#21019;&#24314;&#30340;&#24402;&#22240;&#20250;&#26356;&#20934;&#30830;&#22320;&#36873;&#25321;&#34987;&#25200;&#21160;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#26680;&#32447;&#24615;&#27169;&#22411;&#26159;&#36328;&#22810;&#20010;&#25968;&#25454;&#39046;&#22495;&#24182;&#26377;&#25928;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.14585</link><description>&lt;p&gt;
&#36890;&#36807;&#20266;&#31070;&#32463;&#20999;&#32447;&#26680;&#20195;&#29702;&#27169;&#22411;&#25552;&#20379;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Robust Explanations for Deep Neural Networks via Pseudo Neural Tangent Kernel Surrogate Models. (arXiv:2305.14585v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#35268;&#33539;&#21270;&#30340;&#20266;&#31070;&#32463;&#20999;&#32447;&#26680;&#65292;&#35777;&#26126;&#20102;&#23427;&#33021;&#22815;&#26356;&#22909;&#22320;&#19982;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#20989;&#25968;&#30456;&#20851;&#65292;&#27604;&#22522;&#20110;&#23884;&#20837;&#21644;&#24433;&#21709;&#30340;&#26367;&#20195;&#21697;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#20174;&#23427;&#21019;&#24314;&#30340;&#24402;&#22240;&#20250;&#26356;&#20934;&#30830;&#22320;&#36873;&#25321;&#34987;&#25200;&#21160;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#26680;&#32447;&#24615;&#27169;&#22411;&#26159;&#36328;&#22810;&#20010;&#25968;&#25454;&#39046;&#22495;&#24182;&#26377;&#25928;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#25968;&#25454;&#24402;&#23646;&#20219;&#21153;&#65292;&#35299;&#37322;&#22411;AI&#30340;&#36827;&#27493;&#20043;&#19968;&#26159;&#36890;&#36807;&#35299;&#37322;&#31034;&#20363;&#31574;&#30053;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#23558;&#20915;&#31574;&#24402;&#22240;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#23578;&#26410;&#30456;&#20114;&#27604;&#36739;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#26159;&#21542;&#24418;&#25104;&#31070;&#32463;&#32593;&#32476;(NN)&#30340;&#30495;&#27491;&#20195;&#29702;&#27169;&#22411;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#35777;&#26126;&#20102;&#32447;&#24615;&#29305;&#24449;&#31354;&#38388;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#65306;(1)&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#35268;&#33539;&#21270;&#30340;&#20266;&#31070;&#32463;&#20999;&#32447;&#26680;(pNTK)&#65292;&#23427;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#20013;&#19982;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#20989;&#25968;&#26356;&#30456;&#20851;&#65292;&#27604;&#22522;&#20110;&#23884;&#20837;&#21644;&#24433;&#21709;&#30340;&#26367;&#20195;&#21697;&#26356;&#20026;&#26377;&#25928;&#65307;(2)&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#35268;&#33539;&#21270;pNTK&#21019;&#24314;&#30340;&#24402;&#22240;&#27604;&#36825;&#20123;&#26367;&#20195;&#21697;&#26356;&#20934;&#30830;&#22320;&#36873;&#25321;&#34987;&#25200;&#21160;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#26680;&#32447;&#24615;&#27169;&#22411;&#26159;&#36328;&#22810;&#20010;&#25968;&#25454;&#39046;&#22495;&#24182;&#26377;&#25928;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the ways recent progress has been made on explainable AI has been via explain-by-example strategies, specifically, through data attribution tasks. The feature spaces used to attribute decisions to training data, however, have not been compared against one another as to whether they form a truly representative surrogate model of the neural network (NN). Here, we demonstrate the efficacy of surrogate linear feature spaces to neural networks through two means: (1) we establish that a normalized psuedo neural tangent kernel (pNTK) is more correlated to the neural network decision functions than embedding based and influence based alternatives in both computer vision and large language model architectures; (2) we show that the attributions created from the normalized pNTK more accurately select perturbed training data in a data poisoning attribution task than these alternatives. Based on these observations, we conclude that kernel linear models are effective surrogate models across m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#28145;&#24230;&#27169;&#22411;&#30340;&#20107;&#21518;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#31867;&#21035; - &#20869;&#22312;&#21487;&#35299;&#37322;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#24120;&#35265;&#30340;&#35299;&#37322;&#35780;&#20272;&#25351;&#26631;&#65292;&#25552;&#20986;&#20102;&#26102;&#38388;&#24207;&#21015;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.14582</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#28145;&#24230;&#27169;&#22411;&#30340;&#35299;&#37322;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Interpretation of Time-Series Deep Models: A Survey. (arXiv:2305.14582v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#28145;&#24230;&#27169;&#22411;&#30340;&#20107;&#21518;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#31867;&#21035; - &#20869;&#22312;&#21487;&#35299;&#37322;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#24120;&#35265;&#30340;&#35299;&#37322;&#35780;&#20272;&#25351;&#26631;&#65292;&#25552;&#20986;&#20102;&#26102;&#38388;&#24207;&#21015;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#24320;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#19981;&#30452;&#35266;&#24615;&#65292;&#35299;&#37322;&#24615;&#38382;&#39064;&#8212;&#8212;&#25105;&#20204;&#22914;&#20309;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#20869;&#37096;&#21407;&#29702;&#8212;&#8212;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#31867;&#20284;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#31867;&#20284;&#30740;&#31350;&#25512;&#21160;&#20102;&#35768;&#22810;&#20107;&#21518;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#26041;&#27861;&#20063;&#21487;&#20197;&#38416;&#26126;&#22914;&#20309;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;&#21453;&#21521;&#20256;&#25773;&#12289;&#25200;&#21160;&#12289;&#36924;&#36817;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#24819;&#30528;&#37325;&#20171;&#32461;&#20869;&#22312;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#21487;&#20154;&#29702;&#35299;&#30340;&#20449;&#24687;&#35774;&#35745;&#21040;&#27169;&#22411;&#20013;&#30340;&#26032;&#39062;&#35299;&#37322;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29992;&#20110;&#35299;&#37322;&#30340;&#24120;&#35265;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#26410;&#26469;&#30740;&#31350;&#30340;&#20960;&#20010;&#26041;&#21521;&#12290;&#20540;&#24471;&#19968;&#25552;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#19981;&#20165;&#24635;&#32467;&#20102;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#28145;&#24230;&#27169;&#22411;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#20107;&#21518;&#26041;&#27861;&#65292;&#36824;&#20984;&#26174;&#20102;&#21457;&#23637;&#20869;&#22312;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#26032;&#20852;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models developed for time-series associated tasks have become more widely researched nowadays. However, due to the unintuitive nature of time-series data, the interpretability problem -- where we understand what is under the hood of these models -- becomes crucial. The advancement of similar studies in computer vision has given rise to many post-hoc methods, which can also shed light on how to explain time-series models. In this paper, we present a wide range of post-hoc interpretation methods for time-series models based on backpropagation, perturbation, and approximation. We also want to bring focus onto inherently interpretable models, a novel category of interpretation where human-understandable information is designed within the models. Furthermore, we introduce some common evaluation metrics used for the explanations, and propose several directions of future researches on the time-series interpretability problem. As a highlight, our work summarizes not only the well
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36873;&#21462;&#36974;&#34109;&#25968;&#25454;&#30340;&#26041;&#27861;&#65288;Difference-Masking&#65289;&#65292;&#20197;&#25552;&#39640;&#22312;&#32487;&#32493;&#39044;&#35757;&#32451;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26041;&#27861;&#26159;&#36890;&#36807;&#32771;&#34385;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#19982;&#39044;&#35757;&#32451;&#22495;&#30340;&#19981;&#21516;&#20043;&#22788;&#26469;&#36827;&#34892;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#21319;&#32487;&#32493;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#19988;&#20855;&#26377;&#36328;&#20219;&#21153;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14577</link><description>&lt;p&gt;
&#24046;&#24322;&#24615;&#36974;&#25377;&#65306;&#36873;&#25321;&#22312;&#32487;&#32493;&#39044;&#35757;&#32451;&#20013;&#36974;&#25377;&#20160;&#20040;
&lt;/p&gt;
&lt;p&gt;
Difference-Masking: Choosing What to Mask in Continued Pretraining. (arXiv:2305.14577v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36873;&#21462;&#36974;&#34109;&#25968;&#25454;&#30340;&#26041;&#27861;&#65288;Difference-Masking&#65289;&#65292;&#20197;&#25552;&#39640;&#22312;&#32487;&#32493;&#39044;&#35757;&#32451;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26041;&#27861;&#26159;&#36890;&#36807;&#32771;&#34385;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#19982;&#39044;&#35757;&#32451;&#22495;&#30340;&#19981;&#21516;&#20043;&#22788;&#26469;&#36827;&#34892;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#21319;&#32487;&#32493;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#19988;&#20855;&#26377;&#36328;&#20219;&#21153;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;(SSL)&#65292;&#29305;&#21035;&#26159;&#36974;&#25377;&#39044;&#27979;&#30446;&#26631;&#30340;&#30446;&#26631;&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#26159;&#38543;&#26426;&#22320;&#36827;&#34892;&#26631;&#35760;&#21644;&#36974;&#25377;&#65292;&#32780;&#22312;&#25945;&#32946;&#39046;&#22495;&#26377;&#24378;&#28872;&#30340;&#30452;&#35273;&#35748;&#20026;&#65292;&#20915;&#23450;&#20160;&#20040;&#38656;&#35201;&#36974;&#25377;&#21487;&#20197;&#23454;&#36136;&#24615;&#22320;&#25913;&#21892;&#23398;&#20064;&#32467;&#26524;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#24046;&#24322;&#36974;&#25377;(Difference-Masking)&#65292;&#19968;&#31181;&#33258;&#21160;&#36873;&#25321;&#36974;&#25377;&#20160;&#20040;&#30340;&#26041;&#27861;&#65292;&#22312;&#32487;&#32493;&#39044;&#35757;&#32451;&#20013;&#36890;&#36807;&#32771;&#34385;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#19982;&#39044;&#35757;&#32451;&#22495;&#30340;&#19981;&#21516;&#20043;&#22788;&#26469;&#23454;&#29616;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#24046;&#24322;&#36974;&#25377;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#35270;&#39057;&#20219;&#21153;&#30340;&#32487;&#32493;&#39044;&#35757;&#32451;&#35774;&#32622;&#20013;&#20248;&#20110;&#22522;&#32447;&#12290;&#24046;&#24322;&#24615;&#36974;&#25377;&#30340;&#36328;&#20219;&#21153;&#36866;&#29992;&#24615;&#25903;&#25345;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;SSL&#39044;&#35757;&#32451;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) and the objective of masking-and-predicting in particular have led to promising SSL performance on a variety of downstream tasks. However, while most approaches randomly mask tokens, there is strong intuition from the field of education that deciding what to mask can substantially improve learning outcomes. We introduce Difference-Masking, an approach that automatically chooses what to mask during continued pretraining by considering what makes an unlabelled target domain different from the pretraining domain. Empirically, we find that Difference-Masking outperforms baselines on continued pretraining settings across four diverse language and multimodal video tasks. The cross-task applicability of Difference-Masking supports the effectiveness of our framework for SSL pretraining in language, vision, and other domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#26377;&#20559;&#38388;&#25509;&#20851;&#31995;&#20462;&#25913;&#65288;BIRM&#65289;&#65292;&#20197;&#20943;&#36731;&#20998;&#24067;&#24335;&#35789;&#23884;&#20837;&#20013;&#30340;&#38388;&#25509;&#20559;&#35265;&#65292;&#36890;&#36807;&#32771;&#34385;&#26631;&#35760;&#20559;&#35265;&#23646;&#24615;&#30340;&#21333;&#35789;&#22312;&#23384;&#22312;&#24773;&#20917;&#19979;&#32473;&#23450;&#19968;&#23545;&#21333;&#35789;&#30340;&#20849;&#29616;&#27010;&#29575;&#22914;&#20309;&#21464;&#21270;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#28857;&#24179;&#22343;&#20559;&#35265;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.14574</link><description>&lt;p&gt;
&#25506;&#27979;&#21644;&#20943;&#23569;&#35789;&#23884;&#20837;&#20013;&#30340;&#38388;&#25509;&#21051;&#26495;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
Detecting and Mitigating Indirect Stereotypes in Word Embeddings. (arXiv:2305.14574v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#26377;&#20559;&#38388;&#25509;&#20851;&#31995;&#20462;&#25913;&#65288;BIRM&#65289;&#65292;&#20197;&#20943;&#36731;&#20998;&#24067;&#24335;&#35789;&#23884;&#20837;&#20013;&#30340;&#38388;&#25509;&#20559;&#35265;&#65292;&#36890;&#36807;&#32771;&#34385;&#26631;&#35760;&#20559;&#35265;&#23646;&#24615;&#30340;&#21333;&#35789;&#22312;&#23384;&#22312;&#24773;&#20917;&#19979;&#32473;&#23450;&#19968;&#23545;&#21333;&#35789;&#30340;&#20849;&#29616;&#27010;&#29575;&#22914;&#20309;&#21464;&#21270;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#28857;&#24179;&#22343;&#20559;&#35265;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35265;&#30340;&#35789;&#23884;&#20837;&#26041;&#27861;&#20250;&#23398;&#20064;&#21040;&#22312;&#20351;&#29992;&#21333;&#35789;&#26102;&#23384;&#22312;&#30340;&#31038;&#20250;&#20559;&#35265;&#21644;&#26377;&#23475;&#21051;&#26495;&#21360;&#35937;&#12290;&#36825;&#20123;&#20559;&#35265;&#19981;&#20165;&#23384;&#22312;&#20110;&#35789;&#26412;&#36523;&#21644;&#20854;&#26126;&#30830;&#30340;&#21051;&#26495;&#21360;&#35937;&#26631;&#35760;&#20043;&#38388;&#65292;&#32780;&#19988;&#36824;&#23384;&#22312;&#20110;&#20849;&#20139;&#30456;&#20851;&#21051;&#26495;&#21360;&#35937;&#30340;&#35789;&#20043;&#38388;&#12290;&#36825;&#31181;&#31216;&#20026;&#8220;&#38388;&#25509;&#20559;&#35265;&#8221;&#30340;&#29616;&#35937;&#24050;&#32463;&#38459;&#30861;&#20102;&#20043;&#21069;&#30340;&#35797;&#22270;&#28040;&#38500;&#36825;&#20123;&#20559;&#35265;&#30340;&#23581;&#35797;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#26377;&#20559;&#38388;&#25509;&#20851;&#31995;&#20462;&#25913;&#65288;BIRM&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#23884;&#20837;&#35789;&#20043;&#21069;&#36890;&#36807;&#20462;&#25913;&#21333;&#35789;&#20043;&#38388;&#30340;&#26377;&#20559;&#20851;&#31995;&#26469;&#20943;&#36731;&#20998;&#24067;&#24335;&#35789;&#23884;&#20837;&#20013;&#30340;&#38388;&#25509;&#20559;&#35265;&#12290;&#26041;&#27861;&#26159;&#36890;&#36807;&#32771;&#34385;&#22312;&#26631;&#35760;&#20559;&#35265;&#23646;&#24615;&#30340;&#21333;&#35789;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#32473;&#23450;&#19968;&#23545;&#21333;&#35789;&#30340;&#20849;&#29616;&#27010;&#29575;&#22914;&#20309;&#21464;&#21270;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#28857;&#24179;&#22343;&#20559;&#35265;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#24120;&#35265;&#30340;&#27979;&#35797;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#31245;&#24494;&#20943;&#23569;&#35821;&#20041;&#26041;&#38754;&#30340;&#21516;&#26102;&#65292;&#20943;&#23569;&#20102;&#35789;&#23884;&#20837;&#20013;&#30340;&#20559;&#35265;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Societal biases in the usage of words, including harmful stereotypes, are frequently learned by common word embedding methods. These biases manifest not only between a word and an explicit marker of its stereotype, but also between words that share related stereotypes. This latter phenomenon, sometimes called "indirect bias,'' has resisted prior attempts at debiasing. In this paper, we propose a novel method called Biased Indirect Relationship Modification (BIRM) to mitigate indirect bias in distributional word embeddings by modifying biased relationships between words before embeddings are learned. This is done by considering how the co-occurrence probability of a given pair of words changes in the presence of words marking an attribute of bias, and using this to average out the effect of a bias attribute. To evaluate this method, we perform a series of common tests and demonstrate that measures of bias in the word embeddings are reduced in exchange for minor reduction in the semantic
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21464;&#23384;&#20648;&#30340;&#27880;&#24847;&#21147;&#31070;&#32463;&#36807;&#31243; (CMANPs) &#21450;&#20854;&#27880;&#24847;&#21147;&#22359; CMAB&#65292;&#33021;&#22312;&#24120;&#25968;&#20869;&#23384;&#19979;&#36827;&#34892;&#26465;&#20214;&#12289;&#26597;&#35810;&#21644;&#26356;&#26032;&#25805;&#20316;&#65292;&#24182;&#22312;&#20803;&#22238;&#24402;&#21644;&#23569;&#26679;&#26412;&#22238;&#24402;&#20219;&#21153;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.14567</link><description>&lt;p&gt;
&#19981;&#21464;&#23384;&#20648;&#30340;&#27880;&#24847;&#21147;&#31070;&#32463;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Constant Memory Attentive Neural Processes. (arXiv:2305.14567v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14567
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21464;&#23384;&#20648;&#30340;&#27880;&#24847;&#21147;&#31070;&#32463;&#36807;&#31243; (CMANPs) &#21450;&#20854;&#27880;&#24847;&#21147;&#22359; CMAB&#65292;&#33021;&#22312;&#24120;&#25968;&#20869;&#23384;&#19979;&#36827;&#34892;&#26465;&#20214;&#12289;&#26597;&#35810;&#21644;&#26356;&#26032;&#25805;&#20316;&#65292;&#24182;&#22312;&#20803;&#22238;&#24402;&#21644;&#23569;&#26679;&#26412;&#22238;&#24402;&#20219;&#21153;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36807;&#31243; (Neural Processes, NPs) &#26159;&#20272;&#35745;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;NPs &#21253;&#21547;&#19968;&#20010;&#32534;&#30721;&#26465;&#20214;&#25968;&#25454;&#38598;&#30340;&#26465;&#20214;&#38454;&#27573;&#12289;&#19968;&#20010;&#20351;&#29992;&#32534;&#30721;&#39044;&#27979;&#30340;&#26597;&#35810;&#38454;&#27573;&#21644;&#19968;&#20010;&#20351;&#29992;&#26032;&#25968;&#25454;&#28857;&#26356;&#26032;&#32534;&#30721;&#30340;&#26356;&#26032;&#38454;&#27573;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;&#20869;&#23384;&#65292;&#36825;&#20010;&#20869;&#23384;&#38543;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21576;&#32447;&#24615;&#25110;&#20108;&#27425;&#20989;&#25968;&#22686;&#38271;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#30340;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21464;&#23384;&#20648;&#30340;&#27880;&#24847;&#21147;&#31070;&#32463;&#36807;&#31243; (Constant Memory Attentive Neural Processes, CMANPs)&#65292;&#23427;&#30340;&#26465;&#20214;&#12289;&#26597;&#35810;&#21644;&#26356;&#26032;&#38454;&#27573;&#22343;&#21482;&#38656;&#35201;&#24120;&#25968;&#20869;&#23384;&#12290;&#22312;&#26500;&#24314; CMANPs &#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#36890;&#29992;&#27880;&#24847;&#21147;&#22359;&#65292;&#31216;&#20026; Constant Memory Attention Block (CMAB)&#65292;&#23427;&#21487;&#20197;&#22312;&#24120;&#25968;&#20869;&#23384;&#20013;&#35745;&#31639;&#36755;&#20986;&#24182;&#36827;&#34892;&#24120;&#25968;&#30340;&#26356;&#26032;&#35745;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CMANPs &#22312;&#20803;&#22238;&#24402;&#21644;&#23569;&#26679;&#26412;&#22238;&#24402;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#20445;&#25345;&#24120;&#25968;&#20869;&#23384;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Processes (NPs) are efficient methods for estimating predictive uncertainties. NPs comprise of a conditioning phase where a context dataset is encoded, a querying phase where the model makes predictions using the context dataset encoding, and an updating phase where the model updates its encoding with newly received datapoints. However, state-of-the-art methods require additional memory which scales linearly or quadratically with the size of the dataset, limiting their applications, particularly in low-resource settings. In this work, we propose Constant Memory Attentive Neural Processes (CMANPs), an NP variant which only requires constant memory for the conditioning, querying, and updating phases. In building CMANPs, we propose Constant Memory Attention Block (CMAB), a novel general-purpose attention block that can compute its output in constant memory and perform updates in constant computation. Empirically, we show CMANPs achieve state-of-the-art results on meta-regression an
&lt;/p&gt;</description></item><item><title>GiPH&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;gpNet&#21644;GNN&#23398;&#20064;&#19968;&#20010;&#25918;&#32622;&#31574;&#30053;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#21160;&#24577;&#35774;&#22791;&#32676;&#38598;&#65292;&#24182;&#22312;&#21160;&#24577;&#35774;&#22791;&#38598;&#32676;&#19979;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.14562</link><description>&lt;p&gt;
GiPH: &#36890;&#29992;&#30340;&#33258;&#36866;&#24212;&#24322;&#26500;&#35745;&#31639;&#25918;&#32622;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GiPH: Generalizable Placement Learning for Adaptive Heterogeneous Computing. (arXiv:2305.14562v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14562
&lt;/p&gt;
&lt;p&gt;
GiPH&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;gpNet&#21644;GNN&#23398;&#20064;&#19968;&#20010;&#25918;&#32622;&#31574;&#30053;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#21160;&#24577;&#35774;&#22791;&#32676;&#38598;&#65292;&#24182;&#22312;&#21160;&#24577;&#35774;&#22791;&#38598;&#32676;&#19979;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30446;&#26631;&#35774;&#22791;&#32676;&#38598;&#20013;&#20180;&#32454;&#25918;&#32622;&#35745;&#31639;&#24212;&#29992;&#31243;&#24207;&#23545;&#20110;&#23454;&#29616;&#20302;&#24212;&#29992;&#23436;&#25104;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20010;&#38382;&#39064;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#26159;NP&#38590;&#30340;&#21644;&#32452;&#21512;&#24615;&#30340;&#12290;&#36817;&#24180;&#26469;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#21487;&#20197;&#24212;&#29992;&#20110;&#26032;&#24212;&#29992;&#31243;&#24207;&#30340;&#25918;&#32622;&#31574;&#30053;&#65292;&#36825;&#21463;&#21040;&#23558;&#31070;&#32463;&#32593;&#32476;&#25918;&#32622;&#22312;&#20113;&#26381;&#21153;&#22120;&#19978;&#30340;&#38382;&#39064;&#30340;&#21551;&#21457;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#35774;&#22791;&#32676;&#38598;&#26159;&#22266;&#23450;&#30340;&#65292;&#22312;&#31227;&#21160;&#25110;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#20013;&#65292;&#24322;&#26500;&#35774;&#22791;&#36827;&#20837;&#21644;&#31163;&#24320;&#29305;&#23450;&#24212;&#29992;&#31243;&#24207;&#30340;&#33539;&#22260;&#26159;&#19981;&#30830;&#23450;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GiPH&#30340;&#26032;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;1&#65289;&#19968;&#31181;&#31216;&#20026;gpNet&#30340;&#26032;&#22411;&#22270;&#34920;&#24449;&#65292;&#26377;&#25928;&#22320;&#32534;&#30721;&#36873;&#25321;&#33391;&#22909;&#25918;&#32622;&#25152;&#38656;&#30340;&#20449;&#24687;&#65292;&#20197;&#21450;2&#65289;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#23398;&#20064;gpNet&#20449;&#24687;&#30340;&#27719;&#24635;&#65292;&#20174;&#32780;&#23398;&#20064;&#21487;&#20197;&#25512;&#24191;&#21040;&#21160;&#24577;&#35774;&#22791;&#32676;&#38598;&#30340;&#31574;&#30053;&#12290;GiPH&#23558;&#25918;&#32622;&#38382;&#39064;&#36716;&#21270;&#20026;&#22270;&#25628;&#32034;&#38382;&#39064;&#65292;&#20854;&#20013;GNN&#39044;&#27979;&#21487;&#33021;&#25918;&#32622;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GiPH&#22312;&#20855;&#26377;&#21160;&#24577;&#35774;&#22791;&#32676;&#38598;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#24182;&#19988;&#21487;&#20197;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#35774;&#22791;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Careful placement of a computational application within a target device cluster is critical for achieving low application completion time. The problem is challenging due to its NP-hardness and combinatorial nature. In recent years, learning-based approaches have been proposed to learn a placement policy that can be applied to unseen applications, motivated by the problem of placing a neural network across cloud servers. These approaches, however, generally assume the device cluster is fixed, which is not the case in mobile or edge computing settings, where heterogeneous devices move in and out of range for a particular application. We propose a new learning approach called GiPH, which learns policies that generalize to dynamic device clusters via 1) a novel graph representation gpNet that efficiently encodes the information needed for choosing a good placement, and 2) a scalable graph neural network (GNN) that learns a summary of the gpNet information. GiPH turns the placement problem 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#36127;&#21453;&#39304;&#26426;&#21046;&#26469;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2305.14561</link><description>&lt;p&gt;
&#36127;&#21453;&#39304;&#35757;&#32451;&#65306;&#25552;&#39640;NVCiM DNN&#21152;&#36895;&#22120;&#40065;&#26834;&#24615;&#30340;&#26032;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Negative Feedback Training: A Novel Concept to Improve Robustness of NVCiM DNN Accelerators. (arXiv:2305.14561v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#36127;&#21453;&#39304;&#26426;&#21046;&#26469;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#38750;&#25381;&#21457;&#24615;&#23384;&#20648;&#22120;(NVM)&#23454;&#29616;&#30340;&#20869;&#23384;&#35745;&#31639;(CiM)&#20026;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290; CiM&#21152;&#36895;&#22120;&#36890;&#36807;&#22312;&#21516;&#19968;&#30005;&#36335;&#26495;&#32467;&#26500;&#20013;&#23384;&#20648;&#32593;&#32476;&#26435;&#37325;&#21644;&#25191;&#34892;&#30697;&#38453;&#25805;&#20316;&#65292;&#20197;&#26368;&#23567;&#30340;&#38754;&#31215;&#38656;&#27714;&#21644;&#24322;&#24120;&#30340;&#33021;&#25928;&#65292;&#25552;&#20379;DNN&#25512;&#29702;&#21152;&#36895;&#12290;&#28982;&#32780;&#65292;NVM&#35774;&#22791;&#30340;&#38543;&#26426;&#24615;&#21644;&#20869;&#22312;&#21464;&#21270;&#24448;&#24448;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#65292;&#22914;&#19982;&#39044;&#26399;&#32467;&#26524;&#30456;&#27604;&#20943;&#23569;&#20998;&#31867;&#31934;&#24230;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#20943;&#36731;&#35774;&#22791;&#21464;&#24322;&#24182;&#22686;&#24378;&#40065;&#26834;&#24615;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#25972;&#20307;&#35843;&#33410;&#24182;&#32570;&#20047;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#38480;&#21046;&#12290;&#21463;&#21040;&#36127;&#21453;&#39304;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#20986;&#21475;&#26426;&#21046;&#20316;&#20026;&#36127;&#21453;&#39304;&#65292;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compute-in-Memory (CiM) utilizing non-volatile memory (NVM) devices presents a highly promising and efficient approach for accelerating deep neural networks (DNNs). By concurrently storing network weights and performing matrix operations within the same crossbar structure, CiM accelerators offer DNN inference acceleration with minimal area requirements and exceptional energy efficiency. However, the stochasticity and intrinsic variations of NVM devices often lead to performance degradation, such as reduced classification accuracy, compared to expected outcomes. Although several methods have been proposed to mitigate device variation and enhance robustness, most of them rely on overall modulation and lack constraints on the training process. Drawing inspiration from the negative feedback mechanism, we introduce a novel training approach that uses a multi-exit mechanism as negative feedback to enhance the performance of DNN models in the presence of device variation. Our negative feedbac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;Transformer&#27169;&#22411;&#34920;&#31034;&#31354;&#38388;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21452;&#23556;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;BERT-INN&#65292;&#26469;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#21452;&#23556;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.14555</link><description>&lt;p&gt;
&#25152;&#26377;&#36947;&#36335;&#36890;&#24448;&#32599;&#39532;&#65311;&#25506;&#31350;Transformer&#34920;&#31034;&#30340;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
All Roads Lead to Rome? Exploring the Invariance of Transformers' Representations. (arXiv:2305.14555v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;Transformer&#27169;&#22411;&#34920;&#31034;&#31354;&#38388;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21452;&#23556;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;BERT-INN&#65292;&#26469;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#21452;&#23556;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#24102;&#26469;&#20102;&#26497;&#22823;&#30340;&#36827;&#23637;&#65292;&#22240;&#27492;&#24341;&#21457;&#20102;&#23545;&#27169;&#22411;&#23398;&#20064;&#30340;&#34920;&#31034;&#30340;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#34920;&#31034;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25506;&#31350;Transformer&#26159;&#21542;&#23398;&#20064;&#21040;&#20102;&#26412;&#36136;&#19978;&#21516;&#26500;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#25110;&#32773;&#36825;&#20123;&#34920;&#31034;&#31354;&#38388;&#26159;&#21542;&#23545;&#20854;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#31181;&#23376;&#25935;&#24863;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#23556;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;BERT-INN&#65292;&#26469;&#27604;&#20854;&#20182;&#29616;&#26377;&#21452;&#23556;&#26041;&#27861;&#65288;&#22914;&#35268;&#33539;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#65289;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#21452;&#23556;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#21644;&#22823;&#37327;&#23454;&#39564;&#23637;&#31034;&#20102;BERT-INN&#30340;&#20248;&#21183;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#23545;&#40784;&#37325;&#29616;&#30340;BERT&#23884;&#20837;&#65292;&#20197;&#33719;&#24471;&#23545;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#26377;&#24847;&#20041;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#38142;&#25509;&#22312;&#25991;&#31456;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models bring propelling advances in various NLP tasks, thus inducing lots of interpretability research on the learned representations of the models. However, we raise a fundamental question regarding the reliability of the representations. Specifically, we investigate whether transformers learn essentially isomorphic representation spaces, or those that are sensitive to the random seeds in their pretraining process. In this work, we formulate the Bijection Hypothesis, which suggests the use of bijective methods to align different models' representation spaces. We propose a model based on invertible neural networks, BERT-INN, to learn the bijection more effectively than other existing bijective methods such as the canonical correlation analysis (CCA). We show the advantage of BERT-INN both theoretically and through extensive experiments, and apply it to align the reproduced BERT embeddings to draw insights that are meaningful to the interpretability research. Our code is at 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GANSpace&#26041;&#27861;&#20013;&#28508;&#31354;&#38388;&#30340;&#35821;&#20041;&#21464;&#21270;&#65292;&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#20013;&#21457;&#29616;&#65292;ICA&#26041;&#27861;&#30456;&#27604;PCA&#26041;&#27861;&#26356;&#36866;&#21512;&#25805;&#20316;&#36136;&#37327;&#21644;&#35299;&#32544;&#65292;&#24182;&#19988;&#26080;&#35770;GAN&#30340;&#22823;&#23567;&#22914;&#20309;&#65292;&#20854;&#25511;&#21046;&#26041;&#21521;&#37117;&#26159;&#22522;&#26412;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.14551</link><description>&lt;p&gt;
&#36890;&#36807;&#30697;&#38453;&#20998;&#35299;&#25506;&#32034;GAN&#28508;&#31354;&#38388;&#20013;&#30340;&#35821;&#20041;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Exploring Semantic Variations in GAN Latent Spaces via Matrix Factorization. (arXiv:2305.14551v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GANSpace&#26041;&#27861;&#20013;&#28508;&#31354;&#38388;&#30340;&#35821;&#20041;&#21464;&#21270;&#65292;&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#20013;&#21457;&#29616;&#65292;ICA&#26041;&#27861;&#30456;&#27604;PCA&#26041;&#27861;&#26356;&#36866;&#21512;&#25805;&#20316;&#36136;&#37327;&#21644;&#35299;&#32544;&#65292;&#24182;&#19988;&#26080;&#35770;GAN&#30340;&#22823;&#23567;&#22914;&#20309;&#65292;&#20854;&#25511;&#21046;&#26041;&#21521;&#37117;&#26159;&#22522;&#26412;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23427;&#20204;&#30340;&#38750;&#32447;&#24615;&#21644;&#39640;&#32500;&#24230;&#65292;&#20351;&#29992;GAN&#36827;&#34892;&#21463;&#25511;&#25968;&#25454;&#29983;&#25104;&#26159;&#20196;&#20154;&#28212;&#26395;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#23637;&#31034;&#20102;GANSpace&#25152;&#23398;&#20064;&#21040;&#30340;&#22270;&#20687;&#25805;&#20316;&#65306;&#65288;a&#65289;GANSpace&#29983;&#25104;&#20102;&#19968;&#31995;&#21015;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#25805;&#20316;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#39640;&#24230;&#32416;&#32544;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#28508;&#22312;&#30340;&#29992;&#36884;&#65307;&#65288;b&#65289;&#29992;ICA&#26367;&#25442;PCA&#21487;&#20197;&#25913;&#21892;&#25805;&#20316;&#30340;&#36136;&#37327;&#21644;&#35299;&#32544;&#65307;&#65288;c&#65289;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#21487;&#33021;&#23545;GAN&#30340;&#22823;&#23567;&#25935;&#24863;&#65292;&#20294;&#26080;&#35770;&#23427;&#20204;&#30340;&#22797;&#26434;&#24615;&#22914;&#20309;&#65292;&#22312;&#23427;&#20204;&#30340;&#28508;&#31354;&#38388;&#20013;&#37117;&#21487;&#20197;&#35266;&#23519;&#21040;&#22522;&#26412;&#30340;&#25511;&#21046;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controlled data generation with GANs is desirable but challenging due to the nonlinearity and high dimensionality of their latent spaces. In this work, we explore image manipulations learned by GANSpace, a state-of-the-art method based on PCA. Through quantitative and qualitative assessments we show: (a) GANSpace produces a wide range of high-quality image manipulations, but they can be highly entangled, limiting potential use cases; (b) Replacing PCA with ICA improves the quality and disentanglement of manipulations; (c) The quality of the generated images can be sensitive to the size of GANs, but regardless of their complexity, fundamental controlling directions can be observed in their latent spaces.
&lt;/p&gt;</description></item><item><title>&#24207;&#21015;&#24314;&#27169;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#27604;Q-Learning&#21644;Imitation Learning&#26356;&#36866;&#21512;&#22312;&#31232;&#30095;&#22870;&#21169;&#21644;&#20302;&#36136;&#37327;&#25968;&#25454;&#35774;&#32622;&#19979;&#30340;&#36873;&#25321;&#65292;&#22312;&#20219;&#21153;&#33539;&#22260;&#22686;&#21152;&#26102;&#65292;&#24207;&#21015;&#24314;&#27169;&#21644;&#27169;&#20223;&#23398;&#20064;&#26356;&#21487;&#21462;&#12290;</title><link>http://arxiv.org/abs/2305.14550</link><description>&lt;p&gt;
&#24207;&#21015;&#24314;&#27169;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#31454;&#20105;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence Modeling is a Robust Contender for Offline Reinforcement Learning. (arXiv:2305.14550v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14550
&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#24314;&#27169;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#27604;Q-Learning&#21644;Imitation Learning&#26356;&#36866;&#21512;&#22312;&#31232;&#30095;&#22870;&#21169;&#21644;&#20302;&#36136;&#37327;&#25968;&#25454;&#35774;&#32622;&#19979;&#30340;&#36873;&#25321;&#65292;&#22312;&#20219;&#21153;&#33539;&#22260;&#22686;&#21152;&#26102;&#65292;&#24207;&#21015;&#24314;&#27169;&#21644;&#27169;&#20223;&#23398;&#20064;&#26356;&#21487;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20351;&#20195;&#29702;&#33021;&#22815;&#20174;&#38745;&#24577;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#26368;&#22823;&#21270;&#25910;&#30410;&#31574;&#30053;&#12290;&#31163;&#32447;RL&#30340;&#19977;&#22823;&#33539;&#24335;&#26159;Q-Learning&#12289;Imitation Learning&#21644;Sequence Modeling&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#38382;&#39064;&#26159;&#65306;&#22312;&#20160;&#20040;&#26465;&#20214;&#19979;&#65292;&#21738;&#31181;&#33539;&#24335;&#34987;&#20248;&#20808;&#36873;&#25321;&#65311;&#25105;&#20204;&#36890;&#36807;&#25506;&#32034;&#20195;&#34920;&#24615;&#31639;&#27861;&#8212;&#8212;&#20445;&#23432;Q-Learning(CQL)&#12289;&#34892;&#20026;&#20811;&#38534; (BC)&#21644;&#20915;&#31574;Transformer (DT)&#8212;&#8212;&#22312;&#24120;&#29992;&#30340;D4RL&#21644;Robomimic&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#26469;&#23545;&#36825;&#20010;&#38382;&#39064;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#26377;&#38024;&#23545;&#24615;&#30340;&#23454;&#39564;&#26469;&#29702;&#35299;&#23427;&#20204;&#22312;&#25968;&#25454;&#23376;&#20248;&#24615;&#21644;&#20219;&#21153;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65306;(1)&#24207;&#21015;&#24314;&#27169;&#38656;&#35201;&#27604;Q-Learning&#26356;&#22810;&#30340;&#25968;&#25454;&#26469;&#23398;&#20064;&#31454;&#20105;&#24615;&#31574;&#30053;&#65292;&#20294;&#26356;&#21152;&#31283;&#20581;&#65307;(2)&#24207;&#21015;&#24314;&#27169;&#22312;&#31232;&#30095;&#22870;&#21169;&#21644;&#20302;&#36136;&#37327;&#25968;&#25454;&#35774;&#32622;&#20013;&#27604;Q-Learning&#21644;Imitation Learning&#37117;&#35201;&#22909;&#24471;&#22810;&#65307;(3)&#38543;&#30528;&#20219;&#21153;&#33539;&#22260;&#30340;&#22686;&#21152;&#65292;&#24207;&#21015;&#24314;&#27169;&#21644;&#27169;&#20223;&#23398;&#20064;&#26356;&#21487;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) allows agents to learn effective, return-maximizing policies from a static dataset. Three major paradigms for offline RL are Q-Learning, Imitation Learning, and Sequence Modeling. A key open question is: which paradigm is preferred under what conditions? We study this question empirically by exploring the performance of representative algorithms -- Conservative Q-Learning (CQL), Behavior Cloning (BC), and Decision Transformer (DT) -- across the commonly used D4RL and Robomimic benchmarks. We design targeted experiments to understand their behavior concerning data suboptimality and task complexity. Our key findings are: (1) Sequence Modeling requires more data than Q-Learning to learn competitive policies but is more robust; (2) Sequence Modeling is a substantially better choice than both Q-Learning and Imitation Learning in sparse-reward and low-quality data settings; and (3) Sequence Modeling and Imitation Learning are preferable as task horizon inc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20307;&#25928;&#24212;&#24320;&#20851;&#24518;&#38459;&#22120;&#30340;&#20869;&#23384;&#35745;&#31639;&#27169;&#22359;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#23454;&#29616;&#20102;&#19968;&#31181;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#26041;&#26696;&#65292;&#20351;&#29992;&#20302;&#31934;&#24230;&#20869;&#23384;&#35745;&#31639;&#65288;CIM&#65289;&#27169;&#22359;&#21152;&#36895;&#26114;&#36149;&#30340;&#21521;&#37327;-&#30697;&#38453;&#20056;&#27861;&#65288;VMM&#65289;&#25805;&#20316;&#65292;&#24182;&#22312;&#25968;&#23383;&#21333;&#20803;&#20013;&#31215;&#32047;&#39640;&#31934;&#24230;&#30340;&#26435;&#37325;&#26356;&#26032;&#65292;&#36890;&#21270;&#32047;&#35745;&#30340;&#26435;&#37325;&#26356;&#26032;&#20540;&#36229;&#36807;&#38408;&#20540;&#26102;&#65292;&#25165;&#26356;&#26032;&#24518;&#38459;&#22120;&#35774;&#22791;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;CIM&#27169;&#22359;&#30456;&#23545;&#20110;&#20256;&#32479;&#25968;&#23383;&#23454;&#29616;&#30340;&#21152;&#36895;&#21644;&#25928;&#29575;&#21487;&#36798;&#21040;3&#20493;&#21644;2.5&#20493;&#65292;&#24182;&#19988;&#19982;&#20840;&#31934;&#24230;&#25968;&#23383;&#35757;&#32451;&#30456;&#27604;&#65292;&#26435;&#37325;&#26356;&#26032;&#31934;&#24230;&#26368;&#39640;&#21487;&#36798;94%&#12290;</title><link>http://arxiv.org/abs/2305.14547</link><description>&lt;p&gt;
&#22522;&#20110;&#20307;&#25928;&#24212;&#24320;&#20851;&#24518;&#38459;&#22120;&#30340;&#20869;&#23384;&#35745;&#31639;&#27169;&#22359;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Bulk-Switching Memristor-based Compute-In-Memory Module for Deep Neural Network Training. (arXiv:2305.14547v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20307;&#25928;&#24212;&#24320;&#20851;&#24518;&#38459;&#22120;&#30340;&#20869;&#23384;&#35745;&#31639;&#27169;&#22359;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#23454;&#29616;&#20102;&#19968;&#31181;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#26041;&#26696;&#65292;&#20351;&#29992;&#20302;&#31934;&#24230;&#20869;&#23384;&#35745;&#31639;&#65288;CIM&#65289;&#27169;&#22359;&#21152;&#36895;&#26114;&#36149;&#30340;&#21521;&#37327;-&#30697;&#38453;&#20056;&#27861;&#65288;VMM&#65289;&#25805;&#20316;&#65292;&#24182;&#22312;&#25968;&#23383;&#21333;&#20803;&#20013;&#31215;&#32047;&#39640;&#31934;&#24230;&#30340;&#26435;&#37325;&#26356;&#26032;&#65292;&#36890;&#21270;&#32047;&#35745;&#30340;&#26435;&#37325;&#26356;&#26032;&#20540;&#36229;&#36807;&#38408;&#20540;&#26102;&#65292;&#25165;&#26356;&#26032;&#24518;&#38459;&#22120;&#35774;&#22791;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;CIM&#27169;&#22359;&#30456;&#23545;&#20110;&#20256;&#32479;&#25968;&#23383;&#23454;&#29616;&#30340;&#21152;&#36895;&#21644;&#25928;&#29575;&#21487;&#36798;&#21040;3&#20493;&#21644;2.5&#20493;&#65292;&#24182;&#19988;&#19982;&#20840;&#31934;&#24230;&#25968;&#23383;&#35757;&#32451;&#30456;&#27604;&#65292;&#26435;&#37325;&#26356;&#26032;&#31934;&#24230;&#26368;&#39640;&#21487;&#36798;94%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#26356;&#39640;&#24615;&#33021;&#21644;&#26356;&#22909;&#30340;&#21151;&#33021;&#65292;&#20294;&#27169;&#22411;&#35757;&#32451;&#38656;&#35201;&#39640;&#24378;&#24230;&#30340;&#35745;&#31639;&#26102;&#38388;&#21644;&#33021;&#37327;&#12290;&#22522;&#20110;&#24518;&#38459;&#22120;&#30340;&#20869;&#23384;&#35745;&#31639;&#65288;CIM&#65289;&#27169;&#22359;&#21487;&#20197;&#22312;&#21407;&#22320;&#21644;&#24182;&#34892;&#25191;&#34892;&#21521;&#37327;-&#30697;&#38453;&#20056;&#27861;&#65288;VMM&#65289;&#65292;&#22312;DNN&#25512;&#29702;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#26497;&#22823;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;CIM&#30340;&#27169;&#22411;&#35757;&#32451;&#38754;&#20020;&#30528;&#38750;&#32447;&#24615;&#26435;&#37325;&#26356;&#26032;&#12289;&#22120;&#20214;&#21464;&#21270;&#21644;&#27169;&#25311;&#35745;&#31639;&#30005;&#36335;&#20302;&#31934;&#24230;&#31561;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#20307;&#25928;&#24212;&#24320;&#20851;&#24518;&#38459;&#22120;&#30340;CIM&#27169;&#22359;&#23454;&#29616;&#20102;&#19968;&#31181;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#26041;&#26696;&#20197;&#32531;&#35299;&#36825;&#20123;&#24433;&#21709;&#12290;&#20302;&#31934;&#24230;CIM&#27169;&#22359;&#29992;&#20110;&#21152;&#36895;&#26114;&#36149;&#30340;VMM&#25805;&#20316;&#65292;&#32780;&#39640;&#31934;&#24230;&#26435;&#37325;&#26356;&#26032;&#22312;&#25968;&#23383;&#21333;&#20803;&#20013;&#32047;&#31215;&#12290;&#24403;&#32047;&#31215;&#30340;&#26435;&#37325;&#26356;&#26032;&#20540;&#36229;&#36807;&#39044;&#23450;&#20041;&#30340;&#38408;&#20540;&#26102;&#25165;&#26356;&#25913;&#24518;&#38459;&#22120;&#35774;&#22791;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#37319;&#29992;SoC&#35774;&#35745;&#23454;&#29616;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#25968;&#23383;&#23454;&#29616;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;CIM&#27169;&#22359;&#23454;&#29616;&#20102;&#39640;&#36798;3&#20493;&#30340;&#21152;&#36895;&#21644;2.5&#20493;&#30340;&#33021;&#37327;&#25928;&#29575;&#65292;&#24182;&#19988;&#19982;&#20840;&#31934;&#24230;&#25968;&#23383;&#35757;&#32451;&#30456;&#27604;&#65292;&#26368;&#39640;&#21487;&#36798;94%&#30340;&#26435;&#37325;&#26356;&#26032;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The need for deep neural network (DNN) models with higher performance and better functionality leads to the proliferation of very large models. Model training, however, requires intensive computation time and energy. Memristor-based compute-in-memory (CIM) modules can perform vector-matrix multiplication (VMM) in situ and in parallel, and have shown great promises in DNN inference applications. However, CIM-based model training faces challenges due to non-linear weight updates, device variations, and low-precision in analog computing circuits. In this work, we experimentally implement a mixed-precision training scheme to mitigate these effects using a bulk-switching memristor CIM module. Lowprecision CIM modules are used to accelerate the expensive VMM operations, with high precision weight updates accumulated in digital units. Memristor devices are only changed when the accumulated weight update value exceeds a pre-defined threshold. The proposed scheme is implemented with a system-on
&lt;/p&gt;</description></item><item><title>Whisper&#27169;&#22411;&#26159;&#19968;&#20010;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#35821;&#38899;&#20219;&#21153;&#12290;&#26412;&#31687;&#25991;&#31456;&#25506;&#35752;&#20102;Whisper&#34920;&#31034;&#22312;&#20854;&#20182;&#22235;&#31181;&#35821;&#38899;&#20219;&#21153;&#21644;&#8220;&#37326;&#22806;&#8221;&#20219;&#21153;&#20013;&#30340;&#21487;&#36801;&#31227;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Whisper&#20855;&#26377;&#36328;&#20219;&#21153;&#30340;&#30495;&#23454;&#29615;&#22659;&#37096;&#32626;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14546</link><description>&lt;p&gt;
&#22522;&#20110;Whisper&#30340;&#34920;&#31034;&#22312;&#21508;&#31181;&#20132;&#21449;&#20219;&#21153;&#19979;&#30340;&#21487;&#36801;&#31227;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Transferability of Whisper-based Representations for "In-the-Wild" Cross-Task Downstream Speech Applications. (arXiv:2305.14546v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14546
&lt;/p&gt;
&lt;p&gt;
Whisper&#27169;&#22411;&#26159;&#19968;&#20010;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#35821;&#38899;&#20219;&#21153;&#12290;&#26412;&#31687;&#25991;&#31456;&#25506;&#35752;&#20102;Whisper&#34920;&#31034;&#22312;&#20854;&#20182;&#22235;&#31181;&#35821;&#38899;&#20219;&#21153;&#21644;&#8220;&#37326;&#22806;&#8221;&#20219;&#21153;&#20013;&#30340;&#21487;&#36801;&#31227;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Whisper&#20855;&#26377;&#36328;&#20219;&#21153;&#30340;&#30495;&#23454;&#29615;&#22659;&#37096;&#32626;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#27169;&#22411;&#22312;&#21508;&#31181;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#23548;&#33268;&#20102;&#21487;&#29992;&#20110;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#30340;&#36890;&#29992;&#35821;&#38899;&#34920;&#31034;&#65292;&#20174;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#21040;&#35828;&#35805;&#20154;&#35782;&#21035;&#31561;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;Whisper&#65292;&#36890;&#36807;&#22823;&#37327;&#30340;&#24369;&#30417;&#30563;&#25968;&#25454;&#36827;&#34892;&#20102;ASR&#30340;&#35757;&#32451;&#65307; &#23427;&#20248;&#20110;&#20960;&#31181;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#12290;&#37492;&#20110;Whisper&#22312;ASR&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#65292;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35813;&#34920;&#31034;&#22312;SUPERB&#22522;&#20934;&#27979;&#35797;&#20013;&#20854;&#20182;&#22235;&#20010;&#35821;&#38899;&#20219;&#21153;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;Whisper&#34920;&#31034;&#22312;&#34987;&#29615;&#22659;&#22122;&#22768;&#21644;&#25151;&#38388;&#28151;&#21709;&#30772;&#22351;&#30340;&#8220;&#37326;&#22806;&#8221;&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Whisper&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#29615;&#22659;&#26465;&#20214;&#19979;&#37117;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#22240;&#27492;&#23637;&#31034;&#20102;&#22312;&#36328;&#20219;&#21153;&#30340;&#30495;&#23454;&#29615;&#22659;&#37096;&#32626;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large self-supervised pre-trained speech models have achieved remarkable success across various speech-processing tasks. The self-supervised training of these models leads to universal speech representations that can be used for different downstream tasks, ranging from automatic speech recognition (ASR) to speaker identification. Recently, Whisper, a transformer-based model was proposed and trained on large amount of weakly supervised data for ASR; it outperformed several state-of-the-art self-supervised models. Given the superiority of Whisper for ASR, in this paper we explore the transferability of the representation for four other speech tasks in SUPERB benchmark. Moreover, we explore the robustness of Whisper representation for ``in the wild'' tasks where speech is corrupted by environment noise and room reverberation. Experimental results show Whisper achieves promising results across tasks and environmental conditions, thus showing potential for cross-task real-world deployment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;DF2M&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#39640;&#32500;&#20989;&#25968;&#26102;&#38388;&#24207;&#21015;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#21360;&#24230;&#33258;&#21161;&#39184;&#36807;&#31243;&#21644;&#28145;&#24230;&#26680;&#20989;&#25968;&#30340;&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#25429;&#25417;&#26102;&#38388;&#21160;&#24577;&#65292;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;DF2M&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21331;&#36234;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14543</link><description>&lt;p&gt;
DF2M&#65306;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#29992;&#20110;&#39640;&#32500;&#20989;&#25968;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#28145;&#24230;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DF2M: An Explainable Deep Bayesian Nonparametric Model for High-Dimensional Functional Time Series. (arXiv:2305.14543v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;DF2M&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#39640;&#32500;&#20989;&#25968;&#26102;&#38388;&#24207;&#21015;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#21360;&#24230;&#33258;&#21161;&#39184;&#36807;&#31243;&#21644;&#28145;&#24230;&#26680;&#20989;&#25968;&#30340;&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#25429;&#25417;&#26102;&#38388;&#21160;&#24577;&#65292;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;DF2M&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21331;&#36234;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;Deep Functional Factor Model(DF2M)&#65292;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#39640;&#32500;&#20989;&#25968;&#26102;&#38388;&#24207;&#21015;&#30340;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#27169;&#22411;&#12290;DF2M&#21033;&#29992;&#21360;&#24230;&#33258;&#21161;&#39184;&#36807;&#31243;&#21644;&#28145;&#24230;&#26680;&#20989;&#25968;&#30340;&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#26469;&#25429;&#25417;&#38750;&#39532;&#23572;&#31185;&#22827;&#21644;&#38750;&#32447;&#24615;&#26102;&#38388;&#21160;&#24577;&#12290;&#19982;&#35768;&#22810;&#40657;&#21283;&#23376;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19981;&#21516;&#65292;DF2M&#36890;&#36807;&#26500;&#24314;&#22240;&#23376;&#27169;&#22411;&#24182;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34701;&#20837;&#26680;&#20989;&#25968;&#20013;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#21464;&#20998;&#25512;&#29702;&#31639;&#27861;&#26469;&#25512;&#26029;DF2M&#12290;&#22235;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;DF2M&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21331;&#36234;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present Deep Functional Factor Model (DF2M), a Bayesian nonparametric model for analyzing high-dimensional functional time series. The DF2M makes use of the Indian Buffet Process and the multi-task Gaussian Process with a deep kernel function to capture non-Markovian and nonlinear temporal dynamics. Unlike many black-box deep learning models, the DF2M provides an explainable way to use neural networks by constructing a factor model and incorporating deep neural networks within the kernel function. Additionally, we develop a computationally efficient variational inference algorithm for inferring the DF2M. Empirical results from four real-world datasets demonstrate that the DF2M offers better explainability and superior predictive accuracy compared to conventional deep learning models for high-dimensional functional time series.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36991;&#20813;&#36807;&#28388;&#27668;&#27873;&#12289;&#26356;&#20844;&#24179;&#22320;&#20998;&#25285;&#22810;&#26679;&#21270;&#36127;&#25285;&#30340;&#31038;&#20132;&#32593;&#32476;&#20869;&#23481;&#31574;&#21010;&#21644;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.14537</link><description>&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#26497;&#31471;&#21270;&#38450;&#33539;
&lt;/p&gt;
&lt;p&gt;
Disincentivizing Polarization in Social Networks. (arXiv:2305.14537v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14537
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36991;&#20813;&#36807;&#28388;&#27668;&#27873;&#12289;&#26356;&#20844;&#24179;&#22320;&#20998;&#25285;&#22810;&#26679;&#21270;&#36127;&#25285;&#30340;&#31038;&#20132;&#32593;&#32476;&#20869;&#23481;&#31574;&#21010;&#21644;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#32593;&#32476;&#19978;&#65292;&#31639;&#27861;&#20010;&#24615;&#21270;&#23558;&#29992;&#25143;&#24102;&#20837;&#20102;&#36807;&#28388;&#27668;&#27873;&#20013;&#65292;&#24456;&#23569;&#30475;&#21040;&#20559;&#31163;&#20182;&#20204;&#20852;&#36259;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36991;&#20813;&#36807;&#28388;&#27668;&#27873;&#30340;&#20869;&#23481;&#31574;&#21010;&#21644;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#20197;&#21450;&#31639;&#27861;&#20445;&#35777;&#21644;&#20960;&#20046;&#21305;&#37197;&#30340;&#19979;&#38480;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#24179;&#21488;&#22312;$T$&#26102;&#38388;&#27493;&#38271;&#20869;&#19982;$n$&#29992;&#25143;&#36827;&#34892;&#20114;&#21160;&#65292;&#20174;$k$&#31867;&#21035;&#20026;&#27599;&#20010;&#29992;&#25143;&#36873;&#25321;&#20869;&#23481;&#12290;&#20687;&#22810;&#33218;&#36172;&#21338;&#26426;&#19968;&#26679;&#65292;&#24179;&#21488;&#25509;&#25910;&#38543;&#26426;&#22870;&#21169;&#12290;&#20026;&#20102;&#36991;&#20813;&#36807;&#28388;&#27668;&#27873;&#65292;&#25105;&#20204;&#20381;&#36182;&#20110;&#36825;&#26679;&#19968;&#20010;&#30452;&#35273;&#65306;&#22914;&#26524;&#26576;&#20123;&#29992;&#25143;&#30475;&#21040;&#26576;&#20123;&#31867;&#21035;&#30340;&#20869;&#23481;&#65292;&#21017;&#25152;&#26377;&#29992;&#25143;&#37117;&#24212;&#35813;&#33267;&#23569;&#30475;&#21040;&#19968;&#23567;&#37096;&#20998;&#35813;&#20869;&#23481;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#36825;&#31181;&#30452;&#35273;&#30340;&#19968;&#20010;&#22825;&#30495;&#30340;&#24418;&#24335;&#21270;&#65292;&#35777;&#26126;&#23427;&#20855;&#26377;&#24847;&#24819;&#19981;&#21040;&#30340;&#21518;&#26524;&#65306;&#23427;&#23548;&#33268;"&#22810;&#25968;&#20154;&#30340;&#26292;&#25919;"&#65292;&#20351;&#23569;&#25968;&#20852;&#36259;&#30340;&#20154;&#20998;&#25285;&#20102;&#22810;&#26679;&#21270;&#30340;&#36127;&#25285;&#12290;&#36825;&#23548;&#33268;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#26356;&#20844;&#24179;&#22320;&#20998;&#25285;&#20102;&#36825;&#31181;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;
On social networks, algorithmic personalization drives users into filter bubbles where they rarely see content that deviates from their interests. We present a model for content curation and personalization that avoids filter bubbles, along with algorithmic guarantees and nearly matching lower bounds. In our model, the platform interacts with $n$ users over $T$ timesteps, choosing content for each user from $k$ categories. The platform receives stochastic rewards as in a multi-arm bandit. To avoid filter bubbles, we draw on the intuition that if some users are shown some category of content, then all users should see at least a small amount of that content. We first analyze a naive formalization of this intuition and show it has unintended consequences: it leads to ``tyranny of the majority'' with the burden of diversification borne disproportionately by those with minority interests. This leads us to our model which distributes this burden more equitably. We require that the probabili
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31526;&#21512;&#24615;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;CF-GNN&#65289;&#65292;&#36890;&#36807;&#23558;&#31526;&#21512;&#24615;&#39044;&#27979;&#65288;CP&#65289;&#25193;&#23637;&#21040;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#20013;&#65292;&#23545;GNN&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20102;&#26377;&#25928;&#20272;&#35745;&#12290;CF-GNN&#29983;&#25104;&#30340;&#39044;&#27979;&#38598;/&#21306;&#38388;&#21487;&#26681;&#25454;&#39044;&#23450;&#20041;&#30340;&#35206;&#30422;&#27010;&#29575;&#20445;&#35777;&#21253;&#21547;&#30495;&#23454;&#26631;&#31614;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#19968;&#31181;&#20943;&#23569;&#39044;&#27979;&#38598;&#22823;&#23567;/&#21306;&#38388;&#38271;&#24230;&#30340;&#25299;&#25169;&#24847;&#35782;&#36755;&#20986;&#26657;&#27491;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.14535</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#31526;&#21512;&#24615;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#22270;&#19978;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification over Graph with Conformalized Graph Neural Networks. (arXiv:2305.14535v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31526;&#21512;&#24615;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;CF-GNN&#65289;&#65292;&#36890;&#36807;&#23558;&#31526;&#21512;&#24615;&#39044;&#27979;&#65288;CP&#65289;&#25193;&#23637;&#21040;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#20013;&#65292;&#23545;GNN&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20102;&#26377;&#25928;&#20272;&#35745;&#12290;CF-GNN&#29983;&#25104;&#30340;&#39044;&#27979;&#38598;/&#21306;&#38388;&#21487;&#26681;&#25454;&#39044;&#23450;&#20041;&#30340;&#35206;&#30422;&#27010;&#29575;&#20445;&#35777;&#21253;&#21547;&#30495;&#23454;&#26631;&#31614;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#19968;&#31181;&#20943;&#23569;&#39044;&#27979;&#38598;&#22823;&#23567;/&#21306;&#38388;&#38271;&#24230;&#30340;&#25299;&#25169;&#24847;&#35782;&#36755;&#20986;&#26657;&#27491;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#29992;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;GNN&#32570;&#20047;&#20005;&#26684;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38169;&#35823;&#25104;&#26412;&#26174;&#33879;&#30340;&#29615;&#22659;&#20013;&#30340;&#21487;&#38752;&#37096;&#32626;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21512;&#24615;GNN&#65288;CF-GNN&#65289;&#65292;&#23558;&#31526;&#21512;&#24615;&#39044;&#27979;&#65288;CP&#65289;&#25193;&#23637;&#21040;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#20013;&#65292;&#20197;&#33719;&#24471;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#32473;&#23450;&#22270;&#20013;&#30340;&#23454;&#20307;&#65292;CF-GNN&#29983;&#25104;&#19968;&#20010;&#39044;&#27979;&#38598;/&#21306;&#38388;&#65292;&#20197;&#20808;&#39564;&#35206;&#30422;&#27010;&#29575;&#65288;&#20363;&#22914;90%&#65289;&#30340;&#26041;&#24335;&#20445;&#35777;&#21253;&#21547;&#30495;&#23454;&#26631;&#31614;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#25490;&#21015;&#19981;&#21464;&#26465;&#20214;&#65292;&#20351;&#24471;CP&#22312;&#22270;&#25968;&#25454;&#19978;&#25104;&#31435;&#65292;&#24182;&#25552;&#20379;&#20102;&#27979;&#35797;&#26102;&#38388;&#35206;&#30422;&#29575;&#30340;&#31934;&#30830;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#38500;&#20102;&#26377;&#25928;&#30340;&#35206;&#30422;&#65292;&#20943;&#23569;&#39044;&#27979;&#38598;&#22823;&#23567;/&#21306;&#38388;&#38271;&#24230;&#23545;&#20110;&#23454;&#38469;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#21457;&#29616;&#38750;&#31526;&#21512;&#24615;&#24471;&#20998;&#21644;&#32593;&#32476;&#32467;&#26500;&#20043;&#38388;&#23384;&#22312;&#20851;&#38190;&#32852;&#31995;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#24320;&#21457;&#20855;&#26377;&#25299;&#25169;&#24847;&#35782;&#30340;&#36755;&#20986;&#26657;&#27491;&#27169;&#22411;&#26469;&#23398;&#20064;&#26356;&#26032;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are powerful machine learning prediction models on graph-structured data. However, GNNs lack rigorous uncertainty estimates, limiting their reliable deployment in settings where the cost of errors is significant. We propose conformalized GNN (CF-GNN), extending conformal prediction (CP) to graph-based models for guaranteed uncertainty estimates. Given an entity in the graph, CF-GNN produces a prediction set/interval that provably contains the true label with pre-defined coverage probability (e.g. 90%). We establish a permutation invariance condition that enables the validity of CP on graph data and provide an exact characterization of the test-time coverage. Moreover, besides valid coverage, it is crucial to reduce the prediction set size/interval length for practical use. We observe a key connection between non-conformity scores and network structures, which motivates us to develop a topology-aware output correction model that learns to update the predicti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#28151;&#21512;&#26469;&#28040;&#38500;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#65292;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#26032;&#26679;&#26412;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#32463;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#22810;&#31181;&#20219;&#21153;&#23454;&#39564;&#39564;&#35777;&#65292;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14521</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#28151;&#21512;&#28040;&#38500;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Eliminating Spurious Correlations from Pre-trained Models via Data Mixing. (arXiv:2305.14521v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#28151;&#21512;&#26469;&#28040;&#38500;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#65292;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#26032;&#26679;&#26412;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#32463;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#22810;&#31181;&#20219;&#21153;&#23454;&#39564;&#39564;&#35777;&#65292;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25910;&#25947;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#24448;&#24448;&#21033;&#29992;&#20102;&#26576;&#20123;&#23646;&#24615;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#22312;&#29305;&#23450;&#31867;&#21035;&#30340;&#22823;&#22810;&#25968;&#31034;&#20363;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20294;&#24182;&#19981;&#36275;&#20197;&#39044;&#27979;&#36825;&#20123;&#31867;&#21035;&#12290;&#23398;&#21040;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#21487;&#33021;&#20250;&#22312;&#23545;&#26032;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#21518;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#20250;&#38477;&#20302;&#27169;&#22411;&#23545;&#19981;&#23637;&#29616;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#31034;&#20363;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#20197;&#28040;&#38500;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#19968;&#23567;&#32452;&#24102;&#26377;&#34394;&#20551;&#23646;&#24615;&#30340;&#31034;&#20363;&#65292;&#24182;&#36890;&#36807;&#25968;&#25454;&#28151;&#21512;&#26469;&#24179;&#34913;&#25152;&#26377;&#31867;&#21035;&#20013;&#30340;&#34394;&#20551;&#23646;&#24615;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;NLP&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#65292;&#21253;&#25324;&#28040;&#38500;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models pre-trained on large datasets have achieved remarkable convergence and robustness properties. However, these models often exploit spurious correlations between certain attributes and labels, which are prevalent in the majority of examples within specific categories but are not predictive of these categories in general. The learned spurious correlations may persist even after fine-tuning on new data, which degrades models' performance on examples that do not exhibit the spurious correlation. In this work, we propose a simple and highly effective method to eliminate spurious correlations from pre-trained models. The key idea of our method is to leverage a small set of examples with spurious attributes, and balance the spurious attributes across all classes via data mixing. We theoretically confirm the effectiveness of our method, and empirically demonstrate its state-of-the-art performance on various vision and NLP tasks, including eliminating spurious correlation
&lt;/p&gt;</description></item><item><title>CongFu&#26159;&#19968;&#31181;&#29992;&#20110;&#33647;&#29289;&#21327;&#21516;&#39044;&#27979;&#30340;&#26032;&#22411;&#26465;&#20214;&#22270;&#34701;&#21512;&#23618;&#65292;&#37319;&#29992;&#27880;&#24847;&#26426;&#21046;&#21644;&#29942;&#39048;&#25216;&#26415;&#25552;&#21462;&#26412;&#22320;&#22270;&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#20840;&#23616;&#19978;&#19979;&#25991;&#20013;&#26377;&#26465;&#20214;&#22320;&#34701;&#21512;&#22270;&#25968;&#25454;&#12290;&#20854;&#27169;&#22359;&#21270;&#26550;&#26500;&#20351;&#24471;&#21487;&#20197;&#26356;&#25442;&#22270;&#23618;&#27169;&#22359;&#65292;&#21253;&#25324;&#35835;&#20986;&#21644;&#22270;&#32534;&#30721;&#22120;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#22312;12&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;11&#20010;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20854;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14517</link><description>&lt;p&gt;
CongFu: &#29992;&#20110;&#33647;&#29289;&#21327;&#21516;&#39044;&#27979;&#30340;&#26465;&#20214;&#22270;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
CongFu: Conditional Graph Fusion for Drug Synergy Prediction. (arXiv:2305.14517v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14517
&lt;/p&gt;
&lt;p&gt;
CongFu&#26159;&#19968;&#31181;&#29992;&#20110;&#33647;&#29289;&#21327;&#21516;&#39044;&#27979;&#30340;&#26032;&#22411;&#26465;&#20214;&#22270;&#34701;&#21512;&#23618;&#65292;&#37319;&#29992;&#27880;&#24847;&#26426;&#21046;&#21644;&#29942;&#39048;&#25216;&#26415;&#25552;&#21462;&#26412;&#22320;&#22270;&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#20840;&#23616;&#19978;&#19979;&#25991;&#20013;&#26377;&#26465;&#20214;&#22320;&#34701;&#21512;&#22270;&#25968;&#25454;&#12290;&#20854;&#27169;&#22359;&#21270;&#26550;&#26500;&#20351;&#24471;&#21487;&#20197;&#26356;&#25442;&#22270;&#23618;&#27169;&#22359;&#65292;&#21253;&#25324;&#35835;&#20986;&#21644;&#22270;&#32534;&#30721;&#22120;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#22312;12&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;11&#20010;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#21327;&#21516;&#26159;&#25351;&#22810;&#31181;&#33647;&#29289;&#32852;&#21512;&#20316;&#29992;&#25152;&#20135;&#29983;&#30340;&#21512;&#25104;&#25928;&#24212;&#65292;&#23545;&#20110;&#20248;&#21270;&#27835;&#30103;&#32467;&#26524;&#38750;&#24120;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#21487;&#33021;&#30340;&#33647;&#29289;&#32452;&#21512;&#25968;&#37327;&#24040;&#22823;&#65292;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#23548;&#33268;&#33647;&#29289;&#21327;&#21516;&#25968;&#25454;&#26377;&#38480;&#65292;&#38656;&#35201;&#39044;&#27979;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#22270;&#34701;&#21512;&#23618;&#8212;&#8212;CongFu&#65292;&#29992;&#20110;&#39044;&#27979;&#33647;&#29289;&#21327;&#21516;&#20316;&#29992;&#12290;CongFu&#37319;&#29992;&#27880;&#24847;&#26426;&#21046;&#21644;&#29942;&#39048;&#25216;&#26415;&#26469;&#25552;&#21462;&#26412;&#22320;&#22270;&#19978;&#19979;&#25991;&#24182;&#22312;&#20840;&#23616;&#19978;&#19979;&#25991;&#20013;&#26377;&#26465;&#20214;&#22320;&#34701;&#21512;&#22270;&#25968;&#25454;&#12290;&#20854;&#27169;&#22359;&#21270;&#26550;&#26500;&#20351;&#24471;&#21487;&#20197;&#26356;&#25442;&#22270;&#23618;&#27169;&#22359;&#65292;&#21253;&#25324;&#35835;&#20986;&#21644;&#22270;&#32534;&#30721;&#22120;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#20026;&#20102;&#35780;&#20272;CongFu&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#23545;&#22235;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#33647;&#29289;&#21327;&#21516;&#39044;&#27979;&#35774;&#32622;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;CongFu&#22312;12&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;11&#20010;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drug synergy, characterized by the amplified combined effect of multiple drugs, presents a critical phenomenon for optimizing therapeutic outcomes. However, limited data on drug synergy, arising from the vast number of possible drug combinations and computational costs, motivate the need for predictive methods. In this work, we introduce CongFu, a novel Conditional Graph Fusion Layer, designed to predict drug synergy. CongFu employs an attention mechanism and a bottleneck to extract local graph contexts and conditionally fuse graph data within a global context. Its modular architecture enables flexible replacement of layer modules, including readouts and graph encoders, facilitating customization for diverse applications. To evaluate the performance of CongFu, we conduct comprehensive experiments on four datasets, encompassing three distinct setups for drug synergy prediction. Remarkably, CongFu achieves state-of-the-art results on 11 out of 12 benchmark datasets, demonstrating its abi
&lt;/p&gt;</description></item><item><title>Chakra&#26159;&#19968;&#31181;&#24320;&#25918;&#30340;&#22270;&#24418;&#27169;&#24335;&#65292;&#29992;&#20110;&#26631;&#20934;&#21270;&#24037;&#20316;&#36127;&#36733;&#35268;&#33539;&#65292;&#25429;&#25417;&#20851;&#38190;&#25805;&#20316;&#21644;&#20381;&#36182;&#39033;&#65292;&#20197;&#25512;&#36827;&#24615;&#33021;&#22522;&#20934;&#21644;&#21327;&#21516;&#35774;&#35745;&#65292;&#21516;&#26102;&#25552;&#20379;&#19968;&#32452;&#24037;&#20855;&#21644;&#33021;&#21147;&#65292;&#20197;&#20415;&#22312;&#27169;&#25311;&#22120;&#21644;&#20223;&#30495;&#22120;&#20013;&#23454;&#29616;&#26410;&#26469;&#31995;&#32479;&#30340;&#21327;&#21516;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.14516</link><description>&lt;p&gt;
Chakra: &#21033;&#29992;&#26631;&#20934;&#21270;&#25191;&#34892;&#36319;&#36394;&#25512;&#36827;&#24615;&#33021;&#22522;&#20934;&#21644;&#21327;&#21516;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Chakra: Advancing Performance Benchmarking and Co-design using Standardized Execution Traces. (arXiv:2305.14516v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14516
&lt;/p&gt;
&lt;p&gt;
Chakra&#26159;&#19968;&#31181;&#24320;&#25918;&#30340;&#22270;&#24418;&#27169;&#24335;&#65292;&#29992;&#20110;&#26631;&#20934;&#21270;&#24037;&#20316;&#36127;&#36733;&#35268;&#33539;&#65292;&#25429;&#25417;&#20851;&#38190;&#25805;&#20316;&#21644;&#20381;&#36182;&#39033;&#65292;&#20197;&#25512;&#36827;&#24615;&#33021;&#22522;&#20934;&#21644;&#21327;&#21516;&#35774;&#35745;&#65292;&#21516;&#26102;&#25552;&#20379;&#19968;&#32452;&#24037;&#20855;&#21644;&#33021;&#21147;&#65292;&#20197;&#20415;&#22312;&#27169;&#25311;&#22120;&#21644;&#20223;&#30495;&#22120;&#20013;&#23454;&#29616;&#26410;&#26469;&#31995;&#32479;&#30340;&#21327;&#21516;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20934;&#27979;&#35797;&#21644;&#21327;&#21516;&#35774;&#35745;&#23545;&#20110;&#25512;&#21160;ML&#27169;&#22411;&#12289;ML&#36719;&#20214;&#21644;&#19979;&#19968;&#20195;&#30828;&#20214;&#30340;&#20248;&#21270;&#21644;&#21019;&#26032;&#33267;&#20851;&#37325;&#35201;&#12290;&#23436;&#25972;&#30340;&#24037;&#20316;&#36127;&#36733;&#22522;&#20934;&#27979;&#35797;&#65292;&#20363;&#22914;MLPerf&#65292;&#22312;&#31995;&#32479;&#23436;&#20840;&#35774;&#35745;&#21644;&#37096;&#32626;&#21518;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#36719;&#20214;&#21644;&#30828;&#20214;&#22534;&#26632;&#20043;&#38388;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#65292;&#21457;&#25381;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#21019;&#26032;&#30340;&#36895;&#24230;&#35201;&#27714;&#37319;&#29992;&#26356;&#25935;&#25463;&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#21644;&#20351;&#29992;&#22522;&#20934;&#27979;&#35797;&#65292;&#20511;&#21161;&#27169;&#25311;&#22120;&#21644;&#20223;&#30495;&#22120;&#36827;&#34892;&#26410;&#26469;&#31995;&#32479;&#21327;&#21516;&#35774;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Chakra&#65292;&#19968;&#31181;&#24320;&#25918;&#30340;&#22270;&#24418;&#27169;&#24335;&#65292;&#29992;&#20110;&#26631;&#20934;&#21270;&#24037;&#20316;&#36127;&#36733;&#35268;&#33539;&#65292;&#25429;&#25417;&#20851;&#38190;&#25805;&#20316;&#21644;&#20381;&#36182;&#39033;&#65292;&#21363;&#25191;&#34892;&#36319;&#36394;&#65288;ET&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#20114;&#34917;&#30340;&#24037;&#20855;/&#33021;&#21147;&#65292;&#20197;&#20351;&#21508;&#31181;&#27169;&#25311;&#22120;&#12289;&#20223;&#30495;&#22120;&#21644;&#22522;&#20934;&#27979;&#35797;&#21487;&#20197;&#25910;&#38598;&#12289;&#29983;&#25104;&#21644;&#37319;&#29992;Chakra ET&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#20351;&#29992;&#29983;&#25104;AI&#27169;&#22411;&#23398;&#20064;&#20102;&#25968;&#21315;&#20010;Chakra ET&#30340;&#28508;&#22312;&#32479;&#35745;&#23646;&#24615;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#26469;&#32508;&#21512;Chakra ET&#12290;
&lt;/p&gt;
&lt;p&gt;
Benchmarking and co-design are essential for driving optimizations and innovation around ML models, ML software, and next-generation hardware. Full workload benchmarks, e.g. MLPerf, play an essential role in enabling fair comparison across different software and hardware stacks especially once systems are fully designed and deployed. However, the pace of AI innovation demands a more agile methodology to benchmark creation and usage by simulators and emulators for future system co-design. We propose Chakra, an open graph schema for standardizing workload specification capturing key operations and dependencies, also known as Execution Trace (ET). In addition, we propose a complementary set of tools/capabilities to enable collection, generation, and adoption of Chakra ETs by a wide range of simulators, emulators, and benchmarks. For instance, we use generative AI models to learn latent statistical properties across thousands of Chakra ETs and use these models to synthesize Chakra ETs. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; RetICL &#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#22320;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#27169;&#22411;&#30340;&#31034;&#20363;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#23558;&#24207;&#21015;&#31034;&#20363;&#36873;&#25321;&#38382;&#39064;&#20316;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#19988;&#20248;&#21270;&#36873;&#25321;&#20026;&#20351;&#20219;&#21153;&#34920;&#29616;&#26368;&#20339;&#30340;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.14502</link><description>&lt;p&gt;
&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30340;&#39034;&#24207;&#26816;&#32034;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;RetICL
&lt;/p&gt;
&lt;p&gt;
RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning. (arXiv:2305.14502v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; RetICL &#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#22320;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#27169;&#22411;&#30340;&#31034;&#20363;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#23558;&#24207;&#21015;&#31034;&#20363;&#36873;&#25321;&#38382;&#39064;&#20316;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#19988;&#20248;&#21270;&#36873;&#25321;&#20026;&#20351;&#20219;&#21153;&#34920;&#29616;&#26368;&#20339;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#20013;&#30340;&#35768;&#22810;&#21457;&#23637;&#37117;&#38598;&#20013;&#22312;&#20419;&#20351;&#23427;&#20204;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#25552;&#31034;&#26041;&#27861;&#26159;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20854;&#20013;&#27169;&#22411;&#22312;&#32473;&#23450;&#19968;&#20010;&#65288;&#25110;&#22810;&#20010;&#65289;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#65288;&#21487;&#33021;&#26159;&#26032;&#30340;&#65289;&#29983;&#25104;/&#39044;&#27979;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#31034;&#20363;&#30340;&#36873;&#25321;&#21487;&#33021;&#23545;&#20219;&#21153;&#30340;&#34920;&#29616;&#20135;&#29983;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#25214;&#21040;&#22909;&#30340;&#31034;&#20363;&#24182;&#19981;&#26159;&#31616;&#21333;&#30340;&#65292;&#22240;&#20026;&#20195;&#34920;&#24615;&#31034;&#20363;&#32452;&#30340;&#23450;&#20041;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#30340;&#19981;&#21516;&#32780;&#22823;&#19981;&#30456;&#21516;&#12290;&#34429;&#28982;&#23384;&#22312;&#35768;&#22810;&#36873;&#25321;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#29420;&#31435;&#22320;&#23545;&#31034;&#20363;&#36827;&#34892;&#35780;&#20998;&#65292;&#24573;&#30053;&#23427;&#20204;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#31034;&#20363;&#30340;&#39034;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#26041;&#27861;&#8212;&#8212;In-Context Learning&#30340;&#26816;&#32034;RetICL&#65292;&#29992;&#20110;&#24314;&#27169;&#21644;&#36880;&#27493;&#36873;&#25321;&#19978;&#19979;&#25991;&#31034;&#20363;&#12290;&#25105;&#20204;&#25226;&#39034;&#24207;&#31034;&#20363;&#36873;&#25321;&#30340;&#38382;&#39064;&#20316;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent developments in large language models focus on prompting them to perform specific tasks. One effective prompting method is in-context learning, where the model performs a (possibly new) generation/prediction task given one (or more) examples. Past work has shown that the choice of examples can make a large impact on task performance. However, finding good examples is not straightforward since the definition of a representative group of examples can vary greatly depending on the task. While there are many existing methods for selecting in-context examples, they generally score examples independently, ignoring the dependency between them and the order in which they are provided to the large language model. In this work, we propose Retrieval for In-Context Learning (RetICL), a learnable method for modeling and optimally selecting examples sequentially for in-context learning. We frame the problem of sequential example selection as a Markov decision process, design an example r
&lt;/p&gt;</description></item><item><title>Point2SSM&#21487;&#20197;&#30452;&#25509;&#20174;&#28857;&#20113;&#20013;&#26500;&#24314;&#20986;&#35299;&#21078;&#32479;&#35745;&#24418;&#24577;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#26080;&#22122;&#22768;&#34920;&#38754;&#32593;&#26684;&#25110;&#20108;&#36827;&#21046;&#20307;&#31215;&#65292;&#20381;&#36182;&#20110;&#20551;&#35774;&#25110;&#39044;&#23450;&#20041;&#27169;&#26495;&#65292;&#20197;&#21450;&#21516;&#26102;&#20248;&#21270;&#23548;&#33268;&#38271;&#26102;&#38388;&#25512;&#26029;&#26032;&#25968;&#25454;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14486</link><description>&lt;p&gt;
Point2SSM&#65306;&#20174;&#28857;&#20113;&#23398;&#20064;&#35299;&#21078;&#32467;&#26500;&#30340;&#24418;&#24577;&#21464;&#24322;
&lt;/p&gt;
&lt;p&gt;
Point2SSM: Learning Morphological Variations of Anatomies from Point Cloud. (arXiv:2305.14486v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14486
&lt;/p&gt;
&lt;p&gt;
Point2SSM&#21487;&#20197;&#30452;&#25509;&#20174;&#28857;&#20113;&#20013;&#26500;&#24314;&#20986;&#35299;&#21078;&#32479;&#35745;&#24418;&#24577;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#26080;&#22122;&#22768;&#34920;&#38754;&#32593;&#26684;&#25110;&#20108;&#36827;&#21046;&#20307;&#31215;&#65292;&#20381;&#36182;&#20110;&#20551;&#35774;&#25110;&#39044;&#23450;&#20041;&#27169;&#26495;&#65292;&#20197;&#21450;&#21516;&#26102;&#20248;&#21270;&#23548;&#33268;&#38271;&#26102;&#38388;&#25512;&#26029;&#26032;&#25968;&#25454;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Point2SSM&#65292;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#28857;&#20113;&#31934;&#30830;&#22320;&#26500;&#24314;&#22522;&#20110;&#23545;&#24212;&#20851;&#31995;&#30340;&#35299;&#21078;&#32479;&#35745;&#24418;&#24577;&#27169;&#22411;&#65288;SSM&#65289;&#12290; SSM&#22312;&#20020;&#24202;&#30740;&#31350;&#20013;&#38750;&#24120;&#20851;&#38190;&#65292;&#29992;&#20110;&#20998;&#26512;&#39592;&#39612;&#21644;&#22120;&#23448;&#20013;&#30340;&#32676;&#20307;&#27700;&#24179;&#24418;&#24577;&#21464;&#24322;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;SSM&#21019;&#24314;&#26041;&#27861;&#20855;&#26377;&#19968;&#23450;&#23616;&#38480;&#24615;&#65292;&#22914;&#38656;&#35201;&#26080;&#22122;&#22768;&#34920;&#38754;&#32593;&#26684;&#25110;&#20108;&#36827;&#21046;&#20307;&#31215;&#65292;&#20381;&#36182;&#20110;&#20551;&#35774;&#25110;&#39044;&#23450;&#20041;&#27169;&#26495;&#65292;&#20197;&#21450;&#38024;&#23545;&#25972;&#20010;&#38431;&#21015;&#30340;&#21516;&#26102;&#20248;&#21270;&#23548;&#33268;&#38271;&#26102;&#38388;&#25512;&#26029;&#26032;&#25968;&#25454;&#12290;Point2SSM&#36890;&#36807;&#25552;&#20379;&#25968;&#25454;&#39537;&#21160;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#20174;&#21407;&#22987;&#28857;&#20113;&#20013;&#30452;&#25509;&#25512;&#26029;SSM&#65292;&#20943;&#23569;&#20102;&#25512;&#26029;&#36127;&#25285;&#65292;&#24182;&#25552;&#39640;&#20102;&#36866;&#29992;&#24615;&#65292;&#22240;&#20026;&#28857;&#20113;&#26356;&#23481;&#26131;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Point2SSM, a novel unsupervised learning approach that can accurately construct correspondence-based statistical shape models (SSMs) of anatomy directly from point clouds. SSMs are crucial in clinical research for analyzing the population-level morphological variation in bones and organs. However, traditional methods for creating SSMs have limitations that hinder their widespread adoption, such as the need for noise-free surface meshes or binary volumes, reliance on assumptions or predefined templates, and simultaneous optimization of the entire cohort leading to lengthy inference times given new data. Point2SSM overcomes these barriers by providing a data-driven solution that infers SSMs directly from raw point clouds, reducing inference burdens and increasing applicability as point clouds are more easily acquired. Deep learning on 3D point clouds has seen recent success in unsupervised representation learning, point-to-point matching, and shape correspondence; however, t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#30693;&#35782;&#22270;&#35889;&#26597;&#35810;&#30340;&#30740;&#31350;&#36827;&#23637;&#21644;&#26368;&#26032;&#25216;&#26415;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.14485</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs Querying. (arXiv:2305.14485v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14485
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#30693;&#35782;&#22270;&#35889;&#26597;&#35810;&#30340;&#30740;&#31350;&#36827;&#23637;&#21644;&#26368;&#26032;&#25216;&#26415;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#22914;DBpedia&#12289;Freebase&#12289;YAGO&#12289;Wikidata&#21644;NELL&#31561;&#34987;&#26500;&#24314;&#29992;&#26469;&#23384;&#20648;&#22823;&#35268;&#27169;&#12289;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20107;&#23454;&#65288;&#20027;&#39064;&#12289;&#35859;&#35821;&#12289;&#23545;&#35937;&#65289;&#19977;&#20803;&#32452;&#65292;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#19968;&#20010;&#22270;&#24418;&#65292;&#20854;&#20013;&#19968;&#20010;&#33410;&#28857;&#65288;&#20027;&#39064;&#25110;&#23545;&#35937;&#65289;&#20195;&#34920;&#20855;&#26377;&#23646;&#24615;&#30340;&#23454;&#20307;&#65292;&#24182;&#19988;&#26377;&#21521;&#36793;&#65288;&#35859;&#35789;&#65289;&#26159;&#20004;&#20010;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;Web&#25628;&#32034;&#12289;&#38382;&#31572;&#12289;&#35821;&#20041;&#25628;&#32034;&#12289;&#20010;&#20154;&#21161;&#25163;&#12289;&#20107;&#23454;&#26816;&#26597;&#21644;&#25512;&#33616;&#20013;&#65292;&#26597;&#35810; KG &#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#23613;&#31649;&#22312; KG &#26500;&#24314;&#21644;&#32500;&#25252;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#26368;&#36817;&#30475;&#21040;&#20102;KG &#26597;&#35810;&#21644;&#38382;&#31572;&#30740;&#31350;&#26041;&#38754;&#30340;&#28608;&#22686;&#12290;&#25105;&#20204;&#35843;&#26597;&#30340;&#30446;&#30340;&#26159;&#21452;&#37325;&#30340;&#12290;&#39318;&#20808;&#65292;KG&#26597;&#35810;&#30340;&#30740;&#31350;&#30001;&#22810;&#20010;&#31038;&#32676;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#22914;&#25968;&#25454;&#24211;&#12289;&#25968;&#25454;&#25366;&#25496;&#12289;&#35821;&#20041;&#32593;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#65292;&#20851;&#27880;&#28857;&#21644;&#26415;&#35821;&#19981;&#21516;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#26368;&#36817;&#30340;KG&#26597;&#35810;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#26368;&#26032;&#25216;&#26415;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) such as DBpedia, Freebase, YAGO, Wikidata, and NELL were constructed to store large-scale, real-world facts as (subject, predicate, object) triples -- that can also be modeled as a graph, where a node (a subject or an object) represents an entity with attributes, and a directed edge (a predicate) is a relationship between two entities. Querying KGs is critical in web search, question answering (QA), semantic search, personal assistants, fact checking, and recommendation. While significant progress has been made on KG construction and curation, thanks to deep learning recently we have seen a surge of research on KG querying and QA. The objectives of our survey are two-fold. First, research on KG querying has been conducted by several communities, such as databases, data mining, semantic web, machine learning, information retrieval, and natural language processing (NLP), with different focus and terminologies; and also in diverse topics ranging from graph databases
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861; SIRLC&#65292;&#21487;&#20197;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26377;&#25928;&#22320;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#22806;&#37096;&#26631;&#31614;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;NLP&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.14483</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21453;&#24605;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#25552;&#21319;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Language Model Self-improvement by Reinforcement Learning Contemplation. (arXiv:2305.14483v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861; SIRLC&#65292;&#21487;&#20197;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26377;&#25928;&#22320;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#22806;&#37096;&#26631;&#31614;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;NLP&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#24120;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#30417;&#30563;&#26469;&#33719;&#21462;&#65292;&#36825;&#26679;&#24456;&#26174;&#28982;&#26159;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#35328;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#21453;&#24605;&#33258;&#25105;&#25552;&#21319;&#65288;SIRLC&#65289;&#30340;&#26032;&#22411;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#22806;&#37096;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#25913;&#21892;LLMs&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#35266;&#23519;&#32467;&#26524;&#65306;&#35821;&#35328;&#27169;&#22411;&#27604;&#29983;&#25104;&#25991;&#26412;&#26356;&#23481;&#26131;&#35780;&#20272;&#25991;&#26412;&#36136;&#37327;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;SIRLC&#32473;LLMs&#20998;&#37197;&#20102;&#21452;&#37325;&#35282;&#33394;&#65292;&#19968;&#26041;&#38754;&#20316;&#20026;&#23398;&#29983;&#29983;&#25104;&#26080;&#26631;&#31614;&#38382;&#39064;&#30340;&#31572;&#26696;&#65292;&#21478;&#19968;&#26041;&#38754;&#20316;&#20026;&#25945;&#24072;&#35780;&#20272;&#29983;&#25104;&#30340;&#25991;&#26412;&#24182;&#25454;&#27492;&#32473;&#20986;&#20998;&#25968;&#12290;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#20197;&#26368;&#22823;&#21270;&#35780;&#20272;&#20998;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SIRLC&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;NLP&#20219;&#21153;&#65292;&#20363;&#22914;&#25512;&#29702;&#38382;&#39064;&#12289;&#25991;&#26412;&#20998;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have exhibited remarkable performance across various natural language processing (NLP) tasks. However, fine-tuning these models often necessitates substantial supervision, which can be expensive and time-consuming to obtain. This paper introduces a novel unsupervised method called LanguageModel Self-Improvement by Reinforcement Learning Contemplation (SIRLC) that improves LLMs without reliance on external labels. Our approach is grounded in the observation that it is simpler for language models to assess text quality than to generate text. Building on this insight, SIRLC assigns LLMs dual roles as both student and teacher. As a student, the LLM generates answers to unlabeled questions, while as a teacher, it evaluates the generated text and assigns scores accordingly. The model parameters are updated using reinforcement learning to maximize the evaluation score. We demonstrate that SIRLC can be applied to various NLP tasks, such as reasoning problems, text 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22359;&#22352;&#26631;&#22810;&#23618;&#27425;&#20248;&#21270;&#26041;&#27861;&#30340;&#35299;&#20915;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20351;&#29992;PINNs&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#38382;&#39064;&#65292;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.14477</link><description>&lt;p&gt;
&#22359;&#22352;&#26631;&#22810;&#23618;&#27425;&#20248;&#21270;&#26041;&#27861;&#21450;&#20854;&#22312;&#22522;&#20110;&#29289;&#29702;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Block-Coordinate Approach of Multi-level Optimization with an Application to Physics-Informed Neural Networks. (arXiv:2305.14477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14477
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22359;&#22352;&#26631;&#22810;&#23618;&#27425;&#20248;&#21270;&#26041;&#27861;&#30340;&#35299;&#20915;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20351;&#29992;PINNs&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#38382;&#39064;&#65292;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23618;&#27425;&#26041;&#27861;&#24191;&#27867;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#35745;&#31639;&#20248;&#21183;&#24182;&#21033;&#29992;&#20102;&#28041;&#21450;&#23376;&#38382;&#39064;&#20043;&#38388;&#30340;&#20114;&#34917;&#24615;&#12290;&#22312;&#23558;&#22810;&#23618;&#27425;&#26041;&#27861;&#20174;&#22359;&#22352;&#26631;&#30340;&#35270;&#35282;&#37325;&#26032;&#35299;&#37322;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#30340;&#22810;&#23618;&#27425;&#31639;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#35780;&#20272;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#20351;&#29992;&#29289;&#29702;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#20960;&#20010;&#27979;&#35797;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26356;&#22909;&#35299;&#20915;&#26041;&#26696;&#21644;&#26174;&#30528;&#30340;&#35745;&#31639;&#33410;&#30465;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-level methods are widely used for the solution of large-scale problems, because of their computational advantages and exploitation of the complementarity between the involved sub-problems. After a re-interpretation of multi-level methods from a block-coordinate point of view, we propose a multi-level algorithm for the solution of nonlinear optimization problems and analyze its evaluation complexity. We apply it to the solution of partial differential equations using physics-informed neural networks (PINNs) and show on a few test problems that the approach results in better solutions and significant computational savings
&lt;/p&gt;</description></item><item><title>FLAIR#2&#25968;&#25454;&#38598;&#23558;&#36229;&#39640;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#21333;&#26102;&#30456;&#33322;&#31354;&#22270;&#20687;&#21644;Copernicus Sentinel-2&#21355;&#26143;&#22270;&#20687;&#30340;&#26102;&#38388;&#21644;&#20809;&#35889;&#20449;&#24687;&#36827;&#34892;&#34701;&#21512;&#65292;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#65292;&#20197;&#25552;&#39640;&#25105;&#20204;&#23545;&#39046;&#22303;&#30340;&#35748;&#35782;&#21644;&#25512;&#21160;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2305.14467</link><description>&lt;p&gt;
FLAIR #2&#65306;&#26469;&#33258;&#22810;&#28304;&#20809;&#23398;&#22270;&#20687;&#30340;&#32441;&#29702;&#21644;&#26102;&#38388;&#20449;&#24687;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
FLAIR #2: textural and temporal information for semantic segmentation from multi-source optical imagery. (arXiv:2305.14467v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14467
&lt;/p&gt;
&lt;p&gt;
FLAIR#2&#25968;&#25454;&#38598;&#23558;&#36229;&#39640;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#21333;&#26102;&#30456;&#33322;&#31354;&#22270;&#20687;&#21644;Copernicus Sentinel-2&#21355;&#26143;&#22270;&#20687;&#30340;&#26102;&#38388;&#21644;&#20809;&#35889;&#20449;&#24687;&#36827;&#34892;&#34701;&#21512;&#65292;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#65292;&#20197;&#25552;&#39640;&#25105;&#20204;&#23545;&#39046;&#22303;&#30340;&#35748;&#35782;&#21644;&#25512;&#21160;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
FLAIR#2&#25968;&#25454;&#38598;&#21253;&#25324;&#20004;&#31181;&#38750;&#24120;&#19981;&#21516;&#30340;&#25968;&#25454;&#31867;&#22411;&#65292;&#36825;&#20123;&#25968;&#25454;&#31867;&#22411;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#30446;&#30340;&#26159;&#32472;&#21046;&#22303;&#22320;&#35206;&#30422;&#22270;&#12290;&#25968;&#25454;&#34701;&#21512;&#24037;&#20316;&#27969;&#31243;&#25552;&#20986;&#20102;&#21033;&#29992;&#36229;&#39640;&#31354;&#38388;&#20998;&#36776;&#29575;&#65288;VHR&#65289;&#21333;&#26102;&#30456;&#33322;&#31354;&#22270;&#20687;&#30340;&#33391;&#22909;&#31354;&#38388;&#21644;&#32441;&#29702;&#20449;&#24687;&#20197;&#21450;Copernicus Sentinel-2&#21355;&#26143;&#22270;&#20687;&#30340;&#26102;&#38388;&#21644;&#20809;&#35889;&#20016;&#23500;&#24615;&#12290;&#38543;&#30528;&#39640;&#36136;&#37327;&#22320;&#29699;&#35266;&#27979;&#65288;EO&#65289;&#25968;&#25454;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#27861;&#22269;&#22320;&#29702;&#21644;&#26862;&#26519;&#20449;&#24687;&#22269;&#23478;&#30740;&#31350;&#25152;&#65288;IGN&#65289;&#27491;&#22312;&#31215;&#26497;&#25506;&#32034;&#23558;&#36825;&#20123;&#29305;&#24449;&#24322;&#36136;&#30340;&#25968;&#25454;&#19982;&#25968;&#25454;&#38598;&#25104;&#30340;&#21019;&#26032;&#31574;&#30053;&#12290;IGN&#22240;&#27492;&#25552;&#20379;&#20102;&#27492;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#21019;&#26032;&#24182;&#25552;&#39640;&#25105;&#20204;&#23545;&#39046;&#22303;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
The FLAIR #2 dataset hereby presented includes two very distinct types of data, which are exploited for a semantic segmentation task aimed at mapping land cover. The data fusion workflow proposes the exploitation of the fine spatial and textural information of very high spatial resolution (VHR) mono-temporal aerial imagery and the temporal and spectral richness of high spatial resolution (HR) time series of Copernicus Sentinel-2 satellite images. The French National Institute of Geographical and Forest Information (IGN), in response to the growing availability of high-quality Earth Observation (EO) data, is actively exploring innovative strategies to integrate these data with heterogeneous characteristics. IGN is therefore offering this dataset to promote innovation and improve our knowledge of our territories.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#20986;&#29616;&#30340;&#25991;&#21270;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#29616;&#35937;&#65292;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#31561;&#20843;&#20010;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#23545;&#20110;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2305.14456</link><description>&lt;p&gt;
&#22312;&#31048;&#31095;&#20043;&#21518;&#21917;&#21860;&#37202;&#65311;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Having Beer after Prayer? Measuring Cultural Bias in Large Language Models. (arXiv:2305.14456v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14456
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#20986;&#29616;&#30340;&#25991;&#21270;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#29616;&#35937;&#65292;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#31561;&#20843;&#20010;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#23545;&#20110;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23384;&#22312;&#25991;&#21270;&#20559;&#35265;&#65311;&#35821;&#35328;&#27169;&#22411;&#31526;&#21512;&#25152;&#26381;&#21153;&#31038;&#21306;&#30340;&#25991;&#21270;&#22240;&#32032;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#34920;&#26126;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#20559;&#35265;&#65292;&#20542;&#21521;&#20110;&#20135;&#29983;&#35199;&#26041;&#25991;&#21270;&#30456;&#20851;&#20869;&#23481;&#32780;&#38750;&#38463;&#25289;&#20271;&#25991;&#21270;&#30456;&#20851;&#20869;&#23481;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20174;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#19978;&#25910;&#38598;&#30340;&#33258;&#28982;&#20986;&#29616;&#30340;&#19978;&#19979;&#25991;&#21644;&#22522;&#20110;&#21487;&#33021;&#24615;&#35780;&#20998;&#30340;&#25351;&#26631;&#26469;&#37327;&#21270;&#36825;&#31181;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#38463;&#25289;&#20271;&#35821;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#35199;&#26041;&#25991;&#21270;&#20559;&#35265;&#65292;&#21253;&#25324;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#12290;&#24403;&#36755;&#20837;&#30340;&#38463;&#25289;&#20271;&#35821;&#21477;&#23376;&#36234;&#25509;&#36817;&#33521;&#35821;&#26102;&#65292;&#27169;&#22411;&#20063;&#26356;&#23481;&#26131;&#34920;&#29616;&#20986;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#20154;&#20204;&#23545;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#35774;&#35745;&#20013;&#24212;&#26356;&#22810;&#32771;&#34385;&#25991;&#21270;&#22240;&#32032;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are language models culturally biased? It is important that language models conform to the cultural aspects of the communities they serve. However, we show in this paper that language models suffer from a significant bias towards Western culture when handling and generating text in Arabic, often preferring, and producing Western-fitting content as opposed to the relevant Arab content. We quantify this bias through a likelihood scoring-based metric using naturally occurring contexts that we collect from online social media. Our experiments reveal that both Arabic monolingual and multilingual models exhibit bias towards Western culture in eight different cultural aspects: person names, food, clothing, location, literature, beverage, religion, and sports. Models also tend to exhibit more bias when prompted with Arabic sentences that are more linguistically aligned with English. These findings raise concerns about the cultural relevance of current language models. Our analyses show that pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#23545;DWP&#21464;&#20998;&#36817;&#20284;&#21518;&#39564;&#30340;&#25913;&#36827;&#31639;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2305.14454</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#21464;&#20998;&#36817;&#20284;&#21518;&#39564;&#29992;&#20110;&#28145;&#24230;Wishart&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
An Improved Variational Approximate Posterior for the Deep Wishart Process. (arXiv:2305.14454v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#23545;DWP&#21464;&#20998;&#36817;&#20284;&#21518;&#39564;&#30340;&#25913;&#36827;&#31639;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26680;&#36807;&#31243;&#26159;&#19968;&#31867;&#26368;&#36817;&#24341;&#20837;&#30340;&#28145;&#24230;&#36125;&#21494;&#26031;&#27169;&#22411;&#65292;&#20855;&#26377;&#31070;&#32463;&#32593;&#32476;&#30340;&#28789;&#27963;&#24615;&#65292;&#20294;&#23436;&#20840;&#20351;&#29992;Gram&#30697;&#38453;&#12290;&#23427;&#20204;&#36890;&#36807;&#20132;&#26367;&#20174;&#27491;&#21322;&#23450;&#30697;&#38453;&#20998;&#24067;&#20013;&#21462;&#26679;Gram&#30697;&#38453;&#24182;&#24212;&#29992;&#30830;&#23450;&#24615;&#36716;&#25442;&#26469;&#25805;&#20316;&#12290;&#24403;&#20998;&#24067;&#34987;&#36873;&#25321;&#20026;Wishart&#20998;&#24067;&#26102;&#65292;&#27169;&#22411;&#34987;&#31216;&#20026;&#28145;&#24230;Wishart&#36807;&#31243;(DWP)&#12290;&#36825;&#20010;&#29305;&#23450;&#30340;&#27169;&#22411;&#24456;&#26377;&#36259;&#65292;&#22240;&#20026;&#23427;&#30340;&#20808;&#39564;&#31561;&#20215;&#20110;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;(DGP)&#30340;&#20808;&#39564;&#65292;&#20294;&#21516;&#26102;&#23427;&#23545;&#26059;&#36716;&#23545;&#31216;&#24615;&#19981;&#21464;&#65292;&#23548;&#33268;&#21518;&#39564;&#20998;&#24067;&#26356;&#31616;&#21333;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#65288;&#8220;&#28145;&#24230;Wishart&#36807;&#31243;&#30340;&#21464;&#20998;&#21518;&#39564;&#36817;&#20284;&#8221;Ober and Aitchison 2021a&#65289;&#23454;&#29616;&#20102;&#23545;DWP&#30340;&#23454;&#38469;&#25512;&#26029;&#65292;&#20854;&#20013;&#20316;&#32773;&#20351;&#29992;Wishart&#20998;&#24067;&#30340;Bartlett&#20998;&#35299;&#30340;&#25512;&#24191;&#20316;&#20026;&#21464;&#20998;&#21518;&#39564;&#36817;&#20284;&#12290;&#28982;&#32780;&#65292;&#35813;&#35770;&#25991;&#20013;&#30340;&#39044;&#27979;&#24615;&#33021;&#19981;&#22914;&#20854;&#20182;&#23545;&#27604;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep kernel processes are a recently introduced class of deep Bayesian models that have the flexibility of neural networks, but work entirely with Gram matrices. They operate by alternately sampling a Gram matrix from a distribution over positive semi-definite matrices, and applying a deterministic transformation. When the distribution is chosen to be Wishart, the model is called a deep Wishart process (DWP). This particular model is of interest because its prior is equivalent to a deep Gaussian process (DGP) prior, but at the same time it is invariant to rotational symmetries, leading to a simpler posterior distribution. Practical inference in the DWP was made possible in recent work ("A variational approximate posterior for the deep Wishart process" Ober and Aitchison 2021a) where the authors used a generalisation of the Bartlett decomposition of the Wishart distribution as the variational approximate posterior. However, predictive performance in that paper was less impressive than o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#30340;&#20219;&#24847;&#20998;&#36776;&#29575;&#27668;&#20505;&#25968;&#25454;&#38477;&#23610;&#24230;&#26041;&#27861;&#65292;&#36890;&#36807;&#23567;&#37319;&#26679;&#35757;&#32451;&#65292;&#24182;&#33021;&#23558;&#20854;&#36755;&#20837;&#38646;&#26679;&#26412;&#38477;&#23610;&#24230;&#21040;&#20219;&#24847;&#26410;&#35265;&#39640;&#20998;&#36776;&#29575;&#65292;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#38477;&#23610;&#24230;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.14452</link><description>&lt;p&gt;
&#20219;&#24847;&#20998;&#36776;&#29575;&#27668;&#20505;&#25968;&#25454;&#38477;&#23610;&#24230;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Fourier Neural Operators for Arbitrary Resolution Climate Data Downscaling. (arXiv:2305.14452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14452
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#30340;&#20219;&#24847;&#20998;&#36776;&#29575;&#27668;&#20505;&#25968;&#25454;&#38477;&#23610;&#24230;&#26041;&#27861;&#65292;&#36890;&#36807;&#23567;&#37319;&#26679;&#35757;&#32451;&#65292;&#24182;&#33021;&#23558;&#20854;&#36755;&#20837;&#38646;&#26679;&#26412;&#38477;&#23610;&#24230;&#21040;&#20219;&#24847;&#26410;&#35265;&#39640;&#20998;&#36776;&#29575;&#65292;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#38477;&#23610;&#24230;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#27169;&#25311;&#22312;&#24341;&#23548;&#25105;&#20204;&#20102;&#35299;&#27668;&#20505;&#21464;&#21270;&#21644;&#24212;&#23545;&#20854;&#24433;&#21709;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20197;&#39640;&#31354;&#38388;&#20998;&#36776;&#29575;&#26469;&#35299;&#26512;&#22797;&#26434;&#30340;&#27668;&#20505;&#36807;&#31243;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#12290;&#20026;&#20102;&#21152;&#36895;&#27668;&#20505;&#27169;&#25311;&#65292;&#31070;&#32463;&#32593;&#32476;&#24050;&#29992;&#20110;&#20174;&#24555;&#36895;&#36816;&#34892;&#30340;&#20302;&#20998;&#36776;&#29575;&#27169;&#25311;&#20013;&#38477;&#23610;&#24230;&#27668;&#20505;&#21464;&#37327;&#65292;&#20294;&#39640;&#20998;&#36776;&#29575;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#38590;&#20197;&#33719;&#24471;&#25110;&#32570;&#20047;&#65292;&#22823;&#22823;&#38480;&#21046;&#20102;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#30340;&#38477;&#23610;&#24230;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#23567;&#30340;&#19978;&#37319;&#26679;&#22240;&#23376;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#21487;&#20197;&#23558;&#20854;&#36755;&#20837;&#38646;&#26679;&#26412;&#38477;&#23610;&#24230;&#21040;&#20219;&#24847;&#26410;&#35265;&#39640;&#20998;&#36776;&#29575;&#12290;&#25105;&#20204;&#30340;&#38477;&#23610;&#24230;&#27169;&#22411;&#22312;ERA5&#27668;&#20505;&#27169;&#22411;&#25968;&#25454;&#21644;Navier-Stokes&#26041;&#31243;&#35299;&#27861;&#25968;&#25454;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#22312;&#26631;&#20934;&#21333;&#20998;&#36776;&#29575;&#38477;&#23610;&#24230;&#21644;&#38646;&#26679;&#26412;&#25512;&#24191;&#21040;&#26356;&#39640;&#19978;&#37319;&#26679;&#22240;&#23376;&#26041;&#38754;&#65292;&#22343;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#21367;&#31215;&#21644;&#29983;&#25104;&#23545;&#25239;&#24335;&#38477;&#23610;&#24230;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Climate simulations are essential in guiding our understanding of climate change and responding to its effects. However, it is computationally expensive to resolve complex climate processes at high spatial resolution. As one way to speed up climate simulations, neural networks have been used to downscale climate variables from fast-running low-resolution simulations, but high-resolution training data are often unobtainable or scarce, greatly limiting accuracy. In this work, we propose a downscaling method based on the Fourier neural operator. It trains with data of a small upsampling factor and then can zero-shot downscale its input to arbitrary unseen high resolution. Evaluated both on ERA5 climate model data and on the Navier-Stokes equation solution data, our downscaling model significantly outperforms state-of-the-art convolutional and generative adversarial downscaling models, both in standard single-resolution downscaling and in zero-shot generalization to higher upsampling facto
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#31232;&#30095;&#32593;&#26684;&#20248;&#21270;&#32467;&#26500;&#21270;&#26680;&#25554;&#20540;&#26041;&#27861;&#65288;SKI&#65289;&#65292;&#22312;&#20445;&#35777;&#25554;&#20540;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#36755;&#20837;&#28857;&#32500;&#24230;&#36739;&#39640;&#24102;&#26469;&#30340;&#35745;&#31639;&#22256;&#38590;&#65292;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#31232;&#30095;&#32593;&#26684;&#30697;&#38453;&#20056;&#27861;&#31639;&#27861;&#20197;&#21450;&#39640;&#25928;&#25554;&#20540;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.14451</link><description>&lt;p&gt;
&#31232;&#30095;&#32593;&#26684;&#30340;&#26680;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Kernel Interpolation with Sparse Grids. (arXiv:2305.14451v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#31232;&#30095;&#32593;&#26684;&#20248;&#21270;&#32467;&#26500;&#21270;&#26680;&#25554;&#20540;&#26041;&#27861;&#65288;SKI&#65289;&#65292;&#22312;&#20445;&#35777;&#25554;&#20540;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#36755;&#20837;&#28857;&#32500;&#24230;&#36739;&#39640;&#24102;&#26469;&#30340;&#35745;&#31639;&#22256;&#38590;&#65292;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#31232;&#30095;&#32593;&#26684;&#30697;&#38453;&#20056;&#27861;&#31639;&#27861;&#20197;&#21450;&#39640;&#25928;&#25554;&#20540;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#26680;&#25554;&#20540;&#65288;SKI&#65289;&#36890;&#36807;&#20351;&#29992;&#24863;&#24212;&#28857;&#30340;&#23494;&#38598;&#32593;&#26684;&#25554;&#20540;&#26680;&#21327;&#26041;&#24046;&#20989;&#25968;&#65292;&#21152;&#36895;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#25512;&#26029;&#12290;&#23545;&#24212;&#30340;&#26680;&#30697;&#38453;&#39640;&#24230;&#32467;&#26500;&#21270;&#65292;&#22240;&#27492;&#26131;&#20110;&#24555;&#36895;&#36827;&#34892;&#32447;&#24615;&#20195;&#25968;&#35745;&#31639;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;SKI&#22312;&#36755;&#20837;&#28857;&#30340;&#32500;&#24230;&#26041;&#38754;&#30340;&#35268;&#27169;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#23494;&#38598;&#32593;&#26684;&#22823;&#23567;&#38543;&#30528;&#32500;&#24230;&#30340;&#22686;&#21152;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;SKI&#26694;&#26550;&#20869;&#20351;&#29992;&#31232;&#30095;&#32593;&#26684;&#12290;&#36825;&#20123;&#32593;&#26684;&#33021;&#22815;&#36827;&#34892;&#20934;&#30830;&#30340;&#25554;&#20540;&#65292;&#20294;&#28857;&#25968;&#38543;&#32500;&#24230;&#30340;&#22686;&#21152;&#26356;&#24930;&#12290;&#25105;&#20204;&#36129;&#29486;&#20102;&#19968;&#31181;&#26032;&#30340;&#36817;&#20284;&#32447;&#24615;&#26102;&#38388;&#30340;&#31232;&#30095;&#32593;&#26684;&#26680;&#30697;&#38453;&#30690;&#37327;&#20056;&#27861;&#31639;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#23558;&#31232;&#30095;&#32593;&#26684;&#19982;&#22522;&#20110;&#31616;&#21333;&#24418;&#24335;&#30340;&#39640;&#25928;&#25554;&#20540;&#26041;&#26696;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#36825;&#20123;&#25913;&#36827;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SKI&#21487;&#20197;&#25193;&#23637;&#21040;&#26356;&#39640;&#30340;&#32500;&#24230;&#32780;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured kernel interpolation (SKI) accelerates Gaussian process (GP) inference by interpolating the kernel covariance function using a dense grid of inducing points, whose corresponding kernel matrix is highly structured and thus amenable to fast linear algebra. Unfortunately, SKI scales poorly in the dimension of the input points, since the dense grid size grows exponentially with the dimension. To mitigate this issue, we propose the use of sparse grids within the SKI framework. These grids enable accurate interpolation, but with a number of points growing more slowly with dimension. We contribute a novel nearly linear time matrix-vector multiplication algorithm for the sparse grid kernel matrix. Next, we describe how sparse grids can be combined with an efficient interpolation scheme based on simplices. With these changes, we demonstrate that SKI can be scaled to higher dimensions while maintaining accuracy.
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#21327;&#21516;&#36807;&#28388;&#26032;&#26041;&#27861;&#29992;&#20110;&#31283;&#20581;&#23545;&#35805;&#29702;&#35299;&#65292;&#22312;&#21382;&#21490;&#29992;&#25143;-&#23454;&#20307;&#20132;&#20114;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#22810;&#36339;&#23458;&#25143;&#20146;&#21644;&#21147;&#20016;&#23500;&#27599;&#20010;&#29992;&#25143;&#30340;&#32034;&#24341;&#65292;&#24182;&#20351;&#29992;&#26377;&#38480;&#20869;&#23384;BFGS&#31639;&#27861;&#35843;&#25972;&#27599;&#20010;&#32034;&#24341;&#30340;&#26435;&#37325;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20010;&#24615;&#21270;&#26597;&#35810;&#37325;&#20889;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.14449</link><description>&lt;p&gt;
&#22270;&#35889;&#36935;&#35265;LLM&#65306;&#19968;&#31181;&#29992;&#20110;&#31283;&#20581;&#23545;&#35805;&#29702;&#35299;&#30340;&#21327;&#21516;&#36807;&#28388;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding. (arXiv:2305.14449v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14449
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#21327;&#21516;&#36807;&#28388;&#26032;&#26041;&#27861;&#29992;&#20110;&#31283;&#20581;&#23545;&#35805;&#29702;&#35299;&#65292;&#22312;&#21382;&#21490;&#29992;&#25143;-&#23454;&#20307;&#20132;&#20114;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#22810;&#36339;&#23458;&#25143;&#20146;&#21644;&#21147;&#20016;&#23500;&#27599;&#20010;&#29992;&#25143;&#30340;&#32034;&#24341;&#65292;&#24182;&#20351;&#29992;&#26377;&#38480;&#20869;&#23384;BFGS&#31639;&#27861;&#35843;&#25972;&#27599;&#20010;&#32034;&#24341;&#30340;&#26435;&#37325;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20010;&#24615;&#21270;&#26597;&#35810;&#37325;&#20889;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;&#20363;&#22914;Alexa&#65292;Siri&#65292;Google Assistant&#31561;&#65289;&#38656;&#35201;&#29702;&#35299;&#23384;&#22312;&#32570;&#38519;&#30340;&#26597;&#35810;&#20197;&#30830;&#20445;&#31283;&#20581;&#30340;&#20250;&#35805;&#29702;&#35299;&#24182;&#20943;&#23569;&#29992;&#25143;&#25705;&#25830;&#12290;&#36825;&#20123;&#26377;&#32570;&#38519;&#30340;&#26597;&#35810;&#36890;&#24120;&#26159;&#30001;&#29992;&#25143;&#30340;&#27495;&#20041;&#21644;&#38169;&#35823;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20013;&#30340;&#38169;&#35823;&#24341;&#36215;&#30340;&#12290;&#20010;&#24615;&#21270;&#26597;&#35810;&#37325;&#20889;&#65288;&#20010;&#24615;&#21270;QR&#65289;&#26088;&#22312;&#20943;&#23569;&#36523;&#20307;&#21644;&#23614;&#37096;&#29992;&#25143;&#26597;&#35810;&#27969;&#37327;&#20013;&#30340;&#32570;&#38519;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#19982;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36807;&#21435;&#25104;&#21151;&#30340;&#29992;&#25143;&#20132;&#20114;&#30340;&#32034;&#24341;&#12290;&#26412;&#25991;&#25552;&#20986;&#25105;&#20204;&#30340;&#8220;&#21327;&#21516;&#26597;&#35810;&#37325;&#20889;&#8221;&#26041;&#27861;&#65292;&#19987;&#27880;&#20110;&#37325;&#20889;&#29992;&#25143;&#21382;&#21490;&#20013;&#27809;&#26377;&#20986;&#29616;&#36807;&#30340;&#26032;&#22411;&#29992;&#25143;&#20132;&#20114;&#12290;&#35813;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#8220;&#29992;&#25143;&#21453;&#39304;&#20132;&#20114;&#22270;&#8221;&#65288;FIG&#65289;&#65292;&#30001;&#21382;&#21490;&#29992;&#25143;-&#23454;&#20307;&#20132;&#20114;&#32452;&#25104;&#65292;&#24182;&#21033;&#29992;&#22810;&#36339;&#23458;&#25143;&#20146;&#21644;&#21147;&#26469;&#20016;&#23500;&#27599;&#20010;&#29992;&#25143;&#30340;&#32034;&#24341;&#65288;&#21363;&#21327;&#21516;&#29992;&#25143;&#32034;&#24341;&#65289;&#65292;&#20174;&#32780;&#24110;&#21161;&#35206;&#30422;&#26410;&#26469;&#26410;&#26366;&#35265;&#36807;&#30340;&#23384;&#22312;&#32570;&#38519;&#30340;&#26597;&#35810;&#12290;&#20026;&#20102;&#38450;&#27490;&#36825;&#20123;&#26032;&#30340;&#20016;&#23500;&#32034;&#24341;&#34987;&#22122;&#22768;&#21453;&#39304;&#20132;&#20114;&#25152;&#25903;&#37197;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26377;&#38480;&#20869;&#23384;BFGS&#65288;LLM&#65289;&#31639;&#27861;&#21644;&#22238;&#36864;&#26041;&#26696;&#26469;&#35843;&#25972;&#27599;&#20010;&#32034;&#24341;&#30340;&#26435;&#37325;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20010;&#24615;&#21270;QR&#26041;&#27861;&#65292;&#24182;&#22312;&#26410;&#30475;&#21040;&#30340;&#29992;&#25143;&#20132;&#20114;&#19978;&#21462;&#24471;&#20102;&#36817;&#20046;&#23436;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational AI systems (e.g. Alexa, Siri, Google Assistant, etc.) need to understand queries with defects to ensure robust conversational understanding and reduce user frictions. The defective queries are often induced by user ambiguities and mistakes, or errors in the automatic speech recognition (ASR) and natural language understanding (NLU).  Personalized query rewriting (personalized QR) targets reducing defects in the torso and tail user query traffic, and it typically relies on an index of past successful user interactions with the conversational AI. This paper presents our "Collaborative Query Rewriting" approach that focuses on rewriting novel user interactions unseen in the user history. This approach builds a "user Feedback Interaction Graph" (FIG) consisting of historical user-entity interactions, and leverages multi-hop customer affinity to enrich each user's index (i.e. the Collaborative User Index) that would help cover future unseen defective queries. To counteract th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21367;&#31215;&#12289;&#33258;&#27880;&#24847;&#21644;&#20869;&#21367;&#31215;&#31561;&#29305;&#24449;&#31639;&#23376;&#20043;&#38388;&#30340;&#26412;&#36136;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Evolution&#30340;&#20844;&#24335;&#65292;&#36890;&#36807;&#24341;&#20837;&#36827;&#21270;&#26680;&#26469;&#23454;&#29616;&#19981;&#21516;&#29305;&#24449;&#31639;&#23376;&#30340;&#32479;&#19968;&#12290;</title><link>http://arxiv.org/abs/2305.14409</link><description>&lt;p&gt;
&#20174;&#39640;&#23618;&#27425;&#30340;&#35282;&#24230;&#65292;&#36827;&#21270;&#65306;&#19968;&#31181;&#29305;&#24449;&#31639;&#23376;&#30340;&#32479;&#19968;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolution: A Unified Formula for Feature Operators from a High-level Perspective. (arXiv:2305.14409v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21367;&#31215;&#12289;&#33258;&#27880;&#24847;&#21644;&#20869;&#21367;&#31215;&#31561;&#29305;&#24449;&#31639;&#23376;&#20043;&#38388;&#30340;&#26412;&#36136;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Evolution&#30340;&#20844;&#24335;&#65292;&#36890;&#36807;&#24341;&#20837;&#36827;&#21270;&#26680;&#26469;&#23454;&#29616;&#19981;&#21516;&#29305;&#24449;&#31639;&#23376;&#30340;&#32479;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#29305;&#24449;&#31639;&#23376;&#65288;&#20363;&#22914;&#21367;&#31215;&#12289;&#33258;&#27880;&#24847;&#21644;&#20869;&#21367;&#31215;&#65289;&#21033;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#25552;&#21462;&#21644;&#32858;&#21512;&#29305;&#24449;&#12290;&#20174;&#23427;&#20204;&#30340;&#25968;&#23398;&#20844;&#24335;&#20013;&#24456;&#38590;&#21457;&#29616;&#30456;&#20284;&#20043;&#22788;&#12290;&#28982;&#32780;&#65292;&#36825;&#19977;&#20010;&#31639;&#23376;&#37117;&#25317;&#26377;&#21516;&#26679;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#22312;&#26412;&#36136;&#19978;&#24182;&#27809;&#26377;&#21306;&#21035;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20174;&#19968;&#20010;&#39640;&#23618;&#27425;&#30340;&#35282;&#24230;&#25506;&#32034;&#21508;&#31181;&#29305;&#24449;&#31639;&#23376;&#30340;&#26412;&#36136;&#65292;&#31561;&#25928;&#22320;&#36716;&#25442;&#23427;&#20204;&#30340;&#32452;&#20214;&#65292;&#24182;&#22312;&#26356;&#39640;&#30340;&#32500;&#24230;&#20869;&#25506;&#32034;&#23427;&#20204;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28165;&#26224;&#32780;&#20855;&#20307;&#30340;&#19981;&#21516;&#29305;&#24449;&#31639;&#23376;&#30340;&#32479;&#19968;&#20844;&#24335;&#65292;&#31216;&#20026;Evolution&#12290;Evolution&#21033;&#29992;&#36827;&#21270;&#20989;&#25968;&#29983;&#25104;&#36827;&#21270;&#26680;&#65292;&#35813;&#26680;&#25552;&#21462;&#24182;&#32858;&#21512;&#36755;&#20837;&#29305;&#24449;&#26144;&#23556;&#20013;&#29305;&#23450;&#20301;&#32622;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#20174;&#25968;&#23398;&#19978;&#25512;&#23548;&#20986;&#20102;&#20174;&#36825;&#20123;&#29305;&#24449;&#31639;&#23376;&#30340;&#20256;&#32479;&#20844;&#24335;&#21040;&#36827;&#21270;&#20844;&#24335;&#30340;&#31561;&#25928;&#36716;&#25442;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#32479;&#19968;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#32479;&#19968;&#20844;&#24335;&#30340;&#20248;&#21183;&#20197;&#21450;&#22914;&#20309;&#23558;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#38382;&#39064;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, different types of feature operators (e.g., convolution, self-attention and involution) utilize different approaches to extract and aggregate the features. Resemblance can be hardly discovered from their mathematical formulas. However, these three operators all serve the same paramount purpose and bear no difference in essence. Hence we probe into the essence of various feature operators from a high-level perspective, transformed their components equivalently, and explored their mathematical expressions within higher dimensions. We raise one clear and concrete unified formula for different feature operators termed as Evolution. Evolution utilizes the Evolution Function to generate the Evolution Kernel, which extracts and aggregates the features in certain positions of the input feature map. We mathematically deduce the equivalent transformation from the traditional formulas of these feature operators to Evolution and prove the unification. In addition, we discuss the for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22312;&#32447;&#26102;&#23578;&#34892;&#19994;&#20013;&#65292;&#36890;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#38144;&#37327;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#25968;&#25454;&#37327;&#22823;&#12289;&#19981;&#35268;&#21017;&#24615;&#12289;&#39640;&#39057;&#29575;&#26356;&#36845;&#30340;&#20135;&#21697;&#30446;&#24405;&#21644;&#22266;&#23450;&#24211;&#23384;&#20551;&#35774;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.14406</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22312;&#32447;&#26102;&#23578;&#34892;&#19994;&#38144;&#37327;&#39044;&#27979;&#65306;&#19968;&#39033;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Learning based Forecasting: a case study from the online fashion industry. (arXiv:2305.14406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22312;&#32447;&#26102;&#23578;&#34892;&#19994;&#20013;&#65292;&#36890;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#38144;&#37327;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#25968;&#25454;&#37327;&#22823;&#12289;&#19981;&#35268;&#21017;&#24615;&#12289;&#39640;&#39057;&#29575;&#26356;&#36845;&#30340;&#20135;&#21697;&#30446;&#24405;&#21644;&#22266;&#23450;&#24211;&#23384;&#20551;&#35774;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#26102;&#23578;&#34892;&#19994;&#30340;&#38144;&#37327;&#39044;&#27979;&#30001;&#20110;&#20854;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#36866;&#21512;&#37319;&#29992;&#20840;&#29699;&#25968;&#25454;&#39537;&#21160;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#20123;&#25361;&#25112;&#21253;&#25324;&#25968;&#25454;&#37327;&#22823;&#12289;&#19981;&#35268;&#21017;&#24615;&#12289;&#20135;&#21697;&#30446;&#24405;&#39640;&#39057;&#29575;&#26356;&#36845;&#20197;&#21450;&#22266;&#23450;&#24211;&#23384;&#20551;&#35774;&#12290;&#34429;&#28982;&#26631;&#20934;&#30340;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#26041;&#27861;&#21487;&#20197;&#24212;&#23545;&#20854;&#20013;&#22823;&#37096;&#20998;&#38382;&#39064;&#65292;&#20294;&#22266;&#23450;&#24211;&#23384;&#20551;&#35774;&#38656;&#35201;&#36890;&#36807;&#23494;&#20999;&#25511;&#21046;&#20215;&#26684;&#19982;&#38656;&#27714;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#29305;&#27530;&#22788;&#29702;&#12290;&#22312;&#36825;&#39033;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;&#36825;&#19968;&#39044;&#27979;&#38382;&#39064;&#30340;&#25968;&#25454;&#21644;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Demand forecasting in the online fashion industry is particularly amendable to global, data-driven forecasting models because of the industry's set of particular challenges. These include the volume of data, the irregularity, the high amount of turn-over in the catalog and the fixed inventory assumption. While standard deep learning forecasting approaches cater for many of these, the fixed inventory assumption requires a special treatment via controlling the relationship between price and demand closely. In this case study, we describe the data and our modelling approach for this forecasting problem in detail and present empirical results that highlight the effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>NeuralMatrix&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21333;&#20010;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#21152;&#36895;&#22120;&#19978;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#65292;&#24182;&#21487;&#22312;&#20445;&#25345;&#25512;&#29702;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#36798;113&#20493;&#33267;19.44&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.14405</link><description>&lt;p&gt;
NeuralMatrix: &#23558;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#31227;&#21160;&#21040;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#20197;&#23454;&#29616;&#39640;&#25928;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
NeuralMatrix: Moving Entire Neural Networks to General Matrix Multiplication for Efficient Inference. (arXiv:2305.14405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14405
&lt;/p&gt;
&lt;p&gt;
NeuralMatrix&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21333;&#20010;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#21152;&#36895;&#22120;&#19978;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#65292;&#24182;&#21487;&#22312;&#20445;&#25345;&#25512;&#29702;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#36798;113&#20493;&#33267;19.44&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NeuralMatrix&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23427;&#20351;&#24471;&#21487;&#20197;&#22312;&#21333;&#20010;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#65288;GEMM&#65289;&#21152;&#36895;&#22120;&#19978;&#35745;&#31639;&#22810;&#21151;&#33021;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#22522;&#20110;ASIC&#30340;&#21152;&#36895;&#22120;&#30340;&#19987;&#29992;&#24615;&#38480;&#21046;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;CPU&#21644;GPU&#31561;&#36890;&#29992;&#22788;&#29702;&#22120;&#30456;&#27604;&#30340;&#24212;&#29992;&#29305;&#23450;&#21152;&#36895;&#27700;&#24179;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#23558;DNN&#35745;&#31639;&#20013;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36816;&#31639;&#26144;&#23556;&#21040;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#20197;&#21450;&#20351;&#29992;GEMM&#21152;&#36895;&#22120;&#23545;DNN&#25512;&#29702;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#19977;&#31181;&#27969;&#34892;&#31867;&#21035;&#30340;&#21508;&#31181;DNN&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65288;&#21363;CNN&#65292;Transformers&#21644;GNN&#65289;&#20316;&#20026;&#31034;&#20363;&#30340;&#25903;&#25745;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;DNN&#36716;&#25442;&#20026;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#21518;&#20165;&#20250;&#20986;&#29616;&#39640;&#36798;2.02&#65285;&#30340;&#20934;&#30830;&#24230;&#25439;&#22833;&#65292;&#21516;&#26102;&#23558;&#21534;&#21520;&#37327;&#19982;&#21151;&#29575;&#30340;&#27604;&#20540;&#19982;CPU&#21644;GPU&#30456;&#27604;&#25552;&#39640;&#20102;113&#20493;&#21040;19.44&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce NeuralMatrix, a novel framework that enables the computation of versatile deep neural networks (DNNs) on a single general matrix multiplication (GEMM) accelerator. The proposed approach overcomes the specificity limitations of ASIC-based accelerators while achieving application-specific acceleration levels compared to general-purpose processors such as CPUs and GPUs. We address the challenges of mapping both linear and nonlinear operations in DNN computation to general matrix multiplications and the impact of using a GEMM accelerator on DNN inference accuracy. Extensive experiments are conducted on various DNN models from three popular categories (i.e., CNN, Transformers, and GNN) as illustrative backbone models. Our results demonstrate that DNNs suffer only up to a 2.02% accuracy loss after being converted to general matrix multiplication, while achieving 113x to 19.44x improvements in throughput per power compared to CPUs and GPUs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33041;&#32467;&#26500;-&#21151;&#33021;&#34701;&#21512;&#34920;&#31034;&#23398;&#20064;&#65288;BSFL&#65289;&#27169;&#22411;&#65292;&#20174;&#25193;&#25955;&#24352;&#37327;&#25104;&#20687;&#65288;DTI&#65289;&#21644;&#38745;&#24687;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#20013;&#26377;&#25928;&#22320;&#23398;&#20064;&#34701;&#21512;&#34920;&#31034;&#36827;&#34892;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65288;MCI&#65289;&#20998;&#26512;&#65292;&#37319;&#29992;&#35299;&#32806;-&#34701;&#21512;&#26694;&#26550;&#21644;&#30693;&#35782;&#24863;&#30693;&#21464;&#21387;&#22120;&#27169;&#22359;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14404</link><description>&lt;p&gt;
&#34701;&#21512;&#32467;&#26500;&#21151;&#33021;&#30340;&#23545;&#25239;&#35299;&#32806;VAE&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;MCI&#20998;&#26512;&#30340;&#33041;&#32467;&#26500;-&#21151;&#33021;&#34701;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Brain Structure-Function Fusing Representation Learning using Adversarial Decomposed-VAE for Analyzing MCI. (arXiv:2305.14404v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33041;&#32467;&#26500;-&#21151;&#33021;&#34701;&#21512;&#34920;&#31034;&#23398;&#20064;&#65288;BSFL&#65289;&#27169;&#22411;&#65292;&#20174;&#25193;&#25955;&#24352;&#37327;&#25104;&#20687;&#65288;DTI&#65289;&#21644;&#38745;&#24687;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#20013;&#26377;&#25928;&#22320;&#23398;&#20064;&#34701;&#21512;&#34920;&#31034;&#36827;&#34892;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65288;MCI&#65289;&#20998;&#26512;&#65292;&#37319;&#29992;&#35299;&#32806;-&#34701;&#21512;&#26694;&#26550;&#21644;&#30693;&#35782;&#24863;&#30693;&#21464;&#21387;&#22120;&#27169;&#22359;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#33041;&#32467;&#26500;&#21644;&#21151;&#33021;&#36830;&#25509;&#29305;&#24449;&#25972;&#21512;&#36215;&#26469;&#23545;&#20110;&#25506;&#32034;&#22823;&#33041;&#31185;&#23398;&#21644;&#20020;&#24202;&#20998;&#26512;&#35748;&#30693;&#38556;&#30861;&#37117;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#65292;&#20294;&#26159;&#26377;&#25928;&#22320;&#23558;&#32467;&#26500;&#21644;&#21151;&#33021;&#29305;&#24449;&#34701;&#21512;&#36215;&#26469;&#25506;&#32034;&#22823;&#33041;&#32593;&#32476;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33041;&#32467;&#26500;-&#21151;&#33021;&#34701;&#21512;&#34920;&#31034;&#23398;&#20064;&#65288;BSFL&#65289;&#27169;&#22411;&#65292;&#20174;&#25193;&#25955;&#24352;&#37327;&#25104;&#20687;&#65288;DTI&#65289;&#21644;&#38745;&#24687;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#20013;&#26377;&#25928;&#22320;&#23398;&#20064;&#34701;&#21512;&#34920;&#31034;&#36827;&#34892;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65288;MCI&#65289;&#20998;&#26512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24320;&#21457;&#20102;&#35299;&#32806;-&#34701;&#21512;&#26694;&#26550;&#65292;&#39318;&#20808;&#23558;&#29305;&#24449;&#31354;&#38388;&#20998;&#35299;&#20026;&#27599;&#31181;&#27169;&#24577;&#30340;&#22343;&#21248;&#31354;&#38388;&#21644;&#29420;&#29305;&#31354;&#38388;&#30340;&#24182;&#38598;&#65292;&#28982;&#21518;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#35299;&#32806;&#21518;&#30340;&#29305;&#24449;&#20197;&#23398;&#20064;&#19982;MCI&#30456;&#20851;&#30340;&#34920;&#24449;&#12290;&#27492;&#22806;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#30693;&#35782;&#24863;&#30693;&#21464;&#21387;&#22120;&#27169;&#22359;&#65292;&#20197;&#33258;&#21160;&#25429;&#33719;&#25972;&#20010;&#22823;&#33041;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#36830;&#25509;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22343;&#21248;&#37319;&#26679;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#23545;&#32467;&#26500;&#21644;&#21151;&#33021;&#36830;&#25509;&#30340;&#19981;&#21516;&#23610;&#24230;&#37117;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;&#23545;&#19968;&#20010;&#22823;&#22411;&#20844;&#24320;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;BSFL&#27169;&#22411;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating the brain structural and functional connectivity features is of great significance in both exploring brain science and analyzing cognitive impairment clinically. However, it remains a challenge to effectively fuse structural and functional features in exploring the brain network. In this paper, a novel brain structure-function fusing-representation learning (BSFL) model is proposed to effectively learn fused representation from diffusion tensor imaging (DTI) and resting-state functional magnetic resonance imaging (fMRI) for mild cognitive impairment (MCI) analysis. Specifically, the decomposition-fusion framework is developed to first decompose the feature space into the union of the uniform and the unique spaces for each modality, and then adaptively fuse the decomposed features to learn MCI-related representation. Moreover, a knowledge-aware transformer module is designed to automatically capture local and global connectivity features throughout the brain. Also, a uniform
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;&#37325;&#35201;&#24615;&#20998;&#25968;SP-LAMP&#30340;&#20998;&#23618;&#33258;&#36866;&#24212;&#32467;&#26500;&#35009;&#21098;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#32452;&#32972;&#21253;&#27714;&#35299;&#22120;&#22312;&#24310;&#36831;&#32422;&#26463;&#19979;&#26368;&#22823;&#21270;SP-LAMP&#20998;&#25968;&#26469;&#25351;&#23548;&#32593;&#32476;&#35009;&#21098;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#20248;&#21270;&#32467;&#26524;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.14403</link><description>&lt;p&gt;
&#32771;&#34385;&#24310;&#36831;&#30340;&#20998;&#23618;&#33258;&#36866;&#24212;&#32467;&#26500;&#35009;&#21098;
&lt;/p&gt;
&lt;p&gt;
Layer-adaptive Structured Pruning Guided by Latency. (arXiv:2305.14403v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14403
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;&#37325;&#35201;&#24615;&#20998;&#25968;SP-LAMP&#30340;&#20998;&#23618;&#33258;&#36866;&#24212;&#32467;&#26500;&#35009;&#21098;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#32452;&#32972;&#21253;&#27714;&#35299;&#22120;&#22312;&#24310;&#36831;&#32422;&#26463;&#19979;&#26368;&#22823;&#21270;SP-LAMP&#20998;&#25968;&#26469;&#25351;&#23548;&#32593;&#32476;&#35009;&#21098;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#20248;&#21270;&#32467;&#26524;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#35009;&#21098;&#21487;&#20197;&#31616;&#21270;&#32593;&#32476;&#32467;&#26500;&#24182;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#12290;&#36890;&#36807;&#32467;&#21512;&#37096;&#32626;&#26368;&#32456;&#27169;&#22411;&#30340;&#24213;&#23618;&#30828;&#20214;&#21644;&#25512;&#29702;&#24341;&#25806;&#65292;&#20351;&#29992;&#24310;&#36831;&#21327;&#21516;&#25439;&#22833;&#20989;&#25968;&#26469;&#25351;&#23548;&#32593;&#32476;&#35009;&#21098;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#29616;&#26377;&#30340;&#20248;&#21270;&#24310;&#36831;&#30340;&#35009;&#21098;&#26041;&#27861;&#24050;&#32463;&#23637;&#29616;&#20986;&#39046;&#20808;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#24573;&#30053;&#32593;&#32476;&#20013;&#30340;&#30828;&#20214;&#29305;&#24449;&#21644;&#36830;&#25509;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#23616;&#37325;&#35201;&#24615;&#20998;&#25968;SP-LAMP(&#32467;&#26500;&#35009;&#21098;&#23618;&#33258;&#36866;&#24212;&#22522;&#20110;&#24133;&#24230;&#30340;&#35009;&#21098;)&#65292;&#36890;&#36807;&#20174;&#38750;&#32467;&#26500;&#21270;&#35009;&#21098;&#21040;&#32467;&#26500;&#21270;&#35009;&#21098;&#20013;&#23548;&#20986;&#20840;&#23616;&#37325;&#35201;&#24615;&#20998;&#25968;LAMP&#26469;&#35745;&#31639;SP-LAMP&#12290;&#22312;SP-LAMP&#20013;&#65292;&#27599;&#20010;&#23618;&#37117;&#21253;&#25324;&#19968;&#20010;SP-LAMP&#20998;&#25968;&#20026;1&#30340;&#36807;&#28388;&#22120;&#65292;&#20854;&#20313;&#30340;&#36807;&#28388;&#22120;&#20998;&#32452;&#12290;&#25105;&#20204;&#21033;&#29992;&#20998;&#32452;&#32972;&#21253;&#27714;&#35299;&#22120;&#65292;&#22312;&#24310;&#36831;&#32422;&#26463;&#19979;&#26368;&#22823;&#21270;SP-LAMP&#20998;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#25910;&#38598;&#24310;&#36831;&#30340;&#31574;&#30053;&#65292;&#20351;&#20854;&#26356;&#21152;&#20934;&#30830;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;ResNet50/ResNet1&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured pruning can simplify network architecture and improve inference speed. Combined with the underlying hardware and inference engine in which the final model is deployed, better results can be obtained by using latency collaborative loss function to guide network pruning together. Existing pruning methods that optimize latency have demonstrated leading performance, however, they often overlook the hardware features and connection in the network. To address this problem, we propose a global importance score SP-LAMP(Structured Pruning Layer-Adaptive Magnitude-based Pruning) by deriving a global importance score LAMP from unstructured pruning to structured pruning. In SP-LAMP, each layer includes a filter with an SP-LAMP score of 1, and the remaining filters are grouped. We utilize a group knapsack solver to maximize the SP-LAMP score under latency constraints. In addition, we improve the strategy of collect the latency to make it more accurate. In particular, for ResNet50/ResNet1
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;DARTS&#20248;&#21270;&#32852;&#21512;CNN&#21644;LSTM&#30340;&#20307;&#31995;&#32467;&#26500;&#20197;&#25552;&#39640;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20197;&#24448;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14402</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21516;iable&#30340;&#25628;&#32034;&#20307;&#31995;&#26550;&#26500;&#25552;&#39640;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving Speech Emotion Recognition Performance using Differentiable Architecture Search. (arXiv:2305.14402v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14402
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;DARTS&#20248;&#21270;&#32852;&#21512;CNN&#21644;LSTM&#30340;&#20307;&#31995;&#32467;&#26500;&#20197;&#25552;&#39640;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20197;&#24448;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;(SER)&#26159;&#23454;&#29616;&#24773;&#24863;&#24863;&#30693;&#20132;&#20114;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#28145;&#24230;&#23398;&#20064;(DL)&#25913;&#21892;&#20102;SER&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#35774;&#35745;DL&#20307;&#31995;&#32467;&#26500;&#38656;&#35201;&#20808;&#21069;&#30340;&#32463;&#39564;&#21644;&#23454;&#39564;&#35780;&#20272;&#12290;&#40723;&#21169;&#22320;&#65292;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;(NAS)&#20801;&#35768;&#33258;&#21160;&#25628;&#32034;&#26368;&#20248;DL&#27169;&#22411;&#12290;&#29305;&#21035;&#22320;&#65292;&#21487;&#21306;&#20998;&#30340;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;(DARTS)&#26159;&#19968;&#31181;&#20351;&#29992;NAS&#25628;&#32034;&#26368;&#20248;&#21270;&#27169;&#22411;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;DARTS&#29992;&#20110;&#32852;&#21512;CNN&#21644;LSTM&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#25913;&#21892;SER&#24615;&#33021;&#12290;&#25105;&#20204;&#36873;&#25321;CNN LSTM&#32806;&#21512;&#30340;&#21407;&#22240;&#26159;&#32467;&#26524;&#34920;&#26126;&#31867;&#20284;&#30340;&#27169;&#22411;&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#12290;&#34429;&#28982;SER&#30740;&#31350;&#20154;&#21592;&#24050;&#23558;CNN&#21644;RNN&#20998;&#21035;&#32771;&#34385;&#65292;&#20294;DARTs&#21516;&#26102;&#29992;&#20110;CNN&#21644;LSTM&#30340;&#21487;&#34892;&#24615;&#20173;&#38656;&#35201;&#25506;&#32034;&#12290;&#36890;&#36807;&#23545;IEMOCAP&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20351;&#29992;DA&#30340;&#26368;&#20339;&#25253;&#21578;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech Emotion Recognition (SER) is a critical enabler of emotion-aware communication in human-computer interactions. Deep Learning (DL) has improved the performance of SER models by improving model complexity. However, designing DL architectures requires prior experience and experimental evaluations. Encouragingly, Neural Architecture Search (NAS) allows automatic search for an optimum DL model. In particular, Differentiable Architecture Search (DARTS) is an efficient method of using NAS to search for optimised models. In this paper, we propose DARTS for a joint CNN and LSTM architecture for improving SER performance. Our choice of the CNN LSTM coupling is inspired by results showing that similar models offer improved performance. While SER researchers have considered CNNs and RNNs separately, the viability of using DARTs jointly for CNN and LSTM still needs exploration. Experimenting with the IEMOCAP dataset, we demonstrate that our approach outperforms best-reported results using DA
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;&#35821;&#20041;&#20449;&#24687;&#24230;&#37327;&#21644;&#23398;&#20064;&#20989;&#25968;&#30340;&#28436;&#21270;&#21382;&#21490;&#65292;&#24182;&#20171;&#32461;&#20102;&#20316;&#32773;&#30340;&#35821;&#20041;&#20449;&#24687;G&#29702;&#35770;&#21450;&#20854;&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#12289;&#26368;&#22823;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#20998;&#31867;&#21644;&#28151;&#21512;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#35752;&#35770;&#20102;&#22914;&#20309;&#29702;&#35299;SeMI&#21644;Shannon&#30340;MI&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#20854;&#20182;&#30456;&#20851;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2305.14397</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#23398;&#20064;&#20989;&#25968;&#21644;&#35821;&#20041;&#20449;&#24687;&#24230;&#37327;&#30340;&#28436;&#21464;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Reviewing Evolution of Learning Functions and Semantic Information Measures for Understanding Deep Learning. (arXiv:2305.14397v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14397
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;&#35821;&#20041;&#20449;&#24687;&#24230;&#37327;&#21644;&#23398;&#20064;&#20989;&#25968;&#30340;&#28436;&#21270;&#21382;&#21490;&#65292;&#24182;&#20171;&#32461;&#20102;&#20316;&#32773;&#30340;&#35821;&#20041;&#20449;&#24687;G&#29702;&#35770;&#21450;&#20854;&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#12289;&#26368;&#22823;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#20998;&#31867;&#21644;&#28151;&#21512;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#35752;&#35770;&#20102;&#22914;&#20309;&#29702;&#35299;SeMI&#21644;Shannon&#30340;MI&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#20854;&#20182;&#30456;&#20851;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#26032;&#36235;&#21183;&#26159;&#20114;&#20449;&#24687;&#31070;&#32463;&#20272;&#35745;&#65288;MINE&#65289;&#21644;&#20449;&#24687;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#65288;InfoNCE&#65289;&#12290;&#22312;&#36825;&#31181;&#36235;&#21183;&#20013;&#65292;&#30456;&#20284;&#24615;&#20989;&#25968;&#21644;&#20272;&#31639;&#30340;&#20114;&#20449;&#24687;&#65288;EMI&#65289;&#34987;&#29992;&#20316;&#23398;&#20064;&#21644;&#30446;&#26631;&#20989;&#25968;&#12290;&#24039;&#21512;&#30340;&#26159;&#65292;EMI&#26412;&#36136;&#19978;&#19982;&#20316;&#32773;30&#24180;&#21069;&#25552;&#20986;&#30340;&#35821;&#20041;&#20114;&#20449;&#24687;&#65288;SeMI&#65289;&#30456;&#21516;&#12290;&#26412;&#25991;&#39318;&#20808;&#22238;&#39038;&#20102;&#35821;&#20041;&#20449;&#24687;&#24230;&#37327;&#21644;&#23398;&#20064;&#20989;&#25968;&#30340;&#28436;&#21270;&#21382;&#21490;&#12290;&#28982;&#21518;&#65292;&#23427;&#31616;&#35201;&#20171;&#32461;&#20102;&#20316;&#32773;&#30340;&#35821;&#20041;&#20449;&#24687;G&#29702;&#35770;&#21450;&#20854;&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#12289;&#26368;&#22823;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#20998;&#31867;&#21644;&#28151;&#21512;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#12290;&#25509;&#19979;&#26469;&#65292;&#26412;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#29702;&#35299;SeMI&#21644;&#39321;&#20892;&#30340;MI&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#27169;&#31946;&#29109;&#21644;&#35206;&#30422;&#29109;&#36825;&#20004;&#31181;&#24191;&#20041;&#29109;&#12289;&#33258;&#32534;&#30721;&#22120;&#12289;&#21513;&#24067;&#26031;&#20998;&#24067;&#21644;&#20998;&#21306;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new trend in deep learning, represented by Mutual Information Neural Estimation (MINE) and Information Noise Contrast Estimation (InfoNCE), is emerging. In this trend, similarity functions and Estimated Mutual Information (EMI) are used as learning and objective functions. Coincidentally, EMI is essentially the same as Semantic Mutual Information (SeMI) proposed by the author 30 years ago. This paper first reviews the evolutionary histories of semantic information measures and learning functions. Then, it briefly introduces the author's semantic information G theory with the rate-fidelity function R(G) (G denotes SeMI, and R(G) extends R(D)) and its applications to multi-label learning, the maximum Mutual Information (MI) classification, and mixture models. Then it discusses how we should understand the relationship between SeMI and Shan-non's MI, two generalized entropies (fuzzy entropy and coverage entropy), Autoencoders, Gibbs distributions, and partition functions from the perspe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FITNESS&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21435;&#38500;&#25935;&#24863;&#29305;&#24449;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#22240;&#26524;&#24433;&#21709;&#26469;&#20943;&#23569;&#20559;&#35265;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#20844;&#24179;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14396</link><description>&lt;p&gt;
FITNESS&#65306;&#19968;&#31181;&#20943;&#36731;&#26426;&#22120;&#23398;&#20064;&#36719;&#20214;&#20013;&#20559;&#35265;&#30340;&#22240;&#26524;&#21435;&#30456;&#20851;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FITNESS: A Causal De-correlation Approach for Mitigating Bias in Machine Learning Software. (arXiv:2305.14396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FITNESS&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21435;&#38500;&#25935;&#24863;&#29305;&#24449;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#22240;&#26524;&#24433;&#21709;&#26469;&#20943;&#23569;&#20559;&#35265;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#20844;&#24179;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#22312;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#19978;&#30340;&#36719;&#20214;&#22312;&#21253;&#25324;&#22823;&#23398;&#25307;&#29983;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#20445;&#38505;&#21644;&#21496;&#27861;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#36825;&#20123;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#12290;&#20559;&#35265;&#25968;&#25454;&#38598;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#21644;&#28508;&#22312;&#26377;&#23475;&#30340;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;&#36825;&#26679;&#30340;&#20851;&#38190;&#20915;&#31574;&#31995;&#32479;&#20013;&#65292;&#36164;&#28304;&#30340;&#20998;&#37197;&#21487;&#33021;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#36825;&#21487;&#33021;&#21152;&#21095;&#23545;&#26576;&#20123;&#32676;&#20307;&#30340;&#27495;&#35270;&#65292;&#24182;&#36896;&#25104;&#37325;&#22823;&#30340;&#31038;&#20250;&#21160;&#33633;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#26679;&#30340;&#19981;&#20844;&#24179;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#20943;&#36731;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#36825;&#20123;&#30740;&#31350;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#39640;&#20102;&#35757;&#32451;&#27169;&#22411;&#30340;&#20844;&#27491;&#24615;&#65292;&#20294;&#20195;&#20215;&#26159;&#29306;&#29298;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FITNESS&#65292;&#19968;&#31181;&#36890;&#36807;&#21435;&#30456;&#20851;&#25935;&#24863;&#29305;&#24449;&#65288;&#20363;&#22914;&#24615;&#21035;&#65289;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#22240;&#26524;&#24433;&#21709;&#26469;&#20943;&#23569;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#20174;&#29305;&#24449;&#20013;&#21435;&#30456;&#20851;&#33719;&#24471;&#26356;&#28165;&#26224;&#30340;&#22240;&#26524;&#20449;&#24687;&#65292;&#21516;&#26102;&#32500;&#25345;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software built on top of machine learning algorithms is becoming increasingly prevalent in a variety of fields, including college admissions, healthcare, insurance, and justice. The effectiveness and efficiency of these systems heavily depend on the quality of the training datasets. Biased datasets can lead to unfair and potentially harmful outcomes, particularly in such critical decision-making systems where the allocation of resources may be affected. This can exacerbate discrimination against certain groups and cause significant social disruption. To mitigate such unfairness, a series of bias-mitigating methods are proposed. Generally, these studies improve the fairness of the trained models to a certain degree but with the expense of sacrificing the model performance. In this paper, we propose FITNESS, a bias mitigation approach via de-correlating the causal effects between sensitive features (e.g., the sex) and the label. Our key idea is that by de-correlating such effects from a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#35299;&#37322;&#20013;&#20351;&#29992;&#36335;&#24452;&#24402;&#22240;&#31574;&#30053;&#26102;&#65292;&#21487;&#20197;&#36890;&#36807;&#29305;&#23450;&#26465;&#20214;&#36991;&#20813;&#21453;&#30452;&#35273;&#32467;&#26524;&#12290;&#24182;&#25552;&#20986;&#19968;&#31181;&#26041;&#26696;&#24110;&#21161;&#38450;&#27490;&#35270;&#35273;&#27169;&#22411;&#35299;&#37322;&#26080;&#25928;&#21270;&#36335;&#24452;&#24402;&#22240;&#30340;&#20844;&#29702;&#23646;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#21487;&#38752;&#30340;&#35270;&#35273;&#27169;&#22411;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.14395</link><description>&lt;p&gt;
&#22522;&#20110;&#36335;&#24452;&#24402;&#22240;&#30340;&#21487;&#20449;&#35270;&#35273;&#27169;&#22411;&#35299;&#37322;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards credible visual model interpretation with path attribution. (arXiv:2305.14395v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#35299;&#37322;&#20013;&#20351;&#29992;&#36335;&#24452;&#24402;&#22240;&#31574;&#30053;&#26102;&#65292;&#21487;&#20197;&#36890;&#36807;&#29305;&#23450;&#26465;&#20214;&#36991;&#20813;&#21453;&#30452;&#35273;&#32467;&#26524;&#12290;&#24182;&#25552;&#20986;&#19968;&#31181;&#26041;&#26696;&#24110;&#21161;&#38450;&#27490;&#35270;&#35273;&#27169;&#22411;&#35299;&#37322;&#26080;&#25928;&#21270;&#36335;&#24452;&#24402;&#22240;&#30340;&#20844;&#29702;&#23646;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#21487;&#38752;&#30340;&#35270;&#35273;&#27169;&#22411;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36335;&#24452;&#24402;&#22240;&#26694;&#26550;&#26368;&#21021;&#21463;&#21338;&#24328;&#35770;&#21551;&#21457;&#65292;&#30001;&#20110;&#20854;&#20844;&#29702;&#24615;&#36136;&#32780;&#22312;&#21518;&#22788;&#29702;&#27169;&#22411;&#35299;&#37322;&#24037;&#20855;&#20013;&#33073;&#39062;&#32780;&#20986;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#21457;&#23637;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#20173;&#28982;&#21487;&#33021;&#20986;&#29616;&#21453;&#30452;&#35273;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#29305;&#21035;&#38024;&#23545;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#26041;&#27861;&#20063;&#26080;&#27861;&#31526;&#21512;&#35813;&#26694;&#26550;&#25152;&#23459;&#31216;&#30340;&#20844;&#29702;&#23646;&#24615;&#30340;&#22522;&#30784;&#30452;&#35273;&#12290;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#30340;&#30740;&#31350;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#20351;&#29992;&#36335;&#24452;&#24402;&#22240;&#31574;&#30053;&#36827;&#34892;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#35299;&#37322;&#21487;&#20197;&#36991;&#20813;&#21453;&#30452;&#35273;&#32467;&#26524;&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#26041;&#26696;&#65292;&#20197;&#38450;&#27490;&#35270;&#35273;&#27169;&#22411;&#35299;&#37322;&#26080;&#25928;&#21270;&#36335;&#24452;&#24402;&#22240;&#30340;&#20844;&#29702;&#23646;&#24615;&#12290;&#36825;&#20123;&#35265;&#35299;&#34987;&#32467;&#21512;&#25104;&#19968;&#31181;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#21487;&#38752;&#30340;&#35270;&#35273;&#27169;&#22411;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#36890;&#36807;&#22810;&#20010;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#35780;&#20272;&#24471;&#21040;&#20102;&#23454;&#35777;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Originally inspired by game-theory, path attribution framework stands out among the post-hoc model interpretation tools due to its axiomatic nature. However, recent developments show that this framework can still suffer from counter-intuitive results. Moreover, specifically for deep visual models, the existing path-based methods also fall short on conforming to the original intuitions that are the basis of the claimed axiomatic properties of this framework. We address these problems with a systematic investigation, and pinpoint the conditions in which the counter-intuitive results can be avoided for deep visual model interpretation with the path attribution strategy. We also devise a scheme to preclude the conditions in which visual model interpretation can invalidate the axiomatic properties of path attribution. These insights are combined into a method that enables reliable visual model interpretation. Our findings are establish empirically with multiple datasets, models and evaluati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#24314;&#31435;&#30340;&#21069;&#39069;&#21494;&#30382;&#23618;&#35745;&#31639;&#27169;&#22411;&#12290;&#36890;&#36807;&#25345;&#20037;&#27963;&#21160;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#35268;&#21017;&#65292;&#27169;&#25311;&#20102;&#20219;&#21153;&#20999;&#25442;&#21644;&#25439;&#20260;&#29366;&#24577;&#19979;&#31070;&#32463;&#20803;&#30340;&#21709;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#20026;&#30740;&#31350;&#31070;&#32463;&#20803;&#36866;&#24212;&#21644;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2305.14394</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30740;&#31350;&#21069;&#39069;&#21494;&#30382;&#23618;&#20013;&#30340;&#20219;&#21153;&#20999;&#25442;&#21644;&#31361;&#35302;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Spiking Neural Network Model of Prefrontal Cortex to study Task Switching with Synaptic deficiency. (arXiv:2305.14394v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#24314;&#31435;&#30340;&#21069;&#39069;&#21494;&#30382;&#23618;&#35745;&#31639;&#27169;&#22411;&#12290;&#36890;&#36807;&#25345;&#20037;&#27963;&#21160;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#35268;&#21017;&#65292;&#27169;&#25311;&#20102;&#20219;&#21153;&#20999;&#25442;&#21644;&#25439;&#20260;&#29366;&#24577;&#19979;&#31070;&#32463;&#20803;&#30340;&#21709;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#20026;&#30740;&#31350;&#31070;&#32463;&#20803;&#36866;&#24212;&#21644;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26500;&#24314;&#20102;&#21069;&#39069;&#21494;&#30382;&#23618;&#65288;PFC&#65289;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#20197;&#20102;&#35299;&#31070;&#32463;&#20803;&#22312;&#30701;&#26102;&#21644;&#38271;&#26102;&#30340;&#21050;&#28608;&#21464;&#21270;&#19979;&#22914;&#20309;&#36866;&#24212;&#21644;&#21709;&#24212;&#20219;&#21153;&#20999;&#25442;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#22312;&#25105;&#20204;&#30340;&#33033;&#20914;&#20307;&#31995;&#32467;&#26500;&#27169;&#22411;&#20013;&#27169;&#25311;&#21463;&#25439;&#29366;&#24577;&#26469;&#25506;&#35752; PFC &#25439;&#20260;&#25152;&#24341;&#36215;&#30340;&#34892;&#20026;&#32570;&#38519;&#12290;&#23613;&#31649;&#24050;&#32463;&#23384;&#22312;&#19968;&#20123; PFC &#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#20294;&#36824;&#27809;&#26377;&#20351;&#29992; SNN &#26469;&#24314;&#27169;&#23427;&#20204;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#21442;&#25968;&#25509;&#36817;&#29983;&#29289;&#23398;&#21487;&#34892;&#20540;&#30340; SNN &#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#26080;&#30417;&#30563;&#30340;&#33033;&#20914;&#26102;&#24207;&#20381;&#36182;&#24615;&#22609;&#24615;&#65288;STDP&#65289;&#23398;&#20064;&#35268;&#21017;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#32852;&#32467;&#20027;&#20041;&#26550;&#26500;&#65292;&#23637;&#29616;&#20102;&#25345;&#20037;&#27963;&#21160;&#31561;&#31070;&#32463;&#29616;&#35937;&#65292;&#26377;&#21161;&#20110;&#20135;&#29983;&#30701;&#26399;&#25110;&#24037;&#20316;&#35760;&#24518;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#29305;&#28857;&#36890;&#36807;&#20851;&#38381;&#31361;&#35302;&#36890;&#36335;&#26469;&#27169;&#25311;&#25439;&#20260;&#65292;&#24182;&#35760;&#24405;&#23398;&#20064;&#27169;&#24335;&#30340;&#26435;&#37325;&#35843;&#25972;&#21644;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#23398;&#20064;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we build a computational model of Prefrontal Cortex (PFC) using Spiking Neural Networks (SNN) to understand how neurons adapt and respond to tasks switched under short and longer duration of stimulus changes. We also explore behavioral deficits arising out of the PFC lesions by simulating lesioned states in our Spiking architecture model. Although there are some computational models of the PFC, SNN's have not been used to model them. In this study, we use SNN's having parameters close to biologically plausible values and train the model using unsupervised Spike Timing Dependent Plasticity (STDP) learning rule. Our model is based on connectionist architectures and exhibits neural phenomena like sustained activity which helps in generating short-term or working memory. We use these features to simulate lesions by deactivating synaptic pathways and record the weight adjustments of learned patterns and capture the accuracy of learning tasks in such conditions. All our experi
&lt;/p&gt;</description></item><item><title>FEDORA&#26159;&#19968;&#20010;&#39134;&#34892;&#20107;&#20214;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#32570;&#23569;&#23436;&#25972;&#25968;&#25454;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#24110;&#21161;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#23454;&#29616;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#23548;&#33322;&#21644;&#36991;&#38556;&#12290;</title><link>http://arxiv.org/abs/2305.14392</link><description>&lt;p&gt;
FEDORA&#65306;&#29992;&#20110;&#21453;&#24212;&#34892;&#20026;&#30340;&#39134;&#34892;&#20107;&#20214;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FEDORA: Flying Event Dataset fOr Reactive behAvior. (arXiv:2305.14392v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14392
&lt;/p&gt;
&lt;p&gt;
FEDORA&#26159;&#19968;&#20010;&#39134;&#34892;&#20107;&#20214;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#32570;&#23569;&#23436;&#25972;&#25968;&#25454;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#24110;&#21161;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#23454;&#29616;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#23548;&#33322;&#21644;&#36991;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#20307;&#22312;&#39134;&#34892;&#20013;&#20351;&#29992;&#26497;&#23569;&#25968;&#30340;&#31070;&#32463;&#20803;&#21644;&#26497;&#20302;&#30340;&#22833;&#35823;&#29575;&#25191;&#34892;&#22797;&#26434;&#30340;&#39640;&#36895;&#26426;&#21160;&#65292;&#31361;&#26174;&#20102;&#36825;&#20123;&#36164;&#28304;&#21463;&#38480;&#21046;&#30340;&#29983;&#29289;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#20107;&#20214;&#39537;&#21160;&#30828;&#20214;&#36880;&#28176;&#25104;&#20026;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#23454;&#29616;&#22797;&#26434;&#35270;&#35273;&#20219;&#21153;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#23548;&#33322;&#21644;&#36991;&#38556;&#21253;&#25324;&#20960;&#20010;&#29420;&#31435;&#20294;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#22914;&#20809;&#27969;&#20272;&#35745;&#12289;&#28145;&#24230;&#20272;&#35745;&#12289;&#21516;&#26102;&#23450;&#20301;&#19982;&#24314;&#22270;&#65288;SLAM&#65289;&#12289;&#29289;&#20307;&#26816;&#27979;&#21644;&#35782;&#21035;&#12290;&#20026;&#20102;&#30830;&#20445;&#36825;&#20123;&#20219;&#21153;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20182;&#20204;&#24517;&#39035;&#22312;&#21333;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#25968;&#25454;&#38598;&#21482;&#25552;&#20379;&#25152;&#38656;&#25968;&#25454;&#30340;&#36873;&#23450;&#23376;&#38598;&#65292;&#36825;&#20351;&#24471;&#32593;&#32476;&#38388;&#30340;&#19968;&#33268;&#24615;&#38590;&#20197;&#23454;&#29616;&#12290;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#21478;&#19968;&#20010;&#38480;&#21046;&#26159;&#25552;&#20379;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#36776;&#29575;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FEDORA&#65292;
&lt;/p&gt;
&lt;p&gt;
The ability of living organisms to perform complex high speed manoeuvers in flight with a very small number of neurons and an incredibly low failure rate highlights the efficacy of these resource-constrained biological systems. Event-driven hardware has emerged, in recent years, as a promising avenue for implementing complex vision tasks in resource-constrained environments. Vision-based autonomous navigation and obstacle avoidance consists of several independent but related tasks such as optical flow estimation, depth estimation, Simultaneous Localization and Mapping (SLAM), object detection, and recognition. To ensure coherence between these tasks, it is imperative that they be trained on a single dataset. However, most existing datasets provide only a selected subset of the required data. This makes inter-network coherence difficult to achieve. Another limitation of existing datasets is the limited temporal resolution they provide. To address these limitations, we present FEDORA, a 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlpacaFarm&#30340;&#20302;&#25104;&#26412;&#27169;&#25311;&#22120;&#65292;&#35813;&#27169;&#25311;&#22120;&#20026;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35774;&#35745;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#25552;&#20986;&#33258;&#21160;&#35780;&#20272;&#24182;&#25552;&#20379;&#21442;&#32771;&#23454;&#29616;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#12289;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.14387</link><description>&lt;p&gt;
AlpacaFarm: &#19968;&#31181;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#27169;&#25311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. (arXiv:2305.14387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14387
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlpacaFarm&#30340;&#20302;&#25104;&#26412;&#27169;&#25311;&#22120;&#65292;&#35813;&#27169;&#25311;&#22120;&#20026;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35774;&#35745;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#25552;&#20986;&#33258;&#21160;&#35780;&#20272;&#24182;&#25552;&#20379;&#21442;&#32771;&#23454;&#29616;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#12289;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22240;&#20854;&#33391;&#22909;&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#32780;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#24320;&#21457;&#36825;&#20123;LLMs&#38656;&#35201;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#35757;&#32451;&#30340;&#22797;&#26434;&#19988;&#23578;&#19981;&#26126;&#30830;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#23558;&#27492;&#25351;&#20196;&#36319;&#38543;&#36807;&#31243;&#22797;&#21046;&#21644;&#29702;&#35299;&#38754;&#20020;&#19977;&#22823;&#25361;&#25112;&#65306; &#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#65292;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;AlpacaFarm&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#36825;&#26159;&#19968;&#20010;&#20302;&#25104;&#26412;&#30340;&#27169;&#25311;&#22120;&#65292;&#21487;&#29992;&#20110;&#20174;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#12290;&#31532;&#19968;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#20854;&#25104;&#26412;&#27604;&#20247;&#21253;&#24037;&#20316;&#32773;&#20415;&#23452;45&#20493;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#21453;&#39304;&#20855;&#26377;&#39640;&#24230;&#19968;&#33268;&#24615;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#30495;&#23454;&#19990;&#30028;&#20132;&#20114;&#20013;&#33719;&#24471;&#30340;&#20154;&#31867;&#25351;&#20196;&#36827;&#34892;&#39564;&#35777;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20026;&#20960;&#31181;&#20174;&#37197;&#23545;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;PPO&#65292;best-of-n&#65292;expert iteration&#31561;&#65289;&#25552;&#20379;&#20102;&#21442;&#32771;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their ability to follow user instructions well. Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following process faces three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM prompts to simulate human feedback that are 45x cheaper than crowdworkers and display high agreement with humans. Second, we propose an automatic evaluation and validate it against human instructions obtained on real-world interactions. Third, we contribute reference implementations for several methods (PPO, best-of-n, expert iteration, and more) that learn from pairwise feedback
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;GPT-3&#29983;&#25104;&#23450;&#21046;&#21270;&#32451;&#20064;&#65292;&#25945;&#25480;&#25968;&#23398;&#24212;&#29992;&#39064;&#35299;&#20915;&#26041;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#23398;&#29983;&#27169;&#22411;&#30340;&#24369;&#28857;&#24182;&#20197;&#25945;&#32946;&#31185;&#23398;&#21407;&#29702;&#20026;&#22522;&#30784;&#36827;&#34892;&#23450;&#21046;&#21270;&#30340;&#23398;&#20064;&#20307;&#39564;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.14386</link><description>&lt;p&gt;
&#35753;GPT&#25104;&#20026;&#25968;&#23398;&#25945;&#24072;&#65306;&#20351;&#29992;&#23450;&#21046;&#21270;&#32451;&#20064;&#29983;&#25104;&#25945;&#25480;&#25968;&#23398;&#24212;&#29992;&#39064;&#35299;&#20915;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation. (arXiv:2305.14386v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;GPT-3&#29983;&#25104;&#23450;&#21046;&#21270;&#32451;&#20064;&#65292;&#25945;&#25480;&#25968;&#23398;&#24212;&#29992;&#39064;&#35299;&#20915;&#26041;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#23398;&#29983;&#27169;&#22411;&#30340;&#24369;&#28857;&#24182;&#20197;&#25945;&#32946;&#31185;&#23398;&#21407;&#29702;&#20026;&#22522;&#30784;&#36827;&#34892;&#23450;&#21046;&#21270;&#30340;&#23398;&#20064;&#20307;&#39564;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#25968;&#23398;&#24212;&#29992;&#39064;&#35299;&#20915;&#33021;&#21147;&#25552;&#28860;&#20026;&#26356;&#23567;&#12289;&#26356;&#39640;&#25928;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#32771;&#34385;&#23398;&#29983;&#27169;&#22411;&#30340;&#24369;&#28857;&#65292;&#36890;&#36807;&#29983;&#25104;&#19982;&#25945;&#32946;&#31185;&#23398;&#21407;&#29702;&#65288;&#22914;&#30693;&#35782;&#36319;&#36394;&#21644;&#20010;&#24615;&#21270;&#23398;&#20064;&#65289;&#23545;&#40784;&#30340;&#26377;&#38024;&#23545;&#24615;&#30340;&#32451;&#20064;&#26469;&#20419;&#36827;&#23450;&#21046;&#21270;&#30340;&#23398;&#20064;&#20307;&#39564;&#12290;&#25105;&#20204;&#35753;GPT-3&#25104;&#20026;&#25968;&#23398;&#25945;&#24072;&#65292;&#36845;&#20195;&#25191;&#34892;&#20004;&#20010;&#27493;&#39588;&#65306;1&#65289;&#22312;&#30001;GPT&#29983;&#25104;&#30340;&#32451;&#20064;&#20876;&#19978;&#35780;&#20272;&#23398;&#29983;&#27169;&#22411;&#30340;&#24403;&#21069;&#23398;&#20064;&#29366;&#20917;&#65307;2&#65289;&#20351;&#29992;GPT-3&#29983;&#25104;&#30340;&#23450;&#21046;&#21270;&#32451;&#20064;&#26679;&#26412;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#20013;&#27604;LLMs&#65288;&#20363;&#22914;&#65292;GPT-3&#21644;PaLM&#65289;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#21442;&#25968;&#25968;&#37327;&#26126;&#26174;&#36739;&#23569;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#26041;&#27861;&#20013;&#21508;&#20010;&#32452;&#20214;&#36827;&#34892;&#20102;&#32508;&#21512;&#20998;&#26512;&#65292;&#20197;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel approach for distilling math word problem solving capabilities from large language models (LLMs) into smaller, more efficient student models. Our approach is designed to consider the student model's weaknesses and foster a tailored learning experience by generating targeted exercises aligned with educational science principles, such as knowledge tracing and personalized learning. Concretely, we let GPT-3 be a math tutor and run two steps iteratively: 1) assessing the student model's current learning status on a GPT-generated exercise book, and 2) improving the student model by training it with tailored exercise samples generated by GPT-3. Experimental results reveal that our approach outperforms LLMs (e.g., GPT-3 and PaLM) in accuracy across three distinct benchmarks while employing significantly fewer parameters. Furthermore, we provide a comprehensive analysis of the various components within our methodology to substantiate their efficacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#33258;&#21160;&#35782;&#21035;CBCT&#25195;&#25551;&#20013;&#19979;&#39052;&#31649;&#30340;&#23450;&#20301;&#34920;&#29616;&#65292;&#22312;165&#20010;&#19981;&#21516;&#26679;&#26412;&#20013;&#30340;&#34920;&#29616;&#34920;&#26126;&#20102;&#20854;&#21487;&#37325;&#22797;&#24615;&#21644;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.14385</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#19979;&#39052;&#31649;&#33258;&#21160;&#23450;&#20301;&#22312;CBCT&#25968;&#25454;&#38598;&#20013;&#30340;&#21487;&#22797;&#29616;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Reproducibility analysis of automated deep learning based localisation of mandibular canals on a temporal CBCT dataset. (arXiv:2305.14385v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#33258;&#21160;&#35782;&#21035;CBCT&#25195;&#25551;&#20013;&#19979;&#39052;&#31649;&#30340;&#23450;&#20301;&#34920;&#29616;&#65292;&#22312;165&#20010;&#19981;&#21516;&#26679;&#26412;&#20013;&#30340;&#34920;&#29616;&#34920;&#26126;&#20102;&#20854;&#21487;&#37325;&#22797;&#24615;&#21644;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26415;&#21069;&#25918;&#23556;&#23398;&#35782;&#21035;&#19979;&#39052;&#31649;&#23545;&#20110;&#21475;&#33108;&#39052;&#38754;&#22806;&#31185;&#25163;&#26415;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65288;DLS&#65289;&#22312;165&#20010;&#19981;&#21516;&#26679;&#26412;&#30340;&#38181;&#24418;&#26463;&#35745;&#31639;&#26426;&#20307;&#23618;&#25668;&#24433;&#65288;CBCT&#65289;&#25195;&#25551;&#20013;&#30340;&#23450;&#20301;&#34920;&#29616;&#65292;&#24182;&#19982;&#32463;&#39564;&#20016;&#23500;&#30340;&#25918;&#23556;&#31185;&#21307;&#24072;&#27880;&#37322;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;DLS&#30340;&#21487;&#37325;&#22797;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#23545;&#31216;&#24179;&#22343;&#26354;&#32447;&#36317;&#31163;&#65288;SMCD&#65289;&#12289;&#24179;&#22343;&#23545;&#31216;&#34920;&#38754;&#36317;&#31163;&#65288;ASSD&#65289;&#21644;Dice&#30456;&#20284;&#31995;&#25968;&#65288;DSC&#65289;&#26469;&#35780;&#20272;DLS&#30340;&#34920;&#29616;&#12290;&#21478;&#22806;&#65292;&#19977;&#20010;&#19987;&#23478;&#35780;&#20998;&#20004;&#27425;&#20351;&#29992;0-4 Likert&#37327;&#34920;&#35780;&#20272;&#35786;&#26029;&#26377;&#25928;&#24615;&#12290;&#35780;&#20272;&#20102;Likert&#35780;&#20998;&#30340;&#37325;&#22797;&#24615;&#12290;SMCD&#30340;RC&#20026;0.969&#27627;&#31859;&#65292;&#20013;&#20301;&#25968;&#65288;&#22235;&#20998;&#20301;&#36317;&#65289;&#30340;SMCD&#21644;ASSD&#20998;&#21035;&#20026;0.643&#65288;0.186&#65289;&#27627;&#31859;&#21644;0.351&#65288;0.135&#65289;&#27627;&#31859;&#65292;&#24179;&#22343;&#65288;&#26631;&#20934;&#24046;&#65289;&#30340;DSC&#20026;0.548&#12290;
&lt;/p&gt;
&lt;p&gt;
Preoperative radiological identification of mandibular canals is essential for maxillofacial surgery. This study demonstrates the reproducibility of a deep learning system (DLS) by evaluating its localisation performance on 165 heterogeneous cone beam computed tomography (CBCT) scans from 72 patients in comparison to an experienced radiologist's annotations. We evaluated the performance of the DLS using the symmetric mean curve distance (SMCD), the average symmetric surface distance (ASSD), and the Dice similarity coefficient (DSC). The reproducibility of the SMCD was assessed using the within-subject coefficient of repeatability (RC). Three other experts rated the diagnostic validity twice using a 0-4 Likert scale. The reproducibility of the Likert scoring was assessed using the repeatability measure (RM). The RC of SMCD was 0.969 mm, the median (interquartile range) SMCD and ASSD were 0.643 (0.186) mm and 0.351 (0.135) mm, respectively, and the mean (standard deviation) DSC was 0.548
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#25361;&#25112;&#65292;&#26088;&#22312;&#25552;&#39640;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#32570;&#20047;&#31995;&#32479;&#24615;&#21644;&#32467;&#26500;&#21270;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;&#35813;&#25968;&#25454;&#25361;&#25112;&#26088;&#22312;&#26816;&#26597;&#27169;&#22411;&#30340;&#19981;&#33391;&#34892;&#20026;&#65292;&#29305;&#21035;&#26159;&#23545;&#25239;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2305.14384</link><description>&lt;p&gt;
Adversarial Nibbler: &#19968;&#31181;&#25552;&#39640;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#25968;&#25454;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Adversarial Nibbler: A Data-Centric Challenge for Improving the Safety of Text-to-Image Models. (arXiv:2305.14384v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#25361;&#25112;&#65292;&#26088;&#22312;&#25552;&#39640;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#32570;&#20047;&#31995;&#32479;&#24615;&#21644;&#32467;&#26500;&#21270;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;&#35813;&#25968;&#25454;&#25361;&#25112;&#26088;&#22312;&#26816;&#26597;&#27169;&#22411;&#30340;&#19981;&#33391;&#34892;&#20026;&#65292;&#29305;&#21035;&#26159;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#30001;&#20110;&#35745;&#31639;&#33021;&#21147;&#21644;&#25968;&#25454;&#37327;&#30340;&#25193;&#22823;&#65292;&#29983;&#25104;&#24335;AI&#38761;&#21629;&#25512;&#21160;&#20102;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#30340;&#24191;&#27867;&#39044;&#35757;&#32451;&#12290;&#20855;&#22791;&#20135;&#29983;&#36924;&#30495;&#32780;&#23500;&#26377;&#21019;&#36896;&#21147;&#30340;&#20869;&#23481;&#65292;&#20363;&#22914;DALL-E&#65292;MidJourney&#65292;Imagen&#25110;Stable Diffusion&#30340;&#36825;&#20123;T2I&#27169;&#22411;&#24050;&#32463;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#21463;&#20247;&#12290;&#28982;&#32780;&#65292;&#22240;&#20026;&#39044;&#35757;&#32451;&#25152;&#20351;&#29992;&#30340;&#20114;&#32852;&#32593;&#29228;&#21462;&#30340;&#26410;&#32463;&#31579;&#36873;&#30340;&#25968;&#25454;&#38598;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#65292;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#28508;&#22312;&#30340;&#36896;&#25104;&#24191;&#27867;&#20260;&#23475;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#29983;&#25104;&#20855;&#26377;&#26292;&#21147;&#12289;&#24615;&#26292;&#21147;&#12289;&#20559;&#35265;&#21644;&#36140;&#25439;&#24615;&#30340;&#21051;&#26495;&#21360;&#35937;&#30340;&#22270;&#20687;&#12290;&#23613;&#31649;&#23384;&#22312;&#27492;&#31867;&#39118;&#38505;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#31995;&#32479;&#24615;&#21644;&#32467;&#26500;&#21270;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#26469;&#26816;&#26597;&#27169;&#22411;&#34892;&#20026;&#65292;&#29305;&#21035;&#26159;&#23545;&#25915;&#20987;&#29616;&#26377;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#35780;&#20272;&#23433;&#20840;&#24615;&#30340;&#19968;&#20010;&#20856;&#22411;&#29942;&#39048;&#22312;&#20110;&#23454;&#29616;&#35780;&#20272;&#38598;&#21512;&#20013;&#21508;&#31181;&#31867;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#23454;&#20363;&#30340;&#24191;&#27867;&#35206;&#30422;&#29575;&#65292;&#21363;&#30830;&#23450;&#8220;&#26410;&#30693;&#30340;&#26410;&#30693;&#8221;&#25110;&#21333;&#20010;&#25915;&#20987;&#23454;&#20363;&#19981;&#36275;&#20197;&#23637;&#31034;&#27169;&#22411;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generative AI revolution in recent years has been spurred by an expansion in compute power and data quantity, which together enable extensive pre-training of powerful text-to-image (T2I) models. With their greater capabilities to generate realistic and creative content, these T2I models like DALL-E, MidJourney, Imagen or Stable Diffusion are reaching ever wider audiences. Any unsafe behaviors inherited from pretraining on uncurated internet-scraped datasets thus have the potential to cause wide-reaching harm, for example, through generated images which are violent, sexually explicit, or contain biased and derogatory stereotypes. Despite this risk of harm, we lack systematic and structured evaluation datasets to scrutinize model behavior, especially adversarial attacks that bypass existing safety filters. A typical bottleneck in safety evaluation is achieving a wide coverage of different types of challenging examples in the evaluation set, i.e., identifying 'unknown unknowns' or lon
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#21270;&#28151;&#21512;&#27169;&#22411;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#20197;&#21450;&#22522;&#20110;&#29983;&#25104;&#36807;&#31243;&#30340;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#30340;&#20998;&#31867;&#35299;&#37322;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;&#31867;&#21035;&#34920;&#31034;&#21644;&#29305;&#24449;&#38598;&#21512;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#21050;&#28608;&#19979;&#65292;&#25903;&#25345;&#38646;-shot&#23398;&#20064;&#65292;&#24182;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.14383</link><description>&lt;p&gt;
&#19968;&#20010;&#38477;&#32500;&#20154;&#31867;&#20998;&#31867;&#30340;&#29702;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Rational Model of Dimension-reduced Human Categorization. (arXiv:2305.14383v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14383
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#21270;&#28151;&#21512;&#27169;&#22411;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#20197;&#21450;&#22522;&#20110;&#29983;&#25104;&#36807;&#31243;&#30340;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#30340;&#20998;&#31867;&#35299;&#37322;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;&#31867;&#21035;&#34920;&#31034;&#21644;&#29305;&#24449;&#38598;&#21512;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#21050;&#28608;&#19979;&#65292;&#25903;&#25345;&#38646;-shot&#23398;&#20064;&#65292;&#24182;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#31185;&#23398;&#20013;&#29616;&#26377;&#30340;&#27169;&#22411;&#36890;&#24120;&#20551;&#35774;&#20154;&#31867;&#22312;&#24515;&#29702;&#31354;&#38388;&#20013;&#36827;&#34892;&#20998;&#32423;&#27010;&#25324;&#34892;&#20026;&#65292;&#20294;&#26159;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#31867;&#21035;&#34920;&#31034;&#21487;&#33021;&#20250;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;&#20154;&#20204;&#19968;&#33324;&#20381;&#36182;&#20110;&#19968;&#32452;&#21487;&#34892;&#20294;&#36275;&#22815;&#30340;&#29305;&#24449;&#26469;&#29702;&#35299;&#22797;&#26434;&#30340;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#21270;&#27010;&#29575;&#20027;&#25104;&#20998;&#28151;&#21512;&#27169;&#22411;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#21516;&#26102;&#23398;&#20064;&#31867;&#21035;&#34920;&#31034;&#21644;&#32463;&#27982;&#30340;&#29305;&#24449;&#38598;&#21512;&#12290;&#35813;&#27169;&#22411;&#25429;&#25417;&#20102;&#20154;&#31867;&#20998;&#31867;&#20013;&#30340;&#32500;&#24230;&#20559;&#24046;&#24182;&#25903;&#25345;&#38646;-shot&#23398;&#20064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20869;&#21033;&#29992;&#29983;&#25104;&#36807;&#31243;&#65292;&#25552;&#20379;&#39640;&#32500;&#21050;&#28608;&#19979;&#26356;&#22909;&#30340;&#20998;&#31867;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#21644;&#34892;&#20026;&#23454;&#39564;&#39564;&#35777;&#20102;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing models in cognitive science typically assume human categorization as graded generalization behavior in a multidimensional psychological space. However, category representations in these models may suffer from the curse of dimensionality in a natural setting. People generally rely on a tractable yet sufficient set of features to understand the complex environment. We propose a rational model of categorization based on a hierarchical mixture of probabilistic principal components, that simultaneously learn category representations and an economical collection of features. The model captures dimensional biases in human categorization and supports zero-shot learning. We further exploit a generative process within a low-dimensional latent space to provide a better account of categorization with high-dimensional stimuli. We validate the model with simulation and behavioral experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#37197;&#23545;&#25968;&#25454;&#23398;&#20064;MCR&#30340;&#26041;&#27861;&#65292;&#21483;&#20570;C-MCR&#65292;&#24182;&#19988;&#22312;&#26032;&#31354;&#38388;&#20013;&#20351;&#29992;&#37325;&#21472;&#27169;&#24577;B&#30340;&#25968;&#25454;&#26469;&#23545;&#40784;&#20004;&#20010;MCR&#12290;&#36890;&#36807;&#36825;&#20010;&#26041;&#27861;&#65292;&#38750;&#37325;&#21472;&#27169;&#24577;&#23545;&#65288;A&#65292;C&#65289;&#20063;&#21487;&#20197;&#20351;&#29992;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2305.14381</link><description>&lt;p&gt;
&#36830;&#25509;&#22810;&#27169;&#24577;&#23545;&#27604;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Connecting Multi-modal Contrastive Representations. (arXiv:2305.14381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#37197;&#23545;&#25968;&#25454;&#23398;&#20064;MCR&#30340;&#26041;&#27861;&#65292;&#21483;&#20570;C-MCR&#65292;&#24182;&#19988;&#22312;&#26032;&#31354;&#38388;&#20013;&#20351;&#29992;&#37325;&#21472;&#27169;&#24577;B&#30340;&#25968;&#25454;&#26469;&#23545;&#40784;&#20004;&#20010;MCR&#12290;&#36890;&#36807;&#36825;&#20010;&#26041;&#27861;&#65292;&#38750;&#37325;&#21472;&#27169;&#24577;&#23545;&#65288;A&#65292;C&#65289;&#20063;&#21487;&#20197;&#20351;&#29992;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23545;&#27604;&#34920;&#31034;&#65288;MCR&#65289;&#23398;&#20064;&#26088;&#22312;&#23558;&#19981;&#21516;&#30340;&#27169;&#24577;&#32534;&#30721;&#21040;&#19968;&#20010;&#35821;&#20041;&#23545;&#40784;&#30340;&#20849;&#20139;&#31354;&#38388;&#20013;&#12290;&#35813;&#33539;&#20363;&#22312;&#21508;&#31181;&#27169;&#24335;&#19979;&#30340;&#22823;&#37327;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25968;&#25454;&#23545;&#30340;&#20381;&#36182;&#38480;&#21046;&#20102;&#20854;&#22312;&#26356;&#22810;&#27169;&#24577;&#19978;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#38656;&#37197;&#23545;&#25968;&#25454;&#23398;&#20064;MCR&#30340;&#35757;&#32451;&#39640;&#25928;&#26041;&#27861;&#65292;&#31216;&#20026;&#36830;&#25509;&#22810;&#27169;&#24577;&#23545;&#27604;&#34920;&#31034;&#65288;C-MCR&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#65288;A&#65292;B&#65289;&#21644;&#65288;B&#65292;C&#65289;&#27169;&#24577;&#23545;&#19978;&#39044;&#35757;&#32451;&#20004;&#20010;&#29616;&#26377;&#30340;MCR&#20043;&#21518;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#25237;&#24433;&#21040;&#19968;&#20010;&#26032;&#30340;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#37325;&#21472;&#27169;&#24577;B&#30340;&#25968;&#25454;&#26469;&#22312;&#26032;&#31354;&#38388;&#20013;&#23545;&#40784;&#20004;&#20010;MCR&#12290;&#21516;&#26102;&#65292;&#30001;&#20110;&#27169;&#24577;&#23545;&#65288;A&#65292;B&#65289;&#21644;&#65288;B&#65292;C&#65289;&#22312;&#27599;&#20010;MCR&#20869;&#24050;&#32463;&#23545;&#40784;&#65292;&#22240;&#27492;&#36890;&#36807;&#37325;&#21472;&#27169;&#24577;&#23398;&#20064;&#21040;&#30340;&#36830;&#25509;&#20063;&#21487;&#20197;&#36716;&#31227;&#21040;&#38750;&#37325;&#21472;&#27169;&#24577;&#23545;&#65288;A&#65292;C&#65289;&#12290;&#20026;&#20102;&#21457;&#25381;C-MCR&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#20010;&#35821;&#20041;&#22686;&#24378;&#30340;int
&lt;/p&gt;
&lt;p&gt;
Multi-modal Contrastive Representation (MCR) learning aims to encode different modalities into a semantically aligned shared space. This paradigm shows remarkable generalization ability on numerous downstream tasks across various modalities. However, the reliance on massive high-quality data pairs limits its further development on more modalities. This paper proposes a novel training-efficient method for learning MCR without paired data called Connecting Multi-modal Contrastive Representations (C-MCR). Specifically, given two existing MCRs pre-trained on (A, B) and (B, C) modality pairs, we project them to a new space and use the data from the overlapping modality B to aligning the two MCRs in the new space. Meanwhile, since the modality pairs (A, B) and (B, C) are already aligned within each MCR, the connection learned by overlapping modality can also be transferred to non-overlapping modality pair (A, C). To unleash the potential of C-MCR, we further introduce a semantic-enhanced int
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#32858;&#21512;&#22836;&#27880;&#24847;&#21147;(Grouped Head Attention)&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#32452;&#32422;&#26463;&#36827;&#34892;&#35757;&#32451;&#65292;&#20026;&#27880;&#24847;&#21147;&#22836;&#36827;&#34892;&#20998;&#32452;&#65292;&#20854;&#20013;&#27599;&#20010;&#32452;&#19987;&#27880;&#20110;&#19968;&#20010;&#37325;&#35201;&#32780;&#29420;&#29305;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;MHA&#30340;&#20887;&#20313;&#24615;&#21644;&#36807;&#24230;&#21442;&#25968;&#21270;&#38382;&#39064;&#65292;&#24182;&#23548;&#33268;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;MHA&#65292;&#36827;&#32780;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.14380</link><description>&lt;p&gt;
&#23547;&#25214;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#25903;&#26609;
&lt;/p&gt;
&lt;p&gt;
Finding the Pillars of Strength for Multi-Head Attention. (arXiv:2305.14380v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#32858;&#21512;&#22836;&#27880;&#24847;&#21147;(Grouped Head Attention)&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#32452;&#32422;&#26463;&#36827;&#34892;&#35757;&#32451;&#65292;&#20026;&#27880;&#24847;&#21147;&#22836;&#36827;&#34892;&#20998;&#32452;&#65292;&#20854;&#20013;&#27599;&#20010;&#32452;&#19987;&#27880;&#20110;&#19968;&#20010;&#37325;&#35201;&#32780;&#29420;&#29305;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;MHA&#30340;&#20887;&#20313;&#24615;&#21644;&#36807;&#24230;&#21442;&#25968;&#21270;&#38382;&#39064;&#65292;&#24182;&#23548;&#33268;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;MHA&#65292;&#36827;&#32780;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;(Multi-Head Attention, MHA)&#30340;&#19968;&#20123;&#38382;&#39064;&#65292;&#20363;&#22914;&#20887;&#20313;&#24615;&#21644;&#36807;&#24230;&#21442;&#25968;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MHA&#30340;&#22836;&#26368;&#21021;&#35774;&#35745;&#20026;&#20174;&#19981;&#21516;&#30340;&#34920;&#24449;&#23376;&#31354;&#38388;&#20013;&#20851;&#27880;&#20449;&#24687;&#65292;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#19968;&#20123;&#27880;&#24847;&#21147;&#22836;&#21487;&#33021;&#23398;&#20064;&#31867;&#20284;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20462;&#21098;&#26469;&#25552;&#39640;&#25928;&#29575;&#32780;&#19981;&#20250;&#25439;&#23475;&#24615;&#33021;&#12290;&#21463;&#26368;&#23567;&#20887;&#20313;&#29305;&#24449;&#36873;&#25321;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20551;&#35774;&#32858;&#28966;&#20110;&#26368;&#20855;&#20195;&#34920;&#24615;&#21644;&#29420;&#29305;&#24615;&#30340;&#29305;&#24449;&#65292;&#24182;&#26368;&#23567;&#21270;&#36164;&#28304;&#28040;&#32791;&#65292;&#21487;&#20197;&#32531;&#35299;&#19978;&#36848;&#38382;&#39064;&#65292;&#24182;&#23548;&#33268;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;MHA&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32858;&#21512;&#22836;&#27880;&#24847;&#21147;(Grouped Head Attention)&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#32452;&#32422;&#26463;&#36827;&#34892;&#35757;&#32451;&#65292;&#20026;&#27880;&#24847;&#21147;&#22836;&#36827;&#34892;&#20998;&#32452;&#65292;&#20854;&#20013;&#27599;&#20010;&#32452;&#19987;&#27880;&#20110;&#19968;&#20010;&#37325;&#35201;&#32780;&#29420;&#29305;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#25237;&#31080;&#20445;&#30041;&#31243;&#24207;(Voting-to-Stay)&#65292;&#20197;&#21024;&#38500;&#20887;&#20313;&#22836;&#65292;&#20174;&#32780;&#23454;&#29616;&#20855;&#26377;&#26356;&#36731;&#37327;&#32423;&#26435;&#37325;&#30340;&#36716;&#25442;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#30693;&#21517;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#28040;&#34701;&#30740;&#31350;&#25552;&#20379;&#20102;&#25903;&#25345;&#24615;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have revealed some issues of Multi-Head Attention (MHA), e.g., redundancy and over-parameterization. Specifically, the heads of MHA were originally designed to attend to information from different representation subspaces, whereas prior studies found that some attention heads likely learn similar features and can be pruned without harming performance. Inspired by the minimum-redundancy feature selection, we assume that focusing on the most representative and distinctive features with minimum resources can mitigate the above issues and lead to more effective and efficient MHAs. In particular, we propose Grouped Head Attention, trained with a self-supervised group constraint that group attention heads, where each group focuses on an essential but distinctive feature subset. We additionally propose a Voting-to-Stay procedure to remove redundant heads, thus achieving a transformer with lighter weights. Moreover, our method achieves significant performance gains on three well
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;CNN-LSTM&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#32929;&#24066;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#23454;&#26102;&#25968;&#25454;&#19981;&#20934;&#30830;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14378</link><description>&lt;p&gt;
&#20351;&#29992;CNN-LSTM&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#32929;&#24066;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Predicting Stock Market Time-Series Data using CNN-LSTM Neural Network Model. (arXiv:2305.14378v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14378
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;CNN-LSTM&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#32929;&#24066;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#23454;&#26102;&#25968;&#25454;&#19981;&#20934;&#30830;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32929;&#24066;&#36890;&#24120;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20195;&#34920;&#20102;&#23545;&#20225;&#19994;&#30340;&#25152;&#26377;&#26435;&#32034;&#36180;&#12290;&#22914;&#26524;&#20844;&#21496;&#27809;&#26377;&#36275;&#22815;&#30340;&#32929;&#31080;&#65292;&#23601;&#26080;&#27861;&#22312;&#37329;&#34701;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#39044;&#27979;&#20844;&#21496;&#32929;&#31080;&#24066;&#22330;&#34920;&#29616;&#20960;&#20046;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#27599;&#27425;&#20844;&#21496;&#32929;&#31080;&#20215;&#26684;&#37117;&#22312;&#21464;&#21270;&#32780;&#19981;&#26159;&#24658;&#23450;&#30340;&#65292;&#22240;&#27492;&#24456;&#38590;&#30830;&#23450;&#32929;&#31080;&#25968;&#25454;&#12290;&#20294;&#26159;&#65292;&#22914;&#26524;&#24050;&#30693;&#20844;&#21496;&#22312;&#32929;&#31080;&#24066;&#22330;&#19978;&#30340;&#26089;&#26399;&#34920;&#29616;&#65292;&#25105;&#20204;&#21487;&#20197;&#36319;&#36394;&#25968;&#25454;&#24182;&#21521;&#32929;&#31080;&#25345;&#26377;&#20154;&#25552;&#20379;&#39044;&#27979;&#65292;&#20197;&#20415;&#26126;&#26234;&#22320;&#20915;&#23450;&#22788;&#29702;&#20844;&#21496;&#32929;&#31080;&#12290;&#20026;&#27492;&#65292;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24050;&#34987;&#21457;&#26126;&#65292;&#20294;&#30001;&#20110;&#35768;&#22810;&#21407;&#22240;&#65288;&#22914;&#32570;&#20047;&#39640;&#32423;&#24211;&#12289;&#20351;&#29992;&#23454;&#26102;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#30340;&#19981;&#20934;&#30830;&#24615;&#31561;&#65289;&#65292;&#23427;&#20204;&#24182;&#27809;&#26377;&#25104;&#21151;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#35782;&#21035;&#32929;&#31080;&#25968;&#25454;&#30340;&#29305;&#24449;&#24182;&#21019;&#24314;CNN-LSTM&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#20934;&#30830;&#22320;&#39044;&#27979;&#32929;&#24066;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stock market is often important as it represents the ownership claims on businesses. Without sufficient stocks, a company cannot perform well in finance. Predicting a stock market performance of a company is nearly hard because every time the prices of a company stock keeps changing and not constant. So, its complex to determine the stock data. But if the previous performance of a company in stock market is known, then we can track the data and provide predictions to stockholders in order to wisely take decisions on handling the stocks to a company. To handle this, many machine learning models have been invented but they didn't succeed due to many reasons like absence of advanced libraries, inaccuracy of model when made to train with real time data and much more. So, to track the patterns and the features of data, a CNN-LSTM Neural Network can be made. Recently, CNN is now used in Natural Language Processing (NLP) based applications, so by identifying the features from stock data and c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISCS&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26080;&#38480;&#23398;&#20064;&#19981;&#21516;&#30340;&#36830;&#32493;&#25216;&#33021;&#65292;&#22312;MuJoCo Ant&#26426;&#22120;&#20154;&#25511;&#21046;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20854;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#20855;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14377</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#22320;&#22312;&#29699;&#38754;&#19978;&#23398;&#20064;&#36830;&#32493;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Discovery of Continuous Skills on a Sphere. (arXiv:2305.14377v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISCS&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26080;&#38480;&#23398;&#20064;&#19981;&#21516;&#30340;&#36830;&#32493;&#25216;&#33021;&#65292;&#22312;MuJoCo Ant&#26426;&#22120;&#20154;&#25511;&#21046;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20854;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#20855;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30740;&#31350;&#20102;&#35768;&#22810;&#31181;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#19981;&#21516;&#30340;&#25216;&#33021;&#24110;&#21161;&#26426;&#22120;&#20154;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#34892;&#20026;&#65292;&#32780;&#19981;&#38656;&#35201;&#22806;&#37096;&#22870;&#21169;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#23398;&#20064;&#30340;&#26159;&#26377;&#38480;&#25968;&#37327;&#30340;&#31163;&#25955;&#25216;&#33021;&#65292;&#22240;&#27492;&#23427;&#20204;&#23637;&#29616;&#20986;&#30340;&#34892;&#20026;&#22810;&#26679;&#24615;&#20063;&#24456;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#8220;&#22312;&#29699;&#38754;&#19978;&#23398;&#20064;&#36830;&#32493;&#25216;&#33021;&#30340;&#21457;&#29616;&#8221;&#65292;&#21487;&#20197;&#23398;&#20064;&#26080;&#38480;&#31181;&#19981;&#21516;&#30340;&#25216;&#33021;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#25216;&#33021;&#26159;&#36890;&#36807;&#26368;&#22823;&#21270;&#25216;&#33021;&#21644;&#29366;&#24577;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#30340;&#65292;&#32780;&#27599;&#31181;&#25216;&#33021;&#37117;&#23545;&#24212;&#20110;&#29699;&#38754;&#19978;&#30340;&#19968;&#20010;&#36830;&#32493;&#20540;&#12290;&#30001;&#20110;&#25216;&#33021;&#22312;DISCS&#20013;&#30340;&#34920;&#31034;&#26159;&#36830;&#32493;&#30340;&#65292;&#25152;&#20197;&#21487;&#20197;&#23398;&#20064;&#26080;&#38480;&#22810;&#31181;&#19981;&#21516;&#30340;&#25216;&#33021;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#26041;&#27861;&#21644;DISCS&#24212;&#29992;&#20110;MuJoCo Ant&#26426;&#22120;&#20154;&#25511;&#21046;&#29615;&#22659;&#20013;&#65292;&#24182;&#23637;&#31034;&#20102;DISCS&#21487;&#20197;&#27604;&#20854;&#20182;&#26041;&#27861;&#23398;&#20064;&#21040;&#26356;&#22810;&#30340;&#22810;&#26679;&#21270;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, methods for learning diverse skills to generate various behaviors without external rewards have been actively studied as a form of unsupervised reinforcement learning. However, most of the existing methods learn a finite number of discrete skills, and thus the variety of behaviors that can be exhibited with the learned skills is limited. In this paper, we propose a novel method for learning potentially an infinite number of different skills, which is named discovery of continuous skills on a sphere (DISCS). In DISCS, skills are learned by maximizing mutual information between skills and states, and each skill corresponds to a continuous value on a sphere. Because the representations of skills in DISCS are continuous, infinitely diverse skills could be learned. We examine existing methods and DISCS in the MuJoCo Ant robot control environments and show that DISCS can learn much more diverse skills than the other methods.
&lt;/p&gt;</description></item><item><title>PTGB &#26159;&#19968;&#20010;&#39044;&#35757;&#32451; GNN &#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#20869;&#24515;&#33041;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#20805;&#20998;&#21033;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.14376</link><description>&lt;p&gt;
PTGB&#65306;&#39044;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#33041;&#32593;&#32476;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
PTGB: Pre-Train Graph Neural Networks for Brain Network Analysis. (arXiv:2305.14376v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14376
&lt;/p&gt;
&lt;p&gt;
PTGB &#26159;&#19968;&#20010;&#39044;&#35757;&#32451; GNN &#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#20869;&#24515;&#33041;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#20805;&#20998;&#21033;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22823;&#33041;&#26159;&#31070;&#32463;&#29983;&#29289;&#31995;&#32479;&#30340;&#20013;&#26530;&#26530;&#32445;&#65292;&#22312;&#22797;&#26434;&#30340;&#26041;&#24335;&#19979;&#25511;&#21046;&#34892;&#20026;&#21644;&#35748;&#30693;&#12290;&#26368;&#36817;&#31070;&#32463;&#31185;&#23398;&#21644;&#31070;&#32463;&#24433;&#20687;&#20998;&#26512;&#26041;&#38754;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#23545;&#24863;&#20852;&#36259;&#30340;&#33041;&#21306;&#22495;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#23427;&#20204;&#23545;&#31070;&#32463;&#21457;&#32946;&#21644;&#30142;&#30149;&#35786;&#26029;&#30340;&#24433;&#21709;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#20998;&#26512;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#28145;&#24230;&#27169;&#22411;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#34987;&#24212;&#29992;&#20110;&#33041;&#32593;&#32476;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#28145;&#24230;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#30001;&#20110;&#25968;&#25454;&#37319;&#38598;&#30340;&#22797;&#26434;&#24615;&#21644;&#20849;&#20139;&#38480;&#21046;&#31561;&#21407;&#22240;&#65292;&#33041;&#32593;&#32476;&#25968;&#25454;&#38598;&#20013;&#26631;&#35760;&#25968;&#25454;&#36890;&#24120;&#24456;&#23569;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#29616;&#26377;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; PTGB&#65292;&#19968;&#31181; GNN &#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#25429;&#33719;&#20869;&#22312;&#30340;&#33041;&#32593;&#32476;&#32467;&#26500;&#65292;&#26080;&#35770;&#20020;&#24202;&#32467;&#26524;&#22914;&#20309;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;PTGB &#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;1&#65289;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#38024;&#23545;&#33041;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The human brain is the central hub of the neurobiological system, controlling behavior and cognition in complex ways. Recent advances in neuroscience and neuroimaging analysis have shown a growing interest in the interactions between brain regions of interest (ROIs) and their impact on neural development and disorder diagnosis. As a powerful deep model for analyzing graph-structured data, Graph Neural Networks (GNNs) have been applied for brain network analysis. However, training deep models requires large amounts of labeled data, which is often scarce in brain network datasets due to the complexities of data acquisition and sharing restrictions. To make the most out of available training data, we propose PTGB, a GNN pre-training framework that captures intrinsic brain network structures, regardless of clinical outcomes, and is easily adaptable to various downstream tasks. PTGB comprises two key components: (1) an unsupervised pre-training technique designed specifically for brain netw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#33410;&#28857;&#25490;&#24207;&#26041;&#27861;&#65288;MGL2Rank&#65289;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#36947;&#36335;&#32593;&#32476;&#30340;&#20016;&#23500;&#29305;&#24449;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.14375</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#22270;&#34701;&#21512;&#30340;&#36947;&#36335;&#32593;&#32476;&#33410;&#28857;&#37325;&#35201;&#24615;&#25490;&#24207;&#26041;&#27861;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to Rank the Importance of Nodes in Road Networks Based on Multi-Graph Fusion. (arXiv:2305.14375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#33410;&#28857;&#25490;&#24207;&#26041;&#27861;&#65288;MGL2Rank&#65289;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#36947;&#36335;&#32593;&#32476;&#30340;&#20016;&#23500;&#29305;&#24449;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22478;&#24066;&#35268;&#21010;&#39046;&#22495;&#20013;&#65292;&#35782;&#21035;&#20855;&#26377;&#24378;&#20256;&#25773;&#33021;&#21147;&#30340;&#37325;&#35201;&#33410;&#28857;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35838;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#33410;&#28857;&#37325;&#35201;&#24615;&#30340;&#26041;&#27861;&#20165;&#32771;&#34385;&#25299;&#25169;&#20449;&#24687;&#21644;&#20132;&#36890;&#27969;&#37327;&#65292;&#24573;&#30053;&#20102;&#36947;&#36335;&#32593;&#32476;&#30340;&#22810;&#26679;&#24615;&#29305;&#24449;&#65292;&#22914;&#36710;&#36947;&#25968;&#37327;&#21644;&#36947;&#36335;&#27573;&#30340;&#24179;&#22343;&#36895;&#24230;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#33410;&#28857;&#25490;&#24207;&#26041;&#27861;&#65288;MGL2Rank&#65289;&#65292;&#23427;&#38598;&#25104;&#20102;&#36947;&#36335;&#32593;&#32476;&#30340;&#20016;&#23500;&#29305;&#24449;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#20010;&#37319;&#26679;&#31639;&#27861;&#65288;MGWalk&#65289;&#65292;&#21033;&#29992;&#22810;&#22270;&#34701;&#21512;&#26469;&#24314;&#31435;&#22522;&#20110;&#23646;&#24615;&#30340;&#36947;&#36335;&#27573;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23884;&#20837;&#27169;&#22359;&#65292;&#29992;&#20110;&#23398;&#20064;&#27599;&#20010;&#36947;&#36335;&#27573;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#26368;&#21518;&#65292;&#24471;&#21040;&#30340;&#33410;&#28857;&#34920;&#31034;&#29992;&#20110;&#23398;&#20064;&#36947;&#36335;&#27573;&#30340;&#37325;&#35201;&#24615;&#25490;&#24207;&#12290;&#25105;&#20204;&#22312;&#20013;&#22269;&#27784;&#38451;&#24066;&#21306;&#22495;&#36947;&#36335;&#32593;&#32476;&#19978;&#36827;&#34892;&#20102;&#20223;&#30495;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;MGL2Rank&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MGL2Rank&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#33410;&#28857;&#25490;&#24207;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying important nodes with strong propagation capabilities in road networks is a significant topic in the field of urban planning. However, existing methods for evaluating nodes importance consider only topological information and traffic volumes, ignoring the diversity of characteristics in road networks, such as the number of lanes and average speed of road segments, limiting their performance. To address this issue, this paper proposes a graph learning-based node ranking method (MGL2Rank) that integrates the rich characteristics of the road network. In this method, we first develop a sampling algorithm (MGWalk) that utilizes multi-graph fusion to establish association between road segments based on their attributes. Then, an embedding module is proposed to learn latent representation for each road segment. Finally, the obtained node representation is used to learn importance ranking of road segments. We conduct simulation experiments on the regional road network of Shenyang ci
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#30340;&#38598;&#25104;&#21322;&#30417;&#30563;&#33258;&#36866;&#24212;&#20849;&#25391;&#29702;&#35770;&#27169;&#22411;&#65288;SSL-ART&#65289;&#65292;&#20854;&#37319;&#29992;&#20102;&#38750;&#30417;&#30563;&#27169;&#31946;ART&#32593;&#32476;&#21644;&#30417;&#30563;&#27169;&#31946;ARTMAP&#32467;&#26500;&#23454;&#29616;&#23545;&#26410;&#26631;&#35760;&#26679;&#26412;&#21644;&#26631;&#35760;&#26679;&#26412;&#30340;&#23398;&#20064;&#21644;&#26144;&#23556;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#19968;&#23545;&#22810;&#30340;&#26144;&#23556;&#26041;&#26696;&#20943;&#23569;&#20887;&#20313;&#21407;&#22411;&#33410;&#28857;&#24182;&#26368;&#23567;&#21270;&#22122;&#22768;&#26679;&#26412;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.14373</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#35299;&#37322;&#33021;&#21147;&#30340;&#38598;&#25104;&#21322;&#30417;&#30563;&#33258;&#36866;&#24212;&#20849;&#25391;&#29702;&#35770;&#27169;&#22411;&#29992;&#20110;&#27169;&#24335;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
An Ensemble Semi-Supervised Adaptive Resonance Theory Model with Explanation Capability for Pattern Classification. (arXiv:2305.14373v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14373
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#30340;&#38598;&#25104;&#21322;&#30417;&#30563;&#33258;&#36866;&#24212;&#20849;&#25391;&#29702;&#35770;&#27169;&#22411;&#65288;SSL-ART&#65289;&#65292;&#20854;&#37319;&#29992;&#20102;&#38750;&#30417;&#30563;&#27169;&#31946;ART&#32593;&#32476;&#21644;&#30417;&#30563;&#27169;&#31946;ARTMAP&#32467;&#26500;&#23454;&#29616;&#23545;&#26410;&#26631;&#35760;&#26679;&#26412;&#21644;&#26631;&#35760;&#26679;&#26412;&#30340;&#23398;&#20064;&#21644;&#26144;&#23556;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#19968;&#23545;&#22810;&#30340;&#26144;&#23556;&#26041;&#26696;&#20943;&#23569;&#20887;&#20313;&#21407;&#22411;&#33410;&#28857;&#24182;&#26368;&#23567;&#21270;&#22122;&#22768;&#26679;&#26412;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#27169;&#22411;&#38656;&#35201;&#22797;&#26434;&#30340;&#32467;&#26500;&#21644;&#36845;&#20195;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#21450;&#38754;&#20020;&#35299;&#37322;&#20854;&#39044;&#27979;&#32467;&#26524;&#32473;&#29992;&#25143;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#30340;SSL&#27169;&#22411;&#65292;&#20351;&#29992;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;Adaptive Resonance Theory&#65288;ART&#65289;&#32593;&#32476;&#31995;&#21015;&#65292;&#31216;&#20026;SSL-ART&#12290;&#39318;&#20808;&#65292;SSL-ART&#37319;&#29992;&#38750;&#30417;&#30563;&#27169;&#31946;ART&#32593;&#32476;&#26469;&#21019;&#24314;&#19968;&#20123;&#21407;&#22411;&#33410;&#28857;&#65292;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#23427;&#21033;&#29992;&#30417;&#30563;&#27169;&#31946;ARTMAP&#32467;&#26500;&#65292;&#20351;&#29992;&#26631;&#35760;&#30340;&#26679;&#26412;&#23558;&#24050;&#24314;&#31435;&#30340;&#21407;&#22411;&#33410;&#28857;&#26144;&#23556;&#21040;&#30446;&#26631;&#31867;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#19968;&#23545;&#22810;&#30340;&#26144;&#23556;&#26041;&#26696;&#65292;&#23558;&#21407;&#22411;&#33410;&#28857;&#19982;&#19968;&#20010;&#20197;&#19978;&#30340;&#31867;&#26631;&#31614;&#20851;&#32852;&#12290;SSL-ART&#30340;&#20027;&#35201;&#20248;&#28857;&#21253;&#25324;&#65306;&#65288;i&#65289;&#25191;&#34892;&#22312;&#32447;&#23398;&#20064;&#65292;&#65288;ii&#65289;&#36890;&#36807;OtM&#26144;&#23556;&#26041;&#26696;&#20943;&#23569;&#20887;&#20313;&#21407;&#22411;&#33410;&#28857;&#24182;&#26368;&#23567;&#21270;&#22122;&#22768;&#26679;&#26412;&#30340;&#24433;&#21709;&#65292;&#65288;iii&#65289;&#25552;&#20379;&#35299;&#37322;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Most semi-supervised learning (SSL) models entail complex structures and iterative training processes as well as face difficulties in interpreting their predictions to users. To address these issues, this paper proposes a new interpretable SSL model using the supervised and unsupervised Adaptive Resonance Theory (ART) family of networks, which is denoted as SSL-ART. Firstly, SSL-ART adopts an unsupervised fuzzy ART network to create a number of prototype nodes using unlabeled samples. Then, it leverages a supervised fuzzy ARTMAP structure to map the established prototype nodes to the target classes using labeled samples. Specifically, a one-to-many (OtM) mapping scheme is devised to associate a prototype node with more than one class label. The main advantages of SSL-ART include the capability of: (i) performing online learning, (ii) reducing the number of redundant prototype nodes through the OtM mapping scheme and minimizing the effects of noisy samples, and (iii) providing an explan
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20998;&#26512;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#12289;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#21644;&#24357;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;dMRI&#65289;&#25968;&#25454;&#65292;&#25104;&#21151;&#39044;&#27979;&#36229;&#36807;80%&#30340;&#31934;&#31070;&#20998;&#35010;&#30151;&#65292;&#21487;&#20197;&#28508;&#22312;&#29992;&#20110;&#35813;&#30142;&#30149;&#30340;&#35786;&#26029;&#12290;</title><link>http://arxiv.org/abs/2305.14370</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#39044;&#27979;&#21644;&#35786;&#26029;&#31934;&#31070;&#20998;&#35010;&#30151;&#20013;&#30340;&#20316;&#29992;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on the Role of Artificial Intelligence in the Prediction and Diagnosis of Schizophrenia. (arXiv:2305.14370v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14370
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20998;&#26512;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#12289;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#21644;&#24357;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;dMRI&#65289;&#25968;&#25454;&#65292;&#25104;&#21151;&#39044;&#27979;&#36229;&#36807;80%&#30340;&#31934;&#31070;&#20998;&#35010;&#30151;&#65292;&#21487;&#20197;&#28508;&#22312;&#29992;&#20110;&#35813;&#30142;&#30149;&#30340;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#34987;&#29992;&#20110;&#25512;&#26029;&#20154;&#31867;&#30142;&#30149;&#21644;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#36817;&#20284;&#32467;&#35770;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#20998;&#26512;&#25968;&#25454;&#65292;&#24182;&#20135;&#29983;&#26356;&#22909;&#12289;&#26356;&#21487;&#38752;&#30340;&#32467;&#26524;&#12290;&#31934;&#31070;&#20998;&#35010;&#30151;&#26159;&#19968;&#31181;&#24930;&#24615;&#31934;&#31070;&#38556;&#30861;&#65292;&#24433;&#21709;&#25968;&#30334;&#19975;&#20154;&#30340;&#29983;&#27963;&#12290;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#34987;&#24320;&#21457;&#29992;&#20110;&#39044;&#27979;&#21644;&#39044;&#38450;&#36825;&#31181;&#30142;&#30149;&#65292;&#24182;&#26377;&#21487;&#33021;&#29992;&#20110;&#35786;&#26029;&#24739;&#26377;&#35813;&#30142;&#30149;&#30340;&#20010;&#20154;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#22238;&#39038;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#26816;&#27979;&#21644;&#39044;&#27979;&#31934;&#31070;&#20998;&#35010;&#30151;&#30340;&#35770;&#25991;&#65292;&#20351;&#29992;&#20102;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#12289;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#21644;&#24357;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;dMRI&#65289;&#12290;&#25105;&#20204;&#36873;&#25321;&#26816;&#32034;&#31574;&#30053;&#65292;&#35780;&#20272;&#20102;2019&#24180;&#33267;2022&#24180;&#30340;&#21313;&#31687;&#35770;&#25991;&#12290;&#25152;&#26377;&#30740;&#31350;&#37117;&#25104;&#21151;&#39044;&#27979;&#20102;80%&#20197;&#19978;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#36825;&#20123;&#30740;&#31350;&#30340;&#25688;&#35201;&#24182;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning is employed in healthcare to draw approximate conclusions regarding human diseases and mental health problems. Compared to older traditional methods, it can help to analyze data more efficiently and produce better and more dependable results. Millions of people are affected by schizophrenia, which is a chronic mental disorder that can significantly impact their lives. Many machine learning algorithms have been developed to predict and prevent this disease, and they can potentially be implemented in the diagnosis of individuals who have it. This survey aims to review papers that have focused on the use of deep learning to detect and predict schizophrenia using EEG signals, functional magnetic resonance imaging (fMRI), and diffusion magnetic resonance imaging (dMRI). With our chosen search strategy, we assessed ten publications from 2019 to 2022. All studies achieved successful predictions of more than 80%. This review provides summaries of the studies and compares their
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#39034;&#24207;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;SVAE&#65289;&#23398;&#20064;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#20302;&#32500;&#36924;&#36817;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NODE&#65289;&#34920;&#31034;&#28508;&#22312;&#21160;&#21147;&#23398;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#24179;&#28369;&#21160;&#21147;&#23398;&#21487;&#20197;&#27604;&#32463;&#20856;&#26041;&#27861;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#35748;&#30693;&#36807;&#31243;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#20219;&#21153;&#30456;&#20851;&#33041;&#21306;&#30340;&#31354;&#38388;&#23450;&#20301;&#33021;&#21147;&#24471;&#21040;&#20102;&#26126;&#26174;&#25913;&#21892;&#12290;&#38750;&#32447;&#24615;&#25237;&#24433;&#21040;&#28508;&#22312;&#31354;&#38388;&#21487;&#20197;&#22686;&#24378;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14369</link><description>&lt;p&gt;
&#20174;&#25972;&#20010;&#22823;&#33041;&#25968;&#25454;&#20013;&#23398;&#20064;&#20302;&#32500;&#21160;&#24577;&#21487;&#20197;&#25552;&#39640;&#20219;&#21153;&#25429;&#25417;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning low-dimensional dynamics from whole-brain data improves task capture. (arXiv:2305.14369v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#39034;&#24207;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;SVAE&#65289;&#23398;&#20064;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#20302;&#32500;&#36924;&#36817;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NODE&#65289;&#34920;&#31034;&#28508;&#22312;&#21160;&#21147;&#23398;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#24179;&#28369;&#21160;&#21147;&#23398;&#21487;&#20197;&#27604;&#32463;&#20856;&#26041;&#27861;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#35748;&#30693;&#36807;&#31243;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#20219;&#21153;&#30456;&#20851;&#33041;&#21306;&#30340;&#31354;&#38388;&#23450;&#20301;&#33021;&#21147;&#24471;&#21040;&#20102;&#26126;&#26174;&#25913;&#21892;&#12290;&#38750;&#32447;&#24615;&#25237;&#24433;&#21040;&#28508;&#22312;&#31354;&#38388;&#21487;&#20197;&#22686;&#24378;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#35748;&#30693;&#36807;&#31243;&#21644;&#31934;&#31070;&#38556;&#30861;&#30340;&#31070;&#32463;&#21160;&#21147;&#23398;&#23545;&#20110;&#25105;&#20204;&#29702;&#35299;&#22823;&#33041;&#27963;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#22522;&#20110;&#20307;&#32032;&#30340;&#25972;&#20010;&#22823;&#33041;&#32500;&#24230;&#32422;&#31616;&#25216;&#26415;&#19981;&#33021;&#20805;&#20998;&#25429;&#25417;&#36825;&#20123;&#21160;&#24577;&#65292;&#20135;&#29983;&#30340;&#28508;&#22312;&#26102;&#38388;&#24207;&#21015;&#19982;&#34892;&#20026;&#20219;&#21153;&#20851;&#32852;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#39034;&#24207;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;SVAE&#65289;&#23398;&#20064;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#20302;&#32500;&#36924;&#36817;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NODE&#65289;&#34920;&#31034;&#28508;&#22312;&#21160;&#21147;&#23398;&#31995;&#32479;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#21457;&#29616;&#21487;&#20197;&#27604;&#32463;&#20856;&#26041;&#27861;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#35748;&#30693;&#36807;&#31243;&#30340;&#24179;&#28369;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#23637;&#29616;&#20102;&#23545;&#20219;&#21153;&#30456;&#20851;&#33041;&#21306;&#30340;&#31354;&#38388;&#23450;&#20301;&#33021;&#21147;&#24471;&#21040;&#20102;&#26126;&#26174;&#25913;&#21892;&#65292;&#24182;&#19988;&#20174;fMRI&#36816;&#21160;&#20219;&#21153;&#35760;&#24405;&#20013;&#21457;&#29616;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;&#32467;&#26500;&#65292;&#22914;&#36816;&#21160;&#20307;&#20687;&#22270;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#38750;&#32447;&#24615;&#25237;&#24433;&#21040;&#28508;&#22312;&#31354;&#38388;&#21487;&#20197;&#22686;&#24378;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#36825;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The neural dynamics underlying brain activity are critical to understanding cognitive processes and mental disorders. However, current voxel-based whole-brain dimensionality reduction techniques fall short of capturing these dynamics, producing latent timeseries that inadequately relate to behavioral tasks. To address this issue, we introduce a novel approach to learning low-dimensional approximations of neural dynamics by using a sequential variational autoencoder (SVAE) that represents the latent dynamical system via a neural ordinary differential equation (NODE). Importantly, our method finds smooth dynamics that can predict cognitive processes with accuracy higher than classical methods. Our method also shows improved spatial localization to task-relevant brain regions and identifies well-known structures such as the motor homunculus from fMRI motor task recordings. We also find that non-linear projections to the latent space enhance performance for specific tasks, offering a promi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#24773;&#24863;&#20998;&#26512;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#38271;&#26102;&#38388;&#31383;&#21475;&#20869;&#30340;&#32929;&#31080;&#36235;&#21183;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;&#25216;&#26415;&#32929;&#31080;&#25968;&#25454;&#21644;&#22836;&#26465;&#26032;&#38395;&#25968;&#25454;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2305.14368</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#21644;&#24773;&#24863;&#20998;&#26512;&#25903;&#25345;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Support for Stock Trend Prediction Using Transformers and Sentiment Analysis. (arXiv:2305.14368v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#24773;&#24863;&#20998;&#26512;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#38271;&#26102;&#38388;&#31383;&#21475;&#20869;&#30340;&#32929;&#31080;&#36235;&#21183;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;&#25216;&#26415;&#32929;&#31080;&#25968;&#25454;&#21644;&#22836;&#26465;&#26032;&#38395;&#25968;&#25454;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32929;&#31080;&#36235;&#21183;&#20998;&#26512;&#26159;&#19968;&#20010;&#26377;&#24433;&#21709;&#21147;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20027;&#39064;&#65292;&#30001;&#20110;&#20854;&#21033;&#28070;&#20016;&#21402;&#19988;&#20869;&#22312;&#28151;&#20081;&#30340;&#29305;&#24615;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#35768;&#22810;&#26088;&#22312;&#20934;&#30830;&#39044;&#27979;&#32929;&#31080;&#36235;&#21183;&#30340;&#27169;&#22411;&#37117;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;RNN&#30340;&#38480;&#21046;&#65292;&#22914;&#26799;&#24230;&#28040;&#22833;&#21644;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#20002;&#22833;&#65292;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#25216;&#26415;&#32929;&#31080;&#25968;&#25454;&#21644;&#24773;&#24863;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#23545;&#38271;&#26102;&#38388;&#31383;&#21475;&#20869;&#32929;&#31080;&#36235;&#21183;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#36817;&#19977;&#24180;&#30340;&#27599;&#26085;&#25216;&#26415;&#32929;&#31080;&#25968;&#25454;&#21644;&#22836;&#26465;&#26032;&#38395;&#25968;&#25454;&#12290;&#20165;&#22522;&#20110;&#25216;&#26415;&#25968;&#25454;&#30340;&#32929;&#31080;&#39044;&#27979;&#21487;&#33021;&#20250;&#21463;&#21040;&#28382;&#21518;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#32929;&#31080;&#25351;&#26631;&#26080;&#27861;&#26377;&#25928;&#22320;&#22240;&#24212;&#24066;&#22330;&#26032;&#38395;&#30340;&#21464;&#21270;&#12290;&#20351;&#29992;&#24773;&#24863;&#20998;&#26512;&#26469;&#20998;&#26512;&#22836;&#26465;&#26032;&#38395;&#21487;&#20197;&#24110;&#21161;&#24212;&#23545;&#30001;&#26032;&#38395;&#25253;&#36947;&#24341;&#36215;&#30340;&#24066;&#22330;&#26465;&#20214;&#30340;&#19981;&#21487;&#39044;&#35265;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#34913;&#37327;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stock trend analysis has been an influential time-series prediction topic due to its lucrative and inherently chaotic nature. Many models looking to accurately predict the trend of stocks have been based on Recurrent Neural Networks (RNNs). However, due to the limitations of RNNs, such as gradient vanish and long-term dependencies being lost as sequence length increases, in this paper we develop a Transformer based model that uses technical stock data and sentiment analysis to conduct accurate stock trend prediction over long time windows. This paper also introduces a novel dataset containing daily technical stock data and top news headline data spanning almost three years. Stock prediction based solely on technical data can suffer from lag caused by the inability of stock indicators to effectively factor in breaking market news. The use of sentiment analysis on top headlines can help account for unforeseen shifts in market conditions caused by news coverage. We measure the performance
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#31867;&#30340;&#36719;&#32452;&#32455;&#21487;&#29992;&#20110;&#29289;&#29702;&#20648;&#22791;&#35745;&#31639;&#20013;&#30340;&#36719;&#20307;&#65292;&#22312;&#20223;&#30495;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#35745;&#31639;&#20219;&#21153;&#20013;&#20855;&#26377;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.14366</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#20307;&#36719;&#32452;&#32455;&#36827;&#34892;&#20449;&#24687;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Information processing via human soft tissue. (arXiv:2305.14366v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#31867;&#30340;&#36719;&#32452;&#32455;&#21487;&#29992;&#20110;&#29289;&#29702;&#20648;&#22791;&#35745;&#31639;&#20013;&#30340;&#36719;&#20307;&#65292;&#22312;&#20223;&#30495;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#35745;&#31639;&#20219;&#21153;&#20013;&#20855;&#26377;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#20307;&#36719;&#32452;&#32455;&#21487;&#34987;&#29992;&#20316;&#29289;&#29702;&#20648;&#22791;&#35745;&#31639;&#20013;&#30340;&#36719;&#20307;&#12290;&#36719;&#29983;&#29289;&#32452;&#32455;&#20855;&#26377;&#24212;&#21147;&#24212;&#21464;&#38750;&#32447;&#24615;&#21644;&#31896;&#24377;&#24615;&#31561;&#29305;&#24615;&#65292;&#28385;&#36275;&#29289;&#29702;&#20648;&#22791;&#35745;&#31639;&#30340;&#35201;&#27714;&#65292;&#21253;&#25324;&#38750;&#32447;&#24615;&#21644;&#35760;&#24518;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#20154;&#20307;&#36719;&#32452;&#32455;&#30340;&#21160;&#21147;&#23398;&#20316;&#20026;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20223;&#30495;&#30340;&#29289;&#29702;&#20648;&#22791;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#20010;&#27010;&#24565;&#65292;&#20174;&#20154;&#20307;&#21442;&#19982;&#32773;&#20013;&#33719;&#21462;&#25163;&#33109;&#20851;&#33410;&#30340;&#23624;&#20280;&#36816;&#21160;&#30340;&#20851;&#33410;&#35282;&#24230;&#25968;&#25454;&#21644;&#20851;&#32852;&#36816;&#21160;&#30340;&#32908;&#32905;&#30340;&#36229;&#22768;&#22270;&#20687;&#12290;&#31995;&#32479;&#30340;&#36755;&#20837;&#26159;&#25163;&#33109;&#20851;&#33410;&#30340;&#35282;&#24230;&#65292;&#32780;&#32908;&#32905;&#20869;&#30340;&#21464;&#24418;&#22330;&#65288;&#20174;&#36229;&#22768;&#22270;&#20687;&#20013;&#33719;&#24471;&#65289;&#20195;&#34920;&#20102;&#20648;&#22791;&#30340;&#29366;&#24577;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36719;&#32452;&#32455;&#30340;&#21160;&#21147;&#23398;&#23545;&#20110;&#36890;&#36807;&#29289;&#29702;&#20648;&#22791;&#35745;&#31639;&#26469;&#20223;&#30495;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#35745;&#31639;&#20219;&#21153;&#20855;&#26377;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study demonstrates that the soft biological tissues of humans can be used as a type of soft body in physical reservoir computing. Soft biological tissues possess characteristics such as stress-strain nonlinearity and viscoelasticity that satisfy the requirements for physical reservoir computing, including nonlinearity and memory. The aim of this study was to utilize the dynamics of human soft tissues as a physical reservoir for the emulation of nonlinear dynamical systems. To demonstrate this concept, joint angle data during motion in the flexion-extension direction of the wrist joint, and ultrasound images of the muscles associated with that motion, were acquired from human participants. The input to the system was the angle of the wrist joint, while the deformation field within the muscle (obtained from ultrasound images) represented the state of the reservoir. The results indicate that the dynamics of soft tissue have a positive impact on the computational task of emulating non
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#24052;&#29995;&#27931;&#22827;&#20449;&#21495;&#20256;&#36882;&#26041;&#27861;&#22914;&#20309;&#36827;&#34892;&#36830;&#32493;&#23398;&#20064;&#32780;&#19981;&#20250;&#24536;&#35760;&#20043;&#21069;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#30740;&#31350;&#21457;&#29616;&#31639;&#27861;&#36873;&#25321;&#65288;&#22914;on-policy&#21644;off-policy&#26041;&#27861;&#20197;&#21450;&#34920;&#31034;&#36873;&#25321;&#65289;&#20250;&#24433;&#21709;&#20256;&#36882;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14365</link><description>&lt;p&gt;
&#38754;&#21521;&#20154;&#26426;&#20132;&#20114;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#36830;&#32493;&#23398;&#20064;&#24052;&#29995;&#27931;&#22827;&#20449;&#21495;&#20256;&#36882;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Continually Learned Pavlovian Signalling Without Forgetting for Human-in-the-Loop Robotic Control. (arXiv:2305.14365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#24052;&#29995;&#27931;&#22827;&#20449;&#21495;&#20256;&#36882;&#26041;&#27861;&#22914;&#20309;&#36827;&#34892;&#36830;&#32493;&#23398;&#20064;&#32780;&#19981;&#20250;&#24536;&#35760;&#20043;&#21069;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#30740;&#31350;&#21457;&#29616;&#31639;&#27861;&#36873;&#25321;&#65288;&#22914;on-policy&#21644;off-policy&#26041;&#27861;&#20197;&#21450;&#34920;&#31034;&#36873;&#25321;&#65289;&#20250;&#24433;&#21709;&#20256;&#36882;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#32930;&#20307;&#26159;&#19968;&#31181;&#36741;&#21161;&#20154;&#20204;&#26085;&#24120;&#29983;&#27963;&#20219;&#21153;&#30340;&#22797;&#26434;&#35774;&#22791;&#12290;&#23613;&#31649;&#20808;&#36827;&#30340;&#26426;&#26800;&#20551;&#32930;&#23637;&#31034;&#20102;&#31867;&#20284;&#20110;&#29983;&#29289;&#32930;&#20307;&#30340;&#36816;&#21160;&#33021;&#21147;&#65292;&#20294;&#29992;&#25143;&#25253;&#21578;&#23427;&#20204;&#38590;&#20197;&#20351;&#29992;&#24182;&#19988;&#19981;&#30452;&#35266;&#12290;&#22240;&#27492;&#65292;&#25552;&#20379;&#26356;&#26377;&#25928;&#30340;&#26469;&#33258;&#35774;&#22791;&#21040;&#29992;&#25143;&#30340;&#21453;&#39304;&#24050;&#32463;&#25104;&#20026;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#30340;&#35805;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#39044;&#27979;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#20855;&#20307;&#22320;&#35828;&#65292;&#19968;&#31181;&#21517;&#20026;&#24052;&#29995;&#27931;&#22827;&#20449;&#21495;&#20256;&#36882;&#30340;&#26041;&#27861;&#65292;&#24050;&#34987;&#25552;&#20986;&#20316;&#20026;&#25913;&#36827;&#20551;&#32930;&#21453;&#39304;&#35843;&#33410;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#22312;&#36830;&#32493;&#20351;&#29992;&#36807;&#31243;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;&#20854;&#20013;&#19968;&#20010;&#25361;&#25112;&#26159;&#65292;&#36825;&#20123;&#23398;&#20064;&#26041;&#27861;&#20250;&#22312;&#29992;&#25143;&#24320;&#22987;&#25104;&#21151;&#22320;&#34892;&#21160;&#25152;&#25552;&#20379;&#30340;&#21453;&#39304;&#26102;&#24536;&#35760;&#20808;&#21069;&#23398;&#20064;&#30340;&#39044;&#27979;&#12290;&#26412;&#30740;&#31350;&#30452;&#25509;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#23601;&#31639;&#27861;&#36873;&#25321;&#65288;&#22914;on-policy&#21644;off-policy&#26041;&#27861;&#20197;&#21450;&#34920;&#31034;&#36873;&#25321;&#65289;&#23545;&#24052;&#29995;&#27931;&#22827;&#20449;&#21495;&#20256;&#36882;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#26032;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial limbs are sophisticated devices to assist people with tasks of daily living. Despite advanced robotic prostheses demonstrating similar motion capabilities to biological limbs, users report them difficult and non-intuitive to use. Providing more effective feedback from the device to the user has therefore become a topic of increased interest. In particular, prediction learning methods from the field of reinforcement learning -- specifically, an approach termed Pavlovian signalling -- have been proposed as one approach for better modulating feedback in prostheses since they can adapt during continuous use. One challenge identified in these learning methods is that they can forget previously learned predictions when a user begins to successfully act upon delivered feedback. The present work directly addresses this challenge, contributing new evidence on the impact of algorithmic choices, such as on- or off-policy methods and representation choices, on the Pavlovian signalling f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#29983;&#29289;&#21551;&#21457;&#30340;&#38750;&#32447;&#24615;&#25968;&#25454;&#34920;&#36798;&#26041;&#27861;&#8212;&#8212;&#20020;&#30028;&#20998;&#26512;&#65292;&#23427;&#33021;&#22815;&#22312;&#21463;&#25511;&#33258;&#32452;&#32455;&#20020;&#30028;&#31995;&#32479;&#20869;&#23454;&#29616;&#26080;&#23610;&#24230;&#22320;&#34920;&#36798;&#25968;&#25454;&#65292;&#24182;&#19988;&#20165;&#20381;&#36182;&#20110;&#28151;&#27788;&#36895;&#29575;&#25511;&#21046;&#12290;&#37319;&#29992;&#21160;&#24577;&#34892;&#20026;&#24211;&#30340;&#27010;&#24565;&#65292;&#35813;&#26041;&#27861;&#20445;&#30041;&#20102;&#25968;&#25454;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#23558;&#25968;&#25454;&#38477;&#32500;&#21040;&#24635;&#20307;&#25968;&#25454;&#29305;&#24449;&#30340;&#25237;&#24433;&#36755;&#20986;&#65292;&#20174;&#32780;&#20351;&#24471;&#21160;&#24577;&#21709;&#24212;&#26356;&#20026;&#31616;&#21333;&#12290;</title><link>http://arxiv.org/abs/2305.14361</link><description>&lt;p&gt;
&#20020;&#30028;&#20998;&#26512;&#65306;&#29983;&#29289;&#21551;&#21457;&#30340;&#38750;&#32447;&#24615;&#25968;&#25454;&#34920;&#36798;
&lt;/p&gt;
&lt;p&gt;
Criticality Analysis: Bio-inspired Nonlinear Data Representation. (arXiv:2305.14361v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#29983;&#29289;&#21551;&#21457;&#30340;&#38750;&#32447;&#24615;&#25968;&#25454;&#34920;&#36798;&#26041;&#27861;&#8212;&#8212;&#20020;&#30028;&#20998;&#26512;&#65292;&#23427;&#33021;&#22815;&#22312;&#21463;&#25511;&#33258;&#32452;&#32455;&#20020;&#30028;&#31995;&#32479;&#20869;&#23454;&#29616;&#26080;&#23610;&#24230;&#22320;&#34920;&#36798;&#25968;&#25454;&#65292;&#24182;&#19988;&#20165;&#20381;&#36182;&#20110;&#28151;&#27788;&#36895;&#29575;&#25511;&#21046;&#12290;&#37319;&#29992;&#21160;&#24577;&#34892;&#20026;&#24211;&#30340;&#27010;&#24565;&#65292;&#35813;&#26041;&#27861;&#20445;&#30041;&#20102;&#25968;&#25454;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#23558;&#25968;&#25454;&#38477;&#32500;&#21040;&#24635;&#20307;&#25968;&#25454;&#29305;&#24449;&#30340;&#25237;&#24433;&#36755;&#20986;&#65292;&#20174;&#32780;&#20351;&#24471;&#21160;&#24577;&#21709;&#24212;&#26356;&#20026;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#20449;&#24687;&#22788;&#29702;&#20013;&#65292;&#20219;&#24847;&#25968;&#25454;&#30340;&#34920;&#36798;&#26159;&#26368;&#38590;&#25417;&#25720;&#30340;&#20803;&#32032;&#20043;&#19968;&#12290;&#29983;&#29289;&#31995;&#32479;&#20013;&#21576;&#29616;&#30340;&#20449;&#24687;&#30340;&#25391;&#24133;&#21644;&#39057;&#29575;&#36890;&#24120;&#26159;&#23545;&#25968;&#30340;&#65292;&#36825;&#20351;&#24471;&#20449;&#24687;&#30340;&#31616;&#21333;&#23553;&#35013;&#21464;&#24471;&#19981;&#21487;&#33021;&#12290;&#20020;&#30028;&#20998;&#26512;&#65288;CA&#65289;&#26159;&#19968;&#31181;&#22312;&#21463;&#25511;&#33258;&#32452;&#32455;&#20020;&#30028;&#31995;&#32479;&#20869;&#36827;&#34892;&#20449;&#24687;&#34920;&#36798;&#30340;&#29983;&#29289;&#21551;&#21457;&#26041;&#27861;&#65292;&#23427;&#20801;&#35768;&#26080;&#23610;&#24230;&#30340;&#34920;&#36798;&#12290;&#26681;&#25454;&#21160;&#24577;&#34892;&#20026;&#24211;&#30340;&#27010;&#24565;&#65292;&#36825;&#31181;&#34920;&#36798;&#24211;&#23558;&#20135;&#29983;&#21160;&#24577;&#38750;&#32447;&#24615;&#34920;&#36798;&#65292;&#20854;&#20013;&#33258;&#30456;&#20284;&#25968;&#25454;&#23558;&#21019;&#36896;&#21160;&#24577;&#38750;&#32447;&#24615;&#34920;&#36798;&#12290;&#36825;&#31181;&#25968;&#25454;&#30340;&#29420;&#29305;&#25237;&#24433;&#20445;&#30041;&#20102;&#22810;&#32500;&#37051;&#22495;&#20013;&#25968;&#25454;&#30340;&#30456;&#20284;&#24615;&#12290;&#36755;&#20837;&#21487;&#20197;&#34987;&#38477;&#32500;&#21040;&#20445;&#30041;&#24635;&#20307;&#25968;&#25454;&#29305;&#24449;&#30340;&#25237;&#24433;&#36755;&#20986;&#65292;&#20294;&#20855;&#26377;&#26356;&#31616;&#21333;&#30340;&#21160;&#24577;&#21709;&#24212;&#12290;&#35813;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#24212;&#29992;&#20110;&#22522;&#30784;&#21463;&#25511;&#27169;&#22411;&#30340;&#28151;&#27788;&#36895;&#29575;&#25511;&#21046;&#65292;&#20174;&#32780;&#20801;&#35768;&#23558;&#20219;&#24847;&#25968;&#25454;&#32534;&#30721;&#20026;&#20020;&#30028;&#31995;&#32479;&#20869;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The representation of arbitrary data in a biological system is one of the most elusive elements of biological information processing. The often logarithmic nature of information in amplitude and frequency presented to biosystems prevents simple encapsulation of the information contained in the input. Criticality Analysis (CA) is a bio-inspired method of information representation within a controlled self-organised critical system that allows scale-free representation. This is based on the concept of a reservoir of dynamic behaviour in which self-similar data will create dynamic nonlinear representations. This unique projection of data preserves the similarity of data within a multidimensional neighbourhood. The input can be reduced dimensionally to a projection output that retains the features of the overall data, yet has much simpler dynamic response. The method depends only on the rate control of chaos applied to the underlying controlled models, that allows the encoding of arbitrary
&lt;/p&gt;</description></item><item><title>EMMa&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#12289;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;280&#19975;&#20010;&#24102;&#20016;&#23500;&#26448;&#26009;&#27880;&#37322;&#30340;&#20122;&#39532;&#36874;&#20135;&#21697;&#21015;&#34920;&#23545;&#35937;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#21487;&#25193;&#23637;&#24615;&#26497;&#24378;&#65292;&#21487;&#20197;&#29992;&#20110;&#22810;&#20010;&#20219;&#21153;&#65292;&#27599;&#20010;&#23545;&#35937;&#23646;&#24615;&#37117;&#21487;&#20197;&#21253;&#21547;&#22312;&#27169;&#22411;&#36755;&#20837;&#25110;&#36755;&#20986;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.14352</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#26448;&#26009;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#21487;&#25193;&#23637;&#23545;&#35937;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
An Extensible Multimodal Multi-task Object Dataset with Materials. (arXiv:2305.14352v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14352
&lt;/p&gt;
&lt;p&gt;
EMMa&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#12289;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;280&#19975;&#20010;&#24102;&#20016;&#23500;&#26448;&#26009;&#27880;&#37322;&#30340;&#20122;&#39532;&#36874;&#20135;&#21697;&#21015;&#34920;&#23545;&#35937;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#21487;&#25193;&#23637;&#24615;&#26497;&#24378;&#65292;&#21487;&#20197;&#29992;&#20110;&#22810;&#20010;&#20219;&#21153;&#65292;&#27599;&#20010;&#23545;&#35937;&#23646;&#24615;&#37117;&#21487;&#20197;&#21253;&#21547;&#22312;&#27169;&#22411;&#36755;&#20837;&#25110;&#36755;&#20986;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;EMMa&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#26377;&#20016;&#23500;&#30340;&#26448;&#26009;&#27880;&#37322;&#30340;&#20122;&#39532;&#36874;&#20135;&#21697;&#21015;&#34920;&#30340;&#21487;&#25193;&#23637;&#12289;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#21547;&#36229;&#36807;280&#19975;&#20010;&#23545;&#35937;&#65292;&#27599;&#20010;&#23545;&#35937;&#37117;&#26377;&#22270;&#20687;&#12289;&#21015;&#34920;&#25991;&#26412;&#12289;&#37325;&#37327;&#12289;&#20215;&#26684;&#12289;&#20135;&#21697;&#35780;&#32423;&#20197;&#21450;&#22312;&#20122;&#39532;&#36874;&#20135;&#21697;&#20998;&#31867;&#27861;&#20013;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;182&#31181;&#29289;&#29702;&#26448;&#26009;&#30340;&#32508;&#21512;&#20998;&#31867;&#65288;&#20363;&#22914;&#65292;&#22609;&#26009;$\rightarrow$&#28909;&#22609;&#24615;$\rightarrow$&#20122;&#20811;&#21147;&#65289;&#12290;&#27599;&#20010;&#23545;&#35937;&#37117;&#29992;&#27492;&#20998;&#31867;&#20013;&#30340;&#19968;&#20010;&#25110;&#22810;&#20010;&#26448;&#26009;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#26234;&#33021;&#26631;&#35760;&#26694;&#26550;&#65292;&#20197;&#24555;&#36895;&#23558;&#26032;&#30340;&#20108;&#36827;&#21046;&#26631;&#35760;&#28155;&#21152;&#21040;&#25152;&#26377;&#23545;&#35937;&#20013;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#25163;&#21160;&#26631;&#35760;&#65292;&#20174;&#32780;&#20351;&#25968;&#25454;&#38598;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#23545;&#35937;&#23646;&#24615;&#22343;&#21487;&#21253;&#21547;&#22312;&#27169;&#22411;&#36755;&#20837;&#25110;&#36755;&#20986;&#20013;&#65292;&#24102;&#26469;&#20102;&#20219;&#21153;&#37197;&#32622;&#30340;&#32452;&#21512;&#21487;&#33021;&#24615;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#20135;&#21697;&#21015;&#34920;&#25991;&#26412;&#20013;&#35757;&#32451;&#27169;&#22411;&#39044;&#27979;&#23545;&#35937;&#31867;&#21035;&#65292;&#25110;&#32773;&#20174;&#20135;&#21697;&#21015;&#34920;&#22270;&#20687;&#20013;&#39044;&#27979;&#29289;&#36136;&#21644;&#20215;&#26684;&#12290;EMMa&#20026;&#30740;&#31350;&#31038;&#21306;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#31934;&#31616;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#26377;&#21161;&#20110;&#25512;&#21160;&#27169;&#22411;&#30340;&#21457;&#23637;&#21644;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present EMMa, an Extensible, Multimodal dataset of Amazon product listings that contains rich Material annotations. It contains more than 2.8 million objects, each with image(s), listing text, mass, price, product ratings, and position in Amazon's product-category taxonomy. We also design a comprehensive taxonomy of 182 physical materials (e.g., Plastic $\rightarrow$ Thermoplastic $\rightarrow$ Acrylic). Objects are annotated with one or more materials from this taxonomy. With the numerous attributes available for each object, we develop a Smart Labeling framework to quickly add new binary labels to all objects with very little manual labeling effort, making the dataset extensible. Each object attribute in our dataset can be included in either the model inputs or outputs, leading to combinatorial possibilities in task configurations. For example, we can train a model to predict the object category from the listing text, or the mass and price from the product listing image. EMMa offe
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#24320;&#28304;&#36719;&#20214;Raidionics&#65292;&#26088;&#22312;&#26631;&#20934;&#21270;&#21644;&#33258;&#21160;&#21270;&#32959;&#30244;&#20998;&#21106;&#21644;&#29983;&#25104;&#21253;&#21547;&#30456;&#20851;&#32959;&#30244;&#29305;&#24449;&#30340;&#20020;&#24202;&#25253;&#21578;&#12290;&#35813;&#36719;&#20214;&#21253;&#25324;&#26415;&#21069;&#20998;&#21106;&#27169;&#22411;&#65292;&#23545;&#20110;&#26368;&#24120;&#35265;&#30340;&#32959;&#30244;&#31867;&#22411;&#65292;&#20998;&#21106;&#34920;&#29616;&#30456;&#24403;&#22343;&#21248;&#65292;&#24179;&#22343;Dice&#31995;&#25968;&#32422;&#20026;85%&#12290;</title><link>http://arxiv.org/abs/2305.14351</link><description>&lt;p&gt;
Raidionics&#65306;&#19968;&#31181;&#29992;&#20110;&#26415;&#21069;&#21644;&#26415;&#21518;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#32959;&#30244;&#20998;&#21106;&#21644;&#26631;&#20934;&#21270;&#25253;&#21578;&#30340;&#24320;&#25918;&#36719;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Raidionics: an open software for pre- and postoperative central nervous system tumor segmentation and standardized reporting. (arXiv:2305.14351v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14351
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#24320;&#28304;&#36719;&#20214;Raidionics&#65292;&#26088;&#22312;&#26631;&#20934;&#21270;&#21644;&#33258;&#21160;&#21270;&#32959;&#30244;&#20998;&#21106;&#21644;&#29983;&#25104;&#21253;&#21547;&#30456;&#20851;&#32959;&#30244;&#29305;&#24449;&#30340;&#20020;&#24202;&#25253;&#21578;&#12290;&#35813;&#36719;&#20214;&#21253;&#25324;&#26415;&#21069;&#20998;&#21106;&#27169;&#22411;&#65292;&#23545;&#20110;&#26368;&#24120;&#35265;&#30340;&#32959;&#30244;&#31867;&#22411;&#65292;&#20998;&#21106;&#34920;&#29616;&#30456;&#24403;&#22343;&#21248;&#65292;&#24179;&#22343;Dice&#31995;&#25968;&#32422;&#20026;85%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24739;&#26377;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#32959;&#30244;&#30340;&#24739;&#32773;&#65292;&#39044;&#21518;&#35780;&#20272;&#12289;&#27835;&#30103;&#20915;&#31574;&#21644;&#26415;&#21518;&#35780;&#20272;&#26159;&#36890;&#36807;&#20998;&#26512;&#19968;&#32452;&#30913;&#20849;&#25391;&#65288;MR&#65289;&#25195;&#25551;&#26469;&#36827;&#34892;&#30340;&#12290;&#30446;&#21069;&#32570;&#20047;&#26631;&#20934;&#21270;&#21644;&#33258;&#21160;&#21270;&#32959;&#30244;&#20998;&#21106;&#21644;&#29983;&#25104;&#21253;&#21547;&#30456;&#20851;&#32959;&#30244;&#29305;&#24449;&#30340;&#20020;&#24202;&#25253;&#21578;&#30340;&#24320;&#25918;&#24335;&#24037;&#20855;&#65292;&#20351;&#24471;&#28508;&#22312;&#30340;&#20915;&#31574;&#20027;&#35266;&#24615;&#23384;&#22312;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Raidionics&#24320;&#28304;&#36719;&#20214;&#65292;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#21644;&#31283;&#23450;&#30340;&#22788;&#29702;&#21518;&#31471;&#12290;&#35813;&#36719;&#20214;&#21253;&#25324;&#38024;&#23545;&#26368;&#24120;&#35265;&#30340;&#32959;&#30244;&#31867;&#22411;&#65288;&#21363;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#12289;&#20302;&#32423;&#21035;&#33014;&#36136;&#30244;&#12289;&#39635;&#27597;&#32454;&#32990;&#30244;&#21644;&#36716;&#31227;&#30244;&#65289;&#30340;&#26415;&#21069;&#20998;&#21106;&#27169;&#22411;&#65292;&#20197;&#21450;&#19968;&#31181;&#26089;&#26399;&#26415;&#21518;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#20998;&#21106;&#27169;&#22411;&#12290;&#22235;&#31181;&#19981;&#21516;&#30340;&#33041;&#32959;&#30244;&#31867;&#22411;&#30340;&#26415;&#21069;&#20998;&#21106;&#34920;&#29616;&#30456;&#24403;&#22343;&#21248;&#65292;&#24179;&#22343;Dice&#31995;&#25968;&#32422;&#20026;85%&#65292;&#24739;&#32773;-wis
&lt;/p&gt;
&lt;p&gt;
For patients suffering from central nervous system tumors, prognosis estimation, treatment decisions, and postoperative assessments are made from the analysis of a set of magnetic resonance (MR) scans. Currently, the lack of open tools for standardized and automatic tumor segmentation and generation of clinical reports, incorporating relevant tumor characteristics, leads to potential risks from inherent decisions' subjectivity. To tackle this problem, the proposed Raidionics open-source software has been developed, offering both a user-friendly graphical user interface and stable processing backend. The software includes preoperative segmentation models for each of the most common tumor types (i.e., glioblastomas, lower grade gliomas, meningiomas, and metastases), together with one early postoperative glioblastoma segmentation model. Preoperative segmentation performances were quite homogeneous across the four different brain tumor types, with an average Dice around 85% and patient-wis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#20195;&#30721;&#65292;&#24182;&#36890;&#36807;&#36171;&#20104;&#27169;&#22411;&#20808;&#21069;&#30456;&#20851;&#30340;&#32534;&#36753;&#30693;&#35782;&#65292;&#26469;&#35299;&#20915;&#20195;&#30721;&#22810;&#26679;&#24615;&#21644;&#24320;&#21457;&#20154;&#21592;&#24847;&#22270;&#38590;&#20197;&#25429;&#25417;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14129</link><description>&lt;p&gt;
GrACE&#65306;&#20351;&#29992;&#30456;&#20851;&#20195;&#30721;&#32534;&#36753;&#29983;&#25104;&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
GrACE: Generation using Associated Code Edits. (arXiv:2305.14129v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#20195;&#30721;&#65292;&#24182;&#36890;&#36807;&#36171;&#20104;&#27169;&#22411;&#20808;&#21069;&#30456;&#20851;&#30340;&#32534;&#36753;&#30693;&#35782;&#65292;&#26469;&#35299;&#20915;&#20195;&#30721;&#22810;&#26679;&#24615;&#21644;&#24320;&#21457;&#20154;&#21592;&#24847;&#22270;&#38590;&#20197;&#25429;&#25417;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20154;&#21592;&#20250;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#32534;&#36753;&#20195;&#30721;&#65292;&#20854;&#21407;&#22240;&#21253;&#25324;&#20462;&#22797;&#38169;&#35823;&#25110;&#28155;&#21152;&#26032;&#21151;&#33021;&#12290;&#35774;&#35745;&#26377;&#25928;&#30340;&#20195;&#30721;&#32534;&#36753;&#39044;&#27979;&#26041;&#27861;&#19968;&#30452;&#26159;&#19968;&#20010;&#27963;&#36291;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22240;&#20026;&#20195;&#30721;&#32534;&#36753;&#30340;&#22810;&#26679;&#24615;&#21644;&#25429;&#25417;&#24320;&#21457;&#20154;&#21592;&#24847;&#22270;&#30340;&#38590;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#36171;&#20104;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20808;&#21069;&#30456;&#20851;&#30340;&#32534;&#36753;&#30693;&#35782;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#26377;&#21161;&#20110;&#35299;&#20915;&#20195;&#30721;&#26356;&#25913;&#30340;&#22810;&#26679;&#24615;&#65292;&#32780;&#23558;&#20195;&#30721;&#29983;&#25104;&#30340;&#26465;&#20214;&#35774;&#23450;&#20026;&#20808;&#21069;&#32534;&#36753;&#26377;&#21161;&#20110;&#25429;&#25417;&#28508;&#22312;&#30340;&#24320;&#21457;&#20154;&#21592;&#24847;&#22270;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#20004;&#31181;&#30693;&#21517;&#30340;LLMs&#65292;Codex&#21644;CodeT5&#65292;&#20998;&#21035;&#36827;&#34892;&#38646;&#26679;&#26412;&#21644;&#24494;&#35843;&#35774;&#32622;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#20808;&#21069;&#32534;&#36753;&#30340;&#30693;&#35782;&#26174;&#33879;&#25552;&#39640;&#20102;LLMs&#30340;&#24615;&#33021;&#65292;&#24182;&#20351;&#20854;&#22312;&#21069;1&#20010;&#24314;&#35758;&#20013;&#29983;&#25104;29&#65285;&#21644;54&#65285;&#26356;&#27491;&#30830;&#30340;&#32534;&#36753;&#20195;&#30721;&#65292;&#30456;&#23545;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#31526;&#21495;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developers expend a significant amount of time in editing code for a variety of reasons such as bug fixing or adding new features. Designing effective methods to predict code edits has been an active yet challenging area of research due to the diversity of code edits and the difficulty of capturing the developer intent. In this work, we address these challenges by endowing pre-trained large language models (LLMs) of code with the knowledge of prior, relevant edits. The generative capability of the LLMs helps address the diversity in code changes and conditioning code generation on prior edits helps capture the latent developer intent. We evaluate two well-known LLMs, Codex and CodeT5, in zero-shot and fine-tuning settings respectively. In our experiments with two datasets, the knowledge of prior edits boosts the performance of the LLMs significantly and enables them to generate 29% and 54% more correctly edited code in top-1 suggestions relative to the current state-of-the-art symbolic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20195;&#20215;&#24863;&#30693;&#30340;&#27169;&#22411;&#36873;&#25321;BO&#26041;&#27861;SADCBO&#65292;&#36890;&#36807;&#23545;&#21518;&#39564;&#20195;&#29702;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#20998;&#26512;&#26469;&#23398;&#20064;&#20851;&#20110;&#29615;&#22659;&#30340;&#30456;&#20851;&#24773;&#22659;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24179;&#22343;&#27169;&#22411;&#39044;&#27979;&#26469;&#26368;&#23567;&#21270;&#20248;&#21270;&#20195;&#20215;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14120</link><description>&lt;p&gt;
&#22522;&#20110;&#20195;&#20215;&#24863;&#30693;&#30340;&#24773;&#22659;&#21464;&#37327;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cost-aware learning of relevant contextual variables within Bayesian optimization. (arXiv:2305.14120v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20195;&#20215;&#24863;&#30693;&#30340;&#27169;&#22411;&#36873;&#25321;BO&#26041;&#27861;SADCBO&#65292;&#36890;&#36807;&#23545;&#21518;&#39564;&#20195;&#29702;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#20998;&#26512;&#26469;&#23398;&#20064;&#20851;&#20110;&#29615;&#22659;&#30340;&#30456;&#20851;&#24773;&#22659;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24179;&#22343;&#27169;&#22411;&#39044;&#27979;&#26469;&#26368;&#23567;&#21270;&#20248;&#21270;&#20195;&#20215;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#22659;&#36125;&#21494;&#26031;&#20248;&#21270;(CBO)&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#21487;&#38024;&#23545;&#35774;&#35745;&#21464;&#37327;&#20248;&#21270;&#40657;&#30418;&#26114;&#36149;&#30340;&#35780;&#20272;&#20989;&#25968;&#65292;&#24182;&#21516;&#26102;&#26377;&#25928;&#22320;&#25972;&#21512;&#20851;&#20110;&#29615;&#22659;&#30340;&#30456;&#20851;&#24773;&#22659;&#20449;&#24687;&#65292;&#22914;&#23454;&#39564;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#24773;&#22659;&#21464;&#37327;&#30340;&#30456;&#20851;&#24615;&#19981;&#19968;&#23450;&#26159;&#39044;&#20808;&#24050;&#30693;&#30340;&#12290;&#27492;&#22806;&#65292;&#26377;&#26102;&#36824;&#21487;&#20197;&#26368;&#20248;&#21270;&#24773;&#22659;&#21464;&#37327;&#26412;&#36523;&#65292;&#36825;&#26159;&#24403;&#21069;CBO&#31639;&#27861;&#26410;&#32771;&#34385;&#30340;&#35774;&#32622;&#12290;&#20248;&#21270;&#24773;&#22659;&#21464;&#37327;&#21487;&#33021;&#26159;&#26114;&#36149;&#30340;&#65292;&#36825;&#24341;&#20986;&#20102;&#30830;&#23450;&#19968;&#20010;&#26368;&#23567;&#30456;&#20851;&#23376;&#38598;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#20316;&#20026;&#19968;&#20010;&#20195;&#20215;&#24863;&#30693;&#30340;&#27169;&#22411;&#36873;&#25321;BO&#20219;&#21153;&#26469;&#26500;&#26550;&#65292;&#37319;&#29992;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#24773;&#22659;BO (SADCBO) &#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#29305;&#23450;&#36755;&#20837;&#28857;&#21518;&#39564;&#20195;&#29702;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#20998;&#26512;&#26469;&#23398;&#20064;&#24773;&#22659;&#21464;&#37327;&#30340;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#24179;&#22343;&#27169;&#22411;&#39044;&#27979;&#26469;&#26368;&#23567;&#21270;&#20248;&#21270;&#30340;&#20195;&#20215;&#12290;SADCBO&#22312;&#22810;&#20010;&#21512;&#25104;&#21644;&#30495;&#23454;&#22522;&#20934;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#26174;&#31034;&#20986;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contextual Bayesian Optimization (CBO) is a powerful framework for optimizing black-box, expensive-to-evaluate functions with respect to design variables, while simultaneously efficiently integrating relevant contextual information regarding the environment, such as experimental conditions. However, in many practical scenarios, the relevance of contextual variables is not necessarily known beforehand. Moreover, the contextual variables can sometimes be optimized themselves, a setting that current CBO algorithms do not take into account. Optimizing contextual variables may be costly, which raises the question of determining a minimal relevant subset. In this paper, we frame this problem as a cost-aware model selection BO task and address it using a novel method, Sensitivity-Analysis-Driven Contextual BO (SADCBO). We learn the relevance of context variables by sensitivity analysis of the posterior surrogate model at specific input points, whilst minimizing the cost of optimization by lev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13971</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#32422;&#26463;&#30340;&#35821;&#35328;&#27169;&#22411;&#28789;&#27963;&#35299;&#30721;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#23569;&#37327;&#26679;&#26412;&#34920;&#29616;&#65292;&#20294;&#22312;&#29983;&#25104;&#20449;&#24687;&#25552;&#21462;&#25152;&#38656;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26102;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#20010;&#38480;&#21046;&#28304;&#20110;LLM&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#20542;&#21521;&#20110;&#29983;&#25104;&#33258;&#30001;&#25991;&#26412;&#32780;&#19981;&#26159;&#36981;&#24490;&#29305;&#23450;&#35821;&#27861;&#30340;&#31934;&#30830;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#35299;&#30721;&#27493;&#39588;&#20013;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#26469;&#20016;&#23500;&#27169;&#22411;&#12290;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#21482;&#26377;&#31526;&#21512;&#35821;&#27861;&#20135;&#29983;&#35268;&#21017;&#30340;&#26377;&#25928;&#20196;&#29260;&#33021;&#34987;&#32771;&#34385;&#21040;&#12290;&#36825;&#26679;&#23601;&#24378;&#21046;&#21482;&#20135;&#29983;&#26377;&#25928;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38750;&#24120;&#36890;&#29992;&#21644;&#28789;&#27963;&#65292;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;(CFG)&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#33258;&#23450;&#20041;&#32422;&#26463;beam&#25628;&#32034;&#23454;&#29616;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;NLP&#20219;&#21153;&#30340;&#36755;&#20986;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#24418;&#24335;&#35821;&#35328;&#65292;&#20351;&#23427;&#20204;&#36866;&#21512;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#30452;&#25509;&#20351;&#29992;&#12290;&#23545;&#20110;&#36755;&#20986;&#31354;&#38388;&#21462;&#20915;&#20110;&#36755;&#20837;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36755;&#20837;&#30340;CFG&#65292;&#26681;&#25454;&#29305;&#23450;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#26356;&#26032;&#20135;&#29983;&#35268;&#21017;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs have shown impressive few-shot performance across many tasks. However, they still struggle when it comes to generating complex output structures, such as those required for Information Extraction. This limitation stems from the fact that LLMs, without finetuning, tend to generate free text rather than precise structures that follow a specific grammar. In this work, we propose to enrich the decoding step with formal grammar constraints. During beam search, only valid token continuations compliant with the grammar production rules are considered. This enforces the generation of valid sequences exclusively. Our framework is highly general and flexible, allowing any Context-Free Grammar (CFG) to be integrated into our custom constrained beam search implementation. We demonstrate that the outputs of many NLP tasks can be represented as formal languages, making them suitable for direct use in our framework. For task where the output space is dependent on the input, we propose input-depe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#21644;&#28151;&#21512;&#27169;&#22411;&#31561;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20844;&#20849;&#30340;&#20013;&#22269;&#25163;&#35821;&#35782;&#21035;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#31181;&#34701;&#21512;&#20102;CNN&#21644;LSTM&#30340;&#28151;&#21512;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.13941</link><description>&lt;p&gt;
&#25163;&#35821;&#35782;&#21035;&#25216;&#26415;&#21644;&#31639;&#27861;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comparative Analysis of Techniques and Algorithms for Recognising Sign Language. (arXiv:2305.13941v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#21644;&#28151;&#21512;&#27169;&#22411;&#31561;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20844;&#20849;&#30340;&#20013;&#22269;&#25163;&#35821;&#35782;&#21035;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#31181;&#34701;&#21512;&#20102;CNN&#21644;LSTM&#30340;&#28151;&#21512;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#35821;&#26159;&#19968;&#31181;&#35270;&#35273;&#35821;&#35328;&#65292;&#22686;&#24378;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#27807;&#36890;&#65292;&#24182;&#19988;&#32463;&#24120;&#20316;&#20026;&#20808;&#22825;&#24615;&#21548;&#21147;&#20007;&#22833;&#32773;&#20027;&#35201;&#30340;&#20132;&#27969;&#26041;&#24335;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20351;&#29992;&#25163;&#35821;&#30340;&#20808;&#22825;&#24615;&#21548;&#21147;&#20007;&#22833;&#32773;&#24182;&#19981;&#22810;&#65292;&#20182;&#20204;&#32463;&#24120;&#38754;&#20020;&#31038;&#20132;&#23396;&#31435;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#21019;&#24314;&#20154;&#26426;&#30028;&#38754;&#31995;&#32479;&#65292;&#20026;&#21548;&#21147;&#38556;&#30861;&#20154;&#22763;&#25552;&#20379;&#31038;&#20132;&#24179;&#21488;&#12290;&#24066;&#22330;&#19978;&#22823;&#22810;&#25968;&#21830;&#29992;&#25163;&#35821;&#32763;&#35793;&#31995;&#32479;&#26159;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#65292;&#20215;&#26684;&#26114;&#36149;&#65292;&#20351;&#29992;&#36215;&#26469;&#20063;&#24456;&#22256;&#38590;&#12290;&#23613;&#31649;&#36843;&#20999;&#38656;&#35201;&#22522;&#20110;&#35270;&#35273;&#30340;&#31995;&#32479;&#65292;&#20294;&#39318;&#20808;&#24517;&#39035;&#20811;&#26381;&#20960;&#20010;&#25361;&#25112;&#12290;&#26089;&#26399;&#36830;&#32493;&#25163;&#35821;&#35782;&#21035;&#25216;&#26415;&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#24456;&#38590;&#21253;&#21547;&#26102;&#38388;&#20449;&#24687;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#29616;&#22312;&#27491;&#22312;&#24212;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#23558;&#25163;&#37096;&#21644;&#25163;&#35821;&#21160;&#20316;&#36716;&#21270;&#20026;&#21475;&#35821;&#25110;&#20070;&#38754;&#35821;&#35328;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#21644;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20844;&#20849;&#30340;&#20013;&#22269;&#25163;&#35821;&#35782;&#21035;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#31181;&#34701;&#21512;&#20102;CNN&#21644;LSTM&#30340;&#28151;&#21512;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sign language is a visual language that enhances communication between people and is frequently used as the primary form of communication by people with hearing loss. Even so, not many people with hearing loss use sign language, and they frequently experience social isolation. Therefore, it is necessary to create human-computer interface systems that can offer hearing-impaired people a social platform. Most commercial sign language translation systems now on the market are sensor-based, pricey, and challenging to use. Although vision-based systems are desperately needed, they must first overcome several challenges. Earlier continuous sign language recognition techniques used hidden Markov models, which have a limited ability to include temporal information. To get over these restrictions, several machine learning approaches are being applied to transform hand and sign language motions into spoken or written language. In this study, we compare various deep learning techniques for recogn
&lt;/p&gt;</description></item><item><title>Flover&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#24182;&#34892;&#25512;&#26029;&#30340;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24182;&#34892;&#24615;&#19981;&#36275;&#21644;&#28789;&#27963;&#24615;&#24046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#21152;&#39640;&#25928;&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13484</link><description>&lt;p&gt;
Flover&#65306;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#33258;&#22238;&#24402;&#27169;&#22411;&#24182;&#34892;&#25512;&#26029;&#30340;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Flover: A Temporal Fusion Framework for Efficient Autoregressive Model Parallel Inference. (arXiv:2305.13484v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13484
&lt;/p&gt;
&lt;p&gt;
Flover&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#24182;&#34892;&#25512;&#26029;&#30340;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24182;&#34892;&#24615;&#19981;&#36275;&#21644;&#28789;&#27963;&#24615;&#24046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#21152;&#39640;&#25928;&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#24555;&#36895;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#27169;&#22411;&#25512;&#26029;&#24615;&#33021;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#23588;&#20854;&#26159;&#22312;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#24182;&#34987;&#37096;&#32626;&#22312;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#24773;&#20917;&#19979;&#12290;&#33258;&#22238;&#24402;&#27169;&#22411;&#30001;&#20110;&#22312;&#20247;&#22810;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#22240;&#27492;&#22791;&#21463;&#20851;&#27880;&#12290;&#36825;&#20123;&#27169;&#22411;&#35774;&#35745;&#19978;&#37319;&#29992;&#20102;&#19968;&#31181;&#26102;&#38388;&#20381;&#36182;&#32467;&#26500;&#65292;&#20854;&#20013;&#24403;&#21069;token&#30340;&#27010;&#29575;&#20998;&#24067;&#21463;&#21040;&#21069;&#38754;token&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26412;&#36136;&#19978;&#30340;&#24207;&#21015;&#29305;&#24615;&#36981;&#24490;&#39532;&#23572;&#21487;&#22827;&#38142;&#20551;&#35774;&#65292;&#32570;&#20047;&#26102;&#38388;&#24182;&#34892;&#24615;&#65292;&#22240;&#27492;&#23384;&#22312;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#22312;&#24037;&#19994;&#32972;&#26223;&#19979;&#65292;&#25512;&#26029;&#35831;&#27714;&#36981;&#24490;&#27850;&#26494;&#26102;&#38388;&#20998;&#24067;&#65292;&#38656;&#35201;&#19981;&#21516;&#30340;&#21709;&#24212;&#38271;&#24230;&#65292;&#36825;&#31181;&#24182;&#34892;&#24615;&#30340;&#32570;&#22833;&#26356;&#21152;&#26126;&#26174;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#22914;&#21160;&#24577;&#25209;&#22788;&#29702;&#21644;&#24182;&#21457;&#27169;&#22411;&#23454;&#20363;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#31895;&#31890;&#24230;&#30340;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#30340;&#24320;&#38144;&#21644;&#32570;&#20047;&#28789;&#27963;&#24615;&#65292;&#26080;&#27861;&#23454;&#29616;&#26368;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving field of deep learning, the performance of model inference has become a pivotal aspect as models become more complex and are deployed in diverse applications. Among these, autoregressive models stand out due to their state-of-the-art performance in numerous generative tasks. These models, by design, harness a temporal dependency structure, where the current token's probability distribution is conditioned on preceding tokens. This inherently sequential characteristic, however, adheres to the Markov Chain assumption and lacks temporal parallelism, which poses unique challenges. Particularly in industrial contexts where inference requests, following a Poisson time distribution, necessitate diverse response lengths, this absence of parallelism is more profound. Existing solutions, such as dynamic batching and concurrent model instances, nevertheless, come with severe overheads and a lack of flexibility, these coarse-grained methods fall short of achieving optimal la
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#36523;&#20307;&#20869;&#22312;&#21160;&#26426;&#36827;&#34892;&#20102;&#37327;&#21270;&#24314;&#27169;&#65292;&#21457;&#29616;&#23545;&#25239;&#24615;&#22870;&#21169;&#27169;&#22411;&#21487;&#20197;&#26368;&#22909;&#22320;&#39044;&#27979;&#20154;&#31867;&#23545;&#29289;&#29702;&#24773;&#22659;&#30340;&#36259;&#21619;&#21453;&#24212;&#65292;&#36824;&#21457;&#29616;&#31616;&#21333;&#22330;&#26223;&#29305;&#24449;&#27169;&#22411;&#26080;&#27861;&#22312;&#25152;&#26377;&#24773;&#22659;&#20013;&#39044;&#27979;&#20154;&#31867;&#21453;&#24212;&#65292;&#23558;&#23545;&#25239;&#27169;&#22411;&#21644;&#22330;&#26223;&#20013;&#30896;&#25758;&#25968;&#37327;&#36827;&#34892;&#32447;&#24615;&#32452;&#21512;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#23545;&#20154;&#31867;&#21453;&#24212;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#34920;&#26126;&#20154;&#31867;&#36861;&#27714;&#39640;&#20449;&#24687;&#22686;&#30410;&#21644;&#36523;&#20307;&#27963;&#21160;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2305.13452</link><description>&lt;p&gt;
&#27979;&#37327;&#21644;&#24314;&#27169;&#36523;&#20307;&#20869;&#22312;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Measuring and Modeling Physical Intrinsic Motivation. (arXiv:2305.13452v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#36523;&#20307;&#20869;&#22312;&#21160;&#26426;&#36827;&#34892;&#20102;&#37327;&#21270;&#24314;&#27169;&#65292;&#21457;&#29616;&#23545;&#25239;&#24615;&#22870;&#21169;&#27169;&#22411;&#21487;&#20197;&#26368;&#22909;&#22320;&#39044;&#27979;&#20154;&#31867;&#23545;&#29289;&#29702;&#24773;&#22659;&#30340;&#36259;&#21619;&#21453;&#24212;&#65292;&#36824;&#21457;&#29616;&#31616;&#21333;&#22330;&#26223;&#29305;&#24449;&#27169;&#22411;&#26080;&#27861;&#22312;&#25152;&#26377;&#24773;&#22659;&#20013;&#39044;&#27979;&#20154;&#31867;&#21453;&#24212;&#65292;&#23558;&#23545;&#25239;&#27169;&#22411;&#21644;&#22330;&#26223;&#20013;&#30896;&#25758;&#25968;&#37327;&#36827;&#34892;&#32447;&#24615;&#32452;&#21512;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#23545;&#20154;&#31867;&#21453;&#24212;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#34920;&#26126;&#20154;&#31867;&#36861;&#27714;&#39640;&#20449;&#24687;&#22686;&#30410;&#21644;&#36523;&#20307;&#27963;&#21160;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26159;&#26377;&#39537;&#21160;&#21147;&#30340;&#20114;&#21160;&#24615;&#20195;&#29702;&#65292;&#20182;&#20204;&#36861;&#27714;&#26377;&#36259;&#30340;&#29289;&#29702;&#21160;&#21147;&#23398;&#24773;&#22659;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24418;&#24335;&#21270;&#30340;&#29289;&#29702;&#20869;&#22312;&#21160;&#26426;&#24418;&#24335;&#12290;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;&#20154;&#31867;&#23545;&#22810;&#31181;&#29289;&#29702;&#24773;&#22659;&#30340;&#35780;&#20998;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#29616;&#20381;&#36182;&#20110;&#31616;&#21333;&#22330;&#26223;&#29305;&#24449;&#30340;&#27169;&#22411;&#21040;&#20381;&#36182;&#20110;&#21069;&#21521;&#29289;&#29702;&#39044;&#27979;&#30340;&#27169;&#22411;&#30340;&#21508;&#31181;&#20869;&#22312;&#21160;&#26426;&#20551;&#35774;&#26469;&#24314;&#27169;&#20154;&#31867;&#30340;&#36259;&#21619;&#21453;&#24212;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#20154;&#31867;&#21453;&#24212;&#30340;&#21333;&#19968;&#26368;&#20339;&#39044;&#27979;&#22120;&#26159;&#38024;&#23545;&#29289;&#29702;&#39044;&#27979;&#25439;&#22833;&#25512;&#23548;&#20986;&#30340;&#23545;&#25239;&#24615;&#22870;&#21169;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#31616;&#21333;&#30340;&#22330;&#26223;&#29305;&#24449;&#27169;&#22411;&#19981;&#33021;&#22312;&#25152;&#26377;&#24773;&#22659;&#20013;&#25512;&#24191;&#20182;&#20204;&#23545;&#20154;&#31867;&#21453;&#24212;&#30340;&#39044;&#27979;&#12290;&#26368;&#21518;&#65292;&#23558;&#23545;&#25239;&#27169;&#22411;&#19982;&#22330;&#26223;&#20013;&#30896;&#25758;&#25968;&#37327;&#36827;&#34892;&#32447;&#24615;&#32452;&#21512;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#23545;&#20154;&#31867;&#21453;&#24212;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#34920;&#26126;&#20154;&#31867;&#20542;&#21521;&#20110;&#36861;&#27714;&#39640;&#20449;&#24687;&#22686;&#30410;&#21644;&#36523;&#20307;&#27963;&#21160;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans are interactive agents driven to seek out situations with interesting physical dynamics. Here we formalize the functional form of physical intrinsic motivation. We first collect ratings of how interesting humans find a variety of physics scenarios. We then model human interestingness responses by implementing various hypotheses of intrinsic motivation including models that rely on simple scene features to models that depend on forward physics prediction. We find that the single best predictor of human responses is adversarial reward, a model derived from physical prediction loss. We also find that simple scene feature models do not generalize their prediction of human responses across all scenarios. Finally, linearly combining the adversarial model with the number of collisions in a scene leads to the greatest improvement in predictivity of human responses, suggesting humans are driven towards scenarios that result in high information gain and physical activity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21516;&#26102;&#23398;&#20064;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#23558;&#36801;&#31227;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#21407;&#29702;&#24212;&#29992;&#20110;&#21860;&#37202;&#29983;&#20135;&#20013;&#30340;&#21860;&#37202;&#33457;&#21697;&#31181;&#20998;&#31867;&#65292;&#21033;&#29992;&#36741;&#21161;&#25968;&#25454;&#38598;&#30340;&#21327;&#21516;&#20316;&#29992;&#22686;&#24378;&#33719;&#21462;&#39640;&#24230;&#30456;&#20851;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#21516;&#26102;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2305.13447</link><description>&lt;p&gt;
&#21516;&#26102;&#23398;&#20064;&#27491;&#21017;&#21270;&#26041;&#27861;&#65306;&#20197;&#21860;&#37202;&#33457;&#20998;&#31867;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Regularization Through Simultaneous Learning: A Case Study for Hop Classification. (arXiv:2305.13447v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21516;&#26102;&#23398;&#20064;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#23558;&#36801;&#31227;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#21407;&#29702;&#24212;&#29992;&#20110;&#21860;&#37202;&#29983;&#20135;&#20013;&#30340;&#21860;&#37202;&#33457;&#21697;&#31181;&#20998;&#31867;&#65292;&#21033;&#29992;&#36741;&#21161;&#25968;&#25454;&#38598;&#30340;&#21327;&#21516;&#20316;&#29992;&#22686;&#24378;&#33719;&#21462;&#39640;&#24230;&#30456;&#20851;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#21516;&#26102;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#25311;&#21512;&#20173;&#28982;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#19968;&#20010;&#26222;&#36941;&#25361;&#25112;&#65292;&#23548;&#33268;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#19981;&#20339;&#12290;&#37319;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#26159;&#25269;&#21046;&#36825;&#19968;&#25361;&#25112;&#30340;&#24120;&#35265;&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65306;Simultaneous Learning&#65292;&#23427;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#21407;&#29702;&#65292;&#19987;&#38376;&#24212;&#29992;&#20110;&#21860;&#37202;&#29983;&#20135;&#20013;&#30340;&#21860;&#37202;&#33457;&#21697;&#31181;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#36741;&#21161;&#25968;&#25454;&#38598;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#19982;&#30446;&#26631;&#25968;&#25454;&#38598;&#21327;&#21516;&#24037;&#20316;&#65292;&#20174;&#32780;&#22686;&#24378;&#33719;&#21462;&#39640;&#24230;&#30456;&#20851;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#30340;&#26368;&#32456;&#23618;&#36827;&#34892;&#25112;&#30053;&#24615;&#20462;&#25913;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#21516;&#26102;&#20998;&#31867;&#65292;&#26080;&#38656;&#23558;&#23427;&#20204;&#35270;&#20026;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#21253;&#25324;&#32452;&#38388;&#24809;&#32602;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;InceptionV3&#21644;ResNet50&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#25351;&#23450;&#20102;UFOP-HVD&#21860;&#37202;&#33457;&#21494;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Overfitting remains a prevalent challenge in deep neural networks, leading to suboptimal real-world performance. Employing regularization techniques is a common strategy to counter this challenge, improving model generalization. This paper proposes Simultaneous Learning, a novel regularization approach drawing on Transfer Learning and Multi-task Learning principles, applied specifically to the classification of hop varieties - an integral component of beer production. Our approach harnesses the power of auxiliary datasets in synergy with the target dataset to amplify the acquisition of highly relevant features. Through a strategic modification of the model's final layer, we enable the simultaneous classification of both datasets without the necessity to treat them as disparate tasks. To realize this, we formulate a loss function that includes an inter-group penalty. We conducted experimental evaluations using the InceptionV3 and ResNet50 models, designating the UFOP-HVD hop leaf datase
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#22823;&#35268;&#27169;&#30340;&#21333;&#27169;&#24577;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30005;&#21830;&#24212;&#29992;&#20013;&#35270;&#35273;&#34920;&#31034;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#28040;&#34701;&#30740;&#31350;&#21644;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.13399</link><description>&lt;p&gt;
&#39640;&#25928;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Large-Scale Vision Representation Learning. (arXiv:2305.13399v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#22823;&#35268;&#27169;&#30340;&#21333;&#27169;&#24577;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30005;&#21830;&#24212;&#29992;&#20013;&#35270;&#35273;&#34920;&#31034;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#28040;&#34701;&#30740;&#31350;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#21333;&#27169;&#24577;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20102;&#35299;&#20135;&#21697;&#20869;&#23481;&#30340;&#35270;&#35273;&#34920;&#31034;&#23545;&#30005;&#21830;&#25512;&#33616;&#12289;&#25628;&#32034;&#21644;&#24191;&#21578;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#21644;&#23545;&#27604;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#26377;&#25928;&#24494;&#35843;&#22823;&#35268;&#27169;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;&#22810;&#31181;&#39044;&#35757;&#32451;&#30340;&#39592;&#24178;&#26550;&#26500;&#65292;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#31995;&#21015;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#30005;&#23376;&#21830;&#21153;&#24212;&#29992;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#30340;&#25361;&#25112;&#65292;&#24182;&#31361;&#20986;&#20102;&#26356;&#26377;&#25928;&#22320;&#35757;&#32451;&#12289;&#35780;&#20272;&#21644;&#25552;&#20379;&#35270;&#35273;&#34920;&#31034;&#30340;&#21162;&#21147;&#12290;&#25105;&#20204;&#20026;&#20960;&#20010;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#21253;&#25324;&#25105;&#20204;&#30340;&#35270;&#35273;&#30456;&#20284;&#24191;&#21578;&#25512;&#33616;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25152;&#24471;&#35270;&#35273;&#34920;&#31034;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#31163;&#32447;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#31163;&#32447;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#35270;&#35273;&#30456;&#20284;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we present our approach to single-modality vision representation learning. Understanding vision representations of product content is vital for recommendations, search, and advertising applications in e-commerce. We detail and contrast techniques used to fine tune large-scale vision representation learning models in an efficient manner under low-resource settings, including several pretrained backbone architectures, both in the convolutional neural network as well as the vision transformer family. We highlight the challenges for e-commerce applications at-scale and highlight the efforts to more efficiently train, evaluate, and serve visual representations. We present ablation studies for several downstream tasks, including our visually similar ad recommendations. We evaluate the offline performance of the derived visual representations in downstream tasks. To this end, we present a novel text-to-image generative offline evaluation method for visually similar recommenda
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#31867;&#20989;&#25968;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;(mfDNN)&#65292;&#21487;&#20197;&#35299;&#20915;&#20989;&#25968;&#35266;&#27979;&#20540;&#22312;&#22810;&#32500;&#22495;&#19978;&#30340;&#26080;&#38480;&#32500;&#29305;&#24449;&#38590;&#20197;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;&#22312;&#22810;&#31867;&#20998;&#31867;&#35774;&#32622;&#20013;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#36890;&#36807;&#20351;&#29992;&#24102;&#26377;ReLU&#30340;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#27492;&#32593;&#32476;&#21487;&#20197;&#22312;&#29616;&#20195;&#35745;&#31639;&#24037;&#20855;&#19979;&#23454;&#29616;&#12290;&#36890;&#36807;&#23545;&#27169;&#25311;&#25968;&#25454;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;mfDNN&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13349</link><description>&lt;p&gt;
&#22810;&#32500;&#20989;&#25968;&#25968;&#25454;&#30340;&#22810;&#31867;&#20998;&#31867;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multiclass classification for multidimensional functional data through deep neural networks. (arXiv:2305.13349v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13349
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#31867;&#20989;&#25968;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;(mfDNN)&#65292;&#21487;&#20197;&#35299;&#20915;&#20989;&#25968;&#35266;&#27979;&#20540;&#22312;&#22810;&#32500;&#22495;&#19978;&#30340;&#26080;&#38480;&#32500;&#29305;&#24449;&#38590;&#20197;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;&#22312;&#22810;&#31867;&#20998;&#31867;&#35774;&#32622;&#20013;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#36890;&#36807;&#20351;&#29992;&#24102;&#26377;ReLU&#30340;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#27492;&#32593;&#32476;&#21487;&#20197;&#22312;&#29616;&#20195;&#35745;&#31639;&#24037;&#20855;&#19979;&#23454;&#29616;&#12290;&#36890;&#36807;&#23545;&#27169;&#25311;&#25968;&#25454;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;mfDNN&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20989;&#25968;&#35266;&#27979;&#20540;&#22312;&#22810;&#32500;&#22495;&#19978;&#30340;&#20869;&#22312;&#26080;&#38480;&#32500;&#29305;&#24449;&#20351;&#24471;&#26631;&#20934;&#20998;&#31867;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#19981;&#36866;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#31867;&#20989;&#25968;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(mfDNN)&#20998;&#31867;&#22120;&#20316;&#20026;&#19968;&#31181;&#21019;&#26032;&#30340;&#25968;&#25454;&#25366;&#25496;&#21644;&#20998;&#31867;&#24037;&#20855;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#24102;&#26377;&#25972;&#27969;&#32447;&#24615;&#21333;&#20803;(ReLU)&#28608;&#27963;&#20989;&#25968;&#30340;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#22312;&#22810;&#31867;&#20998;&#31867;&#35774;&#32622;&#20013;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#25439;&#22833;&#12290;&#36825;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#23454;&#29616;&#20013;&#20351;&#29992;&#29616;&#20195;&#35745;&#31639;&#24037;&#20855;&#12290;&#23545;&#20110;&#23436;&#20840;&#35266;&#23519;&#21644;&#31163;&#25955;&#35266;&#23519;&#30340;&#22810;&#32500;&#20989;&#25968;&#25968;&#25454;&#65292;&#36824;&#25512;&#23548;&#20986;&#20102;&#35823;&#20998;&#31867;&#39118;&#38505;&#20989;&#25968;&#30340;&#25910;&#25947;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;mfDNN&#22312;&#27169;&#25311;&#25968;&#25454;&#21644;&#26469;&#33258;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#30340;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The intrinsically infinite-dimensional features of the functional observations over multidimensional domains render the standard classification methods effectively inapplicable. To address this problem, we introduce a novel multiclass functional deep neural network (mfDNN) classifier as an innovative data mining and classification tool. Specifically, we consider sparse deep neural network architecture with rectifier linear unit (ReLU) activation function and minimize the cross-entropy loss in the multiclass classification setup. This neural network architecture allows us to employ modern computational tools in the implementation. The convergence rates of the misclassification risk functions are also derived for both fully observed and discretely observed multidimensional functional data. We demonstrate the performance of mfDNN on simulated data and several benchmark datasets from different application domains.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BEHRT&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#21307;&#30103;&#27010;&#24565;&#23884;&#20837;&#65292;&#22312;&#19981;&#20256;&#36755;&#20219;&#20309;&#38544;&#31169;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#22810;&#20010;&#21307;&#30103;&#20013;&#24515;&#30340;EHR&#25968;&#25454;&#30340;&#20849;&#21516;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#30149;&#20154;&#19979;&#19968;&#27425;&#35786;&#26029;&#26041;&#38754;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.13052</link><description>&lt;p&gt;
&#22522;&#20110;BEHRT&#30340;&#21307;&#30103;&#27010;&#24565;&#23884;&#20837;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning of Medical Concepts Embedding using BEHRT. (arXiv:2305.13052v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13052
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BEHRT&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#21307;&#30103;&#27010;&#24565;&#23884;&#20837;&#65292;&#22312;&#19981;&#20256;&#36755;&#20219;&#20309;&#38544;&#31169;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#22810;&#20010;&#21307;&#30103;&#20013;&#24515;&#30340;EHR&#25968;&#25454;&#30340;&#20849;&#21516;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#30149;&#20154;&#19979;&#19968;&#27425;&#35786;&#26029;&#26041;&#38754;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#21253;&#21547;&#30149;&#20154;&#30340;&#35786;&#26029;&#12289;&#33647;&#29289;&#12289;&#27835;&#30103;&#31561;&#21307;&#30103;&#35760;&#24405;&#12290;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#34987;&#35270;&#20026;&#25935;&#24863;&#30340;&#21307;&#30103;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#21307;&#30103;&#20013;&#24515;&#30340;EHR&#25968;&#25454;&#36890;&#24120;&#19981;&#33021;&#34987;&#20849;&#20139;&#65292;&#36825;&#20351;&#24471;&#20351;&#29992;&#22810;&#20010;&#20013;&#24515;&#30340;EHR&#25968;&#25454;&#21019;&#24314;&#39044;&#27979;&#27169;&#22411;&#21464;&#24471;&#22256;&#38590;&#65292;&#32780;&#36825;&#23545;&#20110;&#27169;&#22411;&#30340;&#24378;&#20581;&#24615;&#21644;&#26222;&#36866;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#31639;&#27861;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#20301;&#32622;&#30340;&#25968;&#25454;&#19978;&#23398;&#20064;&#20849;&#20139;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#23558;&#25152;&#26377;&#25968;&#25454;&#23384;&#20648;&#22312;&#20013;&#24515;&#20301;&#32622;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#21307;&#30103;&#27010;&#24565;&#23884;&#20837;&#12290;&#36825;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#21483;&#20570;BEHRT&#65292;&#26159;&#22312;&#19968;&#20010;&#22823;&#22411;EHR&#25968;&#25454;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#65292;&#23427;&#23558;&#21307;&#30103;&#27010;&#24565;&#32534;&#30721;&#25104;&#39640;&#32500;&#21521;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22810;&#20010;&#21307;&#30103;&#20013;&#24515;&#30340;EHR&#25968;&#25454;&#20013;&#33719;&#21462;&#21307;&#30103;&#27010;&#24565;&#30340;&#23884;&#20837;&#65292;&#32780;&#19981;&#38656;&#35201;&#22312;&#23427;&#20204;&#20043;&#38388;&#20132;&#25442;&#35760;&#24405;&#12290;&#25105;&#20204;&#23558;&#27169;&#22411;&#24212;&#29992;&#20110;&#19968;&#20010;&#23454;&#38469;&#30340;&#39044;&#27979;&#20219;&#21153;&#65292;&#20351;&#29992;&#22810;&#20010;&#20013;&#24515;&#30340;EHR&#25968;&#25454;&#26469;&#39044;&#27979;&#19979;&#19968;&#27425;&#35786;&#26029;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic Health Records (EHR) data contains medical records such as diagnoses, medications, procedures, and treatments of patients. This data is often considered sensitive medical information. Therefore, the EHR data from the medical centers often cannot be shared, making it difficult to create prediction models using multi-center EHR data, which is essential for such models' robustness and generalizability. Federated Learning (FL) is an algorithmic approach that allows learning a shared model using data in multiple locations without the need to store all data in a central place. An example of a prediction model's task is to predict future diseases. More specifically, the model needs to predict patient's next visit diagnoses, based on current and previous clinical data. Such a prediction model can support care providers in making clinical decisions and even provide preventive treatment. We propose a federated learning approach for learning medical concepts embedding. This pre-trained
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SCNN&#30340;&#27169;&#22359;&#21270;&#21644;&#35299;&#37322;&#24615;&#30340;&#39044;&#27979;&#26694;&#26550;&#65292;&#26088;&#22312;&#21333;&#29420;&#23545;&#31354;&#38388;-&#26102;&#38388;&#27169;&#24335;&#30340;&#27599;&#20010;&#25104;&#20998;&#36827;&#34892;&#24314;&#27169;&#12290;SCNN&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;MTS&#29983;&#25104;&#36807;&#31243;&#65292;&#23558;MTS&#25968;&#25454;&#20998;&#35299;&#20026;&#32467;&#26500;&#21270;&#21644;&#24322;&#26500;&#25104;&#20998;&#65292;&#28982;&#21518;&#20998;&#21035;&#25512;&#26029;&#36825;&#20123;&#25104;&#20998;&#30340;&#28436;&#21270;&#65292;&#33021;&#22815;&#23454;&#29616;&#27604;&#29616;&#26377;&#20808;&#36827;&#27169;&#22411;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13036</link><description>&lt;p&gt;
&#23398;&#20064;&#32467;&#26500;&#21270;&#25104;&#20998;&#65306;&#36808;&#21521;&#27169;&#22359;&#21270;&#19988;&#21487;&#35299;&#37322;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Structured Components: Towards Modular and Interpretable Multivariate Time Series Forecasting. (arXiv:2305.13036v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SCNN&#30340;&#27169;&#22359;&#21270;&#21644;&#35299;&#37322;&#24615;&#30340;&#39044;&#27979;&#26694;&#26550;&#65292;&#26088;&#22312;&#21333;&#29420;&#23545;&#31354;&#38388;-&#26102;&#38388;&#27169;&#24335;&#30340;&#27599;&#20010;&#25104;&#20998;&#36827;&#34892;&#24314;&#27169;&#12290;SCNN&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;MTS&#29983;&#25104;&#36807;&#31243;&#65292;&#23558;MTS&#25968;&#25454;&#20998;&#35299;&#20026;&#32467;&#26500;&#21270;&#21644;&#24322;&#26500;&#25104;&#20998;&#65292;&#28982;&#21518;&#20998;&#21035;&#25512;&#26029;&#36825;&#20123;&#25104;&#20998;&#30340;&#28436;&#21270;&#65292;&#33021;&#22815;&#23454;&#29616;&#27604;&#29616;&#26377;&#20808;&#36827;&#27169;&#22411;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#26159;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#19968;&#20010;&#26497;&#20026;&#37325;&#35201;&#21644;&#22522;&#26412;&#30340;&#38382;&#39064;&#12290; MTS&#39044;&#27979;&#20013;&#30340;&#26680;&#24515;&#38382;&#39064;&#26159;&#22914;&#20309;&#26377;&#25928;&#22320;&#24314;&#27169;&#22797;&#26434;&#30340;&#31354;&#38388; - &#26102;&#38388;&#27169;&#24335;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#19988;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#21333;&#29420;&#23545;&#31354;&#38388; - &#26102;&#38388;&#27169;&#24335;&#30340;&#27599;&#20010;&#25104;&#20998;&#36827;&#34892;&#24314;&#27169;&#12290; &#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#21629;&#21517;&#20026;SCNN&#65292;&#32553;&#20889;&#20026;&#32467;&#26500;&#21270;&#22522;&#20110;&#25104;&#20998;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate time-series (MTS) forecasting is a paramount and fundamental problem in many real-world applications. The core issue in MTS forecasting is how to effectively model complex spatial-temporal patterns. In this paper, we develop a modular and interpretable forecasting framework, which seeks to individually model each component of the spatial-temporal patterns. We name this framework SCNN, short for Structured Component-based Neural Network. SCNN works with a pre-defined generative process of MTS, which arithmetically characterizes the latent structure of the spatial-temporal patterns. In line with its reverse process, SCNN decouples MTS data into structured and heterogeneous components and then respectively extrapolates the evolution of these components, the dynamics of which is more traceable and predictable than the original MTS. Extensive experiments are conducted to demonstrate that SCNN can achieve superior performance over state-of-the-art models on three real-world data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#22312;&#21482;&#26377;&#37096;&#20998;&#35206;&#30422;&#25968;&#25454;&#38598;&#21644;&#24369;&#21487;&#23454;&#29616;&#20989;&#25968;&#31867;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#35206;&#30422;&#20998;&#24067;&#30340;&#38468;&#21152;&#20391;&#20449;&#24687;&#23454;&#29616;&#20102;&#26679;&#26412;&#26377;&#25928;&#31163;&#32447;RL&#65292;&#24182;&#23637;&#31034;&#20102;&#35206;&#30422;&#20998;&#24067;&#22312;&#20808;&#39564;&#30693;&#35782;&#21644;&#25152;&#38656;&#38468;&#21152;&#25968;&#25454;&#37327;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.12679</link><description>&lt;p&gt;
&#24102;&#26377;&#38468;&#21152;&#35206;&#30422;&#20998;&#24067;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning with Additional Covering Distributions. (arXiv:2305.12679v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#22312;&#21482;&#26377;&#37096;&#20998;&#35206;&#30422;&#25968;&#25454;&#38598;&#21644;&#24369;&#21487;&#23454;&#29616;&#20989;&#25968;&#31867;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#35206;&#30422;&#20998;&#24067;&#30340;&#38468;&#21152;&#20391;&#20449;&#24687;&#23454;&#29616;&#20102;&#26679;&#26412;&#26377;&#25928;&#31163;&#32447;RL&#65292;&#24182;&#23637;&#31034;&#20102;&#35206;&#30422;&#20998;&#24067;&#22312;&#20808;&#39564;&#30693;&#35782;&#21644;&#25152;&#38656;&#38468;&#21152;&#25968;&#25454;&#37327;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#20989;&#25968;&#36924;&#36817;&#20174;&#26085;&#24535;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#65292;&#21363;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;&#23613;&#31649;&#24050;&#32463;&#20184;&#20986;&#20102;&#24456;&#22810;&#21162;&#21147;&#65292;&#22312;&#20855;&#26377;&#29702;&#35770;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#30340;&#29616;&#26377;&#31639;&#27861;&#20013;&#65292;&#36890;&#24120;&#20551;&#35774;&#20855;&#26377;&#25506;&#32034;&#24615;&#25968;&#25454;&#35206;&#30422;&#25110;&#24378;&#21487;&#23454;&#29616;&#30340;&#20989;&#25968;&#31867;&#65292;&#36825;&#22312;&#29616;&#23454;&#20013;&#24456;&#38590;&#28385;&#36275;&#12290;&#34429;&#28982;&#26368;&#36817;&#26377;&#19968;&#20123;&#25104;&#21151;&#35299;&#20915;&#36825;&#20123;&#24378;&#20551;&#35774;&#30340;&#20316;&#21697;&#65292;&#20294;&#23427;&#20204;&#35201;&#20040;&#38656;&#35201;&#21482;&#33021;&#30001;&#19968;&#37096;&#20998;MDP&#28385;&#36275;&#30340;&#38388;&#38553;&#20551;&#35774;&#65292;&#35201;&#20040;&#20351;&#29992;&#34892;&#20026;&#27491;&#21017;&#21270;&#65292;&#20351;&#24471;&#23398;&#20064;&#31574;&#30053;&#30340;&#26368;&#20248;&#24615;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#36793;&#38469;&#37325;&#35201;&#24615;&#25277;&#26679;(MIS)&#30340;&#31616;&#21333;&#31639;&#27861;&#30340;&#26679;&#26412;&#26377;&#38480;&#20445;&#35777;&#65292;&#35777;&#26126;&#20102;&#22312;&#32473;&#23450;&#35206;&#30422;&#20998;&#24067;&#30340;&#38468;&#21152;&#20391;&#20449;&#24687;&#19979;&#20165;&#20855;&#26377;&#37096;&#20998;&#35206;&#30422;&#25968;&#25454;&#38598;&#21644;&#24369;&#21487;&#23454;&#29616;&#20989;&#25968;&#31867;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#29992;MDP&#30340;&#26679;&#26412;&#26377;&#25928;&#31163;&#32447;RL&#26159;&#21487;&#33021;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35206;&#30422;&#20998;&#24067;&#22312;&#20808;&#39564;&#30693;&#35782;&#21644;&#25152;&#38656;&#38468;&#21152;&#25968;&#25454;&#37327;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#23427;&#33021;&#22815;&#26377;&#30410;&#20110;&#23398;&#20064;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study learning optimal policies from a logged dataset, i.e., offline RL, with function approximation. Despite the efforts devoted, existing algorithms with theoretic finite-sample guarantees typically assume exploratory data coverage or strong realizable function classes, which is hard to be satisfied in reality. While there are recent works that successfully tackle these strong assumptions, they either require the gap assumptions that only could be satisfied by part of MDPs or use the behavior regularization that makes the optimality of learned policy even intractable. To solve this challenge, we provide finite-sample guarantees for a simple algorithm based on marginalized importance sampling (MIS), showing that sample-efficient offline RL for general MDPs is possible with only a partial coverage dataset and weak realizable function classes given additional side information of a covering distribution. Furthermore, we demonstrate that the covering distribution trades off prior knowl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.11490</link><description>&lt;p&gt;
LLM&#33258;&#36523;&#21487;&#35835;&#21462;&#21644;&#29983;&#25104;CXR&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
LLM Itself Can Read and Generate CXR Images. (arXiv:2305.11490v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11490
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#20110;&#36817;&#26399;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#21457;&#23637;&#65292;&#20154;&#20204;&#27491;&#31215;&#26497;&#23581;&#35797;&#23558;LLMs&#30340;&#23454;&#29992;&#24615;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#24050;&#32463;&#26377;&#20154;&#23581;&#35797;&#36830;&#25509;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#19988;&#20063;&#22312;&#19981;&#26029;&#23581;&#35797;&#20026;LLMs&#28155;&#21152;&#35270;&#35273;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23581;&#35797;&#21482;&#20351;&#29992;LLMs&#20316;&#20026;&#22270;&#20687;&#35299;&#30721;&#22120;&#65292;&#27809;&#26377;&#23581;&#35797;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26469;&#29983;&#25104;&#22270;&#20687;&#12290;&#36890;&#36807;&#37319;&#29992;VQ-GAN&#26694;&#26550;&#65292;&#23558;&#22270;&#20687;&#30340;&#28508;&#22312;&#34920;&#31034;&#35270;&#20026;&#19968;&#31181;&#25991;&#26412;&#26631;&#35760;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#65292;&#20197;&#20687;&#25991;&#26412;&#19968;&#26679;&#35835;&#21462;&#21644;&#29983;&#25104;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#25110;&#35757;&#32451;&#19987;&#38376;&#30340;&#32593;&#32476;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;LLM&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#22240;&#20026;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#20449;&#24687;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#32763;&#35793;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building on the recent remarkable development of large language models (LLMs), active attempts are being made to extend the utility of LLMs to multimodal tasks. There have been previous efforts to link language and visual information, and attempts to add visual capabilities to LLMs are ongoing as well. However, existing attempts use LLMs only as image decoders and no attempt has been made to generate images in the same line as the natural language. By adopting a VQ-GAN framework in which latent representations of images are treated as a kind of text tokens, we present a novel method to fine-tune a pre-trained LLM to read and generate images like text without any structural changes, extra training objectives, or the need for training an ad-hoc network while still preserving the of the instruction-following capability of the LLM. We apply this framework to chest X-ray (CXR) image and report generation tasks as it is a domain in which translation of complex information between visual and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#27867;&#21270;&#30340;&#28145;&#24230;&#22270;&#24418;&#36716;&#25442;&#26041;&#27861;&#65292;&#20351;&#29992;&#36229;&#32593;&#32476;&#36827;&#34892;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#65292;&#36890;&#36807;&#21152;&#20837;&#28508;&#22312;&#21464;&#37327;&#26469;&#35757;&#32451;&#27867;&#21270;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11389</link><description>&lt;p&gt;
&#38754;&#21521;&#39046;&#22495;&#27867;&#21270;&#30340;&#28145;&#24230;&#22270;&#24418;&#36716;&#25442;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization Deep Graph Transformation. (arXiv:2305.11389v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#27867;&#21270;&#30340;&#28145;&#24230;&#22270;&#24418;&#36716;&#25442;&#26041;&#27861;&#65292;&#20351;&#29992;&#36229;&#32593;&#32476;&#36827;&#34892;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#65292;&#36890;&#36807;&#21152;&#20837;&#28508;&#22312;&#21464;&#37327;&#26469;&#35757;&#32451;&#27867;&#21270;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#22270;&#24418;&#20174;&#19968;&#31181;&#27169;&#24335;&#36716;&#21464;&#20026;&#21478;&#19968;&#31181;&#27169;&#24335;&#30340;&#22270;&#24418;&#36716;&#25442;&#26159;&#19968;&#20010;&#37325;&#35201;&#21644;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#34429;&#28982;&#22312;&#24320;&#21457;&#20808;&#36827;&#30340;&#22270;&#24418;&#36716;&#25442;&#25216;&#26415;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22810;&#36827;&#23637;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#38656;&#30340;&#22522;&#26412;&#20551;&#35774;&#26159;&#27979;&#35797;&#21644;&#35757;&#32451;&#25968;&#25454;&#20445;&#25345;&#30456;&#21516;&#30340;&#20998;&#24067;&#65292;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#12290;&#22240;&#27492;&#65292;&#39044;&#27979;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#21487;&#29992;&#30340;&#22270;&#24418;&#30340;&#39046;&#22495;&#27867;&#21270;&#22270;&#24418;&#36716;&#25442;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#38656;&#35201;&#35299;&#20915;&#22810;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21253;&#25324;&#65288;1&#65289;&#24403;&#35757;&#32451;&#25152;&#26377;&#36755;&#20837;-&#36755;&#20986;&#27169;&#24335;&#32452;&#21512;&#26102;&#30340;&#26497;&#31471;&#31354;&#38388;&#22797;&#26434;&#24230;&#12289;&#65288;2&#65289;&#36755;&#20837;&#21644;&#36755;&#20986;&#27169;&#24335;&#20043;&#38388;&#30340;&#22270;&#24418;&#25299;&#25169;&#24046;&#24322;&#65292;&#20197;&#21450;(3)&#22914;&#20309;&#23558;&#27169;&#22411;&#27867;&#21270;&#21040;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#23384;&#22312;&#30340;&#65288;&#26410;&#35265;&#36807;&#30340;&#65289;&#30446;&#26631;&#22495;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#36755;&#20837;-&#22810;&#36755;&#20986;&#12289;&#36229;&#32593;&#32476;&#30340;&#22270;&#24418;&#36716;&#25442;&#26041;&#27861;&#65288;MultiHyperGNN&#65289;&#65292;&#23427;&#21033;&#29992;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#26469;&#32534;&#30721;&#36755;&#20837;&#21644;&#36755;&#20986;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24182;&#29983;&#25104;&#36755;&#20986;&#22270;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#39046;&#22495;&#27867;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#20837;&#25429;&#25417;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#30340;&#28508;&#22312;&#21464;&#37327;&#65292;&#26469;&#20197;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph transformation that predicts graph transition from one mode to another is an important and common problem. Despite much progress in developing advanced graph transformation techniques in recent years, the fundamental assumption typically required in machine-learning models that the testing and training data preserve the same distribution does not always hold. As a result, domain generalization graph transformation that predicts graphs not available in the training data is under-explored, with multiple key challenges to be addressed including (1) the extreme space complexity when training on all input-output mode combinations, (2) difference of graph topologies between the input and the output modes, and (3) how to generalize the model to (unseen) target domains that are not in the training data. To fill the gap, we propose a multi-input, multi-output, hypernetwork-based graph neural network (MultiHyperGNN) that employs a encoder and a decoder to encode topologies of both input an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#36890;&#36807;&#22312;&#31243;&#24207;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;&#20351;&#27809;&#26377;&#38024;&#23545;&#23398;&#20064;&#35821;&#35328;&#35821;&#20041;&#25552;&#20379;&#24402;&#32435;&#20559;&#24046;&#65292;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#33021;&#22815;&#23398;&#20064;&#21547;&#20041;&#12290;&#32447;&#24615;&#25506;&#27979;&#22120;&#33021;&#22815;&#20174;&#27169;&#22411;&#29366;&#24577;&#20013;&#25552;&#21462;&#31243;&#24207;&#29366;&#24577;&#30340;&#25277;&#35937;&#65292;&#20934;&#30830;&#24615;&#19982;&#27169;&#22411;&#27867;&#21270;&#21040;&#26032;&#31243;&#24207;&#30340;&#33021;&#21147;&#26174;&#33879;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.11169</link><description>&lt;p&gt;
&#22312;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#35821;&#20041;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Evidence of Meaning in Language Models Trained on Programs. (arXiv:2305.11169v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11169
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#36890;&#36807;&#22312;&#31243;&#24207;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;&#20351;&#27809;&#26377;&#38024;&#23545;&#23398;&#20064;&#35821;&#35328;&#35821;&#20041;&#25552;&#20379;&#24402;&#32435;&#20559;&#24046;&#65292;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#33021;&#22815;&#23398;&#20064;&#21547;&#20041;&#12290;&#32447;&#24615;&#25506;&#27979;&#22120;&#33021;&#22815;&#20174;&#27169;&#22411;&#29366;&#24577;&#20013;&#25552;&#21462;&#31243;&#24207;&#29366;&#24577;&#30340;&#25277;&#35937;&#65292;&#20934;&#30830;&#24615;&#19982;&#27169;&#22411;&#27867;&#21270;&#21040;&#26032;&#31243;&#24207;&#30340;&#33021;&#21147;&#26174;&#33879;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#23613;&#31649;&#34987;&#35757;&#32451;&#21482;&#26159;&#25191;&#34892;&#25991;&#26412;&#19978;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65292;&#29305;&#21035;&#26159;&#19968;&#20010;&#31243;&#24207;&#35821;&#26009;&#24211;&#65292;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#33021;&#22815;&#23398;&#20064;&#21547;&#20041;&#12290;&#27599;&#20010;&#31243;&#24207;&#37117;&#20197;&#65288;&#25991;&#26412;&#65289;&#36755;&#20837;&#36755;&#20986;&#31034;&#20363;&#30340;&#24418;&#24335;&#20316;&#20026;&#35268;&#33539;&#12290;&#19982;&#31243;&#24207;&#19968;&#36215;&#24037;&#20316;&#20351;&#25105;&#20204;&#33021;&#22815;&#31934;&#30830;&#22320;&#23450;&#20041;&#19982;&#35821;&#35328;&#20013;&#26377;&#20851;&#21547;&#20041;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#27491;&#30830;&#24615;&#21644;&#35821;&#20041;&#65289;&#65292;&#20351;&#24471;&#31243;&#24207;&#32508;&#21512;&#25104;&#20026;&#19968;&#20010;&#20013;&#38388;&#27979;&#35797;&#24179;&#21488;&#65292;&#29992;&#20110;&#34920;&#24449;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#21547;&#20041;&#30340;&#23384;&#22312;&#65288;&#25110;&#19981;&#23384;&#22312;&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#31243;&#24207;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;Transformer&#27169;&#22411;&#65292;&#28982;&#21518;&#25506;&#26597;&#20102;&#24050;&#32463;&#23436;&#25104;&#35268;&#33539;&#30340;&#31243;&#24207;&#26102;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#38544;&#34255;&#29366;&#24577;&#12290;&#23613;&#31649;&#27809;&#26377;&#38024;&#23545;&#23398;&#20064;&#35821;&#35328;&#35821;&#20041;&#25552;&#20379;&#24402;&#32435;&#20559;&#24046;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#65292;&#32447;&#24615;&#25506;&#27979;&#22120;&#33021;&#22815;&#20174;&#27169;&#22411;&#29366;&#24577;&#20013;&#25552;&#21462;&#24403;&#21069;&#21644;&#26410;&#26469;&#31243;&#24207;&#29366;&#24577;&#30340;&#25277;&#35937;&#12290;&#27492;&#22806;&#65292;&#32447;&#24615;&#25506;&#27979;&#22120;&#30340;&#20934;&#30830;&#24615;&#19982;&#27169;&#22411;&#27867;&#21270;&#21040;&#26032;&#31243;&#24207;&#30340;&#33021;&#21147;&#24378;&#26377;&#21147;&#12289;&#32479;&#35745;&#23398;&#26174;&#33879;&#22320;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present evidence that language models can learn meaning despite being trained only to perform next token prediction on text, specifically a corpus of programs. Each program is preceded by a specification in the form of (textual) input-output examples. Working with programs enables us to precisely define concepts relevant to meaning in language (e.g., correctness and semantics), making program synthesis well-suited as an intermediate testbed for characterizing the presence (or absence) of meaning in language models.  We first train a Transformer model on the corpus of programs, then probe the trained model's hidden states as it completes a program given a specification. Despite providing no inductive bias toward learning the semantics of the language, we find that a linear probe is able to extract abstractions of both current and future program states from the model states. Moreover, there is a strong, statistically significant correlation between the accuracy of the probe and the mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#8212;&#8212;&#23558;&#20854;&#19982;&#19990;&#30028;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#26377;&#30446;&#30340;&#30340;&#35268;&#21010;&#21644;&#38543;&#26426;&#25506;&#32034;&#33719;&#24471;&#20016;&#23500;&#30340;&#23454;&#20307;&#32463;&#39564;&#36827;&#34892;&#24494;&#35843;, &#20197;&#25552;&#39640;&#20854;&#22312;&#29289;&#29702;&#29615;&#22659;&#19979;&#30340;&#25512;&#29702;&#21644;&#34892;&#20026;&#33021;&#21147;&#65292;&#24182;&#22312;&#35821;&#35328;&#22522;&#20934;&#19978;&#20445;&#25345;&#25110;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10626</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36935;&#35265;&#19990;&#30028;&#27169;&#22411;&#65306;&#23454;&#20307;&#32463;&#39564;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Language Models Meet World Models: Embodied Experiences Enhance Language Models. (arXiv:2305.10626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#8212;&#8212;&#23558;&#20854;&#19982;&#19990;&#30028;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#26377;&#30446;&#30340;&#30340;&#35268;&#21010;&#21644;&#38543;&#26426;&#25506;&#32034;&#33719;&#24471;&#20016;&#23500;&#30340;&#23454;&#20307;&#32463;&#39564;&#36827;&#34892;&#24494;&#35843;, &#20197;&#25552;&#39640;&#20854;&#22312;&#29289;&#29702;&#29615;&#22659;&#19979;&#30340;&#25512;&#29702;&#21644;&#34892;&#20026;&#33021;&#21147;&#65292;&#24182;&#22312;&#35821;&#35328;&#22522;&#20934;&#19978;&#20445;&#25345;&#25110;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LMs) &#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#22312;&#22788;&#29702;&#29289;&#29702;&#29615;&#22659;&#19979;&#30340;&#31616;&#21333;&#25512;&#29702;&#21644;&#35268;&#21010;&#38382;&#39064;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#20363;&#22914;&#29702;&#35299;&#29289;&#20307;&#27704;&#24658;&#25110;&#35268;&#21010;&#23478;&#24237;&#27963;&#21160;&#12290;&#36825;&#31181;&#38480;&#21046;&#28304;&#20110; LM &#20165;&#21463;&#20070;&#38754;&#35821;&#35328;&#35757;&#32451;&#65292;&#32570;&#23569;&#24517;&#35201;&#30340;&#23454;&#20307;&#30693;&#35782;&#21644;&#25216;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378; LM &#30340;&#26041;&#27861;&#65292;&#21363;&#23558;&#20854;&#19982;&#19990;&#30028;&#27169;&#22411;&#30456;&#32467;&#21512;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#33719;&#24471;&#22810;&#26679;&#21270;&#30340;&#23454;&#20307;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#19968;&#33324;&#35821;&#35328;&#33021;&#21147;&#12290;&#26412;&#26041;&#27861;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#37096;&#32626;&#19968;&#20010;&#34701;&#20837;&#23454;&#20307;&#32463;&#39564;&#30340;&#20195;&#29702;&#65292;&#29305;&#21035;&#26159;&#19968;&#20010;&#27169;&#25311;&#29289;&#29702;&#19990;&#30028;&#30340;&#20223;&#30495;&#22120;(VirtualHome)&#65292;&#36890;&#36807;&#26377;&#30446;&#30340;&#30340;&#35268;&#21010;&#21644;&#38543;&#26426;&#25506;&#32034;&#33719;&#24471;&#22810;&#26679;&#21270;&#30340;&#23454;&#20307;&#32463;&#39564;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36825;&#20123;&#32463;&#39564;&#24494;&#35843; LM &#65292;&#20197;&#25945;&#25480;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#21508;&#31181;&#25512;&#29702;&#21644;&#34892;&#20026;&#33021;&#21147;&#65292;&#20363;&#22914;&#35268;&#21010;&#21644;&#23436;&#25104;&#30446;&#26631;&#12289;&#29289;&#20307;&#27704;&#24658;&#21644;&#36319;&#36394;&#31561;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#20197;&#21033;&#29992;&#20854;&#20182;&#27169;&#25311;&#22120;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#25110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#30528;&#25552;&#39640;&#20102; LM &#22312;&#19968;&#31995;&#21015;&#29289;&#29702;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#30041;&#24182;&#32463;&#24120;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;&#35821;&#35328;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LMs) have shown remarkable capabilities across numerous tasks, they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills. In this paper, we propose a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities. Our approach deploys an embodied agent in a world model, particularly a simulator of the physical world (VirtualHome), and acquires a diverse set of embodied experiences through both goal-oriented planning and random exploration. These experiences are then used to finetune LMs to teach diverse abilities of reasoning and acting in the physical world, e.g., planning and completing goals, object permanence and tracking, etc. Moreover, it is
&lt;/p&gt;</description></item><item><title>DoReMi&#26041;&#27861;&#20351;&#29992;&#20998;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#35757;&#32451;&#23567;&#22411;&#20195;&#29702;&#27169;&#22411;&#20197;&#20135;&#29983;&#22495;&#26435;&#37325;&#65292;&#20877;&#20351;&#29992;&#36825;&#20123;&#26435;&#37325;&#37325;&#26032;&#37319;&#26679;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#30456;&#27604;&#20351;&#29992;&#40664;&#35748;&#26435;&#37325;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#22312;The Pile&#21644;GLaM&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;6.5%&#21644;4.7%&#30340;few-shot&#19979;&#28216;&#20934;&#30830;&#24230;&#65292;&#20998;&#21035;&#20351;&#29992;2.6&#20493;&#21644;&#30456;&#21516;&#30340;&#35757;&#32451;&#27493;&#39588;&#36798;&#21040;&#22522;&#32447;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.10429</link><description>&lt;p&gt;
DoReMi: &#20248;&#21270;&#25968;&#25454;&#28151;&#21512;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining. (arXiv:2305.10429v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10429
&lt;/p&gt;
&lt;p&gt;
DoReMi&#26041;&#27861;&#20351;&#29992;&#20998;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#35757;&#32451;&#23567;&#22411;&#20195;&#29702;&#27169;&#22411;&#20197;&#20135;&#29983;&#22495;&#26435;&#37325;&#65292;&#20877;&#20351;&#29992;&#36825;&#20123;&#26435;&#37325;&#37325;&#26032;&#37319;&#26679;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#30456;&#27604;&#20351;&#29992;&#40664;&#35748;&#26435;&#37325;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#22312;The Pile&#21644;GLaM&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;6.5%&#21644;4.7%&#30340;few-shot&#19979;&#28216;&#20934;&#30830;&#24230;&#65292;&#20998;&#21035;&#20351;&#29992;2.6&#20493;&#21644;&#30456;&#21516;&#30340;&#35757;&#32451;&#27493;&#39588;&#36798;&#21040;&#22522;&#32447;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#25968;&#25454;&#22495;&#30340;&#28151;&#21512;&#27604;&#20363;&#65288;&#20363;&#22914;&#65292;&#32500;&#22522;&#30334;&#31185;&#12289;&#22270;&#20070;&#12289;&#32593;&#39029;&#25991;&#26412;&#65289;&#26497;&#22823;&#22320;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DoReMi&#30340;Domain Reweighting with Minimax Optimization&#26041;&#27861;&#65292;&#23427;&#39318;&#20808;&#20351;&#29992;&#20998;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#65288;Group DRO&#65289;&#35757;&#32451;&#19968;&#20010;&#23567;&#20195;&#29702;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#22495;&#26435;&#37325;&#65288;&#28151;&#21512;&#27604;&#20363;&#65289;&#65292;&#32780;&#19981;&#38656;&#35201;&#30693;&#36947;&#19979;&#28216;&#20219;&#21153;&#30340;&#30693;&#35782;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#22495;&#26435;&#37325;&#37325;&#26032;&#37319;&#26679;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#26356;&#22823;&#30340;&#65292;&#20840;&#23610;&#23544;&#30340;&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;DoReMi&#22312;&#19968;&#20010;280M&#21442;&#25968;&#30340;&#20195;&#29702;&#27169;&#22411;&#19978;&#65292;&#26356;&#26377;&#25928;&#22320;&#25214;&#21040;&#35757;&#32451;&#19968;&#20010;8B&#21442;&#25968;&#27169;&#22411;&#65288;30&#20493;&#22823;&#65289;&#30340;&#22495;&#26435;&#37325;&#12290;&#22312;The Pile&#19978;&#65292;&#21363;&#20351;&#22312;&#20943;&#23567;&#19968;&#20123;&#22495;&#30340;&#27604;&#37325;&#26102;&#65292;DoReMi&#20063;&#33021;&#25552;&#39640;&#25152;&#26377;&#22495;&#30340;perplexity&#12290;&#30456;&#27604;&#20351;&#29992;The Pile&#30340;&#40664;&#35748;&#22495;&#26435;&#37325;&#35757;&#32451;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;DoReMi&#23558;&#24179;&#22343;few-shot&#19979;&#28216;&#20934;&#30830;&#24230;&#25552;&#39640;&#20102;6.5%&#65292;&#24182;&#20351;&#29992;2.6&#20493;&#30340;&#35757;&#32451;&#27493;&#39588;&#36798;&#21040;&#22522;&#32447;&#20934;&#30830;&#24230;&#12290;&#22312;GLaM&#25968;&#25454;&#38598;&#19978;&#65292;DoReMi&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;4.7%&#65288;&#27425;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65289;&#30340;few-shot&#20934;&#30830;&#24230;&#65292;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#27493;&#39588;&#19979;&#25552;&#39640;&#20102;9.0%&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full-sized model. In our experiments, we use DoReMi on a 280M-parameter proxy model to find domain weights for training an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves perplexity across all domains, even when it downweights a domain. DoReMi improves average few-shot downstream accuracy by 6.5% over a baseline model trained using The Pile's default domain weights and reaches the baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has no know
&lt;/p&gt;</description></item><item><title>ZeroFlow&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20351;&#29992;&#26080;&#26631;&#31614;&#26041;&#27861;&#29983;&#25104;&#20266;&#26631;&#31614;&#20197;&#30417;&#30563;&#21069;&#21521;&#20256;&#36882;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#20351;&#29992;&#38646;&#20154;&#24037;&#26631;&#31614;&#24773;&#20917;&#19979;&#23545;&#22823;&#35268;&#27169;&#28857;&#20113;&#36827;&#34892;&#23454;&#26102;&#22330;&#26223;&#27969;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.10424</link><description>&lt;p&gt;
ZeroFlow: &#36890;&#36807;&#33976;&#39311;&#23454;&#29616;&#24555;&#36895;&#38646;&#26631;&#31614;&#22330;&#26223;&#27969;
&lt;/p&gt;
&lt;p&gt;
ZeroFlow: Fast Zero Label Scene Flow via Distillation. (arXiv:2305.10424v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10424
&lt;/p&gt;
&lt;p&gt;
ZeroFlow&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20351;&#29992;&#26080;&#26631;&#31614;&#26041;&#27861;&#29983;&#25104;&#20266;&#26631;&#31614;&#20197;&#30417;&#30563;&#21069;&#21521;&#20256;&#36882;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#20351;&#29992;&#38646;&#20154;&#24037;&#26631;&#31614;&#24773;&#20917;&#19979;&#23545;&#22823;&#35268;&#27169;&#28857;&#20113;&#36827;&#34892;&#23454;&#26102;&#22330;&#26223;&#27969;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#27969;&#20272;&#35745;&#26159;&#25551;&#36848;&#36830;&#32493;&#28857;&#20113;&#20043;&#38388;&#30340;&#19977;&#32500;&#36816;&#21160;&#22330;&#30340;&#20219;&#21153;&#12290;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#24378;&#22823;&#30340;&#20808;&#39564;&#30693;&#35782;&#21644;&#27979;&#35797;&#26102;&#20248;&#21270;&#25216;&#26415;&#65292;&#20294;&#23545;&#20110;&#22823;&#35268;&#27169;&#28857;&#20113;&#38656;&#35201;&#25968;&#21313;&#31186;&#30340;&#26102;&#38388;&#65292;&#20351;&#20854;&#26080;&#27861;&#20316;&#20026;&#23454;&#26102;&#24212;&#29992;&#31243;&#24207;&#65288;&#22914;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;&#65289;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#20803;&#20351;&#29992;&#12290;&#21069;&#21521;&#20256;&#36882;&#26041;&#27861;&#30456;&#23545;&#24555;&#36895;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#28857;&#20113;&#30340;&#36816;&#34892;&#26102;&#38388;&#22312;&#25968;&#21313;&#33267;&#25968;&#30334;&#27627;&#31186;&#20043;&#38388;&#65292;&#20294;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#21147;&#30417;&#30563;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#33976;&#39311;&#26694;&#26550; Scene Flow via Distillation&#65292;&#20351;&#29992;&#26080;&#26631;&#31614;&#20248;&#21270;&#26041;&#27861;&#26469;&#29983;&#25104;&#20266;&#26631;&#31614;&#20197;&#30417;&#30563;&#21069;&#21521;&#20256;&#36882;&#27169;&#22411;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#20010;&#26694;&#26550;&#20013;&#30340; ZeroFlow&#65292;&#20351;&#29992;&#38646;&#20154;&#24037;&#26631;&#31614;&#65292;&#22312;&#22823;&#35268;&#27169;&#28857;&#20113;&#19978;&#23454;&#26102;&#29983;&#25104;&#22330;&#26223;&#27969;&#20272;&#35745;&#32467;&#26524;&#65292;&#21516;&#26102;&#36136;&#37327;&#31454;&#20105;&#29366;&#24577;&#19979;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#27979;&#35797;&#26102; ZeroFlow
&lt;/p&gt;
&lt;p&gt;
Scene flow estimation is the task of describing the 3D motion field between temporally successive point clouds. State-of-the-art methods use strong priors and test-time optimization techniques, but require on the order of tens of seconds for large-scale point clouds, making them unusable as computer vision primitives for real-time applications such as open world object detection. Feed forward methods are considerably faster, running on the order of tens to hundreds of milliseconds for large-scale point clouds, but require expensive human supervision. To address both limitations, we propose Scene Flow via Distillation, a simple distillation framework that uses a label-free optimization method to produce pseudo-labels to supervise a feed forward model. Our instantiation of this framework, ZeroFlow, produces scene flow estimates in real-time on large-scale point clouds at quality competitive with state-of-the-art methods while using zero human labels. Notably, at test-time ZeroFlow is ove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;Lasso&#22238;&#24402;&#23545;&#20110;&#31614;&#21517;&#21464;&#25442;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#23545;&#20110;&#19981;&#21516;&#30340;&#36807;&#31243;&#21644;&#26102;&#38388;&#24207;&#21015;&#65292;&#36873;&#25321;&#36866;&#24403;&#30340;&#31614;&#21517;&#23450;&#20041;&#21644;&#38543;&#26426;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;Lasso&#22238;&#24402;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10413</link><description>&lt;p&gt;
&#20351;&#29992;Lasso&#30340;&#31614;&#21517;&#19968;&#33268;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Consistency of Signatures Using Lasso. (arXiv:2305.10413v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;Lasso&#22238;&#24402;&#23545;&#20110;&#31614;&#21517;&#21464;&#25442;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#23545;&#20110;&#19981;&#21516;&#30340;&#36807;&#31243;&#21644;&#26102;&#38388;&#24207;&#21015;&#65292;&#36873;&#25321;&#36866;&#24403;&#30340;&#31614;&#21517;&#23450;&#20041;&#21644;&#38543;&#26426;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;Lasso&#22238;&#24402;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31614;&#21517;&#21464;&#25442;&#26159;&#36830;&#32493;&#21644;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#36845;&#20195;&#36335;&#24452;&#31215;&#20998;&#65292;&#23427;&#20204;&#30340;&#26222;&#36941;&#38750;&#32447;&#24615;&#36890;&#36807;&#32447;&#24615;&#21270;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#12290;&#26412;&#25991;&#22312;&#29702;&#35770;&#21644;&#25968;&#20540;&#19978;&#37325;&#26032;&#23457;&#35270;&#20102;Lasso&#22238;&#24402;&#23545;&#20110;&#31614;&#21517;&#21464;&#25442;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#26356;&#25509;&#36817;&#24067;&#26391;&#36816;&#21160;&#25110;&#20855;&#26377;&#36739;&#24369;&#36328;&#32500;&#24230;&#30456;&#20851;&#24615;&#30340;&#36807;&#31243;&#21644;&#26102;&#38388;&#24207;&#21015;&#65292;&#31614;&#21517;&#23450;&#20041;&#20026;It\^o&#31215;&#20998;&#30340;Lasso&#22238;&#24402;&#26356;&#20855;&#19968;&#33268;&#24615;&#65307;&#23545;&#20110;&#22343;&#20540;&#22238;&#24402;&#36807;&#31243;&#21644;&#26102;&#38388;&#24207;&#21015;&#65292;&#20854;&#31614;&#21517;&#23450;&#20041;&#20026;Stratonovich&#31215;&#20998;&#22312;Lasso&#22238;&#24402;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#32479;&#35745;&#25512;&#26029;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#31614;&#21517;&#21644;&#38543;&#26426;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Signature transforms are iterated path integrals of continuous and discrete-time time series data, and their universal nonlinearity linearizes the problem of feature selection. This paper revisits the consistency issue of Lasso regression for the signature transform, both theoretically and numerically. Our study shows that, for processes and time series that are closer to Brownian motion or random walk with weaker inter-dimensional correlations, the Lasso regression is more consistent for their signatures defined by It\^o integrals; for mean reverting processes and time series, their signatures defined by Stratonovich integrals have more consistency in the Lasso regression. Our findings highlight the importance of choosing appropriate definitions of signatures and stochastic models in statistical inference and machine learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#28216;&#25103;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30495;&#23454;&#21644;&#27169;&#25311;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10361</link><description>&lt;p&gt;
&#38750;&#21512;&#20316;&#21338;&#24328;&#20013;&#30340;&#20154;&#31867;&#36873;&#25321;&#39044;&#27979;&#65306;&#22522;&#20110;&#27169;&#25311;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Human Choice Prediction in Non-Cooperative Games: Simulation-based Off-Policy Evaluation. (arXiv:2305.10361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#28216;&#25103;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30495;&#23454;&#21644;&#27169;&#25311;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#26381;&#28216;&#25103;&#22312;&#32463;&#27982;&#21644;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#24182;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22522;&#20110;&#35821;&#35328;&#30340;&#35828;&#26381;&#28216;&#25103;&#20013;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30495;&#23454;&#21644;&#27169;&#25311;&#20154;&#31867; - &#26426;&#22120;&#20154;&#20132;&#20114;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#30495;&#23454;&#20132;&#20114;&#21644;&#27169;&#25311;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Persuasion games have been fundamental in economics and AI research, and have significant practical applications. Recent works in this area have started to incorporate natural language, moving beyond the traditional stylized message setting. However, previous research has focused on on-policy prediction, where the train and test data have the same distribution, which is not representative of real-life scenarios. In this paper, we tackle the challenging problem of off-policy evaluation (OPE) in language-based persuasion games. To address the inherent difficulty of human data collection in this setup, we propose a novel approach which combines real and simulated human-bot interaction data. Our simulated data is created by an exogenous model assuming decision makers (DMs) start with a mixture of random and decision-theoretic based behaviors and improve over time. We present a deep learning training algorithm that effectively integrates real interaction and simulated data, substantially im
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36870;&#20248;&#21270;&#23398;&#20064;&#30340;&#26032;&#27010;&#24565;&#8212;&#8212;&#20869;&#24515;&#27010;&#24565;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#21487;&#34892;&#20984;&#24418;&#24335;&#65292;&#24182;&#24320;&#21457;&#20102;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;ASL&#20197;&#21450;&#19968;&#38454;&#31639;&#27861;Stochastic Approximate Mirror Descent&#65288;SAM&#65289;&#26469;&#23398;&#20064;&#19987;&#23478;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.07730</link><description>&lt;p&gt;
&#36870;&#20248;&#21270;&#23398;&#20064;&#65306;&#20869;&#24515;&#25104;&#26412;&#12289;&#22686;&#24378;&#27425;&#20248;&#25439;&#22833;&#21644;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning in Inverse Optimization: Incenter Cost, Augmented Suboptimality Loss, and Algorithms. (arXiv:2305.07730v1 [math.OC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36870;&#20248;&#21270;&#23398;&#20064;&#30340;&#26032;&#27010;&#24565;&#8212;&#8212;&#20869;&#24515;&#27010;&#24565;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#21487;&#34892;&#20984;&#24418;&#24335;&#65292;&#24182;&#24320;&#21457;&#20102;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;ASL&#20197;&#21450;&#19968;&#38454;&#31639;&#27861;Stochastic Approximate Mirror Descent&#65288;SAM&#65289;&#26469;&#23398;&#20064;&#19987;&#23478;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36870;&#20248;&#21270;&#23398;&#20064;&#20013;&#65292;&#19987;&#23478;&#20195;&#29702;&#20154;&#35299;&#20915;&#21442;&#25968;&#21270;&#20110;&#22806;&#37096;&#20449;&#21495;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#20174;&#23398;&#20064;&#30340;&#35282;&#24230;&#65292;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#19968;&#20010;&#20449;&#21495;&#21644;&#30456;&#24212;&#26368;&#20248;&#34892;&#21160;&#30340;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#19987;&#23478;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;&#21463;&#21040;&#19982;&#36870;&#20248;&#21270;&#38598;&#19968;&#33268;&#30340;&#25104;&#26412;&#21521;&#37327;&#30340;&#20960;&#20309;&#24418;&#29366;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31867;&#20284;&#20110;Besbes&#31561;&#20154;&#26368;&#36817;&#25552;&#20986;&#30340;&#22806;&#24515;&#27010;&#24565;&#30340; "&#20869;&#24515;"&#27010;&#24565;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20869;&#24515;&#25104;&#26412;&#21521;&#37327;&#30340;&#20960;&#20309;&#21644;&#40065;&#26834;&#24615;&#35299;&#37322;&#65292;&#24182;&#24320;&#21457;&#20102;&#30456;&#24212;&#30340;&#21487;&#34892;&#20984;&#24418;&#24335;&#65292;&#19982;&#22806;&#24515;&#30456;&#21453;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22806;&#25509;&#22278;&#31561;&#25928;&#20110;&#19968;&#20010;&#38590;&#20197;&#22788;&#29702;&#30340;&#20248;&#21270;&#31243;&#24207;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;&#22686;&#24378;&#27425;&#20248;&#25439;&#22833;&#65288;ASL&#65289;&#65292;&#20316;&#20026;&#20869;&#24515;&#27010;&#24565;&#30340;&#19968;&#31181;&#26494;&#24347;&#24418;&#24335;&#65292;&#29992;&#20110;&#22788;&#29702;&#19981;&#19968;&#33268;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#21033;&#29992;ASL&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#38454;&#31639;&#27861;&#65292;&#21629;&#21517;&#20026;&#38543;&#26426;&#36924;&#36817;&#38236;&#20687;&#19979;&#38477;&#12290;&#36825;&#31181;&#31639;&#27861;&#24102;&#26469;&#20102;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Inverse Optimization (IO), an expert agent solves an optimization problem parametric in an exogenous signal. From a learning perspective, the goal is to learn the expert's cost function given a dataset of signals and corresponding optimal actions. Motivated by the geometry of the IO set of consistent cost vectors, we introduce the "incenter" concept, a new notion akin to circumcenter recently proposed by Besbes et al. [2022]. Discussing the geometric and robustness interpretation of the incenter cost vector, we develop corresponding tractable convex reformulations, which are in contrast with the circumcenter, which we show is equivalent to an intractable optimization program. We further propose a novel loss function called Augmented Suboptimality Loss (ASL), as a relaxation of the incenter concept, for problems with inconsistent data. Exploiting the structure of the ASL, we propose a novel first-order algorithm, which we name Stochastic Approximate Mirror Descent. This algorithm com
&lt;/p&gt;</description></item><item><title>Mediapipe&#21644;CNN&#29992;&#20110;&#23454;&#26102;&#32654;&#22269;&#25163;&#35821;&#25163;&#21183;&#35782;&#21035;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#20934;&#30830;&#29575;&#21487;&#36798;99.95&#65285;&#65292;&#26377;&#28508;&#21147;&#29992;&#20110;&#21548;&#21147;&#38556;&#30861;&#20154;&#22763;&#30340;&#36890;&#20449;&#35774;&#22791;&#65292;&#24182;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#30456;&#20284;&#25163;&#35821;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2305.05296</link><description>&lt;p&gt;
Mediapipe&#21644;CNN&#29992;&#20110;&#23454;&#26102;&#32654;&#22269;&#25163;&#35821;&#25163;&#21183;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Mediapipe and CNNs for Real-Time ASL Gesture Recognition. (arXiv:2305.05296v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05296
&lt;/p&gt;
&lt;p&gt;
Mediapipe&#21644;CNN&#29992;&#20110;&#23454;&#26102;&#32654;&#22269;&#25163;&#35821;&#25163;&#21183;&#35782;&#21035;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#20934;&#30830;&#29575;&#21487;&#36798;99.95&#65285;&#65292;&#26377;&#28508;&#21147;&#29992;&#20110;&#21548;&#21147;&#38556;&#30861;&#20154;&#22763;&#30340;&#36890;&#20449;&#35774;&#22791;&#65292;&#24182;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#30456;&#20284;&#25163;&#35821;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#29616;&#20195;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35782;&#21035;&#32654;&#22269;&#25163;&#35821;&#65288;ASL&#65289;&#36816;&#21160;&#30340;&#23454;&#26102;&#31995;&#32479;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;Mediapipe&#24211;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#36827;&#34892;ASL&#25163;&#21183;&#20998;&#31867;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21487;&#20197;&#20197;99.95&#65285;&#30340;&#20934;&#30830;&#29575;&#26816;&#27979;&#25152;&#26377;ASL&#23383;&#27597;&#65292;&#34920;&#26126;&#23427;&#22312;&#20026;&#21548;&#21147;&#38556;&#30861;&#20154;&#22763;&#35774;&#35745;&#30340;&#36890;&#20449;&#35774;&#22791;&#20013;&#26377;&#28508;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20063;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#20855;&#26377;&#30456;&#20284;&#25163;&#37096;&#36816;&#21160;&#30340;&#25163;&#35821;&#65292;&#20174;&#32780;&#21487;&#33021;&#25552;&#39640;&#21548;&#21147;&#20007;&#22833;&#20154;&#22763;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#20351;&#29992;Mediapipe&#21644;CNN&#36827;&#34892;&#23454;&#26102;&#25163;&#35821;&#35782;&#21035;&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper describes a realtime system for identifying American Sign Language (ASL) movements that employs modern computer vision and machine learning approaches. The suggested method makes use of the Mediapipe library for feature extraction and a Convolutional Neural Network (CNN) for ASL gesture classification. The testing results show that the suggested system can detect all ASL alphabets with an accuracy of 99.95%, indicating its potential for use in communication devices for people with hearing impairments. The proposed approach can also be applied to additional sign languages with similar hand motions, potentially increasing the quality of life for people with hearing loss. Overall, the study demonstrates the effectiveness of using Mediapipe and CNN for real-time sign language recognition, making a significant contribution to the field of computer vision and machine learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#21464;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22240;&#26410;&#35266;&#23519;&#21464;&#37327;&#32780;&#22312;&#35266;&#27979;&#25968;&#25454;&#20013;&#23548;&#33268;&#38169;&#35823;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#36866;&#29992;&#20110;&#36830;&#32493;&#21464;&#37327;&#31995;&#32479;&#65292;&#36890;&#36807;&#25552;&#20986;&#27491;&#21017;&#26465;&#20214;&#25511;&#21046;&#31163;&#25955;&#21270;&#35823;&#24046;&#26469;&#35782;&#21035;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.05281</link><description>&lt;p&gt;
&#24102;&#26377;&#26410;&#35266;&#23519;&#21464;&#37327;&#30340;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#65306;&#19968;&#31181;&#20195;&#29702;&#21464;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery with Unobserved Variables: A Proxy Variable Approach. (arXiv:2305.05281v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#21464;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22240;&#26410;&#35266;&#23519;&#21464;&#37327;&#32780;&#22312;&#35266;&#27979;&#25968;&#25454;&#20013;&#23548;&#33268;&#38169;&#35823;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#36866;&#29992;&#20110;&#36830;&#32493;&#21464;&#37327;&#31995;&#32479;&#65292;&#36890;&#36807;&#25552;&#20986;&#27491;&#21017;&#26465;&#20214;&#25511;&#21046;&#31163;&#25955;&#21270;&#35823;&#24046;&#26469;&#35782;&#21035;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26410;&#35266;&#23519;&#21464;&#37327;&#65288;&#20363;&#22914;&#28508;&#22312;&#28151;&#26434;&#25110;&#20013;&#20171;&#65289;&#30340;&#23384;&#22312;&#21487;&#33021;&#20250;&#23548;&#33268;&#38169;&#35823;&#30340;&#22240;&#26524;&#35782;&#21035;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#36817;&#31471;&#22240;&#26524;&#25506;&#32034;&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#26410;&#35266;&#23519;&#21464;&#37327;&#30340;&#20195;&#29702;&#26469;&#35843;&#25972;&#20559;&#24046;&#12290;&#29305;&#21035;&#22320;&#65292;&#22522;&#20110;&#20551;&#35774;&#26816;&#39564;&#30340;&#26041;&#27861;&#36890;&#36807;&#27979;&#35797;&#24341;&#21457;&#30340;&#32447;&#24615;&#36829;&#35268;&#26469;&#35782;&#21035;&#22240;&#26524;&#36793;&#32536;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#36866;&#29992;&#20110;&#26377;&#20005;&#26684;&#32423;&#21035;&#32422;&#26463;&#30340;&#31163;&#25955;&#25968;&#25454;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#25193;&#23637;&#36817;&#31471;&#20551;&#35774;&#26816;&#39564;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20197;&#36866;&#29992;&#20110;&#30001;&#36830;&#32493;&#21464;&#37327;&#32452;&#25104;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#26159;&#25552;&#20986;&#32473;&#23450;&#38544;&#34255;&#22240;&#23376;&#30340;&#35266;&#27979;&#21464;&#37327;&#30340;&#26465;&#20214;&#20998;&#24067;&#30340;&#27491;&#21017;&#26465;&#20214;&#65292;&#20351;&#24471;&#22914;&#26524;&#25105;&#20204;&#23558;&#20854;&#35266;&#23519;&#20195;&#29702;&#20197;&#36275;&#22815;&#30340;&#26377;&#38480;&#32454;&#26684;&#31163;&#25955;&#21270;&#65292;&#21017;&#28041;&#21450;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#21487;&#20197;&#26377;&#25928;&#22320;&#21463;&#21040;&#25511;&#21046;&#12290;&#22522;&#20110;&#36825;&#20010;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#20855;&#26377;&#19968;&#33324;&#20808;&#39564;&#38480;&#21046;&#30340;&#26032;&#36817;&#31471;&#22240;&#26524;&#25628;&#32034;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering causal relations from observational data is important. The existence of unobserved variables (e.g. latent confounding or mediation) can mislead the causal identification. To overcome this problem, proximal causal discovery methods attempted to adjust for the bias via the proxy of the unobserved variable. Particularly, hypothesis test-based methods proposed to identify the causal edge by testing the induced violation of linearity. However, these methods only apply to discrete data with strict level constraints, which limits their practice in the real world. In this paper, we fix this problem by extending the proximal hypothesis test to cases where the system consists of continuous variables. Our strategy is to present regularity conditions on the conditional distributions of the observed variables given the hidden factor, such that if we discretize its observed proxy with sufficiently fine, finite bins, the involved discretization error can be effectively controlled. Based o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#29702;&#21464;&#37327;&#26041;&#27861;&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#23376;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#20013;&#26080;&#38656;&#21442;&#25968;&#32422;&#26463;&#22320;&#35782;&#21035;&#25972;&#20010;&#22240;&#26524;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.05276</link><description>&lt;p&gt;
&#20174;&#23376;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#20013;&#20351;&#29992;&#20195;&#29702;&#21464;&#37327;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery from Subsampled Time Series with Proxy Variables. (arXiv:2305.05276v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#29702;&#21464;&#37327;&#26041;&#27861;&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#23376;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#20013;&#26080;&#38656;&#21442;&#25968;&#32422;&#26463;&#22320;&#35782;&#21035;&#25972;&#20010;&#22240;&#26524;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#26159;&#35768;&#22810;&#31185;&#23398;&#30740;&#31350;&#30340;&#26680;&#24515;&#20852;&#36259;&#12290;&#37319;&#26679;&#39057;&#29575;&#36828;&#20302;&#20110;&#22240;&#26524;&#24433;&#21709;&#39057;&#29575;&#26159;&#27492;&#31867;&#25512;&#26029;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20110;&#27169;&#22411;&#21644;&#38750;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20294;&#26159;&#35201;&#20040;&#23616;&#38480;&#20110;&#32447;&#24615;&#24773;&#20917;&#65292;&#35201;&#20040;&#26080;&#27861;&#24314;&#31435;&#21487;&#35782;&#21035;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#21442;&#25968;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#20174;&#23376;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#35782;&#21035;&#25972;&#20010;&#22240;&#26524;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#30340;&#24605;&#24819;&#26159;&#65292;&#23376;&#37319;&#26679;&#30340;&#25361;&#25112;&#20027;&#35201;&#26469;&#33258;&#20110;&#8220;&#26410;&#35266;&#23519;&#21040;&#8221;&#30340;&#26102;&#38388;&#27493;&#65292;&#22240;&#27492;&#24212;&#20351;&#29992;&#20026;&#26410;&#35266;&#23519;&#21040;&#21464;&#37327;&#35774;&#35745;&#30340;&#24037;&#20855;&#22788;&#29702;&#27492;&#38382;&#39064;&#12290;&#22312;&#36825;&#20123;&#24037;&#20855;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20195;&#29702;&#21464;&#37327;&#26041;&#27861;&#29305;&#21035;&#36866;&#21512;&#65292;&#22240;&#20026;&#26410;&#35266;&#23519;&#21040;&#21464;&#37327;&#30340;&#20195;&#29702;&#21464;&#37327;&#33258;&#28982;&#26159;&#22312;&#35266;&#23519;&#21040;&#30340;&#26102;&#38388;&#27493;&#19978;&#26412;&#36523;&#12290;&#26681;&#25454;&#36825;&#31181;&#30452;&#35273;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#32467;&#26500;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inferring causal structures from time series data is the central interest of many scientific inquiries. A major barrier to such inference is the problem of subsampling, i.e., the frequency of measurements is much lower than that of causal influence. To overcome this problem, numerous model-based and model-free methods have been proposed, yet either limited to the linear case or failed to establish identifiability. In this work, we propose a model-free algorithm that can identify the entire causal structure from subsampled time series, without any parametric constraint. The idea is that the challenge of subsampling arises mainly from \emph{unobserved} time steps and therefore should be handled with tools designed for unobserved variables. Among these tools, we find the proxy variable approach particularly fits, in the sense that the proxy of an unobserved variable is naturally itself at the observed time step. Following this intuition, we establish comprehensive structural identifiabili
&lt;/p&gt;</description></item><item><title>TDC'22&#26159;&#31532;&#19968;&#23626;&#38754;&#21521;ICDs&#20302;&#21151;&#32791;&#24494;&#25511;&#21046;&#22120;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#65288;AI/ML&#65289;&#31639;&#27861;&#21019;&#26032;&#31454;&#36187;&#12290;&#26412;&#27425;&#31454;&#36187;&#30340;&#25361;&#25112;&#26159;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#26032;&#22411;&#23454;&#26102;&#26816;&#27979;&#31639;&#27861;&#65292;&#23545;&#21361;&#21450;&#29983;&#21629;&#30340;&#23460;&#24615;&#24515;&#24459;&#22833;&#24120;&#36827;&#34892;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.05105</link><description>&lt;p&gt;
&#38754;&#21521;&#21361;&#21450;&#29983;&#21629;&#30340;&#23460;&#24615;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#30340;&#24494;&#23567;&#26426;&#22120;&#23398;&#20064;&#35774;&#35745;&#31454;&#36187;
&lt;/p&gt;
&lt;p&gt;
TinyML Design Contest for Life-Threatening Ventricular Arrhythmia Detection. (arXiv:2305.05105v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05105
&lt;/p&gt;
&lt;p&gt;
TDC'22&#26159;&#31532;&#19968;&#23626;&#38754;&#21521;ICDs&#20302;&#21151;&#32791;&#24494;&#25511;&#21046;&#22120;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#65288;AI/ML&#65289;&#31639;&#27861;&#21019;&#26032;&#31454;&#36187;&#12290;&#26412;&#27425;&#31454;&#36187;&#30340;&#25361;&#25112;&#26159;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#26032;&#22411;&#23454;&#26102;&#26816;&#27979;&#31639;&#27861;&#65292;&#23545;&#21361;&#21450;&#29983;&#21629;&#30340;&#23460;&#24615;&#24515;&#24459;&#22833;&#24120;&#36827;&#34892;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#19968;&#23626;ACM/IEEE&#24494;&#23567;&#26426;&#22120;&#23398;&#20064;&#35774;&#35745;&#31454;&#36187;&#65288;TDC&#65289;&#20110;2022&#24180;&#22312;&#31532;41&#23626;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#22269;&#38469;&#20250;&#35758;&#65288;ICCAD&#65289;&#19978;&#20030;&#34892;&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#26376;&#30740;&#21457;&#31454;&#36187;&#12290;TDC'22&#19987;&#27880;&#20110;&#38656;&#35201;&#22312;&#21487;&#26893;&#20837;&#35774;&#22791;&#19978;&#21019;&#26032;&#21644;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#65288;AI/ML&#65289;&#31639;&#27861;&#30340;&#30495;&#23454;&#21307;&#30103;&#38382;&#39064;&#12290;TDC'22&#30340;&#25361;&#25112;&#38382;&#39064;&#26159;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#26032;&#22411;&#23454;&#26102;&#26816;&#27979;&#31639;&#27861;&#65292;&#29992;&#20110;&#24515;&#33039;&#38500;&#39076;&#22120;&#65288;ICDs&#65289;&#19978;&#20351;&#29992;&#30340;&#20302;&#21151;&#29575;&#24494;&#25511;&#21046;&#22120;&#23545;&#21361;&#21450;&#29983;&#21629;&#30340;&#23460;&#24615;&#24515;&#24459;&#22833;&#24120;&#36827;&#34892;&#26816;&#27979;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;90&#20010;&#21463;&#35797;&#32773;&#30340;8&#31181;&#19981;&#21516;&#24515;&#24459;&#31867;&#22411;&#30340;&#36229;&#36807;38,000&#20010;5&#31186;&#24515;&#20869;&#30005;&#22270;&#65288;IEGM&#65289;&#29255;&#27573;&#12290;&#19987;&#29992;&#30828;&#20214;&#24179;&#21488;&#26159;STMicroelectronics&#21046;&#36896;&#30340;NUCLEO-L432KC&#12290;TDC'22&#38754;&#21521;&#20840;&#29699;&#22810;&#20154;&#22242;&#38431;&#65292;&#21560;&#24341;&#20102;&#26469;&#33258;50&#22810;&#20010;&#32452;&#32455;&#30340;150&#22810;&#25903;&#38431;&#20237;&#21442;&#36187;&#12290;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#36825;&#19968;&#21307;&#30103;&#38382;&#39064;&#65292;
&lt;/p&gt;
&lt;p&gt;
The first ACM/IEEE TinyML Design Contest (TDC) held at the 41st International Conference on Computer-Aided Design (ICCAD) in 2022 is a challenging, multi-month, research and development competition. TDC'22 focuses on real-world medical problems that require the innovation and implementation of artificial intelligence/machine learning (AI/ML) algorithms on implantable devices. The challenge problem of TDC'22 is to develop a novel AI/ML-based real-time detection algorithm for life-threatening ventricular arrhythmia over low-power microcontrollers utilized in Implantable Cardioverter-Defibrillators (ICDs). The dataset contains more than 38,000 5-second intracardiac electrograms (IEGMs) segments over 8 different types of rhythm from 90 subjects. The dedicated hardware platform is NUCLEO-L432KC manufactured by STMicroelectronics. TDC'22, which is open to multi-person teams world-wide, attracted more than 150 teams from over 50 organizations. This paper first presents the medical problem, da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#26031;Copula&#28151;&#21512;&#27169;&#22411;&#65288;GCMM&#65289;&#30340;&#24615;&#36136;&#65292;&#24320;&#21457;&#20102;&#22522;&#20110;&#25193;&#23637;&#26399;&#26395;&#26368;&#22823;&#31639;&#27861;&#30340;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;GCMM&#30456;&#27604;&#20110;GMM&#21487;&#20197;&#26356;&#22909;&#22320;&#25311;&#21512;&#25968;&#25454;&#24182;&#23454;&#29616;&#26356;&#28145;&#20837;&#30340;&#25968;&#25454;&#25366;&#25496;&#12290;</title><link>http://arxiv.org/abs/2305.01479</link><description>&lt;p&gt;
&#39640;&#26031;Copula&#28151;&#21512;&#27169;&#22411;&#30340;&#24615;&#36136;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the properties of Gaussian Copula Mixture Models. (arXiv:2305.01479v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#26031;Copula&#28151;&#21512;&#27169;&#22411;&#65288;GCMM&#65289;&#30340;&#24615;&#36136;&#65292;&#24320;&#21457;&#20102;&#22522;&#20110;&#25193;&#23637;&#26399;&#26395;&#26368;&#22823;&#31639;&#27861;&#30340;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;GCMM&#30456;&#27604;&#20110;GMM&#21487;&#20197;&#26356;&#22909;&#22320;&#25311;&#21512;&#25968;&#25454;&#24182;&#23454;&#29616;&#26356;&#28145;&#20837;&#30340;&#25968;&#25454;&#25366;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;Copula&#28151;&#21512;&#27169;&#22411;&#65288;GCMM&#65289;&#26159;&#20351;&#29992;Copula&#27010;&#24565;&#30340;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#25512;&#24191;&#12290;&#26412;&#25991;&#32473;&#20986;&#20102;&#20854;&#25968;&#23398;&#23450;&#20041;&#65292;&#24182;&#30740;&#31350;&#20102;&#20284;&#28982;&#20989;&#25968;&#30340;&#24615;&#36136;&#12290;&#22522;&#20110;&#36825;&#20123;&#23646;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#25193;&#23637;&#26399;&#26395;&#26368;&#22823;&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#28151;&#21512;Copula&#30340;&#21442;&#25968;&#65292;&#32780;&#27599;&#20010;&#32452;&#20214;&#23545;&#24212;&#30340;&#36793;&#38469;&#20998;&#24067;&#21017;&#20351;&#29992;&#21333;&#29420;&#30340;&#38750;&#21442;&#25968;&#32479;&#35745;&#26041;&#27861;&#36827;&#34892;&#20272;&#35745;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;GMM&#65292;GCMM&#22312;&#30456;&#21516;&#25968;&#37327;&#30340;&#32858;&#31867;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25311;&#21512;&#65307;&#27492;&#22806;&#65292;GCMM&#21487;&#20197;&#21033;&#29992;&#27599;&#20010;&#32500;&#24230;&#19978;&#30340;&#19981;&#21516;&#27493;&#25968;&#25454;&#23454;&#29616;&#26356;&#28145;&#20837;&#30340;&#25968;&#25454;&#25366;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian copula mixture models (GCMM) are the generalization of Gaussian Mixture models using the concept of copula. Its mathematical definition is given and the properties of likelihood function are studied in this paper. Based on these properties, extended Expectation Maximum algorithms are developed for estimating parameters for the mixture of copulas while marginal distributions corresponding to each component is estimated using separate nonparametric statistical methods. In the experiment, GCMM can achieve better goodness-of-fitting given the same number of clusters as GMM; furthermore, GCMM can utilize unsynchronized data on each dimension to achieve deeper mining of data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20801;&#35768;$k$&#20010;&#33218;&#30340;&#25439;&#22833;&#20989;&#25968;&#38543;&#26102;&#38388;&#32780;&#33258;&#30001;&#21464;&#21270;&#30340;&#23545;&#25239;&#24615;&#32447;&#24615;&#19978;&#19979;&#25991;&#36172;&#21338;&#24773;&#22659;&#12290;&#22312;&#20551;&#35774;&#29615;&#22659;&#36739;&#20026;&#28201;&#21644;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#20851;&#20110;Learner's Losses $V_T$&#30340;&#20108;&#38454;&#25439;&#22833;&#20540;&#37327;&#32423;&#20026;$\tilde O(K\sqrt{d V_T})$&#21644;&#20851;&#20110;&#26368;&#20339;&#31574;&#30053;$L_T^*$&#30340;&#19968;&#38454;&#25439;&#22833;&#20540;&#37327;&#32423;&#20026;$\tilde O(K\sqrt{d L_T^*})$&#30340;&#30028;&#12290;</title><link>http://arxiv.org/abs/2305.00832</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#32447;&#24615;&#19978;&#19979;&#25991;&#36172;&#21338;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
First- and Second-Order Bounds for Adversarial Linear Contextual Bandits. (arXiv:2305.00832v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20801;&#35768;$k$&#20010;&#33218;&#30340;&#25439;&#22833;&#20989;&#25968;&#38543;&#26102;&#38388;&#32780;&#33258;&#30001;&#21464;&#21270;&#30340;&#23545;&#25239;&#24615;&#32447;&#24615;&#19978;&#19979;&#25991;&#36172;&#21338;&#24773;&#22659;&#12290;&#22312;&#20551;&#35774;&#29615;&#22659;&#36739;&#20026;&#28201;&#21644;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#20851;&#20110;Learner's Losses $V_T$&#30340;&#20108;&#38454;&#25439;&#22833;&#20540;&#37327;&#32423;&#20026;$\tilde O(K\sqrt{d V_T})$&#21644;&#20851;&#20110;&#26368;&#20339;&#31574;&#30053;$L_T^*$&#30340;&#19968;&#38454;&#25439;&#22833;&#20540;&#37327;&#32423;&#20026;$\tilde O(K\sqrt{d L_T^*})$&#30340;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#32447;&#24615;&#19978;&#19979;&#25991;&#36172;&#21338;&#30340;&#24773;&#22659;&#65292;&#35813;&#24773;&#22659;&#20801;&#35768;&#19982;K&#20010;&#33218;&#30456;&#20851;&#32852;&#30340;&#25439;&#22833;&#20989;&#25968;&#38543;&#26102;&#38388;&#32780;&#33258;&#30001;&#21464;&#21270;&#12290; &#20551;&#35774;d&#32500;&#19978;&#19979;&#25991;&#20174;&#24050;&#30693;&#20998;&#24067;&#20013;&#32472;&#21046;&#65292;&#37027;&#20040;&#22312;T&#36718;&#28216;&#25103;&#26399;&#38388;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#39044;&#26399;&#36951;&#25022;&#23558;&#20197;$\tilde O(\sqrt{Kd T})$&#30340;&#36895;&#24230;&#22686;&#38271;&#12290;&#22312;&#20551;&#35774;&#19978;&#19979;&#25991;&#30340;&#23494;&#24230;&#26159;&#23545;&#25968;&#20985;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#20108;&#38454;&#30028;&#65292;&#20854;&#22312;&#32047;&#31215;&#25439;&#22833;&#30340;&#20108;&#27425;&#30697;$V_T$&#26041;&#38754;&#30340;&#37327;&#32423;&#20026;$\tilde O(K\sqrt{d V_T})$&#65292;&#20197;&#21450;&#19968;&#20010;&#19982;&#20043;&#23494;&#20999;&#30456;&#20851;&#30340;&#19968;&#38454;&#30028;&#65292;&#20854;&#22312;&#26368;&#20339;&#31574;&#30053;&#30340;&#32047;&#31215;&#25439;&#22833;$L_T^*$&#26041;&#38754;&#30340;&#37327;&#32423;&#20026;$\tilde O(K\sqrt{d L_T^*})$&#12290;&#30001;&#20110;$V_T$&#25110;$L_T^*$&#21487;&#33021;&#26126;&#26174;&#23567;&#20110;$T$&#65292;&#22240;&#27492;&#27599;&#24403;&#29615;&#22659;&#30456;&#23545;&#28201;&#21644;&#26102;&#65292;&#20415;&#20250;&#25913;&#21892;&#26368;&#22351;&#24773;&#20917;&#30340;&#36951;&#25022;&#12290;&#26412;&#25991;&#20351;&#29992;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#30340;&#36830;&#32493;&#25351;&#25968;&#26435;&#37325;&#31639;&#27861;&#30340;&#25130;&#26029;&#29256;&#26412;&#26469;&#33719;&#24471;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
We consider the adversarial linear contextual bandit setting, which allows for the loss functions associated with each of $K$ arms to change over time without restriction. Assuming the $d$-dimensional contexts are drawn from a fixed known distribution, the worst-case expected regret over the course of $T$ rounds is known to scale as $\tilde O(\sqrt{Kd T})$. Under the additional assumption that the density of the contexts is log-concave, we obtain a second-order bound of order $\tilde O(K\sqrt{d V_T})$ in terms of the cumulative second moment of the learner's losses $V_T$, and a closely related first-order bound of order $\tilde O(K\sqrt{d L_T^*})$ in terms of the cumulative loss of the best policy $L_T^*$. Since $V_T$ or $L_T^*$ may be significantly smaller than $T$, these improve over the worst-case regret whenever the environment is relatively benign. Our results are obtained using a truncated version of the continuous exponential weights algorithm over the probability simplex, which
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;N-Sigma&#26041;&#27861;&#26469;&#32479;&#35745;&#27979;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#65292;&#20197;&#24320;&#21457;&#22522;&#20110;&#20559;&#24046;&#20998;&#26512;&#30340;&#26032;&#39118;&#38505;&#35780;&#20272;&#26694;&#26550;&#65292;&#23588;&#20854;&#22312;&#20154;&#33080;&#35782;&#21035;&#25216;&#26415;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#30340;&#20248;&#21183;&#21644;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.13680</link><description>&lt;p&gt;
&#24212;&#29992;&#20110;&#20154;&#33080;&#29983;&#29289;&#35782;&#21035;&#30340;AI&#27169;&#22411;&#20559;&#24046;&#30340;&#32479;&#35745;&#26041;&#27861;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Measuring Bias in AI Models with Application to Face Biometrics: An Statistical Approach. (arXiv:2304.13680v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;N-Sigma&#26041;&#27861;&#26469;&#32479;&#35745;&#27979;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#65292;&#20197;&#24320;&#21457;&#22522;&#20110;&#20559;&#24046;&#20998;&#26512;&#30340;&#26032;&#39118;&#38505;&#35780;&#20272;&#26694;&#26550;&#65292;&#23588;&#20854;&#22312;&#20154;&#33080;&#35782;&#21035;&#25216;&#26415;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#30340;&#20248;&#21183;&#21644;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27431;&#30431;&#22996;&#21592;&#20250;&#21457;&#24067;&#20102;&#26032;&#30340;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#27861;&#35268;&#26694;&#26550;&#33609;&#26696;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#39118;&#38505;&#30340;&#27861;&#24459;&#26041;&#27861;&#12290;&#33609;&#26696;&#24378;&#35843;&#38656;&#35201;&#20026;&#19981;&#21516;&#30340;AI&#29992;&#36884;&#24320;&#21457;&#20805;&#20998;&#30340;&#39118;&#38505;&#35780;&#20272;&#12290;&#36825;&#31181;&#39118;&#38505;&#35780;&#20272;&#24212;&#21253;&#25324;&#26816;&#27979;&#21644;&#20943;&#36731;AI&#20013;&#30340;&#20559;&#24046;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#27979;&#37327;&#33258;&#21160;&#20915;&#31574;&#31995;&#32479;&#20013;&#20559;&#24046;&#30340;&#32479;&#35745;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#37325;&#28857;&#26159;&#20154;&#33080;&#35782;&#21035;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20559;&#24046;&#30340;&#26041;&#24335;&#65292;&#35813;&#26041;&#24335;&#22522;&#20110;N-Sigma&#26041;&#27861;&#30340;&#32479;&#35745;&#26041;&#27861;&#12290;N-Sigma&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#19968;&#33324;&#31185;&#23398;&#65288;&#22914;&#29289;&#29702;&#21644;&#31038;&#20250;&#39046;&#22495;&#65289;&#20013;&#30340;&#20551;&#35774;&#65292;&#20854;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#26469;&#24320;&#21457;&#22522;&#20110;&#20559;&#24046;&#20998;&#26512;&#30340;&#26032;&#39118;&#38505;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#35752;&#35770;&#20854;&#20027;&#35201;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The new regulatory framework proposal on Artificial Intelligence (AI) published by the European Commission establishes a new risk-based legal approach. The proposal highlights the need to develop adequate risk assessments for the different uses of AI. This risk assessment should address, among others, the detection and mitigation of bias in AI. In this work we analyze statistical approaches to measure biases in automatic decision-making systems. We focus our experiments in face recognition technologies. We propose a novel way to measure the biases in machine learning models using a statistical approach based on the N-Sigma method. N-Sigma is a popular statistical approach used to validate hypotheses in general science such as physics and social areas and its application to machine learning is yet unexplored. In this work we study how to apply this methodology to develop new risk assessment frameworks based on bias analysis and we discuss the main advantages and drawbacks with respect t
&lt;/p&gt;</description></item><item><title>PTW&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#29983;&#25104;&#22120;&#27700;&#21360;&#25216;&#26415;&#65292;&#21487;&#20197;&#27604;&#20174;&#22836;&#24320;&#22987;&#27700;&#21360;&#25216;&#26415;&#24555;&#19977;&#20010;&#25968;&#37327;&#32423;&#21644;&#26356;&#22909;&#30340;&#20445;&#30041;&#20102;&#29983;&#25104;&#22120;&#30340;&#22270;&#20687;&#36136;&#37327;&#65292;&#35299;&#20915;&#20102;&#24694;&#24847;&#29992;&#25143;&#21033;&#29992;&#25552;&#20379;&#30340;&#27169;&#22411;&#21046;&#20316;&#20986;&#26377;&#23475;&#30340;&#28145;&#24230;&#20266;&#36896;&#32780;&#19981;&#20250;&#34987;&#21457;&#29616;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.07361</link><description>&lt;p&gt;
PTW: &#38024;&#23545;&#39044;&#35757;&#32451;&#22270;&#20687;&#29983;&#25104;&#22120;&#30340;&#20851;&#38190;&#35843;&#25972;&#22411;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
PTW: Pivotal Tuning Watermarking for Pre-Trained Image Generators. (arXiv:2304.07361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07361
&lt;/p&gt;
&lt;p&gt;
PTW&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#29983;&#25104;&#22120;&#27700;&#21360;&#25216;&#26415;&#65292;&#21487;&#20197;&#27604;&#20174;&#22836;&#24320;&#22987;&#27700;&#21360;&#25216;&#26415;&#24555;&#19977;&#20010;&#25968;&#37327;&#32423;&#21644;&#26356;&#22909;&#30340;&#20445;&#30041;&#20102;&#29983;&#25104;&#22120;&#30340;&#22270;&#20687;&#36136;&#37327;&#65292;&#35299;&#20915;&#20102;&#24694;&#24847;&#29992;&#25143;&#21033;&#29992;&#25552;&#20379;&#30340;&#27169;&#22411;&#21046;&#20316;&#20986;&#26377;&#23475;&#30340;&#28145;&#24230;&#20266;&#36896;&#32780;&#19981;&#20250;&#34987;&#21457;&#29616;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#26159;&#27867;&#25351;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#22120;&#32508;&#21512;&#20986;&#26469;&#30340;&#20869;&#23481;&#65292;&#33509;&#34987;&#8220;&#35823;&#29992;&#8221;&#65292;&#21487;&#20197;&#30772;&#22351;&#25968;&#23383;&#23186;&#20307;&#30340;&#20449;&#20219;&#12290;&#32780;&#21046;&#20316;&#39640;&#36136;&#37327;&#30340;&#28145;&#24230;&#20266;&#36896;&#38656;&#35201;&#25509;&#35302;&#22823;&#22411;&#21644;&#22797;&#26434;&#30340;&#29983;&#25104;&#22120;&#65292;&#21482;&#26377;&#23569;&#25968;&#23454;&#20307;&#21487;&#20197;&#35757;&#32451;&#21644;&#25552;&#20379;&#36825;&#20123;&#29983;&#25104;&#22120;&#12290;&#23041;&#32961;&#30340;&#26159;&#65292;&#24694;&#24847;&#29992;&#25143;&#20250;&#21033;&#29992;&#25552;&#20379;&#30340;&#27169;&#22411;&#21046;&#20316;&#20986;&#26377;&#23475;&#30340;&#28145;&#24230;&#20266;&#36896;&#65292;&#32780;&#19981;&#20250;&#34987;&#21457;&#29616;&#12290;&#20026;&#20102;&#21487;&#25506;&#27979;&#36215;&#26469;&#65292;&#28145;&#24230;&#20266;&#36896;&#38656;&#35201;&#22312;&#29983;&#25104;&#22120;&#20013;&#23884;&#20837;&#35782;&#21035;&#30721;&#65292;&#20197;&#20415;&#22312;&#21518;&#32493;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#36827;&#34892;&#25552;&#21462;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Pivotal Tuning Watermarking (PTW)&#65292;&#36825;&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#29983;&#25104;&#22120;&#27700;&#21360;&#25216;&#26415;&#65292;(i) &#27604;&#20174;&#22836;&#24320;&#22987;&#27700;&#21360;&#25216;&#26415;&#24555;&#19977;&#20010;&#25968;&#37327;&#32423;&#65292;(ii) &#19981;&#38656;&#35201;&#20219;&#20309;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#27700;&#21360;&#25216;&#26415;&#65292;&#21516;&#26102;&#32553;&#25918;&#21040;&#20102;&#27604;&#30456;&#20851;&#24037;&#20316;&#22823;4&#20493;&#30340;&#29983;&#25104;&#22120;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;PTW&#21487;&#20197;&#23884;&#20837;&#26356;&#38271;&#30340;&#35782;&#21035;&#30721;&#65292;&#21516;&#26102;&#26356;&#22909;&#22320;&#20445;&#30041;&#20102;&#29983;&#25104;&#22120;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20005;&#26684;&#30340;&#22522;&#20110;&#21338;&#24328;&#35770;&#30340;&#23450;&#20041;&#26469;&#35780;&#20272;&#27700;&#21360;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deepfakes refer to content synthesized using deep generators, which, when \emph{misused}, have the potential to erode trust in digital media. Synthesizing high-quality deepfakes requires access to large and complex generators only few entities can train and provide. The threat are malicious users that exploit access to the provided model and generate harmful deepfakes without risking detection. Watermarking makes deepfakes detectable by embedding an identifiable code into the generator that is later extractable from its generated images. We propose Pivotal Tuning Watermarking (PTW), a method for watermarking pre-trained generators (i) three orders of magnitude faster than watermarking from scratch and (ii) without the need for any training data. We improve existing watermarking methods and scale to generators $4 \times$ larger than related work. PTW can embed longer codes than existing methods while better preserving the generator's image quality. We propose rigorous, game-based defini
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23454;&#38469;&#25968;&#25454;&#36827;&#34892;&#38081;&#36335;&#36710;&#36742;&#36724;&#25215;&#25925;&#38556;&#26816;&#27979;&#30340;&#22768;&#23398;&#20998;&#26512;&#26041;&#27861;&#65292;&#20351;&#29992;Mel&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;(MFCC)&#20316;&#20026;&#29305;&#24449;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#21487;&#38752;&#22320;&#26816;&#27979;&#36724;&#25215;&#25925;&#38556;&#12290;</title><link>http://arxiv.org/abs/2304.07307</link><description>&lt;p&gt;
&#22522;&#20110;&#23454;&#38469;&#25968;&#25454;&#30340;&#38081;&#36335;&#36710;&#36742;&#36724;&#25215;&#25925;&#38556;&#30340;&#22768;&#23398;&#20998;&#26512;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Airborne-Sound Analysis for the Detection of Bearing Faults in Railway Vehicles with Real-World Data. (arXiv:2304.07307v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23454;&#38469;&#25968;&#25454;&#36827;&#34892;&#38081;&#36335;&#36710;&#36742;&#36724;&#25215;&#25925;&#38556;&#26816;&#27979;&#30340;&#22768;&#23398;&#20998;&#26512;&#26041;&#27861;&#65292;&#20351;&#29992;Mel&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;(MFCC)&#20316;&#20026;&#29305;&#24449;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#21487;&#38752;&#22320;&#26816;&#27979;&#36724;&#25215;&#25925;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22312;&#36710;&#36742;&#27491;&#24120;&#36816;&#34892;&#26399;&#38388;&#35760;&#24405;&#30340;&#22768;&#23398;&#20449;&#21495;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#38081;&#36335;&#36710;&#36742;&#36724;&#25215;&#25925;&#38556;&#26816;&#27979;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Mel&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;(MFCC)&#20316;&#20026;&#29305;&#24449;&#65292;&#24182;&#20316;&#20026;&#36755;&#20837;&#25552;&#20379;&#32473;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#20998;&#31867;&#22120;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#22312;&#38024;&#23545;&#29616;&#20195;&#36890;&#21220;&#38081;&#36335;&#36710;&#36742;&#36827;&#34892;&#27979;&#37327;&#27963;&#21160;&#20013;&#33719;&#24471;&#30340;&#23454;&#38469;&#25968;&#25454;&#35780;&#20272;&#30340;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#25152;&#36873;&#25321;&#30340;MFCC&#29305;&#24449;&#21487;&#20197;&#21487;&#38752;&#22320;&#26816;&#27979;&#36724;&#25215;&#25925;&#38556;&#65292;&#21363;&#20351;&#20986;&#29616;&#22312;&#35757;&#32451;&#20013;&#26410;&#21253;&#25324;&#30340;&#36724;&#25215;&#25439;&#20260;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the challenging problem of detecting bearing faults in railway vehicles by analyzing acoustic signals recorded during regular operation. For this, we introduce Mel Frequency Cepstral Coefficients (MFCCs) as features, which form the input to a simple Multi-Layer Perceptron classifier. The proposed method is evaluated with real-world data that was obtained for state-of-the-art commuter railway vehicles in a measurement campaign. The experiments show that with the chosen MFCC features bearing faults can be reliably detected even for bearing damages that were not included in training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#19968;&#32500;&#27531;&#24046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#20102;&#25968;&#25454;&#22686;&#24378;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#22312;ICPHM 2023&#25968;&#25454;&#25361;&#25112;&#36187;&#20013;&#24212;&#29992;&#20110;&#22826;&#38451;&#40831;&#36718;&#25925;&#38556;&#20998;&#31867;&#20219;&#21153;&#12290;&#21363;&#20351;&#38754;&#23545;&#22810;&#20010;&#25805;&#20316;&#26465;&#20214;&#19979;&#33719;&#24471;&#30340;&#25968;&#25454;&#65292;&#35813;&#32593;&#32476;&#20173;&#33021;&#20934;&#30830;&#39044;&#27979;&#34987;&#26816;&#27979;&#40831;&#36718;&#31665;&#30340;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2304.07305</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#30340;&#19968;&#32500;&#27531;&#24046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;ICPHM 2023&#25968;&#25454;&#25361;&#25112;&#36187;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
1-D Residual Convolutional Neural Network coupled with Data Augmentation and Regularization Techniques for the ICPHM 2023 Data Challenge. (arXiv:2304.07305v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#19968;&#32500;&#27531;&#24046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#20102;&#25968;&#25454;&#22686;&#24378;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#22312;ICPHM 2023&#25968;&#25454;&#25361;&#25112;&#36187;&#20013;&#24212;&#29992;&#20110;&#22826;&#38451;&#40831;&#36718;&#25925;&#38556;&#20998;&#31867;&#20219;&#21153;&#12290;&#21363;&#20351;&#38754;&#23545;&#22810;&#20010;&#25805;&#20316;&#26465;&#20214;&#19979;&#33719;&#24471;&#30340;&#25968;&#25454;&#65292;&#35813;&#32593;&#32476;&#20173;&#33021;&#20934;&#30830;&#39044;&#27979;&#34987;&#26816;&#27979;&#40831;&#36718;&#31665;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;ICPHM 2023&#25361;&#25112;&#36187;&#20013;&#23545;&#20110;&#21033;&#29992;&#25391;&#21160;&#20998;&#26512;&#36827;&#34892;&#24037;&#19994;&#31995;&#32479;&#20581;&#24247;&#30417;&#27979;&#30340;&#22826;&#38451;&#40831;&#36718;&#25925;&#38556;&#20998;&#31867;&#20219;&#21153;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19977;&#36890;&#36947;&#26102;&#38388;&#22495;&#25391;&#21160;&#20449;&#21495;&#30340;&#27531;&#24046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#32467;&#21512;&#25968;&#25454;&#22686;&#24378;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23613;&#31649;&#20854;&#21487;&#35757;&#32451;&#21442;&#25968;&#23569;&#20110;30,000&#20010;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#31867;&#21035;&#20998;&#31867;&#22330;&#26223;&#20013;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20135;&#29983;&#20102;&#38750;&#24120;&#22909;&#30340;&#32467;&#26524;&#12290;&#21363;&#20351;&#38754;&#23545;&#22810;&#20010;&#25805;&#20316;&#26465;&#20214;&#19979;&#33719;&#24471;&#30340;&#25968;&#25454;&#65292;&#35813;&#32593;&#32476;&#20173;&#33021;&#20934;&#30830;&#39044;&#27979;&#34987;&#26816;&#27979;&#40831;&#36718;&#31665;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we present our contribution to the ICPHM 2023 Data Challenge on Industrial Systems' Health Monitoring using Vibration Analysis. For the task of classifying sun gear faults in a gearbox, we propose a residual Convolutional Neural Network that operates on raw three-channel time-domain vibration signals. In conjunction with data augmentation and regularization techniques, the proposed model yields very good results in a multi-class classification scenario with real-world data despite its relatively small size, i.e., with less than 30,000 trainable parameters. Even when presented with data obtained from multiple operating conditions, the network is still capable to accurately predict the condition of the gearbox under inspection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38416;&#36848;&#20102;&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#24615;&#19968;&#38454;&#26597;&#35810;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26469;&#33258;&#27169;&#31946;&#36923;&#36753;&#29702;&#35770;&#30340;&#26032;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#35299;&#20915;&#26032;&#20844;&#24335;&#65292;&#24182;&#22312;&#29616;&#26377;&#20844;&#24335;&#20013;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.07063</link><description>&lt;p&gt;
&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#24615;&#19968;&#38454;&#26597;&#35810;&#25512;&#29702;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Existential First Order Queries Inference on Knowledge Graphs. (arXiv:2304.07063v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38416;&#36848;&#20102;&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#24615;&#19968;&#38454;&#26597;&#35810;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26469;&#33258;&#27169;&#31946;&#36923;&#36753;&#29702;&#35770;&#30340;&#26032;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#35299;&#20915;&#26032;&#20844;&#24335;&#65292;&#24182;&#22312;&#29616;&#26377;&#20844;&#24335;&#20013;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#21033;&#29992;&#35266;&#23519;&#21040;&#30340;&#20449;&#24687;&#26469;&#39044;&#27979;&#32570;&#22833;&#30340;&#20449;&#24687;&#12290;&#29305;&#21035;&#22320;&#65292;&#22238;&#31572;&#19968;&#38454;&#36923;&#36753;&#20844;&#24335;&#26159;&#29305;&#21035;&#24863;&#20852;&#36259;&#30340;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#28165;&#26224;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#20102;&#19968;&#32452;&#23454;&#20307;&#30340;&#23884;&#20837;&#65292;&#24182;&#23558;&#36923;&#36753;&#36816;&#31639;&#35270;&#20026;&#38598;&#21512;&#36816;&#31639;&#12290;&#23613;&#31649;&#26377;&#24456;&#22810;&#30740;&#31350;&#36981;&#24490;&#30456;&#21516;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#32570;&#20047;&#20174;&#36923;&#36753;&#35282;&#24230;&#36827;&#34892;&#31995;&#32479;&#26816;&#26597;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20808;&#21069;&#30740;&#31350;&#35843;&#26597;&#30340;&#26597;&#35810;&#33539;&#22260;&#65292;&#24182;&#20934;&#30830;&#22320;&#30830;&#23450;&#20102;&#23427;&#19982;&#25972;&#20010;&#23384;&#22312;&#24615;&#20844;&#24335;&#23478;&#26063;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#21313;&#20010;&#26032;&#20844;&#24335;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#35752;&#35770;&#20102;&#21516;&#26102;&#20986;&#29616;&#30340;&#26032;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26469;&#33258;&#27169;&#31946;&#36923;&#36753;&#29702;&#35770;&#30340;&#26032;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#35299;&#20915;&#26032;&#20844;&#24335;&#65292;&#24182;&#22312;&#29616;&#26377;&#20844;&#24335;&#20013;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning on knowledge graphs is a challenging task because it utilizes observed information to predict the missing one. Specifically, answering first-order logic formulas is of particular interest because of its clear syntax and semantics. Recently, the query embedding method has been proposed which learns the embedding of a set of entities and treats logic operations as set operations. Though there has been much research following the same methodology, it lacks a systematic inspection from the standpoint of logic. In this paper, we characterize the scope of queries investigated previously and precisely identify the gap between it and the whole family of existential formulas. Moreover, we develop a new dataset containing ten new formulas and discuss the new challenges coming simultaneously. Finally, we propose a new search algorithm from fuzzy logic theory which is capable of solving new formulas and outperforming the previous methods in existing formulas.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;PopulAtion Parameter Averaging (PAPA)&#65292;&#33021;&#21516;&#26102;&#25317;&#26377;&#38598;&#25104;&#30340;&#26222;&#36941;&#24615;&#19982;&#26435;&#37325;&#24179;&#22343;&#30340;&#25928;&#29575;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03094</link><description>&lt;p&gt;
PopulAtion Parameter Averaging (PAPA)&#65288;&#20154;&#21475;&#21442;&#25968;&#24179;&#22343;&#65289;
&lt;/p&gt;
&lt;p&gt;
PopulAtion Parameter Averaging (PAPA). (arXiv:2304.03094v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03094
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;PopulAtion Parameter Averaging (PAPA)&#65292;&#33021;&#21516;&#26102;&#25317;&#26377;&#38598;&#25104;&#30340;&#26222;&#36941;&#24615;&#19982;&#26435;&#37325;&#24179;&#22343;&#30340;&#25928;&#29575;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#26041;&#27861;&#23558;&#22810;&#20010;&#27169;&#22411;&#30340;&#39044;&#27979;&#32452;&#21512;&#36215;&#26469;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#26356;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#20123;&#25104;&#26412;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#36827;&#34892;&#24179;&#22343;&#26469;&#23558;&#23427;&#20204;&#21512;&#24182;&#25104;&#19968;&#20010;&#65288;&#27169;&#22411;&#27748;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#36890;&#24120;&#27604;&#38598;&#25104;&#34920;&#29616;&#26356;&#24046;&#12290;&#24403;&#26435;&#37325;&#36275;&#22815;&#30456;&#20284;&#65288;&#22312;&#26435;&#37325;&#25110;&#29305;&#24449;&#31354;&#38388;&#20013;&#65289;&#21487;&#20197;&#24456;&#22909;&#22320;&#24179;&#22343;&#65292;&#20294;&#36275;&#22815;&#19981;&#21516;&#20197;&#20174;&#32452;&#21512;&#20013;&#21463;&#30410;&#26102;&#65292;&#26435;&#37325;&#24179;&#22343;&#25165;&#26159;&#26377;&#30410;&#30340;&#12290;&#22522;&#20110;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PopulAtion Parameter Averaging (PAPA)&#65292;&#19968;&#31181;&#23558;&#38598;&#25104;&#30340;&#26222;&#36941;&#24615;&#19982;&#26435;&#37325;&#24179;&#22343;&#30340;&#25928;&#29575;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;PAPA&#21033;&#29992;&#19981;&#21516;&#27169;&#22411;&#65288;&#22312;&#19981;&#21516;&#25968;&#25454;&#39034;&#24207;&#65292;&#22686;&#24378;&#21644;&#27491;&#21017;&#21270;&#19978;&#35757;&#32451;&#65289;&#30340;&#20154;&#21475;&#65292;&#32780;&#20598;&#23572;&#65288;&#19981;&#35201;&#22826;&#39057;&#32321;&#65292;&#20063;&#19981;&#35201;&#22826;&#31232;&#30095;&#65289;&#29992;&#32593;&#32476;&#30340;&#26435;&#37325;&#26469;&#20195;&#26367;&#20154;&#21475;&#26435;&#37325;&#30340;&#24179;&#22343;&#20540;&#12290;PAPA&#20943;&#23569;&#20102;&#24179;&#22343;&#20540;&#21644;&#38598;&#25104;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensemble methods combine the predictions of multiple models to improve performance, but they require significantly higher computation costs at inference time. To avoid these costs, multiple neural networks can be combined into one by averaging their weights (model soups). However, this usually performs significantly worse than ensembling. Weight averaging is only beneficial when weights are similar enough (in weight or feature space) to average well but different enough to benefit from combining them. Based on this idea, we propose PopulAtion Parameter Averaging (PAPA): a method that combines the generality of ensembling with the efficiency of weight averaging. PAPA leverages a population of diverse models (trained on different data orders, augmentations, and regularizations) while occasionally (not too often, not too rarely) replacing the weights of the networks with the population average of the weights. PAPA reduces the performance gap between averaging and ensembling, increasing th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26080;&#20559;&#12289;&#39640;&#25928;&#12289;&#36866;&#24403;&#30340;&#21333;&#35843;&#24067;&#23572;&#20989;&#25968;&#23398;&#20064;&#31639;&#27861;&#65292;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#20551;&#35774;&#30340;&#22823;&#23567;&#21644;&#35780;&#20272;&#26102;&#38388;&#37117;&#20026;$2^{\tilde{O}(\sqrt{n}/\varepsilon)}$&#65292;&#35813;&#31639;&#27861;&#35299;&#20915;&#20102;&#26679;&#26412;&#39640;&#25928;&#31639;&#27861;&#26080;&#27861;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.02700</link><description>&lt;p&gt;
&#26080;&#20559;&#20851;&#20110;&#22369;&#24230;&#20989;&#25968;&#30340;&#36866;&#24403;&#23398;&#20064;&#65306;&#36234;&#36807;&#40657;&#30418;&#20462;&#27491;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Agnostic proper learning of monotone functions: beyond the black-box correction barrier. (arXiv:2304.02700v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26080;&#20559;&#12289;&#39640;&#25928;&#12289;&#36866;&#24403;&#30340;&#21333;&#35843;&#24067;&#23572;&#20989;&#25968;&#23398;&#20064;&#31639;&#27861;&#65292;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#20551;&#35774;&#30340;&#22823;&#23567;&#21644;&#35780;&#20272;&#26102;&#38388;&#37117;&#20026;$2^{\tilde{O}(\sqrt{n}/\varepsilon)}$&#65292;&#35813;&#31639;&#27861;&#35299;&#20915;&#20102;&#26679;&#26412;&#39640;&#25928;&#31639;&#27861;&#26080;&#27861;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26080;&#20559;&#12289;&#39640;&#25928;&#12289;&#36866;&#24403;&#30340;&#21333;&#35843;&#24067;&#23572;&#20989;&#25968;&#23398;&#20064;&#31639;&#27861;&#12290;&#32473;&#23450;&#26410;&#30693;&#20989;&#25968;$f:\{\pm 1\}^n \rightarrow \{\pm 1\}$&#30340;$2^{\tilde{O}(\sqrt{n}/\varepsilon)}$&#20010;&#22343;&#21248;&#38543;&#26426;&#26679;&#26412;&#65292;&#31639;&#27861;&#36755;&#20986;&#19968;&#20010;&#20551;&#35774;$g:\{\pm 1\}^n \rightarrow \{\pm 1\}$&#65292;&#35813;&#20551;&#35774;&#26159;&#21333;&#35843;&#30340;&#65292;&#24182;&#19988;&#19982;$f$&#30340;&#36317;&#31163;&#20026;$(\mathrm{opt} + \varepsilon)$&#65292;&#20854;&#20013;$\mathrm{opt}$&#26159;$f$&#19982;&#26368;&#36817;&#21333;&#35843;&#20989;&#25968;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#65288;&#22240;&#27492;&#20063;&#26159;&#20551;&#35774;&#30340;&#22823;&#23567;&#21644;&#35780;&#20272;&#26102;&#38388;&#65289;&#20063;&#26159;$2^{\tilde{O}(\sqrt{n}/\varepsilon)}$&#65292;&#20960;&#20046;&#19982;Blais&#31561;&#20154;&#65288;RANDOM '15&#65289;&#30340;&#19979;&#30028;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#36824;&#32473;&#20986;&#19968;&#20010;&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#26410;&#30693;&#20989;&#25968;$f$&#21040;&#21333;&#35843;&#24615;&#30340;&#28155;&#21152;&#35823;&#24046;$\varepsilon$&#30340;&#36317;&#31163;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#20026;$2^{\tilde{O}(\sqrt{n}/\varepsilon)}$&#12290;&#20197;&#21069;&#65292;&#38024;&#23545;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#24050;&#30693;&#26377;&#26679;&#26412;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#20294;&#36825;&#20123;&#31639;&#27861;&#24182;&#19981;&#26159;&#36816;&#34892;&#26102;&#38388;&#26377;&#25928;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We give the first agnostic, efficient, proper learning algorithm for monotone Boolean functions. Given $2^{\tilde{O}(\sqrt{n}/\varepsilon)}$ uniformly random examples of an unknown function $f:\{\pm 1\}^n \rightarrow \{\pm 1\}$, our algorithm outputs a hypothesis $g:\{\pm 1\}^n \rightarrow \{\pm 1\}$ that is monotone and $(\mathrm{opt} + \varepsilon)$-close to $f$, where $\mathrm{opt}$ is the distance from $f$ to the closest monotone function. The running time of the algorithm (and consequently the size and evaluation time of the hypothesis) is also $2^{\tilde{O}(\sqrt{n}/\varepsilon)}$, nearly matching the lower bound of Blais et al (RANDOM '15). We also give an algorithm for estimating up to additive error $\varepsilon$ the distance of an unknown function $f$ to monotone using a run-time of $2^{\tilde{O}(\sqrt{n}/\varepsilon)}$. Previously, for both of these problems, sample-efficient algorithms were known, but these algorithms were not run-time efficient. Our work thus closes this g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#27979;&#37327;&#29109;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;ID-Entropy&#65292;&#23427;&#21487;&#20197;&#29992;&#20110;&#22810;&#36718;&#25968;&#25454;&#21464;&#25442;&#21644;&#25197;&#26354;&#65292;&#21516;&#26102;&#21487;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#32500;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.02223</link><description>&lt;p&gt;
&#26412;&#22320;&#22266;&#26377;&#32500;&#24230;&#29109;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local Intrinsic Dimensional Entropy. (arXiv:2304.02223v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#27979;&#37327;&#29109;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;ID-Entropy&#65292;&#23427;&#21487;&#20197;&#29992;&#20110;&#22810;&#36718;&#25968;&#25454;&#21464;&#25442;&#21644;&#25197;&#26354;&#65292;&#21516;&#26102;&#21487;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29109;&#27979;&#37327;&#20381;&#36182;&#20110;&#27010;&#29575;&#20998;&#24067;&#22312;&#26679;&#26412;&#31354;&#38388;X&#19978;&#30340;&#23637;&#24067;&#24773;&#20917;&#65292;&#26368;&#22823;&#21487;&#23454;&#29616;&#29109;&#19982;&#26679;&#26412;&#31354;&#38388;&#22522;&#25968;|X|&#25104;&#27604;&#20363;&#12290;&#23545;&#20110;&#26377;&#38480;|X|&#65292;&#36825;&#20135;&#29983;&#20102;&#28385;&#36275;&#35768;&#22810;&#37325;&#35201;&#23646;&#24615;&#65288;&#22914;&#23545;&#21452;&#23556;&#30340;&#19981;&#21464;&#24615;&#65289;&#30340;&#24378;&#22823;&#29109;&#27979;&#37327;&#65292;&#32780;&#21516;&#26679;&#19981;&#33021;&#28385;&#36275;&#36830;&#32493;&#31354;&#38388;&#30340;&#35201;&#27714;&#65288;&#20854;&#20013;|X|=&#26080;&#31351;&#22823;&#65289;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;R&#21644;R^d&#65288;d&#22312;Z+&#20013;&#65289;&#20855;&#26377;&#30456;&#21516;&#30340;&#22522;&#25968;&#65288;&#26469;&#33258;Cantor&#30340;&#23545;&#24212;&#35770;&#35777;&#65289;&#65292;&#22522;&#25968;&#20381;&#36182;&#24615;&#29109;&#27979;&#37327;&#26080;&#27861;&#32534;&#30721;&#25968;&#25454;&#32500;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#20102;&#23545;&#36830;&#32493;&#31354;&#38388;&#23450;&#20041;&#29109;&#27979;&#37327;&#20013;&#22522;&#25968;&#21644;&#20998;&#24067;&#23637;&#24067;&#30340;&#20316;&#29992;&#65292;&#36825;&#20123;&#36830;&#32493;&#31354;&#38388;&#21487;&#20197;&#36827;&#34892;&#22810;&#36718;&#21464;&#25442;&#21644;&#25197;&#26354;&#65292;&#20363;&#22914;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#22914;&#26524;&#29992;&#20998;&#24067;&#30340;&#23616;&#37096;&#22266;&#26377;&#32500;&#24230;&#30340;&#24179;&#22343;&#20540;&#26469;&#34920;&#31034;&#27979;&#37327;&#29109;&#65292;&#34987;&#31216;&#20026;ID-Entropy&#65292;&#37027;&#20040;&#21487;&#20197;&#20316;&#20026;&#36830;&#32493;&#31354;&#38388;&#30340;&#24378;&#22823;&#29109;&#27979;&#37327;&#65292;&#21516;&#26102;&#25429;&#25417;&#25968;&#25454;&#30340;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most entropy measures depend on the spread of the probability distribution over the sample space X, and the maximum entropy achievable scales proportionately with the sample space cardinality |X|. For a finite |X|, this yields robust entropy measures which satisfy many important properties, such as invariance to bijections, while the same is not true for continuous spaces (where |X|=infinity). Furthermore, since R and R^d (d in Z+) have the same cardinality (from Cantor's correspondence argument), cardinality-dependent entropy measures cannot encode the data dimensionality. In this work, we question the role of cardinality and distribution spread in defining entropy measures for continuous spaces, which can undergo multiple rounds of transformations and distortions, e.g., in neural networks. We find that the average value of the local intrinsic dimension of a distribution, denoted as ID-Entropy, can serve as a robust entropy measure for continuous spaces, while capturing the data dimen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#27491;&#21322;&#23450;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#39640;&#25928;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#27491;&#21322;&#23450;-PSD&#27169;&#22411;&#22312;&#31934;&#24230;$\varepsilon$&#19979;&#29983;&#25104;iid&#26679;&#26412;&#12290;&#31639;&#27861;&#22797;&#26434;&#24230;&#20026;$O(T d \log(1/\varepsilon) m^2 + d m^{\beta+1} \log(T)/\varepsilon^2)$&#65292;&#20854;&#20013;$T$&#26159;&#26102;&#38388;&#27493;&#25968;&#65292;$\beta$&#26159;Fokker-Planck&#35299;&#30340;&#27491;&#21017;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17109</link><description>&lt;p&gt;
&#27491;&#21322;&#23450;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#39640;&#25928;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Efficient Sampling of Stochastic Differential Equations with Positive Semi-Definite Models. (arXiv:2303.17109v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#27491;&#21322;&#23450;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#39640;&#25928;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#27491;&#21322;&#23450;-PSD&#27169;&#22411;&#22312;&#31934;&#24230;$\varepsilon$&#19979;&#29983;&#25104;iid&#26679;&#26412;&#12290;&#31639;&#27861;&#22797;&#26434;&#24230;&#20026;$O(T d \log(1/\varepsilon) m^2 + d m^{\beta+1} \log(T)/\varepsilon^2)$&#65292;&#20854;&#20013;$T$&#26159;&#26102;&#38388;&#27493;&#25968;&#65292;$\beta$&#26159;Fokker-Planck&#35299;&#30340;&#27491;&#21017;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#24050;&#30693;&#28418;&#31227;&#20989;&#25968;&#21644;&#25193;&#25955;&#30697;&#38453;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#39640;&#25928;&#37319;&#26679;&#30340;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#19968;&#20010;&#26368;&#36817;&#30340;&#27010;&#29575;&#27169;&#22411;&#65288;&#27491;&#21322;&#23450;-PSD&#27169;&#22411;&#65289;\citep{rudi2021psd}&#65292;&#20174;&#20013;&#21487;&#20197;&#33719;&#24471;&#31934;&#24230;&#20026;$\varepsilon$&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;iid&#65289;&#26679;&#26412;&#65292;&#20854;&#25104;&#26412;&#20026;$m^2 d \log(1/\varepsilon)$&#65292;&#20854;&#20013;$m$&#26159;&#27169;&#22411;&#30340;&#32500;&#24230;&#65292;$d$&#26159;&#31354;&#38388;&#30340;&#32500;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;&#65306;&#39318;&#20808;&#35745;&#31639;&#28385;&#36275;&#19982;SDE&#30456;&#20851;&#32852;&#30340;Fokker-Planck&#26041;&#31243;&#65288;&#25110;&#20854;&#20998;&#25968;&#21464;&#20307;&#65289;&#30340;PSD&#27169;&#22411;&#65292;&#35823;&#24046;&#20026;$\varepsilon$&#65292;&#28982;&#21518;&#20174;&#29983;&#25104;&#30340;PSD&#27169;&#22411;&#20013;&#37319;&#26679;&#12290;&#20551;&#35774;Fokker-Planck&#35299;&#20855;&#26377;&#19968;&#23450;&#30340;&#27491;&#21017;&#24615;&#65288;&#21363;$\beta$&#38454;&#21487;&#24494;&#24615;&#20197;&#21450;&#20854;&#38646;&#28857;&#30340;&#19968;&#20123;&#20960;&#20309;&#26465;&#20214;&#65289;&#65292;&#25105;&#20204;&#24471;&#21040;&#19968;&#20010;&#31639;&#27861;&#65306;&#65288;a&#65289;&#22312;&#20934;&#22791;&#38454;&#27573;&#65292;&#33719;&#24471;&#20855;&#26377;L2&#36317;&#31163;$\varepsilon$&#30340;PSD&#27169;&#22411;&#20316;&#20026;&#30495;&#23454;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#20272;&#35745;&#65307;&#65288;b&#65289;&#22312;&#37319;&#26679;&#38454;&#27573;&#65292;&#20197;&#31934;&#24230;$\varepsilon$&#29983;&#25104;SDE&#35299;&#30340;iid&#26679;&#26412;&#12290;&#25152;&#24471;&#21040;&#30340;&#22797;&#26434;&#24230;&#20026;$O(T d \log(1/\varepsilon) m^2 + d m^{\beta+1} \log(T)/\varepsilon^2)$&#65292;&#20854;&#20013;$T$&#26159;SDE&#30340;&#26102;&#38388;&#27493;&#25968;&#65292;$\beta$&#26159;Fokker-Planck&#35299;&#30340;&#27491;&#21017;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper deals with the problem of efficient sampling from a stochastic differential equation, given the drift function and the diffusion matrix. The proposed approach leverages a recent model for probabilities \citep{rudi2021psd} (the positive semi-definite -- PSD model) from which it is possible to obtain independent and identically distributed (i.i.d.) samples at precision $\varepsilon$ with a cost that is $m^2 d \log(1/\varepsilon)$ where $m$ is the dimension of the model, $d$ the dimension of the space. The proposed approach consists in: first, computing the PSD model that satisfies the Fokker-Planck equation (or its fractional variant) associated with the SDE, up to error $\varepsilon$, and then sampling from the resulting PSD model. Assuming some regularity of the Fokker-Planck solution (i.e. $\beta$-times differentiability plus some geometric condition on its zeros) We obtain an algorithm that: (a) in the preparatory phase obtains a PSD model with L2 distance $\varepsilon$ fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26500;&#25104;&#30005;&#24433;&#27785;&#28024;&#24863;&#30340;&#20855;&#20307;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#21592;&#39537;&#21160;&#30340;&#25668;&#20687;&#26426;&#31227;&#21160;&#29983;&#25104;&#31995;&#32479;&#65292;&#20197;&#23454;&#29616;&#24773;&#24863;&#21644;&#31354;&#38388;&#30340;&#27785;&#28024;&#24863;&#12290;</title><link>http://arxiv.org/abs/2303.17041</link><description>&lt;p&gt;
&#27785;&#28024;&#24863;&#31192;&#35776;&#65306;&#22522;&#20110;&#28436;&#21592;&#30340;&#33258;&#21160;&#29983;&#25104;&#30005;&#24433;&#25668;&#24433;&#26426;&#31227;&#21160;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
The secret of immersion: actor driven camera movement generation for auto-cinematography. (arXiv:2303.17041v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26500;&#25104;&#30005;&#24433;&#27785;&#28024;&#24863;&#30340;&#20855;&#20307;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#21592;&#39537;&#21160;&#30340;&#25668;&#20687;&#26426;&#31227;&#21160;&#29983;&#25104;&#31995;&#32479;&#65292;&#20197;&#23454;&#29616;&#24773;&#24863;&#21644;&#31354;&#38388;&#30340;&#27785;&#28024;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27785;&#28024;&#24863;&#22312;&#35774;&#35745;&#30005;&#24433;&#26102;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#28982;&#32780;&#65292;&#27785;&#28024;&#24335;&#25293;&#25668;&#30340;&#22256;&#38590;&#38459;&#30861;&#20102;&#35774;&#35745;&#24072;&#21019;&#36896;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#25104;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26500;&#25104;&#30005;&#24433;&#27785;&#28024;&#24863;&#30340;&#20855;&#20307;&#32452;&#25104;&#37096;&#20998;&#65292;&#32771;&#34385;&#20102;&#31354;&#38388;&#12289;&#24773;&#24863;&#21644;&#32654;&#23398;&#31561;&#26041;&#38754;&#65292;&#21516;&#26102;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#34987;&#32467;&#21512;&#21040;&#20102;&#19968;&#20010;&#39640;&#32423;&#35780;&#20272;&#26426;&#21046;&#20013;&#12290;&#22312;&#36825;&#26679;&#30340;&#27785;&#28024;&#26426;&#21046;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#25668;&#20687;&#26426;&#25511;&#21046;&#31995;&#32479;&#65292;&#33021;&#22815;&#22312;3D&#34394;&#25311;&#29615;&#22659;&#20013;&#29983;&#25104;&#22522;&#20110;&#28436;&#21592;&#39537;&#21160;&#30340;&#25668;&#20687;&#26426;&#31227;&#21160;&#65292;&#20197;&#33719;&#24471;&#27785;&#28024;&#24335;&#30005;&#24433;&#24207;&#21015;&#12290;&#29983;&#25104;&#36807;&#31243;&#20013;&#25552;&#20986;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#23558;&#28436;&#21592;&#36816;&#21160;&#36716;&#25442;&#20026;&#20197;&#24773;&#24863;&#22240;&#32032;&#20026;&#26465;&#20214;&#30340;&#25668;&#20687;&#26426;&#36712;&#36857;&#65292;&#30830;&#20445;&#20102;&#28436;&#21592;&#19982;&#25668;&#20687;&#26426;&#30340;&#29289;&#29702;&#21644;&#24515;&#29702;&#21516;&#27493;&#20197;&#23454;&#29616;&#31354;&#38388;&#21644;&#24773;&#24863;&#30340;&#27785;&#28024;&#24863;&#12290;&#36890;&#36807;&#21152;&#20837;&#25511;&#21046;&#25668;&#20687;&#26426;&#25238;&#21160;&#20197;&#34920;&#36798;&#19981;&#21516;&#24515;&#29702;&#29366;&#24577;&#30340;&#27491;&#21017;&#21270;&#65292;&#24773;&#24863;&#27785;&#28024;&#24863;&#24471;&#21040;&#20102;&#36827;&#19968;&#27493;&#21152;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
Immersion plays a vital role when designing cinematic creations, yet the difficulty in immersive shooting prevents designers to create satisfactory outputs. In this work, we analyze the specific components that contribute to cinematographic immersion considering spatial, emotional, and aesthetic level, while these components are then combined into a high-level evaluation mechanism. Guided by such a immersion mechanism, we propose a GAN-based camera control system that is able to generate actor-driven camera movements in the 3D virtual environment to obtain immersive film sequences. The proposed encoder-decoder architecture in the generation flow transfers character motion into camera trajectory conditioned on an emotion factor. This ensures spatial and emotional immersion by performing actor-camera synchronization physically and psychologically. The emotional immersion is further strengthened by incorporating regularization that controls camera shakiness for expressing different mental
&lt;/p&gt;</description></item><item><title>KeGNN&#26159;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21487;&#20197;&#32467;&#21512;&#20808;&#21069;&#30340;&#30693;&#35782;&#26469;&#20248;&#21270;&#22270;&#25968;&#25454;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.15487</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Knowledge Enhanced Graph Neural Networks. (arXiv:2303.15487v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15487
&lt;/p&gt;
&lt;p&gt;
KeGNN&#26159;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21487;&#20197;&#32467;&#21512;&#20808;&#21069;&#30340;&#30693;&#35782;&#26469;&#20248;&#21270;&#22270;&#25968;&#25454;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25968;&#25454;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#65292;&#24182;&#19988;&#20855;&#26377;&#21508;&#31181;&#24212;&#29992;&#65292;&#20363;&#22914;&#33258;&#28982;&#31185;&#23398;&#12289;&#31038;&#20132;&#32593;&#32476;&#25110;&#35821;&#20041;&#32593;&#12290;&#23613;&#31649;&#23500;&#21547;&#20449;&#24687;&#65292;&#20294;&#22270;&#24418;&#36890;&#24120;&#22122;&#22768;&#21644;&#19981;&#23436;&#25972;&#12290;&#22240;&#27492;&#65292;&#22270;&#34917;&#20840;&#20219;&#21153;&#65292;&#22914;&#33410;&#28857;&#20998;&#31867;&#25110;&#38142;&#25509;&#39044;&#27979;&#65292;&#24050;&#32463;&#21463;&#21040;&#20851;&#27880;&#12290;&#19968;&#26041;&#38754;&#65292;&#31070;&#32463;&#26041;&#27861;&#65288;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;&#65289;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#22788;&#29702;&#22122;&#22768;&#22270;&#30340;&#31283;&#20581;&#24037;&#20855;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#31526;&#21495;&#26041;&#27861;&#21487;&#20197;&#23545;&#22270;&#36827;&#34892;&#31934;&#30830;&#25512;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;KeGNN&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#22270;&#25968;&#25454;&#19978;&#23398;&#20064;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#20004;&#31181;&#33539;&#20363;&#65292;&#24182;&#20801;&#35768;&#23558;&#20808;&#21069;&#30340;&#30693;&#35782;&#38598;&#25104;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#12290;&#20174;&#26412;&#36136;&#19978;&#35762;&#65292;KeGNN&#30001;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#32452;&#25104;&#65292;&#20854;&#20013;&#22522;&#20110;&#30446;&#26631;&#23558;&#30693;&#35782;&#22686;&#24378;&#23618;&#22534;&#21472;&#22312;&#20854;&#19978;&#65292;&#20197;&#20351;&#38024;&#23545;&#20808;&#21069;&#30693;&#35782;&#30340;&#39044;&#27979;&#24471;&#21040;&#20248;&#21270;&#12290;&#25105;&#20204;&#23558;KeGNN&#19982;&#20004;&#20010;&#26631;&#20934;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#19968;&#36215;&#23454;&#20363;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#20808;&#21069;&#30340;&#30693;&#35782;&#38598;&#25104;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#21487;&#20197;&#25552;&#39640;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph data is omnipresent and has a large variety of applications such as natural science, social networks or semantic web. Though rich in information, graphs are often noisy and incomplete. Therefore, graph completion tasks such as node classification or link prediction have gained attention. On the one hand, neural methods such as graph neural networks have proven to be robust tools for learning rich representations of noisy graphs. On the other hand, symbolic methods enable exact reasoning on graphs. We propose KeGNN, a neuro-symbolic framework for learning on graph data that combines both paradigms and allows for the integration of prior knowledge into a graph neural network model. In essence, KeGNN consists of a graph neural network as a base on which knowledge enhancement layers are stacked with the objective of refining predictions with respect to prior knowledge. We instantiate KeGNN in conjunction with two standard graph neural networks: Graph Convolutional Networks and Graph 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#31639;&#27861;&#26694;&#26550;&#26469;&#23558;&#23618;&#27425;&#32858;&#31867;&#26041;&#27861;&#24212;&#29992;&#20110;&#22270;&#20013;&#30340;&#31038;&#21306;&#26816;&#27979;&#12290;&#30456;&#20284;&#24230;&#30697;&#38453;&#12289;&#29305;&#24449;&#21521;&#37327;&#30697;&#38453;&#21644;&#33410;&#28857;&#27431;&#20960;&#37324;&#24471;&#21521;&#37327;&#34920;&#31034;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#22522;&#20110;&#36830;&#25509;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#21487;&#20197;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#21644;&#28857;&#38388;&#20114;&#20449;&#24687;&#27491;&#21521;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.12212</link><description>&lt;p&gt;
&#22522;&#20110;&#33410;&#28857;&#30456;&#20284;&#24230;&#12289;&#22270;&#34920;&#31034;&#23398;&#20064;&#21644;&#23618;&#27425;&#32858;&#31867;&#30340;&#22797;&#26434;&#32593;&#32476;&#31038;&#32676;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Community detection in complex networks via node similarity, graph representation learning, and hierarchical clustering. (arXiv:2303.12212v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#31639;&#27861;&#26694;&#26550;&#26469;&#23558;&#23618;&#27425;&#32858;&#31867;&#26041;&#27861;&#24212;&#29992;&#20110;&#22270;&#20013;&#30340;&#31038;&#21306;&#26816;&#27979;&#12290;&#30456;&#20284;&#24230;&#30697;&#38453;&#12289;&#29305;&#24449;&#21521;&#37327;&#30697;&#38453;&#21644;&#33410;&#28857;&#27431;&#20960;&#37324;&#24471;&#21521;&#37327;&#34920;&#31034;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#22522;&#20110;&#36830;&#25509;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#21487;&#20197;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#21644;&#28857;&#38388;&#20114;&#20449;&#24687;&#27491;&#21521;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#32676;&#26816;&#27979;&#26159;&#20998;&#26512;&#23454;&#38469;&#22270;&#21644;&#22797;&#26434;&#32593;&#32476;&#65288;&#22914;&#31038;&#20132;&#12289;&#20132;&#36890;&#12289;&#24341;&#29992;&#12289;&#32593;&#32476;&#23433;&#20840;&#20197;&#21450;&#39135;&#29289;&#38142;&#65289;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#21463;&#21040;&#31038;&#21306;&#26816;&#27979;&#19982;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#32858;&#31867;&#20043;&#38388;&#30340;&#35768;&#22810;&#30456;&#20284;&#24615;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#23558;&#23618;&#27425;&#32858;&#31867;&#26041;&#27861;&#24212;&#29992;&#20110;&#22270;&#20013;&#30340;&#31038;&#21306;&#26816;&#27979;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22522;&#20110;&#33410;&#28857;&#30456;&#20284;&#24230;&#30697;&#38453;&#12289;&#20854;&#29305;&#24449;&#21521;&#37327;&#30697;&#38453;&#20197;&#21450;&#33410;&#28857;&#30340;&#27431;&#20960;&#37324;&#24471;&#21521;&#37327;&#34920;&#31034;&#65292;&#24212;&#29992;&#21508;&#31181;&#22522;&#20110;&#36830;&#25509;&#30340;&#32858;&#31867;&#31639;&#27861;&#65288;&#21333;&#38142;&#25509;&#12289;&#20840;&#36830;&#25509;&#12289;&#24179;&#22343;&#38142;&#25509;&#12289;Ward&#12289;Genie&#65289;&#26469;&#26597;&#25214;&#31038;&#21306;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#26694;&#26550;&#36873;&#25321;&#20998;&#26512;&#65292;&#21253;&#25324;&#26368;&#20808;&#36827;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#22270;&#34920;&#31034;&#65289;&#20197;&#21450;&#19968;&#20010;&#24050;&#30693;&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#32467;&#26524;&#30340;&#28857;&#38388;&#20114;&#20449;&#24687;&#27491;&#21521;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community detection is a critical challenge in the analysis of real-world graphs and complex networks, including social, transportation, citation, cybersecurity networks, and food webs. Motivated by many similarities between community detection and clustering in Euclidean spaces, we propose three algorithm frameworks to apply hierarchical clustering methods for community detection in graphs. We show that using our methods, it is possible to apply various linkage-based (single-, complete-, average- linkage, Ward, Genie) clustering algorithms to find communities based on vertex similarity matrices, eigenvector matrices thereof, and Euclidean vector representations of nodes. We convey a comprehensive analysis of choices for each framework, including state-of-the-art graph representation learning algorithms, such as Deep Neural Graph Representation, and a vertex proximity matrix known to yield high-quality results in machine learning -- Positive Pointwise Mutual Information. Overall, we te
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20803;&#35821;&#35328;&#8212;&#8212;LDL&#65292;&#29992;&#20110;&#23450;&#20041;DL&#65292;&#35813;&#20803;&#35821;&#35328;&#20174;&#35821;&#27861;&#21644;&#35821;&#20041;&#20004;&#26041;&#38754;&#19978;&#25552;&#39640;DL&#30340;&#24418;&#24335;&#21270;&#31243;&#24230;&#65292;&#20351;&#24471;&#23545;DL&#30340;&#24615;&#36136;&#21644;&#23454;&#29616;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#30740;&#31350;&#25104;&#20026;&#20102;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.10650</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#36923;&#36753;&#30340;&#36923;&#36753;&#65306;&#36208;&#21521;DL&#30340;&#32479;&#19968;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Logic of Differentiable Logics: Towards a Uniform Semantics of DL. (arXiv:2303.10650v2 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10650
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20803;&#35821;&#35328;&#8212;&#8212;LDL&#65292;&#29992;&#20110;&#23450;&#20041;DL&#65292;&#35813;&#20803;&#35821;&#35328;&#20174;&#35821;&#27861;&#21644;&#35821;&#20041;&#20004;&#26041;&#38754;&#19978;&#25552;&#39640;DL&#30340;&#24418;&#24335;&#21270;&#31243;&#24230;&#65292;&#20351;&#24471;&#23545;DL&#30340;&#24615;&#36136;&#21644;&#23454;&#29616;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#30740;&#31350;&#25104;&#20026;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#21487;&#24494;&#20998;&#36923;&#36753;&#65288;DL&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#28385;&#36275;&#36923;&#36753;&#35268;&#33539;&#30340;&#26041;&#27861;&#12290;DL&#21253;&#25324;&#35821;&#27861;&#21644;&#23558;&#35821;&#27861;&#20013;&#30340;&#34920;&#36798;&#24335;&#36716;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#30340;&#35299;&#37322;&#20989;&#25968;&#12290;&#36825;&#20123;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19982;&#26631;&#20934;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#19968;&#36215;&#20351;&#29992;&#12290; &#29616;&#26377;DL&#30340;&#22810;&#26679;&#24615;&#21644;&#23545;&#20854;&#24418;&#24335;&#21270;&#31243;&#24230;&#30340;&#19981;&#21516;&#22788;&#29702;&#20351;&#24471;&#23545;&#23427;&#20204;&#30340;&#24615;&#36136;&#21644;&#23454;&#29616;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#30740;&#31350;&#21464;&#24471;&#22256;&#38590;&#12290;&#35813;&#35770;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20803;&#35821;&#35328;&#8212;&#8212;LDL&#20316;&#20026;DL&#23450;&#20041;&#30340;&#31995;&#32479;&#26694;&#26550;&#65292;&#20174;&#35821;&#27861;&#21644;&#35821;&#20041;&#20004;&#26041;&#38754;&#19978;&#25552;&#39640;DL&#30340;&#24418;&#24335;&#21270;&#31243;&#24230;&#65292;&#20351;&#24471;&#23545;DL&#30340;&#24615;&#36136;&#21644;&#23454;&#29616;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#30740;&#31350;&#25104;&#20026;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable logics (DL) have recently been proposed as a method of training neural networks to satisfy logical specifications. A DL consists of a syntax in which specifications are stated and an interpretation function that translates expressions in the syntax into loss functions. These loss functions can then be used during training with standard gradient descent algorithms. The variety of existing DLs and the differing levels of formality with which they are treated makes a systematic comparative study of their properties and implementations difficult. This paper remedies this problem by suggesting a meta-language for defining DLs that we call the Logic of Differentiable Logics, or LDL. Syntactically, it generalises the syntax of existing DLs to FOL, and for the first time introduces the formalism for reasoning about vectors and learners. Semantically, it introduces a general interpretation function that can be instantiated to define loss functions arising from different existing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;&#40657;&#30418;&#21464;&#20998;&#25512;&#29702;&#65288;BBVI&#65289;&#28385;&#36275;SGD&#25991;&#29486;&#20013;&#30340;ABC&#26465;&#20214;&#65292;&#35813;&#32467;&#26524;&#36866;&#29992;&#20110;&#24179;&#28369;&#21644;&#20108;&#27425;&#22686;&#38271;&#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#65292;&#21516;&#26102;&#25105;&#20204;&#30340;&#32467;&#26524;&#25512;&#24191;&#21040;&#24191;&#27867;&#24212;&#29992;&#20110;BBVI&#23454;&#36341;&#20013;&#30340;&#38750;&#32447;&#24615;&#21327;&#26041;&#24046;&#21442;&#25968;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.10472</link><description>&lt;p&gt;
&#40657;&#30418;&#21464;&#20998;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#23454;&#29992;&#21305;&#37197;&#26799;&#24230;&#26041;&#24046;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Practical and Matching Gradient Variance Bounds for Black-Box Variational Bayesian Inference. (arXiv:2303.10472v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#40657;&#30418;&#21464;&#20998;&#25512;&#29702;&#65288;BBVI&#65289;&#28385;&#36275;SGD&#25991;&#29486;&#20013;&#30340;ABC&#26465;&#20214;&#65292;&#35813;&#32467;&#26524;&#36866;&#29992;&#20110;&#24179;&#28369;&#21644;&#20108;&#27425;&#22686;&#38271;&#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#65292;&#21516;&#26102;&#25105;&#20204;&#30340;&#32467;&#26524;&#25512;&#24191;&#21040;&#24191;&#27867;&#24212;&#29992;&#20110;BBVI&#23454;&#36341;&#20013;&#30340;&#38750;&#32447;&#24615;&#21327;&#26041;&#24046;&#21442;&#25968;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#40657;&#30418;&#21464;&#20998;&#25512;&#29702;&#65288;BBVI&#65289;&#30340;&#26799;&#24230;&#26041;&#24046;&#26159;&#24314;&#31435;&#20854;&#25910;&#25947;&#24615;&#21644;&#31639;&#27861;&#25913;&#36827;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#23578;&#26410;&#34920;&#26126;BBVI&#30340;&#26799;&#24230;&#26041;&#24046;&#28385;&#36275;&#29992;&#20110;&#30740;&#31350;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#25910;&#25947;&#30340;&#26465;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#24212;&#29992;&#20110;&#24179;&#28369;&#21644;&#20108;&#27425;&#22686;&#38271;&#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#26102;&#65292;BBVI&#28385;&#36275;&#19982;SGD&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;ABC&#26465;&#20214;&#30456;&#21305;&#37197;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25512;&#24191;&#21040;&#24191;&#27867;&#24212;&#29992;&#20110;BBVI&#23454;&#36341;&#20013;&#30340;&#38750;&#32447;&#24615;&#21327;&#26041;&#24046;&#21442;&#25968;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#24179;&#22343;&#22330;&#21442;&#25968;&#21270;&#30340;&#26041;&#24046;&#20855;&#26377;&#32463;&#36807;&#39564;&#35777;&#30340;&#20248;&#36234;&#32500;&#24230;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the gradient variance of black-box variational inference (BBVI) is a crucial step for establishing its convergence and developing algorithmic improvements. However, existing studies have yet to show that the gradient variance of BBVI satisfies the conditions used to study the convergence of stochastic gradient descent (SGD), the workhorse of BBVI. In this work, we show that BBVI satisfies a matching bound corresponding to the $ABC$ condition used in the SGD literature when applied to smooth and quadratically-growing log-likelihoods. Our results generalize to nonlinear covariance parameterizations widely used in the practice of BBVI. Furthermore, we show that the variance of the mean-field parameterization has provably superior dimensional dependence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36229;&#32423;&#30417;&#27979;&#30340;&#36828;&#31243;&#23454;&#29616;&#29492;&#30168;&#26089;&#26399;&#35786;&#26029;&#30340;&#31574;&#30053;&#12290;&#35813;&#31574;&#30053;&#20197;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20026;&#22522;&#30784;&#65292;&#21487;&#23454;&#29616;&#39640;&#28789;&#25935;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#30149;&#30151;&#20998;&#31867;&#65292;&#21516;&#26102;&#25104;&#26412;&#20302;&#12289;&#26131;&#29992;&#24615;&#39640;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.09780</link><description>&lt;p&gt;
Mpox-AISM&#65306;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36229;&#32423;&#30417;&#27979;&#20197;&#36943;&#21046;&#29492;&#30168;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Mpox-AISM: AI-Mediated Super Monitoring for Forestalling Monkeypox Spread. (arXiv:2303.09780v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36229;&#32423;&#30417;&#27979;&#30340;&#36828;&#31243;&#23454;&#29616;&#29492;&#30168;&#26089;&#26399;&#35786;&#26029;&#30340;&#31574;&#30053;&#12290;&#35813;&#31574;&#30053;&#20197;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20026;&#22522;&#30784;&#65292;&#21487;&#23454;&#29616;&#39640;&#28789;&#25935;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#30149;&#30151;&#20998;&#31867;&#65292;&#21516;&#26102;&#25104;&#26412;&#20302;&#12289;&#26131;&#29992;&#24615;&#39640;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#21450;&#26102;&#12289;&#20415;&#25463;&#21644;&#20934;&#30830;&#35786;&#26029;&#26089;&#26399;&#24739;&#32773;&#26159;&#36943;&#21046;&#29492;&#30168;&#20256;&#25773;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36828;&#31243;&#12289;&#23454;&#26102;&#30340;&#22312;&#32447;&#21487;&#35270;&#21270;&#31574;&#30053;&#65292;&#31216;&#20026;&#8220;&#36229;&#32423;&#30417;&#27979;&#8221;&#65292;&#29992;&#20110;&#26500;&#24314;&#20302;&#25104;&#26412;&#12289;&#26041;&#20415;&#12289;&#21450;&#26102;&#21644;&#26080;&#19987;&#19994;&#30693;&#35782;&#30340;&#29492;&#30168;&#26089;&#26399;&#35786;&#26029;&#12290;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#32452;&#35013;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#20171;&#23548;&#30340;&#8220;&#36229;&#32423;&#30417;&#27979;&#8221;&#65288;Mpox-AISM&#65289;&#65292;&#26681;&#25454;&#25968;&#25454;&#38598;&#29305;&#24449;&#21644;&#29492;&#30168;&#28436;&#21464;&#36235;&#21183;&#20197;&#21450;&#19982;&#39640;&#30456;&#20284;&#24230;&#30340;&#20854;&#20182;&#19971;&#31181;&#30382;&#32932;&#30149;&#30340;&#19987;&#19994;&#20998;&#31867;&#65292;&#22240;&#27492;&#36825;&#20123;&#21151;&#33021;&#19982;&#21512;&#29702;&#30340;&#31243;&#24207;&#30028;&#38754;&#21644;&#38408;&#20540;&#35774;&#32622;&#30830;&#20445;&#20102;&#20854;&#28789;&#25935;&#24230;&#36229;&#36807;95.9&#65285;&#65292;&#29305;&#24322;&#24230;&#20960;&#20046;&#36798;&#21040;100&#65285;&#12290;&#22240;&#27492;&#65292;&#22312;&#20114;&#32852;&#32593;&#21644;&#36890;&#35759;&#32456;&#31471;&#30340;&#20113;&#26381;&#21153;&#30340;&#24110;&#21161;&#19979;&#65292;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#28508;&#22312;&#22320;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#29492;&#30168;&#30340;&#26089;&#26399;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenge on forestalling monkeypox (Mpox) spread is the timely, convenient and accurate diagnosis for earlystage infected individuals. Here, we propose a remote and realtime online visualization strategy, called "Super Monitoring" to construct a low cost, convenient, timely and unspecialized diagnosis of early-stage Mpox. Such AI-mediated "Super Monitoring" (Mpox-AISM) invokes a framework assembled by deep learning, data augmentation and self-supervised learning, as well as professionally classifies four subtypes according to dataset characteristics and evolution trend of Mpox and seven other types of dermatopathya with high similarity, hence these features together with reasonable program interface and threshold setting ensure that its Recall (Sensitivity) was beyond 95.9% and the specificity was almost 100%. As a result, with the help of cloud service on Internet and communication terminal, this strategy can be potentially utilized for the real-time detection of earlystage Mpox 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#20998;&#24067;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;M/EEG&#20449;&#21495;&#19978;&#30340;&#33041;&#40836;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#20102;&#20999;&#29255;Wasserstein&#36317;&#31163;&#24182;&#35777;&#26126;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#39046;&#22495;&#36866;&#24212;&#30340;&#22823;&#33041;&#35745;&#31639;&#26426;&#30028;&#38754;&#24212;&#29992;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#20063;&#35777;&#26126;&#20102;&#20854;&#25928;&#29575;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.05798</link><description>&lt;p&gt;
&#23545;M/EEG&#20449;&#21495;&#19978;&#30340;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#36827;&#34892;&#20999;&#29255;Wasserstein&#36317;&#31163;&#30340;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Sliced-Wasserstein on Symmetric Positive Definite Matrices for M/EEG Signals. (arXiv:2303.05798v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#20998;&#24067;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;M/EEG&#20449;&#21495;&#19978;&#30340;&#33041;&#40836;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#20102;&#20999;&#29255;Wasserstein&#36317;&#31163;&#24182;&#35777;&#26126;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#39046;&#22495;&#36866;&#24212;&#30340;&#22823;&#33041;&#35745;&#31639;&#26426;&#30028;&#38754;&#24212;&#29992;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#20063;&#35777;&#26126;&#20102;&#20854;&#25928;&#29575;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22788;&#29702;&#30005;&#25110;&#32773;&#30913;&#24615;&#33041;&#30005;&#22270;&#35760;&#24405;&#26102;&#65292;&#35768;&#22810;&#30417;&#30563;&#24335;&#39044;&#27979;&#20219;&#21153;&#36890;&#36807;&#20351;&#29992;&#21327;&#26041;&#24046;&#30697;&#38453;&#26469;&#27719;&#24635;&#20449;&#21495;&#36827;&#34892;&#35299;&#20915;&#12290;&#20351;&#29992;&#36825;&#20123;&#30697;&#38453;&#36827;&#34892;&#23398;&#20064;&#38656;&#35201;&#20351;&#29992;&#20285;&#39532;&#23612;&#20960;&#20309;&#26469;&#35828;&#26126;&#23427;&#20204;&#30340;&#32467;&#26500;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#20998;&#24067;&#65292;&#24182;&#19988;&#22312;M / EEG&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#19978;&#23637;&#31034;&#20102;&#20854;&#35745;&#31639;&#25928;&#29575;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#27979;&#37327;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#20043;&#38388;&#30340;&#20999;&#29255;Wasserstein&#36317;&#31163;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#20854;&#23646;&#24615;&#21644;&#20869;&#26680;&#26041;&#27861;&#23558;&#27492;&#36317;&#31163;&#24212;&#29992;&#20110;&#20174;MEG&#25968;&#25454;&#39044;&#27979;&#33041;&#40836;&#65292;&#24182;&#23558;&#20854;&#19982;&#22522;&#20110;Riemannian&#20960;&#20309;&#30340;&#26368;&#26032;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#22312;&#39046;&#22495;&#36866;&#24212;&#30340;&#22823;&#33041;&#35745;&#31639;&#26426;&#30028;&#38754;&#24212;&#29992;&#20013;&#65292;&#23427;&#21487;&#20197;&#26159;Wasserstein&#36317;&#31163;&#30340;&#26377;&#25928;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
When dealing with electro or magnetoencephalography records, many supervised prediction tasks are solved by working with covariance matrices to summarize the signals. Learning with these matrices requires using Riemanian geometry to account for their structure. In this paper, we propose a new method to deal with distributions of covariance matrices and demonstrate its computational efficiency on M/EEG multivariate time series. More specifically, we define a Sliced-Wasserstein distance between measures of symmetric positive definite matrices that comes with strong theoretical guarantees. Then, we take advantage of its properties and kernel methods to apply this distance to brain-age prediction from MEG data and compare it to state-of-the-art algorithms based on Riemannian geometry. Finally, we show that it is an efficient surrogate to the Wasserstein distance in domain adaptation for Brain Computer Interface applications.
&lt;/p&gt;</description></item><item><title>FairShap&#26159;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#37325;&#20272;&#35745;&#31639;&#27861;&#20915;&#31574;&#20844;&#24179;&#24615;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;Shapley&#20540;&#20272;&#20540;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#21644;&#31934;&#30830;&#24615;&#65292;&#24182;&#19988;&#26131;&#20110;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.01928</link><description>&lt;p&gt;
&#22522;&#20110;Shapley&#20540;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#25968;&#25454;&#20877;&#21152;&#26435;&#26041;&#27861;FairShap
&lt;/p&gt;
&lt;p&gt;
FairShap: A Data Re-weighting Approach for Algorithmic Fairness based on Shapley Values. (arXiv:2303.01928v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01928
&lt;/p&gt;
&lt;p&gt;
FairShap&#26159;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#37325;&#20272;&#35745;&#31639;&#27861;&#20915;&#31574;&#20844;&#24179;&#24615;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;Shapley&#20540;&#20272;&#20540;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#21644;&#31934;&#30830;&#24615;&#65292;&#24182;&#19988;&#26131;&#20110;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20844;&#24179;&#24615;&#26159;&#26497;&#20854;&#37325;&#35201;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#28982;&#32780;&#24403;&#21069;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36235;&#21183;&#35201;&#27714;&#20351;&#29992;&#36890;&#24120;&#23384;&#22312;&#20559;&#24046;&#30340;&#28023;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19987;&#27880;&#20110;&#24314;&#27169;&#21644;&#32416;&#27491;&#25968;&#25454;&#20559;&#24046;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#25104;&#20026;&#26377;&#20215;&#20540;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Shapley&#20540;&#36827;&#34892;&#25968;&#25454;&#20272;&#20540;&#30340;&#39044;&#22788;&#29702;&#65288;&#20877;&#21152;&#26435;&#65289;&#26041;&#27861;FairShap&#65292;&#29992;&#20110;&#20844;&#24179;&#30340;&#31639;&#27861;&#20915;&#31574;&#21046;&#23450;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#27169;&#22411;&#26080;&#20851;&#19988;&#26131;&#20110;&#35299;&#37322;&#65292;&#22240;&#20026;&#23427;&#34913;&#37327;&#27599;&#20010;&#35757;&#32451;&#25968;&#25454;&#28857;&#23545;&#39044;&#23450;&#20041;&#30340;&#20844;&#24179;&#25351;&#26631;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20855;&#26377;&#19981;&#21516;&#30340;&#24615;&#36136;&#65292;&#26377;&#21508;&#31181;&#22521;&#35757;&#22330;&#26223;&#21644;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20135;&#29983;&#26356;&#20844;&#24179;&#30340;&#27169;&#22411;&#24182;&#19988;&#20934;&#30830;&#24230;&#26356;&#39640;&#25110;&#30456;&#20284;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#30452;&#26041;&#22270;&#21644;&#28508;&#31354;&#38388;&#21487;&#35270;&#21270;&#26469;&#35828;&#26126;FairShap&#30340;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#23545;&#20110;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#30830;&#20445;&#31639;&#27861;&#20915;&#31574;&#20844;&#24179;&#24615;&#26159;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic fairness is of utmost societal importance, yet the current trend in large-scale machine learning models requires training with massive datasets that are typically biased. In this context, pre-processing methods that focus on modeling and correcting bias in the data emerge as valuable approaches. In this paper, we propose FairShap, a novel pre-processing (re-weighting) method for fair algorithmic decision-making through data valuation by means of Shapley Values. Our approach is model agnostic and easily interpretable, as it measures the contribution of each training data point to a predefined fairness metric. We empirically validate FairShap on several state-of-the-art datasets of different nature, with a variety of training scenarios and models and show how it outperforms other methods, yielding fairer models with higher or similar levels of accuracy. We also illustrate FairShap's interpretability by means of histograms and latent space visualizations. We believe that this 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20195;&#25968;&#35821;&#35328;&#29255;&#27573;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#19978;&#32852;&#31995;&#30340;&#26694;&#26550;&#65292;&#24182;&#20174;MATLANG&#23450;&#20041;&#20102;&#19968;&#20010;&#31526;&#21512;3-WL&#27979;&#35797;&#30340;&#35821;&#27861;&#65292;&#36827;&#32780;&#24471;&#20986;&#19968;&#20010;&#31526;&#21512;3-WL GNN&#27169;&#22411;&#30340;G$^2$N$^2$&#12290;&#27492;&#22806;&#65292;&#35821;&#27861;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#35745;&#31639;&#38271;&#24230;&#20026;&#20845;&#21450;&#20197;&#19979;&#30340;&#29615;&#21644;&#24358;&#29615;&#30340;&#20195;&#25968;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20248;&#31168;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.01590</link><description>&lt;p&gt;
&#25216;&#26415;&#25253;&#21578;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#20063;&#21487;&#20197;&#21464;&#24471;&#35821;&#27861;&#21270;
&lt;/p&gt;
&lt;p&gt;
Technical report: Graph Neural Networks go Grammatical. (arXiv:2303.01590v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20195;&#25968;&#35821;&#35328;&#29255;&#27573;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#19978;&#32852;&#31995;&#30340;&#26694;&#26550;&#65292;&#24182;&#20174;MATLANG&#23450;&#20041;&#20102;&#19968;&#20010;&#31526;&#21512;3-WL&#27979;&#35797;&#30340;&#35821;&#27861;&#65292;&#36827;&#32780;&#24471;&#20986;&#19968;&#20010;&#31526;&#21512;3-WL GNN&#27169;&#22411;&#30340;G$^2$N$^2$&#12290;&#27492;&#22806;&#65292;&#35821;&#27861;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#35745;&#31639;&#38271;&#24230;&#20026;&#20845;&#21450;&#20197;&#19979;&#30340;&#29615;&#21644;&#24358;&#29615;&#30340;&#20195;&#25968;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20248;&#31168;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#19968;&#20010;&#20195;&#25968;&#35821;&#35328;&#30340;&#19968;&#20010;&#29255;&#27573;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24418;&#24335;&#19978;&#32852;&#31995;&#36215;&#26469;&#12290;&#23427;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#65288;CFG&#65289;&#65292;&#23558;&#20195;&#25968;&#25805;&#20316;&#32452;&#32455;&#25104;&#21487;&#20197;&#32763;&#35793;&#20026;GNN&#23618;&#27169;&#22411;&#30340;&#29983;&#25104;&#35268;&#21017;&#12290;&#30001;&#20110;&#30452;&#25509;&#20174;&#35821;&#35328;&#27966;&#29983;&#20986;&#30340;CFG&#30340;&#35268;&#21017;&#21644;&#21464;&#37327;&#21253;&#21547;&#20887;&#20313;&#65292;&#22240;&#27492;&#20171;&#32461;&#20102;&#19968;&#31181;&#35821;&#27861;&#31616;&#21270;&#26041;&#26696;&#65292;&#20351;&#24471;&#23558;&#20854;&#32763;&#35793;&#20026;GNN&#23618;&#25104;&#20026;&#21487;&#33021;&#12290;&#24212;&#29992;&#36825;&#31181;&#31574;&#30053;&#65292;&#20174;MATLANG&#23450;&#20041;&#20102;&#19968;&#20010;&#31526;&#21512;&#31532;&#19977;&#38454;Weisfeiler-Lehman&#65288;3-WL&#65289;&#27979;&#35797;&#35201;&#27714;&#30340;&#35821;&#27861;&#12290;&#20174;&#36825;&#20010;3-WL CFG&#20013;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#35777;&#26126;&#31526;&#21512;3-WL GNN&#27169;&#22411;&#30340;G$^2$N$^2$&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#35821;&#27861;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#35745;&#31639;&#38271;&#24230;&#20026;&#20845;&#21450;&#20197;&#19979;&#30340;&#29615;&#21644;&#24358;&#29615;&#30340;&#20195;&#25968;&#20844;&#24335;&#65292;&#20174;&#32780;&#38416;&#26126;&#20102;3-WL&#30340;&#35745;&#25968;&#33021;&#21147;&#12290;&#22810;&#20010;&#23454;&#39564;&#35777;&#26126;&#65292;G$^2$N$^2$&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#35201;&#27604;&#20854;&#20182;3-WL GNN&#26356;&#20026;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a framework to formally link a fragment of an algebraic language to a Graph Neural Network (GNN). It relies on Context Free Grammars (CFG) to organise algebraic operations into generative rules that can be translated into a GNN layer model. Since the rules and variables of a CFG directly derived from a language contain redundancies, a grammar reduction scheme is presented making tractable the translation into a GNN layer. Applying this strategy, a grammar compliant with the third-order Weisfeiler-Lehman (3-WL) test is defined from MATLANG. From this 3-WL CFG, we derive a provably 3-WL GNN model called G$^2$N$^2$. Moreover, this grammatical approach allows us to provide algebraic formulas to count the cycles of length up to six and chordal cycles at the edge level, which enlightens the counting power of 3-WL. Several experiments illustrate that G$^2$N$^2$ efficiently outperforms other 3-WL GNNs on many downstream tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#23545;&#40784;&#34920;&#26684;&#32467;&#26500;&#35782;&#21035;&#22522;&#20934;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#26377;&#25928;&#20943;&#23569;&#25968;&#25454;&#38598;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#27880;&#37322;&#38169;&#35823;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.00716</link><description>&lt;p&gt;
&#23545;&#40784;&#34920;&#26684;&#32467;&#26500;&#35782;&#21035;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Aligning benchmark datasets for table structure recognition. (arXiv:2303.00716v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#23545;&#40784;&#34920;&#26684;&#32467;&#26500;&#35782;&#21035;&#22522;&#20934;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#26377;&#25928;&#20943;&#23569;&#25968;&#25454;&#38598;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#27880;&#37322;&#38169;&#35823;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#32467;&#26500;&#35782;&#21035;(TSR)&#22522;&#20934;&#25968;&#25454;&#38598;&#24517;&#39035;&#32463;&#36807;&#31934;&#24515;&#22788;&#29702;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#30340;&#27880;&#37322;&#20855;&#26377;&#19968;&#33268;&#24615;&#12290;&#20294;&#21363;&#20351;&#25968;&#25454;&#38598;&#30340;&#27880;&#37322;&#26159;&#33258;&#27965;&#30340;&#65292;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#20063;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#25439;&#23475;&#22522;&#20110;&#23427;&#20204;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#23545;&#40784;&#36825;&#20123;&#22522;&#20934;&#25968;&#25454;&#38598;$\unicode{x2014}$&#21024;&#38500;&#23427;&#20204;&#20043;&#38388;&#30340;&#38169;&#35823;&#21644;&#19981;&#19968;&#33268;&#24615;$\unicode{x2014}$&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#30340;&#26041;&#27861;&#26469;&#23637;&#31034;&#36825;&#19968;&#28857;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#20010;&#22266;&#23450;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#21363;&#34920;&#26684;&#21464;&#25442;&#22120;(TATR)&#12290;&#22312;PubTables-1M&#19978;&#35757;&#32451;&#30340;TATR&#30340;&#22522;&#20934;&#23436;&#20840;&#21305;&#37197;&#31934;&#24230;&#20026;65%&#65292;&#22312;FinTabNet&#19978;&#35757;&#32451;&#20026;42%&#65292;&#32452;&#21512;&#20026;69%&#12290;&#22312;&#20943;&#23569;&#27880;&#37322;&#38169;&#35823;&#21644;&#25968;&#25454;&#38598;&#38388;&#19981;&#19968;&#33268;&#24615;&#21518;&#65292;TATR&#22312;ICDAR-2013&#19978;&#30340;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#65292;&#24403;&#22312;PubTables-1M&#19978;&#35757;&#32451;&#26102;&#20026;75%&#65292;&#22312;FinTabNet&#19978;&#35757;&#32451;&#26102;&#20026;65%&#65292;&#32452;&#21512;&#20026;76%&#12290;
&lt;/p&gt;
&lt;p&gt;
Benchmark datasets for table structure recognition (TSR) must be carefully processed to ensure they are annotated consistently. However, even if a dataset's annotations are self-consistent, there may be significant inconsistency across datasets, which can harm the performance of models trained and evaluated on them. In this work, we show that aligning these benchmarks$\unicode{x2014}$removing both errors and inconsistency between them$\unicode{x2014}$improves model performance significantly. We demonstrate this through a data-centric approach where we adopt one model architecture, the Table Transformer (TATR), that we hold fixed throughout. Baseline exact match accuracy for TATR evaluated on the ICDAR-2013 benchmark is 65% when trained on PubTables-1M, 42% when trained on FinTabNet, and 69% combined. After reducing annotation mistakes and inter-dataset inconsistency, performance of TATR evaluated on ICDAR-2013 increases substantially to 75% when trained on PubTables-1M, 65% when traine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#24433;&#21709;&#30446;&#26631;&#23567;&#21306;&#21644;&#21608;&#22260;&#23567;&#21306;&#24615;&#33021;&#30340;&#23567;&#21306;&#21442;&#25968;&#36827;&#34892;&#26234;&#33021;&#35843;&#33410;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#20010;&#20849;&#20139;&#36890;&#29992;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#24182;&#22312;&#31163;&#32447;&#23398;&#20064;&#38454;&#27573;&#23545;&#26234;&#33021;&#20307;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#26368;&#32456;&#23558;&#32593;&#32476;&#24341;&#23548;&#21040;&#26368;&#20339;&#37197;&#32622;&#12290;</title><link>http://arxiv.org/abs/2302.12899</link><description>&lt;p&gt;
&#20855;&#26377;&#20849;&#21516;&#31574;&#30053;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#22825;&#32447;&#20542;&#26012;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning with Common Policy for Antenna Tilt Optimization. (arXiv:2302.12899v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#24433;&#21709;&#30446;&#26631;&#23567;&#21306;&#21644;&#21608;&#22260;&#23567;&#21306;&#24615;&#33021;&#30340;&#23567;&#21306;&#21442;&#25968;&#36827;&#34892;&#26234;&#33021;&#35843;&#33410;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#20010;&#20849;&#20139;&#36890;&#29992;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#24182;&#22312;&#31163;&#32447;&#23398;&#20064;&#38454;&#27573;&#23545;&#26234;&#33021;&#20307;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#26368;&#32456;&#23558;&#32593;&#32476;&#24341;&#23548;&#21040;&#26368;&#20339;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35843;&#25972;&#24433;&#21709;&#30446;&#26631;&#23567;&#21306;&#21644;&#21608;&#22260;&#23567;&#21306;&#24615;&#33021;&#30340;&#23567;&#21306;&#21442;&#25968;&#26469;&#20248;&#21270;&#26080;&#32447;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#20849;&#20139;&#19968;&#31181;&#36890;&#29992;&#31574;&#30053;&#65292;&#24182;&#32771;&#34385;&#20102;&#37051;&#36817;&#23567;&#21306;&#30340;&#20449;&#24687;&#26469;&#30830;&#23450;&#29366;&#24577;&#21644;&#22870;&#21169;&#12290;&#20026;&#20102;&#36991;&#20813;&#22312;&#23398;&#20064;&#30340;&#21021;&#26399;&#24433;&#21709;&#32593;&#32476;&#24615;&#33021;&#65292;&#26234;&#33021;&#20307;&#22312;&#31163;&#32447;&#23398;&#20064;&#30340;&#26089;&#26399;&#38454;&#27573;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#22312;&#27492;&#38454;&#27573;&#65292;&#36890;&#36807;&#38745;&#24577;&#32593;&#32476;&#27169;&#25311;&#22120;&#30340;&#21453;&#39304;&#21644;&#32771;&#34385;&#21508;&#31181;&#24773;&#20917;&#65292;&#33719;&#24471;&#19968;&#20010;&#21021;&#22987;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#36890;&#36807;&#24314;&#35758;&#23567;&#24133;&#22686;&#37327;&#21464;&#21270;&#26469;&#26234;&#33021;&#35843;&#33410;&#27979;&#35797;&#32593;&#32476;&#30340;&#23567;&#21306;&#21442;&#25968;&#65292;&#24182;&#32531;&#24930;&#22320;&#24341;&#23548;&#32593;&#32476;&#26397;&#21521;&#26368;&#20339;&#37197;&#32622;&#12290;&#26234;&#33021;&#20307;&#25552;&#20986;&#26368;&#20339;&#21464;&#21270;&#30340;&#24314;&#35758;&#26159;&#21033;&#29992;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#33719;&#24471;&#30340;&#27169;&#25311;&#22120;&#32463;&#39564;&#65292;&#20294;&#23427;&#20204;&#20063;&#21487;&#20197;&#32487;&#32493;&#20174;&#24403;&#21069;&#32593;&#32476;&#35835;&#25968;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a method for optimizing wireless networks by adjusting cell parameters that affect both the performance of the cell being optimized and the surrounding cells. The method uses multiple reinforcement learning agents that share a common policy and take into account information from neighboring cells to determine the state and reward. In order to avoid impairing network performance during the initial stages of learning, agents are pre-trained in an earlier phase of offline learning. During this phase, an initial policy is obtained using feedback from a static network simulator and considering a wide variety of scenarios. Finally, agents can intelligently tune the cell parameters of a test network by suggesting small incremental changes, slowly guiding the network toward an optimal configuration. The agents propose optimal changes using the experience gained with the simulator in the pre-training phase, but they can also continue to learn from current network readings af
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22240;&#26524;&#31574;&#30053;&#20998;&#31867;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#30495;&#23454;&#22240;&#26524;&#20316;&#29992;&#19979;&#24179;&#34913;&#31574;&#30053;&#24615;&#34892;&#20026;&#21644;&#20998;&#24067;&#36716;&#21464;&#65292;&#23454;&#29616;&#23545;&#20110;&#29305;&#24449;&#20462;&#25913;&#30340;&#40065;&#26834;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2302.06280</link><description>&lt;p&gt;
&#22240;&#26524;&#31574;&#30053;&#20998;&#31867;&#65306;&#20004;&#31181;&#36716;&#21464;&#20043;&#26053;
&lt;/p&gt;
&lt;p&gt;
Causal Strategic Classification: A Tale of Two Shifts. (arXiv:2302.06280v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22240;&#26524;&#31574;&#30053;&#20998;&#31867;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#30495;&#23454;&#22240;&#26524;&#20316;&#29992;&#19979;&#24179;&#34913;&#31574;&#30053;&#24615;&#34892;&#20026;&#21644;&#20998;&#24067;&#36716;&#21464;&#65292;&#23454;&#29616;&#23545;&#20110;&#29305;&#24449;&#20462;&#25913;&#30340;&#40065;&#26834;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#29992;&#25143;&#21487;&#20197;&#20174;&#26576;&#20123;&#39044;&#27979;&#32467;&#26524;&#20013;&#33719;&#30410;&#26102;&#65292;&#20182;&#20204;&#21487;&#33021;&#20250;&#20542;&#21521;&#20110;&#36890;&#36807;&#31574;&#30053;&#24615;&#20462;&#25913;&#20854;&#29305;&#24449;&#26469;&#23454;&#29616;&#36825;&#20123;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#31574;&#30053;&#20998;&#31867;&#30340;&#30446;&#26631;&#26159;&#35757;&#32451;&#20855;&#26377;&#23545;&#36825;&#31181;&#34892;&#20026;&#31283;&#20581;&#24615;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26694;&#26550;&#20551;&#35774;&#26356;&#25913;&#29305;&#24449;&#19981;&#20250;&#26356;&#25913;&#23454;&#38469;&#32467;&#26524;&#65292;&#36825;&#25551;&#32472;&#20102;&#29992;&#25143;&#8220;&#25805;&#32437;&#8221;&#31995;&#32479;&#30340;&#24773;&#24418;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#28040;&#38500;&#20102;&#36825;&#31181;&#20551;&#35774;&#65292;&#24182;&#30740;&#31350;&#20102;&#30495;&#23454;&#32467;&#26524;&#20250;&#21457;&#29983;&#21464;&#21270;&#30340;&#22240;&#26524;&#31574;&#30053;&#35774;&#32622;&#20013;&#30340;&#23398;&#20064;&#12290;&#20197;&#20934;&#30830;&#24615;&#20026;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31574;&#30053;&#24615;&#34892;&#20026;&#21644;&#22240;&#26524;&#20316;&#29992;&#22312;&#20004;&#31181;&#20114;&#34917;&#30340;&#20998;&#24067;&#36716;&#21464;&#24418;&#24335;&#19979;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#36825;&#20123;&#21464;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#36825;&#20004;&#20010;&#21147;&#37327;&#20043;&#38388;&#24179;&#34913;&#24182;&#20801;&#35768;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;&#22312;&#21512;&#25104;&#21644;&#21322;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
When users can benefit from certain predictive outcomes, they may be prone to act to achieve those outcome, e.g., by strategically modifying their features. The goal in strategic classification is therefore to train predictive models that are robust to such behavior. However, the conventional framework assumes that changing features does not change actual outcomes, which depicts users as "gaming" the system. Here we remove this assumption, and study learning in a causal strategic setting where true outcomes do change. Focusing on accuracy as our primary objective, we show how strategic behavior and causal effects underlie two complementing forms of distribution shift. We characterize these shifts, and propose a learning algorithm that balances between these two forces and over time, and permits end-to-end training. Experiments on synthetic and semi-synthetic data demonstrate the utility of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#21518;&#39564;&#37319;&#26679;&#26041;&#27861;&#26469;&#21435;&#38500;&#21253;&#21547;&#32467;&#26500;&#24615;&#22122;&#22768;&#30340;&#25968;&#25454;&#65292;&#30456;&#27604;&#20110;&#24120;&#35268;&#26041;&#27861;&#26377;&#36739;&#22909;&#30340;&#34920;&#29616;&#65292;&#23545;&#20110;&#21307;&#23398;&#25104;&#20687;&#31561;&#39046;&#22495;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2302.05290</link><description>&lt;p&gt;
&#37319;&#29992;&#25193;&#25955;&#27169;&#22411;&#21435;&#38500;&#32467;&#26500;&#24615;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Removing Structured Noise with Diffusion Models. (arXiv:2302.05290v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#21518;&#39564;&#37319;&#26679;&#26041;&#27861;&#26469;&#21435;&#38500;&#21253;&#21547;&#32467;&#26500;&#24615;&#22122;&#22768;&#30340;&#25968;&#25454;&#65292;&#30456;&#27604;&#20110;&#24120;&#35268;&#26041;&#27861;&#26377;&#36739;&#22909;&#30340;&#34920;&#29616;&#65292;&#23545;&#20110;&#21307;&#23398;&#25104;&#20687;&#31561;&#39046;&#22495;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#19981;&#36866;&#23450;&#21453;&#38382;&#39064;&#38656;&#35201;&#20180;&#32454;&#21046;&#23450;&#26377;&#20851;&#24863;&#20852;&#36259;&#20449;&#21495;&#30340;&#20808;&#39564;&#20449;&#24565;&#65292;&#24182;&#23545;&#23427;&#20204;&#22312;&#26377;&#22122;&#22768;&#27979;&#37327;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#20934;&#30830;&#30340;&#25551;&#36848;&#12290;&#22522;&#20110;&#31232;&#30095;&#24615;&#30340;&#25163;&#24037;&#21046;&#23450;&#20449;&#21495;&#20808;&#39564;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#25152;&#21462;&#20195;&#65292;&#24182;&#19988;&#20960;&#20010;&#22242;&#38431;&#26368;&#36817;&#23637;&#31034;&#20102;&#26368;&#26032;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#21644;&#28789;&#27963;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#21518;&#39564;&#37319;&#26679;&#33539;&#24335;&#21487;&#20197;&#25193;&#23637;&#21040;&#21253;&#25324;&#20016;&#23500;&#12289;&#32467;&#26500;&#21270;&#30340;&#22122;&#22768;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;&#26465;&#20214;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#65292;&#20854;&#20013;&#21253;&#21547;&#22122;&#22768;&#21644;&#20449;&#21495;&#29983;&#25104;&#20998;&#24067;&#30340;&#23398;&#20064;&#20998;&#25968;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#23384;&#22312;&#32467;&#26500;&#24615;&#22122;&#22768;&#30340;&#21453;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#22686;&#30410;&#65292;&#20248;&#20110;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#21644;&#23545;&#25239;&#32593;&#32476;&#30340;&#31454;&#20105;&#22522;&#32447;&#12290;&#36825;&#22312;&#21307;&#23398;&#25104;&#20687;&#31561;&#39046;&#22495;&#24320;&#36767;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#36827;&#34892;&#26356;&#20934;&#30830;&#24314;&#27169;&#30340;&#26032;&#26426;&#20250;&#21644;&#30456;&#20851;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving ill-posed inverse problems requires careful formulation of prior beliefs over the signals of interest and an accurate description of their manifestation into noisy measurements. Handcrafted signal priors based on e.g. sparsity are increasingly replaced by data-driven deep generative models, and several groups have recently shown that state-of-the-art score-based diffusion models yield particularly strong performance and flexibility. In this paper, we show that the powerful paradigm of posterior sampling with diffusion models can be extended to include rich, structured, noise models. To that end, we propose a joint conditional reverse diffusion process with learned scores for the noise and signal-generating distribution. We demonstrate strong performance gains across various inverse problems with structured noise, outperforming competitive baselines that use normalizing flows and adversarial networks. This opens up new opportunities and relevant practical applications of diffusi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#30446;&#26631;&#39044;&#27979;&#25193;&#25955;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#19981;&#33021;&#24456;&#22909;&#22320;&#24314;&#27169;&#22270;&#25299;&#25169;&#32467;&#26500;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22270;&#29983;&#25104;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.03596</link><description>&lt;p&gt;
&#24102;&#26377;&#30446;&#26631;&#39044;&#27979;&#25193;&#25955;&#28151;&#21512;&#30340;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Graph Generation with Destination-Predicting Diffusion Mixture. (arXiv:2302.03596v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#30446;&#26631;&#39044;&#27979;&#25193;&#25955;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#19981;&#33021;&#24456;&#22909;&#22320;&#24314;&#27169;&#22270;&#25299;&#25169;&#32467;&#26500;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22270;&#29983;&#25104;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22270;&#26159;&#29702;&#35299;&#20854;&#38750;&#27431;&#20960;&#37324;&#24471;&#32467;&#26500;&#22797;&#26434;&#24615;&#30340;&#30495;&#23454;&#20219;&#21153;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#34429;&#28982;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#29983;&#25104;&#26041;&#38754;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#30528;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#19981;&#36866;&#21512;&#24314;&#27169;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#22240;&#20026;&#23398;&#20064;&#21435;&#22122;&#22768;&#26679;&#26412;&#19981;&#33021;&#26126;&#30830;&#22320;&#25429;&#25417;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#39044;&#27979;&#25193;&#25955;&#36807;&#31243;&#30340;&#30446;&#26631;&#65292;&#21363;&#20855;&#26377;&#27491;&#30830;&#25299;&#25169;&#20449;&#24687;&#30340;&#21407;&#22987;&#22270;&#20316;&#20026;&#25968;&#25454;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#65292;&#24314;&#27169;&#20102;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#29983;&#25104;&#36807;&#31243;&#35774;&#35745;&#20026;&#19968;&#20010;&#20197;&#25968;&#25454;&#20998;&#24067;&#20013;&#30340;&#32456;&#28857;&#20026;&#26465;&#20214;&#30340;&#25193;&#25955;&#36807;&#31243;&#28151;&#21512;&#65292;&#23427;&#23558;&#36807;&#31243;&#25512;&#21521;&#39044;&#27979;&#30340;&#30446;&#26631;&#65292;&#24182;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#39044;&#27979;&#30446;&#26631;&#30340;&#26032;&#22411;&#26080;&#20223;&#30495;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#23558;&#36825;&#31181;&#31574;&#30053;&#32435;&#20837;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#19978;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generation of graphs is a major challenge for real-world tasks that require understanding the complex nature of their non-Euclidean structures. Although diffusion models have achieved notable success in graph generation recently, they are ill-suited for modeling the structural information of graphs since learning to denoise the noisy samples does not explicitly capture the graph topology. To tackle this limitation, we propose a novel generative framework that models the topology of graphs by predicting the destination of the diffusion process, which is the original graph that has the correct topology information, as a weighted mean of data. Specifically, we design the generative process as a mixture of diffusion processes conditioned on the endpoint in the data distribution, which drives the process toward the predicted destination, resulting in rapid convergence. We introduce new simulation-free training objectives for predicting the destination, and further discuss the advantages of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#23545;&#35805;&#24335;&#30340;&#35299;&#37322;&#27169;&#22411;&#65292;&#24357;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20197;&#21019;&#24314;&#26356;&#26377;&#25928;&#21644;&#24191;&#27867;&#24212;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2302.03460</link><description>&lt;p&gt;
&#27880;&#24847;&#30041;&#19979;&#31354;&#38553;&#65281;&#29992;&#40065;&#26364;&#21151;&#33021;&#29702;&#35770;&#26500;&#24314;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#29702;&#35299;&#20043;&#38388;&#30340;&#26725;&#26753;
&lt;/p&gt;
&lt;p&gt;
Mind the Gap! Bridging Explainable Artificial Intelligence and Human Understanding with Luhmann's Functional Theory of Communication. (arXiv:2302.03460v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#23545;&#35805;&#24335;&#30340;&#35299;&#37322;&#27169;&#22411;&#65292;&#24357;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20197;&#21019;&#24314;&#26356;&#26377;&#25928;&#21644;&#24191;&#27867;&#24212;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#24050;&#20174;&#19968;&#31181;&#20027;&#35201;&#30340;&#25216;&#26415;&#23398;&#31185;&#21457;&#23637;&#25104;&#19982;&#31038;&#20250;&#31185;&#23398;&#32039;&#23494;&#30456;&#20132;&#30340;&#39046;&#22495;&#12290;&#20154;&#31867;&#20559;&#22909;&#23545;&#27604;&#30340;&#35299;&#37322;&#65292;&#30830;&#20999;&#32780;&#35328;&#26159;&#21453;&#20107;&#23454;&#30340;&#35299;&#37322;&#65292;&#23545;&#20110;&#36825;&#31181;&#36716;&#21464;&#36215;&#21040;&#20102;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#21551;&#21457;&#21644;&#24341;&#39046;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#20854;&#20182;&#21516;&#26679;&#37325;&#35201;&#30340;&#35266;&#23519;&#21364;&#21463;&#21040;&#20102;&#24456;&#23569;&#30340;&#20851;&#27880;&#12290;&#20154;&#31867;&#35299;&#37322;&#32773;&#24076;&#26395;&#36890;&#36807;&#23545;&#35805;&#24335;&#30340;&#20132;&#20114;&#19982;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#32773;&#36827;&#34892;&#20132;&#27969;&#30340;&#24895;&#26395;&#22312;&#31038;&#21306;&#20013;&#22522;&#26412;&#34987;&#24573;&#35270;&#12290;&#36825;&#32473;&#36825;&#31181;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#21644;&#24191;&#27867;&#24212;&#29992;&#24102;&#26469;&#20102;&#24456;&#22810;&#25361;&#25112;&#65292;&#22240;&#20026;&#26681;&#25454;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#25552;&#20379;&#21333;&#19968;&#30340;&#20248;&#21270;&#35299;&#37322;&#21487;&#33021;&#20250;&#22833;&#36133;&#65292;&#24182;&#19988;&#19981;&#33021;&#28385;&#36275;&#20854;&#25509;&#25910;&#32773;&#30340;&#29420;&#29305;&#38656;&#27714;&#65292;&#37492;&#20110;&#20154;&#31867;&#30693;&#35782;&#21644;&#24847;&#22270;&#30340;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#21033;&#29992;&#23612;&#20811;&#25289;&#26031;&#183;&#40065;&#26364;&#21644;&#20854;&#20182;&#20132;&#27969;&#23398;&#32773;&#38416;&#36848;&#30340;&#35265;&#35299;&#65292;&#25552;&#20986;&#20102;&#21521;&#26356;&#23545;&#35805;&#24335;&#30340;&#35299;&#37322;&#27169;&#22411;&#30340;&#36716;&#21464;&#65292;&#20854;&#20013;&#35299;&#37322;&#32773;&#21644;&#34987;&#35299;&#37322;&#32773;&#20043;&#38388;&#30340;&#20449;&#24687;&#25345;&#32493;&#20132;&#27969;&#26159;&#26680;&#24515;&#12290;&#36890;&#36807;&#36825;&#31181;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#24314;&#31435;&#26356;&#26377;&#25928;&#21644;&#24191;&#27867;&#24212;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#24357;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decade explainable artificial intelligence has evolved from a predominantly technical discipline into a field that is deeply intertwined with social sciences. Insights such as human preference for contrastive -- more precisely, counterfactual -- explanations have played a major role in this transition, inspiring and guiding the research in computer science. Other observations, while equally important, have received much less attention. The desire of human explainees to communicate with artificial intelligence explainers through a dialogue-like interaction has been mostly neglected by the community. This poses many challenges for the effectiveness and widespread adoption of such technologies as delivering a single explanation optimised according to some predefined objectives may fail to engender understanding in its recipients and satisfy their unique needs given the diversity of human knowledge and intention. Using insights elaborated by Niklas Luhmann and, more recently,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35821;&#20041;&#20998;&#21106;&#30340;&#20449;&#24687;&#36335;&#24452;&#35268;&#21010;&#26694;&#26550;&#65292;&#20351;&#26080;&#20154;&#26426;&#33021;&#22815;&#33258;&#20027;&#33719;&#21462;&#20449;&#24687;&#20016;&#23500;&#30340;&#35757;&#32451;&#24433;&#20687;&#20197;&#20379;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2302.03347</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26080;&#20154;&#26426;&#21322;&#30417;&#30563;&#20998;&#21106;&#30340;&#20449;&#24687;&#36335;&#24452;&#35268;&#21010;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Informative Path Planning Framework for Active Learning in UAV-based Semantic Mapping. (arXiv:2302.03347v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35821;&#20041;&#20998;&#21106;&#30340;&#20449;&#24687;&#36335;&#24452;&#35268;&#21010;&#26694;&#26550;&#65292;&#20351;&#26080;&#20154;&#26426;&#33021;&#22815;&#33258;&#20027;&#33719;&#21462;&#20449;&#24687;&#20016;&#23500;&#30340;&#35757;&#32451;&#24433;&#20687;&#20197;&#20379;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#22312;&#33322;&#31354;&#27979;&#32472;&#21644;&#30417;&#27979;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#20351;&#24471;&#33258;&#21160;&#35821;&#20041;&#20998;&#21106;&#33021;&#22815;&#26356;&#22909;&#22320;&#35299;&#37322;&#22823;&#35268;&#27169;&#22797;&#26434;&#29615;&#22659;&#12290;&#24120;&#29992;&#30340;&#29992;&#20110;&#20998;&#21106;&#30340;&#26377;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#20381;&#36182;&#20110;&#22823;&#37327;&#20687;&#32032;&#32423;&#26631;&#27880;&#25968;&#25454;&#65292;&#32780;&#26631;&#27880;&#36153;&#29992;&#39640;&#26114;&#19988;&#36153;&#26102;&#12290;&#32780;&#33322;&#31354;&#29615;&#22659;&#30340;&#39046;&#22495;&#29305;&#23450;&#35270;&#35273;&#22806;&#35266;&#36890;&#24120;&#38459;&#30861;&#20102;&#20351;&#29992;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36890;&#29992;&#35268;&#21010;&#26694;&#26550;&#65292;&#20351;&#24471;&#26080;&#20154;&#26426;&#21487;&#20197;&#33258;&#20027;&#33719;&#21462;&#20449;&#24687;&#20016;&#23500;&#30340;&#35757;&#32451;&#24433;&#20687;&#20197;&#20379;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#12290;&#26412;&#25991;&#23558;&#22810;&#20010;&#37319;&#38598;&#20989;&#25968;&#32467;&#21512;&#24182;&#34701;&#21512;&#21040;&#27010;&#29575;&#22320;&#24418;&#22270;&#20013;&#12290;&#28982;&#21518;&#23558;&#22320;&#22270;&#20013;&#30340;&#21508;&#31181;&#20449;&#24687;&#34701;&#21512;&#21040;&#26080;&#20154;&#26426;&#30340;&#35268;&#21010;&#30446;&#26631;&#20013;&#12290;&#26080;&#20154;&#26426;&#20250;&#33258;&#36866;&#24212;&#22320;&#33719;&#21462;&#20449;&#24687;&#20016;&#23500;&#30340;&#33322;&#31354;&#24433;&#20687;&#65292;&#20197;&#36827;&#34892;&#20154;&#24037;&#26631;&#27880;&#29992;&#20110;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unmanned aerial vehicles (UAVs) are frequently used for aerial mapping and general monitoring tasks. Recent progress in deep learning enabled automated semantic segmentation of imagery to facilitate the interpretation of large-scale complex environments. Commonly used supervised deep learning for segmentation relies on large amounts of pixel-wise labelled data, which is tedious and costly to annotate. The domain-specific visual appearance of aerial environments often prevents the usage of models pre-trained on publicly available datasets. To address this, we propose a novel general planning framework for UAVs to autonomously acquire informative training images for model re-training. We leverage multiple acquisition functions and fuse them into probabilistic terrain maps. Our framework combines the mapped acquisition function information into the UAV's planning objectives. In this way, the UAV adaptively acquires informative aerial images to be manually labelled for model re-training. E
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;BabelCode&#26694;&#26550;&#21644;Translating Python Programming Puzzles&#65288;TP3&#65289;&#22522;&#20934;&#27979;&#35797;&#65292;&#25506;&#35752;&#20102;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;14&#31181;&#32534;&#31243;&#35821;&#35328;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#24179;&#34913;&#20998;&#24067;&#26377;&#21161;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2302.01973</link><description>&lt;p&gt;
&#27979;&#37327;&#32534;&#31243;&#35821;&#35328;&#20998;&#24067;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Measuring The Impact Of Programming Language Distribution. (arXiv:2302.01973v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01973
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;BabelCode&#26694;&#26550;&#21644;Translating Python Programming Puzzles&#65288;TP3&#65289;&#22522;&#20934;&#27979;&#35797;&#65292;&#25506;&#35752;&#20102;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;14&#31181;&#32534;&#31243;&#35821;&#35328;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#24179;&#34913;&#20998;&#24067;&#26377;&#21161;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#29992;&#20110;&#35780;&#20272;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#21482;&#38598;&#20013;&#22312;&#24456;&#23569;&#30340;&#19968;&#37096;&#20998;&#32534;&#31243;&#35821;&#35328;&#19978;&#65292;&#19981;&#21253;&#25324;&#35768;&#22810;&#27969;&#34892;&#30340;&#35821;&#35328;&#65292;&#20363;&#22914;Go&#25110;Rust&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BabelCode&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#25191;&#34892;&#30340;&#35780;&#20272;&#20219;&#20309;&#35821;&#35328;&#20013;&#30340;&#20219;&#20309;&#22522;&#20934;&#27979;&#35797;&#12290;BabelCode&#20351;&#24471;&#21487;&#20197;&#23545;&#27169;&#22411;&#30340;&#20869;&#23384;&#12289;&#36816;&#34892;&#26102;&#38388;&#21644;&#21333;&#20010;&#27979;&#35797;&#26696;&#20363;&#32467;&#26524;&#36827;&#34892;&#26032;&#30340;&#23450;&#24615;&#24615;&#33021;&#35843;&#26597;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;Translating Python Programming Puzzles&#65288;TP3&#65289;&#30340;&#26032;&#20195;&#30721;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26469;&#33258;Python Programming Puzzles&#65288;Schuster&#31561;&#20154;&#65292;2021&#65289;&#22522;&#20934;&#27979;&#35797;&#65292;&#28041;&#21450;&#23558;&#19987;&#23478;&#32423;Python&#20989;&#25968;&#32763;&#35793;&#25104;&#20219;&#20309;&#35821;&#35328;&#12290;&#36890;&#36807;&#23545;BabelCode&#21644;TP3&#22522;&#20934;&#27979;&#35797;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#24179;&#34913;14&#31181;&#35821;&#35328;&#30340;&#20998;&#24067;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#12290;&#22312;&#24179;&#34913;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#24179;&#22343;&#32780;&#35328;&#65292;&#30456;&#23545;&#20110;&#19981;&#24179;&#34913;&#20998;&#24067;&#30340;&#24773;&#20917;&#65292;&#25152;&#26377;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;$pass@k$&#32467;&#26524;&#24179;&#22343;&#25552;&#39640;&#20102;12.34%&#12290;
&lt;/p&gt;
&lt;p&gt;
Current benchmarks for evaluating neural code models focus on only a small subset of programming languages, excluding many popular languages such as Go or Rust. To ameliorate this issue, we present the BabelCode framework for execution-based evaluation of any benchmark in any language. BabelCode enables new investigations into the qualitative performance of models' memory, runtime, and individual test case results. Additionally, we present a new code translation dataset called Translating Python Programming Puzzles (TP3) from the Python Programming Puzzles (Schuster et al. 2021) benchmark that involves translating expert-level python functions to any language. With both BabelCode and the TP3 benchmark, we investigate if balancing the distributions of 14 languages in a training dataset improves a large language model's performance on low-resource languages. Training a model on a balanced corpus results in, on average, 12.34% higher $pass@k$ across all tasks and languages compared to the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ARCO&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20998;&#23618;&#32452;&#37319;&#26679;&#29702;&#35770;&#12290;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#20272;&#35745;&#30340;&#27010;&#24565;&#26469;&#26500;&#24314;ARCO&#65292;&#24182;&#34920;&#26126;&#26576;&#20123;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;</title><link>http://arxiv.org/abs/2302.01735</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65306;&#26041;&#24046;&#32553;&#20943;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Rethinking Semi-Supervised Medical Image Segmentation: A Variance-Reduction Perspective. (arXiv:2302.01735v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ARCO&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20998;&#23618;&#32452;&#37319;&#26679;&#29702;&#35770;&#12290;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#20272;&#35745;&#30340;&#27010;&#24565;&#26469;&#26500;&#24314;ARCO&#65292;&#24182;&#34920;&#26126;&#26576;&#20123;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes ARCO, a semi-supervised contrastive learning (CL) framework with stratified group sampling theory in medical image segmentation. The concept of variance-reduced estimation is used to build ARCO, and certain variance-reduction techniques are shown to be particularly beneficial in medical image segmentation.
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#23545;&#27604;&#23398;&#20064;&#26159;&#25552;&#39640;&#35270;&#35273;&#34920;&#31034;&#36136;&#37327;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#35821;&#20041;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#30340;&#26679;&#26412;&#23545;&#26469;&#23454;&#29616;&#12290;&#36825;&#26159;&#36890;&#36807;&#35266;&#23519;&#21040;&#65292;&#22312;&#27809;&#26377;&#35775;&#38382;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#26524;&#37319;&#26679;&#20855;&#26377;&#30495;&#27491;&#19981;&#21516;&#35299;&#21078;&#29305;&#24449;&#30340;&#36127;&#26679;&#26412;&#65292;&#21017;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#36825;&#20123;&#26679;&#26412;&#21487;&#33021;&#26469;&#33258;&#30456;&#20284;&#30340;&#35299;&#21078;&#29305;&#24449;&#65292;&#27169;&#22411;&#21487;&#33021;&#38590;&#20197;&#21306;&#20998;&#23569;&#25968;&#23614;&#31867;&#26679;&#26412;&#65292;&#20351;&#24471;&#23614;&#31867;&#26356;&#23481;&#26131;&#34987;&#38169;&#35823;&#20998;&#31867;&#65292;&#36825;&#36890;&#24120;&#23548;&#33268;&#27169;&#22411;&#23849;&#28291;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ARCO&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20998;&#23618;&#32452;&#37319;&#26679;&#29702;&#35770;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#20272;&#35745;&#30340;&#27010;&#24565;&#26469;&#26500;&#24314;ARCO&#65292;&#24182;&#34920;&#26126;&#26576;&#20123;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
For medical image segmentation, contrastive learning is the dominant practice to improve the quality of visual representations by contrasting semantically similar and dissimilar pairs of samples. This is enabled by the observation that without accessing ground truth label, negative examples with truly dissimilar anatomical features, if sampled, can significantly improve the performance. In reality, however, these samples may come from similar anatomical features and the models may struggle to distinguish the minority tail-class samples, making the tail classes more prone to misclassification, both of which typically lead to model collapse. In this paper, we propose ARCO, a semi-supervised contrastive learning (CL) framework with stratified group sampling theory in medical image segmentation. In particular, we first propose building ARCO through the concept of variance-reduced estimation, and show that certain variance-reduction techniques are particularly beneficial in medical image se
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#23398;&#32422;&#26463;&#30340;&#36816;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;&#26367;&#20195;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#30830;&#20445;&#39044;&#27979;&#30340;&#36712;&#36857;&#22312;&#21160;&#21147;&#23398;&#19978;&#26159;&#21487;&#34892;&#30340;&#65292;&#21516;&#26102;&#36890;&#36807;&#26500;&#24314;&#36866;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#39044;&#27979;&#21306;&#22495;&#65292;&#37327;&#21270;&#20102;&#19981;&#30830;&#23450;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#33258;&#20027;&#36187;&#36710;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#39044;&#27979;&#31934;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2302.01060</link><description>&lt;p&gt;
&#29289;&#29702;&#23398;&#32422;&#26463;&#19979;&#24102;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Physics Constrained Motion Prediction with Uncertainty Quantification. (arXiv:2302.01060v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01060
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#23398;&#32422;&#26463;&#30340;&#36816;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;&#26367;&#20195;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#30830;&#20445;&#39044;&#27979;&#30340;&#36712;&#36857;&#22312;&#21160;&#21147;&#23398;&#19978;&#26159;&#21487;&#34892;&#30340;&#65292;&#21516;&#26102;&#36890;&#36807;&#26500;&#24314;&#36866;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#39044;&#27979;&#21306;&#22495;&#65292;&#37327;&#21270;&#20102;&#19981;&#30830;&#23450;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#33258;&#20027;&#36187;&#36710;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#39044;&#27979;&#31934;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#21160;&#24577;&#29289;&#20307;&#30340;&#36816;&#21160;&#23545;&#20110;&#20445;&#35777;&#33258;&#20027;&#31995;&#32479;&#30340;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#20010;&#25361;&#25112;&#26159;&#36816;&#21160;&#39044;&#27979;&#31639;&#27861;&#24212;&#36981;&#23432;&#21160;&#21147;&#23398;&#32422;&#26463;&#65292;&#24182;&#23558;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20026;&#32622;&#20449;&#24230;&#30340;&#34913;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#23398;&#32422;&#26463;&#30340;&#36816;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;&#26367;&#20195;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#30830;&#20445;&#39044;&#27979;&#30340;&#36712;&#36857;&#22312;&#21160;&#21147;&#23398;&#19978;&#26159;&#21487;&#34892;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;&#38598;&#25104;&#36807;&#31243;&#65292;&#21253;&#25324;&#24847;&#22270;&#39044;&#27979;&#21644;&#36712;&#36857;&#39044;&#27979;&#65292;&#21516;&#26102;&#28385;&#36275;&#20102;&#21160;&#21147;&#23398;&#32422;&#26463;&#12290;&#36890;&#36807;&#20351;&#29992;&#27969;&#34892;&#30340;&#32479;&#35745;&#24037;&#20855;&#65292;&#26500;&#24314;&#20102;&#36866;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#39044;&#27979;&#21306;&#22495;&#65292;&#37327;&#21270;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#33258;&#20027;&#36187;&#36710;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#30456;&#27604;&#22522;&#32447;&#65292;&#29289;&#29702;&#23398;&#32422;&#26463;&#19979;&#24102;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#36816;&#21160;&#39044;&#27979;&#23454;&#29616;&#20102;41%&#26356;&#22909;&#30340;ADE&#65292;56%&#26356;&#22909;&#30340;FDE&#21644;19%&#26356;&#22909;&#30340;IoU&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting the motion of dynamic agents is a critical task for guaranteeing the safety of autonomous systems. A particular challenge is that motion prediction algorithms should obey dynamics constraints and quantify prediction uncertainty as a measure of confidence. We present a physics-constrained approach for motion prediction which uses a surrogate dynamical model to ensure that predicted trajectories are dynamically feasible. We propose a two-step integration consisting of intent and trajectory prediction subject to dynamics constraints. We also construct prediction regions that quantify uncertainty and are tailored for autonomous driving by using conformal prediction, a popular statistical tool. Physics Constrained Motion Prediction achieves a 41% better ADE, 56% better FDE, and 19% better IoU over a baseline in experiments using an autonomous racing dataset.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19978;&#19979;&#25991;&#20462;&#21098;&#20803;&#23398;&#20064;&#23454;&#29616;&#22823;&#35268;&#27169;&#31070;&#32463;&#22330;&#35757;&#32451;&#30340;&#20248;&#21270;&#65292; &#26174;&#33879;&#33410;&#30465;&#20869;&#23384;&#65292;&#24182;&#33021;&#22312;&#30701;&#26102;&#38388;&#20869;&#23398;&#20064;&#39640;&#36136;&#37327;&#31070;&#32463;&#22330;&#12290;</title><link>http://arxiv.org/abs/2302.00617</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#20462;&#21098;&#20803;&#23398;&#20064;&#23398;&#20064;&#22823;&#35268;&#27169;&#31070;&#32463;&#22330;
&lt;/p&gt;
&lt;p&gt;
Learning Large-scale Neural Fields via Context Pruned Meta-Learning. (arXiv:2302.00617v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00617
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#20462;&#21098;&#20803;&#23398;&#20064;&#23454;&#29616;&#22823;&#35268;&#27169;&#31070;&#32463;&#22330;&#35757;&#32451;&#30340;&#20248;&#21270;&#65292; &#26174;&#33879;&#33410;&#30465;&#20869;&#23384;&#65292;&#24182;&#33021;&#22312;&#30701;&#26102;&#38388;&#20869;&#23398;&#20064;&#39640;&#36136;&#37327;&#31070;&#32463;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#22312;&#32447;&#19978;&#19979;&#25991;&#28857;&#36873;&#25321;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#31070;&#32463;&#22330;&#35757;&#32451;&#30340;&#39640;&#25928;&#20248;&#21270;&#20803;&#23398;&#20064;&#25216;&#26415;&#65292;&#20174;&#32780;&#23454;&#29616;&#26174;&#33879;&#30340;&#20869;&#23384;&#33410;&#30465;&#12290;&#36890;&#36807;&#23558;&#27599;&#20010;&#23398;&#20064;&#27493;&#39588;&#38598;&#20013;&#22312;&#20855;&#26377;&#26368;&#39640;&#26399;&#26395;&#31435;&#21363;&#27169;&#22411;&#36136;&#37327;&#25913;&#36827;&#30340;&#25968;&#25454;&#23376;&#38598;&#19978;&#65292;&#23454;&#29616;&#20840;&#23616;&#32467;&#26500;&#30340;&#20960;&#20046;&#21363;&#26102;&#24314;&#27169;&#21644;&#39640;&#39057;&#32454;&#33410;&#30340;&#21518;&#32493;&#32454;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#24341;&#23548;&#26657;&#27491;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#20803;&#23398;&#20064;&#21021;&#22987;&#21270;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#20943;&#23569;&#19978;&#19979;&#25991;&#38598;&#26102;&#24341;&#20837;&#30340;&#20219;&#20309;&#35823;&#24046;&#30340;&#26368;&#23567;&#21270;&#65292;&#24182;&#21516;&#26102;&#32531;&#35299;&#20102;&#22522;&#20110;&#20248;&#21270;&#30340;&#20803;&#23398;&#20064;&#25152;&#24102;&#26469;&#30340;&#30701;&#35270;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#20803;&#27979;&#35797;&#26102;&#36827;&#34892;&#26799;&#24230;&#37325;&#26032;&#32553;&#25918;&#65292;&#20174;&#32780;&#22312;&#26174;&#33879;&#32553;&#30701;&#20248;&#21270;&#36807;&#31243;&#30340;&#21516;&#26102;&#23398;&#20064;&#26497;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#22330;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#27169;&#22411;&#26080;&#20851;&#65292;&#30452;&#35266;&#26131;&#25026;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#37325;&#26500;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an efficient optimization-based meta-learning technique for large-scale neural field training by realizing significant memory savings through automated online context point selection. This is achieved by focusing each learning step on the subset of data with the highest expected immediate improvement in model quality, resulting in the almost instantaneous modeling of global structure and subsequent refinement of high-frequency details. We further improve the quality of our meta-learned initialization by introducing a bootstrap correction resulting in the minimization of any error introduced by reduced context sets while simultaneously mitigating the well-known myopia of optimization-based meta-learning. Finally, we show how gradient re-scaling at meta-test time allows the learning of extremely high-quality neural fields in significantly shortened optimization procedures. Our framework is model-agnostic, intuitive, straightforward to implement, and shows significant reconst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#36817;&#20284;&#31639;&#27861;&#30340;&#20998;&#23618;&#32858;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#28385;&#36275;&#19979;&#30028;&#30340;&#25351;&#25968;&#26102;&#38388;&#22797;&#26434;&#24230;&#31639;&#27861;&#21644;&#19968;&#20010;&#31169;&#26377;&#30340;$1+o(1)$&#36924;&#36817;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#23545;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#39564;&#35777;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.00037</link><description>&lt;p&gt;
&#20855;&#26377;&#21487;&#35777;&#26126;&#36817;&#20284;&#20445;&#35777;&#30340;&#24046;&#20998;&#38544;&#31169;&#20998;&#23618;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Differentially-Private Hierarchical Clustering with Provable Approximation Guarantees. (arXiv:2302.00037v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#36817;&#20284;&#31639;&#27861;&#30340;&#20998;&#23618;&#32858;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#28385;&#36275;&#19979;&#30028;&#30340;&#25351;&#25968;&#26102;&#38388;&#22797;&#26434;&#24230;&#31639;&#27861;&#21644;&#19968;&#20010;&#31169;&#26377;&#30340;$1+o(1)$&#36924;&#36817;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#23545;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#39564;&#35777;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#32858;&#31867;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20855;&#26377;&#24736;&#20037;&#30340;&#21382;&#21490;&#21644;&#20247;&#22810;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#22312;Dasgupta(2016)&#24341;&#20837;&#30340;&#20005;&#26684;&#26694;&#26550;&#19979;&#65292;&#39318;&#27425;&#30740;&#31350;&#20102;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#36817;&#20284;&#31639;&#27861;&#30340;&#20998;&#23618;&#32858;&#31867;&#12290;&#25105;&#20204;&#23545;&#35813;&#38382;&#39064;&#36827;&#34892;&#20102;&#24378;&#26377;&#21147;&#30340;&#19979;&#30028;&#20998;&#26512;&#65306;&#20219;&#20309;$\epsilon$-&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#23545;&#20110;&#36755;&#20837;&#38598;&#21512;$V$&#37117;&#24517;&#39035;&#26174;&#31034;$O(|V|^2/\epsilon)$&#30340;&#21152;&#24615;&#35823;&#24046;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#36817;&#20284;&#31639;&#27861;&#65292;&#20854;&#21152;&#24615;&#35823;&#24046;&#20026;$O(|V|^{2.5}/\epsilon)$&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#28385;&#36275;&#19979;&#30028;&#30340;&#25351;&#25968;&#26102;&#38388;&#22797;&#26434;&#24230;&#31639;&#27861;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#19979;&#30028;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#38543;&#26426;&#22359;&#27169;&#22411;&#19978;&#65292;&#36825;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#22270;&#24418;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#22359;&#30340;&#20998;&#31163;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31169;&#26377;&#30340;$1+o(1)$&#36924;&#36817;&#31639;&#27861;&#65292;&#21516;&#26102;&#36824;&#24674;&#22797;&#20102;&#22359;&#30340;&#30830;&#20999;&#20540;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#39564;&#35777;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Clustering is a popular unsupervised machine learning method with decades of history and numerous applications. We initiate the study of differentially private approximation algorithms for hierarchical clustering under the rigorous framework introduced by (Dasgupta, 2016). We show strong lower bounds for the problem: that any $\epsilon$-DP algorithm must exhibit $O(|V|^2/ \epsilon)$-additive error for an input dataset $V$. Then, we exhibit a polynomial-time approximation algorithm with $O(|V|^{2.5}/ \epsilon)$-additive error, and an exponential-time algorithm that meets the lower bound. To overcome the lower bound, we focus on the stochastic block model, a popular model of graphs, and, with a separation assumption on the blocks, propose a private $1+o(1)$ approximation algorithm which also recovers the blocks exactly. Finally, we perform an empirical study of our algorithms and validate their performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Shortcut Fine-Tuning&#65288;SFT&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;&#30452;&#25509;&#26368;&#23567;&#21270;&#31215;&#20998;&#27010;&#29575;&#24230;&#37327;&#65288;IPM&#65289;&#26469;&#23545;DDPM&#37319;&#26679;&#22120;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;DDPM&#37319;&#26679;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2301.13362</link><description>&lt;p&gt;
&#20351;&#29992;Shortcut Fine-Tuning&#20248;&#21270;DDPM&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Optimizing DDPM Sampling with Shortcut Fine-Tuning. (arXiv:2301.13362v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Shortcut Fine-Tuning&#65288;SFT&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;&#30452;&#25509;&#26368;&#23567;&#21270;&#31215;&#20998;&#27010;&#29575;&#24230;&#37327;&#65288;IPM&#65289;&#26469;&#23545;DDPM&#37319;&#26679;&#22120;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;DDPM&#37319;&#26679;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Shortcut Fine-Tuning&#65288;SFT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39044;&#35757;&#32451;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#30340;&#24555;&#36895;&#37319;&#26679;&#25361;&#25112;&#12290;SFT&#25552;&#20513;&#36890;&#36807;&#30452;&#25509;&#26368;&#23567;&#21270;&#31215;&#20998;&#27010;&#29575;&#24230;&#37327;&#65288;IPM&#65289;&#26469;&#23545;DDPM&#37319;&#26679;&#22120;&#36827;&#34892;&#24494;&#35843;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#21521;&#21518;&#25193;&#25955;&#36807;&#31243;&#12290;&#36825;&#20351;&#37319;&#26679;&#22120;&#33021;&#22815;&#21457;&#29616;&#19968;&#26465;&#26367;&#20195;&#30340;&#26356;&#39640;&#25928;&#30340;&#37319;&#26679;&#25463;&#24452;&#65292;&#20559;&#31163;&#21521;&#21518;&#25193;&#25955;&#36807;&#31243;&#12290;&#36890;&#36807;&#25511;&#21046;&#35282;&#24230;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;SFT-PG&#65306;&#20351;&#29992;Policy Gradient&#36827;&#34892;&#30340;Shortcut Fine-Tuning&#65292;&#24182;&#35777;&#26126;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;&#25193;&#25955;&#27169;&#22411;&#30456;&#23545;&#20110;IPM&#30340;&#26799;&#24230;&#19979;&#38477;&#31561;&#20215;&#20110;&#25191;&#34892;Policy Gradient&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#23581;&#35797;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#26469;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#12290;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#24494;&#35843;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#29616;&#26377;&#30340;&#24555;&#36895;DDPM&#37319;&#26679;&#22120;&#65292;&#20174;&#32780;&#23548;&#33268;&#26174;&#30528;&#30340;&#21152;&#36895;&#21644;&#36136;&#37327;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose Shortcut Fine-Tuning (SFT), a new approach for addressing the challenge of fast sampling of pretrained Denoising Diffusion Probabilistic Models (DDPMs). SFT advocates for the fine-tuning of DDPM samplers through the direct minimization of Integral Probability Metrics (IPM), instead of learning the backward diffusion process. This enables samplers to discover an alternative and more efficient sampling shortcut, deviating from the backward diffusion process. Inspired by a control perspective, we propose a new algorithm SFT-PG: Shortcut Fine-Tuning with Policy Gradient, and prove that under certain assumptions, gradient descent of diffusion models with respect to IPM is equivalent to performing policy gradient. To our best knowledge, this is the first attempt to utilize reinforcement learning (RL) methods to train diffusion models. Through empirical evaluation, we demonstrate that our fine-tuning method can further enhance existing fast DDPM samplers, resulting i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#30740;&#31350;&#35270;&#35282;&#65292;&#22238;&#31572;&#20102;&#24403;&#22270;&#33410;&#28857;&#25968;&#37327;&#21464;&#24471;&#38750;&#24120;&#22823;&#26102;GNN&#30340;&#34892;&#20026;&#22914;&#20309;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35777;&#26126;&#19981;&#26029;&#22686;&#22823;&#30340;&#22270;&#26144;&#23556;&#21040;GNN&#20998;&#31867;&#22120;&#30340;&#29305;&#23450;&#36755;&#20986;&#30340;&#27010;&#29575;&#36235;&#20110;&#38646;&#25110;&#19968;&#65292;&#24314;&#31435;&#20102;&#36825;&#20123;GNN&#30340;&#38646;&#19968;&#23450;&#24459;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#29702;&#35770;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2301.13060</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38646;&#19968;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Zero-One Laws of Graph Neural Networks. (arXiv:2301.13060v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#30740;&#31350;&#35270;&#35282;&#65292;&#22238;&#31572;&#20102;&#24403;&#22270;&#33410;&#28857;&#25968;&#37327;&#21464;&#24471;&#38750;&#24120;&#22823;&#26102;GNN&#30340;&#34892;&#20026;&#22914;&#20309;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35777;&#26126;&#19981;&#26029;&#22686;&#22823;&#30340;&#22270;&#26144;&#23556;&#21040;GNN&#20998;&#31867;&#22120;&#30340;&#29305;&#23450;&#36755;&#20986;&#30340;&#27010;&#29575;&#36235;&#20110;&#38646;&#25110;&#19968;&#65292;&#24314;&#31435;&#20102;&#36825;&#20123;GNN&#30340;&#38646;&#19968;&#23450;&#24459;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#29702;&#35770;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#26159;&#29992;&#20110;&#23545;&#22270;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#26631;&#20934;&#20307;&#31995;&#32467;&#26500;&#12290;&#36825;&#23548;&#33268;&#20102;&#22823;&#37327;&#30340;&#24037;&#20316;&#20998;&#26512;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#20182;&#20204;&#30340;&#34920;&#31034;&#21644;&#22806;&#25512;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#35270;&#35282;&#65292;&#22238;&#31572;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#24403;&#22270;&#33410;&#28857;&#30340;&#25968;&#37327;&#21464;&#24471;&#38750;&#24120;&#22823;&#26102;&#65292;GNNs&#30340;&#34892;&#20026;&#22914;&#20309;&#65311;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#24403;&#25105;&#20204;&#20174;Erd&#337;s-R&#233;nyi&#27169;&#22411;&#20013;&#32472;&#21046;&#19981;&#26029;&#22686;&#22823;&#30340;&#22270;&#26102;&#65292;&#36825;&#20123;&#22270;&#26144;&#23556;&#21040;GNN&#20998;&#31867;&#22120;&#30340;&#29305;&#23450;&#36755;&#20986;&#30340;&#27010;&#29575;&#36235;&#20110;&#38646;&#25110;&#19968;&#12290;&#36825;&#20010;&#31867;&#21253;&#25324;&#27969;&#34892;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#12290;&#36825;&#20010;&#32467;&#26524;&#24314;&#31435;&#20102;&#36825;&#20123;GNN&#30340;&#38646;&#19968;&#23450;&#24459;&#65292;&#24182;&#19988;&#31867;&#27604;&#20110;&#20854;&#20182;&#25910;&#25947;&#23450;&#24459;&#65292;&#24102;&#26469;&#20102;&#23427;&#20204;&#22312;&#29702;&#35770;&#19978;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#35266;&#23519;&#21040;&#29702;&#35770;&#19982;&#23454;&#36341;&#30456;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are de facto standard deep learning architectures for machine learning on graphs. This has led to a large body of work analyzing the capabilities and limitations of these models, particularly pertaining to their representation and extrapolation capacity. We offer a novel theoretical perspective on the representation and extrapolation capacity of GNNs, by answering the question: how do GNNs behave as the number of graph nodes become very large? Under mild assumptions, we show that when we draw graphs of increasing size from the Erd\H{o}s-R\'enyi model, the probability that such graphs are mapped to a particular output by a class of GNN classifiers tends to either zero or to one. This class includes the popular graph convolutional network architecture. The result establishes 'zero-one laws' for these GNNs, and analogously to other convergence laws, entails theoretical limitations on their capacity. We empirically verify our results, observing that the theoret
&lt;/p&gt;</description></item><item><title>ForkMerge&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23427;&#24110;&#21161;&#32531;&#35299;&#20102;&#36741;&#21161;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;ATL&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.12618</link><description>&lt;p&gt;
ForkMerge: &#32531;&#35299;&#36741;&#21161;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#36127;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
ForkMerge: Mitigating Negative Transfer in Auxiliary-Task Learning. (arXiv:2301.12618v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12618
&lt;/p&gt;
&lt;p&gt;
ForkMerge&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23427;&#24110;&#21161;&#32531;&#35299;&#20102;&#36741;&#21161;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;ATL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36741;&#21161;&#20219;&#21153;&#23398;&#20064;&#65288;ATL&#65289;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#19982;&#30446;&#26631;&#20219;&#21153;&#30456;&#20851;&#30340;&#30693;&#35782;&#26469;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26377;&#26102;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#20250;&#23548;&#33268;&#27604;&#20165;&#23398;&#20064;&#30446;&#26631;&#20219;&#21153;&#30340;&#20934;&#30830;&#29575;&#26356;&#20302;&#65292;&#36825;&#34987;&#31216;&#20026;&#36127;&#36801;&#31227;&#12290;&#36825;&#20010;&#38382;&#39064;&#36890;&#24120;&#24402;&#22240;&#20110;&#20219;&#21153;&#20043;&#38388;&#30340;&#26799;&#24230;&#20914;&#31361;&#65292;&#24182;&#19988;&#22312;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#24120;&#24120;&#36890;&#36807;&#21327;&#35843;&#20219;&#21153;&#26799;&#24230;&#26469;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#30053;&#20102;&#36741;&#21161;&#30446;&#26631;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36127;&#36801;&#31227;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#25105;&#20204;&#20174;&#20248;&#21270;&#21644;&#27867;&#21270;&#35282;&#24230;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ForkMerge&#65292;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23427;&#20250;&#23450;&#26399;&#23558;&#27169;&#22411;&#20998;&#20026;&#22810;&#20010;&#20998;&#25903;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#30446;&#26631;&#39564;&#35777;&#38169;&#35823;&#33258;&#21160;&#25628;&#32034;&#19981;&#21516;&#30340;&#20219;&#21153;&#26435;&#37325;&#65292;&#24182;&#21160;&#24577;&#22320;&#21512;&#24182;&#25152;&#26377;&#20998;&#25903;&#26469;&#36807;&#28388;&#26377;&#23475;&#30340;&#20219;&#21153;&#21442;&#25968;&#26356;&#26032;&#12290;&#22312;&#19968;&#31995;&#21015;&#22522;&#20934;&#20219;&#21153;&#20013;&#65292;ForkMerge&#20248;&#20110;&#29616;&#26377;&#30340;ATL&#26041;&#27861;&#65292;&#24182;&#20943;&#36731;&#20102;&#36127;&#36801;&#31227;&#65292;&#35777;&#26126;&#20854;&#22312;&#25552;&#39640;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auxiliary-Task Learning (ATL) aims to improve the performance of the target task by leveraging the knowledge obtained from related tasks. Occasionally, learning multiple tasks simultaneously results in lower accuracy than learning only the target task, which is known as negative transfer. This problem is often attributed to the gradient conflicts among tasks, and is frequently tackled by coordinating the task gradients in previous works. However, these optimization-based methods largely overlook the auxiliary-target generalization capability. To better understand the root cause of negative transfer, we experimentally investigate it from both optimization and generalization perspectives. Based on our findings, we introduce ForkMerge, a novel approach that periodically forks the model into multiple branches, automatically searches the varying task weights by minimizing target validation errors, and dynamically merges all branches to filter out detrimental task-parameter updates. On a ser
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#21028;&#21035;&#29109;&#32858;&#31867;&#30340;&#30456;&#20851;&#29702;&#35770;&#21450;&#20854;&#19982;K-means&#21644;SVM&#30340;&#21306;&#21035;&#21644;&#30456;&#20284;&#20043;&#22788;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#36827;&#28145;&#24230;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11405</link><description>&lt;p&gt;
&#21028;&#21035;&#29109;&#32858;&#31867;&#21450;&#20854;&#19982;K-means&#21644;SVM&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Discriminative Entropy Clustering and its Relation to K-means and SVM. (arXiv:2301.11405v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11405
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#21028;&#21035;&#29109;&#32858;&#31867;&#30340;&#30456;&#20851;&#29702;&#35770;&#21450;&#20854;&#19982;K-means&#21644;SVM&#30340;&#21306;&#21035;&#21644;&#30456;&#20284;&#20043;&#22788;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#36827;&#28145;&#24230;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21028;&#21035;&#27169;&#22411;&#20013;&#65292;&#26368;&#22823;&#21270;&#27169;&#22411;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#24418;&#24335;&#19978;&#19982; softmax &#39044;&#27979;&#30340;&#8220;&#20915;&#31574;&#24615;&#8221;&#21644;&#8220;&#20844;&#24179;&#24615;&#8221;&#26377;&#20851;&#65292;&#20174;&#32780;&#28608;&#21457;&#20102;&#22522;&#20110;&#29109;&#30340;&#26080;&#30417;&#30563;&#25439;&#22833;&#20989;&#25968;&#30340;&#20351;&#29992;&#12290; &#26368;&#36817;&#65292;&#22522;&#20110;&#36825;&#26679;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#33258;&#25105;&#26631;&#35760;&#26041;&#27861;&#20195;&#34920;&#20102;&#28145;&#24230;&#32858;&#31867;&#30340;&#26368;&#26032;&#30740;&#31350;&#26041;&#21521;&#12290; &#39318;&#20808;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#29109;&#32858;&#31867;&#26041;&#27861;&#30340;&#35768;&#22810;&#36890;&#29992;&#23646;&#24615;&#65292;&#21253;&#25324;&#23427;&#20204;&#19982; K-means &#21644;&#26080;&#30417;&#30563; SVM &#25216;&#26415;&#30340;&#20851;&#31995;&#12290; &#25105;&#20204;&#35777;&#26126;&#19982; K-&#22343;&#20540;&#26377;&#30528;&#26681;&#26412;&#30340;&#21306;&#21035;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#34920;&#26126;&#20102;&#19982;&#22522;&#20110; SVM &#30340;&#32858;&#31867;&#30340;&#30456;&#20284;&#24615;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#26174;&#24335;&#30340;&#36793;&#38469;&#26368;&#22823;&#21270;&#19982;&#29109;&#32858;&#31867;&#32852;&#31995;&#36215;&#26469;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20132;&#21449;&#29109;&#30340;&#24120;&#35265;&#24418;&#24335;&#23545;&#20110;&#20266;&#26631;&#31614;&#38169;&#35823;&#19981;&#31283;&#20581;&#12290;&#25105;&#20204;&#30340;&#26032;&#25439;&#22833;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#23548;&#33268;&#20102;&#19968;&#31181;&#26032;&#30340; EM &#31639;&#27861;&#65292;&#22312;&#35768;&#22810;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#25913;&#36827;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximization of mutual information between the model's input and output is formally related to "decisiveness" and "fairness" of the softmax predictions, motivating such unsupervised entropy-based losses for discriminative models. Recent self-labeling methods based on such losses represent the state of the art in deep clustering. First, we discuss a number of general properties of such entropy clustering methods, including their relation to K-means and unsupervised SVM-based techniques. Disproving some earlier published claims, we point out fundamental differences with K-means. On the other hand, we show similarity with SVM-based clustering allowing us to link explicit margin maximization to entropy clustering. Finally, we observe that the common form of cross-entropy is not robust to pseudo-label errors. Our new loss addresses the problem and leads to a new EM algorithm improving the state of the art on many standard benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#35328;&#35299;&#37322;&#30340;&#21435;&#20559;&#35265;(B2T)&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#22270;&#20687;&#26631;&#39064;&#20013;&#30340;&#20851;&#38190;&#35789;&#65292;&#27604;&#36739;&#20851;&#38190;&#35789;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#35782;&#21035;&#21644;&#20943;&#32531;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#20559;&#35265;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2301.11104</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#35299;&#37322;&#30340;&#21435;&#20559;&#35265;: &#36890;&#36807;&#35821;&#35328;&#35299;&#37322;&#28040;&#38500;&#26410;&#30693;&#30340;&#35270;&#35273;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Bias-to-Text: Debiasing Unknown Visual Biases through Language Interpretation. (arXiv:2301.11104v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#35328;&#35299;&#37322;&#30340;&#21435;&#20559;&#35265;(B2T)&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#22270;&#20687;&#26631;&#39064;&#20013;&#30340;&#20851;&#38190;&#35789;&#65292;&#27604;&#36739;&#20851;&#38190;&#35789;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#35782;&#21035;&#21644;&#20943;&#32531;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#20559;&#35265;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#22312;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26102;&#26500;&#25104;&#37325;&#35201;&#38382;&#39064;&#65292;&#20294;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#35786;&#26029;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21435;&#20559;&#35265;(B2T)&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#35821;&#35328;&#35299;&#37322;&#26469;&#35782;&#21035;&#21644;&#32531;&#35299;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#20363;&#22914;&#22270;&#35937;&#20998;&#31867;&#22120;&#21644;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#23545;&#35270;&#35273;&#20559;&#24046;&#30340;&#35821;&#35328;&#25551;&#36848;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#24418;&#24335;&#65292;&#20351;&#24471;&#33021;&#22815;&#21457;&#29616;&#26032;&#30340;&#20559;&#35265;&#24182;&#26377;&#25928;&#22320;&#23545;&#27169;&#22411;&#36827;&#34892;&#21435;&#20559;&#35265;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#34987;&#35823;&#39044;&#27979;&#25110;&#29983;&#25104;&#30340;&#22270;&#20687;&#26631;&#39064;&#20013;&#30340;&#24120;&#35265;&#20851;&#38190;&#35789;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#36890;&#36807;&#27604;&#36739;&#20559;&#35265;&#20851;&#38190;&#35789;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#36991;&#20813;&#26631;&#39064;&#20013;&#30340;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;B2T&#26694;&#26550;&#20013;&#30340;&#20559;&#35265;&#20851;&#38190;&#35789;&#23545;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#21435;&#20559;&#35265;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#21508;&#31181;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biases in models pose a critical issue when deploying machine learning systems, but diagnosing them in an explainable manner can be challenging. To address this, we introduce the bias-to-text (B2T) framework, which uses language interpretation to identify and mitigate biases in vision models, such as image classifiers and text-to-image generative models. Our language descriptions of visual biases provide explainable forms that enable the discovery of novel biases and effective model debiasing. To achieve this, we analyze common keywords in the captions of mispredicted or generated images. Here, we propose novel score functions to avoid biases in captions by comparing the similarities between bias keywords and those images. Additionally, we present strategies to debias zero-shot classifiers and text-to-image diffusion models using the bias keywords from the B2T framework. We demonstrate the effectiveness of our framework on various image classification and generation tasks. For classifi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#24322;&#36136;&#22270;&#20013;&#30340;&#26377;&#31526;&#21495;&#20256;&#25773;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31867;&#21035;&#22270;&#30340;&#26032;&#31574;&#30053;&#65292;&#24182;&#20811;&#26381;&#20102;&#20854;&#24102;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2301.08918</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#26377;&#31526;&#21495;&#20256;&#25773;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Revisiting Signed Propagation for Graph Neural Networks. (arXiv:2301.08918v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08918
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#24322;&#36136;&#22270;&#20013;&#30340;&#26377;&#31526;&#21495;&#20256;&#25773;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31867;&#21035;&#22270;&#30340;&#26032;&#31574;&#30053;&#65292;&#24182;&#20811;&#26381;&#20102;&#20854;&#24102;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#20256;&#36882;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#21516;&#36136;&#22270;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22312;&#24322;&#36136;&#22270;&#19978;&#23427;&#20204;&#30340;&#24615;&#33021;&#21364;&#24456;&#24046;&#65292;&#35768;&#22810;&#30740;&#31350;&#32773;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#25552;&#20986;&#20102;&#22823;&#37327;&#26041;&#26696;&#12290;&#29305;&#21035;&#22320;&#65292;&#32763;&#36716;&#36793;&#30340;&#31526;&#21495;&#26159;&#22522;&#20110;&#22362;&#23454;&#29702;&#35770;&#22522;&#30784;&#30340;&#24182;&#19988;&#21487;&#20197;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#20998;&#26512;&#20551;&#23450;&#20102;&#20108;&#20803;&#20998;&#31867;&#22330;&#26223;&#65292;&#22240;&#27492;&#21463;&#21040;&#24212;&#29992;&#33539;&#22260;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#23558;&#20197;&#21069;&#30340;&#29702;&#35299;&#25193;&#23637;&#21040;&#22810;&#31867;&#21035;&#24773;&#20917;&#65292;&#24182;&#25351;&#20986;&#20004;&#20010;&#32570;&#28857;&#65306;&#65288;1&#65289;&#22810;&#36339;&#37051;&#23621;&#30340;&#31526;&#21495;&#21462;&#20915;&#20110;&#28040;&#24687;&#20256;&#36882;&#36335;&#24452;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#19968;&#33268;&#24615;&#65307;&#65288;2&#65289;&#36825;&#20063;&#22686;&#21152;&#20102;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65288;&#20363;&#22914;&#65292;&#20914;&#31361;&#35777;&#25454;&#65289;&#65292;&#21487;&#33021;&#24433;&#21709;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#12290;&#22312;&#29702;&#35770;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#22810;&#31867;&#21035;&#30340;&#22270;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#32467;&#21512;&#20102;&#21407;&#26377;&#26041;&#26696;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#20811;&#26381;&#20102;&#20854;&#32570;&#28857;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Message-passing Graph Neural Networks (GNNs), which collect information from adjacent nodes, achieve satisfying results on homophilic graphs. However, their performances are dismal in heterophilous graphs, and many researchers have proposed a plethora of schemes to solve this problem. Especially, flipping the sign of edges is rooted in a strong theoretical foundation, and attains significant performance enhancements. Nonetheless, previous analyses assume a binary class scenario and they may suffer from confined applicability. This paper extends the prior understandings to multi-class scenarios and points out two drawbacks: (1) the sign of multi-hop neighbors depends on the message propagation paths and may incur inconsistency, (2) it also increases the prediction uncertainty (e.g., conflict evidence) which can impede the stability of the algorithm. Based on the theoretical understanding, we introduce a novel strategy that is applicable to multi-class graphs. The proposed scheme combine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#28608;&#21169;&#23454;&#39564;&#65292;&#24471;&#20986;GNN&#27169;&#22411;&#36880;&#28176;&#23398;&#20064;&#26222;&#36890;&#39046;&#22495;&#20013;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30340;&#23454;&#35777;&#35266;&#23519;&#32467;&#26524;&#12290;&#36890;&#36807;&#23558;&#19987;&#19994;&#36923;&#36753;&#24341;&#20837;&#21040;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#21487;&#20197;&#25552;&#39640;GNN&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.08496</link><description>&lt;p&gt;
&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#24341;&#20837;&#19987;&#19994;&#36923;&#36753;&#21040;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Introducing Expertise Logic into Graph Representation Learning from A Causal Perspective. (arXiv:2301.08496v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#28608;&#21169;&#23454;&#39564;&#65292;&#24471;&#20986;GNN&#27169;&#22411;&#36880;&#28176;&#23398;&#20064;&#26222;&#36890;&#39046;&#22495;&#20013;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30340;&#23454;&#35777;&#35266;&#23519;&#32467;&#26524;&#12290;&#36890;&#36807;&#23558;&#19987;&#19994;&#36923;&#36753;&#24341;&#20837;&#21040;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#21487;&#20197;&#25552;&#39640;GNN&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#25968;&#25454;&#30001;&#20110;&#27880;&#20837;&#20102;&#20154;&#31867;&#20808;&#39564;&#30693;&#35782;&#65292;&#22240;&#27492;&#35821;&#20041;&#23494;&#38598;&#65292;&#27169;&#22411;&#21487;&#20197;&#20174;&#36825;&#20123;&#25968;&#25454;&#20013;&#26377;&#25928;&#22320;&#23398;&#20064;&#35821;&#20041;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#37325;&#26032;&#23457;&#35270;GNN&#23398;&#20064;&#33539;&#24335;&#65292;&#25105;&#20204;&#21457;&#29616;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#19982;GNN&#27169;&#25311;&#30340;&#30693;&#35782;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#28982;&#20196;&#20154;&#22256;&#24785;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28608;&#21169;&#23454;&#39564;&#65292;&#24182;&#24471;&#20986;&#23454;&#35777;&#35266;&#23519;&#32467;&#26524;&#31216;GNN&#36880;&#28176;&#23398;&#20064;&#21040;&#20102;&#26222;&#36890;&#39046;&#22495;&#20013;&#30340;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#35266;&#23519;&#23558;&#19987;&#19994;&#36923;&#36753;&#24341;&#20837;&#21040;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#21518;&#26524;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#24341;&#23548;GNN&#27169;&#22411;&#23398;&#20064;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#32435;&#20837;GNN&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30830;&#20445;GNN&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#33719;&#24471;&#19987;&#19994;&#30693;&#35782;&#65292;&#21516;&#26102;&#21487;&#20197;&#23558;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Benefiting from the injection of human prior knowledge, graphs, as derived discrete data, are semantically dense so that models can efficiently learn the semantic information from such data. Accordingly, graph neural networks (GNNs) indeed achieve impressive success in various fields. Revisiting the GNN learning paradigms, we discover that the relationship between human expertise and the knowledge modeled by GNNs still confuses researchers. To this end, we introduce motivating experiments and derive an empirical observation that the GNNs gradually learn human expertise in general domains. By further observing the ramifications of introducing expertise logic into graph representation learning, we conclude that leading the GNNs to learn human expertise can improve the model performance. Hence, we propose a novel graph representation learning method to incorporate human expert knowledge into GNN models. The proposed method ensures that the GNN model can not only acquire the expertise held
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;&#22270;&#23884;&#20837;&#23398;&#20064;&#65292;&#23427;&#23558;&#19968;&#32452;&#22270;&#20316;&#20026;&#36755;&#20837;&#65292;&#33258;&#21160;&#25552;&#21462;&#29305;&#24449;&#24182;&#23558;&#22270;&#32534;&#30721;&#20026;&#20302;&#32500;&#34920;&#31034;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#20351;&#24471;&#22270;&#23884;&#20837;&#23398;&#20064;&#36866;&#24212;&#20102;&#26085;&#30410;&#22686;&#38271;&#30340;&#22270;&#35268;&#27169;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#24456;&#22810;&#25104;&#21151;&#65292;&#20294;&#26159;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20250;&#21463;&#21040;&#35745;&#31639;&#29942;&#39048;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2301.05860</link><description>&lt;p&gt;
&#22270;&#23884;&#20837;&#23398;&#20064;&#30340;&#29616;&#29366;&#21644;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
State of the Art and Potentialities of Graph-level Learning. (arXiv:2301.05860v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;&#22270;&#23884;&#20837;&#23398;&#20064;&#65292;&#23427;&#23558;&#19968;&#32452;&#22270;&#20316;&#20026;&#36755;&#20837;&#65292;&#33258;&#21160;&#25552;&#21462;&#29305;&#24449;&#24182;&#23558;&#22270;&#32534;&#30721;&#20026;&#20302;&#32500;&#34920;&#31034;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#20351;&#24471;&#22270;&#23884;&#20837;&#23398;&#20064;&#36866;&#24212;&#20102;&#26085;&#30410;&#22686;&#38271;&#30340;&#22270;&#35268;&#27169;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#24456;&#22810;&#25104;&#21151;&#65292;&#20294;&#26159;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20250;&#21463;&#21040;&#35745;&#31639;&#29942;&#39048;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#29616;&#20102;&#20851;&#31995;&#25968;&#25454;&#30340;&#19968;&#31181;&#20248;&#36234;&#33021;&#21147;&#65292;&#22914;&#21270;&#21512;&#29289;&#12289;&#34507;&#30333;&#36136;&#21644;&#31038;&#20132;&#32593;&#32476;&#31561;&#12290;&#22240;&#27492;&#65292;&#22270;&#23884;&#20837;&#23398;&#20064;&#65292;&#23558;&#19968;&#32452;&#22270;&#20316;&#20026;&#36755;&#20837;&#65292;&#24050;&#24212;&#29992;&#20110;&#35768;&#22810;&#20219;&#21153;&#65292;&#21253;&#25324;&#27604;&#36739;&#12289;&#22238;&#24402;&#12289;&#20998;&#31867;&#31561;&#12290;&#20256;&#32479;&#30340;&#23398;&#20064;&#19968;&#32452;&#22270;&#30340;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#29305;&#24449;&#65292;&#22914;&#20122;&#32467;&#26500;&#12290;&#20294;&#36825;&#20123;&#26041;&#27861;&#34429;&#28982;&#21463;&#30410;&#20110;&#33391;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#24120;&#24120;&#22240;&#26080;&#27861;&#36991;&#20813;&#22270;&#21516;&#26500;&#38382;&#39064;&#32780;&#21463;&#21040;&#35745;&#31639;&#29942;&#39048;&#30340;&#24433;&#21709;&#12290;&#30456;&#21453;&#65292;&#28145;&#24230;&#23398;&#20064;&#36890;&#36807;&#33258;&#21160;&#25552;&#21462;&#29305;&#24449;&#21644;&#23558;&#22270;&#32534;&#30721;&#20026;&#20302;&#32500;&#34920;&#31034;&#65292;&#24110;&#21161;&#22270;&#23884;&#20837;&#23398;&#20064;&#36866;&#24212;&#26085;&#30410;&#22686;&#38271;&#30340;&#22270;&#35268;&#27169;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#28145;&#24230;&#22270;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#36896;&#25104;&#20102;&#35768;&#22810;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#20840;&#38754;&#30340;&#32508;&#36848;&#26469;&#22238;&#39038;&#20174;&#20256;&#32479;&#23398;&#20064;&#21040;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#22270;&#23884;&#20837;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs have a superior ability to represent relational data, like chemical compounds, proteins, and social networks. Hence, graph-level learning, which takes a set of graphs as input, has been applied to many tasks including comparison, regression, classification, and more. Traditional approaches to learning a set of graphs heavily rely on hand-crafted features, such as substructures. But while these methods benefit from good interpretability, they often suffer from computational bottlenecks as they cannot skirt the graph isomorphism problem. Conversely, deep learning has helped graph-level learning adapt to the growing scale of graphs by extracting features automatically and encoding graphs into low-dimensional representations. As a result, these deep graph learning methods have been responsible for many successes. Yet, there is no comprehensive survey that reviews graph-level learning starting with traditional learning and moving through to the deep learning approaches. This article 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#30721;&#22120;&#35843;&#25972;&#65288;DecT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#35299;&#30721;&#22120;&#32593;&#32476;&#26469;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36755;&#20986;&#31471;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#36755;&#20837;&#31471;&#30340;&#39640;&#35745;&#31639;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2212.08408</link><description>&lt;p&gt;
&#35299;&#30721;&#22120;&#35843;&#25972;&#65306;&#23558;&#39640;&#25928;&#35821;&#35328;&#29702;&#35299;&#20316;&#20026;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Decoder Tuning: Efficient Language Understanding as Decoding. (arXiv:2212.08408v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#30721;&#22120;&#35843;&#25972;&#65288;DecT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#35299;&#30721;&#22120;&#32593;&#32476;&#26469;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36755;&#20986;&#31471;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#36755;&#20837;&#31471;&#30340;&#39640;&#35745;&#31639;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PTMs&#65289;&#30340;&#35268;&#27169;&#19981;&#26029;&#22686;&#21152;&#65292;&#21482;&#21521;&#29992;&#25143;&#25552;&#20379;&#25512;&#29702;API&#65288;&#21363;&#27169;&#22411;&#20026;&#26381;&#21153;&#65288;MaaS&#65289;&#35774;&#32622;&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#20570;&#27861;&#12290;&#20026;&#20102;&#36866;&#24212;&#21442;&#25968;&#20923;&#32467;&#30340;PTMs&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#38598;&#20013;&#22312;&#36755;&#20837;&#31471;&#65292;&#23547;&#25214;&#24378;&#26377;&#21147;&#30340;&#25552;&#31034;&#26469;&#21050;&#28608;&#27169;&#22411;&#20135;&#29983;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#36755;&#20837;&#31471;&#30340;&#36866;&#24212;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#32570;&#23569;&#26799;&#24230;&#20449;&#21495;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#25968;&#21315;&#20010;API&#26597;&#35810;&#65292;&#23548;&#33268;&#39640;&#35745;&#31639;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#30721;&#22120;&#35843;&#25972;&#65288;DecT&#65289;&#65292;&#23427;&#19982;&#24403;&#21069;&#26041;&#27861;&#30456;&#21453;&#65292;&#36890;&#36807;&#20248;&#21270;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#35299;&#30721;&#22120;&#32593;&#32476;&#26469;&#36866;&#24212;PTMs&#30340;&#36755;&#20986;&#31471;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;DecT&#39318;&#20808;&#25552;&#21462;&#34987;&#25552;&#31034;&#21050;&#28608;&#30340;&#36755;&#20986;&#20998;&#25968;&#20316;&#20026;&#21021;&#22987;&#39044;&#27979;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36824;&#22312;&#36755;&#20986;&#34920;&#31034;&#19978;&#35757;&#32451;&#20102;&#21478;&#19968;&#20010;&#35299;&#30721;&#22120;&#32593;&#32476;&#65292;&#20197;&#32467;&#21512;&#21518;&#39564;&#25968;&#25454;&#30693;&#35782;&#12290;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65292;DecT&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#35757;&#32451;&#65292;&#24182;&#19988;&#27599;&#20010;&#26679;&#26412;&#21482;&#38656;&#35201;&#19968;&#20010;PTM&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the evergrowing sizes of pre-trained models (PTMs), it has been an emerging practice to only provide the inference APIs for users, namely model-as-a-service (MaaS) setting. To adapt PTMs with model parameters frozen, most current approaches focus on the input side, seeking for powerful prompts to stimulate models for correct answers. However, we argue that input-side adaptation could be arduous due to the lack of gradient signals and they usually require thousands of API queries, resulting in high computation and time costs. In light of this, we present Decoder Tuning (DecT), which in contrast optimizes task-specific decoder networks on the output side. Specifically, DecT first extracts prompt-stimulated output scores for initial predictions. On top of that, we train an additional decoder network on the output representations to incorporate posterior data knowledge. By gradient-based optimization, DecT can be trained within several seconds and requires only one PTM query per sampl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21069;&#21521;&#30452;&#25509;&#21453;&#39304;&#23545;&#40784;&#31639;&#27861;&#65288;FDFA&#65289;&#65292;&#32467;&#21512;&#20102;Activity-Perturbed&#21069;&#21521;&#26799;&#24230;&#21644;&#21160;&#37327;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;DNN&#20013;&#30340;&#20302;&#26041;&#24046;&#26799;&#24230;&#20272;&#35745;&#20540;&#12290;</title><link>http://arxiv.org/abs/2212.07282</link><description>&lt;p&gt;
&#20302;&#26041;&#24046;&#21069;&#21521;&#26799;&#24230;&#31639;&#27861;&#65306;&#30452;&#25509;&#21453;&#39304;&#23545;&#40784;&#32467;&#21512;&#21160;&#37327;&#27861;
&lt;/p&gt;
&lt;p&gt;
Low-Variance Forward Gradients using Direct Feedback Alignment and Momentum. (arXiv:2212.07282v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21069;&#21521;&#30452;&#25509;&#21453;&#39304;&#23545;&#40784;&#31639;&#27861;&#65288;FDFA&#65289;&#65292;&#32467;&#21512;&#20102;Activity-Perturbed&#21069;&#21521;&#26799;&#24230;&#21644;&#21160;&#37327;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;DNN&#20013;&#30340;&#20302;&#26041;&#24046;&#26799;&#24230;&#20272;&#35745;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#30417;&#30563;&#23398;&#20064;&#36890;&#24120;&#20351;&#29992;&#35823;&#24046;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#31639;&#27861;&#36827;&#34892;&#12290;&#20294;&#26159;&#65292;&#21453;&#21521;&#20256;&#25773;&#26399;&#38388;&#30340;&#38169;&#35823;&#39034;&#24207;&#20256;&#25773;&#21644;&#26435;&#37325;&#20256;&#36755;&#38480;&#21046;&#20102;&#20854;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#22240;&#27492;&#20154;&#20204;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#23547;&#25214;BP&#30340;&#26412;&#22320;&#26367;&#20195;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21069;&#21521;&#30452;&#25509;&#21453;&#39304;&#23545;&#40784;&#65288;FDFA&#65289;&#31639;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;Activity-Perturbed&#21069;&#21521;&#26799;&#24230;&#65292;&#30452;&#25509;&#21453;&#39304;&#23545;&#40784;&#21644;&#21160;&#37327;&#27861;&#26469;&#35745;&#31639;DNN&#20013;&#30340;&#20302;&#26041;&#24046;&#26799;&#24230;&#20272;&#35745;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised learning in Deep Neural Networks (DNNs) is commonly performed using the error Backpropagation (BP) algorithm. The sequential propagation of errors and the transport of weights during the backward pass limits its efficiency and scalability. Therefore, there is growing interest in finding local alternatives to BP. Recently, methods based on Forward-Mode Automatic Differentiation have been proposed, such as the Forward Gradient algorithm and its variants. However, Forward Gradients suffer from high variance in large DNNs, which affects convergence. In this paper, we address the large variance of Forward Gradients and propose the Forward Direct Feedback Alignment (FDFA) algorithm that combines Activity-Perturbed Forward Gradients with Direct Feedback Alignment and momentum to compute low-variance gradient estimates in DNNs. Our results provides both theoretical proof and empirical evidence that our proposed method achieves lower variance compared to previous Forward Gradient tec
&lt;/p&gt;</description></item><item><title>&#32654;&#22269;&#30340;&#39640;&#36136;&#37327;&#21307;&#30103;&#20445;&#20581;&#23545;&#20110;&#26576;&#20123;&#31038;&#20250;&#32463;&#27982;&#32676;&#20307;&#26469;&#35828;&#25104;&#26412;&#36807;&#39640;&#65292;&#22240;&#27492;&#31038;&#20250;&#32463;&#27982;&#22240;&#32032;&#22914;&#25910;&#20837;&#21644;&#25945;&#32946;&#31243;&#24230;&#19982;&#20581;&#24247;&#30340;&#24635;&#20307;&#25351;&#26631;&#26377;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.04285</link><description>&lt;p&gt;
&#31038;&#20250;&#32463;&#27982;&#22240;&#32032;&#23545;&#20581;&#24247;&#24046;&#36317;&#30340;&#24433;&#21709;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Impact of Socioeconomic Factors on Health Disparities. (arXiv:2212.04285v3 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04285
&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#30340;&#39640;&#36136;&#37327;&#21307;&#30103;&#20445;&#20581;&#23545;&#20110;&#26576;&#20123;&#31038;&#20250;&#32463;&#27982;&#32676;&#20307;&#26469;&#35828;&#25104;&#26412;&#36807;&#39640;&#65292;&#22240;&#27492;&#31038;&#20250;&#32463;&#27982;&#22240;&#32032;&#22914;&#25910;&#20837;&#21644;&#25945;&#32946;&#31243;&#24230;&#19982;&#20581;&#24247;&#30340;&#24635;&#20307;&#25351;&#26631;&#26377;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#30340;&#39640;&#36136;&#37327;&#21307;&#30103;&#20445;&#20581;&#23545;&#20110;&#26576;&#20123;&#31038;&#20250;&#32463;&#27982;&#32676;&#20307;&#26469;&#35828;&#25104;&#26412;&#36807;&#39640;&#12290;&#26412;&#25991;&#21033;&#29992;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#21644;&#30142;&#30149;&#39044;&#38450;&#25511;&#21046;&#20013;&#24515;&#30340;&#25968;&#25454;&#65292;&#30830;&#23450;&#29305;&#23450;&#31038;&#20250;&#32463;&#27982;&#22240;&#32032;&#19982;&#29305;&#23450;&#21644;&#19968;&#33324;&#20581;&#24247;&#25351;&#26631;&#20043;&#38388;&#30340;&#20851;&#32852;&#31243;&#24230;&#65292;&#24182;&#37319;&#29992;&#21487;&#35270;&#21270;&#20998;&#26512;&#21644;&#39044;&#27979;&#24314;&#27169;&#26469;&#35782;&#21035;&#21464;&#37327;&#20043;&#38388;&#26356;&#22797;&#26434;&#30340;&#20851;&#31995;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25910;&#20837;&#21644;&#25945;&#32946;&#31243;&#24230;&#31561;&#26576;&#20123;&#31038;&#20250;&#32463;&#27982;&#22240;&#32032;&#19982;&#20581;&#24247;&#30340;&#24635;&#20307;&#25351;&#26631;&#26377;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality healthcare in the US can be cost-prohibitive for certain socioeconomic groups. In this paper, we examined data from the US Census and the CDC to determine the degree to which specific socioeconomic factors correlate with both specific and general health metrics. We employed visual analysis to find broad trends and predictive modeling to identify more complex relationships between variables. Our results indicate that certain socioeconomic factors, like income and educational attainment, are highly correlated with aggregate measures of health.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27010;&#36848;&#20102;&#28608;&#20809;&#31561;&#31163;&#23376;&#29289;&#29702;&#20013;&#36866;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#28608;&#20809;&#31561;&#31163;&#23376;&#21152;&#36895;&#21644;&#24815;&#24615;&#32422;&#26463;&#32858;&#21464;&#31561;&#23376;&#39046;&#22495;&#65292;&#21033;&#29992;&#22823;&#25968;&#25454;&#30340;&#39640;&#32423;&#25216;&#26415;&#21644;&#27169;&#22411;&#26469;&#26377;&#25928;&#22788;&#29702;&#21644;&#21463;&#30410;&#20110;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2212.00026</link><description>&lt;p&gt;
&#28608;&#20809;&#31561;&#31163;&#23376;&#29289;&#29702;&#20013;&#25968;&#25454;&#39537;&#21160;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data-driven Science and Machine Learning Methods in Laser-Plasma Physics. (arXiv:2212.00026v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;&#28608;&#20809;&#31561;&#31163;&#23376;&#29289;&#29702;&#20013;&#36866;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#28608;&#20809;&#31561;&#31163;&#23376;&#21152;&#36895;&#21644;&#24815;&#24615;&#32422;&#26463;&#32858;&#21464;&#31561;&#23376;&#39046;&#22495;&#65292;&#21033;&#29992;&#22823;&#25968;&#25454;&#30340;&#39640;&#32423;&#25216;&#26415;&#21644;&#27169;&#22411;&#26469;&#26377;&#25928;&#22788;&#29702;&#21644;&#21463;&#30410;&#20110;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39640;&#21151;&#29575;&#28608;&#20809;&#30340;&#19981;&#26029;&#21319;&#32423;&#21644;&#24191;&#27867;&#20351;&#29992;&#65292;&#28608;&#20809;&#31561;&#31163;&#23376;&#29289;&#29702;&#23398;&#36817;&#20960;&#21313;&#24180;&#26469;&#24471;&#21040;&#20102;&#36805;&#36895;&#21457;&#23637;&#12290;&#26089;&#26399;&#30340;&#23454;&#39564;&#21644;&#25968;&#20540;&#30740;&#31350;&#21463;&#21040;&#21442;&#25968;&#25506;&#32034;&#30340;&#38480;&#21046;&#65292;&#20165;&#38480;&#20110;&#21333;&#27425;&#23454;&#39564;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#25216;&#26415;&#30340;&#25913;&#36827;&#20351;&#24471;&#20154;&#20204;&#33021;&#22815;&#25910;&#38598;&#36234;&#26469;&#36234;&#22810;&#30340;&#25968;&#25454;&#65292;&#26080;&#35770;&#26159;&#22312;&#23454;&#39564;&#20013;&#36824;&#26159;&#27169;&#25311;&#20013;&#12290;&#36825;&#28608;&#21457;&#20102;&#20154;&#20204;&#23545;&#20351;&#29992;&#26469;&#33258;&#25968;&#23398;&#12289;&#32479;&#35745;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#39640;&#32423;&#25216;&#26415;&#26469;&#22788;&#29702;&#21644;&#21463;&#30410;&#20110;&#22823;&#25968;&#25454;&#30340;&#20852;&#36259;&#12290;&#21516;&#26102;&#65292;&#22797;&#26434;&#30340;&#24314;&#27169;&#25216;&#26415;&#20063;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#20173;&#28982;&#21482;&#26377;&#31232;&#30095;&#25968;&#25454;&#37327;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#26088;&#22312;&#27010;&#36848;&#19982;&#28608;&#20809;&#31561;&#31163;&#23376;&#29289;&#29702;&#30456;&#20851;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#36866;&#29992;&#20110;&#28608;&#20809;&#31561;&#31163;&#23376;&#21152;&#36895;&#21644;&#24815;&#24615;&#32422;&#26463;&#32858;&#21464;&#31561;&#37325;&#35201;&#23376;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Laser-plasma physics has developed rapidly over the past few decades as high-power lasers have become both increasingly powerful and more widely available. Early experimental and numerical research in this field was restricted to single-shot experiments with limited parameter exploration. However, recent technological improvements make it possible to gather an increasing amount of data, both in experiments and simulations. This has sparked interest in using advanced techniques from mathematics, statistics and computer science to deal with, and benefit from, big data. At the same time, sophisticated modeling techniques also provide new ways for researchers to effectively deal with situations in which still only sparse amounts of data are available. This paper aims to present an overview of relevant machine learning methods with focus on applicability to laser-plasma physics, including its important sub-fields of laser-plasma acceleration and inertial confinement fusion.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29983;&#25104;&#35299;&#27602;&#25968;&#25454;&#26469;&#27835;&#30103;&#20010;&#20307;&#19981;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21644;&#22522;&#20110;&#28508;&#22312;&#31354;&#38388;&#36229;&#24179;&#38754;&#30340;&#26041;&#27861;&#29983;&#25104;&#36817;&#20284;&#36981;&#24490;&#25968;&#25454;&#20998;&#24067;&#30340;&#35299;&#27602;&#25968;&#25454;&#65292;&#21487;&#20197;&#28040;&#38500;&#27169;&#22411;&#22312;&#38750;&#30495;&#23454;&#26679;&#26412;&#19978;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2211.15897</link><description>&lt;p&gt;
&#23398;&#20064;&#29992;&#20110;&#28040;&#38500;&#20010;&#20154;&#19981;&#20844;&#24179;&#24615;&#30340;&#35299;&#27602;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Learning Antidote Data to Individual Unfairness. (arXiv:2211.15897v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29983;&#25104;&#35299;&#27602;&#25968;&#25454;&#26469;&#27835;&#30103;&#20010;&#20307;&#19981;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21644;&#22522;&#20110;&#28508;&#22312;&#31354;&#38388;&#36229;&#24179;&#38754;&#30340;&#26041;&#27861;&#29983;&#25104;&#36817;&#20284;&#36981;&#24490;&#25968;&#25454;&#20998;&#24067;&#30340;&#35299;&#27602;&#25968;&#25454;&#65292;&#21487;&#20197;&#28040;&#38500;&#27169;&#22411;&#22312;&#38750;&#30495;&#23454;&#26679;&#26412;&#19978;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26102;&#65292;&#20844;&#24179;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#25152;&#26377;&#20844;&#24179;&#24615;&#27010;&#24565;&#20013;&#65292;&#20010;&#20307;&#20844;&#24179;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#27010;&#24565;&#65292;&#23427;&#28304;&#33258;&#19968;&#20010;&#20849;&#35782;&#65306;`&#30456;&#20284;&#30340;&#20010;&#20307;&#24212;&#35813;&#24471;&#21040;&#30456;&#20284;&#30340;&#23545;&#24453;'&#65292;&#20197;&#27492;&#26469;&#25551;&#36848;&#20010;&#21035;&#26696;&#20363;&#30340;&#20844;&#24179;&#23545;&#24453;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#24120;&#23558;&#20010;&#20307;&#20844;&#24179;&#24615;&#25551;&#36848;&#20026;&#22312;&#26679;&#26412;&#19978;&#25200;&#21160;&#25935;&#24863;&#23646;&#24615;&#26102;&#30340;&#39044;&#27979;&#19981;&#21464;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;DRO&#65289;&#33539;&#24335;&#26469;&#35299;&#20915;&#23427;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#27839;&#30528;&#35206;&#30422;&#25935;&#24863;&#20449;&#24687;&#30340;&#26041;&#21521;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#27809;&#26377;&#32771;&#34385;&#29305;&#24449;&#30456;&#20851;&#24615;&#25110;&#20869;&#22312;&#30340;&#25968;&#25454;&#32422;&#26463;&#65292;&#22240;&#27492;&#21487;&#33021;&#20250;&#20351;&#27169;&#22411;&#20248;&#21270;&#21040;&#31163;&#26354;&#38754;&#36739;&#36828;&#21644;&#19981;&#29616;&#23454;&#30340;&#26679;&#26412;&#12290;&#37492;&#20110;&#27492;&#32570;&#28857;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#21644;&#29983;&#25104;&#36817;&#20284;&#36981;&#24490;&#25968;&#25454;&#20998;&#24067;&#30340;&#35299;&#27602;&#25968;&#25454;&#65292;&#29992;&#26469;&#27835;&#30103;&#20010;&#20307;&#19981;&#20844;&#24179;&#24615;&#12290;&#36825;&#20123;&#29983;&#25104;&#30340;&#26354;&#38754;&#19978;&#35299;&#27602;&#25968;&#25454;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21644;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#31354;&#38388;&#36229;&#24179;&#38754;&#30340;&#26041;&#27861;&#36827;&#34892;&#29983;&#25104;&#21644;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness is essential for machine learning systems deployed in high-stake applications. Among all fairness notions, individual fairness, deriving from a consensus that `similar individuals should be treated similarly,' is a vital notion to describe fair treatment for individual cases. Previous studies typically characterize individual fairness as a prediction-invariant problem when perturbing sensitive attributes on samples, and solve it by Distributionally Robust Optimization (DRO) paradigm. However, such adversarial perturbations along a direction covering sensitive information used in DRO do not consider the inherent feature correlations or innate data constraints, therefore could mislead the model to optimize at off-manifold and unrealistic samples. In light of this drawback, in this paper, we propose to learn and generate antidote data that approximately follows the data distribution to remedy individual unfairness. These generated on-manifold antidote data can be used through a g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#26631;&#31614;&#36716;&#31227;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24212;&#20998;&#24067;&#30340;&#21464;&#21270;&#26469;&#25552;&#21319;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#31867;&#21035;&#26631;&#31614;&#21644;&#22122;&#22768;&#22240;&#32032;&#30340;&#20381;&#36182;&#20851;&#31995;&#38543;&#22495;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.15646</link><description>&lt;p&gt;
&#36229;&#36234;&#19981;&#21464;&#24615;&#65306;&#38024;&#23545;&#20855;&#26377;&#8220;&#34394;&#20551;&#8221;&#30456;&#20851;&#24615;&#30340;&#20998;&#24067;&#30340;&#27979;&#35797;&#26102;&#26631;&#31614;&#36716;&#31227;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Beyond Invariance: Test-Time Label-Shift Adaptation for Distributions with "Spurious" Correlations. (arXiv:2211.15646v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#26631;&#31614;&#36716;&#31227;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24212;&#20998;&#24067;&#30340;&#21464;&#21270;&#26469;&#25552;&#21319;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#31867;&#21035;&#26631;&#31614;&#21644;&#22122;&#22768;&#22240;&#32032;&#30340;&#20381;&#36182;&#20851;&#31995;&#38543;&#22495;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#21487;&#33021;&#23545;&#39044;&#27979;&#27169;&#22411;p(y|x)&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#33391;&#24433;&#21709;&#12290;&#25105;&#20204;&#32771;&#34385;&#23384;&#22312;&#38468;&#21152;&#20803;&#25968;&#25454;&#26631;&#31614;&#65288;&#20363;&#22914;&#32452;&#26631;&#31614;&#65289;z&#30340;&#24773;&#20917;&#65292;&#35813;&#26631;&#31614;&#21487;&#20197;&#35828;&#26126;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20551;&#35774;&#25551;&#36848;&#31867;&#21035;&#26631;&#31614;y&#21644;&#8220;&#22122;&#22768;&#8221;&#22240;&#32032;z&#20043;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#20808;&#39564;&#20998;&#24067;p(y, z)&#21487;&#33021;&#20250;&#38543;&#30528;&#22495;&#30340;&#21464;&#21270;&#32780;&#25913;&#21464;&#65292;&#35201;&#20040;&#26159;&#30001;&#20110;&#36825;&#20123;&#39033;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30340;&#21464;&#21270;&#65292;&#35201;&#20040;&#26159;&#30001;&#20110;&#20854;&#20013;&#19968;&#20010;&#21464;&#37327;&#30340;&#36793;&#38469;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#20551;&#35774;&#29305;&#24449;&#30340;&#29983;&#25104;&#27169;&#22411;p(x|y, z)&#22312;&#22495;&#38388;&#26159;&#19981;&#21464;&#30340;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#36825;&#30456;&#24403;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;&#8220;&#26631;&#31614;&#36716;&#31227;&#8221;&#20551;&#35774;&#30340;&#25193;&#23637;&#29256;&#26412;&#65292;&#20854;&#20013;&#26631;&#31614;&#29616;&#22312;&#20063;&#21253;&#25324;&#22122;&#22768;&#22240;&#32032;z&#12290;&#22522;&#20110;&#27492;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#26631;&#31614;&#36716;&#31227;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26410;&#26631;&#35760;&#26679;&#26412;&#24212;&#29992;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#26469;&#36866;&#24212;p(y, z)&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Changes in the data distribution at test time can have deleterious effects on the performance of predictive models $p(y|x)$. We consider situations where there are additional meta-data labels (such as group labels), denoted by $z$, that can account for such changes in the distribution. In particular, we assume that the prior distribution $p(y, z)$, which models the dependence between the class label $y$ and the "nuisance" factors $z$, may change across domains, either due to a change in the correlation between these terms, or a change in one of their marginals. However, we assume that the generative model for features $p(x|y, z)$ is invariant across domains. We note that this corresponds to an expanded version of the widely used "label shift" assumption, where the labels now also include the nuisance factors $z$. Based on this observation, we propose a test-time label shift correction that adapts to changes in the joint distribution $p(y, z)$ using EM applied to unlabeled samples from 
&lt;/p&gt;</description></item><item><title>SciRepEval&#26159;&#31532;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#25913;&#36827;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.13308</link><description>&lt;p&gt;
SciRepEval&#65306;&#19968;&#20010;&#29992;&#20110;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#22810;&#26684;&#24335;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. (arXiv:2211.13308v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13308
&lt;/p&gt;
&lt;p&gt;
SciRepEval&#26159;&#31532;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#25913;&#36827;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#30340;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#21487;&#20197;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#20215;&#20540;&#36755;&#20837;&#29305;&#24449;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#34920;&#31034;&#30340;&#29616;&#26377;&#22522;&#20934;&#26410;&#33021;&#25429;&#25417;&#21040;&#30456;&#20851;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; SciRepEval&#65292;&#31532;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#12290;&#23427;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#29616;&#23454;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013; 11 &#20010;&#26159;&#26032;&#20219;&#21153;&#65306;&#20998;&#31867;&#12289;&#22238;&#24402;&#12289;&#25490;&#21517;&#21644;&#25628;&#32034;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#22522;&#20934;&#26469;&#30740;&#31350;&#21644;&#25913;&#36827;&#31185;&#23398;&#25991;&#26723;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22914;&#20309;&#22312;&#20219;&#21153;&#26684;&#24335;&#26041;&#38754;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#65292;&#31616;&#21333;&#30340;&#22810;&#20219;&#21153;&#35757;&#32451;&#20063;&#19981;&#33021;&#25913;&#36827;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23398;&#20064;&#27599;&#20010;&#25991;&#26723;&#30340;&#22810;&#20010;&#23884;&#20837;&#65292;&#27599;&#20010;&#23884;&#20837;&#19987;&#38376;&#38024;&#23545;&#19981;&#21516;&#30340;&#26684;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#20219;&#21153;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learned representations of scientific documents can serve as valuable input features for downstream tasks, without the need for further fine-tuning. However, existing benchmarks for evaluating these representations fail to capture the diversity of relevant tasks. In response, we introduce SciRepEval, the first comprehensive benchmark for training and evaluating scientific document representations. It includes 25 challenging and realistic tasks, 11 of which are new, across four formats: classification, regression, ranking and search. We then use the benchmark to study and improve the generalization ability of scientific document representation models. We show how state-of-the-art models struggle to generalize across task formats, and that simple multi-task training fails to improve them. However, a new approach that learns multiple embeddings per document, each tailored to a different format, can improve performance. We experiment with task-format-specific control codes and adapters in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25289;&#27604;&#25991;&#23398;&#30340;&#20998;&#31867;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#20854;&#39118;&#26684;&#26469;&#26816;&#27979;Midrash Tanhuma&#20013;&#30340;&#22833;&#33853;&#26448;&#26009;&#12290;</title><link>http://arxiv.org/abs/2211.09710</link><description>&lt;p&gt;
&#32763;&#35793;&#65306;&#21033;&#29992;&#39118;&#26684;&#20998;&#31867;&#26469;&#26816;&#27979;&#22833;&#33853;&#30340;&#12298;Midrash Tanhuma&#12299;&#26448;&#26009;
&lt;/p&gt;
&lt;p&gt;
Style Classification of Rabbinic Literature for Detection of Lost Midrash Tanhuma Material. (arXiv:2211.09710v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25289;&#27604;&#25991;&#23398;&#30340;&#20998;&#31867;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#20854;&#39118;&#26684;&#26469;&#26816;&#27979;Midrash Tanhuma&#20013;&#30340;&#22833;&#33853;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Midrash&#38598;&#21512;&#26159;&#22797;&#26434;&#30340;&#25289;&#27604;&#25991;&#29486;&#20316;&#21697;&#65292;&#30001;&#22810;&#31181;&#35821;&#35328;&#30340;&#25991;&#26412;&#32452;&#25104;&#65292;&#32463;&#36807;&#19981;&#31283;&#23450;&#30340;&#21475;&#22836;&#21644;&#20070;&#38754;&#20256;&#36882;&#36807;&#31243;&#28436;&#21464;&#32780;&#26469;&#12290;&#30830;&#23450;&#36825;&#31181;&#21512;&#38598;&#20013;&#30340;&#19968;&#20010;&#32473;&#23450;&#27573;&#33853;&#30340;&#36215;&#28304;&#24182;&#19981;&#24635;&#26159;&#30452;&#35266;&#30340;&#65292;&#24120;&#24120;&#26159;&#23398;&#32773;&#20043;&#38388;&#30340;&#20105;&#35758;&#65292;&#28982;&#32780;&#23545;&#20110;&#23398;&#32773;&#20204;&#29702;&#35299;&#27573;&#33853;&#21450;&#20854;&#19982;&#25289;&#27604;&#25991;&#38598;&#20013;&#20854;&#20182;&#25991;&#26412;&#30340;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39118;&#26684;&#30340;&#25289;&#27604;&#25991;&#23398;&#20998;&#31867;&#31995;&#32479;&#65292;&#21033;&#29992;&#26368;&#36817;&#21457;&#24067;&#30340;&#38024;&#23545;&#24076;&#20271;&#26469;&#35821;&#30340;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#22833;&#33853;&#30340;Midrash Tanhuma&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
Midrash collections are complex rabbinic works that consist of text in multiple languages, which evolved through long processes of unstable oral and written transmission. Determining the origin of a given passage in such a compilation is not always straightforward and is often a matter of dispute among scholars, yet it is essential for scholars' understanding of the passage and its relationship to other texts in the rabbinic corpus.  To help solve this problem, we propose a system for classification of rabbinic literature based on its style, leveraging recently released pretrained Transformer models for Hebrew. Additionally, we demonstrate how our method can be applied to uncover lost material from Midrash Tanhuma.
&lt;/p&gt;</description></item><item><title>NESTER&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#65292;&#23558;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#25152;&#26377;&#35201;&#27714;&#38598;&#25104;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.04370</link><description>&lt;p&gt;
NESTER&#65306;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
NESTER: An Adaptive Neurosymbolic Method for Treatment Effect Estimation. (arXiv:2211.04370v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04370
&lt;/p&gt;
&lt;p&gt;
NESTER&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#65292;&#23558;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#25152;&#26377;&#35201;&#27714;&#38598;&#25104;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#26159;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#12290;&#22522;&#20110;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#24402;&#32435;&#20559;&#32622;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#27599;&#31181;&#29616;&#26377;&#30340;&#25216;&#26415;&#37117;&#36890;&#36807;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#27491;&#21017;&#21270;&#22120;&#26469;&#35299;&#20915;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#30340;&#29305;&#23450;&#26041;&#38754;&#65292;&#20363;&#22914;&#25511;&#21046;&#20542;&#21521;&#24471;&#20998;&#12289;&#24378;&#21046;&#38543;&#26426;&#21270;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#31216;&#20026;&#31070;&#32463;&#31526;&#21495;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#22120;&#65288;NESTER&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;NESTER&#23558;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#25152;&#26377;&#35201;&#27714;&#38598;&#25104;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#24402;&#32435;&#20559;&#32622;&#30340;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65288;DSL&#65289;&#12290;&#25105;&#20204;&#36824;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;NESTER&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20840;&#38754;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;NESTER&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Treatment effect estimation from observational data is a central problem in causal inference. Methods based on potential outcomes framework solve this problem by exploiting inductive biases and heuristics from causal inference. Each existing technique addresses a specific aspect of treatment effect estimation, such as controlling propensity score, enforcing randomization, etc., by designing neural network architectures and regularizers. In this paper, we propose an adaptive method called Neurosymbolic Treatment Effect Estimator (NESTER), a generalized method for treatment effect estimation. NESTER brings together all the desiderata for treatment effect estimation into one framework. For this purpose, we design a Domain Specific Language (DSL) for the treatment effect estimation based on inductive biases used in literature. We also theoretically study NESTER's capability for the treatment effect estimation task. Our comprehensive empirical results show that NESTER performs better on ben
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22312;&#32447;&#23398;&#20064;&#32852;&#21512;&#28304;&#21644;&#20449;&#36947;&#32534;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#30340;&#36807;&#24230;&#25311;&#21512;&#33021;&#21147;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#65292;&#20174;&#32780;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#25968;&#25454;&#20256;&#36755;&#12290;</title><link>http://arxiv.org/abs/2211.04339</link><description>&lt;p&gt;
&#36808;&#21521;&#33258;&#36866;&#24212;&#35821;&#20041;&#36890;&#20449;&#65306;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#38750;&#32447;&#24615;&#21464;&#25442;&#28304;&#36890;&#36947;&#32534;&#30721;&#30340;&#39640;&#25928;&#25968;&#25454;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
Toward Adaptive Semantic Communications: Efficient Data Transmission via Online Learned Nonlinear Transform Source-Channel Coding. (arXiv:2211.04339v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22312;&#32447;&#23398;&#20064;&#32852;&#21512;&#28304;&#21644;&#20449;&#36947;&#32534;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#30340;&#36807;&#24230;&#25311;&#21512;&#33021;&#21147;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#65292;&#20174;&#32780;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#25968;&#25454;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#35821;&#20041;&#36890;&#20449;&#39046;&#22495;&#25512;&#21160;&#20102;&#31471;&#21040;&#31471;&#25968;&#25454;&#20256;&#36755;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24378;&#22823;&#34920;&#31034;&#33021;&#21147;&#65292;&#23398;&#20064;&#30340;&#25968;&#25454;&#20256;&#36755;&#26041;&#26696;&#34920;&#29616;&#20986;&#27604;&#24050;&#26377;&#30340;&#28304;&#30721;&#21644;&#20449;&#36947;&#32534;&#30721;&#26041;&#27861;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#30740;&#31350;&#37325;&#28857;&#20027;&#35201;&#38598;&#20013;&#22312;&#26550;&#26500;&#21644;&#27169;&#22411;&#25913;&#36827;&#26041;&#38754;&#65292;&#26397;&#21521;&#38745;&#24577;&#30446;&#26631;&#22495;&#12290;&#23613;&#31649;&#36825;&#20123;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#27169;&#22411;&#23481;&#37327;&#30340;&#38480;&#21046;&#21644;&#19981;&#23436;&#32654;&#30340;&#20248;&#21270;&#21644;&#25512;&#24191;&#65292;&#29305;&#21035;&#26159;&#22312;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#25110;&#20449;&#36947;&#21709;&#24212;&#19982;&#27169;&#22411;&#35757;&#32451;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#20173;&#28982;&#26159;&#27425;&#20248;&#30340;&#65292;&#32780;&#36825;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#24456;&#21487;&#33021;&#21457;&#29983;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#23398;&#20064;&#32852;&#21512;&#28304;&#21644;&#20449;&#36947;&#32534;&#30721;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36807;&#24230;&#25311;&#21512;&#23646;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20197;&#36731;&#37327;&#32423;&#22312;&#32447;&#26041;&#24335;&#26356;&#26032;&#37096;&#32626;&#21518;&#30340;&#29616;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emerging field semantic communication is driving the research of end-to-end data transmission. By utilizing the powerful representation ability of deep learning models, learned data transmission schemes have exhibited superior performance than the established source and channel coding methods. While, so far, research efforts mainly concentrated on architecture and model improvements toward a static target domain. Despite their successes, such learned models are still suboptimal due to the limitations in model capacity and imperfect optimization and generalization, particularly when the testing data distribution or channel response is different from that adopted for model training, as is likely to be the case in real-world. To tackle this, we propose a novel online learned joint source and channel coding approach that leverages the deep learning model's overfitting property. Specifically, we update the off-the-shelf pre-trained models after deployment in a lightweight online fashion
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#38646;&#35757;&#32451;&#35823;&#24046;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#20013;&#8220;&#22351;&#8221;&#26041;&#26696;&#30340;&#21344;&#27604;&#38543;&#35757;&#32451;&#25968;&#25454;&#30340;&#22686;&#21152;&#32780;&#25351;&#25968;&#32423;&#36882;&#20943;&#65292;&#24182;&#33021;&#35299;&#37322;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#20986;&#20046;&#24847;&#26009;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.03570</link><description>&lt;p&gt;
&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#20998;&#31867;&#22120;&#27867;&#21270;&#33021;&#21147;&#24378;&#65292;&#22240;&#20026;&#31967;&#31957;&#30340;&#35299;&#20915;&#26041;&#26696;&#24456;&#23569;
&lt;/p&gt;
&lt;p&gt;
Highly over-parameterized classifiers generalize since bad solutions are rare. (arXiv:2211.03570v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#38646;&#35757;&#32451;&#35823;&#24046;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#20013;&#8220;&#22351;&#8221;&#26041;&#26696;&#30340;&#21344;&#27604;&#38543;&#35757;&#32451;&#25968;&#25454;&#30340;&#22686;&#21152;&#32780;&#25351;&#25968;&#32423;&#36882;&#20943;&#65292;&#24182;&#33021;&#35299;&#37322;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#20986;&#20046;&#24847;&#26009;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20854;&#20013;&#36890;&#36807;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#23398;&#20064;&#23548;&#33268;&#38646;&#35757;&#32451;&#35823;&#24046;&#12290;&#22312;&#36825;&#20123;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#35774;&#32622;&#20013;&#65292;&#26377;&#35768;&#22810;&#20855;&#26377;&#38646;&#35757;&#32451;&#35823;&#24046;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#65292;&#20854;&#20013;&#19968;&#20123;&#27604;&#20854;&#20182;&#30340;&#26356;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#19968;&#23450;&#30340;&#26465;&#20214;&#19979;&#65292;&#8220;&#22351;&#8221;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#20998;&#25968;&#65292;&#20854;&#30495;&#23454;&#35823;&#24046;&#22823;&#20110;&#949;&#65292;&#19982;&#35757;&#32451;&#25968;&#25454;n&#30340;&#25968;&#37327;&#25351;&#25968;&#32423;&#22320;&#36882;&#20943;&#21040;&#38646;&#12290;&#35813;&#33539;&#22260;&#21462;&#20915;&#20110;&#29992;&#20110;&#32473;&#23450;&#20998;&#31867;&#38382;&#39064;&#30340;&#20998;&#31867;&#22120;&#20989;&#25968;&#38598;&#21512;&#19978;&#30495;&#23454;&#35823;&#24046;&#30340;&#20998;&#24067;&#65292;&#19981;&#19968;&#23450;&#21462;&#20915;&#20110;&#20998;&#31867;&#22120;&#20989;&#25968;&#38598;&#21512;&#30340;&#22823;&#23567;&#25110;&#22797;&#26434;&#24230;&#65288;&#20363;&#22914;&#21442;&#25968;&#25968;&#37327;&#65289;&#12290;&#36825;&#21487;&#33021;&#35299;&#37322;&#20102;&#21363;&#20351;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20063;&#20855;&#26377;&#20986;&#20046;&#24847;&#26009;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;MNIST&#30340;&#23376;&#38598;&#19978;&#36890;&#36807;&#23454;&#39564;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#25968;&#23398;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the generalization of over-parameterized classifiers where Empirical Risk Minimization (ERM) for learning leads to zero training error. In these over-parameterized settings there are many global minima with zero training error, some of which generalize better than others. We show that under certain conditions the fraction of "bad" global minima with a true error larger than {\epsilon} decays to zero exponentially fast with the number of training data n. The bound depends on the distribution of the true error over the set of classifier functions used for the given classification problem, and does not necessarily depend on the size or complexity (e.g. the number of parameters) of the classifier function set. This might explain the unexpectedly good generalization even of highly over-parameterized Neural Networks. We support our mathematical framework with experiments on a synthetic data set and a subset of MNIST.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#22810;&#35821;&#35328;&#30340;&#24615;&#21035;&#19981;&#26126;&#30830;&#30340;TTS&#22768;&#38899;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#24615;&#21035;&#24863;&#30693;&#26041;&#27861;&#20174;&#28508;&#22312;&#35828;&#35805;&#20154;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#37319;&#26679;&#65292;&#25104;&#21151;&#29983;&#25104;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#12289;&#19968;&#33268;&#24615;&#21644;&#24615;&#21035;&#19981;&#26126;&#30830;&#24615;&#26356;&#24378;&#30340;&#22768;&#38899;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#39564;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2211.00375</link><description>&lt;p&gt;
&#29983;&#25104;&#22810;&#35821;&#35328;&#30340;&#24615;&#21035;&#19981;&#26126;&#30830;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#22768;&#38899;
&lt;/p&gt;
&lt;p&gt;
Generating Multilingual Gender-Ambiguous Text-to-Speech Voices. (arXiv:2211.00375v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00375
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#22810;&#35821;&#35328;&#30340;&#24615;&#21035;&#19981;&#26126;&#30830;&#30340;TTS&#22768;&#38899;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#24615;&#21035;&#24863;&#30693;&#26041;&#27861;&#20174;&#28508;&#22312;&#35828;&#35805;&#20154;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#37319;&#26679;&#65292;&#25104;&#21151;&#29983;&#25104;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#12289;&#19968;&#33268;&#24615;&#21644;&#24615;&#21035;&#19981;&#26126;&#30830;&#24615;&#26356;&#24378;&#30340;&#22768;&#38899;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#39564;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#29992;&#25143;&#30028;&#38754;&#30340;&#24615;&#21035;&#26159;&#20854;&#34987;&#24863;&#30693;&#36523;&#20221;&#30340;&#20851;&#38190;&#20803;&#32032;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30028;&#38754;&#24320;&#22987;&#37319;&#29992;&#19981;&#26126;&#30830;&#30340;&#24615;&#21035;&#65292;&#32780;&#19981;&#26159;&#26126;&#30830;&#30028;&#23450;&#20026;&#30007;&#24615;&#25110;&#22899;&#24615;&#12290;&#36825;&#39033;&#24037;&#20316;&#35299;&#20915;&#20102;&#22312;&#22810;&#35828;&#35805;&#20154;&#65292;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#29983;&#25104;&#26032;&#30340;&#24615;&#21035;&#19981;&#26126;&#30830;&#30340;TTS&#35821;&#38899;&#30340;&#20219;&#21153;&#12290;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;&#25552;&#20986;&#30340;&#24615;&#21035;&#24863;&#30693;&#26041;&#27861;&#26377;&#25928;&#22320;&#20174;&#28508;&#22312;&#30340;&#35828;&#35805;&#20154;&#23884;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#37319;&#26679;&#26469;&#23454;&#29616;&#30340;&#12290;&#24191;&#27867;&#30340;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#28165;&#26970;&#22320;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#29983;&#25104;&#19968;&#31995;&#21015;&#26032;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#22768;&#38899;&#65292;&#36825;&#20123;&#22768;&#38899;&#22312;&#25152;&#26377;&#32771;&#23519;&#30340;&#35821;&#35328;&#20013;&#37117;&#34987;&#35748;&#20026;&#27604;&#22522;&#32447;&#22768;&#38899;&#26356;&#20855;&#24615;&#21035;&#19981;&#26126;&#30830;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#24615;&#21035;&#35748;&#30693;&#34987;&#21457;&#29616;&#22312;&#21548;&#20247;&#30340;&#20004;&#20010;&#20154;&#21475;&#32479;&#35745;&#22240;&#32032;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#65306;&#27597;&#35821;&#21644;&#24615;&#21035;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#21487;&#38752;&#22320;&#29983;&#25104;&#22810;&#31181;&#24615;&#21035;&#19981;&#26126;&#30830;&#22768;&#38899;&#30340;&#31995;&#32479;&#24615;&#21644;&#32463;&#36807;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The gender of any voice user interface is a key element of its perceived identity. Recently, there has been increasing interest in interfaces where the gender is ambiguous rather than clearly identifying as female or male. This work addresses the task of generating novel gender-ambiguous TTS voices in a multi-speaker, multilingual setting. This is accomplished by efficiently sampling from a latent speaker embedding space using a proposed gender-aware method. Extensive objective and subjective evaluations clearly indicate that this method is able to efficiently generate a range of novel, diverse voices that are consistent and perceived as more gender-ambiguous than a baseline voice across all the languages examined. Interestingly, the gender perception is found to be robust across two demographic factors of the listeners: native language and gender. To our knowledge, this is the first systematic and validated approach that can reliably generate a variety of gender-ambiguous voices.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22312;&#23567;&#22411;&#20851;&#38190;&#35789;&#26816;&#27979;&#27169;&#22411;&#19978;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#23427;&#21487;&#20197;&#25552;&#39640;&#22312;&#26631;&#31614;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2210.01703</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#25552;&#39640;&#26080;&#26631;&#31614;&#20851;&#38190;&#35789;&#26816;&#27979;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Improving Label-Deficient Keyword Spotting Through Self-Supervised Pretraining. (arXiv:2210.01703v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22312;&#23567;&#22411;&#20851;&#38190;&#35789;&#26816;&#27979;&#27169;&#22411;&#19978;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#23427;&#21487;&#20197;&#25552;&#39640;&#22312;&#26631;&#31614;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#35789;&#26816;&#27979;&#25216;&#26415;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#34701;&#20837;&#21508;&#31181;&#31995;&#32479;&#65292;&#20363;&#22914;&#35821;&#38899;&#21161;&#25163;&#12290;&#20026;&#20102;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20381;&#36182;&#22823;&#37327;&#24050;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#20165;&#21487;&#24212;&#29992;&#20110;&#36825;&#20123;&#25968;&#25454;&#21487;&#29992;&#30340;&#24773;&#20917;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#26131;&#20110;&#33719;&#24471;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#32531;&#35299;&#36825;&#31181;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#38024;&#23545;&#23567;&#22411;&#20851;&#38190;&#35789;&#26816;&#27979;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#30830;&#23450;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#21487;&#22312;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#23567;&#22411;&#20851;&#38190;&#35789;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;Data2Vec&#23545;&#19977;&#20010;&#32039;&#20945;&#22411;&#22522;&#20110;transformer&#27169;&#22411;&#30340;&#20851;&#38190;&#35789;&#26816;&#27979;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#26631;&#31614;&#19981;&#36275;&#30340;Google&#35821;&#38899;&#21629;&#20196;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;Data2Vec&#39044;&#35757;&#32451;&#21487;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#26631;&#31614;&#32570;&#22833;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keyword Spotting (KWS) models are becoming increasingly integrated into various systems, e.g. voice assistants. To achieve satisfactory performance, these models typically rely on a large amount of labelled data, limiting their applications only to situations where such data is available. Self-supervised Learning (SSL) methods can mitigate such a reliance by leveraging readily-available unlabelled data. Most SSL methods for speech have primarily been studied for large models, whereas this is not ideal, as compact KWS models are generally required. This paper explores the effectiveness of SSL on small models for KWS and establishes that SSL can enhance the performance of small KWS models when labelled data is scarce. We pretrain three compact transformer-based KWS models using Data2Vec, and fine-tune them on a label-deficient setup of the Google Speech Commands data set. It is found that Data2Vec pretraining leads to a significant increase in accuracy, with label-deficient scenarios sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Prompt Feature&#65288;GPF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#36890;&#29992;&#22320;&#35843;&#25972;&#39044;&#20808;&#35757;&#32451;&#36807;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25805;&#20316;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#33021;&#22815;&#23545;&#24212;&#20219;&#20309;&#24418;&#24335;&#30340;Prompt&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2209.15240</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;Prompt&#35843;&#25972;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Universal Prompt Tuning for Graph Neural Networks. (arXiv:2209.15240v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Prompt Feature&#65288;GPF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#36890;&#29992;&#22320;&#35843;&#25972;&#39044;&#20808;&#35757;&#32451;&#36807;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25805;&#20316;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#33021;&#22815;&#23545;&#24212;&#20219;&#20309;&#24418;&#24335;&#30340;Prompt&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Prompt&#35843;&#25972;&#22312;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#26041;&#38754;&#24341;&#36215;&#20102;&#30740;&#31350;&#28909;&#28526;&#12290;&#19982;&#35821;&#35328;&#39046;&#22495;&#37319;&#29992;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#31574;&#30053;&#19981;&#21516;&#65292;&#22270;&#24418;&#39046;&#22495;&#23637;&#31034;&#20102;&#22810;&#26679;&#21270;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#35774;&#35745;&#36866;&#24403;&#30340;&#22522;&#20110;Prompt&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35843;&#25972;&#26041;&#27861;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Prompt Feature (GPF) &#30340;&#36890;&#29992;Prompt&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#36866;&#29992;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#31574;&#30053;&#19979;&#30340;&#39044;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;GPF&#22312;&#36755;&#20837;&#22270;&#24418;&#30340;&#29305;&#24449;&#31354;&#38388;&#19978;&#25805;&#20316;&#65292;&#29702;&#35770;&#19978;&#21487;&#23454;&#29616;&#19982;&#20219;&#20309;&#24418;&#24335;&#30340;Prompt&#20989;&#25968;&#31561;&#25928;&#30340;&#25928;&#26524;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19981;&#20877;&#38656;&#35201;&#26126;&#30830;&#35828;&#26126;&#27599;&#20010;&#39044;&#35757;&#32451;&#31574;&#30053;&#23545;&#24212;&#30340;Prompt&#20989;&#25968;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#37319;&#29992;GPF&#26469;&#23454;&#29616;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, prompt tuning has sparked a research surge in adapting pre-trained models. Unlike the unified pre-training strategy employed in the language field, the graph field exhibits diverse pre-training strategies, posing challenges in designing appropriate prompt-based tuning methods for graph neural networks. While some pioneering work has devised specialized prompting functions for models that employ edge prediction as their pre-training tasks, these methods are limited to specific pre-trained GNN models and lack broader applicability. In this paper, we introduce a universal prompt-based tuning method called Graph Prompt Feature (GPF) for pre-trained GNN models under any pre-training strategy. GPF operates on the input graph's feature space and can theoretically achieve an equivalent effect to any form of prompting function. Consequently, we no longer need to illustrate the prompting function corresponding to each pre-training strategy explicitly. Instead, we employ GPF to o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#28369;&#30340;&#21407;&#23376;&#26426;&#22120;&#23398;&#20064;&#22522;&#30784;&#65292;&#36890;&#36807;&#35299;Laplacian&#29305;&#24449;&#20540;&#38382;&#39064;&#33719;&#24471;&#21407;&#23376;&#23494;&#24230;&#30340;&#26368;&#24179;&#28369;&#22522;&#30784;&#21487;&#24212;&#29992;&#20110;&#21508;&#24335;&#21270;&#23398;&#29615;&#22659;&#20043;&#20013;&#12290;</title><link>http://arxiv.org/abs/2209.01948</link><description>&lt;p&gt;
&#21407;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24179;&#28369;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
A smooth basis for atomistic machine learning. (arXiv:2209.01948v2 [physics.chem-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#28369;&#30340;&#21407;&#23376;&#26426;&#22120;&#23398;&#20064;&#22522;&#30784;&#65292;&#36890;&#36807;&#35299;Laplacian&#29305;&#24449;&#20540;&#38382;&#39064;&#33719;&#24471;&#21407;&#23376;&#23494;&#24230;&#30340;&#26368;&#24179;&#28369;&#22522;&#30784;&#21487;&#24212;&#29992;&#20110;&#21508;&#24335;&#21270;&#23398;&#29615;&#22659;&#20043;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30456;&#20114;&#21407;&#23376;&#20301;&#32622;&#30340;&#30456;&#20851;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#22987;&#20110;&#23545;&#27599;&#20010;&#31995;&#32479;&#20013;&#30340;&#21407;&#23376;&#30340;&#37051;&#22495;&#23494;&#24230;&#36827;&#34892;&#31163;&#25955;&#25551;&#36848;&#12290;&#23545;&#31216;&#24615;&#32771;&#34385;&#25903;&#25345;&#20351;&#29992;&#29699;&#35856;&#20989;&#25968;&#25193;&#23637;&#35813;&#23494;&#24230;&#30340;&#35282;&#24230;&#20381;&#36182;&#24615;&#65292;&#20294;&#23578;&#26410;&#26377;&#26126;&#30830;&#30340;&#29702;&#30001;&#36873;&#25321;&#19968;&#20010;&#24452;&#21521;&#22522;&#30784;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22260;&#32469;&#24863;&#20852;&#36259;&#21407;&#23376;&#30340;&#29699;&#24418;&#22495;&#20869;&#35299;Laplacian&#29305;&#24449;&#20540;&#38382;&#39064;&#20135;&#29983;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#20250;&#22312;&#29699;&#20869;&#29983;&#25104;&#19968;&#20010;&#32473;&#23450;&#22823;&#23567;&#30340;&#26368;&#24179;&#28369;&#30340;&#22522;&#30784;&#65292;&#24182;&#19988;Laplacian&#29305;&#24449;&#24577;&#30340;&#24352;&#37327;&#31215;&#20063;&#20026;&#22312;&#30456;&#24212;&#39640;&#32500;&#29699;&#20869;&#25193;&#23637;&#21407;&#23376;&#23494;&#24230;&#30340;&#20219;&#20309;&#39640;&#38454;&#30456;&#20851;&#24615;&#25552;&#20379;&#20102;&#26368;&#24179;&#28369;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#20010;&#26080;&#30417;&#30563;&#30340;&#25968;&#25454;&#38598;&#36136;&#37327;&#25351;&#26631;&#65292;&#24182;&#34920;&#26126;Laplacian&#29305;&#24449;&#24577;&#22522;&#30784;&#30340;&#24615;&#33021;&#27604;&#19968;&#20123;&#24191;&#27867;&#20351;&#29992;&#30340;&#26367;&#20195;&#26041;&#26696;&#35201;&#22909;&#24471;&#22810;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#21270;&#23398;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning frameworks based on correlations of interatomic positions begin with a discretized description of the density of other atoms in the neighbourhood of each atom in the system. Symmetry considerations support the use of spherical harmonics to expand the angular dependence of this density, but there is as yet no clear rationale to choose one radial basis over another. Here we investigate the basis that results from the solution of the Laplacian eigenvalue problem within a sphere around the atom of interest. We show that this generates the smoothest possible basis of a given size within the sphere, and that a tensor product of Laplacian eigenstates also provides the smoothest possible basis for expanding any higher-order correlation of the atomic density within the appropriate hypersphere. We consider several unsupervised metrics of the quality of a basis for a given dataset, and show that the Laplacian eigenstate basis has a performance that is much better than some widely
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#24577;&#33258;&#36866;&#24212;&#36830;&#32493;RL&#65288;DaCoRL&#65289;&#31639;&#27861;&#65292;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36816;&#29992;&#20102;&#28176;&#36827;&#24335;&#19978;&#19979;&#25991;&#21270;&#23398;&#20064;&#26041;&#27861;&#36880;&#27493;&#32858;&#31867;&#20219;&#21153;&#65292;&#23398;&#20064;&#19968;&#20010;&#19978;&#19979;&#25991;&#26465;&#20214;&#21270;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#20013;&#29615;&#22659;&#19982;&#20219;&#21153;&#21464;&#21270;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.00347</link><description>&lt;p&gt;
&#21160;&#24577;&#33258;&#36866;&#24212;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#65306;&#36890;&#36807;&#28176;&#36827;&#24335;&#19978;&#19979;&#25991;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dynamics-Adaptive Continual Reinforcement Learning via Progressive Contextualization. (arXiv:2209.00347v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.00347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#24577;&#33258;&#36866;&#24212;&#36830;&#32493;RL&#65288;DaCoRL&#65289;&#31639;&#27861;&#65292;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36816;&#29992;&#20102;&#28176;&#36827;&#24335;&#19978;&#19979;&#25991;&#21270;&#23398;&#20064;&#26041;&#27861;&#36880;&#27493;&#32858;&#31867;&#20219;&#21153;&#65292;&#23398;&#20064;&#19968;&#20010;&#19978;&#19979;&#25991;&#26465;&#20214;&#21270;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#20013;&#29615;&#22659;&#19982;&#20219;&#21153;&#21464;&#21270;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#65288;CRL&#65289;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#36805;&#36895;&#35843;&#25972;RL&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#38543;&#30528;&#29615;&#22659;&#22312;&#20854;&#29983;&#21629;&#21608;&#26399;&#20869;&#30340;&#25913;&#21464;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23398;&#20064;&#21040;&#30340;&#20449;&#24687;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;DaCoRL&#65292;&#21363;&#21160;&#24577;&#33258;&#36866;&#24212;&#36830;&#32493;RL&#12290;DaCoRL&#20351;&#29992;&#28176;&#36827;&#24335;&#19978;&#19979;&#25991;&#21270;&#23398;&#20064;&#23398;&#20064;&#19968;&#20010;&#19978;&#19979;&#25991;&#26465;&#20214;&#21270;&#31574;&#30053;&#65292;&#23558;&#19968;&#31995;&#21015;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#38745;&#24577;&#20219;&#21153;&#36880;&#27493;&#32858;&#31867;&#20026;&#19968;&#31995;&#21015;&#19978;&#19979;&#25991;&#65292;&#24182;&#36873;&#25321;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22810;&#22836;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge of continual reinforcement learning (CRL) in dynamic environments is to promptly adapt the RL agent's behavior as the environment changes over its lifetime, while minimizing the catastrophic forgetting of the learned information. To address this challenge, in this article, we propose DaCoRL, i.e., dynamics-adaptive continual RL. DaCoRL learns a context-conditioned policy using progressive contextualization, which incrementally clusters a stream of stationary tasks in the dynamic environment into a series of contexts and opts for an expandable multihead neural network to approximate the policy. Specifically, we define a set of tasks with similar dynamics as an environmental context and formalize context inference as a procedure of online Bayesian infinite Gaussian mixture clustering on environment features, resorting to online Bayesian inference to infer the posterior distribution over contexts. Under the assumption of a Chinese restaurant process prior, this technique c
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#29305;&#23450;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#27867;&#21270;&#26694;&#26550;(DRM)&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#22810;&#20998;&#31867;&#22120;&#38598;&#25104;&#21644;&#22312;&#32447;&#30446;&#26631;&#26679;&#26412;&#36866;&#24212;&#26469;&#20943;&#23569;&#36866;&#24212;&#24615;&#24046;&#36317;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2208.08661</link><description>&lt;p&gt;
&#38754;&#21521;&#39046;&#22495;&#29305;&#23450;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#22806;&#26679;&#26412;&#27867;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Domain-Specific Risk Minimization for Out-of-Distribution Generalization. (arXiv:2208.08661v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#29305;&#23450;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#27867;&#21270;&#26694;&#26550;(DRM)&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#22810;&#20998;&#31867;&#22120;&#38598;&#25104;&#21644;&#22312;&#32447;&#30446;&#26631;&#26679;&#26412;&#36866;&#24212;&#26469;&#20943;&#23569;&#36866;&#24212;&#24615;&#24046;&#36317;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#22312;&#28304;&#22495;&#19978;&#23398;&#20064;&#30340;&#20551;&#35774;&#26469;&#25512;&#26029;&#26410;&#35265;&#30446;&#26631;&#22495;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20551;&#35774;&#21487;&#33021;&#19982;&#38024;&#23545;&#30446;&#26631;&#22495;&#30340;&#26368;&#20248;&#20551;&#35774;&#30456;&#21435;&#29978;&#36828;&#65292;&#36825;&#31181;&#24046;&#36317;&#34987;&#31216;&#20026;&#8220;&#36866;&#24212;&#24615;&#24046;&#36317;&#8221;&#12290;&#22914;&#26524;&#19981;&#21033;&#29992;&#27979;&#35797;&#26679;&#26412;&#20013;&#30340;&#39046;&#22495;&#20449;&#24687;&#65292;&#36866;&#24212;&#24615;&#24046;&#36317;&#30340;&#20272;&#35745;&#21644;&#26368;&#23567;&#21270;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#36825;&#22952;&#30861;&#20102;&#25105;&#20204;&#23558;&#27169;&#22411;&#30828;&#21270;&#21040;&#20219;&#20309;&#26410;&#30693;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#26126;&#30830;&#32771;&#34385;&#36866;&#24212;&#24615;&#24046;&#36317;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#40723;&#21169;&#36890;&#36807;&#20004;&#31181;&#31574;&#30053;&#26469;&#20943;&#23569;&#24046;&#36317;&#65306;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#22810;&#20010;&#20998;&#31867;&#22120;&#26469;&#20016;&#23500;&#20551;&#35774;&#31354;&#38388;&#65292;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24046;&#36317;&#20272;&#35745;&#26041;&#27861;&#26469;&#25351;&#23548;&#20026;&#30446;&#26631;&#36873;&#25321;&#26356;&#22909;&#30340;&#20551;&#35774;&#12290;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#36890;&#36807;&#20351;&#29992;&#22312;&#32447;&#30446;&#26631;&#26679;&#26412;&#26469;&#30452;&#25509;&#36866;&#24212;&#27169;&#22411;&#21442;&#25968;&#26469;&#26368;&#23567;&#21270;&#24046;&#36317;&#12290;&#25105;&#20204;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#32479;&#19968;&#20102;&#36825;&#20004;&#31181;&#31574;&#30053;&#65292;&#20801;&#35768;&#26377;&#25928;&#22320;&#20943;&#23569;&#36866;&#24212;&#24615;&#24046;&#36317;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;DRM&#22312;&#20302;&#25968;&#25454;&#26465;&#20214;&#19979;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;DG&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent domain generalization (DG) approaches typically use the hypothesis learned on source domains for inference on the unseen target domain. However, such a hypothesis can be arbitrarily far from the optimal one for the target domain, induced by a gap termed ``adaptivity gap''. Without exploiting the domain information from the unseen test samples, adaptivity gap estimation and minimization are intractable, which hinders us to robustify a model to any unknown distribution. In this paper, we first establish a generalization bound that explicitly considers the adaptivity gap. Our bound motivates two strategies to reduce the gap: the first one is ensembling multiple classifiers to enrich the hypothesis space, then we propose effective gap estimation methods for guiding the selection of a better hypothesis for the target. The other method is minimizing the gap directly by adapting model parameters using online target samples. We thus propose \textbf{Domain-specific Risk Minimization (DRM
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26088;&#22312;&#35299;&#20915;&#20020;&#26102;&#22242;&#38431;&#21327;&#20316;&#25361;&#25112;&#65292;&#37319;&#29992;&#26368;&#20339;&#21709;&#24212;&#22810;&#26679;&#24615;&#29983;&#25104;&#38431;&#21451;&#20197;&#25552;&#39640;&#23398;&#20064;&#32773;&#30340;&#31283;&#20581;&#24615;&#12290;&#19982;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#19981;&#20250;&#20986;&#29616;&#34920;&#38754;&#19978;&#30456;&#20284;&#30340;&#38431;&#21451;&#65292;&#20351;&#24471;&#22312;&#19982;&#26410;&#30693;&#38431;&#21451;&#21327;&#20316;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2207.14138</link><description>&lt;p&gt;
&#21033;&#29992;&#26368;&#20339;&#21709;&#24212;&#22810;&#26679;&#24615;&#29983;&#25104;&#20276;&#20387;&#20197;&#35757;&#32451;&#31283;&#20581;&#30340;&#20020;&#26102;&#22242;&#38431;&#21327;&#20316;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Generating Teammates for Training Robust Ad Hoc Teamwork Agents via Best-Response Diversity. (arXiv:2207.14138v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#35299;&#20915;&#20020;&#26102;&#22242;&#38431;&#21327;&#20316;&#25361;&#25112;&#65292;&#37319;&#29992;&#26368;&#20339;&#21709;&#24212;&#22810;&#26679;&#24615;&#29983;&#25104;&#38431;&#21451;&#20197;&#25552;&#39640;&#23398;&#20064;&#32773;&#30340;&#31283;&#20581;&#24615;&#12290;&#19982;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#19981;&#20250;&#20986;&#29616;&#34920;&#38754;&#19978;&#30456;&#20284;&#30340;&#38431;&#21451;&#65292;&#20351;&#24471;&#22312;&#19982;&#26410;&#30693;&#38431;&#21451;&#21327;&#20316;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#26102;&#22242;&#38431;&#21327;&#20316;(AHT)&#26159;&#35774;&#35745;&#19968;&#20010;&#24378;&#22823;&#30340;&#23398;&#20064;&#20195;&#29702;&#30340;&#25361;&#25112;&#65292;&#35813;&#20195;&#29702;&#33021;&#22815;&#22312;&#27809;&#26377;&#20808;&#21069;&#21327;&#35843;&#26426;&#21046;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#19982;&#26410;&#30693;&#30340;&#38431;&#21451;&#21512;&#20316;&#12290;&#26089;&#26399;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#25163;&#24037;&#21046;&#20316;&#30340;&#38431;&#21451;&#31574;&#30053;&#26469;&#35757;&#32451;&#23398;&#20064;&#32773;&#26469;&#35299;&#20915;AHT&#38382;&#39064;&#65292;&#36825;&#20123;&#31574;&#30053;&#36890;&#24120;&#26159;&#22522;&#20110;&#19987;&#23478;&#23545;&#23398;&#20064;&#32773;&#21487;&#33021;&#36935;&#21040;&#30340;&#31574;&#30053;&#30340;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#35774;&#35745;&#30340;&#12290;&#28982;&#32780;&#65292;&#20026;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#35757;&#32451;&#23454;&#29616;&#38431;&#21451;&#31574;&#30053;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#30340;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#35757;&#32451;&#23398;&#20064;&#32773;&#20351;&#29992;&#36890;&#36807;&#20248;&#21270;&#20449;&#24687;&#29702;&#35770;&#22810;&#26679;&#24615;&#24230;&#37327;&#29983;&#25104;&#30340;&#38431;&#21451;&#31574;&#30053;&#26469;&#25552;&#39640;&#20854;&#31283;&#20581;&#24615;&#12290;&#20248;&#21270;&#29616;&#26377;&#30340;&#20449;&#24687;&#29702;&#35770;&#22810;&#26679;&#24615;&#24230;&#37327;&#20197;&#29992;&#20110;&#29983;&#25104;&#38431;&#21451;&#31574;&#30053;&#30340;&#38382;&#39064;&#22312;&#20110;&#20250;&#20986;&#29616;&#34920;&#38754;&#19978;&#19981;&#21516;&#30340;&#38431;&#21451;&#12290;&#24403;&#29992;&#20110;AHT&#35757;&#32451;&#26102;&#65292;&#34920;&#38754;&#19978;&#19981;&#21516;&#30340;&#38431;&#21451;&#34892;&#20026;&#21487;&#33021;&#26080;&#27861;&#25552;&#39640;&#23398;&#20064;&#32773;&#22312;&#19982;&#26410;&#30693;&#30340;&#38431;&#21451;&#21327;&#20316;&#26102;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ad hoc teamwork (AHT) is the challenge of designing a robust learner agent that effectively collaborates with unknown teammates without prior coordination mechanisms. Early approaches address the AHT challenge by training the learner with a diverse set of handcrafted teammate policies, usually designed based on an expert's domain knowledge about the policies the learner may encounter. However, implementing teammate policies for training based on domain knowledge is not always feasible. In such cases, recent approaches attempted to improve the robustness of the learner by training it with teammate policies generated by optimising information-theoretic diversity metrics. The problem with optimising existing information-theoretic diversity metrics for teammate policy generation is the emergence of superficially different teammates. When used for AHT training, superficially different teammate behaviours may not improve a learner's robustness during collaboration with unknown teammates. In 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#19968;&#33268;&#25490;&#24207;&#26694;&#26550;&#65292;&#21363;RankDice/RankIoU&#65292;&#29992;&#20110;&#35299;&#20915;&#30001;&#20110;&#29616;&#26377;&#30340;&#20998;&#21106;&#26694;&#26550;&#23545;&#20110;Dice/IoU&#25351;&#26631;&#32570;&#20047;&#19968;&#33268;&#24615;&#32780;&#21487;&#33021;&#23548;&#33268;&#30340;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2206.13086</link><description>&lt;p&gt;
RankSEG:&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#25490;&#24207;&#30340;&#20998;&#21106;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
RankSEG: A Consistent Ranking-based Framework for Segmentation. (arXiv:2206.13086v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#19968;&#33268;&#25490;&#24207;&#26694;&#26550;&#65292;&#21363;RankDice/RankIoU&#65292;&#29992;&#20110;&#35299;&#20915;&#30001;&#20110;&#29616;&#26377;&#30340;&#20998;&#21106;&#26694;&#26550;&#23545;&#20110;Dice/IoU&#25351;&#26631;&#32570;&#20047;&#19968;&#33268;&#24615;&#32780;&#21487;&#33021;&#23548;&#33268;&#30340;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#21106;&#24050;&#25104;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#26412;&#39046;&#22495;&#65292;&#23427;&#23558;&#26631;&#31614;&#20998;&#37197;&#32473;&#27599;&#20010;&#20687;&#32032;/&#29305;&#24449;&#65292;&#20197;&#20174;&#22270;&#20687;/&#25991;&#26412;&#20013;&#25552;&#21462;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#12290;&#20026;&#20102;&#35780;&#20272;&#20998;&#21106;&#24615;&#33021;&#65292;&#20351;&#29992;Dice&#21644;IoU&#25351;&#26631;&#26469;&#34913;&#37327;&#23454;&#38469;&#20540;&#21644;&#39044;&#27979;&#20998;&#21106;&#20043;&#38388;&#30340;&#37325;&#21472;&#31243;&#24230;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19982;Dice/IoU&#25351;&#26631;&#30456;&#20851;&#30340;&#20998;&#21106;&#29702;&#35770;&#22522;&#30784;&#65292;&#21253;&#25324;&#31867;&#27604;&#20110;&#20998;&#31867;&#30340;&#36125;&#21494;&#26031;&#35268;&#21017;&#21644;Dice-/IoU-&#26657;&#20934;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;&#38408;&#20540;&#30340;&#26694;&#26550;&#23545;&#20110;Dice/IoU&#25351;&#26631;&#32570;&#20047;&#19968;&#33268;&#24615;&#65292;&#22240;&#27492;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#33268;&#25490;&#24207;&#26694;&#26550;&#65292;&#21363;RankDice/RankIoU&#65292;&#21463;&#36125;&#21494;&#26031;&#20998;&#21106;&#35268;&#21017;&#30340;&#25554;&#20837;&#27861;&#21017;&#30340;&#21551;&#21457;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19977;&#20010;&#20351;&#29992;GPU&#24182;&#34892;&#25191;&#34892;&#30340;&#25968;&#23383;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Segmentation has emerged as a fundamental field of computer vision and natural language processing, which assigns a label to every pixel/feature to extract regions of interest from an image/text. To evaluate the performance of segmentation, the Dice and IoU metrics are used to measure the degree of overlap between the ground truth and the predicted segmentation. In this paper, we establish a theoretical foundation of segmentation with respect to the Dice/IoU metrics, including the Bayes rule and Dice-/IoU-calibration, analogous to classification-calibration or Fisher consistency in classification. We prove that the existing thresholding-based framework with most operating losses are not consistent with respect to the Dice/IoU metrics, and thus may lead to a suboptimal solution. To address this pitfall, we propose a novel consistent ranking-based framework, namely RankDice/RankIoU, inspired by plug-in rules of the Bayes segmentation rule. Three numerical algorithms with GPU parallel exe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Machine Personality Inventory (MPI)&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#20010;&#24615;&#65292;&#36890;&#36807;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LLM)&#20855;&#26377;&#20010;&#24615;&#12290;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;Personality Prompting (P^2)&#26041;&#27861;&#65292;&#29992;&#20110;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#35825;&#23548;LLM&#20855;&#26377;&#29305;&#23450;&#20010;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.07550</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#35825;&#23548;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating and Inducing Personality in Pre-trained Language Models. (arXiv:2206.07550v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Machine Personality Inventory (MPI)&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#20010;&#24615;&#65292;&#36890;&#36807;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LLM)&#20855;&#26377;&#20010;&#24615;&#12290;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;Personality Prompting (P^2)&#26041;&#27861;&#65292;&#29992;&#20110;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#35825;&#23548;LLM&#20855;&#26377;&#29305;&#23450;&#20010;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#36215;&#28304;&#20110;&#21746;&#23398;&#25506;&#32034;&#65292;&#20851;&#27880;&#20010;&#20307;&#22312;&#24605;&#32771;&#12289;&#24773;&#24863;&#21644;&#34892;&#20026;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#26500;&#24314;&#33021;&#22815;&#19982;&#20154;&#31867;&#26085;&#24120;&#21512;&#20316;&#30340;&#31038;&#20132;&#26426;&#22120;&#65292;&#25105;&#20204;&#24819;&#30693;&#36947;&#65306;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#21542;&#25317;&#26377;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20010;&#24615;&#65311;&#22914;&#26524;&#26159;&#65292;&#25105;&#20204;&#22914;&#20309;&#35780;&#20272;&#23427;&#20204;&#65311;&#36827;&#19968;&#27493;&#22320;&#65292;&#22312;&#27492;&#35780;&#20272;&#26694;&#26550;&#30340;&#22522;&#30784;&#19978;&#65292;&#22914;&#20309;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#35825;&#23548;&#20855;&#26377;&#29305;&#23450;&#20010;&#24615;&#30340;&#35821;&#35328;&#27169;&#22411;&#65311;&#20026;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26426;&#22120;&#20010;&#24615;&#24211;(Machine Personality Inventory, MPI)&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#30340;&#20010;&#24615;&#12290;MPI&#36981;&#24490;&#26631;&#20934;&#21270;&#30340;&#20010;&#24615;&#27979;&#35797;&#65292;&#22522;&#20110;&#20116;&#22240;&#32032;&#20154;&#26684;&#29702;&#35770;&#21644;&#20154;&#26684;&#35780;&#20272;&#24211;&#24314;&#31435;&#12290;&#36890;&#36807;&#29992;MPI&#31995;&#32479;&#22320;&#35780;&#20272;LLM&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;LLM&#30340;&#20010;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#31181;&#20010;&#24615;&#25552;&#31034;(Personality Prompting, P^2)&#26041;&#27861;&#65292;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#35825;&#23548;LLMs&#20855;&#26377;&#29305;&#23450;&#30340;&#20010;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Originating as a philosophical quest, the study of personality concerns how individuals differ in thinking, feeling, and behaving. Towards building social machines that work with humans on a daily basis, we are motivated to ask: Do existing Large Language Models (LLMs) possess personalities akin to their human counterparts? If so, how can we evaluate them? Further, given this evaluation framework, how can we induce a particular personality in a controllable fashion? To answer these three questions, we propose the Machine Personality Inventory (MPI) dataset for evaluating the machine personality; MPI follows standardized personality tests, built upon the Big Five Personality Factors (Big Five) theory and personality assessment inventories. By systematically evaluating LLMs with MPI, we provide the first piece of evidence showing the existence of personality in LLMs. We further devise a Personality Prompting (P^2) method to induce LLMs with a specific personality in a controllable manner
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22686;&#24378;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#21028;&#21035;&#22120;&#29992;&#20110;&#23545;&#29983;&#25104;&#25968;&#25454;&#21450;&#20854;&#22686;&#24378;&#21442;&#25968;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#21028;&#21035;&#22120;&#30340;&#34920;&#29616;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2205.15677</link><description>&lt;p&gt;
&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#20013;&#30340;&#33258;&#25105;&#30417;&#30563;&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Augmentation-Aware Self-Supervision for Data-Efficient GAN Training. (arXiv:2205.15677v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22686;&#24378;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#21028;&#21035;&#22120;&#29992;&#20110;&#23545;&#29983;&#25104;&#25968;&#25454;&#21450;&#20854;&#22686;&#24378;&#21442;&#25968;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#21028;&#21035;&#22120;&#30340;&#34920;&#29616;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#21028;&#21035;&#22120;&#23481;&#26131;&#36807;&#25311;&#21512;&#12290;&#20808;&#21069;&#25552;&#20986;&#30340;&#21487;&#24494;&#22686;&#24378;&#25216;&#26415;&#25913;&#21892;&#20102;GAN&#35757;&#32451;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;&#20294;&#26159;&#65292;&#22686;&#24378;&#25216;&#26415;&#38544;&#24335;&#22320;&#24341;&#20837;&#20102;&#19981;&#33391;&#19981;&#21464;&#24615;&#22240;&#32032;&#65292;&#22240;&#20026;&#23427;&#24573;&#30053;&#20102;&#30001;&#25968;&#25454;&#36716;&#25442;&#24341;&#36215;&#30340;&#26631;&#31614;&#31354;&#38388;&#35821;&#20041;&#21464;&#21270;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#21028;&#21035;&#22120;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#26368;&#32456;&#24433;&#21709;&#29983;&#25104;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#20943;&#36731;&#19981;&#21464;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#21516;&#26102;&#32487;&#25215;&#25968;&#25454;&#22686;&#24378;&#30340;&#22909;&#22788;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#21028;&#21035;&#22120;&#65292;&#35813;&#21028;&#21035;&#22120;&#21487;&#20197;&#39044;&#27979;&#22686;&#24378;&#25968;&#25454;&#30340;&#21442;&#25968;&#12290;&#29305;&#21035;&#22320;&#65292;&#30495;&#23454;&#25968;&#25454;&#21644;&#29983;&#25104;&#25968;&#25454;&#30340;&#39044;&#27979;&#30446;&#26631;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38656;&#35201;&#21306;&#21035;&#24320;&#26469;&#12290;&#25105;&#20204;&#36824;&#40723;&#21169;&#29983;&#25104;&#22120;&#23545;&#25239;&#22320;&#29983;&#25104;&#20854;&#22686;&#24378;&#21442;&#25968;&#21487;&#20197;&#34987;&#21028;&#21035;&#22120;&#20934;&#30830;&#39044;&#27979;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22810;&#20449;&#24687;&#37327;&#21644;&#26356;&#39640;&#25928;&#30340;&#21028;&#21035;&#22120;&#65292;&#25552;&#39640;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training generative adversarial networks (GANs) with limited data is challenging because discriminator is prone to overfitting. Previously proposed differentiable augmentation demonstrates improved data efficiency of training GANs. However, the augmentation implicitly introduces undesired invariance to augmentation for the discriminator since it ignores the change of semantics in the label space caused by data transformation, which may limit the representation learning ability of the discriminator and ultimately affect the generative modeling performance of the generator. To mitigate the negative impact of invariance while inheriting the benefits of data augmentation, we propose a novel augmentation-aware self-supervised discriminator that predicts the augmentation parameter of the augmented data. Particularly, the prediction targets of real data and generated data are required to be distinguished since they are different during training. We further encourage the generator to adversari
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#27169;&#22411;&#65292;&#24182;&#25299;&#23637;&#20004;&#20010;&#22522;&#26412;&#30340;&#26102;&#24577;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#20219;&#21153;&#65292;&#21363;&#20869;&#25554;&#38142;&#25509;&#39044;&#27979;&#21644;&#22806;&#25554;&#38142;&#25509;&#39044;&#27979;&#21040;&#19968;&#27425;&#23398;&#20064;&#30340;&#22659;&#20917;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#37117;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.10621</link><description>&lt;p&gt;
&#23398;&#20064;&#19968;&#27425;&#20851;&#31995;&#20803;&#34920;&#31034;&#20197;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Meta Representations of One-shot Relations for Temporal Knowledge Graph Link Prediction. (arXiv:2205.10621v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#27169;&#22411;&#65292;&#24182;&#25299;&#23637;&#20004;&#20010;&#22522;&#26412;&#30340;&#26102;&#24577;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#20219;&#21153;&#65292;&#21363;&#20869;&#25554;&#38142;&#25509;&#39044;&#27979;&#21644;&#22806;&#25554;&#38142;&#25509;&#39044;&#27979;&#21040;&#19968;&#27425;&#23398;&#20064;&#30340;&#22659;&#20917;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#37117;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#38745;&#24577;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#23569;&#37327;&#20851;&#31995;&#23398;&#20064;&#21463;&#21040;&#20102;&#26356;&#22823;&#30340;&#20851;&#27880;&#65292;&#32780;&#23545;&#20110;&#26102;&#24577;&#30693;&#35782;&#22270;&#35889;&#30340;&#23569;&#37327;&#23398;&#20064;&#20960;&#20046;&#27809;&#26377;&#34987;&#30740;&#31350;&#12290;&#19982;&#30693;&#35782;&#22270;&#35889;&#30456;&#27604;&#65292;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#21253;&#21547;&#20016;&#23500;&#30340;&#26102;&#38388;&#20449;&#24687;&#65292;&#22240;&#27492;&#38656;&#35201;&#26102;&#38388;&#25512;&#29702;&#25216;&#26415;&#36827;&#34892;&#24314;&#27169;&#12290;&#36825;&#22312;&#26102;&#38388;&#32972;&#26223;&#19979;&#23398;&#20064;&#23569;&#37327;&#20851;&#31995;&#25552;&#20986;&#20102;&#26356;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#22522;&#20110;&#20043;&#21069;&#30340;&#38745;&#24577;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#23569;&#37327;&#20851;&#31995;&#23398;&#20064;&#65292;&#23558;&#20004;&#20010;&#22522;&#26412;&#30340;&#26102;&#24577;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#20219;&#21153;&#65288;&#21363;&#20869;&#25554;&#21644;&#22806;&#25554;&#38142;&#25509;&#39044;&#27979;&#65289;&#25512;&#24191;&#21040;&#19968;&#27425;&#23398;&#20064;&#30340;&#22659;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#26102;&#24577;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#19968;&#27425;&#24615;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20004;&#20010;&#20219;&#21153;&#30340;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot relational learning for static knowledge graphs (KGs) has drawn greater interest in recent years, while few-shot learning for temporal knowledge graphs (TKGs) has hardly been studied. Compared to KGs, TKGs contain rich temporal information, thus requiring temporal reasoning techniques for modeling. This poses a greater challenge in learning few-shot relations in the temporal context. In this paper, we follow the previous work that focuses on few-shot relational learning on static KGs and extend two fundamental TKG reasoning tasks, i.e., interpolated and extrapolated link prediction, to the one-shot setting. We propose four new large-scale benchmark datasets and develop a TKG reasoning model for learning one-shot relations in TKGs. Experimental results show that our model can achieve superior performance on all datasets in both TKG link prediction tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#28789;&#27963;&#30340;&#29615;&#22659;&#20013;&#24314;&#27169;&#27010;&#29575;&#28418;&#31227;&#12290;&#36890;&#36807;&#27169;&#25311;&#21021;&#22987;&#29289;&#20307;&#20301;&#32622;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20135;&#29983;&#29289;&#20307;&#20301;&#32622;&#30340;&#27010;&#29575;&#36712;&#36857;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#22312;&#24212;&#23545;&#19981;&#23436;&#20840;&#20449;&#24687;&#26102;&#39044;&#27979;&#20986;&#21512;&#29702;&#30340;&#28418;&#31227;&#27010;&#29575;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.05891</link><description>&lt;p&gt;
&#19968;&#31181;&#23398;&#20064;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;Lagrangian&#28418;&#31227;&#30340;DNN&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A DNN Framework for Learning Lagrangian Drift With Uncertainty. (arXiv:2204.05891v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.05891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#28789;&#27963;&#30340;&#29615;&#22659;&#20013;&#24314;&#27169;&#27010;&#29575;&#28418;&#31227;&#12290;&#36890;&#36807;&#27169;&#25311;&#21021;&#22987;&#29289;&#20307;&#20301;&#32622;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20135;&#29983;&#29289;&#20307;&#20301;&#32622;&#30340;&#27010;&#29575;&#36712;&#36857;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#22312;&#24212;&#23545;&#19981;&#23436;&#20840;&#20449;&#24687;&#26102;&#39044;&#27979;&#20986;&#21512;&#29702;&#30340;&#28418;&#31227;&#27010;&#29575;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#25454;&#20013;&#23384;&#22312;&#26410;&#35299;&#20915;&#30340;&#29289;&#29702;&#29616;&#35937;&#65292;&#20363;&#22914;&#28023;&#19978;&#36855;&#22833;&#30340;&#29289;&#20307;&#65292;&#23548;&#33268;Lagrangian&#28418;&#31227;&#30340;&#37325;&#24314;&#36890;&#24120;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#12290;&#36890;&#24120;&#36890;&#36807;&#24341;&#20837;&#28418;&#31227;&#30340;&#38543;&#26426;&#24615;&#26469;&#20811;&#26381;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#23545;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#20570;&#20986;&#29305;&#23450;&#30340;&#20551;&#35774;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#22312;&#28789;&#27963;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#24314;&#27169;&#27010;&#29575;&#28418;&#31227;&#65292;&#28040;&#38500;&#20102;&#36825;&#31181;&#32422;&#26463;&#12290;&#20351;&#29992;&#28023;&#27915;&#29615;&#27969;&#27169;&#25311;&#65292;&#36890;&#36807;&#27169;&#25311;&#21021;&#22987;&#29289;&#20307;&#20301;&#32622;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20135;&#29983;&#29289;&#20307;&#20301;&#32622;&#30340;&#27010;&#29575;&#36712;&#36857;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#28418;&#31227;&#27010;&#29575;&#20223;&#30495;&#22120;&#65292;&#32473;&#23450;&#23436;&#20840;&#24050;&#30693;&#30340;&#36895;&#24230;&#65292;&#21487;&#20197;&#23545;&#28418;&#31227;&#27010;&#29575;&#36827;&#34892;&#19968;&#22825;&#30340;&#39044;&#27979;&#65292;&#24182;&#35266;&#23519;&#21040;&#19982;&#25968;&#20540;&#27169;&#25311;&#30340;&#33391;&#22909;&#19968;&#33268;&#24615;&#12290;&#23581;&#35797;&#20102;&#20960;&#31181;&#25439;&#22833;&#20989;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#36755;&#20837;&#20449;&#24687;&#19981;&#23436;&#20840;&#30340;&#27169;&#22411;&#65292;&#23545;&#25105;&#20204;&#30340;&#26694;&#26550;&#36827;&#34892;&#20102;&#25512;&#24191;&#12290;&#22312;&#36825;&#20123;&#26356;&#38590;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#21512;&#29702;&#30340;&#39044;&#27979;&#65292;&#23613;&#31649;&#22312;&#35780;&#20272;&#27169;&#22411;&#26102;&#65292;&#25968;&#25454;&#28418;&#31227;&#30340;&#24433;&#21709;&#21464;&#24471;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reconstructions of Lagrangian drift, for example for objects lost at sea, are often uncertain due to unresolved physical phenomena within the data. Uncertainty is usually overcome by introducing stochasticity into the drift, but this approach requires specific assumptions for modelling uncertainty. We remove this constraint by presenting a purely data-driven framework for modelling probabilistic drift in flexible environments. Using ocean circulation model simulations, we generate probabilistic trajectories of object location by simulating uncertainty in the initial object position. We train an emulator of probabilistic drift over one day given perfectly known velocities and observe good agreement with numerical simulations. Several loss functions are tested. Then, we strain our framework by training models where the input information is imperfect. On these harder scenarios, we observe reasonable predictions although the effects of data drift become noticeable when evaluating the model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#26684;&#32467;&#26500;&#35782;&#21035;&#65288;TSR&#65289;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20026;&#32593;&#26684;&#34920;&#30456;&#20284;&#24230;&#65288;GriTS&#65289;&#12290;&#20351;&#29992;&#21551;&#21457;&#24335;&#31639;&#27861;&#35299;&#20915;&#20108;&#32500;&#26368;&#30456;&#20284;&#23376;&#32467;&#26500;&#65288;2D-MSS&#65289;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#29702;&#24819;&#12290;</title><link>http://arxiv.org/abs/2203.12555</link><description>&lt;p&gt;
GriTS: &#29992;&#20110;&#34920;&#26684;&#32467;&#26500;&#35782;&#21035;&#30340;&#32593;&#26684;&#34920;&#30456;&#20284;&#24230;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
GriTS: Grid table similarity metric for table structure recognition. (arXiv:2203.12555v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.12555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#26684;&#32467;&#26500;&#35782;&#21035;&#65288;TSR&#65289;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20026;&#32593;&#26684;&#34920;&#30456;&#20284;&#24230;&#65288;GriTS&#65289;&#12290;&#20351;&#29992;&#21551;&#21457;&#24335;&#31639;&#27861;&#35299;&#20915;&#20108;&#32500;&#26368;&#30456;&#20284;&#23376;&#32467;&#26500;&#65288;2D-MSS&#65289;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#29702;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34920;&#26684;&#32467;&#26500;&#35782;&#21035;&#65288;TSR&#65289;&#35780;&#20272;&#30340;&#26032;&#31867;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20026;&#32593;&#26684;&#34920;&#30456;&#20284;&#24230;&#65288;GriTS&#65289;&#12290;&#19982;&#20197;&#24448;&#30340;&#24230;&#37327;&#26631;&#20934;&#19981;&#21516;&#65292;GriTS&#30452;&#25509;&#20197;&#30697;&#38453;&#30340;&#33258;&#28982;&#24418;&#24335;&#35780;&#20272;&#39044;&#27979;&#34920;&#30340;&#27491;&#30830;&#24615;&#12290;&#20026;&#20102;&#21019;&#24314;&#30697;&#38453;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#25105;&#20204;&#23558;&#20108;&#32500;&#26368;&#38271;&#20844;&#20849;&#23376;&#32467;&#26500;&#65288;2D-LCS&#65289;&#38382;&#39064;&#25512;&#24191;&#21040;&#20108;&#32500;&#26368;&#30456;&#20284;&#23376;&#32467;&#26500;&#65288;2D-MSS&#65289;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#20135;&#29983;&#20102;&#30495;&#23454;&#30456;&#20284;&#24615;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;&#22312;&#23454;&#36341;&#20013;&#36825;&#20123;&#36793;&#30028;&#20043;&#38388;&#20960;&#20046;&#27809;&#26377;&#21306;&#21035;&#12290;&#25105;&#20204;&#23558;GriTS&#19982;&#20854;&#20182;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#20197;&#23454;&#39564;&#35777;&#26126;&#30697;&#38453;&#30456;&#20284;&#24615;&#22312;TSR&#24615;&#33021;&#35780;&#20272;&#26041;&#38754;&#27604;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#29702;&#24819;&#30340;&#34892;&#20026;&#12290;&#26368;&#21518;&#65292;GriTS&#23558;&#21333;&#20803;&#25299;&#25169;&#35782;&#21035;&#12289;&#21333;&#20803;&#23450;&#20301;&#35782;&#21035;&#21644;&#21333;&#20803;&#20869;&#23481;&#25552;&#21462;&#19977;&#20010;&#23376;&#20219;&#21153;&#32479;&#19968;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a new class of metric for table structure recognition (TSR) evaluation, called grid table similarity (GriTS). Unlike prior metrics, GriTS evaluates the correctness of a predicted table directly in its natural form as a matrix. To create a similarity measure between matrices, we generalize the two-dimensional largest common substructure (2D-LCS) problem, which is NP-hard, to the 2D most similar substructures (2D-MSS) problem and propose a polynomial-time heuristic for solving it. This algorithm produces both an upper and a lower bound on the true similarity between matrices. We show using evaluation on a large real-world dataset that in practice there is almost no difference between these bounds. We compare GriTS to other metrics and empirically validate that matrix similarity exhibits more desirable behavior than alternatives for TSR performance evaluation. Finally, GriTS unifies all three subtasks of cell topology recognition, cell location recognition, and c
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;GFlowNets&#20316;&#20026;&#29983;&#25104;&#22120;&#65292;&#32467;&#21512;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#22870;&#21169;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#22810;&#26679;&#30340;&#29983;&#29289;&#24207;&#21015;&#35774;&#35745;&#20505;&#36873;&#35299;&#65292;&#24182;&#25104;&#21151;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#29992;&#24615;&#21644;&#20449;&#24687;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.04115</link><description>&lt;p&gt;
&#21033;&#29992;GFlowNets&#36827;&#34892;&#29983;&#29289;&#24207;&#21015;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Biological Sequence Design with GFlowNets. (arXiv:2203.04115v3 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.04115
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;GFlowNets&#20316;&#20026;&#29983;&#25104;&#22120;&#65292;&#32467;&#21512;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#22870;&#21169;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#22810;&#26679;&#30340;&#29983;&#29289;&#24207;&#21015;&#35774;&#35745;&#20505;&#36873;&#35299;&#65292;&#24182;&#25104;&#21151;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#29992;&#24615;&#21644;&#20449;&#24687;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22836;&#35774;&#35745;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#29983;&#29289;&#24207;&#21015;&#65292;&#22914;&#34507;&#30333;&#36136;&#21644;DNA&#24207;&#21015;&#65292;&#36890;&#24120;&#28041;&#21450;&#21040;&#20960;&#36718;&#20998;&#23376;&#26500;&#24605;&#21644;&#26114;&#36149;&#30340;&#28287;&#23454;&#39564;&#35780;&#20272;&#36807;&#31243;&#12290;&#36825;&#20123;&#23454;&#39564;&#21487;&#33021;&#21253;&#21547;&#22810;&#20010;&#38454;&#27573;&#65292;&#38543;&#30528;&#35780;&#20272;&#31934;&#24230;&#21644;&#25104;&#26412;&#30340;&#22686;&#21152;&#65292;&#20505;&#36873;&#20154;&#20250;&#34987;&#36807;&#28388;&#31579;&#36873;&#12290;&#36825;&#20351;&#24471;&#22810;&#26679;&#24615;&#25104;&#20026;&#26500;&#24605;&#38454;&#27573;&#30340;&#19968;&#20010;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#26368;&#36817;&#25552;&#20986;&#30340;GFlowNets&#20316;&#20026;&#29983;&#25104;&#22810;&#26679;&#20505;&#36873;&#35299;&#30340;&#31639;&#27861;&#65292;&#26088;&#22312;&#22312;&#27599;&#36718;&#21518;&#33719;&#24471;&#19968;&#25209;&#26377;&#29992;&#30340;&#65288;&#20363;&#22914;&#65292;peptide&#30340;&#39044;&#27979;&#25239;&#24494;&#29983;&#29289;&#27963;&#24615;&#31561;&#21151;&#25928;&#20989;&#25968;&#23450;&#20041;&#65289;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#20505;&#36873;&#20154;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#26696;&#65292;&#23558;&#29616;&#26377;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#19982;&#22870;&#21169;&#20989;&#25968;&#19968;&#36215;&#21512;&#24182;&#21040;GFlowNets&#20013;&#21152;&#36895;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#23454;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Design of de novo biological sequences with desired properties, like protein and DNA sequences, often involves an active loop with several rounds of molecule ideation and expensive wet-lab evaluations. These experiments can consist of multiple stages, with increasing levels of precision and cost of evaluation, where candidates are filtered. This makes the diversity of proposed candidates a key consideration in the ideation phase. In this work, we propose an active learning algorithm leveraging epistemic uncertainty estimation and the recently proposed GFlowNets as a generator of diverse candidate solutions, with the objective to obtain a diverse batch of useful (as defined by some utility function, for example, the predicted anti-microbial activity of a peptide) and informative candidates after each round. We also propose a scheme to incorporate existing labeled datasets of candidates, in addition to a reward function, to speed up learning in GFlowNets. We present empirical results on 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;Facebook&#21644;&#30701;&#20449;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#23545;&#22522;&#20110;&#35821;&#35328;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#27867;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#24046;&#24322;&#24182;&#19981;&#20250;&#26174;&#33879;&#24433;&#21709;&#36825;&#20123;&#27169;&#22411;&#22312;&#36825;&#20004;&#20010;&#24179;&#21488;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.01802</link><description>&lt;p&gt;
Facebook&#21644;&#30701;&#20449;&#25991;&#26412;&#20043;&#38388;&#30340;&#19981;&#21516;&#26426;&#20250;&#19981;&#20250;&#22952;&#30861;&#22522;&#20110;&#35821;&#35328;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Different Affordances on Facebook and SMS Text Messaging Do Not Impede Generalization of Language-Based Predictive Models. (arXiv:2202.01802v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.01802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;Facebook&#21644;&#30701;&#20449;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#23545;&#22522;&#20110;&#35821;&#35328;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#27867;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#24046;&#24322;&#24182;&#19981;&#20250;&#26174;&#33879;&#24433;&#21709;&#36825;&#20123;&#27169;&#22411;&#22312;&#36825;&#20004;&#20010;&#24179;&#21488;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#30340;&#31227;&#21160;&#35774;&#22791;&#20581;&#24247;&#24178;&#39044;&#36890;&#24120;&#20351;&#29992;&#22312;&#38750;&#31227;&#21160;&#35774;&#22791;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#65292;&#30001;&#20110;&#25910;&#38598;&#22823;&#37327;&#30701;&#20449;&#25968;&#25454;&#30340;&#22256;&#38590;&#21644;&#39640;&#26114;&#30340;&#36153;&#29992;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#36825;&#20123;&#24179;&#21488;&#20043;&#38388;&#30340;&#27169;&#22411;&#24046;&#24322;&#21644;&#27867;&#21270;&#23545;&#20110;&#27491;&#30830;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20351;&#29992;120&#20010;&#20849;&#20139;&#36825;&#20004;&#31181;&#24179;&#21488;&#30340;&#29992;&#25143;&#26679;&#26412;&#65292;&#30740;&#31350;&#20102;Facebook&#21644;&#30701;&#20449;&#20043;&#38388;&#30340;&#24515;&#29702;&#35821;&#35328;&#24046;&#24322;&#20197;&#21450;&#20854;&#23545;&#39046;&#22495;&#22806;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#29992;&#25143;&#20351;&#29992;Facebook&#20998;&#20139;&#32463;&#39564;&#65288;&#22914;&#20241;&#38386;&#65289;&#65292;&#32780;&#20351;&#29992;&#30701;&#20449;&#36827;&#34892;&#20219;&#21153;&#23548;&#21521;&#21644;&#20250;&#35805;&#30446;&#30340;&#65288;&#20363;&#22914;&#35745;&#21010;&#30830;&#35748;&#65289;&#65292;&#21453;&#26144;&#20102;&#24046;&#24322;&#30340;&#26426;&#20250;&#12290;&#20026;&#20102;&#26816;&#39564;&#36825;&#20123;&#24046;&#24322;&#30340;&#19979;&#28216;&#25928;&#24212;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22522;&#20110;Facebook&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;Facebook&#21644;&#30701;&#20449;&#19978;&#20272;&#35745;&#24180;&#40836;&#65292;&#24615;&#21035;&#65292;&#25233;&#37057;&#65292;&#29983;&#27963;&#28385;&#24847;&#24230;&#21644;&#21387;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;Facebook&#21644;&#30701;&#20449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20013;&#27809;&#26377;&#26174;&#30528;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive mobile device-based health interventions often use machine learning models trained on non-mobile device data, such as social media text, due to the difficulty and high expense of collecting large text message (SMS) data. Therefore, understanding the differences and generalization of models between these platforms is crucial for proper deployment. We examined the psycho-linguistic differences between Facebook and text messages, and their impact on out-of-domain model performance, using a sample of 120 users who shared both. We found that users use Facebook for sharing experiences (e.g., leisure) and SMS for task-oriented and conversational purposes (e.g., plan confirmations), reflecting the differences in the affordances. To examine the downstream effects of these differences, we used pre-trained Facebook-based language models to estimate age, gender, depression, life satisfaction, and stress on both Facebook and SMS. We found no significant differences in correlations between 
&lt;/p&gt;</description></item><item><title>Fenrir&#26159;&#19968;&#31181;&#36890;&#36807;&#27010;&#29575;&#25968;&#20540;&#26041;&#27861;&#23558;&#21021;&#22987;&#20540;&#38382;&#39064;&#36716;&#21270;&#20026;&#39640;&#26031;-&#39532;&#23572;&#31185;&#22827;&#22238;&#24402;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20351;&#24494;&#20998;&#26041;&#31243;&#21442;&#25968;&#20272;&#35745;&#20219;&#21153;&#26356;&#23481;&#26131;&#35299;&#20915;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#37096;&#20998;&#35266;&#27979;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#26576;&#20123;&#36867;&#33073;&#23616;&#37096;&#26368;&#20248;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2202.01287</link><description>&lt;p&gt;
Fenrir: &#29289;&#29702;&#22686;&#24378;&#21021;&#22987;&#21270;&#38382;&#39064;&#22238;&#24402;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fenrir: Physics-Enhanced Regression for Initial Value Problems. (arXiv:2202.01287v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.01287
&lt;/p&gt;
&lt;p&gt;
Fenrir&#26159;&#19968;&#31181;&#36890;&#36807;&#27010;&#29575;&#25968;&#20540;&#26041;&#27861;&#23558;&#21021;&#22987;&#20540;&#38382;&#39064;&#36716;&#21270;&#20026;&#39640;&#26031;-&#39532;&#23572;&#31185;&#22827;&#22238;&#24402;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20351;&#24494;&#20998;&#26041;&#31243;&#21442;&#25968;&#20272;&#35745;&#20219;&#21153;&#26356;&#23481;&#26131;&#35299;&#20915;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#37096;&#20998;&#35266;&#27979;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#26576;&#20123;&#36867;&#33073;&#23616;&#37096;&#26368;&#20248;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#27010;&#29575;&#25968;&#20540;&#26041;&#27861;&#23558;&#21021;&#22987;&#20540;&#38382;&#39064;&#36716;&#21270;&#20026;&#30001;&#21021;&#22987;&#20540;&#38382;&#39064;&#21160;&#21147;&#23398;&#21442;&#25968;&#21270;&#30340;&#39640;&#26031;-&#39532;&#23572;&#31185;&#22827;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#24120;&#35265;&#30340;&#24494;&#20998;&#26041;&#31243;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#34987;&#31616;&#21270;&#20026;&#39640;&#26031;-&#39532;&#23572;&#31185;&#22827;&#22238;&#24402;&#30340;&#36229;&#21442;&#25968;&#20272;&#35745;&#65292;&#36825;&#24448;&#24448;&#26356;&#23481;&#26131;&#35299;&#20915;&#12290;&#25105;&#20204;&#38416;&#36848;&#20102;&#35813;&#26041;&#27861;&#19982;&#32463;&#20856;&#25968;&#20540;&#31215;&#20998;&#21644;&#26799;&#24230;&#21305;&#37197;&#26041;&#27861;&#30340;&#20851;&#31995;&#21644;&#20248;&#21183;&#12290;&#29305;&#21035;&#22320;&#65292;&#19982;&#26799;&#24230;&#21305;&#37197;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#37096;&#20998;&#35266;&#27979;&#65292;&#24182;&#20855;&#26377;&#26576;&#20123;&#21487;&#36867;&#33073;&#32463;&#20856;&#25968;&#20540;&#31215;&#20998;&#23616;&#37096;&#26368;&#20248;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19982;&#31454;&#20105;&#26041;&#27861;&#30456;&#24403;&#25110;&#31245;&#24494;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show how probabilistic numerics can be used to convert an initial value problem into a Gauss--Markov process parametrised by the dynamics of the initial value problem. Consequently, the often difficult problem of parameter estimation in ordinary differential equations is reduced to hyperparameter estimation in Gauss--Markov regression, which tends to be considerably easier. The method's relation and benefits in comparison to classical numerical integration and gradient matching approaches is elucidated. In particular, the method can, in contrast to gradient matching, handle partial observations, and has certain routes for escaping local optima not available to classical numerical integration. Experimental results demonstrate that the method is on par or moderately better than competing approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DSEE&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26435;&#37325;&#26356;&#26032;&#21644;&#26368;&#32456;&#27169;&#22411;&#26435;&#37325;&#30340;&#31232;&#30095;&#20808;&#39564;&#65292;&#20197;&#23454;&#29616;&#36164;&#28304;&#21644;&#21442;&#25968;&#25928;&#29575;&#39640;&#30340;&#24494;&#35843;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DSEE&#21487;&#26174;&#33879;&#20943;&#23569;&#21442;&#25968;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#24494;&#35843;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2111.00160</link><description>&lt;p&gt;
DSEE&#65306;&#21452;&#31232;&#30095;&#23884;&#20837;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
DSEE: Dually Sparsity-embedded Efficient Tuning of Pre-trained Language Models. (arXiv:2111.00160v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.00160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DSEE&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26435;&#37325;&#26356;&#26032;&#21644;&#26368;&#32456;&#27169;&#22411;&#26435;&#37325;&#30340;&#31232;&#30095;&#20808;&#39564;&#65292;&#20197;&#23454;&#29616;&#36164;&#28304;&#21644;&#21442;&#25968;&#25928;&#29575;&#39640;&#30340;&#24494;&#35843;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DSEE&#21487;&#26174;&#33879;&#20943;&#23569;&#21442;&#25968;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#24494;&#35843;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24040;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26680;&#24515;&#65292;&#26159;&#24494;&#35843;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#36215;&#28857;&#12290;&#20294;&#35813;&#33539;&#20363;&#20173;&#23384;&#22312;&#20004;&#20010;&#30171;&#28857;&#65306;&#65288;a&#65289;&#38543;&#30528;&#39044;&#35757;&#32451;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22823;&#65288;&#20363;&#22914;&#65292;GPT-3&#26377;175B&#20010;&#21442;&#25968;&#65289;&#65292;&#21363;&#20351;&#26159;&#24494;&#35843;&#30340;&#36807;&#31243;&#20063;&#21487;&#33021;&#32791;&#26102;&#21644;&#35745;&#31639;&#36164;&#28304;&#23494;&#38598;&#65307;&#65288;b&#65289;&#40664;&#35748;&#24773;&#20917;&#19979;&#65292;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#19982;&#20854;&#36215;&#28857;&#19968;&#26679;&#22823;&#65292;&#36825;&#26082;&#19981;&#26126;&#26234;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#26356;&#19987;&#19994;&#30340;&#21151;&#33021;&#65292;&#20063;&#19981;&#23454;&#29992;&#65292;&#22240;&#20026;&#35768;&#22810;&#24494;&#35843;&#27169;&#22411;&#23558;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#30171;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#21644;&#21442;&#25968;&#26377;&#25928;&#30340;&#24494;&#35843;&#26694;&#26550;&#65292;&#21033;&#29992;&#26435;&#37325;&#26356;&#26032;&#21644;&#26368;&#32456;&#27169;&#22411;&#26435;&#37325;&#20013;&#30340;&#31232;&#30095;&#20808;&#39564;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21517;&#20026;&#21452;&#31232;&#30095;&#23884;&#20837;&#39640;&#25928;&#35843;&#20248;&#65288;DSEE&#65289;&#65292;&#26088;&#22312;&#23454;&#29616;&#20004;&#20010;&#20851;&#38190;&#30446;&#26631;&#65306;&#65288;i&#65289;&#36890;&#36807;&#24378;&#21046;&#31232;&#30095;&#24863;&#30693;&#20302;&#31209;&#36924;&#36817;&#39044;&#35757;&#32451;&#27169;&#22411;&#23454;&#29616;&#21442;&#25968;&#26377;&#25928;&#30340;&#24494;&#35843;&#65292;&#65288;ii&#65289;&#36890;&#36807;&#22312;&#24494;&#35843;&#26399;&#38388;&#21033;&#29992;&#31232;&#30095;&#24863;&#30693;&#26816;&#26597;&#28857;&#21644;&#22312;&#26368;&#32456;&#27169;&#22411;&#26435;&#37325;&#20013;&#20351;&#29992;&#21160;&#24577;&#31232;&#30095;&#26469;&#23454;&#29616;&#36164;&#28304;&#26377;&#25928;&#30340;&#24494;&#35843;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DSEE&#21487;&#22312;&#26174;&#33879;&#20943;&#23569;&#21442;&#25968;&#21644;&#25512;&#29702;&#26102;&#38388;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#24494;&#35843;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gigantic pre-trained models have become central to natural language processing (NLP), serving as the starting point for fine-tuning towards a range of downstream tasks. However, two pain points persist for this paradigm: (a) as the pre-trained models grow bigger (e.g., 175B parameters for GPT-3), even the fine-tuning process can be time-consuming and computationally expensive; (b) the fine-tuned model has the same size as its starting point by default, which is neither sensible due to its more specialized functionality, nor practical since many fine-tuned models will be deployed in resource-constrained environments. To address these pain points, we propose a framework for resource- and parameter-efficient fine-tuning by leveraging the sparsity prior in both weight updates and the final model weights. Our proposed framework, dubbed Dually Sparsity-Embedded Efficient Tuning (DSEE), aims to achieve two key objectives: (i) parameter efficient fine-tuning - by enforcing sparsity-aware low-r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;AI&#31639;&#27861;&#30340;&#22522;&#20110;MALDI-ToF&#36136;&#35889;&#30340;COVID-19&#35786;&#26029;&#26041;&#27861;&#65292;&#20197;&#25552;&#20379;&#20934;&#30830;&#12289;&#32463;&#27982;&#23454;&#24800;&#19988;&#24555;&#36895;&#30340;COVID-19&#26816;&#27979;&#26381;&#21153;&#12290;</title><link>http://arxiv.org/abs/2109.14099</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110; MALDI-ToF &#36136;&#35889;&#30340; COVID-19 &#35786;&#26029; AI &#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Explainable-AI approach for Diagnosis of COVID-19 using MALDI-ToF Mass Spectrometry. (arXiv:2109.14099v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.14099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;AI&#31639;&#27861;&#30340;&#22522;&#20110;MALDI-ToF&#36136;&#35889;&#30340;COVID-19&#35786;&#26029;&#26041;&#27861;&#65292;&#20197;&#25552;&#20379;&#20934;&#30830;&#12289;&#32463;&#27982;&#23454;&#24800;&#19988;&#24555;&#36895;&#30340;COVID-19&#26816;&#27979;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20005;&#37325;&#24613;&#24615;&#21628;&#21560;&#32508;&#21512;&#30151;&#20896;&#29366;&#30149;&#27602;&#31867;&#22411;2&#65288;SARS-CoV-2&#65289;&#24341;&#36215;&#20102;&#20840;&#29699;&#22823;&#27969;&#34892;&#65292;&#24182;&#26497;&#22823;&#22320;&#24433;&#21709;&#20102;&#20840;&#29699;&#32463;&#27982;&#12290;&#20934;&#30830;&#12289;&#32463;&#27982;&#23454;&#24800;&#19988;&#24555;&#36895;&#30340;&#26816;&#27979;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#23545;&#35782;&#21035;&#24863;&#26579;&#32773;&#21644;&#20943;&#36731;&#20256;&#25773;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#21457;&#24067;&#20102;&#22810;&#31181;&#21478;&#31867;&#30340;&#26816;&#27979; COVID-19 &#30340;&#24179;&#21488;&#65292;&#36825;&#20123;&#24179;&#21488;&#19982;&#24403;&#21069;&#40644;&#37329;&#26631;&#20934;&#23454;&#26102;&#32858;&#21512;&#37238;&#38142;&#21453;&#24212;&#65288;RT-PCR&#65289;&#32467;&#26524;&#20855;&#26377;&#39640;&#24230;&#19968;&#33268;&#24615;&#12290;&#36825;&#20123;&#26032;&#26041;&#27861;&#19981;&#38656;&#35201;&#40763;&#21693;&#25325;&#23376;&#65288;NP&#65289;&#65292;&#20813;&#38500;&#20102;&#22797;&#26434;&#35797;&#21058;&#30340;&#38656;&#35201;&#65292;&#24182;&#20943;&#36731;&#20102; RT-PCR &#35797;&#21058;&#30340;&#20379;&#24212;&#36127;&#25285;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#65288;AI&#65289;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#25552;&#20379;&#32467;&#26524;&#20449;&#24515;&#12290;&#24403;&#21069; COVID-19 &#30740;&#31350;&#20013;&#30340; AI &#24212;&#29992;&#36890;&#24120;&#32570;&#20047;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#29983;&#29289;&#22522;&#30784;&#65292;&#25105;&#20204;&#30340; AI &#26041;&#27861;&#26159;&#26368;&#26089;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340; AI&#65288;X-AI&#65289;&#31639;&#27861;&#36827;&#34892; COVID-19 &#35786;&#26029;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#20351;&#29992;&#36136;&#35889;&#12290;
&lt;/p&gt;
&lt;p&gt;
The severe acute respiratory syndrome coronavirus type-2 (SARS-CoV-2) caused a global pandemic and immensely affected the global economy. Accurate, cost-effective, and quick tests have proven substantial in identifying infected people and mitigating the spread. Recently, multiple alternative platforms for testing coronavirus disease 2019 (COVID-19) have been published that show high agreement with current gold standard real-time polymerase chain reaction (RT-PCR) results. These new methods do away with nasopharyngeal (NP) swabs, eliminate the need for complicated reagents, and reduce the burden on RT-PCR test reagent supply. In the present work, we have designed an artificial intelligence-based (AI) testing method to provide confidence in the results. Current AI applications for COVID-19 studies often lack a biological foundation in the decision-making process, and our AI approach is one of the earliest to leverage explainable AI (X-AI) algorithms for COVID-19 diagnosis using mass spec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#25506;&#32034;&#26041;&#27861;&#20197;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#22870;&#21169;&#31232;&#23569;&#26102;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#39640;&#20445;&#30495;&#24230;&#30340;&#24037;&#19994;&#32423;&#27169;&#25311;&#22120;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30456;&#27604;&#29616;&#26377;&#31639;&#27861;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2109.12509</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#28145;&#24230;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Deep Exploration for Recommendation Systems. (arXiv:2109.12509v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.12509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#25506;&#32034;&#26041;&#27861;&#20197;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#22870;&#21169;&#31232;&#23569;&#26102;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#39640;&#20445;&#30495;&#24230;&#30340;&#24037;&#19994;&#32423;&#27169;&#25311;&#22120;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30456;&#27604;&#29616;&#26377;&#31639;&#27861;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#24212;&#20174;&#24310;&#36831;&#21453;&#39304;&#20013;&#25506;&#32034;&#21644;&#23398;&#20064;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#24448;&#24448;&#20391;&#37325;&#20110;&#20174;&#29992;&#25143;&#23545;&#21333;&#20010;&#25512;&#33616;&#30340;&#21709;&#24212;&#20013;&#23398;&#20064;&#12290;&#36825;&#20123;&#24037;&#20316;&#21033;&#29992;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20294;&#25918;&#24323;&#20102;&#23398;&#20064;&#29992;&#25143;&#20043;&#21518;&#30340;&#34892;&#20026;&#12290;&#22312;&#36807;&#21435;&#30340;&#24037;&#20316;&#20013;&#65292;&#34429;&#28982;&#33268;&#21147;&#20110;&#20174;&#38543;&#21518;&#30340;&#34892;&#20026;&#20013;&#23398;&#20064;&#65292;&#20294;&#32570;&#20047;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#24341;&#23548;&#24182;&#33719;&#21462;&#26377;&#24847;&#20041;&#30340;&#24310;&#36831;&#21453;&#39304;&#12290;&#24403;&#22870;&#21169;&#36739;&#23569;&#26102;&#65292;&#36890;&#36807;&#24341;&#23548;&#25506;&#32034;&#26377;&#24847;&#20041;&#30340;&#24310;&#36831;&#21453;&#39304;&#21464;&#24471;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;&#25512;&#33616;&#31995;&#32479;&#24320;&#21457;&#20102;&#28145;&#24230;&#25506;&#32034;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25512;&#33616;&#31995;&#32479;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#28145;&#24230;&#25506;&#32034;&#26041;&#27861;&#22312;&#21333;&#27493;&#25506;&#32034;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26159;&#22312;&#39640;&#20445;&#30495;&#24230;&#30340;&#24037;&#19994;&#32423;&#27169;&#25311;&#22120;&#19979;&#36827;&#34892;&#30340;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30456;&#27604;&#29616;&#26377;&#31639;&#27861;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern recommendation systems ought to benefit by probing for and learning from delayed feedback. Research has tended to focus on learning from a user's response to a single recommendation. Such work, which leverages methods of supervised and bandit learning, forgoes learning from the user's subsequent behavior. Where past work has aimed to learn from subsequent behavior, there has been a lack of effective methods for probing to elicit informative delayed feedback. Effective exploration through probing for delayed feedback becomes particularly challenging when rewards are sparse. To address this, we develop deep exploration methods for recommendation systems. In particular, we formulate recommendation as a sequential decision problem and demonstrate benefits of deep exploration over single-step exploration. Our experiments are carried out with high-fidelity industrial-grade simulators and establish large improvements over existing algorithms.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#22240;&#26524;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26497;&#23567;&#21270;&#20998;&#26512;&#65292;&#26088;&#22312;&#22238;&#31572;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36716;&#31227;&#31283;&#23450;&#20449;&#24687;&#26102;&#24212;&#35813;&#36716;&#31227;&#21738;&#20010;&#23376;&#38598;&#20174;&#32780;&#36798;&#21040;&#26368;&#20339;&#30340;&#27867;&#21270;&#33021;&#21147;&#36825;&#19968;&#38382;&#39064;</title><link>http://arxiv.org/abs/2107.01876</link><description>&lt;p&gt;
&#25105;&#20204;&#24212;&#35813;&#36716;&#31227;&#21738;&#31181;&#19981;&#21464;&#24615;&#65311;&#19968;&#31181;&#22240;&#26524;&#26497;&#23567;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Which Invariance Should We Transfer? A Causal Minimax Learning Approach. (arXiv:2107.01876v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.01876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#22240;&#26524;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26497;&#23567;&#21270;&#20998;&#26512;&#65292;&#26088;&#22312;&#22238;&#31572;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36716;&#31227;&#31283;&#23450;&#20449;&#24687;&#26102;&#24212;&#35813;&#36716;&#31227;&#21738;&#20010;&#23376;&#38598;&#20174;&#32780;&#36798;&#21040;&#26368;&#20339;&#30340;&#27867;&#21270;&#33021;&#21147;&#36825;&#19968;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26080;&#27861;&#21487;&#38752;&#24212;&#23545;&#25968;&#25454;&#38598;&#21464;&#21270;&#65292;&#22240;&#27492;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#35797;&#22270;&#23558;&#31283;&#23450;&#20449;&#24687;&#36716;&#31227;&#21040;&#30475;&#19981;&#35265;&#30340;&#29615;&#22659;&#20013;&#12290;&#29305;&#21035;&#22320;&#65292;&#22522;&#20110;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#30340;&#26041;&#27861;&#36890;&#36807;do-operator&#28040;&#38500;&#21487;&#21464;&#30340;&#22240;&#26524;&#26426;&#21046;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#24471;&#21040;&#30340;&#31283;&#23450;&#39044;&#27979;&#22240;&#20026;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#35782;&#21035;&#31283;&#23450;&#20449;&#24687;&#32780;&#26356;&#21152;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65306;&#20026;&#20102;&#36798;&#21040;&#26368;&#20339;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24212;&#35813;&#36716;&#31227;&#36825;&#25972;&#20010;&#31283;&#23450;&#20449;&#24687;&#20013;&#30340;&#21738;&#20010;&#23376;&#38598;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#22240;&#26524;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26497;&#23567;&#21270;&#20998;&#26512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#21028;&#26029;&#25972;&#20010;&#31283;&#23450;&#38598;&#26159;&#21542;&#26368;&#20248;&#30340;&#22270;&#24418;&#26465;&#20214;&#12290;&#24403;&#36825;&#20010;&#26465;&#20214;&#22833;&#36133;&#26102;&#65292;&#25105;&#20204;&#24778;&#35766;&#22320;&#21457;&#29616;&#65292;&#36890;&#36807;&#19968;&#20010;&#20363;&#23376;&#65292;&#36825;&#20010;&#25972;&#20010;&#31283;&#23450;&#38598;&#34429;&#28982;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#31283;&#23450;&#20449;&#24687;&#65292;&#20294;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#36716;&#31227;&#38598;&#12290;&#20026;&#20102;&#30830;&#23450;&#26368;&#20248;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22240;&#26524;&#26368;&#23567;&#21547;&#20041;&#30340;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#20223;&#30495;&#21644;&#23454;&#38469;&#25968;&#25454;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major barrier to deploying current machine learning models lies in their non-reliability to dataset shifts. To resolve this problem, most existing studies attempted to transfer stable information to unseen environments. Particularly, independent causal mechanisms-based methods proposed to remove mutable causal mechanisms via the do-operator. Compared to previous methods, the obtained stable predictors are more effective in identifying stable information. However, a key question remains: which subset of this whole stable information should the model transfer, in order to achieve optimal generalization ability? To answer this question, we present a comprehensive minimax analysis from a causal perspective. Specifically, we first provide a graphical condition for the whole stable set to be optimal. When this condition fails, we surprisingly find with an example that this whole stable set, although can fully exploit stable information, is not the optimal one to transfer. To identify the o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#22120;&#30340;&#26032;&#30340;DAG&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#20351;&#20851;&#32852;&#38750;&#32447;&#24615;&#19988;&#25968;&#25454;&#26102;&#21464;&#21487;&#34892;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#27169;&#25311;&#21644;&#33041;&#36830;&#25509;&#32593;&#32476;&#20998;&#26512;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2106.01474</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#21270;&#12289;&#30417;&#30563;&#21644;&#29983;&#25104;&#23545;&#25239;&#23398;&#20064;&#27979;&#35797;&#26377;&#21521;&#26080;&#29615;&#22270;
&lt;/p&gt;
&lt;p&gt;
Testing Directed Acyclic Graph via Structural, Supervised and Generative Adversarial Learning. (arXiv:2106.01474v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.01474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#22120;&#30340;&#26032;&#30340;DAG&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#20351;&#20851;&#32852;&#38750;&#32447;&#24615;&#19988;&#25968;&#25454;&#26102;&#21464;&#21487;&#34892;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#27169;&#25311;&#21644;&#33041;&#36830;&#25509;&#32593;&#32476;&#20998;&#26512;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#12290;&#34429;&#28982;&#26377;&#19968;&#22823;&#31867;DAG&#20272;&#35745;&#26041;&#27861;&#65292;&#20294;DAG&#25512;&#26029;&#35299;&#20915;&#26041;&#26696;&#30456;&#23545;&#36739;&#23569;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#24378;&#21152;&#19968;&#20123;&#29305;&#23450;&#30340;&#27169;&#22411;&#32467;&#26500;&#65292;&#22914;&#32447;&#24615;&#27169;&#22411;&#25110;&#21487;&#21152;&#27169;&#22411;&#65292;&#24182;&#20551;&#35774;&#25968;&#25454;&#35266;&#27979;&#29420;&#31435;&#12290;&#25105;&#20204;&#30340;&#27979;&#35797;&#26041;&#27861;&#20801;&#35768;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#32852;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#25968;&#25454;&#26159;&#26102;&#21464;&#30340;&#12290;&#25105;&#20204;&#22522;&#20110;&#19968;&#20123;&#39640;&#24230;&#28789;&#27963;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#22120;&#26500;&#24314;&#20102;&#36825;&#20010;&#27979;&#35797;&#12290;&#22312;&#20801;&#35768;&#27599;&#20010;&#21463;&#35797;&#32773;&#30340;&#21463;&#35797;&#20154;&#25968;&#25110;&#26102;&#38388;&#28857;&#25968;&#21457;&#25955;&#20026;&#26080;&#31351;&#22823;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#27979;&#35797;&#30340;&#28176;&#36817;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#21644;&#33041;&#36830;&#25509;&#32593;&#32476;&#20998;&#26512;&#23637;&#31034;&#20102;&#27979;&#35797;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we propose a new hypothesis testing method for directed acyclic graph (DAG). While there is a rich class of DAG estimation methods, there is a relative paucity of DAG inference solutions. Moreover, the existing methods often impose some specific model structures such as linear models or additive models, and assume independent data observations. Our proposed test instead allows the associations among the random variables to be nonlinear and the data to be time-dependent. We build the test based on some highly flexible neural networks learners. We establish the asymptotic guarantees of the test, while allowing either the number of subjects or the number of time points for each subject to diverge to infinity. We demonstrate the efficacy of the test through simulations and a brain connectivity network analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22240;&#26524;&#27169;&#24335;&#26469;&#26816;&#26597;&#36755;&#20837;&#21464;&#37327;&#22522;&#20934;&#20540;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#35745;&#31639;Shapley&#20540;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26368;&#20248;&#22522;&#20934;&#20540;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2105.10719</link><description>&lt;p&gt;
&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#22320;&#34920;&#31034;&#25513;&#30721;&#29366;&#24577;&#20197;&#35745;&#31639;DNN&#30340;Shapley&#20540;?
&lt;/p&gt;
&lt;p&gt;
Can We Faithfully Represent Masked States to Compute Shapley Values on a DNN?. (arXiv:2105.10719v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.10719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22240;&#26524;&#27169;&#24335;&#26469;&#26816;&#26597;&#36755;&#20837;&#21464;&#37327;&#22522;&#20934;&#20540;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#35745;&#31639;Shapley&#20540;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26368;&#20248;&#22522;&#20934;&#20540;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#25513;&#30422;&#19968;&#20123;&#36755;&#20837;&#21464;&#37327;&#65292;&#24182;&#35745;&#31639;&#25513;&#30422;&#21518;&#30340;&#36755;&#20837;&#26679;&#26412;&#30340;&#36755;&#20986;&#21464;&#21270;&#65292;&#26159;&#35745;&#31639;&#26679;&#26412;&#36755;&#20837;&#21464;&#37327;&#23646;&#24615;&#30340;&#20856;&#22411;&#26041;&#27861;&#12290;&#20154;&#20204;&#36890;&#24120;&#20351;&#29992;&#22522;&#20934;&#20540;&#26469;&#23631;&#34109;&#36755;&#20837;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#29702;&#35770;&#26469;&#39564;&#35777;&#22522;&#20934;&#20540;&#26159;&#21542;&#20934;&#30830;&#22320;&#20195;&#34920;&#20102;&#36755;&#20837;&#21464;&#37327;&#32570;&#22833;&#30340;&#24773;&#20917;&#65292;&#21363;&#21435;&#38500;&#20102;&#36755;&#20837;&#21464;&#37327;&#30340;&#25152;&#26377;&#20449;&#21495;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;DNN&#30340;&#25512;&#29702;&#24471;&#20998;&#21487;&#20197;&#20005;&#26684;&#22320;&#20998;&#35299;&#20026;DNN&#32534;&#30721;&#30340;&#19968;&#32452;&#22240;&#26524;&#27169;&#24335;&#65288;&#25110;&#27010;&#24565;&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#22240;&#26524;&#27169;&#24335;&#26469;&#26816;&#26597;&#22522;&#20934;&#20540;&#30340;&#20934;&#30830;&#24615;&#12290;&#26356;&#20026;&#37325;&#35201;&#30340;&#26159;&#65292;&#24050;&#32463;&#35777;&#26126;&#22240;&#26524;&#27169;&#24335;&#21487;&#20197;&#35299;&#37322;&#20026;Shapley&#20540;&#30340;&#22522;&#26412;&#29702;&#30001;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26368;&#20248;&#22522;&#20934;&#20540;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masking some input variables of a deep neural network (DNN) and computing output changes on the masked input sample represent a typical way to compute attributions of input variables in the sample. People usually mask an input variable using its baseline value. However, there is no theory to examine whether baseline value faithfully represents the absence of an input variable, \emph{i.e.,} removing all signals from the input variable. Fortunately, recent studies show that the inference score of a DNN can be strictly disentangled into a set of causal patterns (or concepts) encoded by the DNN. Therefore, we propose to use causal patterns to examine the faithfulness of baseline values. More crucially, it is proven that causal patterns can be explained as the elementary rationale of the Shapley value. Furthermore, we propose a method to learn optimal baseline values, and experimental results have demonstrated its effectiveness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#26799;&#24230;&#19979;&#38477;&#23545;&#25239;&#35757;&#32451;&#30340;&#36830;&#32493;&#26102;&#38388;&#36924;&#36817;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#26032;&#30340;&#26799;&#24230;&#27969;&#35270;&#35282;&#29702;&#35770;&#19978;&#35777;&#23454;&#20102;&#23545;&#25239;&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2105.08037</link><description>&lt;p&gt;
&#26799;&#24230;&#19979;&#38477;&#23545;&#25239;&#35757;&#32451;&#30340;&#36830;&#32493;&#26102;&#38388;&#36924;&#36817;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Adversarial Training for Gradient Descent: Analysis Through its Continuous-time Approximation. (arXiv:2105.08037v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.08037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#26799;&#24230;&#19979;&#38477;&#23545;&#25239;&#35757;&#32451;&#30340;&#36830;&#32493;&#26102;&#38388;&#36924;&#36817;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#26032;&#30340;&#26799;&#24230;&#27969;&#35270;&#35282;&#29702;&#35770;&#19978;&#35777;&#23454;&#20102;&#23545;&#25239;&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#24050;&#25104;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#26799;&#24230;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#31561;&#39046;&#22495;&#20013;&#26368;&#26377;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;&#20043;&#19968;&#65292;&#21487;&#20197;&#26377;&#25928;&#23545;&#25239;&#25968;&#25454;&#28857;&#19978;&#30340;&#23545;&#25239;&#25200;&#21160;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#23545;&#25239;&#35757;&#32451;&#30340; mini-max &#28216;&#25103;&#30340;&#36830;&#32493;&#26102;&#38388;&#36924;&#36817;&#65292;&#35813;&#36924;&#36817;&#26041;&#27861;&#20801;&#35768;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#20854;&#23545;&#25239;&#35757;&#32451;&#23545;&#24212;&#26041;&#27861;&#20043;&#38388;&#36827;&#34892;&#31934;&#30830;&#21644;&#20998;&#26512;&#24615;&#30340;&#27604;&#36739;&#65292;&#24182;&#20174;&#26032;&#30340;&#26799;&#24230;&#27969;&#35270;&#35282;&#29702;&#35770;&#19978;&#35777;&#23454;&#20102;&#23545;&#25239;&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#22810;&#31181;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#35777;&#23454;&#20102;&#19978;&#36848;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training has gained great popularity as one of the most effective defenses for deep neural network and more generally for gradient-based machine learning models against adversarial perturbations on data points. This paper establishes a continuous-time approximation for the mini-max game of adversarial training. This approximation approach allows for precise and analytical comparisons between stochastic gradient descent and its adversarial training counterpart; and confirms theoretically the robustness of adversarial training from a new gradient-flow viewpoint. The analysis is then corroborated through various analytical and numerical examples.
&lt;/p&gt;</description></item></channel></rss>